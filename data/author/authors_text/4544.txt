Architecture and Design Considerations in NESPOLE!:
a Speech Translation System for E-commerce Applications
Alon Lavie,
Chad Langley,
Alex Waibel
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
Fabio Pianesi,
Gianni Lazzari,
Paolo Coletti
ITC-irst
Trento, Italy
Loredana Taddei,
Franco Balducci
AETHRA
Ancona, Italy
1. INTRODUCTION
NESPOLE! 1 is a speech-to-speech machine translation research
project funded jointly by the European Commission and the US
NSF. The main goal of the NESPOLE! project is to advance the
state-of-the-art of speech-to-speech translation in a real-world set-
ting of common users involved in e-commerce applications. The
project is a collaboration between three European research labs
(IRST in Trento Italy, ISL at University of Karlsruhe in Germany,
CLIPS at UJF in Grenoble France), a US research group (ISL at
Carnegie Mellon in Pittsburgh) and two industrial partners (APT
- the Trentino provincial tourism bureau, and Aethra - an Italian
tele-communications commercial company). The speech-to-speech
translation approach taken by the project builds upon previous work
that the research partners conducted within the context of the C-
STAR consortium (see http://www.c-star.org). The pro-
totype system developed in NESPOLE! is intended to provide ef-
fective multi-lingual speech-to-speech communication between all
pairs of four languages (Italian, German, French and English) within
broad, but yet restricted domains. The first showcase currently un-
der development is in the domain of tourism and travel information.
The NESPOLE! speech translation system is designed to be an
integral part of advanced e-commerce technology of the next gener-
ation. We envision a technological scenario in which multi-modal
(speech, video and gesture) interaction plays a significant role, in
addition to the passive browsing of pre-designed web pages as is
common in e-commerce today. The interaction between client and
provider will need to support online communication with agents
(both real and artificial) on the provider side. The language barrier
then becomes a significant obstacle for such online communica-
tion between the two parties, when they do not speak a common
language. Within the tourism and travel domain, one can imagine
a scenario in which users (the clients) are planning a recreational
trip and are searching for specific detailed information about the
1NESPOLE! - NEgotiating through SPOken Lan-
guage in E-commerce. See the project website at
http://nespole.itc.it/
.
regions they wish to visit. Initial general information is obtained
from a web site of a tourism information provider. When more
detailed or special information is required, the customer has the
option of opening an online video-conferencing connection with a
human agent of the tourism information provider. Speech transla-
tion is integrated within the video-conference connection; the two
parties each speak in their native language and hear the synthesized
translation of the speech of the other participant. Text translation
(in the form of subtitles) can also be provided. Some multi-modal
communication between the parties is also available. The provider
agent can send web pages to the display of the customer, and both
sides can annotate and refer to pictures and diagrams presented on
a shared whiteboard application.
In this paper we describe the design considerations behind the ar-
chitecture that we have developed for the NESPOLE! speech trans-
lation system in the scenario described above. In order to make the
developed prototype as realistic as possible for use by a common
user, we assume only minimal hardware and software is available
on the customer side. This does include a PC-type video camera,
commercially available internet video-conferencing software (such
as Microsoft Netmeeting), standard audio and video hardware and
a standard web browser. However, no speech recognition and/or
translation software is assumed to reside locally on the PC of the
customer. This implies a server-type architecture in which speech
recognition and translation are accomplished via interaction with
a dedicated server. The extent to which this server is centralized
or distributed is one of the major design considerations taken into
account in our system.
2. NESPOLE! INTERLINGUA-BASED
TRANSLATION APPROACH
Our translation approach builds upon previous work that we have
conducted within the context of the C-STAR consortium. We use
an interlingua-based approach with a relatively shallow task-oriented
interlingua representation [2] [1], that was initially designed for the
C-STAR consortium and has been significantly extended for the
NESPOLE! project. Interlingual machine translation is convenient
when more than two languages are involved because it does not re-
quire each language to be connected by a set of transfer rules to
each other language in each direction [3]. Adding a new language
that has all-ways translation with existing languages requires only
writing one analyzer that maps utterances into the interlingua and
one generator that maps interlingua representations into sentences.
The interlingua approach also allows each partner group to imple-
ment an analyzer and generator for its home language only. A fur-
Figure 1: General Architecture of NESPOLE! System
ther advantage is that it supports a paraphrase generation back into
the language of the speaker. This provides the user with some con-
trol in case the analysis of an utterance failed to produce a correct
interlingua. The following are three examples of utterances tagged
with their corresponding interlingua representation:
Thank you very much
c:thank
And we?ll see you on February twelfth.
a:closing (time=(february, md12))
On the twelfth we have a single and a double
available.
a:give-information+availability+room
(room-type=(single & double),time=(md12))
3. NESPOLE! SYSTEM ARCHITECTURE
DESIGN
Several main considerations were taken into account in the de-
sign of the NESPOLE! Human Language Technology (HLT) server
architecture: (1) The desire to cleanly separate the actual HLT
system from the communication channel between the two parties,
which makes use of the speech translation capabilities provided by
the HLT system; (2) The desire to allow each research site to in-
dependently develop its language specific analysis and generation
modules, and to allow each site to easily integrate new and im-
proved components into the global NESPOLE! HLT system; and
(3) The desire of the research partners to build to whatever ex-
tent possible upon software components previously developed in
the context of the C-STAR consortium. We will discuss the ex-
tent to which the designed architecture achieves these goals after
presenting an overview of the architecture itself.
Figure 1 shows the general architecture of the current NESPOLE!
system. Communication between the client and agent is facilitated
by a dedicated module - the Mediator. This module is designed to
control the video-conferencing connection between the client and
the agent, and to integrate the speech translation services into the
communication. The mediator handles audio and video data as-
sociated with the video-conferencing application and binary data
associated with a shared whiteboard application. Standard H.323
data formats are used for these three types of data transfer. Speech-
to-speech translation of the utterances captured by the mediator is
accomplished through communication with the NESPOLE! global
HLT server. This is accomplished via socket connections with
language-specific HLT servers. The communication between the
mediator and each HLT server consists mainly of linear PCM au-
dio packets (some text and control messages are also supported and
are described later in this section).
Communication with Mediator
Speech
Recognizer
Module
Parser/Analysis
IF
text
Analysis Chain
Speech
Synthsizer
Generation
Module
IF
text
.
Generation
Chain
Communication with CommSwitch
audio audio
Language X HLT Server
Figure 2: Architecture of NESPOLE! Language-specific HLT Servers
The global NESPOLE! HLT server comprises four separate lang-
uage-specific servers. Additional language-specific HLT servers
can easily be integrated in the future. The internal architecture
of each language-specific HLT server is shown in figure 2. Each
language-specific HLT server consists of an analysis chain and a
generation chain. The analysis chain receives an audio stream cor-
responding to a single utterance and performs speech recognition
followed by parsing and analysis of the input utterance into the in-
terlingua representation (IF). The interlingua is then transmitted to
a central HLT communication switch (the CS), that forwards it to
the HLT servers for the other languagesas appropriate. IF messages
received from the central communication switch are processed by
the generation chain. A generation module first generates text in
the target language from the IF. The text utterance is then sent to
a speech synthesis module that produces an audio stream for the
utterance. The audio is then communicated externally to the me-
diator, in order to be integrated back into the video-conferencing
stream between the two parties.
The mediator can, in principle, support multiple one-to-one com-
munication sessions between client and agent. However, the de-
sign supports multiple mediators, which, for example, could each
be dedicated to a different provider application. Communication
with the mediator is initiated by the client by an explicit action
via the web browser. This opens a communication channel to the
mediator, which contacts the agent station, establishes the video-
conferencing connection between client and agent, and starts the
whiteboard application. The specific pair of languages for a dia-
logue is determined in advance from the web page from which the
client initiates the communication. The mediator then establishes a
socket communication channel with the two appropriate language
specific HLT servers. Communication between the two language
specific HLT servers, in the form of IF messages, is facilitated by
the NESPOLE! global communication switch (the CS). The lan-
guage specific HLT servers may in fact be physically distributed
over the internet. Each language specific HLT server is set to ser-
vice analysis requests coming from the mediator side, and genera-
tion requests arriving from the CS.
Some further functionality beyond that described above is also
supported. As described earlier, the ability to produce a textual
paraphrase of an input utterance and to display it back to the orig-
inal speaker provides useful user control in the case of translation
failures. This is supported in our system in the following way. In
addition to the translated audio, each HLT server also forwards the
generated text in the output language to the mediator, which then
displays the text on a dedicated application window on the PC of
the target user. Additionally, at the end of the processing of an in-
put utterance by the analysis chain of an HLT server, the resulting
IF is passed internally to the generation chain, which produces a
text generation from the IF. The result is a textual paraphrase of the
input utterance in the source language. This text is then sent back
to the mediator, which forwards it to the party from which the ut-
terance originated. The paraphrase is then displayed to the original
speaker in the dedicated application window. If the paraphrase is
wrong, it is likely that the produced IF was incorrect, and thus the
translation would also be wrong. The user may then use a button
on the application interface to signal that the last displayed para-
phrase was wrong. This action triggers a message that is forwarded
by the mediator to the other party, indicating that the last displayed
translation should be ignored. Further functionality is planned to
support synchronization between multi-modal events on the white-
board and their corresponding speech actions. As these are in very
preliminary stages of planning we do not describe them here.
4. DISCUSSION AND CONCLUSIONS
We believe that the architectural design described above has sev-
eral strengths and advantages. The clean separation of the HLT
server dedicated to the speech translation services from the exter-
nal communication modules between the two parties allows the re-
search partners to develop the HLT modules with a large degree
of independence. Furthermore, this separation will allow us in the
future to explore other types of mediators for different types of ap-
plications. One such application being proposed for development
within the C-STAR consortium is a speech-to-speech translation
service over mobile phones. The HLT server architecture described
here would be able to generally support such alternative external
communication modalities as well.
The physical distribution of the individual language specific HLT
servers allows each site to independently develop, integrate and
test its own analysis and generation modules. The organization of
each language specific HLT server as an independent module al-
lows each of the research sites to develop its unique approaches to
analysis and generation, while adhering to a simple communication
protocol between the HLT servers and externally with the mediator.
This allowed the research partners to ?jump-start? the project with
analysis and generation modules previously developed for the C-
STAR consortium, and incrementally develop these modules over
time. Furthermore, the global NESPOLE! communication switch
(the CS) supports testing of analysis and generation among the four
languages in isolation from the external parts of the system. Cur-
rently, requests for analysis of a textual utterance can be transmitted
to the HLT servers via the CS, with the resulting IF sent (via the CS)
to all HLT servers for generation. This gives us great flexibility in
developing and testing our translation system. The functionality of
the CS was originally developed for our previous C-STAR project,
and was reused with little modification.
Support for additional languages is also very easy to incorpo-
rate into the system by adding new language-specific HLT servers.
Any new language specific HLT server needs only to adhere to the
communication protocols with both the global NESPOLE! commu-
nication switch (the CS) and the external mediator. The C-STAR
consortium plans to use the general architecture described here for
its next phase of collaboration, with support for at least three asian
languages (Japanese, Korean and Chinese) in addition to the lan-
guages currently covered by the NESPOLE! project.
The first prototype of the NESPOLE! speech translation system
is currently in advanced stages of full integration. A showcase
demonstration of the prototype system to the European Commis-
sion is currently scheduled for late April 2001.
5. ACKNOWLEDGMENTS
The research work reported here was supported in part by the
National Science Foundation under Grant number 9982227. Any
opinions, findings and conclusions or recomendations expressed in
this material are those of the author(s) and do not necessarily reflect
the views of the National Science Foundation (NSF).
6. REFERENCES
[1] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[2] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[3] S. Nirenburg, J. Carbonell, M. Tomita, and K. Goodman.
Machine Translation: A Knowledge-Based Approach. Morgan
Kaufmann, San Mateo, California, 1992.
Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
Spoken Language Parsing Using Phrase-Level Grammars and Trainable 
Classifiers 
Chad Langley, Alon Lavie, Lori Levin, Dorcas Wallace, Donna Gates, and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
{clangley|alavie|lsl|dorcas|dmg|kay}@cs.cmu.edu 
 
Abstract 
In this paper, we describe a novel 
approach to spoken language analysis 
for translation, which uses a combination 
of grammar-based phrase-level parsing 
and automatic classification. The job of 
the analyzer is to produce a shallow 
semantic interlingua representation for 
spoken task-oriented utterances. The 
goal of our hybrid approach is to provide 
accurate real-time analyses while 
improving robustness and portability to 
new domains and languages. 
1 Introduction 
Interlingua-based approaches to Machine 
Translation (MT) are highly attractive in systems 
that support a large number of languages. For each 
source language, an analyzer that converts the 
source language into the interlingua is required. 
For each target language, a generator that converts 
the interlingua into the target language is needed. 
Given analyzers and generators for all supported 
languages, the system simply connects the source 
language analyzer with the target language 
generator to perform translation. 
Robust and accurate analysis is critical in 
interlingua-based translation systems. In speech-to-
speech translation systems, the analyzer must be 
robust to speech recognition errors, spontaneous 
speech, and ungrammatical inputs as described by 
Lavie (1996). Furthermore, the analyzer should run 
in (near) real time. 
In addition to accuracy, speed, and robustness, 
the portability of the analyzer with respect to new 
domains and new languages is an important 
consideration. Despite continuing improvements in 
speech recognition and translation technologies, 
restricted domains of coverage are still necessary 
in order to achieve reasonably accurate machine 
translation. Porting translation systems to new 
domains or even expanding the coverage in an 
existing domain can be very difficult and time-
consuming.  This creates significant challenges in 
situations where translation is needed for a new 
domain within relatively short notice. Likewise, 
demand can be high for translation systems that 
can be rapidly expanded to include new languages 
that were not previously considered important. 
Thus, it is important that the analysis approach 
used in a translation system be portable to new 
domains and languages. 
One approach to analysis in restricted domains 
is to use semantic grammars, which focus on 
parsing semantic concepts rather than syntactic 
structure. Semantic grammars can be especially 
useful for parsing spoken language because they 
are less susceptible to syntactic deviations caused 
by spontaneous speech effects. However, the focus 
on meaning rather than syntactic structure 
generally makes porting to a new domain quite 
difficult. Since semantic grammars do not exploit 
syntactic similarities across domains, completely 
new grammars must usually be developed. 
While grammar-based parsing can provide very 
accurate analyses on development data, it is 
difficult for a grammar to completely cover a 
domain, a problem that is exacerbated by spoken 
input. Furthermore, it generally takes a great deal 
of effort by human experts to develop a high-
coverage grammar. On the other hand, machine 
learning approaches can generalize beyond training 
data and tend to degrade gracefully in the face of 
noisy input. Machine learning methods may, 
however, be less accurate on clearly in-domain 
input than grammars and may require a large 
amount of training data. 
We describe a prototype version of an analyzer 
that combines phrase-level parsing and machine 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 15-22.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
learning techniques to take advantage of the 
benefits of each. Phrase-level semantic grammars 
and a robust parser are used to extract low-level 
interlingua arguments from an utterance. Then, 
automatic classifiers assign high-level domain 
actions to semantic segments in the utterance. 
2 MT System Overview 
The analyzer we describe is used for English and 
German in several multilingual human-to-human 
speech-to-speech translation systems, including the 
NESPOLE! system (Lavie et al, 2002). The goal 
of NESPOLE! is to provide translation for 
common users within real-world e-commerce 
applications. The system currently provides 
translation in the travel and tourism domain 
between English, French, German and Italian.  
NESPOLE! employs an interlingua-based 
translation approach that uses four basic steps to 
perform translation. First, an automatic speech 
recognizer processes spoken input. The best-
ranked hypothesis from speech recognition is then 
passed through the analyzer to produce interlingua. 
Target language text is then generated from the 
interlingua. Finally, the target language text is 
synthesized into speech. 
This interlingua-based translation approach 
allows for distributed development of the 
components for each language. The components 
for each language are assembled into a translation 
server that accepts speech, text, or interlingua as 
input and produces interlingua, text, and 
synthesized speech. In addition to the analyzer 
described here, the English translation server uses 
the JANUS Recognition Toolkit for speech 
recognition, the GenKit system (Tomita & Nyberg, 
1988) for generation, and the Festival system 
(Black et al, 1999) for synthesis. 
NESPOLE! uses a client-server architecture 
(Lavie et al, 2001) to enable users who are 
browsing the web pages of a service provider (e.g. 
a tourism bureau) to seamlessly connect to a 
human agent who speaks a different language. 
Using commercially available software such as 
Microsoft NetMeeting?, a user is connected to the 
NESPOLE! Mediator, which establishes 
connections with the agent and with translation 
servers for the appropriate languages. During a 
dialogue, the Mediator transmits spoken input from 
the users to the translation servers and synthesized 
translations from the servers to the users. 
3 The Interlingua 
The interlingua used in the NESPOLE! system is 
called Interchange Format (IF) (Levin et al, 1998; 
Levin et al, 2000). The IF defines a shallow 
semantic representation for task-oriented 
utterances that abstracts away from language-
specific syntax and idiosyncrasies while capturing 
the meaning of the input. Each utterance is divided 
into semantic segments called semantic dialog 
units (SDUs), and an IF is assigned to each SDU. 
An IF representation consists of four parts: a 
speaker tag, a speech act, an optional sequence of 
concepts, and an optional set of arguments. The 
representation takes the following form: 
 
speaker : speech act +concept* (argument*) 
 
The speaker tag indicates the role of the speaker 
in the dialogue. The speech act captures the 
speaker?s intention. The concept sequence, which 
may contain zero or more concepts, captures the 
focus of an SDU. The speech act and concept 
sequence are collectively referred to as the domain 
action (DA). The arguments use a feature-value 
representation to encode specific information from 
the utterance. Argument values can be atomic or 
complex. The IF specification defines all of the 
components and describes how they can be legally 
combined. Several examples of utterances with 
corresponding IFs are shown below. 
 
Thank you very much. 
a:thank 
Hello. 
c:greeting (greeting=hello) 
How far in advance do I need to book a room for the Al-
Cervo Hotel? 
c:request-suggestion+reservation+room ( 
   suggest-strength=strong, 
   time=(time-relation=before, 
     time-distance=question), 
   who=i, 
   room-spec=(room, identifiability=no, 
     location=(object-name=cervo_hotel))) 
4 The Hybrid Analysis Approach 
Our hybrid analysis approach uses a combination 
of grammar-based parsing and machine learning 
techniques to transform spoken utterances into the 
IF representation described above. The speaker tag 
is assumed to be given. Thus, the goal of the 
analyzer is to identify the DA and arguments.  
The hybrid analyzer operates in three stages. 
First, semantic grammars are used to parse an 
utterance into a sequence of arguments. Next, the 
utterance is segmented into SDUs. Finally, the DA 
is identified using automatic classifiers. 
4.1 Argument Parsing 
The first stage in analysis is parsing an utterance 
for arguments. During this stage, utterances are 
parsed with phrase-level semantic grammars using 
the robust SOUP parser (Gavald?, 2000). 
4.1.1 The Parser 
The SOUP parser is a stochastic, chart-based, top-
down parser that is designed to provide real-time 
analysis of spoken language using context-free 
semantic grammars. One important feature 
provided by SOUP is word skipping. The amount 
of skipping allowed is configurable and a list of 
unskippable words can be defined. Another feature 
that is critical for phrase-level argument parsing is 
the ability to produce analyses consisting of 
multiple parse trees. SOUP also supports modular 
grammar development (Woszczyna et al, 1998). 
Subgrammars designed for different domains or 
purposes can be developed independently and 
applied in parallel during parsing. Parse tree nodes 
are then marked with a subgrammar label. When 
an input can be parsed in multiple ways, SOUP can 
provide a ranked list of interpretations. 
In the prototype analyzer, word skipping is only 
allowed between parse trees. Only the best-ranked 
argument parse is used for further processing. 
4.1.2 The Grammars 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument 
grammar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains phrase-
level rules for parsing arguments defined in the IF. 
Top-level argument grammar nonterminals 
correspond to top-level arguments in the IF. 
The pseudo-argument grammar contains top-
level nonterminals that do not correspond to 
interlingua concepts. These rules are used for 
parsing common phrases that can be grouped into 
classes to capture more useful information for the 
classifiers. For example, all booked up, full, and 
sold out might be grouped into a class of phrases 
that indicate unavailability. In addition, rules in the 
pseudo-argument grammar can be used for 
contextual anchoring of ambiguous arguments. For 
example, the arguments [who=] and [to-whom=] 
have the same values. To parse these arguments 
properly in a sentence like ?Can you send me the 
brochure??, we use a pseudo-argument grammar 
rule, which refers to the arguments [who=] and [to-
whom=] within the appropriate context.  
The cross-domain grammar contains rules for 
parsing whole DAs that are domain-independent. 
For example, this grammar contains rules for 
greetings (Hello, Good bye, Nice to meet you, etc.). 
Cross-domain grammar rules do not cover all 
possible domain-independent DAs. Instead, the 
rules focus on DAs with simple or no argument 
lists. Domain-independent DAs with complex 
argument lists are left to the classifiers. Cross-
domain rules play an important role in the 
prediction of SDU boundaries. 
Finally, the shared grammar contains common 
grammar rules that can be used by all other 
subgrammars. These include definitions for most 
of the arguments, since many can also appear as 
sub-arguments. RHSs in the argument grammar 
contain mostly references to rules in the shared 
grammar. This method eliminates redundant rules 
in the argument and shared grammars and allows 
for more accurate grammar maintenance. 
4.2 Segmentation 
The second stage of processing in the hybrid 
analysis approach is segmentation of the input into 
SDUs. The IF representation assigns DAs at the 
SDU level. However, since dialogue utterances 
often consist of multiple SDUs, utterances must be 
segmented into SDUs before DAs can be assigned. 
Figure 1 shows an example utterance containing 
four arguments segmented into two SDUs. 
 
SDU1  SDU2  
greeting= disposition= visit-spec= location= 
hello i would like to take a vacation in val di fiemme 
Figure 1. Segmentation of an utterance into SDUs. 
The argument parse may contain trees for cross-
domain DAs, which by definition cover a complete 
SDU. Thus, there must be an SDU boundary on 
both sides of a cross-domain tree. Additionally, no 
SDU boundaries are allowed within parse trees. 
The prototype analyzer drops words skipped 
between parse trees, leaving only a sequence of 
trees. The parse trees on each side of a potential 
boundary are examined, and if either tree was 
constructed by the cross-domain grammar, an SDU 
boundary is inserted. Otherwise, a simple statistical 
model similar to the one described by Lavie et al 
(1997) estimates the likelihood of a boundary. 
The statistical model is based only on the root 
labels of the parse trees immediately preceding and 
following the potential boundary position. Suppose 
the position under consideration looks like 
[A1?A2], where there may be a boundary between 
arguments A1 and A2. The likelihood of an SDU 
boundary is estimated using the following formula: 
 
])C([A  ])C([A
])AC([  ])C([A])AF([A
21
21
21
+
?+?
??  
 
The counts C([A1?]), C([?A2]), C([A1]), C([A2]) 
are computed from the training data. An evaluation 
of this baseline model is presented in section 6.  
4.3 DA Classification 
The third stage of analysis is the identification of 
the DA for each SDU using automatic classifiers. 
After segmentation, a cross-domain parse tree may 
cover an SDU. In this case, analysis is complete 
since the parse tree contains the DA. Otherwise, 
automatic classifiers are used to assign the DA. In 
the prototype analyzer, the DA classification task 
is split into separate subtasks of classifying the 
speech act and concept sequence. This reduces the 
complexity of each subtask and allows for the 
application of specialized techniques to identify 
each component. 
One classifier is used to identify the speech act, 
and a second classifier identifies the concept 
sequence. Both classifiers are implemented using 
TiMBL (Daelemans et al, 2000), a memory-based 
learner. Speech act classification is performed first. 
Input to the speech act classifier is a set of binary 
features that indicate whether each of the possible 
argument and pseudo-argument labels is present in 
the argument parse for the SDU. No other features 
are currently used. Concept sequence classification 
is performed after speech act classification. The 
concept sequence classifier uses the same feature 
set as the speech act classifier with one additional 
feature: the speech act assigned by the speech act 
classifier. We present an evaluation of this baseline 
DA classification scheme in section 6. 
4.4 Using the IF Specification 
The IF specification imposes constraints on how 
elements of the IF representation can legally 
combine. DA classification can be augmented with 
knowledge of constraints from the IF specification, 
providing two advantages over otherwise na?ve 
classification. First, the analyzer must produce 
valid IF representations in order to be useful in a 
translation system. Second, using knowledge from 
the IF specification can improve the quality of the 
IF produced, and thus the translation. 
Two elements of the IF specification are 
especially relevant to DA classification. First, the 
specification defines constraints on the 
composition of DAs. There are constraints on how 
concepts are allowed to pair with speech acts as 
well as ordering constraints on how concepts are 
allowed to combine to form a valid concept 
sequence. These constraints can be used to 
eliminate illegal DAs during classification. The 
second important element of the IF specification is 
the definition of how arguments are licensed by 
speech acts and concepts. In order for an IF to be 
valid, at least one speech act or concept in the DA 
must license each argument. 
The prototype analyzer uses the IF specification 
to aid classification and guarantee that a valid IF 
representation is produced. The speech act and 
concept sequence classifiers each provide a ranked 
list of possible classifications. When the best 
speech act and concept sequence combine to form 
an illegal DA or form a legal DA that does not 
license all of the arguments, the analyzer attempts 
to find the next best legal DA that licenses the 
most arguments. Each of the alternative concept 
sequences (in ranked order) is combined with each 
of the alternative speech acts (in ranked order). For 
each possible legal DA, the analyzer checks if all 
of the arguments found during parsing are licensed. 
If a legal DA is found that licenses all of the 
arguments, then the process stops. If not, one 
additional fallback strategy is used. The analyzer 
then tries to combine the best classified speech act 
with each of the concept sequences that occurred in 
the training data, sorted by their frequency of 
occurrence. Again, the analyzer checks if each 
legal DA licenses all of the arguments and stops if 
such a DA is found. If this step fails to produce a 
legal DA that licenses all of the arguments, the 
best-ranked DA that licenses the most arguments is 
returned. In this case, any arguments that are not 
licensed by the selected DA are removed. This 
approach is used because it is generally better to 
select an alternative DA and retain more arguments 
than to keep the best DA and lose the information 
represented by the arguments. An evaluation of 
this strategy is presented in the section 6. 
5 Grammar Development and 
Classifier Training 
During grammar development, it is generally 
useful to see how changes to the grammar affect 
the IF representations produced by the analyzer. In 
a purely grammar-based analysis approach, full 
interlingua representations are produced as the 
result of parsing, so testing new grammars simply 
requires loading them into the parser. Because the 
grammars used in our hybrid approach parse at the 
argument level, testing grammar modifications at 
the complete IF level requires retraining the 
segmentation model and the DA classifiers. 
 When new grammars are ready for testing, 
utterance-IF pairs for the appropriate language are 
extracted from the training database. Each 
utterance-IF pair in the training data consists of a 
single SDU with a manually annotated IF. Using 
the new grammars, the argument parser is applied 
to each utterance to produce an argument parse. 
The counts used by the segmentation model are 
then recomputed based on the new argument 
parses. Since each utterance contains a single 
SDU, the counts C([?A2]) and C([A1?]) can be 
computed directly from the first and last arguments 
in the parse respectively. 
Next, the training examples for the DA 
classifiers are constructed. Each training example 
for the speech act classifier consists of the speech 
act from the annotated IF and a vector of binary 
features with a positive value set for each argument 
or pseudo-argument label that occurs in the 
argument parse. The training examples for the 
concept sequence classifiers are similar with the 
addition of the annotated speech act to the feature 
vector. After the training examples are constructed, 
new classifiers are trained. 
Two tools are available to support easy testing 
during grammar development. First, the entire 
training process can be run using a single script. 
Retraining for a new grammar simply requires 
running the script with pointers to the new 
grammars. Then, a special development mode of 
the translation servers allows the grammar writers 
to load development grammars and their 
corresponding segmentation model and DA 
classifiers. The translation server supports input in 
the form of individual utterances or files and 
allows the grammar developers to look at the 
results of each stage of the analysis process. 
6 Evaluation 
We present the results from recent experiments to 
measure the performance of the analyzer 
components and of end-to-end translation using the 
analyzer. We also report the results of an ablation 
experiment that used earlier versions of the 
analyzer and IF specification. 
6.1 Translation Experiment 
 
Acceptable Perfect 
SR Hypotheses 66% 56% 
Translation from 
Transcribed Text 58% 43% 
Translation from 
SR Hypotheses 45% 32% 
Table 1. English-to-English end-to-end translation 
 
Acceptable Perfect 
Translation from 
Transcribed Text 55% 38% 
Translation from 
SR Hypotheses 43% 27% 
Table 2. English-to-Italian end-to-end translation 
Tables 1 and 2 show end-to-end translation 
results of the NESPOLE! system. In this 
experiment, the input was a set of English 
utterances. The utterances were paraphrased back 
into English via the interlingua (Table 1) and 
translated into Italian (Table 2). The data used to 
train the DA classifiers consisted of 3350 SDUs 
annotated with IF representations. The test set 
contained 151 utterances consisting of 332 SDUs 
from 4 unseen dialogues. Translations were 
compared to human transcriptions and graded as 
described in (Levin et al, 2000). A grade of 
perfect, ok, or bad was assigned to each 
translation by human graders. A grade of perfect 
or ok is considered acceptable. The table shows the 
average of grades assigned by three graders. 
The row in Table 1 labeled SR Hypotheses 
shows the grades when the speech recognizer 
output is compared directly to human transcripts. 
As these grades show, recognition errors can be a 
major source of unacceptable translations. These 
grades provide a rough bound on the translation 
performance that can be expected when using input 
from the speech recognizer since meaning lost due 
to recognition errors cannot be recovered. The 
rows labeled Translation from Transcribed Text 
show the results when human transcripts are used 
as input. These grades reflect the combined 
performance of the analyzer and generator. The 
rows labeled Translation from SR Hypotheses 
show the results when the speech recognizer 
produces the input utterances. As expected, 
translation performance was worse with the 
introduction of recognition errors. 
 
Precision Recall 
70% 54% 
Table 3. SDU boundary detection performance 
Table 3 shows the performance of the 
segmentation model on the test set. The SDU 
boundary positions assigned automatically were 
compared with manually annotated positions. 
 
 
Classifier Accuracy 
Speech Act 65% 
Concept Sequence 54% 
Domain Action 43% 
Table 4. Classifier accuracy on transcription 
 
Frequency 
Speech Act 33% 
Concept Sequence 40% 
Domain Action 14% 
Table 5. Frequency of most common DA elements 
Table 4 shows the performance of the DA 
classifiers, and Table 5 shows the frequency of the 
most common DA, speech act, and concept 
sequence in the test set. Transcribed utterances 
were used as input and were segmented into SDUs 
before analysis. This experiment is based on only 
293 SDUs. For the remaining SDUs in the test set, 
it was not possible to assign a valid representation 
based on the current IF specification. 
These results demonstrate that it is not always 
necessary to find the canonical DA to produce an 
acceptable translation. This can be seen by 
comparing the Domain Action accuracy from Table 
4 with the Transcribed grades from Table 1. 
Although the DA classifiers produced the 
canonical DA only 43% of the time, 58% of the 
translations were graded as acceptable. 
 
 
Changed 
Speech Act 5% 
Concept Sequence 26% 
Domain Action 29% 
Table 6. DA elements changed by IF specification 
In order to examine the effects of using IF 
specification constraints, we looked at the 182 
SDUs which were not parsed by the cross-domain 
grammar and thus required DA classification. 
Table 6 shows how many DAs, speech acts, and 
concept sequences were changed as a result of 
using the constraints. DAs were changed either 
because the DA was illegal or because the DA did 
not license some of the arguments. Without the IF 
specification, 4% of the SDUs would have been 
assigned an illegal DA, and 29% of the SDUs 
(those with a changed DA) would have been 
assigned an illegal IF. Furthermore, without the IF 
specification, 0.38 arguments per SDU would have 
to be dropped while only 0.07 arguments per SDU 
were dropped when using the fallback strategy. 
The mean number of arguments per SDU was 1.47. 
6.2 Ablation Experiment 
Classification Accuracy (16-fold Cross 
Validation)
0
0.2
0.4
0.6
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
e
a
n
 
A
cc
ur
a
c
y
Speech Act
Concept
Sequence
Domain Action
 
Figure 2: DA classifier accuracy with varying 
amounts of data 
Figure 2 shows the results of an ablation 
experiment that examined the effect of varying the 
training set size on DA classification accuracy. 
Each point represents the average accuracy using a 
16-fold cross validation setup. 
The training data contained 6409 SDU-
interlingua pairs. The data were randomly divided 
into 16 test sets containing 400 examples each. In 
each fold, the remaining data were used to create 
training sets containing 500, 1000, 2000, 3000, 
4000, 5000, and 6009 examples. 
The performance of the classifiers appears to 
begin leveling off around 4000 training examples. 
These results seem promising with regard to the 
portability of the DA classifiers since a data set of 
this size could be constructed in a few weeks. 
7 Related Work 
Lavie et al (1997) developed a method for 
identifying SDU boundaries in a speech-to-speech 
translation system. Identifying SDU boundaries is 
also similar to sentence boundary detection. 
Stevenson and Gaizauskas (2000) use TiMBL 
(Daelemans et al, 2000) to identify sentence 
boundaries in speech recognizer output, and Gotoh 
and Renals (2000) use a statistical approach to 
identify sentence boundaries in automatic speech 
recognition transcripts of broadcast speech. 
Munk (1999) attempted to combine grammars 
and machine learning for DA classification. In 
Munk?s SALT system, a two-layer HMM was used 
to segment and label arguments and speech acts. A 
neural network identified the concept sequences. 
Finally, semantic grammars were used to parse 
each argument segment. One problem with SALT 
was that the segmentation was often inaccurate and 
resulted in bad parses. Also, SALT did not use a 
cross-domain grammar or interlingua specification. 
Cattoni et al (2001) apply statistical language 
models to DA classification. A word bigram model 
is trained for each DA in the training data. To label 
an utterance, the most likely DA is assigned. 
Arguments are identified using recursive transition 
networks. IF specification constraints are used to 
find the most likely valid DA and arguments. 
8 Discussion and Future Work 
One of the primary motivations for developing the 
hybrid analysis approach described here is to 
improve the portability of the analyzer to new 
domains and languages. We expect that moving 
from a purely grammar-based parsing approach to 
this hybrid approach will help attain this goal. 
The SOUP parser supports portability to new 
domains by allowing separate grammar modules 
for each domain and a grammar of rules shared 
across domains (Woszczyna et al, 1998). This 
modular grammar design provides an effective 
method for adding new domains to existing 
grammars. Nevertheless, developing a full 
semantic grammar for a new domain requires 
significant effort by expert grammar writers. 
The hybrid approach reduces the manual labor 
required to port to new domains by incorporating 
machine learning. The most labor-intensive part of 
developing full semantic grammars for producing 
IF is writing DA-level rules. This is exactly the 
work eliminated by using automatic DA classifiers. 
Furthermore, the phrase-level argument grammars 
used in the analyzer contain fewer rules than a full 
semantic grammar. The argument-level grammars 
are also less domain-dependent than the full 
grammars and thus more reusable. The DA 
classifiers should also be more tolerant than full 
grammars of deviations from the domain. 
We analyzed the grammars from a previous 
version of the translation system, which produced 
complete IFs using strictly grammar-based parsing, 
to estimate what portion of the grammar was 
devoted to the identification of domain actions. 
Approximately 2200 rules were used to cover 400 
DAs. Nonlexical rules made up about half of the 
grammar, and the DA rules accounted for about 
20% of the nonlexical rules. Using these figures, 
we can project the number of DA rules that would 
have to be added to the current system, which uses 
our hybrid analysis approach. The database for the 
new system contains approximately 600 DAs. 
Assuming the average number of rules per DA is 
the same as before, roughly 3300 DA-level rules 
would have to be added to the current grammar, 
which has about 17500 nonlexical rules, to cover 
the DAs in the database. 
Our hybrid approach should also improve the 
portability of the analyzer to new languages. Since 
grammars are language specific, adding a new 
language still requires writing new argument 
grammars. Then the DA classifiers simply need to 
be retrained on data for the new language. If 
training data for the new language were not 
available, DA classifiers using only language-
independent features, from the IF for example, 
could be trained on data for existing languages and 
used for the new language. Such classifiers could 
be used as a starting point until training data was 
available in the new language. 
The experimental results indicate the promise 
of the analysis approach we have described. The 
level of performance reported here was achieved 
using a simple segmentation model and simple DA 
classifiers with limited feature sets. We expect that 
performance will substantially improve with a 
more informed design of the segmentation model 
and DA classifiers. We plan to examine various 
design options, including richer feature sets and 
alternative classification techniques. We are also 
planning experiments to evaluate robustness and 
portability when the coverage of the NESPOLE! 
system is expanded to the medical domain later 
this year. In these experiments, we will measure 
the effort needed to write new argument grammars, 
the extent to which existing argument grammars 
are reusable, and the effort required to expand the 
argument grammar to include DA-level rules. 
9 Acknowledgements 
The research work reported here was supported by 
the National Science Foundation under Grant 
number 9982227. Special thanks to Alex Waibel 
and everyone in the NESPOLE! group for their 
support on this work. 
References 
Black, A., P. Taylor, and R. Caley. 1999. The 
Festival Speech Synthesis System: System 
Documentation. Human Computer Research 
Centre, University of Edinburgh, Scotland. 
http://www.cstr.ed.ac.uk/projects/festival/ma
nual 
Cattoni, R., M. Federico, and A. Lavie. 2001. 
Robust Analysis of Spoken Input Combining 
Statistical and Knowledge-Based Information 
Sources. In Proceedings of the IEEE Automatic 
Speech Recognition and Understanding 
Workshop, Trento, Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2000. TiMBL: Tilburg Memory 
Based Learner, version 3.0, Reference Guide. 
ILK Technical Report 00-01. 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz 
Gavald?, M. 2000. SOUP: A Parser for Real-
World Spontaneous Speech. In Proceedings of 
the IWPT-2000, Trento, Italy. 
Gotoh, Y. and S. Renals. Sentence Boundary 
Detection in Broadcast Speech Transcripts. 2000. 
In Proceedings on the International Speech 
Communication Association Workshop: 
Automatic Speech Recognition: Challenges for 
the New Millennium, Paris. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. 
Enhancing the Usability and Performance of 
NESPOLE! ? a Real-World Speech-to-Speech 
Translation System. In Proceedings of HLT-
2002, San Diego, CA. 
Lavie, A., C. Langley, A. Waibel, et al 2001. 
Architecture and Design Considerations in 
NESPOLE!: a Speech Translation System for E-
commerce Applications. In Proceedings of HLT-
2001, San Diego, CA. 
Lavie, A., D. Gates, N. Coccaro, and L. Levin. 
1997. Input Segmentation of Spontaneous Speech 
in JANUS: a Speech-to-speech Translation 
System. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-
96 Workshop, E. Maier, M. Mast, and S. 
Luperfoy (eds.), LNCS series, Springer Verlag. 
Lavie, A. 1996. GLR*: A Robust Grammar-
Focused Parser for Spontaneously Spoken 
Language. PhD dissertation, Technical Report 
CMU-CS-96-126, Carnegie Mellon University, 
Pittsburgh, PA. 
Levin, L., D. Gates, A. Lavie, et al 2000. 
Evaluation of a Practical Interlingua for Task-
Oriented Dialogue. In Workshop on Applied 
Interlinguas: Practical Applications of 
Interlingual Approaches to NLP, Seattle. 
Levin, L., D. Gates, A. Lavie, and A. Waibel. 
1998. An Interlingua Based on Domain Actions 
for Machine Translation of Task-Oriented 
Dialogues. In Proceedings of ICSLP-98, Vol. 4, 
pp. 1155-1158, Sydney, Australia. 
Munk, M. 1999. Shallow Statistical Parsing for 
Machine Translation. Diploma Thesis, Karlsruhe 
University. 
Stevenson, M. and R. Gaizauskas. Experiments on 
Sentence Boundary Detection. 2000. In 
Proceedings of ANLP and NAACL-2000, Seattle. 
Tomita, M. and E. H. Nyberg. 1988. Generation 
Kit and Transformation Kit, Version 3.2: User?s 
Manual. Technical Report CMU-CMT-88-
MEMO, Carnegie Mellon University, Pittsburgh, 
PA. 
Woszczyna, M., M. Broadhead, D. Gates, et al 
1998. A Modular Approach to Spoken Language 
Translation for Large Domains. In Proceedings 
of AMTA-98, Langhorne, PA. 
Domain Specific Speech Acts for Spoken Language Translation 
Lori Levin, Chad Langley, Alon Lavie,  
Donna Gates, Dorcas Wallace and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, United States 
{lsl,clangley,alavie,dmg,dorcas,kay+}@cs.cmu.edu 
Abstract 
We describe a coding scheme for ma-
chine translation of spoken task-
oriented dialogue. The coding scheme 
covers two levels of speaker intention ? 
domain independent speech acts and 
domain dependent domain actions. Our 
database contains over 14,000 tagged 
sentences in English, Italian, and Ger-
man. We argue that domain actions, and 
not speech acts, are the relevant dis-
course unit for improving translation 
quality. We also show that, although 
domain actions are domain specific, the 
approach scales up to large domains 
without an explosion of domain actions 
and can be coded with high inter-coder 
reliability across research sites. Fur-
thermore, although the number of do-
main actions is on the order of ten times 
the number of speech acts, sparseness is 
not a problem for the training of classi-
fiers for identifying the domain action. 
We describe our work on developing 
high accuracy speech act and domain 
action classifiers, which is the core of 
the source language analysis module of 
our NESPOLE machine translation sys-
tem. 
1 Introduction 
The NESPOLE and C-STAR machine translation 
projects use an interlingua representation based 
on speaker intention rather than literal meaning. 
The speaker's intention is represented as a 
domain independent speech act followed by do-
main dependent concepts. We use the term 
domain action to refer to the combination of a 
speech act with domain specific concepts. Exam-
ples of domain actions and speech acts are shown 
in Figure 1. 
 
c:give-information+party  
?I will be traveling with my husband and 
our two children ages two and eleven? 
 
c:request-information+existence+facility  
?Do they have parking available?" 
?Is there someplace to go ice skating?" 
 
c:give-information+view+information-
object  
?I see the bus icon?  
 
Figure 1: Examples of Speech Acts and Domain 
Actions. 
 
Domain actions are constructed compositionally 
from an inventory of speech acts and an inven-
tory of concepts. The allowable combinations of 
speech acts and concepts are formalized in a hu-
man- and machine-readable specification docu-
ment. The specification document is supported 
by a database of over 14,000 tagged sentences in 
English, German, and Italian. 
The discourse community has long recog-
nized the potential for improving NLP systems 
by identifying speaker intention. It has been hy-
pothesized that predicting speaker intention of 
the next utterance would improve speech recog-
nition (Reithinger et al, Stolcke et al), or reduce 
ambiguity for machine translation (Qu et al, 
1996, Qu et al, 1997). Identifying speaker inten-
tion is also critical for sentence generation. 
We argue in this paper that the explicit repre-
sentation of speaker intention using domain ac-
tions can serve as the basis for an effective 
language-independent representation of meaning 
for speech-to-speech translation and that the 
relevant units of speaker intention are the domain 
specific domain action as well as the domain in-
dependent speech act. After a brief description of 
our database, we present linguistic motivation for 
domain actions. We go on to show that although 
domain actions are domain specific, there is not 
an explosion or exponential growth of domain 
actions when we scale up to a larger domain or 
port to a new domain. Finally we will show that, 
although the number of domain actions is on the 
order of ten times the number of speech acts, 
data sparseness is not a problem in training a 
domain action classifier. We present extensive 
work on developing a high-accuracy classifier 
for domain actions using a variety of classifica-
tion approaches and conclusions on the adequacy 
of these approaches to the task of domain action 
classification.  
2 Data Collection Scenario and Data-
base 
Our study is based on data that was collected for 
the NESPOLE and C-STAR speech-to-speech 
translation projects. Three domains are included. 
The NESPOLE travel domain covers inquiries 
about vacation packages. The C-STAR travel 
domain consists largely of reservation and pay-
ment dialogues and overlaps only about 50% in 
vocabulary with the NESPOLE travel domain. 
The medical assistance domain includes dia-
logues about chest pain and flu-like symptoms. 
There were two data collection protocols for 
the NESPOLE travel domain ? monolingual and 
bilingual. In the monolingual protocol, an Eng-
lish speaker in the United States had a conversa-
tion with an Italian travel agent speaking (non-
native) English in Italy. Monolingual data was 
also collected for German, French and Italian. 
Bilingual data was collected during user studies 
with, for example, an English speaker in the 
United States talking to an Italian-speaking travel 
agent in Italy, with the NESPOLE system pro-
viding the translation between the two parties. 
The C-STAR data consists of only monolingual 
role-playing dialogues with both speakers at the 
same site. The medical dialogues are monolin-
gual with doctors playing the parts of both doctor 
and patient. 
The dialogues were transcribed and multi-
sentence utterances were broken down into mul-
tiple Semantic Dialogue Units (SDUs) that each 
correspond to one domain action. Some SDUs 
have been translated into other NESPOLE or C-
STAR languages. Over 14,000 SDUs have been 
tagged with interlingua representations including 
domain actions as well as argument-value pairs. 
Table 1 summarizes the number of tagged SDUs 
in complete dialogues in the interlingua database. 
There are some additional tagged dialogue frag-
ments that are not counted. Figure 2 shows an 
excerpt from the database. 
 
 
 
English NESPOLE Travel 4691 
English C-STAR Travel 2025 
German NESPOLE Travel 1538 
Italian NESPOLE Travel 2248 
English Medical Assistance 2001 
German Medical Assistance 1152 
Italian Medical Assistance 935 
Table 1: Tagged SDUs in the Interlingua Data-
base. 
 
e709wa.19.0  comments: DATA from 
e709_1_0018_ITAGOR_00 
 
e709wa.19.1  olang ITA  lang ITA Prv CMU   
?hai in mente una localita specifica?" 
e709wa.19.1  olang ITA  lang GER  Prv CMU   
?haben Sie einen bestimmten Ort im Sinn?" 
e709wa.19.1  olang ITA  lang FRE  Prv 
CLIPS ?" 
e709wa.19.1  olang ITA  lang ENG  Prv CMU   
?do you have a specific place in mind" 
e709wa.19.1                   IF  Prv CMU   
a:request-information+disposition+object  
(object-spec=(place, modifier=specific, 
identifiability=no), disposi-
tion=(intention, who=you)) 
e709wa.19.1  comments: Tagged by dmg 
 
Figure 2: Excerpt from the Interlingua Database. 
3 Linguistic Argument for Domain Ac-
tions 
Proponents of Construction Grammar (Fillmore 
et. al. 1988, Goldberg 1995) have argued that 
human languages consist of constructional units 
that include a syntactic structure along with its 
associated semantics and pragmatics. Some con-
structions follow the typical syntactic rules of the 
language but have a semantic or pragmatic focus 
that is not compositionally predictable from the 
parts. Other constructions do not even follow the 
typical syntax of the language (e.g., Why not go? 
with no tensed verb). 
Our work with multilingual machine transla-
tion of spoken language shows that fixed expres-
sions cannot be translated literally. For example, 
Why not go to the meeting? can be translated 
into Japanese as Kaigi ni itte mitara doo? (meet-
ing to going see/try-if how), which differs from 
the English in several ways. It does not have a 
word corresponding to not; it has a word that 
means see/try that does not appear in the English 
sentence; and so on. In order to produce an ac-
ceptable translation, we must find a common 
ground between the English fixed expression 
Why not V-inf? and the Japanese fixed expression 
-te mittara doo?. The common ground is the 
speaker's intention (in this case, to make a sug-
gestion) rather than the syntax or literal meaning. 
Speaker intention is partially captured with a 
direct or indirect speech act. However, whereas 
speech acts are generally domain independent, 
task-oriented language abounds with fixed ex-
pressions that have domain specific functions. 
For example, the phrases We have? or There 
are? in the hotel reservation domain express 
availability of rooms in addition to their more 
literal meanings of possession and existence. In 
the past six years, we have been successful in 
using domain specific domain actions as the ba-
sis for translation of limited-domain task-
oriented spoken language (Levin et al, 1998, 
Levin et al 2002; Langley and Lavie, 2003) 
4 Scalability and Portability of Domain 
Actions 
Domain actions, like speech acts, convey speaker 
intention. However, domain actions also repre-
sent components of meaning and are therefore 
more numerous than domain independent speech 
acts. 1168 unique domain actions are used in our 
NESPOLE database, in contrast to only 72 
speech acts. We show in this section that domain 
actions yield good coverage of task-oriented do-
mains, that domain actions can be coded effec-
tively by humans, and that scaling up to larger 
domains or porting to new domains is feasible 
without an explosion of domain actions.  
 
Coverage of Task-Oriented Domains: Our 
NESPOLE domain action database contains dia-
logues from two task-oriented domains: medical 
assistance and travel. Table 2 shows the number 
of speech acts and concepts that are used in the 
travel and medical domains.  The 1168 unique 
domain actions that appear in our database are 
composed of the 72 speech acts and 125 con-
cepts. 
 
 Travel Medical Combined 
DAs 880 459 1168 
SAs 67 44 72 
Concepts 91 74 125 
Table 2: DA component counts in NESPOLE 
data. 
 
Our domain action based interlingua has quite 
high coverage of the travel and medical dia-
logues we have collected. To measure how well 
the interlingua covers a domain, we define the 
no-tag rate as the percent of sentences that are 
not covered by the interlingua, according to a 
human expert. The no-tag rate for the English 
NESPOLE travel dialogues is 4.3% for dialogues 
that have been used for system development.  
We have also estimated the domain action no-
tag rate for unseen data using the NESPOLE 
travel database (English, German, and Italian 
combined). We randomly selected 100 SDUs as 
seen data and extracted their domain actions. We 
then randomly selected 100 additional SDUs 
from the remaining data and estimated the no-tag 
rate by counting the number of SDUs not cov-
ered by the domain actions in the seen data. We 
then added the unseen data to the seen data set 
and randomly selected 100 new SDUs. We re-
peated this process until the entire database had 
been seen, and we repeated the entire sampling 
process 10 times. Although the number of do-
main actions increases steadily with the database 
size (Figure 4), the no-tag rate for unseen data 
stabilizes at less than 10%.  
We also randomly selected half of the SDUs 
(4200) from the database as seen data and ex-
tracted the domain actions. Holding the seen data 
set fixed, we then estimated the no-tag rates in 
increasing amounts of unseen data from the re-
maining half of the database. We repeated this 
process 10 times. With a fixed amount of seen 
data, the no-tag rate remains stable for increasing 
amounts of unseen data. We observed similar no-
tag rate results for the medical assistance domain 
and for the combination of travel and medical 
domains. 
It is also important to note that although there 
is a large set of uncommon domain actions, the 
top 105 domain actions cover 80% of the sen-
tences in the travel domain database. Thus do-
main actions are practical for covering task-
oriented domains. 
 
Intercoder Agreement: Intercoder agreement is 
another indicator of manageability of the domain 
action based interlingua. We calculate intercoder 
agreement as percent agreement. Three interlin-
gua experts at one NESPOLE site achieved 94% 
agreement (average pairwise agreement) on 
speech acts and 88% agreement on domain ac-
tions. Across sites, expert agreement on speech 
acts is still quite high (89%), although agreement 
on domain actions is lower (62%). Since many 
domain actions are similar in meaning, some dis-
agreement can be tolerated without affecting 
translation quality. 
 
Figure 3: DAs to cover data (English). 
Figure 4: DAs to cover data (All languages). 
 
Scalability and Portability: The graphs in Figure 
3 and Figure 4 illustrate growth in the number of 
domain actions as the database size increases and 
as new domains are added. The x-axis represents 
the sample size randomly selected from the data-
base. The y-axis shows the number of unique 
domain actions (types) averaged over 10 samples 
of each size. Figure 3 shows the growth in do-
main actions for three English databases 
(NESPOLE travel, C-STAR travel, and medical 
assistance) as well as the growth in domain ac-
tions for a database consisting of equal amounts 
of data from each domain. Figure 4 shows the 
growth in domain actions for combined English, 
German, and Italian data in the NESPOLE travel 
and medical domains.  
Figure 3 and Figure 4 show that the number 
of domain actions increases steadily as the data-
base grows. However, closer examination reveals 
that scalability to larger domains and portability 
to new domains are in fact feasible.  The curves 
representing combined domains (travel plus 
medical in Figure 4 and NESPOLE travel, C-
STAR travel, and medical in Figure 3) show only 
a small increase in the number of domain actions 
when two domains are combined. In fact, there is 
a large overlap between domains.  In Table 3 the 
Overlap columns show the number of DA types 
and tokens that are shared between the travel and 
medical domains. We can see around 70% of DA 
tokens are covered by DA types that occur in 
both domains. 
 
 
DA 
Types 
Type 
Overlap 
DA 
Tokens 
Token 
Overlap 
NESPOLE 
Travel 880 171 8477 
6004 
(70.8%) 
NESPOLE 
Medical 459 171 4088 
2743 
(67.1%) 
Table 3: DA Overlap (All languages). 
5 A Hybrid Analysis Approach for Pars-
ing Domain Actions 
Langley et al (2002; Langley and Lavie, 2003) 
describe the hybrid analysis approach that is used 
in the NESPOLE! system (Lavie et al, 2002). 
The hybrid analysis approach combines gram-
mar-based phrasal parsing and machine learning 
techniques to transform utterances into our inter-
lingua representation. Our analyzer operates in 
three stages to identify the domain action and 
arguments. 
First, an input utterance is parsed into a se-
quence of arguments using phrase-level semantic 
grammars and the SOUP parser (Gavald?, 2000). 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument gram-
mar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains 
phrase-level rules for parsing arguments defined 
in the interlingua. The pseudo-argument gram-
mar contains rules for parsing common phrases 
that are not covered by interlingua arguments. 
For example, all booked up, full, and sold out 
might be grouped into a class of phrases that in-
dicate unavailability. The cross-domain grammar 
contains rules for parsing complete DAs that are 
domain independent. For example, this grammar 
contains rules for greetings (Hello, Good bye, 
Nice to meet you, etc.). Finally, the shared 
grammar contains low-level rules that can be 
used by all other subgrammars. 
After argument parsing, the utterance is seg-
mented into SDUs using memory-based learning 
(k-nearest neighbor) techniques. Spoken utter-
ances often consist of several SDUs. Since DAs 
are assigned at the SDU level, it is necessary to 
segment utterances before assigning DAs. 
0
100
200
300
400
500
600
700
800
900
0 1000 2000 3000 4000 5000 6000 7000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical C-STAR
Nespole Travel+Medical C-STAR + Nespole Travel+Medical
0
100
200
300
400
500
600
700
800
900
1000
0 1000 2000 3000 4000 5000 6000 7000 8000 9000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical Nespole Travel+Medical
The final stage in the hybrid analysis ap-
proach is domain action classification.  
6 Domain Action Classification 
Identifying the domain action is a critical step in 
the analysis process for our interlingua-based 
translation systems. One possible approach 
would be to manually develop grammars de-
signed to parse input utterances all the way to the 
domain action level. However, while grammar-
based parsing may provide very accurate analy-
ses, it is generally not feasible to develop a 
grammar that completely covers a domain. This 
problem is exacerbated with spoken input, where 
disfluencies and deviations from the grammar are 
very common. Furthermore, a great deal of effort 
by human experts is generally required to de-
velop a wide-coverage grammar. 
An alternative to writing full domain action 
grammars is to train classifiers to identify the 
DA. Machine learning approaches allow the ana-
lyzer to generalize beyond training data and tend 
to degrade gracefully in the face of noisy input. 
Machine learning methods may, however, be less 
accurate than grammars, especially on common 
in-domain input, and may require a large amount 
of training data in order to achieve adequate lev-
els of performance. In the hybrid analyzer de-
scribed above, classifiers are used to identify the 
DA for domain specific portions of utterances 
that are not covered by the cross-domain gram-
mar. 
We tested classifiers trained to classify com-
plete DAs. We also split the DA classification 
task into two subtasks: speech act classification 
and concept sequence classification. This simpli-
fies the task of each classifier, allows for the use 
of different approaches and/or feature sets for 
each task, and reduces data sparseness. Our hy-
brid analyzer uses the output of each classifier 
along with the interlingua specification to iden-
tify the DA (Langley et al, 2002; Langley and 
Lavie, 2003). 
7 Experimental Setup 
We conducted experiments to assess the per-
formance of several machine-learning ap-
proaches on the DA classification tasks. We 
evaluated all of the classifiers on English and 
German input in the NESPOLE travel domain.  
7.1 Corpus 
The corpus used in all of the experiments was the 
NESPOLE! travel and tourism database. Since 
our goal was to evaluate the SA and concept se-
quence classifiers and not segmentation, we cre-
ated training examples for each SDU in the 
database rather than for each utterance. Table 4 
contains statistics regarding the contents of the 
corpus for our classification tasks. Table 5 shows 
the frequency of the most common domain ac-
tion, speech act, and concept sequence in the 
corpus. These frequencies provide a baseline that 
would be achieved by a simple classifier that al-
ways returned the most common class. 
 
 English German 
SDUs 8289 8719 
Domain Actions 972 1001 
Speech Acts 70 70 
Concept Sequences 615 638 
Vocabulary Size 1946 2815 
Table 4: Corpus Statistics. 
 
 English German 
DA (acknowledge) 19.2% 19.7% 
SA (give-information) 41.4% 40.7% 
Concept Sequence 
(No concepts) 
38.9% 40.3% 
Table 5: Most frequent DAs, SAs, and CSs. 
 
All of the results presented in this paper were 
produced using a 20-fold cross validation setup. 
The corpus was randomly divided into 20 sets of 
equal size. Each of the sets was held out as the 
test set for one fold with the remaining 19 sets 
used as training data. Within each language, the 
same random split was used for all of the classi-
fication experiments. Because the same split of 
the data was used for different classifiers, the 
results of two classifiers on the same test set are 
directly comparable. Thus, we tested for signifi-
cance using two-tailed matched pair t-tests. 
7.2 Machine Learning Approaches 
We evaluated the performance of four different 
machine-learning approaches on the DA classifi-
cation tasks: memory-based learning (k-Nearest-
Neighbor), decision trees, neural networks, and 
na?ve Bayes n-gram classifiers. We selected 
these approaches because they vary substantially 
in the their representations of the training data 
and their methods for selecting the best class. 
Our purpose was not to implement each ap-
proach from scratch but to test the approach for 
our particular task. Thus, we chose to use exist-
ing software for each approach ?off the shelf.? 
The ease of acquiring and setting up the software 
influenced our choice. Furthermore, the ease of 
incorporating the software into our online trans-
lation system was also a factor. 
Our memory-based classifiers were imple-
mented using TiMBL (Daelemans et al, 2002). 
We used C4.5 (Quinlan, 1993) for our decision 
tree classifiers. Our neural network classifiers 
were implemented using SNNS (Zell et al, 
1998). We used Rainbow (McCallum, 1996) for 
our na?ve Bayes n-gram classifiers. 
8 Experiments 
In our first experiment, we compared the per-
formance of the four machine learning ap-
proaches. Each SDU was parsed using the 
argument and pseudo-argument grammars de-
scribed above. The feature set for the DA and SA 
classifiers consisted of binary features indicating 
the presence or absence of labels from the 
grammars in the parse forest for the SDU. The 
feature set included 212 features for English and 
259 features for German. The concept sequence 
classifiers used the same feature set with the ad-
dition of the speech act. 
In the SA classification experiment, the 
TiMBL classifier used the IB1 (k-NN) algorithm 
with 1 neighbor and gain ratio feature weighting. 
The C4.5 classifier required at least one instance 
per branch and used node post-pruning. Both the 
TiMBL and C4.5 classifiers used the binary fea-
tures described above and produced the single 
best class as output. The SNNS classifier used a 
simple feed-forward network with 1 input unit 
for each binary feature, 1 hidden layer containing 
15 units, and 1 output unit for each speech act. 
The network was trained using backpropagation. 
The order of presentation of the training exam-
ples was randomized in each epoch, and the 
weights were updated after each training exam-
ple presentation. In order to simulate the binary 
features used by the other classifiers as closely as 
possible, the Rainbow classifier used a simple 
unigram model whose vocabulary was the set of 
labels included in the binary feature set. The 
setup for the DA classification experiment was 
identical except that the neural network had 50 
hidden units. 
The setup of the classifiers for the concept se-
quence classification experiment was very simi-
lar. The TiMBL and C4.5 classifiers were set up 
exactly as in the DA and SA experiments with 
one extra feature whose value was the speech act. 
The SNNS concept sequence classifier used a 
similar network with 50 hidden units. The SA 
feature was represented as a set of binary input 
units. The Rainbow classifier was set up exactly 
as in the DA and SA experiments. The SA fea-
ture was not included. 
As mentioned above, both experiments used a 
20-fold cross-validation setup. In each fold, the 
TiMBL, C4.5, and Rainbow classifiers were sim-
ply trained on 19 subsets of the data and tested 
on the remaining set. The SNNS classifiers re-
quired a more complex setup to determine the 
number of epochs to train the neural network for 
each test set. Within each fold, a cross-validation 
setup was used to determine the number of train-
ing epochs. Each of the 19 training subsets for a 
fold was used as a validation set. The network 
was trained on the remaining 18 subsets until the 
accuracy on the validation set did not improve 
for 50 consecutive epochs. The network was then 
trained on all 19 training subsets for the average 
number of epochs from the validation sets. This 
process was used for all 20-folds in the SA clas-
sification experiment. For the DA and concept 
sequence experiments, this process ran for ap-
proximately 1.5 days for each fold. Thus, this 
process was run for the first two folds, and the 
average number of epochs from those folds was 
used for training. 
 
 English German 
TiMBL 49.69% 46.51% 
C4.5 48.90% 46.58% 
SNNS 49.39% 46.21% 
Rainbow 39.74% 38.32% 
Table 6: Domain Action classifier accuracy. 
 
 English German 
TiMBL 69.82% 67.57% 
C4.5 70.41% 67.90% 
SNNS 71.52% 67.61% 
Rainbow 51.39% 46.00% 
Table 7: Speech Act classifier accuracy. 
 
 English German 
TiMBL 69.59% 67.08% 
C4.5 68.47% 66.45% 
SNNS 71.35% 68.67% 
Rainbow 51.64% 51.50% 
Table 8: Concept Sequence classifier accuracy. 
 Table 6, Table 7, and Table 8 show the aver-
age accuracy of each learning approach on the 
20-fold cross validation experiments for domain 
action, speech act, and concept classification re-
spectively. For DA classification, there were no 
significant differences between the TiMBL, 
C4.5, and SNNS classifiers for English or Ger-
man. In the SA experiment, the difference be-
tween the TiMBL and C4.5 classifiers for 
English was not significant. The SNNS classifier 
was significantly better than both TiMBL and 
C4.5 (at least p=0.0001). For German SA classi-
fication, there were no significant differences 
between the TiMBL, C4.5, and SNNS classifiers. 
For concept sequence classification, SNNS was 
significantly better than TiMBL and C4.5 (at 
least p=0.0001) for both English and German. 
For English only, TiMBL was significantly better 
than C4.5 (p=0.005). 
For both languages, the Rainbow classifier 
performed much worse than the other classifiers. 
However, the unigram model over arguments did 
not exploit the strengths of the n-gram classifica-
tion approach. Thus, we ran another experiment 
in which the Rainbow classifier was trained on 
simple word bigrams. No stemming or stop 
words were used in building the bigram models. 
 
 English German 
Domain Action 48.59% 48.09% 
Speech Act 79.00% 77.46% 
Concept Sequence 56.87% 57.77% 
Table 9: Rainbow accuracy with word bigrams. 
 
Table 9 shows the average accuracy of the 
Rainbow word bigram classifiers using the same 
20-fold cross-validation setup as in the previous 
experiments. As we expected, using word bi-
grams rather than parse label unigrams improved 
the performance of the Rainbow classifiers. For 
German DA classification, the word bigram clas-
sifier was significantly better than all of the pre-
vious German DA classifiers (at least p=0.005). 
Furthermore, the Rainbow word bigram SA clas-
sifiers for both languages outperformed all of the 
SA classifiers that used only the parse labels. 
Although the argument parse labels provide 
an abstraction of the words present in an SDU, 
the words themselves also clearly provided use-
ful information for classification, at least for the 
SA task. Thus, we conducted additional experi-
ments to examine whether combining parse and 
word information could further improve per-
formance. 
We chose to incorporate word information 
into the TiMBL classifiers used in the first ex-
periment. Although the SNNS SA classifier per-
formed significantly better than the TiMBL SA 
classifier for English, there was no significant 
difference for SA classification in German. Fur-
thermore, because of the complexity and time 
required for training with SNNS, we preferred 
working with TiMBL. 
We tested two approaches to adding word in-
formation to the TiMBL classifier. In both ap-
proaches, the word-based information for each 
fold was computed only based on the data in the 
training set. In our first approach, we added bi-
nary features for the 250 words that had the 
highest mutual information with the class. Each 
feature indicated the presence or absence of the 
word in the SDU. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 3 neighbors, and unweighted voting. The 
second approach we tested combined the Rain-
bow word bigram classifier with the TiMBL 
classifier. We added one input feature for each 
possible speech act to the TiMBL classifier. The 
value of each SA feature was the probability of 
the speech act computed by the Rainbow word 
bigram classifier. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 11 neighbors, and inverse linear distance 
weighted voting. 
 
 English German 
TiMBL + words 78.59% 75.98% 
TiMBL + Rainbow 81.25% 78.93% 
Table 10: Word+Parse SA classifier accuracy. 
 
Table 10 shows the average accuracy of the 
SA classifiers that combined parse and word in-
formation using the same 20-fold cross-
validation setup as the previous experiments. 
Although adding binary features for individual 
words improved performance over the classifiers 
with no word information, it did not allow the 
combined classifiers to outperform the Rainbow 
word bigram classifiers. However, for both 
languages, adding the probabilities computed by 
the Rainbow bigram model resulted in a SA clas-
sifier that outperformed all previous classifiers. 
The improvement in accuracy was highly signifi-
cant for both languages. 
We conducted a similar experiment for com-
bining parse and word information in the concept 
sequence classifiers. The first condition was 
analogous to the first condition in the combined 
SA classification experiment. The second condi-
tion was slightly different. A concept sequence 
can be broken down into a set of individual con-
cepts. The set of individual concepts is much 
smaller than the set of concept sequences (110 
for English and 111 for German). Thus, we used 
a Rainbow word bigram classifier to compute the 
probability of each individual concept rather than 
the complete concept sequence. The probabilities 
for the individual concepts were added to the 
parse label features for the combined classifier. 
In both conditions, the performance of the com-
bined classifiers was roughly the same as the 
classifiers that used only parse labels as features. 
 
 English German 
TiMBL + words 56.48% 54.98% 
Table 11: Word+Parse DA classifier accuracy. 
 
Table 11 shows the average accuracy of DA 
classifiers for English and German using a setup 
similar to the first approach in the combined SA 
experiment. In this experiment, we added binary 
features for the 250 words that the highest mu-
tual information with the class. We used a 
TiMBL classifier with gain ratio feature weight-
ing and one neighbor. The improvement in accu-
racy for both languages was highly significant. 
 
 English German 
TiMBL SA 
+ TiMBL CS 49.63% 46.50% 
TiMBL+Rainbow SA 
+ TiMBL CS 57.74% 53.93% 
Table 12: DA accuracy of SA+CS classifiers. 
 
Finally, Table 12 shows the results from two 
tests to compare the performance of combining 
the best output of the SA and concept sequence 
classifiers with the performance of the complete 
DA classifiers. In the first test, we combined the 
output from the TiMBL SA and CS classifiers 
shown in Table 7 and Table 8. The performance 
of the combined SA+CS classifiers was almost 
identical to that of the TiMBL DA classifiers 
shown in Table 6. In the second test, we com-
bined our best SA classifier (TiMBL+Rainbow, 
shown in Table 10) with the TiMBL CS classi-
fier. In this case, we had mixed results. The per-
formance of the combined classifiers was better 
than our best DA classifier for English and worse 
for German. 
9 Discussion 
One of our main goals was to determine the fea-
sibility of automatically classifying domain ac-
tions. As the data in Table 4 show, DA 
classification is a challenging problem with ap-
proximately 1000 classes. Even when the task is 
divided into subproblems of identifying the SA 
and concept sequence, the subtasks remain diffi-
cult. The difficulty is compounded by relatively 
sparse training data with unevenly distributed 
classes. Although the most common classes in 
our training corpus had over 1000 training exam-
ples, many of the classes had only 1 or 2 exam-
ples. 
Despite these difficulties, our results indicate 
that domain action classification is feasible. For 
SA classification in particular we were able to 
achieve very strong performance. Although per-
formance on concept sequence and DA classifi-
cation is not as high, it is still quite strong, 
especially given that there are an order of magni-
tude more classes than in SA classification. 
Based on our experiments, it appears that all of 
the learning approaches we tested were able to 
cope with data sparseness at the level found in 
our data, with the possible exception of the na?ve 
Bayes n-gram approach (Rainbow) for the con-
cept sequence task. 
One additional point worth noting is that there 
is evidence that domain action classification 
could be performed reasonably well using only 
word-based information. Although our best-
performing classifiers combined word and argu-
ment parse information, the na?ve Bayes word 
bigram classifier (Rainbow) performed very well 
on the SA classification task. With additional 
data, the performance of the concept sequence 
and DA word bigram classifiers could be ex-
pected to improve. Cattoni et al (2001) also ap-
ply statistical language models to DA 
classification. A word bigram model is trained 
for each DA, and the DA with the highest likeli-
hood is assigned to each SDU. Arguments are 
identified using recursive transition networks, 
and interlingua specification constraints are used 
to find the most likely valid interlingua represen-
tation. Although it is clear that argument infor-
mation is useful for the task, it appears that 
words alone can be used to achieve reasonable 
performance. 
Another goal of our experiments was to help 
in the selection of a machine learning approach 
to be used in our hybrid analyzer. Certainly one 
of the most important considerations is how well 
the learning approach performs the task. For SA 
classification, the combination of parse features 
and word bigram probabilities clearly gave the 
best performance. For concept sequence classifi-
cation, no learning approach clearly outper-
formed any other (with the exception that the 
na?ve Bayes n-gram approach performed worse 
than other approaches). However, the perform-
ance of the classifiers is not the only considera-
tion to be made in selecting the classifier for our 
hybrid analyzer. 
Several additional factors are also important 
in selecting the particular machine learning ap-
proach to be used. One important attribute of the 
learning approach is the speed of both classifica-
tion and training. Since the classifiers are part of 
a translation system designed for use between 
two humans to facilitate (near) real-time com-
munication, the DA classifiers must classify in-
dividual utterances online very quickly. 
Furthermore, since humans must write and test 
the argument grammars, training and batch 
classification should be fast so that the grammar 
writers can update the grammars, retrain the clas-
sifiers, and test efficiently. 
The machine learning approach should also 
be able to easily accommodate both continuous 
and discrete features from a variety of sources. 
Possible sources for features include words 
and/or phrases in an utterance, the argument 
parse, the interlingua representation of the argu-
ments, and properties of the dialogue (e.g. 
speaker tag). The classifier should be able to eas-
ily combine features from any or all of these 
sources. 
Another desirable attribute for the machine 
learning approach is the ability to produce a 
ranked list of possible classes. Our interlingua 
specification defines how speech acts and con-
cepts are allowed to combine as well as how ar-
guments are licensed by the domain action. 
These constraints can be used to select an alter-
native DA if the best DA violates the specifica-
tion. 
Based on all of these considerations, the 
TiMBL+Rainbow classifier, which combines 
parse label features with word bigram probabili-
ties, seems like an excellent choice for speech act 
classification. It was the most accurate classifier 
that we tested. Furthermore, the main TiMBL 
classifier meets all of the requirements discussed 
above except the ability to produce a complete 
ranked list of the classes for each instance. How-
ever, such a list could be produced as a backup 
from the Rainbow probability features. Adding 
new features to the combined classifier would 
also be very easy because TiMBL was the pri-
mary classifier in the combination. Finally, since 
both TiMBL and Rainbow provide an online 
server mode for classifying single instances, in-
corporating the combined classifier into an 
online translation system would not be difficult. 
Since there were no significant differences in the 
performance of most of the concept sequence 
classifiers, this combined approach is probably 
also a good option for that task. 
10 Conclusion 
We have described a representation of 
speaker intention that includes domain independ-
ent speech acts as well as domain dependent do-
main actions. We have shown that domain 
actions are a useful level of abstraction for ma-
chine translation of task-oriented dialogue, and 
that, in spite of their domain specificity, they are 
scalable to larger domains and portable to new 
domains.  
We have also presented classifiers for domain 
actions that have been comparatively tested and 
used successfully in the NESPOLE speech-to-
speech translation system. We experimentally 
compared the effectiveness of several machine-
learning approaches for classification of domain 
actions, speech acts, and concept sequences on 
two input languages. Despite the difficulty of the 
classification tasks due to a large number of 
classes and relatively sparse data, the classifiers 
exhibited strong performance on all tasks. We 
also demonstrated how the combination of two 
learning approaches could be used to improve 
performance and overcome the weaknesses of 
the individual approaches. 
Acknowledgements: NESPOLE was funded 
by NSF (Grant number 9982227) and the EU. 
The NESPOLE partners are ITC-irst, Universite 
Joseph Fourrier, Universitat Karlsruhe, APT 
Trentino travel board, and AETHRA telecom-
munications. We would like to acknowledge the 
contribution of the following people in particu-
lar: Fabio Pianesi, Emanuele Pianta, Nadia 
Mana, and Herve Blanchon. 
References 
Cattoni, R., M. Federico, and A. Lavie. 2001. Robust 
Analysis of Spoken Input Combining Statistical 
and Knowledge-Based Information Sources. In 
Proceedings of the IEEE ASRU Workshop, Trento, 
Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2002. TiMBL: Tilburg Memory 
Based Learner, version 4.3, Reference Guide. ILK 
Technical Report 02-10. Available from 
http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.
gz. 
Fillmore, C.J., Kay, P. and O'Connor, M.C. 1988. 
Regularity and Idiomaticity in Grammatical Con-
structions. Language, 64(3), 501-538. 
Gavald?, M. 2000. SOUP: A Parser for Real-World 
Spontaneous Speech. In Proceedings of IWPT-
2000, Trento, Italy. 
Goldberg, Adele E. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. 
Chicago University Press. 
Langley, C. and A. Lavie. 2003. Parsing Domain Ac-
tions with Phrase-Level Grammars and Memory-
Based Learners. To appear in Proceedings of 
IWPT-2003. Nancy, France. 
Langley, C., A. Lavie, L. Levin, D. Wallace, D. 
Gates, and K. Peterson. 2002. Spoken Language 
Parsing Using Phrase-Level Grammars and Train-
able Classifiers. In Workshop on Algorithms for 
Speech-to-Speech Machine Translation at ACL-02. 
Philadelphia, PA. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. Enhancing 
the Usability and Performance of NESPOLE! ? a 
Real-World Speech-to-Speech Translation System. 
In Proceedings of HLT-2002. San Diego, CA. 
Levin, L., D. Gates, A. Lavie, A. Waibel. 1998. An 
Interlingua Based on Domain Actions for Machine 
Translation of Task-Oriented Dialogues. In Pro-
ceedings of ICSLP 98, Vol. 4, pages 1155-1158, 
Sydney, Australia. 
Levin, L., D. Gates, D. Wallace, K. Peterson, A. La-
vie F. Pianesi, E. Pianta, R. Cattoni, N. Mana. 
2002. Balancing Expressiveness and Simplicity in 
an Interlingua for Task Based Dialogue. In Pro-
ceedings of Workshop on Spoken Language Trans-
lation. ACL-02, Philadelphia. 
McCallum, A. K. 1996. Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. 
http://www.cs.cmu.edu/~mccallum/bow. 
Qu, Y., B. DiEugenio, A. Lavie, L. Levin and C.P. 
Rose. 1997. Minimizing Cumulative Error in Dis-
course Context. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-96 
Workshop, E. Maier, M. Mast and S. LuperFoy 
(eds.), LNCS series, Springer Verlag. 
Qu, Y., C. P. Rose, and B. DiEugenio. 1996. Using 
Discourse Predictions for Ambiguity Resolution. In 
Proceedings of COLING-1996. 
Quinlan, J. R. 1993. C4.5: Programs for Machine 
Learning. San Mateo: Morgan Kaufmann. 
Reithinger, N., R. Engel, M. Kipp, M. Klesen. 1996. 
Predicting Dialogue Acts for a Speech-To-Speech 
Translation System. DFKI GmbH Saarbruecken. 
Verbmobil-Report 151. 
http://verbmobil.dfki.de/cgi-
bin/verbmobil/htbin/doc-access.cgi 
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. 
Bates, D. Jurafsky, P. Taylor, R. Martin, M. 
Meteer, and C. Van Ess-Dykema. 2000. Dialogue 
Act Modeling for Automatic Tagging and Recogni-
tion of Conversational Speech. Computational Lin-
guistics 26:3, 339-371.  
Zell, A., G. Mamier, M. Vogt, et al 1998. SNNS: 
Stuttgart Neural Network Simulator User Manual, 
Version 4.2. 
