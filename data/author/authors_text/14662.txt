Coling 2008: Companion volume ? Posters and Demonstrations, pages 51?54
Manchester, August 2008
Scaling up Analogical Learning
Philippe Langlais
Universite? de Montre?al / Dept. I.R.O.
C.P. 6128, Que?bec, H3C3J7, Canada
felipe@iro.umontreal.ca
Franc?ois Yvon
Univ. Paris Sud 11 & LIMSI-CNRS
F-91401 Orsay, France
yvon@limsi.fr
Abstract
Recent years have witnessed a growing in-
terest in analogical learning for NLP ap-
plications. If the principle of analogical
learning is quite simple, it does involve
complex steps that seriously limit its ap-
plicability, the most computationally de-
manding one being the identification of
analogies in the input space. In this study,
we investigate different strategies for ef-
ficiently solving this problem and study
their scalability.
1 Introduction
Analogical learning (Pirrelli and Yvon, 1999) be-
longs to the family of lazy learning techniques
(Aha, 1997). It allows to map forms belong-
ing to an input space I into forms of an output
space O, thanks to a set of known observations,
L = {(i, o) : i ? I, o ? O}. I(u) and O(u)
respectively denote the projection of an observa-
tion u into the input space and output space: if
u ? (i, o), then I(u) ? i and O(u) ? o. For an
incomplete observation u ? (i, ?), the inference of
O(u) involves the following steps:
1. building E
I
(u) the set of analogical triplets
of I(u), that is E
I
(u) = {(s, v, w) ? L
3
:
[I(s) : I(v) = I(w) : I(u) ]}
2. building the set of solutions to the target equa-
tions formed by projecting source triplets:
E
O
(u) = {t ? O : [O(s) : O(v) = O(w) :
t ] ,?(s, v, w) ? E
I
(u)}
3. selecting candidates among E
O
(u).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
where [x : y = z : t ] denotes an analogical pro-
portion, that is a relation between these four items,
meaning that ?x is to y as z is to t?, in a sense to
be specified. See (Lepage, 1998) or (Stroppa and
Yvon, 2005) for possible interpretations.
Analogical learning has recently regained some
interest in the NLP community. Lepage and De-
noual (2005) proposed a machine translation sys-
tem entirely based on the concept of formal anal-
ogy, that is, analogy on forms. Stroppa and
Yvon (2005) applied analogical learning to sev-
eral morphological tasks also involving analogies
on words. Langlais and Patry (2007) applied it to
the task of translating unknown words in several
European languages, an idea investigated as well
by Denoual (2007) for a Japanese to English trans-
lation task.
If the principle of analogical learning is quite
simple, it does involve complex steps that seriously
limit its applicability. As a matter of fact, we are
only aware of studies where analogical learning is
applied to restricted tasks, either because they ar-
bitrarily concentrate on words (Stroppa and Yvon,
2005; Langlais and Patry, 2007; Denoual, 2007)
or because they focus on limited data (Lepage and
Denoual, 2005; Denoual, 2007).
In this study, we investigate different strategies
for making step 1 of analogical learning tractable.
We propose a data-structure and algorithms that
allow to control the balance between speed and
recall. For very high-dimensional input spaces
(hundreds of thousand of elements), we propose
a heuristic which reduces computation time with a
limited impact on recall.
2 Identifying input analogical relations
2.1 Existing approaches
A brute-force approach for identifying the input
triplets that define an analogy with the incomplete
observation u = (t , ?) consists in enumerating
51
triplets in the input space and checking for an ana-
logical relation with the unknown form t :
E
I
(u) = { ?x, y, z? : ?x, y, z? ? I
3
,
[x : y = z : t ] }
This amounts to check o(|I|3) analogies, which is
manageable for toy problems only.
Langlais and Patry (2007) deal with an input
space in the order of tens of thousand forms (the
typical size of a vocabulary) using following strat-
egy for E
I
(u). It consists in solving analogical
equations [y : x = t : ? ] for some pairs ?x, y?
belonging to the neighborhood1 of I(u), denoted
N (t). Those solutions that belong to the input
space are the z-forms retained.
E
I
(u) = { ?x, y, z? : ?x, y? ? N (t)
2
,
[y : x = t : z] }
This strategy (hereafter named LP) directly fol-
lows from a symmetrical property of an analogy
([x : y = z : t ] ? [y : x = t : z]), and reduces
the search procedure to the resolution of a number
of analogical equations which is quadratic with the
number of pairs ?x, y? sampled.
2.2 Exhaustive tree-count search
The strategy we propose here exploits a prop-
erty on character counts that an analogical relation
must fulfill (Lepage, 1998):
[x : y = z : t ] ? |x|
c
+ |t |
c
= |y|
c
+ |z|
c
?c ? A
where A is the alphabet on which the forms are
built, and |x|
c
stands for the number of occur-
rences of character c in x . In the sequel, we de-
note C(?x, t?) = {?y, z? ? I2 : |x|
c
+ |t |
c
=
|y|
c
+ |z|
c
?c ? A} the set of pairs that satisfy
the count property with respect to ?x, t? .
The strategy we propose consists in first select-
ing an x-form in the input space. This enforces a
set of necessary constraints on the counts of char-
acters that any two forms y and z must satisfy for
[x : y = z : t ] to be true. By considering all forms
x in turn,2 we collect a set of candidate triplets for
t . A verification of those that define with t a anal-
ogy must then be carried out. Formally, we built:
E
I
(u) = { ?x, y, z? : x ? I,
?y, z? ? C(?x, t?),
[x : y = z : t ] }
1The authors proposed to sample x and y among the clos-
est forms in terms of edit-distance to I(u) .
2Anagram forms do not have to be considered separately.
This strategy will only work if (i) the number
of quadruplets to check is much smaller than the
number of triplets we can form in the input space
(which happens to be the case in practice), and if
(ii) we can efficiently identify the pairs ?y, z? that
satisfy a set of constraints on character counts. To
this end, we propose to organize the input space
thanks to a data structure called a tree-count (see
Section 3), which is easy to built and supports effi-
cient runtime retrieval.
2.3 Sampled tree-count search
As shown in (Langlais and Yvon, 2008), using
tree-count to constraint the search allows to ex-
haustively solve step 1 for reasonably large input
spaces. Computing analogies in very large input
space (hundreds of thousand forms) however re-
mains computationally demanding, as the retrieval
algorithm must be carried out o(I) times. In this
case, we propose to sample the x-forms:
E
I
(u) = { ?x, y, z? : x ? N (t),
?y, z? ? C(?x, t?),
[x : y = t : z] }
There is unfortunately no obvious way of se-
lecting a good subset N (t) of input forms, as
analogies does not necessarily entail the similar-
ity of ?diagonal? forms, as illustrated by the anal-
ogy [une pomme verte : des pommes vertes =
une voiture rouge : des voitures rouges], which
involves singular/plural commutations in French
nominal groups. In this situation, randomly se-
lecting a subset of the input space seems to be a
reasonable strategy (hereafter RAND).
For some analogies however, the first and
last forms share some sequences of charac-
ters. This is obvious in [dream : dreamer =
dreams : dreamers], but can be more subtle, as
in our first example [This guy drinks too much :
This boat sinks = These guys drank too much :
These boats sank ] where the diagonal terms
share some n-grams reminiscent of the number
(This/These) and tense (drink /drank ) commuta-
tions involved.
We thus propose a sampling strategy (hereafter
EV) which selects x-forms that share with t some
sequences of characters. To this end, input forms
are represented in a vector space whose dimen-
sions are frequent character n-grams, retaining the
k-most frequent n-grams, where n ? [min;max].
A form is thus encoded as a binary vector of
52
dimension k, in which ith coefficient indicates
whether the form contains an occurrence of the ith
n-gram.3 At runtime, we select the N forms that
are the closest to a given form t , according to a
distance4. Figure 1 illustrates some forms selected
by this process. For comparison purposes, we also
tested a sampling strategy which consists in select-
ing the x-forms that are closest to the source form
t , according to the usual edit-distance (hereafter
ED).
establish a report ? order to establish a ? has
tabled this report ? is about the report ? basis
of the report ? other problem is that ? problem
that arises ? problem is that those
Figure 1: The 8 nearest neighbors of to establish
a report in a vector space computed from an input
space of over a million phrases.
3 The tree-count data-structure
A tree-count is a tree which encodes a set of forms.
Nodes are labeled by an alphabetical symbol and
contain a (possibly empty) set of pointers to forms.
A vertice from a node n labeled c to a node m is
weighted by the count of c in the forms encoded
by m, that is, the set of forms that can be reached
from this node and its descendants. Thus, a path
in a tree-count represents a set of constraints on
the counts of the characters encountered along this
path. This structure allows for instance the identi-
fication of anagrams in a set of forms: it suffices to
search the tree-count for nodes that contain more
than one pointer to forms in the vocabulary.
An example of a tree-count is provided in Fig-
ure 2 for a small set of forms. The node double
circled in this figure is labeled by the character d
and encodes the 6 input forms that contain 1 oc-
currence of ?o? and 1 occurrence of ?s?. One form
is os , referenced by the pointer m , the other five
forms are found by descending the tree from this
node; among which gods and dogs , two anagrams
encoded by the leave which set of pointers is b, k.
3.1 Construction time
The construction of a tree-count from a set of
forms only needs an arbitrary order on the char-
acters of the alphabet. This is the order in which
we will encounter them while descending the
3Typical values are min=max=3 and k=20000.
4We used the Manhattan distance in this study.
?u
?
p
a,l
?a
n
?
g
b,k
c
? k
? y
f,i
g,h
 s
? l
e,j
dm
d
? m
? t? s
? o
0
1 2
1
1 1
1
1
1
0
0 1
1
1
21
1
20
1
Figure 2: The tree-count encoding the set:
{soup(a), gods(b), odds(c), sos(d), solo(e),
tokyo(f), moot(g), moto(h), kyoto(i), oslo(j),
dogs(k), opus(l), os(m), a(n)}. The character la-
beling a node is represented in a box; the counts of
each character labels each vertice. Roman letters
in nodes represent pointers to input forms; greek
symbols label internal nodes.
tree. The lack of space prevents us to report the
construction algorithm (see (Langlais and Yvon,
2008)), but it is important to note that it only in-
volves a simple traversal of the input forms and is
therefore time efficient. Also worth mentioning,
our construction procedure only stores necessary
nodes. This means that when enumerating char-
acters in order, we only store zero-count nodes as
required. As a result, the depth of a tree-count is
typically much lower than the size of the alphabet.
3.2 Retrieval time
The retrieval of C(?x, t?) can be performed by
traversing the tree-count while maintaining a fron-
tier, that is, the set of pairs of nodes in the tree-
count that satisfy the constraints on counts encoun-
tered so far. Imagine, for instance, that we are
looking for the pairs of forms that contain exactly
3 occurrences of characters o , 2 of characters s
and 1 character l , and no other character. Start-
ing from the root node labelled by o , there is only
one pair of nodes that satisfy the constraint on o:
the frontier is therefore {(?, ?)}. The constraint
on s leads to the frontier {(d, ?)} (since the count
of t must be null). Finally, descending this node
yields the frontier {(m, (e, j))}, which identifies
the pairs (os, solo) and (os, oslo) to be the only
53
ones satisfying the initial set of constraints.
The complexity of retrieval is mainly dominated
by the size of the frontier built while traversing a
tree-count. In practice, because of the sparsity of
the space we manipulate in NLP applications, re-
trieval is also a fast operation.
4 Checking for an analogy
Stroppa (2005) provides a dynamic programming
algorithm for checking that a quadruplet is an anal-
ogy, whose complexity is o(|x| ? |y| ? |z| ? |t |).5
Depending on the application, a large number of
calls to this algorithm must be performed during
step 1 of analogical learning. The following prop-
erty helps cutting down the computations:
[x : y = z : t ] ?
(x[1] ? {y[1], z[1]}) ? (t [1] ? {y[1], z[1]})
(x[$] ? {y[$], z[$]}) ? (t [$] ? {y[$], z[$]})
where ?[$] denotes the last character of ?. A simple
and efficient trick consists in calling the analogy
checking routine only for those triplets that pass
this test.
5 Discussion
We investigated the aforementioned search strate-
gies by translating 1 000 new words (resp. phrases)
thanks to a translation table populated with pairs of
words (resp. pairs of phrases). We studied the scal-
ability of each strategy by varying the size of the
transfer table (small, medium, large). Precise fig-
ures can be found in (Langlais and Yvon, 2008);
we summarize here the main outcomes.
On the word-task, we compared the tree-count
search strategy to the LP one. On the largest word-
set (84 000 input words), the former (exact) strat-
egy could find an average of 34 597 input analogies
for 964 test-words at an average response time of
1.2 seconds per word, while with the latter strat-
egy, an average of 56 analogies could be identified
for 890 test-words, in an average of 6.3 seconds.
On the sequence-task, where input spaces are
much larger, we compared the various sampling
strategies presented in Section 2.3. We set N, the
number of sampled input forms, to 103 for all
sampling strategies. On the medium size dataset
(293 000 input phrases), both ED and RAND per-
form badly compared to EV. With the two for-
mer filtering strategies, we could at best identify
5In this study, we used the definition of a formal analogy
provided by Stroppa and Yvon (2005). Lepage (1998) pro-
poses a less general definition, which is faster to check.
17 input analogies for 38% of the test-phrases (at
an average response time of 9 seconds), while with
EV, an average 46 analogies could be identified for
75% of the test-phrases (in 3 seconds on average).
Finally, we checked that the approach we pro-
posed scales to very large datasets (several mil-
lions of input phrases), which to the best of our
knowledge is simply out of the reach of existing
approaches. This opens up interesting prospects
for analogical learning, such as enriching a phrase-
based table of the kind being used in statistical ma-
chine translation.
Acknowledgment
This study has been accomplished while the first
author was visiting Te?le?com ParisTech.
References
Aha, David A. 1997. Editorial. Artificial Intelligence
Review, 11(1-5):7?10. Special Issue on Lazy Learn-
ing.
Denoual, Etienne. 2007. Analogical translation of
unknown words in a statistical machine translation
framework. In Machine Translation Summit, XI,
Copenhagen, Sept. 10-14.
Langlais, Philippe and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
EMNLP-CoNLL, pages 877?886, Prague, Czech Re-
public, June.
Langlais, Philippe and Franc?ois Yvon. 2008. Scaling
up analogies. Technical report, Te?le?com ParisTech,
France.
Lepage, Yves and ?Etienne Denoual. 2005. Purest
ever example-based machine translation: Detailed
presentation and assessment. Machine Translation,
29:251?282.
Lepage, Yves. 1998. Solving analogies on words: an
algorithm. In COLING-ACL, pages 728?734, Mon-
treal, Canada.
Pirrelli, Vitto and Franc?ois Yvon. 1999. The hidden
dimension: a paradigmatic view of data-driven NLP.
Journal of Experimental & Theroretical Artifical In-
telligence, 11:391?408.
Stroppa, Nicolas and Franc?ois Yvon. 2005. An ana-
logical learner for morphological analysis. In 9th
Conf. on Computational Natural Language Learning
(CoNLL), pages 120?127, Ann Arbor, MI, June.
Stroppa, Nicolas. 2005. De?finitions et caracte?risations
de mode`les a` base d?analogies pour l?apprentissage
automatique des langues naturelles. Ph.D. thesis,
ENST, Paris, France, Nov.
54
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487?495,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Improvements in Analogical Learning:
Application to Translating multi-Terms of the Medical Domain
Philippe Langlais
DIRO
Univ. of Montreal, Canada
felipe@iro.umontreal.ca
Franc?ois Yvon and Pierre Zweigenbaum
LIMSI-CNRS
Univ. Paris-Sud XI, France
{yvon,pz}@limsi.fr
Abstract
Handling terminology is an important
matter in a translation workflow. However,
current Machine Translation (MT) sys-
tems do not yet propose anything proactive
upon tools which assist in managing termi-
nological databases. In this work, we in-
vestigate several enhancements to analog-
ical learning and test our implementation
on translating medical terms. We show
that the analogical engine works equally
well when translating from and into a mor-
phologically rich language, or when deal-
ing with language pairs written in differ-
ent scripts. Combining it with a phrase-
based statistical engine leads to significant
improvements.
1 Introduction
If machine translation is to meet commercial
needs, it must offer a sensible approach to trans-
lating terms. Currently, MT systems offer at best
database management tools which allow a human
(typically a translator, a terminologist or even the
vendor of the system) to specify bilingual ter-
minological entries. More advanced tools are
meant to identify inconsistencies in terminological
translations and might prove useful in controlled-
language situations (Itagaki et al, 2007).
One approach to translate terms consists in us-
ing a domain-specific parallel corpus with stan-
dard alignment techniques (Brown et al, 1993) to
mine new translations. Massive amounts of par-
allel data are certainly available in several pairs
of languages for domains such as parliament de-
bates or the like. However, having at our disposal
a domain-specific (e.g. computer science) bitext
with an adequate coverage is another issue. One
might argue that domain-specific comparable (or
perhaps unrelated) corpora are easier to acquire,
in which case context-vector techniques (Rapp,
1995; Fung and McKeown, 1997) can be used
to identify the translation of terms. We certainly
agree with that point of view to a certain extent,
but as discussed by Morin et al (2007), for many
specific domains and pairs of languages, such re-
sources simply do not exist. Furthermore, the task
of translation identification is more difficult and
error-prone.
Analogical learning has recently regained some
interest in the NLP community. Lepage and De-
noual (2005) proposed a machine translation sys-
tem entirely based on the concept of formal anal-
ogy, that is, analogy on forms. Stroppa and
Yvon (2005) applied analogical learning to sev-
eral morphological tasks also involving analogies
on words. Langlais and Patry (2007) applied it to
the task of translating unknown words in several
European languages, an idea investigated as well
by Denoual (2007) for a Japanese to English trans-
lation task.
In this study, we improve the state-of-the-art of
analogical learning by (i) proposing a simple yet
effective implementation of an analogical solver;
(ii) proposing an efficient solution to the search is-
sue embedded in analogical learning, (iii) investi-
gating whether a classifier can be trained to recog-
nize bad candidates produced by analogical learn-
ing. We evaluate our analogical engine on the task
of translating terms of the medical domain; a do-
main well-known for its tendency to create new
words, many of which being complex lexical con-
structions. Our experiments involve five language
pairs, including languages with very different mor-
phological systems.
487
In the remainder of this paper, we first present
in Section 2 the principle of analogical learn-
ing. Practical issues in analogical learning are
discussed in Section 3 along with our solutions.
In Section 4, we report on experiments we con-
ducted with our analogical device. We conclude
this study and discuss future work in Section 5.
2 Analogical Learning
2.1 Definitions
A proportional analogy, or analogy for short, is a
relation between four items noted [x : y = z : t ]
which reads as ?x is to y as z is to t?. Among pro-
portional analogies, we distinguish formal analo-
gies, that is, those we can identify at a graphemic
level, such as [adrenergic beta-agonists, adren-
ergic beta-antagonists, adrenergic alpha-agonists,
adrenergic alpha-antagonists].
Formal analogies can be defined in terms of
factorizations1. Let x be a string over an alpha-
bet ?, a factorization of x, noted fx, is a se-
quence of n factors fx = (f1x, . . . , f
n
x ), such that
x = f1x  f
2
x  . . .  f
n
x , where  denotes the
concatenation operator. After (Stroppa and Yvon,
2005) we thus define a formal analogy as:
Definition 1 ?(x, y, z, t) ? ??
4
, [x : y = z : t] iff
there exist factorizations (fx, fy, fz, ft) ? (?
?d)4
of (x, y, z, t) such that, ?i ? [1, d], (f iy, f
i
z) ?{
(f ix, f
i
t ), (f
i
t , f
i
x)
}
. The smallest d for which this
definition holds is called the degree of the analogy.
Intuitively, this definition states that (x, y, z, t)
are made up of a common set of alternating sub-
strings. It is routine to check that it captures the
exemplar analogy introduced above, based on the
following set of factorizations:
fx ? (adrenergic bet, a-agonists)
fy ? (adrenergic bet, a-antagonists)
fz ? (adrenergic alph, a-agonists)
ft ? (adrenergic alph, a-antagonists)
As no smaller factorization can be found, the de-
gree of this analogy is 2. In the sequel, we call
an analogical equation an analogy where one item
(usually the fourth) is missing and we note it [x :
y = z : ? ].
1Factorizations of strings correspond to segmentations.
We keep the former term, to emphasize the genericity of the
definition, which remains valid for other algebraic structures,
for which factorization and segmentation are no longer syno-
mymous.
2.2 Analogical Inference
Let L = {(i, o) | i ? I, o ? O} be a learning set
of observations, where I (O) is the set of possible
forms of the input (output) linguistic system under
study. We denote I(u) (O(u)) the projection of u
into the input (output) space; that is, if u = (i, o),
then I(u) ? i and O(u) ? o. For an incomplete
observation u = (i, ?), the inference procedure is:
1. building EI(u) = {(x, y, z) ? L3 | [I(x) :
I(y) = I(z) : I(u) ]}, the set of input triplets
that define an analogy with I(u) .
2. building EO(u) = {o ? O | ?(x, y, z) ?
EI(u) s.t. [O(x) : O(y) = O(z) : o]} the set
of solutions to the equations obtained by pro-
jecting the triplets of EI(u) into the output
space.
3. selecting candidates among EO(u).
In the sequel, we distinguish the generator
which implements the first two steps, from the se-
lector which implements step 3.
To give an example, assume L contains
the following entries: (beeta-agonistit, adren-
ergic beta-agonists), (beetasalpaajat, adrenergic
beta-antagonists) and (alfa-agonistit, adrener-
gic alpha-agonists). We might translate the
Finnish term alfasalpaajat into the English term
adrenergic alpha-antagonists by 1) identifying
the input triplet: (beeta-agonistit, beetasalpaa-
jat, alfa-agonistit) ; 2) projecting it into the equa-
tion [adrenergic beta-agonists : adrenergic beta-
antagonists = adrenergic alpha-agonists : ? ]; and
solving it: adrenergic alpha-antagonists is one of
its solutions.
During inference, analogies are recognized in-
dependently in the input and the output space, and
nothing pre-establishes which subpart of one in-
put form corresponds to which subpart of the out-
put one. This ?knowledge? is passively captured
thanks to the inductive bias of the learning strat-
egy (an analogy in the input space corresponds to
one in the output space). Also worth mentioning,
this procedure does not rely on any pre-defined no-
tion of word. This might come at an advantage for
languages that are hard to segment (Lepage and
Lardilleux, 2007).
3 Practical issues
Each step of analogical learning, that is, search-
ing for input triplets, solving output equations and
488
selecting good candidates involves some practical
issues. Since searching for input triplets might in-
volve the need for solving (input) equations, we
discuss the solver first.
3.1 The solver
Lepage (1998) proposed an algorithm for solving
an analogical equation [x : y = z : ? ]. An
alignment between x and y and between x and z
is first computed (by edit-distance) as illustrated
in Figure 1. Then, the three strings are synchro-
nized using x as a backbone of the synchroniza-
tion. The algorithm can be seen as a deterministic
finite-state machine where a state is defined by the
two edit-operations being visited in the two tables.
This is schematized by the two cursors in the fig-
ure. Two actions are allowed: copy one symbol
from y or z into the solution and move one or both
cursors.
x: r e a d e r x: r e a d e r
y: r e a d a b l e z: d o e r
4 4
Figure 1: Illustration of the synchronization done
by the solver described in (Lepage, 1998).
There are two things to realize with this algo-
rithm. First, since several (minimal-cost) align-
ments can be found between two strings, several
synchronizations are typically carried out while
solving an equation, leading to (possibly many)
different solutions. Indeed, in adverse situations,
an exponential number of synchronizations will
have to be computed. Second, the algorithm fails
to deliver an expected form in a rather frequent
situation where two identical symbols align fortu-
itously in two strings. This is for instance the case
in our running example where the symbol d in
doer aligns to the one in reader, which puzzles the
synchronization. Indeed, dabloe is the only form
proposed to [reader : readable = doer : ? ], while
the expected one is doable. The algorithm would
have no problem, however, to produce the form
writable out of the equation [reader : readable =
writer : ? ].
Yvon et al (2004) proposed an analogical
solver which is not exposed to the latter prob-
lem. It consists in building a finite state transducer
which generates the solutions to [x : y = z : ? ]
while recognizing the form x.
Theorem 1 t is a solution to [x : y = z : ?] iff
t belongs to {y ? z}\x.
shuffle and complement are two rational op-
erations. The shuffle of two strings w and
v, noted w ? v, is the regular language con-
taining the strings obtained by selecting (with-
out replacement) alternatively in w and v, se-
quences of characters in a left-to-right man-
ner. For instance, spondyondontilalgiatis and
ondspondonylaltitisgia are two strings belong-
ing to spondylalgia ? ondontitis). The comple-
mentary set of w with respect to v, noted w\v, is
the set of strings formed by removing from w, in
a left-to-right manner, the symbols in v. For in-
stance, spondylitis and spydoniltis are belong-
ing to spondyondontilalgiatis \ ondontalgia.
Our implementation of the two rational operations
are sketched in Algorithm 1.
Because the shuffle of two strings may con-
tain an exponential number of elements with re-
spect to the length of those strings, building such
an automaton may face combinatorial problems.
Our solution simply consists in randomly sam-
pling strings in the shuffle set. Our solver, depicted
in Algorithm 2, is thus controlled by a sampling
size s, the impact of which is illustrated in Ta-
ble 1. By increasing s, the solver generates more
(mostly spurious) solutions, but also increases the
relative frequency with which the expected output
is generated. In practice, provided a large enough
sampling size,2 the expected form very often ap-
pears among the most frequent ones.
s nb (solution,frequency)
10 11 (doable,7) (dabloe,3) (adbloe,3)
102 22 (doable,28) (dabloe,21) (abldoe,21)
103 29 (doable,333) (dabloe,196) (abldoe,164)
Table 1: The 3-most frequent solutions generated
by our solver, for different sampling sizes s, for
the equation [reader : readable = doer : ? ]. nb
indicates the number of (different) solutions gen-
erated. According to our definition, there are 32
distinct solutions to this equation. Note that our
solver has no problem producing doable.
3.2 Searching for input triplets
A brute-force approach to identifying the input
triplets that define an analogy with the incom-
plete observation u = (t, ?) consists in enumerat-
ing triplets in the input space and checking for an
2We used s = 2 000 in this study.
489
function shuffle(y,z)
Input: ?y, z? two forms
Output: a random word in y ? z
if y =  then
return z
else
n? rand(1,|y|)
return y[1:n] . shuffle(z,y[n+1:])
function complementary(m,x,r,s)
Input: m ? y ? z, x
Output: the set m \ x
if (m = ) then
if (x = ) then
s? s ? r
else
complementary(m[2:],x,r.m[1],s)
if m[1] = x[1] then
complementary(m[2:],x[2:],r,s)
Algorithm 1: Simulation of the two rational op-
erations required by the solver. x[a:b] denotes the
sequence of symbols x starting from index a to
index b inclusive. x[a:] denotes the suffix of x
starting at index a.
analogical relation with t. This amounts to check
o(|I|3) analogies, which is manageable for toy
problems only. Instead, Langlais and Patry (2007)
proposed to solve analogical equations [y : x = t :
? ] for some pairs ?x, y? belonging to the neighbor-
hood3 of I(u), denotedN (t). Those solutions that
belong to the input space are the z-forms retained;
EI(u) = { ?x, y, z? : x ? N (t) , y ? N (x),
z ? [y : x = t : ? ] ? I }
This strategy (hereafter named LP) directly fol-
lows from a symmetrical property of an analogy
([x : y = z : t ] ? [y : x = t : z]), and reduces
the search procedure to the resolution of a number
of analogical equations which is quadratic with the
number of pairs ?x, y? sampled.
We found this strategy to be of little use for
input spaces larger than a few tens of thousands
forms. To solve this problem, we exploit a prop-
erty on symbol counts that an analogical relation
must fulfill (Lepage, 1998):
[x : y = z : t ]? |x|c + |t|c = |y|c + |z|c ?c ? A
3The authors proposed to sample x and y among the clos-
est forms in terms of edit-distance to I(u).
function solver(?x, y, z?, s)
Input: ?x, y, z?, a triplet, s the sampling size
Output: a set of solutions to [x : y = z : ? ]
sol? ?
for i? 1 to s do
?a, b? ? odd(rand(0, 1))? ?z, y? : ?y, z?
m ? shuffle(a,b )
c? complementary(m,x,,{})
sol? sol ? c
return sol
Algorithm 2: A Stroppa&Yvon flavored solver.
rand(a, b) returns a random integer between a
and b (included). The ternary operator ?: is to
be understood as in the C language.
where A is the alphabet on which the forms are
built, and |x|c stands for the number of occur-
rences of symbol c in x.
Our search strategy (named TC) begins by se-
lecting an x-form in the input space. This en-
forces a set of necessary constraints on the counts
of characters that any two forms y and z must sat-
isfy for [x : y = z : t ] to be true. By considering
all forms x in turn,4 we collect a set of candidate
triplets for t. A verification of those that define
with t an analogy must then be carried out. For-
mally, we built:
EI(u) = { ?x, y, z? : x ? I,
?y, z? ? C(?x, t?),
[x : y = z : t ] }
where C(?x, t?) denotes the set of pairs ?y, z?
which satisfy the count property.
This strategy will only work if (i) the number
of quadruplets to check is much smaller than the
number of triplets we can form in the input space
(which happens to be the case in practice), and
if (ii) we can efficiently identify the pairs ?y, z?
that satisfy a set of constraints on character counts.
To this end, we proposed in (Langlais and Yvon,
2008) to organize the input space into a data struc-
ture which supports efficient runtime retrieval.
3.3 The selector
Step 3 of analogical learning consists in selecting
one or several solutions from the set of candidate
forms produced by the generator. We trained in
a supervised manner a binary classifier to distin-
guish good translation candidates (as defined by
4Anagram forms do not have to be considered separately.
490
a reference) from spurious ones. We applied to
this end the voted-perceptron algorithm described
by Freund and Schapire (1999). Online voted-
perceptrons have been reported to work well in a
number of NLP tasks (Collins, 2002; Liang et al,
2006). Training such a classifier is mainly a matter
of feature engineering. An example e is a pair of
source-target analogical relations (r, r?) identified
by the generator, and which elects t? as a transla-
tion for the term t:
e ? (r, r?) ? ([x : y = z : t], [x? : y? = z? : t?])
where x?, y?, and z? are respectively the projections
of the source terms x, y and z. We investigated
many features including (i) the degree of r and r?,
(ii) the frequency with which a form is generated,5
(iii) length ratios between t and t?, (iv) likelihoods
scores (min, max, avg.) computed by a character-
based n-gram model trained on a large general cor-
pus (without overlap to DEV or TRAIN), etc.
4 Experiments
4.1 Calibrating the engine
We compared the two aforementioned searching
strategies on a task of identifying triplets in an
input space of French words for 1 000 randomly
selected test words. We considered input spaces
of various sizes. The results are reported in Ta-
ble 2. TC clearly outperforms LP by systemati-
cally identifying more triplets in much less time.
For the largest input space of 84 000 forms, TC
could identify an average of 746 triplets for 946
test words in 1.2 seconds, while the best compro-
mise we could settle with LP allows the identifi-
cation of 56 triplets on average for 889 words in
6.3 seconds on average. Note that in this exper-
iment, LP was calibrated for each input space so
that the best compromise between recall (%s) and
speed could be found. Reducing the size of the
neighborhood in LP improves computation time,
but significantly affects recall. In the following,
we only consider the TC search strategy.
4.2 Experimental Protocol
Datasets The data we used in this study comes
from the Medical Subject Headings (MeSH) the-
saurus. This thesaurus is used by the US National
Library of Medicine to index the biomedical sci-
5A form t? may be generated thanks to many examples.
s %s (s) s %s (s) s %s (s)
TC 34 83.1 0.2 261 94.1 0.5 746 96.4 1.2
LP 17 71.7 7.4 46 85.0 7.6 56 88.9 6.3
|I| 20 000 50 000 84 076
Table 2: Average number s of input analogies
found over 1 000 test words as a function of the
size of the input space. %s stands for the percent-
age of source forms for which (at least) one source
triplet is found; and (s) indicates the average time
(counted in seconds) to treat one form.
entific literature in the MEDLINE database.6 Its
preferred terms are called ?Main Headings?. We
collected pairs of source and target Main Head-
ings (TTY = ?MH?) with the same MeSH identi-
fiers (SDUI).
We considered five language pairs with three
relatively close European languages (English-
French, English-Spanish and English-Swedish), a
more distant one (English-Finnish) and one pair
involving different scripts (English-Russian).7
The material was split in three randomly se-
lected parts, so that the development and test ma-
terial contain exactly 1 000 terms each. The char-
acteristics of this material are reported in Table 3.
For the Finnish-English and Swedish-English lan-
guage pairs, the ratio of uni-terms in the Foreign
language (uf%) is twice the ratio of uni-terms in
the English counterpart. This is simply due to
the agglutinative nature of these two languages.
For instance, according to MeSH, the English
multi-term speech articulation tests corresponds
to the Finnish uni-term a?a?nta?miskokeet and to the
Swedish one artikulationstester. The ratio of out-
of-vocabulary forms (space-separated words un-
seen in TRAIN) in the TEST material is rather
high: between 36% and 68% for all Foreign-
to-English translation directions, but Finnish-to-
English, where surprisingly, only 6% of the word
forms are unknown.
Evaluation metrics For each experimental con-
dition, we compute the following measures:
Coverage the fraction of input words for which
the system can generate translations. If Nt words
receive translations among N , coverage is Nt/N .
6The MeSH thesaurus and its translations are included in
the UMLS Metathesaurus.
7Russian MeSH is normally written in Cyrillic, but some
terms are simply English terms written in uppercase Latin
script (e.g., ACHROMOBACTER for English Achromobac-
ter). We removed those terms.
491
TRAIN TEST DEV TEST
f nb uf% ue% nb uf% uf% oov%
FI 19 787 63.7 33.7 1 000 64.2 64.0 5.7
FR 17 230 29.8 29.3 1 000 30.8 28.3 36.3
RU 21 407 38.6 38.6 1 000 38.5 40.2 44.4
SP 19 021 31.1 31.1 1 000 31.7 33.3 36.6
SW 17 090 67.9 32.5 1 000 67.4 67.9 68.4
Table 3: Main characteristics of our datasets. nb
indicates the number of pairs of terms in a bi-
text, uf% (ue%) stands for the percentage of uni-
terms in the Foreign (English) part. oov% indi-
cates the percentage of out-of-vocabulary forms
(space-separated forms of TEST unseen in TRAIN).
Precision among the Nt words for which the
system proposes an answer, precision is the pro-
portion of those for which a correct translation is
output. Depending on the number of output trans-
lations k that one is willing to examine, a correct
translation will be output for Nk input words. Pre-
cision at rank k is thus defined as Pk = Nk/Nt.
Recall is the proportion of the N input words
for which a correct translation is output. Recall at
rank k is defined as Rk = Nk/N .
In all our experiments, candidate translations
are sorted in decreasing order of frequency with
which they were generated.
4.3 The generator
The performances of the generator on the 10
translation sessions are reported in Table 4.
The coverage of the generator varies between
38.5% (French-to-English) and 47.1% (English-
to-Finnish), which is rather low. In most cases, the
silence of the generator is due to a failure to iden-
tify analogies in the input space (step 1). The last
column of Table 4 reports the maximum recall we
can obtain if we consider all the candidates output
by the generator. The relative accuracy of the gen-
erator, expressed by the ratio ofR? to cov, ranges
from 64.3% (English-French) to 79.1% (Spanish-
to-English), for an average value of 73.8% over
all translation directions. This roughly means that
one fourth of the test terms with at least one solu-
tion do not contain the reference.
Overall, we conclude that analogical learning
offers comparable performances for all transla-
tion directions, although some fluctuations are ob-
served. We do not observe that the approach is
affected by language pairs which do not share the
Cov P1 R1 P100 R100 R?
? FI 47.1 31.6 14.9 57.7 27.2 31.9
FR 41.2 35.4 14.6 60.4 24.9 26.5
RU 46.2 40.5 18.7 69.9 32.3 34.8
SP 47.0 41.5 19.5 69.1 32.5 35.9
SW 42.8 36.0 15.4 66.8 28.6 31.9
? FI 44.8 36.6 16.4 66.7 29.9 33.2
FR 38.5 47.0 18.1 69.9 26.9 29.4
RU 42.1 49.4 20.8 70.3 29.6 32.3
SP 42.6 47.7 20.3 75.1 32.0 33.7
SW 44.6 40.8 18.2 69.5 31.0 32.9
Table 4: Main characteristics of the generator, as a
function of the translation directions (TEST).
same script (Russian/English). The best (worse)
case (as far as R? is concerned) corresponds to
translating into Spanish (French).
Admittedly, the largest recall andR? values re-
ported in Table 4 are disappointing. Clearly, for
analogical learning to work efficiently, enough lin-
guistic phenomena must be attested in the TRAIN
material. To illustrate this, we collected for the
Spanish-English language pair a set of medical
terms from the Medical Drug Regulatory Activi-
ties thesaurus (MedDRA) which contains roughly
three times more terms than the Spanish-English
material used in this study. This extra material al-
lows to raise the coverage to 73.4% (Spanish to
English) and 79.7% (English to Spanish), an abso-
lute improvement of more than 30%.
4.4 The selector
We trained our classifiers on the several millions
of examples generated while translating the devel-
opment material. Since we considered numerous
feature representations in this study, this implies
saving many huge datafiles on disk. In order to
save some space, we decided to remove forms that
were generated less than 3 times.8 Each classifier
was trained using 20 epochs.
It is important to note that we face a very unbal-
anced task. For instance, for the English to Finnish
task, the generator produces no less than 2.7 mil-
lions of examples, among which only 4 150 are
positive ones. Clearly, classifying all the examples
as negative will achieve a very high classification
accuracy, but will be of no practical use. There-
fore, we measure the ability of a classifier to iden-
8Averaged over all translation directions, this incurs an
absolute reduction of the coverage of 3.4%.
492
FI?EN FR?EN RU?EN SP?EN SW?EN
p r p r p r p r p r
argmax-f1 41.3 56.7 46.7 63.9 48.1 65.6 49.2 63.4 43.2 61.0
s-best 53.6 61.3 57.5 68.4 61.9 66.7 64.3 70.0 53.1 64.4
Table 5: Precision (p) and recall (r) of some classifiers on the TEST material.
tify the few positive forms among the set of candi-
dates. We measure precision as the percentage of
forms selected by the classifier that are sanctioned
by the reference lexicon, and recall as the percent-
age of forms selected by the classifier over the to-
tal number of sanctioned forms that the classifier
could possibly select. (Recall that the generator
often fails to produce oracle forms.)
The performance measured on the TEST mate-
rial of the best classifier we monitored on DEV
are reported in Table 5 for the Foreign-to-English
translation directions (we made consistent obser-
vations on the reverse directions). For compari-
son purposes, we implemented a baseline classi-
fier (lines argmax-f1) which selects the most-
frequent candidate form. This is the selector
used as a default in several studies on analogi-
cal learning (Lepage and Denoual, 2005; Stroppa
and Yvon, 2005). The baseline identifies between
56.7% to 65.6% of the sanctioned forms, at pre-
cision rates ranging from 41.3% to 49.2%. We
observe for all translation directions that the best
classifier we trained systematically outperforms
this baseline, both in terms of precision and recall.
4.4.1 The overall system
Table 6 shows the overall performance of the ana-
logical translation device in terms of precision, re-
call and coverage rates as defined in Section 4.2.
Overall, our best configuration (the one embed-
ding the s-best classifier) translates between
19.3% and 22.5% of the test material, with a preci-
sion ranging from 50.4% to 63.2%. This is better
than the variant which always proposes the most
frequent generated form (argmax-f1). Allowing
more answers increases both precision and recall.
If we allow up to 10 candidates per source term,
the analogical translator translates one fourth of
the terms (26.1%) with a precision of 70.9%, aver-
aged over all translation directions. The oracle
variant, which looks at the reference for select-
ing the good candidates produced by the genera-
tor, gives an upper bound of the performance that
could be obtained with our approach: less than
a third of the source terms can be translated cor-
rectly. Recall however that increasing the TRAIN
material leads to drastic improvements in cover-
age.
4.5 Comparison with a PB-SMT engine
To put these figures in perspective, we mea-
sured the performance of a phrase-based statisti-
cal MT (PB-SMT) engine trained to handle the
same translation task. We trained a phrase table
on TRAIN, using the standard approach.9 How-
ever, because of the small training size, and the
rather huge OOV rate of the translation tasks we
address, we did not train translation models on
word-tokens, but at the character level. There-
fore a phrase is indeed a sequence of charac-
ters. This idea has been successively investigated
in a Catalan-to-Spanish translation task by Vi-
lar et al (2007). We tuned the 8 coefficients of
the so-called log-linear combination maximized
at decoding time on the first 200 pairs of terms
of the DEV corpora. On the DEV set, BLEU
scores10 range from 67.2 (English-to-Finnish) to
77.0 (Russian-to-English).
Table 7 reports the precision and recall of both
translation engines. Note that because the SMT
engine always propose a translation, its precision
equals its recall. First, we observe that the preci-
sion of the SMT engine is not high (between 17%
and 31%), which demonstrates the difficulty of
the task. The analogical device does better for all
translation directions (see Table 6), but at a much
lower recall, remaining silent more than half of
the time. This suggests that combining both sys-
tems could be advantageous. To verify this, we
ran a straightforward combination: whenever the
analogical device produces a translation, we pick
it; otherwise, the statistical output is considered.
The gains of the resulting system over the SMT
alone are reported in column ?B. Averaged over
9We used the scripts distributed by Philipp Koehn to train
the phrase-table, and Pharaoh (Koehn, 2004) for producing
the translations.
10We computed BLEU scores at the character level.
493
FI?EN FR?EN RU?EN SP?EN SW?EN
k Pk Rk Pk Rk Pk Rk Pk Rk Pk Rk
argmax-f 1 41.3 17.3 46.7 16.8 47.8 18.6 48.7 19.2 43.4 18.1
10 61.6 25.8 62.8 22.6 61.7 24.0 69.3 27.3 62.1 25.9
s-best 1 53.5 20.8 56.9 19.3 58.5 20.3 63.2 22.5 50.4 21
10 69.4 27.0 69.0 23.4 71.8 24.9 78.4 27.9 65.7 27.4
oracle 1 100 30.5 100 26.3 100 28.5 100 30.6 100 29.5
Table 6: Precision and recall at rank 1 and 10 for the Foreign-to-English translation tasks (TEST).
all translation directions, BLEU scores increase on
TEST from 66.2 to 71.5, that is, an absolute im-
provement of 5.3 points.
? EN ? EN
Psmt ?B Psmt ?B
FI 20.2 +7.4 21.6 +6.4
FR 19.9 +5.3 17.0 +6.0
RU 24.1 +3.1 28.0 +6.4
SP 22.1 +4.9 26.4 +5.5
SW 25.9 +4.2 31.6 +3.2
Table 7: Translation performances on TEST. Psmt
stands for the precision and recall of the SMT en-
gine. ?B indicates the absolute gain in BLEU
score of the combined system.
We noticed a tendency of the statistical engine
to produce literal translations; a default the ana-
logical device does not show. For instance, the
Spanish term instituciones de atencio?n ambulato-
ria is translated word for word by Pharaoh into
institutions, atention ambulatory while analogical
learning produces ambulatory care facilities. We
also noticed that analogical learning sometimes
produces wrong translations based on morpholog-
ical regularities that are applied blindly. This is,
for instance, the case in a Russian/English exam-
ple where mouthal manifestations is produced, in-
stead of oral manifestations.
5 Discussion and future work
In this study, we proposed solutions to practical is-
sues involved in analogical learning. A simple yet
effective implementation of a solver is described.
A search strategy is proposed which outperforms
the one described in (Langlais and Patry, 2007).
Also, we showed that a classifier trained to se-
lect good candidate translations outperforms the
most-frequently-generated heuristic used in sev-
eral works on analogical learning.
Our analogical device was used to translate
medical terms in different language pairs. The
approach rates comparably across the 10 transla-
tion directions we considered. In particular, we
do not see a drop in performance when trans-
lating into a morphology rich language (such as
Finnish), or when translating into languages with
different scripts. Averaged over all translation di-
rections, the best variant could translate in first po-
sition 21% of the terms with a precision of 57%,
while at best, one could translate 30% of the terms
with a perfect precision. We show that the ana-
logical translations are of better quality than those
produced by a phrase-based engine trained at the
character level, albeit with much lower recall. A
straightforward combination of both approaches
led an improvement of 5.3 BLEU points over the
SMT alone. Better SMT performance could be
obtained with a system based on morphemes, see
for instance (Toutanova et al, 2008). However,
since lists of morphemes specific to the medical
domain do not exist for all the languages pairs we
considered here, unsupervised methods for acquir-
ing morphemes would be necessary, which is left
as a future work. In any case, this comparison is
meaningful, since both the SMT and the analogi-
cal device work at the character level.
This work opens up several avenues. First, we
will test our approach on terminologies from dif-
ferent domains, varying the size of the training
material. Second, analyzing the segmentation in-
duced by analogical learning would be interesting.
Third, we need to address the problem of com-
bining the translations produced by analogy into a
front-end statistical translation engine. Last, there
is no reason to constrain ourselves to translating
terminology only. We targeted this task in the first
place, because terminology typically plugs trans-
lation systems, but we think that analogical learn-
ing could be useful for translating infrequent enti-
ties.
494
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
M. Collins. 2002. Discriminative training methods for
hidden markov models: theory and experiments with
perceptron algorithms. In EMNLP, pages 1?8, Mor-
ristown, NJ, USA.
E. Denoual. 2007. Analogical translation of unknown
words in a statistical machine translation framework.
In MT Summit, XI, pages 10?14, Copenhagen.
Y. Freund and R. E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Mach.
Learn., 37(3):277?296.
P. Fung and K. McKeown. 1997. Finding terminology
translations from non-parallel corpora. In 5th An-
nual Workshop on Very Large Corpora, pages 192?
202, Hong Kong.
M. Itagaki, T. Aikawa, and X. He. 2007. Auto-
matic validation of terminology translation consis-
tency with statistical method. In MT Summit XI,
pages 269?274, Copenhagen, Denmark.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models.
In AMTA, pages 115?124, Washington, DC, USA.
P. Langlais and A. Patry. 2007. Translating unknown
words by analogical learning. In EMNLP-CoNLL,
pages 877?886, Prague, Czech Republic.
P. Langlais and F. Yvon. 2008. Scaling up analogi-
cal learning. In 22nd International Conference on
Computational Linguistics (COLING 2008), pages
51?54, Manchester, United Kingdom.
Y. Lepage and E. Denoual. 2005. ALEPH: an EBMT
system based on the preservation of proportion-
nal analogies between sentences across languages.
In International Workshop on Statistical Language
Translation (IWSLT), Pittsburgh, PA, October.
Y. Lepage and A. Lardilleux. 2007. The GREYC Ma-
chine Translation System for the IWSLT 2007 Eval-
uation Campaign. In IWLST, pages 49?53, Trento,
Italy.
Y. Lepage. 1998. Solving analogies on words: an algo-
rithm. In COLING-ACL, pages 728?734, Montreal,
Canada.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In 21st COLING and 44th ACL,
pages 761?768, Sydney, Australia.
E. Morin, B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining - using brain,
not brawn comparable corpora. In 45th ACL, pages
664?671, Prague, Czech Republic.
R. Rapp. 1995. Identifying word translation in non-
parallel texts. In 33rd ACL, pages 320?322, Cam-
bridge,Massachusetts, USA.
N. Stroppa and F. Yvon. 2005. An analogical learner
for morphological analysis. In 9th CoNLL, pages
120?127, Ann Arbor, MI.
K Toutanova, H. Suzuki, and A. Ruopp. 2008. Ap-
plying morphology generation models to machine
translation. In ACL-8 HLT, pages 514?522, Colom-
bus, Ohio, USA.
D. Vilar, J. Peter, and H. Ney. 2007. Can we trans-
late letters? In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 33?
39, Prague, Czech Republic, June.
F. Yvon, N. Stroppa, A. Delhay, and L. Miclet. 2004.
Solving analogical equations on words. Techni-
cal Report D005, E?cole Nationale Supe?rieure des
Te?le?communications, Paris, France, July.
495
Proceedings of the Third Workshop on Statistical Machine Translation, pages 107?110,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?08
Daniel D?chelotte, Gilles Adda, Alexandre Allauzen, H?l?ne Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais? and Fran?ois Yvon
LIMSI/CNRS
firstname.lastname@limsi.fr
Abstract
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
1 Introduction
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
2 Base System architecture
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI?s transla-
tion systems. They are fairly standard phrase-based
?Univ. Montr?al, felipe@iro.umontreal.ca
Other
Targettext
Targettext
MosestextSource or
Translation model 4g language model 4g language model
and extractionRescoring
$n$?besttranslations
LM InterpolationPhrase pairextraction Neural network
or
or
+ News Co.EuroparlEuroparl EuroparlNews Co.sources
Figure 1: Generic architecture of LIMSI?s SMT systems.
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al, 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
2.1 Translation models
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
107
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to ?adapt? a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (D?chelotte, 2007)1 , which
adapts a ?European Parliament? system using a ?Eu-
ropean and Spanish Parliaments? development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
2.2 Language Models
2.2.1 Europarl language models
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
2.2.2 News language models
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(D?chelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven?t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
2.2.3 Neural network language models
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [?1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
2.3 Tuning procedure
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system?s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
2.4 Rescoring and post processing
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
108
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
Table 1: Comparison of two tokenization policies
All results on Europarl test2007
CI system CS system
En?Fr 27.23 27.55
Fr?En 30.96 30.98
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the ?lowcase? systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
3 Experiments with the base system
3.1 Word tokenization and case
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
3.2 Language Models
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
4 Experiments with factored models
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
4.1 Overview
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
4.2 Training
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill?s tagger trained
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
109
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
4.3 Results and lessons learned
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our ?stan-
dard? system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
5 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems? performance.
References
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC?08, To appear.
D. D?chelotte. 2007. Traduction automatique de la pa-
role par m?thodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868?876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224?227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
110
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 100?104,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?09
Alexandre Allauzen, Josep Crego, Aur?lien Max and Fran?ois Yvon
LIMSI/CNRS and Universit? Paris-Sud 11, France
BP 133, 91403 Orsay C?dex
firstname.lastname@limsi.fr
Abstract
This paper describes our Statistical Ma-
chine Translation systems for the WMT09
(en:fr) shared task. For this evaluation, we
have developed four systems, using two
different MT Toolkits: our primary sub-
mission, in both directions, is based on
Moses, boosted with contextual informa-
tion on phrases, and is contrasted with a
conventional Moses-based system. Addi-
tional contrasts are based on the Ncode
toolkit, one of which uses (part of) the En-
glish/French GigaWord parallel corpus.
1 Introduction
This paper describes our Statistical Machine
Translation systems for the WMT09 (en:fr) shared
task. For this evaluation, we have developed four
systems, using two different MT toolkits: our
primary submission, in both direction, is based
on Moses, boosted with contextual information
on phrases; we also provided a contrast with a
vanilla Moses-based system. Additional contrasts
are based on the N-code decoder, one of which
takes advantage of (part of) the English/French Gi-
gaWord parallel corpus.
2 System architecture and resources
In this section, we describe the main characteris-
tics of the baseline phrase-based systems used in
this evaluation and the resources that were used to
train our models.
2.1 Pre- and post-processing tools
All the available textual corpora were processed
and normalized using in-house text processing
tools. Our last year experiments (D?chelotte et
al., 2008) revealed that using better normalization
tools provides a significant reward in BLEU, a fact
that we could observe again this year. The down-
side is the need to post-process our outputs so as
to ?detokenize? them for scoring purposes, which
is unfortunately an error-prone process.
Based again on last year?s experiments, our sys-
tems are built in ?true case?: the first letter of each
sentence is lowercased when it should be, and the
remaining tokens are left as is.
Finally, the N-code (see 2.5) and the context-
aware (see 3) systems require the source to be
morpho-syntactically analysed. This was per-
formed using the TreeTagger1 for both languages.
2.2 Alignment and translation models
Our baseline translation models (see 2.4 and 2.5)
use all the parallel corpora distributed for this eval-
uation: Europarl V4, news commentary (2006-
2009) and the additional news data, totalling 1.5M
sentences. Our preliminary attempts with larger
translation models using the GigaWord corpus are
reported in section 3.2. All these corpora were
aligned with GIZA++2 using default settings.
2.3 Language Models
To train our language models (LMs), we took ad-
vantage of the a priori information that the test
set would be of newspaper/newswire genre. We
1http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger.
2http://www.fjoch.com/GIZA++.html.
100
Source Period M. words
News texts 1994-06 3 317
En BN transcripts 2000-07 341
WMT 86
Newswires 1994-07 723
Newspapers 1987-06 486
Fr WEB 2008 23
WMT 46
News-train08 167
Table 1: Corpora used to train the target language
models in English and French.
thus built much larger LMs for translating both to
French and to English, and optimized their combi-
nation on the first part of the official development
data (dev2009a).
Corpora and vocabulary Statistics regarding
the training material are summarized in table 1 in
terms of source, time period, and millions of oc-
currences. ?WMT? stands for all text provided
for the evaluation. Development sets and the large
training corpora (news-train08 and the GigaWord
corpus) were not included. Altogether, these data
contain a total number of 3.7 billion tokens for En-
glish and 1.4 billion tokens for French.
To estimate such large LMs, a vocabulary was
first defined for both languages by including all to-
kens in the WMT parallel data. This initial vocab-
ulary of 130K words was then extended by adding
the most frequent words observed in the additional
training data. This procedure yielded a vocabulary
of one million words in both languages.
Language model training The training data
were divided into several sets based on dates on
genres (resp. 7 and 9 sets for English and French).
On each set, a standard 4-gram LM was estimated
from the 1M word vocabulary with in-house tools
using absolute discounting interpolated with lower
order models. The resulting LMs were then lin-
early interpolated using interpolation coefficients
chosen so as to minimise perplexity of the devel-
opment set (dev2009a). Due to memory limita-
tions, the final LMs were pruned using perplexity
as pruning criterion.
Out of vocabulary word and perplexity To
evaluate our vocabulary and LMs, we used the of-
ficial devtest and test sets. The out-of-vocabulary
(OOV) rate was drastically reduced by increasing
the vocabulary size, the mean OOV rate decreas-
ing from 2.5% to 0.7%, a trend observed in both
languages.
For French, using a small LM trained on the
"WMT" data only resulted in a perplexity of 301
on the devtest corpus and 299 on the test set. Us-
ing all additional data yielded a large decrease in
perplexity (106 on the devtest and 108 on the test);
again the same trend was observed for English.
2.4 A Moses baseline
Our baseline system was a vanilla phrase-based
system built with Moses (Koehn et al, 2007) us-
ing default settings. Phrases were extracted using
the ?grow-diag-final-and? heuristics, using a max-
imum phrase length of 7; non-contextual phrase
scores contain the 4 translation model scores, plus
a fixed phrase penalty; 6 additional scores param-
eterize the lexicalized reordering model. Default
decoding options were used (20 alternatives per
phrase, maximum distortion distance of 7, etc.)
2.5 A N-code baseline
N-code implements the n-gram-based approach
to Statistical Machine Translation (Mari?o et al,
2006). In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training
such a model requires to reorder source sentences
so as to match the target word order. This is also
performed via a stochastic finite-state reordering
model, which uses part-of-speech information to
generalise reordering patterns beyond lexical reg-
ularities. The reordering model is trained on a ver-
sion of the parallel corpora where the source sen-
tences have been reordered via the unfold heuris-
tics (Crego and Mari?o, 2007). A conventional n-
gram language model of the target language pro-
vides the third component of the system.
In all our experiments, we used 4-gram reorder-
ing models and bilingual tuple models built using
Kneser-Ney backoff (Chen and Goodman, 1996).
The maximum tuple size was also set to 7.
2.6 Tuning procedure
The Moses-based systems were tuned using the
implementation of minimum error rate train-
ing (MERT) (Och, 2003) distributed with the
Moses decoder, using the development corpus
(dev2009a). For the context-less systems, tun-
ing concerned the 14 usual weights; tuning the
101
22 weights of the context-aware systems (see 3.1)
proved to be much more challenging, and the
weights used in our submissions are probably far
from optimal. The N-code systems only rely on
9 weights, since they dispense with the lexical re-
ordering model; these weights were tuned on the
same dataset, using an in-house implementation of
the simplex algorithm.
3 Extensions
3.1 A context-aware system
In phrase-based translation, source phrases are
translated irrespective of their (source) context.
This is often not perceived as a limitation as
(i) typical text domains usually contain only few
senses for polysemous words, thus limiting the
use of word sense disambiguation (WSD); and (ii)
using long-span target language models (4-grams
and more) often capture sufficient context to se-
lect the more appropriate translation for a source
phrase based on the target context. In fact, at-
tempts at using source contexts in phrase-based
SMT have to date failed to show important gains
on standard evaluation test sets (Carpuat and Wu,
2007; Stroppa et al, 2007; Gimpel and Smith,
2008; Max et al, 2008). Importantly, in all con-
ditions where gains have been obtained, the tar-
get language was the ?morphologically-poor? En-
glish.
Nonetheless, there seems to be a clear consen-
sus on the importance of better exploiting source
contexts in SMT, so as to improve phrase disam-
biguation. The following sentence extract from
the devtest corpus is a typical example where the
lack of context in our phrase-based system yields
an incorrect translation:
Source: the long weekend comes with a price . . .
Target: Le long week-end vient avec un prix . . .
(the long weekend comes accompanied by a price)
While grammatically correct, the French trans-
lation sounds unnatural, and getting the correct
meaning requires knowledge of the idiom in the
source language. In such a situation, the right con-
text of the phrase comes with can be successfully
used to propose a better translation.3
From an engineering perspective, integrating
context into phrase-based SMT systems can be
performed by (i) transforming source words into
unique tokens, so as to record the original context
3Our context-aware phrase-based system indeed proposes
the appropriate translation: Le long week-end a un prix.
of each entry of the phrase table; and by (ii) adding
one or several contextual scores to the phrase ta-
ble. Using standard MERT, the corresponding
weights can be optimized on development data.
A typical contextual score corresponds to
p(e|f , C(f)), where C(f) is some contextual in-
formation about the source phrase f . An exter-
nal disambiguation system can be used to pro-
vide one global context score (Stroppa et al, 2007;
Carpuat and Wu, 2007; Max et al, 2008)); alter-
natively, several scores based on single features
can be estimated using relative frequencies (Gim-
pel and Smith, 2008):
p(e|f , C(f)) =
count(e, f , C(f))
?
e? count(e?, f , C(f))
For these experiments, we followed the latter ap-
proach, restricting ourselves to features represent-
ing the local context up to a fixed distance d (using
the values 1 and 2 in our experiments) from the
source phrase f endstart:
? lexical context features:
? left context: p(e|f , f start?1start?d )
? right context: p(e|f , f end+dend+1 )
? shallow syntactic features (denoting tF1 the
sequence of POS tags for the source sen-
tence):
? left context: p(e|f , tstart?1start?d)
? right context: p(e|f , tend+dend+1)
As in (Gimpel and Smith, 2008), we filtered out
all translations for which p(e|f) < 0.0002. This
was necessary to make score computation practi-
cal given our available hardware resources.
Results on the devtest corpus for
English?French were similar for the context-
aware phrase-based and the baseline phrase-based
system; small gains were achieved in the reverse
direction (see Table 2). The same trend was
observed on the test data.
Manual inspection of the output of the base-
line and context-aware systems on the devtest
corpus for English?French translation confirmed
two facts: (1) performing phrase translation dis-
ambiguation is only useful if a more appropriate
translation has been seen during training ; and (2)
phrase translation disambiguation can capture im-
portant source dependencies that the target lan-
guage model can not recover. The following ex-
102
ample, involving an unseen sense4 (ball in the se-
mantic field of dance rather than sports), illus-
trates our first remark:
Source: about 500 people attended the ball .
Baseline : Environ 500 personnes ont assist? ? la
balle.
+Context: Environ 500 personnes ont particip? ?
la balle.
The next example is a case where contextual in-
formation helped selecting an appropriate transla-
tion, in constrast to the baseline system.
Source: . . . the new method for calculating pen-
sions due to begin next year . . .
Baseline : . . . le nouveau mode de calcul des pen-
sions due ? commencer l?ann?e prochaine . . .
+Context: . . . la nouvelle m?thode de calcul des
pensions qui va d?buter l?ann?e prochaine . . .
3.2 Preliminary experiments with the
GigaWord parallel corpus
One exciting novelty of this year?s campaign was
the availability of a very large parallel corpus for
the en:fr pair, containing about 20M aligned sen-
tences.
Our preliminary work consisted in selecting the
most useful pairs of sentences, based on their av-
erage perplexity, as computed on our develop-
ment language models. The top ranking sen-
tences (about 8M sentences) were then fed into the
usual system development procedure: alignment,
reordering (for the N-code system), phrase pair
extraction, model estimation. Given the unusual
size of this corpus, each of these steps proved
extremely resource intensive, and, for some sys-
tems, actually failed to complete. Contrarily, the
N-code systems, conceptually simpler, proved to
scale nicely.
Given the very late availability of this cor-
pus, our experiments were very limited and we
eventually failed to deliver the test submissions
of our ?GigaWord? system. Preliminary exper-
iments using the N-code systems (see Table 2),
however, showed a clear improvement of perfor-
mance. There is no reason to doubt that similar
gains would be observed with the Moses systems.
3.3 Experiments
The various systems presented above were all de-
veloped according to the same procedure: train-
ing used all the available parallel text; tuning was
4This was confirmed after careful inspection of the phrase
tables of the baseline system.
en ? fr fr ? en
Moses Ncode Moses Ncode
small LM 20.06 18.98 21.14 20.41
Large LM 22.93 21.95 22.20 22.28
+context 23.06 22.69
+giga 23.21 23.14
Table 2: Results on the devtest set
performed on dev2009a (1000 sentences), and our
internal tests were performed on dev2009b (1000
sentences). Results are reported in table 2.
Our primary submission corresponds to
the +context entry, our first contrast to
Moses+LargeLM, and our second contrast to
Ncode+largeLM. Due to lack of time, no official
submission was submitted for the +giga variant.
For the record, the score we eventually obtained
on the test corpus was 26.81, slightly better than
our primary submission which obtained a score of
25.74 (all these numbers were computed on the
complete test set).
4 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT?09 shared task. We
used last year experiments to build competitive
systems, which greatly benefited from in-house
normalisation and language modeling tools.
One motivation for taking part in this campaign
was to use the GigaWord corpus. Even if time did
not allow us to submit a system based on this data,
it was a interesting opportunity to confront our-
selves with the technical challenge of scaling up
our system development tools to very large paral-
lel corpora. Our preliminary results indicate that
this new resource can actually help improve our
systems.
Naturally, future work includes adapting our
systems so that they can use models learnt from
corpora of the size of the GigaWord corpus. In
parallel, we intend to keep on working on context-
aware systems to study the impact of more types
of scores, e.g. based on grammatical dependencies
as in (Max et al, 2008). Given the difficulties we
had tuning our systems, we feel that a preliminary
task should be improving our tuning tools before
addressing these developments.
103
Acknowledgments
This work was partly realised as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
References
M. Carpuat and D. Wu. 2007. Context-Dependent
Phrasal Translation Lexicons for Statistical Machine
Translation. In Proceedings of Machine Translation
Summit XI, pages 73?80, Copenhagen, Denmark.
F. Casacuberta and E. Vidal. 2004. Machine transla-
tion with inferred stochastic finite-state transducers.
Computational Linguistics, 30(3):205?225.
S. F. Chen and J. T. Goodman. 1996. An empirical
study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 310?318, Santa Cruz, NM.
J. M. Crego and J. B. Mari?o. 2007. Improving
SMT by coupling reordering and decoding. Ma-
chine Translation, 20(3):199?215.
D. D?chelotte, G. Adda, A. Allauzen, O. Galibert, J.-L.
Gauvain, H. Meynard, and F. Yvon. 2008. Limsi?s
statistical translation systems for WMT?08. In Pro-
ceedings of the NAACL-HTL Statistical Machine
Translation Workshop, pages 107-100, Columbus,
Ohio.
K. Gimpel and N. A. Smith. 2008. Rich Source-Side
Context for Statistical Machine Translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 9?17, Columbus, Ohio.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session, Prague, Czech Republic.
A. Max, R. Makhloufi, and P. Langlais. 2008. Explo-
rations in using grammatical dependencies for con-
textual phrase translation disambiguation. In Pro-
ceedings of EAMT, poster session, Hamburg, Ger-
many.
J. B. Mari?o, R. E. Banchs R, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M. R. Costa-
Juss?. 2006. N-gram-based machine translation.
Computational Linguistics, 32(4):527?549.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In Proceedings of the 11th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI?07),
pages 231?240, Sk?vde, Sweden.
104
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 120?127, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
An Analogical Learner for Morphological Analysis
Nicolas Stroppa & Franc?ois Yvon
GET/ENST & LTCI, UMR 5141
46 rue Barrault, 75013 Paris, France
{stroppa,yvon}@enst.fr
Abstract
Analogical learning is based on a two-
step inference process: (i) computation
of a structural mapping between a new
and a memorized situation; (ii) transfer
of knowledge from the known to the un-
known situation. This approach requires
the ability to search for and exploit such
mappings, hence the need to properly de-
fine analogical relationships, and to effi-
ciently implement their computation.
In this paper, we propose a unified defini-
tion for the notion of (formal) analogical
proportion, which applies to a wide range
of algebraic structures. We show that this
definition is suitable for learning in do-
mains involving large databases of struc-
tured data, as is especially the case in Nat-
ural Language Processing (NLP). We then
present experimental results obtained on
two morphological analysis tasks which
demonstrate the flexibility and accuracy of
this approach.
1 Introduction
Analogical learning (Gentner et al, 2001) is based
on a two-step inductive process. The first step con-
sists in the construction of a structural mapping be-
tween a new instance of a problem and solved in-
stances of the same problem. Once this mapping
is established, solutions for the new instance can be
induced, based on one or several analogs. The im-
plementation of this kind of inference process re-
quires techniques for searching for, and reasoning
with, structural mappings, hence the need to prop-
erly define the notion of analogical relationships and
to efficiently implement their computation.
In Natural Language Processing (NLP), the typ-
ical dimensionality of databases, which are made
up of hundreds of thousands of instances, makes
the search for complex structural mappings a very
challenging task. It is however possible to take ad-
vantage of the specific nature of linguistic data to
work around this problem. Formal (surface) analog-
ical relationships between linguistic representations
are often a good sign of deeper analogies: a surface
similarity between the word strings write and writer
denotes a deeper (semantic) similarity between the
related concepts. Surface similarities can of course
be misleading. In order to minimize such confu-
sions, one can take advantage of other specificities
of linguistic data: (i) their systemic organization in
(pseudo)-paradigms, and (ii) their high level of re-
dundancy. In a large lexicon, we can indeed expect
to find many instances of pairs like write-writer: for
instance read -reader, review-reviewer...
Complementing surface analogies with statistical
information thus has the potential to make the search
problem tractable, while still providing with many
good analogs. Various attempts have been made to
use surface analogies in various contexts: automatic
word pronunciation (Yvon, 1999), morphological
analysis (Lepage, 1999a; Pirrelli and Yvon, 1999)
and syntactical analysis (Lepage, 1999b). These ex-
periments have mainly focused on linear represen-
120
tations of linguistic data, taking the form of finite
sequences of symbols, using a restrictive and some-
times ad-hoc definition of the notion of an analogy.
The first contribution of this paper is to propose a
general definition of formal analogical proportions
for algebraic structures commonly used in NLP:
attribute-value vectors, words on finite alphabets and
labeled trees. The second contribution is to show
how these formal definitions can be used within an
instance-based learning framework to learn morpho-
logical regularities.
This paper is organized as follows. In Section 2,
our interpretation of analogical learning is intro-
duced and related to other models of analogical
learning and reasoning. Section 3 presents a general
algebraic framework for defining analogical propor-
tions as well as its instantiation to the case of words
and labeled trees. This section also discusses the
algorithmic complexity of the inference procedure.
Section 4 reports the results of experiments aimed
at demonstrating the flexibility of this model and at
assessing its generalization performance. We con-
clude by discussing current limitations of this model
and by suggesting possible extensions.
2 Principles of analogical learning
2.1 Analogical reasoning
The ability to identify analogical relationships be-
tween what looks like unrelated situations, and to
use these relationships to solve complex problems,
lies at the core of human cognition (Gentner et al,
2001). A number of models of this ability have
been proposed, based on symbolic (e.g. (Falken-
heimer and Gentner, 1986; Thagard et al, 1990;
Hofstadter and the Fluid Analogies Research group,
1995)) or subsymbolic (e.g. (Plate, 2000; Holyoak
and Hummel, 2001)) approaches. The main focus
of these models is the dynamic process of analogy
making, which involves the identification of a struc-
tural mappings between a memorized and a new sit-
uation. Structural mapping relates situations which,
while being apparently very different, share a set of
common high-level relationships. The building of
a structural mapping between two situations utilizes
several subparts of their descriptions and the rela-
tionships between them.
Analogy-making seems to play a central role in
our reasoning ability; it is also invoked to explain
some human skills which do not involve any sort of
conscious reasoning. This is the case for many tasks
related to the perception and production of language:
lexical access, morphological parsing, word pronun-
ciation, etc. In this context, analogical models have
been proposed as a viable alternative to rule-based
models, and many implementation of these low-
level analogical processes have been proposed such
as decision trees, neural networks or instance-based
learning methods (see e.g. (Skousen, 1989; Daele-
mans et al, 1999)). These models share an accepta-
tion of analogy which mainly relies on surface simi-
larities between instances.
Our learner tries to bridge the gap between these
approaches and attempts to remain faithful to the
idea of structural analogies, which prevails in the
AI literature, while also exploiting the intuitions of
large-scale, instance-based learning models.
2.2 Analogical learning
We consider the following supervised learning task:
a learner is given a set S of training instances
{X1, . . . , Xn} independently drawn from some un-
known distribution. Each instance Xi is a vector
containing m features: ?Xi1, . . . , Xim?. Given S,
the task is to predict the missing features of partially
informed new instances. Put in more standard terms,
the set of known (resp. unknown) features for a new
value X forms the input space (resp. output space):
the projections of X onto the input (resp. output)
space will be denoted I(X) (resp. O(X)). This set-
ting is more general than the simpler classification
task, in which only one feature (the class label) is
unknown, and covers many other interesting tasks.
The inference procedure can be sketched as fol-
lows: training examples are simply stored for fu-
ture use; no generalization (abstraction) of the data
is performed, which is characteristic of lazy learning
(Aha, 1997). Given a new instance X , we identify
formal analogical proportions involving X in the in-
put space; known objects involved in these propor-
tions are then used to infer the missing features.
An analogical proportion is a relation involv-
ing four objects A, B, C and D, denoted by
A : B :: C : D and which reads A is to B as C is
to D. The definition and computation of these pro-
portions are studied in Section 3. For the moment,
121
we contend that it is possible to construct analogical
proportions between (possibly partially informed)
objects in S. Let I(X) be a partially described ob-
ject not seen during training. The analogical infer-
ence process is formalized as:
1. Construct the set T (X) ? S3 defined as:
T (X) = {(A,B,C) ? S3 |
I(A) : I(B) :: I(C) : I(X)}
2. For each (A,B,C) ? T (X), compute hy-
potheses O?(X) by solving the equation:
O?(X) = O(A) : O(B) :: O(C) :?
This inference procedure shows lots of similari-
ties with the k-nearest neighbors classifier (k-NN)
which, given a new instance, (i) searches the training
set for close neighbors, (ii) compute the unknown
class label according to the neighbors? labels. Our
model, however, does not use any metric between
objects: we only rely on the definition of analogical
proportions, which reveal systemic, rather than su-
perficial, similarities. Moreover, inputs and outputs
are regarded in a symmetrical way: outputs are not
restricted to a set of labels, and can also be structured
objects such as sequences. The implementation of
the model still has to address two specific issues.
? When exploring S3, an exhaustive search eval-
uates |S|3 triples, which can prove to be in-
tractable. Moreover, objects in S may be
unequally relevant, and we might expect the
search procedure to treat them accordingly.
? Whenever several competing hypotheses are
proposed for O?(X), a ranking must be per-
formed. In our current implementation, hy-
potheses are ranked based on frequency counts.
These issues are well-known problems for k-NN
classifiers. The second one does not appear to be
critical and is usually solved based on a majority
rule. In contrast, a considerable amount of effort has
been devoted to reduce and optimize the search pro-
cess, via editing and condensing methods, as stud-
ied e.g. in (Dasarathy, 1990; Wilson and Martinez,
2000). Proposals for solving this problem are dis-
cussed in Section 3.4.
3 An algebraic framework for analogical
proportions
Our inductive model requires the availability of a de-
vice for computing analogical proportions on feature
vectors. We consider that an analogical proportion
holds between four feature vectors when the propor-
tion holds for all components. In this section, we
propose a unified algebraic framework for defining
analogical proportions between individual features.
After giving the general definition, we present its in-
stantiation for two types of features: words over a
finite alphabet and sets of labelled trees.
3.1 Analogical proportions
Our starting point will be analogical proportions in
a set U , which we define as follows: ?x, y, z, t ?
U, x : y :: z : t if and only if either x = y and z = t
or x = z and y = t. In the sequel, we assume that
U is additionally provided with an associative inter-
nal composition law?, which makes (U,?) a semi-
group. The generalization of proportions to semi-
groups involves two key ideas: the decomposition of
objects into smaller parts, subject to alternation con-
straints. To formalize the idea of decomposition, we
define the factorization of an element u in U as:
Definition 1 (Factorization)
A factorization of u ? U is a sequence u1 . . . un,
with ?i, ui ? U , such that: u1 ? . . . ? un = u.
Each term ui is a factor of u.
The alternation constraint expresses the fact that
analogically related objects should be made of alter-
nating factors: for x : y :: z : t to hold, each factor
in x should be found alternatively in y and in z. This
yields a first definition of analogical proportions:
Definition 2 (Analogical proportion)
(x, y, z, t) ? U form an analogical proportion, de-
noted by x : y :: z : t if and only if there exists some
factorizations x1? . . . ?xd = x, y1? . . . ?yd = y,
z1 ? . . . ? zd = z, t1 ? . . . ? td = t such that
?i, (yi, zi) ? {(xi, ti), (ti, xi)}. The smallest d for
which such factorizations exist is termed the degree
of the analogical proportion.
This definition is valid for any semigroup, and a
fortiori for any richer algebraic structure. Thus, it
readily applies to the case of groups, vector spaces,
free monoids, sets and attribute-value structures.
122
3.2 Words over Finite Alphabets
3.2.1 Analogical Proportions between Words
Let ? be a finite alphabet. ?? denotes the set of
finite sequences of elements of ?, called words over
?. ??, provided with the concatenation operation .
is a free monoid whose identity element is the empty
word ?. For w ? ??, w(i) denotes the ith symbol in
w. In this context, definition (2) can be re-stated as:
Definition 3 (Analogical proportion in (??,.))
(x, y, z, t) ? ?? form an analogical proportion, de-
noted by x : y :: z : t if and only if there exists some
integer d and some factorizations x1 . . . xd = x,
y1 . . . yd = y, z1 . . . zd = z, t1 . . . td = t such that
?i, (yi, zi) ? {(xi, ti), (ti, xi)}.
An example of analogy between words is:
viewing : reviewer :: searching : researcher
with x1 = ?, x2 = view, x3 = ing and t1 = re,
t2 = search, t3 = er. This definition generalizes
the proposal of (Lepage, 1998). It does not ensure
the existence of a solution to an analogical equation,
nor its uniqueness when it exists. (Lepage, 1998)
gives a set of necessary conditions for a solution to
exist. These conditions also apply here. In particu-
lar, if t is a solution of x : y :: z :?, then t contains,
in the same relative order, all the symbols in y and z
that are not in x. As a consequence, all solutions of
an equation have the same length.
3.2.2 A Finite-state Solver
Definition (3) yields an efficient procedure for
solving analogical equations, based on finite-state
transducers. The main steps of the procedure are
sketched here. A full description can be found in
(Yvon, 2003). To start with, let us introduce the no-
tions of complementary set and shuffle product.
Complementary set If v is a subword of w, the
complementary set of v with respect to w, denoted
by w\v is the set of subwords of w obtained by re-
moving from w, in a left-to-right fashion, the sym-
bols in v. For example, eea is a complementary sub-
word of xmplr with respect to exemplar. When v is
not a subword of w, w\v is empty. This notion can
be generalized to any regular language.
The complementary set of v with respect to w is
a regular set: it is the output language of the finite-
state transducer Tw (see Figure 1) for the input v.
0 1 k
w(1) : ?
? : w(1)
w(k) : ?
? : w(k)
Figure 1: The transducer Tw computing comple-
mentary sets wrt w.
Shuffle The shuffle u ? v of two words u and v is
introduced e.g. in (Sakarovitch, 2003) as follows:
u ? v = {u1v1u2v2 . . . unvn, st. ui, vi ? ??,
u1 . . . un = u, v1 . . . vn = v}
The shuffle of two words u and v contains all the
words w which can be composed using all the sym-
bols in u and v, subject to the condition that if a
precedes b in u (or in v), then it precedes b in w.
Taking, for instance, u = abc and v = def , the
words abcdef , abdefc, adbecf are in u ? v; this
is not the case with abefcd. This operation gen-
eralizes straightforwardly to languages. The shuf-
fle of two regular languages is regular (Sakarovitch,
2003); the automaton A, computing K?L, is derived
from the automata AK = (?, QK , q0K , FK , ?K) and
AL = (?, QL, q0L, FL, ?L) recognizing respectively
K and L as the product automata A = (?, QK ?
QL, (q0K , q
0
L), FK ? FL, ?), where ? is defined as:
?((qK , qL), a) = (rK , rL) if and only if either
?K(qK , a) = rK and qL = rL or ?L(qL, a) = rL
and qK = rK .
The notions of complementary set and shuffle are
related through the following property, which is a
direct consequence of the definitions.
w ? u ? v ? u ? w\v
Solving analogical equations The notions of
shuffle and complementary sets yield another
characterization of analogical proportion between
words, based on the following proposition:
Proposition 1.
?x, y, z, t ? ??, x : y :: z : t? x ? t ? y ? z 6= ?
An analogical proportion is thus established if the
symbols in x and t are also found in y and z, and ap-
pear in the same relative order. A corollary follows:
123
Proposition 2.
t is a solution of x : y :: z :?? t ? (y ? z)\x
The set of solutions of an analogical equation
x : y :: z :? is a regular set, which can be computed
with a finite-state transducer. It can also be shown
that this analogical solver generalizes the approach
based on edit distance proposed in (Lepage, 1998).
3.3 Trees
Labelled trees are very common structures in NLP
tasks: they can represent syntactic structures, or
terms in a logical representation of a sentence. To
express the definition of analogical proportion be-
tween trees, we introduce the notion of substitution.
Definition 4 (Substitution)
A (single) substitution is a pair (variable ? tree).
The application of the substitution (v ? t?) to a tree
t consists in replacing each leaf of t labelled by v by
the tree t?. The result of this operation is denoted:
t(v ? t?). For each variable v, we define the binary
operator /v as t /v t? = t (v ? t?).
Definition 2 can then be extended as:
Definition 5 (Analogical proportion (trees))
(x, y, z, t) ? U form an analogical propor-
tion, denoted by x : y :: z : t iff there exists some
variables (v1, . . . , vn?1) and some factorizations
x1 /v1 . . . /vn?1 xn = x, y1 /v1 . . . /vn?1 yn = y,
z1 /v1 . . . /vn?1 zn = z, t1 /v1 . . . /vn?1 tn = t such
that ?i, (yi, zi) ? {(xi, ti), (ti, xi)}.
An example of such a proportion is illustrated on
Figure 2 with syntactic parse trees.
This definition yields an effective algorithm
computing analogical proportions between trees
(Stroppa and Yvon, 2005). We consider here a sim-
pler heuristic approach, consisting in (i) linearizing
labelled trees into parenthesized sequences of sym-
bols and (ii) using the analogical solver for words
introduced above. This approach yields a faster, al-
beit approximative algorithm, which makes analogi-
cal inference tractable even for large tree databases.
3.4 Algorithmic issues
We have seen how to compute analogical relation-
ships for features whose values are words and trees.
S
NP
the police
VP
have VP
impounded
NP
his car
:
S
NP
his car
VP
AUX
have
VP
been VP
impounded
PP
by NP
the police
::
S
NP
the mouse
VP
has VP
eaten
NP
the cat
:
S
NP
the cat
VP
AUX
has
VP
been VP
eaten
PP
by NP
the mouse
Figure 2: Analogical proportion between trees.
If we use, for trees, the solver based on tree lin-
earizations, the resolution of an equation amounts,
in both cases, to solving analogies on words.
The learning algorithm introduced in Section 2.2
is a two-step procedure: a search step and a trans-
fer step. The latter step only involves the resolu-
tion of (a restricted number of) analogical equations.
When x, y and z are known, solving x : y :: z :?
amounts to computing the output language of the
transducer representing (y ? z)\x: the automaton
for this language has a number of states bounded by
|x |? |y |? |z |. Given the typical length of words in
our experiments, and given that the worst-case ex-
ponential bound for determinizing this automaton is
hardly met, the solving procedure is quite efficient.
The problem faced during the search procedure
is more challenging: given x, we need to retrieve
all possible triples (y, z, t) in a finite set L such
that x : y :: z : t. An exhaustive search requires
the computation of the intersection of the finite-
state automaton representing the output language of
(L ? L)\x with the automaton for L. Given the size
of L in our experiments (several hundreds of thou-
sands of words), a complete search is intractable and
we resort to the following heuristic approach.
L is first split into K bins {L1, ..., LK}, with |Li |
small with respect to |L |. We then randomly select
k bins and compute, for each bin Li, the output lan-
guage of (Li ?Li)\x, which is then intersected with
L: we thus only consider triples containing at least
124
two words from the same bin. It has to be noted that
the bins are not randomly constructed: training ex-
amples are grouped into inflectional or derivational
families. To further speed up the search, we also im-
pose an upper bound on the degree of proportions.
All triples retrieved during these k partial searches
are then merged and considered for the transfer step.
The computation of analogical relationships has
been implemented in a generic analogical solver;
this solver is based on Vaucanson, an automata ma-
nipulation library using high performance generic
programming (Lombardy et al, 2003).
4 Experiments
4.1 Methodology
The main purpose of these experiments is to demon-
strate the flexibility of the analogical learner. We
considered two different supervised learning tasks,
both aimed at performing the lexical analysis of iso-
lated word forms. Each of these tasks represents a
possible instantiation of the learning procedure in-
troduced in Section 2.2.
The first experiment consists in computing one
or several vector(s) of morphosyntactic features to
be associated with a form. Each vector comprises
the lemma, the part-of-speech, and, based on the
part-of-speech, additional features such as number,
gender, case, tense, mood, etc. An (English) in-
put/output pair for this tasks thus looks like: in-
put=replying; output={reply; V-pp--}, where the
placeholder ?-? denotes irrelevant features. Lexi-
cal analysis is useful for many applications: a POS
tagger, for instance, needs to ?guess? the possi-
ble part(s)-of-speech of unknown words (Mikheev,
1997). For this task, we use the definition of analog-
ical proportions for ?flat? feature vectors (see sec-
tion 3.1) and for word strings (section 3.2). The
training data is a list of fully informed lexical en-
tries; the test data is a list of isolated word forms
not represented in the lexicon. Bins are constructed
based on inflectional families.
The second experiment consists in computing a
morphological parse of unknown lemmas: for each
input lemma, the output of the system is one or sev-
eral parse trees representing a possible hierarchical
decomposition of the input into (morphologically
categorized) morphemes (see Figure 3). This kind
of analysis makes it possible to reconstruct the series
of morphological operations deriving a lemma, to
compute its root, its part-of-speech, and to identify
morpheme boundaries. This information is required,
for instance, to compute the pronunciation of an un-
known word; or to infer the compositional meaning
of a complex (derived or compound) lemma. Bins
gather entries sharing a common root.
input=acrobatically; output =
B


HH
H
A
 HH
N
acrobat
A|N.
ic
B|A.
ally
Figure 3: Input/output pair for task 2. Bound mor-
phemes have a compositional type: B|A. denotes a
suffix that turns adjectives into adverbs.
These experiments use the English, German, and
Dutch morphological tables of the CELEX database
(Burnage, 1990). For task 1, these tables contain
respectively 89 000, 342 000 and 324 000 different
word forms, and the number of features to predict is
respectively 6, 12, and 10. For task 2, which was
only conducted with English lemma, the total num-
ber of different entries is 48 407.
For each experiment, we perform 10 runs, using
1 000 randomly selected entries for testing1. Gen-
eralization performance is measured as follows: the
system?s output is compared with the reference val-
ues (due to lexical ambiguity, a form may be asso-
ciated in the database with several feature vectors
or parse trees). Per instance precision is computed
as the relative number of correct hypotheses, i.e.
hypotheses which exactly match the reference: for
task 1, all features have to be correct; for task 2, the
parse tree has to be identical to the reference tree.
Per instance recall is the relative number of refer-
ence values that were actually hypothesized. Preci-
sion and recall are averaged over the test set; num-
bers reported below are averaged over the 10 runs.
Various parameters affect the performance: k, the
number of randomly selected bins considered during
the search step (see Section 3.4) and d, the upper
1Due to lexical ambiguity, the number of tested instances is
usually greater than 1 000.
125
bound of the degree of extracted proportions.
4.2 Experimental results
Experimental results for task 1 are given in Tables 1,
2 and 3. For each main category, two recall and pre-
cision scores are computed: one for the sole lemma
and POS attributes (left column); and one for the
lemma and all the morpho-syntactic features (on the
right). In these experiments, parameters are set as
follows: k = 150 and d = 3. As k grows, both recall
and precision increase (up to a limit); k = 150 ap-
pears to be a reasonable trade-off between efficiency
and accuracy. A further increase of d does not sig-
nificantly improve accuracy: taking d = 3 or d = 4
yields very comparable results.
Lemma + POS Lemma + Features
Rec. Prec. Rec. Prec.
Nouns 76.66 94.64 75.26 95.37
Verbs 94.83 97.14 94.79 97.37
Adjectives 26.68 72.24 27.89 87.67
Table 1: Results on task 1 for English
Lemma + POS Lemma + Features
Rec. Prec. Rec. Prec.
Nouns 71.39 92.17 54.59 74.75
Verbs 96.75 97.85 93.26 94.36
Adjectives 91.59 96.09 90.02 95.33
Table 2: Results on task 1 for Dutch
Lemma + POS Lemma + Features
Rec. Prec. Rec. Prec.
Nouns 93.51 98.28 77.32 81.70
Verbs 99.55 99.69 90.50 90.63
Adjectives 99.14 99.28 99.01 99.15
Table 3: Results on task 1 for German
As a general comment, one can note that high
generalization performance is achieved for lan-
guages and categories involving rich inflectional
paradigms: this is exemplified by the performance
on all German categories. English adjectives, at
the other end of this spectrum, are very difficult to
analyze. A simple and effective workaround for
this problem consists in increasing the size the sub-
lexicons (Li in Section 3.4) so as to incorporate in a
given bin all the members of the same derivational
(rather than inflectional) family. For Dutch, these
results are comparable with the results reported in
(van den Bosch and Daelemans, 1999), who report
an accuracy of about 92% on the task of predicting
the main syntactic category.
Rec. Prec.
Morphologically Complex 46.71 70.92
Others 17.00 46.86
Table 4: Results on task 2 for English
The second task is more challenging since the ex-
act parse tree of a lemma must be computed. For
morphologically complex lemmas (involving affixa-
tion or compounding), it is nevertheless possible to
obtain acceptable results (see Table 4, showing that
some derivational phenomena have been captured.
Further analysis is required to assess more precisely
the potential of this method.
From a theoretical perspective, it is important to
realize that our model does not commit us to a
morpheme-based approach of morphological pro-
cesses. This is obvious in task 1; and even if
task 2 aims at predicting a morphematic parse of in-
put lemmas, this goal is achieved without segment-
ing the input lemma into smaller units. For in-
stance, our learner parses the lemma enigmatically
as: [[[.N enigma][.A|N ical]]B|A. ly], that is with-
out trying to decide to which morph the orthographic
t should belong. In this model, input and output
spaces are treated symmetrically and correspond to
distinct levels of representation.
5 Discussion and future work
In this paper, we have presented a generic analog-
ical inference procedure, which applies to a wide
range of actual learning tasks, and we have detailed
its instantiation for common feature types. Prelimi-
nary experiments have been conducted on two mor-
phological analysis tasks and have shown promising
generalization performance.
These results suggest that our main hypotheses
are valid: (i) searching for triples is tractable even
with databases containing several hundred of thou-
sands instances; (ii) formal analogical proportions
are a reliable sign of deeper analogies between lin-
126
guistic entities; they can thus be used to devise flex-
ible and effective learners for NLP tasks.
This work is currently being developed in various
directions: first, we are gathering additional experi-
mental results on several NLP tasks, to get a deeper
understanding of the generalization capabilities of
our analogical learner. One interesting issue, not
addressed in this paper, is the integration of vari-
ous forms of linguistic knowledge in the definition
of analogical proportions, or in the specification of
the search procedure. We are also considering al-
ternative heuristic search procedures, which could
improve or complement the approaches presented in
this paper. A possible extension would be to define
and take advantage of non-uniform distributions of
training instances, which could be used both during
the searching and ranking steps. We finally believe
that this approach might also prove useful in other
application domains involving structured data and
are willing to experiment with other kinds of data.
References
David W. Aha. 1997. Editorial. Artificial Intelligence
Review, 11(1-5):7?10. Special Issue on Lazy Learn-
ing.
Gavin Burnage. 1990. CELEX: a guide for users. Tech-
nical report, University of Nijmegen, Center for Lexi-
cal Information, Nijmegen.
Walter Daelemans, Antal Van Den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34(1?3):11?41.
B.V. Dasarathy, editor. 1990. Nearest neighbor (NN)
Norms: NN Pattern Classification Techniques. IEEE
Computer Society Press, Los Alamitos, CA.
Brian Falkenheimer and Dedre Gentner. 1986. The
structure-mapping engine. In Proceedings of the meet-
ing of the American Association for Artificial Intelli-
gence (AAAI), pages 272?277.
Dedre Gentner, Keith J. Holyoak, and Boicho N.
Konikov, editors. 2001. The Analogical Mind. The
MIT Press, Cambridge, MA.
Douglas Hofstadter and the Fluid Analogies Research
group, editors. 1995. Fluid Concepts and Creative
Analogies. Basic Books.
Keith J. Holyoak and John E. Hummel. 2001. Under-
standing analogy within a biological symbol system.
In Dedre Gentner, Keith J. Holyoak, and Boicho N.
Konikov, editors, The analogical mind, pages 161?
195. The MIT Press, Cambridge, MA.
Yves Lepage. 1998. Solving analogies on words: An
algorithm. In Proceedings of COLING-ACL ?98, vol-
ume 2, pages 728?735, Montre?al, Canada.
Yves Lepage. 1999a. Analogy+tables=conjugation.
In G. Friedl and H.G. Mayr, editors, Proceedings of
NLDB ?99, pages 197?201, Klagenfurt, Germany.
Yves Lepage. 1999b. Open set experiments with direct
analysis by analogy. In Proceedings of NLPRS ?99,
volume 2, pages 363?368, Beijing, China.
Sylvain Lombardy, Raphae?l Poss, Yann Re?gis-Gianas,
and Jacques Sakarovitch. 2003. Introducing Vaucan-
son. In Proceedings of CIAA 2003, pages 96?107.
Andrei Mikheev. 1997. Automatic rule induction for
unknown word guessing. Computational Linguistics,
23(3):405?423.
Vito Pirrelli and Franc?ois Yvon. 1999. Analogy in the
lexicon: a probe into analogy-based machine learning
of language. In Proceedings of the 6th International
Symposium on Human Communication, Santiago de
Cuba, Cuba.
Tony A. Plate. 2000. Analogy retrieval and processing
with distributed vector representations. Expert sys-
tems, 17(1):29?40.
Jacques Sakarovitch. 2003. Ele?ments de the?orie des au-
tomates. Vuibert, Paris.
Royal Skousen. 1989. Analogical Modeling of Lan-
guage. Kluwer, Dordrecht.
Nicolas Stroppa and Franc?ois Yvon. 2005. Formal
models of analogical relationships. Technical report,
ENST, Paris, France.
Paul Thagard, Keith J. Holoyak, Greg Nelson, and David
Gochfeld. 1990. Analog retrieval by constraint satis-
faction. Artificial Intelligence, 46(3):259?310.
Antal van den Bosch and Walter Daelemans. 1999.
Memory-based morphological processing. In Pro-
ceedings of ACL, pages 285?292, Maryland.
D. Randall Wilson and Tony R. Martinez. 2000. Reduc-
tion techniques for instance-based learning algorithms.
Machine Learning, 38(3):257?286.
Franc?ois Yvon. 1999. Pronouncing unknown words us-
ing multi-dimensional analogies. In Proc. Eurospeech,
volume 1, pages 199?202, Budapest, Hungary.
Franc?ois Yvon. 2003. Finite-state machines solving
analogies on words. Technical report, ENST.
127
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 441?448
Manchester, August 2008
Normalizing SMS: are two metaphors better than one ?
Catherine Kobus
Orange Labs
2, avenue Pierre Marzin
F-22300 Lannion
Fran?ois Yvon
Univ Paris-Sud 11 & LIMSI/CNRS
BP 133
F-91403 Orsay Cedex
{catherine.kobus,geraldine.damnati}@orange-ftgroup.com, yvon@limsi.fr
G?raldine Damnati
Orange Labs
2, avenue Pierre Marzin
F-22300 Lannion
Abstract
Electronic written texts used in computer-
mediated interactions (e-mails, blogs,
chats, etc) present major deviations from
the norm of the language. This paper
presents an comparative study of systems
aiming at normalizing the orthography of
French SMS messages: after discussing
the linguistic peculiarities of these mes-
sages, and possible approaches to their au-
tomatic normalization, we present, evalu-
ate and contrast two systems, one draw-
ing inspiration from the Machine Transla-
tion task; the other using techniques that
are commonly used in automatic speech
recognition devices. Combining both ap-
proaches, our best normalization system
achieves about 11% Word Error Rate on a
test set of about 3000 unseen messages.
1 Introduction
The rapid dissemination of electronic communi-
cation devices (e-mails, Short Messaging Systems
(SMS), chatrooms, instant messaging programs,
blogs, etc) has triggered the emergence of new
forms of written texts (see eg. (Crystal, 2001;
V?ronis and Guimier de Neef, 2006)). Addressed
to relatives or peers, written on the spur of the
moment, using interfaces, each with its specific
constraints (computer keyboards, PDAs, mobile
phones keypads), these electronic messages are
characterised by massive and systematic devia-
tions from the orthographic norm, as well as by
a non conventional use of alphabetical symbols.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
In fact, letter and punctuation marks are not only
used to conventionally encode a phonetic content,
but also to introduce meta-discourse, and to sig-
nal emotions, verbal effects (eg. laughters), or at-
titudes (humor, derision, emphasis etc). If each
media enforces its own set of constraints and pro-
motes idiosyncratic forms of writings, these new
types of texts nonetheless share a lot of common-
alities. To effectively process these messages, it
is thus necessary to develop robust language pro-
cessing tools, capable of bearing with the extreme
form of ?noise? they contain. In this study, we
focus more specifically on SMS, which, due to
the paucity of their input interface (mobile phone
keypads) seem to constitute the most challenging
type of data. To a large extent, the techniques we
present in this paper are also applicable to other
types of electronic messages.
The ?SMS language? (or ?texting language?)
has been the subject of several linguistic studies
(notably, for French, (Anis, 2001; Fairon et al,
2006)), which have emphasized its main charac-
teristics, amongst which the extraordinary ortho-
graphic variability of lexical forms. In brief, this
variability results partly from the mixing of several
encoding systems: in SMS, the usual alphabetic
system competes with a more ?phonetic? type of
writing (e.g. rite for right
1
), as well as with traces
of a ?consonantic? spelling (vowels are deleted,
as in wrk for work or cn for can), and with
non-conventional use of letters or numbers, some-
times used to encode the phonetic value of their
1
We will illustrate this general presentation of the SMS
language using examples taken from English messages, even
though our systems deal with French messages. As far as we
can see, the same types of deviations from the orthographic
norm are observed in both languages, albeit in different pro-
portions. A more thorough comparison of both languages cer-
tainly remains to be carried out.
441
spelling, as in ani1 for anyone. These spelling sys-
tems can also be mixed as in Rtst (for artist) or
bcum (for become). This variability is also the re-
sult of an informal style of communication, which
licenses many deviations from the orthographic
(simplification of repeated consonants, use of non-
conventional abreviations) and grammatical (ab-
sence of case distinction, erratic use of punctuation
marks, non-respect of agreement or tense mark-
ers, etc) prescriptions, notwithstanding truly unin-
tentional typos. Finally, practitioners of the tex-
ting language excel in devising acronyms which
condense, sometimes in a radical way, multi-word
units: this is for instance the case with afair, which
stands for as far as I recall. As a result, from a
natural language processing (NLP) point of view,
these messages contain an abnormally high rate of
out-of-vocabulary forms, and the ambiguity of ex-
isting word forms is aggravated, two factors that
contribute to degrade the performance of natural
language processing tools. Recovering a normal-
ized orthography seems thus to be a necessary pre-
processing step for many real-world NLP applica-
tions, such as text-to-speech, translation, or text
mining applications (filtering, routing, information
retrieval, etc).
These short messages have so far received rel-
atively little attention from the NLP community
2
:
see, for English, (Aw et al, 2006; Choudhury et
al., 2007), which both address the problem with
statistical learning techniques, and, for French,
(Guimier de Neef et al, 2007), which details
a complete pipe-line of hand-crafted, symbolic,
modules. In fact, the problem of normalizing
SMS shares a lot of commonalities with other NLP
applications, and can be addressed from several
viewpoints. The first, maybe the most natural an-
gle, is to make an analogy with the spelling cor-
rection problem. This problem has been exten-
sively studied in the past and a variety of statisti-
cal approaches are readily available, most notably
the ?noisy channel? approach (see eg. (Church and
Gale, 1991; Brill and Moore, 2000; Toutanova and
Moore, 2002)). An alternative metaphor is the
translation metaphor: under this view, the normal-
ization task is accomplished by taking the SMS
2
A couple of on-line SMS-to-English translation systems
are accessible on the Internet, see notably http://www.
transl8it.com/ and http://www.lingo2word.
com/; ?Netspeak? dictionaries, again for English, also
abound. The situation is more or less comparable for French,
see eg http://www.traducteur-sms.com/.
language as a foreign language, and using standard
(statistical) translation techniques. Both views
have their own merit, and their limitations, which
we shall review shortly. In this paper, we propose
yet another metaphor, which stems from the simi-
larities between the SMS language and speech, and
notably from the fact that in SMS, word separators
are much less reliable than in conventional writ-
ings. As a result, it seems necessary to implement
techniques, which are, as in speech recognition,
capable of recovering the correct word segmenta-
tion.
The main contribution of this work is to present,
evaluate and contrast two approaches to the SMS
normalization problem. We demonstrate that both
approaches have different advantages and pitfalls,
and show their combination yields significant im-
provements wrt. to both single systems.
This paper is organized as follows: in section 2,
we discuss these three metaphors; then go on to
describe our own implementation of two different
systems for normalizing SMS in 3. A comparative
evaluation of these systems is conducted in sec-
tion 4, where we emphasize the complementarity
of both approaches, and assess the performance of
a combined systems. Section 5 presents a sum-
mary of the main findings and future prospects.
2 Three metaphors for SMS
normalization systems
In this section, we set the general problem of SMS
normalization, and discuss, based on the analysis
of SMS examples, the relevance of various NLP
approaches to this task.
2.1 The "spell checking" metaphor
A first approach to the problem considers each in-
put token as ?noisy? version of the correct word
form: normalization is thus viewed as a spell
checking task. The spell-checking problem has re-
ceived considerable attention in the past, and a va-
riety of correction techniques have been proposed:
in this context, noisy channel models (Church and
Gale, 1991; Brill and Moore, 2000; Toutanova and
Moore, 2002), to quote just a few, constitute one of
the predominant and most successfull approaches.
Under this paradigm, correction is performed on a
word-per-word basis, and concerns primarily out-
of-vocabulary tokens: the general assumptions are
that most words are correctly spelled, and that
in-vocabulary words should preferably be left un-
442
touched. In this context, the best correction(s) w
of an erroneous word v is retrieved from the dictio-
nary by combining the individual context indepen-
dant probability ofw with an error model probabil-
ity, which computes the probability of mistyping v
forw, based on the surface similarity between both
forms. The key component here is the error model,
which should not only capture orthographic simi-
larities (Brill and Moore, 2000), but also phonetic
similarities (Toutanova and Moore, 2002).
This framework easily extends to the case where
several words should simultaneously be corrected:
it is simply a matter of exploring the lattice of all
possible corrections, which can be re-ranked us-
ing conventional tools such as statistical language
models. This is basically the idea behind the reac
system (Michel Simard, 2001),which recovers the
correct accentuation of unaccented French texts
using an error model, complemented with a sta-
tistical language model.
As far as SMS are concerned, this approach
is essentially the one followed by (Choudhury
et al, 2007), which models the joint probabil-
ity of observing the word w represented by the
character sequence c
1
...c
l
by a Hidden Markov
Model (HMM) whose topology takes into account
both ?graphemic? variants (typos, omissions of re-
peated letters, etc) and ?phonemic? variants (i.e
spellings that resemble the word?s pronunciation).
This HMM is initialized by considering the word?s
received orthography and phonology, with addi-
tional transitions to account for the possibility of
inserting, substituting or deleting symbols. For the
most frequent words in the corpus, the various pa-
rameters associated with these transitions are esti-
mated on a training corpus; various heuristic are
then used to plug these values in the HMMs that
model the less frequent dictionary items.
2.2 The ?translation? metaphor
A second approach to the problem consists in
adopting the translation metaphor: using this anal-
ogy, the SMS language is just another foreign lan-
guage and the normalization can be viewed as a
?pure? machine translation (MT) task.
Using machine translation tools might be re-
garded as ?an overkill? (Choudhury et al, 2007),
considering the close relationships between source
and target languages. Furthermore, learning the
kinds of many-to-many correspondences between
source and target sentences that make up for the
high translation accuracy of phrase-based systems
might be seen as introducing an unnecessary com-
plexity, as SMS tend to be shorter, in terms of
words, than their normalized counterparts. This
suggests that looking for many (on the normal-
ized side) to one (on the SMS side) might be good
enough to capture most pairings. Finally, statistical
machine translation tools incorporate mechanisms
to model the possible mismatch in word order be-
tween source and target, which are virtually non-
existing when it comes to translating SMS.
This metaphor is, nonetheless, the one resorted
to in (Aw et al, 2006), which uses a statisti-
cal phrase-based machine translation tool to con-
vert English SMS texts into standardized English.
This system incorporates some of the peculiarities
of this translation task, which both simplifies the
construction of the phrase-table and the decoding
search algorithm. Using this system, (Aw et al,
2006) reports a 0.81 BLEU (Papineni et al, 2001)
score on a set of 5,000 English SMS.
Normalization as translation is certainly a nat-
ural, and simple to implement, idea. Using
phrase-based systems, it becomes possible to
model (context-dependant) one-to-many relation-
ships that are out-of-reach of the spell checking
approach. We feel that it still overlooks some as-
pects of the task, notably the fact that the lexi-
cal creativity attested in SMS messages can hardly
be captured in a static phrase table, where corre-
spondences between SMS phrases and normalized
phrases are learned by rote, rather than modeled.
2.3 The "speech recognition" metaphor
The SMS language has on occasions been de-
scribed, sometimes abusively, as being closer to
oral productions than to regular written texts. If
we do not subscribe to this view, we nonetheless
feel that a third metaphor is worth considering, that
we call the ?automatic speech recognition? (ASR)
metaphor. This analogy stems from the fact that,
for a significant fraction of tokens, the spelling of
SMS forms tends to be a closer approximation of
the phonemic representation of a word than of is
its normative spelling.
In the speech recognition metaphor, an SMS
message is thus primarily viewed as an alpha-
betic/syllabic approximation of a phonetic form.
Given a suitable mechanism for converting the
SMS stream into a phone lattice, the problem of
SMS normalization becomes very similar to that
443
of speech recognition, that is, the decoding of a
word sequence in a (weighted) phone lattice. It
actually becomes a much simpler problem, as (i)
the acoustic ambiguity of speech input is typically
much higher than the phonemic indeterminacy of
SMS messages, (ii) some segmentation informa-
tion is already available in the SMS text, which is
in sharp contrast with (continuous) speech recogni-
tion, where word boundaries have to be uncovered.
Based on this general principle, we devised an
ASR-like normalization system, which has three
additional benefits: using a phonemic approxima-
tion provides the system with the ability to correct
some (unintentional) typos; adopting an ASR-like
architecture provides us with a ?natural? frame-
work for resegmenting agglutinated word forms;
finally, in the larger context of SMS-to-speech ap-
plications, which is one of our targeted applica-
tions, the computation of a phonemic representa-
tion of the message can prove extremely valuable.
3 Two normalization systems
3.1 The MT-like system
Our first normalization system is entirely based on
open-source, public domain packages for statisti-
cal machine translation. Giza++ (Och and Ney,
2003) is used to induce, based on statistical princi-
ples (Brown et al, 1990), an automatic word align-
ment of SMS tokens with their normalized coun-
terparts; Moses (Koehn et al, 2007) is used to
learn the various parameters of the phrase-based
model, to optimize the weight combination and to
perform the translation using a multi-stack search
algorithm; the SRI language model toolkit (Stol-
cke, 2002) is finally used to estimate statistical lan-
guage models. For this system, the training set has
been split in a learning set
3
(approximately 25000
messages) and a development set (about 11700
messages), which is used to tune parameters.
As suggested in the previous sections, we have
constrained both systems to consider only ?mono-
tonic? alignments between the source and the tar-
get languages.
3.2 The ASR-like system
In a nutshell, our second normalization system
mimics the behavior of speech recognition sys-
tem and decodes SMS message through a non-
deterministic phonemic transduction; based on
preliminary experiments, this simple architecture
3
See section 4 for a description of the corpora.
was augmented by an additional mechanism which
specifically deals with out-of-vocabulary tokens.
In the following, we denote ? the set of alpha-
betic symbols, ? the set of phonemic symbols, and
? the set of lexical items. Using these notations,
our architecture can be described as a pipe-line of
the following components:
? the first processing step consists in a
dictionary-based grapheme-to-phoneme con-
version of some highly idiosyncratic forms,
which deals with tokens
4
whose spelling in
SMS does not reflect the phonemic content of
the corresponding lexical item(s). This is, for
instance, the case for common abbreviations
(eg. btw for by the way) and for instances
of ?consonantic? spellings. The dictionnary
used in the experiments reported above con-
tains about 4,200 entries.
This module is implemented as a finite-
state transducer E which transduces letter
sequences in ?
?
into mixed grapheme and
phoneme sequences (in (? ? ?)
?
).
? the second module converts the graphemic
portions of the input message into a phone-
mic string using a set of manually encoded
non-deterministic letter-to-phone rules; these
rules notably encode the possibility for each
symbol to encode its spelling (eg. u for /ju/ or
R for /@r/). Our system currently comprises
about 150 letter-to-phone rules. The output
of this module is a phone lattice, which rep-
resents all the possible pronunciations of the
complete input stream.
This module is also implemented as a finite-
state transducer P representing a rational
relation between (? ? ?)
?
and ?
?
: each
grapheme-to-phoneme rule is compiled into a
finite-state transducer; these individual rules
are then, once properly ordered, combined
through the composition operator. The result-
ing finite-state machine is denoted P .
? this phone lattice is then turned into a word
based lattice, using an inverted pronuncia-
tion dictionary, which registers the known
associations between phone sequences and
words. This inverted dictionary contains ap-
proximately 21K words, which are the most
4
At this stage, we take advantage of usual word separators
to identify tokens in the message.
444
frequent words in our reference training cor-
pus. This module is also implemented as
a finite-state transducer D, which maps se-
quences in ?
?
to sequences in ?
?
.
A key aspect of this module is its ability to
alter the original tokenization, by freely in-
serting word separators whenever a phonetic
word is recognized, no matter whether it cor-
responds to a complete input token or not.
This mechanism is illustrated on Figure 1. As
a result, this system can bear with agglutina-
tions (i.e. absence of one or several word sep-
arators) in the input sequence.
? the final processing step consists in searching
the word lattice for the most probable word
sequence, as computed by a statistical lan-
guage model (here, a smoothed n-gram lan-
guage model estimated on the training cor-
pus). Here again, the entire process is com-
puted through finite state operations: the out-
put language of the previous steps is inter-
sected with the stochastic language model
S (a weighted finite-state acceptor), and the
most likely path is computed through dy-
namic programming.
As mentioned earlier, each module is imple-
mented as a finite state acceptor or transducer;
these modules are built and combined using tools
from the FSM (Mohri et al, 1998) and the GRM
(Allauzen et al, 2005) toolkit. As a result, the
entire normalization process is computed by a
weighted transducer (E ?P ?D ?S), which can be
optimized off-line as is commonly done in finite-
state speech recognition systems (Mohri and Riley,
1998).
In addition to these four main modules, the pre-
processing module of the ASR-like system con-
tains a number of small enhancements that im-
prove the normalization of dates and hours. We
furthermore had to modify the processing of out-
of-vocabulary words: in the architecture sketched
above, any word that does not belong to the vocab-
ulary has to be decomposed into smaller, known,
words, causing systematic errors. Our final ASR-
like system allows these forms to be either decom-
posed phonetically or copied verbatim in the out-
put. A complete description of this system is given
in (Kobus et al, 2008).
1
2
3
0/0
4
5
6
<e
ps>
:<e
ps>
_#
:<e
ps>
p:p
au
l
O:<
ep
s>
l:<
ep
s>
_#
:<e
ps>
<e
ps>
:<e
ps>
l:lo
uis
w:<
ep
s>
i:<
ep
s>
Figure 1: Transducing phone sequences into word
sequences with a dictionary
This simplistic inverted dictionary recognizes two
phonemic sequences: /lwi/ (for Louis) and /pOl/
(for Paul). Upon recognition of any such sequence,
two transitions loop back to the initial state: one
carries the input symbol ?#?, which is used when-
ever a word separator is encountered; the other is
an ? transition, which allows to re-segment the in-
put stream.
4 Experiments
4.1 Experimental protocol
The experiments reported below use two corpora.
The first one has been collected at the University
of Aix-en-Provence (Hocq, 2006); it contains ap-
proximately 9700 messages. The second corpus
has been gathered in Belgium by the Catholic Uni-
versity of Louvain, and totals about 30000 mes-
sages (Fairon and Paumier, 2006). Both corpora
contain, for each message, a reference normaliza-
tion which has been produced and validated by
human annotators. Both corpora were merged,
lowercased, stripped from punctuation signs and
standardized (in particular with respect to the
anonymization conventions). This database was
split in a training set (about 36700 messages) and
a distinct test set of about 3000 messages. The
training set was used both to train and tune the
MT-like system and to estimate a 3-gram language
model required in both approaches, using standard
back-off procedures. Some relevant statistics re-
garding the sub-corpora that were used for training
are given in Table 4.1.
For the evaluation, contrarily to (Aw et al, 2006;
Guimier de Neef et al, 2007), who by analogy
445
Aix Louvain Total
# messages 8,700 28,000 36,700
Original messages
avg. length 14.3 21.6 19.9
# tokens 124 700 606 100 730 800
# types 13 600 37 900 43 600
% unknown 43.7 % 30.4 % 32.7 %
Normalized messages
avg. length 15.4 23.2 21.3
# tokens 133 800 650 100 783 900
# types 8 200 20 800 23 300
Table 1: Statistics of the training corpora
Statistics on the original messages are computed
after preprocessing (punctuation removal, etc.);
the length of a message is the number of tokens;
% unknown is the percentage of tokens that do not
occur in the normalized message.
with the machine translation task, assess their sys-
tem with the BLEU metric (Papineni et al, 2001),
we decided to measure the performance of our nor-
malization tool with the Word Error Rate (WER)
and Sentence Error Rate (SER) metrics. This
choice is motivated by the fact that the outcome
of the normalization process is _ notwithstanding
a couple of arbitrary normalization decisions _ al-
most deterministic, and does not warrant the use
of BLEU, which is more appropriate to evaluate
tasks with multiple references. Additionally, we
feel that error rates are easier to interprete than
BLEU values; for the sake of comparisons, some
BLEU scores will nonetheless be reported.
4.2 Baseline results
In a first series of experiments, we evaluate our two
systems and analyze their respective strengths and
weakness. Table 4.2 reports the results of these ex-
periments; the line ?initial? gives the corresponding
numbers for the original messages, which gives a
rough idea of the number of words that must be
modified. As these results demonstrate, the MT-
like system proves to be much more accurate than
the ASR-like system.
Looking at the errors, the main problem with
the latter system stems from the loss of the origi-
nal spelling and tokenization information incurred
during the grapheme-to-phoneme conversion step:
as a consequence, many words that were correctly
spelled in the input message are erroneously reseg-
mented and decoded. This is evidenced by the high
Sub Ins Del WER SER
initial 33.23 0.42 8.54 42.18 91.39
ASR-like 11.94 2.21 2.36 16.51 76.05
MT-like 7.34 0.71 4.22 12.26 63.41
Table 2: Evaluation of the baseline systems
Columns ?Sub?, ?Ins?, and ?Del? report respectively
the number of substitution, insertion and deletion
errors at the word level.
number of substitutions and insertions produced
by this system, a large number of which concern
function words. This phenomena is accentuated by
the excessive liberality of grapheme-to-phoneme
rules. For instance, to account for the erratic use of
accentuated letters in SMS, the most general pro-
nunciation rule for letter e (incidentally, e is the
most frequent letter in French) predicts five pro-
nunciations: /@/, /e/, /E/, /?/, /?/, plus the pos-
sibility of being deleted. As a result, first group
verbal forms such as aime (?I or he/he love(s)?)
yield (at least) six different pronunciations, which
result in many more combinations after resegmen-
tation, such as aime (?(I, you, he/she) love(s)?),
aim? (?loved?), aimer (?to love?), aime et (?I love
and), aime est (I love is) etc. The same occurs with
de (?of (the)?, ?some-SING?) and le (?the-SING?),
which, through the phonemic encoding/decoding,
become systematically ambiguous with their cor-
responding plural des and les. Sorting out the cor-
rect combination seems to be too hard a task for
the statistical language model, since all these hy-
potheses include very high frequency tokens.
The MT-like system is significantly less error-
prone: an error analysis reveals that the most com-
mon errors concern the insertion or deletion of
function words, which can be attributed to noisy
alignments in the phrase table. Another frequent
source of error stems from agglutinated forms, no-
tably combinations of clitic(s)+verb (e.g. jtombrai
for je tomberai (?I will fall?), jrentr for je rentre
(?I am coming back?)) or (preposition and/or ar-
ticle)+noun or verbs (e.g. cours 2droit for cours
de droit (?law class?), dsortir for de sortir (?to
go out?)...): whenever these forms are met in the
training corpus, they can be correctly decoded;
however, many novel forms only occur in the test
set, owing to the fact that these types of aggluti-
nations are ?productive? (in the appropriate con-
text). Contrarily to the findings of (Choudhury
et al, 2007), which considered English messages,
446
our corpus study reveals that this phenomena is far
from marginal, and is a systematic source of errors
for theMT system. It is noteworthy that about 17%
of the tokens in the test SMS corpus do not occur in
the training set, when the ?true? out-of-vocabulary
rate (computed on the reference messages) is only
about 2.1 %.
Both systems are finally at pain to correctly
recover the right number/gender/tense agreement,
which is a general problem with n-gram language
models in French, aggravated here by some irre-
ducible indeterminacy: should ?d?sol? ? (masc.) in
?je suis d?sol? ? be corrected as ?d?sol?e? (fem) ?
ultimately, this depends on the sex of the sender,
which may be deduced from some other, poten-
tially long distant, part of the message; the same
indeterminacy occurs with the normalization of
?1 ?, which can be mapped to ?un? (masc.) or ?une?
(fem); with ?aurai ? (?I will have?), which can be
mapped with ?aurai ? or ?aurais? (?I would have?),
etc.
4.3 System combination
The analysis of normalization errors reveals that
both systems have different strengths and weak-
nesses, suggesting that they could be used in com-
bination. Indeed, oracle selection of the best out-
put on a per message basis would yield an over-
all 9,63 WER, about 2.5 points absolute below the
performance of the MT-like system.
Various ways to combine both approaches have
been considered: we eventually decided to use the
MT-like system for producing a first normaliza-
tion; out-of-vocabulary tokens in the original SMS
appear untouched in this output. For each of these,
we use the ASR-like system to produce a series of
?local? hypothesis, which are combined in a word
lattice. This lattice is rescored with the statisti-
cal language model to yield the final output. This
simple combination proved to yield significant im-
provements, decreasing the word-error rate from
12.26 to 10.82. The corresponding BLEU score
is close to 0,8, in line with the findings of (Aw
et al, 2006) for English, and comparing favorably
with the 0.68 score reported in (Guimier de Neef et
al., 2007) (for French, using a different test bed).
Preliminary experiments suggest that using n-best
list outputs from Moses instead of just the one best
could buy us an small additional WER decrease.
The typical improvements brought by the com-
bined system are illustrated by the following exam-
ple, where two cases of agglutinated word forms
are corrected, resulting in a correct output:
SMS oubli?2tdir: tom a pom? c
foto dlui en string
MT-like oubli?2tdir tom a paum? ses
photos dlui en string
Combined oubli? de te dire tom a paum?
ses photos de lui en string
(I) forgot to tell you (that)
tom lost photos of himself in
a thong
5 Conclusion and Perspectives
In this paper, we studied various ways to address
the problem of normalization of SMS, by drawing
analogy with related NLP problems, and accord-
ingly reusing as much as possible existing tools or
modules. Following (Aw et al, 2006), we found
that using off-the-shell statistical MT systems al-
lows to achieve very satisfactory WER; combin-
ing this system with a system based on an anal-
ogy with the speech recognition problem yields
an additional 1.5 absolute improvement in WER.
As it stands, our statistical normalization system
seems to be sufficiently efficient to be used for text
mining purposes; it also provides a useful tool to
quantitatively analyze the various mechanisms in-
volved in SMS spelling. The problem nonetheless
remains far from being solved: our best system
still makes at least one error on about 60% of the
test messages.
There are a number of obvious improvements
we might consider, such as using more accu-
rate grapheme-to-phoneme rules, or plugging in a
larger statistical language model, but we feel these
would buy us only small increase in performance.
As a first step to improve our normalization sys-
tem, we would rather like to combine the existing
approaches with a spell-checking approach. The
most natural way to proceed would be to devise an
alternative letter-to-word finite-state transducer C,
aimed at converting space separated sequences of
alphabetic symbols to the corresponding sequence
of words, allowing for usual spelling errors (dele-
tion/insertion of a letter, substitution, etc). Using
the notations of section 3.2, the normalization sys-
tem would thus be computed by the following fi-
nite state machine: ([E ? P ?D] ? C) ? S.
Another natural extension would be to make this
finite-state transducer stochastic: again, this would
447
be a rather simple matter to train this transducer us-
ing the forward-backward algorithm (see (Jansche,
2003)) on the available training data.
6 acknowledgments
The authors wish to thank E. Guimier de Neef for
providing us with one of the databases and other
useful resources. Many thanks to our anonymous
reviewers for helpful comments.
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark.
2005. The design principles and algorithms of a
weighted grammar library. International Journal of
Foundations of Computer Science, 16(3):403?421.
Anis, Jacques. 2001. Parlez-vous texto ? Guide des
nouveaux langages du r?seau. ?ditions du Cherche
Midi.
Aw, Aiti, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization. In Proceedings of COLING/ACL, pages
33?40.
Brill, Eric and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 286?
293, Hong Kong.
Brown, Peter F., John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990.
A statistical approach to machine translation. Jour-
nal of Natural Language Engineering, 16(2):79?85.
Choudhury, Monojit, Rahul Saraf, Vijit Jain, Sudeshna
Sarkar, and Anupam Basu. 2007. Investigation and
modeling of the structure of texting language. In
Proceedings of the IJCAIWorkshop on "Analytics for
Noisy Unstructured Text Data", pages 63?70, Hyder-
abad, India.
Church, Kenneth W. and William Gale. 1991. Proba-
bility scoring for spelling correction. Statistics and
Computing, 1:91?103.
Crystal, David. 2001. Language and the Internet.
Cambridge University Press.
Fairon, C?drick and S?bastien Paumier. 2006. A trans-
lated corpus of 30,000 French SMS. In Proceedings
of LREC 2006, Genoa, Italy.
Fairon, C?drick, Jean Ren? Klein, and S?bastien Pau-
mier. 2006. Le langage SMS. UCL Presses Univer-
sitaires de Louvain.
Guimier de Neef, ?milie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS :
?valuation et bilan quantitatif. In Actes de TALN,
pages 123?132, Toulouse, France.
Hocq, S. 2006. ?tude des sms en fran?ais : constitution
et exploitation d?un corpus align? sms-langue stan-
dard. Technical report, Universit? Aix-Marseille.
Jansche, Martin. 2003. Inference of string mappings
for language technology. Ph.D. thesis, Ohio State
University.
Kobus, Catherine, Fran?ois Yvon, and G?raldine
Damnati. 2008. Transcrire les SMS comme on re-
conna?t la parole. In Actes de la Conf?rence sur
le Traitement Automatique des Langues (TALN?08),
pages 128?138, Avignon, France.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. Annual Meeting of the Association for Com-
putational Linguistics (ACL), demonstration session,
Prague, Czech Republic.
Michel Simard, Alexandre Deslauriers. 2001. Real-
time automatic insertion of accents in French text.
Natural Language Engineering, 7(2):143?165.
Mohri, Mehryar andMichael Riley. 1998. Network op-
timisation for large vocabulary speech recognition.
Speech Communication, 25(3):1?12.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
1998. A rational design for a weighted finite-state
transducer library. Lecture Notes in Computer Sci-
ence, (1438).
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Papineni, Kishore, Salim Roukos, ToddWard, andWei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Langage Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Toutanova, Kristina and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 144?
151, Philadelphia, PA.
V?ronis, Jean and ?milie Guimier de Neef. 2006. Le
traitement des nouvelles formes de communication
?crite. In Sabah, G?rard, editor, Compr?hension
automatique des langues et interaction, pages 227?
248. Paris: Herm?s Science.
448
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 593?600
Manchester, August 2008
Robust Similarity Measures for Named Entities Matching
Erwan Moreau1
Institut Te?le?com ParisTech
& LTCI CNRS
erwan.moreau@enst.fr
Franc?ois Yvon
Univ. Paris Sud
& LIMSI CNRS
yvon@limsi.fr
Olivier Cappe?
Institut Te?le?com ParisTech
& LTCI CNRS
cappe@enst.fr
Abstract
Matching coreferent named entities with-
out prior knowledge requires good similar-
ity measures. Soft-TFIDF is a fine-grained
measure which performs well in this task.
We propose to enhance this kind of met-
rics, through a generic model in which
measures may be mixed, and show experi-
mentally the relevance of this approach.
1 Introduction
In this paper, we study the problem of matching
coreferent named entities (NE in short) in text col-
lections, focusing primarily on orthographic vari-
ations in nominal groups (we do not handle the
case of pronominal references). Identifying textual
variations in entities is useful in many text min-
ing and/or information retrieval tasks (see for ex-
ample (Pouliquen et al, 2006)). As described in
the literature (e.g. (Christen, 2006)), textual dif-
ferences between entities are due to various rea-
sons: typographical errors, names written in dif-
ferent ways (with/without first name/title, etc.),
abbreviations, lack of precision in organization
names, transliterations, etc. For example, one
wants ?Mr. Rumyantsev? to match with ?Alexan-
der Rumyanstev? but not with ?Mr. Ryabev?.
Here we do not address the related problem of dis-
ambiguation2 (e.g. knowing whether a given oc-
currence of ?George Bush? refers to the 41st or
43rd president of the USA), because it is techni-
cally very different from the matching problem.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1Now at LIPN - Univ. Paris 13 & UMR CNRS 7030.
2Which is essential in the Web People Search task.
There are different ways to tackle the problem
of NE matching: the first and certainly most reli-
able one consists in studying the specific features
of the data, and then use any available tool to de-
sign a specialized method for the matching task.
This approach will generally take advantage of
language-specific (e.g. in (Freeman et al, 2006))
and domain-specific knowledge, of any external
resources (e.g. database, names dictionaries, etc.),
and of any information about the entities to pro-
cess, e.g. their type (person name, organization,
etc.), or internal structure (e.g. in (Prager et al,
2007)). In such an in-depth approach, supervised
learning is helpful: it has been used for example
in a database context3 in (Bilenko et al, 2003), but
this approach requires labeled data which is usu-
ally costly. All those data specific appproaches
would necessitate some sort of human expertise.
The second approach is the robust one: we
propose here to try to match any kind of NE,
extracted from ?real world? (potentially noisy)
sources, without any kind of prior knowledge4.
One looks for coreferent NE, whatever their type,
source, language5 or quality6. Such robust simi-
larity methods may be useful for a lot of generic
tasks, in which maximum accuracy is not the main
criterion, or simply where the required resources
are not available.
The literature on string comparison metrics is
abundant, containing both general techniques and
3The matching task is quite different in this framework,
because one observes records (structured information).
4In this kind of knowledge are included the need for hand-
tuning parameters or defining specific thresholds.
5Actually we have only studied English and French (our
approach is neither ?multilingual?, in the sense that it is not
specific to multilingual documents).
6In particular, this task clearly depends on the NE recog-
nition step, which may introduce errors.
593
more linguistically motivated measures, see e.g.
(Cohen et al, 2003) for a review. From a bird?s eye
view, these measures can be sorted in two classes:
?Sequential character-based methods? and ?Bag-
of-words methods?7. Both classes show relevant
results, but do not capture the same kind of simi-
larity. In a robust approach for NE matching, one
needs a more fine-grained method, which performs
at least as well as bag-of-words methods, without
ignoring coreferent pairs that such methods miss.
A first attempt in this direction was introduced
in (Cohen et al, 2003), in the form of a measure
called Soft-TFIDF. We will show that this measure
has theoretical pitfalls and a few practical draw-
backs. Nevertheless, Soft-TFIDF outperforms the
better standard string similarity measures in the
NE matching task. That is why we propose to gen-
eralize and improve its principle, and show exper-
imentally that this approach is relevant.
In section 2 we introduce standard similar-
ity measures and enhance the definition of Soft-
TFIDF. Then we define a generic model in which
similarity measures may be combined (section 3).
Finally, section 4 shows that experiments with two
different corpora validate our approach.
2 Approximate matching methods
We present below some of the main string similar-
ity measures used to match named entities (Chris-
ten, 2006; Cohen et al, 2003; Bilenko et al, 2003).
2.1 Classical metrics
2.1.1 Sequential character based methods
Levenshtein edit distance. This well-known dis-
tance metric d represents the minimum number
of insertions, deletions or substitutions needed to
transform a string x into another string y. For ex-
ample, d(kitten, sitting) = 3 (k 7? s, e 7? i,
? 7? g). The corresponding normalized similarity
measure is defined as s = 1 ? d/max(|x|, |y|). A
lot of variants and/or improvements exist (Navarro,
2001), among which:
? Damerau. One basic edit operation is added:
a transposition consists in swapping two
characters;
? Needleman-Wunch. Basic edit operation
costs are parameterized: G is the cost of a gap
7We omit measures based on phonetic similarity such
as Soundex, because they are language-specific and/or type-
specific (person names).
(insertion or deletion), and there is a function
cost(c, c
?
) which gives the cost of substituting
c with c? for any pair of characters (c, c?).
Jaro metric (Winkler, 1999). This measure is
based on the number and the order of common
characters. Given two strings x = a
1
. . . a
n
and
y = b
1
. . . b
m
, let H = min(n,m)/2: a
i
is in com-
mon with y if there exists b
j
in y such that a
i
= b
j
and i ? H ? j ? i + H . Let x? = a?
1
. . . a
?
n
?
(resp. y? = b?
1
. . . b
?
m
?
) be the sequence of charac-
ters from x (resp. y) in common with y (resp. x),
in the order they appear in x (resp. y). Any posi-
tion i such that a?
i
6= b
?
i
is called a transposition.
Let T be the number of transpositions between x?
and y? divided by 2:
Jaro(x, y) =
1
3
?
(
|x
?
|
|x|
+
|y
?
|
|y|
+
|y
?
|?T
|y
?
|
)
2.1.2 Bag-of-words methods
With these methods, each NE is represented as
a set of features (generally words or characters n-
grams8). Let X = {x
i
}
1?i?n
and Y = {y
i
}
1?i?m
be the sets representing the entities x, y. Simplest
measures only count the number of elements in
common9, e.g:
Overlap(x, y) =
|X ? Y |
min(|X|, |Y |)
Some more subtle techniques are based on a
vector representation of entities x and y, which
may take into account parameters that are are
not included in the sets themselves. Let A =
(a
1
, . . . , a
|?|
) and B = (b
1
, . . . , b
|?|
) be such vec-
tors10, the widely used cosine similarity is:
cos(A,B) =
?
|?|
i=1
a
i
b
i
?
?
|?|
i=1
a
2
i
?
?
|?|
i=1
b
2
i
Traditionally, TF-IDF weights are used in
vectors (Term Frequency-Inverse Document Fre-
quency). In the NE case, this value represents the
importance each feature w (e.g. word) has for an
entity x belonging to the set E of entities:
tf(w, x) =
n
w,x
?
w
?
??
n
w
?
,x
, idf(w) = log
|E|
|{x ? E|w ? x}|
,
tfidf(w, x) = tf(w, x) ? idf(w).
with n
w,x
the number of times w appears in x.
Thus the similarity score is CosTFIDF(x, y) =
Cos(A,B), where each a
i
(resp. b
i
) in A (resp. in
B) is tfidf(w
i
, x) (resp. tfidf(w
i
, y)).
8In the remaining the term n-grams is always used for
characters n-grams.
9
|E| denotes the number of elements in E.
10
? is the vocabulary, containing all possible features.
594
2.2 Special measures for NE matching
Experiments show that sequential character-based
measures catch mainly coreferent pairs of long NE
that differ only by a few characters. Bag-of-words
methods suit better to the NE matching problem,
since they are more flexible about word order and
position. But a lot of coreferent pairs can not be
identified by such measures, because of small dif-
ferences between words: for example, ?Director
ElBaradei? and ?Director-General ElBareidi? is
out of reach for such methods. That is why ?sec-
ond level? measures are relevant: their principle is
to apply a sub-measure sim? to all pairs of words
between the two NE and to compute a final score
based on these values. This approach is possible
because NE generally contain only a few words.
Monge-Elkan measure belongs to this category:
it simply computes the average of the better pairs
of words according to the sub-measure:
sim(x, y) =
1
n
n
?
i=1
m
max
j=1
(sim
?
(x
i
, y
j
)).
But experiments show that Monge-Elkan does
not perform well. Actually, its very simple behav-
ior favors too much short entities, because averag-
ing penalizes a lot every non-matching word.
A more elaborated measure is proposed in (Co-
hen et al, 2003): Soft-TFIDF is intended precisely
to take advantage of the good results obtained with
Cosine/TFIDF, without automatically discarding
words which are not strictly identical. The original
definition is the following: let CLOSE(?,X, Y )
be the set of words w ? X such that there ex-
ists a word v ? Y such that sim?(w, v) > ?. Let
N(w, Y ) = max({sim
?
(w, v)|v ? Y }). For any
w ? CLOSE(?,X, Y ), let
S
w,X,Y
= weight(w,X) ? weight(w, Y ) ?N(w, Y ),
where weight(w,Z) = tfidf(w,Z)?
?
w?Z
tfidf(w,Z)
2
.
Finally,
SoftTFIDF(X,Y ) =
?
w?CLOSE(?,X,Y )
S
w,X,Y
.
This definition is not entirely correct, be-
cause weight(w, Y ) = 0 if w /? Y (in other
words, w must appear in both X and Y , thus
SoftTFIDF(X,Y ) would always be equal to
CosTFIDF(X,Y )). We propose instead the fol-
lowing corrected definition, which corresponds to
the implementation the authors provided in the
package SecondString11:
11http://secondstring.sourceforge.net
Let CLOSEST(?,w,Z) = {v ? Z | ?v? ? Z :
sim
?
(w, v) ? sim
?
(w, v
?
) ? sim
?
(w, v) > ?}.
SoftTFIDF(X,Y ) =
?
w?X
weight(w,X) ? ?
w,Y
,
where ?
w,Z
= 0 if CLOSEST(?,w,Z) = ?, and
?
w,Z
= weight(w
?
, Z) ? sim
?
(w,w
?
) otherwise,
with12 w? ? CLOSEST(?,w,Z).
As one may see, SoftTFIDF relies on the same
principle than Monge-Elkan: for each word x
i
in the first entity, find a word y
j
in the second
one that maximizes sim?(x
i
, y
j
). Therefore, these
measures have both the drawback not to be sym-
metric. Furthermore, there is another theoretical
pitfall with SoftTFIDF: in Monge-Elkan, the fi-
nal score is simply normalized in [0, 1] using the
average among words of the first entity. Accord-
ing to the principle of the Cosine angle of TF-
IDF-weighted vectors, SoftTFIDF uses both vec-
tors norms. However the way words are ?approx-
imately matched? does not forbid the matching of
a given word in the second entity twice: in this
case, normalization is wrong because this word is
counted only once in the norm of the second vec-
tor. Consequently there is a potential overflow: ac-
tually it is not hard to find simple examples where
the final score is greater than 1, even if this case is
unlikely with real NE and a high threshold ?.
3 Generalizing Soft-TFIDF
3.1 A unifying framework for similarity
measures
We propose to formalize similarity measures in the
generic model below. This model is intended to
define, compare and possibly mix different kinds
of measures. The underlying idea is simply that
most measures may be viewed as a process follow-
ing different steps: representation as a sequence of
features13 (e.g. tokenization), alignment and a way
to compute the final score. We propose to define a
similarity measure sim through these three steps,
each of them is modeled as a function14:
Representation. Given a set F of features, let
features(e) = ?a
1
, . . . , a
n
? be a function that as-
12If |CLOSEST(?, w, Z)| > 1, pick any such w? in the
set. In the case of matching words between NE, this should
almost never happen.
13We use the word feature for the sake of generality.
14Of course, alternative definitions may be relevant. In par-
ticular one may wish to allow the alignment function to return
a set of graphs instead of only one. In the same way, one may
wish to add a special vertex ? to the graph, in order to repre-
sent the fact that a feature is not matched by adding an edge
between this feature and ?.
595
signs an (ordered) sequence of features to any en-
tity e (a
i
? F for any i). Features may be of any
kind (e.g. characters, words, n-grams, or even con-
textual elements of the entity) ;
Alignment. Given a function simF : F 2 7? R
which defines similarity between any pair of fea-
tures, let algn(?a
1
, . . . , a
n
?, ?a
?
1
, . . . , a
?
n
?
?) = G
be a function which assigns a graph G to any pair
of features sequences. G = (V,E) is a bipartite
weighted graph where:
? The set of vertices is V = A ? A?, where
A and A? are the partitions defined as A =
{v
1
, . . . , v
n
} and A? = {v?
1
, . . . , v
?
n
?
}. Each
v
i
(resp. v?
i
) represents (the position of) the
corresponding feature a
i
(resp. a?
i
) ;
? The set of weighted edges is E =
{(v
i
j
, v
?
i
?
j
, s
j
)}
1?j?|E|
, where v
i
j
? A,
v
?
i
?
j
? A
?
. Weights s
j
generally depend on
sim
F
(a
i
j
, a
?
i
?
j
).
Scoring. Finally sim = score(G), where score
assigns a real value (possibly normalized in [0, 1])
to the alignment G.
The representation step is not particularly origi-
nal, since different kinds of representation have al-
ready been used both with sequential methods and
?bag-of-features? methods. However our model
also entails an alignment step, which does not exist
with bag-of-features methods. Actually, the align-
ment is implicit with such methods, and we will
show that making it visible is essential in the case
of NE matching.
In the remaining of this paper we will only con-
sider normalized metrics (scores belong to [0, 1]).
3.2 Revisiting classical similarity measures
Measures presented in section 2 may be defined
within the model presented above. This mod-
elization is only intended to provide a theoretical
viewpoint on the measures: for all practical pur-
poses, standard implementations are clearly more
efficient. Below we do not detail the represen-
tation step, because there is no difficulty with it,
and also because it is interesting to consider that
any measure may be used with different kinds
of features, as we will show in the next section.
Let S = ?a
1
, . . . , a
n
? = features(e) and S? =
?a
?
1
, . . . , a
?
n
?
? = features(e
?
) for any pair of enti-
ties (e, e?).
3.2.1 Levenshtein-like similarity
The function align
lev
(S, S
?
) is defined in the
following way: let G
lev
be the set of all graphs
G = (V,E) such that any pair of edges
(v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E satisfies (i
j
<
i
k
? i
?
j
< i
?
k
) ? (i
j
> i
k
? i
?
j
> i
?
k
). This
constraint ensures that the sequential order of fea-
tures is respected15 , and that no feature may be
matched twice. In the simplest form of Leven-
shtein16, simF (a, b) = 1 if a = b and 0 otherwise:
for any (v
i
j
, v
?
i
?
j
, s
j
) ? E, s
j
= sim
F
(a
i
j
, a
?
i
?
j
).
Let
sim(G) = M ?n
g
?cost
g
?|E|+
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
,
where M = max(n, n?) and n
g
is the number of
vertices that are not connected (i.e. the number of
inserted or deleted words). cost
g
= 1 in the simple
Levenshtein form, but may be a parameter in the
Needleman-Wunch variant (gap cost). In brief, the
principle in this definition is to count the positions
where no edit operation is needed: thus maximiz-
ing sim(G) is equivalent to minimizing the cost of
an alignment:
align
lev
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?)|G? ? G
lev
}).
Finally, the function score
lev
is simply defined
as score
lev
(G) = sim(G)/max(n, n
?
). It is not
hard to see that this definition is equivalent to the
usual one (see section 2): basically, the graph rep-
resents the concept called trace in (Wagner and
Fischer, 1974), except that the cost function is ?re-
versed? to become a similarity function.
Figure 1: Example of Levenshtein alignment
k
i
t
t
s
i
t
t
i
n
g
e
n
0
1
1
1
0
1
Suppose cost
g
= 1:
sim(G) = M ?n
g
?|E|+
?
e
j
?E
s
j
sim(G) = 7 ? 1 ? 6 + 4
sim(G) = 4
score
lev
(G) = 4/7.
3.2.2 Bag of features
For all simple measures using only sets of fea-
tures, the function align
bag
(S, S
?
) is defined in
the following way: let G be the set of all graphs
15Constraints are a bit more complex for Damerau.
16In the Needleman-Wunch variant, simF should depend
on the cost function, e.g.: simF (a, b) = 1? cost(a, b).
596
G = (V,E) such that if (v
i
j
, v
?
i
?
j
, s
j
) ? E then
a
i
j
= a
?
i
?
j
(equivalently simF (a
i
j
, a
?
i
?
j
) = 1). Now
let once(G) be the set of all G ? G such that
any pair of edges (v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E
satisfies i
j
6= i
k
? i
?
j
6= i
?
k
(at most one match
for each feature), and a
i
j
6= a
i
k
(a feature oc-
curring several times is matched only once). Let
sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
for any G = (V,E).
align
bag
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?) |G? ? once(G)}).
Since all weights are equal to 1, one may show
that sim(G) = |S ? S?| for any G ? once(G).
Thus the score function is simply used for nor-
malization, depending on the given measure: for
example, score
overlap
(G) =
sim(G)
min(n, n
?
)
.
3.2.3 Soft-TFIDF
The case of Cosine measure with TFIDF
weighted vectors is a bit different. Here we define
the SoftTFIDF version: let algn
soft
(S, S
?
) be the
graph G = (V,E) defined as17 (v
i
j
, v
?
i
?
j
, s
j
) ? E if
and only if a?
i
?
j
= select(CLOSEST(?, a
i
j
, S
?
)),
where CLOSEST is the function defined in sec-
tion 2 and select(E) is a function returning the
first element in E if |E| > 0, and is undefined
otherwise18. For any such edge, the weight s
j
is
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Once again, let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
.
score
soft
(G) = sim(G)/(?S|| ? ?S
?
?), where
??a
1
, . . . , a
n
?|| =
?
?
?
?
n
?
i=1
(
idf(a
i
)
n
)
2
.
Although it is not explicitly used in this defini-
tion, term frequency is taken into account through
the number of edges: suppose a given term t ap-
pears m times in S and m? times in S?, all m ver-
tices corresponding to t in A (the partition repre-
senting S) will be connected to all m? vertices cor-
responding to t in A?. Thus there will be m ? m?
edges, which is exactly the unnormalized product
17In the simple case of CosTFIDF, the condition would be:
(v
i
j
, v
?
i
?
j
, s
j
) ? E if and only if a
i
j
= a
?
i
?
j
. In other words,
all identical features (and only they) are connected.
18
?the first element? means that select(E) may return any
e ? E, provided the same element is always returned for the
same set.
of term frequencies tf(t, S) ? tf(t, S?) ?n ?n?. Thus
summing m ? m? times idf(t)/n ? idf(t)/n? in
sim(G) is equal to tfidf(t, S) ? tfidf(t, S?) (nor-
malization is computed in the same way).
3.3 Meta-Levenshtein: Soft-TFIDF with
Levenshtein alignment
We have shown in part 2.2 that there are some
pitfalls in Soft-TFIDF, especially in the way the
alignment is computed: no symmetry, possible
score overflow. But experiments show that tak-
ing words IDF into account increases performance,
and that Soft-TFIDF, i.e. the possible matching
of words that are not strictly identical, increases
performance (see section 4). That is why improv-
ing this kind of measure is interesting. Follow-
ing the model we proposed above, we propose to
mix the cosine-like similarity used in Soft-TFIDF
with a Levenshtein-like alignment. The following
measure, called Meta-Levenshtein (ML for short),
takes IDFs into account but is not a bag-of-features
metrics.
Let us define align
ML
in the following way: let
G
ML
be defined exactly as the set of graphs G
lev
(see part 3.2.1), except that weights are defined as
in the case of Soft-TFIDF: for any G = (V,E) ?
G
lev
and for any edge (v
i
j
, v
?
i
?
j
, s
j
) ? E, let
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
, and
align
ML
(S, S
?
) = G, where G is such that
sim(G) = max({sim(G
?
) |G
?
? G
ML
}). Finally,
score
ML
(G) = sim(G)/(?S|| ? ?S
?
?).
Compared to Soft-TFIDF, ML solves the prob-
lem of symmetry (ML(S, S?) = ML(S?, S)), and
also the potential overflow, because no feature may
be matched twice (see fig. 2). Of course, the align-
ment is less flexible in ML, since it must satisfy the
sequential order of features. Practically, this mea-
sure may be efficiently implemented in the same
way as Levenshtein similarity, including option-
ally the Damerau extension for transpositions. We
have also tested a simple variant with possible ex-
tended transpositions, i.e. cases like ABC com-
pared to CA, where both C and A are matched.
3.4 Recursive combinations for NE matching
One of the points we want to emphasize through
the generic framework presented above is the mod-
597
Figure 2: Soft-TFIDF vs. ML alignment
With sim(A,D) ? ?, and sim(C,E) ? sim(B,E) ? ?:
A
C
B
A
D
E
F
Soft-TFIDF
A
C
B
A
D
E
F
ML
ularity of similarity measures. Our viewpoint is
that traditional measures may be seen not only in
their original context, but also as modular param-
eterized functions. The first application of such a
definition is already in use in the form of measures
like Monge-Elkan or Soft-TFIDF, which rely on
some sub-measure to compare words inside NEs.
But we will show that modularity is also useful
at a lower level: measures concerning words may
rely on similarity between (for example) n-grams,
and even at this restricted level numerous possible
kinds of similarity may be used.
Moreover, from the viewpoint of applications it
is not very costly to compute similarities between
n-grams and even between words. The number
of n-grams is clearly bounded, and the number of
words is not so high because there are only about 2
words by entity in average, and overall some words
appear very often in entities19.
4 Experiments
4.1 Data
Two corpora were used. Both contain mainly news
and press articles, collected from various interna-
tional sources. The first one, called ?Iran Nu-
clear Threat? (INT in short), is in English and
was extracted from the NTI (Nuclear Threat Ini-
tiative) web site20. It is 236,000 words long. Our
second corpus, called ?French Speaking Medias?
(FSM in short), is 856,000 words long. It was ex-
tracted from a regular crawling of a set of French-
speaking international newspapers web sites dur-
ing a short time-frame (in July 2007). GATE21
was used as the named entities recognizer for INT,
whereas Arisem22 performed the tagging of NEs
19In the corpora we studied, 1172 NE (resp. 2533) contain
1107 distinct words (resp. 2785).
20http://www.nti.org
21http://gate.ac.uk
22http://www.arisem.com
for FSM. Recognition errors23 appear in both cor-
pora, but significantly less in FSM. We restricted
the sets of NEs to those recognized as locations,
organizations and persons, and decided to work
only on entities appearing at least twice. Finally
for INT (resp. FSM) we obtain 1,588 distinct
NE (resp. 3,278) accounting altogether for 33,147
(resp. 23,725) occurrences.
Of course, it would be too costly to manually
label as match (positive) or non-match (negative)
the whole set containing n ? (n ? 1)/2 pairs, for
the observed values of n. The approach consist-
ing in labeling only a randomly chosen subset of
pairs is ineffective, because of the disproportion
between the number of negative and positive pairs
(less than 0.1%). Therefore we tried to find all pos-
itive pairs, assuming the remaining lot are nega-
tive. Practically, the labeling step was based only
on the best pairs as identified by a large set of
measures24. The guidelines we used for labeling
are the following: any incomplete, over-tagged or
simply wrongly recognized NE is discarded. Then
remaining pairs are classified as positive (corefer-
ent), negative (non-coreferent), or ?don?t know?25.
Corpus Discarded Pos. Neg. Don?t know
INT 416 / 1,588 764 2,821 302
FSM 745 / 3,278 741 32,348 419
According to our initial hypotheses, all non-
tagged pairs are considered as negative in the ex-
periments below. ?Don?t know? pairs are ignored.
As a further note, about 20% of the pairs are not
orthographically similar (e.g. acronyms and their
expansion): these pairs are out of reach of our tech-
niques, and would require additional knowledge.
4.2 Observations
4.2.1 Taking IDF into account
To evaluate the contribution of IDF26 in scor-
ing the coreference degree between NE, let us ob-
23Mainly truncated entities, over-tagged entities, and com-
mon nouns beginning with a capital letter.
24This is a potential methodological bias, but we hope to
have kept its effect as low as possible: the measures we used
are quite diverse and do not assign good scores to the same
pairs; therefore, for each measure, we expect that the poten-
tial misses (false negatives) will be matched by some other
measure, thus allowing a fair evaluation of its performance.
A few positive pairs are manually added (mainly acronyms).
25All ambiguous cases, mainly due to some missing preci-
sion (e.g. ?Ministry of Foreign Affairs? and ?Russian Min-
istry of Foreign Affairs?), and more rarely homonymy (e.g.
?Lebedev? and ?[Valery|Oleg] Lebedev?)
26It may be noticed that the Term Frequency in TFIDF is
rarely important, since a given word appear almost always
only once in a NE.
598
serve the differences among best scored pairs for
measures Bag-of-words Cosine and Cosine over
TFIDF weighted vectors. For example, the for-
mer will assign 0.5 to pair ?Prime Minister Tony
Blair?/?Blair? (from corpus INT), whereas the
latter gives 0.61. As expected, IDF weights lighten
the effect of non-informative words and strengthen
important words. In both corpora, The F1-measure
for TFIDF Cosine is about 10 points (in average)
better than for Bag-of-words Cosine (see fig. 3).
4.2.2 Soft-TFIDF problems: normalization,
threshold and sub-measure
As we have explained in section 2.2, the Soft-
TFIDF measure (Cohen et al, 2003) may suffer
from normalization problems. This is probably
the reason why the authors seem to use it parsi-
moniously, i.e. only in the case words are very
close (which is verified using a high threshold
?). Indeed, problems occur when the sub-measure
and/or the threshold are not carefully chosen, caus-
ing performances drop: using Jaro measure with
a very low threshold (0.2 here), performances
are even worst than Bag-of-words cosine (see fig.
3). This is due to the double matching problem:
for example, pair ?Tehran Times (Tehran)?/?Inter
Press Service? (from INT) is scored more than 1.0
because ?Tehran? matches ?Inter? twice: even
with a low score as a coefficient, ?Inter? has a
high IDF compared to ?Press? and ?Service?, so
counting it twice makes normalization wrong.
However, this problem may be solved by choos-
ing a more adequate sub-measure: experiments
show that using the CosTFIDF measure with bi-
grams or trigrams outperforms standard CosT-
FIDF. Of course, there are some positive pairs
that are found ?later? by Soft-TFIDF, since it may
only increase score. But the ?soft? comparison
brings back to the top ranked pairs a lot of positive
ones. In both corpora, the best sub-measure found
is CosTFIDF with trigrams. ?Mohamed ElBa-
radei?/?Director Mohammad ElBaradei? (INT)
or ?Chine?/?China? (FSM) are typical positive
pairs found by this measure but not by standard
CosTFIDF. Here no threshold is needed anymore
because the sub-measure has been chosen with
care, depending on the data, in order to avoid the
normalization problem. This is clearly a drawback
for Soft-TFIDF: it may perform well, but only with
hand-tuning sub-measure and/or threshold.
4.2.3 Beyond Soft-TFIDF: (recursive) ML
In the FSM corpus, replacing Soft-TFIDF with
(simple) Meta-Levenshtein at the word level does
not decrease performance, even though the align-
ment is more constrained in the latter case. Us-
ing the same sub-measure to compare words (tri-
grams CosTFIDF), it does neither increase perfor-
mance. A few positive pairs are missed in the INT
corpus, due to the more flexible word order in En-
glish: ?U.S. State Department?/?US Department
of State? is such an example (12 among 764 are
concerned). This problem is easily solved with the
ML variant with extended transposition (see part
3.3): in both corpora, there are no positive pairs
requiring more than a gap of one word in the align-
ment. Thus this measure is not only performant but
also robust, since it does not need any hand-tuning.
As a second step, we want to improve results
by selecting a more fine-grained sub-measure. We
have tried several ideas, such as using different
kinds of n-grams similarity inside the words sim-
ilarity measure. Firstly, trigrams performed bet-
ter than bigrams or simple characters. Secondly,
the best trigrams similarity method found is actu-
ally very simple: it consists in using CosTFIDF
computed on the trigrams contexts, i.e. the set of
closest27 trigrams of all occurrences of the given
trigram. Unsurprisingly, good scores are generally
obtained for pairs of trigrams that have common
characters. But it seems that this approach also
enhances robustness, because it finds similarities
between ?close characters?: in the French corpus,
one observes quite good scores between trigrams
containing an accentuated version and the non ac-
centuated version of the same character. Further-
more, some character encoding errors are some-
how corrected this way28. This is possibly the rea-
son why the improvement of results is better in
FSM than in INT (see table 1).
Finally, using also ML to compute similarity
between words29 yields the best results. This
means that compared to the simple CosTFIDF sub-
measure, one does not compare bags of trigrams
but ordered sequences of trigrams30.
27We have tried different window sizes for such contexts,
from 2 to 10 trigrams long: performances were approximately
the same. We only consider trigrams found in the entities.
28For example, the ?? in the name ?Lugovo??? appears also in
FSM as i, as y, as a`, and is sometimes deleted.
29i.e. not only between sequences of words: in this case
ML is run between trigrams at the word level, and then an-
other time between words at the NE level.
30It is hard to tell whether it is the sequential alignment or
599
Figure 3: F1-Measures for FSM (percentages)
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
F1
-M
ea
su
re
n best scored pairs (considered as positive)
Bag of words Cosine
Cosine TFIDF (words)
Soft-TFIDF (Jaro)
Soft-TFIDF (TFIDF 3g)
ML (ML/contexts 3g)
Example: for Cosine TFIDF with words, if the threshold is
set in such a way that (only) the 1000 top ranked pairs are
classified as positive, then the F1-measure is around 60%.
Table 1: Best F1-measures (percentages)
INT FSM
Measure F1 P R F1 P R
Cosine 51.6 63.2 43.6 59.5 76.2 48.7
CosTFIDF 62.6 71.7 55.6 69.9 84.2 59.8
Soft TFIDF/3g 68.6 74.2 63.9 73.1 79.8 67.6
ML/ML-context 70.6 72.6 68.7 77.0 82.5 72.2
P/R: Corresponding Precision/Recall.
4.3 Global results
Results are synthesized in table 1, which is based
on the maximum F1-measure for each measure.
One observes that F1-measure is 3 to 6 points bet-
ter for Soft-TFIDF than for standard TF-IDF, and
that our measure still increases F1-measure by 2
(INT) to 4 points (FSM). Results show that its
contribution consists mainly in improving the re-
call, which means that our measure is able to catch
more positive pairs than Soft-TFIDF: for exam-
ple, the pair ?Fatah Al Islam?/ ?Fateh el-Islam?
(FSM) is scored 0.54 by SoftTFIDF and 0.70 by
ML. Our measure remains the best for all values of
n in fig. 3, and results are similar for F0.5-measure
and F2-measure: thus, irrespective of specific ap-
plication needs which may favor precision or re-
call, ML seems preferable.
5 Conclusion
In conclusion, we have proposed a generic model
to show that similarity measures may be combined
in numerous ways. We have tested such a combi-
nation, based on Soft-TFIDF, which performs bet-
the ?right? use of the trigrams sub-measure which is responsi-
ble for the improvement, since the only possible comparison
at this level is Soft-TFIDF.
ter than all existing similarity metrics on two cor-
pora. Our measure is robust, since it does not rely
on any kind of prior knowledge. Thus it may be
easily used, in particular in applications where NE
matching is useful but is not the essential task.
Acknowledgements
This work has been funded by the National Project
Cap Digital - Infom@gic. We thank Lo??s Rigouste
(Pertimm) and Nicolas Dessaigne and Aure?lie Mi-
geotte (Arisem) for providing us with the anno-
tated French corpus.
References
Bilenko, Mikhail, Raymond J. Mooney, William W.
Cohen, Pradeep Ravikumar, and Stephen E. Fien-
berg. 2003. Adaptive name matching in information
integration. IEEE Intelligent Systems, 18(5):16?23.
Christen, Peter. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Department of Computer
Science, The Australian National University, Can-
berra 0200 ACT, Australia, September.
Cohen, William W., Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks. In
Kambhampati, Subbarao and Craig A. Knoblock,
editors, Proceedings of IJCAI-03 Workshop on
Information Integration on the Web (IIWeb-03),
August 9-10, 2003, Acapulco, Mexico, pages 73?78.
Freeman, Andrew, Sherri L. Condon, and Christopher
Ackerman. 2006. Cross linguistic name matching
in English and Arabic. In Moore, Robert C., Jeff A.
Bilmes, Jennifer Chu-Carroll, and Mark Sanderson,
editors, Proc. HLT-NAACL.
Navarro, Gonzalo. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88.
Pouliquen, Bruno, Ralf Steinberger, Camelia Ignat,
Irina Temnikova, Anna Widiger, Wajdi Zaghouani,
and Jan Zizka. 2006. Multilingual person name
recognition and transliteration. CORELA - Cogni-
tion, Representation, Langage.
Prager, John, Sarah Luger, and Jennifer Chu-Carroll.
2007. Type nanotheories: a framework for term
comparison. In Proceedings of CIKM ?07, pages
701?710, New York, NY, USA. ACM.
Wagner, R. and M. Fischer. 1974. The string-to-string
correction problem. JACM, 21(1):168?173.
Winkler, W. E. 1999. The state of record linkage
and current research problems. Technical Report
RR99/04, US Bureau of the Census.
600
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 232?240,
Beijing, August 2010
Local lexical adaptation in Machine Translation through triangulation:
SMT helping SMT
Josep Maria Crego
LIMSI-CNRS
jmcrego@limsi.fr
Aur?lien Max
LIMSI-CNRS
Univ. Paris Sud
amax@limsi.fr
Fran?ois Yvon
LIMSI-CNRS
Univ. Paris Sud
yvon@limsi.fr
Abstract
We present a framework where auxiliary
MT systems are used to provide lexical
predictions to a main SMT system. In
this work, predictions are obtained by
means of pivoting via auxiliary languages,
and introduced into the main SMT sys-
tem in the form of a low order language
model, which is estimated on a sentence-
by-sentence basis. The linear combination
of models implemented by the decoder
is thus extended with this additional lan-
guage model. Experiments are carried out
over three different translation tasks using
the European Parliament corpus. For each
task, nine additional languages are used
as auxiliary languages to obtain the trian-
gulated predictions. Translation accuracy
results show that improvements in trans-
lation quality are obtained, even for large
data conditions.
1 Introduction
Important improvements are yet to come regard-
ing the performance of Statistical Machine Trans-
lation systems. Dependence on training data and
limited modelling expressiveness are the focus of
many research efforts, such as using monolingual
corpora for the former and syntactic models for
the latter.
Another promising approach consists in ex-
ploiting complementary sources of information
in order to build better translations, as done by
consensus-based system combination (e.g. (Ma-
tusov et al, 2008)). This, however, requires to
have several systems available for the same lan-
guage pair. Considering that the same training
data would be available to all systems, differences
in translation modelling are expected to produce
redundant and complementary hypotheses. Mul-
tisource translation (e.g. (Och and Ney, 2001;
Schwartz, 2008)) is a variant, involving source
texts available in several languages which can be
translated by systems for different language pairs
and whose outputs can be successfully combined
into better translations (Schroeder et al, 2009).
One theoretical expectation of multisource trans-
lation is that it can successfully reduce ambiguity
of the original source text, but does so under the
rare conditions of availability of existing (accu-
rate) translations. In contrast, pivot-based system
combination (e.g. (Utiyama and Isahara, 2007;
Wu and Wang, 2007)) aims at compensating the
lack of training data for a given language pair by
producing translation hypotheses obtained by piv-
oting via an intermediary language for which bet-
ter systems are available.
These techniques generally produce a search
space that differs from that of the direct transla-
tion systems. As such, they create a new transla-
tion system out of various systems for which di-
agnosis becomes more difficult.
This paper instead focusses on improving a sin-
gle system, which should be state-of-the-art as
regards data and models. We propose a frame-
work in which information coming from external
sources is used to boost lexical choices and guide
the decoder into making more informed choices.1
1We performed initial experiments where the comple-
mentary information was exploited during n-best list rerank-
ing (Max et al, 2010), but except for the multisource condi-
tion the list of hypotheses contained too little useful variation
232
Complementary sources can be of different na-
ture: they can involve other automatic systems
(for the same or different language pairs) and/or
human knowledge. Furthermore, complementary
information is injected at the lexical level, thus
making targeted fine-grained lexical predictions
useful. Importantly, those predictions are ex-
ploited at the sentence level2, so as to allow for
efficient use of source contextual information.
The second contribution of this paper is an in-
stantiation of the proposed framework. Auto-
matically pivoting via auxiliary languages is used
to make complementary predictions that are ex-
ploited through language model adaptation by the
decoder for a given language pair. For this appar-
ently difficult condition, where predictions result
from automatic translations involving two sys-
tems, we manage to report significant improve-
ments, measured with respect to the target and the
source text, under various configurations.
This paper is organized as follows. We first re-
view related work in section 2.1, and describe the
distinctive characteristics of our approach in Sec-
tion 2.2. Section 2.3 presents our instantiation of
the framework based on lexical boosting via aux-
iliary language triangulation. Experiments involv-
ing three language pairs of various complexity and
different amounts of training data are described in
Section 3. We finally conclude by discussing the
prospects offered by our proposed framework in
Section 4.
2 A framework for sentence-level lexical
boosting
2.1 Related work
The idea of using more than one translation sys-
tem to improve translation performance is not new
and has been implemented in many different ways
which we briefly review here.
System combination An often used strategy
consists in combining the output of several sys-
tems for a fixed language pair, and to rescore the
resulting set of hypotheses taking into account
all the available translations and scores. Various
to lead to measurable improvements.
2We plan to experiment next on using predictions at the
document level.
proposals have been made to efficiently perform
such a combination, using auxiliary data struc-
tures such as n-best lists, word lattices or con-
sensus networks (see for instance (Kumar and
Byrne, 2004; Rosti et al, 2007; Matusov et al,
2008; Hildebrand and Vogel, 2008; Tromble et al,
2008)). Theses techniques have proven extremely
effective and have allowed to deliver very signifi-
cant gains in several recent evaluation campaigns
(Callison-Burch et al, 2008).
Multisource translation A related, yet more re-
sourceful approach, consists in trying to combine
several systems providing translations from differ-
ent sources into the same target, provided such
multilingual sources are available. (Och and Ney,
2001) propose to select the most promising trans-
lation amongst the hypotheses produced by sev-
eral Foreign?English systems, where output se-
lection is based on the translation scores. The
intuition that if a system assigns a high figure
of merits to the translation of a particular sen-
tence, then this translation should be preferred,
is implemented in the MAX combination heuris-
tics, whose relative (lack of) success is discussed
in (Schwartz, 2008). A similar idea is explored in
(Nomoto, 2004), where the sole target language
model score is used to rank competing outputs.
(Schroeder et al, 2009) propose to combine the
available sources prior to translation, under the
form of a multilingual lattice, which is decoded
with a multisource phrase table. (Chen et al,
2008) integrate the available auxiliary information
in a different manner, and discuss how to improve
the translation model of the primary system: the
idea is to use the entries in the phrase table of
the auxiliary system to filter out those acciden-
tal correspondences that pollute the main transla-
tion model. The most effective implementation of
multisource translation to date however consists
in using mono-source system combination tech-
niques (Schroeder et al, 2009).
Translation through pivoting The use of aux-
iliary systems has also been proposed in another
common situation, as a possible remedy to the
lack of parallel data for a particular language pair,
or for a particular domain. Assume, for instance,
that one wishes to build a translation system for
233
the pair A ? B, for which the parallel data
is sparse; assuming further that such parallel re-
sources exist for pairs A ? C and for C ? B,
it is then tempting to perform the translation in-
directly through pivoting, by first translating from
A to C, then from C to B. Direct implementa-
tions of this idea are discussed e.g. in (Utiyama
and Isahara, 2007). Pivoting can also intervene
earlier in the process, for instance as a means
to automatically generate the missing parallel re-
source, an idea that has also been considered to
adapt an existing translation systems to new do-
mains (Bertoldi and Federico, 2009). Pivoting can
finally be used to fix or improve the translation
model: (Cohn and Lapata, 2007) augments the
phrase table for a baseline bilingual system with
supplementary phrases obtained by pivoting into
a third language.
Triangulation in translation Triangulation
techniques are somewhat more general and only
require the availabily of one auxiliary system (or
one auxiliary parallel corpus). For instance, the
authors of (Chen et al, 2008) propose to use the
translation model of an auxiliary C ? B system
to filter-out the phrase-table of a primary A ? B
system.
2.2 Our framework
As in other works, we propose to make use of sev-
eral MT systems (of any type) to improve trans-
lation performance, but contrarily to these works
we concentrate on improving one particular sys-
tem. Our framework is illustrated on Figure 1.
The main system (henceforth, direct system), cor-
responding to configuration 1, is a SMT system,
translating from German to English in the exam-
ple. Auxiliary information may originate from
various sources (2-6) and enter into the decoder.
A new model is dynamically built and is used to
guide the exploration of the search space to the
best hypothesis. Several auxiliary models can be
used at once and can be weighted by standard op-
timization techniques using development data, so
that bad sources are not used in practice, or by
exploiting a priori information. In the implemen-
tation described in section 2.3, this information is
updated by the auxiliary source at each sentence.
Figure 1: Lexical boosting framework with vari-
ous configurations for auxiliary predictions
We now briefly describe various possible con-
figurations to make some links to previous works
explicit. Configuration 2 translates the same
source text by means of another system for the
same language pair, as would be done in system
combination, except that here a new complete de-
coding is performed by the direct system. Con-
figuration 3, which will be detailed in section 2.3,
uses translations obtained by triangulating via an
auxiliary language (Spanish in the example). Us-
ing this two-step translation is common to pivot
approaches, but our approach is different in that
the result of the triangulation is only used as aux-
iliary information for the decoding of the direct
system. Configurations 4 and 5 are instances of
multisource translation, where a paraphrase or a
translation of the source text is available. Lastly,
configuration 6 illustrates the case where a human
translator, with knowledge of the target language
and at least of one of the available source lan-
guages, could influence the decoding by provid-
ing desired3 words (e.g. only for source words or
phrases that would be judged difficult to translate).
This human supervision through a feedback text in
real time is similar to the proposal of (Dymetman
et al, 2003).
Given this framework, several questions arise,
3The proposal as it is limits the hypotheses produced by
the system to those that are attainable given its training data.
It is conceivable, however, to find ways of introducing new
knowledge in this framework.
234
the most important underlying this work being
whether the performance of SMT systems can be
improved by using other SMT systems. Another
point of interest is whether improvements made
to auxiliary systems can yield improvement to the
direct system, without the latter undergoing any
modification.
2.3 Lexical boosting via triangulation
Auxiliary translations obtained by pivoting can be
viewed as a source of adaptation data for the target
language model of the direct system. Assuming
we have computed n-best translation hypotheses
of a sentence in the target language, we can then
boost the likeliness of the words and phrases oc-
curring in these hypotheses by deriving an auxil-
iary language model for each test sentence. This
allows us to integrate this auxiliary information
during the search and thus provides a tighter in-
tegration with the direct system. This idea has
successfully been used in speech recognition, us-
ing for instance close captions (Placeway and Laf-
ferty, 1996) or an imperfect translation (Paulik et
al., 2005) to provide auxiliary in-domain adap-
tation data for the recognizer?s language model.
(Simard and Isabelle, 2009) proposed a similar ap-
proach in Machine Translation in which they use
the target-side of an exact match in a translation
memory to build language models on a per sen-
tence basis used in their decoder.
This strategy can be implemented in a straight-
forward manner, by simply training a language
model using the n-best list as an adaptation cor-
pus. Being automatically generated, hypotheses
in the n-best list are not entirely reliable: in par-
ticular, they may contain very unlikely target se-
quences at the junction of two segments. It is how-
ever straightforward to filter these out using the
available phrase alignment information.
This configuration is illustrated on Figure 2: the
direct system (configuration 1) makes use of pre-
dictions from pivoting through an auxiliary lan-
guage (configuration 2), where n-best lists can be
used to produce several hypotheses. In order to
get a upper bound on the potential gains of this ap-
proach, we can run the artificial experiment (con-
figuration 3) where a reference in the target lan-
guage is used as a ?perfect? source of information.
Furthermore, we are interested in the performance
of the simple pivot system alone (configuration 4),
as it gives an indication of the quality of the data
used for LM adaptation.
Figure 2: Architecture of a German?English sys-
tem for lexical boosting via triangulation through
Spanish
3 Experiments and results
3.1 Translation engine
In this study, we used our own machine trans-
lation engine, which implements the n-gram-
based approach to statistical machine translation
(Mari?o et al, 2006). The translation model
is implemented as a stochastic finite-state trans-
ducer trained using a n-gram language model of
(source,target) pairs.
In addition to a bilingual n-gram model, our
SMT system uses six additional models which
are linearly combined following a discriminative
modeling framework: two lexicalized reorder-
ing (Tillmann, 2004) models,a target-language
model, two lexicon models, a ?weak? distance-
based distortion model, a word bonus model and
a translation unit bonus model. Coefficients in
this linear combination are tuned over develop-
ment data with the MERT optimization toolkit4,
slightly modified to use our decoder?s n-best lists.
For this study, we used 3-gram bilingual and
3-gram target language models built using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996); model estimation was performed with the
SRI language modeling toolkit.5 Target language
4http://www.statmt.org/moses
5http://wwww.speech.sri.com/projects/
srilm
235
models were trained on the target side of the bi-
text corpora.
After preprocessing the corpora with standard
tokenization tools, word-to-word alignments are
performed in both directions, source-to-target and
target-to-source. In our system implementation,
the GIZA++ toolkit6 is used to compute the word
alignments. Then, the grow-diag-final-and heuris-
tic is used to obtain the final alignments from
which translation units are extracted. Convergent
studies have showed that systems built accord-
ing to these principles typically achieve a per-
formance comparable to that of the widely used
MOSES phrase-based system for the language
pairs under study.
3.2 Corpora
We have used the Europarl corpus7 for our main
and auxiliary languages. The eleven languages
are: Danish (da), German (de), English (en),
Spanish (es), Finnish (fi), French (fr), Greek
(el), Italian (it), Dutch (nl), Portuguese (pt) and
Swedish (sv).
We focussed on three translation tasks: one
for which translation accuracy, as measured by
automatic metrics, is rather high (fr ? en),
and two for which translation accuracy is lower
(de ? en) and (fr ? de). This will allow us
to check whether the improvements provided by
our method carry over even in situations where the
baseline is strong; conversely, it will allow us to
assess whether the proposed techniques are appli-
cable when the baseline is average or poor.
In order to measure the contribution of each of
the auxiliary languages we used a subset of the
training corpus that is common to all language
pairs, hereinafter referred to as the intersection
data condition. We used the English side of all
training language pairs to collect the same sen-
tences in all languages, summing up to 320, 304
sentence pairs. Some statistics on the data used in
this study are reported in Table 1. Finally, in order
to assess the impact of the training data size over
the results obtained, we also considered a much
more challenging condition for the fr ? de pair,
where we used the entire Europarl data (V5) made
6http://www.fjoch.com/GIZA++.html
7http://www.statmt.org/europarl
available for the fifth Workshop on Statistical Ma-
chine Translation8 for training, and test our sys-
tem on out-of-domain news data. The training
corpus in this condition contains 43.6M French
words and 37.2M German words.
Development and test data for the first con-
dition (intersection) were obtained by leaving
out respectively 500 and 1000 sentences from
the common subset (same sentences for all lan-
guages), while the first 500 sentences of news-
test2008 and the entire newstest2009 official test
sets were used for the full data condition.
Train Dev Test
Words Voc. Words Voc. OOV Words Voc. OOV
da 8.5M 133.5k 13.4k 3.2k 104 25.9k 5.1k 226
de 8.5M 145.3k 13.5k 3.5k 120 26.0k 5.5k 245
en 8.9M 53.7k 14.0k 2.8k 39 27.2k 4.0k 63
es 9.3M 85.3k 14.6k 3.3k 56 28.6k 5.0k 88
fi 6.4M 274.9k 10.1k 4.3k 244 19.6k 7.1k 407
fr 10.3M 67.8k 16.1k 3.2k 47 31.5k 4.8k 87
el 8.9M 128.3k 14.1k 3.9k 72 27.2k 6.2k 159
it 9.0M 78.9k 14.3k 3.4k 61 28.1k 5.1k 99
nl 8.9M 105.0k 14.2k 3.1k 76 27.5k 4.8k 162
pt 9.2M 87.3k 14.5k 3.4k 49 28.3k 5.2k 118
sv 8.0M 140.8k 12.7k 3.3k 116 24.5k 5.2k 226
Table 1: Statistics for the training, development
and test sets of the intersection data condition
3.3 Results
In this section, we report on the experiments car-
ried out to assess the benefits of introducing an
auxiliary language model to the linear combina-
tion of models implemented in our SMT system.
Table 2 reports translation accuracy (BLEU) re-
sults for the main translation tasks considered in
this work (fr ? de), (fr ? en) and (de ? en),
as well as for multiple intermediate tasks needed
for pivoting via auxiliary systems.
For each triplet of languages (src, aux, trg),
columns 4th to 6th show BLEU scores for systems
performing (src ? aux), (aux ? trg) and pivot
translations using aux as the bridge language.
The last two columns display BLEU scores for
the main translation tasks (fr ? de), (fr ? en)
and (de? en). Column src-trg refers to the base-
line (direct) systems, for which no additional lan-
8http://www.statmt.org/wmt10
236
src aux trg src-aux aux-trg pivot src-trg +auxLM
Intersection data condition
fr - de - - - 18.02
da 22.78 20.02 16.27 +0.44
el 24.54 18.51 15.86 +0.76
en 29.53 17.31 15.69 +0.50
es 34.94 18.31 16.76 +0.96
fi 10.71 14.15 11.39 +0.65
it 31.60 16.86 16.54 -0.05
nl 22.71 21.44 16.76 +0.55
pt 33.61 17.47 16.34 -0.12
sv 20.73 19.59 13.73 -0.14
average +0.39
- - ref - - - - +6.46
fr - en - - - 29.53
da 22.78 29.54 25.48 +0.02
de 18.02 24.66 23.50 +0.05
el 24.54 29.37 25.31 +0.07
es 34.94 31.05 27.76 +0.61
fi 10.71 20.56 19.15 +0.44
it 31.60 25.75 25.79 +0.32
nl 22.71 24.49 25.15 +0.01
pt 33.61 29.44 27.27 +0.01
sv 20.73 30.98 23.74 +0.50
average +0.22
- - ref - - - - +11.30
de - en - - - 24.66
da 24.59 29.54 22.73 +0.96
el 19.72 29.37 20.88 +1.02
es 25.48 31.05 21.23 +0.77
fi 12.42 20.56 18.02 +0.94
fr 25.93 29.53 21.55 +0.19
it 18.82 25.75 18.05 +0.19
nl 24.97 24.49 22.62 +0.64
pt 23.15 29.44 21.93 +0.87
sv 19.80 30.98 21.35 +0.69
average +0.69
- - ref - - - - +9.53
Full data condition
fr - de - - - 19.94
es 38.76 20.18 19.36 +0.61
Table 2: Translation accuracy (BLEU) results.
guage model is used; column +auxLM refers to
the same system augmented with the additional
language model. Additional language models are
built from hypotheses obtained by means of pivot
translations, using aux as auxiliary language. The
last score is shown in the form of the difference
(improvement) with respect to the score of the
baseline system.
This table additionally displays the BLEU re-
sults obtained when building the additional lan-
guage models directly from the English reference
translations (see last row of each translation task).
These numbers provide an upper-bound of the ex-
pected improvements. Note finally that numbers
in boldface correspond to the best numbers in their
column for a given language pair.
As detailed above, the additional language
models are built using trg hypotheses obtained by
pivoting via an auxiliary language: (src ? aux)
+ (aux ? trg). Hence, column pivot shows the
quality (measured in terms of BLEU) of the hy-
potheses used to estimate the additional model.
Note that we did not limit the language model to
be estimated from the 1-best pivot hypotheses. In-
stead, we uses n-best translation hypotheses of the
(src ? aux) system and m-best hypotheses of
the (aux ? trg) system. Hence, n ? m target
hypotheses were used as training data to estimate
the additional models. Column +auxLM shows
BLEU scores over the test set after performing
four system optimizations on the development set
to select the best combination of values used for n
and m among: (1, 1), (10, 1), (10, 1) and (10, 10).
All hypotheses used to estimate a language model
are considered equally likely. Language models
are learnt using Witten-Bell discounting. Approx-
imately?1.0 point must be added to BLEU scores
shown in the last 2 columns for 95% confidence
levels.
As expected, pivot translations yield lower
quality scores than the corresponding direct trans-
lations hypotheses. However, pivot hypotheses
may contain better lexical predictions, that the ad-
ditional model helps transfer into the baseline sys-
tem, yielding translations with a higher quality, as
shown in many cases the +auxLM systems results.
The case of using Finnish as an auxiliary language
is particularly remarkable. Even though pivot hy-
potheses obtained through Finnish have the low-
est scores9, they help improve the baseline perfor-
mance as additional language models.
As expected, the translation results of the pair
9Given the agglutinative nature of morphological pro-
cesses in Finnish, reflected in a much lower number of words
per sentence, and a higher number of types (see Table 1),
BLEU scores for this language do not compare directly with
the ones obtained for other languages.
237
with a highest baseline (fr ? en) were on av-
erage less improved than those of the pairs with
lower baselines.
As can also be seen, the contribution of each
auxiliary language varies for each of the three
translation tasks. For instance, Danish (da) pro-
vides a clear improvement to (de ? en) transla-
tions, while no gain is observed for (fr ? en).
No clear patterns seems to emerge, though, and
the correlation between the quality of the pivot
translation and the boost provided by using these
pivot hypotheses remains to be better analyzed.
In order to assess whether the improvements
obtained carry over larger data conditions, we
trained our (fr ? de), (fr ? es) and (es? de)
systems over the entire EPPS data. Results are re-
ported in the bottom part of Table 2. As can be
seen, the (fr ? de) system is still improved by
using the additional language model. However,
the absolute value of the gain under the full condi-
tion (+0.61) is lower than that of the intersection
data condition (+0.96).
3.4 Contrastive evaluation of lexical
translation
In some cases, automatic metrics such as BLEU
cannot show significant differences that can be re-
vealed by fine-grained focussed human evaluation
(e.g. (Vilar et al, 2006)). Furthermore, comput-
ing some similarity between a system?s hypothe-
ses and gold standard references puts a strong
focus on the target side of translation, and does
not allow evaluating translation performance from
the source words that were actually translated.
We therefore use the evaluation methodology de-
scribed in (Max et al, 2010) for a complementary
measure of translation performance that focuses
on the contrastive ability of two systems to ade-
quately translate source words.
Source words from the test corpus were first
aligned with target words in the reference, by au-
tomatically aligning the union of the training and
test corpus using GIZA++.10 The test corpus was
analyzed by the TREETAGGER11 so as to identify
10The obtained alignments are thus strongly influenced by
alignments from the training corpus. It could be noted that
alignments could be manually corrected.
11http://www.ims.uni-stuttgart.de/
Source words? part-of-speech
aux ADJ ADV NOM PRO VER all +Bleu
el - 27 21 114 25 99 286 +0.07+ 62 29 136 27 114 368
es - 33 25 106 26 110 300 +0.61+ 64 38 136 22 117 377
fi - 44 40 106 20 92 302 +0.44+ 49 31 120 23 106 329
it - 55 39 128 35 119 376 +0.32+ 55 39 145 36 121 396
sv - 40 30 138 29 109 346 +0.50+ 69 46 144 23 134 416
Table 3: Contrastive lexical evaluation re-
sults per part-of-speech between the baseline
French?English system and our systems using
various auxiliary languages. ?-? (resp. ?+?) val-
ues indicate numbers of words that only the base-
line system (resp. our system) correctly translated
with respect to the reference translation.
content words, which have a more direct impact
on translation adequacy. When source words are
aligned to several target words, each target word
should be individually searched for in the candi-
date translation, and words from the reference can
only be matched once.
Table 3 shows contrastive results per part-of-
speech between the baseline fr?en system and
systems using various auxiliary languages. Val-
ues in the ?-? row indicate the number of words
that only the baseline system translated as in the
reference translation, and values in the ?+? row
the number of words that only our corresponding
system translated as in the reference. The most
striking result is the contribution of Greek, which,
while giving no gain in terms of BLEU, improved
the translation of 82 content words. This could
be explained, in addition to the lower Bleu3 and
Bleu4 precision, by the fact that the quality of
the translation of grammatical words may have
decreased. On the contrary, Italian brings little
improvement for content words save for nouns.
The mostly negative results on the translation of
pronouns were expected, because this depends on
their antecedent in English and is not the object of
specific modelling from the systems. The trans-
lation of nouns and adjectives benefits the most
from auxiliary translations.
projekte/corplex/TreeTagger
238
Figure 3 illustrates this evaluation by means of
two examples. It should be noted that a recurrent
type of improvement was that of avoiding missing
words, which is here a direct result of their being
boosted in the auxiliary hypotheses.
4 Conclusions and future work
We have presented a framework where auxiliary
MT systems are used to provide useful informa-
tion to a main SMT system. Our experiments
on auxiliary language triangulation have demon-
strated its validity on a difficult configuration and
have shown that improvements in translation qual-
ity could be obtained even under large training
data conditions.
The fact that low quality sources such as pivot
translation can provide useful complementary in-
formation calls for a better understanding of the
phenomena at play. It is very likely that, look-
ing at our results on the contribution of auxiliary
languages, improving the quality of an auxiliary
source can also be achieved by identifying what
a source is good for. For example, in the stud-
ied language configurations predictions of transla-
tions for pronouns in the source text by auxiliary
triangulation does not give access to useful infor-
mation. On the contrary, triangulation with Greek
when translating from French to English seems to
give useful information regarding the translation
of adjectives, a result which was quite unexpected.
Also, it would be interesting to use richer pre-
dictions than short n-grams, such as syntactic
dependencies, but this would require significant
changes on the decoders used. Using dynamic
models at the discourse level rather than only at
the sentence level would also be a useful improve-
ment. Besides the improvements just mentioned,
our future work includes working on several con-
figurations of the framework described in sec-
tion 2.2, in particular investigating the new type
of system combination.
Acknowledgements
This work has been partially funded by OSEO un-
der the Quaero program.
References
Bertoldi, Nicola and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
WMT, Athens, Greece.
Callison-Burch, Chris, Cameron Shaw Fordyce,
Philipp Koehn, Christof Monz, and Josh Schroeder.
2008. Further meta-evaluation of machine transla-
tion. In Proceedings of WMT, Columbus, USA.
Chen, Stanley F. and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of ACL, Santa
Cruz, USA.
Chen, Yu, Andreas Eisele, and Martin Kay. 2008. Im-
proving statistical machine translation efficiency by
triangulation. In Proceedings of LREC, Marrakech,
Morocco.
Cohn, Trevor and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use
of multi-parallel corpora. In Proceedings of ACL,
Prague, Czech Republic.
Dymetman, Marc, Aur?lien Max, and Kenji Yamada.
2003. Towards interactive text understanding. In
Proceedings of ACL, short paper session, Sapporo,
Japan.
Hildebrand, Almut Silja and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
Proceedings of AMTA, Honolulu, USA.
Kumar, Shankar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of NAACL-HLT, Boston, USA.
Mari?o, Jos?, Rafael E. Banchs, Josep Maria Crego,
Adria de Gispert, Patrick Lambert, J.A.R. Fonol-
losa, and Martha Costa-juss?. 2006. N-gram based
machine translation. Computational Linguistics,
32(4):527?549.
Matusov, Evgeny, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose Mari?o,
Matthias Paulik, Salim Roukos, Holger Schwenk,
and Hermann Ney. 2008. System combination for
machine translation of spoken and written language.
IEEE Transactions on Audio, Speech and Language
Processing, 16(7):1222?1237, September.
Max, Aur?lien, Josep M. Crego, and Fran?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of LREC, Valletta,
Malta.
239
ref #357 this concession to the unions ignores the reality that all airlines have different safety procedures which even differ
between aircrafts within each airline .
bas this concession unions ignores the fact that all airlines have different safety procedures which are even within each
of the companies in accordance with the types of equipment .
w.r.t. src cette concession aux syndicats ignore la r?alit? selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?
diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ? appareils .
+aux this concession to the trade unions ignores the reality according to which all the airlines have different safety pro-
cedures which differ even within each of the companies in accordance with the types of equipment .
w.r.t. src cette concession aux syndicats ignore la r?alit? selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?
diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ? appareils .
Figure 3: Example of automatic translations from French to English for the baseline system and when
using Spanish as the auxiliary language. Bold marking indicates source/target words which were cor-
rectly translated according to the reference translation.
Nomoto, Tadashi. 2004. Multi-engine machine trans-
lation with voted language model. In Proceedings
of ACL, Barcelona, Catalunya, Spain.
Och, Franz Josef and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Proceedings of MT
Summit, Santiago de Compostela, Spain.
Paulik, Matthias, Christian F?gen, Thomas Schaaf,
Tanja Schultz, Sebastian St?ker, and Alex Waibel.
2005. Document driven machine translation en-
hanced automatic speech recognition. In Proceed-
ings of InterSpeech, Lisbon, Portugal.
Placeway, Paul and John Lafferty. 1996. Cheating
with imperfect transcripts. In Proceedings of IC-
SLP, Philadelphia, USA.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bin Xiang,
Spyros Matsoukas, Richard Schwatz, and Bonnie J.
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Proceedings of
NAACL-HTL, Rochester, USA.
Schroeder, Josh, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation. In
Proceedings of EACL, Athens, Greece.
Schwartz, Lane. 2008. Multi-source translation meth-
ods. In Proceedings of AMTA, Honolulu, USA.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of Machine
Translation Summit XII, Ottawa, Canada.
Tillmann, Christoph. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of NAACL-HLT, Boston, USA.
Tromble, Roy, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP, Honolulu, USA.
Utiyama, Masao and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Proceedings of NAACL-
HLT, Rochester, USA.
Vilar, David, Jia Xu, Luis Fernando d?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proceedings of LREC,
Genoa, Italy.
Wu, Hua and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine trans-
lation. In Proceedings of ACL, Prague, Czech Re-
public.
240
Coling 2010: Poster Volume, pages 197?205,
Beijing, August 2010
Improving Reordering with Linguistically Informed Bilingual n-grams
Josep Maria Crego
LIMSI-CNRS
jmcrego@limsi.fr
Franc?ois Yvon
LIMSI-CNRS & Univ. Paris Sud
yvon@limsi.fr
Abstract
We present a new reordering model es-
timated as a standard n-gram language
model with units built from morpho-
syntactic information of the source and
target languages. It can be seen as a
model that translates the morpho-syntactic
structure of the input sentence, in contrast
to standard translation models which take
care of the surface word forms. We take
advantage from the fact that such units
are less sparse than standard translation
units to increase the size of bilingual con-
text that is considered during the trans-
lation process, thus effectively account-
ing for mid-range reorderings. Empirical
results on French-English and German-
English translation tasks show that our
model achieves higher translation accu-
racy levels than those obtained with the
widely used lexicalized reordering model.
1 Introduction
Word ordering is one of the major issues in statis-
tical machine translation (SMT), due to the many
word order peculiarities of each language. It is
widely accepted that there is a need for struc-
tural information to account for such differences.
Structural information, such as Part-of-speech
(POS) tags, chunks or constituency/dependency
parse trees, offers a greater potential to learn
generalizations about relationships between lan-
guages than models based on word surface forms,
because such ?surfacist? models fail to infer gen-
eralizations from the training data.
The word ordering problem is typically decom-
posed in a number of related problems which can
be further explained by a variety of linguistic phe-
nomena. Accordingly, we can sort out the re-
ordering problems into three categories based on
the kind of linguistic units involved and/or the
typical distortion distance they imply. Roughly
speaking, we face short-range reorderings when
single words are reordered within a relatively
small window distance. It consist of the easi-
est case as typically, the use of phrases (in the
sense of translation units of the phrase-based ap-
proach to SMT) is believed to adequately perform
such reorderings. Mid-range reorderings involve
reorderings between two or more phrases (trans-
lation units) which are closely positioned, typi-
cally within a window of about 6 words. Many
alternatives have been proposed to tackle mid-
range reorderings through the introduction of lin-
guistic information in MT systems. To the best
of our knowledge, the authors of (Xia and Mc-
Cord, 2004) were the first to address this prob-
lem in the statistical MT paradigm. They auto-
matically build a set of linguistically grounded
rewrite rules, aimed at reordering the source sen-
tence so as to match the word order of the target
side. Similarly, (Collins, et al2005) and (Popovic
and Ney, 2006) reorder the source sentence us-
ing a small set of hand-crafted rules for German-
English translation. (Crego and Marin?o, 2007)
show that the ordering problem can be more accu-
rately solved by building a source-sentence word
lattice containing the most promising reordering
hypotheses, allowing the decoder to decide for the
best word order hypothesis. Word lattices are built
by means of rewrite rules operating on POS tags;
such rules are automatically extracted from the
training bi-text. (Zhang, et al2007) introduce
shallow parse (chunk) information to reorder the
source sentence, aiming at extending the scope of
their rewrite rules, encoding reordering hypothe-
ses in the form of a confusion network that is
then passed to the decoder. These studies tackle
mid-range reorderings by predicting more or less
accurate reordering hypotheses. However, none
197
of them introduce a reordering model to be used
in decoding time. Nowadays, most of SMT sys-
tems implement the well known lexicalized re-
ordering model (Tillman, 2004). Basically, for
each translation unit it estimates the probability
of being translated monotone, swapped or placed
discontiguous with respect to its previous trans-
lation unit. Integrated within the Moses (Koehn,
et al2007) decoder, the model achieves state-of-
the-art results for many translation tasks. One
of the main reasons that explains the success of
the model is that it considers information of the
source- and target-side surface forms, while the
above mentionned approaches attempt to hypoth-
esize reorderings relying only on the information
contained on the source-side words.
Finally, long-range reorderings imply reorder-
ings in the structure of the sentence. Such re-
orderings are necessary to model the translation
for pairs like Arabic-English, as English typically
follows the SVO order, while Arabic sentences
have different structures. Even if several attempts
exist which follow the above idea of making the
ordering of the source sentence similar to the tar-
get sentence before decoding (Niehues and Kolss,
2009), long-range reorderings are typically better
addressed by syntax-based and hierarchical (Chi-
ang, 2007) models. In (Zollmann et al, 2008),
an interesting comparison between phrase-based,
hierarchical and syntax-augmented models is car-
ried out, concluding that hierarchical and syntax-
based models slightly outperform phrase-based
models under large data conditions and for suf-
ficiently non-monotonic language pairs.
Encouraged by the work reported in (Hoang
and Koehn, 2009), we tackle the mid-range re-
ordering problem in SMT by introducing a n-
gram language model of bilingual units built from
POS information. The rationale behind such a
model is double: on the one hand we aim at in-
troducing morpho-syntactic information into the
reordering model, as we believe it plays an im-
portant role for predicting systematic word or-
dering differences between language pairs; at the
same time that it drastically reduces the sparse-
ness problem of standard translation units built
from surface forms. On the other hand, n-gram
language modeling is a robust approach, that en-
ables to account for arbitrary large sequences of
units. Hence, the proposed model takes care of
the translation adequacy of the structural informa-
tion present in translation hypotheses, here intro-
duced in the form of POS tags. We also show how
the new model compares to a widely used lexical-
ized reordering model, which we have also im-
plemented in our particular bilingual n-gram ap-
proach to SMT, as well as to the widely known
Moses SMT decoder, a state-of-the-art decoder
performing lexicalized reordering.
The remaining of this paper is as follows. In
Section 2 we briefly describe the bilingual n-gram
SMT system. Section 3 details the bilingual n-
gram reordering model, the main contribution of
this paper, and introduces additional well known
reordering models. In Section 4, we analyze the
reordering needs of the language pairs considered
in this work and we carry out evaluation experi-
ments. Finally, we conclude and outline further
work in Section 5.
2 Bilingual n-gram SMT
Our SMT system defines a translation hypothesis
t given a source sentence s, as the sentence which
maximizes a linear combination of feature func-
tions:
t?I1 = argmaxtI1
{ M?
m=1
?mhm(sJ1 , tI1)
}
(1)
where ?m is the weight associated with the fea-
ture hm(s, t). The main feature is the log-score of
the translation model based on bilingual n-grams.
This model constitutes a language model of a par-
ticular bi-language composed of bilingual units
which are typically referred to as tuples (Marin?o et
al., 2006). In this way, the translation model prob-
abilities at the sentence level are approximated by
using n-grams of tuples:
p(sJ1 , tI1) =
K?
k=1
p((s, t)k|(s, t)k?1 . . . (s, t)k?n+1)
where s refers to source t to target and (s, t)k to
the kth tuple of the given bilingual sentence pairs,
sJ1 and tI1. It is important to notice that, since
both languages are linked up in tuples, the context
198
information provided by this translation model is
bilingual. As for any standard n-gram language
model, our translation model is estimated over a
training corpus composed of sentences of the lan-
guage being modeled, in this case, sentences of
the bi-language previously introduced. Transla-
tion units consist of the core elements of any SMT
system. In our case, tuples are extracted from a
word aligned corpus in such a way that a unique
segmentation of the bilingual corpus is achieved,
allowing to estimate the n-gram model. Figure 1
presents a simple example illustrating the unique
tuple segmentation for a given word-aligned pair
of sentences (top).
Figure 1: Tuple extraction from an aligned sen-
tence pair.
The resulting sequence of tuples (1) is further
refined to avoid NULL words in source side of the
tuples (2). Once the whole bilingual training data
is segmented into tuples, n-gram language model
probabilities can be estimated. Notice from the
example that the English source words perfect and
translations have been reordered in the final tu-
ple segmentation, while the French target words
are kept in their original order. During decoding,
sentences to be translated are encoded in the form
of word lattices containing the most promising re-
ordering hypotheses, so as to reproduce the word
order modifications introduced during the tuple
extraction process. Hence, at decoding time, only
those reordering hypotheses encoded in the word
lattice are examined. Reordering hypotheses are
introduced following a set of reordering rules au-
tomatically learned from the bi-text corpus word
alignments.
Following on the previous example, the rule
perfect translations ; translations perfect pro-
duces the swap of the English words that is ob-
served for the French and English pair. Typically,
POS information is used to increase the general-
ization power of such rules. Hence, rewrite rules
are built using POS instead of surface word forms.
See (Crego and Marin?o, 2007) for details on tuples
extraction and reordering rules.
3 Reordering Models
In this section, we detail three different reordering
models implemented in our SMT system. As pre-
viously outlined, the purpose of reordering mod-
els is to accurately learn generalizations for the
word order modifications introduced on the source
side during the tuple extraction process.
3.1 Source n-gram Language Model
We employ a n-gram language model estimated
over the source words of the training corpus af-
ter being reordered in the tuple extraction process.
Therefore, the model scores a given source-side
reordering hypothesis according to the reorder-
ings performed in the training sentences.
POS tags are used instead of surface forms
in order to improve generalization and to reduce
sparseness. The model is estimated as any stan-
dard n-gram language model, described by the
following equation:
p(sJ1 , tI1) =
J?
j=1
p(stj |stj?1, . . . , stj?n+1) (2)
where stj relates to the POS tag used for the jth
source word.
The main drawback of this model is the lack
of knowledge of the hypotheses on the target-
side. The probability assigned to a sequence of
source words is only conditioned to the sequence
of source words.
3.2 Lexicalized Reordering Model
A broadly used reordering model for phrase-based
systems is lexicalized reordering (Tillman, 2004).
It introduces a probability distribution for each
phrase pair that indicates the likelihood of being
199
translated monotone, swapped or placed discon-
tiguous to its previous phrase. The ordering of
the next phrase with respect to the current phrase
is typically also modeled. In our implementa-
tion, we modified the three orientation types and
consider: a consecutive type, where the original
monotone and swap orientations are lumped to-
gether, a forward type, specifying discontiguous
forward orientation, and a backward type, spec-
ifying discontiguous backward orientation. Em-
pirical results showed that in our case, the new
orientations slightly outperform the original ones.
This may be explained by the fact that the model
is applied over tuples instead of phrases.
Counts of these three types are updated for
each unit collected during the training process.
Given these counts, we can learn probability dis-
tributions of the form pr(orientation|(st)) where
orientation ? {c, f, b} (consecutive, forward
and backward) and (st) is a translation unit.
Counts are typically smoothed for the estimation
of the probability distribution. A major weakness
of the lexicalized reordering model is due to the
fact that it does not considers phrase neighboring,
i.e. a single probability is learned for each phrase
pair without considering its context. An additional
concern is the problem of sparse data: translation
units may occur only a few times in the training
data, making it hard to estimate reliable probabil-
ity distributions.
3.3 Linguistically Informed Bilingual
n-gram Language Model
The bilingual n-gram LM is estimated as a stan-
dard n-gram LM over translation units built from
POS tags represented as:
p(sJ1 , tI1) =
K?
k=1
p((st)tk|(st)tk?1 . . . (st)tk?n+1)
where (st)tk relates to the kth translation unit,
(st)k, built from POS tags instead of words.
This model aims at alleviating the drawbacks of
the previous two reordering models. On the one
hand it takes into account bilingual information
to model reordering. On the other hand it con-
siders the phrase neighboring when estimating the
reordering probability of a given translation unit.
Figure 2 shows the sequence of translation units
built from POS tags, used in our previous exam-
ple.
Figure 2: Sequence of POS-tagged units used to
estimate the bilingual n-gram LM.
POS-tagged units used in our model are ex-
pected to be much less sparse than those built from
surface forms, allowing to estimate higher order
language models. Therefore, larger bilingual con-
text are introduced in the translation process. This
model can also be seen as a translation model of
the sentence structure. It models the adequacy of
translating sequences of source POS tags into tar-
get POS tags.
Note that the model is not limited to using
POS information. Rather, many other informa-
tion sources could be used (supertags, additional
morphology features, etc.), allowing to model dif-
ferent translation properties. However, we must
take into account that the degree of sparsity of the
model units, which is directly related to the in-
formation they contain, affects the level of bilin-
gual context finally introduced in the translation
process. Since more informed units may yield
more accurate predictions, more informed units
may also force the model to fall to lower n-grams.
Hence, the degree of accuracy and generalization
power of the model units must be carefully bal-
anced to allow good reordering predictions for
contexts as large as possible.
As any standard language model, smoothing is
needed. Empirical results showed that Kneser-
Ney smoothing (Kneser and Ney, 1995) achieved
the best performance among other options (mea-
sured in terms of translation accuracy).
3.4 Decoding Issues
A straightforward implementation of the three
models is carried out by extending the log-linear
combination of equation (1) with the new features.
Note that no additional decoding complexity is
introduced in the baseline decoding implementa-
tion. Considering the bilingual n-gram language
model, the decoder must know the POS tags for
200
each tuple. However, each tuple may be tagged
differently, as words with same surface form may
have different POS tags.
We have implemented two solutions for this sit-
uation. Firstly, we assume that each tuple has a
single POS-tagged version. Accordingly, we se-
lect a single POS-tagged version out of the mul-
tiple choices (the most frequent). Secondly, all
POS-tagged versions of each tuple are allowed.
The second choice implies using more accurate
POS-tagged tuples to model reordering, however,
it overpopulates the search space with spurious
hypotheses, as multiple identical units (with dif-
ferent POS tags) are considered.
Our first empirical findings showed no differ-
ences in translation accuracy for both configura-
tions. Hence, in the remaining of this paper we
only consider the first solution (a single POS-
tagged version of each tuple). The training cor-
pus composed of tagged units out of which our
new model is estimated is accordingly modified to
contain only those tagged units considered in de-
coding. Note that most of the ambiguity present in
word tagging is resolved by the fact that transla-
tion units may contain multiple source and target
side words.
4 Evaluation Framework
In this section, we perform evaluation experi-
ments of our novel reordering model. First, we
give details of the corpora and baseline system
employed in our experiments and analyze the re-
ordering needs of the translation tasks, French-
English and German-English (in both directions).
Finally, we evaluate the performance of our model
and contrast results with other reordering models
and translation systems.
4.1 Corpora
We have used the fifth version of the EPPS and the
News Commentary corpora made available in the
context of the Fifth ACL Workshop on Statistical
Machine Translation. Table 1 presents the basic
statistics for the training and test data sets. Our
test sets correspond to news-test2008 and new-
stest2009 file sets, hereinafter referred to as Tune
and Test respectively.
French, German and English Part-of-speech
tags are computed by means of the TreeTagger 1
toolkit. Additional German tags are obtained us-
ing the RFTagger 2 toolkit, which annotates text
with fine-grained part-of-speech tags (Schmid and
Laws, 2008) with a vocabulary of more than 700
tags containing rich morpho-syntactic information
(gender, number, case, tense, etc.).
Lang. Sent. Words Voc. OOV Refs
Train
French 1.75 M 52.4 M 137 k ? ?
English 1.75 M 47.4 M 138 k ? ?
Tune
French 2, 051 55.3 k 8, 957 1, 282 1
English 2, 051 49.2 k 8, 359 1, 344 1
Test
French 2, 525 72.8 k 10, 832 1, 749 1
English 2, 525 65.1 k 9, 568 1, 724 1
Train
German 1, 61 M 42.2 M 381 k ? ?
English 1, 61 M 44.2 M 137 k ? ?
Tune
German 2, 051 47, 8 k 10, 994 2, 153 1
English 2, 051 49, 2 k 8, 359 1, 491 1
Test
German 2, 525 62, 8 k 12, 856 2, 704 1
English 2, 525 65, 1 k 9, 568 1, 810 1
Table 1: Statistics for the training, tune and test
data sets.
4.2 System Details
After preprocessing the corpora with standard tok-
enization tools, word-to-word alignments are per-
formed in both directions, source-to-target and
target-to-source. In our system implementation,
the GIZA++ toolkit3 is used to compute the
word alignments. Then, the grow-diag-final-and
(Koehn et al, 2005) heuristic is used to obtain the
alignments from which tuples are extracted.
In addition to the tuple n-gram translation
model, our SMT system implements six addi-
tional feature functions which are linearly com-
1www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
2www.ims.uni-stuttgart.de/projekte/corplex/RFTagger
3http://www.fjoch.com/GIZA++.html
201
bined following a discriminative modeling frame-
work (Och and Ney, 2002): a target-language
model which provides information about the tar-
get language structure and fluency; two lexicon
models, which constitute complementary trans-
lation models computed for each given tuple;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which are used in order to compensate for
the system preference for short translations.
All language models used in this work are
estimated using the SRI language modeling
toolkit4. According to our experience, Kneser-
Ney smoothing (Kneser and Ney, 1995) and in-
terpolation of lower and higher n-grams options
are used as they typically achieve the best per-
formance. Optimization work is carried out by
means of the widely used MERT toolkit5 which
has been slightly modified to perform optimiza-
tions embedding our decoder. The BLEU (Pap-
ineni et al, 2002) score is used as objective func-
tion for MERT and to evaluate test performance.
4.3 Reordering in German-English and
French-English Translation
Two factors are found to greatly impact the overall
translation performance: the morphological mis-
match between languages, and their reordering
needs. The vocabulary size is strongly influenced
by the number of word forms for number, case,
tense, mood, etc., while reordering needs refer to
the difference in their syntactic structure. In this
work, we are primarily interested on the reorder-
ing needs of each language pair. Figure 3 displays
a quantitative analysis of the reordering needs for
the language pairs under study.
Figure 3 displays the (%) distribution of the
reordered sequences, according to their size, ob-
served for the training bi-texts of both translation
tasks. Word alignments are used to determine re-
orderings. A reordering sequence can also be seen
as the sequence of words implied in a reorder-
ing rule. Hence, we used the reordering rules ex-
tracted from the training corpus to account for re-
ordering sequences. Coming back to the example
of Figure 1, a single reordering sequence is found,
4http://www.speech.sri.com/projects/srilm/
5http://www.statmt.org/moses/
which considers the source words perfect transla-
tions.
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
2 3 4 5 6 7 >=8
Size (words)
fr-en
de-en
Figure 3: Size (in words) of reorderings (%) ob-
served in training bi-texts.
As can be seen, the French-English and
German-English pairs follow a different distribu-
tion of reorderings according to their size. A
lower number of short-range reorderings are ob-
served for the German-English task while a higher
number of long-range reorderings. Considering
mid-range reorderings (from 5 to 7 words), the
French-English pair shows a lower percentage (?
14%) than the German-English (? 22%). A simi-
lar performance is expected when considering the
opposite translation directions. Note that reorder-
ings are extracted from word-alignments, an au-
tomatic process which is far notoriously error-
prone. The above statistics must be accordingly
considered.
4.4 Results
Translation accuracy (BLEU) results are given in
table 2 for the same baseline system performing
different reordering models: source 6-gram LM
(sLM); lexicalized reordering (lex); bilingual 6-
gram LM (bLM) assuming a single POS-tagged
version of each tuple. In the case of the German-
English translation task we also report results for
the bilingual 5-gram LM built from POS tags ob-
tained from RFTagger containing a richer vocab-
ulary tag set (b+LM). For comparison purposes,
we also show the scores obtained by the Moses
phrase-based system performing lexicalized re-
ordering. Models of both systems are built sharing
the same training data and word alignments.
202
The worst results are obtained by the sLM
model. The fact that it only considers source-
language information results clearly relevant to
accurately model reordering. A very similar
performance is shown by our bilingual n-gram
system and Moses under lexicalized reordering
(bLM and Moses), slightly lower results are
obtained by the n-gram system under French-
English translation.
Config Fr;En En;Fr De;En En;De
sLM 22.32 21.97 17.11 12.23
lex 22.46 22.09 17.31 12.38
bLM 23.03 22.32 17.37 12.58
b+LM ? ? 17.57 12.92
Moses 22.81 22.33 17.22 12.45
Table 2: Translation accuracy (BLEU) results.
When moving from lex to bLM, our system
increases its accuracy results for both tasks and
translation directions. In this case, results are
slightly higher than those obtained by Moses
(same results for English-to-French). Finally, re-
sults for translations performed with the bilingual
n-gram reordering model built from rich German
POS tags (b+LM) achieve the highest accuracy
results for both directions of the German-English
task. Even though results are consistent for all
translation tasks and directions they fall within
the statistical confidence margin. Add ?2.36
to French-English results and ?1.25 to German-
English results for a 95% confidence level. Very
similar results were obtained when estimating our
model for orders from 5 to 7.
In order to better understand the impact of the
proposed reordering model, we have measured the
accuracy of the reordering task. Hence, isolat-
ing the reordering problem from the more general
translation problem. We use BLEU to account the
n-gram matching between the sequence of source
words aligned to the 1-best translation hypothe-
sis, i.e. the permutation of the source words out-
put by the decoder, and the permutation of source
words that monotonizes the word alignments with
respect to the target reference. Note that in or-
der to obtain the word alignments of the test sets
we re-aligned the entire corpus after including the
test set. Table 3 shows the BLEU results of the
reordering task. Bigram, trigram and 4gram pre-
cision scores are also given.
Pair Config BLEU (2g/3g/4g)
Fr;En lex 71.69 (75.0/63.4/55.6)
bLM 71.98 (75.3/63.7/56.0)
En;Fr lex 72.92 (75.5/65.0/57.6)
bLM 73.25 (75.8/65.4/58.1)
De;En lex 62.12 (67.3/52.1/42.5)
b+LM 63.29 (68.3/53.5/44.0)
En;De lex 62.72 (67.9/52.8/43.1)
b+LM 63.36 (68.6/53.6/43.8)
Table 3: Reordering accuracy (BLEU) results.
As can be seen, the bilingual n-gram reordering
model shows higher results for both translation
tasks and directions than lexicalized reordering,
specially for German-English translation. Our
model also obtains higher values of n-gram pre-
cision for all values of n.
Next, we validate the introduction of additional
bilingual context in the translation process. Fig-
ure 4 shows the average size of the translation
unit n-grams used for the test set according to dif-
ferent models (German-English), the surface form
3-gram language model (main translation model),
and the new reordering model when built from the
reduced POS tagset (POS) and using the rich POS
tagset (POS+).
 0
 5
 10
 15
 20
 25
 30
 35
 40
0 1 2 3 4 5 6 7 8 9
Size (units)
word-based bilingual units
POS-based bilingual units
POS+-based bilingual units
Figure 4: Size of translation unit n-grams (%)
seen in test for different n-gram models.
As expected, translation units built from the re-
duced POS tagset are less sparse, enabling us to
203
introduce larger n-grams in the translation pro-
cess. However, the fact that they achieve lower
translation accuracy scores (see Table 2) indicates
that the probabilities associated to these large n-
grams are less accurate. It can also be seen that
the model built from the rich POS tagset uses a
higher number of large n-grams than the language
model built from surface forms.
The availability of mid-range n-grams validates
the introduction of additional bilingual context
achieved by the new model, leading to effec-
tively modeling mid-range reorderings. Notice
additionally that considering the language model
built from surface forms, only a few 4-grams of
the test set are seen in the training set, which
explains the small reduction in performance ob-
served when translating with a bilingual 4-gram
language model (internal results). Similarly, the
results shown in Figure 4 validates the choice of
using bilingual 5-grams for b+LM and 6-grams
for bLM .
Finally, we evaluate the mismatch between the
reorderings collected on the training data, and
those output by the decoder. Table 4 shows the
percentage of reordered sequences found for the
1-best translation hypothesis of the test set ac-
cording to their size. The French-to-English and
German-to-English tasks are considered.
Pair Config 2 3 4 5 6 7 ? 8
Fr;En lex 58 23 10 5 2 1 1
bLM 57 23 11 4 2.5 1.5 1
De;En lex 33 24 22 14 5 1.5 0.5
b+LM 35 25 19 13 5 2.5 0.5
Table 4: Size (%) of the reordered sequences ob-
served when translating the test set.
Very similar distributions are observed for both
reordering models. In parallel, distributions are
also comparable to those presented in Figure 3
for reorderings collected from the training bi-text,
with the exception of long-range and very short-
range reorderings. This may be explained by the
fact that system models, in special the distortion
penalty model, typically prefer monotonic trans-
lations, while the system lacks a model to support
large-range reorderings.
5 Conclusions and Further Work
We have presented a new reordering model based
on bilingual n-grams with units built from lin-
guistic information, aiming at modeling the struc-
tural adequacy of translations. We compared our
new reordering model to the widely used lexical-
ized reordering model when implemented in our
bilingual n-gram system as well as using Moses,
a state-of-the-art phrase-based SMT system.
Our model obtained slightly higher transla-
tion accuracy (BLEU) results. We also analysed
the quality of the reorderings output by our sys-
tem when performing the new reordering model,
which also outperformed the quality of those out-
put by the system performing lexicalized reorder-
ing. The back-off procedure used by standard
language models allows to dynamically adapt the
scope of the context used. Therefore, in the case
of our reordering model, back-off allows to con-
sider always as much bilingual context (n-grams)
as possible. The new model was straightfor-
ward implemented in our bilingual n-gram sys-
tem by extending the log-linear combination im-
plemented by our decoder. No additional decod-
ing complexity was introduced in the baseline de-
coding implementation.
Finally, we showed that mid-range reorder-
ings are present in French-English and German-
English translations and that our reordering model
effectively tackles such reorderings. However, we
saw that long-range reorderings, also present in
these tasks, are yet to be addressed.
We plan to further investigate the use of differ-
ent structural information, such as supertags, and
tags conveying different levels of morphology in-
formation (gender, number, tense, mood, etc.) for
different language pairs.
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program.
References
F. Xia and M. McCord. Improving a Statistical MT
System with Automatically Learned Rewrite Pat-
terns. In Proc. of the COLING 2004, 508?514,
Geneva, Switzerland, August 2004.
204
D. Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228, June
2007.
H. Hoang and Ph. Koehn. Improving Mid-Range Re-
Ordering Using Templates of Factors. In Proc. of
the EACL 2009, 372?379, Athens, Greece, March
2009.
J. M. Crego and J. B. Marin?o. Improving statistical
MT by coupling reordering and decoding. In Ma-
chine Translation, 20(3):199?215, July 2007.
Marin?o, Jose? and Banchs, Rafael E. and Crego, Josep
Maria and de Gispert, Adria and Lambert, Patrick
and Fonollosa, J.A.R. and Costa-jussa`, Marta N-
gram Based Machine Translation. In Computa-
tional Linguistics, 32(4):527?549, 2006
Ch. Tillman. A Unigram Orientation Model for Sta-
tistical Machine Translation. In Proc. of the HLT-
NAACL 2004, 101?104, Boston, MA, USA, May
2004.
M. Collins, Ph. Koehn and I. Kucerova. Clause Re-
structuring for Statistical Machine Translation. In
Proc. of the ACL 2005, 531?540, Ann Arbor, MI,
USA, June 2005.
Ph. Koehn, H. Hoang, A. Birch, Ch. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, Ch.
Moran, R. Zens, Ch. Dyer, O. Bojar, A. Constantin
and E. Herbst. Moses: Open Source Toolkit for Sta-
tistical Machine Translation. In Proc. of the ACL
2007, demonstration session, prague, Czech Repub-
lic, June 2007.
Y. Zhang, R. Zens and H. Ney Improved Chunk-level
Reordering for Statistical Machine Translation. In
Proc. of the IWSLT 2007, 21?28, Trento, Italy, Oc-
tober 2007.
H. Schmid and F. Laws. Estimation of Conditional
Probabilities with Decision Trees and an Applica-
tion to Fine-Grained POS Tagging. In Proc. of the
COLING 2008, 777?784, Manchester, UK, August
2008.
F.J. Och and H. Ney. Improved statistical alignment
models. In Proc. of the ACL 2000, 440?447, Hong
Kong, China, October 2000.
Ph. Koehn, A. Axelrod, A. Birch, Ch. Callison-Burch,
M. Osborne and D. Talbot. Edinburgh System De-
scription for the 2005 IWSLT Speech Translation
Evaluation. In Proc of the IWSLT 2005, Pittsburgh,
PA, October 2005.
F. J. Och and H. Ney. Discriminative Training and
Maximum Entropy Models for Statistical Machine
Translation. In Proc. of the ACL 2002. 295?302,
Philadelphia, PA, July 2002.
A. Stolcke. SRLIM: an extensible language model-
ing toolkit. Proc. of the INTERSPEECH 2002. 901?
904, Denver, CO, September 2008.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. Bleu:
a method for automatic evaluation of machine trans-
lation. In Proc. of the ACL 2002, 311?318, Philadel-
phia, PA, July 2002.
R. Kneser and H. Ney. Improved backing-off for m-
gram language modeling. In Proc. of the ICASSP
1995. 181?184, Detroit, MI, May 1995.
A. Zollmann, A. Venugopal, F. J. Och and J. Ponte.
A Systematic Comparison of Phrase-Based, Hierar-
chical and Syntax-Augmented Statistical MT. In
Proc. of the COLING 2008. 1145?1152, Manch-
ester, UK, August 2008.
M. Popovic and H. Ney. POS-based Word Reorderings
for Statistical Machine Translation. In Proc. of the
LREC 2006. 1278?1283, Genoa, Italy, May 2006.
J. Niehues and M. Kolss. A POS-Based Model for
Long-Range Reorderings in SMT. In Proc. of the
WMT 2009. 206?214, Athens Greece, March 2009.
205
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778?788,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Training continuous space language models:
some practical issues
Le Hai Son and Alexandre Allauzen and Guillaume Wisniewski and Franc?ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
BP 133, 91403 Orsay Cedex
Firstname.Lastname@limsi.fr
Abstract
Using multi-layer neural networks to esti-
mate the probabilities of word sequences is
a promising research area in statistical lan-
guage modeling, with applications in speech
recognition and statistical machine transla-
tion. However, training such models for large
vocabulary tasks is computationally challeng-
ing which does not scale easily to the huge
corpora that are nowadays available. In this
work, we study the performance and behav-
ior of two neural statistical language models
so as to highlight some important caveats of
the classical training algorithms. The induced
word embeddings for extreme cases are also
analysed, thus providing insight into the con-
vergence issues. A new initialization scheme
and new training techniques are then intro-
duced. These methods are shown to greatly re-
duce the training time and to significantly im-
prove performance, both in terms of perplexity
and on a large-scale translation task.
1 Introduction
Statistical language models play an important role in
many practical applications, such as machine trans-
lation and automatic speech recognition. Let V be
a finite vocabulary, statistical language models de-
fine distributions over sequences of words wL1 in V
?
usually factorized as:
P (wL1 ) = P (w1)
L?
l=1
P (wl|w
l?1
1 )
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in real-world Natural Language
Processing applications where V typically contains
dozens of thousands words.
Many approaches to this problem have been pro-
posed over the last decades, the most widely used
being back-off n-gram language models. n-gram
models rely on a Markovian assumption, and de-
spite this simplification, the maximum likelihood es-
timate (MLE) remains unreliable and tends to under-
estimate the probability of very rare n-grams, which
are hardly observed even in huge corpora. Con-
ventional smoothing techniques, such as Kneser-
Ney and Witten-Bell back-off schemes (see (Chen
and Goodman, 1996) for an empirical overview,
and (Teh, 2006) for a Bayesian interpretation), per-
form back-off on lower order distributions to pro-
vide an estimate for the probability of these unseen
events. n-gram language models rely on a discrete
space representation of the vocabulary, where each
word is associated with a discrete index. In this
model, the morphological, syntactic and semantic
relationships which structure the lexicon are com-
pletely ignored, which negatively impact the gen-
eralization performance of the model. Various ap-
proaches have proposed to overcome this limita-
tion, notably the use of word-classes (Brown et al,
1992; Niesler, 1997), of generalized back-off strate-
gies (Bilmes et al, 1997) or the explicit integration
of morphological information in the random-forest
model (Xu and Jelinek, 2004; Oparin et al, 2008).
One of the most successful alternative to date is to
use distributed word representations (Bengio et al,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
778
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated probability estimates are jointly
computed in a multi-layer neural network architec-
ture. This approach has showed significant and
consistent improvements when applied to automatic
speech recognition (Schwenk, 2007; Emami and
Mangu, 2007; Kuo et al, 2010) and machine trans-
lation tasks (Schwenk et al, 2006). Hence, contin-
uous space language models are becoming increas-
ingly used. These successes have revitalized the re-
search on neuronal architectures for language mod-
els, and given rise to several new proposals (see, for
instance, (Mnih and Hinton, 2007; Mnih and Hinton,
2008; Collobert and Weston, 2008)). A major diffi-
culty with these approaches remains the complexity
of training, which does not scale well to the mas-
sive corpora that are nowadays available. Practical
solutions to this problem are discussed in (Schwenk,
2007), which introduces a number of optimization
and tricks to make training doable. Even then, train-
ing a neuronal language model typically takes days.
In this paper, we empirically study the conver-
gence behavior of two multi-layer neural networks
for statistical language modeling, comparing the
standard model of (Bengio et al, 2003) with the log-
bilinear (LBL) model of (Mnih and Hinton, 2007).
Our contributions are the following: we first pro-
pose a reformulation of Mnih and Hinton?s model,
which reveals its similarity with extant models, and
allows a direct and fair comparison with the stan-
dard model. For the standard model, these results
highlight the impact of parameter initialization. We
first investigate a re-initialization method which al-
lows to escape from the local extremum the standard
model converges to. While this method yields a sig-
nificative improvement, the underlying assumption
about the structure of the model does not meet the
requirement of very large-scale tasks. We therefore
introduce a different initialization strategy, called
one vector initialization. Experimental results show
that these novel training strategies drastically reduce
the total training time, while delivering significant
improvements both in terms of perplexity and in a
large-scale translation task.
The rest of this paper is organized as follows. We
first describe, in Section 2, the standard and the LBL
language models. By reformulating the latter, we
show that both models are very similar and empha-
size the remaining differences. Section 2.4 discusses
complexity issues and possible solutions to reduce
the training time. We then report, in Section 3, pre-
liminary experimental results that enlighten some
caveats of the standard approach. Based on these
observations, we introduce in Section 4 novel and
more efficient training schemes, yielding improved
performance and a reduced training time both on
small and large scale experiments.
2 Continuous space language models
Learning a language model amounts to estimate the
parameters of the discrete conditional distribution
over words given each possible history, where the
history corresponds to some function of the preced-
ing words. For an n-gram model, the history con-
tains the n ? 1 preceding words, and the model
parameters correspond to P (wl|w
l?1
l?n+1). Continu-
ous space language models aim at computing these
estimates based on a distributed representation of
words (Bengio et al, 2003), thereby reducing the
sparsity issues that plague conventional maximum
likelihood estimation. In this approach, each word
in the vocabulary is mapped into a real-valued vec-
tor and the conditional probability distributions are
then expressed as a (parameterized) smooth func-
tion of these feature vectors. The formalism of neu-
ral networks allows to express these two steps in a
well-known framework, where, crucially, the map-
ping and the model parameters can be learned in
conjunction. In the next paragraphs, we describe the
two continuous space language models considered
in our study and present the various issues associ-
ated with the training of such models, as well as their
most common remedies.
2.1 The standard model
In the following, we will consider words as indices
in a finite dictionary of size V ; depending on the
context, w will either refer to the word or to its in-
dex in the dictionary. A word w can also be repre-
sented by a 1-of-V coding vector v of RV in which
all elements are null except the wth. In the standard
approach of (Bengio et al, 2003), the feed-forward
network takes as input the n?1 word history and de-
livers an estimate of the probability P (wl|w
l?1
l?n+1)
779
as its output. It consists of three layers.
The first layer builds a continuous representation
of the history by mapping each word into its real-
valued representation. This mapping is defined by
RTv, where R ? RV?m is a projection matrix
and m is the dimension of the continuous projection
word space. The output of this layer is a vector i of
(n ? 1)m real numbers obtained by concatenating
the representations of the context words. The pro-
jection matrix R is shared along all positions in the
history vector and is learned automatically.
The second layer introduces a non-linear trans-
form, where the output layer activation values are
defined by h = tanh (Wihi + bih) , where i is the
input vector, Wih ? RH?(n?1)m and bih ? RH are
the parameters of this layer. The vector h ? RH can
be considered as an higher (more abstract) represen-
tation of the context than i.
The third layer is an output layer that estimates the
desired probability, thanks to the softmax function:
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
(1)
o = Whoh + bho, (2)
where Who ? RV?H and bho ? RV are respec-
tively the projection matrix and the bias term associ-
ated with this layer. The wth component in P corre-
sponds to the estimated probability of the wth word
of the vocabulary given the input history vector.
The standard model has two hyper-parameters
(the dimension of projection space m and the size of
hidden layer, H) that define the architecture of the
neural network and a set of free parameters ? that
need to be learned from data: the projection matrix
R, the weight matrix Wih, the bias vector bih, the
weight matrix Who and the bias vector bho.
In this model, the projection matrices R and Who
play similar roles as they define maps between the
vocabulary and the hidden representation. The fact
that R assigns similar representations to history
words w1 and w2 implies that these words can be
exchanged with little impact on the resulting prob-
ability distribution. Likewise, the similarity of two
lines in Who is an indication that the corresponding
words tend to have a similar behavior, i.e. tend to
have a similar probabilities of occurrence in all con-
texts. In the remainder, we will therefore refer to R
as the matrix representing the context space, and to
Who as the matrix for the prediction space.
2.2 The log-bilinear model
The work reported (Mnih and Hinton, 2007) de-
scribes another parameterization of the architecture
introduced in the previous section. This parameter-
ization is based on Factored Restricted Boltzmann
Machine. According to (Mnih and Hinton, 2007),
this model, termed the log-bilinear language model
(LBL), achieves, for large vocabulary tasks, bet-
ter results in terms of perplexity than the standard
model, even if the reasons beyond this improvement
remain unclear.
In this section, we will describe this model and
show how it relates to the standard model. The LBL
model estimates the n-gram parameters by:
P (wl|w
l?1
l?n+1) =
exp(?E(wl;w
l?1
l?n+1))
?
w exp(?E(w;w
l?1
l?n+1))
(3)
In this equation, E is an energy function defined as:
E(wl;w
l?1
1 ) = ?
(
l?1?
k=l?n+1
vk
TRCTk
)
RTvl
(4)
? brTRTvl ? bv
Tvl
= ?vTl R
(
l?1?
k=l?n+1
CkR
Tvk + br
)
? vTl bv (5)
where R is the projection matrix introduced above,
(vk)l?n+1?k?l?1 are the 1-of-V coding vectors for
the history words and vl is the coding vector for wl;
Ck ? Rm?m is a combination matrix and br and bv
denote bias vectors. All these parameters need to be
learned during training.
Equation (4) can be rewritten using the notations
introduced for the standard model. We then rename
br and bv respectively bih and bho. We also denote
i the concatenation of the (n ? 1) vectors RTvk;
likewise Wih denotes the H ? (n? 1)m matrix ob-
tained by concatenating row-wise the (n ? 1) ma-
trices Ck. With these new notations, equations (4)
780
and (3) can be rewritten as:
h = Wihi + bih
o = Rh + bho
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
This formulation allows to highlight the similarity of
the LBL model and the standard model. These two
models differ only by the activation function of their
hidden layer (linear for the LBL model and tangent
hyperbolic for the standard model) and by their def-
inition of the prediction space: for the LBL model,
the context space and the prediction space are the
same (R = Who, and thus H = m), while in the
standard model, the prediction space is defined in-
dependently from the context space. This restriction
drastically reduces the number of free parameters of
the LBL model.
It is finally noteworthy to outline the similarity
of this model with standard maximum entropy lan-
guage models (Lau et al, 1993; Rosenfeld, 1996).
Let x denote the binary vector formed by stacking
the (n-1) 1-of-V encodings of the history words;
then the conditional probability distributions esti-
mated in the model are proportional to expF (x),
where F is an affine transform of x. The main dif-
ference with MaxEnt language models are thus the
restricted form of the feature functions, which only
test one history word, and the particular representa-
tion of F , which is defined as:
F (x) = RWihR
?Tv + Rbih + bho
where, as before, R? is formed by concatenating
(n? 1) copies of the projection matrix R.
2.3 Training and inference
Training the two models introduced above can be
achieved by maximizing the log-likelihood L of the
parameters ?. This optimization is usually per-
formed by stochastic back-propagation as in (Ben-
gio et al, 2003). For all our experiments, the learn-
ing rate is fixed at 5?10?3. The learning weight de-
cay and the the weight decay (respectively 1? 10?9
and 0) seem to have a minor impact on the results.
Learning starts with a random initialization of the
parameters under the uniform distribution and con-
verges to a local maximum of the log-likelihood
function. Moreover, to prevent overfitting, an early
stopping strategy is adopted: after each epoch, train-
ing is stopped when the likelihood of a validation set
stops increasing.
2.4 Complexity issues
The main problem with neural language models is
their computational complexity. For the two mod-
els presented in this section, the number of floating
point operations needed to predict the label of a sin-
gle example is1:
((n? 1) ?m + 1)?H + (H + 1)? V (6)
where the first term of the sum corresponds to the
computation of the hidden layer and the second one
to the computation of the output layer. The projec-
tion of the context words amounts to select one row
of the projection matrix R, as the words are repre-
sented with a 1-of-V coding vector. We can there-
fore assume that the computation complexity of the
first layer is negligible. Most of the computation
time is thus spent in the output layer, which implies
that the computing time grows linearly with the vo-
cabulary size. Training these models for large scale
tasks is therefore challenging, and a number of tricks
have been introduced to make training and inference
tractable (Schwenk and Gauvain, 2002; Schwenk,
2007).
Short list A simple method to reduce the com-
plexity in inference and in learning is to reduce
the size of the output vocabulary (Schwenk, 2007):
rather than estimating the probability P (wl =
w|wl?1l?n+1) for all words in the vocabulary, we only
estimate it for the N most frequent words of the
training set (the so-called short-list). In this case,
two vocabularies need to be considered, correspond-
ing respectively to the context vocabulary Vc used to
define the history; and the prediction vocabulary Vp.
However, this method fails to deliver any probability
estimate for words outside of the prediction vocab-
ulary, meaning that a fall-back strategy needs to be
defined for those words. In practice, neural network
1Recall that learning requires to repeatedly predict the label
for all the examples in the training set.
781
language models are combined with a conventional
n-gram model as described in (Schwenk, 2007).
Batch mode and resampling Additional speed-
ups can be obtained by propagating several exam-
ples at once through the network (Bilmes et al,
1997). This ?batch mode? allows to factorize the
matrix operations and cut down both inference and
training time. In all our experiments, we used a
batch size of 64. Moreover, the training time is lin-
ear in the number of examples in the training data2.
Training on very large corpora, which, nowadays,
comprise billions of word tokens, cannot be per-
formed exhaustively and requires to adopt resam-
pling strategies, whereby, at each epoch, the system
is trained with only a small random subset of the
training data. This approach enables to effectively
estimate neural language models on very large cor-
pora; it has also been observed empirically that sam-
pling the training data can increase the generaliza-
tion performance (Schwenk, 2007).
3 A head-to-head comparison
In this section, we analyze a first experimental
study of the two neural network language models
introduced in Section 2 in order to better under-
stand the differences between these models espe-
cially in terms of the word representations they in-
duce. Based on this study, we will propose, in the
next section, improvements of both the speed and
the prediction capacity of the models. In all our ex-
periments, 4-gram language models are used.
3.1 Corpus
The data we use for training is a large monolingual
corpus, containing all the English texts in the par-
allel data of the Arabic to English NIST 2009 con-
strained task3. It consists of 176 millions word to-
kens with 532, 557 different word types as the size
of vocabulary. The perplexity is computed with re-
spect to the 2006 NIST test data, which is used here
as our development data.
2Equation (6) gives the complexity of inference for a single
example.
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf
3.2 Convergence study
In a first experiment, we trained the two models in
the same setting: we choose to consider a small
vocabulary comprising the 10, 000 most frequent
words. The same vocabulary is used to constrain
the words occurring in the history and the words
to be predicted. The size of hidden layer is set to
m = H = 200, the history contains the 3 preceding
words, we use a batch size of 64, a resampling rate
of 5% and no weight decay.
Figure 1 displays the perplexity convergence
curve measured on the development data for the
standard and the LBL models4. The convergence
perplexities after the combination with the standard
back-off model are also provided for all the mod-
els in table 2 (see section 4.3). We can observe
that the LBL model converges faster than the stan-
dard model: the latter needs 13 epochs to reach
the stopping criteria, while the former only needs
6 epochs. However, upon convergence, the stan-
dard model reaches a lower perplexity than the LBL
model.
0 2 4 6 8 10 12 14120
130
140
150
160
170
180
epochs
per
plex
ity
Perplexity
standardlog bilinear
Figure 1: Convergence rate of the standard and the LBL
models evaluated by the evolution of the perplexity on a
development set
As described in Section 2.2, the main difference
between the standard and the LBL model is the way
the context and the prediction spaces are defined: in
the standard model, the two spaces are distinct; in
4The use of a back-off 4-model estimated with the modified
Knesser-Ney smoothing on the same training data achieves a
perplexity of 141 on the development data.
782
the LBL model, they are bound to be the same. With
a smaller number of parameters, the LBL model can
not capture as many characteristics of the data as the
standard model, but it converges faster5. This differ-
ence in convergence can be explained by the scarcity
of the updates in the projection matrix R in the
standard model: during backpropagation, only those
weights that are associated with words in the history
are updated. By contrast, each training sample up-
dates all the weights in the prediction matrix Who.
3.3 An analysis of the continuous word space
To deepen our understanding, we propose to further
analyze the induced word embeddings by finding,
for some randomly selected words, the five nearest
neighbors (according to the Euclidian distance) in
the context space and in the prediction space of the
two models. Results are presented in Table 1.
If we look first at the standard model, the global
picture is that for frequent words (is, are, and, to
a lesser extend, have), both spaces seem to define
meaningful neighborhood, corresponding to seman-
tic and syntactic similarities; this is less true for rarer
words, where we see a greater discrepancy between
the context and prediction spaces. For instance, the
date 1947 seems to be randomly associated in the
context space, while the 5 nearest words in the pre-
diction space form a consistent set of dates. The
same trend is also observed for the word Castro. Our
interpretation is that for less frequent words, the pro-
jection vectors are hardly ever updated and remain
close to their original random initialization.
By contrast, the similarities in the (unique) pro-
jection space of the LBL remain consistent for all
frequency ranges, and are very similar to the predic-
tion space of the standard model. This seems to val-
idate our hypothesis that in the standard model, the
prediction space is learned much faster than the con-
text space and corroborates our interpretation of the
impact of the scarce updates of rare words. Another
possible explanation is that there is no clear relation
5We could increase the number of parameters of the LBL
model for a fairer comparison with the standard model. How-
ever, this would also increase the size of the vocabulary and
cause two new issues: on one hand, the time complexity would
drastically increase for the LBL model, and on the other hand,
both models would not be comparable in terms of perplexity as
their vocabulary would be different.
between the context space and the target function:
the context space is learned only indirectly by back-
propagation. As a result, due to the random initial-
ization of the parameters and to data sparsity, many
vectors of R might be blocked in some local max-
ima, meaning that similar vectors cannot be grouped
in a consistent way and that the induced similarity is
more ?loose?.
4 Improving the standard model
In Section 3.2, we observed that slightly better re-
sults can be obtained with the standard rather than
with the LBL model. The latter is however much
faster to train, and seems to induce better projection
matrices. Both effects can be attributed to the partic-
ular parameterization of this model, which uses the
same projection matrix both for the context and for
the prediction spaces. In this section, we propose
several new learning regimes that allowed us to im-
prove the standard model in terms of both speed and
prediction capacity. All these improvements rely on
the idea of sharing word representations. While this
idea is not new (see for instance (Collobert and We-
ston, 2008)), our analysis enables to better under-
stand its impact on the convergence rate. Finally, the
improvements we propose are evaluated on a real-
word machine translation task.
4.1 Improving performances with
re-initialization
The experiments reported in the previous section
suggest that it is possible to improve the perfor-
mances of the standard model by building a better
context space. Thus, we introduce a new learning
regime, called re-initialization which aims to im-
prove the context space by re-injecting the informa-
tion on word neighborhoods that emerges in the pre-
diction space. One possible implementation of this
idea is as follows:
1. train a standard model until convergence;
2. use the prediction space of this model to ini-
tialize the context space of a new model; the
prediction space is chosen randomly;
3. train this new model.
783
Table 1: The 5 closest words in the representation spaces of the standard and LBL language models.
word (frequency) model space 5 most closest words
is standard context was are were be been
900, 350 standard prediction was has would had will
LBL both was reveals proves are ON
are standard context were is was be been
478, 440 standard prediction were could will have can
LBL both were is was FOR ON
have standard context had has of also the
465, 417 standard prediction are were provide remain will
LBL both had has Have were embrace
meeting standard context meetings conference them 10 talks
150, 317 standard prediction undertaking seminar meetings gathering project
LBL both meetings summit gathering festival hearing
Imam standard context PCN rebellion 116. Cuba 49
787 standard prediction Castro Sen Nacional Al- Ross
LBL both Salah Khaled Al- Muhammad Khalid
1947 standard context 36 Mercosur definite 2002-2003 era
774 standard prediction 1965 1945 1968 1964 1975
LBL both 1965 1976 1964 1968 1975
Castro standard context exclusively 12. Boucher Zeng Kelly
768 standard prediction Singh Clark da Obasanjo Ross
LBL both Clark Singh Sabri Rafsanjani Sen
Figure 2: Evolution of the perplexity on a development
set for various initialization regimes.
The evolution of the perplexity with respect to train-
ing epochs for this new method is plotted on Fig-
ure 2, where we only represent the evolution of the
perplexity during the third training step. As can be
seen, at convergence, the perplexity the model esti-
mated with this technique is about 10% smaller than
the perplexity of the standard model.
This result can be explained by considering the re-
initialization as a form of annealing technique: re-
initializing the context space allows to escape from
the local extrema the standard model converges to.
The fact that the prediction space provides a good
initialization of the context space also confirms our
analysis that one difficulty with the standard model
is the estimation of the context space parameters.
4.2 Iterative re-initialization
The re-initialization policy introduced in the previ-
ous section significantly reduces the perplexity, at
the expense of a longer training time, as it requires
to successively train two models. As we now know
that the parameters of the prediction space are faster
to converge, we introduce a second training regime
called iterative re-initialization which aims to take
advantage of this property. We summarize this new
training regime as follows:
1. Train the model for one epoch.
2. Use the prediction space parameters to reini-
tialize the context space.
3. Iterate steps (1) and (2) until convergence.
784
Figure 3: Evolution of the perplexity on the training data
for various initialization regimes.
This regimes yields a model that is somewhat in-
between the standard and LBL models as it adds a
relationship between the two representation spaces,
which lacks in the former model. This relationship is
however not expressed through the tying of the cor-
responding parameters; instead we let the prediction
space guide the convergence of the context space.
As a consequence, we hope that it can achieve a con-
vergence speed as fast as the one of the LBL model
without degrading its prediction capacity.
The result plotted on Figure 2 shows that this in-
deed the case: using this training regime, we ob-
tained a perplexity similar to the one of the stan-
dard model, while at the same time reducing the
total training time by more than a half, which is
of great practical interest (each epoch lasts approxi-
mately 8 hours on a 3GHz Xeon processor).
Figure 3 displays the perplexity convergence
curve measured on the training data for the standard
learning regime as well as for the re-initialization
and iterative re-initialization. These results show
the same trend as for the perplexity measured on
the development data, and suggest a regularization
effect of the re-initialization schemes rather than al-
lowing the models to escape local optima.
4.3 One vector initialization
Principle The new training regimes introduced
above outperform the standard training regime both
in terms of perplexity and of training time. However,
exchanging information between the context and
prediction spaces is only possible when the same
vocabulary is used in both spaces. As discussed
in Section 2.4, this configuration is not realistic for
very large-scale tasks. This is because increasing the
number of predicted word types is much more com-
putationally demanding than increasing the number
of types in the context vocabulary. Thus, the former
vocabulary is typically order of magnitudes larger
than the latter, which means that the re-initialization
strategies can no longer be directly used.
It is nonetheless possible to continue drawing in-
spirations from the observations made in Section 3,
and, crucially, to question the random initialization
strategy. As discussed above, this strategy may ex-
plain why the neighborhoods in the induced con-
text space for the less frequent types were diffi-
cult to interpret. As a straightforward alternative,
we consider a different initialization strategy where
all the words in the context vocabulary are initially
projected onto the same (random) point in the con-
text space. The intuition is that it will be easier to
build meaningful neighborhoods, especially for rare
types, if all words are initially considered similar
and only diverge if there is sufficient evidence in the
training data to suggest that they should. This model
is termed the one vector initialization model.
Experimental evaluation To validate this ap-
proach, we compare the convergence of a standard
model trained (with the standard learning regime)
with the one vector initialization regime. The con-
text vocabulary is defined by the 532, 557 words oc-
curring in the training data and the prediction vo-
cabulary by the 10, 000 most frequent words6. All
other parameters are the same as in the previous
experiments. Based on the curves displayed on
Figure 4, we can observe that the model obtained
with the one vector initialization regime outperforms
the model trained with a completely random ini-
tialization. Moreover, the latter reaches conver-
gence in only 14 epochs, while the learning regime
we propose only needs 9 epochs. Convergence is
even faster than when we used the standard training
regime and a small context vocabulary.
6In this case, the distinction between the context and the pre-
diction vocabulary rules out the possibility of a relevant compar-
ison based on perplexity between the continuous space language
model and a standard back-off language model.
785
0 5 10 15100
110
120
130
140
150
160
170
180
epochs
perp
lexit
y
Perplexity
standardone vector initialization
Figure 4: Perplexity with all-10, 000, 200? 200 models
Table 2: Summary of the perplexity (PPX) results mea-
sured on the same development set with the different con-
tinuous space language models. For all of them, the prob-
abilities are combined with the back-off n-gram model
Vc size Model # epochs PPX
10000 log bilinear 6 239
standard 13 227
iterative reinit. 6 223
reinit. 11 211
all standard 14 276
one vector init. 9 260
To illustrate the impact of our initialization
scheme, we also used a principal component anal-
ysis to represent the induced word representations
in a two dimensional space. Figure 5 represents the
vectors associated with numbers7 in red, while all
other words are represented in blue. Two different
models are used: the standard model on the left, and
the one vector initialization model on the right. We
can observe that, for the standard model, most of
the red points are scattered all over a large portion
of the representation space. On the opposite, for
the one vector initialization model, points associated
with numbers are much more concentrated: this is
simply because all the points are originally identi-
cal, and the training aim to spread the point around
this starting point. We also created the closest word
list reported in Table 3, in a manner similar to Ta-
ble 1. Clearly, the new method seems to yield more
7Number are all the words consisting only of digits, with an
optional sign, point or comma such as: 1947; 0,001; -8,2.
(a) with the standard model (b) with the one vector initial-
ization model
Figure 5: Comparison of the word embedding in the con-
text space for numbers (red points).
meaningful neighborhoods in the context space.
It is finally noteworthy to mention that when used
with a small context vocabulary (as in the experi-
mental setting of Section 4.1) this initialization strat-
egy underperforms the standard initialization. This
is simply due to the much greater data sparsity in
the large context vocabulary experiments, where the
rarer word types are really rare (they typically occur
once or twice). By contrast, the rarer words in the
small vocabulary tasks occurred more than several
hundreds times in the training corpus, which was
more than sufficient to guide the model towards sat-
isfactory projection matrices. This finally suggests
that there still exists room for improvement if we
can find more efficient initialization strategies than
starting from one or several random points.
4.4 Statistical machine translation experiments
As a last experiment, we compare the various mod-
els on a large scale machine translation task. Sta-
tistical language models are key component of cur-
rent statistical machine translation systems (Koehn,
2010), where they both help disambiguate lexical
choices in the target language and influence the
choice of the right word ordering. The integration of
a neural network language model in such a system is
far from easy, given the computational cost of com-
puting word probabilities, a task that is performed
repeatedly during the search of the best translation.
We then had to resort to a two pass decoding ap-
proach: the first pass uses a conventional back-off
language model to produce a n-best list (the n most
likely translations and their associated scores); in the
second pass, the probability of the neural language
model is computed for each hypothesis and the n-
786
Table 3: The 5 closest words in the context space of the standard and one vector initialization language models
word (freq.) model 5 closest words
is standard was are were been remains
900, 350 1 vector init. was are be were been
conducted standard undertaken launched $270,900 Mufamadi 6.44-km-long
18, 388 1 vector init. pursued conducts commissioned initiated executed
Cambodian standard Shyorongi $3,192,700 Zairian depreciations teachers
2, 381 1 vector init. Danish Latvian Estonian Belarussian Bangladeshi
automatically standard MSSD Sarvodaya $676,603,059 Kissana 2,652,627
1, 528 1 vector init. routinely occasionally invariably inadvertently seldom
Tosevski standard $12.3 Action,3 Kassouma 3536 Applique
34 1 vector init. Shafei Garvalov Dostiev Bourloyannis-Vrailas Grandi
October-12 standard 39,572 anti-Hutu $12,852,200 non-contracting Party?s
8 1 vector init. March-26 April-11 October-1 June-30 August4
3727th standard Raqu Tatsei Ayatallah Mesyats Langlois
1 1 vector init. 4160th 3651st 3487th 3378th 3558th
best list is accordingly reordered to produce the final
translations.
The different language models discussed in this
article are evaluated on the Arabic to English
NIST 2009 constrained task. For the continuous
space language model, the training data consists
in the parallel corpus used to train the translation
model (previously described in section 3.1). The de-
velopment data is again the 2006 NIST test set and
the test data is the official 2008 NIST test set. Our
system is built using the open-source Moses toolkit
(Koehn et al, 2007) with default settings. To set
up our baseline results, we used an extensively op-
timized standard back-off 4-grams language model
using Kneser-Ney smoothing described in (Allauzen
et al, 2009). The weights used during the reranking
are tuned using the Minimum Error Rate Training
algorithm (Och, 2003). Performance is measured
based on the BLEU (Papineni et al, 2002) scores,
which are reported in Table 4.
Table 4: BLEU scores on the NIST MT08 test set with
different language models.
Vc size Model # epochs BLEU
all baseline - 37.8
10000 log bilinear 6 38.2
standard 13 38.3
iterative reinit. 6 38.4
reinit. 11 38.4
all standard 14 38.6
one vector init. 9 38.7
All the experimented neural language models
yield to a significant BLEU increase. The best re-
sult is obtained by the one vector initialization stan-
dard model which achieves a 0.9 BLEU improve-
ment. While this results is similar to the one ob-
tained with the standard model, the training time is
reduced here by a third.
5 Conclusion
In this work, we proposed three new methods
for training neural network language models and
showed their efficiency both in terms of computa-
tional complexity and generalization performance in
a real-word machine translation task. These meth-
ods rely on conclusions drawn from a careful study
of the convergence rate of two state-of-the-art mod-
els and are based on the idea of sharing the dis-
tributed word representations during training.
Our work highlights the impact of the initializa-
tion and the training scheme for neural network lan-
guage models. Both our experimental results and
our new training methods can be closely related to
the pre-training techniques introduced by (Hinton
and Salakhutdinov, 2006). Our future work will thus
aim at studying the connections between our empir-
ical observations and the deep learning framework.
Acknowledgments
This work was partly realized as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
787
References
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Franc?ois Yvon. 2009. LIMSI?s statistical transla-
tion systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 100?104, Athens, Greece, March. Association
for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
J. Bilmes, K. Asanovic, C. Chin, and J. Demmel. 1997.
Using phipac to speed error back-propagation learn-
ing. Acoustics, Speech, and Signal Processing, IEEE
International Conference on, 5:4153.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing: deep
neural networks with multitask learning. In Proc.
of ICML?08, pages 160?167, New York, NY, USA.
ACM.
Ahmed Emami and Lidia Mangu. 2007. Empirical study
of neural network language models for Arabic speech
recognition. In Proc. ASRU?07, pages 147?152, Ky-
oto. IEEE.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504?507, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL?07, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for arabic speech recognition. In Proc. ICASSP
2010.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modeling using the maxi-
mum entropy principle. In Proc HLT?93, pages 108?
113, Princeton, New Jersey.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proc. ICML ?07, pages 641?648, New York, NY, USA.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Thomas R. Niesler. 1997. Category-based statistical
language models. Ph.D. thesis, University of Cam-
bridge.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL?03, pages
160?167, Sapporo, Japan.
Ilya Oparin, Ondr?ej Glembek, Luka?s? Burget, and Jan
C?ernocky?. 2008. Morphological random forests for
language modeling of inflectional languages. In Proc.
SLT?08, pages 189?192.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. ACL?02, pages
311?318, Philadelphia.
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10:187?228.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proc. ICASSP, pages 765?
768, Orlando, FL.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
Peng Xu and Frederik Jelinek. 2004. Random forests in
language modeling. In Proceedings of EMNLP?2004,
pages 325?332, Barcelona, Spain.
788
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933?943,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Assessing Phrase-Based Translation Models with Oracle Decoding
Guillaume Wisniewski and Alexandre Allauzen and Fran?cois Yvon
Univ. Paris Sud ; LIMSI?CNRS
91403 ORSAY CEDEX
France
{wisniews,allauzen,yvon}@limsi.fr
Abstract
Extant Statistical Machine Translation (SMT) sys-
tems are very complex softwares, which embed mul-
tiple layers of heuristics and embark very large num-
bers of numerical parameters. As a result, it is diffi-
cult to analyze output translations and there is a real
need for tools that could help developers to better
understand the various causes of errors.
In this study, we make a step in that direction and
present an attempt to evaluate the quality of the
phrase-based translation model. In order to identify
those translation errors that stem from deficiencies
in the phrase table (PT), we propose to compute the
oracle BLEU-4 score, that is the best score that a
system based on this PT can achieve on a reference
corpus. By casting the computation of the oracle
BLEU-1 as an Integer Linear Programming (ILP)
problem, we show that it is possible to efficiently
compute accurate lower-bounds of this score, and re-
port measures performed on several standard bench-
marks. Various other applications of these oracle de-
coding techniques are also reported and discussed.
1 Phrase-Based Machine Translation
1.1 Principle
A Phrase-Based Translation System (PBTS) consists of a
ruleset and a scoring function (Lopez, 2009). The ruleset,
represented in the phrase table, is a set of phrase1pairs
{(f, e)}, each pair expressing that the source phrase f
can be rewritten (translated) into a target phrase e. Trans-
lation hypotheses are generated by iteratively rewriting
portions of the source sentence as prescribed by the rule-
set, until each source word has been consumed by exactly
one rule. The order of target words in an hypothesis is
uniquely determined by the order in which the rewrite op-
eration are performed. The search space of the translation
model corresponds to the set of all possible sequences of
1Following the usage in statistical machine translation literature, we
use ?phrase? to denote a subsequence of consecutive words.
rules applications. The scoring function aims to rank all
possible translation hypotheses in such a way that the best
one has the highest score.
A PBTS is learned from a parallel corpus in two inde-
pendent steps. In a first step, the corpus is aligned at the
word level, by using alignment tools such as Giza++
(Och and Ney, 2003) and some symmetrisation heuris-
tics; phrases are then extracted by other heuristics (Koehn
et al, 2003) and assigned numerical weights. In the
second step, the parameters of the scoring function are
estimated, typically through Minimum Error Rate train-
ing (Och, 2003).
Translating a sentence amounts to finding the best scor-
ing translation hypothesis in the search space. Because
of the combinatorial nature of this problem, translation
has to rely on heuristic search techniques such as greedy
hill-climbing (Germann, 2003) or variants of best-first
search like multi-stack decoding (Koehn, 2004). More-
over, to reduce the overall complexity of decoding, the
search space is typically pruned using simple heuristics.
For instance, the state-of-the-art phrase-based decoder
Moses (Koehn et al, 2007) considers only a restricted
number of translations for each source sequence2 and en-
forces a distortion limit3 over which phrases can be re-
ordered. As a consequence, the best translation hypothe-
sis returned by the decoder is not always the one with the
highest score.
1.2 Typology of PBTS Errors
Analyzing the errors of a SMT system is not an easy task,
because of the number of models that are combined, the
size of these models, and the high complexity of the vari-
ous decision making processes. For a SMT system, three
different kinds of errors can be distinguished (Germann
et al, 2004; Auli et al, 2009): search errors, induction
errors and model errors. The former corresponds to cases
where the hypothesis with the best score is missed by
the search procedure, either because of the use of an ap-
2the ttl option of Moses, defaulting to 20.
3the dl option of Moses, whose default value is 7.
933
proximate search method or because of the restrictions of
the search space. Induction errors correspond to cases
where, given the model, the search space does not contain
the reference. Finally, model errors correspond to cases
where the hypothesis with the highest score is not the best
translation according to the evaluation metric.
Model errors encompass several types of errors that oc-
cur during learning (Bottou and Bousquet, 2008)4. Ap-
proximation errors are errors caused by the use of a re-
stricted and oversimplistic class of functions (here, finite-
state transducers to model the generation of hypotheses
and a linear scoring function to discriminate them) to
model the translation process. Estimation errors corre-
spond to the use of sub-optimal values for both the phrase
pairs weights and the parameters of the scoring function.
The reasons behind these errors are twofold: first, train-
ing only considers a finite sample of data; second, it re-
lies on error prone alignments. As a result, some ?good?
phrases are extracted with a small weight, or, in the limit,
are not extracted at all; and conversely that some ?poor?
phrases are inserted into the phrase table, sometimes with
a really optimistic score.
Sorting out and assessing the impact of these various
causes of errors is of primary interest for SMT system
developers: for lack of such diagnoses, it is difficult to
figure out which components of the system require the
most urgent attention. Diagnoses are however, given the
tight intertwining among the various component of a sys-
tem, very difficult to obtain: most evaluations are limited
to the computation of global scores and usually do not
imply any kind of failure analysis.
1.3 Contribution and organization
To systematically assess the impact of the multiple
heuristic decisions made during training and decoding,
we propose, following (Dreyer et al, 2007; Auli et al,
2009), to work out oracle scores, that is to evaluate the
best achievable performances of a PBTS. We aim at both
studying the expressive power of PBTS and at providing
tools for identifying and quantifying causes of failure.
Under standard metrics such as BLEU (Papineni et al,
2002), oracle scores are difficult (if not impossible) to
compute, but, by casting the computation of the oracle
unigram recall and precision as an Integer Linear Pro-
gramming (ILP) problem, we show that it is possible to
efficiently compute accurate lower-bounds of the oracle
BLEU-4 scores and report measurements performed on
several standard benchmarks.
The main contributions of this paper are twofold. We
first introduce an ILP program able to efficiently find
the best hypothesis a PBTS can achieve. This program
can be easily extended to test various improvements to
4We omit here optimization errors.
phrase-base systems or to evaluate the impact of differ-
ent parameter settings. Second, we present a number of
complementary results illustrating the usage of our or-
acle decoder for identifying and analyzing PBTS errors.
Our experimental results confirm the main conclusions of
(Turchi et al, 2008), showing that extant PBTs have the
potential to generate hypotheses having very high BLEU-
4 score and that their main bottleneck is their scoring
function.
The rest of this paper is organized as follows: in Sec-
tion 2, we introduce and formalize the oracle decoding
problem, and present a series of ILP problems of increas-
ing complexity designed so as to deliver accurate lower-
bounds of oracle score. This section closes with various
extensions allowing to model supplementary constraints,
most notably reordering constraints (Section 2.5). Our
experiments are reported in Section 3, where we first in-
troduce the training and test corpora, along with a de-
scription of our system building pipeline (Section 3.1).
We then discuss the baseline oracle BLEU scores (Sec-
tion 3.2), analyze the non-reachable parts of the reference
translations, and comment several complementary results
which allow to identify causes of failures. Section 4 dis-
cuss our approach and findings with respect to the exist-
ing literature on error analysis and oracle decoding. We
conclude and discuss further prospects in Section 5.
2 Oracle Decoder
2.1 The Oracle Decoding Problem
Definition To get some insights on the errors of phrase-
based systems and better understand their limits, we pro-
pose to consider the oracle decoding problem defined as
follows: given a source sentence, its reference transla-
tion5 and a phrase table, what is the ?best? translation
hypothesis a system can generate? As usual, the quality
of an hypothesis is evaluated by the similarity between
the reference and the hypothesis. Note that in the ora-
cle decoding problem, we are only assessing the ability
of PBT systems to generate good candidate translations,
irrespective of their ability to score them properly.
We believe that studying this problem is interesting for
various reasons. First, as described in Section 3.4, com-
paring the best hypothesis a system could have gener-
ated and the hypothesis it actually generates allows us to
carry on both quantitative and qualitative failure analysis.
The oracle decoding problem can also be used to assess
the expressive power of phrase-based systems (Auli et
al., 2009). Other applications include computing accept-
able pseudo-references for discriminative training (Till-
mann and Zhang, 2006; Liang et al, 2006; Arun and
5The oracle decoding problem can be extended to the case of multi-
ple references. For the sake of simplicity, we only describe the case of
a single reference.
934
Koehn, 2007) or combining machine translation systems
in a multi-source setting (Li and Khudanpur, 2009). We
have also used oracle decoding to identify erroneous or
difficult to translate references (Section 3.3).
Evaluation Measure To fully define the oracle de-
coding problem, a measure of the similarity between a
translation hypothesis and its reference translation has
to be chosen. The most obvious choice is the BLEU-4
score (Papineni et al, 2002) used in most machine trans-
lation evaluations.
However, using this metric in the oracle decoding
problem raises several issues. First, BLEU-4 is a met-
ric defined at the corpus level and is hard to interpret at
the sentence level. More importantly, BLEU-4 is not de-
composable6: as it relies on 4-grams statistics, the con-
tribution of each phrase pair to the global score depends
on the translation of the previous and following phrases
and can not be evaluated in isolation. Because of its non-
decomposability, maximizing BLEU-4 is hard; in partic-
ular, the phrase-level decomposability of the evaluation
metric is necessary in our approach.
To circumvent this difficulty, we propose to evaluate
the similarity between a translation hypothesis and a ref-
erence by the number of their common words. This
amounts to evaluating translation quality in terms of un-
igram precision and recall, which are highly correlated
with human judgements (Lavie et al, ). This measure
is closely related to the BLEU-1 evaluation metric and
the Meteor (Banerjee and Lavie, 2005) metric (when it is
evaluated without considering near-matches and the dis-
tortion penalty). We also believe that hypotheses that
maximize the unigram precision and recall at the sen-
tence level yield corpus level BLEU-4 scores close the
maximal achievable. Indeed, in the setting we will intro-
duce in the next section, BLEU-1 and BLEU-4 are highly
correlated: as all correct words of the hypothesis will be
compelled to be at their correct position, any hypothesis
with a high 1-gram precision is also bound to have a high
2-gram precision, etc.
2.2 Formalizing the Oracle Decoding Problem
The oracle decoding problem has already been consid-
ered in the case of word-based models, in which all trans-
lation units are bound to contain only one word. The
problem can then be solved by a bipartite graph matching
algorithm (Leusch et al, 2008): given a n?m binary ma-
trix describing possible translation links between source
words and target words7, this algorithm finds the subset
of links maximizing the number of words of the reference
that have been translated, while ensuring that each word
6Neither at the sentence (Chiang et al, 2008), nor at the phrase level.
7The (i, j) entry of the matrix is 1 if the ith word of the source can
be translated by the jth word of the reference, 0 otherwise.
is translated only once.
Generalizing this approach to phrase-based systems
amounts to solving the following problem: given a set
of possible translation links between potential phrases of
the source and of the target, find the subset of links so that
the unigram precision and recall are the highest possible.
The corresponding oracle hypothesis can then be easily
generated by selecting the target phrases that are aligned
with one source phrase, disregarding the others. In ad-
dition, to mimic the way OOVs are usually handled, we
match identical OOV tokens appearing both in the source
and target sentences. In this approach, the unigram pre-
cision is always one (every word generated in the oracle
hypothesis matches exactly one word in the reference).
As a consequence, to find the oracle hypothesis, we just
have to maximize the recall, that is the number of words
appearing both in the hypothesis and in the reference.
Considering phrases instead of isolated words has a
major impact on the computational complexity: in this
new setting, the optimal segmentations in phrases of both
the source and of the target have to be worked out in ad-
dition to links selection. Moreover, constraints have to
be taken into account so as to enforce a proper segmenta-
tion of the source and target sentences. These constraints
make it impossible to use the approach of (Leusch et al,
2008) and concur in making the oracle decoding prob-
lem for phrase-based models more complex than it is for
word-based models: it can be proven, using arguments
borrowed from (De Nero and Klein, 2008), that this prob-
lem is NP-hard even for the simple unigram precision
measure.
2.3 An Integer Program for Oracle Decoding
To solve the combinatorial problem introduced in the pre-
vious section, we propose to cast it into an Integer Lin-
ear Programming (ILP) problem, for which many generic
solvers exist. ILP has already been used in SMT to find
the optimal translation for word-based (Germann et al,
2001) and to study the complexity of learning phrase
alignments (De Nero and Klein, 2008) models. Follow-
ing the latter reference, we introduce the following vari-
ables: fi,j (resp. ek,l) is a binary indicator variable that
is true when the phrase contains all spans from between-
word position i to j (resp. k to l) of the source (resp.
target) sentence. We also introduce a binary variable, de-
noted ai,j,k,l, to describe a possible link between source
phrase fi,j and target phrase ek,l. These variables are
built from the entries of the phrase table according to se-
lection strategies introduced in Section 2.4. In the fol-
lowing, index variables are so that:
0 ? i < j ? n, in the source sentence and
0 ? k < l ? m, in the target sentence,
935
where n (resp. m) is the length of the source (resp. target)
sentence.
Solving the oracle decoding problem then amounts to
optimizing the following objective function:
max
i,j,k,l
?
i,j,k,l
ai,j,k,l ? (l ? k) , (1)
under the constraints:
?x ? J1,mK :
?
k,l s.t. k?x?l
ek,l ? 1 (2)
?y ? J1, nK :
?
i,j s.t. i?y?j
fi,j = 1 (3)
?k, l :
?
i,j
ai,j,k,l = fk,l (4)
?i, j :
?
k,l
ai,j,k,l = ei,j (5)
The objective function (1) corresponds to the number
of target words that are generated. The first set of con-
straints (2) ensures that each word in the reference e ap-
pears in no more than one phrase. Maximizing the objec-
tive under these constraints amounts to maximizing the
unigram recall. The second set of constraints (3) ensures
that each word in the source f is translated exactly once,
which guarantees that the search space of the ILP prob-
lem is the same as the search space of a phrase-based sys-
tem. Constraints (4) bind the fk,l and ai,j,k,l variables,
ensuring that whenever a link ai,j,k,l is active, the corre-
sponding phrase fk,l is also active. Constraints (5) play a
similar role for the reference.
The Relaxed Problem Even though it accurately
models the search space of a phrase-based decoder,
this programs is not really useful as is: due to out-of-
vocabulary words or missing entries in the phrase table,
the constraint that all source words should be translated
yields infeasible problems8. We propose to relax this
problem and allow some source words to remain untrans-
lated. This is done by replacing constraints (3) by:
?y ? J1, nK :
?
i,j s.t. i?y?j
fi,j ? 1
To better reflect the behavior of phrase-based decoders,
which attempt to translate all source words, we also need
to modify the objective function as follows:
?
i,j,k,l
ai,j,k,l ? (l ? k) +
?
i,j
fi,j ? (j ? i) (6)
The second term in this new objective ensures that opti-
mal solutions translate as many source words as possible.
8An ILP problem is said to be infeasible when every possible solu-
tion violates at least one constraint.
The Relaxed-Distortion Problem A last caveat
with the Relaxed optimization program is caused by
frequently occurring source tokens, such as function
words or punctuation signs, which can often align with
more than one target word. For lack of taking distor-
tion information into account in our objective function,
all these alignments are deemed equivalent, even if some
of them are clearly more satisfactory than others. This
situation is illustrated on Figure 1.
le chat et le chien
the cat and the dog
Figure 1: Equivalent alignments between ?le? and ?the?. The
dashed lines corresponds to a less interpretable solution.
To overcome this difficulty, we propose a last change
to the objective function:
?
i,j,k,l
ai,j,k,l ? (l ? k) +
?
i,j
fi,j ? (j ? i)
??
?
i,j,k,l
ai,j,k,l|k ? i| (7)
Compared to the objective function of the relaxed prob-
lem (6), we introduce here a supplementary penalty factor
which favors monotonous alignments. For each phrase
pair, the higher the difference between source and target
positions, the higher this penalty. If ? is small enough,
this extra term allows us to select, among all the opti-
mal alignments of the relaxed problem, the one with
the lowest distortion. In our experiments, we set ? to
min {n,m} to ensure that the penalty factor is always
smaller than the reward for aligning two single words.
2.4 Selecting Indicator Variables
In the approach introduced in the previous sections, the
oracle decoding problem is solved by selecting, among
a set of possible translation links, the ones that yield the
solution with the highest unigram recall.
We propose two strategies to build this set of possible
translation links. In the first one, denoted exact match,
an indicator ai,j,k,l is created if there is an entry (f, e) so
that f spans from word position i to j in the source and
e from word position k to l in the target. In this strat-
egy, the ILP program considers exactly the same ruleset
as conventional phrase-based decoders.
We also consider an alternative strategy, which could
help us to identify errors made during the phrase extrac-
tion process. In this strategy, denoted inside match, an
indicator ai,j,k,l is created when the following three cri-
teria are met: i) f spans from position i to j of the source;
ii) a substring of e, denoted e?, spans from position k to l
936
of the reference; iii) (f, e?) is not an entry of the phrase ta-
ble. The resulting set of indicator variables thus contains,
at least, all the variables used in the exact match strategy.
In addition, we license here the use of phrases containing
words that do not occur in the reference. In fact, using
such solutions can yield higher BLEU scores when the
reward for additional correct matches exceeds the cost
incurred by wrong predictions. These cases are symp-
toms of situations where the extraction heuristic failed to
extract potentially useful subphrases.
2.5 Oracle Decoding with Reordering Constraints
The ILP problem introduced in the previous section can
be extended in several ways to describe and test various
improvements to phrase-based systems or to evaluate the
impact of different parameter settings. This flexibility
mainly stems from the possibility offered by our frame-
work to express arbitrary constraints over variables. In
this section, we illustrate these possibilities by describing
how reordering constraints can easily be considered.
As a first example, the Moses decoder uses a distortion
limit to constrain the set of possible reorderings. This
constraint ?enforces (...) that the last word of a phrase
chosen for translation cannot be more than d9 words from
the leftmost untranslated word in the source? (Lopez,
2009) and is expressed as:
?aijkl, ai?j?k?l? s.t. k > k
?,
aijkl ? ai?j?k?l? ? |j ? i
? + 1| ? d,
The maximum distortion limit strategy (Lopez, 2009) is
also easily expressed and take the following form (assum-
ing this constraint is parameterized by d):
?l < m? 1,
ai,j,k,l?ai?,j?,l+1,l? ? |i
? ? j ? 1| < d
Implementing the ?local? or MJ-d (Kumar and Byrne,
2005) reordering strategy is also straightforward, and im-
plies using the following constraints:
?i, k,
?
?
?
?
?
?
?
i??i
ai?,j?,k?,l? ?
?
k??k
ai?,j?,k?,l?
?
?
?
?
?
?
? d
Similarly, It is possible to simulate decoding under the
so-called IBM(d) reordering constraints10 by considering
the following constraints:
?? ? m, max
i,k,l
j??
ai,j,k,l ? j ?
?
i,j,k,l
ai,j,k,l ? (j ? i) ? d
9This corresponds to the dl parameter of Moses
10Under IBM(d) constraints, the translation is done, phrase by phrase,
from the beginning of the sentence until the end and only one of the first
d untranslated phrase can be selected for translation.
In these constraints, the first factor corresponds to the
rightmost translated word of the source and the second
one to the number of translated source words. The con-
straints simply enforce that, at each step of the decoding,
there are no more than d source words that were skipped.
Note that the constraints introduced above are not all
linear in the problem variables; however they can eas-
ily be linearized using standard ILP techniques (Roth and
Yih, 2005).
3 Oracle Decoding for Failure Analysis
3.1 Experimental Setting
We propose to use our oracle decoder to study the ability
of a PBTS to translate from English to French and from
German to English. These two languages pairs present
different challenges: English to French translation is con-
sidered a relatively easy pair, notwithstanding the diffi-
culties of generating the right inflection marks in French.
Translating from German into English is more difficult,
notably due to the productivity of inflectional and com-
pounding processes in German, and also to significant
differences in word ordering between these languages.
Our experiments are based on the corpora distributed
for the WMT?09 constrained tasks (Callison-Burch et
al., 2009). All data are tokenized, cleaned and con-
verted to lowercase letters using the tools provided
by the organizers. We then used a standard training
pipeline to construct the translation model: the bitexts
were aligned using Giza++11, symmetrized using the
grow-diag-final-and heuristic; the phrase table
was extracted and scored using the tools distributed with
Moses.12 Finally, baseline systems were optimized using
WMT?08 test set as development using MERT. Note that,
for all these steps, we used the default value of the var-
ious parameters. The extracted phrase table is then used
to find the oracle alignment on the task test set. Recall
that oracle decoding do not use the scores estimated by
Moses in any way.
In the experiments reported below, two settings are
considered. In the first one, denoted NEWSCO, Moses
was trained only on a small data set taken from the News
Commentary corpus. Using a small sized corpus reduces
both training time and decoding time, which allows us to
quickly test different configurations of the decoder. In a
second setting, denoted EUROPARL, Moses was trained
on a larger corpora containing the entirety of the Europarl
Corpus, but no in-domain data, to provide results on more
realistic conditions. Statistics regarding the different cor-
pora used are reported in Table 1. These statistics are
computed on the lowercase cleaned corpora.
11http://www.fjoch.com/GIZA++.html
12http://statmt.org/moses
937
en ? fr de ? en
NEWSCO EUROPARL NEWSCO EUROPARL
#words 1, 023, 401 21, 616, 114 1, 530, 693 22, 898, 644
#sentences 51, 375 1, 050, 398 71, 691 1, 118, 399
#vocabulary 31, 416 78, 071 78, 140 242, 219
#phrase table 3, 061, 701 46, 003, 525 4, 133, 190 44, 402, 367
% OOV 5.3% 3.1% 8.0% 5.2%
Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size and
percentage of test words not appearing in the train set (OOV).
Finding the oracle alignment amounts to solving the
ILP problems introduced above. Even though ILP prob-
lems are NP-hard in general, there exist several off-the-
shelf ILP solvers able to efficiently find an optimal solu-
tion or decide that the problem is infeasible. In our exper-
iments, we used the free solver SCIP (Achterberg, 2007).
An optimal solution was found for all problems we con-
sidered. Decoding the 3, 027 sentences of WMT?09 test
set takes about 10 minutes (wall time) for the NEWSCO
setting, and several hours for the EUROPARL setting13.
3.2 Oracle BLEU Score
Table 2 reports, for all considered settings, the BLEU-4
scores14 achieved by our oracle decoder, as well as the
number of source words used to generate the oracle hy-
pothesis and the number of target words that are reach-
able. In these experiments, two objective functions were
considered: first, we only consider the objective function
corresponding to the relaxed problem defined by Eq. (6);
second, we introduced an extra term in the objective to
penalize distortion, as described by Eq. (7). Unless ex-
plicitly stated otherwise, we always used the exact match
strategy.
The main result in 2 is that, for the two language pairs
considered, the expressive power of PBTS is not the lim-
iting factor to achieve high translation performance. In
fact, for most sentences in the test set, excellent oracle
hypotheses, which contain a very high proportion of ref-
erence words, are found. This remains true even when the
phrase table is extracted from a small corpus. Given that
the best BLEU-4 scores achieved during the WMT?09
evaluation are about 28 for the English to French task
and 24 for the German to English task ((Callison-Burch
et al, 2009), Tables 26 and 25), these results strongly
suggest that the main bottleneck of current phrase-based
translation systems is their scoring function rather than
their expressive power. As we will discuss in Section 4,
similar conclusions were drawn by (Auli et al, 2009) and
(Turchi et al, 2008).
Several additional comments on these numbers are in
13All our experiments are run on a 8 cores computer, each core being
a 2.2GHz Intel Processor; the decoder is multi-threaded.
14These are computed on lowercase with the default tokenization.
order. Despite these very high BLEU scores, in most
cases, the reference is only partly translated. In the most
favorable case, for the English to French EUROPARL set-
ting, only 26% of the references could be fully gener-
ated15. These numbers are consistent with the results re-
ported in (Auli et al, 2009). Similarly, only about 31%
of the source sentences are completely translated by the
oracle decoder, which supports our choice to consider a
relaxed version of the ILP problem. Finally, Table 2 also
shows that introducing the distortion penalty does not af-
fect the oracle performance of the decoder.
Considering the inside match strategy improves the
performance of the oracle decoder: for instance, for the
English to French NEWSCO setting, oracle decoder with
the inside match strategy achieves a BLEU-4 score of
70.15 (a 2.5 points improvement over the baseline). To
achieve this score, 21.45% of the phrases used during de-
coding were phrases that are not considered by the exact
match strategy. Similar results can be observed for other
settings, which highlights the significance of one kind of
failure of the extraction heuristic: useful ?subphrases? of
actual phrase pairs are not always extracted.
The numbers in Table 2, no matter how good they may
look, should be considered with caution: they only imply
that, for most test sentences, all the information necessary
to produce a good translation is available in the phrase ta-
ble. However, the alignment decisions underlying these
oracle hypotheses are sometimes hard to justify, and one
has to accept that part of these good hypotheses transla-
tions are due to a series of lucky alignment errors. This
is illustrated on Figure 2, which displays one such lucky
oracle alignment based on the misalignment, during train-
ing, of the French preposition ?des? (of the) with the En-
glish noun ?stock?. Such lucky errors are naturally also
observed in the outputs of conventional decoders, even
though phrase table filtering heuristics probably makes
them somewhat more rare.
3.3 Analyzing Non-Reachable Parts of a Reference
Table 3 contains typical examples of sentence pairs that
could not be fully generated by our oracle decoder. They
15Similar numbers were obtained, albeit much more slowly, with the
--constraint option of Moses.
938
training set objective function % source translated % target generated 4-BLEU
en ? fr
NEWSCO
RELAXED 86.04% 84.74% 67.65
RELAXED-DISTORTION 85.99% 84.77% 67.77
EUROPARL
RELAXED 93.66% 93.06% 85.05
RELAXED-DISTORTION 93.65% 93.06% 85.08
de ? en
NEWSCO
RELAXED 82.57% 82.33% 64.60
RELAXED-DISTORTION 82.59% 82.30% 64.65
EUROPARL
RELAXED 90.34% 91.16% 81.77
RELAXED-DISTORTION 90.36% 91.12% 81.77
Table 2: Translation score of the ILP oracle decoder for the various settings described in Section 3.1
stock fall in asia
chute des actions en asie
Figure 2: Example of alignment obtained by our oracle decoder
illustrate the three main reasons which cause some parts
of the reference to remain unreachable:
? phrases are missing from the phrase table, either
because they do not occur in the training corpus
(OOVs) or because they failed to be extracted. In
Table 3, OOV errors are mainly due to past tense
forms translated into verbs conjugated in passe? sim-
ple (?rejeta?, ?rencontre`rent?, ?renoua?) a French
literary tense, mostly used in formal writings.
? obvious errors (misspelled words, misinterpretation
or mistranslation, ...) in the reference. The refer-
ence of the fifth example contains one such error:
the state name ?Nevada? is translated to ?n?e?vadiez?
(literally ?have not escaped?), yielding a very poor
reference sentence.
? parts of the reference have no translation equiva-
lence in the source. This can be either because ref-
erences are produced in ?context? and some pieces
of information are moved across sentence bound-
aries or because these references are non-literal. The
fourth example, which seems to be the translation of
a title, falls into this category: the French part con-
tains a reference to the context (?les SA? is referring
to the bacteria the text is talking about) which is not
in the source text. Non-literal translation are illus-
trated by the third example, where English ?Mon-
day? is translated into French ?la veille? (the day
before).
While the first kind of errors is inherent to the use of
a statistical approach, the last two kinds result from the
quality of the data used in the evaluation and directly im-
pact both training and evaluation of automatic translation
systems: if they should not distort too much comparisons
of MT systems, these errors prevent us from assessing
the ?global? quality of automatic translation and, if sim-
ilar errors are found in the train set, they make learning
harder as some probability mass is wasted to model them.
To provide a more quantitative analysis, we manually
looked at all the non-aligned parts of some WMT?09 ref-
erences and found that out of 800 references, more than
133 contain either an obvious translation error or can not
be achieved by a PBTS16. Note that, while identifying
these errors could be done in many ways, our oracle de-
coder makes it far easier.
3.4 Identifying Causes of Failure
By comparing the hypotheses found by the oracle de-
coder and the ones found by the phrase-based decoder,
causes of failure can be easily identified. In this section,
we will present several measures that allow us to identify
and quantify several causes of failure.
Errors Caused by Search Space Pruning Recall from
Section 1.1 that Moses uses several heuristics to prune the
search space. In particular, there is a distortion limit and
a limit on the number of target phrases considered for one
source phrase. In this paragraph, we evaluate the impact
of these two heuristics on translation quality.
Table 4 presents the average distortion computed on
the oracle hypotheses, as well as the percentage of
phrases used that have a distortion strictly greater than
6 (the default distortion limit of Moses). All these num-
bers are obtained by solving the RELAXED-DISTORTION
problem. Surprisingly enough, the average distortion of
oracle hypotheses is quite small, even for the German to
English task, and the distortion constraint seems to be vi-
olated only in a few cases. It also appears that the distor-
tion of the hypotheses generated in the NEWSCO setting
is significantly larger than in the EUROPARL setting. This
can be explained by the extra degrees of freedom in the
16Annotation at a finer level is an on-going effort; the annotated
corpus is available from http://www.limsi.fr/Individu/
wisniews/oracle decoding.
939
? ? On Monday the American House of Representatives rejected the plan to support the financial
system, into which up to 700 billion dollars (nearly 12 billion Czech crowns) was to be invested.
? Lundi, la chambre des repre?sentants ame?ricaine rejeta le projet de soutient du syste`me financier,
auquel elle aurait du? consacrer jusqu?a` 700 milliards de dollars (pre`s de 12 bilions de kc?).
? ? Representatives of the legislators met with American Finance Minister Henry Paulson Saturday
night in order to give the government fund a final form.
? Dans la nuit de samedi a` dimanche, des repre?sentants des le?gislateurs rencontre`rent le ministre
des finances ame?ricain Henry Paulson, afin de donner au fond du gouvernement une forme finale.
? ? The Prague Stock Market immediately continued its fall from Monday at the beginning of
Tuesday?s trading , when it dropped by nearly six percent.
? Mardi, de`s le de?but des e?changes, la bourse de prague renoua avec sa chute de la veille,
lorsqu?elle perdait presque six pour cent.
? ? Antibiotic Resistance
? Les SA re?sistent aux antibiotiques.
? ? According to Nevada Democratic senator Harry Reid, that is how that legislators are trying to
have Congress to reach a definitive agreement as early as on Sunday.
? D?apre`s le se?nateur de`mocrate n?e?vadiez Harry Reid, les le?gislateurs font de sorte que le Congre`s
aboutisse a` un accord de?finitif de`s dimanche.
Table 3: Output examples of our oracle decoder on the English to French task. Words in bold are non-aligned words and words in
italic are non-aligned out-of-vocabulary words. For clarity the examples have been detokenized and recased.
training set avg.
distortion
%phrases
with a dist.
> 6
en ? fr
NEWSCO 4.57 22.02%
EUROPARL 3.21 13.32%
de ? en
NEWSCO 5.16 25.37%
EUROPARL 3.81 17.21%
Table 4: Average distortion and percentage of phrases with a
distortion greater that Moses default distortion limit.
alignment decisions enabled by the use of larger training
corpora and phrase table.
To evaluate the impact of the second heuristic, we com-
puted the number of phrases discarded by Moses (be-
cause of the default ttl limit) but used in the oracle hy-
potheses. In the English to French NEWSCO setting,
they account for 34.11% of the total number of phrases
used in the oracle hypotheses. When the oracle decoder
is constrained to use the same phrase table as Moses, its
BLEU-4 score drops to 42.78. This shows that filtering
the phrase table prior to decoding discards many useful
phrase pairs and is seriously limiting the best achievable
performance, a conclusion shared with (Auli et al, 2009).
Search Errors Search errors can be identified by com-
paring the score of the best hypothesis found by Moses
and the score of the oracle hypothesis. If the score of the
oracle hypothesis is higher, then there has been a search
error; on the contrary, there has been an estimation error
when the score of the oracle hypothesis is lower than the
score of the best hypothesis found by Moses.
Based on the comparison of the score of Moses hy-
potheses and of oracle hypotheses for the English to
French NEWSCO setting, our preliminary conclusion is
that the number of search errors is quite limited: only
about 5% of the hypotheses of our oracle decoder are ac-
tually getting a better score than Moses solutions. Again,
this shows that the scoring function (model error) is
one of the main bottleneck of current PBTS. Compar-
ing these hypotheses is nonetheless quite revealing: while
Moses mostly selects phrase pairs with high translation
scores and generates monotonous alignments, our ILP de-
coder uses larger reorderings and less probable phrases
to achieve better solutions: on average, the reordering
score of oracle solutions is ?5.74, compared to ?76.78
for Moses outputs. Given the weight assigned through
MERT training to the distortion score, no wonder that
these hypotheses are severely penalized.
The Impact of Phrase Length The observed outputs
do not only depend on decisions made during the search,
but also on decisions made during training. One such
decision is the specification of maximal length for the
source and target phrases. In our framework, evaluating
the impact of this decision is simple: it suffices to change
the definition of indicator variables so as to consider only
alignments between phrases of a given length.
In the English-French NEWSCO setting, the most re-
strictive choice, when only alignments between single
words are authorized, yields an oracle BLEU-4 of 48.68;
however, authorizing phrases up to length 2 allows to
achieve an oracle value of 66.57, very close to the score
achieved when considering all extracted phrases (67.77).
940
This is corroborated with a further analysis of our ora-
cle alignments, which use phrases whose average source
length is 1.21 words (respectively 1.31 for target words).
If many studies have already acknowledged the predomi-
nance of ?small? phrases in actual translations, our oracle
scores suggest that, for this language pair, increasing the
phrase length limit beyond 2 or 3 might be a waste of
computational resources.
4 Related Work
To the best of our knowledge, there are only a few works
that try to study the expressive power of phrase-based ma-
chine translation systems or to provide tools for analyzing
potential causes of failure.
The approach described in (Auli et al, 2009) is very
similar to ours: in this study, the authors propose to find
and analyze the limits of machine translation systems by
studying the reference reachability. A reference is reach-
able for a given system if it can be exactly generated
by this system. Reference reachability is assessed using
Moses in forced decoding mode: during search, all hy-
potheses that deviate from the reference are simply dis-
carded. Even though the main goal of this study was to
compare the search space of phrase-based and hierarchi-
cal systems, it also provides some insights on the impact
of various search parameters in Moses, delivering con-
clusions that are consistent with our main results. As de-
scribed in Section 1.2, these authors also propose a typol-
ogy of the errors of a statistical translation systems, but
do not attempt to provide methods for identifying them.
The authors of (Turchi et al, 2008) study the learn-
ing capabilities of Moses by extensively analyzing learn-
ing curves representing the translation performances as a
function of the number of examples, and by corrupting
the model parameters. Even though their focus is more
on assessing the scoring function, they reach conclusions
similar to ours: the current bottleneck of translation per-
formances is not the representation power of the PBTS
but rather in their scoring functions.
Oracle decoding is useful to compute reachable
pseudo-references in the context of discriminative train-
ing. This is the main motivation of (Tillmann and Zhang,
2006), where the authors compute high BLEU hypothe-
ses by running a conventional decoder so as to maximize
a per-sentence approximation of BLEU-4, under a simple
(local) reordering model.
Oracle decoding has also been used to assess the
limitations induced by various reordering constraints in
(Dreyer et al, 2007). To this end, the authors propose
to use a beam-search based oracle decoder, which com-
putes lower bounds of the best achievable BLEU-4 us-
ing dynamic programming techniques over finite-state
(for so-called local and IBM constraints) or hierarchically
structured (for ITG constraints) sets of hypotheses. Even
though the numbers reported in this study are not directly
comparable with ours17, it seems that our decoder is not
only conceptually much simpler, but also achieves much
more optimistic lower-bounds of the oracle BLEU score.
The approach described in (Li and Khudanpur, 2009) em-
ploys a similar technique, which is to guide a heuristic
search in an hypergraph representing possible translation
hypotheses with n-gram counts matches, which amounts
to decoding with a n-gram model trained on the sole ref-
erence translation. Additional tricks are presented in this
article to speed-up decoding.
Computing oracle BLEU scores is also the subject of
(Zens and Ney, 2005; Leusch et al, 2008), yet with a
different emphasis. These studies are concerned with
finding the best hypotheses in a word graph or in a con-
sensus network, a problem that has various implications
for multi-pass decoding and/or system combination tech-
niques. The former reference describes an exponential
approximate algorithm, while the latter proves the NP-
completeness of this problem and discuss various heuris-
tic approaches. Our problem is somewhat more complex
and using their techniques would require us to built word
graphs containing all the translations induced by arbitrary
segmentations and permutations of the source sentence.
5 Conclusions
In this paper, we have presented a methodology for ana-
lyzing the errors of PBTS, based on the computation of
an approximation of the BLEU-4 oracle score. We have
shown that this approximation could be computed fairly
accurately and efficiently using Integer Linear Program-
ming techniques. Our main result is a confirmation of
the fact that extant PBTS systems are expressive enough
to achieve very high translation performance with respect
to conventional quality measurements. The main efforts
should therefore strive to improve on the way phrases and
hypotheses are scored during training. This gives further
support to attempts aimed at designing context-dependent
scoring functions as in (Stroppa et al, 2007; Gimpel and
Smith, 2008), or at attempts to perform discriminative
training of feature-rich models. (Bangalore et al, 2007).
We have shown that the examination of difficult-to-
translate sentences was an effective way to detect errors
or inconsistencies in the reference translations, making
our approach a potential aid for controlling the quality or
assessing the difficulty of test data. Our experiments have
also highlighted the impact of various parameters.
Various extensions of the baseline ILP program have
been suggested and/or evaluated. In particular, the ILP
formalism lends itself well to expressing various con-
straints that are typically used in conventional PBTS. In
17The best BLEU-4 oracle they achieve on Europarl German to En-
glish is approximately 48; but they considered a smaller version of the
training corpus and the WMT?06 test set.
941
our future work, we aim at using this ILP framework to
systematically assess various search configurations. We
plan to explore how replacing non-reachable references
with high-score pseudo-references can improve discrim-
inative training of PBTS. We are also concerned by de-
termining how tight is our approximation of the BLEU-
4 score is: to this end, we intend to compute the best
BLEU-4 score within the n-best solutions of the oracle
decoding problem.
Acknowledgments
Warm thanks to Houda Bouamor for helping us with the
annotation tool. This work has been partly financed by
OSEO, the French State Agency for Innovation, under
the Quaero program.
References
Tobias Achterberg. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universita?t Berlin.
http://opus.kobv.de/tuberlin/volltexte/
2007/1611/.
Abhishek Arun and Philipp Koehn. 2007. Online learning
methods for discriminative training of phrase based statis-
tical machine translation. In Proc. of MT Summit XI, Copen-
hagen, Denmark.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn.
2009. A systematic analysis of translation model search
spaces. In Proc. of WMT, pages 224?232, Athens, Greece.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved correla-
tion with human judgments. In Proc. of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak.
2007. Statistical machine translation through global lexi-
cal selection and sentence reconstruction. In Proc. of ACL,
pages 152?159, Prague, Czech Republic.
Le?on Bottou and Olivier Bousquet. 2008. The tradeoffs of large
scale learning. In Proc. of NIPS, pages 161?168, Vancouver,
B.C., Canada.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh
Schroeder. 2009. Findings of the 2009 Workshop on Sta-
tistical Machine Translation. In Proc. of WMT, pages 1?28,
Athens, Greece.
David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou
Ng. 2008. Decomposability of translation metrics for
improved evaluation and efficient algorithms. In Proc. of
ECML, pages 610?619, Honolulu, Hawaii.
John De Nero and Dan Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL: HLT, Short Papers,
pages 25?28, Columbus, Ohio.
Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007.
Comparing reordering constraints for smt using efficient bleu
oracle computation. In NAACL-HLT/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages 103?
110, Rochester, New York.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proc. of ACL, pages 228?235,
Toulouse, France.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2004. Fast and optimal decoding for
machine translation. Artificial Intelligence, 154(1-2):127?
143.
Ulrich Germann. 2003. Greedy decoding for statistical ma-
chine translation in almost linear time. In Proc. of NAACL,
pages 1?8, Edmonton, Canada.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-side
context for statistical machine translation. In Proc. of WMT,
pages 9?17, Columbus, Ohio.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. of NAACL, pages
48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proc. of ACL, demonstration session.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Proc.
of AMTA, pages 115?124, Washington DC.
Shankar Kumar and William Byrne. 2005. Local phrase re-
ordering models for statistical machine translation. In Proc.
of HLT, pages 161?168, Vancouver, Canada.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. The
significance of recall in automatic metrics for MT evaluation.
In In Proc. of AMTA, pages 134?143, Washington DC.
Gregor Leusch, Evgeny Matusov, and Hermann Ney. 2008.
Complexity of finding the BLEU-optimal hypothesis in a
confusion network. In Proc. of EMNLP, pages 839?847,
Honolulu, Hawaii.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction
of oracle-best translations from hypergraphs. In Proc. of
NAACL, pages 9?12, Boulder, Colorado.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL, pages 761?768, Sydney,
Australia.
Adam Lopez. 2009. Translation as weighted deduction. In
Proc. of EACL, pages 532?540, Athens, Greece.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Comput.
Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan.
Kishore Papineni, Salim Roukos, ToddWard, andWei-jing Zhu.
2002. Bleu: A method for automatic evaluation of machine
translation. Technical report, Philadelphia, Pennsylvania.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML, pages
737?744, Bonn, Germany.
Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for smt using context-informed
942
features. In Andy Way and Barbara Gawronska, editors,
Proc. of TMI, pages 231?240, Sko?vde, Sweden.
Christoph Tillmann and Tong Zhang. 2006. A discriminative
global training algorithm for statistical mt. In Proc. of ACL,
pages 721?728, Sydney, Australia.
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learn-
ing performance of a machine translation system: a statistical
and computational analysis. In Proc. of WMT, pages 35?43,
Columbus, Ohio.
Richard Zens and Hermann Ney. 2005. Word graphs for sta-
tistical machine translation. In Proc. of the ACL Workshop
on Building and Using Parallel Texts, pages 191?198, Ann
Arbor, Michigan.
943
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120?129,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Computing Lattice BLEU Oracle Scores for Machine Translation
Artem Sokolov Guillaume Wisniewski
LIMSI-CNRS & Univ. Paris Sud
BP-133, 91 403 Orsay, France
{firstname.lastname}@limsi.fr
Franc?ois Yvon
Abstract
The search space of Phrase-Based Statisti-
cal Machine Translation (PBSMT) systems
can be represented under the form of a di-
rected acyclic graph (lattice). The quality
of this search space can thus be evaluated
by computing the best achievable hypoth-
esis in the lattice, the so-called oracle hy-
pothesis. For common SMT metrics, this
problem is however NP-hard and can only
be solved using heuristics. In this work,
we present two new methods for efficiently
computing BLEU oracles on lattices: the
first one is based on a linear approximation
of the corpus BLEU score and is solved us-
ing the FST formalism; the second one re-
lies on integer linear programming formu-
lation and is solved directly and using the
Lagrangian relaxation framework. These
new decoders are positively evaluated and
compared with several alternatives from the
literature for three language pairs, using lat-
tices produced by two PBSMT systems.
1 Introduction
The search space of Phrase-Based Statistical Ma-
chine Translation (PBSMT) systems has the form
of a very large directed acyclic graph. In several
softwares, an approximation of this search space
can be outputted, either as a n-best list contain-
ing the n top hypotheses found by the decoder, or
as a phrase or word graph (lattice) which com-
pactly encodes those hypotheses that have sur-
vived search space pruning. Lattices usually con-
tain much more hypotheses than n-best lists and
better approximate the search space.
Exploring the PBSMT search space is one of
the few means to perform diagnostic analysis and
to better understand the behavior of the system
(Turchi et al 2008; Auli et al 2009). Useful
diagnostics are, for instance, provided by look-
ing at the best (oracle) hypotheses contained in
the search space, i.e, those hypotheses that have
the highest quality score with respect to one or
several references. Such oracle hypotheses can
be used for failure analysis and to better under-
stand the bottlenecks of existing translation sys-
tems (Wisniewski et al 2010). Indeed, the in-
ability to faithfully reproduce reference transla-
tions can have many causes, such as scantiness
of the translation table, insufficient expressiveness
of reordering models, inadequate scoring func-
tion, non-literal references, over-pruned lattices,
etc. Oracle decoding has several other applica-
tions: for instance, in (Liang et al 2006; Chi-
ang et al 2008) it is used as a work-around to
the problem of non-reachability of the reference
in discriminative training of MT systems. Lattice
reranking (Li and Khudanpur, 2009), a promising
way to improve MT systems, also relies on oracle
decoding to build the training data for a reranking
algorithm.
For sentence level metrics, finding oracle hy-
potheses in n-best lists is a simple issue; how-
ever, solving this problem on lattices proves much
more challenging, due to the number of embed-
ded hypotheses, which prevents the use of brute-
force approaches. When using BLEU, or rather
sentence-level approximations thereof, the prob-
lem is in fact known to be NP-hard (Leusch et
al., 2008). This complexity stems from the fact
that the contribution of a given edge to the total
modified n-gram precision can not be computed
without looking at all other edges on the path.
Similar (or worse) complexity result are expected
120
for other metrics such as METEOR (Banerjee and
Lavie, 2005) or TER (Snover et al 2006). The
exact computation of oracles under corpus level
metrics, such as BLEU, poses supplementary com-
binatorial problems that will not be addressed in
this work.
In this paper, we present two original methods
for finding approximate oracle hypotheses on lat-
tices. The first one is based on a linear approxima-
tion of the corpus BLEU, that was originally de-
signed for efficient Minimum Bayesian Risk de-
coding on lattices (Tromble et al 2008). The sec-
ond one, based on Integer Linear Programming, is
an extension to lattices of a recent work on failure
analysis for phrase-based decoders (Wisniewski
et al 2010). In this framework, we study two
decoding strategies: one based on a generic ILP
solver, and one, based on Lagrangian relaxation.
Our contribution is also experimental as we
compare the quality of the BLEU approxima-
tions and the time performance of these new ap-
proaches with several existing methods, for differ-
ent language pairs and using the lattice generation
capacities of two publicly-available state-of-the-
art phrase-based decoders: Moses1 and N-code2.
The rest of this paper is organized as follows.
In Section 2, we formally define the oracle decod-
ing task and recall the formalism of finite state
automata on semirings. We then describe (Sec-
tion 3) two existing approaches for solving this
task, before detailing our new proposals in sec-
tions 4 and 5. We then report evaluations of the
existing and new oracles on machine translation
tasks.
2 Preliminaries
2.1 Oracle Decoding Task
We assume that a phrase-based decoder is able
to produce, for each source sentence f , a lattice
Lf = ?Q,??, with # {Q} vertices (states) and
# {?} edges. Each edge carries a source phrase
fi, an associated output phrase ei as well as a fea-
ture vector h?i, the components of which encode
various compatibility measures between fi and ei.
We further assume that Lf is a word lattice,
meaning that each ei carries a single word3 and
1http://www.statmt.org/moses/
2http://ncode.limsi.fr/
3Converting a phrase lattice to a word lattice is a simple
matter of redistributing a compound input or output over a
that it contains a unique initial state q0 and a
unique final state qF . Let ?f denote the set of all
paths from q0 to qF in Lf . Each path pi ? ?f cor-
responds to a possible translation epi. The job of
a (conventional) decoder is to find the best path(s)
in Lf using scores that combine the edges? fea-
ture vectors with the parameters ?? learned during
tuning.
In oracle decoding, the decoder?s job is quite
different, as we assume that at least a reference
rf is provided to evaluate the quality of each indi-
vidual hypothesis. The decoder therefore aims at
finding the path pi? that generates the hypothesis
that best matches rf . For this task, only the output
labels ei will matter, the other informations can be
left aside.4
Oracle decoding assumes the definition of a
measure of the similarity between a reference
and a hypothesis. In this paper we will con-
sider sentence-level approximations of the popu-
lar BLEU score (Papineni et al 2002). BLEU is
formally defined for two parallel corpora, E =
{ej}Jj=1 and R = {rj}
J
j=1, each containing J
sentences as:
n-BLEU(E ,R) = BP ?
( n?
m=1
pm
)1/n
, (1)
where BP = min(1, e1?c1(R)/c1(E)) is the
brevity penalty and pm = cm(E ,R)/cm(E) are
clipped or modified m-gram precisions: cm(E) is
the total number of wordm-grams in E ; cm(E ,R)
accumulates over sentences the number of m-
grams in ej that also belong to rj . These counts
are clipped, meaning that a m-gram that appears
k times in E and l times in R, with k > l, is only
counted l times. As it is well known, BLEU per-
forms a compromise between precision, which is
directly appears in Equation (1), and recall, which
is indirectly taken into account via the brevity
penalty. In most cases, Equation (1) is computed
with n = 4 and we use BLEU as a synonym for
4-BLEU.
BLEU is defined for a pair of corpora, but, as an
oracle decoder is working at the sentence-level, it
should rely on an approximation of BLEU that can
linear chain of arcs.
4The algorithms described below can be straightfor-
wardly generalized to compute oracle hypotheses under
combined metrics mixing model scores and quality measures
(Chiang et al 2008), by weighting each edge with its model
score and by using these weights down the pipe.
121
evaluate the similarity between a single hypoth-
esis and its reference. This approximation intro-
duces a discrepancy as gathering sentences with
the highest (local) approximation may not result
in the highest possible (corpus-level) BLEU score.
Let BLEU? be such a sentence-level approximation
of BLEU. Then lattice oracle decoding is the task
of finding an optimal path pi?(f) among all paths
?f for a given f , and amounts to the following
optimization problem:
pi?(f) = arg max
pi??f
BLEU?(epi, rf ). (2)
2.2 Compromises of Oracle Decoding
As proved by Leusch et al(2008), even with
brevity penalty dropped, the problem of deciding
whether a confusion network contains a hypoth-
esis with clipped uni- and bigram precisions all
equal to 1.0 is NP-complete (and so is the asso-
ciated optimization problem of oracle decoding
for 2-BLEU). The case of more general word and
phrase lattices and 4-BLEU score is consequently
also NP-complete. This complexity stems from
chaining up of local unigram decisions that, due
to the clipping constraints, have non-local effect
on the bigram precision scores. It is consequently
necessary to keep a possibly exponential num-
ber of non-recombinable hypotheses (character-
ized by counts for each n-gram in the reference)
until very late states in the lattice.
These complexity results imply that any oracle
decoder has to waive either the form of the objec-
tive function, replacing BLEU with better-behaved
scoring functions, or the exactness of the solu-
tion, relying on approximate heuristic search al-
gorithms.
In Table 1, we summarize different compro-
mises that the existing (section 3), as well as
our novel (sections 4 and 5) oracle decoders,
have to make. The ?target? and ?target level?
columns specify the targeted score. None of
the decoders optimizes it directly: their objec-
tive function is rather the approximation of BLEU
given in the ?target replacement? column. Col-
umn ?search? details the accuracy of the target re-
placement optimization. Finally, columns ?clip-
ping? and ?brevity? indicate whether the corre-
sponding properties of BLEU score are considered
in the target substitute and in the search algorithm.
2.3 Finite State Acceptors
The implementations of the oracles described in
the first part of this work (sections 3 and 4) use the
common formalism of finite state acceptors (FSA)
over different semirings and are implemented us-
ing the generic OpenFST toolbox (Allauzen et al
2007).
A (?,?)-semiring K over a set K is a system
?K,?,?, 0?, 1??, where ?K,?, 0?? is a commutative
monoid with identity element 0?, and ?K,?, 1?? is
a monoid with identity element 1?. ? distributes
over ?, so that a ? (b ? c) = (a ? b) ? (a ? c)
and (b? c)? a = (b? a)? (c? a) and element
0? annihilates K (a? 0? = 0?? a = 0?).
Let A = (?, Q, I, F,E) be a weighted finite-
state acceptor with labels in ? and weights in K,
meaning that the transitions (q, ?, q?) in A carry a
weight w ? K. Formally, E is a mapping from
(Q ? ? ? Q) into K; likewise, initial I and fi-
nal weight F functions are mappings from Q into
K. We borrow the notations of Mohri (2009):
if ? = (q, a, q?) is a transition in domain(E),
p(?) = q (resp. n(?) = q?) denotes its origin
(resp. destination) state, w(?) = ? its label and
E(?) its weight. These notations extend to paths:
if pi is a path in A, p(pi) (resp. n(pi)) is its initial
(resp. ending) state and w(pi) is the label along
the path. A finite state transducer (FST) is an FSA
with output alphabet, so that each transition car-
ries a pair of input/output symbols.
As discussed in Sections 3 and 4, several oracle
decoding algorithms can be expressed as shortest-
path problems, provided a suitable definition of
the underlying acceptor and associated semiring.
In particular, quantities such as:
?
pi??(A)
E(pi), (3)
where the total weight of a successful path pi =
?1 . . . ?l in A is computed as:
E(pi) =I(p(?1))?
[
l?
i=1
E(?i)
]
? F (n(?l))
can be efficiently found by generic shortest dis-
tance algorithms over acyclic graphs (Mohri,
2002). For FSA-based implementations over
semirings where ? = max, the optimization
problem (2) is thus reduced to Equation (3), while
the oracle-specific details can be incorporated into
in the definition of ?.
122
oracle target target level target replacement search clipping brevity
ex
is
ti
ng LM-2g/4g 2/4-BLEU sentence P2(e; r) or P4(e; r) exact no no
PB 4-BLEU sentence partial log BLEU (4) appr. no no
PB` 4-BLEU sentence partial log BLEU (4) appr. no yes
th
is
pa
pe
r LB-2g/4g 2/4-BLEU corpus linear appr. lin BLEU (5) exact no yes
SP 1-BLEU sentence unigram count exact no yes
ILP 2-BLEU sentence uni/bi-gram counts (7) appr. yes yes
RLX 2-BLEU sentence uni/bi-gram counts (8) exact yes yes
Table 1: Recapitulative overview of oracle decoders.
3 Existing Algorithms
In this section, we describe our reimplementation
of two approximate search algorithms that have
been proposed in the literature to solve the oracle
decoding problem for BLEU. In addition to their
approximate nature, none of them accounts for the
fact that the count of each matching word has to
be clipped.
3.1 Language Model Oracle (LM)
The simplest approach we consider is introduced
in (Li and Khudanpur, 2009), where oracle decod-
ing is reduced to the problem of finding the most
likely hypothesis under a n-gram language model
trained with the sole reference translation.
Let us suppose we have a n-gram language
model that gives a probability P (en|e1 . . . en?1)
of word en given the n? 1 previous words.
The probability of a hypothesis e is then
Pn(e|r) =
?
i=1 P (ei+n|ei . . . ei+n?1). The lan-
guage model can conveniently be represented as a
FSA ALM , with each arc carrying a negative log-
probability weight and with additional ?-type fail-
ure transitions to accommodate for back-off arcs.
If we train, for each source sentence f , a sepa-
rate language model ALM (rf ) using only the ref-
erence rf , oracle decoding amounts to finding a
shortest (most probable) path in the weighted FSA
resulting from the composition L ?ALM (rf ) over
the (min,+)-semiring:
pi?LM (f) = ShortestPath(L ?ALM (rf )).
This approach replaces the optimization of n-
BLEU with a search for the most probable path
under a simplistic n-gram language model. One
may expect the most probable path to select fre-
quent n-gram from the reference, thus augment-
ing n-BLEU.
3.2 Partial BLEU Oracle (PB)
Another approach is put forward in (Dreyer et
al., 2007) and used in (Li and Khudanpur, 2009):
oracle translations are shortest paths in a lattice
L, where the weight of each path pi is the sen-
tence level log BLEU(pi) score of the correspond-
ing complete or partial hypothesis:
log BLEU(pi) =
1
4
?
m=1...4
log pm. (4)
Here, the brevity penalty is ignored and n-
gram precisions are offset to avoid null counts:
pm = (cm(epi, r) + 0.1)/(cm(epi) + 0.1).
This approach has been reimplemented using
the FST formalism by defining a suitable semir-
ing. Let each weight of the semiring keep a set
of tuples accumulated up to the current state of
the lattice. Each tuple contains three words of re-
cent history, a partial hypothesis as well as current
values of the length of the partial hypothesis, n-
gram counts (4 numbers) and the sentence-level
log BLEU score defined by Equation (4). In the
beginning each arc is initialized with a singleton
set containing one tuple with a single word as the
partial hypothesis. For the semiring operations we
define one common?-operation and two versions
of the ?-operation:
? L1 ?PB L2 ? appends a word on the edge of
L2 to L1?s hypotheses, shifts their recent histories
and updates n-gram counts, lengths, and current
score; ? L1 ?PB L2 ? merges all sets from L1
and L2 and recombinates those having the same
recent history; ? L1 ?PB` L2 ? merges all sets
from L1 and L2 and recombinates those having
the same recent history and the same hypothesis
length.
If several hypotheses have the same recent
history (and length in the case of ?PB`), re-
combination removes all of them, but the one
123
q?
0:0/01:1/0
(a) ?1
q?
00:/10 
:/10
0:0010
:0100:010
:10
(b) ?2
q?
0
0:/10
:/10
0:/10 00
0:/10
00:/10 
:/10
:010
0:0010
:010
0:00100:010
:10
:0010
0:00010
(c) ?3
Figure 1: Examples of the ?n automata for ? = {0, 1} and n = 1 . . . 3. Initial and final states are marked,
respectively, with bold and with double borders. Note that arcs between final states are weighted with 0, while in
reality they will have this weight only if the corresponding n-gram does not appear in the reference.
with the largest current BLEU score. Optimal
path is then found by launching the generic
ShortestDistance(L) algorithm over one of
the semirings above.
The (?PB`,?PB)-semiring, in which the
equal length requirement also implies equal
brevity penalties, is more conservative in recom-
bining hypotheses and should achieve final BLEU
that is least as good as that obtained with the
(?PB,?PB)-semiring5.
4 Linear BLEU Oracle (LB)
In this section, we propose a new oracle based on
the linear approximation of the corpus BLEU in-
troduced in (Tromble et al 2008). While this ap-
proximation was earlier used for Minimum Bayes
Risk decoding in lattices (Tromble et al 2008;
Blackwood et al 2010), we show here how it can
also be used to approximately compute an oracle
translation.
Given five real parameters ?0...4 and a word vo-
cabulary ?, Tromble et al(2008) showed that one
can approximate the corpus-BLEU with its first-
order (linear) Taylor expansion:
lin BLEU(pi) = ?0 |epi|+
4?
n=1
?n
?
u??n
cu(epi)?u(r),
(5)
where cu(e) is the number of times the n-gram
u appears in e, and ?u(r) is an indicator variable
testing the presence of u in r.
To exploit this approximation for oracle decod-
ing, we construct four weighted FSTs ?n con-
taining a (final) state for each possible (n ? 1)-
5See, however, experiments in Section 6.
gram, and all weighted transitions of the kind
(?n?11 , ?n : ?
n
1 /?n ? ??n1 (r), ?
n
2 ), where ?s are
in ?, input word sequence ?n?11 and output se-
quence ?n2 , are, respectively, the maximal prefix
and suffix of an n-gram ?n1 .
In supplement, we add auxiliary states corre-
sponding to m-grams (m < n ? 1), whose func-
tional purpose is to help reach one of the main
(n ? 1)-gram states. There are |?|
n?1?1
|?|?1 , n > 1,
such supplementary states and their transitions are
(?k1 , ?k+1 : ?
k+1
1 /0, ?
k+1
1 ), k = 1 . . . n?2. Apart
from these auxiliary states, the rest of the graph
(i.e., all final states) reproduces the structure of
the well-known de Bruijn graphB(?, n) (see Fig-
ure 1).
To actually compute the best hypothesis, we
first weight all arcs in the input FSA L with ?0 to
obtain ?0. This makes each word?s weight equal
in a hypothesis path, and the total weight of the
path in ?0 is proportional to the number of words
in it. Then, by sequentially composing ?0 with
other ?ns, we discount arcs whose output n-gram
corresponds to a matching n-gram. The amount
of discount is regulated by the ratio between ?n?s
for n > 0.
With all operations performed over the
(min,+)-semiring, the oracle translation is then
given by:
pi?LB = ShortestPath(?0??1??2??3??4).
We set parameters ?n as in (Tromble et al
2008): ?0 = 1, roughly corresponding to the
brevity penalty (each word in a hypothesis adds
up equally to the final path length) and ?n =
?(4p ? rn?1)?1, which are increasing discounts
124
 0 0.2
 0.4 0.6
 0.8 1
p
 0
 0.2
 0.4
 0.6
 0.8
 1
r
 22
 24
 26
 28
 30
 32
 34
 36
BLEU
 22
 24
 26
 28
 30
 32
 34
 36
Figure 2: Performance of the LB-4g oracle for differ-
ent combinations of p and r on WMT11 de2en task.
for matching n-grams. The values of p and r were
found by grid search with a 0.05 step value. A
typical result of the grid evaluation of the LB or-
acle for German to English WMT?11 task is dis-
played on Figure 2. The optimal values for the
other pairs of languages were roughly in the same
ballpark, with p ? 0.3 and r ? 0.2.
5 Oracles with n-gram Clipping
In this section, we describe two new oracle de-
coders that take n-gram clipping into account.
These oracles leverage on the well-known fact
that the shortest path problem, at the heart of
all the oracles described so far, can be reduced
straightforwardly to an Integer Linear Program-
ming (ILP) problem (Wolsey, 1998). Once oracle
decoding is formulated as an ILP problem, it is
relatively easy to introduce additional constraints,
for instance to enforce n-gram clipping. We will
first describe the optimization problem of oracle
decoding and then present several ways to effi-
ciently solve it.
5.1 Problem Description
Throughout this section, abusing the notations,
we will also think of an edge ?i as a binary vari-
able describing whether the edge is ?selected? or
not. The set {0, 1}#{?} of all possible edge as-
signments will be denoted by P . Note that ?, the
set of all paths in the lattice is a subset of P: by
enforcing some constraints on an assignment ? in
P , it can be guaranteed that it will represent a path
in the lattice. For the sake of presentation, we as-
sume that each edge ?i generates a single word
w(?i) and we focus first on finding the optimal
hypothesis with respect to the sentence approxi-
mation of the 1-BLEU score.
As 1-BLEU is decomposable, it is possible to
define, for every edge ?i, an associated reward, ?i
that describes the edge?s local contribution to the
hypothesis score. For instance, for the sentence
approximation of the 1-BLEU score, the rewards
are defined as:
?i =
{
?1 if w(?i) is in the reference,
??2 otherwise,
where ?1 and ?2 are two positive constants cho-
sen to maximize the corpus BLEU score6. Con-
stant ?1 (resp. ?2) is a reward (resp. a penalty)
for generating a word in the reference (resp. not in
the reference). The score of an assignment ? ? P
is then defined as: score(?) =
?#{?}
i=1 ?i ? ?i. This
score can be seen as a compromise between the
number of common words in the hypothesis and
the reference (accounting for recall) and the num-
ber of words of the hypothesis that do not appear
in the reference (accounting for precision).
As explained in Section 2.3, finding the or-
acle hypothesis amounts to solving the shortest
distance (or path) problem (3), which can be re-
formulated by a constrained optimization prob-
lem (Wolsey, 1998):
arg max
??P
#{?}?
i=1
?i ? ?i (6)
s.t.
?
????(qF )
? = 1,
?
???+(q0)
? = 1
?
???+(q)
? ?
?
????(q)
? = 0, q ? Q\{q0, qF }
where q0 (resp. qF ) is the initial (resp. final) state
of the lattice and ??(q) (resp. ?+(q)) denotes the
set of incoming (resp. outgoing) edges of state q.
These path constraints ensure that the solution of
the problem is a valid path in the lattice.
The optimization problem in Equation (6) can
be further extended to take clipping into account.
Let us introduce, for each word w, a variable ?w
that denotes the number of times w appears in the
hypothesis clipped to the number of times, it ap-
pears in the reference. Formally, ?w is defined by:
?w = min
?
?
?
?
???(w)
?, cw(r)
?
?
?
6We tried several combinations of ?1 and ?2 and kept
the one that had the highest corpus 4-BLEU score.
125
where ? (w) is the subset of edges generating w,
and
?
???(w) ? is the number of occurrences of
w in the solution and cw(r) is the number of oc-
currences of w in the reference r. Using the ?
variables, we define a ?clipped? approximation of
1-BLEU:
?1 ?
?
w
?w ??2 ?
?
?
#{?}?
i=1
?i ?
?
w
?w
?
?
Indeed, the clipped number of words in the hy-
pothesis that appear in the reference is given by
?
w ?w, and
?#{?}
i=1 ?i ?
?
w ?w corresponds to
the number of words in the hypothesis that do not
appear in the reference or that are surplus to the
clipped count.
Finally, the clipped lattice oracle is defined by
the following optimization problem:
arg max
??P,?w
(?1 + ?2) ?
?
w
?w ??2 ?
#{?}?
i=1
?i
(7)
s.t. ?w ? 0, ?w ? cw(r), ?w ?
?
???(w)
?
?
????(qF )
? = 1,
?
???+(q0)
? = 1
?
???+(q)
? ?
?
????(q)
? = 0, q ? Q \ {q0, qF }
where the first three sets of constraints are the lin-
earization of the definition of ?w, made possible
by the positivity of ?1 and ?2, and the last three
sets of constraints are the path constraints.
In our implementation we generalized this op-
timization problem to bigram lattices, in which
each edge is labeled by the bigram it generates.
Such bigram FSAs can be produced by compos-
ing the word lattice with ?2 from Section 4. In
this case, the reward of an edge will be defined as
a combination of the (clipped) number of unigram
matches and bigram matches, and solving the op-
timization problem yields a 2-BLEU optimal hy-
pothesis. The approach can be further generalized
to higher-order BLEU or other metrics, as long as
the reward of an edge can be computed locally.
The constrained optimization problem (7) can
be solved efficiently using off-the-shelf ILP
solvers7.
7In our experiments we used Gurobi (Optimization,
2010) a commercial ILP solver that offers free academic li-
cense.
5.2 Shortest Path Oracle (SP)
As a trivial special class of the above formula-
tion, we also define a Shortest Path Oracle (SP)
that solves the optimization problem in (6). As
no clipping constraints apply, it can be solved ef-
ficiently using the standard Bellman algorithm.
5.3 Oracle Decoding through Lagrangian
Relaxation (RLX)
In this section, we introduce another method to
solve problem (7) without relying on an exter-
nal ILP solver. Following (Rush et al 2010;
Chang and Collins, 2011), we propose an original
method for oracle decoding based on Lagrangian
relaxation. This method relies on the idea of re-
laxing the clipping constraints: starting from an
unconstrained problem, the counts clipping is en-
forced by incrementally strengthening the weight
of paths satisfying the constraints.
The oracle decoding problem with clipping
constraints amounts to solving:
arg min
???
?
#{?}?
i=1
?i ? ?i (8)
s.t.
?
???(w)
? ? cw(r), w ? r
where, by abusing the notations, r also denotes
the set of words in the reference. For sake of clar-
ity, the path constraints are incorporated into the
domain (the arg min runs over ? and not over P).
To solve this optimization problem we consider its
dual form and use Lagrangian relaxation to deal
with clipping constraints.
Let ? = {?w}w?r be positive Lagrange mul-
tipliers, one for each different word of the refer-
ence, then the Lagrangian of the problem (8) is:
L(?, ?) = ?
#{?}?
i=1
?i?i+
?
w?r
?w
?
?
?
???(w)
? ? cw(r)
?
?
The dual objective is L(?) = min? L(?, ?)
and the dual problem is: max?,?0 L(?). To
solve the latter, we first need to work out the dual
objective:
?? = arg min
???
L(?, ?)
= arg min
???
#{?}?
i=1
?i
(
?w(?i) ? ?i
)
126
where we assume that ?w(?i) is 0 when word
w(?i) is not in the reference. In the same way
as in Section 5.2, the solution of this problem can
be efficiently retrieved with a shortest path algo-
rithm.
It is possible to optimize L(?) by noticing that
it is a concave function. It can be shown (Chang
and Collins, 2011) that, at convergence, the clip-
ping constraints will be enforced in the optimal
solution. In this work, we chose to use a simple
gradient descent to solve the dual problem. A sub-
gradient of the dual objective is:
?L(?)
??w
=
?
???(w)???
? ? cw(r).
Each component of the gradient corresponds to
the difference between the number of times the
word w appears in the hypothesis and the num-
ber of times it appears in the reference. The algo-
rithm below sums up the optimization of task (8).
In the algorithm ?(t) corresponds to the step size
at the tth iteration. In our experiments we used a
constant step size of 0.1. Compared to the usual
gradient descent algorithm, there is an additional
projection step of ? on the positive orthant, which
enforces the constraint ?  0.
?w, ?(0)w ? 0
for t = 1? T do
??(t) = arg min?
?
i ?i ?
(
?w(?i) ? ?i
)
if all clipping constraints are enforced
then optimal solution found
else for w ? r do
nw ? n. of occurrences of w in ??(t)
?(t)w ? ?
(t)
w + ?(t) ? (nw ? cw(r))
?(t)w ? max(0, ?
(t)
w )
6 Experiments
For the proposed new oracles and the existing ap-
proaches, we compare the quality of oracle trans-
lations and the average time per sentence needed
to compute them8 on several datasets for 3 lan-
guage pairs, using lattices generated by two open-
source decoders: N-code and Moses9 (Figures 3
8Experiments were run in parallel on a server with 64G
of RAM and 2 Xeon CPUs with 4 cores at 2.3 GHz.
9As the ILP (and RLX) oracle were implemented in
Python, we pruned Moses lattices to accelerate task prepa-
ration for it.
decoder fr2en de2en en2de
te
st N-code 27.88 22.05 15.83
Moses 27.68 21.85 15.89
or
ac
le N-code 36.36 29.22 21.18
Moses 35.25 29.13 22.03
Table 2: Test BLEU scores and oracle scores on
100-best lists for the evaluated systems.
and 4). Systems were trained on the data provided
for the WMT?11 Evaluation task10, tuned on the
WMT?09 test data and evaluated on WMT?10 test
set11 to produce lattices. The BLEU test scores
and oracle scores on 100-best lists with the ap-
proximation (4) for N-code and Moses are given
in Table 2. It is not until considering 10,000-best
lists that n-best oracles achieve performance com-
parable to the (mediocre) SP oracle.
To make a fair comparison with the ILP and
RLX oracles which optimize 2-BLEU, we in-
cluded 2-BLEU versions of the LB and LM ora-
cles, identified below with the ?-2g? suffix. The
two versions of the PB oracle are respectively
denoted as PB and PB`, by the type of the ?-
operation they consider (Section 3.2). Parame-
ters p and r for the LB-4g oracle for N-code were
found with grid search and reused for Moses:
p = 0.25, r = 0.15 (fr2en); p = 0.175, r = 0.575
(en2de) and p = 0.35, r = 0.425 (de2en). Cor-
respondingly, for the LB-2g oracle: p = 0.3, r =
0.15; p = 0.3, r = 0.175 and p = 0.575, r = 0.1.
The proposed LB, ILP and RLX oracles were
the best performing oracles, with the ILP and
RLX oracles being considerably faster, suffering
only a negligible decrease in BLEU, compared to
the 4-BLEU-optimized LB oracle. We stopped
RLX oracle after 20 iterations, as letting it con-
verge had a small negative effect (?1 point of the
corpus BLEU), because of the sentence/corpus dis-
crepancy ushered by the BLEU score approxima-
tion.
Experiments showed consistently inferior per-
formance of the LM-oracle resulting from the op-
timization of the sentence probability rather than
BLEU. The PB oracle often performed compara-
bly to our new oracles, however, with sporadic
resource-consumption bursts, that are difficult to
10http://www.statmt.org/wmt2011
11All BLEU scores are reported using the multi-bleu.pl
script.
127
 25
 30
 35
 40
 45
 50
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
 4
 5
 6
BLE
U
avg
. tim
e, s
BLEU
47.8
2
48.1
2
48.2
2
47.7
1
46.7
6
46.4
8
41.2
3
38.9
1
38.7
5
avg. time
(a) fr2en
 25
 30
 35
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 0.5
 1
 1.5
BLE
U
avg
. tim
e, s
BLEU
34.7
9
34.7
0 35.4
9
35.0
9
34.8
5
34.7
6
30.7
8
29.5
3
29.5
3
avg. time
(b) de2en
 15
 20
 25
 30
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 0.5
 1
BLE
U
avg
. tim
e, s
BLEU
24.7
5
24.6
6 25.3
4
24.8
5
24.7
8
24.7
3
22.1
9
20.7
8
20.7
4
avg. time
(c) en2de
Figure 3: Oracles performance for N-code lattices.
 25
 30
 35
 40
 45
 50
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
BLE
U
avg
. tim
e, s
BLEU
43.8
2
44.0
8
44.4
4
43.8
2
43.4
2
43.2
0
41.0
3
36.3
4
36.2
5
avg. time
(a) fr2en
 25
 30
 35
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
 4
BLE
U
avg
. tim
e, s
BLEU
36.4
3 36.9
1 37.7
3
36.5
2
36.7
5
36.6
2
30.5
2
29.5
1
29.4
5
avg. time
(b) de2en
 15
 20
 25
 30
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
 4
 5
 6
 7
 8
 9
BLE
U
avg
. tim
e, s
BLEU
28.6
8
28.6
4 29.
94
28.9
4
28.7
6
28.6
5
26.4
8
21.2
9
21.2
3
avg. time
(c) en2de
Figure 4: Oracles performance for Moses lattices pruned with parameter -b 0.5.
avoid without more cursory hypotheses recom-
bination strategies and the induced effect on the
translations quality. The length-aware PB` oracle
has unexpectedly poorer scores compared to its
length-agnostic PB counterpart, while it should,
at least, stay even, as it takes the brevity penalty
into account. We attribute this fact to the com-
plex effect of clipping coupled with the lack of
control of the process of selecting one hypothe-
sis among several having the same BLEU score,
length and recent history. Anyhow, BLEU scores
of both of PB oracles are only marginally differ-
ent, so the PB`?s conservative policy of pruning
and, consequently, much heavier memory con-
sumption makes it an unwanted choice.
7 Conclusion
We proposed two methods for finding oracle
translations in lattices, based, respectively, on a
linear approximation to the corpus-level BLEU
and on integer linear programming techniques.
We also proposed a variant of the latter approach
based on Lagrangian relaxation that does not rely
on a third-party ILP solver. All these oracles have
superior performance to existing approaches, in
terms of the quality of the found translations, re-
source consumption and, for the LB-2g oracles,
in terms of speed. It is thus possible to use bet-
ter approximations of BLEU than was previously
done, taking the corpus-based nature of BLEU, or
clipping constrainst into account, delivering better
oracles without compromising speed.
Using 2-BLEU and 4-BLEU oracles yields com-
parable performance, which confirms the intuition
that hypotheses sharing many 2-grams, would
likely have many common 3- and 4-grams as well.
Taking into consideration the exceptional speed of
the LB-2g oracle, in practice one can safely opti-
mize for 2-BLEU instead of 4-BLEU, saving large
amounts of time for oracle decoding on long sen-
tences.
Overall, these experiments accentuate the
acuteness of scoring problems that plague modern
decoders: very good hypotheses exist for most in-
put sentences, but are poorly evaluated by a linear
combination of standard features functions. Even
though the tuning procedure can be held respon-
sible for part of the problem, the comparison be-
tween lattice and n-best oracles shows that the
beam search leaves good hypotheses out of the n-
best list until very high value of n, that are never
used in practice.
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program.
128
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proc. of the Int. Conf. on Imple-
mentation and Application of Automata, pages 11?
23.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. of WMT, pages 224?
232, Athens, Greece.
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation with
improved correlation with human judgments. In
Proc. of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation,
pages 65?72, Ann Arbor, MI, USA.
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Efficient path counting transducers
for minimum bayes-risk decoding of statistical ma-
chine translation lattices. In Proc. of the ACL 2010
Conference Short Papers, pages 27?32, Strouds-
burg, PA, USA.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proc. of the 2011 Conf. on
EMNLP, pages 26?37, Edinburgh, UK.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic
and structural translation features. In Proc. of the
2008 Conf. on EMNLP, pages 224?233, Honolulu,
Hawaii.
Markus Dreyer, Keith B. Hall, and Sanjeev P. Khu-
danpur. 2007. Comparing reordering constraints
for SMT using efficient BLEU oracle computation.
In Proc. of the Workshop on Syntax and Structure
in Statistical Translation, pages 103?110, Morris-
town, NJ, USA.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proc. of the
2008 Conf. on EMNLP, pages 839?847, Honolulu,
Hawaii.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proc. of Human Language Technolo-
gies: The 2009 Annual Conf. of the North Ameri-
can Chapter of the ACL, Companion Volume: Short
Papers, pages 9?12, Morristown, NJ, USA.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrim-
inative approach to machine translation. In Proc.
of the 21st Int. Conf. on Computational Linguistics
and the 44th annual meeting of the ACL, pages 761?
768, Morristown, NJ, USA.
Mehryar Mohri. 2002. Semiring frameworks and al-
gorithms for shortest-distance problems. J. Autom.
Lang. Comb., 7:321?350.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, chapter 6, pages 213?254.
Gurobi Optimization. 2010. Gurobi optimizer, April.
Version 3.0.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc. of
the Annual Meeting of the ACL, pages 311?318.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proc. of the 2010 Conf. on
EMNLP, pages 1?11, Stroudsburg, PA, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human anno-
tation. In Proc. of the Conf. of the Association for
Machine Translation in the America (AMTA), pages
223?231.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proc. of the Conf. on EMNLP, pages 620?
629, Stroudsburg, PA, USA.
Marco Turchi, Tijl De Bie, and Nello Cristianini.
2008. Learning performance of a machine trans-
lation system: a statistical and computational anal-
ysis. In Proc. of WMT, pages 35?43, Columbus,
Ohio.
Guillaume Wisniewski, Alexandre Allauzen, and
Franc?ois Yvon. 2010. Assessing phrase-based
translation models with oracle decoding. In Proc.
of the 2010 Conf. on EMNLP, pages 933?943,
Stroudsburg, PA, USA.
L. Wolsey. 1998. Integer Programming. John Wiley
& Sons, Inc.
129
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39?48,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Continuous Space Translation Models with Neural Networks
Le Hai Son and Alexandre Allauzen and Franc?ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
rue John von Neumann, 91403 Orsay cedex, France
Firstname.Lastname@limsi.fr
Abstract
The use of conventional maximum likelihood
estimates hinders the performance of existing
phrase-based translation models. For lack of
sufficient training data, most models only con-
sider a small amount of context. As a par-
tial remedy, we explore here several contin-
uous space translation models, where transla-
tion probabilities are estimated using a con-
tinuous representation of translation units in
lieu of standard discrete representations. In
order to handle a large set of translation units,
these representations and the associated esti-
mates are jointly computed using a multi-layer
neural network with a SOUL architecture. In
small scale and large scale English to French
experiments, we show that the resulting mod-
els can effectively be trained and used on top
of a n-gram translation system, delivering sig-
nificant improvements in performance.
1 Introduction
The phrase-based approach to statistical machine
translation (SMT) is based on the following infer-
ence rule, which, given a source sentence s, selects
the target sentence t and the underlying alignment a
maximizing the following term:
P (t,a|s) =
1
Z(s)
exp
( K?
k=1
?kfk(s, t,a)
)
, (1)
where K feature functions (fk) are weighted by a
set of coefficients (?k), and Z is a normalizing fac-
tor. The phrase-based approach differs from other
approaches by the hidden variables of the translation
process: the segmentation of a parallel sentence pair
into phrase pairs and the associated phrase align-
ments.
This formulation was introduced in (Zens et al,
2002) as an extension of the word based mod-
els (Brown et al, 1993), then later motivated within
a discriminative framework (Och and Ney, 2004).
One motivation for integrating more feature func-
tions was to improve the estimation of the translation
model P (t|s), which was initially based on relative
frequencies, thus yielding poor estimates.
This is because the units of phrase-based mod-
els are phrase pairs, made of a source and a tar-
get phrase; such units are viewed as the events of
discrete random variables. The resulting representa-
tions of phrases (or words) thus entirely ignore the
morphological, syntactic and semantic relationships
that exist among those units in both languages. This
lack of structure hinders the generalization power of
the model and reduces its ability to adapt to other
domains. Another consequence is that phrase-based
models usually consider a very restricted context1.
This is a general issue in statistical Natural Lan-
guage Processing (NLP) and many possible reme-
dies have been proposed in the literature, such as,
for instance, using smoothing techniques (Chen and
Goodman, 1996), or working with linguistically en-
riched, or more abstract, representations. In statisti-
cal language modeling, another line of research con-
siders numerical representations, trained automat-
ically through the use of neural network (see eg.
1typically a small number of preceding phrase pairs for the
n-gram based approach (Crego and Marin?o, 2006), or no con-
text at all, for the standard approach of (Koehn et al, 2007).
39
(Collobert et al, 2011)). An influential proposal,
in this respect, is the work of (Bengio et al, 2003)
on continuous space language models. In this ap-
proach, n-gram probabilities are estimated using a
continuous representation of words in lieu of stan-
dard discrete representations. Experimental results,
reported for instance in (Schwenk, 2007) show sig-
nificant improvements in speech recognition appli-
cations. Recently, this model has been extended in
several promising ways (Mikolov et al, 2011; Kuo
et al, 2010; Liu et al, 2011). In the context of SMT,
Schwenk et al (2007) is the first attempt to esti-
mate translation probabilities in a continuous space.
However, because of the proposed neural architec-
ture, the authors only consider a very restricted set
of translation units, and therefore report only a slight
impact on translation performance. The recent pro-
posal of (Le et al, 2011a) seems especially relevant,
as it is able, through the use of class-based models,
to handle arbitrarily large vocabularies and opens the
way to enhanced neural translation models.
In this paper, we explore various neural architec-
tures for translation models and consider three dif-
ferent ways to factor the joint probability P (s, t)
differing by the units (respectively phrase pairs,
phrases or words) that are projected in continuous
spaces. While these decompositions are theoreti-
cally straightforward, they were not considered in
the past because of data sparsity issues and of the
resulting weaknesses of conventional maximum like-
lihood estimates. Our main contribution is then to
show that such joint distributions can be efficiently
computed by neural networks, even for very large
context sizes; and that their use yields significant
performance improvements. These models are eval-
uated in a n-best rescoring step using the framework
of n-gram based systems, within which they inte-
grate easily. Note, however that they could be used
with any phrase-based system.
The rest of this paper is organized as follows. We
first recollect, in Section 2, the n-gram based ap-
proach, and discuss various implementations of this
framework. We then describe, in Section 3, the neu-
ral architecture developed and explain how it can be
made to handle large vocabulary tasks as well as lan-
guage models over bilingual units. We finally re-
port, in Section 4, experimental results obtained on
a large-scale English to French translation task.
2 Variations on the n-gram approach
Even though n-gram translation models can be
integrated within standard phrase-based systems
(Niehues et al, 2011), the n-gram based frame-
work provides a more convenient way to introduce
our work and has also been used to build the base-
line systems used in our experiments. In the n-
gram based approach (Casacuberta and Vidal, 2004;
Marin?o et al, 2006; Crego and Marin?o, 2006), trans-
lation is divided in two steps: a source reordering
step and a translation step. Source reordering is
based on a set of learned rewrite rules that non-
deterministically reorder the input words so as to
match the target order thereby generating a lattice
of possible reorderings. Translation then amounts
to finding the most likely path in this lattice using a
n-gram translation model 2 of bilingual units.
2.1 The standard n-gram translation model
n-gram translation models (TMs) rely on a spe-
cific decomposition of the joint probability P (s, t),
where s is a sequence of I reordered source words
(s1, ..., sI ) and t contains J target words (t1, ..., tJ ).
This sentence pair is further assumed to be de-
composed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing, and ultimately derives from initial word and
phrase alignments. In this framework, the basic
translation units are tuples, which are the analogous
of phrase pairs, and represent a matching u = (s, t)
between a source s and a target t phrase (see Fig-
ure 1). Using the n-gram assumption, the joint prob-
ability of a segmented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
A first issue with this model is that the elementary
units are bilingual pairs, which means that the under-
lying vocabulary, hence the number of parameters,
can be quite large, even for small translation tasks.
Due to data sparsity issues, such models are bound
2Like in the standard phrase-based approach, the translation
process also involves additional feature functions that are pre-
sented below.
40
to face severe estimation problems. Another prob-
lem with (2) is that the source and target sides play
symmetric roles, whereas the source side is known,
and the target side must be predicted.
2.2 A factored n-gram translation model
To overcome some of these issues, the n-gram prob-
ability in equation (2) can be factored by decompos-
ing tuples in two (source and target) parts :
P (ui|ui?1, ..., ui?n+1) =
P (ti|si, si?1, ti?1, ..., si?n+1, ti?n+1)
? P (si|si?1, ti?1..., si?n+1, ti?n+1)
(3)
Decomposition (3) involves two models: the first
term represents a TM, the second term is best viewed
as a reordering model. In this formulation, the TM
only predicts the target phrase, given its source and
target contexts.
Another benefit of this formulation is that the el-
ementary events now correspond either to source or
to target phrases, but never to pairs of such phrases.
The underlying vocabulary is thus obtained as the
union, rather than the cross product, of phrase in-
ventories. Finally note that the n-gram probability
P (ui|ui?1, ..., ui?n+1) could also factor as:
P (si|ti, si?1, ti?1, ..., si?n+1, ti?n+1)
? P (ti|si?1, ti?1, ..., si?n+1, ti?n+1)
(4)
2.3 A word factored translation model
A more radical way to address the data sparsity is-
sues is to take (source and target) words as the basic
units of the n-gram TM. This may seem to be a step
backwards, since the transition from word (Brown et
al., 1993) to phrase-based models (Zens et al, 2002)
is considered as one of the main recent improvement
in MT. One important motivation for considering
phrases rather than words was to capture local con-
text in translation and reordering. It should then be
stressed that the decomposition of phrases in words
is only re-introduced here as a way to mitigate the
parameter estimation problems. Translation units
are still pairs of phrases, derived from a bilingual
segmentation in tuples synchronizing the source and
target n-gram streams, as defined by equation (3).
In fact, the estimation policy described in section 3
will actually allow us to design n-gram models with
longer contexts than is typically possible in the con-
ventional n-gram approach.
Let ski denote the k
th word of source tuple si.
Considering again the example of Figure 1, s111 is
to the source word nobel, s411 is to the source word
paix, and similarly t211 is the target word peace. We
finally denote hn?1(tki ) the sequence made of the
n ? 1 words preceding tki in the target sentence: in
Figure 1, h3(t211) thus refers to the three word con-
text receive the nobel associated with the target word
peace. Using these notations, equation (3) is rewrit-
ten as:
P (s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (5)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words in ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
Likewise, the contribution of the source phrase
s11 =nobel, de, la, paix is:
P
(
nobel|[receive, the], [recevoir,le]
)
? P
(
de|[receive, the], [le,nobel]
)
? P
(
la|[receive, the], [nobel, de]
)
? P
(
paix|[receive, the], [de,la]
)
.
A benefit of this new formulation is that the involved
vocabularies only contain words, and are thus much
smaller. These models are thus less bound to be af-
fected by data sparsity issues. While the TM defined
by equation (5) derives from equation (3), a variation
can be equivalently derived from equation (4).
41
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented in bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
3 The SOUL model
In the previous section, we defined three different
n-gram translation models, based respectively on
equations (2), (3) and (5). As discussed above, a
major issue with such models is to reliably estimate
their parameters, the numbers of which grow expo-
nentially with the order of the model. This problem
is aggravated in natural language processing, due to
well known data sparsity issues. In this work, we
take advantage of the recent proposal of (Le et al,
2011a): using a specific neural network architecture
(the Structured OUtput Layer model), it becomes
possible to handle large vocabulary language mod-
eling tasks, a solution that we adapt here to MT.
3.1 Language modeling in a continuous space
Let V be a finite vocabulary, n-gram language mod-
els (LMs) define distributions over finite sequences
of tokens (typically words) wL1 in V
+ as follows:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
i?n+1) (6)
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in NLP applications where V
typically contains dozens of thousands words.
In spite of the simplifying n-gram assump-
tion, maximum likelihood estimation remains un-
reliable and tends to underestimate the proba-
bility of very rare n-grams. Smoothing tech-
niques, such as Kneser-Ney and Witten-Bell back-
off schemes (see (Chen and Goodman, 1996) for an
empirical overview, and (Teh, 2006) for a Bayesian
interpretation), perform back-off to lower order dis-
tributions, thus providing an estimate for the proba-
bility of these unseen events.
One of the most successful alternative to date is to
use distributed word representations (Bengio et al,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated estimates are jointly computed
in a multi-layer neural network architecture. Fig-
ure 2 provides a partial representation of this kind
of model and helps figuring out their principles. To
compute the probability P (wi|w
i?1
i?n+1), the n ? 1
context words are projected in the same continu-
ous space using a shared matrix R; these continuous
word representations are then concatenated to build
a single vector that represents the context; after a
non-linear transformation, the probability distribu-
tion is computed using a softmax layer.
The major difficulty with the neural network ap-
proach remains the complexity of inference and
training, which largely depends on the size of the
output vocabulary (i.e. the number of words that
have to be predicted). One practical solution is to re-
strict the output vocabulary to a short-list composed
of the most frequent words (Schwenk, 2007). How-
ever, the usual size of the short-list is under 20k,
which does not seem sufficient to faithfully repre-
sent the translation models of section 2.
3.2 Principles of SOUL
To circumvent this problem, Structured Output
Layer (SOUL) LMs are introduced in (Le et al,
2011a). Following Mnih and Hinton (2008), the
SOULmodel combines the neural network approach
with a class-based LM (Brown et al, 1992). Struc-
42
turing the output layer and using word class informa-
tion makes the estimation of distributions over the
entire vocabulary computationally feasible.
To meet this goal, the output vocabulary is struc-
tured as a clustering tree, where each word belongs
to only one class and its associated sub-classes. If
wi denotes the ith word in a sentence, the sequence
c1:D(wi) = c1, . . . , cD encodes the path for wordwi
in the clustering tree, with D being the depth of the
tree, cd(wi) a class or sub-class assigned to wi, and
cD(wi) being the leaf associated with wi (the word
itself). The probability of wi given its history h can
then be computed as:
P (wi|h) =P (c1(wi)|h)
?
D?
d=2
P (cd(wi)|h, c1:d?1).
(7)
There is a softmax function at each level of the tree
and each word ends up forming its own class (a leaf).
The SOULmodel, represented on Figure 2, is thus
the same as for the standard model up to the output
layer. The main difference lies in the output struc-
ture which involves several layers with a softmax ac-
tivation function. The first (class layer) estimates
the class probability P (c1(wi)|h), while other out-
put sub-class layers estimate the sub-class probabili-
ties P (cd(wi)|h, c1:d?1). Finally, theword layers es-
timate the word probabilities P (cD(wi)|h, c1:D?1).
Words in the short-list remain special, since each of
them represents a (final) class.
Training a SOULmodel can be achieved by maxi-
mizing the log-likelihood of the parameters on some
training corpus. Following (Bengio et al, 2003),
this optimization is performed by stochastic back-
propagation. Details of the training procedure can
be found in (Le et al, 2011b).
Neural network architectures are also interesting
as they can easily handle larger contexts than typical
n-grammodels. In the SOUL architecture, enlarging
the context mainly consists in increasing the size of
the projection layer, which corresponds to a simple
look-up operation. Increasing the context length at
the input layer thus only causes a linear growth in
complexity in the worst case (Schwenk, 2007).
0...0100
10...000
0...0010
wi-1
wi-2
wi-3
R
R
R
shared input space
input layer
hidden layers
shortlist
sub-classlayers
wordlayers
classlayer
Figure 2: The architecture of a SOUL Neural Network
language model in the case of a 4-gram model.
3.3 Translation modeling with SOUL
The SOUL architecture was used successfully to
deliver (monolingual) LMs probabilities for speech
recognition (Le et al, 2011a) and machine transla-
tion (Allauzen et al, 2011) applications. In fact,
using this architecture, it is possible to estimate n-
gram distributions for any kind of discrete random
variables, such as a phrase or a tuple. The SOUL ar-
chitecture can thus be readily used as a replacement
for the standard n-gram TM described in section 2.1.
This is because all the random variables are events
over the same set of tuples.
Adopting this architecture for the other n-gram
TM respectively described by equations (3) and (5)
is more tricky, as they involve two different lan-
guages and thus two different vocabularies: the pre-
dicted unit is a target phrase (resp. word), whereas
the context is made of both source and target phrases
(resp. words). A subsequent modification of the
SOUL architecture was thus performed to make up
for ?mixed? contexts: rather than projecting all the
context words or phrases into the same continuous
space (using the matrix R, see Figure 2), we used
two different projection matrices, one for each lan-
guage. The input layer is thus composed of two vec-
tors in two different spaces; these two representa-
tions are then combined through the hidden layer,
the other layers remaining unchanged.
43
4 Experimental Results
We now turn to an experimental comparison of the
models introduced in Section 2. We first describe
the tasks and data that were used, before presenting
our n-gram based system and baseline set-up. Our
results are finally presented and discussed.
Let us first emphasize that the design and inte-
gration of a SOUL model for large SMT tasks is
far from easy, given the computational cost of com-
puting n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two pass approach:
the first pass uses a conventional back-off n-gram
model to produce a k-best list (the k most likely
translations); in the second pass, the probability of
a m-gram SOUL model is computed for each hy-
pothesis, added as a new feature and the k-best list
is accordingly reordered3. In all the following ex-
periments, we used a fixed context size for SOUL of
m = 10, and used k = 300.
4.1 Tasks and corpora
The two tasks considered in our experiments
are adapted from the text translation track of
IWSLT 2011 from English to French (the ?TED?
talk task): a small data scenario where the only
training data is a small in-domain corpus; and a large
scale condition using all the available training data.
In this article, we only provide a short overview of
the task; all the necessary details regarding this eval-
uation campaign are on the official website4.
The in-domain training data consists of 107, 058
sentence pairs, whereas for the large scale task, all
the data available for the WMT 2011 evaluation5 are
added. For the latter task, the available parallel data
includes a large Web corpus, referred to as the Gi-
gaWord parallel corpus. This corpus is very noisy
and is accordingly filtered using a simple perplexity
criterion as explained in (Allauzen et al, 2011). The
total amount of training data is approximately 11.5
million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
As the provided development data was quite small,
3The probability estimated with the SOULmodel is added as
a new feature to the score of an hypothesis given by Equation 1.
The coefficients are retuned before the reranking step.
4iwslt2011.org
5www.statmt.org/wmt11/
Model Vocabulary size
Small task Large task
src trg src trg
Standard 317k 8847k
Phrase factored 96k 131k 4262k 3972k
Word factored 45k 53k 505k 492k
Table 1: Vocabulary sizes for the English to French tasks
obtained with various SOUL translation (TM) models.
For the factored models, sizes are indicated for both
source (src) and target (trg) sides.
development and test set were inverted, and we fi-
nally used a development set of 1,664 sentences, and
a test set of 934 sentences. The table 1 provides the
sizes of the different vocabularies. The n-gram TMs
are estimated over a training corpus composed of tu-
ple sequences. Tuples are extracted from the word-
aligned parallel data (using MGIZA++6 with default
settings) in such a way that a unique segmentation
of the bilingual corpus is achieved, allowing to di-
rectly estimate bilingual n-gram models (see (Crego
and Marin?o, 2006) for details).
4.2 n-gram based translation system
The n-gram based system used here is based on an
open source implementation described in (Crego et
al., 2011). In a nutshell, the TM is implemented as
a stochastic finite-state transducer trained using a n-
gram model of (source, target) pairs as described in
section 2.1. Training this model requires to reorder
source sentences so as to match the target word or-
der. This is performed by a non-deterministic finite-
state reordering model, which uses part-of-speech
information generated by the TreeTagger to gener-
alize reordering patterns beyond lexical regularities.
In addition to the TM, fourteen feature functions
are included: a target-language model; four lexi-
con models; six lexicalized reordering models (Till-
mann, 2004; Crego et al, 2011); a distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model. The four lexicon mod-
els are similar to the ones used in standard phrase-
based systems: two scores correspond to the rela-
tive frequencies of the tuples and two lexical weights
are estimated from the automatically generated word
6geek.kyloo.net/software
44
alignments. The weights associated to feature func-
tions are optimally combined using the Minimum
Error Rate Training (MERT) (Och, 2003). All the
results in BLEU are obtained as an average of 4 op-
timization runs7.
For the small task, the target LM is a standard
4-gram model estimated with the Kneser-Ney dis-
counting scheme interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1996), while for the large task, the target LM is ob-
tained by linear interpolation of several 4-grammod-
els (see (Lavergne et al, 2011) for details). As for
the TM, all the available parallel corpora were sim-
ply pooled together to train a 3-gram model. Results
obtained with this large-scale system were found to
be comparable to some of the best official submis-
sions.
4.3 Small task evaluation
Table 2 summarizes the results obtained with the
baseline and different SOUL models, TMs and a
target LM. The first comparison concerns the stan-
dard n-gram TM, defined by equation (2), when es-
timated conventionally or as a SOULmodel. Adding
the latter model yields a slight BLEU improvement
of 0.5 point over the baseline. When the SOUL TM
is phrased factored as defined in equation (3) the
gain is of 0.9 BLEU point instead. This difference
can be explained by the smaller vocabularies used
in the latter model, and its improved robustness to
data sparsity issues. Additional gains are obtained
with the word factored TM defined by equation (5):
a BLEU improvement of 0.8 point over the phrase
factored TM and of 1.7 point over the baseline are
respectively achieved. We assume that the observed
improvements can be explained by the joint effect of
a better smoothing and a longer context.
The comparison with the condition where we only
use a SOUL target LM is interesting as well. Here,
the use of the word factored TM still yields to a 0.6
BLEU improvement. This result shows that there
is an actual benefit in smoothing the TM estimates,
rather than only focus on the LM estimates.
Table 3 reports a comparison among the dif-
ferent components and variations of the word
7The standard deviations are below 0.1 and thus omitted in
the reported results.
Model BLEU
dev test
Baseline 31.4 25.8
Adding a SOUL model
Standard TM 32.0 26.3
Phrase factored TM 32.7 26.7
Word factored TM 33.6 27.5
Target LM 32.6 26.9
Table 2: Results for the small English to French task ob-
tained with the baseline system and with various SOUL
translation (TM) or target language (LM) models.
Model BLEU
dev test
Adding a SOUL model
+ P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
32.6 26.9
+ P
(
ski |h
n?1(t1i ), h
n?1(ski )
)
32.1 26.2
+ the combination of both 33.2 27.5
+ P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
31.7 26.1
+ P
(
tki |h
n?1(s1i ), h
n?1(tki )
)
32.7 26.8
+ the combination of both 33.4 27.2
Table 3: Comparison of the different components and
variations of the word factored translation model.
factored TM. In the upper part of the table,
the model defined by equation (5) is evaluated
component by component: first the translation
term P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
, then its distor-
tion counterpart P
(
ski |h
n?1(t1i ), h
n?1(ski )
)
and fi-
nally their combination, which yields the joint prob-
ability of the sentence pair. Here, we observe that
the best improvement is obtained with the transla-
tion term, which is 0.7 BLEU point better than the
latter term. Moreover, the use of just a translation
term only yields a BLEU score equal to the one ob-
tained with the SOUL target LM, and its combina-
tion with the distortion term is decisive to attain the
additional gain of 0.6 BLEU point. The lower part of
the table provides the same comparison, but for the
variation of the word factored TM. Besides a similar
trend, we observe that this variation delivers slightly
lower results. This can be explained by the restricted
context used by the translation term which no longer
includes the current source phrase or word.
45
Model BLEU
dev test
Baseline 33.7 28.2
Adding a word factored SOUL TM
+ in-domain TM 35.2 29.4
+ out-of-domain TM 34.8 29.1
+ out-of-domain adapted TM 35.5 29.8
Adding a SOUL LM
+ out-of-domain adapted LM 35.0 29.2
Table 4: Results for the large English to French trans-
lation task obtained by adding various SOUL translation
and language models (see text for details).
4.4 Large task evaluation
For the large-scale setting, the training material in-
creases drastically with the use of the additional out-
of-domain data for the baseline models. Results are
summarized in Table 4. The first observation is the
large increase of BLEU (+2.4 points) for the base-
line system over the small-scale baseline. For this
task, only the word factored TM is evaluated since
it significantly outperforms the others on the small
task (see section 4.3).
In a first scenario, we use a word factored TM,
trained only on the small in-domain corpus. Even
though the training corpus of the baseline TM is one
hundred times larger than this small in-domain data,
adding the SOUL TM still yields a BLEU increase
of 1.2 point8. In a second scenario, we increase the
training corpus for the SOUL, and include parts of
the out-of-domain data (the WMT part). The result-
ing BLEU score is here slightly worse than the one
obtained with just the in-domain TM, yet delivering
improved results with the respect to the baseline.
In a last attempt, we amended the training regime
of the neural network. In a fist step, we trained con-
ventionally a SOUL model using the same out-of-
domain parallel data as before. We then adapted this
model by running five additional epochs of the back-
propagation algorithm using the in-domain data. Us-
ing this adapted model yielded our best results to
date with a BLEU improvement of 1.6 points over
the baseline results. Moreover, the gains obtained
using this simple domain adaptation strategy are re-
8Note that the in-domain data was already included in the
training corpus of the baseline TM.
spectively of +0.4 and +0.8 BLEU, as compared
with the small in-domain model and the large out-
of-domain model. These results show that the SOUL
TM can scale efficiently and that its structure is well
suited for domain adaptation.
5 Related work
To the best of our knowledge, the first work on ma-
chine translation in continuous spaces is (Schwenk
et al, 2007), where the authors introduced the model
referred here to as the the standard n-gram trans-
lation model in Section 2.1. This model is an ex-
tension of the continuous space language model
of (Bengio et al, 2003), the basic unit is the tuple
(or equivalently the phrase pair). The resulting vo-
cabulary being too large to be handled by neural net-
works without a structured output layer, the authors
had thus to restrict the set of the predicted units to a
8k short-list . Moreover, in (Zamora-Martinez et al,
2010), the authors propose a tighter integration of a
continuous space model with a n-gram approach but
only for the target LM.
A different approach, described in (Sarikaya et
al., 2008), divides the problem in two parts: first the
continuous representation is obtained by an adapta-
tion of the Latent Semantic Analysis; then a Gaus-
sian mixture model is learned using this continu-
ous representation and included in a hidden Markov
model. One problem with this approach is the sep-
aration between the training of the continuous rep-
resentation on the one hand, and the training of the
translation model on the other hand. In comparison,
in our approach, the representation and the predic-
tion are learned in a joined fashion.
Other ways to address the data sparsity issues
faced by translation model were also proposed in the
literature. Smoothing is obviously one possibility
(Foster et al, 2006). Another is to use factored lan-
guage models, introduced in (Bilmes and Kirchhoff,
2003), then adapted for translation models in (Koehn
and Hoang, 2007; Crego and Yvon, 2010). Such ap-
proaches require to use external linguistic analysis
tools which are error prone; moreover, they did not
seem to bring clear improvements, even when trans-
lating into morphologically rich languages.
46
6 Conclusion
In this paper, we have presented possible ways to use
a neural network architecture as a translation model.
A first contribution was to produce the first large-
scale neural translation model, implemented here in
the framework of the n-gram based models, tak-
ing advantage of a specific hierarchical architecture
(SOUL). By considering several decompositions of
the joint probability of a sentence pair, several bilin-
gual translation models were presented and dis-
cussed. As it turned out, using a factorization which
clearly distinguishes the source and target sides, and
only involves word probabilities, proved an effec-
tive remedy to data sparsity issues and provided sig-
nificant improvements over the baseline. Moreover,
this approach was also experimented within the sys-
tems we submitted to the shared translation task of
the seventh workshop on statistical machine trans-
lation (WMT 2012). These experimentations in a
large scale setup and for different language pair cor-
roborate the improvements reported in this article.
We also investigated various training regimes for
these models in a cross domain adaptation setting.
Our results show that adapting an out-of-domain
SOUL TM is both an effective and very fast way to
perform bilingual model adaptation. Adding up all
these novelties finally brought us a 1.6 BLEU point
improvement over the baseline. Even though our
experiments were carried out only within the frame-
work of n-gram basedMT systems, using such mod-
els in other systems is straightforward. Future work
will thus aim at introducing them into conventional
phrase-based systems, such as Moses (Koehn et al,
2007). Given that Moses only implicitly uses n-
gram based information, adding SOUL translation
models is expected to be even more helpful.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
References
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL ?03: Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology, pages 4?6.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego and Franc?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, pages 1?17.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrase-table smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 53?61, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181?184, Detroit, Michigan.
47
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL?07, pages 177?180, Prague, Czech Republic.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for Arabic speech recognition. In Proc. ICASSP
2010.
Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, and
Franc?ois Yvon. 2011. LIMSI?s experiments in do-
main adaptation for IWSLT11. In Proceedings of
the eight International Workshop on Spoken Language
Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011a. Structured out-
put layer neural network language model. In Proceed-
ings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Ilya Oparin, Abdel Messaoudi, Alexan-
dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011b. Large vocabulary SOUL neural network lan-
guage models. In Proceedings of InterSpeech 2011.
Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2011. Improving lvcsr system combination using neu-
ral network language model cross adaptation. In IN-
TERSPEECH, pages 2857?2860.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011. Extensions
of recurrent neural network language model. In Proc.
of ICASSP?11, pages 5528?5531.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider context by using bilingual lan-
guage models in machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 198?206, Edinburgh, Scotland. Associa-
tion for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449, Decem-
ber.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Ruhi Sarikaya, Yonggang Deng, Mohamed Afify, Brian
Kingsbury, and Yuqing Gao. 2008. Machine trans-
lation in continuous space. In Proceedings of Inter-
speech, pages 2350?2353, Brisbane, Australia.
Holger Schwenk, Marta R. Costa-Jussa`, and Jose? A.R.
Fonollosa. 2007. Smooth bilingual n-gram transla-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 430?438, Prague, Czech Re-
public.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104.
Francisco Zamora-Martinez, Maria Jose? Castro-Bleda,
and Holger Schwenk. 2010. N-gram-based Machine
Translation enhanced with Neural Networks for the
French-English BTEC-IWSLT?10 task. In Proceed-
ings of the seventh International Workshop on Spoken
Language Translation (IWSLT), pages 45?52.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
48
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504?513,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Practical very large scale CRFs
Thomas Lavergne
LIMSI ? CNRS
lavergne@limsi.fr
Olivier Cappe?
Te?le?com ParisTech
LTCI ? CNRS
cappe@enst.fr
Franc?ois Yvon
Universite? Paris-Sud 11
LIMSI ? CNRS
yvon@limsi.fr
Abstract
Conditional Random Fields (CRFs) are
a widely-used approach for supervised
sequence labelling, notably due to their
ability to handle large description spaces
and to integrate structural dependency be-
tween labels. Even for the simple linear-
chain model, taking structure into account
implies a number of parameters and a
computational effort that grows quadrati-
cally with the cardinality of the label set.
In this paper, we address the issue of train-
ing very large CRFs, containing up to hun-
dreds output labels and several billion fea-
tures. Efficiency stems here from the spar-
sity induced by the use of a `1 penalty
term. Based on our own implementa-
tion, we compare three recent proposals
for implementing this regularization strat-
egy. Our experiments demonstrate that
very large CRFs can be trained efficiently
and that very large models are able to im-
prove the accuracy, while delivering com-
pact parameter sets.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et
al., 2001; Sutton and McCallum, 2006) constitute
a widely-used and effective approach for super-
vised structure learning tasks involving the map-
ping between complex objects such as strings and
trees. An important property of CRFs is their abil-
ity to handle large and redundant feature sets and
to integrate structural dependency between out-
put labels. However, even for simple linear chain
CRFs, the complexity of learning and inference
This work was partly supported by ANR projects CroTaL
(ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311-
02).
grows quadratically with respect to the number of
output labels and so does the number of structural
features, ie. features testing adjacent pairs of la-
bels. Most empirical studies on CRFs thus ei-
ther consider tasks with a restricted output space
(typically in the order of few dozens of output la-
bels), heuristically reduce the use of features, es-
pecially of features that test pairs of adjacent la-
bels1, and/or propose heuristics to simulate con-
textual dependencies, via extended tests on the ob-
servations (see discussions in, eg., (Punyakanok
et al, 2005; Liang et al, 2008)). Limitating the
feature set or the number of output labels is how-
ever frustrating for many NLP tasks, where the
type and number of potentially relevant features
are very large. A number of studies have tried to
alleviate this problem. Pal et al (2006) propose
to use a ?sparse? version of the forward-backward
algorithm during training, where sparsity is en-
forced through beam pruning. Related ideas are
discussed by Dietterich et al (2004); by Cohn
(2006), who considers ?generalized? feature func-
tions; and by Jeong et al (2009), who use approx-
imations to simplify the forward-backward recur-
sions. In this paper, we show that the sparsity that
is induced by `1-penalized estimation of CRFs can
be used to reduce the total training time, while
yielding extremely compact models. The benefits
of sparsity are even greater during inference: less
features need to be extracted and included in the
potential functions, speeding up decoding with a
lesser memory footprint. We study and compare
three different ways to implement `1 penalty for
CRFs that have been introduced recently: orthant-
wise Quasi Newton (Andrew and Gao, 2007),
stochastic gradient descent (Tsuruoka et al, 2009)
and coordinate descent (Sokolovska et al, 2010),
concluding that these methods have complemen-
1In CRFsuite (Okazaki, 2007), it is even impossible to
jointly test a pair of labels and a test on the observation, bi-
grams feature are only of the form f(yt?1, yt).
504
tary strengths and weaknesses. Based on an effi-
cient implementation of these algorithms, we were
able to train very large CRFs containing more than
a hundred of output labels and up to several billion
features, yielding results that are as good or better
than the best reported results for two NLP bench-
marks, text phonetization and part-of-speech tag-
ging.
Our contribution is therefore twofold: firstly a
detailed analysis of these three algorithms, dis-
cussing implementation, convergence and com-
paring the effect of various speed-ups. This
comparison is made fair and reliable thanks to
the reimplementation of these techniques in the
same software package. Second, the experimen-
tal demonstration that using large output label sets
is doable and that very large feature sets actually
help improve prediction accuracy. In addition, we
show how sparsity in structured feature sets can
be used in incremental training regimes, where
long-range features are progressively incorporated
in the model insofar as the shorter range features
have proven useful.
The rest of the paper is organized as follows: we
first recall the basics of CRFs in Section 2, and dis-
cuss three ways to train CRFs with a `1 penalty in
Section 3. We then detail several implementation
issues that need to be addressed when dealing with
massive feature sets in Section 4. Our experiments
are reported in Section 5. The main conclusions of
this study are drawn in Section 6.
2 Conditional Random Fields
In this section, we recall the basics of Conditional
Random Fields (CRFs) (Lafferty et al, 2001; Sut-
ton and McCallum, 2006) and introduce the nota-
tions that will be used throughout.
2.1 Basics
CRFs are based on the following model
p?(y|x) =
1
Z?(x)
exp
{
K?
k=1
?kFk(x,y)
}
(1)
where x = (x1, . . . , xT ) and y = (y1, . . . , yT )
are, respectively, the input and output sequences2,
and Fk(x,y) is equal to
?T
t=1 fk(yt?1, yt, xt),
where {fk}1?k?K is an arbitrary set of feature
2Our implementation also includes a special label y0, that
is always observed and marks the beginning of a sequence.
functions and {?k}1?k?K are the associated pa-
rameter values. We denote by Y and X , respec-
tively, the sets in which yt and xt take their values.
The normalization factor in (1) is defined by
Z?(x) =
?
y?Y T
exp
{
K?
k=1
?kFk(x,y)
}
. (2)
The most common choice of feature functions is to
use binary tests. In the sequel, we distinguish be-
tween two types of feature functions: unigram fea-
tures fy,x, associated with parameters ?y,x, and bi-
gram features fy?,y,x, associated with parameters
?y?,y,x. These are defined as
fy,x(yt?1, yt, xt) = 1(yt = y, xt = x)
fy?,y,x(yt?1, yt, xt) = 1(yt?1 = y
?, yt = y, xt = x)
where 1(cond.) is equal to 1 when the condition
is verified and to 0 otherwise. In this setting, the
number of parametersK is equal to |Y |2?|X|train,
where | ? | denotes the cardinal and |X|train refers to
the number of configurations of xt observed dur-
ing training. Thus, even in moderate size applica-
tions, the number of parameters can be very large,
mostly due to the introduction of sequential de-
pendencies in the model. This also explains why it
is hard to train CRFs with dependencies spanning
more than two adjacent labels. Using only uni-
gram features {fy,x}(y,x)?Y?X results in a model
equivalent to a simple bag-of-tokens position-
by-position logistic regression model. On the
other hand, bigram features {fy?,y,x}(y,x)?Y 2?X
are helpful in modelling dependencies between
successive labels. The motivations for using si-
multaneously both types of feature functions are
evaluated experimentally in Section 5.
2.2 Parameter Estimation
Given N independent sequences {x(i),y(i)}Ni=1,
where x(i) and y(i) contain T (i) symbols, condi-
tional maximum likelihood estimation is based on
the minimization, with respect to ?, of the negated
conditional log-likelihood of the observations
l(?) = ?
N?
i=1
log p?(y
(i)|x(i)) (3)
=
N?
i=1
{
logZ?(x
(i))?
K?
k=1
?kFk(x
(i),y(i))
}
This term is usually complemented with an addi-
tional regularization term so as to avoid overfitting
505
(see Section 3.1 below). The gradient of l(?) is
?l(?)
??k
=
N?
i=1
T (i)?
t=1
Ep?(y|x(i)) fk(yt?1, yt, x
(i)
t )
?
N?
i=1
T (i)?
t=1
fk(y
(i)
t?1, y
(i)
t , x
(i)
t ) (4)
where Ep?(y|x) denotes the conditional expecta-
tion given the observation sequence, i.e.
Ep?(y|x) fk(yt?1, yt, x
(i)
t ) =
?
(y?,y)?Y 2
fk(y, y
?, xt) P?(yt?1 = y
?, yt = y|x) (5)
Although l(?) is a smooth convex function, its op-
timum cannot be computed in closed form, and
l(?) has to be optimized numerically. The com-
putation of its gradient implies to repeatedly com-
pute the conditional expectation in (5) for all in-
put sequences x(i) and all positions t. The stan-
dard approach for computing these expectations
is inspired by the forward-backward algorithm for
hidden Markov models: using the notations intro-
duced above, the algorithm implies the computa-
tion of the forward
{
?1(y) = exp(?y,x1 + ?y0,y,x1)
?t+1(y) =
?
y? ?t(y
?) exp(?y,xt+1 + ?y?,y,xt+1)
and backward recursions
{
?Ti(y) = 1
?t(y?) =
?
y ?t+1(y) exp(?y,xt+1 + ?y?,y,xt+1),
for all indices 1 ? t ? T and all labels y ? Y .
Then, Z?(x) =
?
y ?T (y) and the pairwise prob-
abilities P?(yt = y?, yt+1 = y|x) are given by
?t(y
?) exp(?y,xt+1 + ?y?,y,xt+1)?t+1(y)/Z?(x)
These recursions require a number of operations
that grows quadratically with |Y |.
3 `1 Regularization in CRFs
3.1 Regularization
The standard approach for parameter estimation in
CRFs consists in minimizing the logarithmic loss
l(?) defined by (3) with an additional `2 penalty
term ?22 ???
2
2, where ?2 is a regularization parame-
ter. The objective function is then a smooth convex
function to be minimized over an unconstrained
parameter space. Hence, any numerical optimiza-
tion strategy may be used and practical solutions
include limited memory BFGS (L-BFGS) (Liu
and Nocedal, 1989), which is used in the popu-
lar CRF++ (Kudo, 2005) and CRFsuite (Okazaki,
2007) packages; conjugate gradient (Nocedal and
Wright, 2006) and Stochastic Gradient Descent
(SGD) (Bottou, 2004; Vishwanathan et al, 2006),
used in CRFsgd (Bottou, 2007). The only caveat
is to avoid numerical optimizers that require the
full Hessian matrix (e.g., Newton?s algorithm) due
to the size of the parameter vector in usual appli-
cations of CRFs.
The most significant alternative to `2 regulariza-
tion is to use a `1 penalty term ?1???1: such regu-
larizers are able to yield sparse parameter vectors
in which many component have been zeroed (Tib-
shirani, 1996). Using a `1 penalty term thus im-
plicitly performs feature selection, where ?1 con-
trols the amount of regularization and the number
of extracted features. In the following, we will
jointly use both penalty terms, yielding the so-
called elastic net penalty (Zhou and Hastie, 2005)
which corresponds to the objective function
l(?) + ?1???1 +
?2
2
???22 (6)
The use of both penalty terms makes it possible
to control the number of non zero coefficients and
to avoid the numerical problems that might occur
in large dimensional parameter settings (see also
(Chen, 2009)). However, the introduction of a `1
penalty term makes the optimization of (6) more
problematic, as the objective function is no longer
differentiable in 0. Various strategies have been
proposed to handle this difficulty. We will only
consider here exact approaches and will not dis-
cuss heuristic strategies such as grafting (Perkins
et al, 2003; Riezler and Vasserman, 2004).
3.2 Quasi Newton Methods
To deal with `1 penalties, a simple idea is that of
(Kazama and Tsujii, 2003), originally introduced
for maxent models. It amounts to reparameteriz-
ing ?k as ?k = ?
+
k ??
?
k , where ?
+
k and ?
?
k are pos-
itive. The `1 penalty thus becomes ?1(?+ ? ??).
In this formulation, the objective function recovers
its smoothness and can be optimized with conven-
tional algorithms, subject to domain constraints.
Optimization is straightforward, but the number
of parameters is doubled and convergence is slow
506
(Andrew and Gao, 2007): the procedure lacks a
mechanism for zeroing out useless parameters.
A more efficient strategy is the orthant-wise
quasi-Newton (OWL-QN) algorithm introduced in
(Andrew and Gao, 2007). The method is based on
the observation that the `1 norm is differentiable
when restricted to a set of points in which each
coordinate never changes its sign (an ?orthant?),
and that its second derivative is then zero, mean-
ing that the `1 penalty does not change the Hessian
of the objective on each orthant. An OWL-QN
update then simply consists in (i) computing the
Newton update in a well-chosen orthant; (ii) per-
forming the update, which might cause some com-
ponent of the parameter vector to change sign; and
(iii) projecting back the parameter value onto the
initial orthant, thereby zeroing out those compo-
nents. In (Gao et al, 2007), the authors show that
OWL-QN is faster than the algorithm proposed by
Kazama and Tsujii (2003) and can perform model
selection even in very high-dimensional problems,
with no loss of performance compared to the use
of `2 penalty terms.
3.3 Stochastic Gradient Descent
Stochastic gradient (SGD) approaches update the
parameter vector based on an crude approximation
of the gradient (4), where the computation of ex-
pectations only includes a small batch of observa-
tions. SGD updates have the following form
?k ? ?k + ?
?l(?)
??k
, (7)
where ? is the learning rate. In (Tsuruoka et al,
2009), various ways of adapting this update to `1-
penalized likelihood functions are discussed. Two
effective ideas are proposed: (i) only update pa-
rameters that correspond to active features in the
current observation, (ii) keep track of the cumu-
lated penalty zk that ?k should have received, had
the gradient been computed exactly, and use this
value to ?clip? the parameter value. This is imple-
mented by patching the update (7) as follows
{
if (?k > 0) ?k ? max(0, ?k ? zk)
else if (?k < 0) ?k ? min(0, ?k ? zk)
(8)
Based on a study of three NLP benchmarks, the
authors of (Tsuruoka et al, 2009) claim this ap-
proach to be much faster than the orthant-wise ap-
proach and yet to yield very comparable perfor-
mance, while selecting slightly larger feature sets.
3.4 Block Coordinate Descent
The coordinate descent approach of Dud??k et
al. (2004) and Friedman et al (2008) uses the
fact that optimizing a mono-dimensional quadratic
function augmented with a `1 penalty can be per-
formed analytically. For arbitrary functions, this
idea can be adapted by considering quadratic ap-
proximations of the objective around the current
value ??
lk,??(?k) =
?l(??)
??k
(?k ? ??k) +
1
2
?2l(??)
??2k
(?k ? ??k)
2
+ ?1|?k|+
?2
2
?2k + C
st (9)
The minimizer of the approximation (9) is simply
?k =
s
{
?2l(??)
??2k
??k ?
?l(??)
??k
, ?1
}
?2l(??)
??2k
+ ?2
(10)
where s is the soft-thresholding function
s(z, ?) =
?
??
??
z ? ? if z > ?
z + ? if z < ??
0 otherwise
(11)
Coordinate descent is ported to CRFs in
(Sokolovska et al, 2010). Making this scheme
practical requires a number of adaptations,
including (i) approximating the second order
term in (10), (ii) performing updates in block,
where a block contains the |Y | ? |Y + 1| fea-
tures ?y?,y,x and ?y,x for a fixed test x on the
observation sequence and (iii) approximating the
Hessian for a block by its diagonal terms. (ii)
is specially critical, as repeatedly cycling over
individual features to perform the update (10)
is only possible with restricted sets of features.
The block update schemes uses the fact that
all features within a block appear in the same
set of sequences, which means that most of the
computations needed to perform theses updates
can be shared within the block. One advantage
of the resulting algorithm, termed BCD in the
following, is that the update of ?k only involves
carrying out the forward-backward recursions for
the set of sequences that contain symbols x such
that at least one {fk(y?, y, x)}(y,y?)?Y 2 is non
null, which can be much smaller than the whole
training set.
507
4 Implementation Issues
Efficiently processing very-large feature and ob-
servation sets requires to pay attention to many
implementation details. In this section, we present
several optimizations devised to speed up training.
4.1 Sparse Forward-Backward Recursions
For all algorithms, the computation time is domi-
nated by the evaluations of the gradient: our im-
plementation takes advantage of the sparsity to ac-
celerate these computations. Assume the set of bi-
gram features {?y?,y,xt+1}(y?,y)?Y 2 is sparse with
only r(xt+1)  |Y |2 non null values and define
the |Y | ? |Y | sparse matrix
Mt(y
?, y) = exp(?y?,y,xt)? 1.
Using M , the forward-backward recursions are
?t(y) =
?
y?
ut?1(y
?) +
?
y?
ut?1(y
?)Mt(y
?, y)
?t(y
?) =
?
y
vt+1(y) +
?
y
Mt+1(y
?, y)vt+1(y)
with ut?1(y) = exp(?y,xt)?t?1(y) and
vt+1(y) = exp(?y,xt+1)?t+1(y). (Sokolovska et
al., 2010) explains how computational savings can
be obtained using the fact that the vector/matrix
products in the recursions above only involve
the sparse matrix Mt+1(y?, y). They can thus be
computed with exactly r(xt+1) multiplications
instead of |Y |2. The same idea can be used
when the set {?y,xt+1}y?Y of unigram features is
sparse. Using this implementation, the complexity
of the forward-backward procedure for x(i) can be
made proportional to the average number of active
features per position, which can be much smaller
than the number of potentially active features.
For BCD, forward-backward can even be made
slightly faster. When computing the gradient wrt.
features ?y,x and ?y?,y,x (for all the values of y
and y?) for sequence x(i), assuming that x only
occurs once in x(i) at position t, all that is needed
is ??t(y), ?t
? ? t and ??t(y),?t
? ? t. Z?(x) is then
recovered as
?
y ?t(y)?t(y). Forward-backward
recursions can thus be truncated: in our experi-
ments, this divided the computational cost by 1,8
on average.
Note finally that forward-backward is per-
formed on a per-observation basis and is easily
parallelized (see also (Mann et al, 2009) for more
powerful ways to distribute the computation when
dealing with very large datasets). In our imple-
mentation, it is distributed on all available cores,
resulting in significant speed-ups for OWL-QN
and L-BFGS; for BCD the gain is less acute, as
parallelization only helps when updating the pa-
rameters for a block of features that are occur in
many sequences; for SGD, with batches of size
one, this parallelization policy is useless.
4.2 Scaling
Most existing implementations of CRFs, eg.
CRF++ and CRFsgd perform the forward-
backward recursions in the log-domain, which
guarantees that numerical over/underflows are
avoided no matter the length T (i) of the sequence.
It is however very inefficient from an implementa-
tion point of view, due to the repeated calls to the
exp() and log() functions. As an alternative way
of avoiding numerical problems, our implementa-
tion, like crfSuite?s, resorts to ?scaling?, a solution
commonly used for HMMs. Scaling amounts to
normalizing the values of ?t and ?t to one, making
sure to keep track of the cumulated normalization
factors so as to compute Z?(x) and the conditional
expectations Ep?(y|x). Also note that in our imple-
mentation, all the computations of exp(x) are vec-
torized, which provides an additional speed up of
about 20%.
4.3 Optimization in Large Parameter Spaces
Processing very large feature vectors, up to bil-
lions of components, is problematic in many ways.
Sparsity has been used here to speed up forward-
backward, but we have made no attempt to accel-
erate the computation of the OWL-QN updates,
which are linear in the size of the parameter vector.
Of the three algorithms, BCD is the most affected
by increases in the number of features, or more
precisely, in the number of features blocks, where
one block correspond to a specific test of the ob-
servation. In the worst case scenario, each block
may require to visit all the training instances,
yielding terrible computational wastes. In prac-
tice though, most blocks only require to process
a small fraction of the training set, and the ac-
tual complexity depends on the average number of
blocks per observations. Various strategies have
been tried to further accelerate BCD, such as pro-
cessing blocks that only visit one observation in
parallel and updating simultaneously all the blocks
that visit all the training instances, leading to a
small speed-up on the POS-tagging task.
508
Working with billions of features finally re-
quires to worry also about memory usage. In this
respect, BCD is the most efficient, as it only re-
quires to store one K-dimensional vector for the
parameter itself. SGD requires two such vectors,
one for the parameter and one for storing the zk
(see Eq. (8)). In comparison, OWL-QN requires
much more memory, due to the internals of the
update routines, which require several histories of
the parameter vector and of its gradient. Typi-
cally, our implementation necessitates in the order
of a dozen K-dimensional vectors. Parallelization
only makes things worse, as each core will also
need to maintain its own copy of the gradient.
5 Experiments
Our experiments use two standard NLP tasks,
phonetization and part-of-speech tagging, chosen
here to illustrate two very different situations, and
to allow for comparison with results reported else-
where in the literature. Unless otherwise men-
tioned, the experiments use the same protocol: 10
fold cross validation, where eight folds are used
for training, one for development, and one for test-
ing. Results are reported in terms of phoneme er-
ror rates or tag error rates on the test set.
Comparing run-times can be a tricky matter, es-
pecially when different software packages are in-
volved. As discussed above, the observed run-
times depend on many small implementation de-
tails. As the three algorithms share as much code
as possible, we believe the comparison reported
hereafter to be fair and reliable. All experiments
were performed on a server with 64G of memory
and two Xeon processors with 4 cores at 2.27 Ghz.
For comparison, all measures of run-times include
the cumulated activity of all cores and give very
pessimistic estimates of the wall time, which can
be up to 7 times smaller. For OWL-QN, we use 5
past values of the gradient to approximate the in-
verse of the Hessian matrix: increasing this value
had no effect on accuracy or convergence and was
detrimental to speed; for SGD, the learning rate
parameter was tuned manually.
Note that we have not spent much time optimiz-
ing the values of ?1 and ?2. Based on a pilot study
on Nettalk, we found that taking ?1 = .5 and ?2 in
the order of 10?5 to yield nearly optimal perfor-
mance, and have used these values throughout.
5.1 Tasks and Settings
5.1.1 Nettalk
Our first benchmark is the word phonetization
task, using the Nettalk dictionary (Sejnowski and
Rosenberg, 1987). This dataset contains approxi-
mately 20,000 English word forms, their pronun-
ciation, plus some prosodic information (stress
markers for vowels, syllabic parsing for con-
sonants). Grapheme and phoneme strings are
aligned at the character level, thanks to the use of
a ?null sound? in the latter string when it is shorter
than the former; likewise, each prosodic mark is
aligned with the corresponding letter. We have de-
rived two test conditions from this database. The
first one is standard and aims at predicting the pro-
nunciation information only. In this setting, the set
of observations (X) contains 26 graphemes, and
the output label set contains |Y | = 51 phonemes.
The second condition aims at jointly predict-
ing phonemic and prosodic information3. The rea-
sons for designing this new condition are twofold:
firstly, it yields a large set of composite labels
(|Y | = 114) and makes the problem computation-
ally challenging. Second, it allows to quantify how
much the information provided by the prosodic
marks help predict the phonemic labels. Both in-
formation are quite correlated, as the stress mark
and the syllable openness, for instance, greatly in-
fluence the realization of some archi-phonemes.
The features used in Nettalk experiments take
the form fy,w (unigram) and fy?,y,w (bigram),
where w is a n-gram of letters. The n-grm feature
sets (n = {1, 3, 5, 7}) includes all features testing
embedded windows of k letters, for all 0 ? k ? n;
the n-grm- setting is similar, but only includes
the window of length n; in the n-grm+ setting,
we add features for odd-size windows; in the n-
grm++ setting, we add all sequences of letters up
to size n occurring in current window. For in-
stance, the active bigram features at position t = 2
in the sequence x=?lemma? are as follows: the 3-
grm feature set contains fy,y? , fy,y?,e and fy?,y,lem;
only the latter appears in the 3-grm- setting. In
the 3-grm+ feature set, we also have fy?,y,le and
fy?,y,em. The 3-grm++ feature set additionally in-
cludes fy?,y,l and fy?,y,m. The number of features
ranges from 360 thousands (1-grm setting) to 1.6
billion (7-grm).
3Given the design of the Nettalk dictionary, this experi-
ment required to modify the original database so as to reas-
sign prosodic marks to phonemes, rather than to letters.
509
Features With Without
Nettalk
3-grm 10.74% 14.3M 14.59% 0.3M
5-grm 8.48% 132.5M 11.54% 2.5M
POS tagging
base 2.91% 436.7M 3.47% 70.2M
Table 1: Features jointly testing label pairs and
the observation are useful (error rates and features
counts.)
`2 `1-sparse `1 % zero
1-grm 84min 41min 57min 44.6%
3-grm- 65min 16min 44min 99.6%
3-grm 72min 48min 58min 19.9%
Table 2: Sparse vs standard forward-backward
(training times and percentages of sparsity of M )
5.1.2 Part-of-Speech Tagging
Our second benchmark is a part-of-speech (POS)
tagging task using the PennTreeBank corpus
(Marcus et al, 1993), which provides us with a
quite different condition. For this task, the number
of labels is smaller (|Y | = 45) than for Nettalk,
and the set of observations is much larger (|X| =
43207). This benchmark, which has been used in
many studies, allows for direct comparisons with
other published work. We thus use a standard ex-
perimental set-up, where sections 0-18 of the Wall
Street Journal are used for training, sections 19-21
for development, and sections 22-24 for testing.
Features are also standard and follow the design
of (Suzuki and Isozaki, 2008) and test the current
words (as written and lowercased), prefixes and
suffixes up to length 4, and typographical charac-
teristics (case, etc.) of the words. Our baseline
feature set alo contains tests on individual and
pairs of words in a window of 5 words.
5.2 Using Large Feature Sets
The first important issue is to assess the benefits
of using large feature sets, notably including fea-
tures testing both a bigram of labels and an obser-
vation. Table 1 compares the results obtained with
and without these features for various setting (us-
ing OWL-QN to perform the optimization), sug-
gesting that for the tasks at hand, these features
are actually helping.
`2 `1 Elastic-net
1-grm 17.81% 17.86% 17.79%
3-grm 10.62% 10.74% 10.70%
5-grm 8.50% 8.45% 8.48%
Table 3: Error rates of the three regularizers on the
Nettalk task.
5.3 Speed, Sparsity, Convergence
The training speed depends of two main factors:
the number of iterations needed to achieve conver-
gence and the computational cost of one iteration.
In this section, we analyze and compare the run-
time efficiency of the three optimizers.
5.3.1 Convergence
As far as convergence is concerned, the two forms
of regularization (`2 and `1) yield the same per-
formance (see Table 3), and the three algorithms
exhibit more or less the same behavior. They
quickly reach an acceptable set of active param-
eters, which is often several orders of magnitude
smaller than the whole parameter set (see results
below in Table 4 and 5). Full convergence, re-
flected by a stabilization of the objective function,
is however not so easily achieved. We have of-
ten observed a slow, yet steady, decrease of the
log-loss, accompanied with a diminution of the
number of active features as the number of iter-
ations increases. Based on this observation, we
have chosen to stop all algorithms based on their
performance on an independent development set,
allowing a fair comparison of the overall training
time; for OWL-QN, it allowed to divide the total
training time by almost 2.
It has finally often been found useful to fine
tune the non-zero parameters by running a final
handful of L-BFGS iterations using only a small
`2 penalty; at this stage, all the other features are
removed from the model. This had a small impact
BCD and SGD?s performance and allowed them to
catch up with OWL-QN?s performance.
5.3.2 Sparsity and the Forward-Backward
As explained in section 4.1, the forward-backward
algorithm can be written so as to use the sparsity
of the matrix My,y?,x. To evaluate the resulting
speed-up, we ran a series of experiments using
Nettalk (see Table 2). In this table, the 3-grm- set-
ting corresponds to maximum sparsity for M , and
training with the sparse algorithm is three times
faster than with the non-sparse version. Throwing
510
Method Iter. # Feat. Error Time
O
W
L
-Q
N 1-grm 63.4 4684 17.79% 11min
7-grm 140.2 38214 8.12% 1h02min
5-grm+ 141.0 43429 7.89% 1h37min
S
G
D 1-grm 21.4 3540 18.21% 9min
5-grm+ 28.5 34319 8.01% 45min
B
C
D
1-grm 28.2 5017 18.27% 27min
7-grm 9.2 3692 8.21% 1h22min
5-grm+ 8.7 47675 7.91% 2h18min
Table 4: Performance on Nettalk
in more features has the effect of making M much
more dense, mitigating the benefits of the sparse
recursions. Nevertheless, even for very large fea-
ture sets, the percentage of zeros in M averages
20% to 30%, and the sparse version remains 10 to
20% faster than the non-sparse one. Note that the
non-sparse version is faster with a `1 penalty term
than with only the `2 term: this is because exp(0)
is faster to evaluate than exp(x) when x 6= 0.
5.3.3 Training Speed and Test Accuracy
Table 4 displays the results achieved on the Nettalk
task. The three algorithms yield very compara-
ble accuracy results, and deliver compact models:
for the 5-gram+ setting, only 50,000 out of 250
million features are selected. SGD is the fastest
of the three, up to twice as fast as OWL-QN and
BCD depending on the feature set. The perfor-
mance it achieves are consistently slightly worst
than the other optimizers, and only catch up when
the parameters are fine-tuned (see above). There
are not so many comparisons for Nettalk with
CRFs, due to the size of the label set. Our results
compare favorably with those reported in (Pal et
al., 2006), where the accuracy attains 91.7% us-
ing 19075 examples for training and 934 for test-
ing, and with those in (Jeong et al, 2009) (88.4%
accuracy with 18,000 (2,000) training (test) in-
stances). Table 5 gives the results obtained for
the larger Nettalk+prosody task. Here, we only
report the results obtained with SGD and BCD.
For OWL-QN, the largest model we could han-
dle was the 3-grm model, which contained 69 mil-
lion features, and took 48min to train. Here again,
performance steadily increase with the number of
features, showing the benefits of large-scale mod-
els. We lack comparisons for this task, which
seems considerably harder than the sole phone-
tization task, and all systems seem to plateau
around 13.5% accuracy. Interestingly, simulta-
Method Error Time
S
G
D 5-grm 14.71% / 8.11% 55min
5-grm+ 13.91% / 7.51% 2h45min
B
C
D
5-grm 14.57% / 8.06% 2h46min
7-grm 14.12% / 7.86% 3h02min
5-grm+ 13.85% / 7.47% 7h14min
5-grm++ 13.69% / 7.36% 16h03min
Table 5: Performance on Nettalk+prosody. Error
is given for both joint labels and phonemic labels.
neously predicting the phoneme and its prosodic
markers allows to improve the accuracy on the pre-
diction of phonemes, which improves of almost a
half point as compared to the best Nettalk system.
For the POS tagging task, BCD appears to be
unpractically slower to train than the others ap-
proaches (SGD takes about 40min to train, OWL-
QN about 1 hour) due the simultaneous increase
in the sequence length and in the number of ob-
servations. As a result, one iteration of BCD typi-
cally requires to repeatedly process over and over
the same sequences: on average, each sequence is
visited 380 times when we use the baseline fea-
ture set. This technique should reserved for tasks
where the number of blocks is small, or, as below,
when memory usage is an issue.
5.4 Structured Feature Sets
In many tasks, the ambiguity of tokens can be re-
duced by looking up increasingly large windows
of local context. This strategy however quickly
runs into a combinatorial increase of the number
of features. A side note of the Nettalk experiments
is that when using embedded features, the active
feature set tends to reflect this hierarchical organi-
zation. This means that when a feature testing a
n-gram is active, in most cases, the features for all
embedded k-grams are also selected.
Based on this observation, we have designed
an incremental training strategy for the POS tag-
ging task, where more specific features are pro-
gressively incorporated into the model if the cor-
responding less specific feature is active. This ex-
periment used BCD, which is the most memory ef-
ficient algorithm. The first iteration only includes
tests on the current word. During the second it-
eration, we add tests on bigram of words, on suf-
fixes and prefixes up to length 4. After four itera-
tions, we throw in features testing word trigrams,
subject to the corresponding unigram block being
active. After 6 iterations, we finally augment the
511
model with windows of length 5, subject to the
corresponding trigram being active. After 10 iter-
ations, the model contains about 4 billion features,
out of which 400,000 are active. It achieves an
error rate of 2.63% (resp. 2.78%) on the develop-
ment (resp. test) data, which compares favorably
with some of the best results for this task (for in-
stance (Toutanova et al, 2003; Shen et al, 2007;
Suzuki and Isozaki, 2008)).
6 Conclusion and Perspectives
In this paper, we have discussed various ways to
train extremely large CRFs with a `1 penalty term
and compared experimentally the results obtained,
both in terms of training speed and of accuracy.
The algorithms studied in this paper have com-
plementary strength and weaknesses: OWL-QN is
probably the method of choice in small or moder-
ate size applications while BCD is most efficient
when using very large feature sets combined with
limited-size observation alphabets; SGD comple-
mented with fine tuning appears to be the preferred
choice in most large-scale applications. Our anal-
ysis demonstrate that training large-scale sparse
models can be done efficiently and allows to im-
prove over the performance of smaller models.
The CRF package developed in the course of this
study implements many algorithmic optimizations
and allows to design innovative training strategies,
such as the one presented in section 5.4. This
package is released as open-source software and
is available at http://wapiti.limsi.fr.
In the future, we intend to study how spar-
sity can be used to speed-up training in the face
of more complex dependency patterns (such as
higher-order CRFs or hierarchical dependency
structures (Rozenknop, 2002; Finkel et al, 2008).
From a performance point of view, it might also
be interesting to combine the use of large-scale
feature sets with other recent improvements such
as the use of semi-supervised learning techniques
(Suzuki and Isozaki, 2008) or variable-length de-
pendencies (Qian et al, 2009).
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of the International Conference on Machine
Learning, pages 33?40, Corvalis, Oregon.
Le?on Bottou. 2004. Stochastic learning. In Olivier
Bousquet and Ulrike von Luxburg, editors, Ad-
vanced Lectures on Machine Learning, Lecture
Notes in Artificial Intelligence, LNAI 3176, pages
146?168. Springer Verlag, Berlin.
Le?on Bottou. 2007. Stochastic gradient descent (sgd)
implementation. http://leon.bottou.org/projects/sgd.
Stanley Chen. 2009. Performance prediction for ex-
ponential language models. In Proceedings of the
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 450?458, Boulder, Colorado, June.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of the 17th
European Conference on Machine Learning, pages
606?613, Berlin, September.
Thomas G. Dietterich, Adam Ashenfelter, and Yaroslav
Bulatov. 2004. Training conditional random fields
via gradient tree boosting. In Proceedings of
the International Conference on Machine Learning,
Banff, Canada.
Miroslav Dud??k, Steven J. Phillips, and Robert E.
Schapire. 2004. Performance guarantees for reg-
ularized maximum entropy density estimation. In
John Shawe-Taylor and Yoram Singer, editors, Pro-
ceedings of the 17th annual Conference on Learning
Theory, volume 3120 of Lecture Notes in Computer
Science, pages 472?486. Springer.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics, pages 959?967, Columbus, Ohio.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear
models via coordinate descent. Technical report,
Department of Statistics, Stanford University.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824?831, Prague, Czech republic.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of crfs for large-scale nat-
ural language data. In Proceedings of the Joint Con-
ference of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing,
pages 281?284, Suntec, Singapore.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 137?144.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net/.
512
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.
Percy Liang, Hal Daume?, III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592?599.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Gideon Mann, Ryan McDonald, Mehryar Mohri,
Nathan Silberman, and Dan Walker. 2009. Efficient
large-scale distributed training of conditional maxi-
mum entropy models. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A.Culotta, editors,
Advances in Neural Information Processing Systems
22, pages 1231?1239.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn treebank. Com-
putational Linguistics, 19(2):313?330.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer.
Naoaki Okazaki. 2007. CRFsuite: A fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Chris Pal, Charles Sutton, and Andrew McCallum.
2006. Sparse forward-backward using minimum di-
vergence beams for fast training of conditional ran-
dom fields. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Process-
ing, Toulouse, France.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremental feature selection
by gradient descent in function space. Journal of
Machine Learning Research, 3:1333?1356.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proceedings of the International
Joint Conference on Artificial Intelligence, pages
1124?1129.
Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing
Huang, and Lide Wu. 2009. Sparse higher order
conditional random fields for improved sequence la-
beling. In Proceedings of the Annual International
Conference on Machine Learning, pages 849?856.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and l1 regularization for re-
laxed maximum-entropy modeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of the confer-
ence on Empirical Methods in Natural Language
Processing, pages 174?181, Barcelona, Spain, July.
Antoine Rozenknop. 2002. Mode`les syntaxiques
probabilistes non-ge?ne?ratifs. Ph.D. thesis, Dpt.
d?informatique, E?cole Polytechnique Fe?de?rale de
Lausanne.
Terrence J. Sejnowski and Charles R. Rosenberg.
1987. Parallel networks that learn to pronounce en-
glish text. Complex Systems, 1.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic.
Nataliya Sokolovska, Thomas Lavergne, Olivier
Cappe?, and Franc?ois Yvon. 2010. Efficient learning
of sparse conditional random fields for supervised
sequence labelling. IEEE Selected Topics in Signal
Processing.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors, In-
troduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of the
Conference of the Association for Computational
Linguistics on Human Language Technology, pages
665?673, Columbus, Ohio.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. J.R.Statist.Soc.B, 58(1):267?
288.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
173?180.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of the Joint Conference
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing, pages
477?485, Suntec, Singapore.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark
Schmidt, and Kevin Murphy. 2006. Accelerated
training of conditional random fields with stochas-
tic gradient methods. In Proceedings of the 23th In-
ternational Conference on Machine Learning, pages
969?976. ACM Press, New York, NY, USA.
Hui Zhou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. J. Royal. Stat.
Soc. B., 67(2):301?320.
513
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 41?48
Manchester, August 2008
Using LDA to detect semantically incoherent documents
Hemant Misra and Olivier Cappe?
LTCI/CNRS and TELECOM ParisTech
{misra,cappe}@enst.fr
Franc?ois Yvon
Univ Paris-Sud 11 and LMISI-CNRS
yvon@limsi.fr
Abstract
Detecting the semantic coherence of a doc-
ument is a challenging task and has sev-
eral applications such as in text segmenta-
tion and categorization. This paper is an
attempt to distinguish between a ?semanti-
cally coherent? true document and a ?ran-
domly generated? false document through
topic detection in the framework of latent
Dirichlet analysis. Based on the premise
that a true document contains only a few
topics and a false document is made up of
many topics, it is asserted that the entropy
of the topic distribution will be lower for
a true document than that for a false docu-
ment. This hypothesis is tested on several
false document sets generated by various
methods and is found to be useful for fake
content detection applications.
1 Introduction
The ?Internet revolution? has dramatically in-
creased the monetary value of higher ranking on
the web search engines index, fostering the ex-
pansion of techniques, collectively known as ?Web
Spam?, that fraudulently help to do so. Internet is
indeed ?polluted? with fake Web sites whose only
purpose is to deceive the search engines by arti-
ficially pushing up the popularity of commercial
sites, or sites promoting illegal content 1. These
fake sites are often forged using very crude content
generation techniques, ranging from web scrap-
ping (blending of chunks of actual contents) to
simple-minded text generation techniques based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1The annual AirWeb challenge http://airweb.
cse.lehigh.edu gives a state-of-the art on current Web
Spam detection techniques.
on random sampling of words (?word salads?),
or randomly replacing words in actual documents
(?word stuffing?) 2. Among these, the latter two
are easy to detect using simple statistical models
of natural texts, but the former is more challeng-
ing, it being made up of actual sentences: recog-
nizing these texts as forged requires either to resort
to plagiarism detection techniques, or to automati-
cally identify their lack of semantic consistency.
Detecting the consistency of texts or of text
chunks has many applications in Natural Language
Processing. So far, it has been used mainly in the
context of automatic text segmentation, where a
change in vocabulary is often the mark of topic
change (Hearst, 1997), and, to a lesser extent, in
discourse studies (see, e.g., (Foltz et al, 1998)).
It could also serve to devise automatic metrics for
text summarization or machine translation tasks.
This paper is an attempt to address the issue
of differentiating between ?true? and ?false? doc-
uments on the basis of their consistency through
topic modeling approach. We have used La-
tent Dirichlet alocation (LDA) (Blei et al, 2002)
model as our main topic modeling tool. One of the
aims of LDA and similar methods, including prob-
abilistic latent semantic analysis (PLSA) (Hof-
mann, 2001), is to produce low dimensionality rep-
resentations of texts in a ?semantic space? such
that most of their inherent statistical characteristics
are preserved. A reduction in dimensionality facil-
itates storage as well as faster retrieval. Modeling
discrete data has many applications in classifica-
tion, categorization, topic detection, data mining,
information retrieval (IR), summarization and col-
laborative filtering (Buntine and Jakulin, 2004).
The aim of this paper is to test LDA for es-
tablishing the semantic coherence of a document
based on the premise that a real (coherent) docu-
ment should discuss only a few number of topics,
2The same techniques are commonly used in mail spams
also.
41
a property hardly granted for forged documents
which are often made up of random assemblage
of words or sentences. As a consequence, the co-
herence of a document may reflect in the entropy
of its posterior topic distribution or in its perplex-
ity for the model. The entropy of the estimated
topic distribution of a true document is expected to
be lower than that of a fake document. Moreover,
the length normalized log-likelihood of a true and
coherent document may be higher as compared to
that of a false and incoherent document.
In this paper, we compare two methods to esti-
mate the posterior topic distribution of test docu-
ments, and this study is also an attempt to inves-
tigate the role of different parameters on the effi-
ciency of these methods.
This paper is organized as follows: In Section 2,
the basics of the LDA model are set. We then dis-
cuss and contrast several approaches to the prob-
lem of inferring the topic distribution of a new
document in Section 3. In Section 4, we describe
the corpus and experimental set-up that are used
to produce the results presented in Section 5. We
summarize our main findings and draw perspec-
tives for future research in Section 6.
2 Latent Dirichlet Allocation
2.1 Basics
LDA is a probabilistic model of text data which
provides a generative analog of PLSA (Blei et al,
2002), and is primarily meant to reveal hidden top-
ics in text documents. In (Griffiths and Steyvers,
2004), the authors used LDA for identifying ?hot
topics? by analyzing the temporal dynamics of top-
ics over a period of time. More recently LDA has
also been used for unsupervised language model
(LM) adaptation in the context of automatic speech
recognition (ASR) (Hsu and Glass, 2006; Tam
and Schultz, 2007; Heidel et al, 2007). Several
extensions of the LDA model, such as hierarchi-
cal LDA (Blei et al, 2004), HMM-LDA (Grif-
fiths et al, 2005), correlated topic models (Blei
and Lafferty, 2005) and hidden topic Markov mod-
els (Gruber et al, 2007), have been proposed, that
introduce more complex dependency patterns in
the model.
Like most of the text mining techniques, LDA
assumes that documents are made up of words and
the ordering of the words within a document is
unimportant (?bag-of-words? assumption). Con-
trary to the simpler Multinomial Mixture Model
(see, e.g., (Nigam et al, 2000) and Section 2.4),
LDA assumes that every document is represented
by a topic distribution and that each topic defines
an underlying distribution on words.
The generative history of a document (a bag-
of-words) collection is the following: Assuming
a fixed and known number of topics n
T
, for each
topic t, a distribution ?
t
over the indexing vocab-
ulary (w = 1 . . . n
W
) is drawn from a Dirichlet
distribution. Then, for each document d, a distri-
bution ?
d
over the topics (t = 1 . . . n
T
) is drawn
from a Dirichlet distribution. For a document d,
the document length l
d
being an exogenous vari-
able, the next step consists of drawing a topic t
i
from ?
d
for each position i = 1...l
d
. Finally, a
word is selected from the chosen topic t
i
. Given
the topic distribution, each word is thus drawn in-
dependently from every other word using a docu-
ment specific mixture model. The probability of
i
th word token is thus:
P (w
i
|?
d
, ?) =
n
T
?
t=1
P (t
i
= t|?
d
)P (w
i
|t
i
, ?) (1)
=
n
T
?
t=1
?
dt
?
tw
(2)
Conditioned on ? and ?
d
, the likelihood of doc-
ument d is a mere product of terms such as (2),
which can be rewritten as:
P (C
d
|?
d
, ?) =
n
W
?
w=1
[
n
T
?
t=1
(?
dt
?
tw
)
]
C
dw
(3)
where C
dw
is the count of word w in d.
2.2 LDA: Training
LDA training consists of estimating the following
two parameter vectors from a text collection: the
topic distribution in each document d (?
dt
, t =
1...n
T
, d = 1...n
D
) and the word distribution in
each topic (?
tw
, t = 1...n
T
, w = 1...n
W
). Both
?
d
and ?
t
define discrete distributions, respectively
over the set of topics and over the set of words.
Various methods have been proposed to estimate
LDA parameters, such as variational method (Blei
et al, 2002), expectation propagation (Minka and
Lafferty, 2002) and Gibbs sampling (Griffiths and
Steyvers, 2004). In this paper, we have used
the latter approach, which boils down to repeat-
edly going through the training data and sampling
the topic assigned to each word token conditioned
42
on the topic assigned to all the other word to-
kens. Given a particular Gibbs sample, the pos-
teriors for ? and ? are 3: Dirichlet with parameters
(K
t1
+?, . . . ,K
tn
W
+?) and Dirichlet with param-
eters (J
d1
+ ?, . . . , J
dn
T
+ ?), respectively, where
K
tw
is the number of times word w is assigned to
topic t and J
dt
is the number of times topic t is as-
signed to some word token in document d. Hence,
?
tw
=
K
tw
+ ?
?
n
W
k=1
K
tk
+ n
W
?
(4)
?
dt
=
J
dt
+ ?
?
n
T
k=1
J
dk
+ n
T
?
(5)
During the Gibbs sampling phase, ?
t
and ?
d
are
sampled from the above posteriors while the final
estimates for these parameters are obtained by av-
eraging the posterior means over the complete set
of Gibbs iteration.
2.3 LDA: Testing
Training LDA model on a text collection already
provides interesting insights regarding the the-
matic structure of the collection. This has been the
primary application of LDA in (Blei et al, 2002;
Griffiths and Steyvers, 2004). Even better, being
a generative model, LDA can be used to make
prediction regarding novel documents (assuming
they use the same vocabulary as the training cor-
pus). In a typical IR setting, where the main fo-
cus is on computing the similarity between a doc-
ument d and a query d?, a natural similarity mea-
sure is given by P (C
d
? |?
d
, ?), computed according
to (3) (Buntine et al, 2004).
An alternative would be to compute the KL di-
vergence between the topic distribution in d and d?,
which however requires to infer the latter quantity.
As the topic distribution of a (new) document gives
its representation along the latent semantic dimen-
sions, computing this value is helpful for many
applications, including text segmentation and text
classification. Methods for efficiently and accu-
rately estimating topic distribution for text docu-
ments are presented and evaluated in Section 3.
2.4 Baseline: Multinomial Mixture Model
The performance of LDA model is compared
with that of the simpler multinomial mixture
model (Nigam et al, 2000; Rigouste et al, 2007).
3assuming non-informative priors with hyper-parameters
? and ? for the Dirichlet distribution over topics and the
Dirichlet distribution over words respectively
In this model, every word in a document belongs
to the same topic, as if the document specific topic
distribution ?
d
in LDA were bound to lie on one
vertex of the [0, 1]nT simplex. Using the same no-
tations as before (except for ?
t
, which now denotes
the position independent probability of topic t in
the collection), the probability of a document is:
P (C
d
|?
t
, ?) =
n
T
?
t=1
?
t
n
W
?
w=1
?
C
dw
tw
(6)
This model can be trained through expectation
maximization (EM), using the following reestima-
tion formulas, where (7) defines the E-step; (8) and
(9) define the M-step.
P (t|C
d
, ?, ?) =
?
t
?
n
W
w=1
(?
?
tw
)
C
dw
?
n
T
t=1
?
t
?
n
W
w=1
(?
tw
)
C
dw
(7)
?
?
t
? ? +
n
D
?
d=1
P (t|C
d
, ?, ?) (8)
?
?
tw
? ? +
n
D
?
d=1
C
dw
P (t|C
d
, ?, ?) (9)
As suggested in (Rigouste et al, 2007), we ini-
tialize the EM algorithm by drawing initial topic
distributions from a prior Dirichlet distribution
with hyper-parameter ? = 1. ? = 0.1 in all the
experiments.
During testing, the parameters of the multino-
mial models are used to estimate the posterior topic
distribution in each document using (7). The like-
lihood of a test document is given by (6).
3 Inferring the Topic Distribution of Test
Documents
P (C
d
|?
d
), the conditional probability of a docu-
ment d given ?
d
is obtained using (3) 4. Computing
the likelihood of a test document requires to inte-
grate this quantity over ?; likewise for the compu-
tation of the posterior distribution of ?. This inte-
gral has no close form solution, but can be approx-
imated using Monte-Carlo sampling techniques as:
P (C
d
) ? 1
M
M
?
m=1
P (C
d
|?(m)) (10)
where ?(m) denotes the mth sample from the
Dirichlet prior, and M is the number of Monte
4The dependence on ? is dropped for simplicity. ? is
learned during training and kept fixed during testing.
43
Carlo samples. Given the typical length of doc-
uments and the large vocabulary size, small scale
experiments convinced us that a cruder approxima-
tion was in order, as the sum in (10) is dominated
by the maximum value. We thus contend ourselves
to solve:
?
?
= argmax
?,
P
t
?
t
=1
P (C
d
|?) (11)
and use this value to approximate P (C
d
) using (3).
The maximization program (11) has no close
form solution. However, the objective function is
differentiable and log-concave, and can be opti-
mized in a number of ways. We considered two
different algorithms: an EM-like approach, ini-
tially introduced in (Heidel et al, 2007), and an ex-
ponentiated gradient approach (Kivinen and War-
muth, 1997; Globerson et al, 2007).
The first approach implements an iterative pro-
cedure based on the following update rule:
?
dt
? 1
l
d
n
W
?
w=1
C
dw
?
dt
?
tw
?
n
T
t
?
=1
?
dt
?
?
t
?
w
(12)
Although no justification was given in (Hei-
del et al, 2007), it can be shown that this
update rule converges towards a global opti-
mum of the likelihood. Let ? and ?? be two
topic distributions in the n
T
-dimensional simplex,
L(?) = log P (C
d
|?), and ?
t
(w, ?) =
?
t
?
tw
P
t
?
?
t
?
?
t
?
w
.
We define an auxiliary function Q(?, ??) =
?
w
C
w
(
?
t
?
t
(w, ?) log(?
?
t
)). Q(?, ?
?
) is concave
in ??, and performs the role played by the auxil-
iary function in the EM algorithm. Simple cal-
culus suffices to prove that (i) the update (12)
maximizes in ?? the function Q(?, ??), and (ii)
Q(?, ?
?
) ? Q(?, ?) ? L(??) ? L(?), which stems
from the concavity of the log. At an optimum of
Q(?, ?
?
) the positivity of the first term implies the
positivity of the second. Maximizing Q using the
update rule (12) thus increases the likelihood and
repeating this update converges towards the opti-
mum value. We experimented both with an un-
smoothed (12) and with a smoothed version of this
update rule. The unsmoothed version yielded a
slightly better result than the smoothed one.
Exponentiated gradient (Kivinen and Warmuth,
1997; Globerson et al, 2007) yields an alternative
update rule:
?
dt
? ?
dt
exp
(
?
n
W
?
w=1
C
dw
?
tw
?
n
T
t
?
=1
?
dt
?
?
t
?
w
)
(13)
where ? defines the convergence rate. In this form,
the update rule does not preserve the normaliza-
tion of ?, which needs to be performed after every
iteration.
A systematic comparison of these rules was car-
ried out, yielding the following conclusions:
? the convergence of the EM-like method is
very fast. Typically, it requires less than half
a dozen iterations to converge. After conver-
gence, the topic distribution estimated by this
method for a subset of train documents was
always very close (as measured by the KL-
divergence) to the respective topic distribu-
tion of the same documents observed at the
end of the LDA training. Taking n
T
= 50,
the average KL divergence for a set of 4,500
documents was found to be less than 0.5.
? exponentiated gradient has a more erratic be-
haviour, and requires a careful tuning of ? on
a per document basis. For large values of ?,
the update rule (13) sometimes fails to con-
verge; smaller values of ? allowed to consis-
tently reach convergence, but required more
iterations (typically 20-30). On a positive
side, on an average, the topic distributions
estimated by this method are better than the
ones obtained with the EM-like algorithm.
Based on these findings, we decided to use the
EM-like algorithm in all our subsequent experi-
ments.
4 Experimental protocol
4.1 Training and test corpora
The Reuters Corpus Volume 1 (RCV1) (Lewis et
al., 2004) is a collection of over 800,000 news
items in English from August 1996 to August
1997. Out of the entire RCV1 dataset, we se-
lected 27,672 documents (news items) for training
(TrainReuters) and 23,326 documents for testing
(TestReuters). The first 4000 documents from the
TestReuters dataset were used as true documents
(TrueReuters) in the experiments reported in this
paper. The vocabulary size in the train set, after
removing the function words, is 93, 214.
Along with these datasets of ?true? documents,
three datasets of fake documents were also cre-
ated. Document generation techniques are many:
here we consider documents made by mixing short
passages from various texts and documents made
44
by assembling randomly chosen words (sometimes
called as ?word salads?). In addition, we also
consider the case of documents generated with a
stochastic language model (LM). Our ?fake? test
documents are thus composed of:
? (SentenceSalad) obtained by randomly pick-
ing sentences from TestReuters.
? (WordSalad) created by generating random
sentences from a conventional unigram LM
trained on TrainReuters.
? (Markovian) created by generating random
sentences from a conventional 3-gram LM
trained on TrainReuters.
Each of these forged document set contains 4,000
documents.
To assess the performance on out-of-domain
data, we replicated the same tests using 2,000
Medline abstracts (Ohta et al, 2002). 1,500 doc-
uments were used either to generate fake docu-
ments by picking sentences randomly or to train an
LM and then using the LM to generate fake docu-
ments. The remaining 500 abstracts were set aside
as ?true? documents (TrueMedline).
4.2 Performance Measurements : EER
The entropy of the topic distribution is computed
as H = ?
?
T
j=1
?
?
dj
log
?
?
dj
. The other measure
of interest is the average ?log-likelihood per word?
(LLPW) 5.
While evaluating the performance of our sys-
tem, two types of errors are encountered: false ac-
ceptance (FA) when a false document is accepted
as a true document and false rejection (FR) when a
true document is rejected as a false document. The
rate of FA and FR is dependent on the threshold
used for taking the decision, and usually the per-
formance of a system is shown by its receiver op-
erating characteristic (ROC) curve which is a plot
between FA and FR rates for different values of
threshold. Instead of reporting the performance of
a system based on two error rates (FA and FR),
the general practice is to report the performance in
terms of equal-error-rate (EER). The EER is the
error rate at the threshold where FA rate = FR rate.
In our system, a threshold on entropy (or
LLPW) is used for taking the decision, and all the
5This measure is directly related to the text per-
plexity in the model, according to perplexity =
2
?average log-likelihood per word
documents having their entropy (or LLPW) below
(or above) the threshold are accepted as true doc-
uments. The EER is obtained on the test set by
changing the threshold on the test set itself, and
the best results thus obtained are reported.
5 Detecting semantic inconsistency
5.1 Detecting fake documents with LDA and
Multinomial mixtures
In the first set of experiments, the LLPW and en-
tropy of the topic distribution (the two measures)
of the Multinomial mixture and LDA models were
compared to check the ability of these two mea-
sures and models in discriminating between true
and false documents. These results are summa-
rized in Table 1.
TrueReuters vs. MultinomialLLPW Entropy
SentenceSalad 15.3% 48.8%
WordSalad 9.3% 35.8%
Markovian 17.6% 38.9%
TrueReuters vs. LDALLPW Entropy
SentenceSalad 18.9% 0.88%
WordSalad 9.9% 0.13%
Markovian 25.0% 0.28%
Table 1: Performance of the Multinomial Mixture
and LDA
For the multinomial mixture model, the LLPW
measure is able to discriminate between true and
false documents to a certain extent. As expected
(not shown here), the LLPW of the true documents
is usually higher than that of the false documents.
In contrast, the entropy of the posterior topic dis-
tribution does not help much in discriminating be-
tween true and false documents. In fact it remains
close to zero (meaning that only one topic is ?ac-
tive?) both for true and false documents.
The behaviour of the LDA scores is entirely dif-
ferent. The perplexity scores (LLPW) of true and
fake texts are comparable, and do not make useful
predictors. In contrast, the entropy of the topic dis-
tribution allows to sort true documents from fake
ones with a very high accuracy for all kinds of fake
texts considered in this paper. Both results stem
from the ability of LDA to assign a different topic
to each word occurrence.
Similar pattern is observed for our three false
test sets (against the TrueReuters set) with small
45
variations The texts generated with a Markov
model, no matter the order, have the highest en-
tropy, reflecting the absence of long range corre-
lation in the generation model. Though the texts
generated by mixing sentences are more confus-
ing with the true documents, the performance is
still less than 1% EER. Texts mixing a high num-
ber of topics (e.g., Sentence Salads) are almost as
likely as natural texts that address only a few top-
ics. However, the former has much higher entropy
of the topic distribution due to a large number of
topics being active in such texts (see also Figure 1).
0 1 2 3 4 5 6
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Entropy
N
or
m
al
iz
ed
 fr
eq
ue
nc
y 
of
 o
cc
ur
en
ce
 
 
True: TrueReuters
False: SentenceSalad
False: WordSalad
False: Markovian
Figure 1: Histogram of entropy of ? for different
true and false document sets.
It is noteworthy that both the predictors (LLPW
and Entropy) give complementary clues regarding
a text category. A linear combination of these two
scores (the weight to the LLPW score is 0.1) al-
lows to substantially improve over these baseline
results, yielding a relative improvement (in EER)
of +20.0% for the sentence salads, +20.8% for the
word salads, and +27.3% for the Markov Models.
5.2 Effect of the number of topics
In this part, we investigate the performance of
LDA in detecting false documents when the num-
ber of topics is changed. Increasing the number
of topics means higher memory requirements both
during training and testing. Though the results are
shown only for SentenceSalad, similar trend is ob-
served for WordSalad and Markovian.
The numbers in Table 2 show that the perfor-
mance obtained with the LLPW score consistently
improve with an increase in the number of top-
ics, though the % improvement obtained when the
number of topics exceeds 200 is marginal. In con-
trast, the best performance in case of entropy is
achieved at 50 topics and slowly degrades when a
more complex model is used.
Number of Topics LLPW Entropy
10 27.9 1.88
50 18.9 0.88
100 16.0 0.93
200 14.8 0.90
300 13.8 1.05
400 13.6 1.10
Table 2: EER from LLPW and Entropy distribution
for TrueReuters against SentenceSalad.
5.3 Detecting ?noisy? documents
In this section, we study fake documents produced
by randomly changing words in true documents
(the TrueReuters dataset). In each document, a
fixed percentage of content words is randomly re-
placed by any other word from the training vocab-
ulary 6. This percentage was varied from 5 to 100
and EER for these corrupted document sets is com-
puted at each % corruption level (Figure 2). As
0 5 10 15 20 25 30 35 40 45 50 60 70 80 90 100
0
5
10
15
20
25
30
35
40
45
50
Noise level (% words changed)
EE
R
: L
LP
W
 a
nd
 E
nt
ro
py
 
 
LLPW
Entropy
Figure 2: EER at various noise levels
expected, the EER is very high at low noise levels,
and as the noise level is increased, EER gets lower.
When only a few words are changed in a true doc-
ument, it retrains the properties of a true document
(high LLPW and low entropy). However, as more
number of words are changed in a true document,
6When the replacement words are chosen from a small set
of very specific words, the fake document generation strategy
is termed as ?word stuffing?.
46
it starts showing the characteristics of a false docu-
ment (low LLPW and high entropy). These results
suggest that our semantic consistency tests are too
crude a measure to detect a small number of in-
consistencies, such as the ones found in the state-
of-the-art OCR or ASR systems? outputs. On the
other hand, it confirms the numerous studies that
have shown that topic detection (and topic adapta-
tion) or text categorization tasks can be performed
with the same accuracy for moderately noisy texts
and clean texts, a finding which warrants the topic-
based LM adaptation strategies deployed in (Hei-
del et al, 2007; Tam and Schultz, 2007).
The difference in the behavior of our two pre-
dictors is striking. The EER obtained using LLPW
drops more quickly than the one obtained with en-
tropy of the topic distribution. It suggests that the
influence of ?corrupting? content words (mostly
with low ?
tw
) is heavy on the LLPW, but the topic
information is not lost till a majority of the ?uncor-
rupted? content words belong to the same topic.
5.4 Effect of the document length
In this section, we study the robustness of our
two predictors with respect to the document length
by progressively increasing the number of content
words in a document (true or fake). As can be seen
from Figure 3, the entropy of the posterior topic
distribution starts to provide a reasonable discrim-
ination (5% EER) when the test documents contain
about 80 to 100 content words, and attains results
comparable to those reported earlier in this paper
when this number doubles. This definitely rules
out this method as a predictor of the semantic con-
sistency of a sentence: we need to consider at least
a paragraph to get acceptable results.
5.5 Testing with out-of-domain data
In this section, we study the robustness of our pre-
dictors on out-of-domain data using a small ex-
cerpt of abstracts from the Medline database. Both
true and fake documents are from this dataset.
The results are summarized in Table 3. The per-
TrueMedline vs. LLPW Entropy
SentenceSalad 31.23% 22.13%
WordSalad 30.03% 19.46%
Markovian 36.51% 23.63%
Table 3: Performance of LDA on PubMed ab-
stracts
formance on out-of-domain documents is poor,
10 20 30 40 50 60 70 80 90100 125 150 175 200 225
0
5
10
15
20
25
30
35
40
45
50
Number of content words
EE
R
: L
LP
W
 a
nd
 E
nt
ro
py
TrueReuters against False sets
 
 
LLPW: SentenceSalad
LLPW: WordSalad
LLPW: Markovian
Entropy: SentenceSalad
Entropy: WordSalad
Entropy: Markovian
Figure 3: EER with change in number of con-
tent words used for LDA analysis. EER based
on: LLPW of TrueReuters and false document sets
(solid line) and Entropy of topic distribution of
TrueReuters and false document sets (dashed line).
though the entropy of the topic distribution is still
the best predictor. The reasons for this failure are
obvious: a majority of the words occurring in these
documents (true or fake) are, from the perspective
of the model, characteristic of one single Reuters
topic (health and medicine). They cannot be dis-
tinguished either in terms of perplexity or in terms
of topic distribution (the entropy is low for all the
documents). It is interesting to note that all the
out-of-domain Medline data can be separated from
the in-domain TrueReuters data with good accu-
racy on the basis of the lower LLPW of the former
as compared to the higher LLPW of the latter.
6 Conclusion
In the LDA framework, this paper investigated two
methods to infer the topic distribution in a test
document. Further, the paper suggested that the
coherence of a document can be evaluated based
on its topic distribution and average LLPW, and
these measures can help to discriminate between
true and false documents. Indeed, through exper-
imental results, it was shown that entropy of the
topic distribution is lower and average LLPW of
true documents is higher for true documents and
the former measure was found to be more effective.
However, the poor performance of this method on
out-of-domain data suggests that we need to use a
much larger training corpus to build a robust fake
document detector. This raises the issue of train-
47
ing LDA model with very large collections. In fu-
ture we would like to explore the potential of this
method for text segmentation tasks.
Acknowledgment
This research was supported by the European
Commission under the contract FP6-027026-K-
Space. The views expressed in this paper are those
of the authors and do not necessarily represent the
views of the commission.
References
Blei, David and John Lafferty. 2005. Correlated topic
models. In Advances in Neural Information Process-
ing Systems (NIPS?18), Vancouver, Canada.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2002. Latent Dirichlet alocation. In Dietterich,
Thomas G., Suzanna Becker, and Zoubin Ghahra-
mani, editors, Advances in Neural Information Pro-
cessing Systems (NIPS), volume 14, pages 601?608,
Cambridge, MA. MIT Press.
Blei, David M., Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested Chinese restaurant process. In
Advances in Neural Information Processing Systems
(NIPS), volume 16, Vancouver, Canada.
Buntine, Wray and Aleks Jakulin. 2004. Apply-
ing discrete PCA in data analysis. In Chickering,
M. and J. Halpern, editors, Proceedings of the 20th
Conference on Uncertainty in Artificial Intelligence
(UAI?04), pages 59?66. AUAI Press 2004.
Buntine, Wray, Jaakko Lo?fstro?m, Jukka Perkio?, Sami
Perttu, Vladimir Poroshin, Tomi Silander, Henry
Tirri, Antti Tuominen, and Ville Tuulos. 2004.
A scalable topic-based open source search engine.
In Proceedings of the IEEE/WIC/ACM International
Conference on Web Intelligence, pages 228?234,
Beijing, China.
Foltz, P.W., W. Kintsch, and T.K. Landauer. 1998. The
measurement of textual coherence with Latent Se-
mantic Analysis. Discourse Processes, 25(2-3):285?
307.
Globerson, Amir, Terry Y. Koo, Xavier Carreras, and
Michael Collins. 2007. Exponentiated gradient al-
gorithms for log-linear structured prediction. In In-
ternational Conference on Machine Learning, Cor-
vallis, Oregon.
Griffiths, Thomas L. and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101 (supl 1):5228?5235.
Griffiths, Thomas L., Mark Steyvers, David M. Blei,
and Joshua Tenenbaum. 2005. Integrating topics
and syntax. In Proceedings of NIPS, 17, Vancouver,
CA.
Gruber, Amit, Michal Rosen-Zvi, and Yair Weiss.
2007. Hidden topic Markov models. In Proceedings
of International Conference on Artificial Intelligence
and Statistics, San Juan, Puerto Rico, March.
Hearst, Marti. 1997. TextTiling: Segmenting texts into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Heidel, Aaron, Hung an Chang, and Lin shan Lee.
2007. Language model adaptation using latent
Dirichlet alocation and an efficient topic inference
algorithm. In Proceedings of European Confer-
ence on Speech Communication and Technology,
Antwerp, Belgium.
Hofmann, Thomas. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning Journal, 42(1):177?196.
Hsu, Bo-June (Paul) and Jim Glass. 2006. Style
& topic language model adaptation using HMM-
LDA. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, Syd-
ney, Australia.
Kivinen, Jyrki and Manfrud K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1?
63.
Lewis, David D., Yiming Yang, Tony Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. Machine Learning Re-
search, 5:361?397.
Minka, Thomas and John Lafferty. 2002. Expectation-
propagation for the generative aspect model. In Pro-
ceedings of the conference on Uncertainty in Artifi-
cial Intelligence (UAI).
Nigam, K., A. K. McCallum, S. Thrun, and T. M.
Mitchell. 2000. Text classification from labeled and
unlabeled documents using EM. Machine Learning,
39(2/3):103?134.
Ohta, Tomoko, Yuka Tateisi, Hideki Mima, Jun ichi
Tsujii, and Jin-Dong Kim. 2002. The GENIA cor-
pus: an annotated research abstract corpus in molec-
ular biology domain. In Proceedings of Human Lan-
guage Technology Conference, pages 73?77.
Rigouste, Lo??s, Olivier Cappe?, and Franc?ois Yvon.
2007. Inference and evaluation of the multino-
mial mixture model for text clustering. Informa-
tion Processing and Management, 43(5):1260?1280,
September.
Tam, Yik-Cheung and Tanja Schultz. 2007. Correlated
latent semantic model for unsupervised LM adapta-
tion. In Proceedings of the International Conference
on Acoustics, Speech and Signal Processing, Hon-
olulu, Hawaii, U.S.A.
48
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54?59,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?10
Alexandre Allauzen, Josep M. Crego, ?Ilknur Durgar El-Kahlout and Franc?ois Yvon
LIMSI/CNRS and Universite? Paris-Sud 11, France
BP 133, 91403 Orsay Cedex
Firstname.Lastname@limsi.fr
Abstract
This paper describes our Statistical Ma-
chine Translation systems for the WMT10
evaluation, where LIMSI participated for
two language pairs (French-English and
German-English, in both directions). For
German-English, we concentrated on nor-
malizing the German side through a proper
preprocessing, aimed at reducing the lex-
ical redundancy and at splitting complex
compounds. For French-English, we stud-
ied two extensions of our in-house N -code
decoder: firstly, the effect of integrating a
new bilingual reordering model; second,
the use of adaptation techniques for the
translation model. For both set of exper-
iments, we report the improvements ob-
tained on the development and test data.
1 Introduction
LIMSI took part in the WMT 2010 evalua-
tion campaign and developed systems for two
languages pairs: French-English and German-
English in both directions. For German-English,
we focused on preprocessing issues and performed
a series of experiments aimed at normalizing the
German side by removing some of the lexical re-
dundancy and by splitting compounds. For this
pair, all the experiments were performed using the
Moses decoder (Koehn et al, 2007). For French-
English, we studied two extensions of our n-gram
based system: first, the effect of integrating a
new bilingual reordering model; second, the use
of adaptation techniques for the translation model.
Decoding is performed using our in-house N -code
(Marin?o et al, 2006) decoder.
2 System architecture and resources
In this section, we describe the main characteris-
tics of the phrase-based systems developed for this
evaluation and the resources that were used to train
our models. As far as resources go, we used all the
data supplied by the 2010 evaluation organizers.
Based on our previous experiments (De?chelotte et
al., 2008) which have demonstrated that better nor-
malization tools provide better BLEU scores (Pap-
ineni et al, 2002), we took advantage of our in-
house text processing tools for the tokenization
and detokenization steps. Only for German data
did we used the TreeTagger (Schmid, 1994) tok-
enizer. Similar to last year?s experiments, all of
our systems are built in ?true-case?.
3 German-English systems
As German is morphologically more complex than
English, the default policy which consists in treat-
ing each word form independently from the oth-
ers is plagued with data sparsity, which poses a
number of difficulties both at training and de-
coding time. When aligning parallel texts at
the word level, German compound words typi-
cally tend to align with more than one English
word; this, in turn, tends to increase the number
of possible translation counterparts for each En-
glish type, and to make the corresponding align-
ment scores less reliable. In decoding, new com-
pounds or unseen morphological variants of ex-
isting words artificially increase the number out-
of-vocabulary (OOV) forms, which severely hurts
the overall translation quality. Several researchers
have proposed normalization (Niessen and Ney,
2004; Corston-oliver and Gamon, 2004; Goldwa-
ter and McClosky, 2005) and compound splitting
(Koehn and Knight, 2003; Stymne, 2008; Stymne,
2009) methods. Our approach here is similar, yet
uses different implementations; we also studied
the joint effect of combining both techniques.
3.1 Reducing the lexical redundancy
In German, determiners, pronouns, nouns and ad-
jectives carry inflection marks (typically suffixes)
54
Input POS Lemma Analysis
In APPR in APPR.In
der* ART d ART.Def.Dat.Sg.Fem
Folge NN Folge N.Reg.Dat.Sg.Fem
befand VVFIN befinden VFIN.Full.3.Sg.Past.Ind
die* ART d ART.Def.Nom.Sg.Fem
derart ADV derart ADV
gesta?rkte* ADJA gesta?rkt ADJA.Pos.Nom.Sg.Fem
Justiz NN Justiz N.Reg.Nom.Sg.Fem
wiederholt ADJD wiederholt ADJD.Pos
gegen APPR gegen APPR.Acc
die* ART d ART.Def.Acc.Sg.Fem
Regierung NN Regierung N.Reg.Acc.Sg.Fem
und KON und CONJ.Coord.-2
insbesondere ADV insbesondere ADV
gegen APPR gegen APPR.Acc
deren* PDAT d PRO.Dem.Subst.-3.Gen.Sg.Fem
Geheimdienste* NN Geheimdienst N.Reg.Acc.Pl.Masc
. $. . SYM.Pun.Sent
Table 1: TreeTagger and RFTagger outputs. Starred word forms are modified during preprocessing.
so as to satisfy agreement constraints. Inflections
vary according to gender, case, and number infor-
mation. For instance, the German definite deter-
miner could be marked in sixteen different ways
according to the possible combinations of genders
(3), case (4) and number (2)1, which are fused
in six different tokens der, das, die, den, dem,
des. With the exception of the plural and gen-
itive cases, all these words translate to the same
English word: the. In order to reduce the size of
the German vocabulary and to improve the robust-
ness of the alignment probabilities, we considered
various normalization strategies for the different
word classes. In a nutshell, normalizing amounts
to collapsing several German forms of a given
lemma into a unique representative, using manu-
ally written normalization patterns. A pattern typ-
ically specifies which forms of a given morpho-
logical paradigm should be considered equivalent
when translating into English. These normaliza-
tion patterns use the lemma information computed
by the TreeTagger and the fine-grained POS infor-
mation computed by the RFTagger (Schmid and
Laws, 2008), which uses a tagset containing ap-
proximately 800 tags. Table 1 displays the analy-
sis of an example sentence. 2
In most cases, normalization patterns replace a
word form by its lemma; in order to partially pre-
1For the plural forms, gender distinctions are neutralized
and the same 4 forms are used for all genders .
2The English reference: Subsequently , the energized judi-
ciary continued ruling against government decisions , embar-
rassing the government ? especially its intelligence agencies
.
serve some inflection marks, we introduced two
generic suffixes, +s and +en which respectively
denote plural and genitive wherever needed. Typ-
ical normalization rules take the following form:
? For articles, adjectives, and pronouns (Indef-
inite , possessive, demonstrative, relative and
reflexive), if a token has;
? Genitive case: replace with lemma+en
(Ex. des, der, des, der ? d+en)
? Plural number: replace with lemma+s
(Ex. die, den ? d+s)
? All other gender, case and number: re-
place with lemma (Ex. der, die, das, die
? d)
? For nouns;
? Plural number: replace with lemma+s
(Ex. Bilder, Bildern, Bilder ? Bild+s))
? All other gender and case: replace with
lemma (Ex Bild, Bilde, Bildes ? Bild;
Using these tags, a normalized version of previ-
ous sentence is as follows: In d Folge befand d de-
rart gesta?rkt Justiz wiederholt gegen d Regierung
und insbesondere gegen d+en Geheimdienst+s.
Several experiments were carried out to assess the
effect of different normalization schemes. Remov-
ing all gender and case information, except for the
genitive for articles, adjectives and pronouns, al-
lowed to achieve the best BLEU scores.
3.2 Compound Splitting
Combining nouns, verbs and adjectives to forge
new words is a very common process in German.
55
It partly explains the difference between the num-
ber of types and tokens between English and Ger-
man in parallel texts. In most cases, compounds
are formed by a mere concatenation of existing
word forms, and can easily be split into simpler
units. As words are freely conjoined, the vocab-
ulary size increases vastly, yielding to sparse data
problems that turn into unreliable parameter esti-
mates. We used the frequency-based segmenta-
tion algorithm initially introduced in (Koehn and
Knight, 2003) to handle compounding. Our im-
plementation extends this technique to handle the
most common letter fillers at word junctions. In
our experiments, we investigated different split-
ting schemes in a manner similar to the work of
(Stymne, 2008).
4 French-English systems
4.1 Baseline N -coder systems
For this language pair, we used our in-house
N -code system, which implements the n-gram-
based approach to SMT. In a nutshell, the transla-
tion model is implemented as a stochastic finite-
state transducer trained using a n-gram model
of (source,target) pairs (Casacuberta and Vidal,
2004). Training this model requires to reorder
source sentences so as to match the target word
order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, our sys-
tem implements eight feature functions which are
optimally combined using a discriminative train-
ing framework (Och, 2003): a target-language
model; two lexicon models, which give comple-
mentary translation scores for each tuple; two
lexicalized reordering models aiming at predict-
ing the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. One novelty this year
are the introduction of lexicalized reordering mod-
els (Tillmann, 2004). Such models require to
estimate reordering probabilities for each phrase
pairs, typically distinguishing three case, depend-
ing whether the current phrase is translated mono-
tone, swapped or discontiguous with respect to the
3Part-of-speech information for English and French is
computed using the above mentioned TreeTagger.
previous (respectively next phrase pair).
In our implementation, we modified the three
orientation types originally introduced and con-
sider: a consecutive type, where the original
monotone and swap orientations are lumped to-
gether, a forward type, specifying a discontiguous
forward orientation, and a backward type, specify-
ing a discontiguous backward orientation. Empir-
ical results showed that in our case, the new orien-
tations slightly outperform the original ones. This
may be explained by the fact that the model is ap-
plied over tuples instead of phrases.
Counts of these three types are updated for
each unit collected during the training process.
Given these counts, we can learn probability dis-
tributions of the form pr(orientation|(st)) where
orientation ? {c, f, b} (consecutive, forward
and backward) and (st) is a translation unit.
Counts are typically smoothed for the estimation
of the probability distribution.
The overall search process is performed by our
in-house n-code decoder. It implements a beam-
search strategy on top of a dynamic programming
algorithm. Reordering hypotheses are computed
in a preprocessing step, making use of reordering
rules built from the word reorderings introduced
in the tuple extraction process. The resulting re-
ordering hypotheses are passed to the decoder in
the form of word lattices (Crego and no, 2006).
4.2 A bilingual POS-based reordering model
For this year evaluation, we also experimented
with an additional reordering model, which is esti-
mated as a standard n-gram language model, over
generalized translation units. In the experiments
reported below, we generalized tuples using POS
tags, instead of raw word forms. Figure 1 displays
the same sequence of tuples when built from sur-
face word forms (top), and from POS tags (bot-
tom).
Figure 1: Sequence of units built from surface
word forms (top) and POS-tags (bottom).
Generalizing units greatly reduces the number
of symbols in the model and enables to take larger
56
n-gram contexts into account: in the experiments
reported below, we used up to 6-grams. This new
model is thus helping to capture the mid-range
syntactic reorderings that are observed in the train-
ing corpus. This model can also be seen as a trans-
lation model of the sentence structure. It models
the adequacy of translating sequences of source
POS tags into target POS tags. Additional details
on these new reordering models can be found in
(Crego and Yvon, 2010).
4.3 Combining translation models
Our main translation model being a conventional
n-gram model over bilingual units, it can directly
take advantage of all the techniques that exist for
these models. To take the diversity of the available
parallel corpora into account, we independently
trained several translation models on subpart of
the training data. These translation models were
then linearly interpolated, where the interpolation
weights are chosen so as to minimize the perplex-
ity on the development set.
5 Language Models
The English and French language models (LMs)
are the same as for the last year?s French-English
task (Allauzen et al, 2009) and are heavily tuned
to the newspaper/newswire genre, using the first
part of the WMT09 official development data
(dev2009a). We used all the authorized news
corpora, including the French and English Gi-
gaword corpora, for translating both into French
(1.4 billion tokens) and English (3.7 billion to-
kens). To estimate such LMs, a vocabulary was
defined for both languages by including all to-
kens in the WMT parallel data. This initial vo-
cabulary of 130K words was then extended with
the most frequent words observed in the training
data, yielding a vocabulary of one million words
in both languages. The training data was divided
into several sets based on dates and genres (resp.
7 and 9 sets for English and French). On each
set, a standard 4-gram LM was estimated from
the 1M word vocabulary with in-house tools using
Kneser-Ney discounting interpolated with lower
order models (Kneser and Ney, 1995; Chen and
Goodman, 1998)4. The resulting LMs were then
linearly combined using interpolation coefficients
4Given the amount of training data, the use of the modi-
fied Kneser-Ney smoothing is prohibitive while previous ex-
periments did not show significant improvements.
chosen so as to minimize perplexity of the de-
velopment set (dev2009a). The final LMs were
finally pruned using perplexity as pruning crite-
rion (Stolcke, 1998).
For German, since we have less training
data, we only used the German monolingual
texts (Europarl-v5, News Commentary and News
Monolingual) provided by the organizers to train
a single n-gram language model, with modified
Kneser-Ney smoothing scheme (Chen and Good-
man, 1998), using the SRILM toolkit (Stolcke,
2002).
6 Tuning
Moses-based systems were tuned using the imple-
mentation of minimum error rate training (MERT)
(Och, 2003) distributed with the Moses decoder,
using the development corpus (news-test2008).
The N -code systems were also tuned by
the same implementation of MERT, which was
slightly modified to match the requirements of our
decoder. The BLEU score is used as objective
function for MERT and to evaluate test perfor-
mance. The interpolation experiment for French-
English was tuned on news-test2008a (first 1025
lines). Optimization was carried out over new-
stest2008b (last 1026 lines).
7 Experiments
For each system, we used all the available par-
allel corpora distributed for this evaluation. We
used Europarl and News commentary corpora for
German-English task and Europarl, News com-
mentary, United Nations and Gigaword corpora
for the French-English tasks. All corpora were
aligned with GIZA++ for word-to-word align-
ments with grow-diag-final-and and default set-
tings. For the German-English tasks, we applied
normalization and compound splitting as a pre-
processing step. For the French-English tasks, we
used new POS-based reordering model and inter-
polation.
7.1 German-English Tasks
We combined our two preprocessing schemes (see
Section 3) by applying compound splitting over
normalized data. Our experiments showed that for
German to English, using 4 characters as the mini-
mum split length and 8 characters as the minimum
compound candidate, and allowing the insertion of
-s -n -en -nen -e -es -er -ien) and the truncation of
57
-e -en -n yielded the best BLEU scores. On the
reverse direction, the best setting is different: 5
characters as minimum split length, 10 characters
as minimum compound candidate, no truncation.
These processes are performed before align-
ment, training, tuning and decoding. Before de-
coding, we also replaced all OOV words with their
lemma. We used the Moses (Koehn et al, 2007)
decoder, with default settings, to obtain the trans-
lations. For translating from English to German,
we used a two-level decoding. The first decoding
step translates English to ?preprocessed German?,
which is then turned into German by undoing the
effect of normalization. In this second step, we
thus aim at restoring inflection marks and at merg-
ing compounds. For this second ?translation? step,
we also use a Moses-based system. To point out
the error rate of the second step, we also translated
the preprocessed reference German text and com-
puted the BLEU score as 97.05. Our experiments
showed that this two-level decoding strategy was
not improving the direct baseline systems. Table 2
reports the BLEU scores5 on newstest2010 of our
official submissions.
System De ? En En ? De
Baseline 20.0 15.3
Norm+Split 21.3 15.0
Table 2: Results for German-English
7.2 French-English tasks
As explained above, in addition to the baseline
system (base), two contrast systems were built.
The first introduces an additional POS-based bilin-
gual 6-gram reordering model (bilrm), the second
implements the bilingual n-gram model after in-
terpolating 4 models trained respectively on the
news, epps, UNdoc and gigaword subparts of the
parallel corpus (interp). Optimization was carried
out over newstest2008b (last 1026 lines) and tested
over newstest2010 (2489 lines). Table 3 reports
translation accuracy for the three systems and for
both translation directions.
As can be seen, the system using the new
reordering model (base+bilrm) outperformed the
baseline system when translating into French,
while no difference was measured when translat-
ing into English. The interpolation experiments
5Scores are computed with the official script mteval-
v11b.pl
System Fr ? En En ? Fr
base 26.52 27.22
base+bilrm 26.50 27.84
base+bilrm+interp 26.84 27.62
Table 3: Results for French-English
did not show any clear impact on performance.
8 Conclusions
In this paper, we presented our statistical MT sys-
tems developed for the WMT?10 shared task, in-
cluding several novelties, namely the preprocess-
ing of German, and the integration of several new
techniques in our n-gram based decoder.
Acknowledgments
This work was partly realized as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, Aure?lien Max,
and Franc?ois Yvon. 2009. LIMSI?s statistical trans-
lation systems for WMT?09. In Proceedings of
WMT?09, Athens, Greece.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.
Simon Corston-oliver and Michael Gamon. 2004.
Normalizing german and english inflectional mor-
phology to improve statistical word alignment. In
Proceedings of the Conference of the Association for
Machine Translation in the Americas, pages 48?57.
Springer Verlag.
Josep M. Crego and Jose? B. Mari no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological analy-
sis. In Proceedings of Human Language Technology
58
Conference and Conference on Empirical Methods
in Natural Language Processing, pages 676?683,
Vancouver, British Columbia, Canada, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL ?03: Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics,
pages 187?193. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. Annual Meeting of the Association for Compu-
tational Linguistics (ACL), demonstration session,
Prague, Czech Republic.
Jose? B. Marin?o, Rafael E. Banchs R, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic information. Computational Lin-
guistics, 30(2):181?204.
Franz J. Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270?274.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Langage Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In GoTAL ?08: Pro-
ceedings of the 6th international conference on Ad-
vances in Natural Language Processing, pages 464?
475, Berlin, Heidelberg. Springer-Verlag.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of german compounds. In EACL
?09: Proceedings of the 12th Conference of the
European Chapter of the Association for Compu-
tational Linguistics: Student Research Workshop,
pages 61?69, Morristown, NJ, USA. Association for
Computational Linguistics.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of the Human Language Technology con-
ference / North American chapter of the Association
for Computational Linguistics 2004, pages 101?104,
Boston, MA, USA.
59
Two Ways to Use a Noisy Parallel News corpus for improving Statistical
Machine Translation
Souhir Gahbiche-Braham He?le`ne Bonneau-Maynard
Universite? Paris-Sud 11
LIMSI-CNRS
91403 Orsay, France
{souhir,hbm,yvon}@limsi.fr
Franc?ois Yvon
Abstract
In this paper, we present two methods to use
a noisy parallel news corpus to improve sta-
tistical machine translation (SMT) systems.
Taking full advantage of the characteristics of
our corpus and of existing resources, we use
a bootstrapping strategy, whereby an existing
SMT engine is used both to detect parallel sen-
tences in comparable data and to provide an
adaptation corpus for translation models. MT
experiments demonstrate the benefits of vari-
ous combinations of these strategies.
1 Introduction
In Statistical Machine Translation (SMT), systems
are created from parallel corpora consisting of a set
of source language texts aligned with its translation
in the target language. Such corpora however only
exist (at least are publicly documented and avail-
able) for a limited number of domains, genres, reg-
isters, and language pairs. In fact, there are a few
language pairs for which parallel corpora can be ac-
cessed, except for very narrow domains such as po-
litical debates or international regulatory texts. An-
other very valuable resource for SMT studies, espe-
cially for under-resource languages, are comparable
corpora, made of pairs of monolingual corpora that
contain texts of similar genres, from similar periods,
and/or about similar topics.
The potential of comparable corpora has long
been established as a useful source from which to
extract bilingual word dictionaries (see eg. (Rapp,
1995; Fung and Yee, 1998)) or to learn multilingual
terms (see e.g. (Lange?, 1995; Smadja et al, 1996)).
More recently, the relative corpus has caused the
usefulness of comparable corpora be reevaluated as
a potential source of parallel fragments, be they
paragraphs, sentences, phrases, terms, chunks, or
isolated words. This tendency is illustrated by the
work of e.g. (Resnik and Smith, 2003; Munteanu
and Marcu, 2005), which combines Information Re-
trieval techniques (to identify parallel documents)
and sentence similarity detection to detect parallel
sentences.
There are many other ways to improve SMT mod-
els with comparable or monolingual data. For in-
stance, the work reported in (Schwenk, 2008) draws
inspiration from recent advances in unsupervised
training of acoustic models for speech recognition
and proposes to use self-training on in-domain data
to adapt and improve a baseline system trained
mostly with out-of-domain data.
As discussed e.g. in (Fung and Cheung, 2004),
comparable corpora are of various nature: there ex-
ists a continuum between truly parallel and com-
pletely unrelated texts. Algorithms for exploiting
comparable corpora should thus be tailored to the
peculiarities of the data on which they are applied.
In this paper, we report on experiments aimed at
using a noisy parallel corpus made out of news sto-
ries in French and Arabic in two different ways: first,
to extract new, in-domain, parallel sentences; sec-
ond, to adapt our translation and language models.
This approach is made possible due to the specifici-
ties of our corpus. In fact, our work is part of a
project aiming at developing a platform for process-
ing multimedia news documents (texts, interviews,
images and videos) in Arabic, so as to streamline the
44
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 44?51,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
work of a major international news agency. As part
as the standard daily work flow, a significant por-
tion of the French news are translated (or adapted)
in Arabic by journalists. Having access to one full
year of the French and Arabic corpus (consisting, to
date, of approximately one million stories (150 mil-
lion words)), we have in our hands an ideal compa-
rable resource to perform large scale experiments.
These experiments aim at comparing various
ways to build an accurate machine translation sys-
tem for the news domain using (i) a baseline system
trained mostly with out-of-domain data (ii) the com-
parable dataset. As will be discussed, given the very
large number of parallel news in the data, our best
option seems to reconstruct an in-domain training
corpus of automatically detected parallel sentences.
The rest of this paper is organized as follows.
In Section 2, we relate our work to some exist-
ing approaches for using comparable corpora. Sec-
tion 3 presents our methodology for extracting par-
allel sentences, while our phrase-table adaptation
strategies are described in Section 4. In Section 5,
we describe our experiments and contrast the results
obtained with several adaptation strategies. Finally,
Section 6 concludes the paper.
2 Related work
From a bird?s eye view, attempts to use comparable
corpora in SMT fall into two main categories: first,
approaches aimed at extracting parallel fragments;
second, approaches aimed at adapting existing re-
sources to a new domain.
2.1 Extracting parallel fragments
Most attempts at automatically extracting parallel
fragments use a two step process (see (Tillmann and
Xu, 2009) for a counter-example): a set of candidate
parallel texts is first identified; within this short list
of possibly paired texts, parallel sentences are then
identified based on some similarity score.
The work reported in (Zhao and Vogel, 2002) con-
centrates on finding parallel sentences in a set of
comparable stories pairs in Chinese/English. Sen-
tence similarity derives from a probabilistic align-
ment model for documents, which enables to recog-
nize parallel sentences based on their length ratio,
as well as on the IBM 1 model score of their word-
to-word alignment. To account for various levels of
parallelism, the model allows some sentences in the
source or target language to remain unaligned.
The work of (Resnik and Smith, 2003) considers
mining a much larger ?corpora? consisting of docu-
ments collected on the Internet. Matched documents
and sentences are primarily detected based on sur-
face and/or formal similarity of the web addresses
or of the page internal structure.
This line of work is developed notably in
(Munteanu and Marcu, 2005): candidate parallel
texts are found using Cross-Lingual Information Re-
trieval (CLIR) techniques; sentence similarity is in-
directly computed using a logistic regression model
aimed at detecting parallel sentences. This formal-
ism allows to enrich baseline features such as the
length ratio, the word-to-word (IBM 1) alignment
scores with supplementary scores aimed at reward-
ing sentences containing identical words, etc. More
recently, (Smith et al, 2010) reported significant im-
provements mining parallel Wikipedia articles us-
ing more sophisticated indicators of sentence par-
allelism, incorporating a richer set of features and
cross-sentence dependencies within a Conditional
Random Fields (CRFs) model. For lack of find
enough parallel sentences, (Munteanu and Marcu,
2006; Kumano and Tokunaga, 2007) consider the
more difficult issue of mining parallel phrases.
In (Abdul-Rauf and Schwenk, 2009), the authors,
rather than computing a similarity score between a
source and a target sentence, propose to use an ex-
isting translation engine to process the source side
of the corpus, thus enabling sentence comparison to
be performed in the target language, using the edit
distance or variants thereof (WER or TER). This
approach is generalized to much larger collections
in (Uszkoreit et al, 2010), which draw advantage
of working in one language to adopt efficient paral-
lelism detection techniques (Broder, 2000).
2.2 Comparable corpora for adaptation
Another very productive use of comparable corpora
is to adapt or specialize existing resources (dictio-
naries, translation models, language models) to spe-
cific domains and/or genres. We will only focus here
on adapting the translation model; a review of the
literature on language model adaptation is in (Bella-
garda, 2001) and the references cited therein.
45
Figure 1: Extraction of parallel corpora
The work in (Snover et al, 2008) is a first step
towards augmenting the translation model with new
translation rules: these rules associate, with a tiny
probability, every phrase in a source document with
the most frequent target phrases found in a compa-
rable corpus specifically built for this document.
The study in (Schwenk, 2008) considers self-
training, which allows to adapt an existing system
to new domains using monolingual (source) data.
The idea is to automatically translate the source side
of an in-domain corpus using a reference translation
system. Then, according to some confidence score,
the best translations are selected to form an adap-
tation corpus, which can serve to retrain the trans-
lation model. The authors of (Cettolo et al, 2010)
follow similar goals with different means: here, the
baseline translation model is used to obtain a phrase
alignment between source and target sentences in
a comparable corpus. These phrase alignments are
further refined, before new phrases not in the origi-
nal phrase-table, can be collected.
The approaches developed below borrow from
both traditions: given (i) the supposed high degree
of parallelism in our data and (ii) the size of the
available comparable data, we are in a position to
apply any of the above described technique. This
is all the easier to do as all stories are timestamped,
which enables to easily spot candidate parallel texts.
In both cases, we will apply a bootstrapping strat-
egy using as baseline a system trained with out-of-
domain data.
3 Extracting Parallel Corpora
This section presents our approach for extracting a
parallel corpus from a comparable in-domain cor-
pora so as to adapt a SMT system to a specific do-
main. Our methodology assumes that both a base-
line out-of-domain translation system and a compa-
rable in-domain corpus are available, two require-
ments that are often met in practice.
As shown in Figure 1, our approach for extracting
an in-domain parallel corpus from the in-domain
comparable corpus consists in 3 steps and closely
follows (Abdul-Rauf and Schwenk, 2009):
translation: translating the source side of the
comparable corpora;
document pairs selection : selecting, in the com-
parable corpus, documents that are similar to the
translated output;
sentence pairs selection : selecting parallel sen-
tences among the selected documents.
The main intuition is that computing document
similarities in one language enables to use simple
and effective comparison procedures, instead of hav-
ing to define ad hoc similarities measures based on
complex underlying alignment models.
The translation step consists here in translating
the source (Arabic) side of the comparable corpus
using a baseline out-of-domain system, which has
been trained on parallel out-of-domain data.
The document selection step consists in trying
to match the automatic translations (source:target)
with the original documents in the target language.
For each (source:target) document, a similarity score
with all the target documents is computed. We con-
tend here with a simple association score, namely
the Dice coefficient, computed as the number of
words in common in both documents, normalized by
the length of the (source:target) document.
A priori knowledge, such as the publication dates
46
of the documents, are used to limit the number of
document pairs to be compared. For each source
document, the target document that has the best
score is then selected as a potential parallel docu-
ment. The resulting pairs of documents are then fil-
tered depending on a threshold Td, so as to avoid
false matches (in the experiments described below,
the threshold has been set so as to favor precision
over recall).
At the end of this step, a set of similar source
and target document pairs has been selected. These
pairs may consist in documents that are exact trans-
lations of each other. In most cases, the documents
are noisy translation and only a subset of their sen-
tences are mutual translation.
The sentence selection step then consists in per-
forming a sentence level alignment of each pair of
documents to select a set of parallel sentences. Sen-
tence alignment is then performed with the hunalign
sentence alignment tool (Varga et al, 2005), which
also provides alignment confidence measures. As
for the document selection step, only sentence pairs
that obtain an alignment score greater than a prede-
fined threshold Ts are selected, where Ts is again
chosen to favor prevision of alignments of recall.
From these, 1 : 1 alignments are retained, yielding
a small, adapted, parallel corpus. This method is
quite different from (Munteanu and Marcu, 2005)?s
work where the sentence selection step is done by a
Maximum Entropy classifier.
4 Domain Adaptation
In the course of mining our comparable corpus, we
have produced a translation into French for all the
source language news stories. This means that we
have three parallel corpora at our disposal:
? The baseline training corpus, which is large
(a hundred million words), delivering a reason-
able translation performance quality of transla-
tion, but out-of-domain;
? The extracted in-domain corpus, which is
much smaller, and potentially noisy;
? The translated in-domain corpus, which is of
medium-size, and much worse in quality than
the others.
Considering these three corpora, different adapta-
tion methods of the translation models are explored.
The first approach is to concatenate the baseline and
in-domain training data (either extracted or trans-
lated) to train a new translation model. Given the
difference in size between the two corpus, this ap-
proach may introduce a bias in the translation model
in favor of out-of-domain.
The second approach is to train separate transla-
tion models with baseline on the one hand, and with
in-domain on the other data and to weight their com-
bination with MERT (Och, 2003). This alleviates
the former problem but increases the number of fea-
tures that need to be trained, running the risk to make
MERT less stable.
A last approach is also considered, which consists
in using only the in-domain data to train the trans-
lation model. In that case, the question is the small
size of the in-domain data.
The comparative experiments on the three ap-
proaches, using the three corpora are described in
next section.
5 Experiments and results
5.1 Context and data
The experiments have been carried out in the con-
text of the Cap Digital SAMAR1 project which aims
at developping a platform for processing multimedia
news in Arabic. Every day, about 250 news in Ara-
bic, 800 in French and in English2 are produced and
accumulated on our disks. News collected from De-
cember 2009 to December 2010 constitute the com-
parable corpora, containing a set of 75,975 news for
the Arabic part and 288,934 news for the French part
(about 1M sentences for Arabic and 5M sentences
for French).
The specificity of this comparable corpus is that
many Arabic stories are known to be translation of
news that were first written in French. The transla-
tions may not be entirely faithful: when translating
a story, the journalist is in fact free to rearrange the
structure, and to some extend, the content of a doc-
ument (see example Figure 2).
In our experiments, the in-domain comparable
corpus then consists in a set of Arabic and French
1http://www.samar.fr
2The English news have not been used in this study.
47
Arabic:
	
?A
	
J

J

?@ 	?? A
	
JK
Y? ?
	
KA? B ?A?g ?


	
? 	?mProceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 542?553,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
From n-gram-based to CRF-based Translation Models
Thomas Lavergne Josep Maria Crego
LIMSI/CNRS
BP 133
F-91 403 Orsay Ce?dex
{lavergne,jmcrego}@limsi.fr
Alexandre Allauzen Franc?ois Yvon
LIMSI/CNRS & Uni. Paris Sud
BP 133
F-91 403 Orsay Ce?dex
{allauzen,yvon}@limsi.fr
Abstract
A major weakness of extant statistical ma-
chine translation (SMT) systems is their lack
of a proper training procedure. Phrase extrac-
tion and scoring processes rely on a chain of
crude heuristics, a situation judged problem-
atic by many. In this paper, we recast the ma-
chine translation problem in the familiar terms
of a sequence labeling task, thereby enabling
the use of enriched feature sets and exact train-
ing and inference procedures. The tractabil-
ity of the whole enterprise is achieved through
an efficient implementation of the conditional
random fields (CRFs) model using a weighted
finite-state transducers library. This approach
is experimentally contrasted with several con-
ventional phrase-based systems.
1 Introduction
A weakness of existing phrase-based SMT systems,
that has been repeatedly highlighted, is their lack
of a proper training procedure. Attempts to de-
sign probabilistic models of phrase-to-phrase align-
ments (e.g. (Marcu and Wong, 2002)) have thus far
failed to overcome the related combinatorial prob-
lems (DeNero and Klein, 2008) and/or to yield im-
proved training heuristics (DeNero et al, 2006).
Phrase extraction and scoring thus rely on a chain
of heuristics see (Koehn et al, 2003), which evolve
phrase alignments from ?symmetrized? word-to-
word alignments obtained with IBM models (Brown
et al, 1990) and the like (Liang et al, 2006b; Deng
and Byrne, 2006; Ganchev et al, 2008). Phrase
scoring is also mostly heuristic and relies on an op-
timized interpolation of several simple frequency-
based scores. Overall, the training procedure of
translation models within conventional phrase-based
(or hierarchical) systems is generally considered un-
satisfactory and the design of better estimation pro-
cedures remains an active research area (Wuebker et
al., 2010).
To overcome the NP-hard problems that derive
from the need to consider all possible permutations
of the source sentence, we make here a radical
simplification and consider training the translation
model given a fixed segmentation and reordering.
This idea is not new, and is one of the grounding
principle of n-gram-based approaches (Casacuberta
and Vidal, 2004; Marin?o et al, 2006) in SMT. The
novelty here is that we will use this assumption to re-
cast machine translation (MT) in the familiar terms
of a sequence labeling task.
This reformulation allows us to make use of the
efficient training and inference tools that exists for
such tasks, most notably linear CRFs (Lafferty et
al., 2001; Sutton and McCallum, 2006). It also en-
ables to easily integrate linguistically informed (de-
scribing morphological or morpho-syntactical prop-
erties of phrases) and/or contextual features into the
translation model. In return, in addition to having
a better trained model, we also expect (i) to make
estimation less sensible to data sparsity issues and
(ii) to improve the ability of our system to make
the correct lexical choices based on the neighbor-
ing source words. As explained in Section 2, this
reformulation borrows much from the general ar-
chitecture of n-gram MT systems and implies to
solve several computational challenges. In our ap-
542
proach, the tractability of the whole enterprise is
achieved through an efficient reimplementation of
CRFs using a public domain library for weighted
finite-state transducers (WFSTs) (see details in Sec-
tion 3). This approach is experimentally contrasted
with more conventional n-gram based and phrase-
based approaches on a standard benchmark in Sec-
tion 4, where we also evaluate the benefits of various
feature sets and training regimes. We finally relate
our new system with alternative proposals for train-
ing discriminatively SMT systems in Section 5, be-
fore drawing some lessons and discussing possible
extensions of this work.
The main contribution of this work are thus (i) a
detailed presentation of the CRF in translation in-
cluding all necessary implementation details and (ii)
an experimental study of various feature functions
and of various ways to integrate target side LM in-
formation.
2 MT as sequence labeling
In this section, we briefly review the n-gram based
approach to SMT, originally introduced in (Casacu-
berta and Vidal, 2004; Marin?o et al, 2006), which
constitutes our starting point. We then describe our
new proposal, which, in essence, consists in replac-
ing the modeling of compound source-target trans-
lation units by a conditional model where the prob-
ability of each target side phrase is conditioned on
the source sentence.
2.1 The n-gram based approach in SMT
The n-gram based approach of (Marin?o et al, 2006)
is a variation of the standard phrase-based model,
characterized by the peculiar form of the translation
model. In this approach, the translation model is
based on bilingual units called tuples. Tuples are
the analogous of phrase pairs, as they represent a
matching u = (e, f) between a source f and a tar-
get e word sequence. The probability of a sequence
of tuples is computed using a conventional n-gram
model as:
p(u1 . . . uI) =
I?
i=1
p(ul|ui?1 . . . ui?n+1).
The probability of a sentence pair (f , e) is then ei-
ther recovered by marginalization, or approximated
by maximization, over all possible joint segmenta-
tions of f and e into tuples.
As for any n-gram model, the parameters are es-
timated using statistics collected in a training corpus
made of sequences of tuples derived from the par-
allel sentences in a two step process. First, a word
alignment is computed using a standard alignment
pipeline1 based on the IBM models. Source words
are then reordered so as to disentangle the align-
ment links and to synchronize the source and tar-
get texts. Special care has to be paid to non-aligned
source words, which have to be collapsed with their
neighbor words. A byproduct of this process is a de-
terministic joint segmentation of parallel sentences
into minimal bilingual units, the tuples, that consti-
tute the basic elements in the model. This process is
illustrated on Figure 1, where the unfolding process
enables the extraction of tuples such as: (demanda,
said ) or (de nouveau, again).
f : demanda de nouveau la femme voile?e
e: the veiled dame said again
f? : la voile?e femme demanda de nouveau
Figure 1: The tuple extraction process
The original (top) and reordered (bottom) French
sentence aligned with its translation.
At test time, the source text is reordered so as
to match the reordering implied by the disentangle-
ment procedure. Various proposals has been made
to perform such source side reordering (Collins et
al., 2005; Xia and McCord, 2004), or even learn-
ing reordering rules based on syntactic or morpho-
syntactic information (Crego and Marin?o, 2007).
The latter approach amounts to accumulate reorder-
ing patterns during the training; test source sen-
tences are then non-deterministically reordered in
all possible ways yielding a word graph. This graph
is then monotonously decoded, where the score of
a translation hypothesis combines information from
the translation models as well as from other infor-
mation sources (lexicalized reordering model, target
1Here, using the MGIZA++ package (Gao and Vogel, 2008).
543
side language model (LM), word and phrase penal-
ties, etc).
2.2 Translating with CRFs
A discriminative version of the n-gram approach
consists in modeling P (e|f) instead of P (e, f),
which can be efficiently performed with CRFs (Laf-
ferty et al, 2001; Sutton and McCallum, 2006). As-
suming matched sequences of observations (x =
xL1 ) and labels (y = y
L
1 ), CRFs express the con-
ditional probability of labels as:
P (yL1 |x
L
1 ) =
1
Z(xL1 ; ?)
exp(?TG(xL1 , y
L
1 )),
where ? is a parameter vector and G denotes a vec-
tor of feature functions testing various properties of
x and y. In the linear-chain CRF, each compo-
nent Gk(xI1, y
I
1) of G is decomposed as a sum of
local features: Gk(xI1, y
I
1) =
?
i gk(x
I
1, yi?1, yi)
2.
CRFs are trained by maximizing the (penalized) log-
likelihood of a corpus containing observations and
their labels.
In principle, the data used to train n-gram trans-
lation models provide all the necessary information
required to train a CRF3. It suffices to consider that
the alphabet of possible observations ranges over all
possible source side fragments, and that each tar-
get side of a tuple is a potential label. The model
thus defines the probability of a segmented target
e? = e?I1 given the segmented and reordered source
sentence f? = f? I1 . To complete the model, one just
needs to define a distribution over source segmen-
tations P (f? |f). Given the deterministic relationship
between e and e? expressed by the ?unsegmentation?
function ? which maps e? with e = ?(e?), we then
have:
P (e|f) =
?
f? ,e|?(e)=e
P (e?, f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f? , f)P (f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f?)P (f? |f)
2Assuming first order dependencies.
3This is a significant difference with (Blunsom et al, 2008),
as we do not need to introduce latent variables during training.
In practice, we will only consider a restricted
number of possible segmentation/reorderings of the
source, denoted L(f), and compute the best transla-
tion e? as ?(e??), where:
e?? = arg max
e
P (e?|f)
? arg max
f??L(f),e
P (e?, |f? , f)P (f? |f) (1)
Even with these simplifying assumptions, this
approach raises several challenging computational
problems. First, training a CRF is quadratic in the
number of labels, of which we will have plenty (typ-
ically hundreds of thousands). A second issue is de-
coding: as we need to consider at test time a combi-
natorial number of possible source reorderings and
segmentations, we can no longer dispense with the
computation of the normalizer Z(f? ; ?) which is re-
quired to compute P (e?, f? |f) as P (f? |f)P (e?|f?) and to
compare hypotheses associated with different values
of f? . We discuss our solutions to these problems in
the next section.
3 Implementation issues
3.1 Training
Basic training The main difficulties in training are
caused by the unusually large number of labels, each
of which corresponds to a (small) sequence of target
words. Hopefully, each observation (source side tu-
ple) occurs with a very small number of different
labels. A first simplification is thus to consider that
the set of possible ?labels? e? for a source sequence
f? is limited to those that are seen in training: all
the other associations (f? , e?) are deemed impossible,
which amounts to setting the corresponding param-
eter value to ??.
A second speed-up is to enforce sparsity in the
model, through the use of a `1 regularization term
(Tibshirani, 1996): on the one hand, this greatly re-
duces the memory usage; furthermore, sparse mod-
els are also prone to various optimization of the
forward-backward computations (Lavergne et al,
2010). As discussed in (Ng, 2004; Turian et al,
2007), this feature selection strategy is well suited
to the task at hand, where the number of possible
features is extremely large. Optimization is per-
544
formed using the Rprop algorithm4 (Riedmiller and
Braun, 1993), which provides the memory efficiency
needed to cope with the very large feature sets con-
sidered here.
Training with a target language model One of
the main strength of the phrase-based ?log-linear?
models is their ability to make use of powerful
target side language models trained on very large
amounts of monolingual texts. This ability is crucial
to achieve good performance and has to be preserved
no matter the difficulties that occur when one moves
away from conventional phrase-based systems (Chi-
ang, 2005; Huang and Chiang, 2007; Blunsom and
Osborne, 2008; Ka?a?ria?inen, 2009). It thus seems
appropriate to include a LM feature function in our
model or alternatively to define:
P (e?|f?) =
1
Z(f? ; ?)
PLM (e?) exp(?
TG(f? , e?)),
where PLM is the target language model and
Z(f? ; ?) =
?
e PLM (e?) exp(?
TG(f? , e?)). Imple-
menting this approach implies to deal with the lack
of synchronization between the units of the trans-
lation models, which are variable-length (possibly
empty) tuples, and the units of the language models,
which are plain words.
In practice, this extension is implemented by per-
forming training and inference over a graph whose
nodes are not only indexed by their position and the
left target context, but also by the required n-gram
(target) history. In most cases, for small values of
n such as considered in this study, the n-gram his-
tory can be deduced from the left target tuple. The
most problematic case is when the left target tuple
is NULL, which require to copy the history from the
previous states. As a consequence, for the values of
n considered here, the impact of this extension on
the total training time is limited.
Reference reachability A recurring problem for
discriminative training approaches is reference un-
reachability (Liang et al, 2006a): this happens when
the model cannot predict the reference translation,
which means in our case that the probability of the
reference cannot be computed. In our implementa-
tion, this only happens when the reference involves
4Adapted to handle a locally non-differentiable objective.
a tuple (f? ,e?) that is too rare to be included in the
model. As a practical workaround, when this hap-
pens for a given training sentence, we make sure
to ?locally? augment the tuple dictionary with the
missing part of the reference, which is then removed
for processing the rest of the training corpus.
3.2 Inference
Our decoder is implemented as a cascade of
weighted finite-state transducers (WFSTs) using the
functionalities of the OpenFst library (Allauzen et
al., 2007). This library provides many basic opera-
tion for WFSTs, notably the left (pi1) and right (pi2)
projections as well as the composition operation (?).
The related notions and algorithms are presented in
detail in (Mohri, 2009), to which we refer the reader.
In essence, our decoder is implemented of a finite-
state cascade involoving the following steps: (i)
source reordering and segmentation (ii) application
of the translation model and (optionally) (iii) com-
position with a target side language model, an ar-
chitecture that is closely related to the proposal of
(Kumar et al, 2006). A more precise account of
these various steps is given below, where we de-
scribe the main finite-state transducers involved in
our decoder:
? S, the acceptor for the source sentence f ;
? R, which implements segmentation and re-
ordering rules;
? T , the tuple dictionary, associating source side
sequences with possible translations based on
the inventory of tuples;
? F , the feature matcher, mapping each feature
with the corresponding parameter value;
Source reordering The computation of R mainly
follows the approach of (Crego and Marin?o, 2007)
and uses a part-of-speech tagged version of the re-
ordered training data. Each reordering pattern seen
in training is generalized as a non-deterministic re-
ordering rule which expresses a possible rearrange-
ment of some subpart of the source sentence. Each
rule is implemented as an elementary finite-state
transducer, and the set of possible word reorderings
is computed as the composition of these transducers.
R is finally obtained by composing the result with a
545
transducer computing all the possible segmentations
of its input into sequences of source side tuples5.
The output of S ? R are sequences of source side
tuples f? ; each path in this transducer is addition-
ally weighted with a simplistic n-tuple segmentation
model, estimated using the source side of the paral-
lel training corpus. Note that these scores are nor-
malized, so that the weight of each path labelled f? in
S ?R is logP (f? |f).
The feature matcher F The feature matcher is
also implemented as a series of elementary weighted
transducers, each transducer being responsible for a
given class of feature functions. The simplest trans-
ducer in this family deals with the class of unigram
feature functions, ie. feature functions that only test
the current observation and label. It is represented
on the left part of Figure 3.2, where for the sake of
readability we only display one example for each
test pattern (here: an unconditional feature that al-
ways returns true for a given label, a test on the
source word, and a test on the source POS label).
As long as dependencies between source and/or tar-
get symbols remain local, they can be captured by
finite-state transducers such as the ones on the mid
and right part of Figure 3.2, which respectively com-
pute bigram target features, and joint bigram source
and target features.
The feature matcher F is computed as the com-
position of these elementary transducers, where we
only include source and target labels that can occur
given the current input sentence. Weights in F are
interpreted in the tropical semiring. exp(F ) is ob-
tained by replacing weights w in F with exp(w) in
the real semiring.
Decoding a word graph If the input segmentation
and reordering were deterministically set, meaning
that the automaton I = pi1(S ? R ? T ) would only
contain one path, decoding would amount to finding
the best path in S ?R ? T ?F . However, we need to
compute:
arg max
e
P (e?|f) = arg max
e
?
f?
P (e?, f? |f)
= arg max
e
?
f?
P (e?|f?)P (f? |f).
5When none is found, we also consider a maximal segmen-
tation into isolated words.
This requires to compare model scores for mul-
tiple source segmentations and reorderings f? , hence
to compute P (f? |f) and P (e?|f?), rather than just the
non-normalized value that is usually used in CRFs.
Computing the normalizer Z(f? ; ?) for all se-
quences in S ?R is performed efficiently using stan-
dard finite-state operations as :
D = det(pi1(pi2(S ?R) ? T ? exp(F ))).
In fact, determinization (in the real semiring) has the
effect of accumulating for each f? the corresponding
normalizer Z(f? ; ?). Replacing each weight w in D
by ? log(w) and using the log semiring enables to
compute? log(Z(f? ; ?)). The best translation is then
obtained as: bestpath(pi2(S?R)??log(D)?T ?F )
in the tropical semiring.
Decoding and Rescoring with a target language
model An alternative manner of using a (large)
target side language model is to use it for rescoring
purposes. The consistent use of finite-state machines
and operations makes it fairly easy to include one
during decoding : it suffices to perform the search in
pi2(S?R)?? log(D)?T ?F ?L, where L represents
a n-gram language model. When combining several
models, notably a source segmentation model and/or
a target language model for rescoring, we have made
sure to rescale the (log)probabilities so as to balance
the language model scores with the CRF scores, and
to use a fixed word bonus to make hypotheses of dif-
ferent length more comparable. All these parameters
are tuned as part of the decoder development pro-
cess. It is finally noteworthy that, in our architecture,
alternative decoding strategies, such as MBR (Ku-
mar and Byrne, 2004) are also readily implemented.
4 Experiments
4.1 Corpora and metrics
For these experiments, we have used a medium size
training corpus, extracted from the datasets made
available for WMT 20116 evaluation campaign, and
have focused on one translation direction, from
French to English7.
Translation model training uses the entire News-
Commentary subpart of the WMT?2011 training
6statmt.org/wmt11
7Results in the other direction suggest similar conclusions.
546
0le : the/?le,the
DET : the/?DET,the
? : the/?the 0 1
? : the/0
? : cat/?the,cat
0 1
? : the/0
chat : cat/?chat,cat
Figure 2: Feature matchers. The star symbol (*) matches any possible observation.
French English
sent? token types token types
train 115 K 3 339 K 60 K 2 816 K 58 K
test 2008 2.0 K 55 K 9 K 49 K 8 K
test 2009 2.5 K 72 K 11 K 65 K 10 K
test 2010 2.5 K 69 K 10 K 61 K 9 K
Table 1: Corpora used for the experiments
data; for language models, we have considered two
approaches (i) a ?large? bigram model highly opti-
mized using all the available monolingual data and
(ii) a ?small? trigram language model trained on
just the English side of the NewsCommentary cor-
pus. The regularization parameters used in training
are tuned using the WMT 2009 test set; the various
parameters implied in the decoding are tuned (for
BLEU) on WMT 2008 test set; the internal tests re-
ported below are performed on the 2010 test lines
(see Table 1) using the best parameters found during
tuning. Various statistics regarding these corpora are
reproduced on Table 1.
All the training corpora were aligned using
MGIZA++ with standard parameters8, and pro-
cessed in the standard tuple extraction pipeline. The
development and test corpora were also processed
analogously. For the sake of comparison, we also
trained a standard n-gram-based and a Moses sys-
tem (Koehn et al, 2007) with default parameters
and a 3-gram target LM trained using only the tar-
get side of our parallel corpus. The development set
(test 2009) was used to tune these two systems. All
performance are measured using BLEU (Papineni et
al., 2002).
8As part of a much larger batch of texts.
4.2 Features
The baseline system is composed only of transla-
tion features [trs] and target bigram features [t2g].
The former correspond to functions of the form
gus,t(f? , e?, i) = I(f?i = s ? e?i = t), where s
and t respectively denote source and target phrases
and I() is the indicator function. These are also
generalized to part-of-speech and also to any pos-
sible source phrase, giving rise to features such as
gu?,t = (f? , e?, i) = I(e?i = t). Target bigram features
correspond to functions of the form gbt,t?(f? , e?, i) =
I(e?i?1 = t? e?i = t?). The last baseline feature is the
copy feature, which fires whenever the source and
target segments are identical.
Supplementary groups of features are considered
in further stages:
? suffix/prefix features [ix]. These features allow
to generalize baseline features on the source
side to fixed length prefixes and suffixes, thus
smoothing the parameters.
? context features [ctx]. These features are sim-
ilar to unigram features, but also test the left
source tuple and the corresponding part-of-
speech.
? segmentation features [seg]. These features are
meant to express a preference for longer tuples
and to regulate the number of target words per
source word. We consider the following feature
functions (|e| denotes the length of e):
? target length features :
gl?,l(f? , e?, i) = I(|e?i| = l)
? source-target length features :
gll,l?(f? , e?, i) = I(|f?i| = l ? |e?i| = l
?)
? source-target length ratio :
gll(f? , e?, i) = I(round(
| efi|
|ei|
) = l)
547
Note that all these features are further condi-
tioned on the target label.
? reordering features [ord]. These features are
meant to model preferences for specific lo-
cal reordering patterns and take into account
neighbor source fragments in e? together with
the current label. Each source side segment
f?i is made of some source words that, prior
to source reordering, were located at indices
i1 . . . il, so that f?i = fi1 . . . fil . The high-
est (resp. lowest) index in this sequence is df?ie
(resp. bf?ic). The leftmost (resp. rightmost) in-
dex is [f?i[ (resp. ]f?i]).
Using these notations, our model includes the
following patterns:
? distortion features, measuring the gaps be-
tween consecutive source fragments :
gol,t(f? , e?, i)=I(?(f?i, e?i)= l ? e?i= t),
where ?(f?i, e?i) ={
bf?ic ? df?i?1e if (df?i?1e ? bf?ic)
df?ie ? bf?i?1c otherwise .
? lexicalized reordering, identifying mono-
tone, swap and discontinuous configura-
tions (Tillman, 2004). The monotonous
test is defined as: gom(f? , e?, i) =
I(]ei?1] = [ei[); the swap and discon-
tinuous configurations are defined analo-
gously.
? ?gappiness? test : this feature is activated
whenever the source indices i1...il contain
one or several gaps.
4.3 Experiments and lessons learned
Training time The first lesson learned is that
training can be performed efficiently. Our baseline
system, which only contains trs and trg contains ap-
proximately 87 million features, out of which a lit-
tle bit more than 600K are selected. Adding up all
supplementary features raises the number of param-
eters to about 130M features, out of which 1.5M are
found useful. All these systems require between 3
and 5 hours to train9. These numbers are obtained
with a `1 penalty term ? 1, which offers a good bal-
ance between accuracy and sparsity.
9All experiments run on a server with 64G of memory and
two Xeon processors with 4 cores at 2.27 Ghz.
Test conditions In order to better assess the
strengths and weaknesses of our approach, we com-
pare several test settings: the most favorable con-
siders only one possible segmentation/reordering f?
for each f , obtained through forced alignment with
the reference; we then consider the more challeng-
ing case where the reordering is fixed, but several
segmentations are considered; then the regular de-
coding task, where both segmentation and reorder-
ing are unknown and where the entire space of all
segmentations and reordering is searched. For each
condition, we also vary (i) the set of features used
and (ii) the target language model used, if any.
Wherever applicable, we also report contrasts with
n-gram-based systems subject to the same input and
comparable resources, varying the order of the tuple
language model, as well as with Moses. Results are
in Table 2.
dev test # feat.
decoding with optimal segmentation/reordering
CRF (trs,trg) 23.8 25.1 660K
CRF +ctx 24.1 25.4 1.5M
CRF +ix,ord,seg 24.3 25.6 1.5M
decoding with optimal reordering
n-gram (2g,3g) 20.6 24.1 755K
n-gram (3g,3g) 21.5 25.2 755K
CRF trs,trg - 22.8 660K
CRF +ctx - 23.1 1.5M
CRF +ix,ord,seg - 23.5 1.5M
regular decoding
Moses (3g) 21.2 20.5
n-gram (2g,3g) 20.6 20.2 755K
n-gram (3g,3g) 21.5 21.2 755K
CRF (trs,trg) - 18.3 660K
CRF +ctx - 18.8 1.5M
CRF +ix,ord,seg - 19.1 1.5M
CRF +ix,ord,seg+3g - 19.1 1.5M
Table 2: Translation performance
Extending the feature set As expected, the use
of increasingly complex feature sets seems benefi-
cial in all experimented conditions. It is noteworthy
that throwing in reordering and contextual features
is helping, even when decoding one single segmen-
tation and reordering. This is because these features
do not help to select the best input reordering, but
548
help choose the best target phrase.
Searching a larger space Going from the sim-
pler to the more difficult conditions yields signif-
icant degradations in the model, as our best score
drops down from 25.6 to 23.5 (with known reorder-
ing) then to 19.1 (regular decoding). This is a clear
indication that our current segmentation/reordering
model is not delivering very useful scores. A similar
loss is incurred by the n-gram system, which loses
4 bleu points between the two conditions.
LM rescoring Our results to date with target side
language models have proven inconclusive, which
might explain why our best results remain between
one and two BLEU points behind the n-gram based
system using comparable information. Note also
that preliminary experiments with incorporating a
large bigram during training have also failed to date
to provide us with improvements over the baseline.
Summary In sum, the results accumulated during
this first round of experiments tend to show that our
CRF model is still underperforming the more es-
tablished baseline by approximately 1 to 1.5 BLEU
point, when provided with comparable resources.
Sources of improvements that have been clearly
identified is the scoring of reordering and segmen-
tations, and the use of a target language model in
training and/or decoding.
5 Related work
Discriminative learning approaches have proven
successful for many NLP tasks, notably thanks to
their ability to cope with flexible linguistic repre-
sentations and to accommodate potentially redun-
dant descriptions. This is especially appealing for
machine translation, where the mapping between
a source word or phrase and its target correlate(s)
seems to involve an large array of factors, such as its
morphology, its syntactic role, its meaning, its lexi-
cal context, etc. (see eg. (Och et al, 2004; Gimpel
and Smith, 2008; Chiang et al, 2009), for inspira-
tion regarding potentially useful features in SMT).
Discriminative learning requires (i) a parameter-
ized scoring function and (ii) a training objective.
The scoring function is usually assumed to be linear
and ranks candidate outputs y for input x accord-
ing to ?TG(x, y), where ? is the parameter vector. ?
andG deterministically imply the input/output map-
ping as x ? arg maxy ?
TG(x, y). Given a set of
training pairs {xi, yi, i = 1 . . . N}, parameters are
learned by optimizing some regularized loss func-
tion of ?, so as to make the inferred input/output
mapping faithfully replicate the observed instances.
Machine translation, like most NLP tasks, does
not easily lend itself to that approach, due to the
complexity of the input/output objects (word or la-
bel strings, parse trees, dependency structures, etc).
This complexity makes inference and learning in-
tractable, as both steps imply the resolution of
the arg max problem over a combinatorially large
space of candidates y. Structured learning tech-
niques (Bakir et al, 2007), developed over the last
decade, rely on decompositions of these objects into
sub-parts as part of a derivation process, and use
conditional independence assumptions between sub-
parts to render the learning and inference problem
tractable. For machine translation, this only pro-
vides part of the solution, as the training data only
contain pairs of word aligned sentences (f , e), but
lack the explicit derivation h from f to e that is re-
quired to train the model in a fully supervised way.
The approach of (Liang et al, 2006a) circumvents
the issue by assuming that the hidden derivation h
can be approximated through forced decoding. As-
suming that h is in fact observed as the optimal
(Viterbi) derivation h? from f to e given the cur-
rent parameter value10, it is straightforward to re-
cast the training of a phrase-based system as a stan-
dard structured learning problem, thus amenable to
training algorithms such as the averaged perceptron
of (Collins, 2002). This approximation is however
not genuine, and the choice of the most appropriate
derivation seems to raises intriguing issues (Watan-
abe et al, 2007; Chiang et al, 2008).
The authors of (Blunsom et al, 2008; Blunsom
and Osborne, 2008) consider models for which it is
computationally possible to marginalize out all pos-
sible derivations of a given translation. As demon-
strated in these papers, this approach is tractable
even when the derivation process is a based on syn-
chronous context-free grammars, rather that finite-
state devices. However, the computational cost as-
10If one actually exists in the model, thus raising the issue of
reference reachability, see discussion in Section 3.
549
sociated with training and inference remains very
high, especially when using a target side language
model, which seems to preclude the application to
large-scale translation tasks11. The recent work of
(Dyer and Resnik, 2010) proceeds from a similar
vein: translation is however modeled as a two step
process, where a set of possible source reorderings,
represented as a parse forest, are associated with
possible target sentences, using, as we do, a finite-
state translation model. This translation model is
trained discriminatively by marginalizing out the
(unobserved) reordering variables; inference can be
performed effectively by intersecting the input parse
forest with a transducer representing translation op-
tions.
A third strategy is to consider a simpler class of
derivation process, which only partly describe the
mapping between f and e. This is, for instance,
the approach of (Bangalore et al, 2007), where a
simple bag-of-word representation of the target sen-
tence is computed using a battery of boolean clas-
sifiers (one for each target word). In this approach,
discriminative training is readily applicable, as the
required supervision is overtly present in example
source-target pairs (f , e); however, a complemen-
tary reshaping/reordering step is necessary to turn
the bag-of-word into a full-fledged translation. This
work was recently revisited in (Mauser et al, 2009),
where a conditional model predicting the presence
of each target phrase provides a supplementary score
for the standard ?log-linear? model.
This line of research has been continued notably
in (Ka?a?ria?inen, 2009), which introduces an exponen-
tial model of bag of phrases (allowing some over-
lap), that enables to capture localized dependencies
between target words, while preserving (to some ex-
tend) the efficiency of training and inference. Su-
pervision is here indirectly provided by word align-
ment and correlated phrase extraction processes
implemented in conventional phrase-based systems
(Koehn et al, 2003). If this model seems to deliver
state-of-the-art performance on large-scale tasks, it
does so at a very high computational cost. More-
over, for lack of an internal modeling of reordering
processes, this approach, like the bag-of-word ap-
11For instance, the experiments reported in (Blunsom and Os-
borne, 2008) use the English-Chinese BTEC, where the average
sentence length is lesser than 10.
proach, seems only appropriate for language pairs
with similar or related word ordering.
The approach developed in this paper fills a gap
between the hierarchical model of (Blunsom et
al., 2008) and the phrase-based model (Ka?a?ria?inen,
2009), with whom we share several important as-
sumptions, such as the use of alignment information
to provide supervision, and the resort to a an ?ex-
ternal?, albeit a more powerful, reordering compo-
nent. Using a finite-state model enables to process
reasonably large corpora, and gives some hopes as to
the scalability of the whole enterprise; it also makes
the integration of a target side language model much
easier than in hierarchical models.
6 Discussion and future work
In this paper, we have given detailed description of
an original phrase-based system implementing a dis-
criminative version of the n-gram model, where the
translation model probabilities are computed with
conditional random fields. We have showed how
to implement this approach using a memory effi-
cient implementation of the optimization algorithms
needed for training: in our approach, training a mid-
scale translation system with hundred of thousands
sentence pairs and millions of features only takes a
couple of hours on a standalone desktop machine.
Using `1 regularization has enabled to assess the
usefulness of various families of features.
We have also detailed a complete decoder im-
plemented as a pipeline of finite-state transducers,
which allows to efficiently combine several models,
to produce n-best lists and word lattices.
The results obtained in a series of preliminary ex-
periments show that our system is already deliver-
ing competitive translations, as acknowledged by a
comparison with two strong phrase-based baselines.
We have already started to implement various opti-
mizations and to experiment with somewhat larger
datasets (up to 500K sentence pairs) and larger fea-
ture sets, notably incorporating word sense disam-
biguation features: this work needs to be contin-
ued. In addition, we intend to explore a number
of extensions of this architecture, such as imple-
menting MBR decoding (Kumar and Byrne, 2004)
or adapting the translation model to new domains
and conditions, using, for instance, the proposal of
550
(Daume III, 2007)12.
One positive side effect of experimenting with
new translation models is that they help reevalu-
ate the performance of the whole translation system
pipeline: in particular, discriminative training seems
to be more sensible to alignments errors than the cor-
responding n-gram system, which suggests to pay
more attention to possible errors in the training data;
we have also seen that the current reordering model
defines a too narrow search space and delivers in-
sufficiently discriminant scores: we will investigate
various ways to further improve the computation and
scoring of hypothetical source reorderings.
Acknowledgements
The authors wish to thank the reviewers for com-
ments and suggestions. This work was achieved as
part of the Quaero Programme, funded by OSEO,
French State agency for innovation.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Go?khan Bakir, Thomas Hofmann, Bernhard Scho?lkopf,
Alexander J.Smola, Ben Taskar, and S.V.N. Vish-
wanathan. 2007. Predicting structured output. MIT
Press.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 152?159,
Prague, Czech Republic.
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 215?223, Honolulu,
Hawaii.
12In a nutshell, this proposal amounts to having three dif-
ferent parameters for each feature; one parameter is trained
as usual; the other two parameters are updated conditionally,
depending whether the training instance comes from the in-
domain or from the out-domain training dataset.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-08: HLT,
pages 200?208, Columbus, Ohio.
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, Hawaii.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
218?226. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Josep M. Crego and Jose? B. Marin?o. 2007. Improving
SMT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic. Association for Compu-
tational Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
08: HLT, Short Papers, pages 25?28, Columbus, Ohio.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
551
surface heuristics. In Proceedings of the ACL work-
shop on Statistical Machine Translation, pages 31?38,
New York City, NY.
Yonggang Deng and William Byrne. 2006. MTTK: An
alignment toolkit for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Demon-
strations, pages 265?268, New York City, USA.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 858?866, Los Angeles,
California. Association for Computational Linguistics.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations ? In Pro-
ceedings of ACL-08: HLT, pages 986?993, Columbus,
Ohio.
Qin Gao and Stephan Vogel. 2008. Parallel implementa-
tions of word alignment tool. In SETQA-NLP ?08.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 9?17, Columbus, Ohio, June.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Matti Ka?a?ria?inen. 2009. Sinuhe ? statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1027?1036, Singapore.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistic, pages 127?133, Edmond-
ton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
Annual Meeting of the Association for Computational
Linguistics (ACL), demonstration session, pages 177?
180, Prague, Czech Republic.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages
169?176, Boston, Massachusetts, USA. Association
for Computational Linguistics.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the International Conference on Ma-
chine Learning, pages 282?289. Morgan Kaufmann,
San Francisco, CA.
Thomas Lavergne, Olivier Capp, and Franois Yvon.
2010. Practical very large scale crfs. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 504?513, Uppsala,
Sweden.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006a. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia.
Percy Liang, Ben Taskar, and Dan Klein. 2006b. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 133?139.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213?254. Springer Verlag.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning, pages 78?86.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
552
Libin Shen, David Smith, Katherine Eng, Viren Jain,
Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL 2004: Main Proceedings, pages 161?168,
Boston, Massachusetts, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Martin Riedmiller and Heinrich Braun. 1993. A direct
adaptive method for faster backpropagation learning:
The RPROP algorithm. In Proceedings of the IEEE
International Conference on Neural Networks, pages
586?591, San Francisco, USA.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. J.R.Statist.Soc.B, 58(1):267?288.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Susan Du-
mais, Daniel Marcu, and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 101?104, Boston,
Massachusetts, USA.
J. Turian, B. Wellington, and I.D. Melamed. 2007. Scal-
able discriminative learning for natural language pars-
ing and translation. In Proc. Neural Information Pro-
cessing Systems (NIPS), volume 19, pages 1409?1417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484, Uppsala, Sweden.
Fei Xia and Michael McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite pat-
terns. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING), pages
508?514, Geneva, Switzerland.
553
Workshop on Computational Linguistics for Literature, pages 36?44,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Aligning Bilingual Literary Works: a Pilot Study
Qian Yu and Aure?lien Max and Franc?ois Yvon
LIMSI/CNRS and Univ. Paris Sud
rue John von Neumann F-91 403 Orsay, France
{fistname.lastname}@limsi.fr
Abstract
Electronic versions of literary works abound on the In-
ternet and the rapid dissemination of electronic read-
ers will make electronic books more and more com-
mon. It is often the case that literary works exist in
more than one language, suggesting that, if properly
aligned, they could be turned into useful resources for
many practical applications, such as writing and lan-
guage learning aids, translation studies, or data-based
machine translation. To be of any use, these bilin-
gual works need to be aligned as precisely as possible,
a notoriously difficult task. In this paper, we revisit
the problem of sentence alignment for literary works
and explore the performance of a new, multi-pass,
approach based on a combination of systems. Ex-
periments conducted on excerpts of ten masterpieces
of the French and English literature show that our
approach significantly outperforms two open source
tools.
1 Introduction
The alignment of bitexts, i.e. of pairs of texts as-
sumed to be mutual translations, consists in find-
ing correspondences between logical units in the in-
put texts. The set of such correspondences is called
an alignment. Depending on the logical units that
are considered, various levels of granularity for the
alignment are obtained. It is usual to align para-
graphs, sentences, phrases or words (see (Wu, 2010;
Tiedemann, 2011) for recent reviews). Alignments
are used in many fields, ranging from Translation
Studies and Computer Assisted Language Learn-
ing (CALL) to Multilingual Natural Language Pro-
cessing (NLP) applications (Cross-Lingual Informa-
tion Retrieval, Writing Aids for Translators, Multi-
lingual Terminology Extraction and Machine Trans-
lation (MT)). For all these applications, sentence
alignments have to be computed.
Sentence alignment is generally thought to be
fairly easy and many efficient sentence alignment
programs are freely available1. Such programs rely
on two main assumptions: (i) the relative order of
sentences is the same on the two sides of the bi-
text, and (ii) sentence parallelism can be identified
using simple surface cues. Hypothesis (i) warrants
efficient sentence alignment algorithms based on
dynamic programming techniques. Regarding (ii),
various surface similarity measures have been pro-
posed: on the one hand, length-based measures
(Gale and Church, 1991; Brown et al, 1991) rely
on the fact that the translation of a short (resp. long)
sentence is short (resp. long). On the other hand,
lexical matching approaches (Kay and Ro?scheisen,
1993; Simard et al, 1993) identify sure anchor
points for the alignment using bilingual dictionar-
ies or surface similarities of word forms. Length-
based approaches are fast but error-prone, while lex-
ical matching approaches seem to deliver more re-
liable results. Most state-of-the-art approaches use
both types of information (Langlais, 1998; Simard
and Plamondon, 1998; Moore, 2002; Varga et al,
2005; Braune and Fraser, 2010).
In most applications, only high-confidence one-
to-one sentence alignments are considered useful
and kept for subsequent processing stages. Indeed,
when the objective is to build subsentential align-
1See, for instance, the Uplug toolbox which integrates sev-
eral sentence alignment tools in a unified framework:
http://sourceforge.net/projects/uplug/
36
ments (at the level of words, terms or phrases), other
types of mappings between sentences are deemed
to be either insufficiently reliable or inappropriate.
As it were, the one-to-one constraint is viewed as a
proxy to literalness/compositionality of the transla-
tion and warrants the search of finer-grained align-
ments. However, for certain types of bitexts2, such
as literary texts, translation often departs from a
straight sentence-by-sentence alignment and using
such a constraint can discard a significant propor-
tion of the bitext. For MT, this is just a regrettable
waste of potentially useful training material (Uszko-
reit et al, 2010), all the more so as parallel liter-
ary texts constitute a very large reservoir of par-
allel texts online. For other applications implying
to mine, visualize or read the actual translations in
their context (second language learning (Nerbonne,
2000; Kraif and Tutin, 2011), translators training,
automatic translation checking (Macklovitch, 1994),
etc.), the entire bitext has to be aligned. Further-
more, areas where the translation is only partial or
approximative need to be identified precisely.
The work reported in this study aims to explore
the quality of existing sentence alignment tech-
niques for literary work and to explore the usability
of a recently proposed multiple-pass approach, espe-
cially designed for recovering many-to-one pairings.
In a nutshell, this approach uses sure one-to-one
mappings detected in a first pass to train a discrim-
inative sentence alignment system, which is then
used to align the regions which remain problem-
atic. Our experiments on the BAF corpus (Simard,
1998) and on a small literary corpus consisting of ten
books show that this approach produces high quality
alignments and also identifies the most problematic
passages better than its competitors.
The rest of this paper is organized as follows:
we first report the results of a pilot study aimed at
aligning our corpus with existing alignment meth-
ods (Section 2). In Section 3, we briefly describe our
two-pass method, including some recent improve-
ments, and present experimental performance on the
BAF corpus. Attempts to apply this technique to our
larger literary corpus are reported and discussed in
2Actual literary bitexts are not so easily found over the Inter-
net, notably due to (i) issues related to variations in the source
text and (ii) issues related to the variations, over time, of the
very notion of what a translation should be like.
Section 4. We discuss further prospects and con-
clude in Section 5.
2 Book alignment with off-the-shelf tools
2.1 A small bilingual library
The corpus used in this study contains a random se-
lection of ten books written mostly in the 19th and
in the early 20th century: five are English classics
translated into French, and five are French classics
translated into English. These books and their trans-
lation are freely available3 from sources such as the
Gutenberg project4 or wikisource5, and are repre-
sentative of the kinds of collections that can be easily
collected from the Internet. These texts have been
preprocessed and tokenized using in-house tools,
yielding word and sentence counts in Table 1.
2.2 Baseline sentence alignments
2.2.1 Public domain tools
Baseline alignments are computed using two
open-source sentence alignment packages, the sen-
tence alignment tool of Moore (2002)6, and Hu-
nalign (Varga et al, 2005). These two tools were
chosen as representative of the current state-of-the-
art in sentence alignment. Moore?s approach im-
plements a two-pass, coarse-to-fine, strategy: a first
pass, based on sentence length cues, computes a
first alignment according to the principles of length-
based approaches (Brown et al, 1991; Gale and
Church, 1991). This alignment is used to train a sim-
plified version of IBM model 1 (Brown et al, 1993),
which provides the alignment system with lexical
association scores; these scores are then used to re-
fine the measure of association between sentences.
This approach is primarily aimed at delivering high
confidence, one-to-one, sentence alignments to be
used as training material for data-intensive MT. Sen-
tences that cannot be reliably aligned are discarded
from the resulting alignment.
3Getting access to more recent books (or their translation) is
problematic, due to copyright issues: literary works fall in the
public domain 70 years after the death of their author.
4http://www.gutenberg.org
5http://wikisource.org
6http://research.microsoft.com/en-us/downloads/
aafd5dcf-4dcc-49b2-8a22-f7055113e656/
37
French side English side
# sents # words # sents # words
English books and their French translation
Emma, J. Austen EM 5,764 134,950 7,215 200,223
Jane Eyre, C. Bronte? JE 9,773 240,032 9,441 237,487
The last of the Mohicans, F. Cooper LM 6,088 189,724 5,629 177,303
Lord Jim, J. Conrad LJ 7962 175,876 7,685 162,498
Vanity fair, W. Thackeray VF 14,534 395,702 12,769 372,027
French books and their English translation
Les confessions, J.J. Rousseau CO 9,572 324,597 8,308 318,658
5 semaines en ballon, J. Verne 5S 7,250 109,268 7,894 121,231
La faute de l?Abbe? Mouret, E. Zola AM 8,604 156,514 7,481 156,692
Les travailleurs de la mer, V. Hugo TM 10,331 170,015 9,613 178,427
Du co?te? de chez Swann, M. Proust SW 4,853 208,020 4,738 232,514
Total 84,731 2,104,698 80,773 2,157,060
Table 1: A small bilingual library
Hunalign7, with default settings, also implements
a two-pass strategy which resembles the approach of
Moore. Their main difference is that Hunalign also
produces many-to-one and one-to-many alignment
links, which are needed to ensure that all the input
sentences appear in the final alignment.
Both systems also deliver confidence measures
for the automatic alignment: a value between 0 and
1 for Moore?s tool, which can be interpreted as a
posterior probability; the values delivered by Hu-
nalign are less easily understood, and range from?1
to some small positive real values (greater than 1).
2.2.2 Evaluation metrics
Sentence alignment tools are usually evaluated
using standard recall [R] and precision [P] mea-
sures, combined in the F-measure [F], with respect
to some manually defined gold alignment (Ve?ronis
and Langlais, 2000). These measures can be com-
puted at various levels of granularity: the level of
alignment links, of sentences, of words, and of char-
acters. As gold references only specify alignment
links, the other references are automatically derived
in the most inclusive way. For instance, if the refer-
ence alignment links state that the pair of source sen-
tences f1, f2 is aligned with target e, the reference
sentence alignment will contain both (f1, e) and
7ftp://ftp.mokk.bme.hu/Hunglish/src/hunalign; we have
used the version that ships with Uplug.
(f2, e); likewise, the reference word alignment will
contain all the possible word alignments between
tokens in the source and the target side. For such
metrics, missing the alignment of a large ?block?
of sentences gets a higher penalty than missing a
small one; likewise, misaligning short sentences is
less penalized than misaligning longer ones. As a
side effect, all metrics, but the more severe one, ig-
nore null alignments. Our results are therefore based
on the link-level and sentence-level F-measure, to
reflect the importance of correctly predicting un-
aligned sentences in our applicative scenario.
2.2.3 Results
Previous comparisons of these alignment tools
on standard benchmarks have shown that both typ-
ically yield near state-of-the-art performance. For
instance, experiments conducted using the literary
subpart of the BAF corpus (Simard, 1998), con-
sisting of a hand-checked alignment of the French
novel De la Terre a` la Lune (From the Earth to
the Moon), by Jules Verne, with a slightly abridged
translation available from the Gutenberg project8,
have yielded the results in Table 2 (Moore?s system
was used with its default parameters, Hunalign with
the --realign option).
All in all, for this specific corpus, Moore?s strat-
egy delivers slightly better sentence alignments than
8http://www.gutenberg.org/ebooks/83
38
P R F % 1-1 links
Alignment based metrics
Hunalign 0.51 0.60 0.55 0.77
Moore 0.85 0.65 0.74 1.00
Sentence based metrics
Hunalign 0.76 0.70 0.73 -
Moore 0.98 0.62 0.76 -
Table 2: Baseline alignment experiments
Figure 1: Percentage of one-to-one links and pseudo-
paragraph size for various baselines
Hunalign does; in particular, it is able to identify 1-
to-1 links with a very high precision.
2.3 Aligning a small library
In a first series of experiments, we simply run the
two alignment tools on our small collection to see
howmuch of it can be aligned with a reasonable con-
fidence. The main results are reproduced in Figure 1,
where we display both the number of 1-to-1 links
extracted by the baselines (as dots on the Figure), as
well as the average size of pseudo-paragraphs (see
definition below) in French and English. As ex-
pected, less 1-to-1 links almost always imply larger
blocks.
As expected, these texts turn out to be rather
difficult to align: in the best case (Swann?s way
(SW)), only about 80% of the total sentences are
aligned by Moore?s system; in the more problem-
atic cases (Emma (EM) and Vanity Fair (VF)), more
than 50% of the book content is actually thrown
away when one only looks at Moore?s alignments.
Hunalign?s results look more positive, as a signifi-
cantly larger number of one-to-one correspondences
is found. Given that this system is overall less reli-
able than Moore?s approach, it might be safe to filter
these alignments and keep only the surer ones (here,
keeping only links having a score greater than 0.5).
The resulting number of sentences falls way below
what is obtained by Moore?s approach.
To conclude, both systems seem to have more dif-
ficulties with the literary material considered here
than with other types of texts. In particular, the
proportion of one-to-one links appears to be signif-
icantly smaller than what is typically reported for
other genres; note, however, that even in the worst
case, one-to-one links still account for about 50% of
the text. Another finding is that the alignment scores
which are output are not very useful: for Moore, fil-
tering low scoring links has very little effect; for Hu-
nalign, there is a sharp transition (around a threshold
of 0.5): below this value, filtering has little effect;
above this value, filtering is too drastic, as shown on
Figure 1.
3 Learning sentence alignments
In this section, we outline the main principles of
the approach developed in this study to improve the
sentence alignments produced by our baseline tools,
with the aim to salvage as many sentences as possi-
ble, which implies to come up with a way for better
detecting many-to-one and one-to-many correspon-
dences. Our starting point is the set of alignments
delivered by Moore?s tool. As discussed above,
these alignments have a very high precision, at the
expense of an unsatisfactory recall. Our sentence
alignment method considers these sentence pairs as
being parallel and uses them to train a binary classi-
fier for detecting parallel sentences. Using the pre-
dictions of this tool, it then attempts to align the re-
maining portions of the bitext (see Figure 2).
In Figure 2, Moore?s links are displayed with
solid lines; these lines delineate parallel pseudo-
paragraphs in the bitexts (appearing in boxed areas),
which we will try to further decompose. Note that
two configurations need to be distinguished: (i) one
side of a paragraph is empty: no further analysis
is performed and a 0-to-many alignment is output;
(ii) both sides of a paragraph are non-empty and de-
fine a i-to-j alignment that will be processed by the
block alignment algorithm described below.
39
Figure 2: Filling alignment gaps
3.1 Detecting parallelism
Assuming the availability of a set of example paral-
lel sentences, the first step of our approach consists
in training a function for scoring candidate align-
ments. Following (Munteanu and Marcu, 2005), we
train a Maximum Entropy classifier9 (Rathnaparkhi,
1998); in principle, many other binary classifiers
would be possible here. Our motivation for using
a maxent approach was to obtain, for each possible
pair of sentences (f ,e), a link posterior probability
P (link|f , e).
We take the sentence alignments of the first step
as positive examples. Negative examples are artifi-
cially generated as follows: for all pairs of positive
instances (e, f) and (e?, f ?) such that e? immediately
follows e, we select the pair (e, f ?) as a negative ex-
ample. This strategy produced a balanced corpus
containing as many negative pairs as positive ones.
However, this approach may give too much weight
on the length ratio feature and it remains to be seen
whether alternative approaches are more suitable.
Formally, the problem is thus to estimate a con-
ditional model for deciding whether two sentences
e and f should be aligned. Denoting Y the corre-
sponding binary variable, this model has the follow-
9Using the implementation available from http://homepages.
inf.ed.ac.uk/lzhang10/maxent toolkit.html.
ing form:
P (Y = 1|e, f) =
1
1 + exp[?
?K
k=1 ?kFk(e, f)]
,
where {Fk(e, f), k = 1 . . .K} denotes a set of fea-
ture functions testing arbitrary properties of e and f ,
and {?k, k = 1 . . .K} is the corresponding set of
parameter values.
Given a set of training sentence pairs, the opti-
mal values of the parameters are set by optimizing
numerically the conditional likelihood; optimization
is performed here using L-BFGS (Liu and Nocedal,
1989); a Gaussian prior over the parameters is used
to ensure numerical stability of the optimization.
In this study, we used the following set of feature
functions:
? lexical features: for each pair of words10 (e, f)
occurring in Ve ? Vf , there is a corresponding
feature Fe,f which fires whenever e ? e and
f ? f .
? length features: denoting le (resp. lf ) the
length of the source (resp. target) sentence,
measured in number of characters, we in-
clude features related to length ratio, defined
as Fr(e, f) =
|le?lf |
max(le,lf )
. Rather than taking the
numerical value, we use a simple discretization
scheme based on 6 bins.
? cognate features: we loosely define cog-
nates11 as words sharing a common prefix of
length at least 3. This gives rise to 4 features,
which are respectively activated when the num-
ber of cognates in the parallel sentence is 0, 1,
2, or greater than 2.
? copy features: an extreme case of similarity
is when a word is copied verbatim from the
source to the target. This happens with proper
nouns, dates, etc. We again derive 4 features,
depending on whether the number of identical
words in f and e is 0, 1, 2 or greater than 2.
10A word is an alphabetic string of characters, excluding
punction marks.
11Cognates are words that share a similar spelling in two or
more different languages, as a result of their similar meaning
and/or common etymological origin, e.g. (English-Spanish):
history - historia, harmonious - armonioso.
40
3.2 Filling alignment gaps
The third step uses the posterior alignment proba-
bilities computed in the second step to fill the gaps
in the first pass alignment. The algorithm can be
glossed as follows. Assume a bitext block compris-
ing the sentences from index i to j in the source
side of the bitext, and from k to l in the target side
such that sentences ei?1 (resp. ej+1) and fk?1 (resp.
el+1) are aligned12.
The first case is when j < i or k > l, in which
case we create a null alignment for fk:l or for ei:j . In
all other situations, we compute:
?i?, j?, k?, l?, i ? i? ? j? ? j, k ? k? ? l? ? l,
ai?,j?,k?,l? = P (Y = 1|ei?:j? , fk?:l?) ? ?S(i
?, j?, k?, l?)
where ei?:j? is obtained by concatenation of all the
sentences in the range [i?:j?], and S(i, j, k, l) = (j ?
i+1)(l?k+1)?1 is proportional to the block size.
The factor ?S(i?, j?, k?, l?) aims at penalizing large
blocks, which, for the sentence-based metrics, yield
much more errors than the small ones. This strategy
implies to compute O(|j ? i + 1|2 ? |k ? l + 1|2)
probabilities, which, given the typical size of these
blocks (see above), can be performed very quickly.
These values are then iteratively visited by de-
creasing order in a greedy fashion. The top-scoring
block i? : j?, k? : l? is retained in the final alignment;
all overlapping blocks are subsequently deleted from
the list and the next best entry is then considered.
This process continues until all remaining blocks
imply null alignments, in which case these n ? 0 or
0 ? n alignments are also included in our solution.
This process is illustrated in Figure 3: assuming
that the best matching link is f2-e2, we delete all
the links that include f2 or e2, as well as links that
would imply a reordering of sentences, meaning that
we also delete links such as f1-e3.
3.3 Experiments
In this section, we report the results of experiments
run using again Jules Verne?s book from the BAF
corpus. Figures are reported in Table 3 where we
contrast our approach with two simple baselines:
(i) keep only Moore?s links; (ii) complete Moore?s
links with one single many-to-many alignment for
12We enclose the source and target texts between begin and
end markers to enforce alignment of the first and last sentences.
Figure 3: Greedy alignment search
P R F
(maxent) (all) (all) (all)
link based
Moore only - 0.85 0.65 0.74
Moore+all links - 0.78 0.75 0.76
Maxent, ? = 0 0.44 0.74 0.81 0.77
Maxent, ? = 0.06 0.42 0.72 0.82 0.77
sentence based
Moore only - 0.98 0.62 0.76
Moore+all links - 0.61 0.88 0.72
Maxent, ? = 0 0.80 0.93 0.80 0.86
Maxent, ? = 0.06 0.91 0.97 0.79 0.87
Table 3: Performance of maxent-based alignments
each block. For the maxent-based approach, we also
report the precision on just those links that are not
predicted by Moore. A more complete set of experi-
ments conducted with other portions of the BAF are
reported elsewhere (Yu et al, 2012) and have shown
to deliver state-of-the-art results.
As expected, complementing the very accurate
prediction of Moore?s systems with our links sig-
nificantly boosts the sentence-based alignment per-
formance: recall rises from 0.62 to 0.80 for ? = 0,
which has a clear effect on the corresponding F-
measure (from 0.76 to 0.86). The performance dif-
ferences with the default strategy of keeping those
blocks unsegmented are also very clear. Sentence-
wise, maxent-based alignments are also quite pre-
cise, especially when the value of ? is chosen with
care (P=0.91 for ?=0.06); however, this optimiza-
tion has a very small overall effect, given that only a
limited number of alignment links are actually com-
puted by the maxent classifier.
41
4 Sentence alignment in the real world
In this section, we analyze the performance obtained
with our combined system, using excerpts of our
small corpus as test set. For this experiment, the
first two to three hundreds sentences in each book,
corresponding to approximately two chapters, were
manually aligned (by one annotator), using the same
guidelines that were used for annotating the BAF
corpus. Except for two books (EM and VF), produc-
ing these manual alignments was found to be quite
straightforward. Results are in Table 4.
A first comment is that both baselines are signifi-
cantly outperformed by our algorithm for almost all
conditions and books. For several books (LM, AM,
SW), the obtained sentence alignments are almost
as precise as those predicted by Moore and have a
much higher recall, resulting in very good overall
alignments. The situation is, of course, much less
satisfactory for other books (EM, VF, 5S). All in all,
our method salvages many useful sentence pairs that
would otherwise be left unaligned.
Moore?s method remains remarkably accurate
throughout the whole collection, even for the most
difficult books. It also outputs a significant propor-
tion of wrong links, which, for lack of reliable confi-
dence estimators, are difficult to spot and contribute
to introduce noise into the maxent training set.
The variation of performance can mostly be at-
tributed to idiosyncrasies in the translation. For in-
stance, Emma (EM) seems very difficult to align,
which can be attributed to the use of an old transla-
tion dating back to 1910 (by P. de Puliga), and which
often looks more like an adaptation than a transla-
tion. Some passages even question the possibility of
producing any sensible (human) alignment between
source and target13:
(en) Her sister, though comparatively but little removed by
matrimony, being settled in London, only sixteen miles off,
was much beyond her daily reach; and many a long October
and November evening must be struggled through at Hart-
field, before Christmas brought the next visit from Isabella
and her husband, and their little children, to fill the house,
and give her pleasant society again.
(fr) La s?ur d?Emma habitait Londres depuis son mariage,
c?est-a`-dire, en re?alite?, a` peu de distance; elle se trouvait
13In this excerpt, in addition to several approximations, the
end of the last sentence (and their children...) is not translated
in French.
ne?anmoins hors de sa porte?e journalie`re, et bien des longues
soire?es d?automne devraient e?tre passe?es solitairement a`
Hartfield avant que Noe?l n?amena?t la visite d?Isabelle et de
son mari.
Les confessions (CO) is much most faithful to the
content, yet, the translator has significantly departed
from Rousseau?s style14, mostly made up of short
sentences, and it is often the case that several French
sentences align with one single English sentence,
which is detrimental to Moore, and by ricochet, to
the quality of maxent predictions. A typical excerpt:
(fr) Pendant deux ans entiers je ne fus ni te?moin ni victime
d?un sentiment violent. Tout nourrissait dans mon coeur les
dispositions qu?il rec?ut de la nature.
(en) Everything contributed to strengthen those propensities
which nature had implanted in my breast, and during the
two years I was neither the victim nor witness of any violent
emotions.
The same goes for Thackeray (VF), with a lot of re-
structurations of the sentences as demonstrated by
the uneven number of sentences on both sides of the
bitext. Lord Jim (LJ) poses another type of diffi-
culty: approximately 100 sentences are missing on
the French side, the rest of the text being fairly paral-
lel (more than 82% of the reference links are actually
1-to-1). Du co?te? de chez Swann (SW) represents the
other extreme of the spectrum, where the translation
sticks as much as possible to the very peculiar style
of Proust: nearly 90% of the reference alignments
are 1-to-1, which explains the very good F-measure
for this book.
It is difficult to analyze more precisely our er-
rors; however, a fairly typical pattern is the infer-
ence of a 1-to-1 link rather than a 2-to-1 link made
up of a short and a long sentence. An example from
Hugo (TM), where our approach prefers to leave
the second English sentence unaligned, even though
the corresponding segment (un enfant...) is the in
French sentence:
(fr) Dans tout le tronc?on de route qui se?pare la premie`re tour
de la seconde tour, il n?y avait que trois passants, un enfant,
un homme et une femme.
(en) Throughout that portion of the highway which separates
the first from the second tower, only three foot-passengers
could be seen. These were a child, a man, and a woman.
A possible walk around for this problem would be
to also add a penalty for null alignments.
14Compare the number of sentences in Table 1.
42
Moore Hunalign Moore+maxent
links P R F links F S 6= 0 S = 0 P R F
fr en links link based
EM 160 217 164 84 0.76 0.39 0.52 173 0.43 72 10 0.52 0.53 0.52
JE 229 205 174 104 0.86 0.51 0.64 198 0.40 95 5 0.64 0.75 0.69
LM 232 205 197 153 0.97 0.76 0.85 203 0.63 64 2 0.79 0.87 0.83
LJ 580 682 515 403 0.94 0.73 0.82 616 0.60 155 15 0.82 0.81 0.76
VF 321 248 219 129 0.92 0.54 0.68 251 0.39 133 3 0.58 0.70 0.63
CO 326 236 213 104 0.86 0.42 0.56 256 0.28 135 3 0.62 0.70 0.66
5S 182 201 153 107 0.76 0.53 0.62 165 0.52 72 10 0.60 0.74 0.66
AM 258 226 222 179 1.00 0.81 0.90 222 0.71 55 0 0.88 0.93 0.90
TM 404 388 358 284 0.89 0.71 0.79 374 0.69 86 16 0.79 0.85 0.82
SW 492 495 463 431 0.94 0.87 0.90 474 0.80 59 9 0.85 0.92 0.88
fr en links sentence based
EM 160 217 206 84 0.85 0.34 0.49 199 0.60 124 0 0.62 0.63 0.62
JE 229 205 270 104 0.92 0.36 0.52 235 0.60 125 0 0.90 0.76 0.82
LM 232 205 238 153 0.99 0.64 0.78 234 0.79 62 0 0.97 0.88 0.92
LJ 580 682 645 403 0.96 0.60 0.74 625 0.78 212 0 0.85 0.81 0.83
VF 321 248 363 129 0.98 0.35 0.52 318 0.62 163 0 0.88 0.71 0.79
CO 326 236 380 104 0.94 0.26 0.41 306 0.48 226 0 0.88 0.76 0.82
5S 182 201 260 107 0.98 0.40 0.57 224 0.70 81 0 0.93 0.67 0.78
AM 258 226 264 179 1.00 0.68 0.81 262 0.84 72 0 0.98 0.94 0.96
TM 404 388 445 284 0.96 0.61 0.75 418 0.82 134 0 0.93 0.87 0.90
SW 492 495 532 431 0.99 0.80 0.88 512 0.88 55 0 0.99 0.90 0.94
Table 4: Evaluating alignment systems on a sample of ?real-world? books
For each book, we report the number of French and English test sentences, the number of reference links and standard performance
measures. For the maxent approach, we also report separately the number of empty (S = 0) and non-empty (S 6= 0) paragraphs.
5 Conclusions and future work
In this paper, we have presented a novel two-pass ap-
proach aimed at improving existing sentence align-
ment methods in contexts where (i) all sentences
need to be aligned and/or (ii) sentence alignment
confidence need to be computed. By running ex-
periments with several variants of this approach, we
have been able to show that it was able to signif-
icantly improve the bare results obtained with the
sole Moore alignment system. Our study shows
that the problem of sentence alignment for literary
texts is far from being solved and additional work
is needed to obtain alignments that could be used in
real applications, such as bilingual reading aids.
The maxent-based approach proposed here is thus
only a first step, and we intend to explore various
extensions: an obvious way to go is to use more
resources (larger training corpora, bilingual dictio-
naries, etc.) and add more features, such as part-of-
speech, lemmas, or alignment features as was done
in (Munteanu and Marcu, 2005). We also plan to
provide a much tighter integration with Moore?s al-
gorithm, which already computes such alignments,
so as to avoid having to recompute them. Finally,
the greedy approach to link selection can easily be
replaced with an exact search based on dynamic pro-
gramming techniques, including dependencies with
the left and right alignment links.
Regarding applications, a next step will be to pro-
duce and evaluate sentence alignments for a much
larger and more diverse set of books, comprising
more than 100 novels, containing books in 7 lan-
guages (French, English, Spanish, Italian, German,
Russian, Portuguese) from various origins. Most
were collected on the Internet from Gutenberg, wik-
isource and GoogleBooks15, and some were col-
lected in the course of the Carmel project (Kraif et
al., 2007). A number of these books are translated
in more than one language, and some are raw OCR
outputs and have not been cleaned from errors.
Acknowledgments
This work has been partly funded through the
?Google Digital Humanities Award? program.
15http://books.google.com
43
References
Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for symmet-
rical and asymmetrical parallel corpora. In Coling
2010: Posters, pages 81?89, Beijing, China. Coling
2010 Organizing Committee.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the 29th annual meeting on Association
for Computational Linguistics, 1991, Berkeley, Cali-
fornia, pages 169?176.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th annual meeting of the Associ-
ation for Computational Linguistics, pages 177?184,
Berkeley, California.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignement. Computational Linguistics,
19(1):121?142.
Olivier Kraif and Agne`s Tutin. 2011. Using a bilingual
annotated corpus as a writing aid: An application for
academic writing for efl users. In In Natalie Ku?bler
(Ed.), editor, Corpora, Language, Teaching, and Re-
sources: From Theory to Practice. Selected papers
from TaLC7, the 7th Conference of Teaching and Lan-
guage Corpora. Peter Lang, Bruxelles.
Olivier Kraif, Marc El-Be`ze, Re?gis Meyer, and Claude
Richard. 2007. Le corpus Carmel: un corpus multi-
lingue de re?cits de voyages. In Proceedings of Teach-
ing and Language Corpora : TaLC?200, Paris.
Philippe Langlais. 1998. A System to Align Com-
plex Bilingual Corpora. Technical report, CTT, KTH,
Stockholm, Sweden, Sept.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Elliot Macklovitch. 1994. Using bi-textual alignment for
translation validation: the TransCheck system. In Pro-
ceedings of the First Conference of the Association for
Machine Translation in the Americas (AMTA), pages
157?168, Columbia.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Stephen D.
Richardson, editor, Proceedings of the annual meet-
ing of tha Association for Machine Translation in
the Americas (AMTA?02), Lecture Notes in Computer
Science 2499, pages 135?144, Tiburon, CA, USA.
Springer Verlag.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
John Nerbonne, 2000. Parallel Texts in Computer-
Assisted Language Learning, chapter 15, pages 354?
369. Text Speech and Language Technology Series.
Kluwer Academic Publishers.
Ardwait Rathnaparkhi. 1998. Maximum Entropy Mod-
els for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Michel Simard and Pierre Plamondon. 1998. Bilingual
sentence alignment: Balancing robustness and accu-
racy. Machine Translation, 13(1):59?80.
Michel Simard, George F. Foster, and Pierre Isabelle.
1993. Using cognates to align sentences in bilingual
corpora. In Ann Gawman, Evelyn Kidd, and Per-
A?ke Larson, editors, Proceedings of the 1993 Confer-
ence of the Centre for Advanced Studies on Collabora-
tive Research, October 24-28, 1993, Toronto, Ontario,
Canada, 2 Volume, pages 1071?1082.
Michel Simard. 1998. The BAF: a corpus of English-
French bitext. In First International Conference on
Language Resources and Evaluation, volume 1, pages
489?494, Granada, Spain.
Jo?rg Tiedemann. 2011. Bitext Alignment. Number 14
in Synthesis Lectures on Human Language Technolo-
gies, Graeme Hirst (ed). Morgan & Claypool Publish-
ers.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 1101?1109, Beijing,
China.
Da?niel Varga, La?szlo? Ne?meth, Pe?ter Hala?csy, Andra?s Ko-
rnai, Viktor Tro?n, and Viktor Nagy. 2005. Parallel cor-
pora for medium density languages. In Proceedings of
RANLP 2005, pages 590?596, Borovets, Bulgaria.
Jean Ve?ronis and Philippe Langlais. 2000. Evaluation
of Parallel Text Alignment Systems. In Jean Ve?ronis,
editor, Parallel Text Processing, Text Speech and Lan-
guage Technology Series, chapter X, pages 369?388.
Kluwer Academic Publishers.
Dekai Wu. 2010. Alignment. In Nitin Indurkhya
and Fred Damerau, editors, CRC Handbook of Natu-
ral Language Processing, number 16, pages 367?408.
CRC Press.
Qian Yu, Aure?lien Max, and Franc?ois Yvon. 2012.
Revisiting sentence alignment algorithms for align-
ment visualization and evaluation. In Proceedings of
the Language Resource and Evaluation Conference
(LREC), Istambul, Turkey.
44
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 1?10,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Measuring the Influence of Long Range Dependencies with Neural Network
Language Models
Le Hai Son and Alexandre Allauzen and Franc?ois Yvon
Univ. Paris-Sud and LIMSI/CNRS
rue John von Neumann, 91 403 Orsay cedex, France
Firstname.Lastname@limsi.fr
Abstract
In spite of their well known limitations,
most notably their use of very local con-
texts, n-gram language models remain an es-
sential component of many Natural Language
Processing applications, such as Automatic
Speech Recognition or Statistical Machine
Translation. This paper investigates the po-
tential of language models using larger con-
text windows comprising up to the 9 previ-
ous words. This study is made possible by
the development of several novel Neural Net-
work Language Model architectures, which
can easily fare with such large context win-
dows. We experimentally observed that ex-
tending the context size yields clear gains in
terms of perplexity and that the n-gram as-
sumption is statistically reasonable as long as
n is sufficiently high, and that efforts should
be focused on improving the estimation pro-
cedures for such large models.
1 Introduction
Conventional n-gram Language Models (LMs) are a
cornerstone of modern language modeling for Natu-
ral Language Processing (NLP) systems such as sta-
tistical machine translation (SMT) and Automatic
Speech Recognition (ASR). After more than two
decades of experimenting with these models in a
variety of languages, genres, datasets and appli-
cations, the vexing conclusion is that these mod-
els are very difficult to improve upon. Many vari-
ants of the simple n-gram model have been dis-
cussed in the literature; yet, very few of these vari-
ants have shown to deliver consistent performance
gains. Among these, smoothing techniques, such as
Good-Turing, Witten-Bell and Kneser-Ney smooth-
ing schemes (see (Chen and Goodman, 1996) for an
empirical overview and (Teh, 2006) for a Bayesian
interpretation) are used to compute estimates for the
probability of unseen events, which are needed to
achieve state-of-the-art performance in large-scale
settings. This is because, even when using the sim-
plifying n-gram assumption, maximum likelihood
estimates remain unreliable and tend to overeresti-
mate the probability of those rare n-grams that are
actually observed, while the remaining lots receive
a too small (null) probability.
One of the most successful alternative to date is
to use distributed word representations (Bengio et
al., 2003) to estimate the n-gram models. In this
approach, the discrete representation of the vocabu-
lary, where each word is associated with an arbitrary
index, is replaced with a continuous representation,
where words that are distributionally similar are rep-
resented as neighbors. This turns n-gram distribu-
tions into smooth functions of the word representa-
tion. These representations and the associated esti-
mates are jointly computed using a multi-layer neu-
ral network architecture. The use of neural-networks
language models was originally introduced in (Ben-
gio et al, 2003) and successfully applied to large-
scale speech recognition (Schwenk and Gauvain,
2002; Schwenk, 2007) and machine translation
tasks (Allauzen et al, 2011). Following these ini-
tial successes, the neural approach has recently been
extended in several promising ways (Mikolov et al,
2011a; Kuo et al, 2010; Liu et al, 2011).
Another difference between conventional and
1
neural network language models (NNLMs) that has
often been overlooked is the ability of the latter to
fare with extended contexts (Schwenk and Koehn,
2008; Emami et al, 2008); in comparison, standard
n-gram LMs rarely use values of n above n = 4
or 5, mainly because of data sparsity issues and
the lack of generalization of the standard estimates,
notwithstanding the complexity of the computations
incurred by the smoothing procedures (see however
(Brants et al, 2007) for an attempt to build very
large models with a simple smoothing scheme).
The recent attempts of Mikolov et al (2011b)
to resuscitate recurrent neural network architectures
goes one step further in that direction, as a recur-
rent network simulates an unbounded history size,
whereby the memory of all the previous words ac-
cumulates in the form of activation patterns on the
hidden layer. Significant improvements in ASR us-
ing these models were reported in (Mikolov et al,
2011b; Mikolov et al, 2011a). It must however be
emphasized that the use of a recurrent structure im-
plies an increased complexity of the training and in-
ference procedures, as compared to a standard feed-
forward network. This means that this approach can-
not handle large training corpora as easily as n-gram
models, which makes it difficult to perform a fair
comparison between these two architectures and to
assess the real benefits of using very large contexts.
The contribution is this paper is two-fold. We
first analyze the results of various NNLMs to assess
whether long range dependencies are efficient in lan-
guage modeling, considering history sizes ranging
from 3 words to an unbounded number of words (re-
current architecture). A by-product of this study is a
slightly modified version of n-gram SOUL model
(Le et al, 2011a) that aims at quantitatively esti-
mating the influence of context words both in terms
of their position and their part-of-speech informa-
tion. The experimental set-up is based on a large
scale machine translation task. We then propose a
head to head comparison between the feed-forward
and recurrent NNLMs. To make this comparison
fair, we introduce an extension of the SOUL model
that approximates the recurrent architecture with a
limited history. While this extension achieves per-
formance that are similar to the recurrent model on
small datasets, the associated training procedure can
benefit from all the speed-ups and tricks of standard
feedforward NNLM (mini-batch and resampling),
which make it able to handle large training corpora.
Furthermore, we show that this approximation can
also be effectively used to bootstrap the training of a
?true? recurrent architecture.
The rest of this paper is organized as follows. We
first recollect, in Section 2, the basics of NNLMs ar-
chitectures. We then describe, in Section 3, a num-
ber of ways to speed up training for our ?pseudo-
recurrent? model. We finally report, in Section 4,
various experimental results aimed at measuring the
impact of large contexts, first in terms of perplexity,
then on a realistic English to French translation task.
2 Language modeling in a continuous
space
Let V be a finite vocabulary, language models de-
fine distributions over sequences1 of tokens (typi-
cally words) wL1 in V
+ as follows:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
1 ) (1)
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in NLP applications where V
typically contains hundreds of thousands words. In
the n-gram model, the context is limited to the n?1
previous words, yielding the following factorization:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
i?n+1) (2)
Neural network language models (Bengio et al,
2003) propose to represent words in a continuous
space and to estimate the probability distribution as
a smooth function of this representation. Figure 1
provides an overview of this approach. The context
words are first projected in a continuous space using
the shared matrix R. Denoting v the 1-of-V coding
vector of word v (all null except for the vth compo-
nent which is set to 1), its projection vector is the
vth line of R: RTv. The hidden layer h is then
computed as a non-linear function of these vectors.
Finally, the probability of all possible outcomes are
computed using one or several softmax layer(s).
1wji denotes a sequence of tokens wi . . . j when j ? i, or
the empty sequence otherwise.
2
  
0...0100
10...000
0...0010
v-3
v-2
v-1
R
R
R
shared input space
input layer
hidden layers
shortlist
sub-classlayers
wordlayers
classlayer
input part output part
W
Figure 1: 4-gram model with SOUL at the output layer.
This architecture can be divided in two parts, with
the hidden layer in the middle: the input part (on the
left hand side of the graph) which aims at represent-
ing the context of the prediction; and the output part
(on the right hand side) which computes the proba-
bility of all possible successor words given the con-
text. In the remaining of this section, we describe
these two parts in more detail.
2.1 Input Layer Structure
The input part computes a continuous representation
of the context in the form of a context vector h to be
processed through the hidden layer.
2.1.1 N -gram Input Layer
Using the standard n-gram assumption of equa-
tion (2), the context is made up of the sole n?1 pre-
vious words. In a n-gram NNLM, these words are
projected in the shared continuous space and their
representations are then concatenated to form a sin-
gle vector i, as illustrated in the left part of Figure 1:
i = {RTv?(n?1);R
Tv?(n?2); . . . ;R
Tv?1}, (3)
where v?k is the kth previous word. A non-linear
transformation is then applied to compute the first
hidden layer h as follows:
h = sigm (Wi+ b) , (4)
with sigm the sigmoid function. This kind of archi-
tecture will be referred to as a feed-forward NNLM.
Conventional n-gram LMs are usually limited to
small values of n, and using n greater that 4 or 5
does not seem to be of much use. Indeed, previ-
ous experiments using very large speech recognition
systems indicated that the gain obtained by increas-
ing the n-gram order from 4 to 5 is almost negli-
gible, whereas the model size increases drastically.
While using large context seems to be very imprac-
tical with back-off LMs, the situation is quite dif-
ferent for NNLMs due to their specific architecture.
In fact, increasing the context length for a NNLM
mainly implies to expend the projection layer with
one supplementary projection vector, which can fur-
thermore be computed very easily through a sim-
ple look-up operation. The overall complexity of
NNLMs thus only grows linearly with n in the worst
case (Schwenk, 2007).
In order to better investigate the impact of each
context position in the prediction, we introduce a
slight modification of this architecture in a man-
ner analog to the proposal of Collobert and Weston
(2008). In this variation, the computation of the hid-
den layer defined by equation (4) is replaced by:
h = sigm
(
max
k
[
WkR
Tv?k
]
+ b
)
, (5)
where Wk is the sub-matrix of W comprising the
columns related to the kth history word, and the max
is to be understood component-wise. The product
WkRT can then be considered as defining the pro-
jection matrix for the kth position. After the projec-
tion of all the context words, the max function se-
lects, for each dimension l, among the n ? 1 values
([WkRTv?k]l) the most active one, which we also
assume to be the most relevant for the prediction.
2.1.2 Recurrent Layer
Recurrent networks are based on a more complex
architecture designed to recursively handle an arbi-
trary number of context words. Recurrent NNLMs
are described in (Mikolov et al, 2010; Mikolov et
al., 2011b) and are experimentally shown to outper-
form both standard back-off LMs and feed-forward
NNLMs in terms of perplexity on a small task. The
key aspect of this architecture is that the input layer
for predicting the ith word wi in a text contains both
a numeric representation vi?1 of the previous word
and the hidden layer for the previous prediction.
3
The hidden layer thus acts as a representation of the
context history that iteratively accumulates an un-
bounded number of previous words representations.
Our reimplementation of recurrent NNLMs
slightly differs from the feed-forward architecture
mainly by its input part.We use the same deep archi-
tecture to model the relation between the input word
presentations and the input layer as in the recurrent
model. However, we explicitly restrict the context to
the n?1 previous words. Note that this architecture
is just a convenient intermediate model that is used
to efficiently train a recurrent model, as described in
Section 3. In the recurrent model, the input layer is
estimated as a recursive function of both the current
input word and the past input layer.
i = sigm(Wi?1 +RTv?1) (6)
As in the standard model, RTv?k associates each
context word v?k to one feature vector (the corre-
sponding row in R). This vector plays the role of
a bias at subsequent input layers. The input part is
thus structured in a series of layers, the relation be-
tween the input layer and the first previous word be-
ing at level 1, the second previous word is at level 2
and so on. In (Mikolov et al, 2010; Mikolov et al,
2011b), recurrent models make use of the entire con-
text, from the current word position all the way back
to the beginning of the document. This greatly in-
creases the complexity of training, as each document
must be considered as a whole and processed posi-
tion per position. By comparison, our reimplemen-
tation only considers a fixed context length, which
can be increased at will, thus simulating a true recur-
rent architecture; this enables us to take advantage
of several techniques during training that speed up
learning (see Section 3). Furthermore, as discussed
below, our preliminary results show that restricting
the context to the current sentence is sufficient to at-
tain optimal performance 2.
2.2 Structured Output Layer
A major difficulty with the neural network approach
is the complexity of inference and training, which
largely depends on the size of the output vocabu-
2The test sets used in MT experiments are made of various
News extracts. Their content is thus not homogeneous and us-
ing words from previous sentences doesn?t seem to be relevant.
lary ,i.e. of the number of words that have to be pre-
dicted. To overcome this problem, Le et al (2011a)
have proposed the structured Output Layer (SOUL)
architecture. Following (Mnih and Hinton, 2008),
the SOUL model combines the neural network ap-
proach with a class-based LM (Brown et al, 1992).
Structuring the output layer and using word class in-
formation makes the estimation of distribution over
large output vocabulary computationally feasible.
In the SOUL LM, the output vocabulary is struc-
tured in a clustering tree, where every word is asso-
ciated to a unique path from the root node to a leaf
node. Denoting wi the ith word in a sentence, the se-
quence c1:D(wi) = c1, . . . , cD encodes the path for
word wi in this tree, with D the tree depth, cd(wi)
the class or sub-class assigned to wi, and cD(wi) the
leaf associated with wi, comprising just the word it-
self. The probability of wi given its history h can
then be computed as:
P (wi|h) =P (c1(wi)|h)
?
D?
d=2
P (cd(wi)|h, c1:d?1).
(7)
There is a softmax function at each level of the
tree and each word ends up forming its own class
(a leaf). The SOUL architecture is represented in
the right part of Figure 1. The first (class layer)
estimates the class probability P (c1(wi)|h), while
sub-class layers estimate the sub-class probabili-
ties P (cd(wi)|h, c1:d?1), d = 2 . . . (D ? 1). Fi-
nally, the word layer estimates the word probabili-
ties P (cD(wi)|h, c1:D?1). As in (Schwenk, 2007),
words in the short-list remain special, as each of
them represents a (final) class on its own right.
3 Efficiency issues
Training a SOUL model can be achieved by maxi-
mizing the log-likelihood of the parameters on some
training corpus. Following (Bengio et al, 2003),
this optimization is performed by Stochastic Back-
Propagation (SBP). Recurrent models are usually
trained using a variant of SBP called the Back-
Propagation Through Time (BPTT) (Rumelhart et
al., 1986; Mikolov et al, 2011a).
Following (Schwenk, 2007), it is possible to
greatly speed up the training of NNLMs using,
4
for instance, n-gram level resampling and bunch
mode training with parallelization (see below); these
methods can drastically reduce the overall training
time, from weeks to days. Adapting these meth-
ods to recurrent models are not straightforward. The
same goes with the SOUL extension: its training
scheme requires to first consider a restricted output
vocabulary (the shortlist), that is then extended to in-
clude the complete prediction vocabulary (Le et al,
2011b). This technique is too time consuming, in
practice, to be used when training recurrent mod-
els. By bounding the recurrence to a dozen or so
previous words, we obtain a recurrent-like n-gram
model that can benefit from a variety of speed-up
techniques, as explained in the next sections.
Note that the bounded-memory approximation is
only used for training: once training is complete, we
derive a true recurrent network using the parameters
trained on its approximation. This recurrent archi-
tecture is then used for inference.
3.1 Reducing the training data
Our usual approach for training large scale models
is based on n-gram level resampling a subset of the
training data at each epoch. This is not directly com-
patible with the recurrent model, which requires to
iterate over the training data sentence-by-sentence in
the same order as they occur in the document. How-
ever, by restricting the context to sentences, data re-
sampling can be carried out at the sentence level.
This means that the input layer is reinitialized at
the beginning of each sentence so as to ?forget?, as
it were, the memory of the previous sentences. A
similar proposal is made in (Mikolov et al, 2011b),
where the temporal dependencies are limited to the
level of paragraph. Another useful trick, which is
also adopted here, is to use different sampling rates
for the various subparts of the data, thus boosting the
use of in-domain versus out-of-domain data.
3.2 Bunch mode
Bunch mode training processes sentences by batches
of several examples, thus enabling matrix operation
that are performed very efficiently by the existing
BLAS library. After resampling, the training data is
divided into several sentence flows which are pro-
cessed simultaneously. While the number of exam-
ples per batch can be as high as 128 without any
visible loss of performance for n-gram NNLM, we
found, after some preliminary experiments, that the
value of 32 seems to yield a good tradeoff between
the computing time and the performance for recur-
rent models. Using such batches, the training time
can be speeded up by a factor of 8 at the price of a
slight loss (less than 2%) in perplexity.
3.3 SOUL training scheme
The SOUL training scheme integrates several steps
aimed at dealing with the fact that the output vocab-
ulary is split in two sub-parts: very frequent words
are in the so-called short-list and are treated differ-
ently from the less frequent ones. This setting can
not be easily reproduced with recurrent models. By
contrast, using the pseudo-recurrent n-gram NNLM,
the SOUL training scheme can be adopted; the re-
sulting parameter values are then plugged in into a
truly recurrent architecture. In the light of the results
reported below, we content ourselves with values of
n in the range 8-10.
4 Experimental Results
We now turn to the experimental part, starting with a
description of the experimental setup. We will then
present an attempt to quantify the relative impor-
tance of history words, followed by a head to head
comparison of the various NNLM architectures dis-
cussed in the previous sections.
4.1 Experimental setup
The tasks considered in our experiments are derived
from the shared translation track of WMT 2011
(translation from English to French). We only pro-
vide here a short overview of the task; all the neces-
sary details regarding this evaluation campaign are
available on the official Web site3 and our system
is described in (Allauzen et al, 2011). Simply note
that our parallel training data includes a large Web
corpus, referred to as the GigaWord parallel cor-
pus. After various preprocessing and filtering steps,
the total amount of training data is approximately
12 million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
To built the target language models, the mono-
lingual corpus was first split into several sub-parts
3http://www.statmt.org/wmt11
5
based on date and genre information. For each of
these sub-corpora, a standard 4-gram LM was then
estimated with interpolated Kneser-Ney smoothing
(Chen and Goodman, 1996). All models were cre-
ated without any pruning nor cutoff. The baseline
back-off n-gram LM was finally built as a linear
combination of several these models, where the in-
terpolation coefficients are chosen so as to minimize
the perplexity of a development set.
All NNLMs are trained following the prescrip-
tions of Le et al (2011b), and they all share the
same inner structure: the dimension of the projec-
tion word space is 500; the size of two hidden lay-
ers are respectively 1000 and 500; the short-list con-
tains 2000 words; and the non-linearity is introduced
with the sigmoid function. For the recurrent model,
the parameter that limits the back-propagation of er-
rors through time is set to 9 (see (Mikolov et al,
2010) for details). This parameter can be considered
to play a role that is similar to the history size in
our pseudo-recurrent n-gram model: a value of 9 in
the recurrent setting is equivalent to n = 10. All
NNLMs are trained with the following resampling
strategy: 75% of in-domain data (monolingual News
data 2008-2011) and 25% of the other data. At each
epoch, the parameters are updated using approxi-
mately 50 millions words for the last training step
and about 140 millions words for the previous ones.
4.2 The usefulness of remote words
In this section, we analyze the influence of each con-
text word with respect to their distance from the pre-
dicted word and to their POS tag. The quantitative
analysis relies on the variant of the n-gram architec-
ture based on (5) (see Section 2.1), which enables
us to keep track of the most important context word
for each prediction. Throughout this study, we will
consider 10-gram NNLMs.
Figure 2 represents the selection rate with respect
to the word position and displays the percentage of
coordinates in the input layer that are selected for
each position. As expected, close words are the most
important, with the previous word accounting for
more than 35% of the components. Remote words
(at a distance between 7 and 9) have almost the
same, weak, influence, with a selection rate close to
2.5%. This is consistent with the perplexity results
of n-gram NNLMs as a function of n, reported in
Tag Meaning Example
ABR abreviation etc FC FMI
ABK other abreviation ONG BCE CE
ADJ adjective officielles alimentaire mondial
ADV adverb contrairement assez alors
DET article; une les la
possessive pronoun ma ta
INT interjection oui adieu tic-tac
KON conjunction que et comme
NAM proper name Javier Mercure Pauline
NOM noun surprise inflation crise
NUM numeral deux cent premier
PRO pronoun cette il je
PRP preposition; de en dans
preposition plus article au du aux des
PUN punctuation; : , -
punctuation citation ?
SENT sentence tag ? . !
SYM symbol %
VER verb ont fasse parlent
<s> start of sentence
Table 1: List of grouped tags from TreeTagger.
Table 2: the difference between all orders from 4-
gram to 8-gram are significant, while the difference
between 8-gram and 10-gram is negligible.
POS tags were computed using the TreeTag-
ger (Schmid, 1994); sub-types of a main tag are
pooled to reduce the total number of categories. For
example, all the tags for verbs are merged into the
same VER class. Adding the token <s> (sentence
start), our tagset contains 17 tags (see Table 1).
The average selection rates for each tag are shown
in Figure 3: for each category, we display (in bars)
the average number of components that correspond
to a word in that category when this word is in pre-
vious position. Rare tags (INT, ABK , ABR and
SENT) seem to provide a very useful information
and have very high selection rates. Conversely, DET,
PUN and PRP words occur relatively frequently and
belong to the less selective group. The two most
frequent tags (NOM and VER ) have a medium se-
lection rate (approximately 0.5).
4.3 Translation experiments
The integration of NNLMs for large SMT tasks is
far from easy, given the computational cost of com-
puting n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two-pass approach:
the first pass uses a conventional back-off n-gram
model to produce a list of the k most likely trans-
lations; in the second pass, the NNLMs probability
6
1 2 3 4 5 6 7 8 90.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Figure 2: Average selection rate per word position for the
max-based NNLM, computed on newstest2009-2011. On
x axis, the number k represents the kth previous word.
0 5 10 150.0
0.2
0.4
0.6
0.8
1.0
PUN DET SYM PRP NUM KON ADV SENT PRO VER <s> ADJ NOM ABR NAM ABK INT
Figure 3: Average selection rate of max function of the
first previous word in terms of word POS-tag information,
computed on newstest2009-2011. The green line repre-
sents the distribution of occurrences of each tag.
of each hypothesis is computed and the k-best list is
accordingly reordered. The NNLM weights are op-
timized as the other feature weights using Minimum
Error Rate Training (MERT) (Och, 2003). For all
our experiments, we used the value k = 300.
To clarify the impact of the language model or-
der in translation performance, we considered three
different ways to use NNLMs. In the first setting,
the NNLM is used alone and all the scores provided
by the MT system are ignored. In the second set-
ting (replace), the NNLM score replaces the score
of the standard back-off LM. Finally, the score of
the NNLM can be added in the linear combination
(add). In the last two settings, the weights used for
Model Perplexity BLEU
alone replace add
Baseline 90 29.4 31.3 -
4-gram 92 29.8 31.1 31.5
6-gram 82 30.2 31.6 31.8
8-gram 78 30.6 31.6 31.8
10-gram 77 30.5 31.7 31.8
recurrent 81 30.4 31.6 31.8
Table 2: Results for the English to French task obtained
with the baseline system and with various NNLMs. Per-
plexity is computed on newstest2009-2011 while BLEU is
on the test set (newstest2010).
n-best reranking are re-tuned with MERT.
Table 2 summarizes the BLEU scores obtained on
the newstest2010 test set. BLEU improvements are
observed with feed-forward NNLMs using a value
of n = 8 with respect to the baseline (n = 4).
Further increase from 8 to 10 only provides a very
small BLEU improvement. These results strengthen
the assumption made in Section 3.3: there seem to
be very little information in remote words (above
n = 7-8). It is also interesting to see that the 4-gram
NNLM achieves a comparable perplexity to the con-
ventional 4-gram model, yet delivers a small BLEU
increase in the alone condition.
Surprisingly4, on this task, recurrent models seem
to be comparable with 8-gram NNLMs. The rea-
son may be the deep architecture of recurrent model
that makes it hard to be trained in a large scale task.
With the recurrent-like n-gram model described in
Section 2.1.2, it is feasible to train a recurrent model
on a large task. With 10% of perplexity reduction as
compared to a backoff model, its yields comparable
performances as reported in (Mikolov et al, 2011a).
To the best of our knowledge, it is the first recurrent
NNLM trained on a such large dataset (2.5 billion
words) in a reasonable time (about 11 days).
5 Related work
There have been many attempts to increase the
context beyond a couple of history words (see eg.
(Rosenfeld, 2000)), for example: by modeling syn-
4Pers. com. with T. Mikolov: on the ?small? WSJ data
set, the recurrent model described in (Mikolov et al, 2011b)
outperforms the 10-gram NNLM.
7
tactic information, that better reflects the ?distance?
between words (Chelba and Jelinek, 2000; Collins
et al, 2005; Schwartz et al, 2011); with a unigram
model of the whole history (Kuhn and Mori, 1990);
by using trigger models (Lau et al, 1993); or by try-
ing to model document topics (Seymore and Rosen-
feld, 1997). One interesting proposal avoids the n-
gram assumption by estimating the probability of a
sentence (Rosenfeld et al, 2001). This approach
relies on a maximum entropy model which incor-
porates arbitrary features. No significant improve-
ments were however observed with this model, a fact
that can be attributed to two main causes: first, the
partition function can not be computed exactly as it
involves a sum over all the possible sentences; sec-
ond, it seems that data sparsity issues for this model
are also adversely affecting the performance.
The recurrent network architecture for LMs was
proposed in (Mikolov et al, 2010) and then ex-
tended in (Mikolov et al, 2011b). The authors pro-
pose a hierarchical architecture similar to the SOUL
model, based however on a simple unigram clus-
tering. For large scale tasks (? 400M training
words), advanced training strategies were investi-
gated in (Mikolov et al, 2011a). Instead of resam-
pling, the data was divided into paragraphs, filtered
and then sorted: the most in-domain data was thus
placed at the end of each epoch. On the other hand,
the hidden layer size was decreased by simulating a
maximum entropy model using a hash function on
n-grams. This part represents direct connections be-
tween input and output layers. By sharing the pre-
diction task, the work of the hidden layer is made
simpler, and can thus be handled with a smaller
number of hidden units. This approach reintroduces
into the model discrete features which are somehow
one main weakness of conventional backoff LMs as
compared to NNLMs. In fact, this strategy can be
viewed as an effort to directly combine the two ap-
proaches (backoff-model and neural network), in-
stead of using a traditional way, through interpola-
tion. Training simultaneously two different models
is computationally very demanding for large vocab-
ularies, even with help of hashing technique; in com-
parison, our approach keeps the model architecture
simple, making it possible to use the efficient tech-
niques developed for n-gram NNLMs.
The use the max, rather than a sum, on the hid-
den layer of neural network is not new. Within the
context of language modeling, it was first proposed
in (Collobert et al, 2011) with the goal to model a
variable number of input features. Our motivation
for using this variant was different, and was mostly
aimed at analyzing the influence of context words
based on the selection rates of this function.
6 Conclusion
In this paper, we have investigated several types
of NNLMs, along with conventional LMs, in or-
der to assess the influence of long range dependen-
cies within sentences in the language modeling task:
from recurrent models that can recursively handle
an arbitrary number of context words to n-gram
NNLMs with n varying between 4 and 10. Our con-
tribution is two-fold.
First, experimental results showed that the influ-
ence of word further than 9 can be neglected for the
statistical machine translation task 5. Therefore, the
n-gram assumption with n ? 10 appears to be well-
founded to handle most sentence internal dependen-
cies. Another interesting conclusion of this study
is that the main issue of the conventional n-gram
model is not its conditional independence assump-
tions, but the use of too small values for n.
Second, by restricting the context of recurrent net-
works, the model can benefit of the advanced train-
ing schemes and its training time can be divided by
a factor 8 without loss on the performances. To the
best of our knowledge, it is the first time that a re-
current NNLM is trained on a such large dataset in
a reasonable time. Finally, we compared these mod-
els within a large scale MT task, with monolingual
data that contains 2.5 billion words. Experimental
results showed that using long range dependencies
(n = 10) with a SOUL language model significantly
outperforms conventional LMs. In this setting, the
use of a recurrent architecture does not yield any im-
provements, both in terms of perplexity and BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, the French State agency
for innovation.
5The same trend is observed in speech recognition.
8
References
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland.
Y Bengio, R Ducharme, P Vincent, and C Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3(6):1137?1155.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 507?514, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing: deep
neural networks with multitask learning. In Proc.
of ICML?08, pages 160?167, New York, NY, USA.
ACM.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Ahmad Emami, Imed Zitouni, and Lidia Mangu. 2008.
Rich morphology based n-gram language models for
arabic. In INTERSPEECH, pages 829?832.
R. Kuhn and R. De Mori. 1990. A cache-based natural
language model for speech recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
12(6):570?583, june.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for arabic speech recognition. In Proc. ICASSP
2010.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modeling using the maxi-
mum entropy principle. In Proc HLT?93, pages 108?
113, Princeton, New Jersey.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011a. Structured out-
put layer neural network language model. In Proceed-
ings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Ilya Oparin, Abdel. Messaoudi, Alexan-
dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011b. Large vocabulary SOUL neural network lan-
guage models. In Proceedings of InterSpeech 2011.
Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2011. Improving lvcsr system combination using neu-
ral network language model cross adaptation. In IN-
TERSPEECH, pages 2857?2860.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceedings
of the 11th Annual Conference of the International
Speech Communication Association (INTERSPEECH
2010), volume 2010, pages 1045?1048. International
Speech Communication Association.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for train-
ing large scale neural network language models. In
Proceedings of ASRU 2011, pages 196?201. IEEE Sig-
nal Processing Society.
Toma?s? Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model. In
Proc. of ICASSP?11, pages 5528?5531.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language models:
A vehicle for linguistic-statistical integration. Com-
puters, Speech and Language, 15:2001.
R. Rosenfeld. 2000. Two decades of statistical language
modeling: Where do we go from here ? Proceedings
of the IEEE, 88(8).
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986.
Parallel distributed processing: explorations in the mi-
crostructure of cognition, vol. 1. chapter Learning
9
internal representations by error propagation, pages
318?362. MIT Press, Cambridge, MA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Lane Schwartz, Chris Callison-Burch, William Schuler,
and Stephen Wu. 2011. Incremental syntactic lan-
guage models for phrase-based translation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 620?631, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proc. ICASSP, pages 765?
768, Orlando, FL.
H. Schwenk and P. Koehn. 2008. Large and diverse lan-
guage models for statistical machine translation. In
International Joint Conference on Natural Language
Processing, pages 661?666, Janv 2008.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518.
Kristie Seymore and Ronald Rosenfeld. 1997. Using
story topics for language model adaptation. In Proc. of
Eurospeech ?97, pages 1987?1990, Rhodes, Greece.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
10
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 157?162,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Non-Linear Models for Confidence Estimation
Yong Zhuang?
Zhejiang University
866 Yuhangtang Road
Hangzhou, China
yong.zhuang22@gmail.com
Guillaume Wisniewski and Franc?ois Yvon
Univ. Paris Sud and LIMSI?CNRS
rue John von Neumann
91403 Orsay CEDEX, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes our work with the data
distributed for the WMT?12 Confidence Es-
timation shared task. Our contribution is
twofold: i) we first present an analysis of
the data which highlights the difficulty of
the task and motivates our approach; ii) we
show that using non-linear models, namely ran-
dom forests, with a simple and limited feature
set, succeeds in modeling the complex deci-
sions required to assess translation quality and
achieves results that are on a par with the sec-
ond best results of the shared task.
1 Introduction
Confidence estimation is the task of predicting the
quality of a system prediction without knowledge
of the expected output. It is an important step
in many Natural Language Processing applications
(Gandrabur et al, 2006). In Machine Translation
(MT), this task has recently gained interest (Blatz
et al, 2004; Specia et al, 2010b; Soricut and Echi-
habi, 2010; Bach et al, 2011). Indeed, professional
translators are more and more requested to post-edit
the outputs of a MT system rather than to produce
a translation from scratch. Knowing in advance the
segments they should focus on would be very help-
ful (Specia et al, 2010a). Confidence estimation is
also of great interest for developers of MT system, as
it provides them with a way to analyze the systems
output and to better understand the main causes of
errors.
Even if several studies have tackled the problem
of confidence estimation in machine translation, un-
til now, very few datasets were publicly available and
comparing the proposed methods was difficult, if not
impossible. To address this issue, WMT?12 orga-
nizers proposed a shared task aiming at predict the
?This work was conducted during an internship at LIMSI?
CNRS
quality of a translation and provided the associated
datasets, baselines and metrics.
This paper describes our work with the data of the
WMT?12 Confidence Estimation shared task. Our
contribution is twofold: i) we first present an analysis
of the provided data that will stress the difficulty of
the task and motivate the choice of our approach; ii)
we show how using non-linear models, namely ran-
dom forests, with a simple and limited features set
succeed in modeling the complex decisions require
to assess translation quality and achieve the second
best results of the shared task.
The rest of this paper is organized as follows: Sec-
tion 2 summarizes our analysis of the data; in Sec-
tion 3, we describe our learning method; our main
results are finally reported in Section 4.
2 Data Analysis
In this section, we quickly analyze the data dis-
tributed in the context of the WMT?12 Confidence
Estimation Shared Task in order to evaluate the diffi-
culty of the task and to find out what predictors shall
be used. We will first describe the datasets, then the
features usually considered in confidence estimation
tasks and finally summarize our analyses.
2.1 Datasets
The datasets used in our experiments were released
for the WMT?12 Quality Estimation Task. All the
data provided in this shared task are based on the
test set of WMT?09 and WMT?10 translation tasks.
The training set is made of 1, 832 English sen-
tences and their Spanish translations as computed by
a standard Moses system. Each sentence pair is ac-
companied by an estimate of its translation quality.
This score is the average of ordinal grades assigned
by three human evaluators. The human grades are in
the range 1 to 5, the latter standing for a very good
translation that hardly requires post-editing, while
the former stands for a bad translation that does
157
not deserve to be edited, meaning that the machine
output useless and that translation should better be
produced from scratch. The test contains 422 sen-
tence pairs, the quality of which has to be predicted.
The training set alo contains additional material,
namely two references (the reference originally given
by WMT and a human post-edited one), which will
allow us to better interpret our results. No references
were provided for the test set.
2.2 Features
Several works have studied the problem of confidence
estimation (Blatz et al, 2004; Specia et al, 2010b) or
related problems such as predicting readability (Ka-
nungo and Orr, 2009) or developing automated essay
scoring systems (Burstein et al, 1998). They all use
the same basic features:
IBM 1 score measures the quality of the ?associa-
tion? of the source and the target sentence using
bag-of-word translation models;
Language model score accounts for the ?flu-
ency?, ?grammaticality? and ?plausibility? of a
target sentence;
Simple surface features like the sentence length,
the number of out-of-vocabulary words or words
that are not aligned. These features are used to
account for the difficulty of the translation task.
More elaborated features, derived, for instance,
from parse trees or dependencies analysis have also
been used in past studies. However they are far more
expensive to compute and rely on the existence of ex-
ternal resources, which may be problematic for some
languages. That is why we only considered a re-
stricted number of basic features in this work1. An-
other reason for considering such a small set of fea-
tures is the relatively small size of the training set: in
our preliminary experiments, considering more fea-
tures, especially lexicalized features that would be of
great interest for failure analysis, always resulted in
overfitting.
2.3 Data Analysis
The distribution of the human scores on the training
set is displayed in Figure 1. Surprisingly enough,
the baseline translation system used to generate the
data seems to be pretty good: 73% of the sentences
have a score higher than 3 on a 1 to 5 scale. It
also appears that most scores are very close: more
than half of them are located around the mean. As
a consequence, it seems that distinguishing between
them will require to model subtle nuances.
1The complete list of features is given in Appendix A.
Figure 1: Distribution of the human scores on the train
set. (HS? stands for Human Scores)
Figure 2 plots the distribution of quality scores
as a function of the Spanish-to-English IBM 1 score
and of the probability of the target sentence. These
two scores were computed with the same models that
were used to train the MT systems that have gener-
ated the training data. It appears that even if the
examples are clustered by their quality, these clusters
overlap and the frontiers between them are fuzzy and
complex. Similar observations were made for others
features.
Figure 2: Quality scores as a function of the Spanish-to-
English IBM 1 score and of the probability of the target
sentence (HS? stands for Human Scores)
These observations prove that a predictor of the
translation quality has to capture complex interac-
tion patterns in the training data. Standard results
from machine learning show that such structures can
be described either by a linear model using a large
number of features or by a non-linear model using a
158
(potentially) smaller set of features. As only a small
number of training examples is available, we decided
to focus on non-linear models in this work.
3 Inferring quality scores
Predicting the quality scores can naturally be cast
as a standard regression task, as the reference scores
used in the evaluation are numerical (real) values.
Regression is the approach adopted in most works
on confidence estimation for MT (Albrecht and Hwa,
2007; Specia et al, 2010b). A simpler way to tackle
the problem would be to recast it as binary classi-
fication task aiming at distinguishing ?good? trans-
lations from ?bad? ones (Blatz et al, 2004; Quirk,
2004). It is also possible, as shown by (Soricut and
Echihabi, 2010), to use ranking approaches. How-
ever, because the shared task is evaluated by com-
paring the actual value of the predictions with the
human scores, using these last two frameworks is not
possible.
In our experiments, following the observations re-
ported in the previous section, we use two well-
known non-linear regression methods: polynomial
regression and random forests. We also consider lin-
ear regression as a baseline. We will now quickly
describe these three methods.
Linear regression (Hastie et al, 2003) is a simple
model in which the prediction is defined by a linear
combination of the feature vector x: y? = ?0 + x>?,
where ?0 and ? are the parameters to estimate.
These parameters are usually learned by minimiz-
ing the sum of squared deviations on the training
set, which is an easy optimization problem with a
close-form solution.
Polynomial regression (Hastie et al, 2003) is a
straightforward generalization of linear regression in
which the relationship between the features and the
label is modeled as a n-th order polynomial. By care-
fully extending the feature vector, the model can be
reduced to a linear regression model and trained in
the same way.
Random forest regressor (Breiman, 2001) is an en-
semble method that learns many regression trees and
predicts an aggregation of their result. In contrast
with standard decision tree, in which each node is
split using the best split among all features, in a ran-
dom forest the split is chosen randomly. In spite of
this simple and counter-intuitive learning strategy,
random forests have proven to be very good ?out-
of-the-box? learners and have achieved state-of-the-
art performance in many tasks, demonstrating both
their robustness to overfitting and their ability to
take into account complex interactions between fea-
tures.
In our experiments, we use the implementation
provided by scikit-learn (Pedregosa et al, 2011).
Hyper-parameters of the random forest (the num-
ber of trees and the stopping criterion) were chosen
by 10-fold cross-validation.
4 Experimental Setting
4.1 Features
In all our experiments, we considered a simple de-
scription of the translation hypotheses relying on
31 features. The complete list of features is given
in Appendix A. All these features have already been
used in works related to ours and are simple fea-
tures that can be easily computed using only a lim-
ited number of external resources.
A key finding in our preliminary experiments is
the need to re-scale the features by dividing their
value by the length of the corresponding sentence
(e.g. the language model score of a source sentence
will be divided by its length of the source sentence,
and the one of a target sentence will be done by its
length of the target sentence). This rescaling makes
features that depend on the sentence length (like the
LM score) comparable and results in a large improve-
ment of the performance of the associated feature.
4.2 Metrics
The two metrics used to evaluate prediction perfor-
mance are the standard metrics for regression: Mean
Absolute Error (MAE) and Root Mean Squared Er-
ror (RMSE) defined by:
MAE =
1
n
n?
i=1
|y?i ? yi|
RMSE =
?
?
?
? 1
n
n?
i=1
(y?i ? yi)
2
where n is the number of examples, yi and y?i the true
label and predicted label of the ith example. MAE
can be understood as the averaged error made in
predicting the quality of a translation. As it is easy
to interpret, we will use it to analyze our results.
RMSE scores are reported to facilitate comparison
with other submissions to the shared task.
All the reported scores have been computed using
the tools provided by the Quality Estimation task
organizers2.
2https://github.com/lspecia/QualityEstimation
159
4.3 Results
Table 1 details the results achieved by the different
methods introduced in the previous section. All of
them achieve similar performances: their MAE is be-
tween 0.64 and 0.66, which is a pretty good result as
the best reported MAE in the shared task is 0.61.
Our best model is the second-best when submissions
are ranked according to their MAE.
Even if their results are very close (significance of
the score differences will be investigated in the fol-
lowing subsection), all non-linear models outperform
a simple linear regression, which corroborates the ob-
servations made in Section 2.
For the polynomial regression, we tried different
polynomial orders in order to achieve an optimal
setting. Even if this method achieves the best re-
sults when the model is selected on the test set, it is
not usable in practice: when we tried to select the
polynomial degree by cross-validation, the regressors
systematically overfitted due to the reduction of the
number of examples. That is why random forests,
which do not suffer from overfitting and can learn
good predictor even when features outnumber exam-
ples, is our method of choice.
4.4 Interpretation
To get a better understanding of the task difficulty
and to make interpretation of the error rate easier,
we train another regressor using an ?oracle? feature:
the hTER score. It is clear that this feature can only
be computed on the training set and that considering
it does not make much sense in a ?real-life? scenario.
However, this feature is supposed to be highly rele-
vant to the quality prediction task and should there-
fore result in a ?large? reduction of the error rates.
Quantifying what ?large? means in this context will
allow us to analyze the results presented in Table 1.
Training a random forest with this additional fea-
ture on 1, 400 examples of the train set chosen ran-
domly reduces the MAE evaluated on the 432 re-
maining examples by 0.10 and the RMSE by 0.12.
This small reduction stresses how difficult the task
is. Comparatively, the 0.02 reduction achieved by
replacing a linear model with a non-linear model
should therefore be considered noteworthy. Further
investigations are required to find out whether the
difficulty of the task results from the way human
scores are collected (low inter-annotators agreement,
bias in the gathering of the collection, ...) or from
the impossibility to solve the task using only surface
features.
Another important question in the analysis of our
results concerns the usability of our approach: an
error of 0.6 seems large on a 1 to 5 scale and may
question the interest of our approach. To allow a fine-
grained analysis, we report the correlation between
the predicted score and the human score (Figure 3)
and the distribution of the absolute error (Figure 4).
These figures show that the actual error is often quite
small: for more than 45% of the examples, the error
is smaller than 0.5 and for 23% it is smaller than 0.2.
Figure 3 also shows that the correlation between our
predictions and the true labels is ?substantial? ac-
cording to the established guidelines of (Landis and
Koch, 1977) (the Pearson correlation coefficient is
greater than 0.6). The difference between the mean
of the two distributions is however quite large. Cen-
tering the predictions on the mean of the true label
may improves the MAE. This observation also sug-
gests that we should try to design evaluation metrics
that do not rely on the actual predicted values.
Figure 3: Correlation between our predictions and the
true label (HS? stands for Human Scores)
5 Conclusion
In this work, we have presented, a simple, yet effi-
cient, method to predict the quality of a translation.
Using simple features and a non-linear model, our
approach has achieved results close to the best sub-
mission to the Confidence Estimation shared task,
which supports the results of our analysis of the data.
In our future work, we aim at considering more fea-
tures, avoiding overfitting thanks to features selec-
tion methods.
Even if a fine-grained analysis of our results shows
the interest and usefulness of our approach, more re-
mains to be done to develop reliable confidence esti-
mation methods. Our results also highlight the need
to continue gathering high-quality resources to train
and investigate confidence estimation systems: even
when considering only very few features, our systems
160
Train Test
Methods parameters MAE RMSE MAE RMSE
linear regression ? 0.58 0.71 0.66 0.82
polynomial regression
n=2 0.55 0.68 0.64 0.79
n=3 0.54 0.67 0.64 0.79
n=4 0.54 0.67 0.65 0.85
random forest cross-validated 0.39 0.46 0.64 0.80
Table 1: Prediction performance achieved by different regressors
Figure 4: Distribution of the absolute error (|yi ? y?i|) of
our predictions
were prone to overfitting. Developing more elabo-
rated systems will therefore only be possible if more
training resource is available.
Acknowledgment
The authors would like to thank Nicolas Usunier for
helpful discussions about ranking and regression us-
ing random forest. This work was partially funded by
the French National Research Agency under project
ANR-CONTINT-TRACE.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 296?303,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: a method for measuring machine transla-
tion confidence. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, HLT
?11, pages 211?219, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Mar-
tin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. In Proceedings of the
17th international conference on Computational lin-
guistics - Volume 1, COLING ?98, pages 206?210,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Simona Gandrabur, George Foster, and Guy Lapalme.
2006. Confidence estimation for nlp applications.
ACM Trans. Speech Lang. Process., 3(3):1?29, Octo-
ber.
T. Hastie, R. Tibshirani, and J. H. Friedman. 2003. The
Elements of Statistical Learning. Springer, July.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proceedings
of the Second ACM International Conference on Web
Search and Data Mining, WSDM ?09, pages 202?211,
New York, NY, USA. ACM.
R. J. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011. Scikit-learn: Machine Learning in
Python . Journal of Machine Learning Research,
12:2825?2830.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence metric. In Proceedings of the
4th International Conference on Language Resources
and Evaluation (LREC), pages 825?828.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 612?621, Uppsala, Sweden, July. Association for
Computational Linguistics.
161
Lucia Specia, Nicola Cancedda, and Marc Dymetman.
2010a. A dataset for assessing machine translation
evaluation metrics. In 7th Conference on Interna-
tional Language Resources and Evaluation (LREC-
2010), pages 3375?3378, Valletta, Malta.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010b. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50, March.
A Features List
Here is the whole list of the 31 features we used in
our experiments (? has been used in the baseline of
the shared task organizer):
? ? Number of tokens in the source sentence
? ? Number of tokens in the target sentence
? ? Average token length in source sentence
? English-Spanish IBM 1 scores
? Spanish-English IBM 1 scores
? English-Spanish IBM 1 scores divided by the
length of source sentence
? English-Spanish IBM 1 scores divided by the
length of target sentence
? Spanish-English IBM 1 scores divided by the
length of source sentence
? Spanish-English IBM 1 scores divided by the
length of target sentence
? Number of out-of-vocabulary in source sentence
? Number of out-of-vocabulary in target sentence
? Out-of-vocabulary rates in source sentence
? Out-of-vocabulary rates in target sentence
? log10(LM probability of source sentence)
? log10(LM probability of target sentence)
? log10(LM probability of source sentence) divided
by the length of source sentence
? log10(LM probability of target sentence) divided
by the length of target sentence
? Ratio of functions words in source sentence
? Ratio of functions words in target sentence
? ? Number of occurrences of the target word
within the target hypothesis (averaged for all
words in the hypothesis - type/token ratio)
? ? Average number of translations per source
word in the sentence (as given by IBM 1 table
thresholded so that prob(t|s) > 0.2)
? ? Average number of translations per source
word in the sentence (as given by IBM 1 table
thresholded so that prob(t|s) > 0.01) weighted
by the inverse frequency of each word in the
source corpus
? ? Percentage of unigrams in quartile 1 of fre-
quency (lower frequency words) in a corpus of
the source language (SMT training corpus)
? ? Percentage of unigrams in quartile 4 of fre-
quency (higher frequency words) in a corpus of
the source sentence
? ? Percentage of bigrams in quartile 1 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of bigrams in quartile 4 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of trigrams in quartile 1 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of trigrams in quartile 4 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of unigrams in the source sentence
seen in a corpus (SMT training corpus)
? ? Number of punctuation marks in the source
sentence
? ? Number of punctuation marks in the target
sentence
162
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIMSI @ WMT?12
Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,
Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,
Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submissions to
the shared translation task. We report results
for French-English and German-English in
both directions. Our submissions use n-code,
an open source system based on bilingual
n-grams. In this approach, both the transla-
tion and target language models are estimated
as conventional smoothed n-gram models; an
approach we extend here by estimating the
translation probabilities in a continuous space
using neural networks. Experimental results
show a significant and consistent BLEU im-
provement of approximately 1 point for all
conditions. We also report preliminary experi-
ments using an ?on-the-fly? translation model.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Seventh Workshop
on Statistical Machine Translation. LIMSI partic-
ipated in the French-English and German-English
tasks in both directions. For this evaluation, we
used n-code, an open source in-house Statistical
Machine Translation (SMT) system based on bilin-
gual n-grams1. The main novelty of this year?s
participation is the use, in a large scale system, of
the continuous space translation models described
in (Hai-Son et al, 2012). These models estimate the
n-gram probabilities of bilingual translation units
using neural networks. We also investigate an alter-
native approach where the translation probabilities
of a phrase based system are estimated ?on-the-fly?
1http://ncode.limsi.fr/
by sampling relevant examples, instead of consider-
ing the entire training set. Finally we also describe
the use in a rescoring step of several additional fea-
tures based on IBM1 models and word sense disam-
biguation information.
The rest of this paper is organized as follows. Sec-
tion 2 provides an overview of the baseline systems
built with n-code, including the standard transla-
tion model (TM). The continuous space translation
models are then described in Section 3. As in our
previous participations, several steps of data pre-
processing, cleaning and filtering are applied, and
their improvement took a non-negligible part of our
work. These steps are summarized in Section 5.
The last two sections report experimental results ob-
tained with the ?on-the-fly? system in Section 6 and
with n-code in Section 7.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o et al,
2006; Crego and Marin?o, 2006). In this framework,
translation is divided in two steps: a source reorder-
ing step and a (monotonic) translation step. Source
reordering is based on a set of learned rewrite rules
that non-deterministically reorder the input words.
Applying these rules result in a finite-state graph of
possible source reorderings, which is then searched
for the best possible candidate translation.
2.1 Features
Given a source sentence s of I words, the best trans-
lation hypothesis t? is defined as the sequence of J
words that maximizes a linear combination of fea-
330
ture functions:
t? = argmax
t,a
{
M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature func-
tion hm and a denotes an alignment between source
and target phrases. Among the feature functions, the
peculiar form of the translation model constitute one
of the main difference between the n-gram approach
and standard phrase-based systems. This will be fur-
ther detailled in section 2.2 and 3.
In addition to the translation model, fourteen
feature functions are combined: a target-language
model (Section 5.3); four lexicon models; six lexi-
calized reordering models (Tillmann, 2004; Crego
et al, 2011) aiming at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two lexi-
cal weights are estimated from the automatic word
alignments. The weights vector ? is learned us-
ing a discriminative training framework (Och, 2003)
(Minimum Error Rate Training (MERT)) using the
newstest2009 as development set and BLEU (Pap-
ineni et al, 2002) as the optimization criteria.
2.2 Standard n-gram translation models
n-gram translation models rely on a specific de-
composition of the joint probability of a sentence
pair P (s, t): a sentence pair is assumed to be
decomposed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL2. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing obtained by ?unfolding? initial word alignments.
In this framework, the basic translation units are
tuples, which are the analogous of phrase pairs and
represent a matching u = (s, t) between a source
s and a target t phrase (see Figure 1). Using the
n-gram assumption, the joint probability of a seg-
2From now on, (s, t) thus denotes an aligned sentence pair,
and we omit the alignment variable a in further developments.
mented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
During the training phase (Marin?o et al, 2006), tu-
ples are extracted from a word-aligned corpus (us-
ing MGIZA++3 with default settings) in such a
way that a unique segmentation of the bilingual
corpus is achieved. A baseline n-gram translation
model is then estimated over a training corpus com-
posed of tuple sequences using modified Knesser-
Ney Smoothing (Chen and Goodman, 1998).
2.3 Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, only those reorder-
ing hypotheses are translated and they are intro-
duced using a set of reordering rules automatically
learned from the word alignments.
In the example in Figure 1, the rule [prix no-
bel de la paix ; nobel de la paix prix] repro-
duces the invertion of the French words that is ob-
served when translating from French into English.
Typically, part-of-speech (POS) information is used
to increase the generalization power of these rules.
Hence, rewrite rules are built using POS rather than
surface word forms (Crego and Marin?o, 2006).
3 SOUL translation models
A first issue with the model described by equa-
tion (2) is that the elementary units are bilingual
pairs. As a consequence, the underlying vocabulary,
hence the number of parameters, can be quite large,
even for small translation tasks. Due to data sparsity
issues, such model are bound to face severe estima-
tion problems. Another problem with (2) is that the
source and target sides play symmetric roles: yet,
in decoding, the source side is known and only the
target side must be predicted.
3.1 A word factored translation model
To overcome these issues, the n-gram probability in
equation (2) can be factored by decomposing tuples
3http://www.kyloo.net/software/doku.php
331
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
in two parts (source and target), and by taking words
as the basic units of the n-gram TM. This may seem
to be a regression with respect to current state-of-
the-art SMT systems, as the shift from the word-
based model of (Brown et al, 1993) to the phrase-
based models of (Zens et al, 2002) is usually con-
sidered as a major breakthrough of the recent years.
Indeed, one important motivation for considering
phrases was to capture local context in translation
and reordering. It should however be emphasized
that the decomposition of phrases into words is only
re-introduced here as a way to mitigate the param-
eter estimation problems. Translation units are still
pairs of phrases, derived from a bilingual segmen-
tation in tuples synchronizing the source and target
n-gram streams. In fact, the estimation policy de-
scribed in section 4 will actually allow us to take into
account larger contexts than is possible with conven-
tional n-gram models.
Let ski denote the k
th word of source tuple si.
Considering the example of Figure 1, s111 denotes
the source word nobel, s411 the source word paix.
We finally denote hn?1(tki ) the sequence made of
the n? 1 words preceding tki in the target sentence:
in Figure 1, h3(t211) thus refers to the three words
context receive the nobel associated with t211 peace.
Using these notations, equation (2) is rewritten as:
P (a, s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (3)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words from ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is:
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
A benefit of this new formulation is that the vo-
cabularies involved only contain words, and are thus
much smaller that tuple vocabularies. These models
are thus less at risk to be plagued by data sparsity is-
sues. Moreover, the decomposition (3) now involves
two models: the first term represents a TM, the sec-
ond term is best viewed as a reordering model. In
this formulation, the TM only predicts the target
phrase, given its source and target contexts.
P (s, t) =
L?
i=1
[ |si|?
k=1
P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
?
|ti|?
k=1
P
(
tki |h
n?1(s1i ), h
n?1(tki )
)] (4)
4 The principles of SOUL
In section 3.1, we defined a n-gram translation
model based on equations (3) and (4). A major diffi-
culty with such models is to reliably estimate their
parameters, the numbers of which grow exponen-
tially with the order of the model. This problem
is aggravated in natural language processing due to
332
the well-known data sparsity issue. In this work,
we take advantage of the recent proposal of (Le et
al., 2011). Using a specific neural network architec-
ture (the Structured OUtput Layer or SOUL model),
it becomes possible to handle large vocabulary lan-
guage modeling tasks. This approach was experi-
mented last year for target language models only and
is now extended to translation models. More details
about the SOUL architecture can be found in (Le et
al., 2011), while its extension to translation models
is more precisely described in (Hai-Son et al, 2012).
The integration of SOUL models for large SMT
tasks is carried out using a two-pass approach: the
first pass uses conventional back-off n-gram trans-
lation and language models to produce a k-best list
(the k most likely translations); in the second pass,
the probability of a m-gram SOUL model is com-
puted for each hypothesis and the k-best list is ac-
cordingly reordered. In all the following experi-
ments, we used a context size for SOUL of m = 10,
and used k = 300. The two decompositions of equa-
tions (3) and (4) are used by introducing 4 scores
during the rescoring step.
5 Corpora and data pre-processing
Concerning data pre-processing, we started from our
submissions from last year (Allauzen et al, 2011)
and mainly upgraded the corpora and the associated
language-dependent pre-processing routines.
5.1 Pre-processing
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte et
al., 2008). Previous experiments have demonstrated
that better normalization tools provide better BLEU
scores: all systems are thus built in ?true-case?.
Compared to last year, the pre-processing of utf-8
characters was significantly improved.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to unknown
forms). When translating from German into En-
glish, the German side is thus normalized using a
specific pre-processing scheme (described in (Al-
lauzen et al, 2010; Durgar El-Kahlout and Yvon,
2010)), which aims at reducing the lexical redun-
dancy by (i) normalizing the orthography, (ii) neu-
tralizing most inflections and (iii) splitting complex
compounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition, for
German, fine-grained POS labels were also needed
for pre-processing and were obtained using the RF-
Tagger (Schmid and Laws, 2008).
5.2 Bilingual corpora
As for last year?s evaluation, we used all the avail-
able parallel data for the German-English language
pair, while only a subpart of the French-English par-
allel data was selected. Word alignment models
were trained using all the data, whereas the transla-
tion models were estimated on a subpart of the par-
allel data: the UN corpus was discarded for this step
and about half of the French-English Giga corpus
was filtered based on a perplexity criterion as in (Al-
lauzen et al, 2011)).
For French-English, we mainly upgraded the
training material from last year by extracting the
new parts from the common data. The word
alignment models trained last year were then up-
dated by running a forced alignment 4 of the new
data. These new word-aligned data was added to
last year?s parallel corpus and constitute the train-
ing material for the translation models and feature
functions described in Section 2. Given the large
amount of available data, three different bilingual
n-gram models are estimated, one for each source of
data: News-Commentary, Europarl, and the French-
English Giga corpus. These models are then added
to the weighted mixture defined by equation (1). For
German-English, we simply used all the available
parallel data to train one single translation models.
5.3 Monolingual corpora and language models
For the monolingual training data, we also used the
same setup as last year. For German, all the train-
ing data allowed in the constrained task were di-
vided into several sets based on dates or genres:
News-Commentary, the news crawled from the Web
grouped by year, and Europarl. For each subset,
a standard 4-gram LM was estimated using inter-
polated Kneser-Ney smoothing (Kneser and Ney,
4The forced alignment step consists in an additional EM it-
eration.
333
1995; Chen and Goodman, 1998). The resulting
LMs are then linearly combined using interpolation
coefficients chosen so as to minimize the perplexity
of the development set. The German vocabulary is
created using all the words contained in the parallel
data and expanded to reach a total of 500k words by
including the most frequent words observed in the
monolingual News data for 2011.
For French and English, the same monolingual
corpora as last year were used5. We did not observe
any perplexity decrease in our attempts to include
the new data specifically provided for this year?s
evaluation. We therefore used the same language
models as in (Allauzen et al, 2011).
6 ?On-the-fly? system
We also developped an alternative approach imple-
menting ?on-the-fly? estimation of the parameter of
a standard phase-based model, using Moses (Koehn
et al, 2007) as the decoder. Implementing on-the-
fly estimation for n-code, while possible in the-
ory, is less appealing due to the computational cost
of estimating a smoothed language model. Given
an input source file, it is possible to compute only
those statistics which are required to translate the
phrases it contains. As in previous works on on-
the-fly model estimation for SMT (Callison-Burch
et al, 2005; Lopez, 2008), we compute a suffix
array for the source corpus. This further enables
to consider only a subset of translation examples,
which we select by deterministic random sampling,
meaning that the sample is chosen randomly with
respect to the full corpus but that the same sample
is always returned for a given value of sample size,
hereafter denoted N . In our experiments, we used
N = 1, 000 and computed from the sample and the
word alignments (we used the same tokenization and
word alignments as in all other submitted systems)
the same translation6 and lexical reordering models
as the standard training scripts of the Moses system.
Experiments were run on the data sets used for
WMT English-French machine translation evalua-
tion tasks, using the same corpora and optimization
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
6An approximation is used for p(f |e), and coherent transla-
tion estimation is used; see (Lopez, 2008).
procedure as in our other experiments. The only no-
table difference is our use of the Moses decoder in-
stead of the n-gram-based system. As shown in Ta-
ble 1, our on-the-fly system achieves a result (31.7
BLEU point) that is slightly worst than the n-code
baseline (32.0) and slightly better than the equiva-
lent Moses baseline (31.5), but does it much faster.
Model estimation for the test file is reduced to 2
hours and 50 minutes, with an additional overhead
for loading and writing files of one and a half hours,
compared to roughly 210 hours for our baseline sys-
tems under comparable hardware conditions.
7 Experimental results
7.1 n-code with SOUL
Table 1 summarizes the experimental results sub-
mitted to the shared translation for French-English
and German-English in both directions. The perfor-
mances are measured in terms of BLEU on new-
stest2011, last year?s test set, and this year?s test
set newstest2012. For the former, BLEU scores are
computed with the NIST script mteva-v13.pl, while
we provide for newstest2012 the results computed
by the organizers 7. The Baseline results are ob-
tained with standard n-gram models estimated with
back-off, both for the bilingual and monolingual tar-
get models. With standard n-gram estimates, the or-
der is limited to n = 4. For instance, the n-code
French-English baseline achieves a 0.5 BLEU point
improvement over a Moses system trained with the
same data setup in both directions.
From Table 1, it can be observed that adding
the SOUL models (translation models and target
language model) consistently improves the base-
line, with an increase of 1 BLEU point. Con-
trastive experiments show that the SOUL target LM
does not bring significant gain when added to the
SOUL translation models. For instance, a gain of
0.3 BLEU point is observed when translating from
French to English with the addition of the SOUL tar-
get LM. In the other translation directions, the differ-
ences are negligible.
7All results come from the official website: http://
matrix.statmt.org/matrix/.
334
Direction System BLEU
test2011 test2012?
en2fr Baseline 32.0 28.9
+ SOUL TM 33.4 29.9
on-the-fly 31.7 28.6
fr2en Baseline 30.2 30.4
+ SOUL TM 31.1 31.5
en2de Baseline 15.4 16.0
+ SOUL TM 16.6 17.0
de2en Baseline 21.8 22.9
+ SOUL TM 22.8 23.9
Table 1: Experimental results in terms of BLEU scores
measured on the newstest2011 and newstest2012. For
newstest2012, the scores are provided by the organizers.
7.2 Experiments with additional features
For this year?s evaluation, we also investigated sev-
eral additional features based on IBM1 models and
word sense disambiguation (WSD) information in
rescoring. As for the SOUL models, these features
are added after the n-best list generation step.
In previous work (Och et al, 2004; Hasan, 2011),
the IBM1 features (Brown et al, 1993) are found
helpful. As the IBM1 model is asymmetric, two
models are estimated, one in both directions. Con-
trary to the reported results, these additional features
do not yield significant improvements over the base-
line system. We assume that the difficulty is to add
information to an already extensively optimized sys-
tem. Moreover, the IBM1 models are estimated on
the same training corpora as the translation system,
a fact that may explain the redundancy of these ad-
ditional features.
In a separate series of experiments, we also add
WSD features calculated according to a variation of
the method proposed in (Apidianaki, 2009). For
each word of a subset of the input (source lan-
guage) vocabulary, a simple WSD classifier pro-
duces a probability distribution over a set of trans-
lations8. During reranking, each translation hypoth-
esis is scanned and the word translations that match
one of the proposed variant are rewarded using an
additional score. While this method had given some
8The difference with the method described in (Apidianaki,
2009) is that no sense clustering is performed, and each transla-
tion is represented by a separate weighted source feature vector
which is used for disambiguation
small gains on a smaller dataset (IWSLT?11), we did
not observe here any improvement over the base-
line system. Additional analysis hints that (i) most
of the proposed variants are already covered by the
translation model with high probabilities and (ii) that
these variants are seldom found in the reference sen-
tences. This means that, in the situation in which
only one reference is provided, the hypotheses with
a high score for the WSD feature are not adequately
rewarded with the actual references.
8 Conclusion
In this paper, we described our submissions to
WMT?12 in the French-English and German-
English shared translation tasks, in both directions.
As for our last year?s participation, our main sys-
tems are built with n-code, the open source Statis-
tical Machine Translation system based on bilingual
n-grams. Our contributions are threefold. First, we
have experimented a new kind of translation mod-
els, where the bilingual n-gram distribution are es-
timated in a continuous space with neural networks.
As shown in past evaluations with target language
model, there is a significant reward for using this
kind of models in a rescoring step. We observed that,
in general, the continuous space translation model
yields a slightly larger improvement than the target
translation model. However, their combination does
not result in an additional gain.
We also reported preliminary results with a sys-
tem ?on-the-fly?, where the training data are sam-
pled according to the data to be translated in order
to train contextually adapted system. While this sys-
tem achieves comparable performance to our base-
line system, it is worth noticing that its total train-
ing time is much smaller than a comparable Moses
system. Finally, we investigated several additional
features based on IBM1 models and word sense dis-
ambiguation information in rescoring. While these
methods have sometimes been reported to help im-
prove the results, we did not observe any improve-
ment here over the baseline system.
Acknowledgment
This work was partially funded by the French State
agency for innovation (OSEO) in the Quaero Pro-
gramme.
335
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
77?85, Athens, Greece, March. Association for Com-
putational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard Un iversity.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
Ilknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franc?ois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Hai-Son, Alexandre Allauzen, and Franc?ois Yvon. 2012.
Continuous space translation models with neural net-
works. In NAACL ?12: Proceedings of the 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology.
Sas?a Hasan. 2011. Triplet Lexicon Models for Statisti-
cal Machine Translation. Ph.D. thesis, RWTH Aachen
University.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
336
May 2 - May 7. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
337
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?9,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
WSD for n-best reranking and local language modeling in SMT
Marianna Apidianaki, Guillaume Wisniewski?, Artem Sokolov, Aure?lien Max?, Franc?ois Yvon?
LIMSI-CNRS
? Univ. Paris Sud
BP 133, F-91403, Orsay Cedex, France
firstname.lastname@limsi.fr
Abstract
We integrate semantic information at two
stages of the translation process of a state-of-
the-art SMT system. A Word Sense Disam-
biguation (WSD) classifier produces a proba-
bility distribution over the translation candi-
dates of source words which is exploited in
two ways. First, the probabilities serve to
rerank a list of n-best translations produced by
the system. Second, the WSD predictions are
used to build a supplementary language model
for each sentence, aimed to favor translations
that seem more adequate in this specific sen-
tential context. Both approaches lead to sig-
nificant improvements in translation perfor-
mance, highlighting the usefulness of source
side disambiguation for SMT.
1 Introduction
Word Sense Disambiguation (WSD) is the task of
identifying the sense of words in texts by reference
to some pre-existing sense inventory. The selec-
tion of the appropriate inventory and WSD method
strongly depends on the goal WSD intends to serve:
recent methods are increasingly oriented towards
the disambiguation needs of specific end applica-
tions, and explicitly aim at improving the overall
performance of complex Natural Language Process-
ing systems (Ide and Wilks, 2007; Carpuat and Wu,
2007). This task-oriented conception of WSD is
manifested in the area of multilingual semantic pro-
cessing: supervised methods, which were previously
shown to give the best results, are being abandoned
in favor of unsupervised ones that do not rely on pre-
annotated training data. Accordingly, pre-defined
semantic inventories, that usually served to provide
the lists of candidate word senses, are being replaced
by senses relevant to the considered applications and
directly identified from corpora by means of word
sense induction methods.
In a multilingual setting, the sense inventories
needed for disambiguation are generally built from
all possible translations of words or phrases in a par-
allel corpus (Carpuat and Wu, 2007; Chan et al,
2007), or by using more complex representations
of the semantics of translations (Apidianaki, 2009;
Mihalcea et al, 2010; Lefever and Hoste, 2010).
However, integrating this semantic knowledge into
Statistical Machine Translation (SMT) raises sev-
eral challenges: the way in which the predictions of
the WSD classifier have to be taken into account;
the type of context exploited for disambiguation;
the target words to be disambiguated (?all-words?
WSD vs. WSD restricted to target words satisfy-
ing specific criteria); the use of a single classifier
versus building separate classifiers for each source
word; the quantity and type of data used for training
the classifier (e.g., use of raw data or of more ab-
stract representations, such as lemmatization, allow-
ing to deal with sparseness issues), and many oth-
ers. Seemingly, the optimal way to take advantage
of WSD predictions remains an open issue.
In this work, we carry out a set of experiments
to investigate the impact of integrating the predic-
tions of a cross-lingual WSD classifier into an SMT
system, at two different stages of the translation pro-
cess. The first approach exploits the probability dis-
tribution built by the WSD classifier over the set of
translations of words found in the parallel corpus,
1
for reranking the translations in the n-best list gen-
erated by the SMT system. Words in the list that
match one of the proposed translations are boosted
and are thus more likely to appear in the final trans-
lation. Our results on the English-French IWSLT?11
task show substantial improvements in translation
quality. The second approach provides a tighter in-
tegration of the WSD classifier with the rest of the
system: using the WSD predictions, an additional
sentence specific language model is estimated and
used during decoding. These additional local mod-
els can be used as an external knowledge source to
reinforce translation hypotheses matching the pre-
diction of the WSD system.
In the rest of the paper, we present related work
on integrating semantic information into SMT (Sec-
tion 2). The WSD classifier used in the current study
is described in Section 3. We then present the two
approaches adopted for integrating the WSD out-
put into SMT (Section 4). Evaluation results are
presented in Section 5, before concluding and dis-
cussing some avenues for future work.
2 Related work
Word sense disambiguation systems generally work
at the word level: given an input word and its con-
text, they predict its (most likely) meaning. At
the same time, state-of-the-art translation systems
all consider groups of words (phrases, tuples, etc.)
rather than single words in the translation process.
This discrepancy between the units used in MT and
those used in WSD is one of the major difficul-
ties in integrating word predictions into the decoder.
This was, for instance, one of the reasons for the
somewhat disappointing results obtained by Carpuat
and Wu (2005) when the output of a WSD system
was directly incorporated into a Chinese-English
SMT system. Because of this difficulty, other cross-
lingual semantics works have considered only sim-
plified tasks, like blank-filling, without addressing
the integration of the WSD models in full-scale MT
systems (Vickrey et al, 2005; Specia, 2006).
Since the pioneering work of Carpuat and Wu
(2005), several more successful ways to take WSD
predictions into account have been proposed. For
instance, Carpuat and Wu (2007) proposed to gen-
eralize the WSD system so that it performs a fully
phrasal multiword disambiguation. However, given
that the number of phrases is far larger than the num-
ber of words, this approach suffers from sparsity
and computational problems, as it requires training
a classifier for each entry of the phrase table.
Chan et al (2007) introduced a way to modify the
rule weights of a hierarchical translation system to
reflect the predictions of their WSD system. While
their approach and ours are built on the same intu-
ition (an adaptation of a model to incorporate word
predictions) their work is specific to hierarchical
systems, while ours can be applied to any decoder
that uses a language model. Haque et al (2009) et
Haque et al (2010) introduce lexico-syntactic de-
scriptions in the form of supertags as source lan-
guage context-informed features in a phrase-based
SMT and a state-of-the-art hierarchical model, re-
spectively, and report significant gains in translation
quality.
Closer to our work, Mauser et al (2009) and Pa-
try and Langlais (2011) train a global lexicon model
that predicts the bag of output words from the bag
of input words. As no explicit alignment between
input and output words is used, words are chosen
based on the (global) input context. For each input
sentence, the decoder considers these word predic-
tions as an additional feature that it uses to define a
new model score which favors translation hypothe-
ses containing words predicted by the global lexicon
model. A difference between this approach and our
work is that instead of using a global lexicon model,
we disambiguate a subset of the words in the input
sentence by employing a WSD classifier that cre-
ates a probability distribution over the translations
of each word in its context.
The unsupervised cross-lingual WSD classifier
used in this work is similar to the one proposed in
Apidianaki (2009). The original classifier disam-
biguates new instances of words in context by se-
lecting the most appropriate cluster of translations
among a set of candidate clusters found in an auto-
matically built bilingual sense inventory. The sense
inventory exploited by the classifier is created by
a cross-lingual word sense induction (WSI) method
that reveals the senses of source words by grouping
their translations into clusters according to their se-
mantic proximity, revealed by a distributional sim-
ilarity calculation. The resulting clusters represent
2
the source words? candidate senses. This WSD
method gave good results in a word prediction task
but, similarly to the work of Vickrey et al (2005)
and of Specia (2006), the predictions are not inte-
grated into a complete MT system.
3 The WSD classifier
Our WSD classifier is a variation of the one intro-
duced in Apidianaki (2009). The main difference
is that here the classifier serves to discriminate be-
tween unclustered translations of a word and to as-
sign a probability to each translation for new in-
stances of the word in context. Each translation is
represented by a source language feature vector that
the classifier uses for disambiguation. All experi-
ments carried out in this study are for the English
(EN) - French (FR) language pair.
3.1 Source Language Feature Vectors
Preprocessing The information needed by the clas-
sifier is gathered from the EN-FR training data pro-
vided for the IWSLT?11 evaluation task.1 The
dataset consists of 107,268 parallel sentences, word-
aligned in both translation directions using GIZA++
(Och and Ney, 2003). We disambiguate EN words
found in the parallel corpus that satisfy the set of
criteria described below.
Two bilingual lexicons are built from the align-
ment results and filtered to eliminate spurious align-
ments. First, translation correspondences with a
probability lower than a threshold are discarded;2
then translations are filtered by part-of-speech
(PoS), keeping for each word only translations per-
taining to the same grammatical category;3 finally,
only intersecting alignments (i.e., correspondences
found in the lexicons of both directions) are retained.
Given that the lexicons contain word forms, the in-
tersection is calculated based on lemmatization in-
formation in order to perform a generalization over
the contents of the lexicons. For instance, if the EN
adjective regular is translated by habituelle (femi-
1http://www.iwslt2011.org/
2The translation probabilities between word tokens are
found in the translation table produced by GIZA++; the thresh-
old is set to 0.01.
3For this filtering, we employ a PoS and lemmatization lex-
icon built after tagging both parts of the training corpus with
TreeTagger (Schmid, 1994).
nine singular form of the adjective habituel) in the
EN-FR lexicon, but is found to translate habituel
(masculine singular form) in the other direction,
the EN-FR correspondence regular/habituelle is re-
tained (because the two variants of the adjective are
reduced to the same lemma).
All lexicon entries satisfying the above criteria are
retained and used for disambiguation. In these initial
experiments, we disambiguate English words having
less than 20 French translations in the lexicon. Each
French translation of an English word that appears
more than once in the training corpus4 is character-
ized by a weighted English feature vector built from
the training data.
Vector building The feature vectors corresponding
to the translations are built by exploiting information
from the source contexts (Apidianaki, 2008; Grefen-
stette, 1994). For each translation of an EN word w,
we extract the content words that co-occur with w
in the corresponding source sentences of the parallel
corpus (i.e. the content words that occur in the same
sentence as w whenever it is translated by this trans-
lation). The extracted source language words con-
stitute the features of the vector built for the transla-
tion.
For each translation Ti of w, let N be the number
of features retained from the corresponding source
context. Each feature Fj (1 ? j ? N) receives a to-
tal weight tw(Fj,Ti) defined as the product of the
feature?s global weight, gw(Fj), and its local weight
with that translation, lw(Fj,Ti):
tw(Fj,Ti) = gw(Fj) ? lw(Fj,Ti) (1)
The global weight of a feature Fj is a function of
the number Ni of translations (Ti?s) to which Fj is re-
lated, and of the probabilities (pi j) that Fj co-occurs
with instances of w translated by each of the Ti?s:
gw(Fj) = 1?
?Ti pi j log(pi j)
Ni
(2)
Each of the pi j?s is computed as the ratio between
the co-occurrence frequency of Fj with w when
translated as Ti, denoted as cooc frequency(Fj,Ti),
4We do not consider hapax translations because they often
correspond to alignment errors.
3
and the total number of features (N) seen with Ti:
pi j =
cooc frequency(Fj,Ti)
N
(3)
Finally, the local weight lw(Fj,Ti) between Fj and Ti
directly depends on their co-occurrence frequency:
lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (4)
3.2 Cross-Lingual WSD
The weighted feature vectors corresponding to the
different translations of an English word are used
for disambiguation.5 As noted in Section 3.1, we
disambiguate source words satisfying a set of crite-
ria. Disambiguation is performed by comparing the
vector associated with each translation to the new
context of the words in the input sentences from the
IWSLT?11 test set.
More precisely, the information contained in each
vector is exploited by the WSD classifier to produce
a probability distribution over the translations, for
each new instance of a word in context. We dis-
ambiguate word forms (not lemmas) in order to di-
rectly use the selected translations in the translated
texts. However, we should note that in some cases
this reduces the role of WSD to distinguishing be-
tween different forms of one word and no different
senses are involved. Using more abstract represen-
tations (corresponding to senses) is one of the per-
spectives of this work.
The classifier assigns a score to each transla-
tion by comparing information in the corresponding
source vector to information found in the new con-
text. Given that the vector features are lemmatized,
the new context is lemmatized as well and the lem-
mas of the content words are gathered in a bag of
words. The adequacy of each translation for a new
instance of a word is estimated by comparing the
translation?s vector with the bag of words built from
the new context. If common features are found be-
tween the new context and a translation vector, an
association score is calculated corresponding to the
mean of the weights of the common features rela-
tively to the translation (i.e. found in its vector). In
5The vectors are not used for clustering the translations as
in Apidianaki (2009) but all translations are considered as can-
didate senses.
Equation (5), (CFj)|CF |j=1 is the set of common fea-
tures between the translation vector Vi and the new
context C and tw is the weight of a CF with transla-
tion Ti (cf. formula (1)).
assoc score(Vi,C) =
?|CF |j=1 tw(CFj,Ti)
|CF| (5)
The scores assigned to the different translations of a
source word are normalized to sum up to one.
In this way, a subset of the words that occur in the
input sentences from the test set are annotated with
their translations and the associated scores (contex-
tual probabilities), as shown in the example in Fig-
ure 1.6 The WSD classifier makes predictions only
for the subset of the words found in the source part
of the parallel test set that were retained from the ini-
tial EN-FR lexicon after filtering. Table 1 presents
the total coverage of the WSD method as well as its
coverage for words of different PoS, with a focus
on content words. We report the number of disam-
biguated words for each content PoS (cf. third col-
umn) and the corresponding percentage, calculated
on the basis of the total number of words pertaining
to this PoS (cf. second column). We observe that
the coverage of the method on nouns and adjectives
is higher than the one on verbs. Given the rich ver-
bal morphology of French, several verbs have a very
high number of translations in the bilingual lexicon
(over 20) and are not handled during disambigua-
tion. The same applies to function words (articles,
prepositions, conjunctions, etc.) included in the ?all
PoS? category.
4 Integrating Semantics into SMT
In this section, we present two ways to integrate
WSD predictions into an SMT decoder. The first
one (Section 4.1) is a simple method based on n-
best reranking. This method, already proposed in
the literature (Specia et al, 2008), allows us to eas-
ily evaluate the impact of WSD predictions on au-
tomatic translation quality. The second one (Sec-
tion 4.2) builds on the idea, introduced in (Crego et
al., 2010), of using an additional language model to
6Some source words are tagged with only one translation
(e.g. stones {pierres(1.000)}) because their other translations
in the lexicon occurred only once in the training corpus and,
consequently, were not considered.
4
PoS # of words # of WSD predictions %
Nouns 5535 3472 62.72
Verbs 5336 1269 23.78
Adjs 1787 1249 69.89
Advs 2224 1098 49.37
all content PoS 14882 7088 47.62
all PoS 27596 8463 30.66
Table 1: Coverage of the WSD method
you know, one of the intense {intenses(0.305), forte(0.306), intense(0.389)} pleasures of
travel {transport(0.334), voyage(0.332), voyager(0.334)} and one of the delights of ethnographic
research {recherche(0.225), research(0.167), e?tudes(0.218), recherches(0.222), e?tude(0.167)} is
the opportunity {possibilite?(0.187), chance(0.185), opportunite?s(0.199), occasion(0.222), opportu-
nite?(0.207)} to live amongst those who have not forgotten {oubli(0.401), oublie?s(0.279), ou-
blie?e(0.321)} the old {ancien(0.079), a?ge(0.089), anciennes(0.072), a?ge?es(0.100), a?ge?s(0.063), an-
cienne(0.072), vieille(0.093), ans(0.088), vieux(0.086), vieil(0.078), anciens(0.081), vieilles(0.099)}
ways {fac?ons(0.162), manie`res(0.140), moyens(0.161), aspects(0.113), fac?on(0.139), moyen(0.124),
manie`re(0.161)} , who still feel their past {passe?e(0.269), autrefois(0.350), passe?(0.381)} in the
wind {e?olienne(0.305), vent(0.392), e?oliennes(0.304)} , touch {touchent(0.236), touchez(0.235),
touche(0.235), toucher(0.293)} it in stones {pierres(1.000)} polished by rain {pluie(1.000)} ,
taste {gou?t(0.500), gou?ter(0.500)} it in the bitter {amer(0.360), ame`re(0.280), amertume(0.360)}
leaves {feuilles(0.500), feuillages(0.500)} of plants {usines(0.239), centrales(0.207), plantes(0.347),
ve?ge?taux(0.207)}.
Figure 1: Input sentence with WSD information
directly integrate the prediction of the WSD system
into the decoder.
4.1 N-best List Reranking
A simple way to influence translation hypotheses se-
lection with WSD information is to use the WSD
probabilities of translation variants to produce an ad-
ditional feature appended to the n-best list after its
generation. The feature value should reflect the de-
gree to which a particular hypothesis includes pro-
posed WSD variants for the respective words. Re-
running the standard MERT optimization procedure
on the augmented features gives a new set of model
weights, that are used to rescore the n-best list.
We propose the following method of features con-
struction. Given the phrase alignment information
between a source sentence and a hypothesis, we ver-
ify if one or more of the proposed WSD variants for
the source word occur in the corresponding phrase of
the translation hypothesis. If this is the case, the cor-
responding probabilities are additively accumulated
for the current hypothesis. At the end, two features
are appended to each hypothesis in the n-best list:
the total score accumulated for the hypothesis and
the same score normalized by the number of words
in the hypothesis.
Two MERT initialization schemes were consid-
ered: (1) all model weights are initialized to zero,
and (2) all the weights of ?standard? features are ini-
tialized to the values found by MERT and the new
WSD features to zero.
4.2 Local Language Models
We propose to adapt the approach introduced in
Crego et al (2010) as an alternative way to inte-
grate the WSD predictions within the decoder: for
each sentence to be translated, an additional lan-
guage model (LM) is estimated and taken into ac-
count during decoding. As this additional ?local?
model depends on the source sentence, it can be
used as an external source of knowledge to reinforce
translation hypotheses complying with criteria pre-
dicted from the whole source sentence. For instance,
the unigram probabilities of the additional LM can
be derived from the (word) predictions of a WSD
system, bigram probabilities from the prediction of
phrases and so on and so forth. Although this ap-
proach was suggested in (Crego et al, 2010), this
5
is, to the best of our knowledge, the first time it is
experimentally validated.
In practice, the predictions of the WSD system
described in Section 3 can be integrated by defining,
for each sentence, an additional unigram language
model as follows:
? each translation predicted by the WSD classi-
fier can be generated by the language model
with the probability estimated by the WSD
classifier; no information about the source
word that has been disambiguated is consid-
ered;
? the probability of unknown words is set to a
small arbitrary constant.
Even if most of the words composing the transla-
tion hypothesis are considered as unknown words,
hypotheses that contain the words predicted by the
WSD system still have a higher LM score and are
therefore preferred. Note that even if we only use
unigram language models in our experiments, as
senses are predicted at the word level, our approach
is able to handle disambiguation of phrases as well.
This approach has two main advantages over ex-
isting ways to integrate WSD predictions in an SMT
system. First, no hard decisions are made: errors
of the WSD can be ?corrected? by the translation.
Second, sense disambiguation at the word level is
naturally and automatically propagated at the phrase
level: the additional LM is influencing all phrase
pairs using one of the predicted words.
Compared to the reranking approach introduced
in the previous section, this method results in a
tighter integration with the decoder. In particu-
lar, the WSD predictions are applied before search-
space pruning and are therefore expected to have a
more important role.
5 Evaluation
5.1 Experimental Setting
In all our experiments, we considered the TED-
talk English to French data set provided by the
IWSLT?11 evaluation campaign, a collection of pub-
lic speeches on a variety of topics. We used the
Moses decoder (Koehn et al, 2007).
The TED-talk corpus is a small data set made
of a monolingual corpus (111,431 sentences) used
to estimate a 4-gram language model with KN-
smoothing, and a bilingual corpus (107,268 sen-
tences) used to extract the phrase table. All data
are tokenized, cleaned and converted to lowercase
letters using the tools provided by the WMT orga-
nizers.7 We then use a standard training pipeline to
construct the translation model: the bitext is aligned
using GIZA++, symmetrized using the grow-diag-
final-and heuristic; the phrase table is extracted and
scored using the tools distributed with Moses. Fi-
nally, systems are optimized using MERT on the
934 sentences of the dev-2010 set. All evalua-
tions are performed on the 1,664 sentences of the
test-2010 set.
5.2 Baseline
In addition to the models introduced in Section 4,
we considered two other supplementary models as
baselines. The first one uses the IBM 1 model esti-
mated during the SMT system training as a simple
WSD system: for each source sentence, a unigram
additional language model is defined by taking, for
each source, the 20 best translations according to the
IBM 1 model and their probability. Model 1 has
been shown to be one of the best performing fea-
tures to be added to an SMT system in a reranking
step (Och et al, 2004) and can be seen as a naive
WSD classifier.
To test the validity of our approach, we repli-
cate the ?oracle? experiments of Crego et al (2010)
and estimate the best gain our method can achieve.
These experiments consist in using the reference to
train a local n-gram language model (with n in the
range 1 to 3) which amounts, in the local language
model method of Section 4.2, to assuming that the
WSD system correctly predicted a single translation
for each source word.
5.3 Results
Table 2 reports the results of our experiments. It
appears that, for the considered task, sense disam-
biguation improves translation performance: n-best
rescoring results in a 0.37 BLEU improvement and
using an additional language model brings about an
improvement of up to a 0.88 BLEU. In both cases,
MERT assigns a large weight to the additional fea-
7http://statmt.org/wmt08/scripts.tgz
6
method BLEU METEOR
baseline ? 29.63 53.78
rescoring WSD (zero init) 30.00 54.26WSD (reinit) 29.58 53.96
additional LM
oracle 3-gram 43.56 64.64
oracle 2-gram 39.36 62.92
oracle 1-gram 42.92 69.39
IBM 1 30.18 54.36
WSD 30.51 54.38
Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions.
PoS baseline WSD
Nouns 67.57 69.06
Verbs 45.97 47.76
Adjectives 51.79 53.94
Adverbs 52.17 56.25
Table 3: Contrastive lexical evaluation: % of words correctly translated within each PoS class
tures during tuning. When rescoring n-best, an im-
provement is observed only when the weights are
initialized to zero and not to the weights resulting
from the previous optimization, maybe because of
the difficulty to exit the local minimum MERT had
found earlier.
As expected, integrating the WSD predictions
with an additional language model results in a larger
improvement than simple rescoring, which shows
the importance of applying this new source of in-
formation early in the translation pipeline, before
search space pruning. Also note that the system us-
ing the IBM 1 predictions is outperformed by the
system using the WSD classifier introduced in Sec-
tion 3, showing the quality of its predictions.
Oracle experiments stress the high potential of
the method introduced in (Crego et al, 2010) as a
way to integrate external sources of knowledge: all
three conditions result in large improvements over
the baseline and the proposed methods. It must,
however, be noted that contrary to the WSD method
introduced in Section 3, these oracle experiments
rely on sense predictions for all source words and
not only content words. Surprisingly enough, pre-
dicting phrases instead of words results only in a
small improvement. Additional experiments are re-
quired to explain why 2-gram oracle achieved such
a low performance.
5.4 Contrastive lexical evaluation
All the measures used for evaluating the impact
of WSD information on translation show improve-
ments, as discussed in the previous section. We
complement these results with another measure of
translation performance, proposed by Max et al
(2010), which allows for a more fine-grained con-
trastive evaluation of the translations produced by
different systems. The method permits to compare
the results produced by the systems on different
word classes and to take into account the source
words that were actually translated. We focus this
evaluation on the classes of content words (nouns,
adjectives, verbs and adverbs) on which WSD had
an important coverage. Our aim is, first, to ex-
plore how these words are handled by a WSD-
informed SMT system (the system using the lo-
cal language models) compared to the baseline sys-
tem that does not exploit any semantic informa-
tion; and, second, to investigate whether their dis-
ambiguation influences the translation of surround-
ing non-disambiguated words.
Table 3 reports the percentage of words cor-
rectly translated by the semantically-informed sys-
tem within each content word class: consistent gains
in translation quality are observed for all parts-of-
speech compared to the baseline, and the best results
are obtained for nouns.
7
baseline WSD
w?2 w?1 w+1 w+2 w?2 w?1 w+1 w+2
Nouns 64.01 68.69 75.17 64.6 65.47 70.46 76.3 66.6
Verbs 68.67 67.58 63 62.19 69.98 68.89 64.85 64.25
Adjectives 63.1 64.39 64.28 66.55 64.09 65.65 64.76 69.33
Adverbs 70.8 69.44 68.67 66.38 71 71.21 70 67.22
Table 4: Impact of WSD prediction on the surrounding words
Table 4 shows how the words surrounding a dis-
ambiguated word w (noun, verb, adjective or adverb)
in the text are handled by the two systems. More
precisely, we look at the translation of words in the
immediate context of w, i.e. at positions w?2, w?1,
w+1 and w+2. The left column reports the percent-
age of correct translations produced by the baseline
system (without disambiguation) for words in these
positions; the right column shows the positive im-
pact that the disambiguation of a word has on the
translation of its neighbors. Note that this time we
look at disambiguated words and their context with-
out evaluating the correctness of the WSD predic-
tions. Nevertheless, even in this case, consistent
gains are observed when WSD information is ex-
ploited. For instance, when a noun is disambiguated,
70.46% and 76.3% of the immediately preceding
(w?1) and following (w+1) words, respectively, are
correctly translated, versus 68.69% and 75.17% of
correct translations produced by the baseline system.
6 Conclusion and future work
The preliminary results presented in this paper on
integrating cross-lingual WSD into a state-of-the-
art SMT system are encouraging. Both adopted ap-
proaches (n-best rescoring and local language mod-
eling) benefit from the predictions of the proposed
cross-lingual WSD classifier. The contrastive eval-
uation results further show that WSD improves not
only the translation of disambiguated words, but also
the translation of neighboring words in the input
texts.
We consider various ways for extending this
work. First, future experiments will involve the use
of more abstract representations of senses than indi-
vidual translations, by applying a cross-lingual word
sense induction method to the training corpus prior
to disambiguation. We will also experiment with
disambiguation at the level of lemmas, to reduce
sparseness issues, and with different ways for han-
dling lemmatized predictions by the SMT systems.
Furthermore, we intend to extend the coverage of the
WSD method by exploring other filtering methods
for cleaning the alignment lexicons, and by address-
ing the disambiguation of words of all PoS.
Acknowledgments
This work was partly funded by the European Union
under the FP7 project META-NET (T4ME), Con-
tract No. 249119, and by OSEO, the French agency
for innovation, as part of the Quaero Program.
References
Marianna Apidianaki. 2008. Translation-oriented Word
Sense Induction Based on Parallel Corpora. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selection
in Translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-09), pages 77?85,
Athens, Greece.
Marine Carpuat and Dekai Wu. 2005. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
387?394, Ann Arbor, Michigan.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation. In Proceedings of the Joint EMNLP-CoNLL
Conference, pages 61?72, Prague, Czech Republic.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 33?40, Prague, Czech Republic.
8
Josep Maria Crego, Aure?lien Max, and Franc?ois Yvon.
2010. Local lexical adaptation in Machine Transla-
tion through triangulation: SMT helping SMT. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 232?
240, Beijing, China.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Rejwanual Haque, Sudip Naskar, Yanjun Ma, and Andy
Way. 2009. Using supertags as source language con-
text in SMT. In Proceedings of the 13th Annual Meet-
ing of the European Association for Machine Transla-
tion (EAMT 2009), pages 234?241, Barcelona, Spain.
Rejwanul Haque, Sudip Kumar Naskar, Antal Van Den
Bosch, and Andy Way. 2010. Supertags as source lan-
guage context in hierarchical phrase-based SMT. In
Proceedings of AMTA 2010: The Ninth Conference of
the Association for Machine Translation in the Ameri-
cas, pages 210?219, Denver, CO.
N. Ide and Y. Wilks. 2007. Making Sense About Sense.
In E. Agirre and P. Edmonds, editors, Word Sense Dis-
ambiguation, Algorithms and Applications, pages 47?
73. Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual ACL Meeting, Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2), ACL 2010, pages
15?20, Uppsala, Sweden.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 210?217,
Singapore, August.
Aure?lien Max, Josep Maria Crego, and Franc?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-Lingual Lexical Sub-
stitution. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), ACL
2010, pages 9?14, Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In Proceedings of HLT-NAACL 2004, pages 161?
168, Boston, Massachusetts, USA.
Alexandre Patry and Philippe Langlais. 2011. Going be-
yond word cooccurrences in global lexical selection
for statistical machine translation using a multilayer
perceptron. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
658?666, Chiang Mai, Thailand, November.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Lucia Specia, Baskaran Sankaran, and Maria Das
Grac?as Volpe Nunes. 2008. n-Best Reranking for the
Efficient Integration of Word Sense Disambiguation
and Statistical Machine Translation. In Proceedings of
the 9th international conference on Computational lin-
guistics and intelligent text processing, CICLing?08,
pages 399?410, Berlin, Heidelberg. Springer-Verlag.
Lucia Specia. 2006. A Hybrid Relational Approach for
WSD - First Results. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages 55?
60, Sydney, Australia.
David Vickrey, Luke Biewald, Marc Teyssier, and
Daphne Koller. 2005. Word-Sense Disambiguation
for Machine Translation. In Proceedings of the Joint
Conference on Human Language Technology / Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP), pages 771?778, Vancouver, Canada.
9
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62?69,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI @ WMT?13
Alexandre Allauzen1,2, Nicolas Pe?cheux1,2, Quoc Khanh Do1,2, Marco Dinarelli2,
Thomas Lavergne1,2, Aure?lien Max1,2, Hai-Son Le3, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Vietnamese Academy of Science and Technology3, Hanoi, Vietnam
lehaison@ioit.ac.vn
Abstract
This paper describes LIMSI?s submis-
sions to the shared WMT?13 translation
task. We report results for French-English,
German-English and Spanish-English in
both directions. Our submissions use
n-code, an open source system based on
bilingual n-grams, and continuous space
models in a post-processing step. The
main novelties of this year?s participation
are the following: our first participation
to the Spanish-English task; experiments
with source pre-ordering; a tighter integra-
tion of continuous space language mod-
els using artificial text generation (for Ger-
man); and the use of different tuning sets
according to the original language of the
text to be translated.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Eighth Workshop on
Statistical Machine Translation. LIMSI partici-
pated in the French-English, German-English and
Spanish-English tasks in both directions. For this
evaluation, we used n-code, an open source in-
house Statistical Machine Translation (SMT) sys-
tem based on bilingual n-grams1, and continuous
space models in a post-processing step, both for
translation and target language modeling.
This paper is organized as follows. Section 2
contains an overview of the baseline systems built
with n-code, including the continuous space mod-
els. As in our previous participations, several
steps of data pre-processing, cleaning and filter-
ing are applied, and their improvement took a non-
negligible part of our work. These steps are sum-
marized in Section 3. The rest of the paper is de-
voted to the novelties of the systems submitted this
1http://ncode.limsi.fr/
year. Section 4 describes the system developed for
our first participation to the Spanish-English trans-
lation task in both directions. To translate from
German into English, the impact of source pre-
ordering is investigated, and experimental results
are reported in Section 5, while for the reverse di-
rection, we explored a text sampling strategy us-
ing a 10-gram SOUL model to allow a tighter in-
tegration of continuous space models during the
translation process (see Section 6). A final section
discusses the main lessons of this study.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o
et al, 2006; Crego and Marin?o, 2006). In this
framework, translation is divided in two steps: a
source reordering step and a (monotonic) transla-
tion step. Source reordering is based on a set of
learned rewrite rules that non-deterministically re-
order the input words. Applying these rules result
in a finite-state graph of possible source reorder-
ings, which is then searched for the best possible
candidate translation.
2.1 Features
Given a source sentence s of I words, the best
translation hypothesis t? is defined as the sequence
of J words that maximizes a linear combination of
feature functions:
t? = argmax
t,a
{ M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature
function hm and a denotes an alignment between
source and target phrases. Among the feature
functions, the peculiar form of the translation
model constitutes one of the main difference be-
tween the n-gram approach and standard phrase-
based systems.
62
In addition to the translation model (TM), four-
teen feature functions are combined: a target-
language model; four lexicon models; six lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two
lexical weights are estimated from the automatic
word alignments. The weight vector ? is learned
using the Minimum Error Rate Training frame-
work (MERT) (Och, 2003) and BLEU (Papineni
et al, 2002) measured on nt09 (newstest2009) as
the optimization criteria.
2.2 Translation Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to repro-
duce the word order modifications introduced dur-
ing the tuple extraction process. Hence, only those
reordering hypotheses are translated and are intro-
duced using a set of reordering rules automatically
learned from the word alignments. Part-of-speech
(POS) information is used to increase the gen-
eralization power of these rules. Hence, rewrite
rules are built using POS, rather than surface word
forms (Crego and Marin?o, 2006).
2.3 SOUL rescoring
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al, 2003; Schwenk
et al, 2006) as a potential means to improve dis-
crete language models (LMs). As for our last year
participation (Le et al, 2012c), we take advantage
of the recent proposal of Le et al (2011). Using
a specific neural network architecture (the Struc-
tured OUtput Layer or SOUL model), it becomes
possible to estimate n-gram models that use large
vocabulary, thereby making the training of large
neural network LMs (NNLMs) feasible both for
target language models and translation models (Le
et al, 2012a). We use the same models as last year,
meaning that the SOUL rescoring was used for all
systems, except for translating into Spanish. See
section 6 and (Le et al, 2012c) for more details.
3 Corpora and data pre-processing
Concerning data pre-processing, we started from
our submissions from last year (Le et al, 2012c)
and mainly upgraded the corpora and the associ-
ated language-dependent pre-processing routines.
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte
et al, 2008). Previous experiments have demon-
strated that better normalization tools provide bet-
ter BLEU scores: all systems are thus built using
the ?true-case? scheme.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to un-
known forms). When translating from German
into English, the German side is thus normalized
using a specific pre-processing scheme (Allauzen
et al, 2010; Durgar El-Kahlout and Yvon, 2010)
which aims at reducing the lexical redundancy by
(i) normalizing the orthography, (ii) neutralizing
most inflections and (iii) splitting complex com-
pounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition,
for German, fine-grained POS labels were also
needed for pre-processing and were obtained us-
ing the RFTagger (Schmid and Laws, 2008).
For Spanish, all the availaible data are tokenized
using FreeLing2 toolkit (Padro? and Stanilovsky,
2012), with default settings and some added rules.
Sentence splitting and morphological analysis are
disabled except for del ? de el and al ? a el.
Moreover, a simple ?true-caser? based on upper-
case word frequency is used, and the specific
Spanish punctuation signs ??? and ??? are removed
and heuristically reintroduced in a post-processing
step. All Spanish texts are POS-tagged also using
Freeling. The EAGLES tag set is however sim-
plified by truncating the category label to the first
two symbols, in order to reduce the sparsity of the
reordering rules estimated by n-code.
For the CommonCrawl corpus, we found that
many sentences are not in the expected language.
For example, in the French side of the French-
English version, most of the first sentences are
in English. Therefore, foreign sentence pairs are
filtered out with a MaxEnt classifier that uses n-
grams of characters as features (n is between 1
and 4). This filter discards approximatively 10%
2http://nlp.lsi.upc.edu/freeling/
63
of the sentence pairs. Moreover, we also observe
that a lot of sentence pairs are not translation of
each other. Therefore, an extra sentence alignment
step is carried out using an in-house implementa-
tion of the tool described in (Moore, 2002). This
last step discards approximately 20% of the cor-
pus. For the Spanish-English task, the same filter-
ing is applied to all the available corpora.
4 System development for the
Spanish-English task
This is our first participation to the Spanish-
English translation task in both directions. This
section provides details about the development of
n-code systems for this language pair.
4.1 Data selection and filtering
The CommonCrawl and UN corpora can be con-
sidered as very noisy and out-of-domain. As de-
scribed in (Allauzen et al, 2011), to select a subset
of parallel sentences, trigram LMs were trained for
both Spanish and English languages on a subset of
the available News data: the Spanish (resp. En-
glish) LM was used to rank the Spanish (resp. En-
glish) side of the corpus, and only those sentences
with perplexity above a given threshold were se-
lected. Finally, the two selected sets were in-
tersected. In the following experiments, the fil-
tered versions of these corpora are used to train
the translation systems unless explicitly stated.
4.2 Spanish language model
To train the language models, we assumed that the
test set would consist in a selection of recent news
texts and all the available monolingual data for
Spanish were used, including the Spanish Giga-
word, Third Edition. A vocabulary is first defined
by including all tokens observed in the News-
Commentary and Europarl corpora. This vocab-
ulary is then expanded with all words that occur
more than 10 times in the recent news texts (LDC-
2007-2011 and news-crawl-2011-2012). This pro-
cedure results in a vocabulary containing 372k
words. Then, the training data are divided into
7 sets based on dates or genres. On each set, a
standard 4-gram LM is estimated from the vocab-
ulary using absolute discounting interpolated with
lower order models (Kneser and Ney, 1995; Chen
and Goodman, 1998). The resulting LMs are then
linearly interpolated using coefficients chosen so
Corpora BLEU
dev nt11 test nt12
es2en N,E 30.2 33.2
N,E,C 30.6 33.7
N,E,U 30.3 33.6
N,E,C,U 30.6 33.7
N,E,C,U (nf) 30.7 33.6
en2es N,E 32.2 33.3
N,E,C,U 32.3 33.6
N,E,C,U (nf) 32.5 33.9
Table 1: BLEU scores achieved with different
sets of parallel corpora. All systems are base-
line n-code with POS factor models. The follow-
ing shorthands are used to denote corpora, : ?N?
stands for News-Commentary, ?E? for Europarl,
?C? for CommonCrawl, ?U? for UN and (nf) for
non filtered corpora.
as to minimise the perplexity evaluated on the de-
velopment set (nt08).
4.3 Experiments
All reported results are averaged on 3 MERT runs.
Table 1 shows the BLEU scores obtained with dif-
ferent corpora setups. We can observe that us-
ing the CommonCrawl corpus improves the per-
formances in both directions, while the impact of
the UN data is less important, especially when
combined with CommonCrawl. The filtering strat-
egy described in Section 4.2 has a slightly posi-
tive impact of +0.1 BLEU point for the Spanish-
to-English direction but yields a 0.2 BLEU point
decrease in the opposite direction.
For the following experiments, all the available
corpora are therefore used: News-Commentary,
Europarl, filtered CommonCrawl and UN. For
each of these corpora, a bilingual n-gram model
is estimated and used by n-code as one individual
model score. An additionnal TM is trained on the
concatenation all these corpora, resulting in a to-
tal of 5 TMs. Moreover, n-code is able to handle
additional ?factored? bilingual models where the
source side words are replaced by the correspond-
ing lemma or even POS tag (Koehn and Hoang,
2007). Table 2 reports the scores obtained with
different settings.
In Table 2, big denotes the use of a wider
context for n-gram TMs (n = 4, 5, 4 instead
of 3, 4, 3 respectively for word-based, POS-based
and lemma-based TMs). Using POS factored
64
Condition BLEU
dev nt11 test nt12
es2en base 30.3 33.5
pos 30.6 33.7
big-pos 30.7 33.7
big-pos-lem 30.7 33.8
en2es base 32.0 33.4
pos 32.3 33.6
big-pos 32.3 33.8
big-pos-pos+ 32.2 33.4
Table 2: BLEU scores for different configuration
of factored translation models. The big prefix de-
notes experiments with the larger context for n-
gram translation models.
models yields a significant BLEU improvement,
as well as using a wider context for n-gram TMs.
Since Spanish is morphologically richer than En-
glish, lemmas are introduced only on the Span-
ish side. An additionnal BLEU improvement is
achieved by adding factored models based on lem-
mas when translating from Spanish to English,
while in the opposite direction it does not seem
to have any clear impact.
For English to Spanish, we also experimented
with a 5-gram target factored model, using the
whole morphosyntactic EAGLES tagset, (pos+ in
Table 2), to add some syntactic information, but
this, in fact, proved harmful.
As several tuning sets were available, experi-
ments were carried out with the concatenation of
nt09 to nt11 as a tuning data set. This yields an im-
provement between 0.1 and 0.3 BLEU point when
testing on nt12 when translating from Spanish to
English.
4.4 Submitted systems
For both directions, the submitted systems are
trained on all the available training data, the cor-
pora CommonCrawl and UN being filtered as de-
scribed previously. A word-based TM and a POS
factored TM are estimated for each training set.
To translate from Spanish to English, the system
is tuned on the concatenation of the nt09 to nt11
datasets with an additionnal 4-gram lemma-based
factored model, while in the opposite direction, we
only use nt11.
dev nt09 test nt11
en2de 15.43 15.35
en-mod2de 15.06 15.00
Table 3: BLEU scores for pre-ordering experi-
ments with a n-code system and the approach pro-
posed by (Neubig et al, 2012)
5 Source pre-ordering for English to
German translation
While distorsion models can efficiently handle
short range reorderings, they are inadequate to
capture long-range reorderings, especially for lan-
guage pairs that differ significantly in their syn-
tax. A promising workaround is the source pre-
ordering method that can be considered similar,
to some extent, to the reordering strategy imple-
mented in n-code; the main difference is that the
latter uses one deterministic (long-range) reorder-
ing on top of conventional distortion-based mod-
els, while the former only considers one single
model delivering permutation lattices. The pre-
ordering approach is illustrated by the recent work
of Neubig et al (2012), where the authors use a
discriminatively trained ITG parser to infer a sin-
gle permutation of the source sentence.
In this section, we investigate the use of this
pre-ordering model in conjunction with the bilin-
gual n-gram approach for translating English into
German (see (Collins et al, 2005) for similar ex-
periments with the reverse translation direction).
Experiments are carried out with the same settings
as described in (Neubig et al, 2012): given the
source side of the parallel data (en), the parser is
estimated to modify the original word order and to
generate a new source side (en-mod); then a SMT
system is built for the new language pair (en-mod
? de). The same reordering model is used to re-
order the test set, which is then translated with the
en-mod? de system.
Results for these experiments are reported in Ta-
ble 3, where nt09 and nt11 are respectively used
as development and test sets. We can observe that
applying pre-ordering on source sentences leads to
small drops in performance for this language pair.
To explain this degradation, the histogram of to-
ken movements performed by the model on the
pre-ordered training data is represented in Fig-
ure 1. We can observe that most of the movements
are in the range [?4,+6] (92% of the total occur-
65
Figure 1: Histogram of token movement size ver-
sus its occurrences performed by the model Neu-
big on the source english data.
rences), which can be already taken into account
by the standard reordering model of the baseline
system. This is reflected also by the following
statistics: surprisingly, only 16% of the total num-
ber of sentences are changed by the pre-ordering
model, and the average sentence-wise Kendall?s ?
and the average displacement of these small parts
of modified sentences are, respectively, 0.027 and
3.5. These numbers are striking for two reasons:
first, English and German have in general quite
different word order, thus our experimental con-
dition should be somehow similar to the English-
Japanese scenario studied in (Neubig et al, 2012);
second, since the model is able to perform pre-
ordering basically at any distance, it is surprising
that a large part of the data remains unmodified.
6 Artificial Text generation with SOUL
While the context size for BOLMs is limited (usu-
ally up to 4-grams) because of sparsity issues,
NNLMs can efficiently handle larger contexts up
to 10-grams without a prohibitive increase of the
overall number of parameters (see for instance the
study in (Le et al, 2012b)). However the major
bottleneck of NNLMs is the computation cost dur-
ing both training and inference. In fact, the pro-
hibitive inference time usually implies to resort to
a two-pass approach: the first pass uses a conven-
tional BOLM to produce a k-best list (the k most
likely translations); in the second pass, the prob-
ability of a NNLM is computed for each hypoth-
esis, which is then added as a new feature before
the k-best list is reranked. Note that to produce the
k-best list, the decoder uses a beam search strategy
to prune the search space. Crucially, this pruning
does not use the NNLMs scores and results in po-
tentially sub-optimal k-best-lists.
6.1 Sampling texts with SOUL
In language modeling, a language is represented
by a corpus that is approximated by a n-gram
model. Following (Sutskever et al, 2011; Deoras
et al, 2013), we propose an additionnal approxi-
mation to allow a tighter integration of the NNLM:
a 10-gram NNLM is first estimated on the training
corpus; texts then are sampled from this model to
create an artificial training corpus; finally, this arti-
ficial corpus is approximated by a 4-gram BOLM.
The training procedure for the SOUL NNLM is
the same as the one described in (Le et al, 2012c).
To sample a sentence from the SOUL model, first
the sentence length is randomly drawn from the
empirical distribution, then each word of the sen-
tence is sampled from the 10-gram distribution es-
timated with the SOUL model.
The convergence of this sampling strategy can
be evaluated by monitoring the perplexity evolu-
tion vs. the number of sentences that are gener-
ated. Figure 2 depicts this evolution by measuring
perplexity on the nt08 set with a step size of 400M
sampled sentences. The baseline BOLM (std) is
estimated on all the available training data that
consist of approximately 300M of running words.
We can observe that the perplexity of the BOLM
estimated on sampled texts (generated texts) de-
creases when the number of sample sentences in-
creases, and tends to reach slowly the perplex-
ity of the baseline BOLM. Moreover, when both
BOLMs are interpolated, an even lower perplex-
ity is obtained, which further decreases with the
amount of sampled training texts.
6.2 Translation results
Experiments are run for translation into German,
which lacks a GigaWord corpus. An artificial cor-
pus containing 3 billions of running words is first
generated as described in Section 6.1. This corpus
is used to estimate a BOLM with standard settings,
that is then used for decoding, thereby approxi-
mating the use of a NNLM during the first pass.
Results reported in Table 4 show that adding gen-
erated texts improves the BLEU scores even when
the SOUL model is added in a rescoring step. Also
note that using the LM trained on the sampled cor-
pus yields the same BLEU score that using the
standard LM.
66
 190 200 210 220 230 240 250 260
 270 280
 2  4  6  8  10  12ppx times 400M sampled sentences
artificial textsartificial texts+stdstd
Figure 2: Perplexity measured on nt08 with the
baseline LM (std), with the LM estimated on the
sampled texts (generated texts), and with the inter-
polation of both.
Therefore, to translate from English to German,
the submitted system includes three BOLMs: one
trained on all the monolingual data, one on artifi-
cial texts and a third one that uses the freely avail-
able deWack corpus3 (1.7 billion words).
target LM BLEU
dev nt09 test nt10
base 15.3 16.5
+genText 15.5 16.8
+SOUL 16.4 17.6
+genText+SOUL 16.5 17.8
Table 4: Impact of the use of sampled texts.
7 Different tunings for different original
languages
As shown by Lembersky et al (2012), the original
language of a text can have a significant impact on
translation performance. In this section, this effect
is assessed on the French to English translation
task. Training one SMT system per original lan-
guage is impractical, since the required informa-
tion is not available for most of parallel corpora.
However, metadata provided by the WMT evalua-
tion allows us to split the development and test sets
according to the original language of the text. To
ensure a sufficient amount of texts for each con-
dition, we used the concatenation of newstest cor-
pora for the years 2008, 2009, 2011, and 2012,
leaving nt10 for testing purposes.
Five different development sets have been cre-
ated to tune five different systems. Experimental
results are reported in Table 7 and show a drastic
3http://wacky.sslmit.unibo.it/doku.php
baseline adapted
original language tuning
cz 22.31 23.83
en 36.41 39.21
fr 31.61 32.41
de 18.46 18.49
es 30.17 29.34
all 29.43 30.12
Table 5: BLEU scores for the French-to-English
translation task measured on nt10 with systems
tuned on development sets selected according to
their original language (adapted tuning).
improvement in terms of BLEU score when trans-
lating back to the original English and a significant
increase for original text in Czech and French. In
this year?s evaluation, Russian was introduced as
a new language, so for sentences originally in this
language, the baseline system was used. This sys-
tem is used as our primary submission to the eval-
uation, with additional SOUL rescoring step.
8 Conclusion
In this paper, we have described our submis-
sions to the translation task of WMT?13 for
the French-English, German-English and Spanish-
English language pairs. Similarly to last year?s
systems, our main submissions use n-code, and
continuous space models are introduced in a post-
processing step, both for translation and target lan-
guage modeling. To translate from English to
German, we showed a slight improvement with
a tighter integration of the continuous space lan-
guage model using a text sampling strategy. Ex-
periments with pre-ordering were disappointing,
and the reasons for this failure need to be better
understood. We also explored the impact of using
different tuning sets according to the original lan-
guage of the text to be translated. Even though the
gain vanishes when adding the SOUL model in a
post-processing step, it should be noted that due to
time limitation this second step was not tuned ac-
cordingly to the original language. We therefore
plan to assess the impact of using different tuning
sets on the post-processing step.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
67
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard Un iversity.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Marin?o.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, and
Kenneth Church. 2013. Approximate inference: A
sampling based modeling technique to capture com-
plex dependencies in a language model. Speech
Communication, 55(1):162 ? 177.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012b. Measuring the influence of long range de-
pendencies with neural network language models.
In Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages 1?
10, Montre?al, Canada.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012c. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012. Language models for machine trans-
lation: Original vs. translated texts. Comput. Lin-
guist., 38(4):799?825, December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Proceed-
ings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, AMTA
?02, pages 135?144, Tiburon, CA, USA. Springer-
Verlag.
68
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02:
Proc. of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318. Associ-
ation for Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. of Interna-
tional Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ?11, pages
1017?1024, New York, NY, USA, June. ACM.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
69
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 398?404,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI Submission for the WMT?13 Quality Estimation Task: an
Experiment with n-gram Posteriors
Anil Kumar Singh
LIMSI
Orsay, France
anil@limsi.fr
Guillaume Wisniewski
Universite? Paris Sud
LIMSI
Orsay, France
wisniews@limsi.fr
Franc?ois Yvon
Universite? Paris Sud
LIMSI
Orsay, France
yvon@limsi.fr
Abstract
This paper describes the machine learning
algorithm and the features used by LIMSI
for the Quality Estimation Shared Task.
Our submission mainly aims at evaluating
the usefulness for quality estimation of n-
gram posterior probabilities that quantify
the probability for a given n-gram to be
part of the system output.
1 Introduction
The dissemination of statistical machine transla-
tion (SMT) systems in the professional translation
industry is still limited by the lack of reliability of
SMT outputs, the quality of which varies to a great
extent. In this context, a critical piece of informa-
tion would be for MT systems to assess their out-
put translations with automatically derived quality
measures. This problem is the focus of a shared
task, the aim of which is to predict the quality
of a translation without knowing any human ref-
erence(s).
To the best of our knowledge, all approaches
so far have tackled quality estimation as a super-
vised learning problem (He et al, 2010; Soricut
and Echihabi, 2010; Specia et al, 2010; Specia,
2011). A wide variety of features have been pro-
posed, most of which can be described as loosely
?linguistic? features that describe the source sen-
tence, the target sentence and the association be-
tween them (Callison-Burch et al, 2012). Sur-
prisingly enough, information used by the decoder
to choose the best translation in the search space,
such as its internal scores, have hardly been con-
sidered and never proved to be useful. Indeed, it is
well-known that these scores are hard to interpret
and to compare across hypotheses. Furthermore,
mapping scores of a linear classifier (such as the
scores estimated by MERT) into consistent prob-
abilities is a difficult task (Platt, 2000; Lin et al,
2007).
This work aims at assessing whether informa-
tion extracted from the decoder search space can
help to predict the quality of a translation. Rather
than using directly the decoder score, we propose
to consider a finer level of information, the n-gram
posterior probabilities that quantifies the probabil-
ity for a given n-gram to be part of the system
output. These probabilities can be directly inter-
preted as the confidence the system has for a given
n-gram to be part of the translation. As they are
directly derived from the number of hypotheses in
the search space that contains this n-gram, these
probabilities might be more reliable than the ones
estimated from the decoder scores.
We first quickly review, in Section 2, the n-gram
posteriors introduced by (Gispert et al, 2013) and
explain how they can be used in the QE task; we
then describe, in Section 3 the different systems
that have developed for our participation in the
WMT?13 shared task on Quality Estimation and
assess their performance in Section 4.
2 n-gram Posterior Probabilities in SMT
Our contribution to the WMT?13 shared task on
quality estimation relies on n-gram posteriors. For
the sake of completeness, we will quickly formal-
ize this notion and summarize the method pro-
posed by (Gispert et al, 2013) to efficiently com-
pute them. We will then describe preliminary ex-
periments to assess their usefulness for predicting
the quality of a translation hypothesis.
2.1 Computing n-gram Posteriors
For a given source sentence F , the n-gram pos-
terior probabilities quantifies the probability for
a given n-gram to be part of the system output.
Their computation relies on all the hypotheses
considered by a SMT system during decoding: in-
tuitively, the more hypotheses a n-gram appears
in, the more confident the system is that this n-
gram is part of the ?correct? translation, and the
398
higher its posterior probability is. Formally, the
posterior of a given n-gram u is defined as:
P (u|E) =
?
(A,E)?E
?u(E) ? P (E,A|F )
where the sum runs over the translation hypothe-
ses contained in the search space E (generally rep-
resented as a lattice); ?u(E) has the value 1 if u
occurs in the translation hypothesis E and 0 oth-
erwise and P (E,A|F ) is the probability that the
source sentence F is translated by the hypothesis
E using a derivation A. Following (Gispert et al,
2013), this probability is estimated by applying a
soft-max function to the score of the decoder:
P (A,E|F ) = exp (??H(E,A, F ))?
(A?,E?)?E exp (H(E?, A?, F ))
where the decoder score H(E,A, F ) is typically
a linear combination of a handful of features, the
weights of which are estimated by MERT (Och,
2003).
n-gram posteriors therefore aggregate two
pieces of information: first, the number of paths in
the lattice (i.e. the number of translation hypothe-
ses of the search path) the n-gram appears in; sec-
ond, the decoder scores of these paths that can be
roughly interpreted as a quality of the path.
Computing P (u|E) requires to enumerate all n-
gram contained in E and to count the number of
paths in which this n-gram appears at least once.
An efficient method to perform this computation
in a single traversal of the lattice is described
in (Gispert et al, 2013). This algorithm has been
reimplemented1 to generate the posteriors used in
this work.
2.2 Analysis of n-gram Posteriors
Figure 1 represents the distribution of n-gram pos-
teriors on the training set of the task 1-1. This dis-
tribution is similar to the ones observed for task 1-
3 and for higher n-gram orders. It appears that, the
distribution is quite irregular and has two modes.
The minor modes corresponds to n-grams that ap-
pear in almost every translation hypotheses and
have posterior probability close to 1. Further anal-
yses show that these n-grams are mainly made of
stop words and of out-of-vocabulary words. The
major mode corresponds to very small n-gram
posteriors (less than 10?1) that the system has only
1Our implementation can be downloaded from http://
perso.limsi.fr/Individu/wisniews/.
a very small confidence in producing. The num-
ber of n-grams that have such a small posterior
suggests that most n-grams occur only in a small
number of paths.
0 5 10 15 20 250
.00
0.0
5
0.1
0
0.1
5
0.2
0
? logP (u|E)
De
nsi
ty
Figure 1: Distribution of the unigram posteriors
observed on the training set of the task 1-1
Using n-gram posteriors to predict the quality
of translation raises a representation issue: the
number of n-grams contained in a sentence varies
with the sentence length (and hence with the num-
ber of posteriors) but this information needs to be
represented in a fixed-length vector describing the
sentence. Similarly to what is usually done in the
quality estimation task, we chose to represent pos-
teriors probability by their histogram: for a given
n-gram order, each posterior is mapped to a bin;
each bin is then represented by a feature equal to
the number of n-gram posteriors it contains. To
account for the irregular distribution of posteriors,
bin breaks are chosen on the training set so as to
ensure that each bin contains the same number of
examples. In our experiments, we considered a
partition of the training data into 20 bins.
3 Systems Description
LIMSI has participated to the tasks 1-1 (predic-
tion of the hTER) and 1-3 (prediction of the post-
edition time). Similar features and learning algo-
rithms have been considered for the two tasks. We
will first quickly describe them before discussing
the specific development made for task 1-3.
399
3.1 Features
In addition to the features described in the previ-
ous section, 176 ?standard? features for quality es-
timation have been considered. The full list of fea-
tures we have considered is given in (Wisniewski
et al, 2013) and the features set can be down-
loaded from our website.2 These features can be
classified into four broad categories:
? Association Features: Measures of the qual-
ity of the ?association? between the source
and the target sentences like, for instance,
features derived from the IBM model 1
scores;
? Fluency Features: Measures of the ?fluency?
or the ?grammaticality? of the target sentence
such as features based on language model
scores;
? Surface Features: Surface features extracted
mainly from the source sentence such as
the number of words, the number of out-
of-vocabulary words or words that are not
aligned;
? Syntactic Features: some simple syntactic
features like the number of nouns, modifiers,
verbs, function words, WH-words, number
words, etc., in a sentence;
These features sets differ, in several ways, from
the baseline feature set provided by the shared task
organizers. First, in addition to features derived
from a language model, it also includes several
features based on large span continuous space lan-
guage models (Le et al, 2011). Such language
models have already proved their efficiency both
for the translation task (Le et al, 2012) and the
quality estimation task (Wisniewski et al, 2013).
Second, each feature was expanded into two ?nor-
malized forms? in which their value was divided
either by the source length or the target length
and, when relevant, into a ?ratio form? in which
the feature value computed on the target sentence
is divided by its value computed in the source sen-
tence. At the end, when all possible feature expan-
sions are considered, each example is described by
395 features.
2http://perso.limsi.fr/Individu/
wisniews/
3.2 Learning Methods
The main focus of this work is to study the rel-
evance of features for quality estimation; there-
fore, only very standard learning methods were
used in our work. For this year submission
both random forests (Breiman, 2001) and elas-
tic net regression (Zou and Hastie, 2005) have
been used. The capacity of random forests to take
into account complex interactions between fea-
tures has proved to be a key element in the re-
sults achieved in our experiments with last year
campaign datasets (Zhuang et al, 2012). As we
are considering a larger features set this year and
the number of examples is comparatively quite
small, we also considered elastic regression, a lin-
ear model trained with L1 and L2 priors as regu-
larizers, hoping that training a sparse model would
reduce the risk of overfitting.
In this study, we have used the implementation
provided by scikit-learn (Pedregosa et al,
2011). As detailed in Section 4.1, cross-validation
has been used to choose the hyper-parameters of
all regressors, namely the number of estimators,
the maximal depth of a tree and the minimum
number of examples in a leaf for the random
forests and the importance of the L1 and the L2
regularizers for the elastic net regressor.
3.3 System for Task 1-3
Like task 1-1, task 1-3 is a regression task that
aims at predicting the time needed to post-edit a
translation hypothesis. From a machine learning
point of view, this task differs from task 1-1 in
three aspects. First, the distributed training set
is much smaller: it is made of only 803 exam-
ples, which increases the risk of overfitting. Sec-
ond, contrary to hTER scores, post-edition time is
not normalized and the label of this task can take
any positive value. Finally and most importantly,
as shown in Figure 2, the label distributions es-
timated on the training set has a long tail which
indicates the presence of several outliers: in the
worse case, it took more than 18 minutes to cor-
rect a single sentence made of 35 words! Such
a long post-edition time most certainly indicates
that the corrector has been distracted when post-
editing the sentence rather than a true difficulty in
the post-edition.
These outliers have a large impact on training
and on testing, as their contributions to both MAE
400
0 200 400 600 800 1000 12000.
00
0
0.0
02
0.0
04
0.0
06
0.0
08
Post-edition time (s)
De
nsi
ty
Figure 2: Kernel density estimate of the post-
edition time distribution used as label in task 1-3.
and MSE,3 directly depends on label values and
can therefore be very large in the case of outliers.
For instance, a simple ridge regression with the
baseline features provided by the shared task or-
ganizer achieves a MAE of 42.641 ? 2.126 on
the test set. When all the examples having a la-
bel higher than 300 are removed from the training
set, the MAE drops to 41.843? 4.134. When out-
liers are removed from both the training and the
test sets, the MAE further drops to 32.803?1.673.
These observations indicate that special care must
be taken when collecting the data and that, maybe,
post-edition times should be clipped to provide a
more reliable estimation of the predictor perfor-
mance.
In the following (and in our submission) only
examples for which the post-edition time was less
than 300 seconds were considered.
4 Results
4.1 Experimental Setup
We have tested different combinations of features
and learning methods using a standard metric for
regression: Mean Absolute Error (MAE) defined
by:
MAE = 1n
n?
i=1
|y?i ? yi|
3The two standard loss functions used to train and evalu-
ate a regressor
where n is the number of examples, yi and y?i
the true label and predicted label of the ith exam-
ple. MAE can be understood as the averaged error
made in predicting the quality of a translation.
Performance of both task 1-1 and task 1-34 was
also evaluated by the Spearman rank correlation
coefficient ? that assesses how well the relation-
ship between two variables can be described using
a monotonic function. While the value of the cor-
relation coefficient is harder to interpret as it not
directly related to the value to predict, it can be
used to compare the performance achieved when
predicting different measures of the post-editing
effort. Indeed, several sentence-level (or docu-
ment level) annotation types can be used to reflect
translation quality (Specia, 2011), such as the time
needed to post-edit a translation hypothesis, the
hTER, or qualitative judgments as it was the case
for the shared task of WMT 2012. Comparing di-
rectly these different settings is complicated, since
each of them requires to optimize a different loss,
and even if the losses are the same, their actual
values will depend on the actual annotation to be
predicted (refer again to the discussion in (Specia,
2011, p5)). Using a metric that relies on the pre-
dicted rank of the example rather than the actual
value predicted allows us to directly compare the
performance achieved on the two tasks.
As the labels for the different tasks were not re-
leased before the evaluation, all the reported re-
sults are obtained on an ?internal? test set, made of
20% of the data released by the shared task or-
ganizers as ?training? data. The remaining data
were used to train the regressor in a 10 folds cross-
validation setting. In order to get reliable estimate
of our methods performances, we used bootstrap
resampling (Efron and Tibshirani, 1993) to com-
pute confidence intervals of the different scores:
10 random splits of the data into a training and
sets were generated; a regressor was then trained
and tested for each of these splits and the resulting
confidence intervals at 95% computed.
4.2 Results
Table 1and Table 2 contain the results achieved by
our different conditions. We used, as a baseline,
the set of 17 features released by the shared task
organizers.
It appears that the differences in MAE between
4The Spearman ? was an official metric only for task 1-
1. For reasons explained in this paragraph, we also used it to
evaluate our results for task 1-3.
401
the different configurations are always very small
and hardly significant. However, the variation of
the Spearman ? are much larger and the difference
observed are practically significant when the inter-
pretation scale of (Landis and Koch, 1977) is used.
We will therefore mainly consider ? in our discus-
sion.
For the two tasks 1-1 and 1-3, the features we
have designed allow us to significantly improve
prediction performance in comparison to the base-
line. For instance, for task 1-1, the correlation
is almost doubled when the features described in
Section 3.1 are used. As expected, random forests
are overfitting and did not manage to outperform
a simple linear classifier. That is why we only
used the elastic net method for our official submis-
sion. Including posterior probabilities in the fea-
ture set did not improve performance much (ex-
cept when only the baseline features are consid-
ered) and sometimes even hurt performance. This
might be caused by an overfitting problem, the
training set becoming too small when new features
are added. We are conducting further experiments
to explain this paradoxical observation.
Another interesting observation that can be
made looking at the results of Table 1 and Ta-
ble 2 is that the prediction of the post-edition time
seems to be easier than the prediction of the hTER:
using the same classifiers and the same features,
the performance for the former task is always far
better than the performance for the latter.
5 Conclusion
In this paper, we described our submission to the
WMT?13 shared task on quality estimation. We
have explored the use of posteriors probability,
hoping that information about the search space
could help in predicting the quality of a transla-
tion. Even if features derived from posterior prob-
abilities have shown to have only a very limited
impact, we managed to significantly improve the
baseline with a standard learning method and sim-
ple features. Further experiments are required to
understand the reasons of this failure.
Our results also highlight the need to continue
gathering high-quality resources to train and in-
vestigate quality estimation systems: even when
considering few features, our systems were prone
to overfitting. Developing more elaborated sys-
tems will therefore only be possible if more train-
ing resource is available. Our experiments also
stress that both the choice of the quality measure
(i.e. the quantity to predict) and of the evaluation
metrics for quality estimation are still open prob-
lems.
6 Acknowledgments
This work was partly supported by ANR
projects Trace (ANR-09-CORD-023) and Tran-
sread (ANR-12-CORD-0015).
References
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
B. Efron and R. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman and Hall/CRC Mono-
graphs on Statistics and Applied Probability Series.
Chapman & Hall.
Adria` Gispert, Graeme Blackwood, Gonzalo Iglesias,
and William Byrne. 2013. N-gram posterior prob-
ability confidence measures for statistical machine
translation: an empirical study. Machine Transla-
tion, 27(2):85?114.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 622?630, Uppsala, Sweden, July.
Association for Computational Linguistics.
R. J. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
Output Layer Neural Network Language Model.
In Proceedings of IEEE International Conference
on Acoustic, Speech and Signal Processing, pages
5524?5527, Prague, Czech Republic.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
402
MAE ?
train test train test
Baseline Features
RandomForest 0.109? 0.013 0.130? 0.004 0.405? 0.008 0.314? 0.016
Elastic 0.127? 0.001 0.129? 0.003 0.336? 0.004 0.319? 0.015
?Linguistic? Features
RandomForest 0.082? 0.019 0.118? 0.003 0.689? 0.003 0.625? 0.009
Elastic 0.107? 0.004 0.115? 0.003 0.705? 0.009 0.660? 0.009
?Linguistic? Features + posteriors
RandomForest 0.088? 0.017 0.116? 0.003 0.694? 0.003 0.615? 0.014
Elastic 0.105? 0.006 0.114? 0.002 0.699? 0.007 0.662? 0.011
Table 1: Results for the task 1-1
MAE ?
train test train test
Baseline Features
RandomForest 25.145? 3.745 33.279? 1.687 0.669? 0.007 0.639? 0.017
Elastic 32.776? 0.795 33.702? 2.328 0.678? 0.006 0.657? 0.018
Baseline Features + Posteriors
RandomForest 33.707? 0.309 35.646? 0.889 0.674? 0.004 0.637? 0.017
Elastic 31.487? 0.261 32.922? 0.789 0.698? 0.004 0.681? 0.016
?Linguistic? Features
RandomForest 25.236? 4.400 33.017? 1.582 0.735? 0.007 0.666? 0.023
Elastic 28.706? 1.273 31.630? 1.612 0.760? 0.006 0.701? 0.017
?Linguistic? Features + Posteriors
RandomForest 22.951? 3.903 33.013? 1.514 0.741? 0.003 0.695? 0.013
Elastic 28.911? 1.020 31.865? 1.636 0.761? 0.008 0.710? 0.017
Table 2: Results for the task 1-3
403
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007. A note on platt?s probabilistic outputs for
support vector machines. Mach. Learn., 68(3):267?
276, October.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
John C. Platt, 2000. Probabilities for SV Machines,
pages 61?74. MIT Press.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine translation evaluation versus quality esti-
mation. Machine Translation, 24(1):39?50, March.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of the 15th conference of EAMT, pages 73?
80, Leuven, Belgium.
Guillaume Wisniewski, Anil Kumar Singh, and
Franc?ois Yvon. 2013. Quality estimation for ma-
chine translation: Some lessons learned. Machine
Translation. accepted for publication.
Yong Zhuang, Guillaume Wisniewski, and Franc?ois
Yvon. 2012. Non-linear models for confidence es-
timation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 157?162,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
404
