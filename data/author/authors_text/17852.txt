Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 14?24, Dublin, Ireland, August 23-29 2014.
Cross-lingual Coreference Resolution of Pronouns
Michal Nov
?
ak and Zden
?
ek
?
Zabokrtsk?y
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, CZ-11800
{mnovak,zabokrtsky}@ufal.mff.cuni.cz
Abstract
This work is, to our knowledge, a first attempt at a machine learning approach to cross-lingual
coreference resolution, i.e. coreference resolution (CR) performed on a bitext. Focusing on CR
of English pronouns, we leverage language differences and enrich the feature set of a standard
monolingual CR system for English with features extracted from the Czech side of the bitext.
Our work also includes a supervised pronoun aligner that outperforms a GIZA++ baseline in
terms of both intrinsic evaluation and evaluation on CR. The final cross-lingual CR system has
successfully outperformed both a monolingual CR and a cross-lingual projection system.
1 Introduction
Coreference resolution (CR) is a well-established task in the field of Natural Language Processing (NLP).
The majority of papers published so far has focused on the monolingual CR, mostly experimenting on
the English data. An important step towards multilingual CR was the CoNLL-2012 Shared Task in
Modeling Multilingual Unrestricted Coreference in OntoNotes, where the participants were asked to
build a CR system that could be applied on three typologically different languages contained in the
OntoNotes corpus (Hovy et al., 2006): English, Chinese, and Arabic.
Same just as in other NLP tasks such as part-of-speech tagging or parsing, recent years have witnessed
a rising interest in cross-lingual projection techniques, mostly aiming at under-resourced languages.
However, little attention is paid to leveraging cross-lingual information for CR in two resource-rich
languages. This is probably due to lack of bilingual resources annotated with coreference since such
techniques would require rich linguistic annotation on both sides of the bitext. Moreover, to solve this
issue using a supervised learner, one needs the gold standard of coreference at least on the target side of
the bitext. On the other hand, given such data, the typological differences in languages can be exploited
to aid a CR system to perform better than if CR is performed independently for each language.
The motivation for solving this task is threefold. Firstly, even though Statistical Machine Translation
(SMT) has been attracting interest of the community for years, most systems do not take information be-
yond the sentence boundary into account, leaving the issues of discourse coherence unresolved. Having
a better-quality bitext with coreference resolved could drive research in discourse-aware SMT forward.
Secondly, although inter-sentential relations are neglected in SMT, current phrase-based system uninten-
tionally resolve some of the coreference links within the sentence, using just the power of phrases. This
might be leveraged by using the SMT output instead of a human-translated output in a cross-lingual CR
scenario. Finally, even monolingual CR may be improved by applying semi-supervised learning methods
in a smart way on a large bilingual corpus with automatic rich annotations, such as CzEng 1.0 (Bojar et
al., 2012).
Our work examines cross-lingual CR on the Czech-English language pair. We focus on CR of English
pronouns, particularly the 3rd person central pronouns. Central pronouns is a term coined by Quirk
(1985) embracing personal, possessive and reflexive pronouns. For the sake of simplicity, we will denote
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
14
3rd person central pronouns by the word pronouns in the following. We ignore noun phrase coreference
for two reasons. First, there has been no data set available for the Czech-English language pair with
noun phrase coreference annotated, yet. Second, the language differences between languages show more
clearly on pronouns than on nouns, as pronouns tend to be more constrained by various grammar rules
across different languages.
Czech and English are typologically distant languages, which is also reflected in different behavior of
pronouns. A cross-lingual CR system could substantially benefit from the necessity of the anaphor and its
antecedent to agree in gender. Czech uses grammatical genders which are more evenly distributed among
nouns than the notional genders
1
used in English, where male and female gender
2
are solely allocated
to living objects. However, benefiting from the pronoun?s gender becomes problematic for personal
pronouns in subject position which are usually dropped from the surface representation in Czech. If their
governing verb is in the past tense, the correct gender can be reconstructed from its form. With the verb in
present or future tense, the pronoun?s gender remains hidden. Possessive pronouns are used to a greater
extent in English than in Czech. Same as articles, they play the role of determiners whereas in Czech,
the determination and possession must be understood from the context. A missing Czech counterpart of
an English possessive pronoun may indicate its antecedent to be in the same sentence. Moreover, Czech
uses reflexive possessive pronouns, whose antecedent is easier to detect than for non-reflexive pronouns.
On the other hand, English reflexive pronouns, unlike the Czech, carry gender and number information
the resolver can benefit from.
In this work, we make to our knowledge a first attempt to leverage the language differences using
a machine learning approach to improve CR on bitexts. To achieve this goal, we create a supervised
CR model, proposing two sets of cross-lingual features: projected features used for Czech CR and an
indicator feature of a projected Czech coreference link obtained by a Czech CR system. Note that for the
latter set (actually comprising only a single feature), the Czech CR system would require gold annotation
of Czech coreference. We did not consider new features that would address specific Czech-English
correspondences.
The fact that a Czech counterpart is missing for many English pronouns has a negative effect on
traditional unsupervised alignment approaches. We address this issue by a supervised aligner of pronouns
that incorporates the result of the traditional aligner as a feature and adds other features that help detect
the true Czech counterparts of English pronouns.
The structure of this paper is as follows: After introducing related work in Section 2 and describing the
data used in experiments in Section 3, we present the design of a supervised approach to improve English
pronoun alignment in Section 4. Section 5 describes the cross-lingual CR system and the experiments
conducted with it. Finally, we discuss the main observations made in the experiments in Section 6 and
conclude the paper in Section 7.
2 Related work
The task of coreference resolution has been studied for a few decades, with supervised systems dominat-
ing the field. The most popular approaches have been thoroughly summarized by Ng (2010).
The system for English CR we use has been built for automatic coreference annotation in the Czech-
English parallel treebank CzEng 1.0 (Bojar et al., 2012). It is an implementation of the so-called mention
ranking model, first introduced by Denis and Baldridge (2007).
Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging
(Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012),
and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with
parsing. Joint parsing of both the source and the target text along with searching for the best alignment
between the trees has been approached in a more (Burkett et al., 2010) or less (Smith and Smith, 2004;
Burkett and Klein, 2008) integrated approach. However, much closer to our work is the research on
1
?Nouns are classified semantically according to their coreferential relations with personal, reflexive and wh-pronouns.?
(Quirk et al., 1985, p.314)
2
Quirk (1985) uses these terms instead of terms masculine and feminine related to grammatical gender.
15
bilingually-informed parsing by Haulrich (2012), in which English trees are used to enrich the feature
set for a Danish parser and vice-versa. Rosa et al. (2012) explored the same approach on the Czech-
English language pair. Moreover, they adapted this technique to parse the output of an SMT system.
As for coreference resolution in a bilingual scenario, most works focus on coreference projection (de
Souza and Orsan, 2011; Rahman and Ng, 2012; Ogrodniczuk, 2013). Research on cross-lingual CR has
been inhibited by the lack of coreference-annotated parallel corpora. There are only few such corpora, for
instance an English-Romanian corpus containing full hand-annotated coreference chains including noun
phrase coreference (Postolache et al., 2006) and two corpora with pronoun coreference annotations ?
Prague Czech-English Dependency Treebank 1.0 (Haji?c et al., 2012, PCEDT) and the recently published
English-German corpus ParCor 1.0 (Guillou et al., 2014).
However, the only attempts at cross-lingual CR date back to the time before these corpora were re-
leased. Harabagiu and Maiorano (2000) designed a CR system for English-Romanian bitexts while
Mitkov and Barbu (2003) focused on the English-French language pair. Both extended their rule-based
monolingual CR systems to apply some high-precision rules from one language to enhance the result
in the other language. They both reported an improvement of about 4% in precision compared to the
monolingual systems.
As concerns a machine learning approach, in the work by Veselovsk?a et al. (2012), PCEDT was
employed in related tasks ? to identifying types of the English personal pronoun it and Czech types of
the unexpressed subject. The tasks have been addressed by the isolated monolingual systems as well as
by taking advantage of the features from the other language.
3 Main source of the data
As mentioned in Section 2, Czech is one of a few languages for which a coreference-annotated parallel
corpus has been built ? The Prague Czech-English Dependency Treebank (Haji?c et al., 2012, PCEDT).
3
PCEDT is a manually annotated Czech-English parallel treebank comprising over 1.2 million words
for each language in almost 50,000 sentence pairs. The English part contains the entire Penn Treebank?
Wall Street Journal Section (Linguistic Data Consortium, 1999) transformed into dependency trees,
whereas the Czech part comprises the translations of all the texts from the English part. The data from
both parts are annotated on three layers of linguistic description following the Prague tectogrammatics
theory (Sgall, 1967; Sgall et al., 1986) ? the morphological layer (where each token from the sentence
gets a lemma and a POS tag), the analytical layer (surface syntax in the form of a dependency tree, where
each node corresponds to a token in the sentence) and the tectogrammatical layer. Tectogrammatical rep-
resentation of a sentence is a dependency tree, where only content words have their own nodes; on the
other hand, it contains additional nodes, e.g., for pronouns unexpressed on the surface. This is also the
layer where the coreference relations are annotated. PCEDT includes annotation of pronoun coreference
and the so-called grammatical coreference
4
for Czech as well as English.
For the purpose of this work, we ignore all annotations originally provided by PCEDT. Annotations
on the tectogrammatical layer, which is in the center of this work?s attention, are mostly manual there.
But to truly simulate the real-world scenario when given just a pair of parallel texts, we need to replace
them with ones carried out in a fully automatic manner. The only two exceptions, where we employ the
gold annotations, are the relations we aim to model, i.e. coreference links and our own annotation of
alignment for English personal pronouns (see Section 4.1).
3.1 Fully Automatic Annotation
We have conducted automatic linguistic analysis on both the English and the Czech part of PCEDT,
transforming the individual sentences into multi-layer dependency tree structures based on the Prague
tectogrammatics theory. The analysis was carried out within the Treex framework (Popel and
?
Zabokrtsk?y,
2010).
3
http://hdl.handle.net/11858/00-097C-0000-0015-8DAF-4
4
Its antecedent is imposed by the grammar of the language, e.g. coreference of relative pronouns.
16
Treex is a multi-purpose open-source framework for NLP applications development, which integrates
a wide range of modules, such as tools for sentence splitting, tokenization, morphological analysis,
part-of-speech tagging, shallow and deep syntax parsing, named entity recognition, anaphora resolution,
among others.
Moreover, we performed an unsupervised word alignment on the complete PCEDT using the
MGIZA++ tool (Gao and Vogel, 2008), which is a multi-threaded version of the popular GIZA++ (Och
and Ney, 2000) that supports applying a saved model on a new sentence pair. We used a model trained
on CzEng 1.0, which is about 300 times bigger in terms of the number of sentence pairs. The resulting
alignment of the intersection and grow-diag-final-and types was subsequently projected onto the
tectogrammatical layer. Furthermore, a simple heuristic was applied to find the English counterparts for
reconstructed Czech personal pronouns. We denote this alignment as the original in the following.
4 Supervised alignment
The alignment described in the previous section is sufficiently accurate for content words, such as verbs,
nouns, and adjectives. However, errors become more frequent as we move to pronouns. Some reasons
for this have already been outlined in Section 1, i.e. dropped subject personal pronouns and omitted
possessive pronouns in Czech. In addition, English uses a pleonastic variant of the pronoun it, which
also has no correspondence in Czech. Personal pronouns function in a sentence as a replacement of
nouns. Thus, it is no exception if a pronoun is translated into a noun. And finally, the translation may be
reworded to such an extent that the pronoun would carry no valuable information, and it disappears. All
these cases are difficult for GIZA++ to tackle.
The pronoun correspondence problem has been already faced concerning the alignment of the personal
pronoun it by Nov?ak et al. (2013). The authors tried to find the Czech counterpart of it by taking the
node that is aligned to the parent of it on the Czech side and picking the argument of the aligned node
that agrees on the semantic role with the particular it. This approach assumed that the unsupervised
alignment of the parent, which is likely to be a content word, is of higher quality than the alignment of
it itself. Furthermore, it relied on high-accuracy semantic role labeling, which could only be justified
because the experiments were conducted on data manually annotated with semantic roles.
As we are working with fully automatic annotations (i.e., much less reliable) and a wider range of
words to align, we cannot just copy this rule-based approach. However, we can take a more robust
approach of supervised machine learning and transform Nov?ak et al.?s rule to one of the features in our
alignment model.
In Section 4.1, we describe the manual annotation of alignment, then introduce the supervised model
in Section 4.2, using features described in Section 4.3. Finally, we show the evaluation results of the
alignment model in Section 4.4.
4.1 Manual Annotation of the Data
Supervised learning requires that the training data are manually labeled with a target variable. For this
purpose, we set aside the section 19 of PCEDT. In this data, all occurrences of English personal pronouns
have been coupled with its Czech counterpart by one human annotator. If no suitable Czech expression
was found, the annotator identified a possible cause of the missing counterpart. The causes were then
categorized into three classes ? pleonastic it, missing possessive pronoun and missing correspondence
due to translation rewording. So far, we do not distinguish these classes in our models and treat them in
the same manner.
We managed to align 471 occurrences of personal pronouns, which account for over 50% of all occur-
rences in the section. The overall statistics of how English personal pronouns are translated into Czech
is shown in Table 1.
It shows that more than 55% of English personal pronouns are dropped from the surface representation
of the Czech sentence, though still present in its deep structure. In contrast, English pleonastic pronouns
are not present even there. An interesting observation is that more than half of English possessives
are either translated as reflexive possessives or completely missing in the Czech sentence. All these
17
CS\EN personal possessive reflexive Total
personal unexpressed 147 1 148
personal 37 2 39
demonstrative 17 1 18
noun 15 6 21
possessive 3 78 81
reflexive possessive 68 68
reflexive 1 2 5 8
other 6 1 3 10
pleonastic 24 24
reword 12 4 16
no possessive 38 38
Total 262 201 8 471
Table 1: The statistics on the correspondence of English personal pronouns to their Czech counterparts.
The last three Czech categories indicate the reason why there is no corresponding word in Czech for an
English pronoun.
phenomena might in the end be a source of helpful information to the CR system.
4.2 Model
The nature of the task of aligning a given English pronoun to its Czech counterpart is to pick the best-
fitting one from a bunch of candidates. The set of candidates consists of all tectogrammatical nodes in
the aligned Czech sentence. To allow the system to select no correspondence for a pronoun, we add a
special candidate representing the null alignment.
We represent the candidate ranking task as a discriminative log-linear model trained in a cost-sensitive,
one-against-all strategy with label-dependent features (csoaa-ldf) provided by the Vowpal Wabbit
5
machine learning toolkit. The feature weights are optimized by running stochastic gradient descent in 40
passes over the training data.
4.3 Features
The feature set consists of the following types of features, which consider an English pronoun and a
Czech candidate from the corresponding Czech tree:
? Original alignment features: presumably the most valuable set of features. It indicates if there is
a link between the two nodes in the original alignment and if there is any between their parents.
? Graph features: we designed these features to somehow reflect the distance between the nodes.
The pair of aligned tectogrammatical trees is treated as a bipartite graph and a shortest path between
the nodes is found using a sequence of dependency edges and a single alignment link. We applied
the Dijkstra algorithm to find the shortest path. We ensure that it only uses a single alignment
link by setting large weights to alignments and small weights to dependency edges, i.e., 100 and 1,
respectively. The features then comprise the length of the shortest path and the sequence of edge
labels (parent, child, alignment).
? Grammatical features: these include lemmas, part-of-speech tags, reflexivity indicators, semantic
role labels both for each of the nodes individually and as a concatenation of the two.
? Combined features: these features combine selected features from the types mentioned above. The
concatenation of parents? alignment and semantic role correspondence mimics the rule Nov?ak et al.
(2013) used to get better Czech counterparts for English it (see Section 4). Furthermore, features
combining lemmas with direct alignment or alignment through parents are included.
5
https://github.com/JohnLangford/vowpal_wabbit/wiki
18
Method Train Test
A P R F A P R F
ORIGINAL ? ? ? ? 73.04 75.55 82.40 78.83
SUPERVISED 88.37 90.18 90.34 90.26 84.50 88.52 86.40 87.45
Table 2: Evaluation results of English-to-Czech pronoun alignment. The quality is measured in terms of
accuracy (A), precision (P), recall (R), and F1-score (F).
4.4 Experiments and Results
The small amount of manually annotated data led us to evaluate alignment models by 10-fold cross-
validation, with the results on the train and test partitions averaged over all folds.
We measured the quality of produced the alignment links in terms of both accuracy and F1-score, i.e.,
as the harmonic mean of precision and recall. While accuracy positively scores also the cases when a
node is correctly labeled as having no alignment, precision and recall neglect these cases at all, thus
describing how good a method is in finding the correct counterpart for a node.
Table 2 shows the performance of the supervised model with the best combination of features and
learning method parameters and compares it to the original alignment described in Section 3.1. It shows
an improvement of about 9% absolute in terms of both accuracy and F-score.
5 Cross-lingual coreference resolver for English
In this section, we describe cross-lingual coreference resolution. The CR system we use definitely does
not aim to compete with current state-of-the-art systems. However, for the purpose of research on cross-
lingual CR, it can be employed as a reasonable baseline.
In Section 5.1, we describe the supervised CR model trained and tested on the data described in
Section 5.2. We elaborate more on the design of English and aligned features in Section 5.3 and Section
5.4, respectively. Finally, several variants of the CR system are evaluated and compared in Section 5.5.
5.1 Coreference model
Our resolver employs a supervised model denoted as mention ranker by Ng (2010). Its advantage lies in
judging all antecedent candidates simultaneously, and then picking the candidate with the highest score
as the predicted antecedent. However, it is unable to exploit features that describe already formed clusters
of mentions belonging to the same entity. A typical issue related to ranking models is how to deal with
non-anaphoric mentions. We use the approach introduced by Rahman and Ng (2009) ? adding a special
candidate that indicates no anaphor.
Since this work focuses only on the so-called pronoun resolution, all the anaphor candidates are En-
glish 3rd person central pronouns, i.e. personal, possessive and reflexive pronouns.
For every anaphor, we collect in the set of its antecedent candidates all semantic nouns
6
from the
previous sentence and the part of the current sentence prior to the anaphor.
CR can be treated as a ranking task, so we represent it in the same way as we handled alignment in
Section 3.1 ? as a discriminative log-linear model trained in the csoaa-ldf strategy by the Vowpal
Wabbit tool. The feature weights are optimized by running stochastic gradient descent in 20-80 passes
(the number differs across the experiments) over the training data.
5.2 Data
Models for coreference were trained on data extracted from sections 00?18 of the automatically analyzed
PCEDT (as described in Section 3). Sections 20?21 have been employed as development testing data
and Sections 22?24 as evaluation testing data. The development set has been used to select the best
configuration, which was subsequently tested on the evaluation set. The training, development, and
evaluation set consist of 19,294, 1,988 and 2,591 instances with 86%, 67%, and 73% anaphoric instances,
respectively.
6
Semantic nouns are all nouns as well as pronouns acting as a noun.
19
5.3 English Features
A wide range of features used by us had already been proven to be beneficial for the task of CR in
multiple prior works. The majority of the features presented here have already been used in the CR
system for Czech (Nguy et al., 2009); we keep just the language-independent. Furthermore, several
grammatical and positional features proposed by Charniak and Elsner (2009) have been added. Finally,
the feature set has been enriched with the information on named entities and WordNet
7
classes. All the
features disregard dependent members of a mention, describing just the head of the mention. They can
be divided into several categories:
? Distance features: number of sentences, clauses, and words between the anaphor and the an-
tecedent candidate; the order of the candidate,
? Grammatical features: morphological number and gender of both the anaphor and the antecedent
candidate, agreement in gender and number; part-of-speech tag,
? Function features: they exploit dependency labels on the analytical layer and semantic roles on
the tectogrammatical layer; they also include an indicator of whether the mention plays a role of an
argument or an adjunct in the governing phrase,
? Parent features: the features of both nodes? parents, e.g. their lemmas or semantic roles, are
compared; an indicator of whether a mention is in coordination,
? Semantic features: WordNet classes the head word is assigned to,
? Named entity features: the named entity category and subcategory returned by Stanford named
entity recognizer.
8
This includes also the indicator of whether the mention is a name of a person,
? Charniak features: anaphor type (pronoun in subject position, in object position, possessive pro-
noun, reflexive pronoun, other); antecedent type (noun, pronoun, other); antecedent syntactic type
(subject, object, prepositional phrase, other).
We denote this feature set as EN in all our experiments.
5.4 Alignment features
The features from the Czech nodes aligned to the given English anaphor and antecedent candidate are
obtained by moving to the corresponding Czech nodes and extracting the features as though we are trying
to resolve a Czech coreference link. As outlined in Section 1, we designed two sets of features: CS and
CS-COREF.
The CS set consists of features introduced by Nguy et. al (2009). Most of them, namely the categories
of distance, function, and parent features, are extracted in the same manner as the English ones in the
previous section. Grammatical features also contain the full positional morphological tag as designed by
Haji?c (2004). Semantic features employ a different knowledge base, replacing WordNet by the Czech
portion of EuroWordNet (Vossen, 1998). In addition to the features more or less shared with the English
side, the Czech feature set includes a probability estimate of the antecedent candidate co-occurring with
its governing verb. This statistics has been collected on Czech National Corpus (CNC, 2005).
The CS-COREF set consists of a single binary feature indicating if there is a coreference relation
between the nodes predicted by the Czech CR system (Nguy et al., 2009), or not.
7
http://wordnet.princeton.edu
8
http://nlp.stanford.edu/software/CRF-NER.shtml
20
5.5 Experiments and Results
The different feature sets proposed in the previous sections suggest an obvious set of experiments. The
system trained only on the monolingual EN features is put as a baseline.
The rest our experimental setups use alignment features, forming three combinations with EN features:
EN + CS, EN + CS-COREF, and EN + CS + CS-COREF. Moreover, these three experiments can be run
on the data provided either with the original or supervised alignment, which serves as extrinsic evaluation
of alignment approaches. This allows us to confirm or deny the hypothesis that the alignment plays a
significant role in cross-lingual CR (see Section 4).
For comparison, we also evaluated the system that simply projects coreference links obtained by the
Czech CR system to English.
The performance of a CR system is usually measured by scores that treat CR as a clustering problem,
e.g., MUC, B
3
, CEAF. As this work focuses merely on a subset of coreference expressions ? pronouns
? and we only compare different feature sets trained in the same framework, we resorted to the simplest
metrics with a sufficient expression power. For each English pronoun we test if its predicted antecedent
hits any of the true antecedents within the window of the current and the previous sentence. Given
this indicator we calculate precision, recall, and F1-score, which takes into account only the nodes for
which a relation with another node exists ? referential pronouns in this case (similarly to the alignment
evaluation in Section 4.4). Likewise, in order to assess quality of detecting non-referential pronouns,
accuracy is computed as well.
The final results are shown in Table 3. The overall higher numbers on the evaluation set than on the
development set probably result from a different proportion of non-anaphoric pronouns (see Section 5.2).
The smaller difference in F1-score than in accuracy also supports this explanation.
The coreference projection scores a great deal below the baseline, which suggests that this approach
is worth using only if manual annotation for at least a small amount of target language data (English in
our case) is extremely expensive.
As for the cross-lingual CR on the original alignment, all three feature set combinations have beaten
the baseline. The EN + CS-COREF system confirmed the added value of the CS-COREF feature, which,
unlike the CS feature set, conveys latent information on true Czech coreference links. Even the combi-
nation of all features performs worse than CS-COREF alone.
Moving to the experiments with supervised alignment, we can see the findings from Section 4.4 con-
firmed also in the extrinsic evaluation. All three systems outperform not only the baseline, but also all
the systems working on the original alignment. Moreover, both accuracy and F1-score order the three
feature combinations in the expected way, where the overall winner improves over the baseline in more
than 1% absolute. This improvement is significant
9
at p-level p ? 0.1 but not at p-level p ? 0.05.
6 Discussion
Using information from Czech parallel texts in English CR led to an improvement in terms of automatic
measures. To see what the main aspects in which the Czech text positively impacts the CR performance
are, we compared the output of the system trained only on the EN features with systems working on
the EN + CS and EN + CS-COREF feature sets. We used the results of the experiments run on the
development set with supervised alignment for this comparison.
Out of 1988 coreference instances in the development set, the EN + CS system improved the output
in 49 cases, while it worsened the output in 23 cases. The rest remained unchanged. Likewise, the EN +
CS-COREF system scored better than the EN one in 63 instances, while it failed in 39 instances.
The inspection of 10% instances for which the systems differed revealed that the cases when the
cross-lingual system scored better than the monolingual concur with the language differences described
in Section 1. We found that in these cases, the pronoun is often a pleonastic it or a possessive pronoun
with a Czech reflexive possessive counterpart. Finally, we noticed improvements in cases where the
Czech antecedent is easier to determine due to agreement in gender and number.
9
Significance has been calculated by bootstrap resampling using 100,000 samples.
21
Setup Train Dev Eval
A P R F A P R F A P R F
EN 79.13 80.12 86.00 82.96 60.97 60.28 79.14 68.43 63.72 63.28 78.78 70.19
Original alignment
CS-COREF projection 28.64 49.57 21.75 30.23 36.55 41.98 24.66 31.07 33.33 42.38 21.58 28.60
EN + CS-COREF 78.31 79.27 85.25 82.15 61.77 61.07 80.45 69.44 64.30 63.74 79.62 70.80
EN + CS 83.32 84.05 89.97 86.91 61.97 61.15 80.23 69.40 64.07 63.72 78.62 70.39
EN + CS + CS-COREF 80.75 81.52 87.61 84.46 62.27 61.33 80.96 69.79 64.03 63.59 79.57 70.69
Supervised alignment
CS-COREF projection 30.74 49.91 24.87 33.20 36.60 41.38 27.61 33.12 33.60 41.85 23.98 30.49
EN + CS 83.19 83.98 89.73 86.76 62.27 61.42 80.60 69.72 64.53 64.13 79.09 70.83
EN + CS-COREF 79.27 80.20 85.89 82.95 62.17 61.27 81.11 69.81 64.65 64.11 79.67 71.05
EN + CS + CS-COREF 81.99 82.78 88.53 85.56 62.68 61.59 81.62 70.20 64.69 64.38 79.67 71.22
Table 3: Evaluation results of monolingual CR, CR via projection, and cross-lingual CR system trained
and tested on the data with both the original and supervised alignment. Performance is measured in terms
of accuracy (A), precision (P), recall (R) and F1-score (F).
We did not encounter an example of improvement for an English possessive pronoun having no Czech
counterpart. We might have inspected too little data for it to appear. However, these cases may get
covered after the features combining English and Czech features will be introduced.
7 Conclusion
This work introduced a largely unexplored task in the field of CR ? cross-lingual CR. Given a Czech-
English bitext, we sought to improve the performance of an English pronoun CR system by enriching
the feature set with features from the aligned Czech text. Consistent improvements over the monolingual
system confirmed that cross-language differences in pronoun behavior are big enough to affect the result.
Furthermore, we have found that the quality of alignment is vital for this task.
In future work, we plan to apply this approach on a much larger parallel corpus and employ semi-
supervised techniques to improve cross-lingual as well as monolingual CR. Moreover, human translation
in the bitext can be replaced with the output of SMT system to see if we can produce valuable features
for CR from the machine-translated source text.
Acknowledgments
This work has been supported by the EU FP7 project Khresmoi (contract no. 257528). This work has
been using language resources developed and/or stored and/or distributed by the LINDAT/CLARIN
project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2010013).
We would like to thank our colleague Ond?rej Du?sek for language correction and three anonymous re-
viewers for their remarks and suggestions.
References
Ond?rej Bojar, Zden?ek
?
Zabokrtsk?y, Ond?rej Du?sek, Petra Galu?s?c?akov?a, Martin Majli?s, David Mare?cek, Ji?r?? Mar?s??k,
Michal Nov?ak, Martin Popel, and Ale?s Tamchyna. 2012. The joy of parallelism with CzEng 1.0. In Proceedings
of LREC 2012, Istanbul, Turkey. European Language Resources Association.
David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of
the Conference on Empirical Methods in Natural Language Processing, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized gram-
mars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, Stroudsburg, PA, USA. Association for Computational Linguistics.
Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the 12th
Conference of the European Chapter of the Association for Computational Linguistics, Stroudsburg, PA, USA.
Association for Computational Linguistics.
22
CNC. 2005. Czech national corpus ? SYN2005. Prague, Czech Republic. Institute of the Czech National Corpus.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies - Volume 1, Stroudsburg, PA, USA. Association for Computational Linguistics.
Jos?e Guilherme Camargo de Souza and Constantin Orsan. 2011. Can projected chains in parallel corpora help
coreference resolution? In Proceedings of the 8th International Conference on Anaphora Processing and
Applications, Berlin, Heidelberg. Springer-Verlag.
Pascal Denis and Jason Baldridge. 2007. A ranking approach to pronoun resolution. In Proceedings of the 20th
International Joint Conference on Artifical Intelligence, San Francisco, CA, USA.Morgan Kaufmann Publishers
Inc.
Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineer-
ing, Testing, and Quality Assurance for Natural Language Processing, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Liane Guillou, Christian Hardmeier, Aaron Smith, Jrg Tiedemann, and Bonnie Webber. 2014. ParCor 1.0: A
parallel pronoun-coreference corpus to support statistical MT. In Proceedings of the 9th International Confer-
ence on Language Resources and Evaluation (LREC-2014), Reykjavik, Iceland. European Language Resources
Association.
Jan Haji?c, Eva Haji?cov?a, Jarmila Panevov?a, Petr Sgall, Ond?rej Bojar, Silvie Cinkov?a, Eva Fu?c??kov?a, Marie
Mikulov?a, Petr Pajas, Jan Popelka, Ji?r?? Semeck?y, Jana
?
Sindlerov?a, Jan
?
St?ep?anek, Josef Toman, Zdeka Ure?sov?a,
and Zden?ek
?
Zabokrtsk?y. 2012. Announcing Prague Czech-English Dependency Treebank 2.0. In Proceedings
of the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey.
European Language Resources Association.
Jan Haji?c. 2004. Disambiguation of Rich Inflection (Computational Morphology of Czech). Nakladatelstv??
Karolinum.
Sanda M. Harabagiu and Steven J. Maiorano. 2000. Multilingual coreference resolution. In Proceedings of the
Sixth Conference on Applied Natural Language Processing, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Martin Wittorff Haulrich. 2012. Data-driven bitext dependency parsing and alignment. Ph.D. thesis, Copenhagen
Business School.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the
90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion
Volume: Short Papers, Stroudsburg, PA, USA. Association for Computational Linguistics.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012. Multilingual named entity recognition using parallel
data and metadata from Wikipedia. In Proceedings of the 50th Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers - Volume 1, Stroudsburg, PA, USA. Association for Computational Linguistics.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei Huang. 2012. Joint bilingual name tagging for
parallel corpora. In Proceedings of the 21st ACM International Conference on Information and Knowledge
Management, New York, NY, USA. ACM.
Linguistic Data Consortium. 1999. Penn Treebank 3. LDC99T42.
Ruslan Mitkov and Catalina Barbu. 2003. Using bilingual corpora to improve pronoun resolution. Languages in
contrast, 4(2).
Vincent Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computational Linguistics, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Giang Linh Nguy, V?aclav Nov?ak, and Zden?ek
?
Zabokrtsk?y. 2009. Comparison of classification and ranking
approaches to pronominal anaphora resolution in Czech. In Proceedings of the SIGDIAL 2009 Conference,
London, UK. The Association for Computational Linguistics.
Michal Nov?ak, Anna Nedoluzhko, and Zden?ek
?
Zabokrtsk?y. 2013. Translation of ?it? in a deep syntax frame-
work. In 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Workshop
on Discourse in Machine Translation, Sofija, Bulgaria. Omnipress, Inc.
23
Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th
Annual Meeting on Association for Computational Linguistics, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Maciej Ogrodniczuk. 2013. Translation- and projection-based unsupervised coreference resolution for Polish. In
Language Processing and Intelligent Information Systems, number 7912, Berlin / Heidelberg. Springer.
Martin Popel and Zden?ek
?
Zabokrtsk?y. 2010. TectoMT: Modular NLP framework. In Lecture Notes in Artificial
Intelligence, Proceedings of the 7th International Conference on Advances in Natural Language Processing
(IceTAL 2010), volume 6233, Berlin / Heidelberg. Springer.
Oana Postolache, Dan Cristea, and Constantin Orasan. 2006. Transferring coreference chains through word align-
ment. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, Genoa,
Italy. European Language Resources Association.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman.
Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2012. Translation-based projection for multilingual coreference resolution. In
Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Stroudsburg, PA, USA. Association for Computational Linguistics.
Rudolf Rosa, Ond?rej Du?sek, David Mare?cek, and Martin Popel. 2012. Using parallel features in parsing of
machine-translated sentences for correction of grammatical errors. In Proceedings of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Translation, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Petr Sgall, Eva Haji?cov?a, and Jarmila Panevov?a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic
Aspects. D. Reidel Publishing Company, Dordrecht.
Petr Sgall. 1967. Generativn?? popis jazyka a ?cesk?a deklinace. Academia, Prague, Czech Republic.
David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: Using English to parse
Korean. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics.
Kate?rina Veselovsk?a, Giang Linh Nguy, and Michal Nov?ak. 2012. Using Czech-English parallel corpora in
automatic identification of ?it?. In The Fifth Workshop on Building and Using Comparable Corpora,
?
Istanbul,
Turkey. European Language Resources Association.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer
Academic Publishers, Norwell, MA, USA.
Tao Zhuang and Chengqing Zong. 2010. Joint inference for bilingual semantic role labeling. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing, Stroudsburg, PA, USA. Association
for Computational Linguistics.
24
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 51?59,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Translation of ?It? in a Deep Syntax Framework
Michal Nova?k, Anna Nedoluzhko and Zdene?k ?Zabokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, CZ-11800
{mnovak,nedoluzko,zabokrtsky}@ufal.mff.cuni.cz
Abstract
We present a novel approach to the trans-
lation of the English personal pronoun it
to Czech. We conduct a linguistic analysis
on how the distinct categories of it are usu-
ally mapped to their Czech counterparts.
Armed with these observations, we design
a discriminative translation model of it,
which is then integrated into the TectoMT
deep syntax MT framework. Features in
the model take advantage of rich syntac-
tic annotation TectoMT is based on, exter-
nal tools for anaphoricity resolution, lex-
ical co-occurrence frequencies measured
on a large parallel corpus and gold coref-
erence annotation. Even though the new
model for it exhibits no improvement in
terms of BLEU, manual evaluation shows
that it outperforms the original solution in
8.5% sentences containing it.
1 Introduction
After it has long been neglected, retaining cohe-
sion of a text larger than a single sentence in Ma-
chine Translation (MT) has recently become a dis-
cussed topic. Correct translation of referential ex-
pressions is in many cases essential for humans to
grasp the meaning of a translated text.
Especially, the translation of pronouns attracts
a higher rate of interest. In the previous works
of Le Nagard and Koehn (2010), Hardmeier and
Federico (2010) and Guillou (2012), it has been
shown that current MT systems perform poorly in
producing the correct forms of pronouns. As re-
gards English, the personal pronoun it is the most
complicated case. Not only can it corefer with al-
most any noun phrase (making it hard to pick the
correct gender and number if the target language is
morphologically rich), but it can also corefer with
a larger discourse segment or play the role of a
filler in certain grammatical constructions.
In this work, we turn our attention to the transla-
tion of the English personal pronoun it into Czech.
Even if we ignore morphology and merge all re-
lated surface forms into one, we cannot find a
single Czech expression that would comprise all
functions of the English it. Moreover, there is no
simple one-to-one mapping from categories of it
to Czech expressions. For instance, one would ex-
pect that the translation of it which is coreferen-
tial with a noun phrase has to agree in number and
gender with the translation of its antecedent. How-
ever, there are cases when it is more suitable to
translate it as the demonstrative pronoun to, whose
gender is always neuter.
The aim of this work is to build an English-to-
Czech translation model for the personal pronoun
it within the TectoMT framework ( ?Zabokrtsky? et
al., 2008). TectoMT is a tree-to-tree translation
system with transfer via tectogrammatical layer,
a deep syntactic layer which follows the Prague
tectogrammatics theory (Sgall, 1967; Sgall et al,
1986) Therefore, its translation model outputs the
deep syntactic representation of a Czech expres-
sion. Selecting the correct grammatical categories
and thus producing a concrete surface form of a
deep syntactic representation is provided by the
translation synthesis stage, which we do not focus
on in this work.
The mapping between it and corresponding
Czech expressions depends on many aspects. We
address them by introducing features based on
syntactic annotation and anaphoricity resolver out-
put. Furthermore, we make use of lexical co-
occurrence counts aggregated on a large auto-
matically annotated Czech-English parallel corpus
CzEng 1.0 (Bojar et al, 2012). Coreference links
also appear to be a source of valuable features.1
In contrast to the related work, we prefer a dis-
criminative model to a commonly used generative
1However, we excluded them from the final model used
in MT as they originate from gold standard annotation.
51
model. The former allows us to feed it with many
syntactic and lexical features that may affect the
output, which would hardly be possible in the lat-
ter.
2 Related Work
Our work addresses a similar issue that has been
explored by Le Nagard and Koehn (2010), Hard-
meier and Federico (2010) and Guillou (2012).
These works attempted to incorporate informa-
tion on coreference relations into MT, aiming to
improve the translation of English pronouns into
morphologically richer languages. The poor re-
sults in the first two works were mainly due to im-
perfect automatic coreference annotation.
The work of Guillou (2012) is of special interest
to this work because it is also focused on English
to Czech translation and makes an extensive use
of the Prague Czech-English Dependency Tree-
bank 2.0 (PCEDT). Instead of automatic corefer-
ence links, they employed gold annotation, reveal-
ing further reasons of small improvements ? the
number of occurrences in the tranining data weak-
ened by including grammatical number and gen-
der in the annotation and availability of only a sin-
gle reference translation.
The first issue is a consequence of the assump-
tion that a Czech pronoun must agree in gen-
der and number with its antecedent. There are
cases, though, when demonstrative pronoun to fits
better and grammatical categories are not propa-
gated. Keeping grammatical information on its
antecedent may in this case result in probably not
harmful but still superfluous partitioning the train-
ing data.
Our work deals also with the second issue, how-
ever, at the cost of partial manual annotating.
The most significant difference of our work
compared to the abovementioned ones lies in the
MT systems used. Whereas they tackle the issue
of pronoun translation within the Moses phrase-
based system (Koehn et al, 2003), we rely on the
translation via deep syntax with TectoMT system
( ?Zabokrtsky? et al, 2008). Our approach is more
linguistically oriented, working with deep syntac-
tic representations and postponing the decisions
about the concrete forms to the synthesis stage.
3 Linguistic Analysis
In English, three main coarse-grained types of
it are traditionally distinguished. Referential it
points to a noun phrase in the preceding or the fol-
lowing context:
(1) Peter has finished writing an article and
showed it to his supervisor.
Anaphoric it refers to a verbal phrase or larger dis-
course segments (so-called discourse deixis).
(2) Peter has discussed the issue with his su-
pervisor and it helped him to finish the ar-
ticle.
Pleonastic it has no antecedent in the preced-
ing/following context and its presence is imposed
only by the syntactic rules of English.
(3) It is difficult to give a good example.
From the perspective of Czech, there are also
three prevailing types of how it can be translated.
The most frequent are personal pronouns or zero
forms.2 In Prague tectogrammatics theory zero
anaphors are reconstructed on the tectogrammat-
ical layer. Same as expressed personal pronouns,
they are represented by a node with the #PersPron
symbol, e.g.
(4) Bushova vla?da ozna?mila, z?e se svu?j pla?n
#PersPron pokus?? vzkr???sit.
The Bush administration has said it will
try to resurrect its plan.
The second typical possibility is the Czech demon-
strative pronoun to (= it, this), which is a form of
a pronoun ten in its neuter singular form, e.g.
(5) Analytik r?ekl, z?e to byla tato moz?nost
poz?adavku, ktera? pevne?js???m cena?m po-
mohla.
The analyst said that it was the possibility
of this demand that helped firm prices.
In many cases, it has no lexical counterpart in
the Czech translation, the English and Czech sen-
tences thus having a different syntactic structure.
These are cases like, for instance:
(6) Obchodn??ci uvedli, z?e je obt??z?ne? nove?
emise REMIC strukturovat, kdyz? se ceny
tolik me?n??.
Dealers noted that it?s difficult to struc-
ture new Remics when prices are moving
widely.
2Czech is a pro-drop language.
52
Figure 1: The mapping of the types of English it
to Czech translations.
There are also some other possibilities of how it
can be translated into Czech, such as the repeti-
tion of the antecedent noun, different genders of
the demonstrative ten (=it, this) in the anaphoric
position, using synonyms and hyperomyns. How-
ever, these cases are not so frequent and they rarely
cannot be converted to one of the three broader
categories.
The correspondence between the course-
grained types of English it and its possible Czech
translations is not one-to-one. As seen from
Figure 1, a personal pronoun/zero anaphora
translates to the referential it (see example 4) and
no lexical counterpart is used when translating the
pleonastic it (see example 6).
However, all types of it can be translated as a
neuter demonstrative to. The typical case ?it refer-
ring to VPs/larger discourse segments = to? was
demonstrated in (5).
The mapping ?referential it = to? is common for
cases where the referent is attributed some further
characteristics, mostly in constructions with a verb
to be like ?It is something.?, such as (7).3 This
is an interesting case for Czech, because a gen-
der and number agreement between the antecedent
and the anaphoric to is generally absent.
(7) Some investors say Friday?s sell-off was a
good thing. ?It was a healthy cleansing,?
says Michael Holland.
Ne?kter??? investor?i r???kaj??, z?e pa?tec?n??
vy?prodej byla dobra? ve?c. ?Byla to zdrava?
oc?ista,? r???ka? Michael Holland.
The ?cleft sentences? (see example 8) and some
other syntactic constructions are the case when
pleonastic it is translated into Czech with the
demonstrative to.
3We suspect that it holds also for he/she/they but such a
claim is not yet empirically supported. For the sake of sim-
plicity, we conduct our research only for it.
(8) But it is Mr. Lane, as movie director, who
has been obsessed with refitting Chaplin?s
Little Tramp in a contemporary way.
Ale je to Lane jako filmovy? rez?ise?r, kdo je
posedly? t??m, z?e zmodernizuje Chaplinu?v
film ?Little Tramp (Maly? tula?k)?.
In some cases, both translations of pleonastic it
are possible: neuter demonstrative to or a different
syntactic construction with no lexical counterpart
of it. Compare the examples from PCEDT where
it with similar syntactic function was translated by
changing the syntactic structure in (9) and using a
neuter to in (10):
(9) ?It was great to have the luxury of time,?
Mr. Rawls said.
?Bylo skve?le?, z?e jsme me?li dostatek c?asu,?
r?ekl Rawls.
(10) ?On days that I?m really busy,? says Ms.
Foster, ?it seems decadent to take time off
for a massage.?
?Ve dnech, kdy ma?m opravdu mnoho
pra?ce,? r???ka? pan?? Fosterova?, ?to vypada?
zvrhle, kdyz? si vyhrad??m c?as na masa?z?.?
4 Translation via Deep Syntax
Following a phrase-based statistical MT approach,
it may be demanding to tackle issues that arise
when translating between typologically different
languages. Translation from English to Czech is a
typical example. One has to deal with a rich mor-
phology, less constrained word order, changes in
clauses bindings, pro-drops etc.
In this work, we make use of the English to
Czech translation implemented within the Tec-
toMT system, first introduced by ?Zabokrtsky? et al
(2008). In contrast to the phrase-based approach,
TectoMT performs a tree-to-tree machine transla-
tion. Given an input English sentence, the trans-
lation process is divided into three stages: analy-
sis, transfer and synthesis. TectoMT at first con-
ducts an automatic analysis including POS tag-
ging, named entity recognition, syntactic parsing,
semantic role labeling, coreference resolution etc.
This results in a deep syntactic representation of
the English sentence, which is subsequently trans-
ferred into Czech, with the translation of lexical
and grammatical information being provided via
several factors. The process proceeds with a rule-
53
based synthesis stage, when a surface Czech sen-
tence is generated from its deep syntactic struc-
ture.
Deep syntactic representation of a sentence fol-
lows the Prague tectogrammatics theory (Sgall,
1967; Sgall et al, 1986). It is a dependency
tree whose nodes correspond to the content words
in the sentence. Personal pronouns missing on
the surface are reconstructed in special nodes.
Nodes are assigned semantic roles (called func-
tors) and grammatical information is comprised in
so called grammatemes. Furthermore, tectogram-
matical representation is a place where corefer-
ence relations are annotated.
4.1 Model of it within TectoMT
The transfer stage, which maps an English tec-
togrammatical tree to a Czech one, is a place
where the translation model of it is applied. For
every English node corresponding to it, a feature
vector is extracted and fed into a discriminative re-
solver that assigns one of the three classes to it ?
PersPron, To and Null, corresponding to the
main Czech types introduced in Section 3.
If labeled as PersPron, the English node
is mapped to a Czech #PersPron node and the
English coreference link is projected. During
the synthesis, it is decided whether the pronoun
should be expressed on a surface, its gender and
number are copied from the antecedent?s head and
finally the correct form (if any) is generated.
Obtaining class To makes things easier. The
English node is only mapped to a Czech node con-
taining the pronoun ten with its gender and num-
ber set to neuter singular, so that later the correct
form to will be generated.
Last, if it is assigned Null, no corresponding
node on the Czech side is generated, but the Czech
counterpart of the governing verb is forced to be in
neuter singular.
5 Prague Czech-English Dependency
Treebank as a source of data
The Prague Czech-English Dependency Treebank
(Hajic? et al, 2011, PCEDT) is a manually parsed
Czech-English parallel corpus comprising over 1.2
million words for each language in almost 50,000
sentence pairs. The English part contains the en-
tire Penn Treebank?Wall Street Journal Section
(Linguistic Data Consortium, 1999). The Czech
part consists of translations of all the texts from
the English part. The data from both parts are
annotated on three layers following the theory of
Prague tectogrammatics ? the morphological layer
(where each token from the sentence gets a lemma
and a POS tag), the analytical layer (surface syn-
tax in the form of a dependency tree, where each
node corresponds to a token in the sentence) and
the tectogrammatical representation (see Section
4).
Sentences of PCEDT have been automatically
morphologically annotated and parsed into ana-
lytical dependency trees.4 The tectogrammatical
trees in both language parts have been annotated
manually (Hajic? et al, 2012). The nodes of Czech
and English trees have been automatically aligned
on analytical as well as tectogrammatical layer
(Marec?ek et al, 2008).
5.1 Extraction of Classes
The shortcomings of the automatic alignment
is particularly harmful for pronouns and zero
anaphors, which can replace a whole range of con-
tent words and their meaning is inferred mainly
from the context. The situation is better for verbs
as their usual parents in dependency trees: since
they carry meaning in a greater extent, their auto-
matic alignment is of a higher quality.
Thus, we did not search for a Czech counterpart
of it by following the alignment of it itself. Using
the fact that the verb alignment is more reliable
and functors in tectogrammatical trees have been
manually corrected, we followed the alignment of
the parent of it (a verb) and selected the Czech sub-
tree with the same tectogrammatical functor as it
had on the English side. If the obtained subtree
is a single node of type #PersPron or ten, we as-
signed class PersPron or To, respectively, to the
corresponding it. This approach relies also on the
assumption that semantic roles do not change in
the translation.
The automatic acquisition of classes covered
more than 60% of instances, the rest had to be la-
beled manually. During the annotation, we obeyed
the following rules:
1. If a demonstrative pronoun to is present in the
Czech sentence or if a personal pronoun is
either present or unexpressed, assign the in-
stance to the corresponding class.
4The English dependency trees were built by automati-
cally transforming the original phrase-structure annotation of
the Penn Treebank.
54
2. Otherwise, ignore the Czech translation pro-
vided in the corpus and follow the most sim-
plistic possible translation which would still
be correct. Assign the instance to the class
which fits it the best.
Note that it may happen that none of the three
options fits, because it is either an idiomatic ex-
pression or larger structural modifications are re-
quired. Such cases are very rare and we left them
out of the data.
The manual annotation was a bottleneck. We
managed to tag the complete testing data, but were
only able to annotate more than just 1/6 of the
training data due to time reasons. We only use
a corresponding proportion of the automatically
labeled training instances in order to respect the
overall distribution.
5.2 Extraction of Features
Given the linguistically supported observation on
both manually and automatically annotated tree-
banks, we designed features to differentiate be-
tween the ways it is translated.
Since this work focuses on MT with transfer via
deep-syntactic layer, it is possible for the proposed
features to exploit morphological, syntactic and a
little of semantic information present on various
annotation layers.
Unlike the target classes, which have to be as-
signed as accurately as possible, extracted fea-
tures must follow the real-world scenario of MT
? the only information that is given is the source
sentence. Thus, whereas extracting classes may
exploit the gold standard linguistic annotation, it
cannot be employed in feature extraction. We ex-
tract them from text automatically annotated by
the same pipeline that is used in the TectoMT anal-
ysis stage.
However, there is an exception where we violate
this approach ? coreference. Performance of state-
of-the-art coreference resolvers is still far from the
ideal, especially for distinguishing between pro-
nouns referring to noun phrases and those refer-
ring to clauses or wider discourse segments. Sim-
ilarly to the work of Guillou (2012) we wanted
to isolate the problem of translating referential
expressions from the task of resolving the entity
they refer to. Therefore, we opted for extracting
the coreferential features from the gold annotation
projected onto automatically analyzed trees. Note
that the results achieved using these features have
to be considered an upper bound for a given set-
ting.
Although the mapping between Czech transla-
tion of it and English categories of it does not al-
low to translate it directly, the category of it es-
timated by an anaphoricity resolver might be a
promising feature. We therefore constructed a bi-
nary feature based on the output of a system iden-
tifying whether a pronoun it is coreferential or
not. We employed the NADA resolver (Bergsma
and Yarowsky, 2011)5 exploiting the web-scale n-
gram data and its tree-based extension presented
in (Veselovska? et al, 2012).
Some verbs are more likely to bind with it that
refers to a longer utterance. Such it is quite con-
sistently translated as a demonstrative to. This
motivated incorporating a parent lemma of an oc-
currence of it into the feature set. However, the
training data is too small to be a sufficient sample
from a distribution over lexical properties. Hence,
we took advantage of the automatically annotated6
Czech-English corpus CzEng 1.0 (Bojar et al,
2012) that comprises more than 15 million sen-
tence pairs. In the manner described in Section
5.1, we collected co-occurrence counts between
a functor that the given it possesses concatenated
with a lemma of its verbal parent and a Czech
counterpart having the same functor (denoted as
csit). We filtered out all occurrences where csit
was neither #PersPron nor ten. Then, for both val-
ues of csit a feature is constructed by looking up
counts for a concrete occurrence in the collected
counts and quantized into 4-5 bins (Bansal and
Klein, 2012) following the formula:
bin(log(
count(functor : parent ? csit)
count(functor : parent)count(csit)
)).
Linguistic analysis carried out in Section 3 sug-
gests the following syntax-oriented features re-
lated to the verb to be. Some nominal predicates
tend to be translated as to, even though it is usually
coreferential in such expressions (see example 7).
So the corresponding binary feature fires if it is a
subject and its parent is the verb to be having an
object (Figure 2a).
Similarly, adjectival predicates that are not fol-
lowed by a subordinating clause connected with
5A probability value returned by this tool was binarized at
a threshold 0.5
6Using the same annotation layers as in PCEDT and Tec-
toMT, i.e. in accordance with the Prague tectogrammatics
theory.
55
Figure 2: Syntactic features capturing typical con-
structions with a verb be.
the main clause by the English connectives to or
that are usually referential and translated as to,
too. We proposed a feature describing these cases,
illustrated in Figure 2b.
In contrast, if an adjectival predicate is followed
by a subordinating clause with the verb being finite
and connected to the main clause by a conjunction
that, in majority of cases it is a pleonastic usage of
it translated as a null subject (see example 6). A
schema of the feature is depicted in Figure 2c.
Being definitely pleonastic, it in cleft sentences
is expressed in Czech either by to or by sentence
rearranging (see example 8). We target this phe-
nomenon by another feature being fired if it is a
subject of the verb to be and if this verb has an ob-
ject and is followed by a relative clause (see Figure
2d).
Finally, we designed two features exploiting
coreference relations. The first one simply indi-
cates if it has an antecedent, while the second fires
if any of the antecedents in the coreferential chain
is a verb phrase. As we noted above, these fea-
tures are based on the gold standard annotation of
coreference.
5.3 Data Description
The data for training and testing a discriminative
translation model of the personal pronoun it were
extracted from PCEDT with classes and features
obtained as described in Section 5.1 and 5.2, re-
spectively. Due to the limited amount of manually
annotated training data, the training set extracted
from sections 00 ? 19 was reduced from 5841 to
940 instances, though. The testing set was an-
notated thoroughly, thus containing 543 instances
extracted from sections 20 ? 21. Every instance
represents an occurrence of it in PCEDT. The dis-
Class Train Test
PersPron 576 322
To 231 138
Null 133 83
Table 1: Distribution of classes in the data sets.
tribution of target classes in the data is shown in
Table 1.
6 Experiments
Experiments were conducted in two settings that
differ in the usage of features extracted from gold
coreferential relations.
To mitigate a possible error caused by a wrong
classifier choice, we built several models based on
various Machine Learning classification methods.
If not explicitly mentioned, the methods below are
applied with default parameters:
? Vowpal Wabbit (Langford, 2012). Binary
logistic regression with one-against-all strat-
egy for handling multiple classes. The opti-
mum has been found using the online method
(Stochastic Gradient Descent). We varied the
parameters of the number of passes over the
data and the L2 regularization weight.
? AI::MaxEntropy.7 Multiclass logistic re-
gression.8 The optimum has been found us-
ing the batch method (L-BFGS).
? sklearn.neighbors.9 k-nearest neighbors
classifier with the parameter k being varied.
? sklearn.tree. Decision tree classifier.
? sklearn.SVC. Support Vector Machines with
one-against-one strategy to handle multiple
classes. We varied the choice of a kernel.
The accuracy evaluated on both training and test
sets is shown in Table 2 (columns Acc:Train and
Acc:Test). The baseline resolver simply picks the
most frequent class in the training set, which is
PersPron. For both experimental settings, the
standard deviation measured on the test set is less
than 1% in total, if the method?s best configuration
of parameters is taken and the result on decision
trees, which we did not tune, is excluded. This
shows that all classifiers are consistent in their de-
cisions.
7http://search.cpan.org/
?
laye/
AI-MaxEntropy-0.20/
8In the field of NLP also called Maximum Entropy.
9All classifiers labeled as sklearn.* are implemented in
the Scikit-learn Python library (Pedregosa et al, 2011).
56
all feats all feats + coref
ML Method Acc:Train Acc:Test BLEU Acc:Train Acc:Test
Baseline 60.70 59.30 0.1401 60.70 59.30
Original TectoMT ? ? 0.1404 ? ?
Vowpal Wabbit (passes=30) 90.62 75.69 ? 90.83 75.87
Vowpal Wabbit (passes=20) 89.99 76.43 0.1403 90.20 76.98
Vowpal Wabbit (passes=10) 87.78 76.24 ? 87.78 76.61
Vowpal Wabbit (passes=30, l2=0.001) 71.23 66.11 ? 83.03 77.16
Vowpal Wabbit (passes=20, l2=0.001) 82.19 74.95 ? 78.19 74.40
Vowpal Wabbit (passes=10, l2=0.001) 75.03 70.17 ? 72.81 70.17
Vowpal Wabbit (passes=30, l2=0.00001) 90.52 75.69 ? 90.94 76.06
Vowpal Wabbit (passes=20, l2=0.00001) 89.99 76.43 ? 90.09 76.98
Vowpal Wabbit (passes=10, l2=0.00001) 87.67 76.24 ? 87.67 76.61
AI::MaxEntropy 85.99 76.61 0.1403 86.09 76.98
sklearn.neighbors (k=1) 91.57 71.64 ? 93.36 72.19
sklearn.neighbors (k=3) 84.62 72.01 ? 84.93 71.82
sklearn.neighbors (k=5) 84.93 74.77 0.1403 84.72 75.87
sklearn.neighbors (k=10) 82.51 73.30 ? 83.14 75.87
sklearn.tree 93.36 73.66 0.1403 94.10 71.82
sklearn.SVC (kernel=linear) 90.83 75.51 0.1402 91.15 76.80
sklearn.SVC (kernel=poly) 60.70 59.30 ? 60.70 59.30
sklearn.SVC (kernel=rbf) 71.23 68.69 ? 73.76 71.27
Table 2: Intrinsic (accuracy on the training and test data) and extrinsic (BLEU score) evaluation of
translation model of it in configuration with (all feats) and without gold coreferential features (all feats
+ coref).
By introducing linguistically motivated features
exploiting the deep-syntactic description of the
sentence, we gained 17% in total over the base-
line. Moreover, adding features based on the gold
coreference annotation results in a further 0.5%
improvement.
7 Evaluation on MT
Although intrinsic evaluation as performed in Sec-
tion 6 can give us a picture of how accurate the
translation model might be, the main purpose of
this work is to integrate it in a full-fledged MT
system. As explained in Section 4, this component
is tailored for TectoMT ? an MT system where the
transfer is provided through a deep-syntactic layer.
The extrinsic evaluation of the proposed method
was carried out on the English-Czech test set for
WMT 2011 Shared Translation Task (Callison-
Burch et al, 2011).10 This data set contains 3,003
English sentences with one Czech reference trans-
lation, out of which 430 contain at least one occur-
rence of it.
Since this test set is provided with no annota-
tion of coreferential links, the model of it that is
involved in experiments on the end-to-end transla-
tion was trained on a complete feature set exclud-
10http://www.statmt.org/wmt11/test.tgz
ing the coreferential features using the Machine
Learning method that performed best in the intrin-
sic test, i.e. AI::MaxEntropy (see Section 6).
The new method was compared to the rule-
based approach originally used in TectoMT, which
works as follows. In the transfer stage, all occur-
rences of it are translated to a demonstrative ten.
In the synthesis stage, another rule is fired, which
determines whether ten is omitted on the surface.
Then, omitting it corresponds either to a structural
change (Null class) or an unexpressed personal
pronoun (a subset of PersPron class). It makes
this original approach difficult to compare with the
scores in Table 2, as the translation model of it
is applied in the transfer stage, where we do not
know yet if a personal pronoun is to be expressed
or not. Thus, we consider it the most appropriate
to use final translated sentences produced by two
versions of TectoMT in order to compare the dif-
ferent way they handle it.
The shift from the original settings to a new
model for it results in 166 changed sentences. In
terms of BLEU score, we observe a marginal drop
from 0.1404 to 0.1403 when using the new ap-
proach.11 Other classifiers achieved the same or
11For comparison, the best system so far ? Chimera (Bojar
et al, 2013) achieves 0.1994 on the same test set. Chimera
combines Moses, TectoMT and rule-based corrections.
57
new better than old 24
old better than new 13
both equally wrong 9
both equally correct 4
Table 3: The results of manual evaluation con-
ducted on 50 sentences translated by TectoMT in
the original settings (old) and with the new trans-
lation model for it (new)
similar score which correlates with the findings
from intrinsic evaluation (see Table 2). It accords
with a similar experience of Le Nagard and Koehn
(2010) and Guillou (2012) and gives another evi-
dence that the BLEU metric is inaccurate for mea-
suring pronoun translation.
Manual evaluation gives a more realistic view.
We randomly sampled 50 out of the 166 sentences
that differ and one annotator assessed which of
the two systems gave a better translation. Table
3 shows that in almost half of the cases the change
was an improvement. Including the sentences that
are acceptable for both settings, the new approach
picked the correct Czech counterpart of it in 22%
more sentences than the original approach. Since
the proportion of the changed sentences accounts
for almost 39% of all sentences containing it, the
overall proportion of improved sentences with it is
around 8.5% in total.
8 Discussion
Inspecting the manually evaluated translation for
types of improvements and losses, we have found
that in none of the changed sentences the original
system decided to omit ten (obtained by the rule)
on the surface. It shows that the new approach
agrees with the original one on the way of omit-
ting personal pronouns and mainly addresses the
overly simplistic assignment of the demonstrative
ten.
The distribution of target classes over cor-
rected sentences is almost uniform. In 13 out
of 24 improvements, the new system succeeded
in correctly resolving the Null class while in
the remaining 11 cases, the corrected class was
PersPron. It took advantage mostly of the
syntax-based features in the former and sugges-
tions given by the NADA anaphoricity resolver in
the latter.
Examining the errors, we observed that the ma-
jority of them are incurred in the structures with
?it is?. These errors stem mostly from incorrect
activation of syntactic features due to parsing and
POS tagging errors. Example 11 (the Czech sen-
tence is an MT output) shows the latter, when the
POS tagger erroneously labeled the word soy as an
adjective. That resulted in activating the feature
for adjectival predicates followed by that (Figure
2c) instead of a feature indicating cleft structures
(Figure 2d), thus preferring the label Null to the
correct To.
(11) SOURCE: It is just soy that all well-known
manufacturers use now.
TECTOMT: Je to jen so?jove?, z?e zna?m??
vy?robci vs?ech pouz???vaj?? te?d.
9 Conclusion
In this work we presented a novel approach to
dealing with the translation of the English personal
pronoun it. We have shown that the mapping be-
tween the categories of it and the ways of trans-
lating it to Czech is not one-to-one. In order to
deal with this, we designed a discriminative trans-
lation model of it for the TectoMT deep syntax MT
framework.
We have built a system that outperforms its pre-
decessor in 8.5% sentences containing it, taking
advantage of the features based on rich syntactic
annotation the MT system provides, external tools
for anaphoricity resolution and features capturing
lexical co-occurrence in a massive parallel corpus,
The main bottleneck that hampered bigger im-
provements is the manual annotation of the train-
ing data. We managed to accomplish it just on 1/6
of the data, which did not provide sufficient evi-
dence for some specific features.
Our main objective of the future work is thus
to reduce a need for manual annotation by dis-
covering ways of automatic extraction of reliable
classes from a semi-manually annotated corpus
such as PCEDT.
Acknowledgments
This work has been supported by the Grant
Agency of the Czech Republic (grants
P406/12/0658 and P406/2010/0875), the grant
GAUK 4226/2011 and EU FP7 project Khresmoi
(contract no. 257528). This work has been using
language resources developed and/or stored and/or
distributed by the LINDAT-Clarin project of the
Ministry of Education of the Czech Republic
(project LM2010013).
58
References
Mohit Bansal and Dan Klein. 2012. Coreference Se-
mantics from Web Features. In Proceedings of the
50th Annual Meeting of the ACL: Long Papers ? Vol-
ume 1, pages 389?398, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Shane Bergsma and David Yarowsky. 2011. NADA:
A Robust System for Non-Referential Pronoun De-
tection. In DAARC, pages 12?23, Faro, Portugal,
October.
Ondr?ej Bojar, Zdene?k ?Zabokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proceedings of LREC 2012, Istanbul, Turkey,
May. ELRA, European Language Resources Associ-
ation.
Ondr?ej Bojar, Rudolf Rosa, and Ales? Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proceedings of the Eight Workshop on Sta-
tistical Machine Translation. Under review.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Liane Guillou. 2012. Improving Pronoun Translation
for Statistical Machine Translation. In Proceedings
of the Student Research Workshop at the 13th Con-
ference of the EACL, pages 1?10, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Silvie Cinkova?, Eva Fuc???kova?, Marie Mikulova?, Petr
Pajas, Jan Popelka, Jir??? Semecky?, Jana ?Sindlerova?,
Jan ?Ste?pa?nek, Josef Toman, Zden?ka Ures?ova?, and
Zdene?k ?Zabokrtsky?. 2011. Prague Czech-English
Dependency Treebank 2.0.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr
Sgall, Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?,
Marie Mikulova?, Petr Pajas, Jan Popelka, Jir???
Semecky?, Jana ?Sindlerova?, Jan ?Ste?pa?nek, Josef
Toman, Zden?ka Ures?ova?, and Zdene?k ?Zabokrtsky?.
2012. Announcing Prague Czech-English Depen-
dency Treebank 2.0. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3153?3160. ELRA.
Christian Hardmeier and Marcello Federico. 2010.
Modelling Pronominal Anaphora in Statistical Ma-
chine Translation. In Marcello Federico, Ian Lane,
Michael Paul, and Franc?ois Yvon, editors, Proceed-
ings of the seventh International Workshop on Spo-
ken Language Translation (IWSLT), pages 283?289.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the 2003 Conference of the NAACL HLT
? Volume 1, pages 48?54, Stroudsburg, PA, USA.
Association for Computational Linguistics.
John Langford. 2012. Vowpal Wabbit.
Ronan Le Nagard and Philipp Koehn. 2010. Aid-
ing Pronoun Translation with Co-Reference Resolu-
tion. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 252?261, Uppsala, Sweden, July. Association
for Computational Linguistics.
Linguistic Data Consortium. 1999. Penn Treebank 3.
LDC99T42.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of the Twelfth EAMT Conference, pages
102?111.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ?Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Jour-
nal of Machine Learning Research, 12:2825?2830,
November.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing Company,
Dordrecht.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague, Czech Republic.
Kater?ina Veselovska?, Giang Linh Nguy, and Michal
Nova?k. 2012. Using Czech-English Parallel Cor-
pora in Automatic Identification of It. In The Fifth
Workshop on Building and Using Comparable Cor-
pora, pages 112?120.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 167?170, Stroudsburg, PA, USA.
Association for Computational Linguistics.
59
