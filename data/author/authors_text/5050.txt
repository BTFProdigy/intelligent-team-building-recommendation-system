Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 42?50, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Word Alignment with Bridge Languages
Shankar Kumar and Franz Och and Wolfgang Macherey
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, U.S.A.
{shankarkumar,och,wmach}@google.com
Abstract
We describe an approach to improve
Statistical Machine Translation (SMT)
performance using multi-lingual, parallel,
sentence-aligned corpora in several bridge
languages. Our approach consists of a sim-
ple method for utilizing a bridge language to
create a word alignment system and a proce-
dure for combining word alignment systems
from multiple bridge languages. The final
translation is obtained by consensus de-
coding that combines hypotheses obtained
using all bridge language word alignments.
We present experiments showing that mul-
tilingual, parallel text in Spanish, French,
Russian, and Chinese can be utilized in
this framework to improve translation
performance on an Arabic-to-English task.
1 Introduction
Word Alignment of parallel texts forms a cru-
cial component of phrase-based statistical machine
translation systems. High quality word alignments
can yield more accurate phrase-pairs which improve
quality of a phrase-based SMT system (Och and
Ney, 2003; Fraser and Marcu, 2006b).
Much of the recent work in word alignment has
focussed on improving the word alignment quality
through better modeling (Och and Ney, 2003; Deng
and Byrne, 2005; Martin et al, 2005) or alternative
approaches to training (Fraser and Marcu, 2006b;
Moore, 2005; Ittycheriah and Roukos, 2005). In
this paper we explore a complementary approach to
improve word alignments using multi-lingual, par-
allel (or multi-parallel) corpora. Two works in the
literature are very relevant to our approach. Borin
(2000) describes a non-statistical approach where a
pivot alignment is used to combine direct translation
and indirect translation via a third language. Filali
and Bilmes (2005) present a multi-lingual extension
to the IBM/HMMmodels. Our current approach dif-
fers from this latter work in that we propose a sim-
ple framework to combine word alignments from
any underlying statistical alignment model without
the need for changing the structure of the model.
While both of the above papers focus on improv-
ing word alignment quality, we demonstrate that
our approach can yield improvements in transla-
tion performance. In particular, we aim to improve
an Arabic-to-English (Ar-En) system using multi-
parallel data from Spanish (Es), French (Fr), Rus-
sian (Ru) and Chinese (Zh). The parallel data in
these languages X ? {Es, Fr,Ru, Zh} is used to
generate word alignments between Arabic-X and
X-English. These alignments are then combined to
obtain multiple word alignments for Arabic-English
and the final translation systems.
The motivation for this approach is two-fold.
First, we believe that parallel corpora available
in several languages provide a better training ma-
terial for SMT systems relative to bilingual cor-
pora. Such multi-lingual parallel corpora are be-
coming widely available; examples include proceed-
ings of the United Nations in six languages (UN,
2006), European Parliament (EU, 2005; Koehn,
2003), JRC Acquis corpus (EU, 2007) and religious
texts (Resnik et al, 1997). Word alignment systems
42
trained on different language-pairs (e.g. French-
English versus Russian-English) make errors which
are somewhat orthogonal. In such cases, incorrect
alignment links between a sentence-pair can be cor-
rected when a translation in a third language is avail-
able. Thus it can help resolve errors in word align-
ment. We combine word alignments using several
bridge languages with the aim of correcting some
of the alignment errors. The second advantage of
this approach is that the word alignment from each
bridge language can be utilized to build a phrase-
based SMT system. This provides a diverse collec-
tion of translation hypotheses for MT system com-
bination (Bangalore et al, 2002; Sim et al, 2007;
Matusov et al, 2006; Macherey and Och, 2007). Fi-
nally, a side benefit of this paper is that it provides a
study that compares alignment qualities and BLEU
scores for models in different languages trained on
parallel text which is held identical across all lan-
guages.
We show that parallel corpora in multiple lan-
guages can be exploited to improve the translation
performance of a phrase-based translation system.
This paper gives specific recipes for using a bridge
language to construct a word alignment and for com-
bining word alignments produced by multiple statis-
tical alignment models.
The rest of this paper is organized as follows: Sec-
tion 2 gives an overview of our framework for gen-
erating word alignments in a single language-pair.
In Section 3, we describe how a bridge language
may be used for producing word alignments. In Sec-
tion 4, we describe a scheme to combine word align-
ments from several bridge languages. Section 5 de-
scribes our experimental setup and reports the align-
ment and translation performance. A final discus-
sion is presented in Section 6.
2 Word Alignment Framework
A statistical translation model (Brown et al, 1993;
Och and Ney, 2003) describes the relationship be-
tween a pair of sentences in the source and target
languages (f = fJ1 , e = e
I
1) using a translation
probability P (f |e). Alignment models introduce a
hidden alignment variable a = aJ1 to specify a map-
ping between source and target words; aj = i in-
dicates that the jth source word is linked to the ith
target word. Alignment models assign a probabil-
ity P (f ,a|e) to the source sentence and alignment
conditioned on the target sentence. The transla-
tion probability is related to the alignment model as:
P (f |e) =
?
a P?(f ,a|e), where ? is a set of param-
eters.
Given a sentence-pair (f , e), the most likely
(Viterbi) word alignment is found as (Brown et al,
1993): a? = argmaxa P (f ,a|e). An alternate cri-
terion is the Maximum A-Posteriori (MAP) frame-
work (Ge, 2004; Matusov et al, 2004). We use a
refinement of this technique.
Given any word alignment model, posterior prob-
abilities can be computed as (Brown et al, 1993)
P (aj = i|e, f) =
?
a
P (a|f , e)?(i, aj), (1)
where i ? {0, 1, ..., I}. The assignment aj = 0
corresponds to the NULL (empty) alignment. These
posterior probabilities form a matrix of size (I+1)?
J , where entries along each column sum to one.
The MAP alignment for each source position j ?
{1, 2, ..., J} is then computed as
aMAP (j) = argmax
i
P (aj = i|e, f). (2)
We note that these posterior probabilities can be
computed efficiently for some alignment models
such as the HMM (Vogel et al, 1996; Och and Ney,
2003), Models 1 and 2 (Brown et al, 1993).
In the next two sections, we describe how poste-
rior probabilities can be used to a) construct align-
ment systems from a bridge language, and b) merge
several alignment systems.
3 Constructing Word Alignment Using a
Bridge Language
We assume here that we have triples of sentences
that are translations of each other in languages F, E,
and the bridge language G: f = fJ1 , e = e
I
1,g =
gK1 . Our goal is to obtain posterior probability es-
timates for the sentence-pair in FE: (f , e) using the
posterior probability estimates for the sentence pairs
in FG: (f ,g) and GE: (g, e). The word alignments
between the above sentence-pairs are referred to as
aFE , aFG, and aGE respectively; the notation aFE
indicates that the alignment maps a position in F to
a position in E.
43
We first express the posterior probability as a sum
over all possible translations g in G and hidden
alignments aFG.
P (aFEj = i|e, f)
=
?
g
P (aFEj = i,g|e, f)
=
?
g,k
P (aFEj = i,g, a
FG
j = k|e, f)
=
?
g,k
{
P (g|e, f)P (aFGj = k|g, e, f)
?P (aFEj = i|a
FG
j = k,g, e, f)
}
(3)
We now make some assumptions to simplify the
above expression. First, there is exactly one trans-
lation g in bridge language G corresponding to the
sentence-pair f , e. Since aGE
aFGj
= i = aFEj , we can
express
P (aFEj = i|a
FG
j = k,g, f , e) = P (a
GE
k = i|g, e).
Finally, alignments in FG do not depend on E.
Under these assumptions, we arrive at the final ex-
pression for the posterior probability FE in terms of
posterior probabilities for GF and EG
P (aFEj = i|e, f) = (4)
K?
k=0
P (aFGj = k|g, f)P (a
GE
k = i|g, e)
The above expression states that the posterior prob-
ability matrix for FE can be obtained using a simple
matrix multiplication of posterior probability ma-
trices for GE and FG. In this multiplication, we
prepend a column to the GE matrix corresponding
to k = 0. This probability P (aGEk = i) when k = 0
is not assigned by the alignment model; we set it as
follows
P (aGEk = i|k = 0) =
{
 i = 0
1?
I i ? {1, 2, ..., I}
The parameter  controls the number of empty align-
ments; a higher value favors more empty alignments
and vice versa. In our experiments, we set  = 0.5.
4 Word Alignment Combination Using
Posterior Probabilities
We next show how Word Alignment Posterior Prob-
abilities can be used for combining multiple word
alignment systems. In our context, we use this pro-
cedure to combine word alignments produced using
multiple bridge languages.
Suppose we have translations in bridge languages
G1, G2, ..., GN , we can generate a posterior prob-
ability matrix for FE using each of the bridge lan-
guages. In addition, we can always generate a poste-
rior probability matrix for FE with the FE alignment
model directly without using any bridge language.
These N + 1 posterior matrices can be combined as
follows. Here, the variable B indicates the bridge
language. B ? {G0, G1, ..., GN}; G0 indicates the
case when no bridge language is used.
P (aFEj = i|e, f) (5)
=
N?
l=0
P (B = Gl, a
FE
j = i|e, f)
=
N?
l=0
P (B = Gl)P (a
FE
j = i|Gl, e, f),
where P (aFEj = i|Gl, j, e, f) is the posterior proba-
bility when bridge language B = Gl. The probabili-
ties P (B = Gl) sum to one over l ? {0, 1, 2, ..., N}
and represent the prior probability of bridge lan-
guage l. In our experiments, we use a uniform prior
P (B = Gl) = 1N+1 . Equation 5 provides us a way
to combine word alignment posterior probabilites
from multiple bridge languages. In our alignment
framework (Section 2), we first interpolate the pos-
terior probability matrices (Equation 5) and then ex-
tract the MAP word alignment (Equation 2) from the
resulting matrix.
5 Experiments
We now present experiments to demonstrate the ad-
vantages of using bridge languages. Our experi-
ments are performed in the open data track of the
NIST Arabic-to-English (A-E) machine translation
task 1.
5.1 Training and Test Data
Our approach to word alignment (Section 3) requires
aligned sentences in multiple languages. For train-
ing alignment models, we use the ODS United Na-
1http://www.nist.gov/speech/tests/mt/
44
Set # of Ar words (K) # of sentences
dev1 48.6 2007
dev2 11.4 498
test 37.8 1610
blind 36.5 1797
Table 1: Statistics for the test data.
tions parallel data (UN, 2006) which contains par-
liamentary documents from 1993 onwards in all six
official languages of the UN: Arabic (Ar), Chinese
(Zh), English (En), French (Fr), Russian (Ru), and
Spanish (Es).
We merge the NIST 2001-2005 Arabic-English
evaluation sets into a pool and randomly sam-
ple this collection to create two development sets
(dev1,dev2) and a test set (test) with 2007, 498, and
1610 sentences respectively. Our blind test (blind)
set is the NIST part of the NIST 06 evaluation set
consisting of 1797 sentences. The GALE portion of
the 06 evaluation set is not used in this paper. We re-
port results on the test and blind sets. Some statistics
computed on the test data are shown in Table 1.
5.2 Alignment Model Training
For training Arabic-English alignment models, we
use Chinese, French, Russian and Spanish as bridge
languages. We train a model for Ar-En and 4 mod-
els each for Ar-X and X-En, where X is the bridge
language. To obtain aligned sentences in these lan-
guage pairs, we train 9 sentence aligners. We then
train alignment models for all 9 language-pairs us-
ing a recipe consisting of 6 Model-1 iterations and
6 HMM iterations. Finally, Word Alignment Poste-
rior Probabilities are generated over the bitext. In
Table 2, we report the perplexities of the alignment
models for the translation directions where either
Arabic or English is predicted. There are 55M Ara-
bic tokens and 58M English tokens. We observe
that the alignment model using Spanish achieves the
lowest perplexity; this value is even lower than the
perplexity of the direct Arabic-English model. Per-
plexity is related to the hardness of the word align-
ment; the results suggest that bridge languages such
as Spanish make alignment task easier while others
do not. We stress that perplexity is not related to the
alignment or the translation performance.
Bridge Perplexity
Lang ? Ar ?En
None 113.8 26.1
Es 99.0 22.9
Fr 138.6 30.2
Ru 128.3 27.5
Zh 126.1 34.6
Table 2: Perplexities of the alignment models.
5.3 Bridge Language Word Alignments
Each of the 4 bridge languages is utilized for con-
structing a word alignment for Arabic-English. Us-
ing each bridge language X, we obtain Arabic-
English word alignments in both translation direc-
tions (AE and EA). The posterior matrix for AE is
obtained using AX and XE matrices while the EA
matrix is obtained from EX and XA matrices (Equa-
tion 4). The AE (EA) matrices from the bridge
languages are then interpolated with the AE (EA)
matrix obtained from the alignment model trained
directly on Arabic-English (Section 4). The MAP
word alignment for AE (EA) direction is computed
from the AE (EA) matrix. We next outline how these
word alignments are utilized in building a phrase-
based SMT system.
5.4 Phrase-based SMT system
Our phrase-based SMT system is similar to the
alignment template system described in Och and
Ney (2004). We first extract an inventory of phrase-
pairs up to length 7 from the union of AE and EA
word alignments. Various feature functions (Och
and Ney, 2004) are then computed over the entries
in the phrase table. 5-gram word language models
in English are trained on a variety of monolingual
corpora (Brants et al, 2007). Minimum Error Rate
Training (MERT) (Och, 2003) under BLEU crite-
rion is used to estimate 20 feature function weights
over the larger development set (dev1).
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004). Decoding is done in two passes. An ini-
tial list of 1000-best hypotheses is generated by the
decoder. This list is then rescored using Minimum
Bayes-Risk (MBR) decoding (Kumar and Byrne,
2004). The MBR scaling parameter is tuned on the
smaller development set (dev2).
45
Bridge Metrics(%)
Language AE EA
Prec Rec AER Prec Rec AER
None 74.1 73.9 26.0 67.3 57.7 37.9
Es 61.7 56.3 41.1 50.0 40.2 55.4
Fr 52.9 48.0 49.7 42.3 33.6 62.5
Ru 57.4 50.8 46.1 40.2 31.6 64.6
Zh 44.3 39.3 58.3 39.7 29.9 65.9
AC1 70.0 65.0 32.6 56.8 46.4 48.9
Table 3: Alignment Performance with Bridge Lan-
guages
5.5 Alignment Results
We first report alignment performance (Table 3) of
the alignment models obtained using the bridge lan-
guages. Alignment results are reported in terms
of Precision (Prec), Recall (Rec) and Alignment
Error Rate (AER). We report these numbers on
a 94-sentence test set with translations in all six
languages and human word alignments in Arabic-
English. Our human word alignments do not dis-
tinguish between Sure and Probable links (Och and
Ney, 2003).
In these experiments, we first identify the com-
mon subset of sentences which have translations in
all six languages. Each of the 9 alignment models
is then trained on this subset. We report Alignment
performance in both translation directions: Arabic-
to-English (AE) and English-to-Arabic (EA). The
first row (None) gives the results when no bridge
language is used.
Among the bridge languages, Spanish gives the
best alignment for Arabic-English while Chinese re-
sults in the worst. This might be related to how dif-
ferent the bridge language is relative to either En-
glish or Arabic. The last row (AC1) shows the per-
formance of the alignment obtained by combining
None/Es/Fr/Ru/Zh alignments. This alignment out-
performs all bridge alignments but is weaker than
the alignment without any bridge language. Our
hypothesis is that a good choice of interpolation
weights (Equation 5) would reduce AER of the AC1
combination. However, we did not investigate these
choices in this paper. We report alignment error rates
here to give the readers an idea of the vastly differ-
ent alignment performance using each of the bridge
languages.
5.6 Translation Results
We now report translation performance of our tech-
niques. We measure performance using the NIST
implementation of case sensitive BLEU-4 on true-
cased translations. We observed in experiments
not reported here that results are almost identical
with/without Minimum Error Rate Training ; we
therefore report the results without the training. We
note that the blind set is the NIST subset of the 2006
NIST evaluation set. The systems reported here are
for the Unlimited Data Track in Arabic-to-English
and obtain competitive performance relative to the
results reported on the NIST official results page 2
We present three sets of experiments. In Table 4,
we describe the first set where all 9 alignment mod-
els are trained on nearly the same set of sentences
(1.9M sentences, 57.5M words in English). This
makes the alignment models in all bridge languages
comparable. In the first rowmarked None, we do not
use a bridge language. Instead, an Ar-En alignment
model is trained directly on the set of sentence pairs.
The next four rows give the performance of align-
ment models trained using the bridge languages Es,
Fr, Ru and Zh respectively. For each language, we
use the procedure (Section 3) to obtain the posterior
probability matrix for Arabic-English from Arabic-
X and X-English matrices. The row AC1 refers to
alignment combination using interpolation of poste-
rior probabilities described in Section 4. We com-
bine posterior probability matrices from the systems
in the first four rows: None, Es, Ru and Zh. We
exclude the Zh system from the AC1 combination
because it is found to degrade the translation perfor-
mance by 0.2 points on the test set.
In the final six rows of Table 4, we show the per-
formance of a consensus decoding technique that
produces a single output hypothesis by combin-
ing translation hypotheses from multiple systems;
this is an MBR-like candidate selection procedure
based on BLEU correlation matrices and is de-
scribed in Macherey and Och (2007). We first report
performance of the consensus output by combining
None systems with/without MERT. Each of the fol-
lowing rows provides the results from consensus de-
coding for adding an extra system both with/without
MERT. Thus, the final row (TC1) combines transla-
2
http://www.nist.gov/speech/tests/mt/mt06eval official results.html
46
tions from 12 systems: None, Es, Fr, Ru, Zh, AC1
with/without MERT. All entries marked with an as-
terisk are better than the None baseline with 95%
statistical significance computed using paired boot-
strap resampling (Koehn, 2004).
35 40 45 50 55 60 65 7037
37.5
38
38.5
39
39.5
40
40.5
None
Es
Fr
Ru
Zh
AC1
100?AER(%)
BLE
U(%
)
Figure 1: 100-AER (%) vs. BLEU(%) on the blind
set for 6 systems from Table 3.
Figure 1 shows the plot between 100-AER% (av-
erage of EA/AE directions) and BLEU for the six
systems in Table 3. We observe that AER is loosely
correlated to BLEU (? = 0.81) though the re-
lation is weak, as observed earlier by Fraser and
Marcu (2006a). Among the bridge languages, Span-
ish gives the lowest AER/highest BLEU while Chi-
nese results in highest AER/lowest BLEU. We can
conclude that Spanish is closest to Arabic/English
while Chinese is the farthest. All the bridge lan-
guages yield lower BLEU/higher AER relative to the
No-Bridge baseline. Therefore, our estimate of the
posterior probability (Equation 4) is always worse
than the posterior probability obtained using a di-
rect model. The alignment combination (AC1) be-
haves differently from other bridge systems in that it
gives a higher AER and a higher BLEU relative to
None baseline. We hypothesize that AC1 is differ-
ent from the bridge language systems since it arises
from a different process: interpolation with the di-
rect model (None).
Both system combination techniques give im-
provements relative to None baseline: alignment
combination AC1 gives a small gain (0.2 points)
while the consensus translation TC1 results in a
larger improvement (0.8 points). The last 4 rows
of the table show that the performance of the hy-
pothesis consensus steadily increases as systems get
added to the None baseline. This shows that while
bridge language systems are weaker than the di-
rect model, they can provide complementary sources
of evidence. To further validate this hypothesis,
we compute inter-system BLEU scores between
None/es and all the systems in Table 5. We observe
that the baseline (None) is very dissimilar from the
rest of the systems. We hypothesize that the baseline
system has an alignment derived from a real align-
ment model while the rest of the bridge systems are
derived using matrix multiplication. The low inter-
system BLEU scores show that the bridge systems
provide diverse hypotheses relative to the baseline
and therefore contribute to gains in consensus de-
coding.
Bridge Lang # Msents BLEU (%)
test blind
None 1.9 52.1 40.1
Es 1.9 51.7 39.8
Fr 1.9 51.2 39.5
Ru 1.9 50.4 38.7
Zh 1.9 48.4 37.1
AC1 1.9 52.1 40.3
Hypothesis Consensus
None 1.9 51.9 39.8
+Es 1.9 52.2 40.0
+Fr 1.9 52.4? 40.5?
+Ru 1.9 52.8? 40.7?
+Zh 1.9 52.6? 40.6?
+AC1 = TC1 1.9 53.0? 40.9?
Table 4: Translation Experiments for Set 1; Results
are reported on the test and blind set: (NIST portion
of 2006 NIST eval set).
Ref None es fr ru zh AC1
None 100.0 60.0 59.8 59.7 59.5 58.7
es 59.6 100.0 79.9 69.3 67.4 70.5
Table 5: Inter-system BLEU scores (%) between
None/es and all systems in Table 3.
To gain some insight about how the bridge sys-
tems help in Table 4, we present an example in Ta-
ble 6. The example shows the consensus Transla-
tions and the 12 input translations for the consensus
decoding. The example suggests that the inputs to
the consensus decoding exhibit diversity.
Table 7 reports the second and third sets of ex-
periments. For both sets, we first train each bridge
language system X using all aligned sentences avail-
47
System MERT Hypothesis
None N The President of the National Conference Visit Iraqi Kurdistan Iraqi
None Y President of the Iraqi National Conference of Iraqi Kurdistan Visit
Es N President of the Iraqi National Congress to Visit Iraqi Kurdistan
Es Y President of the Iraqi National Congress to Visit Iraqi Kurdistan
Fr N President of the Iraqi National Conference Visits Iraqi Kurdistan
Fr Y Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ru N The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ru Y Chairman of the Iraqi National Conference Visit the Iraqi Kurdistan
Zh N The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Zh Y The Chairman of the Iraqi National Conference Visit Iraqi Kurdistan
AC1 N President of the Iraqi National Congress to Visit Iraqi Kurdistan
AC1 Y Chairman of the Iraqi National Congress to Visit Iraqi Kurdistan
TC1 - The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ref - Head of Iraqi National Congress Visits Iraqi Kurdistan
Table 6: An example showing the Consensus Translation (TC1) and the 12 inputs for consensus decoding.
The final row shows the reference translation.
able in Ar, En and X. In Set 2, the first row (Union)
is an alignment model trained on all sentence-pairs
in Ar-En which are available in at least one bridge
language X. AC2 refers to alignment combination
using bridge languages Es/Fr/Ru and Union. TC2
refers to the translation combination from 12 sys-
tems: Es/Fr/Ru/Zh/Union/AC2 with/without Mini-
mum Error Rate training. Finally, the goal in Set 3
(last 3 rows) is to improve the best Arabic-English
system that can be built using all available sen-
tence pairs from the UN corpus. The first row
(Direct) gives the performance of this Ar-En sys-
tem; AC3 refers to alignment combination using
Es/Fr/Ru and Direct. TC3 merges translations from
Es/Fr/Ru/Zh/Direct/AC3. All entries marked with
an asterisk (plus) are better than the Union (Direct)
baseline with 95% statistical significance computed
using paired bootstrap resampling (Koehn, 2004).
The motivation behind Sets 2 and 3 is to train all
bridge language systems on as much bitext as possi-
ble. As a consequence, these systems give better re-
sults than the corresponding systems in Table 4. The
Union system outperforms None by 1.7/1.4 BLEU
points and provides a better baseline. We show un-
der this scenario that system combination techniques
AC2 and TC2 can still give smaller improvements
(0.3/0.5 and 1.0/0.7 points) relative to this baseline.
As mentioned earlier, our approach requires
sentence-aligned corpora. In our experiments, we
use a single sentence aligner for each language pair
(total of 9 aligners). Since these aligners make inde-
pendent decisions on sentence boundaries, we end
up with a smaller pool of sentences (1.9M) that is
common across all language pairs. In contrast, a
sentence aligner that makes simultaneous decisions
in multiple languages would result in a larger set of
common sentence pairs (close to 7M sentence pairs).
Simard (1999) describes a sentence aligner of this
type that improves alignment on a trilingual paral-
lel text. Since we do not currently have access to
such an aligner, we simulate that situation with Sets
2 and 3: AC2/AC3 do not insist that a sentence-pair
be present in all input word alignments. We note that
Set 2 is a data scenario that falls between Sets 1 and
3.
Set 3 provides the best baseline for Arabic-
English based on the UN data by training on
all parallel sentence-pairs. In this situation, sys-
tem combination with bridge languages (AC3/TC3)
gives reasonable improvements in BLEU on the test
set (0.4/1.0 points) but only modest improvements
(0.1/0.4 points) on the blind set. However, this does
show that the bridge systems continue to provide or-
thogonal evidence at different operating points.
6 Discussion
We have described a simple approach to improve
word alignments using bridge languages. This in-
cludes two components: a matrix multiplication to
assemble a posterior probability matrix for the de-
sired language-pair FE using a pair of posterior
probability matrices FG and GE relative to a bridge
language G. The second component is a recipe for
combining word alignment systems by linearly in-
48
Bridge Lang # Msents BLEU (%)
test blind
Es 4.7 53.7 40.9
Fr 4.7 53.2 40.7
Ru 4.5 52.4 39.9
Zh 3.4 49.7 37.9
Set 2
Union 7.2 53.8 41.5
AC2 7.2 54.1 42.0?
TC2 - 54.8? 42.2?
Set 3
Direct 7.0 53.9 42.2
AC3 9.0 54.3+ 42.3
TC3 - 54.9+ 42.6+
Table 7: Translation performance for Sets 2 and 3 on
test and blind:NIST portion of 2006 NIST eval set.
terpolating posterior probability matrices from dif-
ferent sources. In our case, these sources are multi-
ple bridge languages. However, this method is more
generally applicable for combining posterior matri-
ces from different alignment models such as HMM
and Model-4. Such an approach contrasts with the
log-linear HMM/Model-4 combination proposed by
Och and Ney (2003).
There has been recent work by Ayan and Dorr
(2006) on combining word alignments from differ-
ent alignment systems; this paper describes a maxi-
mum entropy framework for this combination. Their
approach operates at the level of the alignment links
and uses maximum entropy to decide whether or
not to include an alignment link in the final out-
put. In contrast, we use posterior probabilities as the
interface between different alignment models. An-
other difference is that this maxent framework re-
quires human word aligned data for training feature
weights. We do not require any human word aligned
data to train our combiner.
Another advantage of our approach is that it is
based on word alignment posterior probability ma-
trices that can be generated by any underlying align-
ment model. Therefore, this method can be used to
combine word alignments generated by fairly dis-
similar word alignment systems as long as the sys-
tems can produce posterior probabilities.
Bridge languages have been used by NLP re-
searchers as a means to induce translation lexicons
between distant languages without the need for par-
allel corpora (Schafer and Yarowsky, 2002; Mann
and Yarowsky, 2001). Our current approach differs
from these efforts in that we use bridge languages to
improve word alignment quality between sentence
pairs. Furthermore, we do not use linguistic insight
to identify bridge languages. In our framework, a
good bridge language is one that provides the best
translation performance using the posterior matrix
multiplication. Our experiments show that Spanish
is a better bridge language relative to Chinese for
Arabic-to-English translation. We speculate that if
our approach was carried out on a data set with hun-
dreds of languages, we might be able to automati-
cally identify language families.
A downside of our approach is the requirement
for exact sentence-aligned parallel data. Except for
a few corpora such as UN, European Parliament etc,
such a resource is hard to find. One solution is to cre-
ate such parallel data by automatic translation and
then retaining reliable translations by using confi-
dence metrics (Ueffing and Ney, 2005).
Our approach to using bridge languages is ex-
tremely simple. Despite its simplicity, the system
combination gives improvements in alignment and
translation performance. In future work, we will
consider several extensions to this framework that
lead to more powerful system combination strategies
using multiple bridge languages. We recall that the
present approach trains bridge systems (e.g. Arabic-
to-French, French-to-English) until the alignment
stage and then uses these for constructing Arabic-
to-English word alignment. An alternate scenario
would be to build phrase-based SMT systems for
Arabic-to-Spanish and Spanish-to-English, and then
obtain Arabic-to-English translation by first trans-
lating from Arabic into Spanish and then Spanish
into English. Such end-to-end bridge systems may
lead to an even more diverse pool of hypotheses that
could further improve system combination.
References
N. Ayan and B. Dorr. 2006. A maximum entropy
approach to combining word alignments. In HLT-
NAACL, New York, New York.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping bilingual data using consensus translation
for a multilingual instant messaging system. In COL-
ING, Taipei, Taiwan.
L. Borin. 2000. You?ll take the high road and I?ll take the
49
low road: Using a third language to improve bilingual
word alignment. In COLING, pages 97?103, Saar-
brucken, Germany.
T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007.
Large language models in machine translation. In
EMNLP, Prague, Czech Republic.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Y. Deng and W. Byrne. 2005. HMM word and
phrase alignment for statistical machine translation. In
EMNLP, Vancouver, Canada.
EU, 2005. European Parliament Proceedings.
http://www.europarl.europa.eu.
EU, 2007. JRC Acquis Corpus. http://langtech.jrc.it/JRC-
Acquis.html.
K. Filali and J. Bilmes. 2005. Leveraging multiple lan-
guages to improve statistical mt word alignments. In
IEEE Workshop on Automatic Speech Recognition and
Understanding, San Juan, Puerto Rico.
A. Fraser and D. Marcu. 2006a. Measuring word align-
ment quality for statistical machine translation. Tech-
nical Report ISI-TR-616, ISI/University of Southern
California.
A. Fraser and D. Marcu. 2006b. Semi-supervised train-
ing for statistical word alignment. In ACL, pages 769?
776, Sydney, Australia.
N. Ge. 2004. Improvements in word alignments. In
Presentation given at DARPA/TIDES workshop.
A. Ittycheriah and S. Roukos. 2005. A maximum en-
tropy word aligner for arabic-english machine transla-
tion. In EMNLP, Vancouver, Canada.
P. Koehn, 2003. European Parlia-
ment Proceedings, Sentence Aligned.
http://people.csail.mit.edu/koehn/publications/europarl/.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In EMNLP, Barcelona, Spain.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
NAACL, pages 169?176, Boston, MA, USA.
W. Macherey and F. Och. 2007. An empirical study on
computing consensus translations from multiple ma-
chine translation systems. In EMNLP, Prague, Czech
Republic.
G. Mann and D. Yarowsky. 2001. Multipath translation
lexicon induction via bridge languages. In NAACL,
Pittsburgh, PA, USA.
J. Martin, R. Mihalcea, and T. Pedersen. 2005. Word
alignment for languages with scarce resources. In ACL
Workshop on Building and Using Parallel Texts, pages
65?74, Ann Arbor, MI, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. InCOL-
ING, Geneva, Switzerland.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
EACL, Trento, Italy.
R. C. Moore. 2005. A discriminative framework for
bilingual word alignment. In EMNLP, Vancouver,
Canada.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 ? 51.
F. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, Sapporo, Japan.
P. Resnik, M. Olsen, and M. Diab. 1997. Creating a
parallel corpus from the book of 2000 tongues. In
Text Encoding Initiative 10th Anniversary User Con-
ference, Providence, RI, USA.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. In CoNLL, Taipei, Taiwan.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and P. C.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination. In
IEEE International Conference on Acoustics, Speech,
and Signal Processing, Honolulu, HI, USA.
M. Simard. 1999. Text translation alignment: Three lan-
guages are better than two. In EMNLP-VLC, College
Park, MD, USA.
N. Ueffing and H. Ney. 2005. Word-level confidence
estimation for machine translation using phrase-based
translation models. In EMNLP, pages 763 ? 770, Van-
couver, Canada.
UN, 2006. ODS UN Parallel Corpus. http://ods.un.org/.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In COLING,
pages 836?841, Copenhagen, Denmark.
50
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 986?995, Prague, June 2007. c?2007 Association for Computational Linguistics
An Empirical Study on Computing Consensus Translations
from Multiple Machine Translation Systems
Wolfgang Macherey
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
wmach@google.com
Franz Josef Och
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
och@google.com
Abstract
This paper presents an empirical study on
how different selections of input translation
systems affect translation quality in system
combination. We give empirical evidence
that the systems to be combined should be
of similar quality and need to be almost
uncorrelated in order to be beneficial for sys-
tem combination. Experimental results are
presented for composite translations com-
puted from large numbers of different re-
search systems as well as a set of transla-
tion systems derived from one of the best-
ranked machine translation engines in the
2006 NIST machine translation evaluation.
1 Introduction
Computing consensus translations from the outputs
of multiple machine translation engines has become
a powerful means to improve translation quality in
many machine translation tasks. Analogous to the
ROVER approach in automatic speech recognition
(Fiscus, 1997), a composite translation is computed
by voting on the translation outputs of multiple
machine translation systems. Depending on how
the translations are combined and how the voting
scheme is implemented, the composite translation
may differ from any of the original hypotheses.
While elementary approaches simply select for each
sentence one of the original translations, more so-
phisticated methods allow for combining transla-
tions on a word or a phrase level.
Although system combination could be shown
to result in substantial improvements in terms of
translation quality (Matusov et al, 2006; Sim et al,
2007), not every possible ensemble of translation
outputs has the potential to outperform the primary
translation system. In fact, an adverse combina-
tion of translation systems may even deteriorate
translation quality. This holds to a greater extent,
when the ensemble of translation outputs contains a
significant number of translations produced by low
performing but highly correlated systems.
In this paper we present an empirical study on
how different ensembles of translation outputs affect
performance in system combination. In particular,
we will address the following questions:
? To what extent can translation quality benefit
from combining systems developed by multiple
research labs?
Despite an increasing number of translation
engines, most state-of-the-art systems in statis-
tical machine translation are nowadays based
on implementations of the same techniques.
For instance, word alignment models are often
trained using the GIZA++ toolkit (Och and
Ney, 2003); error minimizing training criteria
such as the Minimum Error Rate Training
(Och, 2003) are employed in order to learn
feature function weights for log-linear models;
and translation candidates are produced using
phrase-based decoders (Koehn et al, 2003)
in combination with n-gram language models
(Brants et al, 2007).
All these methods are established as de facto
standards and form an integral part of most
statistical machine translation systems. This,
however, raises the question as to what ex-
tent translation quality can be expected to
improve when similarly designed systems are
combined.
? How can a set of diverse translation systems be
built from a single translation engine?
Without having access to different translation
986
engines, it is desirable to build a large number
of diverse translation systems from a single
translation engine that are useful in system
combination. The mere use of N -best lists
and word lattices is often not effective, because
N -best candidates may be highly correlated,
thus resulting in small diversity compared to
the first best hypothesis. Therefore, we need a
canonical way to build a large pool of diverse
translation systems from a single translation
engine.
? How can an ensemble of translation outputs
be selected from a large pool of translation
systems?
Once a large pool of translation systems is
available, we need an effective means to select
a small ensemble of translation outputs for
which the combined system outperforms the
best individual system.
These questions will be investigated on the basis
of three approaches to system combination: (i) an
MBR-like candidate selection method based on
BLEU correlation matrices, (ii) confusion networks
built from word sausages, and (iii) a novel two-
pass search algorithm that aims at finding consensus
translations by reordering bags of words constituting
the consensus hypothesis.
Experiments were performed on two Chinese-
English text translation corpora under the conditions
of the large data track as defined for the 2006 NIST
machine translation evaluation (MT06). Results
are reported for consensus translations built from
system outputs provided by MT06 participants as
well as systems derived from one of the best-ranked
translation engines.
The remainder of this paper is organized as fol-
lows: in Section 2, we describe three combina-
tion methods for computing consensus translations.
In Sections 3.1 and 3.2, we present experimental
results on combining system outputs provided by
MT06 participants. Section 3.3 shows how correla-
tion among translation systems affects performance
in system combination. In Section 3.4, we discuss
how a single translation engine can be modified
in order to produce a large number of diverse
translation systems. First experimental results us-
ing a greedy search algorithm to select a small
ensemble of translation outputs from a large pool
of canonically built translation systems are reported.
A summary presented in Section 4 concludes the
paper.
2 Methods for System Combination
System combination in machine translation aims to
build a composite translation from system outputs
of multiple machine translation engines. Depending
on how the systems are combined and which voting
scheme is implemented, the consensus translation
may differ from any of the original candidate trans-
lations. In this section, we discuss three approaches
to system combination.
2.1 System Combination via Candidate
Selection
The easiest and most straightforward approach to
system combination simply returns one of the orig-
inal candidate translations. Typically, this selection
is made based on translation scores, confidence esti-
mations, language and other models (Nomoto, 2004;
Paul et al, 2005). For many machine translation
systems, however, the scores are often not normal-
ized or may even not be available, which makes
it difficult to apply this technique. We therefore
propose an alternative method based on ?correlation
matrices? computed from the BLEU performance
measure (Papineni et al, 2001).
Let e1, ..., eM denote the outputs of M translation
systems, each given as a sequence of words in
the target language. An element of the BLEU
correlation matrix B  pbijq is defined as the
sentence-based BLEU score between a candidate
translation ei and a pseudo-reference translation ej
pi, j  1, ...,Mq:
bij  BPpei, ejq  exp
$
'
'
%
1
4
4?
n1
log ?npei, ejq
,
/
/
-
.
(1)
Here, BP denotes the brevity penalty factor with ?n
designating the n-gram precisions.
Because the BLEU score is computed on a sen-
tence rather than a corpus-level, n-gram precisions
are capped by the maximum over 12|ei| and ?n in
order to avoid singularities, where |ei| is the length
of the candidate translation 1.
Due to the following properties, B can be inter-
preted as a correlation matrix, although the term
does not hold in a strict mathematical sense: (i)
bij P r0, 1s; (ii) bij  1.0 ?? ei  ej ; (iii) bij 
0.0 ?? eiXej  H, i.e., bij is zero if and only if
none of the words which constitute ei can be found
1 Note that for non-zero n-gram precisions, ?n is always
larger than 12|e| .
987
in ej and vice versa. The BLEU correlation matrix
is in general, however, not symmetric, although in
practice, ||bij  bji|| is typically negligible.
Each translation system m is assigned to a system
prior weight ?m P r0, 1s, which reflects the perfor-
mance of system m relatively to all other translation
systems. If no prior knowledge is available, ?m is
set to 1{M .
Now, let ?  p?1, ..., ?M qJ denote a vector of
system prior weights and let b1, ...,bM denote the
row vectors of the matrix B. Then the translation
system with the highest consensus is given by:
e  em with
m  argmax
em
!
?J  bm
) (2)
The candidate selection rule in Eq. (2) has two useful
properties:
? The selection does not depend on scored trans-
lation outputs; the mere target word sequence
is sufficient. Hence, this technique is also
applicable to rule-based translation systems 2.
? Using the components of the row-vector bm
as feature function values for the candidate
translation em (m  1, ...,M ), the system
prior weights ? can easily be trained using
the Minimum Error Rate Training described in
(Och, 2003).
Note that the candidate selection rule in Eq. (2)
is equivalent to re-ranking candidate translations
according to the Minimum Bayes Risk (MBR) deci-
sion rule (Kumar and Byrne, 2004), provided that
the system prior weights are used as estimations
of the posterior probabilities ppe|fq for a source
sentence f . Due to the proximity of this method
to the MBR selection rule, we call this combination
scheme MBR-like system combination.
2.2 ROVER-Like Combination Schemes
ROVER-like combination schemes aim at comput-
ing a composite translation by voting on confusion
networks that are built from translation outputs
of multiple machine translation engines via an it-
erative application of alignments (Fiscus, 1997).
To accomplish this, one of the original candidate
translations, e.g. em, is chosen as the primary
translation hypothesis, while all other candidates en
pn  mq are aligned with the word sequence of
2 This property is not exclusive to this combination scheme
but also holds for the methods discussed in Sections 2.2 and 2.3.
the primary translation. To limit the costs when
aligning a permutation of the primary translation,
the alignment metric should allow for small shifts
of contiguous word sequences in addition to the
standard edit operations deletions, insertions, and
substitutions. These requirements are met by the
Translation Edit Rate (TER) (Snover et al, 2006):
TERpei, ejq
Del  Ins  Sub  Shift
|ej |
(3)
The outcome of the iterated alignments is a word
transition network which is also known as word
sausage because of the linear sequence of corre-
spondence sets that constitute the network. Since
both the order and the elements of a correspondence
set depend on the choice of the primary transla-
tion, each candidate translation is chosen in turn
as the primary system. This results in a total of
M word sausages that are combined into a single
super network. The word sequence along the cost-
minimizing path defines the composite translation.
To further optimize the word sausages, we replace
each system prior weight ?m with the lp-norm over
the normalized scalar product between the weight
vector ? and the row vector bm:
?1m 
p?J  bmq`
?
m?
p?J  bm?q`
, ` P r0, 8q (4)
As ` approaches  8, ?1m  1 if and only if
system m has the highest consensus among all input
systems; otherwise, ?1m  0. Thus, the word
sausages are able to emulate the candidate selection
rule described in Section 2.1. Setting `  0 yields
uniform system prior weights, and setting B to
the unity matrix provides the original prior weights
vector. Word sausages which take advantage of the
refined system prior weights are denoted by word
sausages+.
2.3 A Two-Pass Search Algorithm
The basic idea of the two-pass search algorithm is
to compute a consensus translation by reordering
words that are considered to be constituents of the
final consensus translation.
Initially, the two-pass search is given a repository
of candidate translations which serve as pseudo
references together with a vector of system prior
weights. In the first pass, the algorithm uses
a greedy strategy to determine a bag of words
which minimizes the position-independent word er-
ror rate (PER). These words are considered to be
988
constituents of the final consensus translation. The
greedy strategy implicitly ranks the constituents,
i.e., words selected at the beginning of the first
phase reduce the PER the most and are considered
to be more important than constituents selected in
the end. The first pass finishes when putting further
constituents into the bag of words does not improve
the PER.
The list of constituents is then passed to a sec-
ond search algorithm, which starts with the empty
string and then expands all active hypotheses by
systematically inserting the next unused word from
the list of constituents at different positions in the
current hypothesis. For instance, a partial consensus
hypothesis of length l expands into l   1 new
hypotheses of length l 1. The resulting hypotheses
are scored with respect to the TER measure based on
the repository of weighted pseudo references. Low-
scoring hypotheses are pruned to keep the space of
active hypotheses small. The algorithm will finish
if either no constituents are left or if expanding the
set of active hypotheses does not further decrease
the TER score. Optionally, the best consensus hy-
pothesis found by the two-pass search is combined
with all input translation systems via the MBR-like
combination scheme described in Section 2.1. This
refinement is called two-pass+.
2.4 Related Work
Research on multi-engine machine translation goes
back to the early nineties. In (Robert and Nirenburg,
1994), a semi-automatic approach is described that
combines outputs from three translation systems to
build a consensus translation. (Nomoto, 2004) and
(Paul et al, 2005) used translation scores, language
and other models to select one of the original
translations as consensus translation. (Bangalore et
al., 2001) used a multiple string alignment algorithm
in order to compute a single confusion network,
on which a consensus hypothesis was computed
through majority voting. Because the alignment
procedure was based on the Levenshtein distance,
it was unable to align translations with significantly
different word orders. (Jayaraman and Lavie, 2005)
tried to overcome this problem by using confi-
dence scores and language models in order to rank
a collection of synthetic combinations of words
extracted from the original translation hypotheses.
Experimental results were only reported for the
METEOR metric (Banerjee and Lavie, 2005). In
(Matusov et al, 2006), pairwise word alignments
of the original translation hypotheses were estimated
for an enhanced statistical alignment model in order
Table 1: Corpus statistics for two Chinese-English
text translation sets: ZHEN-05 is a random
selection of test data used in NIST evaluations prior
to 2006; ZHEN-06 comprises the NIST portion of
the Chinese-English evaluation data used in the
2006 NIST machine translation evaluation.
corpus Chinese English
ZHEN-05 sentences 2390
chars / words 110647 67737
ZHEN-06 sentences 1664
chars / words 64292 41845
to explicitly capture word re-ordering. Although
the proposed method was not compared with other
approaches to system combination, it resulted in
substantial gains and provided new insights into
system combination.
3 Experimental Results
Experiments were conducted on two corpora for
Chinese-English text translations, the first of which
is compiled from a random selected subset of eval-
uation data used in the NIST MT evaluations up to
the year 2005. The second data set consists of the
NIST portion of the Chinese-English data used in
the MT06 evaluation and comprises 1664 Chinese
sentences collected from broadcast news articles
(565 sentences), newswire texts (616 sentences), and
news group texts (483 sentences). Both corpora
provide 4 reference translations per source sentence.
Table 1 summarizes some corpus statistics.
For all experiments, system performance was
measured in terms of the IBM-BLEU score (Pap-
ineni et al, 2001). Compared to the NIST imple-
mentation of the BLEU score, IBM-BLEU follows
the original definition of the brevity penalty (BP)
factor: while in the NIST implementation the BP is
always based on the length of the shortest reference
translation, the BP in the IBM-BLEU score is based
on the length of the reference translation which is
closest to the candidate translation length. Typically,
IBM-BLEU scores tend to be smaller than NIST-
BLEU scores. In the following, BLEU always refers
to the IBM-BLEU score.
Except for the results reported in Section 3.2, we
used uniform system prior weights throughout all
experiments. This turned out to be more stable when
combining different sets of translation systems and
helped to improve generalization.
989
Table 2: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for primary
systems together with consensus systems for the MBR-like candidate selection method obtained by
combining each three adjacent systems with uniform system prior weights. Primary systems are sorted in
descending order with respect to their BLEU score. The 95% confidence intervals are computed using the
bootstrap re-sampling normal approximation method (Noreen, 1989).
combination primary system consensus oracle
BLEU CI 95% BP BLEU ? BP pair-CI 95% BLEU BP
01, 02, 03 32.10 (0.88) 0.93 32.97 (+0.87) 0.92 [+0.29, +1.46] 38.54 0.94
01, 15, 16 32.10 (0.88) 0.93 23.55 ( -8.54) 0.92 [ -9.29, -7.80] 33.55 0.95
02, 03, 04 31.71 (0.90) 0.96 31.55 ( -0.16) 0.92 [ -0.65, +0.29] 37.23 0.95
03, 04, 05 29.59 (0.88) 0.87 29.55 ( -0.04) 0.88 [ -0.53, +0.41] 35.55 0.92
03, 04, 06 29.59 (0.88) 0.87 29.83 (+0.24) 0.90 [ -0.29, +0.71] 35.69 0.93
04, 05, 06 27.70 (0.87) 0.94 28.52 (+0.82) 0.91 [+0.15, +1.49] 34.67 0.94
05, 06, 07 27.05 (0.81) 0.88 28.21 (+1.16) 0.92 [+0.63, +1.66] 33.89 0.94
05, 06, 08 27.05 (0.81) 0.88 28.47 (+1.42) 0.91 [+0.95, +1.95] 34.18 0.93
06, 07, 08 27.02 (0.76) 0.92 28.12 (+1.10) 0.94 [+0.59, +1.59] 33.87 0.95
07, 08, 09 26.75 (0.79) 0.97 27.79 (+1.04) 0.94 [+0.52, +1.51] 33.54 0.95
08, 09, 10 26.41 (0.81) 0.92 26.78 (+0.37) 0.94 [ -0.07, +0.86] 32.47 0.96
09, 10, 11 25.05 (0.84) 0.90 24.96 ( -0.09) 0.94 [ -0.59, +0.46] 30.92 0.97
10, 11, 12 23.48 (0.68) 1.00 24.24 (+0.76) 0.94 [+0.27, +1.30] 30.08 0.96
11, 12, 13 23.26 (0.74) 0.95 24.05 (+0.79) 0.92 [+0.40, +1.23] 29.56 0.93
12, 13, 14 22.38 (0.78) 0.87 22.68 (+0.30) 0.89 [ -0.28, +0.95] 28.58 0.91
13, 14, 15 22.13 (0.72) 0.89 21.29 ( -0.84) 0.90 [ -1.33, -0.33] 26.61 0.92
14, 15, 16 17.42 (0.66) 0.93 18.45 (+1.03) 0.92 [+0.45, +1.56] 23.30 0.95
15 17.20 (0.64) 0.91 ? ? ? ? ? ?
16 15.21 (0.63) 0.96 ? ? ? ? ? ?
3.1 Combining Multiple Research Systems
In a first experiment, we investigated the effect
of combining translation outputs provided from
different research labs. Each translation system
corresponds to a primary system submitted to the
NIST MT06 evaluation 3. Table 2 shows the BLEU
scores together with their corresponding BP factors
for the primary systems of 16 research labs (site
names were anonymized). Primary systems are
sorted in descending order with respect to their
BLEU score. Table 2 also shows the consensus
translation results for the MBR-like candidate selec-
tion method. Except where marked with an asterisk,
all consensus systems are built from the outputs
of three adjacent systems. While only few com-
bined systems show a degradation, the majority of
all consensus translations achieve substantial gains
between 0.2% and 1.4% absolute in terms of BLEU
score on top of the best individual (primary) system.
The column CI provides 95% confidence intervals
for BLEU scores with respect to the primary system
baseline using the bootstrap re-sampling normal
3 For more information see http://www.nist.gov/
speech/tests/mt/mt06eval_official_results.
html
approximation method (Noreen, 1989). The column
?pair-CI? shows 95% confidence intervals relative
to the primary system using the paired bootstrap
re-sampling method (Koehn, 2004). The princi-
ple of the paired bootstrap method is to create a
large number of corresponding virtual test sets by
consistently selecting candidate translations with re-
placement from both the consensus and the primary
system. The confidence interval is then estimated
over the differences between the BLEU scores of
corresponding virtual test sets. Improvements are
considered to be significant if the left boundary of
the confidence interval is larger than zero.
Oracle BLEU scores shown in Table 2 are com-
puted by selecting the best translation among the
three candidates. The oracle scores might indicate a
larger potential of the MBR-like selection rule, and
further gains could be expected if the candidate se-
lection rule is combined with confidence measures.
Table 2 shows that it is important that all trans-
lation systems achieve nearly equal quality; com-
bining high-performing systems with low-quality
translations typically results in clear performance
losses compared to the primary system, which is the
case when combining, e.g., systems 01, 15, and 16.
990
Table 3: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for the
combination of multiple research systems using the MBR-like selection method with uniform and trained
system prior weights. Prior weights are trained using 5-fold cross validation. The 95% confidence intervals
realtive to uniform weights are computed using the paired bootstrap re-sampling method (Koehn, 2004).
# systems combination uniform ? opt. on dev. ? opt. on test
BLEU BP BLEU BP pair-CI 95% BLEU BP
3 01 ? 03 32.98 0.92 33.03 0.93 [ -0.23, +0.34] 33.60 0.93
4 01 ? 04 33.44 0.93 33.46 0.93 [ -0.26, +0.29] 34.97 0.94
5 01 ? 05 33.07 0.92 33.14 0.93 [ -0.29, +0.43] 34.33 0.93
6 01 ? 06 32.86 0.92 33.53 0.93 [+0.26, +1.08] 34.43 0.93
7 01 ? 07 33.08 0.93 33.51 0.93 [+0.04, +0.82] 34.49 0.93
8 01 ? 08 33.12 0.93 33.47 0.93 [ -0.06, +0.75] 34.50 0.94
9 01 ? 09 33.15 0.93 33.22 0.93 [ -0.35, +0.51] 34.68 0.93
10 01 ? 10 33.01 0.93 33.59 0.94 [+0.18, +0.96] 34.79 0.94
11 01 ? 11 32.84 0.94 33.40 0.94 [+0.13, +0.98] 34.76 0.94
12 01 ? 12 32.73 0.93 33.49 0.94 [+0.34, +1.18] 34.83 0.94
13 01 ? 13 32.71 0.93 33.54 0.94 [+0.39, +1.26] 34.91 0.94
14 01 ? 14 32.66 0.93 33.69 0.94 [+0.58, +1.47] 34.97 0.94
15 01 ? 15 32.47 0.93 33.57 0.94 [+0.63, +1.57] 34.99 0.94
16 01 ? 16 32.51 0.93 33.62 0.94 [+0.62, +1.59] 35.00 0.94
3.2 Non-Uniform System Prior Weights
As pointed out in Section 2.1, a useful property
of the MBR-like system selection method is that
system prior weights can easily be trained using
the Minimum Error Rate Training (Och, 2003).
In this section, we investigate the effect of using
non-uniform system weights for the combination of
multiple research systems. Since for each research
system, only the first best translation candidate
was provided, we used a five-fold cross validation
scheme in order to train and evaluate the system
prior weights. For this purpose, all research systems
were consistently split into five random partitions of
almost equal size. The partitioning procedure was
document preserving, i.e., sentences belonging to
the same document were guaranteed to be assigned
to the same partition. Each of the five partitions
played once the role of the evaluation set while
the other four partitions were used as development
data to train the system prior weights. Consensus
systems were computed for each held out set using
the system prior weights estimated on the respec-
tive development sets. The combination results
determined on all held out sets were then concate-
nated and evaluated with respect to the ZHEN-06
reference translations. Table 3 shows the results
for the combinations of up to 16 research systems
using either uniform or trained system prior weights.
System 01 achieved the highest BLEU score on all
five constellations of development partitions and is
therefore the primary system to which all results in
Table 3 compare. In comparison to uniform weights,
consensus translations using trained weights are
more robust toward the integration of low perform-
ing systems into the combination scheme. The
best combined system obtained with trained system
prior weights (01-14) is, however, not significantly
better than the best combined system using uniform
weights (01-04), for which the 95% confidence
interval yields r0.17, 0.66s according to the paired
bootstrap re-sampling method.
Table 3 also shows the theoretically achievable
BLEU scores when optimizing the system prior
weights on the held out data. This provides an upper
bound to what extent system combination might
benefit if an ideal set of system prior weights were
used.
3.3 Effect of Correlation on System
Combination
The degree of correlation among input translation
systems is a key factor which decides whether
translation outputs can be combined such a way that
the overall system performance improves. Correla-
tion can be considered as a reciprocal measure of
diversity: if the correlation is too large (? 90%),
there will be insufficient diversity among the input
systems and the consensus system will at most be
able to only marginally outperform the best indi-
991
Table 4: BLEU scores obtained on ZHEN-05 with uniform prior weights and a 10-way system combination
using the MBR-like candidate selection rule, word sausages, and the two-pass search algorithm together
with their improved versions ?sausages+? and ?two-pass+?, respectively for different sample sizes of the
FBIS training corpus.
sampling primary mbr-like sausages sausages+ two-pass two-pass+
r%s BLEU CI 95% BP BLEU BP BLEU BP BLEU BP BLEU BP BLEU BP
5 27.82 (0.65) 1.00 29.51 1.00 29.00 0.97 30.25 0.99 29.58 0.94 29.93 0.96
10 29.70 (0.69) 1.00 31.42 1.00 30.74 0.98 31.99 0.99 31.30 0.95 31.75 0.97
20 31.37 (0.69) 1.00 32.56 1.00 32.64 1.00 33.17 0.99 32.60 0.96 32.76 0.98
40 32.66 (0.66) 1.00 33.52 1.00 33.23 0.99 33.98 1.00 33.65 0.97 33.88 0.99
80 33.67 (0.66) 1.00 34.17 1.00 33.93 0.99 34.38 1.00 34.20 0.99 34.35 1.00
100 33.90 (0.67) 1.00 34.03 1.00 33.98 1.00 34.02 1.00 33.90 1.00 34.08 1.00
vidual translation system. If the correlation is too
low (? 5%), there might be no consensus among the
input systems and the quality of the consensus trans-
lations will hardly differ from a random selection of
the candidates.
To study how correlation affects performance in
system combination, we built a large number of
systems trained on randomly sampled portions of the
FBIS 4 training data collection. Sample sizes ranged
between 5% and 100% with each larger data set dou-
bling the size of the next smaller collection. For each
sample size, we created 10 data sets, thus resulting in
a total of 610 training corpora. On each data set, a
new translation system was trained from scratch and
4 LDC catalog number: LDC2003E14
 27
 28
 29
 30
 31
 32
 33
 34
 35
 0  10  20  30  40  50  60  70  80  90  100
BL
EU
 [%
]
sampling [%]
consensus system: 10
9
8
7
6
5
4
3
primary system:  1
Figure 1: Incremental system combination on
ZHEN-05 using the MBR-like candidate selection
rule and uniform prior weights. Systems were
trained with different sample sizes of the FBIS data.
used for decoding the ZHEN-05 test sentences. All
60 systems applied the MBR decision rule (Kumar
and Byrne, 2004), which gave an additional 0.5%
gain on average on top of using the maximum a-
posteriori (MAP) decision rule. Systems trained on
equally amounts of training data were incrementally
combined. Figure 1 shows the evolution of the
BLEU scores as a function of the number of sys-
tems as the sample size is increased from 5?100%.
Table 4 shows the BLEU scores obtained with a 10-
way system combination using the MBR-like can-
didate selection rule, word sausages, and the two-
pass search algorithm together with their improved
versions ?sausages+? and ?two-pass+?, respectively.
In order to measure the correlation between the in-
dividual translation systems, we computed the inter-
system BLEU score matrix as shown exemplary
 0  10  20  30  40  50  60  70  80  90  100
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
co
rr
ela
tio
n [%
]
sampling [%]
consensus
Figure 2: Evolution of the correlation on ZHEN-05
averaged over 10 systems in the course of the sample
size.
992
Table 5: Minimum, maximum, and average inter-system BLEU score correlations for (i) the primary
systems of the 2006 NIST machine translation evaluation on the ZHEN-06 test data, (ii) different training
corpus sizes (FBIS), and (iii) a greedy strategy which chooses 15 systems out of a pool of 200 translation
systems.
ZHEN-6 ZHEN-5 ZHEN-5 ZHEN-6
16 primary FBIS sampling, 10 systems 15 systems 15 systems
systems 5% 10% 20% 40% 80% 100% greedy selection ZHEN-5 selection
min 0.08 0.38 0.44 0.47 0.53 0.60 0.72 0.55 0.50
mean 0.18 0.40 0.45 0.50 0.56 0.66 0.79 0.65 0.61
median 0.19 0.40 0.45 0.49 0.56 0.64 0.78 0.63 0.58
max 0.28 0.42 0.47 0.53 0.58 0.70 0.88 0.85 0.83
in Table 6 for the 16 MT06 primary submissions.
Figure 2 shows the evolution of the correlation
averaged over 10 systems as the sample size is
increased from 5?100%. Note that all systems were
optimized using a non-deterministic implementation
of the Minimum Error Rate Training described in
(Och, 2003). Hence, using all of the FBIS corpus
data does not necessarily result in fully correlated
systems, since the training procedure may pick a
different solution for same training data in order
to increase diversity. Both Table 4 and Figure 1
clearly indicate that increasing the correlation (and
thus reducing the diversity) substantially reduces the
potential of a consensus system to outperform the
primary translation system. Ideally, the correlation
should not be larger than 30%.
Especially for low inter-system correlations and
reduced translation quality, both the enhanced ver-
sions of the word sausage combination method
and the two-pass search outperform the MBR-like
candidate selection scheme. This advantage, how-
ever, diminishes as soon as the correlation increases
and translations produced by the individual systems
become more similar.
3.4 Toward Automatic System Generation and
Selection
Sampling the training data is an effective means
to investigate the effect of system correlation on
consensus performance. However, this is done at the
expense of the overall system quality. What we need
instead is a method to reduce correlation without
sacrificing system performance.
A simple, though computationally very expensive
way to build an ensemble of low-correlated sta-
tistical machine translation systems from a single
translation engine is to train a large pool of sys-
tems, in which each of the systems is trained with
a slightly different set of parameters. Changing
only few parameters at a time typically results in
only small changes in system performance but may
have a strong impact on system correlation. In
our experiments we observed that changing pa-
rameters which affect the training procedure at a
very early stage, are most effective and introduce
larger diversity. For instance, changing the training
procedure for word alignment models turned out to
be most beneficial; for details see (Och and Ney,
2003). Other parameters that were changed include
the maximum jump width in word re-ordering, the
choice of feature function weights for the log-linear
translation models, and the set of language models
used in decoding.
Once a large pool of translation systems has
been generated, we need a method to select a
small ensemble of diverse translation outputs that
are beneficial for computing consensus translations.
Here, we used a greedy strategy to rank the systems
with respect to their ability to improve system
Table 6: Inter-system BLEU score matrix for
primary systems of the NIST 2006 TIDES machine
translation evaluation on the ZHEN-06 test data.
Id 01 02 03 04 05    14 15 16
01 1.00 0.27 0.26 0.23 0.26    0.15 0.15 0.12
02 0.27 1.00 0.27 0.22 0.25    0.15 0.15 0.12
03 0.26 0.27 1.00 0.21 0.28    0.15 0.15 0.10
04 0.23 0.22 0.21 1.00 0.19    0.14 0.12 0.12
05 0.26 0.25 0.28 0.19 1.00    0.16 0.17 0.11
06 0.27 0.24 0.25 0.21 0.26    0.16 0.18 0.13
.
.
.
.
.
.
.
.
.
14 0.15 0.15 0.15 0.14 0.16    1.00 0.12 0.08
15 0.15 0.15 0.15 0.12 0.17    0.12 1.00 0.09
16 0.12 0.12 0.10 0.12 0.11    0.08 0.09 1.00
993
 37.7
 37.8
 37.9
 38
 38.1
 38.2
 38.3
 38.4
 5  10  15  20  25  30  35  40
BL
EU
[%
]
number of systems
ZHEN-5: consensus system
primary system
 5  10  15  20  25  30  35  40
 31.2
 31.4
 31.6
 31.8
 32
 32.2
 32.4
 32.6
BL
EU
[%
]
number of systems
ZHEN-6: oracle selection
consensus system
primary system
Figure 3: BLEU score of the consensus translation as a function of the number of systems on the ZHEN-05
sentences (left) and ZHEN-06 sentences (right). The middle curve (right) shows the variation of the BLEU
score on the ZHEN-06 data when the greedy selection of the ZHEN-05 is used.
combination. Initially, the greedy strategy selected
the best individual system and then continued by
adding those systems to the ensemble, which gave
the highest gain in terms of BLEU score according
to the MBR-like system combination method. Note
that the greedy strategy is not guaranteed to increase
the BLEU score of the combined system when a
new system is added to the ensemble of translation
systems.
In a first experiment, we trained approximately
200 systems using different parameter settings in
training. Each system was then used to decode both
the ZHEN-05 and the ZHEN-06 test sentences using
the MBR decision rule. The upper curve in Figure 3
(left) shows the evolution of the BLEU score on
the ZHEN-05 sentences in the course of the number
of selected systems. The upper curve in Figure 3
(right) shows the BLEU score of the consensus
translation as a function of the number of systems
when the selection is done on the ZHEN-06 set. This
serves as an oracle. The middle curve (right) shows
the function of the BLEU score when the system
selection made on the ZHEN-05 set is used in order
to combine the translation outputs for the ZHEN-06
data. Although system combination gave moderate
improvements on top of the primary system, the
greedy strategy still needs further refinements in or-
der to improve generalization. While the correlation
statistics shown in Table 5 indicate that changing the
training parameters helps to substantially decrease
system correlation, there is still need for additional
methods in order to reduce the level of inter-system
BLEU scores such that they fall within the range of
r0.2, 0.3s.
4 Conclusions
In this paper, we presented an empirical study
on how different selections of translation outputs
affect translation quality in system combination.
Composite translations were computed using (i) a
candidate selection method based on inter-system
BLEU score matrices, (ii) an enhanced version of
word sausage networks, and (iii) a novel two-pass
search algorithm which determines and re-orders
bags of words that build the constituents of the final
consensus hypothesis. All methods gave statistically
significant improvements.
We showed that both a high diversity among the
original translation systems and a similar translation
quality among the translation systems are essential
in order to gain substantial improvements on top of
the best individual translation systems.
Experiments were conducted on the NIST portion
of the Chinese English text translation corpus used
for the 2006 NIST machine translation evaluation.
Combined systems were built from primary systems
of up to 16 different research labs as well as systems
derived from one of the best-ranked translation
engines.
We trained a large pool of translation systems
from a single translation engine and presented first
experimental results for a greedy search to select an
ensemble of translation systems for system combi-
nation.
994
References
S. Banerjee and A. Lavie. 2005. METEOR: An
Automatic Metric for MT Evaluation with Improved
Correlation with Human Judgments. In Proceedings
of Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, 43th Annual
Meeting of the Association of Computational Linguis-
tics (ACL-2005), Ann Arbor, MI, USA, June.
S. Bangalore, G. Bodel, and G. Riccardi. 2001. Com-
puting Consensus Translation from Multiple Machine
Translation Systems. In 2001 Automatic Speech
Recognition and Understanding (ASRU) Workshop,
Madonna di Campiglio, Trento, Italy, December.
T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007.
Large Language Models in Machine Tranlation. In
Proceedings of the 2007 Conference on Empirical
Methods in Natural Language Processing, Prague,
Czech Republic. Association for Computational Lin-
guistics.
J. G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Voting
Error Reduction (ROVER). In Proceedings 1997
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?352, Santa Barbara, CA,
USA, December.
S. Jayaraman and A. Lavie. 2005. Multi-Engine Ma-
chine Translation Guided by Explicit Word Matching.
In 10th Conference of the European Association for
Machine Translation (EAMT), pages 143?152, Bu-
dapest, Hungary.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology, pages 48?54,
Edmonton, Canada. Association for Computational
Linguistics.
P. Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388?395, Barcelona,
Spain, August. Association for Computational Lin-
guistics.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In Proc. HLT-NAACL, pages 196?176, Boston, MA,
USA, May.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 33?40, Trento, Italy, April.
T. Nomoto. 2004. Multi-Engine Machine Translation
with Voted Language Model. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 494?501,
Barcelona, Spain, July.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons, Canada.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In 41st Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Technical Report RC22176 (W0109-
022), IBM Research Division, Thomas J. Watson
Research Center, Yorktown Heights, NY, USA.
M. Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma, and
E. Sumita. 2005. Nobody is Perfect: ATR?s Hybrid
Approach to Spoken Language Translation. In Inter-
national Workshop on Spoken Language Translation,
pages 55?62, Pittsburgh, PA, USA, October.
F. Robert and S. Nirenburg. 1994. Three Heads are
Better than One. In Proceedings of the Fourth ACL
Conference on Applied Natural Language Processing,
Stuttgart, Germany, October.
K. C. Sim, W. Byrne, M. Gales, H. Sahbi, and P.C.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination. In
IEEE Int. Conf. on Acoustics, Speech, and Signal
Processing, Honolulu, HI, USA, April.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-
ulla, and R. Weischedel. 2006. A Study of Translation
Edit Rate with Targeted Human Annotation. In
Proceedings of Association for Machine Translation in
the Americas.
995
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620?629,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
Roy W. Tromble1 and Shankar Kumar2 and Franz Och2 and Wolfgang Macherey2
1Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
royt@jhu.edu
2 Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,och,wmach}@google.com
Abstract
We present Minimum Bayes-Risk (MBR) de-
coding over translation lattices that compactly
encode a huge number of translation hypothe-
ses. We describe conditions on the loss func-
tion that will enable efficient implementation
of MBR decoders on lattices. We introduce
an approximation to the BLEU score (Pap-
ineni et al, 2001) that satisfies these condi-
tions. The MBR decoding under this approx-
imate BLEU is realized using Weighted Fi-
nite State Automata. Our experiments show
that the Lattice MBR decoder yields mod-
erate, consistent gains in translation perfor-
mance over N-best MBR decoding on Arabic-
to-English, Chinese-to-English and English-
to-Chinese translation tasks. We conduct a
range of experiments to understand why Lat-
tice MBR improves upon N-best MBR and
study the impact of various parameters on
MBR performance.
1 Introduction
Statistical language processing systems for speech
recognition, machine translation or parsing typically
employ the Maximum A Posteriori (MAP) deci-
sion rule which optimizes the 0-1 loss function. In
contrast, these systems are evaluated using metrics
based on string-edit distance (Word Error Rate), n-
gram overlap (BLEU score (Papineni et al, 2001)),
or precision/recall relative to human annotations.
Minimum Bayes-Risk (MBR) decoding (Bickel and
Doksum, 1977) aims to address this mismatch by se-
lecting the hypothesis that minimizes the expected
error in classification. Thus it directly incorporates
the loss function into the decision criterion. The ap-
proach has been shown to give improvements over
the MAP classifier in many areas of natural lan-
guage processing including automatic speech recog-
nition (Goel and Byrne, 2000), machine transla-
tion (Kumar and Byrne, 2004; Zhang and Gildea,
2008), bilingual word alignment (Kumar and Byrne,
2002), and parsing (Goodman, 1996; Titov and Hen-
derson, 2006; Smith and Smith, 2007).
In statistical machine translation, MBR decoding
is generally implemented by re-ranking an N -best
list of translations produced by a first-pass decoder;
this list typically contains between 100 and 10, 000
hypotheses. Kumar and Byrne (2004) show that
MBR decoding gives optimal performance when the
loss function is matched to the evaluation criterion;
in particular, MBR under the sentence-level BLEU
loss function (Papineni et al, 2001) gives gains on
BLEU. This is despite the fact that the sentence-level
BLEU loss function is an approximation to the exact
corpus-level BLEU.
A different MBR inspired decoding approach is
pursued in Zhang and Gildea (2008) for machine
translation using Synchronous Context Free Gram-
mars. A forest generated by an initial decoding pass
is rescored using dynamic programming to maxi-
mize the expected count of synchronous constituents
in the tree that corresponds to the translation. Since
each constituent adds a new 4-gram to the existing
translation, this approach approximately maximizes
the expected BLEU.
In this paper we explore a different strategy
to perform MBR decoding over Translation Lat-
tices (Ueffing et al, 2002) that compactly encode a
huge number of translation alternatives relative to an
N -best list. This is a model-independent approach
620
in that the lattices could be produced by any statis-
tical MT system ? both phrase-based and syntax-
based systems would work in this framework. We
will introduce conditions on the loss functions that
can be incorporated in Lattice MBR decoding. We
describe an approximation to the BLEU score (Pa-
pineni et al, 2001) that will satisfy these condi-
tions. Our Lattice MBR decoding is realized using
Weighted Finite State Automata.
We expect Lattice MBR decoding to improve
upon N -best MBR primarily because lattices con-
tain many more candidate translations than the N -
best list. This has been demonstrated in speech
recognition (Goel and Byrne, 2000). We conduct
a range of translation experiments to analyze lattice
MBR and compare it with N -best MBR. An impor-
tant aspect of our lattice MBR is the linear approxi-
mation to the BLEU score. We will show that MBR
decoding under this score achieves a performance
that is at least as good as the performance obtained
under sentence-level BLEU score.
The rest of the paper is organized as follows. We
review MBR decoding in Section 2 and give the for-
mulation in terms of a gain function. In Section 3,
we describe the conditions on the gain function for
efficient decoding over a lattice. The implementa-
tion of lattice MBR with Weighted Finite State Au-
tomata is presented in Section 4. In Section 5, we in-
troduce the corpus BLEU approximation that makes
it possible to perform efficient lattice MBR decod-
ing. An example of lattice MBR with a toy lattice
is presented in Section 6. We present lattice MBR
experiments in Section 7. A final discussion is pre-
sented in Section 8.
2 Minimum Bayes Risk Decoding
Minimum Bayes-Risk (MBR) decoding aims to find
the candidate hypothesis that has the least expected
loss under the probability model (Bickel and Dok-
sum, 1977). We begin with a review of MBR decod-
ing for Statistical Machine Translation (SMT).
Statistical MT (Brown et al, 1990; Och and Ney,
2004) can be described as a mapping of a word se-
quence F in the source language to a word sequence
E in the target language; this mapping is produced
by the MT decoder ?(F ). If the reference transla-
tion E is known, the decoder performance can be
measured by the loss function L(E, ?(F )). Given
such a loss function L(E,E?) between an automatic
translation E? and the reference E, and an under-
lying probability model P (E|F ), the MBR decoder
has the following form (Goel and Byrne, 2000; Ku-
mar and Byrne, 2004):
E? = argmin
E??E
R(E?)
= argmin
E??E
?
E?E
L(E,E?)P (E|F ),
where R(E?) denotes the Bayes risk of candidate
translation E? under the loss function L.
If the loss function between any two hypotheses
can be bounded: L(E,E?) ? Lmax, the MBR de-
coder can be rewritten in terms of a gain function
G(E,E?) = Lmax ? L(E,E?):
E? = argmax
E??E
?
E?E
G(E,E?)P (E|F ). (1)
We are interested in performing MBR decoding
under a sentence-level BLEU score (Papineni et al,
2001) which behaves like a gain function: it varies
between 0 and 1, and a larger value reflects a higher
similarity. We will therefore use Equation 1 as the
MBR decoder.
We note that E represents the space of transla-
tions. For N -best MBR, this space E is the N -best
list produced by a baseline decoder. We will investi-
gate the use of a translation lattice for MBR decod-
ing; in this case, E will represent the set of candi-
dates encoded in the lattice.
In general, MBR decoding can use different
spaces for hypothesis selection and risk computa-
tion: argmax and the sum in Equation 1 (Goel,
2001). As an example, the hypothesis could be se-
lected from the N -best list while the risk is com-
puted based on the entire lattice. Therefore, the
MBR decoder can be more generally written as fol-
lows:
E? = argmax
E??Eh
?
E?Ee
G(E,E?)P (E|F ), (2)
where Eh refers to the Hypothesis space from where
the translations are chosen, and Ee refers to the Evi-
dence space that is used for computing the Bayes-
risk. We will present experiments (Section 7) to
show the relative importance of these two spaces.
621
3 Lattice MBR Decoding
We now present MBR decoding on translation lat-
tices. A translation word lattice is a compact rep-
resentation for very large N -best lists of transla-
tion hypotheses and their likelihoods. Formally,
it is an acyclic Weighted Finite State Acceptor
(WFSA) (Mohri, 2002) consisting of states and arcs
representing transitions between states. Each arc is
labeled with a word and a weight. Each path in the
lattice, consisting of consecutive transitions begin-
ning at the distinguished initial state and ending at a
final state, expresses a candidate translation. Aggre-
gation of the weights along the path1 produces the
weight of the path?s candidate H(E,F ) according
to the model. In our setting, this weight will imply
the posterior probability of the translation E given
the source sentence F :
P (E|F ) =
exp (?H(E,F ))
?
E??E exp (?H(E
?, F ))
. (3)
The scaling factor ? ? [0,?) flattens the distribu-
tion when ? < 1, and sharpens it when ? > 1.
Because a lattice may represent a number of can-
didates exponential in the size of its state set, it is of-
ten impractical to compute the MBR decoder (Equa-
tion 1) directly. However, if we can express the gain
function G as a sum of local gain functions gi, then
we now show that Equation 1 can be refactored and
the MBR decoder can be computed efficiently. We
loosely call a gain function local if it can be ap-
plied to all paths in the lattice via WFSA intersec-
tion (Mohri, 2002) without significantly multiplying
the number of states.
In this paper, we are primarily concerned with lo-
cal gain functions that weight n-grams. Let N =
{w1, . . . , w|N |} be the set of n-grams and let a local
gain function gw : E ? E ? R, for w ? N , be as
follows:
gw(E,E
?) = ?w#w(E
?)?w(E), (4)
where ?w is a constant, #w(E?) is the number of
times that w occurs in E?, and ?w(E) is 1 if w ? E
and 0 otherwise. That is, gw is ?w times the number
of occurrences of w in E?, or zero if w does not oc-
cur in E. We first assume that the overall gain func-
tion G(E,E?) can then be written as a sum of local
1using the log semiring?s extend operator
gain functions and a constant ?0 times the length of
the hypothesis E?.
G(E,E?) = ?0|E
?|+
?
w?N
gw(E,E
?) (5)
= ?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
Given a gain function of this form, we can rewrite
the risk (sum in Equation 1) as follows
?
E?E
G(E,E?)P (E|F )
=
?
E?E
(
?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
)
P (E|F )
= ?0|E
?|+
?
w?N
?w#w(E
?)
?
E?Ew
P (E|F ),
where Ew = {E ? E|?w(E) > 0} represents the
paths of the lattice containing the n-gram w at least
once. TheMBR decoder on lattices (Equation 1) can
therefore be written as
E? = argmax
E??E
{
?0|E
?|+
?
w?N
?w#w(E
?)p(w|E)
}
. (6)
Here p(w|E) =
?
E?Ew P (E|F ) is the posterior
probability of the n-gram w in the lattice. We have
thus replaced a summation over a possibly exponen-
tial number of items (E ? E) with a summation over
the number of n-grams that occur in E , which is at
worst polynomial in the number of edges in the lat-
tice that defines E . We compute the posterior proba-
bility of each n-gram w as:
p(w|E) =
?
E?Ew
P (E|F ) =
Z(Ew)
Z(E)
, (7)
where Z(E) =
?
E??E exp(?H(E
?, F )) (denomi-
nator in Equation 3) and
Z(Ew) =
?
E??Ew exp(?H(E
?, F )). Z(E) and
Z(Ew) represent the sums2 of weights of all paths
in the lattices Ew and E respectively.
4 WFSA MBR Computations
We now show how the Lattice MBR Decision Rule
(Equation 6) can be implemented using Weighted
Finite State Automata (Mohri, 1997). There are four
steps involved in decoding starting from weighted
finite-state automata representing the candidate out-
puts of a translation system. We will describe these
2in the log semiring, where log+(x, y) = log(ex + ey) is
the collect operator (Mohri, 2002)
622
steps in the setting where the evidence lattice Ee may
be different from the hypothesis lattice Eh (Equa-
tion 2).
1. Extract the set of n-grams that occur in the ev-
idence lattice Ee. For the usual BLEU score, n
ranges from one to four.
2. Compute the posterior probability p(w|E) of
each of these n-grams.
3. Intersect each n-gram w, with an appropriate
weight (from Equation 6), to an initially un-
weighted copy of the hypothesis lattice Eh.
4. Find the best path in the resulting automaton.
Computing the set of n-grams N that occur in a
finite automaton requires a traversal, in topological
order, of all the arcs in the automaton. Because the
lattice is acyclic, this is possible. Each state q in the
automaton has a corresponding set of n-grams Nq
ending there.
1. For each state q,Nq is initialized to {}, the set
containing the empty n-gram.
2. Each arc in the automaton extends each of its
source state?s n-grams by its word label, and
adds the resulting n-grams to the set of its tar-
get state. ( arcs do not extend n-grams, but
transfer them unchanged.) n-grams longer than
the desired order are discarded.
3. N is the union over all states q of Nq.
Given an n-gram, w, we construct an automaton
matching any path containing the n-gram, and in-
tersect that automaton with the lattice to find the set
of paths containing the n-gram (Ew in Equation 7).
Suppose E represent the weighted lattice, we com-
pute3: Ew = E ? (w w ??), where w = (?? w ??)
is the language that contains all strings that do not
contain the n-gram w. The posterior probability
p(w|E) of n-gram w can be computed as a ratio of
the total weights of paths in Ew to the total weights
of paths in the original lattice (Equation 7).
For each n-gram w ? N , we then construct
an automaton that accepts an input E with weight
3in the log semiring (Mohri, 2002)
equal to the product of the number of times the n-
gram occurs in the input (#w(E)), the n-gram fac-
tor ?w from Equation 6, and the posterior proba-
bility p(w|E). The automaton corresponds to the
weighted regular expression (Karttunen et al, 1996):
w?(w/(?wp(w|E)) w?)?.
We successively intersect each of these automata
with an automaton that begins as an unweighted
copy of the lattice Eh. This automaton must also
incorporate the factor ?0 of each word. This can
be accomplished by intersecting the unweighted lat-
tice with the automaton accepting (?/?0)?. The
resulting MBR automaton computes the total ex-
pected gain of each path. A path in this automa-
ton that corresponds to the word sequence E? has
cost: ?0|E?|+
?
w?N ?w#w(E)p(w|E) (expression
within the curly brackets in Equation 6).
Finally, we extract the best path from the resulting
automaton4, giving the lattice MBR candidate trans-
lation according to the gain function (Equation 6).
5 Linear Corpus BLEU
Our Lattice MBR formulation relies on the decom-
position of the overall gain function as a sum of lo-
cal gain functions (Equation 5). We here describe a
linear approximation to the log(BLEU score) (Pap-
ineni et al, 2001) which allows such a decomposi-
tion. This will enable us to rewrite the log(BLEU)
as a linear function of n-gram matches and the hy-
pothesis length. Our strategy will be to use a first
order Taylor-series approximation to what we call
the corpus log(BLEU) gain: the change in corpus
log(BLEU) contributed by the sentence relative to
not including that sentence in the corpus.
Let r be the reference length of the corpus, c0 the
candidate length, and {cn|1 ? n ? 4} the number
of n-gram matches. Then, the corpus BLEU score
B(r, c0, cn) can be defined as follows (Papineni et
al., 2001):
logB = min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0 ??n
,
? min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0
,
where we have ignored ?n, the difference between
the number of words in the candidate and the num-
4in the (max,+) semiring (Mohri, 2002)
623
ber of n-grams. If L is the average sentence length
in the corpus, ?n ? (n? 1)
c0
L .
The corpus log(BLEU) gain is defined as the
change in log(BLEU) when a new sentence?s (E?)
statistics are added to the corpus statistics:
G = logB? ? logB,
where the counts in B? are those of B plus those for
the current sentence. We will assume that the brevity
penalty (first term in the above approximation) does
not change when adding the new sentence. In exper-
iments not reported here, we found that taking into
account the brevity penalty at the sentence level can
cause large fluctuations in lattice MBR performance
on different test sets. We therefore treat only cns as
variables.
The corpus log BLEU gain is approximated by a
first-order vector Taylor series expansion about the
initial values of cn.
G ?
N?
n=0
(c?n ? cn)
? logB?
?c?n
?
?
?
?
c?n=cn
, (8)
where the partial derivatives are given by
? logB
?c0
=
?1
c0
, (9)
? logB
?cn
=
1
4cn
.
Substituting the derivatives in Equation 8 gives
G = ? logB ? ?
?c0
c0
+
1
4
4?
n=1
?cn
cn
, (10)
where each ?cn = c?n ? cn counts the statistic in
the sentence of interest, rather than the corpus as a
whole. This score is therefore a linear function in
counts of words ?c0 and n-gram matches ?cn. Our
approach ignores the count clipping present in the
exact BLEU score where a correct n-gram present
once in the reference but several times in the hypoth-
esis will be counted only once as correct. Such an
approach is also followed in Dreyer et al (2007).
Using the above first-order approximation to gain
in log corpus BLEU, Equation 9 implies that ?0, ?w
from Section 3 would have the following values:
?0 =
?1
c0
(11)
?w =
1
4c|w|
.
5.1 N-gram Factors
We now describe how the n-gram factors (Equa-
tion 11) are computed. The factors depend on
a set of n-gram matches and counts (cn; n ?
{0, 1, 2, 3, 4}). These factors could be obtained from
a decoding run on a development set. However, do-
ing so could make the performance of lattice MBR
very sensitive to the actual BLEU scores on a partic-
ular run. We would like to avoid such a dependence
and instead, obtain a set of parameters which can
be estimated from multiple decoding runs without
MBR. To achieve this, we make use of the properties
of n-gram matches. It is known that the average n-
gram precisions decay approximately exponentially
with n (Papineni et al, 2001). We now assume that
the number of matches of each n-gram is a constant
ratio r times the matches of the corresponding n? 1
gram.
If the unigram precision is p, we can obtain the
n-gram factors (n ? {1, 2, 3, 4}) (Equation 11) as a
function of the parameters p and r, and the number
of unigram tokens T :
?0 =
?1
T
(12)
?n =
1
4Tp? rn?1
We set p and r to the average values of unigram pre-
cision and precision ratio across multiple develop-
ment sets. Substituting the above factors in Equa-
tion 6, we find that the MBR decision does not de-
pend on T ; therefore any value of T can be used.
6 An Example
Figure 1 shows a toy lattice and the final MBR au-
tomaton (Section 4) for BLEU with a maximum n-
gram order of 2. We note that the MBR hypothesis
(bcde) has a higher decoder cost relative to the MAP
hypothesis (abde). However, bcde gets a higher ex-
pected gain (Equation 6) than abde since it shares
more n-grams with the Rank-3 hypothesis (bcda).
This illustrates how a lattice can help select MBR
translations that can differ from the MAP transla-
tion.
7 Experiments
We now present experiments to evaluate MBR de-
coding on lattices under the linear corpus BLEU
624
01
2
3
4
5
6
7
8
9
10
0
1
2
3
4
7
5
8
6
c/0.013
d/0.013
d/?0.008
d/?0.008
e/0.004
a/0.038
a/0.5
b/0.6
b/0.6
b/0.6
c/0.6
c/0.6
d/0.3
d/0.4
e/0.5
a/0.5
a/0.063
b/0.043
b/0.043
b/0.013
c/0.013
Figure 1: An example translation lattice with decoder
costs (top) and its MBR Automaton for BLEU-2 (bot-
tom). The bold path in the top is the MAP hypothesis
and the bold path in the bottom is the MBR hypothe-
sis. The precision parameters in Equation 12 are set to:
T = 10, p = 0.85, r = 0.72.
Dataset # of sentences
aren zhen enzh
dev1 1353 1788 1664
dev2 663 919 919
blind 1360 1357 1859
Table 1: Statistics over the development and test sets.
gain. We start with a description of the data sets
and the SMT system.
7.1 Development and Blind Test Sets
We present our experiments on the constrained data
track of the NIST 2008 Arabic-to-English (aren),
Chinese-to-English (zhen), and English-to-Chinese
(enzh) machine translation tasks.5 In all language
pairs, the parallel and monolingual data consists of
all the allowed training sets in the constrained track.
For each language pair, we use two development
sets: one for Minimum Error Rate Training (Och,
2003; Macherey et al, 2008), and the other for tun-
ing the scale factor for MBR decoding. Our devel-
opment sets consists of the NIST 2004/2003 evalu-
ation sets for both aren and zhen, and NIST 2006
(NIST portion)/2003 evaluation sets for enzh. We
report results on NIST 2008 which is our blind test
set. Statistics computed over these data sets are re-
ported in Table 1.
5http://www.nist.gov/speech/tests/mt/
7.2 MT System Description
Our phrase-based statistical MT system is similar to
the alignment template system described in Och and
Ney (2004). The system is trained on parallel cor-
pora allowed in the constrained track. We first per-
form sentence and sub-sentence chunk alignment on
the parallel documents. We then train word align-
ment models (Och and Ney, 2003) using 6 Model-1
iterations and 6 HMM iterations. An additional 2 it-
erations of Model-4 are performed for zhen and enzh
pairs. Word Alignments in both source-to-target
and target-to-source directions are obtained using
the Maximum A-Posteriori (MAP) framework (Ma-
tusov et al, 2004). An inventory of phrase-pairs
up to length 5 is then extracted from the union of
source-target and target-source alignments. Several
feature functions are then computed over the phrase-
pairs. 5-gram word language models are trained on
the allowed monolingual corpora. Minimum Error
Rate Training under BLEU is used for estimating
approximately 20 feature function weights over the
dev1 development set.
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004) using two decoding passes. The first de-
coder pass generates either a lattice or anN -best list.
MBR decoding is performed in the second pass. The
MBR scaling parameter (? in Equation 3) is tuned
on the dev2 development set.
7.3 Translation Results
We next report translation results from lattice MBR
decoding. All results will be presented on the NIST
2008 evaluation sets. We report results using the
NIST implementation of the BLEU score which
computes the brevity penalty using the shortest ref-
erence translation for each segment (NIST, 2002
2008). The BLEU scores are reported at the word-
level for aren and zhen but at the character level for
enzh. We measure statistical significance using 95%
confidence intervals computed with paired bootstrap
resampling (Koehn, 2004). In all tables, systems in a
column show statistically significant differences un-
less marked with an asterisk.
We first compare lattice MBR toN -best MBR de-
coding and MAP decoding (Table 2). In these ex-
periments, we hold the likelihood scaling factor ? a
625
BLEU(%)
aren zhen enzh
MAP 43.7 27.9 41.4
N -best MBR 43.9 28.3? 42.0
Lattice MBR 44.9 28.5? 42.6
Table 2: Lattice MBR, N -best MBR & MAP decoding.
On zhen, Lattice MBR and N -best MBR do not show
statistically significant differences.
constant; it is set to 0.2 for aren and enzh, and 0.1
for zhen. The translation lattices are pruned using
Forward-Backward pruning (Sixtus and Ortmanns,
1999) so that the average numbers of arcs per word
(lattice density) is 30. For N -best MBR, we use
N -best lists of size 1000. To match the loss func-
tion, Lattice MBR is performed at the word level for
aren/zhen and at the character level for enzh. Our
lattice MBR is implemented using the Google Open-
Fst library.6 In our experiments, p, r (Equation 12)
have values of 0.85/0.72, 0.80/0.62, and 0.63/0.48
for aren, zhen, and enzh respectively.
We note that Lattice MBR provides gains of 0.2-
1.0 BLEU points over N -best MBR, which in turn
gives 0.2-0.6 BLEU points over MAP. These gains
are obtained on top of a baseline system that has
competitive performance relative to the results re-
ported in the NIST 2008 Evaluation.7 This demon-
strates the effectiveness of lattice MBR decoding as
a realization of MBR decoding which yields sub-
stantial gains over the N -best implementation.
The gains from lattice MBR over N -best MBR
could be due to a combination of factors. These in-
clude: 1) better approximation of the corpus BLEU
score, 2) larger hypothesis space, and 3) larger evi-
dence space. We now present experiments to tease
apart these factors.
Our first experiment restricts both the hypothesis
and evidence spaces in lattice MBR to the 1000-best
list (Table 3). We compare this toN -best MBRwith:
a) sentence-level BLEU, and b) sentence-level log
BLEU.
The results show that when restricted to the 1000-
best list, Lattice MBR performs slightly better than
N -best MBR (with sentence BLEU) on aren/enzh
while N -best MBR is better on zhen. We hypothe-
6http://www.openfst.org/
7
http://www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
BLEU(%)
aren zhen enzh
Lattice MBR, Lin. Corpus BLEU 44.2 28.1 42.2
N -best MBR, Sent. BLEU 43.9? 28.3? 42.0?
N -best MBR, Sent. Log BLEU 44.0? 28.3? 41.9?
Table 3: Lattice and N-best MBR (with Sentence
BLEU/Sentence log BLEU) on a 1000-best list. In each
column, entries with an asterisk do not show statistically
significant differences.
BLEU(%)
Hyp Space Evid Space aren zhen enzh
Lattice Lattice 44.9 28.5 42.6
1000-best Lattice 44.6 28.5 42.6
Lattice 1000-best 44.1? 28.0? 42.1
1000-best 1000-best 44.2? 28.1? 42.2
Table 4: Lattice MBR with restrictions on hypothesis and
evidence spaces. In each column, entries with an asterisk
do not show statistically significant differences.
size that on aren/enzh, the linear corpus BLEU gain
(Equation 10) is better correlated to the actual cor-
pus BLEU than sentence-level BLEU while the op-
posite is true on zhen. N -best MBR gives similar
results with either sentence BLEU or sentence log
BLEU. This confirms that using a log BLEU score
does not change the outcome of MBR decoding and
further justifies our Taylor-series approximation of
the log BLEU score.
We next attempt to understand factors 2 and 3. To
do that, we carry out lattice MBR when either the
hypothesis or the evidence space in Equation 2 is re-
stricted to 1000-best hypotheses (Table 4). For com-
parison, we also include results from lattice MBR
when both hypothesis and evidence spaces are iden-
tical: either the full lattice or the 1000-best list (from
Tables 2 and 3).
These results show that lattice MBR results are
almost unchanged when the hypothesis space is re-
stricted to a 1000-best list. However, when the ev-
idence space is shrunk to a 1000-best list, there is
a significant degradation in performance; these lat-
ter results are almost identical to the scenario when
both evidence and hypothesis spaces are restricted
to the 1000-best list. This experiment throws light
on what makes lattice MBR effective over N -best
MBR. Relative to the N -best list, the translation lat-
tice provides a better estimate of the expected BLEU
score. On the other hand, there are few hypotheses
626
outside the 1000-best list which are selected by lat-
tice MBR.
Finally, we show how the performance of lattice
MBR changes as a function of the lattice density.
The lattice density is the average number of arcs per
word and can be varied using Forward-Backward
pruning (Sixtus and Ortmanns, 1999). Figure 2 re-
ports the average number of lattice paths and BLEU
scores as a function of lattice density. The results
show that Lattice MBR performance generally im-
proves when the size of the lattice is increased.
However, on zhen, there is a small drop beyond a
density of 10. This could be due to low quality (low
posterior probability) hypotheses that get included at
the larger densities and result in a poorer estimate of
the expected BLEU score. On aren and enzh, there
are some gains beyond a lattice density of 30. These
gains are relatively small and come at the expense
of higher memory usage; we therefore work with a
lattice density of 30 in all our experiments. We note
that Lattice MBR is operating over lattices which are
gigantic in comparison to the number of paths in an
N -best list. At a lattice density of 30, the lattices in
aren contain on an average about 1081 hypotheses!
7.4 Lattice MBR Scale Factor
We next examine the role of the scale factor ? in
lattice MBR decoding. The MBR scale factor de-
termines the flatness of the posterior distribution
(Equation 3). It is chosen using a grid search on the
dev2 set (Table 1). Figure 3 shows the variation in
BLEU scores on eval08 as this parameter is varied.
The results show that it is important to tune this fac-
tor. The optimal scale factor is identical for all three
language pairs. In experiments not reported in this
paper, we have found that the optimal scaling factor
on a moderately sized development set carries over
to unseen test sets.
7.5 Maximum n-gram Order
Lattice MBR Decoding (Equation 6) involves com-
puting a posterior probability for each n-gram in the
lattice. We would like to speed up the Lattice MBR
computation (Section 4) by restricting the maximum
order of the n-grams in the procedure. The results
(Table 5) show that on aren, there is no degradation
if we limit the maximum order of the n-grams to
3. However, on zhen/enzh, there is improvement by
BLEU(%)
Max n-gram order aren zhen enzh
1 38.7 26.8 40.0
2 44.1 27.4 42.2
3 44.9 28.0 42.4
4 44.9 28.5 42.6
Table 5: Lattice MBR as a function of max n-gram order.
considering 4-grams. We can therefore reduce Lat-
tice MBR computations in aren.
8 Discussion
We have presented a procedure for performing Min-
imum Bayes-Risk Decoding on translation lattices.
This is a significant development in that the MBR
decoder operates over a very large number of trans-
lations. In contrast, the current N -best implementa-
tion of MBR can be scaled to, at most, a few thou-
sands of hypotheses. If the number of hypotheses
is greater than, say 20,000, the N -best MBR be-
comes computationally expensive. The lattice MBR
technique is efficient when performed over enor-
mous number of hypotheses (up to 1080) since it
takes advantage of the compact structure of the lat-
tice. Lattice MBR gives consistent improvements in
translation performance over N -best MBR decod-
ing, which is used in many state-of-the-art research
translation systems. Moreover, we see gains on three
different language pairs.
There are two potential reasons why Lattice MBR
decoding could outperform N -best MBR: a larger
hypothesis space from which translations could be
selected or a larger evidence space for computing the
expected loss. Our experiments show that the main
improvement comes from the larger evidence space:
a larger set of translations in the lattice provides a
better estimate of the expected BLEU score. In other
words, the lattice provides a better posterior distri-
bution over translation hypotheses relative to an N -
best list. This is a novel insight into the workings
of MBR decoding. We believe this could be possi-
bly employed when designing discriminative train-
ing approaches for machine translation. More gener-
ally, we have found a component in machine transla-
tion where the posterior distribution over hypotheses
plays a crucial role.
We have shown the effect of the MBR scaling fac-
627
10 20 30 40
44
44.2
44.4
44.6
44.8
45 aren
         Lattice Density
33
85
121
161
187
208
B
L
E
U
(
%
)
10 20 30 4028.2
28.3
28.4
28.5
28.6
28.7 zhen
Lattice Density
6
22
37
49
59
65
10 20 30 4041.8
42
42.2
42.4
42.6 enzh
      Lattice Density
3
10
17
25
30
34
Figure 2: Lattice MBR vs. lattice density: aren/zhen/enzh. Each point also shows the loge(Avg. # of paths).
0 0.2 0.4 0.6 0.8 1
44
44.2
44.4
44.6
44.8 aren
Scale Factor 
B
L
E
U
(
%
)
0 0.2 0.4 0.6 0.8 127.9
28
28.1
28.2
28.3
28.4
28.5 zhen
Scale Factor
0 0.2 0.4 0.6 0.8 1
41.8
42
42.2
42.4
42.6
42.8 enzh
Scale Factor
Figure 3: Lattice MBR with various scale factors ?: aren/zhen/enzh.
tor on the performance of lattice MBR. The scale
factor determines the flatness of the posterior distri-
bution over translation hypotheses. A scale of 0.0
means a uniform distribution while 1.0 implies that
there is no scaling. This is an important parameter
that needs to be tuned on a development set. There
has been prior work in MBR speech recognition and
machine translation (Goel and Byrne, 2000; Ehling
et al, 2007) which has shown the need for tuning
this factor. Our MT system parameters are trained
with Minimum Error Rate Training which assigns a
very high posterior probability to the MAP transla-
tion. As a result, it is necessary to flatten the prob-
ability distribution so that MBR decoding can select
hypotheses other than the MAP hypothesis.
Our Lattice MBR implementation is made pos-
sible due to the linear approximation of the BLEU
score. This linearization technique has been applied
elsewhere when working with BLEU: Smith and
Eisner (2006) approximate the expectation of log
BLEU score. In both cases, a linear metric makes
it easier to compute the expectation. While we have
applied lattice MBR decoding to the approximate
BLEU score, we note that our procedure (Section 3)
is applicable to other gain functions which can be
decomposed as a sum of local gain functions. In par-
ticular, our framework might be useful with transla-
tion metrics such as TER (Snover et al, 2006) or
METEOR (Lavie and Agarwal, 2007).
In contrast to a phrase-based SMT system, a syn-
tax based SMT system (e.g. Zollmann and Venu-
gopal (2006)) can generate a hypergraph that rep-
resents a generalized translation lattice with words
and hidden tree structures. We believe that our lat-
tice MBR framework can be extended to such hy-
pergraphs with loss functions that take into account
both BLEU scores as well as parse tree structures.
Lattice and Forest based search and training pro-
cedures are not yet common in statistical machine
translation. However, they are promising because
the search space of translations is much larger than
the typical N -best list (Mi et al, 2008). We hope
that our approach will provide some insight into the
design of lattice-based search procedures along with
the use of non-linear, global loss functions such as
BLEU.
References
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J . Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.
628
Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79?85.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In SSST, NAACL-HLT
2007, pages 103?110, Rochester, NY, USA, April.
N. Ehling, R. Zens, and H. Ney. 2007. Minimum Bayes
Risk Decoding for BLEU. In ACL 2007, pages 101?
104, Prague, Czech Republic, June.
V. Goel and W. Byrne. 2000. Minimum Bayes-Risk Au-
tomatic Speech Recognition. Computer Speech and
Language, 14(2):115?135.
V. Goel. 2001. Minimum Bayes-Risk Automatic Speech
Recognition. Ph.D. thesis, Johns Hopkins University,
Baltimore, MD, USA.
J. Goodman. 1996. Parsing Algorithms and Metrics. In
ACL, pages 177?183, Santa Cruz, CA, USA.
L. Karttunen, J-p. Chanod, G. Grefenstette, and
A. Schiller. 1996. Regular Expressions for Language
Engineering. Natural Language Engineering, 2:305?
328.
P. Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In EMNLP, Barcelona,
Spain.
S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk
word alignments of bilingual texts. In EMNLP, pages
140?147, Philadelphia, PA, USA.
S. Kumar and W. Byrne. 2004. Minimum Bayes-Risk
Decoding for Statistical Machine Translation. In HLT-
NAACL, pages 169?176, Boston, MA, USA.
A. Lavie and A. Agarwal. 2007. METEOR: An Auto-
matic Metric for MT Evaluation with High Levels of
Correlation with Human Judgments. In SMT Work-
shop, ACL, pages 228?231, Prague, Czech Republic.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based Minimum Error Rate Training for Sta-
tistical Machine Translation. In EMNLP, Honolulu,
Hawaii, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Translation.
In COLING, Geneva, Switzerland.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based Trans-
lation. In ACL, Columbus, OH, USA.
M.Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(3).
M. Mohri. 2002. Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3):321?350.
NIST. 2002-2008. The NIST Machine Translation Eval-
uations. http://www.nist.gov/speech/tests/mt/.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 ? 51.
F. Och and H. Ney. 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computa-
tional Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statisti-
cal Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Technical Report RC22176 (W0109-
022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
D. Smith and J. Eisner. 2006. Minimum Risk Anneal-
ing for Training Log-Linear Models. In ACL, Sydney,
Australia.
D. Smith and N. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In EMNLP-CoNLL,
Prague, Czech Republic.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA, Boston,
MA, USA.
I. Titov and J. Henderson. 2006. Loss Minimization in
Parse Reranking. In EMNLP, Sydney, Australia.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
Word Graphs in Statistical Machine Translation. In
EMNLP, Philadelphia, PA, USA.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass De-
coding for Synchronous Context Free Grammars. In
ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
629
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725?734,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Lattice-based Minimum Error Rate Training
for Statistical Machine Translation
Wolfgang Macherey Franz Josef Och Ignacio Thayer Jakob Uszkoreit
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
{wmach,och,thayer,uszkoreit}@google.com
Abstract
Minimum Error Rate Training (MERT) is an
effective means to estimate the feature func-
tion weights of a linear model such that an
automated evaluation criterion for measuring
system performance can directly be optimized
in training. To accomplish this, the training
procedure determines for each feature func-
tion its exact error surface on a given set of
candidate translations. The feature function
weights are then adjusted by traversing the
error surface combined over all sentences and
picking those values for which the resulting
error count reaches a minimum. Typically,
candidates in MERT are represented as N -
best lists which contain the N most probable
translation hypotheses produced by a decoder.
In this paper, we present a novel algorithm that
allows for efficiently constructing and repre-
senting the exact error surface of all trans-
lations that are encoded in a phrase lattice.
Compared to N -best MERT, the number of
candidate translations thus taken into account
increases by several orders of magnitudes.
The proposed method is used to train the
feature function weights of a phrase-based
statistical machine translation system. Experi-
ments conducted on the NIST 2008 translation
tasks show significant runtime improvements
and moderate BLEU score gains over N -best
MERT.
1 Introduction
Many statistical methods in natural language pro-
cessing aim at minimizing the probability of sen-
tence errors. In practice, however, system quality
is often measured based on error metrics that assign
non-uniform costs to classification errors and thus
go far beyond counting the number of wrong de-
cisions. Examples are the mean average precision
for ranked retrieval, the F-measure for parsing, and
the BLEU score for statistical machine transla-
tion (SMT). A class of training criteria that provides
a tighter connection between the decision rule and
the final error metric is known as Minimum Error
Rate Training (MERT) and has been suggested for
SMT in (Och, 2003).
MERT aims at estimating the model parameters
such that the decision under the zero-one loss func-
tion maximizes some end-to-end performance mea-
sure on a development corpus. In combination with
log-linear models, the training procedure allows for
a direct optimization of the unsmoothed error count.
The criterion can be derived from Bayes? decision
rule as follows: Let f  f1, ..., fJ denote a source
sentence (?French?) which is to be translated into a
target sentence (?English?) e  e1, ..., eI . Under
the zero-one loss function, the translation which
maximizes the a posteriori probability is chosen:
e?  argmax
e
 
Prpe|fq
( (1)
Since the true posterior distribution is unknown,
Prpe|fq is modeled via a log-linear translation model
which combines some feature functions hmpe, fq
with feature function weights ?m, m  1, ...,M :
Prpe|fq  p?M1 pe|fq

exp

?M
m1 ?mhmpe, fq

?
e1 exp

?M
m1 ?mhmpe1, fq

(2)
The feature function weights are the parameters of
the model, and the objective of the MERT criterion
is to find a parameter set ?M1 that minimizes the error
count on a representative set of training sentences.
More precisely, let fS1 denote the source sentences
of a training corpus with given reference translations
725
rS1 , and let Cs  tes,1, ..., es,Ku denote a set of K
candidate translations. Assuming that the corpus-
based error count for some translations eS1 is addi-
tively decomposable into the error counts of the indi-
vidual sentences, i.e., EprS1 , eS1 q 
?S
s1 Eprs, esq,
the MERT criterion is given as:
??M1  argmin
?M1
#
S?
s1
E
 
rs, e?pfs;?M1 q

+
(3)
 argmin
?M1
#
S?
s1
K?
k1
Eprs, es,kq?
 
e?pfs;?M1 q, es,k

+
with
e?pfs;?M1 q  argmaxe
#
M?
m1
?mhmpe, fsq
+
(4)
In (Och, 2003), it was shown that linear models can
effectively be trained under the MERT criterion us-
ing a special line optimization algorithm. This line
optimization determines for each feature function
hm and sentence fs the exact error surface on a set
of candidate translations Cs. The feature function
weights are then adjusted by traversing the error
surface combined over all sentences in the training
corpus and moving the weights to a point where the
resulting error reaches a minimum.
Candidate translations in MERT are typically rep-
resented as N -best lists which contain the N most
probable translation hypotheses. A downside of this
approach is, however, that N -best lists can only
capture a very small fraction of the search space.
As a consequence, the line optimization algorithm
needs to repeatedly translate the development corpus
and enlarge the candidate repositories with newly
found hypotheses in order to avoid overfitting on Cs
and preventing the optimization procedure from
stopping in a poor local optimum.
In this paper, we present a novel algorithm that
allows for efficiently constructing and representing
the unsmoothed error surface for all translations
that are encoded in a phrase lattice. The number
of candidate translations thus taken into account
increases by several orders of magnitudes compared
to N -best MERT. Lattice MERT is shown to yield
significantly faster convergence rates while it ex-
plores a much larger space of candidate translations
which is exponential in the lattice size. Despite
this vast search space, we show that the suggested
algorithm is always efficient in both running time
and memory.
The remainder of this paper is organized as fol-
lows. Section 2 briefly reviews N -best MERT and
introduces some basic concepts that are used in
order to develop the line optimization algorithm for
phrase lattices in Section 3. Section 4 presents an
upper bound on the complexity of the unsmoothed
error surface for the translation hypotheses repre-
sented in a phrase lattice. This upper bound is
used to prove the space and runtime efficiency of
the suggested algorithm. Section 5 lists some best
practices for MERT. Section 6 discusses related
work. Section 7 reports on experiments conducted
on the NIST 2008 translation tasks. The paper
concludes with a summary in Section 8.
2 Minimum Error Rate Training on
N -best Lists
The goal of MERT is to find a weights set that
minimizes the unsmoothed error count on a rep-
resentative training corpus (cf. Eq. (3)). This
can be accomplished through a sequence of line
minimizations along some vector directions tdM1 u.
Starting from an initial point ?M1 , computing the
most probable sentence hypothesis out of a set of K
candidate translations Cs  te1, ..., eKu along the
line ?M1   ?  dM1 results in the following optimiza-
tion problem (Och, 2003):
e?pfs; ?q  argmax
ePCs
!
p?M1   ?  dM1 qJ  hM1 pe, fsq
)
 argmax
ePCs
"
?
m
?mhmpe, fsq
loooooooomoooooooon
ape,fsq
  ? 
?
m
dmhmpe, fsq
loooooooomoooooooon
bpe,fsq
*
 argmax
ePCs
 
ape, fsq   ?  bpe, fsq
looooooooooomooooooooooon
pq
( (5)
Hence, the total score pq for any candidate trans-
lation corresponds to a line in the plane with ? as
the independent variable. For any particular choice
of ?, the decoder seeks that translation which yields
the largest score and therefore corresponds to the
topmost line segment.
Overall, the candidate repository Cs defines K
lines where each line may be divided into at most
K line segments due to possible intersections with
the other K  1 lines. The sequence of the topmost
line segments constitute the upper envelope which
is the pointwise maximum over all lines induced by
Cs. The upper envelope is a convex hull and can
be inscribed with a convex polygon whose edges
are the segments of a piecewise linear function in ?
(Papineni, 1999; Och, 2003):
Envpfq  max
ePC
 
ape, fq   ?  bpe, fq : ? P R
( (6)
726
Score
?
Error
count
?
0
0
e1
e2
e5
e6
e8
e1e2
e3
e4
e5
e6
e7
e8
Figure 1: The upper envelope (bold, red curve) for a set
of lines is the convex hull which consists of the topmost
line segments. Each line corresponds to a candidate
translation and is thus related to a certain error count.
Envelopes can efficiently be computed with Algorithm 1.
The importance of the upper envelope is that it pro-
vides a compact encoding of all possible outcomes
that a rescoring of Cs may yield if the parameter
set ?M1 is moved along the chosen direction. Once
the upper envelope has been determined, we can
project its constituent line segments onto the error
counts of the corresponding candidate translations
(cf. Figure 1). This projection is independent of
how the envelope is generated and can therefore be
applied to any set of line segments1.
An effective means to compute the upper enve-
lope is a sweep line algorithm which is often used in
computational geometry to determine the intersec-
tion points of a sequence of lines or line segments
(Bentley and Ottmann, 1979). The idea is to shift
(?sweep?) a vertical ray from 8 to  8 over the
plane while keeping track of those points where two
or more lines intersect. Since the upper envelope
is fully specified by the topmost line segments, it
suffices to store the following components for each
line object ?: the x-intercept ?.x with the left-
adjacent line, the slope ?.m, and the y-intercept ?.y;
a fourth component, ?.t, is used to store the candi-
date translation. Algorithm 1 shows the pseudo code
for a sweep line algorithm which reduces an input
array a[0..K-1] consisting of the K line objects
of the candidate repository Cs to its upper envelope.
By construction, the upper envelope consists of at
most K line segments. The endpoints of each line
1 For lattice MERT, it will therefore suffice to find an
efficient way to compute the upper envelope over all translations
that are encoded in a phrase graph.
Algorithm 1 SweepLine
input: array a[0..K-1] containing lines
output: upper envelope of a
sort(a:m);
j = 0; K = size(a);
for (i = 0; i < K; ++i) {
? = a[i];
?.x = -8;
if (0 < j) {
if (a[j-1].m == ?.m) {
if (?.y <= a[j-1].y) continue;
--j;
}
while (0 < j) {
?.x = (?.y - a[j-1].y)/
(a[j-1].m - ?.m);
if (a[j-1].x < ?.x) break;
--j;
}
if (0 == j) ?.x = -8;
a[j++] = ?;
} else a[j++] = ?;
}
a.resize(j);
return a;
segment define the interval boundaries at which the
decision made by the decoder will change. Hence,
as ? increases from 8 to  8, we will see that
the most probable translation hypothesis will change
whenever ? passes an intersection point.
Let ?fs1 ? ?
fs
2 ? ...? ?
fs
Ns denote the sequence of
interval boundaries and let ?Efs1 ,?Efs2 , ...,?EfsNs
denote the corresponding sequence of changes in the
error count where ?Efsn is the amount by which the
error count will change if ? is moved from a point in
r?fsn1, ?fsn q to a point in r?fsn , ?
fs
n 1q. Both sequences
together provide an exhaustive representation of the
unsmoothed error surface for the sentence fs along
the line ?M1   ?  dM1 . The error surface for the
whole training corpus is obtained by merging the
interval boundaries (and their corresponding error
counts) over all sentences in the training corpus.
The optimal ? can then be found by traversing the
merged error surface and choosing a point from the
interval where the total error reaches its minimum.
After the parameter update, ??M1  ?M1  ?opt dM1 ,
the decoder may find new translation hypotheses
which are merged into the candidate repositories if
they are ranked among the top N candidates. The
relation K  N holds therefore only in the first
iteration. From the second iteration on, K is usually
larger than N . The sequence of line optimizations
and decodings is repeated until (1) the candidate
repositories remain unchanged and (2) ?opt  0.
727
3 Minimum Error Rate Training on
Lattices
In this section, the algorithm for computing the
upper envelope on N -best lists is extended to phrase
lattices. For a description on how to generate
lattices, see (Ueffing et al, 2002).
Formally, a phrase lattice for a source sentence f
is defined as a connected, directed acyclic graph
Gf  pVf , Ef q with vertice set Vf , unique source and
sink nodes s, t P Vf , and a set of arcs Ef ? Vf  Vf .
Each arc is labeled with a phrase ?ij  ei1 , ..., eij
and the (local) feature function values hM1 p?ij , fq.
A path ?  pv0, ?0, v1, ?1, ..., ?n1, vnq in Gf (with
?i P Ef and vi, vi 1 P Vf as the tail and head of
?i, 0 ? i ? n) defines a partial translation epi of f
which is the concatenation of all phrases along this
path. The corresponding feature function values are
obtained by summing over the arc-specific feature
function values:
? : 
v0
?0,1
???????
hM1 p?0,1, fq

v1
?1,2
???????
hM1 p?1,2, fq
  
?n1,n
?????????
hM1 p?n1,n, fq

vn
epi  ?
i,j :vi?vjPpi
?ij  ?0,1  ...  ?n1,n
hM1 pepi, fq 
?
i,j :vi?vjPpi
hM1 p?ij, fq
In the following, we use the notation inpvq and outpvq
to refer to the set of incoming and outgoing arcs for
a node v P Vf . Similarly, headp?q and tailp?q denote
the head and tail of ? P Ef .
To develop the algorithm for computing the up-
per envelope of all translation hypotheses that are
encoded in a phrase lattice, we first consider a node
v P Vf with some incoming and outgoing arcs:
v
v1?
Each path that starts at the source node s and ends in
v defines a partial translation hypothesis which can
be represented as a line (cf. Eq. (5)). We now assume
that the upper envelope for these partial translation
hypotheses is known. The lines that constitute this
envelope shall be denoted by f1, ..., fN . Next we
consider continuations of these partial translation
candidates by following one of the outgoing arcs
Algorithm 2 Lattice Envelope
input: a phrase lattice Gf  pVf , Ef q
output: upper envelope of Gf
a = H;
L = H;
TopSort(Gf);
for v = s to t do {
a = SweepLine(
?
?Pinpvq
L[?]);
foreach (? P inpvq)
L.delete(?);
foreach (? P outpvq) {
L[?] = a;
for (i = 0; i < a.size(); ++i) {
L[?][i].m = a[i].m +
?
m dmhmp?, fq;
L[?][i].y = a[i].y +
?
m ?mhmp?, fq;
L[?][i].p = a[i].p ?v,headp?q;
}
}
}
return a;
? P outpvq. Each such arc defines another line
denoted by gp?q. If we add the slope and y-intercept
of gp?q to each line in the set tf1, ..., fN u, then the
upper envelope will be constituted by segments of
f1   gp?q, ..., fN   gp?q. This operation neither
changes the number of line segments nor their rela-
tive order in the envelope, and therefore it preserves
the structure of the convex hull. As a consequence,
we can propagate the resulting envelope over an
outgoing arc ? to a successor node v1  headp?q.
Other incoming arcs for v1 may be associated with
different upper envelopes, and all that remains is
to merge these envelopes into a single combined
envelope. This is, however, easy to accomplish
since the combined envelope is simply the convex
hull of the union over the line sets which constitute
the individual envelopes. Thus, by merging the
arrays that store the line segments for the incoming
arcs and applying Algorithm 1 to the resulting array
we obtain the combined upper envelope for all
partial translation candidates that are associated with
paths starting at the source node s and ending in
v1. The correctness of this procedure is based on
the following two observations:
(1) A single translation hypothesis cannot consti-
tute multiple line segments of the same envelope.
This is because translations associated with different
line segments are path-disjoint.
(2) Once a partial translation has been discarded
from an envelope because its associated line f? is
completely covered by the topmost line segments
of the convex hull, there is no path continuation
that could bring back f? into the upper envelope
728
again. Proof: Suppose that such a continuation
exists, then this continuation can be represented as
a line g, and since f? has been discarded from the
envelope, the path associated with g must also be a
valid continuation for the line segments f1, ..., fN
that constitute the envelope. Thus it follows that
maxpf1   g, ..., fN   gq  maxpf1, ..., fN q   g ?
f?   g for some ? P R. This, however, is in contra-
diction with the premise that f? ? maxpf1, ..., fN q
for all ? P R.
To keep track of the phrase expansions when
propagating an envelope over an outgoing arc ? P
tailpvq, the phrase label ?v,headp?q has to be appended
from the right to all partial translation hypotheses in
the envelope. The complete algorithm then works
as follows: First, all nodes in the phrase lattice
are sorted in topological order. Starting with the
source node, we combine for each node v the upper
envelopes that are associated with v?s incoming arcs
by merging their respective line arrays and reducing
the merged array into a combined upper envelope
using Algorithm 1. The combined envelope is then
propagated over the outgoing arcs by associating
each ? P outpvq with a copy of the combined
envelope. This copy is modified by adding the
parameters (slope and y-intercept) of the line gp?q
to the envelope?s constituent line segments. The
envelopes of the incoming arcs are no longer needed
and can be deleted in order to release memory. The
envelope computed at the sink node is by construc-
tion the convex hull over all translation hypotheses
represented in the lattice, and it compactly encodes
those candidates which maximize the decision rule
Eq. (1) for any point along the line ?M1   ?  dM1 .
Algorithm 2 shows the pseudo code. Note that
the component ?.x does not change and therefore
requires no update.
It remains to verify that the suggested algorithm
is efficient in both running time and memory. For
this purpose, we first analyze the complexity of
Algorithm 1 and derive from it the running time of
Algorithm 2.
After sorting, each line object in Algorithm 1 is
visited at most three times. The first time is when
it is picked by the outer loop. The second time is
when it either gets discarded or when it terminates
the inner loop. Whenever a line object is visited
for the third time, it is irrevocably removed from
the envelope. The runtime complexity is therefore
dominated by the initial sorting and amounts to
OpK logKq
Topological sort on a phrase lattice G  pV, Eq
can be performed in time ?p|V|   |E |q. As will be
shown in Section 4, the size of the upper envelope
for G can never exceed the size of the arc set E . The
same holds for any subgraph G
rs,vs of G which is
induced by the paths that connect the source node
s with v P V . Since the envelopes propagated from
the source to the sink node can only increase linearly
in the number of previously processed arcs, the total
running time amounts to a worst case complexity of
Op|V|  |E | log |E |q.
4 Upper Bound for Size of Envelopes
The memory efficiency of the suggested algorithm
results from the following theorem which provides
a novel upper bound for the number of cost mini-
mizing paths in a directed acyclic graph with arc-
specific affine cost functions. The bound is not only
meaningful for proving the space efficiency of lattice
MERT, but it also provides deeper insight into the
structure and complexity of the unsmoothed error
surface induced by log-linear models. Since we are
examining a special class of shortest paths problems,
we will invert the sign of each local feature function
value in order to turn the feature scores into cor-
responding costs. Hence, the objective of finding
the best translation hypotheses in a phrase lattice
becomes the problem of finding all cost-minimizing
paths in a graph with affine cost functions.
Theorem: Let G  pV, Eq be a connected directed
acyclic graph with vertex set V , unique source and
sink nodes s, t P V , and an arc set E ? V  V in
which each arc ? P E is associated with an affine
cost function c?p?q  a?  ?   b?, a?, b? P R.
Counting ties only once, the cardinality of the union
over the sets of all cost-minimizing paths for all
? P R is then upper-bounded by |E |:



?
?PR
 
? : ?  ?pG; ?q is a cost-minimizing
path in G given ?
(



? |E | (7)
Proof: The proposition holds for the empty graph
as well as for the case that V  ts, tu with all
arcs ? P E joining the source and sink node. Let
G therefore be a larger graph. Then we perform
an s-t cut and split G into two subgraphs G1 (left
subgraph) and G2 (right subgraph). Arcs spanning
the section boundary are duplicated (with the costs
of the copied arcs in G2 being set to zero) and
connected with a newly added head or tail node:
G: G G1 2c1
c3
c2
c4
c1
c3
c4
c2
0: :
729
The zero-cost arcs in G2 that emerged from the
duplication process are contracted, which can be
done without loss of generality because zero-cost
arcs do not affect the total costs of paths in the
lattice. The contraction essentially amounts to a
removal of arcs and is required in order to ensure
that the sum of edges in both subgraphs does not
exceed the number of edges in G. All nodes in
G1 with out-degree zero are then combined into a
single sink node t1. Similarly, nodes in G2 whose
in-degree is zero are combined into a single source
node s2. Let N1 and N2 denote the number of
arcs in G1 and G2, respectively. By construction,
N1   N2  |E |. Both subgraphs are smaller
than G and thus, due to the induction hypothesis,
their lower envelopes consist of at most N1 and N2
line segments, respectively. We further notice that
either envelope is a convex hull whose constituent
line segments inscribe a convex polygon, in the
following denoted by P1 and P2. Now, we combine
both subgraphs into a single graph G1 by merging
the sink node t1 in G1 with the source node s2
in G2. The merged node is an articulation point
whose removal would disconnect both subgraphs,
and hence, all paths in G1 that start at the source
node s and stop in the sink node t lead through this
articulation point. The graph G1 has at least as many
cost minimizing paths as G, although these paths
as well as their associated costs might be different
from those in G. The additivity of the cost function
and the articulation point allow us to split the costs
for any path from s to t into two portions: the first
portion can be attributed to G1 and must be a line
inside P1; the remainder can be attributed to G2
and must therefore be a line inside P2. Hence, the
total costs for any path in G1 can be bounded by
the convex hull of the superposition of P1 and P2.
This convex hull is again a convex polygon which
consists of at most N1   N2 edges, and therefore,
the number of cost minimizing paths in G1 (and thus
also in G) is upper bounded by N1   N2. l
Corollary: The upper envelope for a phrase lattice
Gf  pVf , Ef q consists of at most |Ef | line segments.
This bound can even be refined and one obtains
(proof omitted) |E | |V| 2. Both bounds are tight.
This result may seem somewhat surprising as it
states that, independent of the choice of the direction
along which the line optimization is performed, the
structure of the error surface is far less complex
than one might expect based on the huge number
of alternative translation candidates that are rep-
resented in the lattice and thus contribute to the
error surface. In fact, this result is a consequence
of using a log-linear model which constrains how
costs (or scores, respectively) can evolve due to
hypothesis expansion. If instead quadratic cost
functions were used, the size of the envelopes could
not be limited in the same way. The above theorem
does not, however, provide any additional guidance
that would help to choose more promising directions
in the line optimization algorithm to find better local
optima. To alleviate this problem, the following
section lists some best practices that we found to be
useful in the context of MERT.
5 Practical Aspects
This section addresses some techniques that we
found to be beneficial in order to improve the
performance of MERT.
(1) Random Starting Points: To prevent the line
optimization algorithm from stopping in a poor local
optimum, MERT explores additional starting points
that are randomly chosen by sampling the parameter
space.
(2) Constrained Optimization: This technique
allows for limiting the range of some or all feature
function weights by defining weights restrictions.
The weight restriction for a feature function hm is
specified as an interval Rm  rlm, rms, lm, rm P
RYt8, 8u which defines the admissible region
from which the feature function weight ?m can be
chosen. If the line optimization is performed under
the presence of weights restrictions, ? needs to be
chosen such that the following constraint holds:
lM1 ? ?M1   ?  dM1 ? rM1 (8)
(3) Weight Priors: Weight priors give a small (pos-
itive or negative) boost ? on the objective function
if the new weight is chosen such that it matches a
certain target value ?m:
?opt  argmin?
!
?
s
E
 
rs, e?pfs; ?q

 
?
m
?p?m   ?  dm, ?mq  ?
)
(9)
A zero-weights prior (?m  0) provides a means of
doing feature selection since the weight of a feature
function which is not discriminative will be set to
zero. An initial-weights prior (?m  ?m) can
be used to confine changes in the parameter update
with the consequence that the new parameter may
be closer to the initial weights set. Initial weights
priors are useful in cases where the starting weights
already yield a decent baseline.
730
(4) Interval Merging: The interval r?fsi , ?fsi 1q of
a translation hypothesis can be merged with the
interval r?fsi1, ?
fs
i q of its left-adjacent translation
hypothesis if the corresponding change in the error
count ?Efsi  0. The resulting interval r?fsi1, ?fsi 1q
has a larger range, and the choice of ?opt may be
more reliable.
(5) Random Directions: If the directions chosen in
the line optimization algorithm are the coordinate
axes of the M -dimensional parameter space, each
iteration will result in the update of a single feature
function only. While this update scheme provides
a ranking of the feature functions according to their
discriminative power (each iteration picks the fea-
ture function for which changing the corresponding
weight yields the highest gain), it does not take
possible correlations between the feature functions
into account. As a consequence, the optimization
procedure may stop in a poor local optimum. On
the other hand, it is difficult to compute a direction
that decorrelates two or more correlated feature
functions. This problem can be alleviated by ex-
ploring a large number of random directions which
update many feature weights simultaneously. The
random directions are chosen as the lines which
connect some randomly distributed points on the
surface of an M -dimensional hypersphere with the
hypersphere?s center. The center of the hypersphere
is defined as the initial parameter set.
6 Related Work
As suggested in (Och, 2003), an alternative method
for the optimization of the unsmoothed error count is
Powell?s algorithm combined with a grid-based line
optimization (Press et al, 2007, p. 509). In (Zens
et al, 2007), the MERT criterion is optimized on
N -best lists using the Downhill Simplex algorithm
(Press et al, 2007, p. 503). The optimization proce-
dure allows for optimizing other objective function
as, e.g., the expected BLEU score. A weakness
of the Downhill Simplex algorithm is, however, its
decreasing robustness for optimization problems in
more than 10 dimensions. A different approach
to minimize the expected BLEU score is suggested
in (Smith and Eisner, 2006) who use deterministic
annealing to gradually turn the objective function
from a convex entropy surface into the more com-
plex risk surface. A large variety of different search
strategies for MERT are investigated in (Cer et al,
2008), which provides many fruitful insights into
the optimization process. In (Duh and Kirchhoff,
2008), MERT is used to boost the BLEU score on
Table 1: Corpus statistics for three text translation sets:
Arabic-to-English (aren), Chinese-to-English (zhen),
and English-to-Chinese (enzh). Development and test
data are compiled from evaluation data used in past
NIST Machine Translation Evaluations.
data set collection # of sentences
aren zhen enzh
dev1 nist02 1043 878 ?
dev2 nist04 1353 1788 ?
blind nist08 1360 1357 1859
N -best re-ranking tasks. The incorporation of a
large number of sparse feature functions is described
in (Watanabe et al, 2007). The paper investigates a
perceptron-like online large-margin training for sta-
tistical machine translation. The described approach
is reported to yield significant improvements on top
of a baseline system which employs a small number
of feature functions whose weights are optimized
under the MERT criterion. A study which is comple-
mentary to the upper bound on the size of envelopes
derived in Section 4 is provided in (Elizalde and
Woods, 2006) which shows that the number of
inference functions of any graphical model as, for
instance, Bayesian networks and Markov random
fields is polynomial in the size of the model if the
number of parameters is fixed.
7 Experiments
Experiments were conducted on the NIST 2008
translation tasks under the conditions of the con-
strained data track for the language pairs Arabic-
to-English (aren), English-to-Chinese (enzh), and
Chinese-to-English (zhen). The development cor-
pora were compiled from test data used in the
2002 and 2004 NIST evaluations. Each corpus set
provides 4 reference translations per source sen-
tence. Table 1 summarizes some corpus statistics.
Table 2: BLEU score results on the NIST-08 test set
obtained after 25 iterations using N -best MERT or 5
iterations using lattice MERT, respectively.
dev1+dev2 blind
task loss N -best lattice N -best lattice
aren MBR 56.6 57.4 42.9 43.9
0-1 56.7 57.4 42.8 43.7
enzh MBR 39.7 39.6 36.5 38.8
0-1 40.4 40.5 35.1 37.6
zhen MBR 39.5 39.7 27.5 28.2
0-1 39.6 39.6 27.0 27.6
731
 35.5
 36
 36.5
 37
 37.5
 38
 38.5
 39
 39.5
 40
 0  5  10  15  20  25
B
LE
U
[%
]
iteration
lattice MERT
N-best MERT
 5
 15
 25
 35
 45
 0  5  10  15  20  25
Figure 2: BLEU scores for N -best MERT and lattice
MERT after each decoding step on the zhen-dev1 corpus.
The grey shaded subfigure shows the complete graph
including the bottom part for N -best MERT.
Translation results were evaluated using the mixed-
case BLEU score metric in the implementation as
suggested by (Papineni et al, 2001).
Translation results were produced with a state-of-
the-art phrase-based SMT system which uses EM-
trained word alignment models (IBM1, HMM) and
a 5-gram language model built from the Web-1T
collection2 . Translation hypotheses produced on the
blind test data were reranked using the Minimum-
Bayes Risk (MBR) decision rule (Kumar and Byrne,
2004; Tromble et al, 2008). Each system uses a log-
linear combination of 20 to 30 feature functions.
In a first experiment, we investigated the conver-
gence speed of lattice MERT and N -best MERT.
2http://www.ldc.upenn.edu, catalog entry: LDC2006T13
 35.6
 35.8
 36
 36.2
 36.4
 36.6
 36.8
 37
-15 -10 -5  0  5  10  15
B
LE
U
[%
]
?
lattice
50-best
 36.28
 36.3
 36.32
 36.34
-0.2 -0.1  0  0.1  0.2
 35
 35.5
 36
 36.5
 37
-40 -20  0  20  40
Figure 3: Error surface of the phrase penalty feature after
the first iteration on the zhen-dev1 corpus.
 0  1  2  3  4  5
 35.5
 36
 36.5
 37
 37.5
 38
 38.5
 39
 39.5
 40
B
LE
U
[%
]
iteration
lattice MERT: 1000 directions
1 direction
Figure 4: BLEU scores on the zhen-dev1 corpus for
lattice MERT with additional directions.
Figure 2 shows the evolution of the BLEU score
in the course of the iteration index on the zhen-
dev1 corpus for either method. In each iteration,
the training procedure translates the development
corpus using the most recent weights set and merges
the top ranked candidate translations (either repre-
sented as phrase lattices or N -best lists) into the
candidate repositories before the line optimization
is performed. For N -best MERT, we used N  50
which yielded the best results. In contrast to lattice
MERT, N -best MERT optimizes all dimensions in
each iteration and, in addition, it also explores a
large number of random starting points before it
re-decodes and expands the hypothesis set. As is
typical for N -best MERT, the first iteration causes
a dramatic performance loss caused by overadapting
the candidate repositories, which amounts to more
than 27.3 BLEU points. Although this performance
loss is recouped after the 5th iteration, the initial
decline makes the line optimization under N -best
MERT more fragile since the optimum found at the
end of the training procedure is affected by the initial
performance drop rather than by the choice of the
initial start weights. Lattice MERT on the other hand
results in a significantly faster convergence speed
and reaches its optimum already in the 5th iteration.
For lattice MERT, we used a graph density of 40
arcs per phrase which corresponds to an N -best size
of more than two octillion p2  1027q entries. This
huge number of alternative candidate translations
makes updating the weights under lattice MERT
more reliable and robust and, compared to N -best
MERT, it becomes less likely that the same feature
weight needs to be picked again and adjusted in
subsequent iterations. Figure 4 shows the evolution
of the BLEU score on the zhen-dev1 corpus using
732
Table 3: BLEU score results on the NIST-08 tests set
obtained after 5 iterations using lattice MERT with
different numbers of random directions in addition to the
optimization along the coordinate axes.
# random dev1+dev2 blind
task directions 0-1 MBR 0-1 MBR
aren ? 57.4 57.4 43.7 43.9
1000 57.6 57.7 43.9 44.5
zhen ? 39.6 39.7 27.6 28.2
500 39.5 39.9 27.9 28.3
lattice MERT with 5 weights updates per iteration.
The performance drop in iteration 1 is also attributed
to overfitting the candidate repository. The decline
of less than 0.5% in terms of BLEU is, however,
almost negligible compared to the performance drop
of more than 27% in case of N -best MERT. The
vast number of alternative translation hypotheses
represented in a lattice also increases the number
of phase transitions in the error surface, and thus
prevents MERT from selecting a low performing
feature weights set at early stages in the optimization
procedure. This is illustrated in Figure 3, where
lattice MERT and N -best MERT find different op-
tima for the weight of the phrase penalty feature
function after the first iteration. Table 2 shows the
BLEU score results on the NIST 2008 blind test
using the combined dev1+dev2 corpus as training
data. While only the aren task shows improvements
on the development data, lattice MERT provides
consistent gains over N -best MERT on all three
blind test sets. The reduced performance for N -best
MERT is a consequence of the performance drop in
the first iteration which causes the final weights to
be far off from the initial parameter set. This can
impair the ability of N -best MERT to generalize to
unseen data if the initial weights are already capable
of producing a decent baseline. Lattice MERT on
the other hand can produce weights sets which are
closer to the initial weights and thus more likely to
retain the ability to generalize to unseen data. It
could therefore be worthwhile to investigate whether
a more elaborated version of an initial-weights prior
allows for alleviating this effect in case of N -
best MERT. Table 3 shows the effect of optimizing
the feature function weights along some randomly
chosen directions in addition to the coordinate axes.
The different local optima found on the development
set by using random directions result in additional
gains on the blind test sets and range from 0.1% to
0.6% absolute in terms of BLEU.
8 Summary
We presented a novel algorithm that allows for
efficiently constructing and representing the un-
smoothed error surface over all sentence hypotheses
that are represented in a phrase lattice. The proposed
algorithm was used to train the feature function
weights of a log-linear model for a statistical ma-
chine translation system under the Minimum Error
Rate Training (MERT) criterion. Lattice MERT was
shown analytically and experimentally to be supe-
rior over N -best MERT, resulting in significantly
faster convergence speed and a reduced number of
decoding steps. While the approach was used to
optimize the model parameters of a single machine
translation system, there are many other applications
in which this framework can be useful, too. One
possible usecase is the computation of consensus
translations from the outputs of multiple machine
translation systems where this framework allows us
to estimate the system prior weights directly on con-
fusion networks (Rosti et al, 2007; Macherey and
Och, 2007). It is also straightforward to extend the
suggested method to hypergraphs and forests as they
are used, e.g., in hierarchical and syntax-augmented
systems (Chiang, 2005; Zollmann and Venugopal,
2006). Our future work will therefore focus on how
much system combination and syntax-augmented
machine translation can benefit from lattice MERT
and to what extent feature function weights can
robustly be estimated using the suggested method.
References
J. L. Bentley and T. A. Ottmann. 1979. Algorithms for
reporting and counting geometric intersections. IEEE
Trans. on Computers, C-28(9):643?647.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and Search for Minimum Error Rate Training.
In Proceedings of the Third Workshop on Statistical
Machine Translation, 46th Annual Meeting of the
Association of Computational Linguistics: Human
Language Technologies (ACL-2008 HLT), pages 26?
34, Columbus, OH, USA, June.
D. Chiang. 2005. A Hierarchical Phrase-based Model
for Statistical Machine Translation. In ACL-2005,
pages 263?270, Ann Arbor, MI, USA, June.
K. Duh and K. Kirchhoff. 2008. Beyond Log-Linear
Models: Boosted Minimum Error Rate Training for
N-best Re-ranking. In Proceedings of the Third
Workshop on Statistical Machine Translation, 46th
Annual Meeting of the Association of Computational
Linguistics: Human Language Technologies (ACL-
2008 HLT), pages 37?40, Columbus, OH, USA, June.
733
S. Elizalde and K. Woods. 2006. Bounds on the Number
of Inference Functions of a Graphical Model, October.
arXiv:math/0610233v1.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In Proc. HLT-NAACL, pages 196?176, Boston, MA,
USA, May.
W. Macherey and F. J. Och. 2007. An Empirical Study
on Computing Consensus Translations from Multiple
Machine Translation Systems. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 986?995, Prague, Czech Republic,
June.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In 41st Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. BLEU: a Method for Automatic Evaluation
of Machine Translation. Technical Report RC22176
(W0109-022), IBM Research Division, Thomas J.
Watson Research Center, Yorktown Heights, NY,
USA.
K. A. Papineni. 1999. Discriminative training via
linear programming. In IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing, volume 2, pages 561?
564, Phoenix, AZ, March.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2007. Numerical Recipes: The Art of
Scientific Computing. Cambridge University Press,
Cambridge, UK, third edition.
A. V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas,
R. Schwartz, and B. Dorr. 2007. Combining outputs
from multiple machine translation systems. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 228?235, Rochester, New York,
April. Association for Computational Linguistics.
D. A. Smith and J. Eisner. 2006. Minimum Risk
Annealing for Training Log-linear Models. In 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (Coling/ACL-2006), pages
787?794, Sydney, Australia, July.
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008.
Lattice minimum bayes-risk decoding for statistical
machine translation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP),
page 10, Waikiki, Honolulu, Hawaii, USA, October.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation
of word graphs in statistical machine translation. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 156?
163, Philadelphia, PE, July.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 764?773,
Prague, Czech Republic.
R. Zens, S. Hasan, and H. Ney. 2007. A Systematic
Comparison of Training Criteria for Statistical Ma-
chine Translation. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
NAACL ?06: Proceedings of the 2006 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 138?141, New York, NY, June.
Association for Computational Linguistics.
734
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163?171,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Efficient Minimum Error Rate Training and
Minimum Bayes-Risk Decoding for
Translation Hypergraphs and Lattices
Shankar Kumar1 and Wolfgang Macherey1 and Chris Dyer2 and Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,wmach,och}@google.com
2Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony@umd.edu
Abstract
Minimum Error Rate Training (MERT)
and Minimum Bayes-Risk (MBR) decod-
ing are used in most current state-of-the-
art Statistical Machine Translation (SMT)
systems. The algorithms were originally
developed to work with N -best lists of
translations, and recently extended to lat-
tices that encode many more hypotheses
than typical N -best lists. We here extend
lattice-based MERT and MBR algorithms
to work with hypergraphs that encode a
vast number of translations produced by
MT systems based on Synchronous Con-
text Free Grammars. These algorithms
are more efficient than the lattice-based
versions presented earlier. We show how
MERT can be employed to optimize pa-
rameters for MBR decoding. Our exper-
iments show speedups from MERT and
MBR as well as performance improve-
ments from MBR decoding on several lan-
guage pairs.
1 Introduction
Statistical Machine Translation (SMT) systems
have improved considerably by directly using the
error criterion in both training and decoding. By
doing so, the system can be optimized for the
translation task instead of a criterion such as like-
lihood that is unrelated to the evaluation met-
ric. Two popular techniques that incorporate the
error criterion are Minimum Error Rate Train-
ing (MERT) (Och, 2003) and Minimum Bayes-
Risk (MBR) decoding (Kumar and Byrne, 2004).
These two techniques were originally developed
for N -best lists of translation hypotheses and re-
cently extended to translation lattices (Macherey
et al, 2008; Tromble et al, 2008) generated by a
phrase-based SMT system (Och and Ney, 2004).
Translation lattices contain a significantly higher
number of translation alternatives relative to N -
best lists. The extension to lattices reduces the
runtimes for both MERT and MBR, and gives per-
formance improvements from MBR decoding.
SMT systems based on synchronous context
free grammars (SCFG) (Chiang, 2007; Zollmann
and Venugopal, 2006; Galley et al, 2006) have
recently been shown to give competitive perfor-
mance relative to phrase-based SMT. For these
systems, a hypergraph or packed forest provides a
compact representation for encoding a huge num-
ber of translation hypotheses (Huang, 2008).
In this paper, we extend MERT and MBR
decoding to work on hypergraphs produced by
SCFG-based MT systems. We present algorithms
that are more efficient relative to the lattice al-
gorithms presented in Macherey et al (2008;
Tromble et al (2008). Lattice MBR decoding uses
a linear approximation to the BLEU score (Pap-
ineni et al, 2001); the weights in this linear loss
are set heuristically by assuming that n-gram pre-
cisions decay exponentially with n. However, this
may not be optimal in practice. We employ MERT
to select these weights by optimizing BLEU score
on a development set.
A related MBR-inspired approach for hyper-
graphs was developed by Zhang and Gildea
(2008). In this work, hypergraphs were rescored to
maximize the expected count of synchronous con-
stituents in the translation. In contrast, our MBR
algorithm directly selects the hypothesis in the
hypergraph with the maximum expected approx-
imate corpus BLEU score (Tromble et al, 2008).
will soon announce 
X1  X2
X1  X2
X1  X2
X1  X2
X1  X2
X1 its future in the 
X1 its future in the 
Suzuki
soon
its future in
X1 announces
Rally World Championship
Figure 1: An example hypergraph.
163
2 Translation Hypergraphs
A translation lattice compactly encodes a large
number of hypotheses produced by a phrase-based
SMT system. The corresponding representation
for an SMT system based on SCFGs (e.g. Chi-
ang (2007), Zollmann and Venugopal (2006), Mi
et al (2008)) is a directed hypergraph or a packed
forest (Huang, 2008).
Formally, a hypergraph is a pair H = ?V, E?
consisting of a vertex set V and a set of hyperedges
E ? V? ? V . Each hyperedge e ? E connects a
head vertex h(e) with a sequence of tail vertices
T (e) = {v1, ..., vn}. The number of tail vertices
is called the arity (|e|) of the hyperedge. If the ar-
ity of a hyperedge is zero, h(e) is called a source
vertex. The arity of a hypergraph is the maximum
arity of its hyperedges. A hyperedge of arity 1 is a
regular edge, and a hypergraph of arity 1 is a regu-
lar graph (lattice). Each hyperedge is labeled with
a rule re from the SCFG. The number of nontermi-
nals on the right-hand side of re corresponds with
the arity of e. An example without scores is shown
in Figure 1. A path in a translation hypergraph in-
duces a translation hypothesis E along with its se-
quence of SCFG rules D = r1, r2, ..., rK which,
if applied to the start symbol, derives E. The se-
quence of SCFG rules induced by a path is also
called a derivation tree for E.
3 Minimum Error Rate Training
Given a set of source sentences FS1 with corre-
sponding reference translations RS1 , the objective
of MERT is to find a parameter set ??M1 which min-
imizes an automated evaluation criterion under a
linear model:
??M1 = argmin
?M1
? SX
s=1
Err
`
Rs, E?(Fs; ?
M
1 )
?
ff
E?(Fs; ?
M
1 ) = argmax
E
? SX
s=1
?mhm(E, Fs)
ff
.
In the context of statistical machine translation,
the optimization procedure was first described in
Och (2003) for N -best lists and later extended to
phrase-lattices in Macherey et al (2008). The al-
gorithm is based on the insight that, under a log-
linear model, the cost function of any candidate
translation can be represented as a line in the plane
if the initial parameter set ?M1 is shifted along a
direction dM1 . Let C = {E1, ..., EK} denote a set
of candidate translations, then computing the best
scoring translation hypothesis E? out of C results in
the following optimization problem:
E?(F ; ?) = argmax
E?C
n
(?M1 + ? ? d
M
1 )
> ? hM1 (E,F )
o
= argmax
E?C
?
X
m
?mhm(E,F )
| {z }
=a(E,F )
+ ? ?
X
m
dmhm(E,F )
| {z }
=b(E,F )
ff
= argmax
E?C
?
a(E,F ) + ? ? b(E,F )
| {z }
(?)
?
Hence, the total score (?) for each candidate trans-
lation E ? C can be described as a line with
? as the independent variable. For any particu-
lar choice of ?, the decoder seeks that translation
which yields the largest score and therefore corre-
sponds to the topmost line segment. If ? is shifted
from ?? to +?, other translation hypotheses
may at some point constitute the topmost line seg-
ments and thus change the decision made by the
decoder. The entire sequence of topmost line seg-
ments is called upper envelope and provides an ex-
haustive representation of all possible outcomes
that the decoder may yield if ? is shifted along
the chosen direction. Both the translations and
their corresponding line segments can efficiently
be computed without incorporating any error crite-
rion. Once the envelope has been determined, the
translation candidates of its constituent line seg-
ments are projected onto their corresponding error
counts, thus yielding the exact and unsmoothed er-
ror surface for all candidate translations encoded
in C. The error surface can now easily be traversed
in order to find that ?? under which the new param-
eter set ?M1 + ?? ? d
M
1 minimizes the global error.
In this section, we present an extension of the
algorithm described in Macherey et al (2008)
that allows us to efficiently compute and repre-
sent upper envelopes over all candidate transla-
tions encoded in hypergraphs. Conceptually, the
algorithm works by propagating (initially empty)
envelopes from the hypergraph?s source nodes
bottom-up to its unique root node, thereby ex-
panding the envelopes by applying SCFG rules to
the partial candidate translations that are associ-
ated with the envelope?s constituent line segments.
To recombine envelopes, we need two operators:
the sum and the maximum over convex polygons.
To illustrate which operator is applied when, we
transform H = ?V, E? into a regular graph with
typed nodes by (1) marking all vertices v ? V with
the symbol ? and (2) replacing each hyperedge
e ? E , |e| > 1, with a small subgraph consisting
of a new vertex v?(e) whose incoming and out-
going edges connect the same head and tail nodes
164
Algorithm 1 ?-operation (Sum)
input: associative map a: V ? Env(V), hyperarc e
output: Minkowski sum of envelopes over T (e)
for (i = 0; i < |T (e)|; ++i) {
v = Ti(e);
pq.enqueue(? v, i, 0?);
}
L = ?;
D = ? e, ?1 ? ? ? ?|e|?
while (!pq.empty()) {
? v, i, j? = pq.dequeue();
` = A[v][j];
D[i+1] = `.D;
if (L.empty() ? L.back().x < `.x) {
if (0 < j) {
`.y += L.back().y - A[v][j-1].y;
`.m += L.back().m - A[v][j-1].m;
}
L.push_back(`);
L.back().D = D;
} else {
L.back().y += `.y;
L.back().m += `.m;
L.back().D[i+1] = `.D;
if (0 < j) {
L.back().y -= A[v][j-1].y;
L.back().m -= A[v][j-1].m;
}
}
if (++j < A[v].size())
pq.enqueue(? v, i, j?);
}
return L;
in the transformed graph as were connected by e
in the original graph. The unique outgoing edge
of v?(e) is associated with the rule re; incoming
edges are not linked to any rule. Figure 2 illus-
trates the transformation for a hyperedge with ar-
ity 3. The graph transformation is isomorphic.
The rules associated with every hyperedge spec-
ify how line segments in the envelopes of a hyper-
edge?s tail nodes can be combined. Suppose we
have a hyperedge e with rule re : X ? aX1bX2c
and T (e) = {v1, v2}. Then we substitute X1 and
X2 in the rule with candidate translations associ-
ated with line segments in envelopes Env(v1) and
Env(v2) respectively.
To derive the algorithm, we consider the gen-
eral case of a hyperedge e with rule re : X ?
w1X1w2...wnXnwn+1. Because the right-hand
side of re has n nonterminals, the arity of e is
|e| = n. Let T (e) = {v1, ..., vn} denote the
tail nodes of e. We now assume that each tail
node vi ? T (e) is associated with the upper en-
velope over all candidate translations that are in-
duced by derivations of the corresponding nonter-
minal symbol Xi. These envelopes shall be de-
Algorithm 2 ?-operation (Max)
input: array L[0..K-1] containing line objects
output: upper envelope of L
Sort(L:m);
j = 0; K = size(L);
for (i = 0; i < K; ++i) {
` = L[i];
`.x = -?;
if (0 < j) {
if (L[j-1].m == `.m) {
if (`.y <= L[j-1].y) continue;
--j;
}
while (0 < j) {
`.x = (`.y - L[j-1].y)/
(L[j-1].m - `.m);
if (L[j-1].x < `.x) break;
--j;
}
if (0 == j) `.x = -?;
L[j++] = `;
} else L[j++] = `;
}
L.resize(j);
return L;
noted by Env(vi). To decompose the problem of
computing and propagating the tail envelopes over
the hyperedge e to its head node, we now define
two operations, one for either node type, to spec-
ify how envelopes associated with the tail vertices
are propagated to the head vertex.
Nodes of Type ???: For a type ? node, the
resulting envelope is the Minkowski sum over
the envelopes of the incoming edges (Berg et
al., 2008). Since the envelopes of the incoming
edges are convex hulls, the Minkowski sum pro-
vides an upper bound to the number of line seg-
ments that constitute the resulting envelope: the
bound is the sum over the number of line seg-
ments in the envelopes of the incoming edges, i.e.:?
?Env(v?(e))
?
? ?
?
v??T (e)
?
?Env(v?)
?
?.
Algorithm 1 shows the pseudo code for comput-
ing the Minkowski sum over multiple envelopes.
The line objects ` used in this algorithm are
encoded as 4-tuples, each consisting of the x-
intercept with `?s left-adjacent line stored as `.x,
the slope `.m, the y-intercept `.y, and the (partial)
derivation tree `.D. At the beginning, the leftmost
line segment of each envelope is inserted into a
priority queue pq. The priority is defined in terms
of a line?s x-intercept such that lower values imply
higher priority. Hence, the priority queue enumer-
ates all line segments from left to right in ascend-
ing order of their x-intercepts, which is the order
needed to compute the Minkowski sum.
Nodes of Type ???: The operation performed
165
=?
= max
Figure 2: Transformation of a hypergraph into
a factor graph and bottom-up propagation of en-
velopes.
at nodes of type ??? computes the convex hull
over the union of the envelopes propagated over
the incoming edges. This operation is a ?max?
operation and it is identical to the algorithm de-
scribed in (Macherey et al, 2008) for phrase lat-
tices. Algorithm 2 contains the pseudo code.
The complete algorithm then works as follows:
Traversing all nodes in H bottom-up in topolog-
ical order, we proceed for each node v ? V over
its incoming hyperedges and combine in each such
hyperedge e the envelopes associated with the tail
nodes T (e) by computing their sum according to
Algorithm 1 (?-operation). For each incoming
hyperedge e, the resulting envelope is then ex-
panded by applying the rule re to its constituent
line segments. The envelopes associated with dif-
ferent incoming hyperedges of node v are then
combined and reduced according to Algorithm 2
(?-operation). By construction, the envelope at
the root node is the convex hull over the line seg-
ments of all candidate translations that can be de-
rived from the hypergraph.
The suggested algorithm has similar properties
as the algorithm presented in (Macherey et al,
2008). In particular, it has the same upper bound
on the number of line segments that constitute the
envelope at the root node, i.e, the size of this enve-
lope is guaranteed to be no larger than the number
of edges in the transformed hypergraph.
4 Minimum Bayes-Risk Decoding
We first review Minimum Bayes-Risk (MBR) de-
coding for statistical MT. An MBR decoder seeks
the hypothesis with the least expected loss under a
probability model (Bickel and Doksum, 1977). If
we think of statistical MT as a classifier that maps
a source sentence F to a target sentence E, the
MBR decoder can be expressed as follows:
E? = argmin
E??G
?
E?G
L(E,E?)P (E|F ), (1)
where L(E,E?) is the loss between any two hy-
potheses E and E?, P (E|F ) is the probability
model, and G is the space of translations (N -best
list, lattice, or a hypergraph).
MBR decoding for translation can be performed
by reranking an N -best list of hypotheses gener-
ated by an MT system (Kumar and Byrne, 2004).
This reranking can be done for any sentence-
level loss function such as BLEU (Papineni et al,
2001), Word Error Rate, or Position-independent
Error Rate.
Recently, Tromble et al (2008) extended
MBR decoding to translation lattices under an
approximate BLEU score. They approximated
log(BLEU) score by a linear function of n-gram
matches and candidate length. If E and E? are the
reference and the candidate translations respec-
tively, this linear function is given by:
G(E,E?) = ?0|E
?|+
?
w
?|w|#w(E
?)?w(E), (2)
where w is an n-gram present in either E or E?,
and ?0, ?1, ..., ?N are weights which are deter-
mined empirically, where N is the maximum n-
gram order.
Under such a linear decomposition, the MBR
decoder (Equation 1) can be written as
E? = argmax
E??G
?0|E
?|+
?
w
?|w|#w(E
?)p(w|G), (3)
where the posterior probability of an n-gram in the
lattice is given by
p(w|G) =
?
E?G
1w(E)P (E|F ). (4)
Tromble et al (2008) implement the MBR
decoder using Weighted Finite State Automata
(WFSA) operations. First, the set of n-grams
is extracted from the lattice. Next, the posterior
probability of each n-gram is computed. A new
automaton is then created by intersecting each n-
gram with weight (from Equation 2) to an un-
weighted lattice. Finally, the MBR hypothesis is
extracted as the best path in the automaton. We
will refer to this procedure as FSAMBR.
The above steps are carried out one n-gram at
a time. For a moderately large lattice, there can
be several thousands of n-grams and the proce-
dure becomes expensive. We now present an alter-
nate approximate procedure which can avoid this
166
enumeration making the resulting algorithm much
faster than FSAMBR.
4.1 Efficient MBR for lattices
The key idea behind this new algorithm is to
rewrite the n-gram posterior probability (Equa-
tion 4) as follows:
p(w|G) =
?
E?G
?
e?E
f(e, w,E)P (E|F ) (5)
where f(e, w,E) is a score assigned to edge e on
path E containing n-gram w:
f(e, w,E) =
?
?
?
1 w ? e, p(e|G) > p(e?|G),
e? precedes e on E
0 otherwise
(6)
In other words, for each pathE, we count the edge
that contributes n-gramw and has the highest edge
posterior probability relative to its predecessors on
the path E; there is exactly one such edge on each
lattice path E.
We note that f(e, w,E) relies on the full path
E which means that it cannot be computed based
on local statistics. We therefore approximate the
quantity f(e, w,E) with f?(e, w,G) that counts
the edge e with n-gram w that has the highest arc
posterior probability relative to predecessors in the
entire lattice G. f?(e, w,G) can be computed lo-
cally, and the n-gram posterior probability based
on f? can be determined as follows:
p(w|G) =
X
E?G
X
e?E
f?(e, w,G)P (E|F ) (7)
=
X
e?E
1w?ef
?(e, w,G)
X
E?G
1E(e)P (E|F )
=
X
e?E
1w?ef
?(e, w,G)P (e|G),
where P (e|G) is the posterior probability of a lat-
tice edge. The algorithm to perform Lattice MBR
is given in Algorithm 3. For each node t in the lat-
tice, we maintain a quantity Score(w, t) for each
n-gram w that lies on a path from the source node
to t. Score(w, t) is the highest posterior probabil-
ity among all edges on the paths that terminate on t
and contain n-gram w. The forward pass requires
computing the n-grams introduced by each edge;
to do this, we propagate n-grams (up to maximum
order ?1) terminating on each node.
4.2 Extension to Hypergraphs
We next extend the Lattice MBR decoding algo-
rithm (Algorithm 3) to rescore hypergraphs pro-
duced by a SCFG based MT system. Algorithm 4
is an extension to the MBR decoder on lattices
Algorithm 3 MBR Decoding on Lattices
1: Sort the lattice nodes topologically.
2: Compute backward probabilities of each node.
3: Compute posterior prob. of each n-gram:
4: for each edge e do
5: Compute edge posterior probability P (e|G).
6: Compute n-gram posterior probs. P (w|G):
7: for each n-gram w introduced by e do
8: Propagate n? 1 gram suffix to he.
9: if p(e|G) > Score(w, T (e)) then
10: Update posterior probs. and scores:
p(w|G) += p(e|G) ? Score(w, T (e)).
Score(w, he) = p(e|G).
11: else
12: Score(w, he) = Score(w, T (e)).
13: end if
14: end for
15: end for
16: Assign scores to edges (given by Equation 3).
17: Find best path in the lattice (Equation 3).
(Algorithm 3). However, there are important dif-
ferences when computing the n-gram posterior
probabilities (Step 3). In this inside pass, we now
maintain both n-gram prefixes and suffixes (up to
the maximum order?1) on each hypergraph node.
This is necessary because unlike a lattice, new n-
grams may be created at subsequent nodes by con-
catenating words both to the left and the right side
of the n-gram. When the arity of the edge is 2,
a rule has the general form aX1bX2c, where X1
and X2 are sequences from tail nodes. As a result,
we need to consider all new sequences which can
be created by the cross-product of the n-grams on
the two tail nodes. E.g. if X1 = {c, cd, d} and
X2 = {f, g}, then a total of six sequences will
result. In practice, such a cross-product is not pro-
Algorithm 4 MBR Decoding on Hypergraphs
1: Sort the hypergraph nodes topologically.
2: Compute inside probabilities of each node.
3: Compute posterior prob. of each hyperedge P (e|G).
4: Compute posterior prob. of each n-gram:
5: for each hyperedge e do
6: Merge the n-grams on the tail nodes T (e). If the
same n-gram is present on multiple tail nodes, keep
the highest score.
7: Apply the rule on e to the n-grams on T (e).
8: Propagate n? 1 gram prefixes/suffixes to he.
9: for each n-gram w introduced by this hyperedge do
10: if p(e|G) > Score(w, T (e)) then
11: p(w|G) += p(e|G) ? Score(w, T (e))
Score(w, he) = p(e|G)
12: else
13: Score(w, he) = Score(w, T (e))
14: end if
15: end for
16: end for
17: Assign scores to hyperedges (Equation 3).
18: Find best path in the hypergraph (Equation 3).
167
hibitive when the maximum n-gram order in MBR
does not exceed the order of the n-gram language
model used in creating the hypergraph. In the lat-
ter case, we will have a small set of unique prefixes
and suffixes on the tail nodes.
5 MERT for MBR Parameter
Optimization
Lattice MBR Decoding (Equation 3) assumes a
linear form for the gain function (Equation 2).
This linear function contains n + 1 parameters
?0, ?1, ..., ?N , where N is the maximum order of
the n-grams involved. Tromble et al (2008) ob-
tained these factors as a function of n-gram preci-
sions derived from multiple training runs. How-
ever, this does not guarantee that the resulting
linear score (Equation 2) is close to the corpus
BLEU. We now describe how MERT can be used
to estimate these factors to achieve a better ap-
proximation to the corpus BLEU.
We recall that MERT selects weights in a lin-
ear model to optimize an error criterion (e.g. cor-
pus BLEU) on a training set. The lattice MBR
decoder (Equation 3) can be written as a lin-
ear model: E? = argmaxE??G
?N
i=0 ?igi(E
?, F ),
where g0(E?, F ) = |E?| and gi(E?, F ) =?
w:|w|=i #w(E
?)p(w|G).
The linear approximation to BLEU may not
hold in practice for unseen test sets or language-
pairs. Therefore, we would like to allow the de-
coder to backoff to the MAP translation in such
cases. To do that, we introduce an additional fea-
ture function gN+1(E,F ) equal to the original de-
coder cost for this sentence. A weight assignment
of 1.0 for this feature function and zeros for the
other feature functions would imply that the MAP
translation is chosen. We now have a total ofN+2
feature functions which we optimize using MERT
to obtain highest BLEU score on a training set.
6 Experiments
We now describe our experiments to evaluate
MERT and MBR on lattices and hypergraphs, and
show how MERT can be used to tune MBR pa-
rameters.
6.1 Translation Tasks
We report results on two tasks. The first one is
the constrained data track of the NIST Arabic-
to-English (aren) and Chinese-to-English (zhen)
translation task1. On this task, the parallel and the
1http://www.nist.gov/speech/tests/mt
Dataset # of sentences
aren zhen
dev 1797 1664
nist02 1043 878
nist03 663 919
Table 1: Statistics over the NIST dev/test sets.
monolingual data included all the allowed train-
ing sets for the constrained track. Table 1 reports
statistics computed over these data sets. Our de-
velopment set (dev) consists of the NIST 2005 eval
set; we use this set for optimizing MBR parame-
ters. We report results on NIST 2002 and NIST
2003 evaluation sets.
The second task consists of systems for 39
language-pairs with English as the target language
and trained on at most 300M word tokens mined
from the web and other published sources. The de-
velopment and test sets for this task are randomly
selected sentences from the web, and contain 5000
and 1000 sentences respectively.
6.2 MT System Description
Our phrase-based statistical MT system is simi-
lar to the alignment template system described in
(Och and Ney, 2004; Tromble et al, 2008). Trans-
lation is performed using a standard dynamic pro-
gramming beam-search decoder (Och and Ney,
2004) using two decoding passes. The first de-
coder pass generates either a lattice or an N -best
list. MBR decoding is performed in the second
pass.
We also train two SCFG-based MT systems:
a hierarchical phrase-based SMT (Chiang, 2007)
system and a syntax augmented machine transla-
tion (SAMT) system using the approach described
in Zollmann and Venugopal (2006). Both systems
are built on top of our phrase-based systems. In
these systems, the decoder generates an initial hy-
pergraph or anN -best list, which are then rescored
using MBR decoding.
6.3 MERT Results
Table 2 shows runtime experiments for the hyper-
graph MERT implementation in comparison with
the phrase-lattice implementation on both the aren
and the zhen system. The first two columns show
the average amount of time in msecs that either
algorithm requires to compute the upper envelope
when applied to phrase lattices. Compared to the
algorithm described in (Macherey et al, 2008)
which is optimized for phrase lattices, the hyper-
graph implementation causes a small increase in
168
Avg. Runtime/sent [msec]
(Macherey 2008) Suggested Alg.
aren zhen aren zhen
phrase lattice 8.57 7.91 10.30 8.65
hypergraph ? ? 8.19 8.11
Table 2: Average time for computing envelopes.
running time. This increase is mainly due to the
representation of line segments; while the phrase-
lattice implementation stores a single backpointer,
the hypergraph version stores a vector of back-
pointers.
The last two columns show the average amount
of time that is required to compute the upper en-
velope on hypergraphs. For comparison, we prune
hypergraphs to the same density (# of edges per
edge on the best path) and achieve identical run-
ning times for computing the error surface.
6.4 MBR Results
We first compare the new lattice MBR (Algo-
rithm 3) with MBR decoding on 1000-best lists
and FSAMBR (Tromble et al, 2008) on lattices
generated by the phrase-based systems; evaluation
is done using both BLEU and average run-time per
sentence (Table 3). Note that N -best MBR uses
a sentence BLEU loss function. The new lattice
MBR algorithm gives about the same performance
as FSAMBR while yielding a 20X speedup.
We next report the performance of MBR on hy-
pergraphs generated by Hiero/SAMT systems. Ta-
ble 4 compares Hypergraph MBR (HGMBR) with
MAP and MBR decoding on 1000 best lists. On
some systems such as the Arabic-English SAMT,
the gains from Hypergraph MBR over 1000-best
MBR are significant. In other cases, Hypergraph
MBR performs at least as well as N -best MBR.
In all cases, we observe a 7X speedup in run-
time. This shows the usefulness of Hypergraph
MBR decoding as an efficient alternative to N -
best MBR.
6.5 MBR Parameter Tuning with MERT
We now describe the results by tuning MBR n-
gram parameters (Equation 2) using MERT. We
first compute N + 1 MBR feature functions on
each edge of the lattice/hypergraph. We also in-
clude the total decoder cost on the edge as as addi-
tional feature function. MERT is then performed
to optimize the BLEU score on a development set;
For MERT, we use 40 random initial parameters as
well as parameters computed using corpus based
statistics (Tromble et al, 2008).
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
MAP 54.2 64.2 40.1 39.0 -
N -best MBR 54.3 64.5 40.2 39.2 3.7
Lattice MBR
FSAMBR 54.9 65.2 40.6 39.5 3.7
LatMBR 54.8 65.2 40.7 39.4 0.2
Table 3: Lattice MBR for a phrase-based system.
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
Hiero
MAP 52.8 62.9 41.0 39.8 -
N -best MBR 53.2 63.0 41.0 40.1 3.7
HGMBR 53.3 63.1 41.0 40.2 0.5
SAMT
MAP 53.4 63.9 41.3 40.3 -
N -best MBR 53.8 64.3 41.7 41.1 3.7
HGMBR 54.0 64.6 41.8 41.1 0.5
Table 4: Hypergraph MBR for Hiero/SAMT systems.
Table 5 shows results for NIST systems. We
report results on nist03 set and present three sys-
tems for each language pair: phrase-based (pb),
hierarchical (hier), and SAMT; Lattice MBR is
done for the phrase-based system while HGMBR
is used for the other two. We select the MBR
scaling factor (Tromble et al, 2008) based on the
development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5
and 1.0 for the aren-phrase, aren-hier, aren-samt,
zhen-phrase zhen-hier and zhen-samt systems re-
spectively. For the multi-language case, we train
phrase-based systems and perform lattice MBR
for all language pairs. We use a scaling factor of
0.7 for all pairs. Additional gains can be obtained
by tuning this factor; however, we do not explore
that dimension in this paper. In all cases, we prune
the lattices/hypergraphs to a density of 30 using
forward-backward pruning (Sixtus and Ortmanns,
1999).
We consider a BLEU score difference to be a)
gain if is at least 0.2 points, b) drop if it is at most
-0.2 points, and c) no change otherwise. The re-
sults are shown in Table 6. In both tables, the fol-
lowing results are reported: Lattice/HGMBR with
default parameters (?5, 1.5, 2, 3, 4) computed us-
ing corpus statistics (Tromble et al, 2008),
Lattice/HGMBR with parameters derived from
MERT both without/with the baseline model cost
feature (mert?b, mert+b). For multi-language
systems, we only show the # of language-pairs
with gains/no-changes/drops for each MBR vari-
ant with respect to the MAP translation.
169
We observed in the NIST systems that MERT
resulted in short translations relative to MAP on
the unseen test set. To prevent this behavior,
we modify the MERT error criterion to include
a sentence-level brevity scorer with parameter ?:
BLEU+brevity(?). This brevity scorer penalizes
each candidate translation that is shorter than the
average length over its reference translations, us-
ing a penalty term which is linear in the difference
between either length. We tune ? on the develop-
ment set so that the brevity score of MBR transla-
tion is close to that of the MAP translation.
In the NIST systems, MERT yields small im-
provements on top of MBR with default param-
eters. This is the case for Arabic-English Hi-
ero/SAMT. In all other cases, we see no change
or even a slight degradation due to MERT.
We hypothesize that the default MBR parame-
ters (Tromble et al, 2008) are well tuned. There-
fore there is little gain by additional tuning using
MERT.
In the multi-language systems, the results show
a different trend. We observe that MBR with de-
fault parameters results in gains on 18 pairs, no
differences on 9 pairs, and losses on 12 pairs.
When we optimize MBR features with MERT, the
number of language pairs with gains/no changes/-
drops is 22/5/12. Thus, MERT has a bigger impact
here than in the NIST systems. We hypothesize
that the default MBR parameters are sub-optimal
for some language pairs and that MERT helps to
find better parameter settings. In particular, MERT
avoids the need for manually tuning these param-
eters by language pair.
Finally, when baseline model costs are added
as an extra feature (mert+b), the number of pairs
with gains/no changes/drops is 26/8/5. This shows
that this feature can allow MBR decoding to back-
off to the MAP translation. When MBR does not
produce a higher BLEU score relative to MAP
on the development set, MERT assigns a higher
weight to this feature function. We see such an
effect for 4 systems.
7 Discussion
We have presented efficient algorithms
which extend previous work on lattice-based
MERT (Macherey et al, 2008) and MBR de-
coding (Tromble et al, 2008) to work with
hypergraphs. Our new MERT algorithm can work
with both lattices and hypergraphs. On lattices, it
achieves similar run-times as the implementation
System BLEU (%)
MAP MBR
default mert-b mert+b
aren.pb 54.2 54.8 54.8 54.9
aren.hier 52.8 53.3 53.5 53.7
aren.samt 53.4 54.0 54.4 54.0
zhen.pb 40.1 40.7 40.7 40.9
zhen.hier 41.0 41.0 41.0 41.0
zhen.samt 41.3 41.8 41.6 41.7
Table 5: MBR Parameter Tuning on NIST systems
MBR wrt. MAP default mert-b mert+b
# of gains 18 22 26
# of no-changes 9 5 8
# of drops 12 12 5
Table 6: MBR on Multi-language systems.
described in Macherey et al (2008). The new
Lattice MBR decoder achieves a 20X speedup
relative to either FSAMBR implementation
described in Tromble et al (2008) or MBR on
1000-best lists. The algorithm gives comparable
results relative to FSAMBR. On hypergraphs
produced by Hierarchical and Syntax Augmented
MT systems, our MBR algorithm gives a 7X
speedup relative to 1000-best MBR while giving
comparable or even better performance.
Lattice MBR decoding is obtained under a lin-
ear approximation to BLEU, where the weights
are obtained using n-gram precisions derived from
development data. This may not be optimal in
practice for unseen test sets and language pairs,
and the resulting linear loss may be quite differ-
ent from the corpus level BLEU. In this paper, we
have described how MERT can be employed to
estimate the weights for the linear loss function
to maximize BLEU on a development set. On an
experiment with 40 language pairs, we obtain im-
provements on 26 pairs, no difference on 8 pairs
and drops on 5 pairs. This was achieved with-
out any need for manual tuning for each language
pair. The baseline model cost feature helps the al-
gorithm effectively back off to the MAP transla-
tion in language pairs where MBR features alone
would not have helped.
MERT and MBR decoding are popular tech-
niques for incorporating the final evaluation met-
ric into the development of SMT systems. We be-
lieve that our efficient algorithms will make them
more widely applicable in both SCFG-based and
phrase-based MT systems.
170
References
M. Berg, O. Cheong, M. Krefeld, and M. Overmars,
2008. Computational Geometry: Algorithms and
Applications, chapter 13, pages 290?296. Springer-
Verlag, 3rd edition.
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
D. Chiang. 2007. Hierarchical phrase based transla-
tion . Computational Linguistics, 33(2):201 ? 228.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation
Models. . In COLING/ACL, Sydney, Australia.
L. Huang. 2008. Advanced Dynamic Programming
in Semiring and Hypergraph Frameworks. In COL-
ING, Manchester, UK.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, Boston, MA, USA.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based Minimum Error Rate Train-
ing for Statistical Machine Translation. In EMNLP,
Honolulu, Hawaii, USA.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based
Translation. In ACL, Columbus, OH, USA.
F. Och and H. Ney. 2004. The Alignment Template
Approach to Statistical Machine Translation. Com-
putational Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176
(W0109-022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
R. Tromble, S. Kumar, F. Och, andW.Macherey. 2008.
Lattice Minimum Bayes-Risk Decoding for Statis-
tical Machine Translation. In EMNLP, Honolulu,
Hawaii.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass
Decoding for Synchronous Context Free Grammars.
In ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
171
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 957?965,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Expected Sequence Similarity Maximization
Cyril Allauzen1, Shankar Kumar1, Wolfgang Macherey1, Mehryar Mohri2,1 and Michael Riley1
1Google Research, 76 Ninth Avenue, New York, NY 10011
2Courant Institute of Mathematical Sciences, 251 Mercer Street, New York, NY 10012
Abstract
This paper presents efficient algorithms for
expected similarity maximization, which co-
incides with minimum Bayes decoding for a
similarity-based loss function. Our algorithms
are designed for similarity functions that are
sequence kernels in a general class of posi-
tive definite symmetric kernels. We discuss
both a general algorithm and a more efficient
algorithm applicable in a common unambigu-
ous scenario. We also describe the applica-
tion of our algorithms to machine translation
and report the results of experiments with sev-
eral translation data sets which demonstrate a
substantial speed-up. In particular, our results
show a speed-up by two orders of magnitude
with respect to the original method of Tromble
et al (2008) and by a factor of 3 or more
even with respect to an approximate algorithm
specifically designed for that task. These re-
sults open the path for the exploration of more
appropriate or optimal kernels for the specific
tasks considered.
1 Introduction
The output of many complex natural language pro-
cessing systems such as information extraction,
speech recognition, or machine translation systems
is a probabilistic automaton. Exploiting the full in-
formation provided by this probabilistic automaton
can lead to more accurate results than just using the
one-best sequence.
Different techniques have been explored in the
past to take advantage of the full lattice, some based
on the use of a more complex model applied to
the automaton as in rescoring, others using addi-
tional data or information for reranking the hypothe-
ses represented by the automaton. One method for
using these probabilistic automata that has been suc-
cessful in large-vocabulary speech recognition (Goel
and Byrne, 2000) and machine translation (Kumar
and Byrne, 2004; Tromble et al, 2008) applications
and that requires no additional data or other com-
plex models is the minimum Bayes risk (MBR) de-
coding technique. This returns that sequence of the
automaton having the minimum expected loss with
respect to all sequences accepted by the automaton
(Bickel and Doksum, 2001). Often, minimizing the
loss function L can be equivalently viewed as max-
imizing a similarity function K between sequences,
which corresponds to a kernel function when it is
positive definite symmetric (Berg et al, 1984). The
technique can then be thought of as an expected se-
quence similarity maximization.
This paper considers this expected similarity max-
imization view. Since different similarity functions
can be used within this framework, one may wish to
select the one that is the most appropriate or relevant
to the task considered. However, a crucial require-
ment for this choice to be realistic is to ensure that
for the family of similarity functions considered the
expected similarity maximization is efficiently com-
putable. Thus, we primarily focus on this algorith-
mic problem in this paper, leaving it to future work
to study the question of determining how to select
the similarity function and report on the benefits of
this choice.
A general family of sequence kernels including
the sequence kernels used in computational biology,
text categorization, spoken-dialog classification, and
many other tasks is that of rational kernels (Cortes
et al, 2004). We show how the expected similarity
maximization can be efficiently computed for these
kernels. In section 3, we describe more specifically
the framework of expected similarity maximization
in the case of rational kernels and the correspond-
ing algorithmic problem. In Section 4, we describe
both a general method for the computation of the ex-
pected similarity maximization, and a more efficient
method that can be used with a broad sub-family
of rational kernels that verify a condition of non-
ambiguity. This latter family includes the class of
n-gram kernels which have been previously used to
957
apply MBR to machine translation (Tromble et al,
2008). We examine in more detail the use and ap-
plication of our algorithms to machine translation
in Section 5. Section 6 reports the results of ex-
periments applying our algorithms in several large
data sets in machine translation. These experiments
demonstrate the efficiency of our algorithm which
is shown empirically to be two orders of magnitude
faster than Tromble et al (2008) and more than 3
times faster than even an approximation algorithm
specifically designed for this problem (Kumar et al,
2009). We start with some preliminary definitions
and algorithms related to weighted automata and
transducers, following the definitions and terminol-
ogy of Cortes et al (2004).
2 Preliminaries
Weighted transducers are finite-state transducers in
which each transition carries some weight in addi-
tion to the input and output labels. The weight set
has the structure of a semiring.
A semiring (K,?,?, 0, 1) verifies all the axioms
of a ring except from the existence of a negative el-
ement ?x for each x ? K, which it may verify or
not. Thus, roughly speaking, a semiring is a ring
that may lack negation. It is specified by a set of
values K, two binary operations ? and ?, and two
designated values 0 and 1. When ? is commutative,
the semiring is said to be commutative.
The real semiring (R+,+,?, 0, 1) is used when
the weights represent probabilities. The log
semiring (R ? {??,+?},?log,+,?, 0) is iso-
morphic to the real semiring via the negative-
log mapping and is often used in practice
for numerical stability. The tropical semiring
(R?, {??,+?},min,+,?, 0) is derived from
the log semiring via the Viterbi approximation and
is often used in shortest-path applications.
Figure 1(a) shows an example of a weighted
finite-state transducer over the real semiring
(R+,+,?, 0, 1). In this figure, the input and out-
put labels of a transition are separated by a colon
delimiter and the weight is indicated after the slash
separator. A weighted transducer has a set of initial
states represented in the figure by a bold circle and
a set of final states, represented by double circles. A
path from an initial state to a final state is an accept-
ing path.
The weight of an accepting path is obtained by
first ?-multiplying the weights of its constituent
0                          a:b/1
1
a:b/2
2/1
a:b/4
3/8
b:a/6
b:a/3
b:a/5
0                        b/1
1
b/2
2/1
b/4
3/8
a/6
a/3
a/5
(a) (b)
Figure 1: (a) Example of weighted transducer T over the
real semiring (R+,+,?, 0, 1). (b) Example of weighted
automaton A. A can be obtained from T by projection on
the output and T (aab, bba) = A(bba) = 1? 2? 6? 8+
2? 4? 5? 8.
transitions and?-multiplying this product on the left
by the weight of the initial state of the path (which
equals 1 in our work) and on the right by the weight
of the final state of the path (displayed after the slash
in the figure). The weight associated by a weighted
transducer T to a pair of strings (x, y) ? ?? ??? is
denoted by T (x, y) and is obtained by ?-summing
the weights of all accepting paths with input label x
and output label y.
For any transducer T , T?1 denotes its inverse,
that is the transducer obtained from T by swapping
the input and output labels of each transition. For all
x, y ? ??, we have T?1(x, y) = T (y, x).
The composition of two weighted transducers T1
and T2 with matching input and output alphabets ?,
is a weighted transducer denoted by T1 ? T2 when
the semiring is commutative and the sum:
(T1 ? T2)(x, y) =
?
z???
T1(x, z)? T2(z, y) (1)
is well-defined and in K for all x, y (Salomaa and
Soittola, 1978).
Weighted automata can be defined as weighted
transducers A with identical input and output labels,
for any transition. Since only pairs of the form (x, x)
can have a non-zero weight associated to them by
A, we denote the weight associated by A to (x, x)
by A(x) and call it the weight associated by A to
x. Similarly, in the graph representation of weighted
automata, the output (or input) label is omitted. Fig-
ure 1(b) shows an example of a weighted automa-
ton. When A and B are weighted automata, A ? B
is called the intersection of A and B. Omitting the
input labels of a weighted transducer T results in a
weighted automaton which is said to be the output
projection of T .
958
3 General Framework
Let X be a probabilistic automaton representing the
output of a complex model for a specific query input.
The model may be for example a speech recognition
system, an information extraction system, or a ma-
chine translation system (which originally motivated
our study). For machine translation, the sequences
accepted by X are the potential translations of the
input sentence, each with some probability given by
X .
Let ? be the alphabet for the task considered, e.g.,
words of the target language in machine translation,
and let L : ?? ? ?? ? R denote a loss function
defined over the sequences on that alphabet. Given
a reference or hypothesis set H ? ??, minimum
Bayes risk (MBR) decoding consists of selecting a
hypothesis x ? H with minimum expected loss with
respect to the probability distribution X (Bickel and
Doksum, 2001; Tromble et al, 2008):
x? = argmin
x?H
E
x??X
[L(x, x?)]. (2)
Here, we shall consider the case, frequent in prac-
tice, where minimizing the loss L is equivalent to
maximizing a similarity measure K : ????? ? R.
When K is a sequence kernel that can be repre-
sented by weighted transducers, it is a rational ker-
nel (Cortes et al, 2004). The problem is then equiv-
alent to the following expected similarity maximiza-
tion:
x? = argmax
x?H
E
x??X
[K(x, x?)]. (3)
When K is a positive definite symmetric rational
kernel, it can often be rewritten as K(x, y) = (T ?
T?1)(x, y), where T is a weighted transducer over
the semiring (R+?{+?},+,?, 0, 1). Equation (3)
can then be rewritten as
x? = argmax
x?H
E
x??X
[(T ? T?1)(x, x?)] (4)
= argmax
x?H
?A(x) ? T ? T?1 ?X?, (5)
where we denote by A(x) an automaton accepting
(only) the string x and by ??? the sum of the weights
of all accepted paths of a transducer.
4 Algorithms
4.1 General method
Equation (5) could suggest computing A(x) ? T ?
T?1 ? X for each possible x ? H . Instead, we
can compute a composition based on an automa-
ton accepting all sequences in H , A(H). This leads
to a straightforward method for determining the se-
quence maximizing the expected similarity having
the following steps:
1. compute the composition X ? T , project on
the output and optimize (epsilon-remove, de-
terminize, minimize (Mohri, 2009)) and let Y2
be the result;1
2. compute the composition Y1 = A(H) ? T ;
3. compute Y1 ? Y2 and project on the input, let Z
be the result;2
4. determinize Z;
5. find the maximum weight path with the label of
that path giving x?.
While this method can be efficient in various scenar-
ios, in some instances the weighted determinization
yielding Z can be both space- and time-consuming,
even though the input is acyclic. The next two sec-
tions describe more efficient algorithms.
Note that in practice, for numerical stability, all
of these computations are done in the log semiring
which is isomorphic to (R+?{+?},+,?, 0, 1). In
particular, the maximum weight path in the last step
is then obtained by using a standard single-source
shortest-path algorithm.
4.2 Efficient method for n-gram kernels
A common family of rational kernels is the family
of n-gram kernels. These kernels are widely use as
a similarity measure in natural language processing
and computational biology applications, see (Leslie
et al, 2002; Lodhi et al, 2002) for instance.
The n-gram kernel Kn of order n is defined as
Kn(x, y) =
?
|z|=n
cx(z)cy(z), (6)
where cx(z) is the number of occurrences of z in
x. Kn is a positive definite symmetric rational ker-
nel since it corresponds to the weighted transducer
Tn ? T?1n where the transducer Tn is defined such
that Tn(x, z) = cx(z) for all x, z ? ?? with |z| = n.
1Equivalent to computing T?1 ? X and projecting on the
input.
2Z is then the projection on the input of A(H)?T ?T?1?X .
959
0a:?
b:?
1a:a
b:b
2a:a
b:b
a:?
b:?
0
1a/0.5
2
b/0.5
3b/1
4b/1
5a/1
6a/1
7a/0.4
8
b/0.6
b/1
9/1
b/1
a/1
(a) (b)
0
1a
2
b
3b
4b
5a
6a
7a
8
b
b
9
b
a
0
1a/1
2
b/1 3/1
a/0.2
b/1.5
a/1.8
b/0.5
(c) (d)
?
a/0
a/0
b/0
b/0
a/0.2
b/1.5
a/1.8
b/0.5
0
1a/0
2
b/0
3b/1.5
4b/0.5
5a/1.8
6a/1.8
7a/0.2
8
b/1.5
b/0.5
9/0
b/1.5
a/1.8
(e) (f)
Figure 2: Efficient method for bigram kernel: (a) Counting transducer T2 for ? = {a, b} (over the real semiring). (b)
Probabilistic automaton X (over the real semiring). (c) The hypothesis automaton A(H) (unweighted). (d) Automaton
Y2 representing the expected bigram counts in X (over the real semiring). (e) Automaton Y1: the context dependency
model derived from Y2 (over the tropical semiring). (f) The composition A(H) ? Y1 (over the tropical semiring).
The transducer T2 for ? = {a, b} is shown in Fig-
ure 2(a).
Taking advantage of the special structure of n-
gram kernels and of the fact that A(H) is an un-
weighted automaton, we can devise a new and sig-
nificantly more efficient method for computing x?
based on the following steps.
1. Compute the expected n-gram counts in X: We
compute the composition X ?T , project on out-
put and optimize (epsilon-remove, determinize,
minimize) and let Y2 be the result. Observe that
the weighted automaton Y2 is a compact repre-
sentation of the expected n-gram counts in X ,
i.e. for an n-gram w (i.e. |w| = n):
Y2(w) =
?
x???
X(x)cx(w)
= E
x?X
[cx(w)] = cX(w).
(7)
2. Construct a context-dependency model: We
compute the weighted automaton Y1 over the
tropical semiring as follow: the set of states is
Q = {w ? ??| |w| ? n and w occurs in X},
the initial state being ? and every state being fi-
nal; the set of transitions E contains all 4-tuple
(origin, label, weight, destination) of the form:
? (w, a, 0, wa) with wa ? Q and |w| ? n?
2 and
? (aw, b, Y2(awb), wb) with Y2(awb) 6= 0
and |w| = n? 2
where a, b ? ? and w ? ??. Observe that
w ? Q when wa ? Q and that aw,wb ? Q
when Y2(awb) 6= 0. Given a string x, we have
Y1(x) =
?
|w|=n
cX(w)cx(w). (8)
Observe that Y1 is a deterministic automaton,
hence Y1(x) can be computed in O(|x|) time.
3. Compute x?: We compute the composition
A(H) ? Y1. x? is then the label of the accepting
path with the largest weight in this transducer
and can be obtained by applying a shortest-path
algorithm to ?A(H) ? Y1 in the tropical semir-
ing.
The main computational advantage of this method
is that it avoids the determinization of Z in the
960
0 1a/1 2/1a/c1
b/c2
0 1a
2/c1
a
3/c2
b
0
b
1a
2/c1
a
3/c2
b
b
a
b
a
0
b/0
1a/0
2
a/0
3
b/0
2?/0b/0 a/0
?      /c1
3?/0?      /c2
b/0
a/0
(a) (b) (c) (d)
Figure 3: Illustration of the construction of Y1 in the unambiguous case. (a) Weighted automaton Y2 (over the real
semiring). (b) Deterministic tree automaton Y ?2 accepting {aa, ab} (over the tropical semiring). (c) Result of deter-
minization of ??Y ?2 (over the tropical semiring). (d) Weighted automaton Y1 (over the tropical semiring).
(+,?) semiring, which can sometimes be costly.
The method has also been shown empirically to be
significantly faster than the one described in the pre-
vious section.
The algorithm is illustrated in Figure 2. The al-
phabet is ? = {a, b} and the counting transducer
corresponding to the bigram kernel is given in Fig-
ure 2(a). The evidence probabilistic automaton X
is given in Figure 2(b) and we use as hypothesis
set the set of strings that were assigned a non-zero
probability by X; this set is represented by the deter-
ministic finite automaton A(H) given in Figure 2(c).
The result of step 1 of the algorithm is the weighted
automaton Y2 over the real semiring given in Fig-
ure 2(d). The result of step 2 is the weighted au-
tomaton Y1 over the tropical semiring is given in
Figure 2(e). Finally, the result of the composition
A(H) ? Y1 (step 3) is the weighted automaton over
the tropical semiring given in Figure 2(f). The re-
sult of the expected similarity maximization is the
string x? = ababa, which is obtained by applying
a shortest-path algorithm to ?A(H) ? Y1. Observe
that the string x with the largest probability in X is
x = bbaba and is hence different from x? = ababa in
this example.
4.3 Efficient method for the unambiguous case
The algorithm presented in the previous section for
n-gram kernels can be generalized to handle a wide
variety of rational kernels.
Let K be an arbitrary rational kernel defined by a
weighted transducer T . Let XT denote the regular
language of the strings output by T . We shall as-
sume that XT is a finite language, though the results
of this section generalize to the infinite case. Let
? denote a new alphabet defined by ? = {#x : x ?
XT } and consider the simple grammar G of context-
dependent batch rules:
? ? #x/x ?. (9)
Each such rule inserts the symbol #x immediately
after an occurrence x in the input string. For batch
context-dependent rules, the context of the applica-
tion for all rules is determined at once before their
application (Kaplan and Kay, 1994). Assume that
this grammar is unambiguous for a parallel applica-
tion of the rules. This condition means that there is
a unique way of parsing an input string using the
strings of XT . The assumption holds for n-gram
sequences, for example, since the rules applicable
are uniquely determined by the n-grams (making the
previous section a special case).
Given an acyclic weighted automaton Y2 over the
tropical semiring accepting a subset of XT , we can
construct a deterministic weighted automaton Y1 for
??L(Y2) when this grammar is unambiguous. The
weight assigned by Y1 to an input string is then the
sum of the weights of the substrings accepted by Y2.
This can be achieved using weighted determiniza-
tion.
This suggests a new method for generalizing Step
2 of the algorithm described in the previous section
as follows (see illustration in Figure 3):
(i) use Y2 to construct a deterministic weighted
tree Y ?2 defined on the tropical semiring ac-
cepting the same strings as Y2 with the same
weights, with the final weights equal to the to-
tal weight given by Y2 to the string ending at
that leaf;
(ii) let Y1 be the weighted automaton obtained by
first adding self-loops labeled with all elements
of ? at the initial state of Y ?2 and then deter-
minizing it, and then inserting new transitions
leaving final states as described in (Mohri and
Sproat, 1996).
961
Step (ii) consists of computing a deterministic
weighted automaton for ??Y ?2 . This step corre-
sponds to the Aho-Corasick construction (Aho and
Corasick, 1975) and can be done in time linear in
the size of Y ?2 .
This approach assumes that the grammar G of
batch context-dependent rules inferred by XT is un-
ambiguous. This can be tested by constructing the
finite automaton corresponding to all rules in G. The
grammar G is unambiguous iff the resulting automa-
ton is unambiguous (which can be tested using a
classical algorithm). An alternative and more ef-
ficient test consists of checking the presence of a
failure or default transition to a final state during
the Aho-Corasick construction, which occurs if and
only if there is ambiguity.
5 Application to Machine Translation
In machine translation, the BLEU score (Papineni et
al., 2001) is typically used as an evaluation metric.
In (Tromble et al, 2008), a Minimum Bayes-Risk
decoding approach for MT lattices was introduced.3
The loss function used in that approach was an ap-
proximation of the log-BLEU score by a linear func-
tion of n-gram matches and candidate length. This
loss function corresponds to the following similarity
measure:
KLB(x, x?) = ?0|x?|+
?
|w|?n
?|w|cx(w)1x?(w).
(10)
where 1x(w) is 1 if w occurs in x and 0 otherwise.
(Tromble et al, 2008) implements the MBR de-
coder using weighted automata operations. First,
the set of n-grams is extracted from the lat-
tice. Next, the posterior probability p(w|X) of
each n-gram is computed. Starting with the un-
weighted lattice A(H), the contribution of each n-
gram w to (10) is applied by iteratively compos-
ing with the weighted automaton corresponding to
w(w/(?|w|p(w|X))w)? where w = ?? \ (??w??).
Finally, the MBR hypothesis is extracted as the best
path in the automaton. The above steps are carried
out one n-gram at a time. For a moderately large lat-
tice, there can be several thousands of n-grams and
the procedure becomes expensive. This leads us to
investigate methods that do not require processing
the n-grams one at a time in order to achieve greater
efficiency.
3Related approaches were presented in (DeNero et al, 2009;
Kumar et al, 2009; Li et al, 2009).
0
1?:?
2
?:?
b:?
3
a:a
a:? b:b
a:?
b:?
Figure 4: Transducer T 1 over the real semiring for the
alphabet {a, b}.
The first idea is to approximate the KLB similar-
ity measure using a weighted sum of n-gram ker-
nels. This corresponds to approximating 1x?(w) by
cx?(w) in (10). This leads us to the following simi-
larity measure:
KNG(x, x?) = ?0|x?|+
?
|w|?n
?|w|cx(w)cx?(w)
= ?0|x?|+
?
1?i?n
?iKi(x, x?)
(11)
Intuitively, the larger the length of w the less likely
it is that cx(w) 6= 1x(w), which suggests comput-
ing the contribution to KLB(x, x?) of lower-order
n-grams (|w| ? k) exactly, but using the approxima-
tion by n-gram kernels for the higher-order n-grams
(|w| > k). This gives the following similarity mea-
sure:
KkNG(x, x?) = ?0|x?|+
?
1?|w|?k
?|w|cx(w)1x?(w)
+
?
k<|w|?n
?|w|cx(w)cx?(w)
(12)
Observe that K0NG = KNG and KnNG = KLB .
All these similarity measures can still be com-
puted using the framework described in Section 4.
Indeed, there exists a transducer Tn over the real
semiring such that Tn(x, z) = 1x(z) for all x ? ??
and z ? ?n. The transducer T 1 for ? = {a, b} is
given by Figure 4. Let us define the similarity mea-
sure Kn as:
Kn(x, x?) = (Tn?T?1n )(x, x?) =
?
|w|=n
cx(w)1x?(w).
(13)
Observe that the framework described in Section 4
can still be applied even though Kn is not symmet-
ric. The similarity measures KLB , KNG and KkNG
962
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
no mbr 38.7 39.2 38.3 33.5 26.5 64.0 51.8 57.3 45.5 43.8
exact 37.0 39.2 38.6 34.3 27.5 65.2 51.4 58.1 45.2 45.0
approx 39.0 39.9 38.6 34.4 27.4 65.2 52.5 58.1 46.2 45.0
ngram 36.6 39.1 38.1 34.4 27.7 64.3 50.1 56.7 44.1 42.8
ngram1 37.1 39.2 38.5 34.4 27.5 65.2 51.4 58.0 45.2 44.8
Table 1: BLEU score (%)
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
exact 3560 7863 5553 6313 5738 12341 23266 11152 11417 11405
approx 168 422 279 335 328 504 1296 528 619 808
ngram 28 72 34 70 43 85 368 105 63 66
ngram1 58 175 96 99 89 368 943 308 167 191
Table 2: MBR Time (in seconds)
can then be expressed as the relevant linear combi-
nation of Ki and Ki.
6 Experimental Results
Lattices were generated using a phrase-based MT
system similar to the alignment template system de-
scribed in (Och and Ney, 2004). Given a source sen-
tence, the system produces a word lattice A that is a
compact representation of a very large N -best list of
translation hypotheses for that source sentence and
their likelihoods. The lattice A is converted into a
lattice X that represents a probability distribution
(i.e. the posterior probability distribution given the
source sentence) following:
X(x) = exp(?A(x))?
y??? exp(?A(y))
(14)
where the scaling factor ? ? [0,?) flattens the dis-
tribution when ? < 1 and sharpens it when ? > 1.
We then applied the methods described in Section 5
to the lattice X using as hypothesis set H the un-
weighted lattice obtained from X .
The following parameters for the n-gram factors
were used:
?0 =
?1
T and ?n =
1
4Tprn?1 for n ? 1. (15)
Experiments were conducted on two language
pairs Arabic-English (aren) and Chinese-English
(zhen) and for a variety of datasets from the NIST
Open Machine Translation (OpenMT) Evaluation.4
The values of ?, p and r used for each pair are given
4http://www.nist.gov/speech/tests/mt
? p r
aren 0.2 0.85 0.72
zhen 0.1 0.80 0.62
Table 3: Parameters used for performing MBR.
in Table 3. We used the IBM implementation of the
BLEU score (Papineni et al, 2001).
We implemented the following methods using the
OpenFst library (Allauzen et al, 2007):
? exact: uses the similarity measure KLB based
on the linearized log-BLEU, implemented as
described in (Tromble et al, 2008);
? approx: uses the approximation to KLB from
(Kumar et al, 2009) and described in the ap-
pendix;
? ngram: uses the similarity measure KNG im-
plemented using the algorithm of Section 4.2;
? ngram1: uses the similarity measure K1NG
also implemented using the algorithm of Sec-
tion 4.2.
The results from Tables 1-2 show that ngram1
performs as well as exact on all datasets5 while be-
ing two orders of magnitude faster than exact and
overall more than 3 times faster than approx.
7 Conclusion
We showed that for broad families of transducers
T and thus rational kernels, the expected similar-
5We consider BLEU score differences of less than 0.4% not
significant (Koehn, 2004).
963
ity maximization problem can be solved efficiently.
This opens up the option of seeking the most appro-
priate rational kernel or transducer T for the spe-
cific task considered. In particular, the kernel K
used in our machine translation applications might
not be optimal. One may well imagine for exam-
ple that some n-grams should be further emphasized
and others de-emphasized in the definition of the
similarity. This can be easily accommodated in the
framework of rational kernels by modifying the tran-
sition weights of T . But, ideally, one would wish
to select those weights in an optimal fashion. As
mentioned earlier, we leave this question to future
work. However, we can offer a brief look at how
one could tackle this question. One method for de-
termining an optimal kernel for the expected sim-
ilarity maximization problem consists of solving a
problem similar to that of learning kernels in classi-
fication or regression. Let X1, . . . , Xm be m lattices
with Ref(X1), . . . ,Ref(Xm) the associated refer-
ences and let x?(K,Xi) be the solution of the ex-
pected similarity maximization for lattice Xi when
using kernel K. Then, the kernel learning optimiza-
tion problem can be formulated as follows:
min
K?K
1
m
m?
i=1
L(x?(K,Xi),Ref(Xi))
s. t. K = T ? T?1 ? Tr[K] ? C,
where K is a convex family of rational kernels and
Tr[K] denotes the trace of the kernel matrix. In
particular, we could choose K as a family of linear
combinations of base rational kernels. Techniques
and ideas similar to those discussed by Cortes et al
(2008) for learning sequence kernels could be di-
rectly relevant to this problem.
A Appendix
We describe here the approximation of the KLB
similarity measure from Kumar et al (2009). We
assume in this section that the lattice X is determin-
istic in order to simplify the notations. The posterior
probability of n-gram w in the lattice X can be for-
mulated as:
p(w|X) =
?
x???
1x(w)P (x|s) =
?
x???
1x(w)X(x)
(16)
where s denotes the source sentence. When using
the similarity measure KLB defined Equation (10),
Equation (3) can then be reformulated as:
x? = argmax
x??H
?0|x?|+
?
w
?|w|cx?(w)p(w|X). (17)
The key idea behind this new approximation algo-
rithm is to rewrite the n-gram posterior probability
(Equation 16) as follows:
p(w|X) =
?
x???
?
e?EX
f(e, w, ?x)X(x) (18)
where EX is the set of transitions of X , ?x is
the unique accepting path labeled by x in X and
f(e, w, ?) is a score assigned to transition e on path
? containing n-gram w:
f(e, w, ?) =
?
?
?
1 if w ? e, p(e|X) > p(e?|X),
and e? precedes e on ?
0 otherwise.
(19)
In other words, for each path ?, we count the tran-
sition that contributes n-gram w and has the highest
transition posterior probability relative to its prede-
cessors on the path ?; there is exactly one such tran-
sition on each lattice path ?.
We note that f(e, w, ?) relies on the full path ?
which means that it cannot be computed based on
local statistics. We therefore approximate the quan-
tity f(e, w, ?) with f?(e, w,X) that counts the tran-
sition e with n-gram w that has the highest arc poste-
rior probability relative to predecessors in the entire
lattice X . f?(e, w,X) can be computed locally, and
the n-gram posterior probability based on f? can be
determined as follows:
p(w|G) =
?
x???
?
e?EX
f?(e, w,X)X(x)
=
?
e?Ex
1w?ef?(e, w,X)
?
x???
1pix(e)X(x)
=
?
e?EX
1w?ef?(e, w,X)P (e|X),
(20)
where P (e|X) is the posterior probability of a lat-
tice transition e ? EX . The algorithm to perform
Lattice MBR is given in Algorithm 1. For each state
t in the lattice, we maintain a quantity Score(w, t)
for each n-gram w that lies on a path from the initial
state to t. Score(w, t) is the highest posterior prob-
ability among all transitions on the paths that termi-
nate on t and contain n-gram w. The forward pass
requires computing the n-grams introduced by each
transition; to do this, we propagate n-grams (up to
maximum order ?1) terminating on each state.
964
Algorithm 1 MBR Decoding on Lattices
1: Sort the lattice states topologically.
2: Compute backward probabilities of each state.
3: Compute posterior prob. of each n-gram:
4: for each transition e do
5: Compute transition posterior probability P (e|X).
6: Compute n-gram posterior probs. P (w|X):
7: for each n-gram w introduced by e do
8: Propagate n? 1 gram suffix to he.
9: if p(e|X) > Score(w, T (e)) then
10: Update posterior probs. and scores:
p(w|X) += p(e|X) ? Score(w, T (e)).
Score(w, he) = p(e|X).
11: else
12: Score(w, he) = Score(w, T (e)).
13: end if
14: end for
15: end for
16: Assign scores to transitions (given by Equation 17).
17: Find best path in the lattice (Equation 17).
References
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
String Matching: An Aid to Bibliographic Search.
Communications of the ACM, 18(6):333?340.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In CIAA 2007, volume 4783 of LNCS, pages
11?23. Springer. http://www.openfst.org.
Christian Berg, Jens Peter Reus Christensen, and Paul
Ressel. 1984. Harmonic Analysis on Semigroups.
Springer-Verlag: Berlin-New York.
Peter J. Bickel and Kjell A. Doksum. 2001. Mathemati-
cal Statistics, vol. I. Prentice Hall.
Corinna Cortes, Patrick Haffner, and Mehryar Mohri.
2004. Rational Kernels: Theory and Algorithms.
Journal of Machine Learning Research, 5:1035?1062.
Corinna Cortes, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Learning sequence kernels. In Pro-
ceedings of MLSP 2008, October.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of ACL and IJCNLP, pages 567?575.
Vaibhava Goel and William J. Byrne. 2000. Minimum
Bayes-risk automatic speech recognition. Computer
Speech and Language, 14(2):115?135.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3).
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In EMNLP,
Barcelona, Spain.
Shankar Kumar and William J. Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL, Boston, MA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Christina S. Leslie, Eleazar Eskin, and William Stafford
Noble. 2002. The Spectrum Kernel: A String Kernel
for SVM Protein Classification. In Pacific Symposium
on Biocomputing, pages 566?575.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of ACL and IJCNLP, pages 593?
601.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watskins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?44.
Mehryar Mohri and Richard Sproat. 1996. An Efficient
Compiler for Weighted Rewrite Rules. In Proceedings
of ACL ?96, Santa Cruz, California.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213?254. Springer.
Franz J. Och and Hermann Ney. 2004. The align-
ment template approach to statistical mchine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division.
Arto Salomaa and Matti Soittola. 1978. Automata-
Theoretic Aspects of Formal Power Series. Springer.
Roy W. Tromble, Shankar Kumar, Franz J. Och, and
Wolfgang Macherey. 2008. Lattice minimum Bayes-
risk decoding for statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 620?
629.
965
