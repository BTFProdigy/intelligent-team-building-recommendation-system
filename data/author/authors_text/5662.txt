Corpus-dependent Association Thesauri for Information Retrieval 
Hiroyuki Kaji "~, Yasutsugu Morimoto "~, Toshiko Aizono "l, and Noriyuki Yamasaki "2 
"~ Central Research Laboratory, Hitachi, Ltd. ,2 Software Division, Hitachi, Ltd. 
1-280 Higashi-Koigakubo, Kokubunji-shi 549-6 Shinano-cho, Totsuka-kt,, Yokohanla-shi 
Tokyo 185-8601, Japan Kanagawa 244-0801, Japan 
{kaji, morimoto, aizono}@crl.hitachi.co.jp, yamasa n@soft.hitachi.co.jp 
Abstract 
This paper presents a method for automati- 
cally generating an association thesaurus 
from a text corpus, and demonstrates it ap- 
plication to information retrieval. The the- 
saurus generation method .consists of ex- 
tracting tenns and co-occurrence data from a 
corpus and analyzing the correlation between 
terms statistically. A new method for dis- 
ambiguating the structure of compound 
nouns, which is a key component for term 
extraction, is also proposed. The automati- 
cally generated thesaurus i  effectively used 
as a tool for exploring infonnation. A the- 
saurus navigator having novel functions uch 
as term clustering, thesaurus overview, and 
zooming-in is proposed. 
1 Introduction 
A thesaurus plays essential roles in information 
retrieval systems. In particular, a domain- 
specific thesaurus greatly improves the effective- 
ness of information retrieval. However, we are 
confronted with the difficult problem of how to 
construct and maintain a domain-specific thesau- 
rus. The goal of our present research is to es- 
tablish a method for autolnatically generating a 
thesaurus from a text corpus of a domain and 
demonstrate its application to information re- 
trieval. 
Thesauri are classified 
into taxonomy-type thesauri 
and association thesauri. 
There has been various 
research on the extraction of 
taxonomic information |'io111 
a corpus, including extrac- 
tion of hyponyms by using 
linguistic patterns (Hearst 
1992) and extraction of synonyms based on the 
similarity of sets of co-occurring words (Ruge 
1991; Grefenstette 1992). However, the perfor- 
mance of these methods is limited, and they 
should be considered as aids to augment hand- 
made thesauri. In contrast, an association the- 
saurus, that is a collection of pairs of semanti- 
cally associated terms, can be possibly generated 
from a corpus entirely automatically. Word 
association orms based on co-occurrence infor- 
mation have been proposed by (Church and 
Hanks 1990). Here we focus on the automatic 
generation of an association thesanrus. 
Association thesauri are as useful as taxon? 
omy-type thesauri in information retrieval. The 
improvement of retrieval effectiveness by using 
an association thesaurus has been reported by a 
number of papers (Jing and Croft 1994; Schutze 
and Pedersen 1994). We propose to use a coro 
pus-dependent association thesaurus interac- 
tively. 
2 Automat ic  Generat ion of an Associa- 
tion Thesaurus  f rom a Corpus 
2.1 Outline of the thesaurus generation 
method 
The proposed thcsaurus generation method con- 
sists of term extraction, co-occurrence data ex- 
traction, and correlation analysis, as shown in Fig. 
1. 
~? Term F 
U Co-occ.rrence 
\[ data extraction r l pmrs and their 
223___ 
C o r r e l a t ! m ~  
Fig. 1. Automatic generation of a thesaurus from a corpus. 
404 
2.1.1 Ternt extraction 
A thesaurus hould consist of terms, each repre- 
senting a domain-specific concept. Most of tile 
terms representing important concepts are nol, lllS, 
simple or compound, that frequently occur in the 
corpus. ThereR)re, we extract both simple 
nonns and compound nouns whose occurrence 
frequencies exceed a predetermined threshold. 
We also use a list of stop words since frequently 
occurring nouns are not always terms. 
Compound nouns are identified by a pattern 
matching method using a part-of-speech sequen- 
ce pattern. Naturally, the pattern is language 
specific. The following is a pattern for Japanese 
compound nouns: 
COMP NOUN := { PREFIX } NOUN + 
SUFFIX } { PREFIX } NOUN + 
A problem in extracting compound nouns is 
that a word sequence matched to the above pat- 
tern, which actually defines just the type of noun 
phrase, is not always a term. We filter out some 
kind of non-term noun phrases by using a list of 
stop words for the first and last elements of com- 
pound nouns. Stop words for the first element 
of compound nouns include referential nouns (e.g. 
jouki (above-mentioned)) and determiner nouns 
(e.g. kaku (each)). Stop words for the last ele- 
ment of compound nouns include time/place 
nouns (e.g. nai (inside)) and relational nouns (e.g. 
koyuu (peculiar)). 
Another importmat problem we are confronted 
with in term extraction is the structural ambiguity 
of compound nouns. For our purpose, we need 
to extract non-maximal compound nouns as well 
sts lnaxin la l  comp()und nouns. Here a non-  
maximal compound noun means one that occurs 
as a part o f  a larger con lpound noun, and a 
n lax imal  compound nonn means  one thstt occurs  
not as a part of a larger compound noun. We 
must disambiguate tile structure of compound 
nouns to correctly extract non-maximal com- 
pound nouns. We have developed a statistical 
disambiguation method, the detail and evaluation 
of which are described in 2.2. 
2.1.2 Co-occurrence data evtraction 
Our purpose is to collect pairs of semantically or 
contextually associated terms, no matter what 
kind of association. So we extract co- 
occurrence in a window. That is, every pair of 
terms occurring together within a window is 
extracted as the window is moved through a text. 
The window size can be specified rather arbitrar- 
ily. Considering our purpose, the window 
should accommodate a few sentences. At tile 
same time, the window size should not be too 
large from the viewpoint of computational load. 
Therefore, 20 to 50 words, excluding function 
words,  seems to be an appropriate value. 
Note that we filter out any pair of words co- 
occurring within a compound noun. If such 
pairs were included in co-occurrence data, they 
would show high correlation. However, they 
would be redundant because compound nouns are 
treated as entities in our thesaurus. 
2.1.3 Correlation analysis 
As a correlation measure between terms, we use 
mutual information (Church and Hanks 1990). 
The mutual inlbrmation between terms t~ and t i is 
defined by the following formula: 
g(ti, tj)/~' g(t,, t,) 
Ml(ti, tj) = log_, /i.i , 
{t'(t0/ i ~ f(t0}' { f(tj)//j~ f(ti, 
where f(t~) is the occurrence frequency of term t~, 
and g(ti,ti ) is the co-occurrence frequency of 
terms t~ and tj. A rnaxinmm nunrber of associat-. 
ed terms for each term is predetermined as well 
as a threshold for tile mutual information, and 
associated terms are selected based on the de-o 
scending order of mutual information. 
Mutual infornaation involves a problem in 
that it is overestimated for low-frequency terms 
(I)unning 1993). Therefore, we determine 
whether two terms are related to each other by a 
log-likelihood ratio test, and we filter out pairs of 
terms that do not pass the test. 
2.2 Disambiguation of compound noun 
structure 
2.2.1 Disantbiguation based on coitus statistics 
Our disanabiguation method is described below 
for tile case of a compound noun consisting of 
three elelnents. A compound noun W~W2W 3 
has two possible structures: WI(W~W3) and 
(W~W,)W 3. We deternfine its structure based 
on the occurrence t}equencies of maxilnal com- 
pound nouns as follows: If tile maximal com- 
pound noun W~W3 occurs more frequently than 
tile inaxinml compound noun W~W,, then tile 
405 
Table 1 
(a) Examples 
Global-statistics-based disambiguation vs. local-statistics-based disambiguation. 
Maximal compound noun 
Deta shori shisutemu 
(data processing system) 
Deta tsuskin seiL:vo souchi 
(Data communication 
controller) 
Kaisen seigyo purosessa 
(Line control processor) 
F req .  Global-statistics-based 
Structure Freq. 
478 (Deta shori) shisutemu 478 
94 Deta (tsuskin seigvo 94 
souchi) 
54 Kaisen (seign'o 54 
purosessa) 
Local-statistics-based 
Structure Freq. 
(Deta shori) shisutemu 368 
Deta (skori shisutemu) 110 
(Deta tsushin) seigg'o souchi 40 
Deta (tsuskin seig:vo souchi) 54 
(Kaisen seign.,o) purosessa 54 
(b) Summary of disambiguation results 
Global-statistics-based 
Correct structure 12,565 words (62.0%) 
Incorrect structure 7,688 words (38.0%) 
Total 20,253 words (100%) 
(Note: Numbers of words are occurrence-based.) 
structure Wl(W2W3) is preferred. On the con? 
trary, if the maximal COlnpound noun W~W2 
occurs more frequently than the maximal com- 
pound noun W2W3, then the structure (W~W2) W3 
is preferred. 
The generalized isambiguation rnle is as 
follows: If a compound noun CN includes two 
compound noun candidates CN~ and CN2, which 
are incompatible with each other, and the maxi- 
mal compound noun CN~ occurs more frequently 
than the maximal compound noun CN> then a 
structure of CN including CN~ is preferred to a 
structure of CN including CN,. 
We have two alternatives regarding the range 
where we count occurrence frequencies of maxi- 
mal compound nouns. One is global-statistics 
which means that frequencies are counted in the 
whole corpus and they are used to disambiguate 
all compound nouns in the corpus. The other is 
local-statistics which means that frequencies are 
counted in each document in the corpus and they 
are used to disambiguate compound nouns in the 
corresponding document. 
2.2.2 Evaluation" Global-statistics vs. local- 
statistics 
We evaluated both the global-statistics-based 
disambiguation and the local-statistics-based 
disambiguation by using a 23.7-M Byte corpus 
consisting of 800 patent documents. Table l(a) 
shows comparative xamples of these methods. 
Evah, ation results for the 200 highest-frequency 
maximal COlnpound nouns consisting of three or 
Local-statistics-based 
more  words are summa ~ 
rized in Table l(b). 
14,921 words (73.7%) They prove that the 
local-statistics-based 5,332 words (26.3%) 
disambiguation method 
20,253 words (100%) is superior to the global- 
statistics-based disam- 
biguation method. 
Note that in the local-statistics-based disam- 
biguation method, we resorted to the global- 
statistics when local-statistics were not available. 
The percentage of cases the local-statistics were 
not available was 25.1 percent. 
(Kobayasi et al 1994) proposed a disam- 
biguation method using collocation information 
and semantic categories, and reported that the 
structure of compound nouns was disambiguated 
at 83% accuracy. Note that their accuracy was 
calculated for compound nouns including unam- 
biguous compound nouns, i.e. those consisting of 
only two words. If it were calculated for com-. 
pound nouns consisting three or more words, it 
would be less than that of our method. Thus, we 
can conclude that our local-statistics-based 
method compares quite well with rather sophisti- 
cated previous methods. 
2.3 Prototype and an experiment 
We implemented a prototype thesaurus generator 
in which the local-statistics-based method was 
used to disambiguate the structure of compound 
nouns. Using this thesaurus generator, we got a 
thesaurus consisting of 38,995 terms froln a 61- 
M Byte corpus consisting of almost 48,000 arti- 
cles in the financial pages of a Japanese newspa- 
per. In this experiment, the threshold for occur- 
fence frequencies of terms in the term extraction 
step was set to 10, and the window size in the co- 
occurrence data extraction step was set to 25. 
406 
The abow+" rtm took 5.4 hours on a HP9000 
C200 workstation. The tlarouglaput is tolerable 
from a practical point of view. We should also 
note that a thesaurus can be updated as efficiently 
as it can be initially generated. Because \ve can 
run the first two steps (extraction of terms and 
extraction of co-occurrence data) in accumulative 
fashion, and we only need to run the third step 
over again when a considerable amount of terms 
and co-occurrence data are accunmlated. 
3 Navigation in an Association Thesau-  
FUS 
3.1 Purpose and outline of the proposed 
thesaurus navigator 
A big problem with toclafs information re- 
trieval systems based on search techniques i that 
they require users, who may not know exactly 
what thcy arc looking for, to explicitly describe 
their information needs. Another problem is 
that mismatched vocabularies between users and 
the corpus would bring poor retrieval results. 
To solve these problems, we propose a corptts-~ 
dependent association-thesaurus navigator en- 
abling users to efficiently explore information 
through a corpus. 
Users' requirements are summarized as fob 
lows: 
They want to grasp the overall information 
structure of a domain.  
They want to know what topics or sub?. 
domains arc contained in the corpus. 
- They want to know terms that appropriately 
describe their w~gue information eeds. 
To meet the above requirements, our pro,- 
posed thesaurus navigator has novel functions 
such as clustering of related terms, generation of 
a thesaurus overview, and zoom-in on a sub-- 
domain of interest. A conceptual image of 
thesaurus navigation using these ftmctions is 
shown in Fig. 2. A typical informatio,> 
exploration session proceeds as follows. 
At the beginning, the system displays an 
overview of a corpus-dependent thesaurus o that 
users can easily enter the information space of 
the corpus. The overview is a kind of summary 
of the corpus, it consists of clusters of generic 
terms of the domain, and makes it easy to under- 
stand what topics or sub-domains are contained 
in the corpus. Looking at the thesaurus over- 
Thesaurtts 
overview 
' . .  . . . .  
O Generic term 
Zoom-in Zoom-in 
. . . .  
// 
? Specific term 
Fig. 2. Conceptual image of thesaurus 
navigation. 
view, the users can select one or a few term 
clusters they have interest in, and the screen will 
zoom in on the cluster(s). The zoomed view 
consists of a nulnber of clusters, each including 
more specific terms than those in the overview. 
Users can repeat his zoom-in operation until they 
reach term clusters representing sufficiently 
specific topics. 
3.2 Functions of the thesaurus navigator 
3.2.1 Clustering of  related terms 
We made a preliminary experiment o evaluate 
standard agglomerative clustering algorithms 
including the single-linkage method, the con> 
pletedinkage method, and the group-average- 
linkage method (Eldqamdouchi and Willett 1989)~ 
Among them, the group--average-linkage m thod 
resulted in the best results, ttowever, several 
potential clusters tended to merge into a large one 
when we repeated the merge operation until a 
predetermined number of clusters were obtained. 
Accordingly, we use the group-average-linkage 
method with an upper limit on the size of a clus- 
ter. 
3.2.2 Generation era thesaurus overview 
Our method tot generating a thesaurus overview 
consists of major-term extraction and term clus- 
tering. The m~0or-term extracting algorithm, 
which is carried out beforehand in batch mode, is 
described below. See 3.2.1 for the term clus- 
tering algorithnl. 
An overview of the thesaurus hould consist 
of generic terms included in the corpus, flow- 
ever, we do not have a definite criterion for get> 
eric terms. So we collect m~oor terms from the 
corpus as follows. The number of m~!jor terms, 
407 
denoted by M below, was set to 300 in the pro- 
totype. 
i) Determine a characteristic term set for each 
doculnent. 
Calculate the weight w~j of term tj for 
document 4 according to the tf-idf (term fre- 
quency - inverse document frequency) for- 
mula. Then select the first re(i) terms in the 
descending order of u, u for each document d,, 
where re(i), the number of characteristic terms 
for document 4, is set to 20% of the total 
number of distinct erms in 4. It is also lim- 
ited to between 5 and 50. 
ii) Select major terms in the corpus. 
Select the first M terms in the descending 
order of the frequency of being contained in 
the characteristic term sets? 
3.2.3 Zoom-in on a term cluster of  interest 
Our method for zooming in on a term cluster 
consists of term-set expansion and term cluster~ 
ing. The term-set expanding algorithm is de~ 
scribed below. See 3.2.1 for the term clustering 
algorithm. 
A user-specified term set To = {t~, 6. . . . .  t,,,} is 
expanded into a term set T,. consisting of M terms 
as follows. M was set to 300 in the prototype. 
i) Set the initial value of 7",. to 7",,. 
ii) Whi le  IT,.I< M for i = 1, 2 . . . .  do; 
While IE, I < Mfor j  = 1,2, ..., m do; 
Add the tenn having the i-th highest 
correlation with tj to T,,; 
end; 
end; 
The reason why the above-described proce- 
dure implements the zoom-in is that generic 
terms tend to have higher correlation with semi- 
generic terms than with specific terms. As- 
suming that high-frequency terms are generic and 
low-frequency terms are specific, we examined 
the distribution of terms by the distance from the 
major terms and the average occurrence frequen- 
cy of terms for each distance. Here the distance 
is the length of the shortest path in a graph that is 
obtained by connecting every pair of associated 
terms with an edge. Table 2 shows the results 
for the example thesaurus mentioned in 2.3. 
According to it, the average occurrence frequen- 
cy decreases with the distance from the major 
terms. Therefore, starting from an overview, 
our method is likely to produce more and more 
specific views. 
3.3 Prototype and an exper iment 
We developed a prototype as a client/server sys- 
tem. The thesaurus navigator is available on 
WWW browsers. It also has an interface to 
text-retrieval engines, through which a term 
cluster is transferred as a query. 
Test use was made with the example thesau- 
rus mentioned in 2.3. The response time for the 
zoom-in operation during the navigation sessions 
was about 8 seconds. This is acceptable given 
the rich information provided by the clustered 
view. Note that the response time is almost 
independent of the size of the thesaurus or corpus, 
because the number of temls to be clustered is 
always constant, as described in 3.2.2 and 3.2.3~ 
An example from navigation sessions is 
shown in Fig. 3. It demonstrates the useflflness 
of the corpus-dependent thesaurus navigation as a 
front-end for text retrieval. The effectiveness of
our thesaurus navigator is summarized as folo 
lows. 
- Improved accessibility to text retrieval systems: 
Users are not necessarily required to input 
terms to describe their information need. 
They need only select from among terms pre~ 
sented on the screen. This makes text re-. 
trieval systems accessible ven for those hav- 
ing vague information eeds, or those unfa- 
miliar with the domain. 
- Improved navigation efficiency: 
The unit of users' cognition is a topic rather 
than a term. That is, they can recognize a 
topic from a cluster of terms at a glance. 
Therefi)re, they can efficiently navigate 
through an information space. 
Table 2 Distribution of terms by distance from major terms. 
Distance flom major terms 0 1 2 
Number of terms 245 4278 23832 
Average occurrence frequency 2642.1 318.6 61.9 
3 4 5 
10408 149 l 82 
10.6 7.7 9.0 - 
408 
I :~#-k ,  99 Y,# -" 
'" " '=' / I \ [ 'C~? 
:::Z ::::::::::::::::::::::::::: : /  ' : " : '  
(a) Thesaurus overview 
1"~ 9~_/.x 9D 27,9 -" 
2'~I~\]J!c?x 72-'_ fAY \ ] "Z : -E .~ : iLtF'kM?Z 
J?,ff ,'ib'.. 9,ft.) v'~t/t!. 7,5_I 
r a~,:?: :,5 ,&5;: i!17.ii8 :~!~. I~f i  ~< ~qT. >g .'JJ/g i~;? E '~ 
3:i~ / ' l ' . , ' i tR ;C,'i:: i" "_Z tA :  
ki~':\[ ' :  ~LL.rJ.=\] 2>EU i::,.;~:'\]:::~: ;?~-:,':::,: i~L i~.  .+ <?.. ~ /: .:. r- ~;" +. 
't :f" Z ?0  ~t ? 
?~le'~ ' ' ~ I ' *~a ,-m/I':" ~ "~I o . . . . . . . . . . . . . . .  : ' " "~7:  . . . . . . . . . . . .  . . . . . . . . . . . .  
(b) Zoom-in 
F ~-)Ea?':~A rivaL752 ?~ o E o ~  ~ aLW.eE 
tlffa ~ Y_22_rD j tm~~ 
f~ 7ai~.NL~ ;r_u'2-?.,- in 97  
,_J 
(c) Further zoom-in 
An overview of the thesaurus was di.sTJko,ed. 
Then the user selected the.fi/ih and seventh 
cluste#w hich he was interested in: {China, 
col!\[L'rence, Asia, cooperation, meeting, 
Vietnam, region, development, technology, 
environment}, and/economy export, aid, 
toward, summit, Soviet Union, debt, Russia, 
reconstruction}. This means that the user 
was interested in "development assistance 
to developing countries or areas ". 
The Jil?h aml seventh cluste~w J)'om (a) were 
shown close up, and clustelw indicating 
more .Vwc~/ic domains were presented. 
The user couM undelwtand which topics the 
respective clustelw suggested: "Economic 
assis'tance for the development ofthe Asia- 
Pat(lie region ", "Global environmental 
problems ", "bTternational debt problems ", 
"'Mattetw on China ", "Energy resource 
development", and so on. Since he wax 
e.vwcially interested in "International debt 
problems ", he selected the third cluster 
{debt, Egypt, Paris Club, creditor nation, 
q\[licial debt, de/'erred, Poland, .fin'eign debt, 
reimbursement, Paris, club, pro,merit, 
.fi, reign}. 
The third cluster fi'om (I 0 was shown close 
up. The resulting screen gave the user a 
choice el'many sT~ecific terms relevant o 
"htternational debt problems ", although 
not all oJ'the clustel:v indicated spec!/ic 
topics. The user was able to retrieve 
documents by simply selectin? terms o/ 
interest./i'om those displayed on the screen. 
Fig. 3. Example of thesaurus navigation. 
409 
4 Comparison with related work 
Let us make a briefcolnparison with related work. 
Both scatter/gather document clustering (Cutting 
et al 1992; Hearst and Pedersen 1996) and Ko- 
honen's self-organizing map (Lin et al 1991; 
Lagus et al 1996; Kohonen 1998) enable explo- 
ration through a corpus. While they treat a 
corpus as a collection of documents, we treat it as 
a collection of terms. Therefore our method can 
elicit finer information structure than these previ- 
ous methods, and moreover, it can be applied to a 
corpus that includes multi-topic doculnents. 
Our method compares quite well with the previ- 
otis methods for throughput and response time. 
5 Conclusion 
We demonstrated the feasibility of automatic 
generation of an association thesaurus from a 
corpus. The proposed thesaurus generation 
method consists of extracting terms and co- 
occurrence data from a corpus and analyzing the 
correlation between terms statistically. As a 
component technology for thesaurus generation, 
a method for disambiguating the structure of 
compound nouns based on corpus statistics was 
developed and evaluated. 
We also demonstrated the information re- 
trieval application of an automatically generated 
association thesaurus. A thesaurus navigator 
having novel functions such as term clustering, 
thesaurus overview, and zooming-in was devel- 
oped. An experiment with an association the- 
saurus generated from a newspaper article corpus 
proved that the thesaurus navigator allows us to 
efficiently explore information through a text 
corpus even when our information needs are 
vague. 
Acknowledgements: This research was st, p- 
ported in part by the Next-generation Digital 
Library System R&D Project of MITI (Ministry 
of International Trade and Industry), IPA (Inlbr- 
mation-technology Promotion Agency), and 
JIPDEC (Japan Information Processing Devel- 
opment Center). We thank Mainichi Newspa- 
pers, Ltd. for permitting us to use the CD-ROMs 
of the '91, '92, '93, '94 and '95 Mainichi Shim- 
bun tbr the experiment. 
References 
Church, K. W., and P. ttanks. 1990. Word association 
norms, mutual information, and lexicography. Con> 
putational Linguistics, 16( 1 ): 22-29. 
Cutting, D. R., D. R. Karger, J. O. Pedersen, and J. W. 
Tukey. 1992. Scatter/gather: A cluster-based ap- 
proach to browsing large doctnnent collections. Proc. 
ACM SIG1R '92, pp. 318-329. 
Dunning, T. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Lin- 
guistics, 19( 1 ): 61-74. 
El-Hamdouchi, A., and P. Willett. 1989. Comparison 
of hierarchical agglomerative clustering methods for 
document retrieval. The Computer Journal, 32(3): 
220-227. 
Grefenstette, G. 1992. Use of syntactic context to 
produce term association lists for text retrieval. Proc. 
ACM SIGIR '92, pp. 89-97. 
Hearst, M. A. 1992. Automatic acquisition ofhypo- 
nyms from large text corpora. Proc. COLING '92, 
pp. 539-545. 
Hearst, M. A., and J. O. Pedersen. 1996. Reexamining 
the cluster hypothesis: Scatter/gather on retrieval 
results. Proc. ACM S1GIR '96, pp. 76-84. 
Jing, Y., and W. B. Croft. 1994.. An association the-. 
saurus for information retrieval. Proc. R1AO '94, 
Cont. on Intelligent Text and linage Handling, pp. 
146-160. 
Kobayasi, Y., T. Tokunaga, and tf. Tanaka. 1994. 
Analysis of Japanese compound nouns using colloo 
cational information. Proc. COLING '94, pp. 865- 
869. 
Kohonen, T. 1998. Self-organization of very large 
document collections: State of the art. Proc. 8th lnt'l 
Cone on Artificial Neural Networks, vol. 1, pp. 65- 
74. 
Lagus, K., T. Honkela, S. Kaski, and T. Kohonen. 
1996. Self organizing maps of document collections: 
a new approach to interactive xploration. Proc. 2nd 
lnt'l Cone on Knowledge Discovery and Data Min- 
ing, pp. 238-243. 
Lin, X., D. Soergel, and G. Marchionini. 1991. A self- 
organizing semantic map for information retrieval. 
Proc. ACM SIGIR '91, pp. 262-269. 
Ruge, G. 1991. Experiments on linguistically based 
term associations. Proc. RIAO '91, Conf. on Intelli- 
gent Text and hnage Handling, pp. 528-545. 
Schutze, tt., and J. O. Pedersen. 1994. A cooccur- 
rence-based thesaurus and two applications to in- 
formation retriewd. Proc. RIAO '94, Conf. on Intel- 
ligent Text and hnage ttandling, pp. 266-274. 
410 
Unsupervised Word Sense Disambiguation
Using Bilingual Comparable Corpora
Hiroyuki Kaji and Yasutsugu Morimoto
Central Research Laboratory, Hitachi, Ltd.
1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japan
{kaji, y-morimo}@crl.hitachi.co.jp
Abstract
An unsupervised method for word sense disam-
biguation using a bilingual comparable corpus was
developed.  First, it extracts statistically significant
pairs of related words from the corpus of each lan-
guage.  Then, aligning pairs of related words
translingually, it calculates the correlation between
the senses of a first-language polysemous word and
the words related to the polysemous word, which
can be regarded as clues for determining the most
suitable sense.  Finally, for each instance of the
polysemous word, it selects the sense that maxi-
mizes the score, i.e., the sum of the correlations
between each sense and the clues appearing in the
context of the instance.  To overcome both the
problem of ambiguity in the translingual alignment
of pairs of related words and that of disparity of
topical coverage between corpora of different lan-
guages, an algorithm for calculating the correlation
between senses and clues iteratively was devised.
An experiment using Wall Street Journal and Ni-
hon Keizai Shimbun corpora showed that the new
method has promising performance; namely, the
applicability and precision of its sense selection are
88.5% and 77.7%, respectively, averaged over 60
test polysemous words.
1 Introduction
Word sense disambiguation (WSD) is an ?intermedi-
ate? task that is necessary for accomplishing most
natural language processing tasks, especially machine
translation and information retrieval.  A variety of
WSD methods have been proposed over the last dec-
ade; however, such methods are still immature.  In
response to this situation, we have developed an unsu-
pervised WSD method using bilingual comparable
corpora.
With the growing amount of texts available in elec-
tronic form, data-driven or corpus-based WSD has
become popular.  The knowledge useful for WSD can
be learned from corpora.  However, supervised
learning methods suffer from the high cost of manually
tagging the sense onto each instance of a polysemous
word in a training corpus.  A number of bootstrapping
methods have been proposed to reduce the sense-
tagging cost (Hearst 1991; Basili 1997).  A variety of
unsupervised WSD methods, which use a machine-
readable dictionary or thesaurus in addition to a corpus,
have also been proposed (Yarowsky 1992; Yarowsky
1995; Karov and Edelman 1998).  Bilingual parallel
corpora, in which the senses of words in the text of one
language are indicated by their counterparts in the text
of another language, have also been used in order to
avoid manually sense-tagging training data (Brown, et
al. 1991).
Unlike the previous methods using bilingual cor-
pora, our method does not require parallel corpora.
The availability of large parallel corpora is extremely
limited.  In contrast, comparable corpora are available
in many domains.  The comparability required by our
method is very weak: any combination of corpora of
different languages in the same domain is acceptable as
a comparable corpus.
Several types of information are useful for WSD
(Ide and Veronis 1998).  Three major types are the
grammatical characteristics of the polysemous word to
be disambiguated, words that are syntactically related
to the polysemous word, and words that are topically
related to the polysemous word.  Among these types,
use of grammatical characteristics, which are language-
dependent, is not compatible with the approach using
bilingual corpora.  On the other hand, since a topical
relation is language-independent, use of topically relat-
ed words is most compatible with the approach using
bilingual corpora.  Accordingly, we focused on using
topically related words as clues for determining the
most suitable sense of a polysemous word.
2 Approach
2.1 Framework
A comparable corpus consists of a first-language cor-
pus and a second-language corpus of the same domain.
Unlike a parallel corpus, we cannot align sentences or
instances of words translingually.  Therefore, we ex-
tract a collection of statistically significant pairs of
related words from each language corpus indepen-
dently of the other language, and then align the pairs of
related words translingually with the assistance of a
bilingual dictionary.  The underlying assumption is
that translations of words that are related in one lan-
guage are also related in the other language (Rapp
1995).
Translingual alignment of pairs of related words
enables us to acquire knowledge useful for WSD (i.e.,
sense-clue pair).  For example, the alignment of (tank,
gasoline) with (???<TANKU>, ????<GASORIN>)
implies that ?gasoline? is a clue for selecting the ?con-
tainer? sense of ?tank?, which is translated as ????
<TANKU>?, and the alignment of (tank, soldier) with (?
?<SENSYA>, ??<HEISI>) implies that ?soldier? is a
clue for selecting the ?military vehicle? sense of ?tank?,
which is translated as ???<SENSYA>?.
Figure 1 shows an overview of our proposed
method for acquiring knowledge for WSD.  In the
framework of translingually aligning pairs of related
words, we encounter two major problems: the ambi-
guity in alignment, and the disparity of topical cover-
age between the corpora of the two languages.  The
following sections discuss how to overcome these
problems.
2.2 Coping with ambiguity in alignment
Matching of pairs of related words via a bilingual dic-
tionary often suggests that a pair in one language can
be aligned with two or more pairs in the other language.
For example, an English pair (tank, troop) can be
aligned with Japanese pairs (??<SUISOU>, ??
<MURE>), (?<SOU>, ??<TASUU>), (??<SENSYA>,
?<GUN>), (??<SENSYA>, ??<TASUU>), and (??
<SENSYA>, ?<TAI>). We resolve this ambiguity on the
assumption that correct alignments are accompanied by
a lot of common related words that can be aligned with
each other.  In the above example, a lot of words
related to both ?tank? and ?troop? can be aligned with
words related to both ???<SENSYA>? and ??<TAI>?
(see Figure 2(b5)).
The plausibility of alignment is evaluated ac-
cording to the set of first-language common related
words that can be aligned with second-language
common related words.  Then, using the plausi-
bility of alignment, the correlation between the
senses of a polysemous word and the clues for se-
lecting the most suitable sense is calculated.  To
precisely evaluate the plausibility of alignment, we
define it as the sum of the correlations between the
sense suggested by the alignment and the common
related words accompanying the alignment.
2.3 Coping with disparity between corpora
Given the disparity of topical coverage between the
corpora of two languages as well as the insufficient
coverage of the bilingual dictionary, the method de-
scribed in the preceding section seems too strict.  As
exemplified in Figure 2, even for a correct alignment of
a first-language pair of related words with a second-
language pair of related words, only a small part of the
first-language common related words can be aligned
with second-language common related words.  To
improve the robustness of the method, instead of the
set of first-language common related words that can be
aligned with second-language common related words,
we use a weighted set consisting of all the first-
language common related words, where those aligned
with second-language common related words are given
 
Comparable corpus 
1st language corpus 2nd language corpus 
Align pairs of related words translingually 
Extract co-occurrence 
data and calculate 
mutual information  
Calculate correlation between 
senses and clues iteratively 
Bilingual 
dictionary 
Senses defined 
by sets of 
translations 
Sense-vs.-clue correlation 
Alignments of pairs of related words accompanied 
by a set of common related words 
Collection of pairs 
of related words 
Collection of pairs 
of related words 
Extract co-occurrence 
data and calculate 
mutual information  
Fig. 1 Overview of the proposed method for
acquiring knowledge for WSD
(a) Common related words of (tank, troop)
Army, Bosnian, Bosnian government, Chechen,
Chechnya, Force, Grozny, Israel, Moscow, Mr. Yelt-
sin, Mr. Yeltsin's, NATO, Pentagon, Republican,
Russia, Russian, Secretary, Serb, U.N., Yeltsin, Yelt-
sin's, air, area, army, assault, battle, bomb, carry, ci-
vilian, commander, control, defense, fight, fire, force,
government, helicopter, military, missile, rebel, sol-
dier, weapon
(b1) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SUISOU>, ??<MURE>)
air, area, fire, government
(b2) Common related words of (tank, troop) that can
be aligned with common related words of (?
<SOU>, ??<TASUU>)
area, army, control, force
(b3) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SENSYA>, ?<GUN>)
area, army, battle, commander, force, government
(b4) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SENSYA>, ??<TASUU>)
Serb, area, army, battle, force, government
(b5) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SENSYA>, ?<TAI>)
Russia, Serb, air, area, army, battle, commander, de-
fense, fight, fire, force, government, helicopter, sol-
dier
Fig. 2 Example of common related words
larger weights than the others.
The disparity of topical coverage between the cor-
pora of two languages and the insufficient coverage of
the bilingual dictionary also cause a lot of pairs of re-
lated words not to be aligned with any pair of related
words.  To recover the failure in alignment, we intro-
duce a ?wild card? pair, with which every first-
language pair of related words is aligned compulsorily.
The alignment with the wild-card pair suggests all
senses of the first-language polysemous word, and it is
accompanied by a set consisting of the first-language
common related words with the same weight.
3 Proposed method
3.1 Defining word senses
We define each sense of a polysemous word x of the
first language by a synonym set consisting of x itself
and one or more of its translations y1, y2, ... into the
second language.  The synonym set is similar to that
in WordNet (Miller 1990) except that it is bilingual, not
monolingual.  Examples of some sets are given be-
low.
{tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}
{tank, ??<SENSYA>}
These synonym sets define the ?container? sense and
the ?military vehicle? sense of ?tank? respectively.
Translations that preserve the ambiguity are prefer-
ably eliminated from the synonym sets defining senses
because they are useless for distinguishing the senses.
An example is given below.
{title, ???<KATAGAKI>, ??<SYOUGOU>, ????
<TAITORU>, ??<KEISYOU>}
{title, ??<DAIMEI>, ??<DAIMOKU>, ??
<HYOUDAI>, ??<SYOMEI>, ????<TAITORU>}
{title, ????<TAITORU>, ???<SENSYUKEN>}
These synonym sets define the ?person?s rank or pro-
fession? sense, the ?name of a book or play? sense, and
the ?championship? sense of ?title?.  A Japanese word
?????<TAITORU>?, which represents all these senses,
is preferably eliminated from all these synonym sets.
3.2 Extraction of pairs of related words
The corpus of each language is statistically processed
in order to extract a collection of pairs of related words
in the language (Kaji et al 2000).  First, we extract
words from the corpus and count the occurrence fre-
quencies of each word.  We reject words whose fre-
quencies are less than a certain threshold.  We also
extract pairs of words co-occurring in a window and
count the co-occurrence frequency of each pair of
words.  In the present implementation, the words are
restricted to nouns and unknown words, which are
probably nouns, and the window size is set to 25 words
excluding function words.
Next, we calculate mutual information MI(x, x?)
between each pair of words x and x?.  MI(x, x?) is de-
fined by the following formula:
)'xPr()xPr(
)'x,xPr(
log)'x,x(MI
?
= ,
where Pr(x) is the occurrence probability of x, and Pr(x,
x?) is the co-occurrence probability of x and x?.  Fi-
nally, we select pairs of words whose mutual informa-
tion value is larger than a certain threshold and at the
same time whose relation is judged to be statistically
significant through a log-likelihood ratio test.
3.3 Alignment of pairs of related words
In this section, RX and RY denote the collections of pairs
of related words extracted from the corpora of the first
language and the second language, respectively.  D
denotes a bilingual dictionary, that is, a collection of
pairs consisting of a first-language word and a second-
language word that are translations of each other.
Let X(x) be the set of clues for determining the sen-
se of a first-language polysemous word x, i.e.,
X(x)={x?|(x, x?)?RX}.
Henceforth, the j-th clue for determining the sense of x
is denoted as x?(j).
Let Y(x, x?(j)) be the set of counterparts of a pair of
first-language related words (x, x?(j)), i.e.,
Y(x, x?(j))=
  {(y, y?) | (y, y?)?RY, (x, y)?D, (x?(j), y?)?D}.
(1) Each pair of first-language related words (x, x?(j)) is
aligned with each counterpart (y, y?) (?Y(x, x?(j))),
and a weighted set of common related words Z((x,
x?(j)), (y, y? )) is constructed as follows:
Z((x, x?(j)), (y, y? )) =
  {x? / w(x?) | (x, x?)?RX, (x?(j), x?)?RX},
where w(x?), which denotes the weight of x?, is set
as follows:
- w(x?) = 1+??MI(y, y?) when ?y? (x?, y?)?D,
(y, y?)?RY, and (y?, y?)?RY .
- w(x?) = 1 otherwise.
The mutual information of the counterpart, MI(y, y?),
was incorporated into the weight according to the as-
sumption that alignments with pairs of strongly relat-
ed words are more plausible than those with pairs of
weakly related words.  The coefficient ? was set
to 5 experimentally.
(2) Each pair of first-language related words (x, x?(j)) is
aligned with the wild-card pair (y0, y0?), and a weight-
ed set of common related words Z((x, x?(j)), (y0, y0?))
is constructed as follows:
Z((x, x?(j)), (y0, y0?)) =
  {x? / w(x?) | (x, x?)?RX, (x?(j), x?)?RX},
where w(x?) = 1 for all x?.
3.4?Calculation of correlation between senses and
clues
We define the correlation between the i-th sense S(i)
and the j-th clue x?(j) of a polysemous word x as fol-
lows:
( ) ( )
( )
( )
,
)k(S),'y,y()),j('x,x(Amaxmax
)i(S),'y,y()),j('x,x(Amax
)j('x,xMI)j('x),i(SC
}y{)k(Sy
)}'y,y{(
))j('x,x(Y)'y,y(k
}y{)i(Sy
)}'y,y{(
))j('x,x(Y)'y,y(
0
00
0
00
??
??
?
??
??
?
?=
??
?
?
??
?
?
where A((x, x?(j)), (y, y), S(i)) denotes the plausibility of
alignment of (x, x?(j)) with (y, y) suggesting S(i).
The first factor in the above formula, i.e., the mutu-
al information between the polysemous word and the j-
th clue, is the base of the correlation.  The numerator
of the second factor is the maximum plausibility of
alignments that suggest the i-th sense of the polyse-
mous word.  The denominator of the second factor
has been introduced to normalize the plausibility.
We define the plausibility of alignment suggesting a
sense as the weighted sum of the correlations between
the sense and the common related words, i.e.,
( )
( )."x),i(SC)"x(w
)i(S),'y,y()),j('x,x(A
))'y,y()),j('x,x((Z"x
?
?
?
=
As the definition of the correlation between senses
and clues is recursive, we calculate it iteratively with
the following initial values: C0(S(i), x?(j))=MI(x, x?(j)).
The number of iteration was set at 6 experimen-
tally.
Figure 3 shows how the correlation values converge.
?Troop? demonstrates a typical pattern of convergence;
namely, while the correlation with the relevant sen-
se is kept constant, that with the irrelevant sense
decreases as the iteration proceeds.  ?Ozone? de-
monstrates the effect of the wild-card pair.  Note
that the correlation values due to an alignment with
the wild-card pair begin to diverge in the second
cycle of iteration.  The alignment with the wild-
card pair, which is shared by all senses, does not
produce any distinction among the senses in the
first cycle of iteration; the divergence is caused by
the difference in correlation values between the
senses and the common related words.
3.5 Selection of the sense of a polysemous word
Consulting sense-vs.-clue correlation data acquired by
the method described in the preceding sections, we
select a sense for each instance of a polysemous word x
in a text.  The score of each sense of the polysemous
word is defined as the sum of the correlations between
the sense and clues appearing in the context, i.e.,
( ) ( )?
?
=
)x(Context)j('x
)j('x),i(SC)i(SScore .
A window of 51 words (25 words before the polyse-
mous word and 25 words after it) is used as the context.
Scores of all senses of a polysemous word are calcu-
lated, and the sense whose score is largest is selected as
the sense of the instance of the polysemous word.
When all scores are zero, no sense can be selected; the
case is called ?inapplicable?.
4 Experiment
4.1 Experimental method
We evaluated our method through an experiment using
corpora of English and Japanese newspaper articles.
The first language was English and the second lan-
guage was Japanese.  A Wall Street Journal corpus
(July, 1994 to Dec., 1995; 189 Mbytes) and a Nihon
Keizai Shimbun corpus (Dec., 1993 to Nov., 1994; 275
Mbytes) were used as the training comparable corpus.
EDR (Japan Electronic Dictionary Research Institute)
English-to-Japanese and Japanese-to-English diction-
aries were merged for the experiment.  The resulting
dictionary included 269,000 English nouns and
276,000 Japanese nouns.  Pairs of related words were
extracted from the corpus of each language under the
following parameter settings:
- threshold for occurrence frequencies of words: 10
- threshold for mutual information: 0.0
These settings were common to the English and Ja-
panese corpora.
We selected 60 English polysemous nouns as the
test words.  Words whose different senses appear in
newspapers were preferred.  The frequencies of the
test words in the training corpus ranged from 39,140
(?share?, the third noun in descending order of fre-
quency) to 106 (?appreciation?, the 2,914th noun).
0
0.5
1
1.5
2
2.5
3
0 1 2 3 4 5 6 7 8 9 10
Iteration
Co
rre
lat
ion
C({tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}, troop)
C({tank, ??<SENSYA>}, troop)
C({tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}, ozone)
C({tank, ??<SENSYA>}, ozone)
C({tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}, safety)
C({tank, ??<SENSYA>}, safety)
Fig. 3 Convergence of correlation between
senses and clues
We defined the senses of each test word.  The number
of senses per test word ranged from 2 to 8, and the
average was 3.4.  For each test word, sense-vs.-clue
correlation data were acquired by the method described
in Sections 3.2 through 3.4.  175 clues on average
were acquired for each test word.
For evaluation, we selected 100 test passages per
test word from a Wall Street Journal corpus (Jan., 1996
to Dec. 1996) whose publishing period was different
from that of the training corpus.  The instances of test
words positioned in the center of each test passage
were disambiguated by the method described in Sec-
tion 3.5, and the results were compared with the manu-
ally selected senses.
4.2 Results and evaluation
We used two measurements, applicability and precision
(Dagan and Itai 1994), to evaluate the performance of
our method.  The applicability is the proportion of
instances of the test word(s) that the method could
disambiguate.  The precision is the proportion of dis-
ambiguated instances of the test word(s) that the
method disambiguated correctly.  The applicability
and precision of the proposed method, averaged over
the 60 test polysemous words, were 88.5% and 77.7%,
respectively.
The performance of our method on six out of the 60
test words is summarized in Table 1.  That is, the in-
stances are classified according to the correct sense and
the sense selected by our method.  These results show
that the performance varies according to the test words,
that our method is better in the case of frequent senses,
but worse in the case of infrequent senses, and that our
method can easily distinguish topic-specific senses, but
not generic senses.
We consider the reason for the poor performance
concerning ?measure? [Table 1(a)] and ?race? [Table
1(c)] as follows.  The second sense of ?measure?,
{measure, ? ? <TAISAKU>, ? ? <SYUDAN>, ? ?
<SYOTI>}, is a very generic sense; therefore effective
clues for identifying the sense could not be acquired.
The first sense of ?race?, {race, ???<REESU>, ??
<KYOUSOU>, ?? <KYOUSOU>, ?? <ARASOI>, ?
<SEN>}, is specific to the ?race for the presidency?
topic and the second sense of ?race?, {race, ??
<ZINSYU>, ?? <MINZOKU>, ?? <SYUZOKU>}, is
specific to the ?racial discrimination? topic; however,
both topics are related to ?politics? and, therefore,
many clues were shared by these two senses.
Comparison with a baseline method, which selects
the most frequent sense of each polysemous word
independently of contexts, was also done.  Since large
sense-tagged corpora were not available, we simulated
the baseline method with a modified version of the
proposed method; namely, for each polysemous word,
the sense that maximizes the sum of correlations with
all clues was selected as the most frequent sense.  The
applicability of the baseline method is 100%, while that
of the proposed method is less than 100%.  To com-
pare with the baseline method, the proposed method
was substituted with the proposed method + baseline
method; namely, the baseline method was applied
when the proposed method was inapplicable.
The average precisions of the baseline method and
the proposed method + baseline method, both of which
attained 100% applicability, were 62.8% and 73.4%
respectively.  Figure 4 visualizes the superiority of the
proposed method + baseline method; the 60 test poly-
semous words are scattered on a plane whose horizon-
tal and vertical coordinates represent the precision of
the baseline method and that of the proposed method +
baseline method, respectively.
5 Discussion
Although it has produced promising results, the devel-
oped WSD method has a few problems.  These limi-
tations, along with future extensions, are discussed
below.
(1) Multilingual distinction of senses
The developed method is based on the premise that
the senses of a polysemous word in a language are
lexicalized differently in another language.  However,
the premise is not always true; that is, the ambiguity of
a word may be preserved by its translations.  As de-
scribed in Section 3.1, we preferably use translations
that do not preserve the ambiguity.  However, doing
so is useless unless such translations are frequently
used words.  An essential approach to solving this
problem is to use two or more second languages (Res-
nik and Yarowsky 2000).
(2) Use of syntactic relations
The developed method extracts clues for WSD ac-
cording to co-occurrence in a window.  However, it is
obvious that doing this is not suitable for all polyse-
mous words.  Syntactic co-occurrence is more useful
for disambiguating some sorts of polysemous words.
It is an important and interesting research issue to ex-
tend our method so that it can acquire clues according
to syntactic co-occurrence.  This extended method
does not replace the present method; however, we
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Baseline method
Pr
op
os
ed
 m
eth
od
 + 
ba
se
lin
e
me
tho
d
Fig. 4 Precision of sense selection
should combine both methods or use the one suitable
for each polysemous word.  It should be noted that
this extension also enables disambiguation of polyse-
mous verbs.
The framework of the method is compatible with
syntactic co-occurrence.  Basically, we only have to
incorporate a parser into the step of extracting pairs of
related words.  A parser of the first language is indis-
pensable, but a parser of the second language is not.
As for the second language, we can use co-occurrence
in a small-sized window instead of syntactic co-
occurrence.
6 Comparison with other methods
While our method aligns pairs of related words that are
statistically extracted, WSD using parallel corpora
aligns instances of words (Brown, et al 1991).  Both
alignment techniques are quite different.  Actually,
from the technological viewpoint, our method is close
to WSD using a second-language monolingual corpus
Table 1 Results of sense selection for six polysemous words
(a) Polysemous word ?measure? (applicability=91.0%; precision=48.4%)
Results
Correct sense S1 S2 S3 ? Total
S1={measure, ?? , ??? , ? , ?? ,
??, ??, ??, ??, ??}
20 0 13 4 37
S2={measure, ??, ??, ??} 4 0 29 5 38
S3={measure, ??, ??, ??} 1 0 24 0 25
Total 25 0 66 9 100
[Note]
S1: a system or instrument for calculating
amount, size, weight, etc.
S2: an action taken to gain a certain end
S3: a law suggested in Parliament
(b) Polysemous word ?promotion? (applicability=96.0%; precision=89.6%)
Results
Correct sense S1 S2 S3 ? Total
S1={promotion, ??, ????, ????, ???????} 71 1 0 1 73
S2={promotion, ??, ??, ??, ??, ??, ??} 6 15 0 3 24
S3={promotion, ??, ??, ??, ??, ??} 2 1 0 0 3
Total 79 17 0 4 100
[Note]
S1: an activity intended to
help sell a product
S2: advancement in rank or
position
S3: action to help something
develop or succeed
(c) Polysemous word ?race? (applicability=79.0%; precision=57.0%)
Results
Correct sense S1 S2 S3 ? Total
S1={race, ???, ??, ??, ??, ?} 28 33 0 15 76
S2={race, ??, ??, ??} 1 17 0 6 24
S3={race, ??, ??} 0 0 0 0 0
Total 29 50 0 21 100
[Note]
S1: any competition, or a contest of speed
S2: one of the groups that humans can be divid-
ed into according to physical features, his-
tory, language, etc.
S3: a channel for a current of water
(d) Polysemous word ?tank? (applicability=89.0%; precision=89.9%)
Results
Correct sense S1 S2 ? Total
S1={tank, ???, ??, ?} 57 1 6 64
S2={tank, ??} 8 23 5 36
Total 65 24 11 100
[Note]
S1: a large container for storing liquid or gas
S2: an enclosed heavily armed, armored vehicle
(e) Polysemous word ?title? (applicability=92.0%; precision=81.5%)
Results
Correct sense S1 S2 S3 S4 ? Total
S1={title, ???, ??, ??} 43 1 0 0 2 46
S2={title, ??, ??, ??, ??} 6 26 0 1 5 38
S3={title, ??, ??, ???} 1 1 0 1 1 4
S4={title, ???} 3 3 0 6 0 12
Total 53 31 0 8 8 100
[Note]
S1: a word or name given to a person to
be used before his/her name as a sign
rank, profession, etc.
S2: a name given to a book, play, etc.
S3: the legal right to own something
S4: the position of being the winner of an
sports competition
(f) Polysemous word ?trial? (applicability=92.0%; precision=92.4%)
Results
Correct sense S1 S2 S3 S4 S5 ? Total
S1={trial, ??, ??, ??} 62 3 0 0 0 5 70
S2={trial, ??, ??, ??, ??, ??} 4 23 0 0 0 2 29
S3={trial, ??} 0 0 0 0 0 1 1
S4={trial, ??, ???} 0 0 0 0 0 0 0
S5={trial, ??, ??, ??} 0 0 0 0 0 0 0
Total 66 26 0 0 0 8 100
[Note]
S1: a legal process in which a court
examines a case
S2: a process of testing to determine
quality, value, usefulness, etc.
S3: a sports competition that tests a
player?s ability
S4: annoying thing or person
S5: difficulties and troubles
(Dagan and Itai 1994; Kikui 1998), where instances of
co-occurrence in a first-language text are aligned with
co-occurrences statistically extracted from the second-
language corpus.  A comparison of our method with
WSD using a second-language monolingual corpus is
given below.
First, our method performs alignment during the
acquisition phase, and transforms word-word correla-
tion data into sense-clue correlation data, which is far
more informative than the original word-word correla-
tion data.  In contrast, a method using a second-
language monolingual corpus uses original word-word
correlation data during the disambiguation phase.
This difference results in a difference in the perfor-
mance of WSD, particularly in a poor-context situation
(e.g., query translation).
Second, our method can acquire sense-clue correla-
tion even from a pair of related words for which align-
ment results in failure [e.g., C({tank, ???<TANKU>,
??<SUISOU>, ?<SOU>}, ozone) in Figure 3].  On
the contrary, a conventional WSD method using a sec-
ond-language monolingual corpus uses only pairs of
related words for which alignment results in success.
Thus, our method can elicit more information than the
conventional method.
Tanaka and Iwasaki (1996) exploited the idea of
translingually aligning word co-occurrences to extract
pairs consisting of a word and its translation form a
non-aligned (comparable) corpus.  The essence of
their method is to obtain a translation matrix that
maximizes the distance between the co-occurrence
matrix of the first language and that of the second lan-
guage.  Their method is useful for extracting corpus-
dependent translations; however, it does not extract
knowledge for WSD, i.e., which co-occurring word
suggests which sense or translation.
7 Conclusion
A method for word sense disambiguation using a bilin-
gual comparable corpus together with sense definitions
by translations into another language was developed.
In this method, knowledge for WSD, i.e., sense-vs.-
clue correlation, is acquired in an unsupervised fashion
as follows.  First, statistically significant pairs of relat-
ed words are extracted from the corpus of each lan-
guage.  Then, aligning pairs of related words translin-
gually, the correlation between the senses of a polyse-
mous word and the clues, i.e., the words related to the
polysemous word, is calculated.  In order to overcome
both the problem of ambiguity in the translingual
alignment of pairs of related words and that of disparity
of topical coverage between corpora of different lan-
guages, an iterative algorithm for calculating the cor-
relation was developed.
WSD for each instance of the polysemous word is
done by selecting the sense that maximizes the score,
i.e., the sum of the correlations between each sense and
the clues appearing in the context of the instance.  An
experiment using corpora of English and Japanese
newspaper articles showed that the performance of the
new method is promising: the applicability and preci-
sion of sense selection were 88.5% and 77.7%, respec-
tively, averaged over 60 test polysemous words.
Acknowledgments: This research was sponsored in
part by the Telecommunications Advancement Organi-
zation of Japan.
References
Basili, Roberto, Michelangelo Della Rocca, and Maria
Tereza Pazienza. 1997. Towards a bootstrapping frame-
work for corpus semantic tagging. In Proceedings of the
ACL-SIGLEX Workshop ?Tagging Text with Lexical Se-
mantics: Why, What, and How?? pages 66-73.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer.  1991. Word-sense disam-
biguation using statistical methods. In Proceedings of the
29th Annual Meeting of the ACL, pages 264-270.
Dagan, Ido and Alon Itai. 1994. Word sense disambiguation
using a second language monolingual corpus. Computa-
tional Linguistics, 20(4): 563-596.
Hearst, Marti A. 1991. Noun homograph disambiguation
using local context in large corpora. In Proceedings of the
7th Annual Conference of the Centre for the New OED and
Text Research: Using Corpora, pages 1-22.
Ide, Nancy and Jean Veronis. 1998. Introduction to the spe-
cial issue on word sense disambiguation: the state of the art.
Computational Linguistics, 24(1): 1-40.
Kaji, Hiroyuki, Yasutsugu Morimoto, Toshiko Aizono,
and Noriyuki Yamasaki. 2000. Corpus-dependent
association thesaurus for information retrieval, In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 404-410.
Karov, Yael and Shimon Edelman. 1998. Similarity-based
word sense disambiguation. Computational Linguistics,
24(1): 41-59.
Kikui, Genichiro. 1998. Term-list translation using mono-
lingual word co-occurrence vectors. In Proceedings of the
17th International Conference on Computational Linguis-
tics, pages 670-674.
Miller, George A. 1990. WordNet: an on-line lexical data-
base. International Journal of Lexicography, 3(4): 235-
312.
Rapp, Reinhard. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd Annual Meeting
of the ACL, pages 320-322.
Resnik, Philip and David Yarowsky. 2000. Distinguishing
systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2): 113-133.
Tanaka, Kumiko and Hideya Iwasaki. 1996. Extraction of
lexical translations from non-aligned corpora, In Proceed-
ings of the 16th International Conference on Computation-
al Linguistics, pages 580-585.
Yarowsky, David. 1992. Word sense disambiguation using
statistical models of Roget's categories trained on large cor-
pora. In Proceedings of the 14th International Conference
on Computational Linguistics, pages 454-460.
Yarowsky, David. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings of
the 33rd Annual Meeting of the ACL, pages 189-196.
