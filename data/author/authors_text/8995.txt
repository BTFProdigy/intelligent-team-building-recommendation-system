Proceedings of NAACL HLT 2007, Companion Volume, pages 193?196,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combined Use of Speaker- and Tone-Normalized Pitch Reset with Pause
Duration for Automatic Story Segmentation in Mandarin Broadcast News
Lei Xie, Chuan Liu and Helen Meng
Human-Computer Communications Laboratory
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong, Hong Kong SAR of China
{lxie, cliu3, hmmeng}se.cuhk.edu.hk
Abstract
This paper investigates the combined use of
pause duration and pitch reset for automatic
story segmentation in Mandarin broadcast
news. Analysis shows that story boundaries
cannot be clearly discriminated from utterance
boundaries by speaker-normalized pitch reset
due to its large variations across different syl-
lable tone pairs. Instead, speaker- and tone-
normalized pitch reset can provide a clear sep-
aration between utterance and story bound-
aries. Experiments using decision trees for
story boundary detection reinforce that raw and
speaker-normalized pitch resets are not effec-
tive for Mandarin Chinese story segmentation.
Speaker- and tone-normalized pitch reset is a
good story boundary indicator. When it is com-
bined with pause duration, a high F-measure
of 86.7% is achieved. Analysis of the decision
tree uncovered four major heuristics that show
how speakers jointly utilize pause duration and
pitch reset to separate speech into stories.
1 Introduction
Pitch reset refers to the speaker?s general pitch declina-
tion through the course of a speech unit, followed by a re-
set to a high pitch at the start of next speech unit, as shown
in Figure 1(a). The speech unit may be of different lev-
els of granularity (Tseng et. al., 2005), such as a speech
segment that conveys a central topic (e.g. a news story), a
prosodic phrase group (PG) or an utterance. These units
are often separated by pauses. Pauses and pitch resets
were shown to be effective story boundary indicators in
English broadcast news segmentation (Shriberg et. al.,
2000; Tu?r et. al., 2001). These previous efforts specifi-
cally point out that pause durations are longer and pitch
resets are more pronounced at story boundaries, when
compared to utterance boundaries in English broadcast
news. However, such story segmentation approaches may
be different for a tonal language such as Mandarin Chi-
nese. The use of similar prosodic features for Chinese
news story segmentation deserves further investigation.
The main reason is that Chinese tonal syllables may com-
plicate the expressions of pitch resets. Chinese syllable
tones are expressed acoustically in pitch trajectories, i.e,
different tones show different pitch value ranges and tra-
jectory patterns,1 as shown in Figure 1(b). Initial work in
(Levow, 2004) has shown that Mandarin words at story
ending positions show a lower pitch as compared with
words at non-story-ending positions. In this paper, we
present a data-oriented study to investigate how the tonal-
ity of Mandarin syllables affects pitch resets at utterance
and story boundaries. To alleviate the effects from tonal-
ity, we propose to use speaker- and tone-normalized pitch
reset with pause duration to separate Mandarin broadcast
audio stream into distinct news stories.
F0 PitchReset
t
SpeechU nit SpeechU nit
(a)
5 5
3
5
2
1
4
5
1
5
Tone 1 (high) Tone 2 (rising) Tone 3 (low) Tone 4 (falling)
4
3
2
1(b)
Figure 1: (a) Pitch reset phenomenon between speech
units; (b) Pitch trajectories for the four Mandarin basic
syllable tones. The speaker pitch range is segmented to
five zones from high to low. The pitch trajectories of the
four tones are 5-5, 3-5, 2-1-4 and 5-1, respectively.
2 Task and Corpus
In a continuous audio stream of broadcast news, there are
programs that consist of speaker changes among anchors,
reporters and interviewees. Other programs may contain
a sequence of news stories reported by a single speaker.
We focus on the latter kind in this investigation, because
the combined use of pause duration and pitch reset to
punctuate the end of a story and the beginning of the next
carries many speaker-dependent characteristics.
We select a subset of TDT2 VOA Mandarin broadcast
news corpus (LDC, 1998) and manually extract the news
sessions reported by a single speaker. We also annotate
1http://www.mandarinbook.net/pronunciation/
193
Table 1: The TDT2 subset used in this study.
Nature
Mandarin news sessions reported
by a single speaker (13.4 hours)
# of News
Sessions
175 (Training: 74, Development:
50, Testing: 51)
Mean Session
Duration
276 seconds, 1071 Mandarin char-
acters
# of Story
Boundaries
1085 (Training: 442, Develop-
ment: 316, Testing: 327)
# of Speakers 11 (7 females and 4 males)
Mean Story
Duration
36 seconds, 105 Mandarin charac-
ters
Transcriptions Dragon ASR recognizer, GB-
encoded word-level transcriptions
in XML format
the news story boundaries in this subset. These single-
speaker sessions typically contain between 3 to 9 short
news stories separated by pauses and constitute about
30% of the entire TDT2 Mandarin corpus (by time du-
ration). The selected subset is divided into training, de-
velopment and testing sets. Details are shown in Table 1.
3 Region of Interest and Pitch Extraction
Previous work on English news segmentation (Shriberg
et. al., 2000) measured pitch resets at inter-word bound-
aries. Since Chinese news transcripts come as a charac-
ter stream and each character is pronounced as a tonal
syllable, it is more reasonable to investigate the pitch re-
set phenomenon at the syllable level. We assume that a
story boundary must occur at an utterance boundary. The
utterances are separated by labeled pauses in the VOA
transcriptions ([P] in Figure 2) and a story may contain
various utterances (between 2 to 38 in the corpus). There-
fore, we only investigate pitch resets in inter-syllable re-
gions across two consecutive utterances as shown in Fig-
ure 2. This is reasonable because there are only 6 story
boundaries (out of 1085) that are not signaled by pause
breaks in the corpus. The region of interest (ROI) is lim-
ited to only two tonal syllables, i.e., the last tonal syllable
of the previous utterance and the first tonal syllable of the
following utterance. We have performed experiments on
window length selection and results have shown a wider
window does not bring a noticeable improvement.
Raw pitch values are extracted by the YIN pitch
tracker (Cheveigne? et. al., 2002). The output pitch tra-
jectories are ranked as ?good? and ?best? by the pitch
tracker. Pitch values for unvoiced and pause segments are
assigned to be zero. We keep the ?best? pitch trajectories
for pitch reset measurements. We focus on pitch resets in
the ROIs and thus obtain pitch contours for the left and
right tonal syllables for each ROIs. However, the corpus
transcription does not provide time annotations for those
tonal syllables. Therefore, in the pitch trajectory of an
! " # $ % & ' ( ) * + , *[P]Character
TonalS yllable lian2h e2g uo2??m i4s hu1z hang3??a n1?n an2 di3??d a2??b a1?g e2?d a2[P]
Translation UnitedN ations Secretary-general Annan Arriveda t Bagdad
[P]Utterance Utterance Utterance Utterance
Story StoryBoundaryUtteranceBoundary
Utterance Utterance
Tone 2???2????2???4???1????3????1????2??????????3???2???1???2???2
Story
UtteranceFinal UtteranceInitial
ROI
[P] [P]
ROI
ROI ROI
Figure 2: Region of interest(ROI) for pitch reset measure.
audio stream, we search forwards and backwards on both
sides of the pause segment for the nearest non-zero pitch
measurement sequences. The two pitch sequences found
are used as the pitch contours for the left and right tonal
syllables of the ROI, respectively. This approximation is
reasonable because a Mandarin tonal syllable usually ex-
hibits a continuous pitch contour within its time duration.
4 Speaker- and Tone-Normalized Pitch
Reset Analysis in Mandarin Broadcast
News
We investigate the pitch reset behavior in the ROIs, i.e.,
the pitch jump between the left and right tonal syllables
at utterance and story boundaries across all corpus audio.
Since pitch is a speaker-related feature, we adopt speaker-
normalized pitch reset, defined as
PR = F0r ? F0l, (1)
where F0l and F0r are the speaker-normalized pitch for
the left and right tonal syllables in the ROIs, which are
calculated using
F0 = (f0 ? ?sf0)/?sf0 . (2)f0 denotes the mean value of the pitch contour of a tonal
syllable uttered by speaker s. ?sf0 and ?sf0 are the pitch
mean and standard deviation calculated for speaker s over
all the ROIs of speaker s in the corpus.
We measure the speaker-normalized pitch resets in all
ROIs, and categorize them into two boundary types, i.e.
utterance boundary and story boundary. To show the ef-
fects of tonality in pitch movement, we also categorize
the pitch resets by different tone combinations (16 com-
binations for 4 Mandarin tones2). Figure 3 plots the mean
PR of each tone combinations for the two boundary
types calculated on the corpus data. We see that the pitch
reset phenomenon holds for all tone combinations, even
for the tone pair (1,3) (i.e. high, low) that has a very small
reset. We perform t-tests (p < 0.0025, one-tailed), which
show that for a given tone pair across a boundary, there
is a significant difference in PR between an utterance
boundary and a story boundary. However, the PR val-
ues vary greatly across different tone pairs. For example,
2The neutral tone is not considered here since its pitch pat-
tern depends heavily on its neighboring tonal syllables.
194
(1,1) (1,2) (1,3) (1,4)(2,1) (2,2) (2,3) (2,4) (3,1)(3,2) (3,3) (3,4) (4,1) (4,2)(4,3) (4,4)0
0.5
1
1.5
2
2.5
StoryB oundary
UtteranceB oundary
ToneP air
Me
an
PR
OverallM ean forStoryB oundaryPR
OverallM ean forUtteranceB oundaryPR
Figure 3: Mean speaker-normalized pitch reset of the 16
tone pairs for story and utterance boundaries.
pitch resets are reduced for the tone pairs (1,3) and (4,3),
but are pronounced for the tone pairs (3,1) and (2,1). The
t-test (p < 0.0025, one-tailed) shows that the PR differ-
ence between utterance boundaries and story boundaries
are not significant. This motivates us to formulate a defi-
nition for speaker- and tone-normalized pitch reset.
The speaker- and tone-normalized pitch reset is defined
as:
PR = F0r ?F0l, (3)
where F0l and F0r are the speaker- & tone-normalized
pitch for the left and right tonal syllables in the ROIs,
respectively, defined as
F0 = (F0 ? ??F0)/??F0 , (4)
where F0 is the speaker-normalized pitch in Equation (2)
of a tonal syllable with tone ? . ??F0 and ??F0 are the pitch
mean and standard deviation calculated for the tonal syl-
lables with tone ? over all ROIs in the corpus. Figure 4
plots the mean PR of each tone combinations for the two
boundary types calculated on the corpus data.
Figure 4 shows a clear separation in speaker- and tone-
normalized pitch reset (PR) between utterance and story
boundaries (shade area in Figure 4). This result is sta-
tistically significant based on a t-test (p < 0.0025, one-
tailed). This observation suggests that speaker- and tone-
normalized pitch reset may be an effective story boundary
indicator for Mandarin broadcast news.
5 Experiments on Story Boundary
Detection
We perform experiments on story boundary detection at
the ROIs in the corpus. Since all ROIs are utterance
boundaries, of which only some are story boundaries, we
take a ?hypothesize and classify? approach in order to
strike a good balance between recall and precision. We
first hypothesize the occurrence of a story boundary if
the ROI has a pause duration that exceeds a threshold.
This is followed by a decision tree classifier that decides
on the existence of a story boundary. We used Quinlan?s
C4.5-style decision tree (Quinlan, 1992) as the classifier,
0
0.5
1
1.5
2
2.5
StoryB oundary
UtteranceB oundary
Me
an
ToneP air
OverallM ean???f orStoryB oundary
OverallM ean????f orUtteranceB oundary
(1,1) (1,2) (1,3) (1,4)(2,1) (2,2) (2,3) (2,4) (3,1)(3,2) (3,3) (3,4) (4,1) (4,2)(4,3) (4,4)
Figure 4: Mean speaker- and tone-normalized pitch reset
of the 16 tone pairs for story and utterance boundaries.
implemented by the IND toolkit.3 The pause duration
threshold was selected by a heuristic search procedure
described as follows: We experimented with pause du-
rations ranging from 0.1 to 4 seconds with step size of
0.1 second. In each case, we hypothesized raw bound-
aries in the training and development sets. A decision tree
was then grown using the raw boundary hypotheses of the
training set, and tested on the raw boundary hypotheses
of the development set. The pause duration leading to the
highest F-measure on the development set was selected
as the optimal threshold for the further experiments on
the testing set.
We develop seven story boundary detectors according
to the features used (see Table 2). The boundary de-
tection results on the testing set are shown in Table 2.
From Table 2, we can see that the detector using pause
duration achieves a high F-measure of 82.2%. This re-
sult is reasonable since VOA Mandarin news broadcast
makes large use of long pauses at story boundaries, es-
pecially at news sessions reported by a single speaker.
The detector using raw pitch reset (pr = f0r?f0l) only
gets a F-measure of 50.8% and the speaker-normalized
pitch reset (PR) achieves a slightly better F-measure of
55.3%. Speaker- and tone-normalized pitch reset (PR)
achieves a superior performance with an F-measure of
71.1%. This result is consistent with the observations
in Section 4. The story boundary indicative ability of
speaker-normalized pitch reset is affected by the tonal-
ity of Mandarin syllable. Speaker- and tone-normalized
pitch reset can alleviate the effects, thus leading to a bet-
ter discrimination. Based on Table 2, when pause is
combined with raw pitch reset, the F-measure degrades
from 82.2% to 68.3%. The F-measure reaches 77.4%
when we combine pause with speaker-normalized pitch
reset. When pause is combined with speaker- and tone-
normalized pitch reset (Pause+PR), the best F-measure
is achieved at 86.7%.
3http://ic.arc.nasa.gov/projects/bayes-group/ind/
195
Table 2: Story boundary detection experiment results(%)
Feature Recall Precision F-Measure
Pause 77.1 88.1 82.2
pr 52.0 49.7 50.8
PR 56.6 54.1 55.3
PR 70.3 72.0 71.1
Pause+pr 66.4 70.3 68.3
Pause+PR 72.2 83.5 77.4
Pause+PR 82.6 91.3 86.7
Table 3: Heuristics for story boundary decision
No. Description StoryBoundary?
1
Pause duration is short (P <
1.475) and pitch reset is small
(PR < 0.401)
No
2
Pause duration is short (P <
1.475) and pitch reset is huge
(PR > 1.112)
Yes
3
Pause duration is long (2.315?
P<4.915) and pitch reset is big
(PR>0.715)
Yes
4
Pause duration is long (P ?
4.915) and pitch reset is low
(PR < 0.3513)
No
Figure 5 shows the top levels of the decision tree ob-
tained using the Pause+PR set. We can observe the com-
plementarity between pause duration and pitch reset in
story boundary detection. This may be summarized in
terms of four major heuristics shown on the tree (labeled
as 1 to 4 in Figure 5). These heuristics cover about 83%
decisions made on the testing set, as described in Table 3.
Heuristics 2 is mainly used to detect possibly miss-
ing story boundaries with short pauses caused by speaker
speaking style, e.g., reporters Li Weiqing and Yang Chen
tend to use short pauses to separate news stories, but they
tend to offset the reduced pauses with pronounced pitch
resets to signify story boundaries. Heuristics 4 detects
possibly false alarms due to broadcast interruptions in
boundary detection. These interruptions (i.e. silences)
usually occur within a news story and may last for sev-
eral seconds (usually > 5 seconds).
6 Summary and Future Work
This paper investigated the combined use of pause dura-
tion and pitch reset for automatic story segmentation in
Mandarin broadcast news. Pitch reset analysis on Man-
darin broadcast news shows that story boundaries cannot
be discriminated from utterance boundaries by speaker-
normalized pitch reset, because speaker-normalized pitch
reset varies greatly across different tone pairs of boundary
syllables. This motivates us to investigate the speaker-
and tone-normalized pitch reset. Analysis shows that
speaker- and tone-normalized pitch reset can clearly sep-
NOT_BND
<1.805 >=1.805
1
>=1.475<1.475
<0.4012
>=2.315<2.315
>=4.915<4.915
P
PP
P
BNDBND
BNDBND
BND
NOT_BND
>=0.4012
>=1.112<1.112
>=0.8803<0.8803
<0.3513>=0.3513>=0.7150 <0.7150
>=0.811<0.811
2 3 4
Sub-treeSub-tree
Sub-tree Sub-tree
Figure 5: Decision tree for story boundary classification
based on the Pause+PR feature set. B denotes story
boundary, and NOT BND denotes not story boundary.
arate utterance boundaries from story boundaries across
all tone pairs. This result shows the difference be-
tween English and Chinese. Previous work for En-
glish (Shriberg et. al., 2000; Tu?r et. al., 2001) shows
that speaker-normalized pitch reset is effective. This
work shows that the same measurement is not sufficient
for Chinese; instead we need to use speaker- and tone-
normalized pitch reset in Chinese story segmentation.
When pause duration is combined with speaker- and tone-
normalized pitch reset, the best performance is achieved
with a high F-measure of 86.7%. Analysis of the deci-
sion tree uncovered four major heuristics that show how
speakers jointly utilize pause and pitch reset to separate
speech into stories.
Future work will investigate the pitch reset phe-
nomenon in Cantonese broadcast news, because Can-
tonese is another major Chinese dialect with more com-
plicated tonal characteristics. We also plan to incorporate
prosodic cues with lexical cues to further improve perfor-
mance in Chinese story segmentation.
References
Shriberg E., Stolcke A., Hakkani-Tu?r D. and Tu?r G. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Comm., 32(1-2):127?154.
Tu?r G. and Hakkani-Tu?r D. 2001. Integrating Prosodic and
Lexical Cues for Automatic Topic Segmentation. Computa-
tional Linguistics, 27(1):31?57.
Levow G. A. 2004. Prosody-based Topic Segmentation for
Mandarin Broadcast News. Proc. of HLT-NAACL, 137?140.
The Linguistic Data Consortium. 1998.
http://projects.ldc.upenn.edu/TDT2/.
de Cheveigne? A. and Kawahara H. 2002. Yin, a fundamental
frequency estimator for speech and music. Journal of the
Acoustic Society of America, 111(4):1917?1930.
Tseng C. Y., Pin S. H., Lee Y., Wang H. M. and Chen Y. C.
2005. Fluent speech prosody: Framework and modeling.
Speech Comm., 46:284?309.
Quinlan J. R. 1992. C4.5: Programs for Machine Learning.
Morgan Kaufmann.
196
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190?195,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Broadcast News Story Segmentation Using Manifold Learning on Latent
Topic Distributions
Xiaoming Lu1,2, Lei Xie1?, Cheung-Chi Leung2, Bin Ma2, Haizhou Li2
1School of Computer Science, Northwestern Polytechnical University, China
2Institute for Infocomm Research, A?STAR, Singapore
luxiaomingnpu@gmail.com, lxie@nwpu.edu.cn, {ccleung,mabin,hli}@i2r.a-star.edu.sg
Abstract
We present an efficient approach for
broadcast news story segmentation using a
manifold learning algorithm on latent top-
ic distributions. The latent topic distribu-
tion estimated by Latent Dirichlet Alloca-
tion (LDA) is used to represent each text
block. We employ Laplacian Eigenmap-
s (LE) to project the latent topic distribu-
tions into low-dimensional semantic rep-
resentations while preserving the intrinsic
local geometric structure. We evaluate t-
wo approaches employing LDA and prob-
abilistic latent semantic analysis (PLSA)
distributions respectively. The effects of
different amounts of training data and dif-
ferent numbers of latent topics on the two
approaches are studied. Experimental re-
sults show that our proposed LDA-based
approach can outperform the correspond-
ing PLSA-based approach. The proposed
approach provides the best performance
with the highest F1-measure of 0.7860.
1 Introduction
Story segmentation refers to partitioning a mul-
timedia stream into homogenous segments each
embodying a main topic or coherent story (Allan,
2002). With the explosive growth of multimedia
data, it becomes difficult to retrieve the most rel-
evant components. For indexing broadcast news
programs, it is desirable to divide each of them
into a number of independent stories. Manual seg-
mentation is accurate but labor-intensive and cost-
ly. Therefore, automatic story segmentation ap-
proaches are highly demanded.
Lexical-cohesion based approaches have been
widely studied for automatic broadcast news story
segmentation (Beeferman et al, 1997; Choi, 1999;
Hearst, 1997; Rosenberg and Hirschberg, 2006;
?corresponding author
Lo et al, 2009; Malioutov and Barzilay, 2006;
Yamron et al, 1999; Tur et al, 2001). In this
kind of approaches, the audio portion of the da-
ta stream is passed to an automatic speech recog-
nition (ASR) system. Lexical cues are extracted
from the ASR transcripts. Lexical cohesion is the
phenomenon that different stories tend to employ
different sets of terms. Term repetition is one of
the most common appearances.
These rigid lexical-cohesion based approach-
es simply take term repetition into consideration,
while term association in lexical cohesion is ig-
nored. Moreover, polysemy and synonymy are not
considered. To deal with these problems, some
topic model techniques which provide conceptu-
al level matching have been introduced to text and
story segmentation task (Hearst, 1997). Proba-
bilistic latent semantic analysis (PLSA) (Hofman-
n, 1999) is a typical instance and used widely.
PLSA is the probabilistic variant of latent seman-
tic analysis (LSA) (Choi et al, 2001), and offers a
more solid statistical foundation. PLSA provides
more significant improvement than LSA for story
segmentation (Lu et al, 2011; Blei and Moreno,
2001).
Despite the success of PLSA, there are con-
cerns that the number of parameters in PLSA
grows linearly with the size of the corpus. This
makes PLSA not desirable if there is a consid-
erable amount of data available, and causes seri-
ous over-fitting problems (Blei, 2012). To deal
with this issue, Latent Dirichlet Allocation (L-
DA) (Blei et al, 2003) has been proposed. LDA
has been proved to be effective in many segmenta-
tion tasks (Arora and Ravindran, 2008; Hall et al,
2008; Sun et al, 2008; Riedl and Biemann, 2012;
Chien and Chueh, 2012).
Recent studies have shown that intrinsic di-
mensionality of natural text corpus is significant-
ly lower than its ambient Euclidean space (Belkin
and Niyogi, 2002; Xie et al, 2012). Therefore,
190
Laplacian Eigenmaps (LE) was proposed to com-
pute corresponding natural low-dimensional struc-
ture. LE is a geometrically motivated dimen-
sionality reduction method. It projects data into
a low-dimensional representation while preserv-
ing the intrinsic local geometric structure infor-
mation (Belkin and Niyogi, 2002). The locali-
ty preserving property attempts to make the low-
dimensional data representation more robust to the
noise from ASR errors (Xie et al, 2012).
To further improve the segmentation perfor-
mance, using latent topic distributions and LE in-
stead of term frequencies to represent text blocks
is studied in this paper. We study the effects of
the size of training data and the number of latent
topics on the LDA-based and the PLSA-based ap-
proaches. Another related work (Lu et al, 2013)
is to use local geometric information to regularize
the log-likelihood computation in PLSA.
2 Our Proposed Approach
In this paper, we propose to apply LE on the L-
DA topic distributions, each of which is estimat-
ed from a text block. The low-dimensional vec-
tors obtained by LE projection are used to detect
story boundaries through dynamic programming.
Moreover, as in (Xie et al, 2012), we incorporate
the temporal distances between block pairs as a
penalty factor in the weight matrix.
2.1 Latent Dirichlet Allocation
Latent Dirichlet alocation (LDA) (Blei et al,
2003) is a generative probabilistic model of a cor-
pus. It considers that documents are represented
as random mixtures over latent topics, where each
topic is characterized by a distribution over terms.
In LDA, given a corpus D = {d1, d2, . . . , dM}
and a set of terms W = (w1, w2, . . . , wV ), the
generative process can be summarized as follows:
1) For each document d, pick a multinomial dis-
tribution ? from a Dirichlet distribution parameter
?, denoted as ? ? Dir(?).
2) For each term w in document d, select a topic
z from the multinomial distribution ?, denoted as
z ? Multinomial(?).
3) Select a term w from P (w|z, ?), which is a
multinomial probability conditioned on the topic.
An LDA model is characterized by two sets of
prior parameters ? and ?. ? = (?1, ?2, . . . , ?K)
represents the Dirichlet prior distributions for each
K latent topics. ? is aK?V matrix, which defines
the latent topic distributions over terms.
2.2 Construction of weight matrix in
Laplacian Eigenmaps
Laplacian Eigenmaps (LE) is introduced to project
high-dimensional data into a low-dimensional rep-
resentation while preserving its locality property.
Given the ASR transcripts of N text blocks, we ap-
ply LDA algorithm to compute the corresponding
latent topic distributions X = [x1, x2, . . . , xN ] in
RK , where K is the number of latent topics, name-
ly the dimensionality of LDA distributions.
We use G to denote an N-node (N is number of
LDA distributions) graph which represents the re-
lationship between all the text block pairs. If dis-
tribution vectors xi and xj come from the same
story, we put an edge between nodes i and j. We
define a weight matrix S of the graph G to denote
the cohesive strength between the text block pairs.
Each element of this weight matrix is defined as:
sij = cos(xi, xj)?|i?j|, (1)
where ?|i?j| serves the penalty factor for the dis-
tance between i and j. ? is a constant lower than
1.0 that we tune from a set of development data.
It makes the cohesive strength of two text blocks
dramatically decrease when their distance is much
larger than the normal length of a story.
2.3 Data projection in Laplacian Eigenmaps
Given the weight matrix S, we define C as the di-
agonal matrix with its element:
cij =
?K
i=1
sij . (2)
Finally, we obtain the Laplacian matrix L, which
is defined as:
L = C? S. (3)
We use Y = [y1, y2, . . . , yN ] (yi is a column
vector) to indicate the low-dimensional represen-
tation of the latent topic distributions X. The pro-
jection from the latent topic distribution space to
the target space can be defined as:
f : xi ? yi. (4)
A reasonable criterion for computing an optimal
mapping is to minimize the objective as follows:
K?
i=1
K?
j=1
? yi ? yj ?2 sij . (5)
Under this constraint condition, we can preserve
the local geometrical property in LDA distribu-
tions. The objective function can be transformed
191
as:
K?
i=1
K?
j=1
(yi ? yj)sij = tr(YTLY). (6)
Meanwhile, zero matrix and matrices with it-
s rank less than K are meaningless solutions for
our task. We impose YTLY = I to prevent this
situation, where I is an identity matrix. By the
Reyleigh-Ritz theorem (Lutkepohl, 1997), the so-
lution can obtained by the Q smallest eigenvalues
of the generalized eigenmaps problem:
XLXT y = ?XCXT y. (7)
With this formula, we calculate the mapping ma-
trix Y, and its row vectors y?1, y?2, . . . , y?Q are in the
order of their eigenvalues ?1 ? ?2 ? . . . ? ?Q.
y?i is a Q-dimensional (Q<K) eigenvectors.
2.4 Story boundary detection
In story boundary detection, dynamic program-
ming (DP) approach is adopted to obtain the glob-
al optimal solution. Given the low-dimensional se-
mantic representation of the test data, an objective
function can be defined as follows:
? =
Ns?
t=1
(
?
i,j?Segt
? yi ? yj ?2), (8)
where yi and yj are the latent topic distributions of
text blocks i and j respectively, and ? yi ? yj ?2
is the Euclidean distance between them. Segt in-
dicates these text blocks assigned to a certain hy-
pothesized story. Ns is the number of hypothe-
sized stories.
The story boundaries which minimize the ob-
jective function ? in Eq.(8) form the optimal re-
sult. Compared with classical local optimal ap-
proach, DP can more effectively capture the s-
mooth story shifts, and achieve better segmenta-
tion performance.
3 Experimental setup
Our experiments were evaluated on the ASR tran-
scripts provided in TDT2 English Broadcast news
corpus1, which involved 1033 news programs. We
separated this corpus into three non-overlapping
sets: a training set of 500 programs for parameter
estimation in topic modeling and LE, a develop-
ment set of 133 programs for empirical tuning and
a test set of 400 programs for performance evalu-
ation.
In the training stage, ASR transcripts with man-
ually labeled boundary tags were provided. Text
1http://projects.ldc.upenn.edu/TDT2/
streams were broken into block units according to
the given boundary tags, with each text block be-
ing a complete story. In the segmentation stage,
we divided test data into text blocks using the time
labels of pauses in the transcripts. If the pause du-
ration between two blocks last for more than 1.0
sec, it was considered as a boundary candidate. To
avoid the segmentation being suffered from ASR
errors and the out-of-vocabulary issue, phoneme
bigram was used as the basic term unit (Xie et al,
2012). Since the ASR transcripts were at word lev-
el, we performed word-to-phoneme conversion to
obtain the phoneme bigram basic units. The fol-
lowing approaches, in which DP was used in story
boundary detection, were evaluated in the experi-
ments:
? PLSA-DP: PLSA topic distributions were
used to compute sentence cohesive strength.
? LDA-DP: LDA topic distributions were used
to compute sentence cohesive strength.
? PLSA-LE-DP: PLSA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesive strength.
? LDA-LE-DP: LDA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesion strength.
For LDA, we used the implementation from
David M. Blei?s webpage2. For PLSA, we used
the Lemur Toolkit3.
F1-measure was used as the evaluation crite-
rion.We followed the evaluation rule: a detected
boundary candidate is considered correct if it lies
within a 15 sec tolerant window on each side of a
reference boundary. A number of parameters were
set through empirical tuning on the developent set.
The penalty factor was set to 0.8. When evaluating
the effects of different size of the training set, the
number of latent topics in topic modeling process
was set to 64. After the number of latent topics
was fixed, the dimensionality after LE projection
was set to 32. When evaluating the effects of d-
ifferent number of latent topics in topic modeling
computation, we fixed the size of the training set
to 500 news programs and changed the number of
latent topics from 16 to 256.
4 Experimental results and analysis
4.1 Effect of the size of training dataset
We used the training set from 100 programs to 500
programs (adding 100 programs in each step) to e-
2http://www.cs.princeton.edu/ blei/lda-c/
3http://www.lemurproject.org/
192
valuate the effects of different size of training data
in both PLSA-based and LDA-based approaches.
Figure 1 shows the results on the development set
and the test set.
0.55
0.6
0.65
0.7
0.75
0.8
100 200 300 400 500
F1-
me
asu
re
PLSA-LE-DP LDA-LE-DP
LDA-DP
PLSA-DP
Development Set
0.55
0.6
0.65
0.7
0.75
0.8
100 200 300 400 500
F1-
me
asu
re
Number of programs in training data
PLSA-LE-DP
LDA-LE-DP
PLSA-DP
LDA-DP
Test Set
Figure 1: Segmentation performance with differ-
ent amounts of training data
LDA-LE-DP approach achieved the best result
(0.7927 and 0.7860) on both the development and
the test sets, when there were 500 programs in the
training set. This demonstrates that LDA model
and LE projection used in combination is excellent
for the story segmentation task. The LE projection
applied on the latent topic representations made
relatively 9.88% and 10.93% improvement over
the LDA-based approach and the PLSA-based ap-
proach, respectively on the test set. We can reveal
that employing LE on PLSA and LDA topic dis-
tributions achieves much better performance than
the corresponding approaches without using LE.
We have compared the performances between
PLSA and LDA. We found that when the train-
ing data size was small, PLSA performed better
than LDA. Both PLSA-based and LDA-based ap-
proaches got better with the increase in the size of
the training data set. All the four approaches had
similar performances on the development set and
the test set.
With the increase in the size of the training da-
ta, the LDA-based approaches were improved dra-
matically. They even outperformed the PLSA-
based approaches when the training data contained
more than 300 programs. This may be attributed
to the fact that LDA needs more training data to
estimate the parameters. When the training data is
not enough, its parameters estimated in the train-
ing stage is not stable for the development and the
test data. Moreover, compared with PLSA, the pa-
rameters in LDA do not grow linearly with the size
of the corpus.
4.2 Effect of the number of latent topics
We evaluated the F1-measure of the four ap-
proaches with different number of latent topics
prior to LE projection. Figure 2 shows the cor-
responding results.
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
16 32 48 64 80 96 128 256
F1-m
easu
re
Number of latent topics
PLSA-DP
LDA-DP
PLSA-LE-DP
LDA-LE-DP
Figure 2: Segmentation performance with differ-
ent numbers of latent topics
The best performances (0.7816-0.7847) were
achieved at the number of latent topics between
64 and 96. When the number of latent topics was
increased from 16 to 64, F1-measure increased.
When the number of latent topics was larger than
96, F1-measure decreased gradually. We found
that the best results were achieved when the num-
ber of topics was close to the real number of top-
ics. There are 80 manually labeled main topics in
the test set.
We observe that LE projection makes the topic
model more stable with different numbers of latent
topics. The best and the worst performances dif-
fered by relatively 9.12% in LDA-DP and 7.97%
in PLSA-DP. However, the relative difference of
2.79% and 2.46% were observed in LDA-LE-DP
and PLSA-LE-DP respectively.
5 Conclusions
Our proposed approach achieves the best F1-
measure of 0.7860. In the task of story segmen-
tation, we believe that LDA can avoid data overfit-
ting problem when there is a sufficient amount of
training data. This is also applicable to LDA-LE-
LP. Moreover, we find that when we apply LE pro-
jection to latent topic distributions, the segmen-
tation performances become less sensitive to the
predefined number of latent topics.
193
Acknowledgments
This work is supported by the National Natu-
ral Science Foundation of China (61175018), the
Natural Science Basic Research Plan of Shaanx-
i Province (2011JM8009) and the Fok Ying Tung
Education Foundation (131059).
References
J. Allan. 2002. Topic Detection and Tracking: Event-
Based Information Organization. Kluwer Academic
Publisher, Norwell, MA.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A Model of Lexical Attraction and repulsion.
In Proceedings of the 8th Conference on European
Chapter of the Association for Computational Lin-
guistics (EACL), pp.373-380.
Freddy Y. Y. Choi. 2000. Advances in Domain In-
dependent Linear Text Segmentation. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference
(NAACL), pp.26-33.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pp.20-57.
Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
Haizhou Li. 2011. Probabilistic Latent Seman-
tic Analysis for Broadcast New Story Segmentation.
In Proceedings of the 11th Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pp.1109-1112.
David M. Blei. 2012. Probabilistic topic models.
Communication of the ACM, vol. 55, pp.77-84.
David M. Blei, Andrew Y. Ng, Michael I. Jordan.
2003. Latent Dirichlet Allocation. the Journal of
Machine Learning Research, vol. 3, pp.993-1022.
Marti A. Hearst. 1997. TextTiling: Segmenting Text
into Multiparagraph subtopic passages. Computa-
tional Liguistic, vol. 23, pp.33-64.
Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke,
Elizabeth Shriberg. 2001. Integrating Prosodic
and Lexicial Cues for Automatic Topic Segmenta-
tion. Computational Liguistic, vol. 27, pp.31-57.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
Segmentation of Broadcast News in English, Man-
darin and Aribic. In Proceedings of the 7th North
American Chapter of the Association for Compu-
tational Linguistics Conference (NAACL), pp.125-
128.
David M. Blei and Pedro J. Moreno. 2001. Topic Seg-
mentation with An Aspect Hidden Markov Model. In
Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrival (SIGIR), pp.343-348.
Wai-Kit Lo, Wenying Xiong, Helen Meng. 2009. Au-
tomatic Story Segmentation Using a Bayesian De-
cision Framwork for Statistical Models of Lexical
Chain Feature. In Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pp.357-364.
Igor Malioutov and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmenation. In Pro-
ceedings of the 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp.25-32.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, Juhanna
Moore. 2001. Latent Semantic Analysis for Tex-
t Segmentation. In Proceedings of the 2001 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), pp.109-117.
Rachit Arora and Balaraman Ravindran. 2008. Latent
Dirichlet Allocation Based Multi-document Summa-
rization. In Proceedings of the 2nd Workshop on
Analytics for Noisy Unstructured Text Data (AND),
pp.91-97.
David Hall, Daniel Jurafsky, Christopher D. Manning.
2008. Latent Studying the History Ideas Using Topic
Models. In Proceedings of the 2008 Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP), pp.363-371.
Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu. 2008.
Text Segmentation with LDA-based Fisher Kernel.
In Proceedings of the 46th Annual Meeting of the As-
socation for Computational Linguistics on Human
Language Technologies (HLT-ACL), pp.269-272.
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
Eigenmaps for Dimensionality Reduction and Da-
ta Representation. Neural Computation, vol. 15,
pp.1383-1396.
Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang.
2012. Laplacian Eigenmaps for Automatic Story
Segmentation of Broadcast News. IEEE Transaction
on Audio, Speech and Language Processing, vol. 20,
pp.264-277.
Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang
Zhai. 2008. Modeling Hidden Topics on Document
Manifold. In Proceedings of the 17th ACM Confer-
ence on Information and Knowledge Managemen-
t (CIKM), pp.911-120.
Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
and Haizhou Li. 2013. Broadcast News Story Seg-
mentation Using Latent Topics on Data Manifold. In
Proceedings of the 38th International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
194
J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van
Mulbregt. 1999. AHiddenMarkov Model Approach
to Text Segmenation and Event Tracking. In Pro-
ceedings of the 1999 International Conference on
Acoustics, Speech, and Signal Processing (ICASSP),
pp.333-336.
Martin Riedl and Chris Biemann. 2012. Text Segmen-
tation with Topic Models. the Journal for Language
Technology and Computational Linguistics, pp.47-
69.
P. Fragkou , V. Petridis , Ath. Kehagias. 2002. A Dy-
namic Programming algorithm for Linear Text Story
Segmentation. the Joural of Intelligent Information
Systems, vol. 23, pp.179-197.
H. Lutkepohl. 1997. Handbook of Matrices. Wiley,
Chichester, UK.
Jen-Tzung Chien and Chuang-Hua Chueh. 2012.
Topic-Based Hieraachical Segmentation. IEEE
Transaction on Audio, Speech and Language Pro-
cessing, vol. 20, pp.55-66.
195
