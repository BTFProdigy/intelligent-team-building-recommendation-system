Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102?111,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Annotation of Search Queries
Michael Bendersky
Dept. of Computer Science
University of Massachusetts
Amherst, MA
bemike@cs.umass.edu
W. Bruce Croft
Dept. of Computer Science
University of Massachusetts
Amherst, MA
croft@cs.umass.edu
David A. Smith
Dept. of Computer Science
University of Massachusetts
Amherst, MA
dasmith@cs.umass.edu
Abstract
Marking up search queries with linguistic an-
notations such as part-of-speech tags, cap-
italization, and segmentation, is an impor-
tant part of query processing and understand-
ing in information retrieval systems. Due
to their brevity and idiosyncratic structure,
search queries pose a challenge to existing
NLP tools. To address this challenge, we
propose a probabilistic approach for perform-
ing joint query annotation. First, we derive
a robust set of unsupervised independent an-
notations, using queries and pseudo-relevance
feedback. Then, we stack additional classi-
fiers on the independent annotations, and ex-
ploit the dependencies between them to fur-
ther improve the accuracy, even with a very
limited amount of available training data. We
evaluate our method using a range of queries
extracted from a web search log. Experimen-
tal results verify the effectiveness of our ap-
proach for both short keyword queries, and
verbose natural language queries.
1 Introduction
Automatic mark-up of textual documents with lin-
guistic annotations such as part-of-speech tags, sen-
tence constituents, named entities, or semantic roles
is a common practice in natural language process-
ing (NLP). It is, however, much less common in in-
formation retrieval (IR) applications. Accordingly,
in this paper, we focus on annotating search queries
submitted by the users to a search engine.
There are several key differences between user
queries and the documents used in NLP (e.g., news
articles or web pages). As previous research shows,
these differences severely limit the applicability of
standard NLP techniques for annotating queries and
require development of novel annotation approaches
for query corpora (Bergsma and Wang, 2007; Barr et
al., 2008; Lu et al, 2009; Bendersky et al, 2010; Li,
2010).
The most salient difference between queries and
documents is their length. Most search queries
are very short, and even longer queries are usually
shorter than the average written sentence. Due to
their brevity, queries often cannot be divided into
sub-parts, and do not provide enough context for
accurate annotations to be made using the stan-
dard NLP tools such as taggers, parsers or chun-
kers, which are trained on more syntactically coher-
ent textual units.
A recent analysis of web query logs by Bendersky
and Croft (2009) shows, however, that despite their
brevity, queries are grammatically diverse. Some
queries are keyword concatenations, some are semi-
complete verbal phrases and some are wh-questions.
It is essential for the search engine to correctly an-
notate the query structure, and the quality of these
query annotations has been shown to be a crucial
first step towards the development of reliable and
robust query processing, representation and under-
standing algorithms (Barr et al, 2008; Guo et al,
2008; Guo et al, 2009; Manshadi and Li, 2009; Li,
2010).
However, in current query annotation systems,
even sentence-like queries are often hard to parse
and annotate, as they are prone to contain mis-
spellings and idiosyncratic grammatical structures.
102
(a) (b) (c)
Term CAP TAG SEG
who L X B
won L V I
the L X B
2004 L X B
kentucky C N B
derby C N I
Term CAP TAG SEG
kindred C N B
where C X B
would C X I
i C X I
be C V I
Term CAP TAG SEG
shih C N B
tzu C N I
health L N B
problems L N I
Figure 1: Examples of a mark-up scheme for annotating capitalization (L ? lowercase, C ? otherwise), POS tags (N ?
noun, V ? verb, X ? otherwise) and segmentation (B/I ? beginning of/inside the chunk).
They also tend to lack prepositions, proper punctu-
ation, or capitalization, since users (often correctly)
assume that these features are disregarded by the re-
trieval system.
In this paper, we propose a novel joint query an-
notation method to improve the effectiveness of ex-
isting query annotations, especially for longer, more
complex search queries. Most existing research fo-
cuses on using a single type of annotation for infor-
mation retrieval such as subject-verb-object depen-
dencies (Balasubramanian and Allan, 2009), named-
entity recognition (Guo et al, 2009), phrase chunk-
ing (Guo et al, 2008), or semantic labeling (Li,
2010).
In contrast, the main focus of this work is on de-
veloping a unified approach for performing reliable
annotations of different types. To this end, we pro-
pose a probabilistic method for performing a joint
query annotation. This method allows us to exploit
the dependency between different unsupervised an-
notations to further improve the accuracy of the en-
tire set of annotations. For instance, our method
can leverage the information about estimated parts-
of-speech tags and capitalization of query terms to
improve the accuracy of query segmentation.
We empirically evaluate the joint query annota-
tion method on a range of query types. Instead of
just focusing our attention on keyword queries, as
is often done in previous work (Barr et al, 2008;
Bergsma and Wang, 2007; Tan and Peng, 2008;
Guo et al, 2008), we also explore the performance
of our annotations with more complex natural lan-
guage search queries such as verbal phrases and wh-
questions, which often pose a challenge for IR appli-
cations (Bendersky et al, 2010; Kumaran and Allan,
2007; Kumaran and Carvalho, 2009; Lease, 2007).
We show that even with a very limited amount of
training data, our joint annotation method signifi-
cantly outperforms annotations that were done in-
dependently for these queries.
The rest of the paper is organized as follows. In
Section 2 we demonstrate several examples of an-
notated search queries. Then, in Section 3, we in-
troduce our joint query annotation method. In Sec-
tion 4 we describe two types of independent query
annotations that are used as input for the joint query
annotation. Section 5 details the related work and
Section 6 presents the experimental results. We draw
the conclusions from our work in Section 7.
2 Query Annotation Example
To demonstrate a possible implementation of lin-
guistic annotation for search queries, Figure 1
presents a simple mark-up scheme, exemplified us-
ing three web search queries (as they appear in a
search log): (a) who won the 2004 kentucky derby,
(b) kindred where would i be, and (c) shih tzu health
problems. In this scheme, each query is marked-
up using three annotations: capitalization, POS tags,
and segmentation indicators.
Note that all the query terms are non-capitalized,
and no punctuation is provided by the user, which
complicates the query annotation process. While
the simple annotation described in Figure 1 can be
done with a very high accuracy for standard docu-
ment corpora, both previous work (Barr et al, 2008;
Bergsma and Wang, 2007; Jones and Fain, 2003)
and the experimental results in this paper indicate
that it is challenging to perform well on queries.
The queries in Figure 1 illustrate this point. Query
(a) in Figure 1 is a wh-question, and it contains
103
a capitalized concept (?Kentucky Derby?), a single
verb, and four segments. Query (b) is a combination
of an artist name and a song title and should be inter-
preted as Kindred ? ?Where Would I Be?. Query (c)
is a concatenation of two short noun phrases: ?Shih
Tzu? and ?health problems?.
3 Joint Query Annotation
Given a search query Q, which consists of a se-
quence of terms (q1, . . . , qn), our goal is to anno-
tate it with an appropriate set of linguistic structures
ZQ. In this work, we assume that the setZQ consists
of shallow sequence annotations zQ, each of which
takes the form
zQ = (?1, . . . , ?n).
In other words, each symbol ?i ? zQ annotates a
single query term.
Many query annotations that are useful for IR
can be represented using this simple form, includ-
ing capitalization, POS tagging, phrase chunking,
named entity recognition, and stopword indicators,
to name just a few. For instance, Figure 1 demon-
strates an example of a set of annotations ZQ. In
this example,
ZQ = {CAP,TAG,SEG}.
Most previous work on query annotation makes
the independence assumption ? every annotation
zQ ? ZQ is done separately from the others. That is,
it is assumed that the optimal linguistic annotation
z?(I)Q is the annotation that has the highest probabil-
ity given the query Q, regardless of the other anno-
tations in the set ZQ. Formally,
z?(I)Q = argmax
zQ
p(zQ|Q) (1)
The main shortcoming of this approach is in the
assumption that the linguistic annotations in the set
ZQ are independent. In practice, there are depen-
dencies between the different annotations, and they
can be leveraged to derive a better estimate of the
entire set of annotations.
For instance, imagine that we need to perform two
annotations: capitalization and POS tagging. Know-
ing that a query term is capitalized, we are more
likely to decide that it is a proper noun. Vice versa,
knowing that it is a preposition will reduce its proba-
bility of being capitalized. We would like to capture
this intuition in the annotation process.
To address the problem of joint query annotation,
we first assume that we have an initial set of annota-
tions Z?(I)Q , which were performed for query Q in-
dependently of one another (we will show an exam-
ple of how to derive such a set in Section 4). Given
the initial set Z?(I)Q , we are interested in obtaining
an annotation set Z?(J)Q , which jointly optimizes the
probability of all the annotations, i.e.
Z?(J)Q = argmax
ZQ
p(ZQ|Z?(I)Q ).
If the initial set of estimations is reasonably ac-
curate, we can make the assumption that the anno-
tations in the set Z?(J)Q are independent given the
initial estimates Z?(I)Q , allowing us to separately op-
timize the probability of each annotation z?(J)Q ?
Z?(J)Q :
z?(J)Q = argmax
zQ
p(zQ|Z?(I)Q ). (2)
From Eq. 2, it is evident that the joint an-
notation task becomes that of finding some opti-
mal unobserved sequence (annotation z?(J)Q ), given
the observed sequences (independent annotation set
Z?(I)Q ).
Accordingly, we can directly use a supervised se-
quential probabilistic model such as CRF (Lafferty
et al, 2001) to find the optimal z?(J)Q . In this CRF
model, the optimal annotation z?(J)Q is the label we
are trying to predict, and the set of independent an-
notations Z?(I)Q is used as the basis for the features
used for prediction. Figure 2 outlines the algorithm
for performing the joint query annotation.
As input, the algorithm receives a training set of
queries and their ground truth annotations. It then
produces a set of independent annotation estimates,
which are jointly used, together with the ground
truth annotations, to learn a CRF model for each an-
notation type. Finally, these CRF models are used
to predict annotations on a held-out set of queries,
which are the output of the algorithm.
104
Input: Qt ? training set of queries.
ZQt ? ground truth annotations for the training set of queries.
Qh ? held-out set of queries.
(1) Obtain a set of independent annotation estimates Z?(I)Qt
(2) Initialize Z?(J)Qt ? ?
(3) for each z?(I)Qt ? Z
?(I)
Qt :
(4) Z ?Qt ? Z
?(I)
Qt \ z
?(I)
Qt
(5) Train a CRF model CRF(zQt) using zQt as a label and Z ?Qt as features.
(6) Predict annotation z?(J)Qh , using CRF(zQt).
(7) Z?(J)Qh ? Z
?(J)
Qh ? z
?(J)
Qh .
Output: Z?(J)Qh ? predicted annotations for the held-out set of queries.
Figure 2: Algorithm for performing joint query annotation.
Note that this formulation of joint query anno-
tation can be viewed as a stacked classification, in
which a second, more effective, classifier is trained
using the labels inferred by the first classifier as fea-
tures. Stacked classifiers were recently shown to be
an efficient and effective strategy for structured clas-
sification in NLP (Nivre and McDonald, 2008; Mar-
tins et al, 2008).
4 Independent Query Annotations
While the joint annotation method proposed in Sec-
tion 3 is general enough to be applied to any set of
independent query annotations, in this work we fo-
cus on two previously proposed independent anno-
tation methods based on either the query itself, or
the top sentences retrieved in response to the query
(Bendersky et al, 2010). The main benefits of these
two annotation methods are that they can be easily
implemented using standard software tools, do not
require any labeled data, and provide reasonable an-
notation accuracy. Next, we briefly describe these
two independent annotation methods.
4.1 Query-based estimation
The most straightforward way to estimate the con-
ditional probabilities in Eq. 1 is using the query it-
self. To make the estimation feasible, Bendersky et
al. (2010) take a bag-of-words approach, and assume
independence between both the query terms and the
corresponding annotation symbols. Thus, the inde-
pentent annotations in Eq. 1 are given by
z?(QRY )Q = argmax
(?1,...,?n)
?
i?(1,...,n)
p(?i|qi). (3)
Following Bendersky et al (2010) we use a large
n-gram corpus (Brants and Franz, 2006) to estimate
p(?i|qi) for annotating the query with capitalization
and segmentation mark-up, and a standard POS tag-
ger1 for part-of-speech tagging of the query.
4.2 PRF-based estimation
Given a short, often ungrammatical query, it is hard
to accurately estimate the conditional probability in
Eq. 1 using the query terms alone. For instance, a
keyword query hawaiian falls, which refers to a lo-
cation, is inaccurately interpreted by a standard POS
tagger as a noun-verb pair. On the other hand, given
a sentence from a corpus that is relevant to the query
such as ?Hawaiian Falls is a family-friendly water-
park?, the word ?falls? is correctly identified by a
standard POS tagger as a proper noun.
Accordingly, the document corpus can be boot-
strapped in order to better estimate the query anno-
tation. To this end, Bendersky et al (2010) employ
the pseudo-relevance feedback (PRF) ? a method
that has a long record of success in IR for tasks such
as query expansion (Buckley, 1995; Lavrenko and
Croft, 2001).
In the most general form, given the set of all re-
trievable sentences r in the corpus C one can derive
p(zQ|Q) =
?
r?C
p(zQ|r)p(r|Q).
Since for most sentences the conditional proba-
bility of relevance to the query p(r|Q) is vanish-
ingly small, the above can be closely approximated
1http://crftagger.sourceforge.net/
105
by considering only a set of sentences R, retrieved
at top-k positions in response to the query Q. This
yields
p(zQ|Q) ?
?
r?R
p(zQ|r)p(r|Q).
Intuitively, the equation above models the query as
a mixture of top-k retrieved sentences, where each
sentence is weighted by its relevance to the query.
Furthermore, to make the estimation of the condi-
tional probability p(zQ|r) feasible, it is assumed that
the symbols ?i in the annotation sequence are in-
dependent, given a sentence r. Note that this as-
sumption differs from the independence assumption
in Eq. 3, since here the annotation symbols are not
independent given the query Q.
Accordingly, the PRF-based estimate for indepen-
dent annotations in Eq. 1 is
z?(PRF )Q = argmax
(?1,...,?n)
?
r?R
?
i?(1,...,n)
p(?i|r)p(r|Q).
(4)
Following Bendersky et al (2010), an estimate of
p(?i|r) is a smoothed estimator that combines the
information from the retrieved sentence r with the
information about unigrams (for capitalization and
POS tagging) and bigrams (for segmentation) from
a large n-gram corpus (Brants and Franz, 2006).
5 Related Work
In recent years, linguistic annotation of search
queries has been receiving increasing attention as an
important step toward better query processing and
understanding. The literature on query annotation
includes query segmentation (Bergsma and Wang,
2007; Jones et al, 2006; Guo et al, 2008; Ha-
gen et al, 2010; Hagen et al, 2011; Tan and Peng,
2008), part-of-speech and semantic tagging (Barr et
al., 2008; Manshadi and Li, 2009; Li, 2010), named-
entity recognition (Guo et al, 2009; Lu et al, 2009;
Shen et al, 2008; Pas?ca, 2007), abbreviation disam-
biguation (Wei et al, 2008) and stopword detection
(Lo et al, 2005; Jones and Fain, 2003).
Most of the previous work on query annotation
focuses on performing a particular annotation task
(e.g., segmentation or POS tagging) in isolation.
However, these annotations are often related, and
thus we take a joint annotation approach, which
combines several independent annotations to im-
prove the overall annotation accuracy. A similar ap-
proach was recently proposed by Guo et al (2008).
There are several key differences, however, between
the work presented here and their work.
First, Guo et al (2008) focus on query refine-
ment (spelling corrections, word splitting, etc.) of
short keyword queries. Instead, we are interested
in annotation of queries of different types, includ-
ing verbose natural language queries. While there
is an overlap between query refinement and annota-
tion, the focus of the latter is on providing linguistic
information about existing queries (after initial re-
finement has been performed). Such information is
especially important for more verbose and gramat-
ically complex queries. In addition, while all the
methods proposed by Guo et al (2008) require large
amounts of training data (thousands of training ex-
amples), our joint annotation method can be effec-
tively trained with a minimal human labeling effort
(several hundred training examples).
An additional research area which is relevant to
this paper is the work on joint structure model-
ing (Finkel and Manning, 2009; Toutanova et al,
2008) and stacked classification (Nivre and Mc-
Donald, 2008; Martins et al, 2008) in natural lan-
guage processing. These approaches have been
shown to be successful for tasks such as parsing and
named entity recognition in newswire data (Finkel
and Manning, 2009) or semantic role labeling in the
Penn Treebank and Brown corpus (Toutanova et al,
2008). Similarly to this work in NLP, we demon-
strate that a joint approach for modeling the linguis-
tic query structure can also be beneficial for IR ap-
plications.
6 Experiments
6.1 Experimental Setup
For evaluating the performance of our query anno-
tation methods, we use a random sample of 250
queries2 from a search log. This sample is manually
labeled with three annotations: capitalization, POS
tags, and segmentation, according to the description
of these annotations in Figure 1. In this set of 250
queries, there are 93 questions, 96 phrases contain-
2The annotations are available at
http://ciir.cs.umass.edu/?bemike/data.html
106
CAP
F1 (% impr) MQA (% impr)
i-QRY 0.641 (-/-) 0.779 (-/-)
i-PRF 0.711?(+10.9/-) 0.811?(+4.1/-)
j-QRY 0.620?(-3.3/-12.8) 0.805?(+3.3/-0.7)
j-PRF 0.718?(+12.0/+0.9) 0.840??(+7.8/+3.6)
TAG
Acc. (% impr) MQA (% impr)
i-QRY 0.893 (-/-) 0.878 (-/-)
i-PRF 0.916?(+2.6/-) 0.914?(+4.1/-)
j-QRY 0.913?(+2.2/-0.3) 0.912?(+3.9/-0.2)
j-PRF 0.924?(+3.5/+0.9) 0.922?(+5.0/+0.9)
SEG
F1 (% impr) MQA (% impr)
i-QRY 0.694 (-/-) 0.672 (-/-)
i-PRF 0.753?(+8.5/-) 0.710?(+5.7/-)
j-QRY 0.817??(+17.7/+8.5) 0.803??(+19.5/+13.1)
j-PRF 0.819??(+18.0/+8.8) 0.803??(+19.5/+13.1)
Table 1: Summary of query annotation performance for
capitalization (CAP), POS tagging (TAG) and segmenta-
tion. Numbers in parentheses indicate % of improvement
over the i-QRY and i-PRF baselines, respectively. Best
result per measure and annotation is boldfaced. ? and ?
denote statistically significant differences with i-QRY and
i-PRF, respectively.
ing a verb, and 61 short keyword queries (Figure 1
contains a single example of each of these types).
In order to test the effectiveness of the joint query
annotation, we compare four methods. In the first
two methods, i-QRY and i-PRF the three annotations
are done independently. Method i-QRY is based on
z?(QRY )Q estimator (Eq. 3). Method i-PRF is based
on the z?(PRF )Q estimator (Eq. 4).
The next two methods, j-QRY and j-PRF, are joint
annotation methods, which perform a joint optimiza-
tion over the entire set of annotations, as described
in the algorithm in Figure 2. j-QRY and j-PRF differ
in their choice of the initial independent annotation
set Z?(I)Q in line (1) of the algorithm (see Figure 2).
j-QRY uses only the annotations performed by i-
QRY (3 initial independent annotation estimates),
while j-PRF combines the annotations performed by
i-QRY with the annotations performed by i-PRF (6
initial annotation estimates). The CRF model train-
ing in line (6) of the algorithm is implemented using
CRF++ toolkit3.
3http://crfpp.sourceforge.net/
The performance of the joint annotation methods
is estimated using a 10-fold cross-validation. In or-
der to test the statistical significance of improve-
ments attained by the proposed methods we use a
two-sided Fisher?s randomization test with 20,000
permutations. Results with p-value < 0.05 are con-
sidered statistically significant.
For reporting the performance of our meth-
ods we use two measures. The first measure is
classification-oriented ? treating the annotation de-
cision for each query term as a classification. In case
of capitalization and segmentation annotations these
decisions are binary and we compute the precision
and recall metrics, and report F1 ? their harmonic
mean. In case of POS tagging, the decisions are
ternary, and hence we report the classification ac-
curacy.
We also report an additional, IR-oriented perfor-
mance measure. As is typical in IR, we propose
measuring the performance of the annotation meth-
ods on a per-query basis, to verify that the methods
have uniform impact across queries. Accordingly,
we report the mean of classification accuracies per
query (MQA). Formally, MQA is computed as
?N
i=1 accQi
N ,
where accQi is the classification accuracy for query
Qi, and N is the number of queries.
The empirical evaluation is conducted as follows.
In Section 6.2, we discuss the general performance
of the four annotation techniques, and compare the
effectiveness of independent and joint annotations.
In Section 6.3, we analyze the performance of the
independent and joint annotation methods by query
type. In Section 6.4, we compare the difficulty
of performing query annotations for different query
types. Finally, in Section 6.5, we compare the effec-
tiveness of the proposed joint annotation for query
segmentation with the existing query segmentation
methods.
6.2 General Evaluation
Table 1 shows the summary of the performance of
the two independent and two joint annotation meth-
ods for the entire set of 250 queries. For independent
methods, we see that i-PRF outperforms i-QRY for
107
CAP Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.750 0.862 0.590 0.839 0.784 0.687
j-PRF 0.687?(-8.4%) 0.839?(-2.7%) 0.671?(+13.7%) 0.913?(+8.8%) 0.814 (+3.8%) 0.732? (+6.6%)
TAG Verbal Phrases Questions Keywords
Acc. MQA Acc. MQA Acc. MQA
i-PRF 0.908 0.908 0.932 0.935 0.880 0.890
j-PRF 0.904 (-0.4%) 0.906 (-0.2%) 0.951? (+2.1%) 0.953? (+1.9%) 0.893 (+1.5%) 0.900 (+1.1%)
SEG Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.751 0.700 0.740 0.700 0.816 0.747
j-PRF 0.772 (+2.8%) 0.742?(+6.0%) 0.858?(+15.9%) 0.838?(+19.7%) 0.844 (+3.4%) 0.853?(+14.2%)
Table 2: Detailed analysis of the query annotation performance for capitalization (CAP), POS tagging (TAG) and
segmentation by query type. Numbers in parentheses indicate % of improvement over the i-PRF baseline. Best result
per measure and annotation is boldfaced. ? denotes statistically significant differences with i-PRF.
all annotation types, using both performance mea-
sures.
In Table 1, we can also observe that the joint anno-
tation methods are, in all cases, better than the cor-
responding independent ones. The highest improve-
ments are attained by j-PRF, which always demon-
strates the best performance both in terms of F1 and
MQA. These results attest to both the importance of
doing a joint optimization over the entire set of an-
notations and to the robustness of the initial annota-
tions done by the i-PRF method. In all but one case,
the j-PRF method, which uses these annotations as
features, outperforms the j-QRY method that only
uses the annotation done by i-QRY .
The most significant improvements as a result of
joint annotation are observed for the segmentation
task. In this task, joint annotation achieves close to
20% improvement in MQA over the i-QRY method,
and more than 10% improvement in MQA over the i-
PRF method. These improvements indicate that the
segmentation decisions are strongly guided by cap-
italization and POS tagging. We also note that, in
case of segmentation, the differences in performance
between the two joint annotation methods, j-QRY
and j-PRF, are not significant, indicating that the
context of additional annotations in j-QRY makes up
for the lack of more robust pseudo-relevance feed-
back based features.
We also note that the lowest performance im-
provement as a result of joint annotation is evi-
denced for POS tagging. The improvements of joint
annotation method j-PRF over the i-PRF method are
less than 1%, and are not statistically significant.
This is not surprising, since the standard POS tag-
gers often already use bigrams and capitalization at
training time, and do not acquire much additional
information from other annotations.
6.3 Evaluation by Query Type
Table 2 presents a detailed analysis of the perfor-
mance of the best independent (i-PRF) and joint (j-
PRF) annotation methods by the three query types
used for evaluation: verbal phrases, questions and
keyword queries. From the analysis in Table 2, we
note that the contribution of joint annotation varies
significantly across query types. For instance, us-
ing j-PRF always leads to statistically significant im-
provements over the i-PRF baseline for questions.
On the other hand, it is either statistically indistin-
guishable, or even significantly worse (in the case of
capitalization) than the i-PRF baseline for the verbal
phrases.
Table 2 also demonstrates that joint annotation
has a different impact on various annotations for the
same query type. For instance, j-PRF has a signif-
icant positive effect on capitalization and segmen-
tation for keyword queries, but only marginally im-
proves the POS tagging. Similarly, for the verbal
phrases, j-PRF has a significant positive effect only
for the segmentation annotation.
These variances in the performance of the j-PRF
method point to the differences in the structure be-
108
Annotation Performance by Query Type
F1
Verbal Phrases Questions Keyword Queries
60
65
70
75
80
85
90
95
10
0
CAP
SEG
TAG
Figure 3: Comparative performance (in terms of F1 for
capitalization and segmentation and accuracy for POS
tagging) of the j-PRF method on the three query types.
tween the query types. While dependence between
the annotations plays an important role for question
and keyword queries, which often share a common
grammatical structure, this dependence is less use-
ful for verbal phrases, which have a more diverse
linguistic structure. Accordingly, a more in-depth
investigation of the linguistic structure of the verbal
phrase queries is an interesting direction for future
work.
6.4 Annotation Difficulty
Recall that in our experiments, out of the overall 250
annotated queries, there are 96 verbal phrases, 93
questions and 61 keyword queries. Figure 3 shows a
plot that contrasts the relative performance for these
three query types of our best-performing joint an-
notation method, j-PRF, on capitalization, POS tag-
ging and segmentation annotation tasks. Next, we
analyze the performance profiles for the annotation
tasks shown in Figure 3.
For the capitalization task, the performance of j-
PRF on verbal phrases and questions is similar, with
the difference below 3%. The performance for key-
word queries is much higher ? with improvement
over 20% compared to either of the other two types.
We attribute this increase to both a larger number
of positive examples in the short keyword queries
(a higher percentage of terms in keyword queries is
capitalized) and their simpler syntactic structure (ad-
SEG F1 MQA
SEG-1 0.768 0.754
SEG-2 0.824? 0.787?
j-PRF 0.819? (+6.7%/-0.6%) 0.803? (+6.5%/+2.1%)
Table 3: Comparison of the segmentation performance
of the j-PRF method to two state-of-the-art segmentation
methods. Numbers in parentheses indicate % of improve-
ment over the SEG-1 and SEG-2 baselines respectively.
Best result per measure and annotation is boldfaced. ?
denotes statistically significant differences with SEG-1.
jacent terms in these queries are likely to have the
same case).
For the segmentation task, the performance is at
its best for the question and keyword queries, and at
its worst (with a drop of 11%) for the verbal phrases.
We hypothesize that this is due to the fact that ques-
tion queries and keyword queries tend to have repet-
itive structures, while the grammatical structure for
verbose queries is much more diverse.
For the tagging task, the performance profile is re-
versed, compared to the other two tasks ? the per-
formance is at its worst for keyword queries, since
their grammatical structure significantly differs from
the grammatical structure of sentences in news arti-
cles, on which the POS tagger is trained. For ques-
tion queries the performance is the best (6% increase
over the keyword queries), since they resemble sen-
tences encountered in traditional corpora.
It is important to note that the results reported in
Figure 3 are based on training the joint annotation
model on all available queries with 10-fold cross-
validation. We might get different profiles if a sep-
arate annotation model was trained for each query
type. In our case, however, the number of queries
from each type is not sufficient to train a reliable
model. We leave the investigation of separate train-
ing of joint annotation models by query type to fu-
ture work.
6.5 Additional Comparisons
In order to further evaluate the proposed joint an-
notation method, j-PRF, in this section we compare
its performance to other query annotation methods
previously reported in the literature. Unfortunately,
there is not much published work on query capi-
talization and query POS tagging that goes beyond
the simple query-based methods described in Sec-
109
tion 4.1. The published work on the more advanced
methods usually requires access to large amounts of
proprietary user data such as query logs and clicks
(Barr et al, 2008; Guo et al, 2008; Guo et al, 2009).
Therefore, in this section we focus on recent work
on query segmentation (Bergsma and Wang, 2007;
Hagen et al, 2010). We compare the segmentation
effectiveness of our best performing method, j-PRF,
to that of these query segmentation methods.
The first method, SEG-1, was first proposed by
Hagen et al (2010). It is currently the most effective
publicly disclosed unsupervised query segmentation
method. SEG-1 method requires an access to a large
web n-gram corpus (Brants and Franz, 2006). The
optimal segmentation for query Q, S?Q, is then ob-
tained using
S?Q = argmax
S?SQ
?
s?S,|s|>1
|s||s|count(s),
where SQ is the set of all possible query segmenta-
tions, S is a possible segmentation, s is a segment
in S, and count(s) is the frequency of s in the web
n-gram corpus.
The second method, SEG-2, is based on a success-
ful supervised segmentation method, which was first
proposed by Bergsma and Wang (2007). SEG-2 em-
ploys a large set of features, and is pre-trained on the
query collection described by Bergsma and Wang
(2007). The features used by the SEG-2 method are
described by Bendersky et al (2009), and include,
among others, n-gram frequencies in a sample of a
query log, web corpus and Wikipedia titles.
Table 3 demonstrates the comparison between the
j-PRF, SEG-1 and SEG-2 methods. When com-
pared to the SEG-1 baseline, j-PRF is significantly
more effective, even though it only employs bigram
counts (see Eq. 4), instead of the high-order n-grams
used by SEG-1, for computing the score of a seg-
mentation. This results underscores the benefit of
joint annotation, which leverages capitalization and
POS tagging to improve the quality of the segmen-
tation.
When compared to the SEG-2 baseline, j-PRF
and SEG-2 are statistically indistinguishable. SEG-2
posits a slightly better F1, while j-PRF has a better
MQA. This result demonstrates that the segmenta-
tion produced by the j-PRF method is as effective as
the segmentation produced by the current supervised
state-of-the-art segmentation methods, which em-
ploy external data sources and high-order n-grams.
The benefit of the j-PRF method compared to the
SEG-2 method, is that, simultaneously with the seg-
mentation, it produces several additional query an-
notations (in this case, capitalization and POS tag-
ging), eliminating the need to construct separate se-
quence classifiers for each annotation.
7 Conclusions
In this paper, we have investigated a joint approach
for annotating search queries with linguistic struc-
tures, including capitalization, POS tags and seg-
mentation. To this end, we proposed a probabilis-
tic approach for performing joint query annotation
that takes into account the dependencies that exist
between the different annotation types.
Our experimental findings over a range of queries
from a web search log unequivocally point to the su-
periority of the joint annotation methods over both
query-based and pseudo-relevance feedback based
independent annotation methods. These findings in-
dicate that the different annotations are mutually-
dependent.
We are encouraged by the success of our joint
query annotation technique, and intend to pursue the
investigation of its utility for IR applications. In the
future, we intend to research the use of joint query
annotations for additional IR tasks, e.g., for con-
structing better query formulations for ranking al-
gorithms.
8 Acknowledgment
This work was supported in part by the Center for In-
telligent Information Retrieval and in part by ARRA
NSF IIS-9014442. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect those of the sponsor.
110
References
Niranjan Balasubramanian and James Allan. 2009. Syn-
tactic query models for restatement retrieval. In Proc.
of SPIRE, pages 143?155.
Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of english web-search queries. In
Proc. of EMNLP, pages 1021?1030.
Michael Bendersky and W. Bruce Croft. 2009. Analysis
of long queries in a large scale search log. In Proc. of
Workshop on Web Search Click Data, pages 8?14.
Michael Bendersky, David Smith, and W. Bruce Croft.
2009. Two-stage query segmentation for information
retrieval. In Proc. of SIGIR, pages 810?811.
Michael Bendersky, W. Bruce Croft, and David A. Smith.
2010. Structural annotation of search queries using
pseudo-relevance feedback. In Proc. of CIKM, pages
1537?1540.
Shane Bergsma and Qin I. Wang. 2007. Learning noun
phrase query segmentation. In Proc. of EMNLP, pages
819?826.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Chris Buckley. 1995. Automatic query expansion using
SMART. In Proc. of TREC-3, pages 69?80.
Jenny R. Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc.
of NAACL, pages 326?334.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Proc. of SIGIR, pages 379?386.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proc. of SIGIR,
pages 267?274.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Braeutigam. 2010. The power of naive query
segmentation. In Proc. of SIGIR, pages 797?798.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query segmentation re-
visited. In Proc. of WWW, pages 97?106.
Rosie Jones and Daniel C. Fain. 2003. Query word dele-
tion prediction. In Proc. of SIGIR, pages 435?436.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proc. of WWW, pages 387?396.
Giridhar Kumaran and James Allan. 2007. A case for
shorter queries, and helping user create them. In Proc.
of NAACL, pages 220?227.
Giridhar Kumaran and Vitor R. Carvalho. 2009. Re-
ducing long queries using query quality predictors. In
Proc. of SIGIR, pages 564?571.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. of ICML, pages 282?289.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance
based language models. In Proc. of SIGIR, pages 120?
127.
Matthew Lease. 2007. Natural language processing for
information retrieval: the time is ripe (again). In Pro-
ceedings of PIKM.
Xiao Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proc. of ACL, pages 1337?
1345, Morristown, NJ, USA.
Rachel T. Lo, Ben He, and Iadh Ounis. 2005. Auto-
matically building a stopword list for an information
retrieval system. In Proc. of DIR.
Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, and
Benoit Dumoulin. 2009. Improving Web search rel-
evance with semantic features. In Proc. of EMNLP,
pages 648?657.
Mehdi Manshadi and Xiao Li. 2009. Semantic Tagging
of Web Search Queries. In Proc. of ACL, pages 861?
869.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP, pages 157?166.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL, pages 950?958.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM, pages 683?690.
Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz,
and Ying Li. 2008. Personal name classification in
web queries. In Proc. of WSDM, pages 149?158.
Bin Tan and Fuchun Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In Proc. of WWW, pages 347?356.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34:161?191,
June.
Xing Wei, Fuchun Peng, and Benoit Dumoulin. 2008.
Analyzing web text association to disambiguate abbre-
viation in queries. In Proc. of SIGIR, pages 751?752.
111
Workshop on Computational Linguistics for Literature, pages 69?77,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Dictionary of Wisdom and Wit:
Learning to Extract Quotable Phrases?
Michael Bendersky
Dept. of Computer Science
University of Massachusetts
Amherst, MA
bemike@cs.umass.edu
David A. Smith
Dept. of Computer Science
University of Massachusetts
Amherst, MA
dasmith@cs.umass.edu
Abstract
Readers suffering from information overload
have often turned to collections of pithy and
famous quotations. While research on large-
scale analysis of text reuse has found effective
methods for detecting widely disseminated
and famous quotations, this paper explores the
complementary problem of detecting, from
internal evidence alone, which phrases are
quotable. These quotable phrases are mem-
orable and succinct statements that people are
likely to find useful outside of their original
context. We evaluate quotable phrase extrac-
tion using a large digital library and demon-
strate that an integration of lexical and shallow
syntactic features results in a reliable extrac-
tion process. A study using a reddit commu-
nity of quote enthusiasts as well as a simple
corpus analysis further demonstrate the prac-
tical applications of our work.
1 Introduction
Readers have been anxious about information over-
load for a long time: not only since the rise of the
web, but with the earlier explosion of printed books,
and even in manuscript culture (Blair, 2010). One
traditional response to the problem has been ex-
cerpting passages that might be useful outside their
original sources, copying them into personal com-
monplace books, and publishing them in dictionaries
such as Bartlett?s Familiar Quotations or the Oxford
?
?The book is a dictionary of wisdom and wit...? (Samuel
Smiles, ?A Publisher and His Friends?) This and all the subse-
quent quotations in this paper were discovered by the proposed
quotable phrase extraction process.
Dictionary of Quotations. Even on the web, collec-
tion of quotable phrases continues to thrive1, as evi-
denced by the popularity of quotation websites such
as BrainyQuote and Wikiquote.
According to a recent estimate, there are close
to 130 million unique book records in world li-
braries today (Taycher, 2010). Many of these
books are being digitized and stored by commercial
providers (e.g., Google Books and Amazon), as well
as non-profit organizations (e.g., Internet Archive
and Project Gutenberg).
As a result of this digitization, the development
of new methods for preserving, accessing and ana-
lyzing the contents of literary corpora becomes an
important research venue with many practical appli-
cations (Michel et al, 2011). One particularly in-
teresting line of work in these large digital libraries
has focused on detecting text reuse, i.e., passages
from one source that are quoted in another (Kolak
and Schilit, 2008).
In contrast, in this paper we explore the modeling
of phrases that are likely to be quoted. This phrase
modeling is done based on internal evidence alone,
regardless of whether or not the phrase actually is
quoted in existing texts.
We call such potential quotation a quotable
phrase ? a meaningful, memorable, and succinct
statement that can be quoted without its original
context. This kind of phrases includes aphorisms,
epigrams, maxims, proverbs, and epigraphs.
1
?Nothing is so pleasant as to display your worldly wis-
dom in epigram and dissertation, but it is a trifle tedious to
hear another person display theirs.? (Kate Sanborn, ?The Wit
of Women?)
69
Book 
Sentence 
Segmentation
Naive Bayes 
Filtering
Quotable Phrase 
Detection
Labeled 
Quotation Set
External
Quotation Set
Extracted 
Quotes
Figure 1: Diagram of the quotable phrase extraction process.
A computational approach to quotable phrase ex-
traction has several practical applications. For in-
stance, it can be used to recommend new additions to
existing quotable phrase collections, especially fo-
cusing on lesser read and studied authors and liter-
ary works2. It can also generate quotable phrases
that will serve as catchy and entertaining previews
for book promotion and advertisement3 .
In this work, we describe such a computational
approach to quotable phrase extraction. Our ap-
proach leverages the Project Gutenberg digital li-
brary and an online collection of quotations to build
a quotable language model. This language model
is further refined by a supervised learning algorithm
that combines lexical and shallow syntactic features.
In addition, we demonstrate that a computational
approach can help to address some intriguing ques-
tions about the nature of quotability. What are the
lexical and the syntactic features that govern the
quotability of a phrase? Which authors and books
are highly quotable? How much variance is there in
the perceived quotability of a given phrase?
The remainder of this paper is organized as fol-
lows. In Section 2 we provide a detailed description
of the entire process of quotable phrase extraction.
In Section 3 we review the related work. In Sections
4 and 5 we evaluate the quotable phrase extraction
process, and provide some corpus quotability analy-
sis. We conclude the paper in Section 6.
2 Quotable Phrase Extraction
There are three unique challenges that need to be
addressed in the design of the process of quotable
2
?There is life in a poet so long as he is quoted...? (Sir Alfred
Comyn Lyall, ?Studies in Literature and History?)
3As an example, see the ?Popular Highlights? feature for
Kindle e-books in the Amazon bookstore.
phrase extraction. The first challenge stems from the
fact that the boundaries of potential quotes are often
ambiguous. A quotable phrase can consist of a sen-
tence fragment, a complete sentence, or a passage of
text that spans several sentences.
The second challenge is that the occurrence of
quotable phrases is a rare phenomena in literary cor-
pora. A randomly selected book passage is unlikely
to be quotable without any additional context.
The third challenge is related to the syntax and se-
mantics of quotable phrases. For instance, consider
the phrase
?Evil men make evil use of the law, though
the law is good, while good men die well, al-
though death is an evil.? (Thomas Aquinas,
?Summa Theologica?)
and contrast it with
?Of the laws that he can see, the great se-
quences of life to death, of evil to sorrow,
of goodness to happiness, he tells in burning
words.? (Henry Fielding, ?The Soul of a Peo-
ple?)
While both of these phrases share a common vocab-
ulary (law, death, good and evil), the latter sentence
contains unresolved pronouns (he, twice) that make
it less amenable to quotation out of context.
Accordingly, we design a three-stage quotable
phrase extraction process, with each stage corre-
sponding to one of challenges described above. The
diagram in Figure 1 provides a high-level overview
of the entire extraction process on a single book.
Next, we provide a brief description of this diagram.
Then, in the following sections, we focus on individ-
ual stages of the extraction process.
To address the first challenge of quote boundary
detection, at the first stage of the extraction process
70
(Sentence Segmentation) we segment the text of the
input book into sentences using an implementation
of the Punkt sentence boundary detection algorithm
(Kiss and Strunk, 2006). In an initial experiment, we
found that 78% of the approximately 4,000 quota-
tions collected from the QuotationsPage4 consist of
a single sentence. From now on, therefore, we make
a simplifying assumption that an extracted quotable
phrase is confined within the sentence boundaries.
The second processing stage (Na??ve Bayes Filter-
ing) aims to address the second challenge (the rar-
ity of quotable phrases) and significantly increases
the ratio of quotable phrases that are considered as
candidates in the final processing stage (Quotable
Phrase Detection). To this end, we use a set of quo-
tations collected from an external resource to build a
quotable language model. Only sentences that have
a sufficiently high likelihood of being drawn from
this language model are considered at the final pro-
cessing stage.
For this final processing stage (Quotable Phrase
Detection), we develop a supervised algorithm that
focuses on the third challenge, and analyzes the syn-
tactic structure of the input sentences. This super-
vised algorithm makes use of structural and syntac-
tic features that may effect phrase quotability, in ad-
dition to the vocabulary of the phrase.
2.1 Na??ve Bayes Filtering
In order to account for the rarity of quotable phrases
in the book corpus, we use a filtering approach based
on a pre-built quotable language model. Using this
filtering approach, we significantly reduce the num-
ber of sentences that need to be considered in the su-
pervised quotable phrase detection stage (described
in Section 2.2). In addition, this approach increases
the ratio of quotable phrases considered at the super-
vised stage, addressing the problem of the sparsity of
positive examples.
To build the quotable language model, we boot-
strap the existing quotation collections on the web.
In particular, we collect approximately 4,000 quotes
on more than 200 subjects from the QuotationsPage.
This collection provides a diverse set of high-quality
quotations on subjects ranging from Laziness and
Genius to Technology and Taxes.
4www.quotationspage.com
Then, we build two separate unigram language
models. The first one is the quotable language
model, which is built using the collected quotations
(LQ). The second one is the background language
model, which is built using the entire book corpus
(LC). Using these language models we compute a
log-likelihood ratio for each processed sentence s,
as
LLRs =
?
w?s
ln p(w|LQ)p(w|LC)
, (1)
where the probabilities p(w|?) are computed using a
maximum likelihood estimate with add-one smooth-
ing.
A sentence s is allowed to pass the filtering stage
if and only if LLRs ? [?, ?], where ?, ? are posi-
tive constants5. The lower bound on the LLRs, ?,
requires the sentence to be highly probable given the
quotable language model LQ. The upper bound on
the LLRs, ?, filters out sentences that are highly im-
probable given the background language model LC .
Finally, the sentences for which LLRs ? [?, ?]
are allowed to pass through the Na??ve Bayes filter.
They are forwarded to the next stage, in which a su-
pervised quotable phrase detection is performed.
2.2 Supervised Quotable Phrase Detection
In a large corpus, a supervised quotable phrase de-
tection method needs to handle a significant num-
ber of input instances (in our corpus, an average-
sized book contains approximately 2,000 sentences).
Therefore, we make use of a simple and efficient
perceptron algorithm, which is implemented follow-
ing the description by Bishop (2006).
We note, however, that the proposed supervised
detection method can be also implemented using a
variety of other binary prediction techniques. In
an initial experiment, we found that more complex
methods (e.g., decision trees) were comparable to or
worse than the simple perceptron algorithm.
Formally, we define a binary function f(s) which
determines whether an input sentence s is a quotable
(q) or a non-quotable (q) phrase, based on:
f(s) =
{
q if wxs > 0
q else, (2)
5In this work, we set ? = 1, ? = 25. This setting is done
prior to seeing any labeled data.
71
Feature Description
Lexical
LLR Sentence log-likelihood ratio (Eq. 1)
#word Number of words in s.
#char Number of characters in s.
wordLenAgg Feature for each aggregate Agg of word length in s.
Agg = {min, max, mean}
#capital Number of capitalized words in s.
#quantifier Number of universal quantifiers in s (from a list of 13 quantifiers, e.g., all, whole, nobody).
#stops Number of common stopwords in s.
beginStop True if s begins with a stopword, False otherwise.
hasDialog True if s contains at least one of the three common dialog terms {say, says, said}.
#abstract Number of abstract concepts (e.g., adventure, charity, stupidity ) in s.
Punctuation
hasP Five features to indicate whether punctuation of type P is present in s.
P = {quotations, parentheses, colon, dash, semi-colon}.
Parts of Speech
#POS Four features for the number of occurrences of part-of-speech POS in s.
POS = {noun, verb, adjective, adverb, pronoun}.
hasComp True if s contains a comparative adjective or adverb, False otherwise.
hasSuper True if s contains a superlative adjective or adverb, False otherwise.
hasPP True if s contains a verb in past participle, False otherwise.
#IGSeq[i] Count of the POS sequence with the i-th highest IG(X,Y ) (Eq. 3) in s.
Table 1: Description of the quotability features that are computed for each sentence s .
where xs is a vector of quotability features com-
puted for the sentence s, and w is a weight vector
associated with these features. The weight vector w
is updated using stochastic gradient descent on the
perceptron error function (Bishop, 2006).
Since Eq. 2 demonstrates that the supervised
quotable phrase detection can be formulated as a
standard binary classification problem, its success
will be largely determined by an appropriate choice
of feature vector xs. As we are unaware of any
previous work on supervised detection of quotable
phrases, we develop an initial set of easy-to-compute
features that considers the lexical and shallow syn-
tactic structure of the analyzed sentence.
2.3 Quotability Features
A decision about phrase quotability is often sub-
jective; it is strongly influenced by personal taste
and circumstances6 . Therefore, the set of features
that we describe in this section is merely a coarse-
grained approximation of the true intrinsic qualities
of a quotable phrase. Nevertheless, it is important to
6
?One man?s beauty another?s ugliness; one man?s wisdom
another?s folly.? (Ralph Waldo Emerson, ?Essays?)
note that these features do prove to be beneficial in
the context of the quote detection task, as is demon-
strated by our empirical evaluation in Section 5.
Table 1 details the quotability features, which are
divided into 3 groups: lexical, punctuation-based
and POS-based features. All of these features are
conceptually simple and can be efficiently computed
even for a large number of input sentences.
Some of these features are inspired by existing
text analysis tasks. For instance, work on readabil-
ity detection for the web (Kanungo and Orr, 2009;
Si and Callan, 2001) examined features which are
similar to the lexical features in Table 1. Parts of
speech features (e.g., the presence of comparative
and superlative adjectives and adverbs) have been
extensively used for sentiment analysis and opinion
mining (Pang and Lee, 2008).
In addition, we use a number of features based on
simple hand-crafted word lists. These lists include
word categories that could be potential indicators of
quotable phrases such as universal quantifiers (e.g.,
all, everyone) and abstract concepts7.
7For abstract concept modeling we use a list of 176 abstract
nouns available at www.englishbanana.com.
72
The novel features in Table 1 that are specifically
designed for quotable phrase detection are based on
part of speech sequences that are highly indicative
of quotable (or, conversely, non-quotable) phrase
(features #IGSeq[i]). In order to compute these
features we first manually label a validation set of
500 sentences that passed the Na??ve Bayes Filtering
(Section 2.1). Then, we apply a POS tagger to these
sentences, and for each POS tag sequence of length
n, seqn, we compute its information gain
IG(X,Y ) = H(X) ?H(X|Y ). (3)
In Eq. 3, X is a binary variable indicating the pres-
ence or the absence of seqn in the sentence, and
Y ? {q, q}.
We select k sequences seqn with the highest value
of IG(X,Y )8. We use the count in the sentence of
the sequence seqn with the i-th highest information
gain as the feature #IGSeq[i]. Intuitively, the fea-
tures #IGSeq[i] measure how many POS tag se-
quences that are indicative of a quotable phrase (or,
conversely, indicative of a non-quotable phrase) the
sentence contains.
3 Related Work
The increasing availability of large-scale digital li-
braries resulted in a recent surge of interest in com-
putational approaches to literary analysis. To name
just a few examples, Genzel et al (2010) examined
machine translation of poetry; Elson et al (2010)
extracted conversational networks from Victorian
novels; and Faruqui and Pado? (2011) predicted for-
mal and informal address in English literature.
In addition, computational methods are increas-
ingly used for identification of complex aspects
of writing such as humor (Mihalcea and Pulman,
2007), double-entendre (Kiddon and Brun, 2011)
and sarcasm (Tsur et al, 2010). However, while
successful, most of this work is still limited to an
analysis of a single aspect of writing style.
In this work, we propose a more general compu-
tational approach that attempts to extract quotable
phrases. A quotability of a phrase can be affected
by various aspects of writing including (but not lim-
8In this work, we set n = 3, k = 50. This setting is done
prior to seeing any labeled data.
Number of books 21, 492
Number of authors 8, 889
Total sentences 4.45 ? 107
After Na??ve Bayes filtering 1.75 ? 107
Table 2: Summary of the Project Gutenberg corpus.
ited to) humor and irony9, use of metaphors10 , and
hyperbole11 .
It is important to note that our approach is con-
ceptually different from the previous work on para-
phrase and quote detection in book corpora (Kolak
and Schilit, 2008), news stories (Liang et al, 2010)
and movie scripts (Danescu-Niculescu-Mizil et al,
2012). While this previous work focuses on mining
popular and oft-used quotations, we are mainly in-
terested in discovering quotable phrases that might
have never been quoted by others.
4 Experimental Setup
To evaluate the quotable phrase extraction process
in its entirety (see Figure 1), we use a collection of
Project Gutenberg (PG) books12 ? a popular digital
library containing full texts of public domain books
in a variety of formats. In particular, we harvest the
entire corpus of 21,492 English books in textual for-
mat from the PG website.
The breakdown of the PG corpus is shown in Ta-
ble 2. The number of detected sentences in the PG
corpus exceeds 44 million. Roughly a third of these
sentences are able to pass through the Na??ve Bayes
Filtering (described in Section 2.1) to the supervised
quotable phrase detection stage (Section 2.2).
For each of these sentences, we compute a set of
lexical and syntactic features described in Section
2.3. For computing the features that require the part
of speech tags, we use the MontyLingua package
(Liu, 2004).
9
?To be born with a riotous imagination and then hardly ever
to let it riot is to be a born newspaper man.? (Zona Gale, ?Ro-
mance Island?)
10
?If variety is the spice of life, his life in the north has been
one long diet of paprika.? (Fullerton Waldo, ?Grenfell: Knight-
Errant of the North?)
11
?The idea of solitude is so repugnant to human nature, that
even death would be preferable.? (William O.S. Gilly, ?Nar-
ratives of Shipwrecks of the Royal Navy; between 1793 and
1849?)
12http://www.gutenberg.org/
73
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Precision?Recall Curves
Recall
Pr
ec
is
io
n
Word Features
All Features
Figure 2: Prec. vs. recall for quotable phrase detection.
We find that the extraction process shown in Fig-
ure 1 is efficient and scalable. On average, the entire
process requires less than ten seconds per book on a
single machine.
The complete set of extracted quotable phrases
and annotations is available upon request from the
authors. In addition, the readers are invited to visit
www.noisypearls.com, where a quotable phrase
from the set is published daily.
5 Evaluation and Analysis
5.1 Na??ve Bayes Filtering Evaluation
In the Na??ve Bayes Filtering stage (see Section 2.1)
we evaluate two criteria. First, we measure its abil-
ity to reduce the number of sentences that pass to the
supervised quotable phrase detection stage. As Ta-
ble 2 shows, the Na??ve Bayes Filtering reduces the
number of these sentences by more than 60%.
Second, we evaluate the recall of the Na??ve Bayes
Filtering. We are primarily interested in its ability
to reliably detect quotable phrases and pass them
through to the next stage, while still reducing the
total number of sentences.
For recall evaluation, we collect a set of
2, 817 previously unseen quotable phrases from the
Goodreads website13, and run them through the
Na??ve Bayes Filtering stage. 2, 262 (80%) of the
quotable phrases pass the filter, indicating a high
quotable phrase recall.
13http://www.goodreads.com/quotes
1 #abstract +91.64
2 #quantifier +61.67
3 hasPP ?60.34
4 #IGSeq[16](VB IN PRP) +39.71
5 #IGSeq[6](PRP MD VB) ?38.78
6 #adjective +37.71
7 #IGSeq[14](DT NN VBD) ?36.88
8 #verb +35.22
9 beginStop +31.73
10 #noun +29.63
Table 3: Top quotability features.
Based on these findings, we conclude that the pro-
posed Na??ve Bayes Filtering is able to reliably detect
quotable phrases, while filtering out a large number
of non-quotable ones. It can be further calibrated to
reduce the number of non-quotable sentences or to
increase the quotable phrase recall, by changing the
setting of the parameters ? and ?, described in Sec-
tion 2.1. In the remainder of this section, we use its
output to analyze the performance of the supervised
quotable phrase detection stage.
5.2 Quotable Phrase Detection Evaluation
To evaluate the performance of the supervised
quotable phrase detection stage (see Section 2.2) we
randomly sample 1,500 sentences that passed the
Na??ve Bayes Filtering (this sample is disjoint from
the sample of 500 sentences used for computing
the IGTagSeq feature in Section 2.3). We anno-
tate these sentences with q (Quotable) and q (Non-
Quotable) labels.
Of these sentences, 381 (25%) are labeled as
Quotable. This ratio of quotable phrases is much
higher than what is expected from a non-filtered con-
tent of a book, which provides an indication that the
Na??ve Bayes Filtering provides a relatively balanced
input to the supervised detection stage.
We use this random sample of 1,500 labeled sen-
tences to train a perceptron algorithm (as described
in Section 2.2) using 10-fold cross-validation. We
train two variants of the perceptron. The first variant
is trained using only the lexical features in Table 1,
while the second variant uses all the features.
Figure 2 compares the precision-recall curves of
these two variants. It demonstrates that using the
syntactic features based on punctuation and part of
speech tags significantly improves the precision of
74
Popular ?? 10 12
Upvoted 1 ??? 10 34
No upvotes ?? 0 14
p(?> 0) = .77
Table 4: Distribution of reddit upvote scores.
quote phrase detection at all recall levels. For in-
stance at the 0.4 recall level, it can improve precision
by almost 25%.
Figure 2 also shows that the proposed method
is reliable for high-precision quotable phrase de-
tection. This is especially important for applica-
tions where recall is given less consideration, such
as book preview using quotable phrases. The pro-
posed method reaches a precision of 0.7 at the 0.1
recall level.
It is also interesting to examine the importance of
different features for the quotable phrase detection.
Table 3 shows the ten highest-weighted features, as
learned by the perceptron algorithm on the entire set
of 1,500 labeled examples.
The part of speech features #IGTagSeq[i] oc-
cupy three of the positions in the Table 3. It is inter-
esting to note that two of them have a high negative
weight. In other words, some of the POS sequences
that have the highest information gain (see Eq. 3)
are sequences that are indicative of non-quotable
phrases, rather than quotable phrases.
The two highest-weighted features are based
on handcrafted word lists (#abstract and
#quantifier, respectively). This demonstrates
the importance of task-specific features such as
these for quotability detection.
Finally, the presence of different parts of speech
in the phrase (nouns, verbs and adjectives), as well
as their verb tenses, are important features. For
instance, the presence of a verb in past participle
(hasPP) is a strong negative indicator of phrase
quotability.
5.3 The reddit Study
As mentioned in Section 2.3, the degree of the
phrase quotability is often subjective, and therefore
its estimation may vary among individuals. To val-
idate that our quotability detection method is not
biased by our training data, and that the detected
quotes will have a universal appeal, we set up a veri-
fication study that leverages an online community of
quote enthusiasts.
For our study, we use reddit, a social content web-
site where the registered users submit content, in the
form of either a link or a text post. Other regis-
tered users then upvote or downvote the submission,
which is used to rank the post.
Specifically, we use the Quotes subreddit14, an ac-
tive reddit community devoted to discovering and
sharing quotable phrases. At the time of this writ-
ing, the Quotes subreddit has more than 12,000 sub-
scribers. A typical post to this subreddit contains a
single quotable phrase with attribution. Any reddit
user can then upvote or downvote the quote based on
its perceived merit.
To validate the quality of the quotes which were
used for training the perceptron algorithm, we sub-
mitted 60 quotes, which were labeled as quotable by
one of the authors, to the Quotes subreddit. At most
one quote per day was submitted, to avoid negative
feedback from the community for ?spamming?.
Table 4 presents the upvote scores of the submit-
ted quotes. An upvote score, denoted ?, is computed
as
?= # upvotes ? # downvotes.
Table 4 validates that the majority of the quotes la-
beled as quotable, were also endorsed by the red-
dit community, and received a non-negative upvote
score. As an illustration, in Table 5, we present five
quotes with the highest upvote scores. Anecdotally,
at the time of this writing, only one of the quotes
in Table 5 (a quote by Mark Twain) appeared in
web search results in contexts other than the origi-
nal book.
5.4 Project Gutenberg Corpus Analysis
In this section, we briefly highlight an interesting ex-
ample of how the proposed computational approach
to quotable phrase extraction can be used for a liter-
ary analysis of the PG digital library. To this end,
we train the supervised quotable phrase detection
method using the entire set of 1,500 manually la-
beled sentences. We then run this model over all the
17.5 million sentences that passed the Na??ve Bayes
filtering stage, and retain only the sentences that get
positive perceptron scores (Eq. 2).
14http://www.reddit.com/r/quotes
75
Quote ?
?One hour of deep agony teaches man more love and wisdom than a whole long life of happiness.? 49
(Walter Elliott, ?Life of Father Hecker?)
?As long as I am on this little planet I expect to love a lot of people and I hope they will love me in return.? 43
(Kate Langley, Bosher, ?Kitty Canary?)
?None of us could live with an habitual truth-teller; but thank goodness none of us has to.? 40
(Mark Twain, ?On the Decay of the Art of Lying?)
?A caged bird simply beats its wings and dies, but a human being does not die of loneliness, even when he prays for death.? 33
(George Moore, ?The Lake?)
?Many will fight as long as there is hope, but few will go down to certain death.? 30
(G. A. Henty, ?For the Temple?)
Table 5: Five quotes with the highest upvote scores on reddit.
(a) Authors (b) Books
1 Henry Drummond .045
2 Ella Wheeler Wilcox .041
3 S. D. Gordon .040
4 Andrew Murray .038
5 Ralph Waldo Emerson .037
6 Orison Swett Marden .034
7 Mary Baker Eddy .031
8 ?Abdu?l-Baha? .029
9 John Hartley .029
10 Rabindranath Tagore .028
1 ?Friendship? (Hugh Black) .113
2 ?The Dhammapada? (Translated by F. Max Muller ) .112
3 ?The Philosophy of Despair? (David Starr Jordan) .106
4 ?Unity of Good? (Mary Baker Eddy) .097
5 ?Laments? (Jan Kochanowski) .084
6 ?Joy and Power? (Henry van Dyke) .079
7 ?Polyeucte? (Pierre Corneille) .078
8 ?The Forgotten Threshold? (Arthur Middleton) .078
9 ?The Silence? (David V. Bush) .077
10 ?Levels of Living? (Henry Frederick Cope) .075
Table 6: Project Gutenberg (a) authors and (b) books with the highest quotability index.
This procedure yields 701,418 sentences, which
we call quotable phrases in the remainder of this
section. These quotable phrases are less than 2% of
the entire Project Gutenberg corpus; however, they
still constitute a sizable collection with some poten-
tially interesting properties.
We propose a simple example of a literary anal-
ysis that can be done using this set of quotable
phrases. We detect books and authors that have a
high quotability index, which is formally defined as
QI(x) = # quotable phrases(x)
# total sentences(x) ,
where x is either a book or an author. To ensure the
statistical validity of our analysis, we limit our atten-
tion to books with at least 25 quotable phrases and
authors with at least 5 books in the PG collection.
Using this definition, we can easily compile a list
of authors and books with the highest quotability in-
dex (see Table 6). An interesting observation is that
many of the authors and books in Table 6 deal with
religious themes: Christianity (e.g., Mary Baker
Eddy, S. D. Gordon), Baha????sm (?Abdu?l-Baha?) and
Buddhism (?The Dhammapada?). This is not sur-
prising considering the figurative language common
in the religious prose15.
15
?If a man speaks or acts with an evil thought, pain follows
6 Conclusions
As the number of digitized books increases, a com-
putational analysis of literary corpora becomes an
active research field with many practical applica-
tions. In this paper, we focus on one such appli-
cation: extraction of quotable phrases from books.
Quotable phrase extraction can be used, among
other things, for finding new original quotations
for dictionaries and online quotation repositories, as
well as for generating catchy previews for book ad-
vertisement.
We develop a quotable phrase extraction process
that includes sentence segmentation, unsupervised
sentence filtering based on a quotable language
model, and a supervised quotable phrase detection
using lexical and syntactic features. Our evaluation
demonstrates that this process can be used for high-
precision quotable phrase extraction, especially in
applications that can tolerate lower recall. A study
using a reddit community of quote enthusiasts as
well as a simple corpus analysis further demonstrate
the practical applications of our work.
him, as the wheel follows the foot of the ox that draws the car-
riage.?(?The Dhammapada?, translated by F. Max Muller )
76
7 Acknowledgments
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by NSF
grant IIS-0910884 and in part by ARRA NSF IIS-
9014442. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
those of the sponsor.
References
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Ann M. Blair. 2010. Too Much to Know: Managing
Scholarly Information before the Modern Age. Yale
University Press.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Proc.
of ACL, page To appear.
David K. Elson, Nicholas Dames, and Kathleen R. McK-
eown. 2010. Extracting social networks from literary
fiction. In Proc. of ACL, pages 138?147.
Manaal Faruqui and Sebastian Pado?. 2011. ?I thou thee,
thou traitor?: predicting formal vs. informal address in
English literature. In Proceedings of ACL-HLT, pages
467?472.
Dmitriy Genzel, Jakob Uszkoreit, and Franz Och. 2010.
?Poetic? Statistical Machine Translation: Rhyme and
Meter. In Proc. of EMNLP, pages 158?166.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proc. of
WSDM, pages 202?211.
Chloe Kiddon and Yuriy Brun. 2011. That?s What She
Said: Double Entendre Identification. In Proc. of
ACL-HLT, pages 89?94.
T. Kiss and J. Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguis-
tics, 32(4):485?525.
Okan Kolak and Bill N. Schilit. 2008. Generating links
by mining quotations. In Proc. of 19th ACM confer-
ence on Hypertext and Hypermedia, pages 117?126.
Jisheng Liang, Navdeep Dhillon, and Krzysztof Koper-
ski. 2010. A large-scale system for annotating and
querying quotations in news feeds. In Proc. of Sem-
Search.
Hugo Liu. 2004. Montylingua: An end-to-end natural
language processor with common sense. Available at:
web.media.mit.edu/?hugo/montylingua.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
Martin A. Nowak, and Erez Lieberman Aiden. 2011.
Quantitative analysis of culture using millions of digi-
tized books. Science, 331(6014):176?182.
Rada Mihalcea and Stephen Pulman. 2007. Character-
izing humour: An exploration of features in humorous
texts. In Proc. of CICLing, pages 337?347.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM, pages 574?
576.
Leonid Taycher. 2010. Books of the world, stand up and
be counted! All 129,864,880 of you. Inside Google
Books blog.
Oren Tsur, Dimitry Davidov, and Avi Rappoport. 2010.
ICWSM?A great catchy name: Semi-supervised
recognition of sarcastic sentences in online product re-
views. In Proc. of ICWSM, pages 162?169.
77
