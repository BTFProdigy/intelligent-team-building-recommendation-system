Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62?69,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI @ WMT?13
Alexandre Allauzen1,2, Nicolas Pe?cheux1,2, Quoc Khanh Do1,2, Marco Dinarelli2,
Thomas Lavergne1,2, Aure?lien Max1,2, Hai-Son Le3, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Vietnamese Academy of Science and Technology3, Hanoi, Vietnam
lehaison@ioit.ac.vn
Abstract
This paper describes LIMSI?s submis-
sions to the shared WMT?13 translation
task. We report results for French-English,
German-English and Spanish-English in
both directions. Our submissions use
n-code, an open source system based on
bilingual n-grams, and continuous space
models in a post-processing step. The
main novelties of this year?s participation
are the following: our first participation
to the Spanish-English task; experiments
with source pre-ordering; a tighter integra-
tion of continuous space language mod-
els using artificial text generation (for Ger-
man); and the use of different tuning sets
according to the original language of the
text to be translated.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Eighth Workshop on
Statistical Machine Translation. LIMSI partici-
pated in the French-English, German-English and
Spanish-English tasks in both directions. For this
evaluation, we used n-code, an open source in-
house Statistical Machine Translation (SMT) sys-
tem based on bilingual n-grams1, and continuous
space models in a post-processing step, both for
translation and target language modeling.
This paper is organized as follows. Section 2
contains an overview of the baseline systems built
with n-code, including the continuous space mod-
els. As in our previous participations, several
steps of data pre-processing, cleaning and filter-
ing are applied, and their improvement took a non-
negligible part of our work. These steps are sum-
marized in Section 3. The rest of the paper is de-
voted to the novelties of the systems submitted this
1http://ncode.limsi.fr/
year. Section 4 describes the system developed for
our first participation to the Spanish-English trans-
lation task in both directions. To translate from
German into English, the impact of source pre-
ordering is investigated, and experimental results
are reported in Section 5, while for the reverse di-
rection, we explored a text sampling strategy us-
ing a 10-gram SOUL model to allow a tighter in-
tegration of continuous space models during the
translation process (see Section 6). A final section
discusses the main lessons of this study.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o
et al, 2006; Crego and Marin?o, 2006). In this
framework, translation is divided in two steps: a
source reordering step and a (monotonic) transla-
tion step. Source reordering is based on a set of
learned rewrite rules that non-deterministically re-
order the input words. Applying these rules result
in a finite-state graph of possible source reorder-
ings, which is then searched for the best possible
candidate translation.
2.1 Features
Given a source sentence s of I words, the best
translation hypothesis t? is defined as the sequence
of J words that maximizes a linear combination of
feature functions:
t? = argmax
t,a
{ M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature
function hm and a denotes an alignment between
source and target phrases. Among the feature
functions, the peculiar form of the translation
model constitutes one of the main difference be-
tween the n-gram approach and standard phrase-
based systems.
62
In addition to the translation model (TM), four-
teen feature functions are combined: a target-
language model; four lexicon models; six lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two
lexical weights are estimated from the automatic
word alignments. The weight vector ? is learned
using the Minimum Error Rate Training frame-
work (MERT) (Och, 2003) and BLEU (Papineni
et al, 2002) measured on nt09 (newstest2009) as
the optimization criteria.
2.2 Translation Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to repro-
duce the word order modifications introduced dur-
ing the tuple extraction process. Hence, only those
reordering hypotheses are translated and are intro-
duced using a set of reordering rules automatically
learned from the word alignments. Part-of-speech
(POS) information is used to increase the gen-
eralization power of these rules. Hence, rewrite
rules are built using POS, rather than surface word
forms (Crego and Marin?o, 2006).
2.3 SOUL rescoring
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al, 2003; Schwenk
et al, 2006) as a potential means to improve dis-
crete language models (LMs). As for our last year
participation (Le et al, 2012c), we take advantage
of the recent proposal of Le et al (2011). Using
a specific neural network architecture (the Struc-
tured OUtput Layer or SOUL model), it becomes
possible to estimate n-gram models that use large
vocabulary, thereby making the training of large
neural network LMs (NNLMs) feasible both for
target language models and translation models (Le
et al, 2012a). We use the same models as last year,
meaning that the SOUL rescoring was used for all
systems, except for translating into Spanish. See
section 6 and (Le et al, 2012c) for more details.
3 Corpora and data pre-processing
Concerning data pre-processing, we started from
our submissions from last year (Le et al, 2012c)
and mainly upgraded the corpora and the associ-
ated language-dependent pre-processing routines.
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte
et al, 2008). Previous experiments have demon-
strated that better normalization tools provide bet-
ter BLEU scores: all systems are thus built using
the ?true-case? scheme.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to un-
known forms). When translating from German
into English, the German side is thus normalized
using a specific pre-processing scheme (Allauzen
et al, 2010; Durgar El-Kahlout and Yvon, 2010)
which aims at reducing the lexical redundancy by
(i) normalizing the orthography, (ii) neutralizing
most inflections and (iii) splitting complex com-
pounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition,
for German, fine-grained POS labels were also
needed for pre-processing and were obtained us-
ing the RFTagger (Schmid and Laws, 2008).
For Spanish, all the availaible data are tokenized
using FreeLing2 toolkit (Padro? and Stanilovsky,
2012), with default settings and some added rules.
Sentence splitting and morphological analysis are
disabled except for del ? de el and al ? a el.
Moreover, a simple ?true-caser? based on upper-
case word frequency is used, and the specific
Spanish punctuation signs ??? and ??? are removed
and heuristically reintroduced in a post-processing
step. All Spanish texts are POS-tagged also using
Freeling. The EAGLES tag set is however sim-
plified by truncating the category label to the first
two symbols, in order to reduce the sparsity of the
reordering rules estimated by n-code.
For the CommonCrawl corpus, we found that
many sentences are not in the expected language.
For example, in the French side of the French-
English version, most of the first sentences are
in English. Therefore, foreign sentence pairs are
filtered out with a MaxEnt classifier that uses n-
grams of characters as features (n is between 1
and 4). This filter discards approximatively 10%
2http://nlp.lsi.upc.edu/freeling/
63
of the sentence pairs. Moreover, we also observe
that a lot of sentence pairs are not translation of
each other. Therefore, an extra sentence alignment
step is carried out using an in-house implementa-
tion of the tool described in (Moore, 2002). This
last step discards approximately 20% of the cor-
pus. For the Spanish-English task, the same filter-
ing is applied to all the available corpora.
4 System development for the
Spanish-English task
This is our first participation to the Spanish-
English translation task in both directions. This
section provides details about the development of
n-code systems for this language pair.
4.1 Data selection and filtering
The CommonCrawl and UN corpora can be con-
sidered as very noisy and out-of-domain. As de-
scribed in (Allauzen et al, 2011), to select a subset
of parallel sentences, trigram LMs were trained for
both Spanish and English languages on a subset of
the available News data: the Spanish (resp. En-
glish) LM was used to rank the Spanish (resp. En-
glish) side of the corpus, and only those sentences
with perplexity above a given threshold were se-
lected. Finally, the two selected sets were in-
tersected. In the following experiments, the fil-
tered versions of these corpora are used to train
the translation systems unless explicitly stated.
4.2 Spanish language model
To train the language models, we assumed that the
test set would consist in a selection of recent news
texts and all the available monolingual data for
Spanish were used, including the Spanish Giga-
word, Third Edition. A vocabulary is first defined
by including all tokens observed in the News-
Commentary and Europarl corpora. This vocab-
ulary is then expanded with all words that occur
more than 10 times in the recent news texts (LDC-
2007-2011 and news-crawl-2011-2012). This pro-
cedure results in a vocabulary containing 372k
words. Then, the training data are divided into
7 sets based on dates or genres. On each set, a
standard 4-gram LM is estimated from the vocab-
ulary using absolute discounting interpolated with
lower order models (Kneser and Ney, 1995; Chen
and Goodman, 1998). The resulting LMs are then
linearly interpolated using coefficients chosen so
Corpora BLEU
dev nt11 test nt12
es2en N,E 30.2 33.2
N,E,C 30.6 33.7
N,E,U 30.3 33.6
N,E,C,U 30.6 33.7
N,E,C,U (nf) 30.7 33.6
en2es N,E 32.2 33.3
N,E,C,U 32.3 33.6
N,E,C,U (nf) 32.5 33.9
Table 1: BLEU scores achieved with different
sets of parallel corpora. All systems are base-
line n-code with POS factor models. The follow-
ing shorthands are used to denote corpora, : ?N?
stands for News-Commentary, ?E? for Europarl,
?C? for CommonCrawl, ?U? for UN and (nf) for
non filtered corpora.
as to minimise the perplexity evaluated on the de-
velopment set (nt08).
4.3 Experiments
All reported results are averaged on 3 MERT runs.
Table 1 shows the BLEU scores obtained with dif-
ferent corpora setups. We can observe that us-
ing the CommonCrawl corpus improves the per-
formances in both directions, while the impact of
the UN data is less important, especially when
combined with CommonCrawl. The filtering strat-
egy described in Section 4.2 has a slightly posi-
tive impact of +0.1 BLEU point for the Spanish-
to-English direction but yields a 0.2 BLEU point
decrease in the opposite direction.
For the following experiments, all the available
corpora are therefore used: News-Commentary,
Europarl, filtered CommonCrawl and UN. For
each of these corpora, a bilingual n-gram model
is estimated and used by n-code as one individual
model score. An additionnal TM is trained on the
concatenation all these corpora, resulting in a to-
tal of 5 TMs. Moreover, n-code is able to handle
additional ?factored? bilingual models where the
source side words are replaced by the correspond-
ing lemma or even POS tag (Koehn and Hoang,
2007). Table 2 reports the scores obtained with
different settings.
In Table 2, big denotes the use of a wider
context for n-gram TMs (n = 4, 5, 4 instead
of 3, 4, 3 respectively for word-based, POS-based
and lemma-based TMs). Using POS factored
64
Condition BLEU
dev nt11 test nt12
es2en base 30.3 33.5
pos 30.6 33.7
big-pos 30.7 33.7
big-pos-lem 30.7 33.8
en2es base 32.0 33.4
pos 32.3 33.6
big-pos 32.3 33.8
big-pos-pos+ 32.2 33.4
Table 2: BLEU scores for different configuration
of factored translation models. The big prefix de-
notes experiments with the larger context for n-
gram translation models.
models yields a significant BLEU improvement,
as well as using a wider context for n-gram TMs.
Since Spanish is morphologically richer than En-
glish, lemmas are introduced only on the Span-
ish side. An additionnal BLEU improvement is
achieved by adding factored models based on lem-
mas when translating from Spanish to English,
while in the opposite direction it does not seem
to have any clear impact.
For English to Spanish, we also experimented
with a 5-gram target factored model, using the
whole morphosyntactic EAGLES tagset, (pos+ in
Table 2), to add some syntactic information, but
this, in fact, proved harmful.
As several tuning sets were available, experi-
ments were carried out with the concatenation of
nt09 to nt11 as a tuning data set. This yields an im-
provement between 0.1 and 0.3 BLEU point when
testing on nt12 when translating from Spanish to
English.
4.4 Submitted systems
For both directions, the submitted systems are
trained on all the available training data, the cor-
pora CommonCrawl and UN being filtered as de-
scribed previously. A word-based TM and a POS
factored TM are estimated for each training set.
To translate from Spanish to English, the system
is tuned on the concatenation of the nt09 to nt11
datasets with an additionnal 4-gram lemma-based
factored model, while in the opposite direction, we
only use nt11.
dev nt09 test nt11
en2de 15.43 15.35
en-mod2de 15.06 15.00
Table 3: BLEU scores for pre-ordering experi-
ments with a n-code system and the approach pro-
posed by (Neubig et al, 2012)
5 Source pre-ordering for English to
German translation
While distorsion models can efficiently handle
short range reorderings, they are inadequate to
capture long-range reorderings, especially for lan-
guage pairs that differ significantly in their syn-
tax. A promising workaround is the source pre-
ordering method that can be considered similar,
to some extent, to the reordering strategy imple-
mented in n-code; the main difference is that the
latter uses one deterministic (long-range) reorder-
ing on top of conventional distortion-based mod-
els, while the former only considers one single
model delivering permutation lattices. The pre-
ordering approach is illustrated by the recent work
of Neubig et al (2012), where the authors use a
discriminatively trained ITG parser to infer a sin-
gle permutation of the source sentence.
In this section, we investigate the use of this
pre-ordering model in conjunction with the bilin-
gual n-gram approach for translating English into
German (see (Collins et al, 2005) for similar ex-
periments with the reverse translation direction).
Experiments are carried out with the same settings
as described in (Neubig et al, 2012): given the
source side of the parallel data (en), the parser is
estimated to modify the original word order and to
generate a new source side (en-mod); then a SMT
system is built for the new language pair (en-mod
? de). The same reordering model is used to re-
order the test set, which is then translated with the
en-mod? de system.
Results for these experiments are reported in Ta-
ble 3, where nt09 and nt11 are respectively used
as development and test sets. We can observe that
applying pre-ordering on source sentences leads to
small drops in performance for this language pair.
To explain this degradation, the histogram of to-
ken movements performed by the model on the
pre-ordered training data is represented in Fig-
ure 1. We can observe that most of the movements
are in the range [?4,+6] (92% of the total occur-
65
Figure 1: Histogram of token movement size ver-
sus its occurrences performed by the model Neu-
big on the source english data.
rences), which can be already taken into account
by the standard reordering model of the baseline
system. This is reflected also by the following
statistics: surprisingly, only 16% of the total num-
ber of sentences are changed by the pre-ordering
model, and the average sentence-wise Kendall?s ?
and the average displacement of these small parts
of modified sentences are, respectively, 0.027 and
3.5. These numbers are striking for two reasons:
first, English and German have in general quite
different word order, thus our experimental con-
dition should be somehow similar to the English-
Japanese scenario studied in (Neubig et al, 2012);
second, since the model is able to perform pre-
ordering basically at any distance, it is surprising
that a large part of the data remains unmodified.
6 Artificial Text generation with SOUL
While the context size for BOLMs is limited (usu-
ally up to 4-grams) because of sparsity issues,
NNLMs can efficiently handle larger contexts up
to 10-grams without a prohibitive increase of the
overall number of parameters (see for instance the
study in (Le et al, 2012b)). However the major
bottleneck of NNLMs is the computation cost dur-
ing both training and inference. In fact, the pro-
hibitive inference time usually implies to resort to
a two-pass approach: the first pass uses a conven-
tional BOLM to produce a k-best list (the k most
likely translations); in the second pass, the prob-
ability of a NNLM is computed for each hypoth-
esis, which is then added as a new feature before
the k-best list is reranked. Note that to produce the
k-best list, the decoder uses a beam search strategy
to prune the search space. Crucially, this pruning
does not use the NNLMs scores and results in po-
tentially sub-optimal k-best-lists.
6.1 Sampling texts with SOUL
In language modeling, a language is represented
by a corpus that is approximated by a n-gram
model. Following (Sutskever et al, 2011; Deoras
et al, 2013), we propose an additionnal approxi-
mation to allow a tighter integration of the NNLM:
a 10-gram NNLM is first estimated on the training
corpus; texts then are sampled from this model to
create an artificial training corpus; finally, this arti-
ficial corpus is approximated by a 4-gram BOLM.
The training procedure for the SOUL NNLM is
the same as the one described in (Le et al, 2012c).
To sample a sentence from the SOUL model, first
the sentence length is randomly drawn from the
empirical distribution, then each word of the sen-
tence is sampled from the 10-gram distribution es-
timated with the SOUL model.
The convergence of this sampling strategy can
be evaluated by monitoring the perplexity evolu-
tion vs. the number of sentences that are gener-
ated. Figure 2 depicts this evolution by measuring
perplexity on the nt08 set with a step size of 400M
sampled sentences. The baseline BOLM (std) is
estimated on all the available training data that
consist of approximately 300M of running words.
We can observe that the perplexity of the BOLM
estimated on sampled texts (generated texts) de-
creases when the number of sample sentences in-
creases, and tends to reach slowly the perplex-
ity of the baseline BOLM. Moreover, when both
BOLMs are interpolated, an even lower perplex-
ity is obtained, which further decreases with the
amount of sampled training texts.
6.2 Translation results
Experiments are run for translation into German,
which lacks a GigaWord corpus. An artificial cor-
pus containing 3 billions of running words is first
generated as described in Section 6.1. This corpus
is used to estimate a BOLM with standard settings,
that is then used for decoding, thereby approxi-
mating the use of a NNLM during the first pass.
Results reported in Table 4 show that adding gen-
erated texts improves the BLEU scores even when
the SOUL model is added in a rescoring step. Also
note that using the LM trained on the sampled cor-
pus yields the same BLEU score that using the
standard LM.
66
 190 200 210 220 230 240 250 260
 270 280
 2  4  6  8  10  12ppx times 400M sampled sentences
artificial textsartificial texts+stdstd
Figure 2: Perplexity measured on nt08 with the
baseline LM (std), with the LM estimated on the
sampled texts (generated texts), and with the inter-
polation of both.
Therefore, to translate from English to German,
the submitted system includes three BOLMs: one
trained on all the monolingual data, one on artifi-
cial texts and a third one that uses the freely avail-
able deWack corpus3 (1.7 billion words).
target LM BLEU
dev nt09 test nt10
base 15.3 16.5
+genText 15.5 16.8
+SOUL 16.4 17.6
+genText+SOUL 16.5 17.8
Table 4: Impact of the use of sampled texts.
7 Different tunings for different original
languages
As shown by Lembersky et al (2012), the original
language of a text can have a significant impact on
translation performance. In this section, this effect
is assessed on the French to English translation
task. Training one SMT system per original lan-
guage is impractical, since the required informa-
tion is not available for most of parallel corpora.
However, metadata provided by the WMT evalua-
tion allows us to split the development and test sets
according to the original language of the text. To
ensure a sufficient amount of texts for each con-
dition, we used the concatenation of newstest cor-
pora for the years 2008, 2009, 2011, and 2012,
leaving nt10 for testing purposes.
Five different development sets have been cre-
ated to tune five different systems. Experimental
results are reported in Table 7 and show a drastic
3http://wacky.sslmit.unibo.it/doku.php
baseline adapted
original language tuning
cz 22.31 23.83
en 36.41 39.21
fr 31.61 32.41
de 18.46 18.49
es 30.17 29.34
all 29.43 30.12
Table 5: BLEU scores for the French-to-English
translation task measured on nt10 with systems
tuned on development sets selected according to
their original language (adapted tuning).
improvement in terms of BLEU score when trans-
lating back to the original English and a significant
increase for original text in Czech and French. In
this year?s evaluation, Russian was introduced as
a new language, so for sentences originally in this
language, the baseline system was used. This sys-
tem is used as our primary submission to the eval-
uation, with additional SOUL rescoring step.
8 Conclusion
In this paper, we have described our submis-
sions to the translation task of WMT?13 for
the French-English, German-English and Spanish-
English language pairs. Similarly to last year?s
systems, our main submissions use n-code, and
continuous space models are introduced in a post-
processing step, both for translation and target lan-
guage modeling. To translate from English to
German, we showed a slight improvement with
a tighter integration of the continuous space lan-
guage model using a text sampling strategy. Ex-
periments with pre-ordering were disappointing,
and the reasons for this failure need to be better
understood. We also explored the impact of using
different tuning sets according to the original lan-
guage of the text to be translated. Even though the
gain vanishes when adding the SOUL model in a
post-processing step, it should be noted that due to
time limitation this second step was not tuned ac-
cordingly to the original language. We therefore
plan to assess the impact of using different tuning
sets on the post-processing step.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
67
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard Un iversity.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Marin?o.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, and
Kenneth Church. 2013. Approximate inference: A
sampling based modeling technique to capture com-
plex dependencies in a language model. Speech
Communication, 55(1):162 ? 177.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012b. Measuring the influence of long range de-
pendencies with neural network language models.
In Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages 1?
10, Montre?al, Canada.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012c. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012. Language models for machine trans-
lation: Original vs. translated texts. Comput. Lin-
guist., 38(4):799?825, December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Proceed-
ings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, AMTA
?02, pages 135?144, Tiburon, CA, USA. Springer-
Verlag.
68
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02:
Proc. of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318. Associ-
ation for Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. of Interna-
tional Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ?11, pages
1017?1024, New York, NY, USA, June. ACM.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
69
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84?89,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The KIT-LIMSI Translation System for WMT 2014
?
Quoc Khanh Do,
?
Teresa Herrmann,
??
Jan Niehues,
?
Alexandre Allauzen,
?
Franc?ois Yvon and
?
Alex Waibel
?
LIMSI-CNRS, Orsay, France
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
surname@limsi.fr
?
firstname.surname@kit.edu
Abstract
This paper describes the joined submis-
sion of LIMSI and KIT to the Shared
Translation Task for the German-to-
English direction. The system consists
of a phrase-based translation system us-
ing a pre-reordering approach. The base-
line system already includes several mod-
els like conventional language models on
different word factors and a discriminative
word lexicon. This system is used to gen-
erate a k-best list. In a second step, the
list is reranked using SOUL language and
translation models (Le et al., 2011).
Originally, SOUL translation models were
applied to n-gram-based translation sys-
tems that use tuples as translation units
instead of phrase pairs. In this article,
we describe their integration into the KIT
phrase-based system. Experimental re-
sults show that their use can yield sig-
nificant improvements in terms of BLEU
score.
1 Introduction
This paper describes the KIT-LIMSI system for
the Shared Task of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation. The sys-
tem participates in the German-to-English trans-
lation task. It consists of two main components.
First, a k-best list is generated using a phrase-
based machine translation system. This system
will be described in Section 2. Afterwards, the k-
best list is reranked using SOUL (Structured OUt-
put Layer) models. Thereby, a neural network lan-
guage model (Le et al., 2011), as well as several
translation models (Le et al., 2012a) are used. A
detailed description of these models can be found
in Section 3. While the translation system uses
phrase pairs, the SOUL translation model uses tu-
ples as described in the n-gram approach (Mari?no
et al., 2006). We describe the integration of the
SOUL models into the translation system in Sec-
tion 3.2. Section 4 summarizes the experimen-
tal results and compares two different tuning al-
gorithms: Minimum Error Rate Training (Och,
2003) and k-best Batch Margin Infused Relaxed
Algorithm (Cherry and Foster, 2012).
2 Baseline system
The KIT translation system is an in-house imple-
mentation of the phrase-based approach and in-
cludes a pre-ordering step. This system is fully
described in Vogel (2003).
To train translation models, the provided Eu-
roparl, NC and Common Crawl parallel corpora
are used. The target side of those parallel corpora,
the News Shuffle corpus and the GigaWord cor-
pus are used as monolingual training data for the
different language models. Optimization is done
with Minimum Error Rate Training as described
in Venugopal et al. (2005), using newstest2012
and newstest2013 as development and test data,
respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side (German) of the
corpus before training. Since the web-crawled
Common Crawl corpus is noisy, this corpus is
first filtered using an SVM classifier as described
in Mediani et al. (2011).
The word alignment is generated using the
GIZA++ Toolkit (Och and Ney, 2003). Phrase
extraction and scoring is done using the Moses
toolkit (Koehn et al., 2007). Phrase pair proba-
bilities are computed using modified Kneser-Ney
smoothing (Foster et al., 2006).
We apply short-range reorderings (Rottmann
and Vogel, 2007) and long-range reorder-
ings (Niehues and Kolss, 2009) based on part-of-
speech tags. The POS tags are generated using
the TreeTagger (Schmid, 1994). Rewriting rules
84
based on POS sequences are learnt automatically
to perform source sentence reordering according
to the target language word order. The long-range
reordering rules are further applied to the training
corpus to create reordering lattices to extract the
phrases for the translation model. In addition,
a tree-based reordering model (Herrmann et al.,
2013) trained on syntactic parse trees (Rafferty
and Manning, 2008; Klein and Manning, 2003)
is applied to the source sentence. In addition
to these pre-reordering models, a lexicalized
reordering model (Koehn et al., 2005) is applied
during decoding.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) using modified Kneser-Ney
smoothing (Chen and Goodman, 1996). The sys-
tem uses a 4-gram word-based language model
trained on all monolingual data and an additional
language model trained on automatically selected
data (Moore and Lewis, 2010). The system fur-
ther applies a language model based on 1000 auto-
matically learned word classes using the MKCLS
algorithm (Och, 1999). In addition, a bilingual
language model (Niehues et al., 2011) is used as
well as a discriminative word lexicon (DWL) us-
ing source context to guide the word choices in the
target sentence.
3 SOUL models for statistical machine
translation
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al., 2003; Schwenk,
2007) as a potential means to improve discrete
language models. The SOUL model (Le et al.,
2011) is a specific neural network architecture that
allows us to estimate n-gram models using large
vocabularies, thereby making the training of large
neural network models feasible both for target lan-
guage models and translation models (Le et al.,
2012a).
3.1 SOUL translation models
While the integration of SOUL target language
models is straightforward, SOUL translation mod-
els rely on a specific decomposition of the joint
probability P (s, t) of a sentence pair, where s is a
sequence of I reordered source words (s
1
, ..., s
I
)
1
1
In the context of the n-gram translation model, (s, t) thus
denotes an aligned sentence pair, where the source words are
reordered.
and t contains J target words (t
1
, ..., t
J
). In the
n-gram approach (Mari?no et al., 2006; Crego et
al., 2011), this segmentation is a by-product of
source reordering, and ultimately derives from ini-
tial word and phrase alignments. In this frame-
work, the basic translation units are tuples, which
are analogous to phrase pairs, and represent a
matching u = (s, t) between a source phrase s
and a target phrase t.
Using the n-gram assumption, the joint proba-
bility of a segmented sentence pair using L tupels
decomposes as:
P (s, t) =
L
?
i=1
P (u
i
|u
i?1
, ..., u
i?n+1
) (1)
A first issue with this decomposition is that the
elementary units are bilingual pairs. Therefore,
the underlying vocabulary and hence the number
of parameters can be quite large, even for small
translation tasks. Due to data sparsity issues, such
models are bound to face severe estimation prob-
lems. Another problem with Equation (1) is that
the source and target sides play symmetric roles,
whereas the source side is known, and the tar-
get side must be predicted. To overcome some
of these issues, the n-gram probability in Equa-
tion (1) can be factored by first decomposing tu-
ples in two (source and target) parts, and then de-
composing the source and target parts at the word
level.
Let s
k
i
denote the k
th
word of source part of the
tuple s
i
. Let us consider the example of Figure 1,
s
1
11
corresponds to the source word nobel, s
4
11
to
the source word paix, and similarly t
2
11
is the tar-
get word peace. We finally define h
n?1
(t
k
i
) as the
sequence of the n?1 words preceding t
k
i
in the tar-
get sentence, and h
n?1
(s
k
i
) as the n?1 words pre-
ceding s
k
i
in the reordered source sentence: in Fig-
ure 1, h
3
(t
2
11
) thus refers to the three word context
receive the nobel associated with the target word
peace. Using these notations, Equation 1 can be
rewritten as:
P (s, t) =
L
?
i=1
[
|t
i
|
?
k=1
P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
?
|s
i
|
?
k=1
P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
]
(2)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
85
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
s :   .... 
t :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org)
French sentence appears at the top of the figure, just above the reordered source s and the target t. The
pair (s, t) decomposes into a sequence of L bilingual units (tuples) u
1
, ..., u
L
. Each tuple u
i
contains a
source and a target phrase: s
i
and t
i
.
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one
for each language; however, the moves of these
windows remain synchronized by the tuple seg-
mentation. Moreover, the context is not limited
to the current phrase, and continues to include
words in adjacent phrases. Equation (2) involves
two terms that will be further denoted as TrgSrc
and Src, respectively P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
and P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
. It is worth notic-
ing that the joint probability of a sentence pair
can also be decomposed by considering the fol-
lowing two terms: P
(
s
k
i
|h
n?1
(s
k
i
), h
n?1
(t
1
i+1
)
)
and P
(
t
k
i
|h
n?1
(s
1
i
), h
n?1
(t
k
i
)
)
. These two terms
will be further denoted by SrcTrg and Trg. There-
fore, adding SOUL translation models means that
4 scores are added to the phrase-based systems.
3.2 Integration
During the training step, the SOUL translation
models are trained as described in (Le et al.,
2012a). The main changes concern the inference
step. Given the computational cost of computing
n-gram probabilities with neural network models,
a solution is to resort to a two-pass approach: the
first pass uses a conventional system to produce
a k-best list (the k most likely hypotheses); in
the second pass, probabilities are computed by the
SOUL models for each hypothesis and added as
new features. Then the k-best list is reordered ac-
cording to a combination of all features including
these new features. In the following experiments,
we use 10-gram SOUL models to rescore 300-
best lists. Since the phrase-based system described
in Section 2 uses source reordering, the decoder
was modified in order to generate k-best lists that
contain necessary word alignment information be-
tween the reordered source sentence and its asso-
ciated target hypothesis. The goal is to recover
the information that is illustrated in Figure 1 and
to apply the n-gram decomposition of a sentence
pair.
These (target and bilingual) neural network
models produce scores for each hypothesis in the
k-best list; these new features, along with the fea-
tures from the baseline system, are then provided
to a new phase which runs the traditional Mini-
mum Error Rate Training (MERT ) (Och, 2003), or
a recently proposed k-best Batch Margin Infused
Relaxed Algorithm (KBMIRA ) (Cherry and Fos-
ter, 2012) for tuning purpose. The SOUL mod-
els used for this year?s evaluation are similar to
those described in Allauzen et al. (2013) and Le
et al. (2012b). However, since compared to these
evaluations less parallel data is available for the
German-to-English task, we use smaller vocabu-
laries of about 100K words.
4 Results
We evaluated the SOUL models on the German-
to-English translation task using two systems to
generate the k-best lists. The first system used
all models of the baseline system except the DWL
model and the other one used all models.
Table 1 summarizes experimental results in
terms of BLEU scores when the tuning is per-
formed using KBMIRA. As described in Section
3, the probability of a phrase pair can be decom-
posed into products of words? probabilities in 2
different ways: we can first estimate the probabil-
ity of words in the source phrase given the context,
and then the probability of the target phrase given
its associated source phrase and context words
(see Equation (2)); or inversely we can generate
the target side before the source side. The for-
mer proceeds by adding Src and TrgSrc scores as
86
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.30 27.42 26.43 27.85
Translation st 26.46 27.70 26.66 28.04
Translation ts 26.48 27.41 26.61 28.00
All Translation 26.50 27.86 26.70 28.08
All SOUL models 26.62 27.84 26.75 28.10
Table 1: Results using KBMIRA
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.18 27.09 26.44 27.54
Translation st 26.36 27.59 26.66 27.80
Translation ts 26.44 27.69 26.63 27.94
All Translation 26.53 27.65 26.69 27.99
All SOUL models 26.47 27.68 26.66 28.01
Table 2: Results using MERT. Results in bold correpond to the submitted system.
2 new features into the k-best list, and the latter by
adding Trg and SrcTrg scores. These 2 methods
correspond respectively to the Translation ts and
Translation st lines in the Table 1. The 4 trans-
lation models may also be added simultaneously
(All Translations). The first line gives baseline
results without SOUL models, while the Target
line shows results in adding only SOUL language
model. The last line (All SOUL models) shows
the results for adding all neural network models
into the baseline systems.
As evident in Table 1, using the SOUL trans-
lation models yields generally better results than
using the SOUL target language model, yielding
about 0.2 BLEU point differences on dev and test
sets. We can therefore assume that the SOUL
translation models provide richer information that,
to some extent, covers that contained in the neural
network language model. Indeed, these 4 trans-
lation models take into account not only lexi-
cal probabilities of translating target words given
source words (or in the inverse order), but also the
probabilities of generating words in the target side
(Trg model) as does a language model, with the
same context length over both source and target
sides. It is therefore not surprising that adding the
SOUL language model along with all translation
models (the last line in the table) does not give sig-
nificant improvement compared to the other con-
figurations. The different ways of using the SOUL
translation models perform very similarly.
Table 2 summarizes the results using MERT in-
stead of KBMIRA. We can observe that using KB-
MIRA results in 0.1 to 0.2 BLEU point improve-
ments compared to MERT. Moreover, this impact
becomes more important when more features are
considered (the last line when all 5 neural net-
work models are added into the baseline systems).
In short, the use of neural network models yields
up to 0.6 BLEU improvement on the DWL sys-
tem, and a 0.8 BLEU gain on the system without
DWL. Unfortunately, the experiments with KB-
MIRA were carried out after the the submission
date. Therefore the submitted system corresponds
to the last line of table 2 indicated in bold.
5 Conclusion
We presented a system with two main features: a
phrase-based translation system which uses pre-
reordering and the integration of SOUL target lan-
guage and translation models. Although the trans-
lation performance of the baseline system is al-
ready very competitive, the rescoring by SOUL
models improve the performance significantly. In
the rescoring step, we used a continuous language
model as well as four continuous translation mod-
87
els. When combining the different SOUL models,
the translation models are observed to be more im-
portant in increasing the translation performance
than the language model. Moreover, we observe a
slight benefit to use KBMIRA instead of the stan-
dard MERT tuning algorithm. It is worth noticing
that using KBMIRA improves the performance
but also reduces the variance of the final results.
As future work, the integration of the SOUL
translation models could be improved in differ-
ent ways. For SOUL translation models, there
is a mismatch between translation units used dur-
ing the training step and those used by the de-
coder. The former are derived using the n-gram-
based approach, while the latter use the conven-
tional phrase extraction heuristic. We assume that
reducing this mismatch could improve the overall
performance. This can be achieved for instance
using forced decoding to infer a segmentation of
the training data into translation units. Then the
SOUL translation models can be trained using
this segmentation. For the SOUL target language
model, in these experiments we only used the En-
glish part of the parallel data for training. Results
may be improved by including all the monolingual
data.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658 as well as the French Ar-
maments Procurement Agency (DGA) under the
RAPID Rapmat project.
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-Son Le, and Franc?ois Yvon. 2013.
Limsi@ wmt13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 60?
67.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proceedings of the 34th Annual Meeting on Associa-
tion for Computational Linguistics (ACL ?96), pages
310?318, Santa Cruz, California, USA.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Josep M. Crego, Franois Yvon, and Jos B. Mari?no.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In EMNLP, pages 53?61.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, Prague, Czech Republic.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. pages 39?48, Montr?eal, Canada,
June. Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. Limsi@ wmt?12. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 330?337. Association for
Computational Linguistics.
88
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518, July.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluating Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
89
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246?253,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIMSI @ WMT?14 Medical Translation Task
Nicolas P
?
echeux
1,2
, Li Gong
1,2
, Quoc Khanh Do
1,2
, Benjamin Marie
2,3
,
Yulia Ivanishcheva
2,4
, Alexandre Allauzen
1,2
, Thomas Lavergne
1,2
,
Jan Niehues
2
, Aur
?
elien Max
1,2
, Franc?ois Yvon
2
Univ. Paris-Sud
1
, LIMSI-CNRS
2
B.P. 133, 91403 Orsay, France
Lingua et Machina
3
, Centre Cochrane franc?ais
4
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submission
to the first medical translation task at
WMT?14. We report results for English-
French on the subtask of sentence trans-
lation from summaries of medical ar-
ticles. Our main submission uses a
combination of NCODE (n-gram-based)
and MOSES (phrase-based) output and
continuous-space language models used in
a post-processing step for each system.
Other characteristics of our submission in-
clude: the use of sampling for building
MOSES? phrase table; the implementation
of the vector space model proposed by
Chen et al. (2013); adaptation of the POS-
tagger used by NCODE to the medical do-
main; and a report of error analysis based
on the typology of Vilar et al. (2006).
1 Introduction
This paper describes LIMSI?s submission to the
first medical translation task at WMT?14. This
task is characterized by high-quality input text
and the availability of large amounts of training
data from the same domain, yielding unusually
high translation performance. This prompted us
to experiment with two systems exploring differ-
ent translation spaces, the n-gram-based NCODE
(?2.1) and an on-the-fly variant of the phrase-
based MOSES (?2.2), and to later combine their
output. Further attempts at improving translation
quality were made by resorting to continuous lan-
guage model rescoring (?2.4), vector space sub-
corpus adaptation (?2.3), and POS-tagging adap-
tation to the medical domain (?3.3). We also per-
formed a small-scale error analysis of the outputs
of some of our systems (?5).
2 System Overview
2.1 NCODE
NCODE implements the bilingual n-gram ap-
proach to SMT (Casacuberta and Vidal, 2004;
Mari?no et al., 2006; Crego and Mari?no, 2006) that
is closely related to the standard phrase-based ap-
proach (Zens et al., 2002). In this framework, the
translation is divided into two steps. To translate
a source sentence f into a target sentence e, the
source sentence is first reordered according to a
set of rewriting rules so as to reproduce the tar-
get word order. This generates a word lattice con-
taining the most promising source permutations,
which is then translated. Since the translation step
is monotonic, the peculiarity of this approach is to
rely on the n-gram assumption to decompose the
joint probability of a sentence pair in a sequence
of bilingual units called tuples.
The best translation is selected by maximizing
a linear combination of feature functions using the
following inference rule:
e
?
= argmax
e,a
K
?
k=1
?
k
f
k
(f , e,a) (1)
where K feature functions (f
k
) are weighted by
a set of coefficients (?
k
) and a denotes the set of
hidden variables corresponding to the reordering
and segmentation of the source sentence. Along
with the n-gram translation models and target n-
gram language models, 13 conventional features
are combined: 4 lexicon models similar to the ones
used in standard phrase-based systems; 6 lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. Features
are estimated during the training phase. Training
source sentences are first reordered so as to match
246
the target word order by unfolding the word align-
ments (Crego and Mari?no, 2006). Tuples are then
extracted in such a way that a unique segmenta-
tion of the bilingual corpus is achieved (Mari?no et
al., 2006) and n-gram translation models are then
estimated over the training corpus composed of tu-
ple sequences made of surface forms or POS tags.
Reordering rules are automatically learned during
the unfolding procedure and are built using part-
of-speech (POS), rather than surface word forms,
to increase their generalization power (Crego and
Mari?no, 2006).
2.2 On-the-fly System (OTF)
We develop an alternative approach implement-
ing an on-the-fly estimation of the parameter of
a standard phrase-based model as in (Le et al.,
2012b), also adding an inverse translation model.
Given an input source file, it is possible to compute
only those statistics which are required to trans-
late the phrases it contains. As in previous works
on on-the-fly model estimation for SMT (Callison-
Burch et al., 2005; Lopez, 2008), we first build
a suffix array for the source corpus. Only a lim-
ited number of translation examples, selected by
deterministic random sampling, are then used by
traversing the suffix array appropriately. A coher-
ent translation probability (Lopez, 2008) (which
also takes into account examples where translation
extraction failed) is then estimated. As we cannot
compute exactly an inverse translation probability
(because sampling is performed independently for
each source phrase), we resort to the following ap-
proximation:
p(
?
f |e?) = min
(
1.0,
p(e?|
?
f)? freq(
?
f)
freq(e?)
)
(2)
where the freq(?) is the number of occurrences of
the given phrase in the whole corpus, and the nu-
merator p(e?|
?
f)?freq(
?
f) represents the predicted
joint count of
?
f and e?. The other models in this
system are the same as in the default configuration
of MOSES.
2.3 Vector Space Model (VSM)
We used the vector space model (VSM) of Chen
et al. (2013) to perform domain adaptation. In
this approach, each phrase pair (
?
f, e?) present in
the phrase table is represented by a C-dimensional
vector of TF-IDF scores, one for each sub-corpus,
where C represents the number of sub-corpora
(see Table 1). Each component w
c
(
?
f, e?) is a stan-
dard TF-IDF weight of each phrase pair for the
c
th
sub-corpus. TF(
?
f, e?) is the raw joint count of
(
?
f, e?) in the sub-corpus; the IDF(
?
f, e?) is the in-
verse document frequency across all sub-corpora.
A similar C-dimensional representation of the
development set is computed as follows: we first
perform word alignment and phrase pairs extrac-
tion. For each extracted phrase pair, we compute
its TF-IDF vector and finally combine all vectors
to obtain the vector for the develompent set:
w
dev
c
=
J
?
j=0
K
?
k=0
count
dev
(
?
f
j
, e?
k
)w
c
(
?
f
j
, e?
k
) (3)
where J and K are the total numbers of source
and target phrases extracted from the development
data, respectively, and count
dev
(
?
f
j
, e?
k
) is the joint
count of phrase pairs (
?
f
j
, e?
k
) found in the devel-
opment set. The similarity score between each
phrase pair?s vector and the development set vec-
tor is added into the phrase table as a VSM fea-
ture. We also replace the joint count with the
marginal count of the source/target phrase to com-
pute an alternative average representation for the
development set, thus adding two VSM additional
features.
2.4 SOUL
Neural networks, working on top of conventional
n-gram back-off language models, have been in-
troduced in (Bengio et al., 2003; Schwenk et al.,
2006) as a potential means to improve discrete
language models. As for our submitted transla-
tion systems to WMT?12 and WMT?13 (Le et al.,
2012b; Allauzen et al., 2013), we take advantage
of the recent proposal of (Le et al., 2011). Using
a specific neural network architecture, the Struc-
tured OUtput Layer (SOUL), it becomes possible
to estimate n-gram models that use large vocab-
ulary, thereby making the training of large neural
network language models feasible both for target
language models and translation models (Le et al.,
2012a). Moreover, the peculiar parameterization
of continuous models allows us to consider longer
dependencies than the one used by conventional
n-gram models (e.g. n = 10 instead of n = 4).
Additionally, continuous models can also be
easily and efficiently adapted as in (Lavergne et
al., 2011). Starting from a previously trained
SOUL model, only a few more training epochs are
247
Corpus Sentences Tokens (en-fr) Description wrd-lm pos-lm
in-domain
COPPA 454 246 10-12M -3 -15
EMEA 324 189 6-7M 26 -1
PATTR-ABSTRACTS 634 616 20-24M 22 21
PATTR-CLAIMS 888 725 32-36M 6 2
PATTR-TITLES 385 829 3-4M 4 -17
UMLS 2 166 612 8-8M term dictionary -7 -22
WIKIPEDIA 8 421 17-18k short titles -5 -13
out-of-domain
NEWSCOMMENTARY 171 277 4-5M 6 16
EUROPARL 1 982 937 54-60M -7 -33
GIGA 9 625 480 260-319M 27 52
all parallel all 17M 397-475M concatenation 33 69
target-lm
medical-data -146M 69 -
wmt13-data -2 536M 49 -
devel/test
DEVEL 500 10-12k khresmoi-summary
LMTEST 3 000 61-69k see Section 3.4
NEWSTEST12 3 003 73-82k from WMT?12
TEST 1 000 21-26k khresmoi-summary
Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English
and French tokens, respectively. Weights (?
k
) from our best NCODE configuration are indicated for each
sub-corpora?s bilingual word language model (wrd-lm) and POS factor language model (pos-lm).
needed on a new corpus in order to adapt the pa-
rameters to the new domain.
3 Data and Systems Preparation
3.1 Corpora
We use all the available (constrained) medical data
extracted using the scripts provided by the orga-
nizers. This resulted in 7 sub-corpora from the
medical domain with distinctive features. As out-
of-domain data, we reuse the data processed for
WMT?13 (Allauzen et al., 2013).
For pre-processing of medical data, we closely
followed (Allauzen et al., 2013) so as to be able to
directly integrate existing translation and language
models, using in-house text processing tools for
tokenization and detokenization steps (D?echelotte
et al., 2008). All systems are built using a
?true case? scheme, but sentences fully capital-
ized (plentiful especially in PATTR-TITLES) are
previously lowercased. Duplicate sentence pairs
are removed, yielding a sentence reduction up to
70% for EMEA. Table 1 summarizes the data used
along with some statistics after the cleaning and
pre-processing steps.
3.2 Language Models
A medical-domain 4-gram language model is built
by concatenating the target side of the paral-
lel data and all the available monolingual data
1
,
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1996), using the
SRILM (Stolcke, 2002) and KENLM (Heafield,
2011) toolkits. Although more similar to term-to-
term dictionaries, UMLS and WIKIPEDIA proved
better to be included in the language model.
The large out-of-domain language model used for
WMT?13 (Allauzen et al., 2013) is additionaly
used (see Table 1).
3.3 Part-of-Speech Tagging
Medical data exhibit many peculiarities, includ-
ing different syntactic constructions and a specific
vocabulary. As standard POS-taggers are known
not to perform very well for this type of texts, we
use a specific model trained on the Penn Treebank
and on medical data from the MedPost project
(Smith et al., 2004). We use Wapiti (Lavergne
et al., 2010), a state-of-the-art CRF implementa-
tion, with a standard feature set. Adaptation is per-
formed as in (Chelba and Acero, 2004) using the
out-of-domain model as a prior when training the
in-domain model on medical data. On a medical
test set, this adaptation leads to a 8 point reduc-
tion of the error rate. A standard model is used for
WMT?13 data. For the French side, due to the lack
of annotaded data for the medical domain, corpora
are tagged using the TreeTagger (Schmid, 1994).
1
Attempting include one language model per sub-corpora
yielded a significant drop in performance.
248
3.4 Proxy Test Set
For this first edition of a Medical Translation Task,
only a very small development set was made avail-
able (DEVEL in Table 1). This made both system
design and tuning challenging. In fact, with such a
small development set, conventional tuning meth-
ods are known to be very unstable and prone to
overfitting, and it would be suboptimal to select
a configuration based on results on the develop-
ment set only.
2
To circumvent this, we artificially
created our own internal test set by randomly se-
lecting 3 000 sentences out from the 30 000 sen-
tences from PATTR-ABSTRACTS having the low-
est perplexity according to 3-gram language mod-
els trained on both sides of the DEVEL set. This
test set, denoted by LMTEST, is however highly
biaised, especially because of the high redundancy
in PATTR-ABSTRACTS, and should be used with
great care when tuning or comparing systems.
3.5 Systems
NCODE We use NCODE with default settings, 3-
gram bilingual translation models on words and 4-
gram bilingual translation factor models on POS,
for each included corpora (see Table 1) and for the
concatenation of them all.
OTF When using our OTF system, all in-
domain and out-of-domain data are concatenated,
respectively. For both corpora, we use a maxi-
mum random sampling size of 1 000 examples and
a maximum phrase length of 15. However, all
sub-corpora but GIGA
3
are used to compute the
vectors for VSM features. Decoding is done with
MOSES
4
(Koehn et al., 2007).
SOUL Given the computational cost of com-
puting n-gram probabilities with neural network
models, we resort to a reranking approach. In
the following experiments, we use 10-gram SOUL
models to rescore 1 000-best lists. SOUL models
provide five new features: a target language model
score and four translation scores (Le et al., 2012a).
We reused the SOUL models trained for our par-
ticipation to WMT?12 (Le et al., 2012b). More-
over, target language models are adapted by run-
ning 6 more epochs on the new medical data.
2
This issue is traditionally solved in Machine Learning by
folded cross-validation, an approach that would be too pro-
hibitive to use here.
3
The GIGA corpus is actually very varied in content.
4
http://www.statmt.org/moses/
System Combination As NCODE and OTF dif-
fer in many aspects and make different errors, we
use system combination techniques to take advan-
tage of their complementarity. This is done by
reranking the concatenation of the 1 000-best lists
of both systems. For each hypothesis within this
list, we use two global features, corresponding
either to the score computed by the correspond-
ing system or 0 otherwise. We then learn rerank-
ing weights using Minimum Error Rate Training
(MERT) (Och, 2003) on the development set for
this combined list, using only these two features
(SysComb-2). In an alternative configuration, we
use the two systems without the SOUL rescoring,
and add instead the five SOUL scores as features in
the system combination reranking (SysComb-7).
Evaluation Metrics All BLEU scores (Pap-
ineni et al., 2002) are computed using cased
multi-bleu with our internal tokenization. Re-
ported results correspond to the average and stan-
dard deviation across 3 optimization runs to bet-
ter account for the optimizer variance (Clark et al.,
2011).
4 Experiments
4.1 Tuning Optimization Method
MERT is usually used to optimize Equation 1.
However, with up to 42 features when using
SOUL, this method is known to become very sen-
sitive to local minima. Table 2 compares MERT,
a batch variant of the Margin Infused Relaxation
Algorithm (MIRA) (Cherry and Foster, 2012) and
PRO (Hopkins and May, 2011) when tuning an
NCODE system. MIRA slightly outperforms PRO
on DEVEL, but seems prone to overfitting. How-
ever this was not possible to detect before the re-
lease of the test set (TEST), and so we use MIRA
in all our experiments.
DEVEL TEST
MERT 47.0? 0.4 44.1? 0.8
MIRA 47.9? 0.0 44.8? 0.1
PRO 47.1? 0.1 45.1? 0.1
Table 2: Impact of the optimization method during
the tuning process on BLEU score, for a baseline
NCODE system.
249
4.2 Importance of the Data Sources
Table 3 shows that using the out-of-domain data
from WMT?13 yields better scores than only using
the provided medical data only. Moreover, com-
bining both data sources drastically boosts perfor-
mance. Table 1 displays the weights (?
k
) given by
NCODE to the different sub-corpora bilingual lan-
guage models. Three corpora seems particulary
useful: EMEA, PATTR-ABSTRACTS and GIGA.
Note that several models are given a negative
weight, but removing them from the model sur-
prisingly results in a drop of performance.
DEVEL TEST
medical 42.2? 0.1 39.6? 0.1
WMT?13 43.0? 0.1 41.0? 0.0
both 48.3? 0.1 45.4? 0.0
Table 3: BLEU scores obtained by NCODE trained
on medical data only, WMT?13 data only, or both.
4.3 Part-of-Speech Tagging
Using the specialized POS-tagging models for
medical data described in Section 3.3 instead of a
standart POS-tagger, a 0.5 BLEU points increase
is observed. Table 4 suggests that a better POS
tagging quality is mainly beneficial to the reorder-
ing mechanism in NCODE, in contrast with the
POS-POS factor models included as features.
Reordering Factor model DEVEL TEST
std std 47.9? 0.0 44.8? 0.1
std spec 47.9? 0.1 45.0? 0.1
spec std 48.4? 0.1 45.3? 0.1
spec spec 48.3? 0.1 45.4? 0.0
Table 4: BLEU results when using a standard POS
tagging (std) or our medical adapted specialized
method (spec), either for the reordering rule mech-
anism (Reordering) or for the POS-POS bilingual
language models features (Factor model).
4.4 Development and Proxy Test Sets
In Table 5, we assess the importance of domain
adaptation via tuning on the development set used
and investigate the benefits of our internal test set.
Best scores are obtained when using the pro-
vided development set in the tuning process. Us-
DEVEL LMTEST NEWSTEST12 TEST
48.3? 0.1 46.8? 0.1 26.2? 0.1 45.4? 0.0
41.8? 0.2 48.9? 0.1 18.5? 0.1 40.1? 0.1
39.8? 0.1 37.4? 0.2 29.0? 0.1 39.0? 0.3
Table 5: Influence of the choice of the develop-
ment set when using our baseline NCODE system.
Each row corresponds to the choice of a develop-
ment set used in the tuning process, indicated by a
surrounded BLEU score.
Table 6: Contrast of our two main systems and
their combination, when adding SOUL language
(LM) and translation (TM) models. Stars indicate
an adapted LM. BLEU results for the best run on
the development set are reported.
DEVEL TEST
NCODE 48.5 45.2
+ SOUL LM 49.4 45.7
+ SOUL LM
?
49.8 45.9
+ SOUL LM + TM 50.1 47.0
+ SOUL LM
?
+ TM 50.1 47.0
OTF 46.6 42.5
+ VSM 46.9 42.8
+ SOUL LM 48.6 44.0
+ SOUL LM
?
48.4 44.2
+ SOUL LM + TM 49.6 44.8
+ SOUL LM
?
+ TM 49.7 44.9
SysComb-2 50.5 46.6
SysComb-7 50.7 46.5
ing NEWSTEST12 as development set unsurpris-
ingly leads to poor results, as no domain adapta-
tion is carried out. However, using LMTEST does
not result in much better TEST score. We also note
a positive correlation between DEVEL and TEST.
From the first three columns, we decided to use the
DEVEL data set as development set for our sub-
mission, which is a posteriori the right choice.
4.5 NCODE vs. OTF
Table 6 contrasts our different approaches. Prelim-
inary experiments suggest that OTF is a compara-
ble but cheaper alternative to a full MOSES sys-
tem.
5
We find a large difference in performance,
5
A control experiment for a full MOSES system (using a
single phrase table) yielded a BLEU score of 45.9 on DEVEL
and 43.2 on TEST, and took 3 more days to complete.
250
extra missing incorrect unknown
word content filler disamb. form style term order word term all
syscomb 4 13 20 47 62 8 18 21 1 11 205
OTF+VSM+SOUL 4 4 31 44 82 6 20 42 3 12 248
Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences.
NCODE outperforming OTF by 2.8 BLEU points
on the TEST set. VSM does not yield any signifi-
cant improvement, contrarily to the work of Chen
et al. (2013); it may be the case all individual sub-
corpus are equally good (or bad) at approximating
the stylistic preferences of the TEST set.
4.6 Integrating SOUL
Table 6 shows the substantial impact of adding
SOUL models for both baseline systems. With
only the SOUL LM, improvements on the test set
range from 0.5 BLEU points for NCODE system
to 1.2 points for the OTF system. The adaptation
of SOUL LM with the medical data brings an ad-
ditional improvement of about 0.2 BLEU points.
Adding all SOUL translation models yield an
improvement of 1.8 BLEU points for NCODE and
of 2.4 BLEU points with the OTF system using
VSM models. However, the SOUL adaptation step
has then only a modest impact. In future work, we
plan to also adapt the translation models in order
to increase the benefit of using in-domain data.
4.7 System Combination
Table 6 shows that performing the system combi-
nation allows a gain up to 0.6 BLEU points on the
DEVEL set. However this gain does not transfer to
the TEST set, where instead a drop of 0.5 BLEU
is observed. The system combination using SOUL
scores showed the best result over all of our other
systems on the DEVEL set, so we chose this (a
posteriori sub-obtimal) configuration as our main
system submission.
Our system combination strategy chose for DE-
VEL about 50% hypotheses among those produced
by NCODE and 25% hypotheses from OTF, the
remainder been common to both systems. As ex-
pected, the system combination prefers hypothe-
ses coming from the best system. We can observe
nearly the same distribution for TEST.
5 Error Analysis
The high level of scores for automatic metrics
encouraged us to perform a detailed, small-scale
analysis of our system output, using the error types
proposed by Vilar et al. (2006). A single annota-
tor analyzed the output of our main submission, as
well as our OTF variant. Results are in Table 7.
Looking at the most important types of errors,
assuming the translation hypotheses were to be
used for rapid assimilation of the text content, we
find a moderate number of unknown terms and in-
correctly translated terms. The most frequent er-
ror types include missing fillers, incorrect disam-
biguation, form and order, which all have some
significant impact on automatic metrics. Compar-
ing more specifically the two systems used in this
small-scale study, we find that our combination
(which reused more than 70% of hypotheses from
NCODE) mostly improves over the OTF variant on
the choice of correct word form and word order.
We may attribute this in part to a more efficient
reordering strategy that better exploits POS tags.
6 Conclusion
In this paper, we have demonstrated a successful
approach that makes use of two flexible transla-
tion systems, an n-gram system and an on-the-fly
phrase-based model, in a new medical translation
task, through various approaches to perform do-
main adaptation. When combined with continu-
ous language models, which yield additional gains
of up to 2 BLEU points, moderate to high-quality
translations are obtained, as confirmed by a fine-
grained error analysis. The most challenging part
of the task was undoubtedly the lack on an internal
test to guide system development. Another inter-
esting negative result lies in the absence of success
for our configuration of the vector space model
of Chen et al. (2013) for adaptation. Lastly, a more
careful integration of medical terminology, as pro-
vided by the UMLS, proved necessary.
7 Acknowledgements
We would like to thank Guillaume Wisniewski and
the anonymous reviewers for their helpful com-
ments and suggestions.
251
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-son Le, and Franc?ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Workshkop on
Statistical Machine Translation, pages 62?69, Sofia,
Bulgaria.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(6):1137?1155.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL, Ann Arbor, USA.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Stanley F. Chen and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 310?318, Santa Cruz, NM.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of ACL, Sofia,
Bulgaria.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability. In Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability, pages 176?181, Portland, Oregon.
Josep M. Crego and Jos?e B. Mari?no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jos?e B. Mari?no.
2011. N-code: an open-source bilingual N-gram
SMT toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel D?echelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, H?el`ene May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1352?1362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Thomas Lavergne, Hai-Son Le, Alexandre Allauzen,
and Franc?ois Yvon. 2011. LIMSI?s experiments
in domain adaptation for IWSLT11. In Mei-Yuh
Hwang and Sebastian St?uker, editors, Proceedings
of the heigth International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In Proceedings of the 2012 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: Human language
technologies, pages 39?48, Montr?eal, Canada, June.
Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
252
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. LIMSI @ WMT12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montr?eal,
Canada.
Adam Lopez. 2008. Tera-Scale Translation Models
via Pattern Matching. In Proceedings of COLING,
Manchester, UK.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
USA, July. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 723?730, Morristown, NJ, USA. Association
for Computational Linguistics.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
post: a part of speech tagger for biomedical text.
Bioinformatics, 20(14):2320?2321.
A. Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901?904, Denver, Colorado,
September.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL, pages 101?104.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In LREC, Genoa, Italy.
Richard Zens, Franz Joseph Och, and Herman Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
KI-2002: Advances in artificial intelligence, volume
2479 of LNAI, pages 18?32. Springer Verlag.
253
