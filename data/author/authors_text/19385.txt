Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1007?1014, Dublin, Ireland, August 23-29 2014.
 
Identification of Basic Phrases for Kazakh Language using 
Maximum Entropy Model 
         Gulila Altenbek*,+      Xiaolong Wang*      Gulizhada Haisha+ 
*School of Computer Science and Technology, Harbin Instituteof Technology,150001,China. 
+College of Information Science and Engineering, Xinjiang University,830046, China. 
+The Base of Kazakh and Kirghiz Language of National Language Resource Monitoring and   
Research Centre Minority Languages, Xinjiang, 830046, China.  
                gla@insun.hit.edu.cn, gla@xju.edu.cn, wangxl@insun.hit.edu.cn 
Abstract 
This paper proposes the definition, classification and structure of the Kazakh basic phrases, and sets up a 
framework for classifying them according to their syntactic functions. Meanwhile, the structure of the 
Kazakh basic phrases were analyzed; and the determination of the Kazakh basic phrases collocation and 
extraction of the Kazakh basic phrases based on rules were followed. The Maximum Entropy (ME) 
model uses for the identification of the phrases from texts and achieved a result of automatic identifica-
tion of Kazakh phrases with an accuracy of 78.22% based on rules System and additional artificial mod-
ification. Design feature of this ME model join rely on templates of Kazakh Word, part of speech, affix-
es. Experimental results show that the accuracy rate reached 87.89?? 
 
1 Introduction 
Automatic phrase identification is an important task in natural language processing. A phrase is a 
group of words that work together. Phrase recognition is a grammatical unit agent between words and 
sentences in natural language processing. Phrase identification Parser has been developed for different 
languages, for example, the Church's Base NP Recognition for English (Church, 1988). The rule-based 
Model and Maximum Entropy Model (ME) are the most commonly used technology for phrase repre-
sentation and parsing. 
Kazakh Language belongs to the Turkish Language group in the Altaic language family. It is an agglu-
tinative language with word structures formed by adding derivational or inflectional affixes to root 
words. Phrase identification is also an indispensable part for Kazakh information processing. In the 
past a few year, we have put forward methods for Kazakh morphological analysis, which includes 
stem extraction, part of speech(POS) tagging, spelling check, etc. Recently, we are working on syntax 
parsing, analysis of phrase structure, automatic identification of phrase and in-depth analysis of sen-
tence structure. 
Kazakh phrases are syntactic units consisting of two or more than two words. The phrases can be clas-
sified into two categories, which are free phrase and fixed phrase. We are exploring methods which 
are more suitable for shallow syntactic parsing of Kazakh according to the nature of Kazakh language. 
The research includes a systematic study on information regularity and disambiguation of the Kazakh 
phrase, and automatic recognition of basic phrases of Kazakh language. We have developed a rule-
based method for the automatic recognition of Kazakh basic phrases, and automatic identification of 
verb phrase, noun phrase and adjective phrase based on maximum entropy in Kazakh language at the 
same time. Moreover, the ambiguity of structures is also resolved based on rules. 
This study solves the problem of Kazakh phrase recognition by providing some effective methods. 
This sets up a basis for further syntactic process and tree bank building. This research also provides a 
way to build database for various fields like knowledge acquisition, syntactic understanding, Chinese-
Kazakh machine translation, the process of large-scale corpus, etc. 
  
This work is lice ced under a Creative Commons Attribution 4.0 International License Place licence. Page numbers and p oceedi gs f ot r are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
1007
 In this paper, our work focuses on identifying noun phrases, adjective phrase and verb phrases, which 
are the most difficult aspects of Kazakh phrase recognition analysis. This is achieved by using rules 
are ME method. 
2 Related work 
There are a variety of techniques used for phrase recognition, which include rule-based technique, sta-
tistical technique, and a combination of them. Church's (1988) approach used manual or semi-
automatic annotation phrase corpus as a training corpus. Another popular method is to use a Chunk 
parsing for statistics model to determine the boundary (Koeling, 2000). Chunk parsing was first intro-
duced by Abney (1991), which is one of the most widely used syntactic parsing methods. The main 
idea of chunk parsing lies in seeking the appropriate breakthrough point, and decomposing the full 
parsing problems into a syntax topology statistical structure and syntactic relations. Zhao and Huang 
(1998) are pioneers in Chinese phrase studies; Tsinghua University had also completed its TCT 
(Tsinghua Chinese Treebank) for Chinese (Zhou, 2004). The method has been also applied into studies 
of other languages, such as Kazakh Base NP recognition (Altenbek et al, 2009), and Uyghur Base VP 
Recognition by CRF (Mamatmin et al, 2012). 
Maximum Entropy was first introduced to NLP area by Berger et al (1996) and Della Pietra et al. 
(1997). Maximum Entropy is an extremely flexible technique for linguistic modelling. It can use a vir-
tually unrestricted and rich feature set in the framework of a probability model. It is a conditional, dis-
criminative model and allows mutually dependent variables (Ratnaparkhi, 1999). 
3 Kazakh Phase Parsing 
3.1 Kazakh Morphology 
Morphological analysis is an important task in natural language processing research. It was developed 
for different languages, included English  (Porter, 1980), Finnish  (Karttunen, 1983), Turkish  (Oflaz-
er, 1994; G?l?en, 2004), and Arabic (Beesley, 1996). 
Comparing with other languages, the Kazakh morphological system uses a large number of suffixes 
and a small number of prefixes. Every word has a root, or a stem (Milat, 2003;Zhang 2004). The basic 
Kazakh phrase is an adjacent and non-nested phrase which does not contain recursive structure.  
3.2 The Categories of Kazakh Phrase 
Parsing is one of the most basic and fundamental components in natural language processing. Chunk 
parsing intends to obtain a fragment without thinking deeply.  
A Kazakh phrase is composed of two or more than two words which connected with meaning and 
grammatical structure. There is only a core word in a Kazakh phrase. In the case of Kazakh, Kazakh 
phrases can be divided into fixed phrases and temporary phrases by the meanings of the phrases.  
Abney propose the first complete description of lexical chunks system. In this study the basic phrase 
chunks base was found according to Abney?s system. The five most common phrase in Kazakh are  
NO. Category Explanation Example (Kazakh) Example (English? 
1 NP noun phrase  The golden autumn 
2 VP verb phrase  Achieve dreams 
3 ADJP adjective phrase  Very clean 
4 NUMP Numeral phrases  Eight & nine thousand 
5 ADVP Adverb phrase  The front of 
Table 1.  Part of Kazakh phrase categories. 
 
 
 
 
 1008
 noun phrase, verb phrase, adjective phrase, Numeral phrases, Adverb phrase as shown in table 1. 
Kazakh language is rich in the external morphology which shows prominent in phrase structure. 
3.3 The Basic Kazakh phrase mark specification 
Basic Kazakh phrase marks both its own attribute, for example part of speech, stems and affixes, and 
types of phrase. We used IOB Tagging to mark the start and end of chunks. 
Basic Kazakh phrase start of chunks Inner tag of chunks Out tag of chunks 
noun phrase B-NP I-NP  
 
O 
verb phrase B-VP I-VP 
adjective phrase B-ADJP I-ADJP 
Adverb phrases B-ADVP I-ADVP 
Numeral phrase B-NUMP I-NUMP 
Table 2.  The Basic Kazakh phrase IOB Tagging. 
4 Statistics and Analysis of Kazakh Phrase Structure 
Referring to modern Kazakh grammar (Milat, 2003; Dingjing Zhong. 2004), the basic rules of phrase 
structure of Kazakh language was summarized. The phrase structures are extracted from the corpus, 
and a set of rules are created based on it as well.  
In the representation of basic phrase structures, the following part of speech tagging symbols are used 
in XML documents of Kazakh corpus: v (verb), n. (noun), adj. (adjective), num. (number), adv. (ad-
verb), pron. (pronoun), ono. (onomatopoeia), int.(interjections), conj. (conjunction), part. (partical). 
The Kazakh phrases Structure divided by the function of phrases in our system are shown below.   
Kazakh verb phrase structure: 
1) n+v;   2) v+v;   3) adv+v;  4) adj+v;  5) v+adv;  6) v+v+v;  7) pron+v; 8) n+part+v; 9) n+conj+v; 
10) ono+v; 11) int+v; 12) v+part+v; 13)v+part; 14) v+conj+v;  15)pron+part+v. 
Kazakh noun phrase structure: 
1) n+n; 2) n+conj+n; 3) pron+conj+pron; 4) pron+n; 5) adj+conj+adj; 6) adj+n; 7) adj+adv+n;  
8) num+n; 9) v+n; 10) [ ]+n. 
Kazakh adjective phrase structure: 
1) adj+n; 2) adj+v; 3) adj+n+v; 4) pron+adj; 5) adv+adj+n; 6) adj+adj+n; 7) num+adv+n;  
Collocations, like v+adv, n+part+v, pron+adv, v+part+v, v+part, also exist in other phrase except verb 
phrase. These conditions easily cause ambiguity.  
5 Rule-based phrase tagging 
Kazakh language has two characteristics that have to be taken into account: agglutinative morphology 
and rather free word order with explicit case marking. 
The corpus we used in this process has been already segmented. The way we extracted stem and affix 
was briefly mentioned in the paper. In this paper we used the segmented results of early work, as it is 
not the core part of the algorithm. 
Input: word segmentation (extraction stem and affix) and POS tagged corpus (test.xml); 
Output: First: Phrase tagged file; Second: Phrase file; 
Based on the basic rules of phrase, we have done extraction of phrases from POS tagged Kazakh cor-
pus. The extraction process is as follows: 
(a) First roughly segmented XML corpus. The common segmentation marks include semicolon, com-
ma, full stop, exclamation mark, question mark. 
(b) For the segmented data, we extract the three elements of basic phrase: part of speech (POS), affix, 
and the word. 
 
 
 
 1009
 (c) Look for the matched rule in the rule set. If found, save the basic phrase. Otherwise go back step 1. 
According to combination rules of basic Kazakh phrase, basic phrase was extracted from corpus and 
modified by manual work. The correct combination of basic Kazakh phrase was marked. 
6 Analysis of Kazakh phrase structure ambiguity 
Ambiguity computer analysis of language structure has been one of the difficulties problems. This ar-
ticle from the delimitation ambiguity and structural relationship is to study two aspects of phrase struc-
ture ambiguity.  
One of the difficulties in Kazakh phrase research is the phrase disambiguation problem. Ambiguous 
reasons is word POS ambiguity, phrase boundaries is not easy to determine, POS with the same se-
quence, E.g.  there are five  ambiguous forms: 
(1) VD form (v + adv ) 
Eg.1a?   is verb phrase. (Admission to reduce) 
Eg.1b?  is adverb phrase. ( Admission to more than) 
(2) ND for (n+adv, pron+adv) 
Eg.2a?  is verb phrase.( Change a new clothes) 
Eg.2b?  is adverb phrase.( Good record) 
(3) NPV form? n+part+v,  pron+part+v) 
Eg.3a?  is verb phrase.( Learn about unity) 
Eg.3b?  is noun phrase.( only Ashan) 
(4) VPV form (v+part+v) 
Eg.4a?  is verb phrase.( came then left) 
Eg.4b?  is adverb phrase. (Relevant research to understand) 
(5) VP form (v+part) 
Eg.5a?  is verb phrase.( Speaking before) 
Eg.5b?  is verb phrase.( Organize the relevant) 
For these ambiguities, we can't simply use the rules to match ways to eliminate, but rather to use max-
imum entropy model to solve the problem. 
7 Kazakh Phrase Identification based Maximum Entropy Model 
Maximum Entropy Model is an effective machine learning model which is proposed to solve the POS 
tagging problem, it using ME model is the ability to incorporate various features into the conditional 
probability. The Kazakh phrase recognition task is presented as follow.  
The entropy model P:    ??? yx yxyxppH , ),log(),()(            (1) 
Note: X represents the environmental context words to be marked and y is the output. 
Maximum Entropy Model? Such a model can be shown to have the following form:  
)(maxarg* pHp Cp??                                                      (2) 
Goal: select a distribution p from a set of allowed distributions that maximizes H(y|X).   
7.1 Feature defined 
Kazakh language is an agglutinative language with word structures formed by adding derivational, 
inflectional affixes or suffixes to root words. The features include words, part of speech (POS), inflec-
tional affixes of the training corpus. It seems that the features are na?ve. However, these three kinds of 
features are the most important components of Kazakh language, and they reflect the characteristic of 
Kazakh language. 
According to its own characteristics of a Kazakh, this feature space is defined as follows: 
(1) the word, including the current word, the previous word and next word. 
 
 
 
 
1010
 (2) part of speech(POS), including the part-of-speech types of the current word, previous word and  
next word. 
(3) Affix ingredients, including the current word and the word about the additional ingredient info-
mation. 
(4) Phrase tag that contains the current word and the words to the right and the left two words Phrase 
marker. 
This rule-based approach was applied to generate the maximum entropy model training corpus. Based 
on Kazakh linguistics, the atomic feature space is as shown in table 3. 
Feature tag Feature explanation Feature tag Feature explanation 
W(-1) 
previous one word POS (-2) 
POS (-1) 
POS of previous two word and POS 
of previous one word 
W(0) 
the current word POS (-1) 
POS (0) 
POS of previous one word and POS 
of the current word 
W(+1) 
next one word POS (0) 
POS (+1) 
POS of the current word and POS of 
next one word 
W(-1) 
W(0) 
previous one word 
and the current word 
POS (+1) 
POS (+2) 
POS of next one word and POS of 
next two word 
W(0) 
W(+1) 
the current word and 
next one word 
POS (-2? 
POS (-1)  
POS (0) 
POS of previous two word and POS 
of previous one word and POS of the 
current word 
W(-1) 
W(0? 
W(+1) 
previous one word 
and the current word 
and next one word 
POS (-1)  
POS (0) 
POS (+1) 
POS of previous one word and POS 
of the current word and POS of next 
one word 
POS (-2) 
POS of previous two 
word 
POS (0)  
POS (+1) 
POS (+2) 
POS of the current word and POS of 
next one word and POS of next two 
word 
POS (-1) 
POS of previous one 
word 
Affix(-1) 
affix of previous word  
POS (0? 
POS of the current 
word 
Affix(0) 
affix of current word 
POS (+1) POS of next one word Affix(1) affix of next one word 
POS (+2) POS of next two word   
Table 3.  Atomic feature templates. 
7.2  Feature selection 
Basic phrases with statistical model recognition need to select a high correlation, and the Kazakh lan-
guage features to train with good effect. Establish model based on rule of the language, this work se-
lected feature through templates. After several rounds of experimental debugging, then used artificial 
selection, twenty one templates were selected for Kazakh verb  phrase, only considered important fea-
tures. According to each one?s feature, templates were defined as follow. 
No. template No. template No. template 
1 LPos,Cpos,RPos 8 CVP,RVP,RRVP 15 CWord,RWord 
2 LLPos,Lpos,CPos 9 LVPCPosRVP 16 LPos,LVP 
3 CPos,Rpos,RRPos 10 LPos, LAffix, LVP 17 RWord,RPos 
4 CPos,CAffix,RPos 11 Cpos, CAffix, CVP 18 RPos,RVP 
5 LPosLAffixCPos 12 CWord,RWord,RAffix 19 CPos,RPos 
6 LVP,CVP,RVP 13 CWord,CPos 20 LPos,CPos 
7 LLVP,LVP,CVP 14 LWord,LPos 21 LWord,LVP 
                           Table 4.  Combined  feature of Kz Base VP. 
In order to get the best template, this work structured and processed six template based on Table 4.  
 
 
1011
 Each information function valued in the context of current word, combine the various function values 
into the premise of features, got the characteristics of the movement through the word tag, then it can 
extract features. 
Template A: [RRPos, RRVP, RWord, RAffix, RPos, RVP, CPos, CVP, CWord, CAffix, LLPos, 
LLVP, LWord, LAffix, LPos, LVP] Observation of effects of all the words in the feature space on the 
result of the experiment.  
Template B: [CPos, CVP, CWord, CAffix, LLPos, LLVP, LWord, LAffix, LPos, LVP] Observation of 
effects of left side two words of the candidate word on the result of the experiment. 
Template C:[ RRPos?RRVP?RWord?RAffix?RPos?RVP?CPos?CVP?CWord,CAffix] Ob-
servation of effects of right side two words of the candidate word on the result of the experiment. 
Template D:[ RWord?RAffix?RPos?RVP?CPos?CVP?CWord,CAffix? LWord?LAffix?
LPos?LVP] Observation of effects of each side one word of the candidate word on the result of the 
experiment. 
Template E:[ RWord?RAffix?RPos?RVP?CPos?CVP?CWord?CAffix, LLPos?LLVP?
LWord?LAffix?LPos?LVP] Observation of effects of left side two words and right side one word 
of the candidate word on the result of the experiment.  
Template F:[ RRPos?RRVP?RWord?RAffix?RPos?RVP?CPos?CVP?CWord?CAffix?
LWord?LAffix?LPos?LVP] Observation of effects of left side one word and right side two words 
of the candidate word on the result of the experiment. 
We selected some corpus from Xinjiang Daily tested on six features above, we got different influences 
of different characters. It shows that the C and F template give us the most highest result, namely the 
two words on the right have the biggest influence to the result. It proves Kazakh verb phrases are 
commonly at the end of the sentence.  
7.3    General threshold selection 
There are two general feature selection methods: incremental feature selection and feature selection of 
based on frequency threshold. The frequency is greater than a threshold value equal to a characteristic. 
Through repeating them many times, the frequency threshold value was characterized k = 5, character-
ized in that the use of the frequency characteristic is greater than 5. 
8 Kazakh Phrase Recognition System 
Kazakh phrase recognition system, which based on Maximum Entropy Model, consists of four mod-
ules, namely, pre-processing module, training module, Feature selection module, identification mod-
ule. System training process as shown flow as figure 1. 
Feature 
module base
Training 
corpus
Learning 
documentPreprocessing Feature selection
Parameter 
estimation 
algorithm
  
Figure 1.  Training data flow diagram. 
System testing process as shown flow as figure 2. 
learning 
documents
Test 
corpus
Preprocessing Feature 
extraction
decoding
Identify  
output
 
Figure 2. Testing data flow diagram. 
The Kazakh basic verb phrase recognition results such as shown figure 3: 
1012
  
Figure 3. The Kazakh language basic verb phrase recognition. 
By following a comprehensive analysis of Kazakh words, the following is the Kazakh shallow parsing 
process: 
?1?Sentence? 
 
Golden autumn is coming, Hambar came to the place whish has very strong winds together with 
sheep. 
?2? POS: 
 
?3?Phrase POS: 
 
9   Experiment Results and Analysis 
9.1 Data set 
In this paper, according to the data set, we used the data of January 2008 of the Xinjiang Daily (Ka-
zakh version) corpus. The corpus consists of the raw texts and the POS tagged XML format texts, ex-
periments were done for phrase extraction. 
9.2 Experiment results 
The experiments of the accuracy rates are evaluated using as follow standard evaluation measures: 
 
Precision:    
%100?? baP          (3)
 
Recall     
%100?? dcR
             (4) 
F-measure    
PR PRF ???? 2
              (5) 
Note: a is number of correctly identified phrases. b is number of identified phrases. c is number of all phrases, d 
is number of should correct identify. 
 
In the test corpus, there are 3000 correct tagged sentences as training data, and other 1000 sentences 
are for the test. 
 
 
 
 
 1013
 Method Precision (%) Recall (%) F-measure (%) 
Rule 78.22 70.01 85.25 
ME 87.89 83.13 87.46 
               Table 5.  Phrase recognition test. 
10 Conclusion 
This paper provided solution for identifying Kazakh basic phrases. We have tried rule-based and the 
maximum entropy methods. The Kazakh words, part of speech, affixes context information are used to 
design template of features for maximum entropy model. Based on statistical methods, higher accura-
cy could be obtained in the test, but it was requires more training data.  
The recognition of basic Kazakh phrase could simplify sentence structure, reduce the difficulty of syn-
tactic analyzer. This work put maximum entropy model into recognition of basic Kazakh phrase. 
However, there are still space for improvement on scale and accuracy rate comparing to English and 
Chinese. In the future, our work will focus on completing of corpus and other models. 
Acknowledgments 
This work is funded by the Natural Science Foundation of P.R. China (NSFC)(No.61363062,  No. 
61063025  and No.61272383), Science and Technology Research and Development Funds of Shen-
zhen City (No. JC201005260118A). 
Reference 
Church K. A stochastic parts program and noun phrase parser for unrestricted text. 1988. In Proceedings of the 
Second Conference on Applied Natural Language Processing. Texas, USA. 19(8):136-143. 
Rob Koeling . Chunking with Maximum Entropy Models. 2000. Proceedings of CoNLL-2000 and LLL-2000. 
109(15):139-141. 
Steven Abney. Parsing by chunks. 1991. Dordrecht: Kluwer Academic Publishers. 257-278. 
Zhao Jun and Huang Changning. 1999. Chinese basic noun phrase structure analysis model, Computer science . 
22(2):141-146? 
Qiang Zhou. 2004. Annotation scheme for Chinese Treebank, Journal of Chinese Information Processing. Vol 
18(4):1-8. 
Gulila Altenbek, Ruina-Sun. 2010. Kazakh Noun Phrase Extraction based on N-gram and Rules, International 
Conference on Asian Language Processing (IALP2010). Harbin, China. 305-308. 
Gulila A. and Dawel, A. and Muheyat, N. 2009. A Study of Word Tagging Corpus for the Modern Kazakh Lan-
guage, Journal of Xinjiang University. 26(4):394-401. 
Zulpiya Mamatmin  et al, 2012.Uyghur Base Verb phrases Recognition . A master's degree thesis, Beijing uni-
versity of posts and telecommunications. 
Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A Maximum Entropy Approach to Natural 
Language ,Processing Computational Linguistics, 22(1):39-71. 
Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models.  Machine Learn-
ing, 341(3):151-176 
Porter, M.F. 1980. An algorithm for suffix stripping, Program, 14(3):130?137. 
Karttunen, Lauri. 1983. KIMMO: A general morphological processor. Texas Linguistic Forum, 22:163?186. 
Kemal Oflazer. 1994. Two-level description of Turkish morphology. Literary and Linguistic Computing, 
9(2):137-148. 
G?l?en, E. and E?ref, A. 2004. An affix stripping morphological analyzer for Turkish, Proceedings of the Inter-
national Conference on Artificial Intelligence and Application, Austria, 299-304. 
Beesley, K.R. 1996. Arabic finite-state morphological analysis and generation. In COLING-96, Copenhagen, 89-
94. 
Milat, A. 2003. Modern Kazakh language, Xinjiang People's press, China. 
Dingjing Zhang. 2004. Practical Grammar of Modern Kazakh Language. Beijing: Central University for Na-
tionalities Press. 
1014
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1341?1349, Dublin, Ireland, August 23-29 2014.
Hybrid Deep Belief Networks for
Semi-supervised Sentiment Classification
Shusen Zhou
?
Qingcai Chen
?
Xiaolong Wang
?
Xiaoling Li
?
?
School of Information and Electrical Engineering, Ludong University, Yantai 264025, China.
?
Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen 518055, China.
zhoushusen@gmail.com, qingcai.chen@hitsz.edu.cn
wangxl@insun.hit.edu.cn, appleling@live.cn
Abstract
In this paper, we develop a novel semi-supervised learning algorithm called hybrid deep be-
lief networks (HDBN), to address the semi-supervised sentiment classification problem with
deep learning. First, we construct the previous several hidden layers using restricted Boltzmann
machines (RBM), which can reduce the dimension and abstract the information of the reviews
quickly. Second, we construct the following hidden layers using convolutional restricted Boltz-
mann machines (CRBM), which can abstract the information of reviews effectively. Third, the
constructed deep architecture is fine-tuned by gradient-descent based supervised learning with an
exponential loss function. We did several experiments on five sentiment classification datasets,
and show that HDBN is competitive with previous semi-supervised learning algorithm. Ex-
periments are also conducted to verify the effectiveness of our proposed method with different
number of unlabeled reviews.
1 Introduction
Recently, more and more people write reviews and share opinions on the World Wide Web, which present
a wealth of information on products and services (Liu et al., 2010). These reviews will not only help other
users make better judgements but they are also useful resources for manufacturers of products to keep
track and manage customer opinions (Wei and Gulla, 2010). However, there are large amount of reviews
for every topic, it is difficult for a user to manually learn the opinions of an interesting topic. Sentiment
classification, which aims to classify a text according to the expressed sentimental polarities of opinions
such as ?positive? or ?negtive?, ?thumb up? or ?thumb down?, ?favorable? or ?unfavorable? (Li et al., 2010),
can facilitate the investigation of corresponding products or services.
In order to learn a good text classifier, a large number of labeled reviews are often needed for training
(Zhen and Yeung, 2010). However, labeling reviews is often difficult, expensive or time consuming
(Chapelle et al., 2006). On the other hand, it is much easier to obtain a large number of unlabeled reviews,
such as the growing availability and popularity of online review sites and personal blogs (Pang and Lee,
2008). In recent years, a new approach called semi-supervised learning, which uses large amount of
unlabeled data together with labeled data to build better learners (Zhu, 2007), has been developed in the
machine learning community.
There are several works have been done in semi-supervised learning for sentiment classification, and
get competitive performance (Li et al., 2010; Dasgupta and Ng, 2009; Zhou et al., 2010). However, most
of the existing semi-supervised learning methods are still far from satisfactory. As shown by several re-
searchers (Salakhutdinov and Hinton, 2007; Hinton et al., 2006), deep architecture, which composed of
multiple levels of non-linear operations, is expected to perform well in semi-supervised learning because
of its capability of modeling hard artificial intelligent tasks. Deep belief networks (DBN) is a represen-
tative deep learning algorithm achieving notable success for text classification, which is a directed belief
nets with many hidden layers constructed by restricted Boltzmann machines (RBM), and refined by a
gradient-descent based supervised learning (Hinton et al., 2006). Ranzato and Szummer (Ranzato and
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1341
Szummer, 2008) propose an algorithm to learn text document representations based on semi-supervised
auto-encoders that are combined to form a deep network. Zhou et al. (Zhou et al., 2010) propose a nov-
el semi-supervised learning algorithm to address the semi-supervised sentiment classification problem
with active learning. The key issue of traditional DBN is the efficiency of RBM training. Convolutional
neural networks (CNN), which are specifically designed to deal with the variability of two dimensional
shapes, have had great success in machine learning tasks and represent one of the early successes of
deep learning (LeCun et al., 1998). Desjardins and Bengio (Desjardins and Bengio, 2008) adapt RBM
to operate in a convolutional manner, and show that the convolutional RBM (CRBM) are more efficient
than standard RBM.
CRBM has been applied successfully to a wide range of visual and audio recognition tasks (Lee et al.,
2009a; Lee et al., 2009b). Though the success of CRBM in addressing two dimensional issues, there
is still no published research on the using of CRBM in textual information processing. In this paper,
we propose a novel semi-supervised learning algorithm called hybrid deep belief networks (HDBN), to
address the semi-supervised sentiment classification problem with deep learning. HDBN is a hybrid of
RBM and CRBM deep architecture, the bottom layers are constructed by RBM, and the upper layers are
constructed by CRBM, then the whole constructed deep architecture is fine tuned by a gradient-descent
based supervised learning based on an exponential loss function.
The remainder of this paper is organized as follows. In Section 2, we introduce our semi-supervised
learning method HDBN in details. Extensive empirical studies conducted on five real-world sentiment
datasets are presented in Section 3. Section 4 concludes our paper.
2 Hybrid deep belief networks
2.1 Problem formulation
The sentiment classification dataset composed of many review documents, each review document com-
posed of a bag of words. To classify these review documents using corpus-based approaches, we need to
preprocess them in advance. The preprocess method for these reviews is similar with (Zhou et al., 2010).
We tokenize and downcase each review and represent it as a vector of unigrams, using binary weight
equal to 1 for terms present in a vector. Moreover, the punctuations, numbers, and words of length one
are removed from the vector. Finally, we combine all the words in the dataset, sort the vocabulary by
document frequency and remove the top 1.5%, because many of these high document frequency words
are stopwords or domain specific general-purpose words.
After preprocess, each review can be represented as a vector of binary weight x
i
. If the j
th
word of
the vocabulary is in the i
th
review, x
i
j
= 1; otherwise, x
i
j
= 0. Then the dataset can be represented as a
matrix:
X =
[
x
1
,x
2
, . . . ,x
R+T
]
=
?
?
?
?
?
x
1
1
, x
2
1
, . . . , x
R+T
1
x
1
2
, x
2
2
, . . . , x
R+T
2
.
.
. ,
.
.
. , . . . ,
.
.
.
x
1
D
, x
2
D
, . . . , x
R+T
D
?
?
?
?
?
(1)
where R is the number of training reviews, T is the number of test reviews, D is the number of feature
words in the dataset. Every column of X corresponds to a sample x, which is a representation of a
review. A sample that has all features is viewed as a vector in R
D
, where the i
th
coordinate corresponds
to the i
th
feature.
The L labeled reviews are chosen randomly from R training reviews, or chosen actively by active
learning, which can be seen as:
X
L
= X
R
(S) , S = [s
1
, ..., s
L
], 1 ? s
i
? R (2)
where S is the index of selected training reviews to be labeled manually.
1342
x1 x2 xD
? ? ? ? ?
? ? ? ?
RBM
h0
h1
w1
? ? ?hM
?
 
?
 
?
 
? ?
wM+1
hM+1
CRBM
? ?hN
?
 
?
 
?
 
f(hN(x), y)
y1
Minimize 
Loss
? ?labels
yCy2
?
?
Figure 1: Architecture of HDBN.
The L labels correspond to L labeled training reviews is denoted as:
Y
L
=
[
y
1
,y
2
, . . . ,y
L
]
=
?
?
?
?
?
y
1
1
, y
2
1
, . . . , y
L
1
y
1
2
, y
2
2
, . . . , y
L
2
.
.
. ,
.
.
. , . . . ,
.
.
.
y
1
C
, y
2
C
, . . . , y
L
C
?
?
?
?
?
(3)
where C is the number of classes. Every column of Y is a vector in R
C
, where the j
th
coordinate
corresponds to the j
th
class.
y
i
j
=
{
1 if x
i
? j
th
class
?1 if x
i
/? j
th
class
(4)
For example, if a review x
i
is positive, y
i
= [1,?1]
?
; otherwise, y
i
= [?1, 1]
?
.
We intend to seek the mapping function X? Y using the L labeled data and all unlabeled data. After
training, we can determine y using the mapping function when a new sample x comes.
2.2 Architecture of HDBN
In this part, we propose a novel semi-supervised learning method HDBN to address the problem for-
mulated in Section 2.1. The sentiment datasets have high dimension (about 10,000), and computation
complexity of convolutional calculation is relatively high, so we use RBM to reduce the dimension of
review with normal calculation firstly. Fig. 1 shows the deep architecture of HDBN, a fully intercon-
nected directed belief nets with one input layer h
0
, N hidden layers h
1
,h
2
, ...,h
N
, and one label layer at
the top. The input layer h
0
has D units, equal to the number of features of sample review x. The hidden
1343
wk
hk-1
hk ??
Group 1
1 0 ?
Group Gk
0 1
??
Group 1
0 1 ?
Group Gk-1
1 0
? ?
1Gk
wkGk-11
?
Figure 2: Architecture of CRBM.
layer hasM layers constructed by RBM andN?M layers constructed by CRBM. The label layer has C
units, equal to the number of classes of label vector y. The numbers of hidden layers and the number of
units for hidden layers, currently, are pre-defined according to the experience or intuition. The seeking
of the mapping function X ? Y, here, is transformed to the problem of finding the parameter space
W = {w
1
,w
2
, . . . ,w
N
} for the deep architecture.
The training of the HDBN can be divided into two stages:
1. HDBN is constructed by greedy layer-wise unsupervised learning using RBMs and CRBMs as
building blocks. L labeled data and all unlabeled data are utilized to find the parameter space W
with N layers.
2. HDBN is trained according to the exponential loss function using gradient descent based supervised
learning. The parameter space W is refined using L labeled data.
2.3 Unsupervised learning
As show in Fig. 1, we construct HDBN layer by layer using RBMs and CRBMs, the details of RBM can
be seen in (Hinton et al., 2006), and CRBM is introduced below.
The architecture of CRBM can be seen in Fig. 2, which is similar to RBM, a two-layer recurrent
neural network in which stochastic binary input groups are connected to stochastic binary output groups
using symmetrically weighted connections. The top layer represents a vector of stochastic binary hidden
feature h
k
and the bottom layer represents a vector of binary visible data h
k?1
, k = M + 1, ..., N . The
k
th
layer consists of G
k
groups, where each group consists of D
k
units, resulting in G
k
? D
k
hidden
units. The layer h
M
is consist of 1 group andD
M
units. w
k
is the symmetric interaction term connecting
corresponding groups between data h
k?1
and feature h
k
. However, comparing with RBM, the weights
of CRBM between the hidden and visible groups are shared among all locations (Lee et al., 2009a), and
the calculation is operated in a convolutional manner (Desjardins and Bengio, 2008).
We define the energy of the state (h
k?1
,h
k
) as:
E
(
h
k?1
,h
k
; ?
)
= ?
G
k?1
?
s=1
G
k
?
t=1
(w?
k
st
? h
k?1
s
) ? h
k
t
?
G
k?1
?
s=1
b
k?1
s
D
k?1
?
u=1
h
k?1
s
?
G
k
?
t=1
c
k
t
D
k
?
v=1
h
k
t
(5)
where ? = (w,b, c) are the model parameters: w
k
st
is a filter between unit s in the layer h
k?1
and unit t
in the layer h
k
, k = M + 1, ..., N . The dimension of the filter w
k
st
is equal to D
k?1
?D
k
+ 1. b
k?1
s
is
the s
th
bias of layer h
k?1
and c
k
t
is the t
th
bias of layer h
k
. A tilde above an array (w?) denote flipping
the array, ? denote valid convolution, and ? denote element-wise product followed by summation, i.e.,
A ?B = trA
T
B (Lee et al., 2009a).
Similar to RBM, Gibbs sampler can be performed based on the following conditional distribution.
1344
The probability of turning on unit v in group t is a logistic function of the states of h
k?1
and w
k
st
:
p
(
h
k
t,v
= 1|h
k?1
)
= sigm
(
c
k
t
+ (
?
s
w?
k
st
? h
k?1
s
)
v
)
(6)
The probability of turning on unit u in group s is a logistic function of the states of h
k
and w
k
st
:
p
(
h
k?1
s,u
= 1|h
k
)
= sigm
(
b
k?1
s
+ (
?
t
w
k
st
? h
k
t
)
u
)
(7)
A star ? denote full convolution.
2.4 Supervised learning
In HDBN, we construct the deep architecture using all labeled reviews with unlabeled reviews by in-
putting them one by one from layer h
0
. The deep architecture is constructed layer by layer from bottom
to top, and each time, the parameter space w
k
is trained by the calculated data in the k ? 1
th
layer.
Algorithm 1: Algorithm of HDBN
Input: data X, Y
L
number of training data R; number of test data T ;
number of layers N ; number of epochs Q;
number of units in every hidden layer D
1
...D
N
;
number of groups in every convolutional hidden layer G
M
...G
N
;
hidden layer h
1
, . . . ,h
M
;
convolutional hidden layer h
M+1
, . . . ,h
N?1
;
parameter space W = {w
1
, . . . ,w
N
};
biases b, c; momentum ? and learning rate ?;
Output: deep architecture with parameter space W
1. Greedy layer-wise unsupervised learning
for k = 1; k ? N ? 1 do
for q = 1; q ? Q do
for r = 1; r ? R+ T do
Calculate the non-linear positive and negative phase:
if k ?M then
Normal calculation.
else
Convolutional calculation according to Eq. 6 and Eq. 7.
end
Update the weights and biases:
w
k
st
= ?w
k
st
+ ?
(
?
h
k?1
s,r
h
k
t,r
?
P
0
?
?
h
k?1
s,r
h
k
t,r
?
P
1
)
end
end
end
2. Supervised learning based on gradient descent
arg min
W
L
?
i=1
C
?
j=1
exp(?h
N
(x
i
j
)y
i
j
)
According to the w
k
calculated by RBM and CRBM, the layer h
k
, k = 1, . . . ,M can be computed as
following when a sample x inputs from layer h
0
:
h
k
t
(x) = sigm
(
c
k
t
+
D
k?1
?
s=1
w
k
st
h
k?1
s
(x)
)
, t = 1, . . . , D
k
(8)
1345
When k = M + 1, . . . , N ? 1, the layer h
k
can be represented as:
h
k
t
(x) = sigm
?
?
c
k
t
+
G
k?1
?
s=1
w?
k
st
? h
k?1
s
(x)
?
?
, t = 1, . . . , G
k
(9)
The parameter space w
N
is initialized randomly, just as backpropagation algorithm.
h
N
t
(x) = c
N
t
+
G
N?1
?D
N?1
?
s=1
w
N
st
h
N?1
s
(x), t = 1, . . . , D
N
(10)
After greedy layer-wise unsupervised learning, h
N
(x) is the representation of x. Then we useL labeled
reviews to refine the parameter space W for better discriminative ability. This task can be formulated as
an optimization problem:
arg min
W
f
(
h
N
(
X
L
)
,Y
L
)
(11)
where
f
(
h
N
(
X
L
)
,Y
L
)
=
L
?
i=1
C
?
j=1
T
(
h
N
j
(
x
i
)
y
i
j
)
(12)
and the loss function is defined as
T (r) = exp(?r) (13)
We use gradient-descent through the whole HDBN to refine the weight space. In the supervised
learning stage, the stochastic activities are replaced by deterministic, real valued probabilities.
2.5 Classification using HDBN
The training procedure of HDBN is given in Algorithm 1. For the training of HDBN architecture, the
parameters are random initialized with normal distribution. All the reviews in the dataset are used to
train the HDBN with unsupervised learning. After training, we can determine the label of the new data
through:
arg
j
maxh
N
(x) (14)
3 Experiments
3.1 Experimental setup
We evaluate the performance of the proposed HDBN method using five sentiment classification datasets.
The first dataset is MOV (Pang et al., 2002), which is a classical movie review dataset. The other four
datasets contain products reviews come from the multi-domain sentiment classification corpus, including
books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT) (Blitzer et al., 2007). Each
dataset contains 1,000 positive and 1,000 negative reviews.
The experimental setup is same as (Zhou et al., 2010). We divide the 2,000 reviews into ten equal-
sized folds randomly, maintaining balanced class distributions in each fold. Half of the reviews in each
fold are random selected as training data and the remaining reviews are used for test. Only the reviews
in the training data set are used for the selection of labeled reviews by active learning. All the algorithms
are tested with cross-validation.
We compare the classification performance of HDBN with four representative semi-supervised learn-
ing methods, i.e., semi-supervised spectral learning (Spectral) (Kamvar et al., 2003), transductive SVM
(TSVM) (Collobert et al., 2006), deep belief networks (DBN) (Hinton et al., 2006), and person-
al/impersonal views (PIV) (Li et al., 2010). Spectral learning, TSVM methods are two baseline methods
for sentiment classification. DBN (Hinton et al., 2006) is the classical deep learning method proposed
recently. PIV (Li et al., 2010) is a new sentiment classification method proposed recently.
1346
Table 1: HDBN structure used in experiment.
Dataset Structure
MOV 100-100-4-2
KIT 50-50-3-2
ELE 50-50-3-2
BOO 50-50-5-2
DVD 50-50-5-2
Table 2: Test accuracy with 100 labeled reviews for semi-supervised learning.
Type MOV KIT ELE BOO DVD
Spectral 67.3 63.7 57.7 55.8 56.2
TSVM 68.7 65.5 62.9 58.7 57.3
DBN 71.3 72.6 73.6 64.3 66.7
PIV - 78.6 70.0 60.1 49.5
HDBN 72.2 74.8 73.8 66.0 70.3
3.2 Performance of HDBN
The HDBN architecture used in all our experiments have 2 normal hidden layer and 1 convolutional
hidden layer, every hidden layer has different number of units for different sentiment datasets. The deep
structure used in our experiments for different datasets can be seen in Table 1. For example, the HDBN
structure used in MOV dataset experiment is 100-100-4-2, which represents the number of units in 2
normal hidden layers are 100, 100 respectively, and in output layer is 2, the number of groups in 1
convolutional hidden layer is 4. The number of unit in input layer is the same as the dimensions of each
datasets. For greedy layer-wise unsupervised learning, we train the weights of each layer independently
with the fixed number of epochs equal to 30 and the learning rate is set to 0.1. The initial momentum
is 0.5 and after 5 epochs, the momentum is set to 0.9. For supervised learning, we run 30 epochs, three
times of linear searches are performed in each epoch.
The test accuracies in cross validation for five datasets and five methods with semi-supervised learning
are shown in Table 2. The results of previous two methods are reported by (Dasgupta and Ng, 2009).
The results of DBN method are reported by (Zhou et al., 2010). Li et al. (Li et al., 2010) reported the
results of PIV method. The result of PIV on MOV dataset is empty, because (Li et al., 2010) did not
report it. HDBN is the proposed method.
Through Table 2, we can see that HDBN gets most of the best results except on KIT dataset, which is
just slight worse than PIV method. However, the preprocess of PIV method is much more complicated
than HDBN, and the PIV results on other datasets are much worse than HDBN method. HDBN method
is adjusted by DBN, all the experiment results on five datasets for HDBN are better than DBN. This
could be contributed by the convolutional computation in HDBN structure, and proves the effectiveness
of our proposed method.
3.3 Performance with variance of unlabeled data
To verify the contribution of unlabeled reviews for our proposed method, we did several experiments
with fewer unlabeled reviews and 100 labeled reviews.
The test accuracies of HDBN with different number of unlabeled reviews and 100 labeled reviews
on five datasets are shown in Fig. 3. The architectures for HDBN used in this experiment are same
as Section 3.2 too, which can be seen in Table 1. We can see that the performance of HDBN is much
worse when just using 400 unlabeled reviews. However, when using more than 1200 unlabeled reviews,
the performance of HDBN is improved obviously. For most of review datasets, the accuracy of HDBN
with 1200 unlabeled reviews is close to the accuracy with 1600 and 2000 unlabeled reviews. This proves
that HDBN can get competitive performance with just few labeled reviews and appropriate number of
1347
400 600 800 1000 1200 1400 1600 1800 200060
62
64
66
68
70
72
74
76
78
80
Number of unlabeled review
Te
st a
ccu
rac
y (%
)
 
 
MOV
KIT
ELE
BOO
DVD
Figure 3: Test accuracy of HDBN with different number of unlabeled reviews on five datasets.
unlabeled reviews. Considering the much time needed for training with more unlabeled reviews and less
accuracy improved for HDBN method, we suggest using appropriate number of unlabeled reviews in real
application.
4 Conclusions
In this paper, we propose a novel semi-supervised learning method, HDBN, to address the sentiment clas-
sification problem with a small number of labeled reviews. HDBN seamlessly incorporate convolutional
computation into the DBN architecture, and use CRBM to abstract the review information effectively.
To the best of our knowledge, HDBN is the first work that uses convolutional neural network to improve
sentiment classification performance. One promising property of HDBN is that it can effectively use the
distribution of large amount of unlabeled data, together with few label information in a unified frame-
work. In particular, HDBN can greatly reduce the dimension of reviews through RBM and abstract the
information of reviews through the cooperate of RBM and CRBM. Experiments conducted on five senti-
ment datasets demonstrate that HDBN outperforms state-of-the-art semi-supervised learning algorithms,
such as SVM and DBN based methods, using just few labeled reviews, which demonstrate the effective
of deep architecture for sentiment classification.
Acknowledgements
This work is supported in part by National Natural Science Foundation of China (No. 61300155, No.
61100115 and No. 61173075), Natural Science Foundation of Shandong Province (No. ZR2012FM008),
Science and Technology Development Plan of Shandong Province (No. 2013GNC11012), Science and
Technology Research and Development Funds of Shenzhen City (No. JC201005260118A and No.
JC201005260175A), and Scientific Research Fund of Ludong University (LY2013004).
References
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In Annual Meeting of the Association of Computational Lin-
guistics, pages 440?447, Prague, Czech Republic. Association for Computational Linguistics.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. 2006. Semi-supervised learning. MIT Press, USA.
Ronan Collobert, Fabian Sinz, Jason Weston, and Leon Bottou. 2006. Large scale transductive svms. Journal of
Machine Learning Research, 7:1687?1712.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy, classify the hard: A semi-supervised approach to automatic
sentiment classfication. In Joint Conference of the 47th Annual Meeting of the Association for Computational
1348
Linguistics and 4th International Joint Conference on Natural Language Processing of the Asian Federation of
Natural Language Processing, pages 701?709, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Guillaume Desjardins and Yoshua Bengio. 2008. Empirical evaluation of convolutional rbms for vision. Technical
report.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets.
Neural Computation, 18:1527?1554.
Sepandar Kamvar, Dan Klein, and Christopher Manning. 2003. Spectral learning. In International Joint Confer-
ences on Artificial Intelligence, pages 561?566, Catalonia, Spain. AAAI Press.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE, 86(11):2278?2324.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. 2009a. Convolutional deep belief networks
for scalable unsupervised learning of hierarchical representations. In International Conference on Machine
Learning, pages 609?616, Montreal, Canada. ACM.
Honglak Lee, Yan Largman, Peter Pham, and Andrew Y. Ng. 2009b. Unsupervised feature learning for audio
classification using convolutional deep belief networks. In Advances in Neural Information Processing Systems,
pages 1096?1103, Vancouver, B.C., Canada. NIPS Foundation.
Shoushan Li, Chu-Ren Huang, Guodong Zhou, and Sophia Yat Mei Lee. 2010. Employing personal/impersonal
views in supervised and semi-supervised sentiment classification. In Annual Meeting of the Association for
Computational Linguistics, pages 414?423, Uppsala, Sweden. Association for Computational Linguistics.
Yang Liu, Xiaohui Yu, Xiangji Huang, and Aijun An. 2010. S-plasa+: Adaptive sentiment analysis with applica-
tion to sales performance prediction. In International ACM SIGIR Conference on Research and Development
in Information Retrieval, pages 873?874, New York, NY, USA. ACM.
Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis, volume 2 of Foundations and Trends in
Information Retrieval.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine
learning techniques. In Conference on Empirical Methods in Natural Language Processing, pages 79?86,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Marc?Aurelio Ranzato and Martin Szummer. 2008. Semi-supervised learning of compact document representa-
tions with deep networks. In International Conference on Machine Learning, pages 792?799, Helsinki, Finland.
ACM.
Ruslan Salakhutdinov and Geoffrey E. Hinton. 2007. Learning a nonlinear embedding by preserving class neigh-
bourhood structure. Journal of Machine Learning Research, 2:412?419.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learning on product reviews via sentiment ontology tree. In Annual
Meeting of the Association for Computational Linguistics, pages 404?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yi Zhen and Dit-Yan Yeung. 2010. Sed: Supervised experimental design and its application to text classification.
In International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 299?
306, Geneva, Switzerland. ACM.
Shusen Zhou, Qingcai Chen, and Xiaolong Wang. 2010. Active deep networks for semi-supervised sentiment
classification. In International Conference on Computational Linguistics, pages 1515?1523, Beijing, China.
Coling 2010 Organizing Committee.
Xiaojin Zhu. 2007. Semi-supervised learning literature survey. Ph.D. thesis.
1349
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 843?847,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Multimodal DBN for Predicting High-Quality Answers in cQA portals
Haifeng Hu, Bingquan Liu, Baoxun Wang, Ming Liu, Xiaolong Wang
School of Computer Science and Technology
Harbin Institute of Technology, China
{hfhu, liubq, bxwang, mliu, wangxl}@insun.hit.edu.cn
Abstract
In this paper, we address the problem for
predicting cQA answer quality as a clas-
sification task. We propose a multimodal
deep belief nets based approach that op-
erates in two stages: First, the joint rep-
resentation is learned by taking both tex-
tual and non-textual features into a deep
learning network. Then, the joint repre-
sentation learned by the network is used
as input features for a linear classifier. Ex-
tensive experimental results conducted on
two cQA datasets demonstrate the effec-
tiveness of our proposed approach.
1 Introduction
Predicting the quality of answers in communi-
ty based Question Answering (cQA) portals is a
challenging task. One straightforward approach
is to use textual features as a text classification
task (Agichtein et al, 2008). However, due to
the word over-sparsity and inherent noise of user-
generated content, the classical bag-of-words rep-
resentation, is not appropriate to estimate the qual-
ity of short texts (Huang et al, 2011). Another typ-
ical approach is to leverage non-textual features to
automatically identify high quality answers (Jeon
et al, 2006; Zhou et al, 2012). However, in this
way, the mining of meaningful textual features
usually tends to be ignored.
Intuitively, combining both textual and non-
textual information extracted from answers is
helpful to improve the performance for predict-
ing the answer quality. However, textual and non-
textual features usually have different kinds of rep-
resentations and the correlations between them are
highly non-linear. Previous study (Ngiam et al,
2011) has shown that it is hard for a shallow model
to discover the correlations over multiple sources.
To this end, a deep learning approach, called
multimodal deep belief nets (mDBN), is intro-
duced to address the above problems to predict the
answer quality. The approach includes two stages:
feature learning and supervised training. In the
former stage, a specially designed deep network is
given to learn the unified representation using both
textual and non-textual information. In the latter
stage, the outputs of the network are then used as
inputs for a linear classifier to make prediction.
The rest of this paper is organized as follows:
The related work is surveyed in Section 2. Then,
the proposed approach and experimental results
are presented in Section 3 and Section 4 respec-
tively. Finally, conclusions and future directions
are drawn in Section 5.
2 Related Work
The typical way to predict the answer quality is
exploring various features and employing machine
learning methods. For example, Jeon et al (2006)
have proposed a framework to predict the qual-
ity of answers by incorporating non-textual fea-
tures into a maximum entropy model. Subsequent-
ly, Agichtein et al (2008) and Bian et al (2009)
both leverage a larger range of features to find high
quality answers. The deep research on evaluating
answer quality has been taken by Shah and Pomer-
antz (2010) using the logistic regression model.
We borrow some of their ideas in this paper.
In deep learning field, extensive studies have
been done by Hinton and his co-workers (Hin-
ton et al, 2006; Hinton and Salakhutdinov, 2006;
Salakhutdinov and Hinton, 2009), who initial-
ly propose the deep belief nets (DBN). Wang
et.al (2010; 2011) firstly apply the DBNs to model
semantic relevance for qa pairs in social communi-
ties. Meanwhile, the feature learning for disparate
sources has also been the hot research topic. Lee
et al (2009) demonstrate that the hidden represen-
tations computed by a convolutional DBN make
excellent features for visual recognition.
843
3 Approach
We consider the problem of high-quality answer
prediction as a classification task. Figure 1 sum-
marizes the framework of our proposed approach.
First, textual features and non-textual features ex-
TextualFeatures Non-textualFeaturesCQAArchives
ClassifierFusion Representation
FeatureLearning  Supervised Training
High-qualityAnswers
Figure 1: Framework of our proposed approach.
tracted from cQA portals are used to train two DB-
N models to learn the high-level representation-
s independently for answers. The two high-level
representations learned by the deep architectures
are then joined together to train a RBM model.
Finally, a linear classifier is trained with the final
shared representation as input to make prediction.
In this section, a deep network for the cQA an-
swer quality prediction is presented. Textual and
non-textual features are typically characterized by
distinct statistical properties and the correlations
between them are highly non-linear. It is very dif-
ficult for a shallow model to discover these corre-
lations and form an informative unified represen-
tation. Our motivation of proposing the mDBN is
to tackle these problems using an unified represen-
tation to enhance the classification performance.
3.1 The Restricted Boltzmann Machines
The basic building block of our feature leaning
component is the Restricted Boltzmann Machine
(RBM). The classical RBM is a two-layer undi-
rected graphical model with stochastic visible u-
nits v and stochastic hidden units h.The visible
layer and the hidden layer are fully connected to
the units in the other layer by a symmetric matrix
w. The classical RBM has been used effectively in
modeling distributions over binary-value data. As
for real-value inputs, the gaussian RBM (Bengio
et al, 2007) can be employed. Different from the
former, the hypothesis for the visible unit in the
gaussian RBM is the normal distribution.
3.2 Feature Learning
The illustration of the feature learning model is
given by Figure 2. Basically, the model consists
of two parts.
In the bottom part (i.e., V -H1, H1-H2), each
data modality is modeled by a two-layer DBN sep-
arately. For clarity, we take the textual modality
as an example to illustrate the construction of the
mDBN in this part. Given a textual input vector v,
the visible layer generates the hidden vector h, by
p(hj = 1|v) = ?(cj +
?
iwijvi).
Then the conditional distribution of v given h, is
p(vi = 1|h) = ?(bi +
?
j wijhj).
where ?(x) = (1 + e?x)?1 denotes the logistic
function. The parameters are updated by perform-
ing gradient ascent using Contrastive Divergence
(CD) algorithm (Hinton, 2002).
After learning the RBMs in the bottom layer,
we treat the activation probabilities of its hidden
units driven by the inputs, as the training data for
training a new layer. The construction procedures
for the non-textual modality are similar to the tex-
tual one, except that we use the gaussian RBM to
model the real-value inputs in the bottom layer.
Finally, we combine the two models by adding
an additional layer, H3, on the top of them to form
the mDBN. The training method is also similar to
the bottom?s, but the input vector is the concatena-
tion of the mapped textual vector and the mapped
non-textual vector.
Figure 2: mDBN for Feature Learning
It should be noted in the network, the bottom
part is essential to form the joint representation
because the correlations between the textual and
non-textual features are highly non-linear. It is
hard for a RBM directly combining the two dis-
parate sources to learn their correlations.
3.3 Supervised Training and Classification
After the above steps, a deep network for feature
learning between textual and non-textual data is
established. Classifiers, either support vector ma-
chine (SVM) or logistic regression (LR), can then
be trained with the unified representation (Ngiam
844
et al, 2011; Srivastava and Salakhutdinov, 2012).
Specifically, the LR classifier is used to make the
final prediction in our experiments since it keeps
to deliver the best performance.
3.4 Basic Features
Textual Features: The textual features ex-
tract from 1,500 most frequent words in the train-
ing dataset after standard preprocessing steps,
namely word segmentation, stopwords removal
and stemming1. As a result, each answer is repre-
sented as a vector containing 1,500 distinct terms
weighted by binary scheme.
Non-textual Features: Referring to
the previous work (Jeon et al, 2006; Shah and
Pomerantz, 2010), we adopt some features used
in theirs and also explore three additional features
marked by ? sign. The complete list is described
in Table 1.
Features Type
Length of question title (description) Integer
Length of answer Integer
Number of unique words for the answer ? Integer
Ratio of the qa length ? Float
Answer?s relative position ? Integer
Number of answers for the question Integer
Number of comments for the question Integer
Number of questions asked by asker (answerer) Integer
Number of questions resolved by asker (answerer) Integer
Asker?s (Answerer?s) total points Integer
Asker?s (Answerer?s) level Integer
Asker?s (Answerer?s) total stars Integer
Asker?s (Answerer?s) best answer ratio Float
Table 1: Summary of non-textual features.
4 Experiments
4.1 Experiment Setup
Datasets: We carry out experiments on two
datasets. One dataset comes from Baidu Zhi-
dao2, which contains 33,740 resolved questions
crawled by us from the ?travel? category. The oth-
er dataset is built by Chen and Nayak (2008) from
Yahoo! Answers3. We refer to these two dataset-
s as ZHIDAO and YAHOO respectively and ran-
domly sample 10,000 questions from each to form
our experimental datasets. According to the us-
er name, we have crawled all the user profile web
pages for non-textual feature collection. To allevi-
ate unnecessary noise, we only select those ques-
tions with number of answers no less than 3 (one
1The stemming step is only used in English corpus.
2http://zhidao.baidu.com
3http://answers.yahoo.com
best answer among them), and those answers at
least have 10 tokens. The statistics on the datasets
used for experiments are summarized in Table 2.
Statistics Items YAHOO ZHIDAO
# of questions 6841 5368
# of answers 74485 22435
# of answers per question 10.9 4.1
# of users 28812 12734
Table 2: Statistics on experimental datasets.
Baselines and Evaluation Metrics: We com-
pare against the following methods as our base-
lines. (1) Logistic Regression (LR): We imple-
ment the approach used by Shah and Pomer-
antz (2010) with textual features LR-T, non-
textual features LR-N and their simple combina-
tion LR-C. (2) DBN: Similar to the mDBN, the
outputs of the last hidden layer by the DBN are
used as inputs for LR model. Based on the fea-
ture sets, we have DBN-T for textual features and
DBN-N for non-textual features.
Since we mainly focus on the high quality an-
swers, the precision, recall and f1 for positive class
and the overall accuracy for both classes are em-
ployed as our evaluation metrics.
Model Architecture and Training Details: To
create the mDBN architecture, we use the classi-
cal RBM with 1500 visible units followed by 2
hidden layers with 1000 and 800 units respective-
ly for the textual branch, and the gaussian RBM
with 20 visible units followed by 2 hidden layers
with 100 and 200 units respectively for the non-
textual branch. On the joint layer of the network,
the layer contains 1000 real-value units.
Each RBM is trained using 1-step CD algorith-
m. During the training stage, a small weight-cost
of 0.0002 is used, and the learning rate for textu-
al data modal is 0.05 while the non-textual data is
0.001. We also adopt a monument of 0.5 for the
first five epochs and 0.9 for the rest epochs. In
addition, all non-textual data vectors are normal-
ized to have zero mean and unit standard variance.
More details for training the deep architecture can
be found in Hinton (2012).
4.2 Results and Analysis
In the first experiment, we compare the perfor-
mance of mDBN with different methods. To make
a fare comparison, we use the liblinear toolkit4 for
logistic regression model with L2 regularization
and randomly select 70% QA pairs as training data
4available at http://www.csie.ntu.edu.tw/ cjlin/liblinear
845
and the rest 30% as testing data. Table 3 and Ta-
ble 4 summarize the average results of the 5 round
experiments on YAHOO and ZHIDAO respectively.
Methods P R F1 Accu.
LR-T 0.374 0.558 0.448 0.542
LR-N 0.524 0.614 0.566 0.686
LR-C 0.493 0.557 0.523 0.662
DBN-T 0.496 0.571 0.531 0.663
DBN-N 0.505 0.578 0.539 0.670
mDBN 0.534 0.631 0.579 0.694
Table 3: Comparing results on YAHOO
It is promising to see that the proposed mDBN
method notably outperforms almost all the other
methods on both datasets over all the metrics as
expected, except for the recall on ZHIDAO. The
main reason for the improvements is that the joint
representation learned by mDBN is able to com-
plement each modality perfectly. In addition, the
mDBN can extract stronger representation through
modeling semantic relationship between textual
and non-textual information, which can effectively
help distinguish more complicated answers from
high quality to low quality.
Methods P R F1 Accu.
LR-T 0.380 0.540 0.446 0.553
LR-N 0.523 0.735 0.611 0.688
LR-C 0.537 0.695 0.606 0.698
DBN-T 0.527 0.730 0.612 0.692
DBN-N 0.539 0.760 0.631 0.703
mDBN 0.590 0.755 0.662 0.743
Table 4: Comparing results on ZHIDAO
The classification performance of the textu-
al features are worse on average compared with
non-textual features, even when the feature learn-
ing strategy is employed. More interestingly, we
find the simple combinations of textual and non-
textual features don?t improve the classification
results compared with using non-textual features
alone.We conjecture that there are mainly three
reasons for the phenomena: First, this is due to the
fact that user-generated content is inherently noisy
with low word frequency, resulting in the sparsity
of employing textual feature. Second, non-textual
features (e.g., answer length) usually own strongly
statistical properties and feature sparsity problem
can be better relieved to some extent. Finally, s-
ince correlations between the textual features and
non-textual features are highly non-linear, con-
catenating these features simply sometimes can
submerge classification performance. In contrast,
mDBN enjoys the advantage of the shared repre-
sentation between textual features and non-textual
features using the deep learning architecture.
We also note that neither the mDBN nor other
approaches perform very well in predicting answer
quality across the two datasets. The best precision
on ZHIDAO and YAHOO are respectively 59.0%
and 53.4%, which means that there are nearly half
of the high quality answers not effectively identi-
fied. One of the possible reason is that the quali-
ty of the corpora influences the result significant-
ly. As shown in Table 2, each question on aver-
age receives more than 4 answers on ZHIDAO and
more than 10 on YAHOO. Therefore, it is possi-
ble that there are several answers with high quali-
ty to the same question. Selecting only one as the
high quality answer is relatively difficult for our
humans, not to mention for the models.
100 500 1000 2000 5000
# iterations
0.50
0.55
0.60
0.65
0.70
0.75
0.80
Precision Recall F1 Accuracy
Figure 3: Influences of iterations for mDBN
In the second experiment, we intend to exam-
ine the performance of mDBN with different num-
ber of iterations. Figure 3 depicts the metrics on
ZHIDAO when the iteration number is varied from
100 to 5000. From the result, the first observa-
tion is that increasing the number of iterations the
performance of mDBN can improve significant-
ly, obtaining the best results for iteration of 1000.
This clearly shows the representation power of the
mDBN again. However, after a large number of it-
erations (large than 1000), the mDBN has a detri-
mental performance. This may be explained by
with large number of iterations, the deep learning
architecture is easier to be overfitting. The similar
trend is also observed on YAHOO.
5 Conclusions and Future work
In this paper, we have provided a new perspec-
tive to predict the cQA answer quality: learning
an informative unified representation between tex-
tual and non-textual features instead of concate-
nating them simply. Specifically, we have pro-
posed a multimodal deep learning framework to
846
form the unified representation. We compare this
with the basic features both in isolation and in
combination. Experimental results have demon-
strated that our proposed approach can capture the
complementarity between textual and non-textual
features, which is helpful to improve the perfor-
mance for cQA answer quality prediction.
For the future work, we plan to explore more se-
mantic analysis to approach the issue for short tex-
t quality evaluation. Additionally, more research
will be taken to put forward other approaches for
learning multimodal representation.
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. Spe-
cial thanks to Chengjie Sun and Deyuan Zhang
for insightful suggestions. This work is supported
by National Natural Science Foundation of China
(NSFC) via grant 61272383 and 61100094.
References
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in
social media. In Proceedings of the internation-
al conference on Web search and web data mining,
pages 183?194. ACM.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems, pages 153?160.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World wide
web, pages 51?60. ACM.
L. Chen and R. Nayak. 2008. Expertise analysis in a
question answer portal for author ranking. In Inter-
national Conference on Web Intelligence and Intel-
ligent Agent Technology, volume 1, pages 134?140.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
G.E. Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural compu-
tation, 14(8):1771?1800.
G.E. Hinton. 2012. A practical guide to training re-
stricted boltzmann machines. Lecture Notes in Com-
puter Science, pages 599?619.
Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011.
Quality-biased ranking of short texts in microblog-
ging services. In Proceedings of the 5th Internation-
al Joint Conference on Natural Language Process-
ing, pages 373?382.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with
non-textual features. In Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 228?235. ACM.
H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. 2009.
Convolutional deep belief networks for scalable un-
supervised learning of hierarchical representation-
s. In Proceedings of the 26th Annual International
Conference on Machine Learning, pages 609?616.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y.
Ng. 2011. Multimodal deep learning. In Proceed-
ings of the 28th International Conference on Ma-
chine Learning (ICML), pages 689?696.
R. Salakhutdinov and G.E. Hinton. 2009. Deep boltz-
mann machines. In Proceedings of the internation-
al conference on artificial intelligence and statistics,
volume 5, pages 448?455.
C. Shah and J. Pomerantz. 2010. Evaluating and pre-
dicting answer quality in community qa. In Pro-
ceeding of the 33rd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 411?418.
N. Srivastava and R. Salakhutdinov. 2012. Multi-
modal learning with deep boltzmann machines. In
Advances in Neural Information Processing System-
s, pages 2231?2239.
B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010.
Modeling semantic relevance for question-answer
pairs in web social communities. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1230?1238. ACL.
B. Wang, B. Liu, X. Wang, C. Sun, and D. Zhang.
2011. Deep learning approaches to semantic rele-
vance modeling for chinese question-answer pairs.
ACM Transactions on Asian Language Information
Processing, 10(4):21:1?21:16.
Z.M. Zhou, M. Lan, Z.Y. Niu, and Y. Lu. 2012. Ex-
ploiting user profile information for answer ranking
in cqa. In Proceedings of the 21st international con-
ference on World Wide Web, pages 767?774. ACM.
847
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 67?72,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PAL: A Chatterbot System for Answering Domain-specific Questions 
Yuanchao Liu1 Ming Liu1 Xiaolong Wang1 Limin Wang2 Jingjing Li1 
 
1 School of Computer Science and Technology, Harbin Institute of Technology, 
Harbin, China 
2. School of public health, Harbin Medical University, Harbin, China 
{lyc,mliu,wangxl,jjl}@insun.hit.edu.cn, wanglimin2008@163.com 
 
Abstract 
In this paper, we propose PAL, a prototype 
chatterbot for answering non-obstructive 
psychological domain-specific questions. This 
system focuses on providing primary 
suggestions or helping people relieve pressure 
by extracting knowledge from online forums, 
based on which the chatterbot system is 
constructed. The strategies used by PAL, 
including semantic-extension-based question 
matching, solution management with personal 
information consideration, and XML-based 
knowledge pattern construction, are described 
and discussed. We also conduct a primary test 
for the feasibility of our system. 
1 Introduction 
A wide variety of chatterbots and 
question-and-answer (Q&A) systems have been 
proposed over the past decades, each with 
strengths that make them appropriate for 
particular applications. With numerous advances 
in information construction, people increasingly 
aim to communicate with computers using natural 
language. For example, chatterbots in some 
e-commerce Web sites can interact with 
customers and provide help similar to a real-life 
secretary (DeeAnna Merz Nagel, 2011; Yvette 
Col?n, 2011). 
  In this paper, we propose PAL (Psychologist of 
Artificial Language), a chatterbot system for 
answering non-obstructive psychological 
questions. Non-obstructive questions refer to 
problems on family, human relationships, 
marriage, life pressure, learning, work and so on. 
In these cases, we expect the chatterbot to play an 
active role by providing tutoring, solution, 
support, advice, or even sympathy depending on 
the help needed by its users.  
  The difference of PAL from existing 
chatterbots lies not only in the specific research 
focus of this paper but also in the strategies we 
designed, such as P-XML templates for storing a 
knowledge base, comprehensive question 
matching method by considering both index and 
semantic similarities, and solution management 
by considering personal information. In the 
following sections, we will briefly discuss related 
work and then introduce our system and its main 
features. 
2 Related Work 
A number of research work on chatterbots 
(Rafael E. Banchs, Haizhou Li, 2012; Ai Ti Aw 
and Lian Hau Lee, 2012), Q&A systems (Shilin 
Ding, Gao Cong, Chin-Yew Lin, 2008; Leila 
Kosseim, 2008; Tiphaine Dalmas, 2007), and 
related natural language understanding 
technologies have recently been conducted 
(Walid S. Saba, 2007; Jing dong, 2009). Several 
studies on the application of natural language 
processing technologies for non-obstructive 
psychological Q&A systems have also been 
published (Hai-hu Shi, 2005).  
Several online psychology counselling Web 
sites with service provided by human experts have 
also been established recently (DeeAnna Merz 
Nagel, 2011; Yvette Col?n, 2011). For these Web 
sites, when the visitors ask similar questions, the 
expert may provide the same or very similar 
answers repeatedly. Based on this observation and 
consideration, we collected a large number of 
counselling Q&A pairs to extract common 
knowledge for the construction of a chatterbot 
system. Advances in automatic language analysis 
and processing are used as the bases for the 
emergence of a complex, task-oriented chatterbot 
system. 
67
3 Basic Framework of PAL 
A running screenshot of PAL is shown in Figure 
1, and its basic system structure is demonstrated 
in Figure 2. As shown in Figure 2, the basic 
principles of PAL are as follows: 
1) All interactions between system and users are 
scheduled by control logic; 
2) When the user inputs a question, the system 
will search through its knowledge base for 
the matching entry, and then 
3) The system will respond with an appropriate 
answer by analysing both the matched entry 
and the dialogue history. 
Figure 1. Running Screenshot of PAL 
 
Lexicon analysis
 &extracting 
features
Knowledge
 base
Dialog control 
logic
XML knowledge Engine
(Running in background)
User
Index 
generation
Semantic 
extension
Keyword 
extraction
Response
Question
Answer 
generation
Crawing Q&A pairs 
from on-line forums
 
Solution 
management
Dialog history 
analysis
 Figure 2. Basic Framework of PAL 
4 Conversation Control Strategy of PAL 
The Q&A process of the PAL system is 
coordinated by control logic to communicate with 
users effectively. The basic control logic strategy 
is shown in Figure 3.  
  
Figure 3. Basic Control Logic of PAL 
68
As shown in Figure 3, the initial state is set to 
welcome mode, and the system can select a 
sentence from the ?sign on? list, which will then 
provide a response. When users enter a question, 
the system will conduct the necessary analysis. 
The system?s knowledge base is indexed by 
Clucene1 beforehand. Thus, the knowledge index 
will be used to search the matched records quickly. 
If the system can find the matched patterns 
directly and the answer is suitable for the current 
user, one answer will be randomly selected to 
generate the response. Historical information and 
personal information will be analysed when 
necessary. We mainly adopted the method of 
ELIZA2
5 Knowledge Construction and 
Question Matching Method 
, which is an open-source program, to 
consider the historical information. A ?not found? 
response list is also set to deal with situations 
when no suitable answers can be identified. Both 
system utterance and user input will be pushed 
into the stack as historical information. Given that 
user questions are at times very simple, the 
combination with historical input may also be 
required to determine its meaning. This step can 
also avoid the duplication of utterances. 
We design P-XML to store the knowledge base 
for PAL, as shown in Figure 4. The knowledge 
base for PAL is mainly derived from the Q&A 
pairs in the BAIDU ZHIDAO community3
<?xml version="1.0" encoding="GB2312"?> 
. One 
question usually has many corresponding 
answers. 
<domain name="*"> <qapair speaker="*">        
<zhidao_question_title>*</zhidao_question_t
itle> 
<zhidao_question_content>*</zhidao_question
_content><zhidao_other_answer 
intersection_number="4">* 
<entity_and_problemword>*</entity_and_probl
emword> <peopleword>*</peopleword>          
</zhidao_other_answer>    
<title_extension>*</title_extension>   
</qapair> 
? 
</domain> 
Figure 4. The Structure of P-XML 
                                                          
1 http://sourceforge.net/projects/clucene/ 
2 http://www.codeforge.cn/article/191554 
3 http://Zhidao.baidu.com 
 
An effective method of capturing the user?s 
meaning accurately is to create an extension for 
questions in the knowledge base. In this paper, the 
extension is primarily a synonym expansion of the 
keywords of questions, with CILIN (Wanxiang 
Che, 2010) as extension knowledge source.  
The questions are indexed by Clucene to 
improve the retrieval efficiency of the search for a 
matched entry in the knowledge base. During the 
knowledge base searching step, both the index of 
the original form and the extension form of the 
problem are used to find the most possible 
matched record for the user?s question, as shown 
in algorithm 1. Algorithm 1 is used to examine the 
similarity between user input and the record 
returned by Clucene, including traditional and 
extension similarities.   
Algorithm 1. Problem-matching method 
Begin  
1) User inputs question Q; 
2) Search from the index of original questions and 
obtain the returned record set RS1; 
3) For the highest ranked record R1 in RS1, 
a) compute the similarity sim1 between 
question R1 and Q; 
b) compute the extension similarity sim2 
between the question extensions of R1 and 
Q;  
4) If sim1 is greater than the threshold value T1 or 
sim2 is greater than the threshold value T2, go to 
the solution management stage and obtain the 
answers of R1, and then find the candidate 
answer using algorithm 2; 
5) Otherwise, a ?not found? prompt is given.  
End 
6 Response Management Method 
 One question usually has many corresponding 
answers in the knowledge base, and these 
answers differ in explanation quality. Thus, the 
basic strategy employed by solution management 
is to select a reliable answer from the matched 
record as response, as shown in algorithm 2. 
Personalised information includes name entity, 
gender, marital status and age information. PAL 
maintains some heuristics rules to help recognize 
such information. Based on these rules, if one 
answer contains personal information, it will be 
selected as the candidate answer only when the 
personal information is consistent with that of the 
current user. Very concise answers that do not 
69
contain personal information can generally be 
selected as a candidate answer. 
 
Algorithm 2.  Answer-selection method 
Begin 
1) User inputs one question Q; 
2) The system extracts the speaker role S and 
personal information from Q; 
3) Use Q as query to conduct information retrieval 
from the index and knowledge base and obtain 
the top matched record set R; 
4) For each matched question Q? in R, test the 
following conditions: 
a) (condition 1) extract the speaker role S? 
in Q?, and examine if S? is equal to S; 
b) (condition 2) extract personal 
information in Q?, and examine if they 
are equal to that of in Q? 
c) For each answer A? of Q? 
i. If no personal information is found 
in A?, A? will be pushed into 
response list; 
ii. If personal information is contained 
in A? and if both conditions 1 and 2 
are true, A? will be pushed into 
response list; 
d) End for 
5) End for 
End 
7 Experiments 
For the current implementation of PAL, the size of 
the knowledge base is approximately 1.2G and 
contains six different topics: ?Husband and 
wife?, ?Family relations?, ?Love affairs?, 
?Adolescence?, ?Feeling and Mood?, and 
?Mental tutors?. Dialogue data collection used in 
PAL is mainly crawled from 
http://zhidao.baidu.com, which is one of the 
largest Chinese online communities. The 
criterion for choosing these six categories is also 
because they are the main topics in BAIDU 
communities about psychological problems. 
Some information on the knowledge base is 
given in Table 1, in which ?Percent of questions 
matched? denotes the number of similar 
questions found when 100 open questions are 
input (we suppose that if the similarity threshold 
is bigger than 0.5, then a similar question will be 
deemed as ?hit? in the knowledge base). 
In 7.1, we examine the feasibility of using 
downloaded dialogue collection for constructing 
the knowledge base. Some dialogue examples are 
given in 7.2.  
 
Domain Avg. ques. 
 length 
Num. of unique 
 Terms in ques. 
Avg. ans. 
 length 
Num. of unique 
terms in ans. 
Percent of questions 
matched (similarity threshold: 0.5) 
Size(MB) 
QS1 58.69 11571 64.13 27312 25 125 
QS2 54.96 10918 64.92 25185 24 292 
QS3 59.66 13530 49.52 13664 15 53 
QS4 42.41 8607 47.11 23492 22 224 
QS5 63.57 11915 48.86 26860 26 276 
QS6 31.82 10009 98.55 20896 25 216 
Table 1. Information of the knowledge base 
 
7.1 System Performance Evaluation 
Additional questions and their corresponding 
answers beyond the knowledge base are also used 
as a test set to evaluate system performance. 
Concretely, suppose question Q has |A| answers in 
the test set. Q is then input into the system. 
Suppose the system output is O, we examine if 
one best answer exists among |A| answers that are 
very similar to O (the similarity is greater than 
threshold T3). If yes, we then assume that one 
suitable answer has been found. In this way, 
precision can be calculated as the number of 
questions that have very similar answers in the 
system divided by the number of all input 
questions.  
The performance evaluation results are shown 
in Figure 5. The horizon axis denotes the 
similarity threshold (T1 for sim1 and T2 for sim2) 
between a user?s input and the questions in the 
knowledge base. Sim1 is the original similarity, 
whereas sim2 is the semantic extension similarity. 
Different thresholds were used (0.5 to 0.9). The 
similarity threshold T3 denotes the similarity 
70
between the answer in the test set and system 
output O. From Figures 5 (A) and (B), different 
T3 values were used (0.5 to 0.8).  
Some observations can be made from Figure 5. 
The average system precision is approximately 
0.5, and the range is from 0.2 to 0.9. Basically, 
when T3 is bigger, the system?s performance 
tends to decrease because a high T3 value denotes 
a strict evaluation standard. Performance also 
differs between different areas, such that D4, D5 
and D6 outperform than D1, D2 and D3.  
When only index is used and both sim1 and 
sim2 are below the corresponding threshold T1 or 
T2, the system can still return record set RS2, but 
the returned answer may be inconsistent with 
user?s question. Thus, incorporating semantic 
search shown in algorithm 1 is necessary. 
  
  
?A?(Sim. Thres. T3=0.5) ?B?(Sim. Thres. T3=0.6) 
  
(C) (Sim. Thres. T3=0.7) (D) (Sim. Thres. T3=0.8) 
Figure 5. Precision of PAL with different similarity thresholds T3 (The X axis denotes different thresholds 
for sim1 (T1) and sim2 (T2). For example, 0.5(sim1) indicates that the threshold for sim1 is 0.5, whereas 
0.5(sim2) suggests that the threshold for sim2 is 0.5. The Y axis stands for the precision value of different 
T1, T2 and T3 values for different areas). 
7.2?Some Interaction Examples 
Table 2 provides an interaction example of PAL in 
English. As users input their questions, the system 
can output the corresponding responses. The entire 
dialogue history can also be displayed. Basically, 
when users describe their troubles, PAL can 
provide some explanation or even try to console the 
user.  
Compared to other chatterbots like Eliza, PAL 
is domain-specific and may provide help to some 
extent. Many different answers can usually be 
identified. Thus, the user could input the same 
question many times, with the system providing 
suggestions from different angles. PAL can also 
output voice by using Xiao-kun and Microsoft 
Anna as the text-to-speech (TTS) engine. The 
speed, volume and echo properties of the voice can 
be adjusted.  
 
Dialog for Figure 1 (in English) 
PAL> Hello, you are welcome to communicate with 
me! You can input from the left side and press enter to 
get answer! 
I> How do I deal with the children who like to 
complain very frequently?  
PAL>Very simple. Don't? let him experience the 
benefits of complaining very frequently. 
I> Why do parents give children so much pressure  
PAL> Children have pressure indeed; but parents also 
have a lot of pressure.  
I> Why am I unable to control my emotions, and often 
quarrel with my parents?  
PAL.> Now you may be in the stage of treason. And 
have your own uncertain plan for the future, want to be 
yourself, and do not want to be disturbed. So when 
parents criticize, you will be unhappy. 
I> How to solve the gap problem?  
PAL.> Communication, and listening . 
Table 2. Example of an interaction with PAL (in English) 
 
71
8 Conclusions 
In this paper, we introduce a simple chatterbot for 
answering non-obstructive psychological 
questions. The knowledge behind the chatterbot is 
learned from the Q&A pairs derived from an 
online forum using several extraction strategies. 
The historical and personal information from the 
dialogues are also incorporated to output an 
appropriate answer. 
For future work, we expect to add more features 
to PAL, e.g., enabling the system to ask questions 
actively and further improving P-XML to form 
richer patterns for storing Q&A knowledge. 
Another interesting aspect would be to add speech 
input as well as TTS and to transform PAL into a 
mobile platform for widespread use.  
Acknowledgments 
This research was supported by the project of The 
National High Technology Research and 
Development Program (863 program) of PR China 
under a research Grant No.2007AA01Z172?
Youth Funds of China social & humanity science 
(10YJCZH099), and Key Laboratory Opening 
Funding of China MOE?MS Key Laboratory of 
Natural Language Processing and Speech 
(HIT.KLOF.2009022). 
References  
Ai Ti Aw and Lian Hau Lee. Personalized 
Normalization for a Multilingual Chat System. 
Proceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics, Jeju, 
Republic of Korea, 8-14 July 2012, pages 31?36, 
DeeAnna Merz Nagel, Kate Anthony. Text-based 
Online Counseling Chat. Online Counseling 
(Second Edition), 2011, Pages 169-182 
Hai-hu Shi, Yan Feng, LI Dong-mei, HU Ying-fei. 
Research on on-line psychology consultation expert 
system based on man-machine interaction technique. 
Computer Engineering and Design. 2005, 
26(12):3307-3309 
Jing dong. Research of sentiment model based on 
HMM and its application in psychological 
consulting expert system. Master?s thesis. Capital 
normal university (china), 2009. 
Leila Kosseim, Jamileh Yousefi. Improving the 
performance of question answering with 
semantically equivalent answer patterns. Data & 
Knowledge Engineering, 2008, 66(1):53-67 
Rafael E. Banchs, Haizhou Li. IRIS: a Chat-oriented 
Dialogue System based on the Vector Space Model. 
Proceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics, Jeju, 
Republic of Korea, 8-14 July 2012. pages 37?42 
Shilin Ding, Gao Cong, Chin-Yew Lin, Xiaoyan Zhu. 
Using Conditional Random Fields to Extract 
Contexts and Answers of Questions from Online 
Forums. Proceedings of 2008 Association for 
Computational Linguistics, Columbus, Ohio, 
USA, June 2008. pages 710?718 
Tiphaine Dalmas, Bonnie Webber. Answer comparison 
in automated question answering. Journal of 
Applied Logic, Volume 5, Issue 1, March 2007, 
Pages 104-120 
Walid S. Saba. Language, logic and ontology: 
Uncovering the structure of commonsense 
knowledge. International Journal of 
Human-Computer Studies, Volume 65, Issue 7, 
July 2007, Pages 610-623 
Wanxiang Che, Zhenghua Li, Ting Liu. LTP: A 
Chinese Language Technology Platform. In 
Proceedings of the Coling 
2010:Demonstrations. August 2010, pp13-16, 
Beijing, China. 
Yvette Col?n, Stephanie Stern. Counseling Groups 
Online: Theory and Framework. Online 
Counseling (Second Edition), 2011, Pages 
183-202. 
 
 
72
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860?865,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
___________________  
*Corresponding author 
Cross-lingual Opinion Analysis via Negative Transfer Detection  
Lin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang1 
1Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School, 
Harbin Institute of Technology, Shenzhen 518055 
2Department Of Computing, the Hong Kong Polytechnic University 
guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn, 
csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn 
 
Abstract 
Transfer learning has been used in opin-
ion analysis to make use of available lan-
guage resources for other resource scarce 
languages. However, the cumulative 
class noise in transfer learning adversely 
affects performance when more training 
data is used. In this paper, we propose a 
novel method in transductive transfer 
learning to identify noises through the 
detection of negative transfers. Evalua-
tion on NLP&CC 2013 cross-lingual 
opinion analysis dataset shows that our 
approach outperforms the state-of-the-art 
systems. More significantly, our system 
shows a monotonic increase trend in per-
formance improvement when more train-
ing data are used.  
1 Introduction 
Mining opinions from text by identifying their 
positive and negative polarities is an important 
task and supervised learning methods have been 
quite successful. However, supervised methods 
require labeled samples for modeling and the 
lack of sufficient training data is the performance 
bottle-neck in opinion analysis especially for re-
source scarce languages. To solve this problem, 
the transfer leaning method (Arnold et al, 2007) 
have been used to make use of samples from a 
resource rich source language to a resource 
scarce target language, also known as cross lan-
guage opinion analysis (CLOA). 
In transductive transfer learning (TTL) where 
the source language has labeled data and the tar-
get language has only unlabeled data, an algo-
rithm needs to select samples from the unlabeled 
target language as the training data and assign 
them with class labels using some estimated con-
fidence. These labeled samples in the target lan-
guage, referred to as the transferred samples, also 
have a probability of being misclassified. During 
training iterations, the misclassification introduc-
es class noise which accumulates, resulting in a 
so called negative transfer that affects the classi-
fication performance.  
In this paper, we propose a novel method 
aimed at reducing class noise for TTL in CLOA. 
The basic idea is to utilize transferred samples 
with high quality to identify those negative trans-
fers and remove them as class noise to reduce 
noise accumulation in future training iterations. 
Evaluations on NLP&CC 2013 CLOA evalua-
tion data set show that our algorithm achieves the 
best result, outperforming the current state-of-
the-art systems. More significantly, our system 
shows a monotonic increasing trend in perfor-
mance when more training data are used beating 
the performance degradation curse of most trans-
fer learning methods when training data reaches 
certain size. 
The rest of the paper is organized as follows. 
Section 2 introduces related works in transfer 
learning, cross lingual opinion analysis, and class 
noise detection technology. Section 3 presents 
our algorithm. Section 4 gives performance eval-
uation. Section 5 concludes this paper. 
2 Related works 
TTL has been widely used before the formal 
concept and definition of TTL was given in (Ar-
nold, 2007). Wan introduced the co-training 
method into cross-lingual opinion analysis (Wan, 
2009; Zhou et al, 2011), and Aue et al intro-
duced transfer learning into cross domain analy-
sis (Aue, 2005) which solves similar problems. 
In this paper, we will use the terms source lan-
guage and target language to refer to all cross 
lingual/domain analysis. 
Traditionally, transfer learning methods focus 
on how to estimate the confidence score of trans-
ferred samples in the target language or domain 
(Blitzer et al 2006, Huang et al, 2007; Sugiya-
ma et al, 2008, Chen et al 2011, Lu et al, 2011). 
In some tasks, researchers utilize NLP tools such 
as alignment to reduce the bias towards that of 
860
 the source language in transfer learning (Meng et 
al., 2012). However, detecting misclassification 
in transferred samples (referred to as class noise) 
and reducing negative transfers are still an unre-
solved problem. 
There are two basic methods for class noise 
detection in machine learning. The first is the 
classification based method (Brodley and Friedl, 
1999; Zhu et al 2003; Zhu 2004; Sluban et al, 
2010) and the second is the graph based method 
(Zighed et al 2002; Muhlenbach et al 2004; 
Jiang and Zhou, 2004). Class noise detection can 
also be applied to semi-supervised learning be-
cause noise can accumulate in iterations too. Li 
employed Zighed?s cut edge weight statistic 
method in self-training (Li and Zhou, 2005) and 
co-training (Li and Zhou, 2011). Chao used Li?s 
method in tri-training (Chao et al 2008). (Fuku-
moto et al 2013) used the support vectors to de-
tect class noise in semi-supervised learning.  
In TTL, however, training and testing samples 
cannot be assumed to have the same distributions. 
Thus, noise detection methods used in semi-
supervised learning are not directly suited in 
TTL. Y. Cheng has tried to use semi-supervised 
method (Jiang and Zhou, 2004) in transfer learn-
ing (Cheng and Li, 2009). His experiment 
showed that their approach would work when the 
source domain and the target domain share simi-
lar distributions. How to reduce negative trans-
fers is still a problem in transfer learning. 
3 Our Approach 
In order to reduce negative transfers, we pro-
pose to incorporate class noise detection into 
TTL. The basic idea is to first select high quality 
labeled samples after certain iterations as indica-
tor to detect class noise in transferred samples. 
We then remove noisy samples that cause nega-
tive transfers from the current accumulated train-
ing set to retain an improved set of training data 
for the remainder of the training phase. This neg-
ative sample reduction process can be repeated 
several times during transfer learning. Two ques-
tions must be answered in this approach: (1) how 
to measure the quality of transferred samples, 
and (2) how to utilize high quality labeled sam-
ples to detect class noise in training data. 
3.1 Estimating Testing Error 
To determine the quality of the transferred 
samples that are added iteratively in the learning 
process, we cannot use training error to estimate 
true error because the training data and the test-
ing data have different distributions. In this work, 
we employ the Probably Approximately Correct 
(PAC) learning theory to estimate the error 
boundary. According to the PAC learning theory, 
the least error boundary ? is determined by the 
size of the training set m and the class noise rate 
?, bound by the following relation: 
  ?   (   )                      ( ) 
In TTL, m increases linearly, yet ? is multi-
plied in each iteration. This means the signifi-
cance of m to performance is higher at the begin-
ning of transfer learning and gradually slows 
down in later iterations. On the contrary, the in-
fluence of class noise increases. That is why per-
formance improves initially and gradually falls to 
negative transfer when noise accumulation out-
performs the learned information as shown in 
Fig.1. In TTL, transferred samples in both the 
training data and test data have the same distribu-
tion. This implies that we can apply the PAC 
theory to analyze the error boundary of the ma-
chine learning model using transferred data. 
 
Figure 1 Negative transfer in the learning process 
According to PAC theorem with an assumed 
fixed probability ? (Angluin and Laird, 1988), 
the least error boundary ? is given by:   
  ?   (   ? )  ( (   ) )       ( ) 
where N is a constant decided by the hypothesis 
space.  In any iteration during TTL, the hypothe-
sis space is the same and the probability ? is 
fixed. Thus the least error boundary is deter-
mined by the size of the transferred sample m 
and the class noise of transferred samples ?. Ac-
cording to (2), we apply a manifold assumption 
based method to estimate ?. Let T be the number 
of iterations to serve as one period. We then es-
timate the least error boundary before and after 
each T to measure the quality of transferred sam-
ples during each T. If the least error boundary is 
reduced, it means that transferred samples used 
in this period are of high quality and can improve 
the performance. Otherwise, the transfer learning 
algorithm should stop.  
861
 3.2 Estimating Class Noise 
For formula (2) to work, we need to know the 
class noise rate ? to calculate the error boundary. 
Obviously, we cannot use conditional probabili-
ties from the training data in the source language 
to estimate the noise rate ? of the transferred 
samples because the distribution of source lan-
guage is different from that of target language. 
Consider a KNN graph on the transferred 
samples using any similarity metric, for example, 
cosine similarity, for any two connected vertex 
(     )and (     ) in the graph from samples to 
classes, the edge weight is given by: 
       (     )                         ( ) 
Furthermore, a sign function for the two vertices 
(     )and (     ), is defined as: 
    {
          
          
                   ( ) 
According to the manifold assumption, the 
conditional probability  (  |  ) can be approxi-
mated by the frequency of  (     ) which is 
equal to  (     ). In opinion annotations, the 
agreement of two annotators is often no larger 
than 0.8. This means that for the best cases 
 (     )=0.2. Hence     follows a Bernoulli 
distribution with p=0.2 for the best cases in 
manual annotations.  
Let      (     )  be the vertices that are 
connected to the     vertex, the statistical magni-
tude of the     vertex can be defined as: 
   ?                                 ( )  
where j refers to the     vertex that is connected 
to the     vertex.  
From the theory of cut edge statics, we know 
that the expectation of    is: 
    (     )  ?                  ( )  
And the variance of    is: 
  
   (     ) (     )  ?    
 
 ( )  
By the Center Limit Theorem (CLT),    fol-
lows the normal distribution: 
(     )
  
  (   )                    ( )  
To detect the noise rate of a sample (     ) , 
we can use (8) as the null hypothesis to test the 
significant level. Let    denotes probability of 
the correct classification for a transferred sample. 
   should follow a normal distribution,  
   
 
?    
?  
 
(    )
 
   
   
  
           ( )  
Note that experiments (Li and Zhou, 2011; 
Cheng and Li, 2009; Brodley and Friedl, 1999) 
have shown that     is related to the error rate of 
the example (     ), but it does not reflect the 
ground-truth probability in statistics. Hence we 
assume the class noise rate of example (     ) is: 
                              (  ) 
 We take the general significant level of 0.05 
to reject the null hypothesis. It means that if    of 
(     ) is larger than 0.95, the sample will be 
considered as a class noisy sample. Furthermore, 
   can be used to estimate the average class noise 
rate of a transferred samples in (2). 
In our proposed approach, we establish the 
quality estimate period T to conduct class noise 
detection to estimate the class noise rate of trans-
ferred samples. Based on the average class noise 
we can get the least error boundary so as to tell if 
an added sample is of high quality. If the newly 
added samples are of high quality, they can be 
used to detect class noise in transferred training 
data. Otherwise, transfer learning should stop. 
The flow chart for negative transfer is in Fig.2. 
SLS(labeled)
TLS
(unlabeled)
Classifier
Top k
TS
 period 1
TS
period 2
TS
 period n
KNN 
graph
Estimate ?i and ?n 
?n ? ?n-1?
Output SLS and TS 
(period 1 to n-1)
No
Yes
Del te TS
 ?i? 0.95 
period 1 to n-1
Input 
Input 
T iterations per period
Transfer
process
Negative
transfer
detection
Figure 2 Flow charts of negative transfer detection 
In the above flow chart, SLS and TLS refer to 
the source and target language samples, respec-
tively. TS refers to the transferred samples. Let T 
denote quality estimate period T in terms of itera-
tion numbers. The transfer process select k sam-
ples in each iteration. When one period of trans-
fer process finishes, the negative transfer detec-
tion will estimate the quality by comparing and 
either select the new transferred samples or re-
move class noise accumulated up to this iteration. 
4 Experiment 
4.1 Experiment Setting 
The proposed approach is evaluated on the 
NLP&CC 2013 cross-lingual opinion analysis (in 
862
 short, NLP&CC) dataset 1 . In the training set, 
there are 12,000 labeled English Amazon.com 
products reviews, denoted by Train_ENG, and 
120 labeled Chinese product reviews, denoted as 
Train_CHN, from three categories, DVD, BOOK, 
MUSIC. 94,651 unlabeled Chinese products re-
views from corresponding categories are used as 
the development set, denoted as Dev_CHN. In 
the testing set, there are 12,000 Chinese product 
reviews (shown in Table.1). This dataset is de-
signed to evaluate the CLOA algorithm which 
uses Train_CHN, Train_ENG and Dev_CHN to 
train a classifier for Test_CHN. The performance 
is evaluated by the correct classification accuracy 
for each category in Test_CHN2:  
          
                                  
    
 
where c is either DVD, BOOK or MUSIC. 
Team DVD Book Music 
Train_CHN 40 40 40 
Train_ENG 4000 4000 4000 
Dev_CHN 17814 47071 29677 
Test_CHN 4000 4000 4000 
Table.1 The NLP&CC 2013 CLOA dataset 
In the experiment, the basic transfer learning 
algorithm is co-training. The Chinese word seg-
mentation tool is ICTCLAS (Zhang et al 2003) 
and Google Translator3 is the MT for the source 
language. The monolingual opinion classifier is 
SVMlight4, word unigram/bigram features are em-
ployed. 
4.2 CLOA Experiment Results 
Firstly, we evaluate the baseline systems 
which use the same monolingual opinion classi-
fier with three training dataset including 
Train_CHN, translated Train_ENG and their un-
ion, respectively.  
 DVD Book Music Accuracy 
Train_CHN 0.552 0.513 0.500 0.522 
Train_ENG 0.729 0.733 0.722 0.728 
Train_CHN 
+Train_ENG 
0.737 0.722 0.742 0.734 
Table.2 Baseline performances  
It can be seen that using the same method, the 
classifier trained by Train_CHN are on avergage 
20% worse than the English counter parts.The 
combined use of Train_CHN and translated 
Train_ENG, however, obtained similar 
                                                 
1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 
2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 
3https://translate.google.com 
4http://svmlight.joachims.org/ 
performance to the English counter parts. This 
means the predominant training comes from the 
English training data. 
In the second set of experiment, we compare  
our proposed approach to the official results in 
NLP&CC 2013 CLOA evaluation and the result 
is given in Table 3. Note that in Table 3, the top 
performer of NLP&CC 2013 CLOA evaluation 
is the HLT-HITSZ system(underscored in the 
table), which used the co-training method in 
transfer learning (Gui et al 2013), proving that 
co-training is quite effective for cross-lingual 
analysis. With the additional negative transfer 
detection, our proposed approach achieves the 
best performance on this dataset outperformed 
the top system (by HLT-HITSZ) by a 2.97% 
which translate to 13.1% error reduction im-
provement to this state-of-the-art system as 
shown in the last row of Table 3.     
Team DVD Book Music Accuracy 
BUAA 0.481 0.498 0.503 0.494 
BISTU 0.647 0.598 0.661 0.635 
HLT-HITSZ 0.777 0.785 0.751 0.771 
THUIR 0.739 0.742 0.733 0.738 
SJTU 0.772 0.724 0.745 0.747 
WHU 0.783 0.770 0.760 0.771 
Our approach 0.816 0.801 0.786 0.801 
Error 
Reduction 
0.152 0.072 0.110 0.131 
Table.3 Performance compares with NLP&CC 
2013 CLOA evaluation results 
To further investigate the effectiveness of our 
method, the third set of experiments evaluate the 
negative transfer detection (NTD) compared to 
co-training (CO) without negative transfer 
detection as shown in Table.4 and Fig.3 Here, we 
use the union of Train_CHN and Train_ENG as 
labeled data and Dev_CHN as unlabeled data to 
be transferred in the learning algorithms. 
 DVD Book Music Mean 
NTD 
Best case 0.816 0.801 0.786 0.801 
Best period 0.809 0.798 0.782 0.796 
Mean 0.805 0.795 0.781 0.794 
CO 
Best case 0.804 0.796 0.783 0.794 
Best period 0.803 0.794 0.781 0.792 
Mean 0.797 0.790 0.775 0.787 
Table.4 CLOA performances 
Taking all categories of data, our proposed 
method improves the overall average precision 
(the best cases) from 79.4% to 80.1% when 
compared to the state of the art system which 
translates to error reduction of 3.40% (p-
value?0.01 in Wilcoxon signed rank test). Alt-
hough the improvement does not seem large, our 
863
  
   
Figure 3 Performance of negative transfer detection vs. co-training 
algorithm shows a different behavior in that it 
can continue to make use of available training 
data to improve the system performance. In other 
words, we do not need to identify the tipping 
point where the performance degradation can 
occur when more training samples are used. Our 
approach has also shown the advantage of stable 
improvement.  
In the most practical tasks, co-training based 
approach has the difficulty to determine when to 
stop the training process because of the negative 
transfer. And thus, there is no sure way to obtain 
the above best average precision. On the contrary, 
the performance of our proposed approach keeps 
stable improvement with more iterations, i.e. our 
approach has a much better chance to ensure the 
best performance. Another experiment is con-
ducted to compare the performance of our pro-
posed transfer learning based approach with su-
pervised learning. Here, the achieved perfor-
mance of 3-folder cross validation are given in 
Table 5. 
 DVD Book Music Average 
Supervised 0.833 0.800 0.801 0.811 
Our approach 0.816 0.801 0.786 0.801 
Table.5 Comparison with supervised learning  
The accuracy of our approach is only 1.0% 
lower than the supervised learning using 2/3 of 
Test_CHN. In the BOOK subset, our approach 
achieves match result. Note that the performance 
gap in different subsets shows positive correla-
tion to the size of Dev_CHN. The more samples 
are given in Dev_CHN, a higher precision is 
achieved even though these samples are unla-
beled. According to the theorem of PAC, we 
know that the accuracy of a classifier training 
from a large enough training set with confined 
class noise rate will approximate the accuracy of 
classifier training from a non-class noise training 
set. This experiment shows that our proposed 
negative transfer detection controls the class 
noise rate in a very limited boundary. Theoreti-
cally speaking, it can catch up with the perfor-
mance of supervised learning if enough unla-
beled samples are available. In fact, such an ad-
vantage is the essence of our proposed approach.  
5 Conclusion 
In this paper, we propose a negative transfer 
detection approach for transfer learning method 
in order to handle cumulative class noise and 
reduce negative transfer in the process of transfer 
learning. The basic idea is to utilize high quality 
samples after transfer learning to detect class 
noise in transferred samples. We take cross lin-
gual opinion analysis as the data set to evaluate 
our method. Experiments show that our proposed 
approach obtains a more stable performance im-
provement by reducing negative transfers. Our 
approach reduced 13.1% errors than the top sys-
tem on the NLP&CC 2013 CLOA evaluation 
dataset. In BOOK category it even achieves bet-
ter result than the supervised learning. Experi-
mental results also show that our approach can 
obtain better performance when the transferred 
samples are added incrementally, which in pre-
vious works would decrease the system perfor-
mance. In future work, we plan to extend this 
method into other language/domain resources to 
identify more transferred samples.  
Acknowledgement 
This research is supported by NSFC 61203378, 
61300112, 61370165, Natural Science Founda-
tion of GuangDong S2013010014475, MOE 
Specialized Research Fund for the Doctoral Pro-
gram of Higher Education 20122302120070,  
Open Projects Program of National Laboratory 
of Pattern Recognition?Shenzhen Foundational 
Research Funding JCYJ20120613152557576, 
JC201005260118A, Shenzhen International Co-
operation Research Funding 
GJHZ20120613110641217 and Hong Kong Pol-
ytechnic University Project code Z0EP. 
DVD Book Music 
864
 Reference 
Angluin, D., Laird, P. 1988. Learning from Noisy 
Examples. Machine Learning, 2(4): 343-370. 
Arnold, A., Nallapati, R., Cohen, W. W. 2007. A 
Comparative Study of Methods for Transductive 
Transfer Learning. In Proc. 7th IEEE ICDM Work-
shops, pages 77-82. 
Aue, A., Gamon, M. 2005. Customizing Sentiment 
Classifiers to New Domains: a Case Study, In Proc. 
of t RANLP. 
Blitzer, J., McDonald, R., Pereira, F. 2006. Domain 
Adaptation with Structural Correspondence Learn-
ing. In Proc. EMNLP, 120-128. 
Brodley, C. E., Friedl, M. A. 1999. Identifying and 
Eliminating Mislabeled Training Instances. Journal 
of Artificial Intelligence Research, 11:131-167. 
Chao, D., Guo, M. Z., Liu, Y.,  Li, H. F. 2008.  Partic-
ipatory Learning based Semi-supervised Classifica-
tion. In Proc. of 4th ICNC, pages 207-216. 
Cheng, Y., Li, Q. Y. 2009. Transfer Learning with 
Data Edit. LNAI, pages 427?434. 
Chen, M., Weinberger, K. Q.,  Blitzer, J. C. 2011.  
Co-Training for Domain Adaptation. In Proc. of 
23th NIPS. 
Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text 
Classification from Positive and Unlabeled Data 
using Misclassified Data Correction. In Proc. of 
51st ACL, pages 474-478. 
Gui, L., Xu, R.,  Xu, J., et al 2013. A Mixed Model 
for Cross Lingual Opinion Analysis. In CCIS, 400, 
pages 93-104. 
Huang, J., Smola, A., Gretton, A., Borgwardt, K.M., 
Scholkopf, B. 2007. Correcting Sample Selection 
Bias by Unlabeled Data. In Proc. of 19th NIPS,  
pages 601-608. 
Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for 
kNN Classifiers with Neural Network Ensemble. In 
LNCS, 3173,  pages 356-361. 
Li, M., Zhou, Z. H. 2005. SETRED: Self-Training 
with Editing. In Proc. of PAKDD, pages 611-621. 
Li, M., Zhou, Z. H. 2011.  COTRADE: Confident Co-
Training With Data Editing. IEEE Transactions on 
Systems, Man, and Cybernetics?Part B: Cyber-
netics, 41(6):1612-1627. 
Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011. 
Joint Bilingual Sentiment Classification with Un-
labeled Parallel Corpora. In Proc. of 49th ACL, 
pages 320-330. 
Meng, X. F., Wei, F. R., Liu, X. H., et al 2012. 
Cross-Lingual Mixture Model for Sentiment Clas-
sification. In Proc. of 50th ACL, pages 572-581. 
Muhlenbach, F., Lallich, S., Zighed, D. A. 2004. 
Identifying and Handling Mislabeled Instances.  
Journal of Intelligent Information System, 22(1): 
89-109. 
Pan, S. J., Yang, Q. 2010. A Survey on Transfer 
Learning, IEEE Transactions on Knowledge and 
Data Engineering, 22(10):1345-1360. 
Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for 
Multi-view Learning and Manifold Co-
Regularization. In Proc. of 25th  ICML, pages 976?
983. 
Sluban, B., Gamberger, D., Lavra, N. 2010.  Advanc-
es in Class Noise Detection. In Proc.19th ECAI,  
pages 1105-1106. 
Sugiyama, M.,  Nakajima, S., Kashima, H., Buenau, 
P.V., Kawanabe, M. 2008. Direct Importance Es-
timation with Model Selection and its Application 
to Covariate Shift Adaptation. In Proc. 20th NIPS. 
Wan, X. 2009. Co-Training for Cross-Lingual Senti-
ment Classification, In Proc. of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP,  235?243. 
Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q. 
2003. HHMM-based Chinese Lexical Analyzer 
ICTCLAS. In 2nd SIGHAN workshop affiliated 
with 41th ACL, pages 184-187. 
 Zhou, X., Wan X., Xiao, J. 2011. Cross-Language 
Opinion Target Extraction in Review Texts. In 
Proc. of IEEE 12th ICDM, pages 1200-1205. 
Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003.  Eliminating 
Class Noise in Large Datasets. In Proc. of 12th 
ICML, pages 920-927. 
Zhu, X. Q. 2004. Cost-guided Class Noise Handling 
for Effective Cost-sensitive Learning In Proc. of 4th  
IEEE ICDM,  pages 297-304. 
Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.  
Separability Index in Supervised Learning. In Proc. 
of PKDD, pages 475-487. 
865
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 25?30,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
WINGS: Writing with Intelligent Guidance and Suggestions 
 
Xianjun Dai, Yuanchao Liu*, Xiaolong Wang, Bingquan Liu 
School of Computer Science and Technology 
Harbin Institute of Technology, China 
{xjdai, lyc, wangxl, liubq}@insun.hit.edu.cn 
 
 
 
Abstract 
Without inspirations, writing may be a 
frustrating task for most people. In this study, 
we designed and implemented WINGS, a 
Chinese input method extended on 
IBus-Pinyin with intelligent writing assistance. 
In addition to supporting common Chinese 
input, WINGS mainly attempts to spark users? 
inspirations by recommending both word 
level and sentence level writing suggestions. 
The main strategies used by WINGS, 
including providing syntactically and 
semantically related words based on word 
vector representation and recommending 
contextually related sentences based on LDA, 
are discussed and described. Experimental 
results suggest that WINGS can facilitate 
Chinese writing in an effective and creative 
manner. 
1 Introduction 
Writing articles may be a challenging task, as we 
usually have trouble in finding the suitable words 
or suffer from lack of ideas. Thus it may be very 
helpful if some writing reference information, 
e.g., words or sentences, can be recommended 
while we are composing an article. 
On the one hand, for non-english users, e.g., 
Chinese, the Chinese input method is our first 
tool for interacting with a computer. Nowadays, 
the most popular Chinese input methods are 
Pinyin-based ones, such as Sougou Pinyin1 and 
Google Pinyin 2 . These systems only present 
accurate results of Pinyin-to-Character 
conversion. Considering these systems? lack of 
suggestions for related words, they hardly 
provide writers with substantial help in writing. 
On the other hand, try to meet the need of writing 
assistance, more and more systems facilitating 
Chinese writing have been available to the public, 
                                                          
* Corresponding author 
1 http://pinyin.sogou.com 
2 http://www.google.com/intl/zh-CN/ime/pinyin 
such as WenXin Super Writing Assistant3 and 
BigWriter4, and among others. However, due to 
their shortcomings of building examples library 
manually and lack of corpus mining techniques, 
most of the time the suggestions made by these 
systems are not creative or contextual. 
  Thus, in this paper, we present Writing with 
INtelligent Guidance and Suggestions (WINGS)5, 
a Chinese input method extended with intelligent 
writing assistance. Through WINGS, users can 
receive intelligent, real-time writing suggestions, 
including both word level and sentence level. 
Different from existing Chinese writing assistants, 
WINGS mainly attempts to spark users? writing 
inspirations from two aspects: providing diverse 
related words to expand users? minds and 
recommending contextual sentences according to 
their writing intentions. Based on corpus mining 
with Natural Language Processing techniques, 
e.g., word vector representation and LDA model, 
WINGS aims to facilitate Chinese writing in an 
effective and creative manner. 
  For example, when using WINGS to type 
?xuxurusheng?, a sequence of Chinese Pinyin 
characters for ?????? (vivid/vividly), the 
Pinyin-to-Character Module will generate ???
??? and some other candidate Chinese words. 
Then the Words Recommending Module 
generates word recommendations for ????
? ?. The recommended words are obtained 
through calculating word similarities based on 
word vector representations as well as rule-based 
strategy (POS patterns). 
In the Sentences Recommending Module, we 
first use ????? ? to retrieve example 
sentences from sentences library. Then the topic 
similarities between the local context and the 
candidate sentences are evaluated for contextual 
                                                          
3 http://www.xiesky.com 
4 http://www.zidongxiezuo.com/bigwriter_intro.php 
5 The DEB package for Ubuntu 64 and recorded video of 
our system demonstration can be accessed at this URL: 
http://yunpan.cn/Qp4gM3HW446Rx (password:63b3) 
25
Chinese Pinyin Sequence
Recommended Words
Recommended Sentences
Pinyin-to-Character results (Original Words)
 
Figure 1. Screenshot of WINGS.  
 
sentence recommendations. 
At last in consideration of users? feedback, we 
introduce a User Feedback Module to our system. 
The recorded feedback data will in turn influence 
the scores of words and sentences in 
Recommending Modules above. 
Figure 1 shows a screenshot of WINGS. 
2 Related Work 
2.1 Input Method 
Chinese input method is one of the most 
important tools for Chinese PC users. Nowadays, 
Pinyin-based input method is the most popular 
one. The main strategy that Pinyin-based input 
method uses is automatically converting Pinyin 
to Chinese characters (Chen and Lee, 2000).  
In recent years, more and more intelligent 
strategies have been adopted by different input 
methods, such as Triivi 6 , an English input 
method that attempts to increase writing speed 
by suggesting words and phrases, and PRIME 
(Komatsu et al., 2005), an English/Japanese 
input system that utilizes visited documents to 
predict the user?s next word to be input. 
In our system the basic process was Pinyin ? 
Characters (words) ? Writing Suggestions 
(including words and sentences). We mainly 
focused on writing suggestions from Characters 
(words) in this paper. As the Pinyin-to-Character 
was the underlining work, we developed our 
system directly on the open source framework of 
the IBus (an intelligent input Bus for Linux and 
Unix OS) and IBus-Pinyin7 input method. 
2.2 Writing Assistant 
As previously mentioned, several systems are 
available in supporting Chinese writing, such as 
WenXin Super Writing Assistant and Big Writer. 
                                                          
6 http://baike.baidu.com/view/4849876.htm 
7 https://code.google.com/p/ibus 
These systems are examples of a retrieval-based 
writing assistant, which is primarily based on a 
large examples library and provides users with a 
search function. 
In contrast, other writing assistants employ 
special NLP strategies. Liu et al. (2011, 2012) 
proposed two computer writing assistants: one 
for writing love letters and the other for blog 
writing. In these two systems, some special 
techniques were used, including text generation, 
synonym substitution, and concept expansion. 
PENS (Liu et al., 2000) and FLOW (Chen et al., 
2012) are two writing assistants designed for 
students of English as a Foreign Language (EFL) 
practicing writing, which are mainly based on 
Statistical Machine Translation (SMT) strategies. 
Compared with the above mentioned systems, 
WINGS is closer to retrieval-based writing 
assistants in terms of function. However, WINGS 
can provide more intelligent suggestions because 
of the introduction of NLP techniques, e.g., word 
vector representation and topic model. 
2.3 Word Representations in Vector Space 
Recently, Mikolov et al. (2013) proposed novel 
model architectures to compute continuous 
vector representations of words obtained from 
very large data sets. The quality of these 
representations was assessed through a word 
similarity task, and according to their report, the 
word vectors provided state-of-the-art 
performance for measuring syntactic and 
semantic word similarities in their test set. Their 
research produced the open source tool 
word2vec8. 
In our system, we used word2vec to train the 
word vectors from a corpus we processed 
beforehand. For the Words Recommending 
Module, these vectors were used to determine the 
similarity among different words. 
                                                          
8 https://code.google.com/p/word2vec 
26
2.4 Latent Dirichlet Allocation 
The topic model Latent Dirichlet Allocation 
(LDA) is a generative probabilistic model of a 
corpus. In this model, documents are represented 
as random mixtures of latent topics, where each 
topic is characterized by the distribution of 
words (Blei et al., 2003). Each document can 
thus be represented as a distribution of topics. 
Gibbs Sampling is a popular and efficient 
strategy used for LDA parameter estimation and 
inference. This technique is used in 
implementing several open sourcing LDA tools, 
such as GibbsLDA++9 (Phan and Nguyen, 2007), 
which was used in this paper. 
In order to generate contextual sentence 
suggestions, we ensured that the sentences 
recommended to the user were topic related to 
the local context (5-10 words previously input) 
based on the LDA model.  
3 Overview of WINGS 
Figure 2 illustrates the overall architecture of 
WINGS.  
Start
Pinyin to Character
Convert pinyin to Chinese words 
(Original words)
Words Recommending 1
1. Calculate similarity between focused 
original word and the rest words in the 
dictionary
2. Get top 200 most similar words as 
the candidate words
Words and word 
vectors
Sentences 
index
Sentences Recommending 1
Use the focused original or 
recommended word to retrieve at most  
200 sentences by Clucene from 
sentences index.
S ntenc s and 
their 
topic vector 
Sentences Recommending 2
1. Infer the topic vector of the local 
context by Gibbs Sammpling. Calculate 
the KL divergence between the local 
context and candidate sentences.
2. The sentence has been used before 
will get a boost in score.
1. Select word or sentence as input
2. Save feedback(User Feedback)
LDA train result 
for inference
Input Pinyin
Pinyin-Character 
mapping data,etc.
Words and 
s ntences 
selected info
YES
End
Continue
NO
Words Recommending 2
1.Boost in score: 1).Whether the 
original and recommended word 
match one of the specified patterns, 
such as A-N, V-N and etc. 2). Whether 
The word has been used before
2. Re-rank candidate words.
 
Figure 2. Overall architecture of WINGS. 
3.1 System Architecture 
Our system is composed of four different 
                                                          
9 http://gibbslda.sourceforge.net 
modules: Pinyin-to-Character Module, Words 
Recommending Module, Sentences 
Recommending Module, and User Feedback 
Module. The following sub-sections discuss 
these modules in detail. 
3.2 Pinyin-to-Character Module 
Our system is based on the open sourcing input 
framework IBus and extended on the 
IBus-Pinyin input method. Thus, the 
Pinyin-to-Character module is adopted from the 
original IBus-Pinyin system. This module 
converts the input Chinese Pinyin sequence into 
a list of candidate Chinese words, which we refer 
to as original words. 
3.3 Words Recommending Module 
? Words vector representations 
In this preparatory step for word 
recommendation, words vector representations 
are obtained using the word2vec tool. This will 
be described in detail in Section 4. 
? Obtain the most related words 
Our system will obtain the focused original 
word and calculate the cosine similarities 
between this word and the rest of the words in 
the dictionary. Thus, we can obtain the top 200 
most similar words according to their cosine 
values. These words are referred to as 
recommended words. According to Mikolov et 
al. (2013), these words are syntactically and 
semantically similar to the original word. 
? Re-rank the recommended words 
In order to further improve word recommending, 
we introduce several special POS patterns (Table 
1). If the POS of the original word and the 
recommended word satisfy one of the POS 
patterns we specified, the score (based on the 
cosine similarity) of the recommended word will 
be boosted. In addition, the score of the word 
selected by the user before will also be boosted. 
Therefore, these words will be ranked higher in 
the recommended words list. 
POS of  
original word 
POS of  
recommended word 
N (noun) A (adjective) 
A (adjective) N (noun) 
N (noun) V (verb) 
Any POS Same with the original word 
Any POS L (idiom) 
Table 1. Special POS patterns. 
3.4 Sentences Recommending Module 
? Sentences topic distribution 
In this preparatory step for sentence 
27
recommendation, sentences topic distribution 
vectors and other parameters are trained using 
the GibbsLDA++. This step will be discussed in 
Section 4. 
? Retrieve relative sentences via CLucene 
The focused original or recommended word will 
be used to search the most related sentences in 
the sentences index via CLucene10. At most 200 
sentences will be taken as candidates, which will 
be called recommended sentences. 
? Re-rank the recommended sentences 
To ensure that the recommended sentences are 
topic related to our local input context (5-10 
words previously input), we use Gibbs Sampling 
to infer the topic vector of the local context, and 
calculate the KL divergence between the local 
context and each recommended sentence. Finally, 
the recommended sentences will be re-ranked 
based on their KL divergences value with respect 
to the local context and the boost score derived 
from the feedback information. 
3.5 User Feedback Module 
This module saves the users? feedback 
information, particularly the number of times 
when users select the recommended words and 
sentences. This information will be used as a 
boost factor for the Words and Sentences 
Recommending Modules. Our reasons for 
introducing this module are two-fold: the users? 
feedback reflects their preference, and at the 
same time, this information can somewhat 
indicate the quality of the words and sentences. 
4 Data Pre-processing 
In this section, the procedure of our data 
pre-processing is discussed in detail. Firstly, our 
raw corpus was crawled from DiYiFanWen11, a 
Chinese writing website that includes all types of 
writing materials. After extracting useful 
composition examples from each raw html file, 
we merged all articles into a single file named 
large corpus. Finally, a total of 324,302 articles 
were merged into the large corpus (with a total 
size of 320 MB). 
For words recommending, each of the articles 
in our large corpus was segmented into words by 
ICTCLAS 12  with POS tags. Subsequently, 
word2vec tool was used on the words sequence 
(with useless symbols filtered). Finally, the 
words, their respective vector representations and 
                                                          
10 http://sourceforge.net/projects/clucene 
11 http://www.diyifanwen.com 
12 http://ictclas.nlpir.org 
main POS tags were combined, and we built 
these data into one binary file. 
For sentences recommending, the large corpus 
was segmented into sentences based on special 
punctuations. Sentences that were either too long 
or too short were discarded. Finally, 2,567,948 
sentences were left, which we called original 
sentences. An index was created on these 
sentences using CLucene. Moreover, we 
segmented these original sentences and filtered 
the punctuations and stop words. Accordingly, 
these new sentences were named segmented 
sentences. We then ran GibbsLDA++ on the 
segmented sentences, and the Gibbs sampling 
result and topic vector of each sentence were 
thus obtained. Finally, we built the original 
sentence and their topic vectors into a binary file. 
The Gibbs sampling data used for inference was 
likewise saved into a binary file. 
  Table 2 lists all information on the resources 
of WINGS.  
Items Information 
Articles corpus size 320 MB 
Articles total count 324,302 
Words total count 101,188 
Sentences total count 2,567,948 
Table 2. Resources information. 
5 Experimental Results 
This section discusses the experimental results of 
WINGS. 
5.1 Words Recommending 
The top 20 recommended words for the sample 
word ???? (teacher) are listed in Table 3. 
Compared with traditional methods (using Cilin, 
Hownet, and so forth.), using the word vectors to 
determine related words will identify more 
diverse and meaningful related words and this 
quality of WINGS is shown in Table 4. With the 
diversity of recommended words, writers? minds 
can be expanded easily.  
1-10: ??(student), ??(conduct class), ??
?(Chinese class), ????(with sincere words 
and earnest wishes), ????(affability), ??
(guide), ?? (lecture), ?? (dais), ????
(patient), ??(the whole class) 
11-20: ??(finish class), ???(remarks), ?
??(math class), ???(be absent-minded), ?
? (ferule), ??? (class adviser), ????
(restless), ??(remember), ????????
(excel one?s master), ??(listen to) 
Table 3. Top 20 recommended words for ???? 
(teacher). 
28
Words about Words 
Person ??, ???, ?? 
Quality ????, ????, ?
??? 
Course ???, ??? 
Teaching ??, ??, ??, ?? 
Teaching facility ??, ?? 
Student behaviour ??, ???, ???? 
Special idiom ???????? 
Others ??, ??? 
Table 4. Diversity of recommended words for 
???? (teacher). 
5.2 Sentences Recommending 
By introducing the topic model LDA, the 
sentences recommended by WINGS are related to 
the topic of the local context. Table 5 presents 
the top 5 recommended sentences for the word 
?????? (vivid/vividly) in two different local 
contexts: one refers to characters in books; the 
other refers to statues and sculptures. Most 
sentences in the first group are related to the first 
context, and most from the second group are 
related to the second context. 
In order to assess the performance of WINGS 
in sentence recommendation, the following 
evaluation was implemented. A total of 10 
Chinese words were randomly selected, and each 
word was given two or three different local 
contexts as above (contexts varied for different 
words). Finally, we obtained a total of 24 groups 
of data, each of which included an original word, 
a local context, and the top 10 sentences 
recommended by WINGS. To avoid the influence 
of personal preferences, 12 students were invited 
to judge whether each sentence in the 24 
different groups was related to their respective 
local context. We believed that a sentence was 
related to its context only when at least 70% of 
the evaluators agreed. The Precision@10 
measure in Information Retrieval was used, and 
the total average was 0.76, as shown in Table 6. 
Additionally, when we checked the sentences 
which were judged not related to their respective 
local context, we found that these sentences were 
generally too short after stop words removal, and 
as a result the topic distributions inferred from 
Gibbs Sampling were not that reliable. 
Context 1 is about characters in books:  
?? (story), ?? (character), ?? (image), 
??(works) 
1??????????????? 
(The characters of this book are depicted 
vividly) 
2???????????????????
?  
(The characters of this book are depicted vividly 
and the story is impressive narrative) 
3????????????  
(The characters of this story are depicted 
vividly) 
4???????????????????
??? 
(His works are full of plot twists, vivid 
characters, and surprising endings) 
5??????????????????  
(The characters in the book are depicted vividly 
by Jing Zhuge) 
Context 2 is about statues and sculptures:  
?? (statue), ?? (sculpture), ?? (stone 
inscription), ??(temple) 
1??????????????  
(The walls are painted with mighty and vivid 
dragons) 
2????????????????  
(On both sides there are standing 18 vivid Arhats 
with different manners) 
3?????????????????  
(the Great Buddha Hall is grand and the statues 
there are vivid) 
4????????????  
(Each statue is vivid and lifelike) 
5???????????????????
??????  
(On each of the eave angles there are 7 vivid 
statues of animals and birds with special 
meanings) 
Table 5. Top 5 recommended sentences for ??
???? (vivid/vividly) in two different local 
contexts.  
 
 
Local 
Context 
word 
1 
word 
2 
word 
3 
word 
4 
word 
5 
word 
6 
word 
7 
word 
8 
word 
9 
word 
10 
1 0.9 0.3 0.9 0.6 0.7 0.8 0.6 0.8 1.0 0.9 
2 0.4 0.7 1.0 0.9 0.9 0.7 1.0 0.5 0.9 0.5 
3 0.9 N/A N/A N/A N/A 0.9 0.8 N/A N/A 0.7 
Average Precision@10 value of the 24 groups data                0.76 
Table 6. Precision@10 value of each word under their respective context and the total average. 
29
5.3 Real Time Performance 
In order to ensure the real time process for each 
recommendation, we used CLucene to index and 
retrieve sentences and memory cache strategy to 
reduce the time cost of fetching sentences? 
information. Table 7 shows the average and max 
responding time of each recommendation of 
randomly selected 200 different words (Our test 
environment is 64-bit Ubuntu 12.04 LTS OS on 
PC with 4GB memory and 3.10GHz Dual-Core 
CPU). 
 
Item Responding time 
Average 154 ms 
Max 181 ms 
Table 7. The average and max responding time 
of 200 different words? recommending process 
6 Conclusion and Future Work 
In this paper, we presented WINGS, a Chinese 
input method extended with writing assistance 
that provides intelligent, real-time suggestions 
for writers. Overall, our system provides 
syntactically and semantically related words, as 
well as recommends contextually related 
sentences to users. As for the large corpus, on 
which the recommended words and sentences are 
based, and the corpus mining based on NLP 
techniques (e.g., word vector representation and 
topic model LDA), experimental results show 
that our system is both helpful and meaningful. 
In addition, given that the writers? feedback is 
recorded, WINGS will become increasingly 
effective for users while in use. Thus, we believe 
that WINGS will considerably benefit writers. 
  In future work, we will conduct more user 
experiments to understand the benefits of our 
system to their writing. For example, we can 
integrate WINGS into a crowdsourcing system 
and analyze the improvement in our users? 
writing. Moreover, our system may still be 
improved further. For example, we are interested 
in adding a function similar to Google Suggest, 
which is based on the query log of the search 
engine, in order to provide more valuable 
suggestions for users. 
 
 
 
 
 
 
 
 
References 
David M. Blei, Andrew Y. Ng and Michael I. Jordan. 
2003. Latent dirichlet allocation. the Journal of 
machine Learning research, 3, pages 993-1022. 
Mei-Hua Chen, Shih-Ting Huang, Hung-Ting Hsieh, 
Ting-Hui Kao and Jason S. Chang. 2012. FLOW: a 
first-language-oriented writing assistant system. In 
Proceedings of the ACL 2012 System 
Demonstrations, pages 157-162. 
Zheng Chen and Kai-Fu Lee. 2000. A new statistical 
approach to Chinese Pinyin input. In Proceedings 
of the 38th annual meeting on association for 
computational linguistics, pages 241-247. 
Hiroyuki Komatsu, Satoru Takabayash and Toshiyuki 
Masui. 2005. Corpus-based predictive text input. In 
Proceedings of the 2005 international conference 
on active media technology, pages 75?80. 
Chien-Liang Liu, Chia-Hoang Lee, Ssu-Han Yu and 
Chih-Wei Chen. 2011. Computer assisted writing 
system. Expert Systems with Applications, 38(1), 
pages 804-811. 
Chien-Liang Liu, Chia-Hoang Lee and Bo-Yuan Ding. 
2012. Intelligent computer assisted blog writing 
system. Expert Systems with Applications, 39(4), 
pages 4496-4504. 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and 
Changning Huang. 2000. PENS: A machine-aided 
English writing system for Chinese users. In 
Proceedings of the 38th Annual Meeting on 
Association for Computational Linguistics, pages 
529-536. 
Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey 
Dean. 2013. Efficient estimation of word 
representations in vector space. arXiv:1301.3781. 
Xuan-Hieu Phan and Cam-Tu Nguyen. 2007. 
GibbsLDA++: A C/C++ implementation of latent 
Dirichlet allocation (LDA). 
30
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 76?82,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
A Mixed Deterministic Model for Coreference Resolution 
         
Bo Yuan1, Qingcai Chen, Yang Xiang, Xiaolong Wang2 
Liping Ge, Zengjian Liu, Meng Liao, Xianbo Si 
Intelligent Computing Research Center, Key Laboratory of Network Oriented Intelligent 
Computation, Computer Science and technology Department, Harbin Institute of Technology 
Shenzhen graduate School, Shenzhen, Guangdong, 518055, China 
{yuanbo.hitsz1, windseedxy, qingcai.chen, geliping123, 
autobotsonearth, dream2009gd, sixianbo}@gmail.com 
wangxl@insun.hit.edu.cn2 
 
 
 
Abstract 
This paper presents a mixed deterministic 
model for coreference resolution in the 
CoNLL-2012 shared task. We separate the 
two main stages of our model, mention 
detection and coreference resolution, into 
several sub-tasks which are solved by 
machine learning method and  
deterministic rules based on multi-filters, 
such as lexical, syntactic, semantic, gender 
and number information. We participate in 
the closed track for English and Chinese, 
and also submit an open result for Chinese 
using tools to generate the required features. 
Finally, we reach the average F1 scores 
58.68, 60.69 and 61.02 on the English 
closed task, Chinese closed and open tasks.  
1 Introduction 
The coreference resolution task is a complicated 
and challenging issue of natural language 
processing. Although many sub-problems, such as 
noun phrase to noun phrase and pronouns to noun 
phrase, are contained in this issue, it is interesting 
that humans do not get too confused when they 
determine whether two mentions refer to the same 
entity. We also believe that automatic systems 
should copy the human behavior (Kai-Wei et al, 
2011). In our understanding, the basis for human 
making judgment on different sub-problems is 
different and limited. Although there are some 
complicated and ambiguous cases in this task, and 
we are not able to cover all the prior knowledge of 
human mind, which plays a vital role in his 
solution, the mixed deterministic model we 
constructed can solve a big part of this task. We 
present a mixed deterministic model for 
coreference resolution in the CoNLL-2012 shared 
task (Sameer et al, 2011). 
Different methods such as Relaxation labeling 
(Emili et al, 2011), Best-Link (Kai-Wei et al, 
2011), Entropy Guided Transformation Learning 
(Cicero et al, 2011) and deterministic models 
(Heeyoung et al, 2011), were attempted in the 
CoNLL-2011 shared task (Sameer et al, 2011). 
The system performance reported by the task 
shows that a big part of this task has been solved 
but some sub-problems need more exploration. 
We also participate in the Chinese closed and 
open tracks. However, the lack of linguistic 
annotations makes it more difficult to build a 
deterministic model. Basic solutions such as Hobbs 
Algorithm and Center Theory have been listed in 
(Wang et al, 2002; Jun et al, 2007). The recent 
research on Chinese contains non-anaphors 
detection using a composite kernel (Kong Fang, et 
al., 2012(a)) and a tree kernel method to anaphora 
resolution of pronouns (Kong Fang et al, 2012(b)). 
We accept the thought of Stanford (Karthik et al, 
2010; Heeyoung et al, 2011). In Stanford system 
the coreference resolution task is divided into 
several problems and each problem is solved by 
rule based methods. For English we did some 
research on mention detection which uses Decision 
Tree to decide whether the mention ?it? should 
refer to some other mention. For Chinese we 
submit closed and open result. The lack of gender, 
76
number and name entities make it more difficult 
for the Chinese closed task and we try to extract 
information from the training data to help enhance 
the performance. For the open task, we use some 
dictionaries such as appellation dictionary, gender 
dictionary, geographical name dictionary and 
temporal word dictionary (Bo et al, 2009), and 
some tools such as conversion of pinyin-to-
character and LTP which is a Chinese parser that 
can generate the features such as Part-of-Speech, 
Parse bit, Named Entities (Liu et al, 2011) to 
generate the similar information. 
We describe the system architecture in section 2. 
Section 3 illustrates the mention detection process. 
Section 4 describes the core process of coreference 
resolution. In section 5, we show the results and 
discussion of several experiments. Finally, we give 
the conclusion of our work in section 6.  
2 System Architecture 
Our system mainly contains mention detection and 
coreference resolution. Recall is the determining 
factor in mention detection stage. The reason is 
that if some mention is missed in this stage, the 
coreference resolution part will miss the chains 
which contain this mention. Yet some mentions 
still need to be distinguished because in some cases 
they refer to no entity. For example ?it?, in the 
sentence ?it + be + weather/ time?, ?it? should refer 
to no entity. But the ?it? in the phrase ?give it to 
me? might refer to some entity. The coreference 
resolution module of our system follows the idea 
of Stanford. In the English task we did some more 
exploration on mention detection, pronoun 
coreference and partial match of noun phrases. The 
Chinese task is more complicated and because 
gender, number and name entities are not provided, 
the feature generation from the training data has to 
be added before the coreference resolution process. 
Some Chinese idiomatic usages are also considered 
in this stage.  
3 Mention detection  
All the NPs, pronouns and the phrases which are 
indexed as named entities are selected as 
candidates. NPs are extracted from the parse tree. 
Yet some mentions do not refer to any entity in 
some cases. In our system we attempt to 
distinguish these mentions in this stage. The reason 
is that the deterministic rules in coreference 
resolution part are not complete to distinguish 
these mentions. The methods below can also be 
added to the coreference resolution part as a pre-
processing. For the conveniences of system design, 
we finish this work in this stage. 
For English, the pronoun ?it? and NPs ?this, that, 
those and these? need to be distinguished. We take 
?it? as an example to illustrate the process. First we 
use regular expressions to select ?it?, which refers 
to no entity, such as ?it + be + weather/ time?, ?it 
happened that? and ?it makes (made) sense that?.  
Second we use Decision Tree (C4.5) to classify the 
two kinds of ?it? based on the training data. The 
features contain the Part-of-Speech, Parse bit, 
Predicate Arguments of ?it?, the word before and 
after ?it?. The number of total ?it? is 9697 and 4043 
of them have an entity to refer to in the training 
data. 
 
Category Precision Recall F 
no entity refered 
entity refered 
0.576 
0.747 
0.596 
0.731 
0.586
0.739
total 0.682 0.679 0.68
 
Table 1: Results of ?it? classification using C4.5 
 
Table 1 shows the classification result of ?it? in 
the development data v4. The number of total ?it? 
is 1401 and 809 of them have an entity to refer to. 
The result is not perfect but can help enhance the 
performance of coreference resolution. However, 
the results of ?this, that, those and these? are not 
acceptable and we skip over these words. We did 
not do any process on ?verb? mention detection and 
coreference resolution. 
In addition, we divide mentions into groups in 
which they are nested in position. And for 
mentions which have the same head word in one 
group, only the mentions with the longest span 
should be left (for the English task and a set of 
Chinese articles). For some Chinese articles of 
which names contain ?chtb?, both in the training 
data and the development data, the nest is 
permitted based on the statistic results.  
For Chinese we also attempt to train a model for 
pronouns ???(you) and ???(that). However, the 
results are not acceptable either since the features 
we select are not enough for the classifier. 
After the mentions have been extracted, the 
related features of each mention are also extracted. 
We transform the ?conll? document into mention 
77
document. Each mention has basic features such as 
position, part-of-speech, parse tree, head word, 
speaker, Arguments, and the gender and number of 
head word. The head word feature is very 
important and regular expression can almost 
accomplish the process but not perfectly. Firstly, 
we extract the key NPs of a mention based on 
parse feature. Then the regular expressions are to 
extract the head word. For example, the mention: 
 (NP (DNP (LCP (NP (NP (NR ??)) (NP (NN ??))) 
(LC ?)) (DEG ?)) (NP (NR ??)) (NP (NN ??))) (NP 
(DNP (LCP (NP (NP (NR ??)) (NP (NN ??))) (LC ?)) 
(DEG ?)) (NP (NR ??)) (NP (NN ??)))  
The key NPs of this mention is: 
(NP (NR ??)) (NP (NN ??)) .The head word of 
this mention is: NN ?? 
However, there are still some cases that need to 
be discussed. For example, the head word of ?the 
leader of people? should be ?leader?, while the head 
word of ?the city of Beijing? should be ?city? and 
?Beijing? for the mentions of ?the city? and 
?Beijing? both have the same meaning with ?the 
city of Beijing?. Finally, we only found the words 
of ?city? and ?country? should be processed. 
4 Coreference resolution  
The deterministic rules are the core methods to 
solve the coreference resolution task. All the 
mentions in the same part can be seen as a list. The 
mentions which refer to the same entity will be 
clustered based on the deterministic rules. After all 
the clusters have generated, the merge program 
will merge the clusters into chains based on the 
position information. The mentions in one chain 
cannot be reduplicative in position. Basically the 
nested mentions are not allowed. 
The process contains two parts NP-NP and NP-
pronoun. Each part has several sub-problems to be 
discussed. First, the same process of English task 
and Chinese task will be illustrated. Then the 
different parts will be discussed separately. 
4.1 NP-NP 
Exact match: the condition of exact match is the 
two NP mentions which have no other larger 
parent mentions in position are coreferential if they 
are exactly the same. The stop words such as ?a?, 
?the?, ?this? and ?that? have been removed.  
Partial match: there are two conditions for 
partial match which are the two mentions have the 
same head word and one of them is a part of the 
other in form simultaneously.  
Alias and abbreviation: some mentions have 
alias or abbreviation. For example the mentions 
?USA? and ?America? should refer to the mention 
?the United States?. 
Similar match:  there are three forms of this 
match. The first one is all the modifiers of two NPs 
are same and the head words are similar based on 
WordNet1 which is provided for the English closed 
task. We only use the English synonym sets of the 
WordNet to solve the first form. The second one is 
the head words are same and the modifiers are not 
conflicted. The third form is that the head words 
and modifiers are all different. The result of similar 
match may be reduplicative with that of exact 
match and partial match. This would be eliminated 
by the merge process. 
4.2 Pronoun - NP 
There are seven categories of pronoun to NP in our 
system. For English second person, it is difficult to 
distinguish the plural form from singular form and 
we put them in one deterministic rule. For each 
kind of pronouns shown below, the first cluster is 
the English form and the second cluster is the 
Chinese form.  
First Person (singular) = {'I', 'my', 'me', 'mine', 
'myself'}{???} 
Second Person= {'you', 'your', 'yours', 'yourself', 
'yourselves'}{???? ????} 
Third Person (male) = {'he', 'him', 'his', 
'himself'}{???} 
Third Person (female) = {'she', 'her', 'hers', 
'herself'}{???} 
Third Person (object) = {'it', 'its', 'itself'}{???} 
First Person (plural) = {'we', 'us', 'our', 'ours', 
'ourselves'}{????} 
Third Person (plural) = {'they', 'them', 'their', 
'theirs', 'themselves'}{????? ?????????} 
In the Chinese task the possessive form of 
pronoun is not considered. For example, the 
mention ???  ? ?(our) is a DNP in the parse 
feature and it contains two words ???? and ???. 
We only selected the NP ????as a mention. The 
reflexive pronouns are composed by two words 
which are the pronoun itself and the word ????. 
                                                          
1 http://wordnet.princeton.edu/ 
78
For example, the mention ??  ???(myself) is 
processed as ???(I or me).  
Gender, number and distance between pronoun 
and NP are the most important features for this part 
(Shane et al, 2006). We only allow pronoun to 
find NPs at first. We find out the first mention of 
which all the features are satisfied ahead of the 
pronoun. If there is no matching mention, search 
backward from the pronoun. For the first person 
and second person, we merged all the pronouns 
with the same form and the same speaker. If the 
context is a conversation of two speakers, the 
second person of a speaker should refer to the first 
person of the other speaker. The scene of multi-
speakers conversation is too difficult to be solved. 
In the Chinese task there are some other 
pronouns. The pronoun ????(both sides) should 
refer to a plural mention which contain ???(and) 
in the middle. The pronoun ?? ? has similar 
meaning of third person and refers to the largest 
NP mention before it. The pronouns ?? ?(this), 
?? ?(that), ??? ?(here), ??? ?(there) are not 
processed for we did not find a good solution.  
However in some cases the provided gender and 
number are not correct or missing and we had to 
label these mentions based on the appellation 
words of the training data. For example, if the 
appellation word of a person is ?Mr.? or ?sir?, the 
gender should be male.  
4.3 Chinese closed task  
For the Chinese closed task NE, the gender and 
number are not provided. We used regular patterns 
to generate these features from the training data. 
In the NE (named entities) feature ?PERSON? is 
a very important category because most pronouns 
will refer to the person entity. To extract 
?PERSON?, we build a PERSON dictionary which 
contains all the PERSON mentions in training data, 
such as ????(Mr.) and ????(Professor).  If the 
same mention appears in the test data, we believe it 
is a person entity. However, the PERSON 
dictionary cannot cover all the PERSON mentions. 
The appellation words are extracted before or after 
the person entity. When some appellation word 
appears in the test data, the NP mention before or 
after the appellation word should be a person entity, 
if they compose a larger NP mention.  
The Gender feature was generated at the same 
time of the ?PERSON? generation. We separate the 
?PERSON? dictionary and appellation dictionary 
into male cluster and female cluster by the 
pronouns in the same chain.  
The generation of number feature is a little 
complicated. Since the Chinese word does not have 
plural form, the numerals and the quantifiers of the 
mention are the main basis to extract the number 
feature. We extract the numerals and the 
quantifiers from the training data and built regular 
expressions for determine the number feature of a 
mention in test data. Other determinative rules for 
number feature extraction are shown below: 
If the word ??? appears in a mention tail, this 
mention is plural. For example ????(student) is 
singular and ?????(students) is plural. 
If the word ???(and) appears in the middle of a 
mention A, and the two parts separated by ??? are 
sub-mentions of A,  mention A should be plural. 
Other words which have the similar meaning of 
???, such as ???, ??? and ???, are considered.  
The time and date coreference resolution is also 
considered. The NP mentions which contain 
temporal words are processed separately since 
these categories of name entity are not provided. 
These temporal words are also extracted from 
training data. Since the head words of these 
mentions are themselves, the two time or date 
mentions are coreferential if they are the same or 
one must be a part of the other?s tail. For example 
??????(this September) and ????(September) 
which are not nested should be coreferential. 
4.4 Chinese open task 
For the Chinese open task we use several tools to 
generate features we need. 
NE generation: LTP is a Chinese parser that can 
generate the features such as Part-of-Speech, Parse 
bit, Named Entities (Liu et al, 2011). We only use 
LTP for the NE generation. However, the NE 
labels of LTP are different with that provided by 
the gold training data and need to be transformed. 
The difference of word segmentation between LTP 
and the provided data also made some errors. At 
last we find the NE feature from LTP does not 
perform well and it will be discussed in section 5. 
The conversion of pinyin-to-character is also 
used in the Chinese open task. The speaker 
provided in the training data is given in pinyin 
form. The speaker might be the ?PERSON? 
mention in the context. When we determine the 
79
pronoun coreference, we need to know whether the 
speaker and the ?PERSON? mention are same.  
Other tools used in open task contain appellation 
dictionary, gender dictionary, geographical name 
dictionary and temporal word dictionary (Bo et al, 
2009). These dictionaries are more complete than 
those used in the closed task, although the 
enhancements are also limited. 
5 Results and Discussion  
Table 2 to table 4 show the results of English 
coreference resolution on the gold and auto 
development and the test data. The results of the 
auto development data and the test data are close 
and lower than that of the gold data. Since the 
deterministic rules can not cover all the cases, 
there is still an improvement if we could make the 
deterministic rules more complete. 
 
Measure R  P F1 
Mention detection 
MUC 
B3 
77.7 
65.1 
69.2 
71.8 
62.9 
70.9 
74.6
64 
70.1
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
46.4 48.9 47.6
60.6
 
Table 2: Results of the English gold development 
data  
 
Measure R  P F1 
Mention detection 
MUC 
B3 
72.4 
62.3 
66.7 
71.5 
62.8 
71.8 
72 
62 
69.1
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
46.4 44.9 45.6
58.9
 
Table 3: Results of the English auto development 
data  
 
Measure R P F1 
Mention detection 
MUC 
B3 
73.2 
62.1 
66.2 
71.9
63 
70.5
72.53
63 
68.3
CEAF(E) 
CEAF(M) 
 BLANC 
(CEAF(E)+MUC+B3)/3 
45.7 
57.3 
72.1 
44.7
57.3
76.9
45.2
57.3
74.2
58.68
 
Table 4: Results of English test data  
 
The results of the closed Chinese performance 
on the gold and auto development and the test data 
are shown in table 5 to table 7. The performance of 
the auto development data and the test data has 
about 4% decline to that of the gold development 
on F1 of coreference resolution. It means the 
Chinese results are also partly affected by the parse 
feature. In fact we attempted to revise the parse 
feature of the auto development data using regular 
expressions. Yet the complicacy and unacceptable 
results made us abandon that.  
 
Measure R  P F1 
Mention detection 
MUC 
B3 
 82.3 
71.6 
76.7 
69.8
64.3
74.2
75.5 
67.7 
75.4 
CEAF(E)
(CEAF(E)+MUC+B3)/3
49 56.5 52.5 
65.2 
 
Table 5: Closed results of the Chinese gold 
development data  
 
Measure R P  F1 
Mention detection 
MUC 
B3 
74.2 
63.6 
73.1 
66 
60 
73.5
70 
61.7 
73.3 
CEAF(E)
(CEAF(E)+MUC+B3)/3
47.3 50.6 48.9 
61.3 
 
Table 6: Closed results of the Chinese auto 
development data 
 
Measure R P F1 
Mention detection 
MUC 
B3 
72.8 
62.4 
73.1 
64.1 
58.4 
72.7 
68.15
60.3
72.9
CEAF(E) 
CEAF(M) 
BLANC 
(CEAF(E)+MUC+B3)/3
47.1 
59.6 
73.7 
50.7 
59.6 
78.2 
48.8
59.6
75.8
60.69
 
Table 7: Closed results of the Chinese test data  
 
The results of the open Chinese performance on 
the gold and auto development and the test data are 
shown in table 8 to table 10. The performance is 
similar with that of the closed task. However, the 
improvement between F1 of the open task and F1 
of the closed task is limited. We also get the F1 of 
the closed and open test results using gold parser 
which are 66.46 and 66.38. The open result is even 
80
lower. This can be explained. The performance 
enhanced by the dictionaries we used for the open 
task are limited because the open dictionaries 
information which appears in the test data is not 
much more than that of the closed dictionaries 
which generated from the training data, although 
the total information of the former is much larger.  
The named entities generated by LTP have some 
errors such as person identification errors and will 
caused coreferential errors in Pronoun-NP stage. 
For the time we did not use LTP well and some 
other open tools such as Wikipedia and Baidu 
Baike should be applied in the open task.  
  
Measure R P F1 
Mention detection 
MUC 
B3 
82.4 
72.3 
77.7 
69.3
63.8
73.3
75.3 
67.8 
75.4 
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
48.3 56.8 52.2 
65.1 
 
Table 8: Open results of the Chinese gold 
development data  
 
Measure R  P F1 
Mention detection 
MUC 
B3 
75.1 
64.9 
74.2 
65.7
59.9
72.6
70.1 
62.3 
73.4 
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
46.7 51.5 49 
61.6 
 
Table 9: Open results of the Chinese auto 
development data 
 
Measure R P F1 
Mention detection 
MUC 
B3 
73.7 
63.7 
74 
64 
58.5
72.2
68.49
61 
73.1 
CEAF(E) 
CEAF(M) 
BLANC 
(CEAF(E)+MUC+B3)/3 
60.1 
46.8 
74.3 
60.1
51.5
78 
60.1 
49 
76 
61.02
 
Table 10: Open results of the Chinese test data  
 
The results of the gold-mention-boundaries and 
gold-mentions data of the English and Chinese 
closed task are shown in table 11 and 12. Although 
the mention detection stage is optimized by the 
gold-mention-boundaries and gold-mentions data 
and the final performance is enhanced, there is still 
space to enhance in the coreference resolution 
stage. The recall of mention detection of gold-
mentions is 99.8. This problem will be explored in 
our future work.  
 
Data R P F1 
Mention detection(A) 
gold-mention-boundaries
Mention detection(B) 
gold-mentions 
75.7 
 
80 
70.8
 
100
73.2 
59.50
88.91
69.88
 
Table 11: Results of the English closed gold-
mention-boundaries and gold-mentions data, (A) is 
the mention detection score of the gold-mention-
boundaries and (B) is the score of the gold-
mentions. 
 
Data R P F1 
Mention detection(A) 
gold-mention-boundaries
Mention detection(B) 
gold-mentions 
82.9 
 
81.7 
66.9
 
99.8
74.02
64.42
89.85
76.05
 
Table 12: Results of the Chinese closed gold-
mention-boundaries and gold-mentions data  
6 Conclusion 
In this paper we described a mixed deterministic 
model for coreference resolution of English and 
Chinese. We start the mention detection from 
extracting candidates based on the parse feature. 
The pre-processing which contains static rules and 
decision tree is applied to remove the defective 
candidates. In the coreference resolution stage the 
task is divided into several sub-problems and for 
each sub-problem the deterministic rules are 
constructed based on limited features. For the 
Chinese closed task we use regular patterns to 
generate named entities, gender and number from 
the training data. Several tools and dictionaries are 
applied for the Chinese open task. The result is not 
as good as we supposed since the feature errors 
caused by these tools also made the coreferential 
errors.  
However, a deeper error analysis is needed in 
the construction of deterministic rules. The feature 
of the predicate arguments is not used well. 
Although the open performance of the Chinese 
task is not good, we still believe that complete and 
accurate prior knowledge can help solve the task.  
81
Acknowledgement 
This work is supported in part by the National 
Natural Science Foundation of China (No. 
61173075 and 60973076), ZTE Foundation and 
Science and Technology Program of Shenzhen.  
References  
Bo Yuan, Qingcai Chen, Xiaolong Wang, Liwei Han. 
2009. Extracting Event Temporal Information based 
on Web. 2009 Second International Symposium on 
Knowledge Acquisition and Modeling, pages.346-
350  
Cicero Nogueira dos Santos, Davi Lopes Carvalho. 
2011. Rule and Tree Ensembles for Unrestricted 
Coreference Resolution. Proceedings of the 15th 
Conference on Computational Natural Language 
Learning: Shared Task, pages 51?55.  
Emili Sapena, Llu??s Padr?o and Jordi Turmo. 2011. 
RelaxCor Participation in CoNLL Shared Task on 
Coreference Resolution. Proceedings of the 15th 
Conference on Computational Natural Language 
Learning: Shared Task, pages 35?39. 
Heeyoung Lee, Yves Peirsman, Angel Chang, 
Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky. 
2011. Stanford?s Multi-Pass Sieve Coreference 
Resolution System at the CoNLL-2011 Shared Task. 
Proceedings of the 15th Conference on 
Computational Natural Language Learning: Shared 
Task, pages 28?34, Portland, Oregon. 
Liu Ting, Che Wanxiang, Li Zhenghua. 2011. Language 
Technology Platform. Journal of Chinese 
Information Processing. 25(6): 53-62 
Jun Lang, Bing Qin, Ting Liu, Sheng Li. 2007. Intra-
document Coreference Resolution: The state of the 
art. Journal of Chinese Language and Computing. 17 
(4):227-253 
Kai-Wei Chang Rajhans Samdani. 2011. Inference 
Protocols for Coreference Resolution. Proceedings of 
the 15th Conference on Computational Natural 
Language Learning: Shared Task, pages 40?44, 
Portland, Oregon. 
Karthik Raghunathan, Heeyoung Lee, Sudarshan 
Rangarajan, Nathanael Chambers, Mihai Surdeanu, 
Dan Jurafsky, Christopher Manning. 2010. A Multi-
Pass Sieve for Coreference Resolution. In EMNLP. 
Kong Fang, Zhu Qiaoming and Zhou Guodong. 2012(a). 
Anaphoricity determination for coreference 
resolution in English and Chinese languages. 
Computer Research and Development (Chinese).  
Kong Fang and Zhou Guodong. 2012(b). Tree kernel-
based pronoun resolution in English and Chinese 
languages. Journal of Software (Chinese). Accepted: 
23(8).  
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, 
Martha Palmer, Ralph Weischedel and Nianwen 
Xue.2011. CoNLL-2011 Shared Task: Modeling 
Unrestricted Coreference in OntoNotes. Proceedings 
of the Fifteenth Conference on Computational 
Natural Language Learning (CoNLL 2011). Portland, 
OR. 
Sameer Pradhan and Alessandro Moschitti and Nianwen 
Xue and Olga Uryupina and Yuchen Zhang. 2012. 
CoNLL-2012 Shared Task: Modeling Multilingual 
Unrestricted Coreference in OntoNotes. Proceedings 
of the Sixteenth Conference on Computational 
Natural Language Learning (CoNLL 2012). Jeju, 
Korea. 
Shane Bergsma and Dekang Lin. 2006. Bootstrapping 
Path-Based Pronoun Resolution Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL, 
pages 33?40, Sydney 
Wang Houfeng. 2002. Survey: Computational Models 
and Technologies in Anaphora Resolution. Journal of 
Chinese Information Processing. 16(6): 9-17. 
82
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115?122,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Hybrid Model for Grammatical Error Correction 
 
 
Yang Xiang, Bo Yuan, Yaoyun Zhang*, Xiaolong Wang?, 
Wen Zheng, Chongqiang Wei 
Intelligent Computing Research Center, Key Laboratory of Network Oriented Intelligent 
Computation, Computer Science and technology Department, 
Harbin Institute of Technology Shenzhen Graduate School, 
Shenzhen, Guangdong, 518055, P.R. China 
{windseedxy, yuanbo.hitsz, xiaoni5122, zhengwen379, weichongqiang}@gmail.com  
wangxl@insun.hit.edu.cn? 
 
 
 
Abstract 
This paper presents a hybrid model for the 
CoNLL-2013 shared task which focuses on the 
problem of grammatical error correction. This 
year?s task includes determiner, preposition, 
noun number, verb form, and subject-verb 
agreement errors which is more comprehen-
sive than previous error correction tasks. We 
correct these five types of errors in different 
modules where either machine learning based 
or rule-based methods are applied. Pre-
processing and post-processing procedures are 
employed to keep idiomatic phrases from be-
ing corrected. We achieved precision of 
35.65%, recall of 16.56%, F1 of 22.61% in the 
official evaluation and precision of 41.75%, 
recall of 20.29%, F1 of 27.3% in the revised 
version. Some further comparisons employing 
different strategies are made in our experi-
ments.   
1 Introduction 
Automatic Grammatical Error Correction (GEC) 
for non-native English language learners has at-
tracted more and more attention with the devel-
opment of natural language processing, machine 
learning and big-data techniques. ?The CoNLL-
2013 shared task focuses on the problem of GEC 
in five different error types including determiner, 
preposition, noun number, verb form, and sub-
ject-verb agreement which is more complicated 
and challenging than previous correction tasks. 
Other than most previous works which concen-
trate most on determiner and preposition errors, 
more error types introduces the possibility of 
correcting multiple interacting errors such as de-
                                                 
? Corresponding author 
terminer vs. noun number and preposition vs. 
verb form. 
Generally, for GEC on annotated data such as 
the NUCLE corpus (Dahlmeier et al, 2013) in 
this year?s shared task which contains both origi-
nal errors and human annotations, there are two 
main types of approaches. One of them is the 
employment of external language materials. Alt-
hough there are minor differences on strategies, 
the main idea of this approach is to use frequen-
cies as a filter, such as n-gram counts, and take 
those phrases that have relatively high frequen-
cies as the correct ones. Typical works are shown 
in (Yi et al, 2008) and (Bergsma et al, 2009). 
Similar methods also exist in HOO shared tasks1 
such as the web 1TB n-gram features used by 
Dahlmeier and Ng (2012a) and the large-scale n-
gram model described by Heilman et al (2012). 
The other type is machine learning based ap-
proach which considers most on local context 
including syntactic and semantic features. Han et 
al. (2006) take maximum entropy as their classi-
fier and apply some simple parameter tuning 
methods. Felice and Pulman (2008) present their 
classifier-based models together with a few rep-
resentative features. Seo et al (2012) invite a 
meta-learning approach and show its effective-
ness. Dahlmeier and Ng (2011) introduce an al-
ternating structure optimization based approach. 
Most of the works mentioned above focus on 
determiner and preposition errors. Besides, Lee 
and Seneff (2008) propose a method to correct 
verb form errors through combining the features 
of parse trees and n-gram counts. To our 
knowledge, no one focused on noun form errors 
in specific researches. 
In this paper, we propose a hybrid model to 
solve the problem of GEC for five error types. 
                                                 
1 http://clt.mq.edu.au/research/projects/hoo/hoo2012 
115
Machine learning based methods are applied to 
solve determiner (ArtOrDet), preposition (Prep) 
and noun form (Nn) problems while rule-based 
methods are proposed for subject-verb agreement 
(SVA) and verb form (Vform) problems. We 
treat corrections of errors in each type as indi-
vidual sub problems the results of which are 
combined through a result combination module. 
Solutions on interacting error corrections were 
considered originally but dropped at last because 
of the bad effects brought about by them such as 
the accumulation of errors which lead to a very 
low performance. We perform feature selection 
and confidence tuning in machine learning based 
modules which contribute a lot to our perfor-
mance. Also, pre-processing and post-processing 
procedures are employed to keep idiomatic 
phrases from being corrected.  
Through experiments, we found that the result 
of the system was affected by many factors such 
as the selection of training samples and features, 
and the settings of confidence parameters in clas-
sifiers. Some of the factors make the whole sys-
tem too sensitive that it can easily be trapped into 
a local optimum. Some comparisons are shown 
in our experiments section. 
No other external language materials are in-
cluded in our model except for several NLP tools 
which will be introduced in ?5.2. We achieved 
precision of 35.65%, recall of 16.56% and F1 of 
22.61% in the official score of our submitted re-
sult. However, it was far from satisfactory main-
ly due to the ill settings of confidence parameters. 
Trying to find out a set of optimal confidence 
parameters, our model is able to reach an upper 
bound of precision of 34.23%, recall of 25.56% 
and F1 of 29.27% on the official test set. For the 
revised version, we achieved precision of 
41.75%, recall of 20.29%, and F1 of 27.3%. 
The remainder of this paper is arranged as fol-
lows. The next section introduces our system 
architecture. Section 3 describes machine learn-
ing based modules. Section 4 shows rule based 
modules. Experiments and analysis are arranged 
in Section 5. Finally, we give our discussion and 
conclusion in Section 6 and 7. 
2 System Architecture 
Initially, we treat errors of each type as individu-
al sub problems. Machine learning based meth-
ods are applied to solve ArtOrDet, Prep and Nn 
problems where similar problem solving steps 
are shared: sample generation, feature extraction, 
training, confidence tuning in development data, 
and testing. We apply some hand-crafted heuris-
tic rules in solving subject-verb agreement (SVA) 
and verb form (Vform) problems. Finally, results 
from different modules are combined together. 
The whole architecture of this GEC system is 
described in Figure 1. 
A pre-processing and a post-processing filter 
are utilized which include filters for some idio-
matic phrases extracted from the training dataset. 
The Frequent Pattern Growth Algorithm (FP-
Growth) is widely used for frequent pattern min-
ing in machine learning. In pre-processing, we 
firstly apply FP-Growth to gather the frequent 
items in the training set. Through some manual 
refinements, a few idiomatic phrases are re-
moved from the candidate set to be corrected. In 
post-processing, the idiomatic phrase list is used 
to check whether a certain collocation is still 
grammatical after several corrections are per-
formed. There are 996 idiomatic phrases in our 
list which is composed by mainly patterns from 
the training set and a series of hand-crafted ones. 
Typical phrases we extracted are in general, 
have/need to be done, on the other hand, a 
large/big number/amount of, at the same time, in 
public, etc.  
 
Figure 1. Architecture of our GEC system. 
3 Machine Learning Based Modules 
For the error types ArtOrDet, Prep and Nn, we 
choose machine learning based methods because 
we consider there is not enough evidence to di-
rectly determine which word or form to be used. 
Moreover, it is impossible to transfer all the cas-
es we encounter into rules. In this section, we 
describe our processing ideas for each error type 
respectively and then specifically introduce our 
feature selection and confidence tuning approach. 
3.1 Determiners 
Determiners in the error type ?ArtOrDet? contain 
articles a/an, the and other determiners such as 
Original texts 
Pre-processing 
Machine learning 
based modules 
Rule based mod-
ules 
Post-processing 
Corrected texts Result combination 
116
this, those, etc. This type of error accounts for a 
large proportion which is of great impact on the 
final result. We consider only articles since the 
other determiners are rarely used and the usages 
of them are sometimes ambiguous. Like ap-
proaches described in some previous works 
(Dahlmeier and Ng, 2012a; Felice and Pulman, 
2008), we assign three types a/an, the and empty 
for each article position and build a multi-class 
classifier.  
For training, developing and testing, all noun 
phrases (NPs) are chosen as candidate samples to 
be corrected. For NPs whose articles have been 
annotated in the corpus, the correct ones are their 
target categories, and for those haven?t been an-
notated, the target categories are their observed 
article types. Samples we make use of can be 
divided into two basic types in each category: 
with and without a wrong article. Two examples 
are shown below: 
with: a/empty big apples ~ empty category 
without: the United States ~ the category 
For each category in a, the, and empty, we use 
the whole with data and take samples of without 
ones from the set of correct NPs to make up 
training instances of one category. The reason 
why we make samples of the without ones is for 
the consideration that the classifier would always 
predicts the observed article and never proposes 
any corrections if given too many without sam-
ples, the case of which is mentioned in (Dahl-
meier and Ng, 2012a). However, we found that 
the ratio of with-without shows little effect in our 
model. The article a is regulated to a or an ac-
cording to pronunciation. 
Syntactic and semantic features are considered 
in feature extraction with the help of WordNet 
and the ?.conll? file provided. We adopt syntac-
tic features such as the surface word, phrase, 
part-of-speech, n-grams, constituent parse tree, 
dependency parse tree and headword of an NP; 
semantic features like noun category and hyper-
nym. Some expand operations are also done 
based on them (reference to Dahlmeier and Ng, 
2012a; Felice and Pulman, 2008). After feature 
extraction, we apply a genetic algorithm to do 
feature subset selection in order to reduce dimen-
sionality and filter out noisy features which is to 
be described in ?3.4. 
Maximum Entropy (ME) has been proven to 
behave well for heterogeneous features in natural 
language processing tasks and we adopt it to 
train our model. We have also tried several other 
classifiers including SVM, decision tree, Na?ve 
Bayes, and RankSVM but finally find ME per-
forms well and stably. It provides confidence 
scores for each category which we will make use 
of downstream.  
3.2 Prepositions 
Preposition error correction task is similar to the 
previous one except the different categories and 
corresponding features. Since there are 36 com-
mon prepositions listed by the shared task, origi-
nally, we assign 37 types including 36 preposi-
tions and empty for each preposition position and 
build a multi-class classifier. For training, devel-
oping and testing, each preposition as well as the 
empty position directly after a verb is considered 
as a candidate. Syntactic and semantic features 
extracted are similar to those in article error cor-
rection except for some specific cases for prepo-
sitions such as the verbs related to prepositions 
and the dependency relations. Similarly, we treat 
those preposition phrases with and without a cer-
tain preposition as the two types of samples in 
training (as described in ?3.1). Two examples are 
listed below: 
with: on/in the 1860s~ in category 
without: have to be done ~ to category 
Through statistics on the training data, we 
found that most prepositions have very few sam-
ples which may not contribute to the perfor-
mance at all and even bring about noise when 
assigned to wrong categories. After several 
rounds of experiments, we finally adopt a classi-
fier with seven prepositions which are frequently 
used in the whole corpus. They are on, of, in, at, 
to, with and for. As to the classifier, ME also 
outperforms the others. 
3.3 Noun Form 
Noun form may be interacting with determiners 
and verbs which may also have errors in the orig-
inal text. So errors may occur in the context fea-
tures extracted from the original text. However, 
if we use the context features that have been cor-
rected, more errors would be employed due to 
the low performance of the previous steps. 
Through statistics, we found that co-occurrence 
between two types of errors such as SVA and 
ArtOrDet only accounts for a small proportion. 
After a few experiments, we decided to give up 
interacting errors so as to avoid accumulated er-
rors.  
This is a binary classification problem. All 
head nouns in NPs are considered as candidates. 
Each category contains with and without samples 
similar to the cases in ?3.1 and ?3.2. Features are 
highly related to the deterministic factors for the 
117
head noun form such as the countability, Word-
Net type, name entity and whether there some 
specific dependency relations including det, 
amod etc.  
ME also outperforms other classifiers. 
3.4 Feature Selection Using Genetic Algo-
rithm 
Features we extracted are excessive and sparse 
after binarization. They bring noise in quality as 
well as complexity in computation and need to 
be selected a priori. In our work, it is a wrapper 
feature selection task. That is, we have to select a 
combination of features that perform well to-
gether rather than make sure each of them be-
haves well. This GEC task is interesting in fea-
ture selection because word surface features that 
are observed only once are also effective while 
we think that they overfit. Genetic algorithm 
(GA) has been proven to be useful in selecting 
wrapper features in classification (ElAlami, 2009; 
Anba-rasi et al 2010). We used GA to select fea-
tures as well as reduce feature dimensionality.  
We convert the features into a binary sequence 
in which each character represents one dimen-
sion.  Let ?1? indicates that we keep this dimen-
sion while ?0? means that we drop it, we use a 
binary sequence such as ?0111000?100? to de-
note a combination of feature dimensions. GA 
functions on the feature sequences and finally 
decides which features should be kept. The fit-
ness function we used is the evaluation measure 
F1 described in ?5.3. 
3.5 Confidence Tuning 
The Maximum Entropy classifier returns a confi-
dence score for each category given a testing 
sample. However, for different samples, the dis-
tribution of predicted scores varies a lot. For 
some samples, the classifier may have a very 
high predicted score for a certain category which 
means the classifier is confident enough to per-
form this prediction. But for some other samples, 
two or more categories may share close scores, 
the case of which means the classifier hesitates 
when telling them apart. 
We introduce a confidence tuning approach on 
the predicted results through a comparison be-
tween the observed category and the predicted 
category which is similar to the ?thresholding? 
approach described in Tetreault and Chodorow 
(2008). The main idea of the confidence tuning 
algorithm is: the choice between keep and drop is 
based on the difference between the confidence 
scores of the predicted category and the observed 
category. If this difference goes beyond a thresh-
old t, the prediction is kept while if it is under t, 
we won?t do any corrections. We believe this 
tuning strategy is especially appropriate in this 
task since to distinguish whether the observed 
category is correct or not affects a lot to the pre-
dicted result.  
The confidence threshold for each category is 
generated through a hill climbing algorithm in 
the development data aimed at maximizing F1-
meaure of the result.  
4 Rule-based Modules 
A few hand-crafted rules are applied to solve the 
verb related corrections including SVA and 
Vform. In these cases, the verb form is only re-
lated to some specific features as described by 
Lee and Seneff (2008). 
4.1 SVA 
SVA (Subject-verb-agreement) is particularly 
related to the noun subject that a verb determines. 
In the dependency tree, the number of the noun 
which has a relation nsubj with the verb deter-
mines the form of this verb. Through observation, 
we find that the verbs to be considered in SVA 
contain only bes (including am, is, are, was, 
were) and the verbs in simple present tense 
whose POSs are labeled with VBZ (singular) or 
VBP(plural).  
To pick out the noun subject is easy except for 
the verb that contained in a subordinate clause. 
We use semantic role labeling (SRL) to help 
solve this problem in which the coordinated can 
be extracted through a trace with the label ?R-
Argument?. The following Figure is an example 
generated by the SRL toolkit mate-tools (Bernd 
Bohnet, 2010)2. 
 
 
Figure 2. SRL for the demo sentence ?Jack, who 
will show me the way, is very tall.? The subject of 
the verb show can be traced through R-A0 -> A0. 
 
  However, the performance of this part is partly 
correlated with the noun form that may have er-
rors in the original text and the wrong SRL result 
brought about because of wrong sentence gram-
mars. 
                                                 
2 http://code.google.com/p/mate-tools/ 
118
4.2 Verb Form 
The cases are more complicated in the verb form 
error correction task. Modal, aspect and voice are 
all forms that should be considered for a verb. 
And sometimes, two or more forms are com-
bined together to perform its role in a sentence. 
For example, in the sentence: 
He has been working in this position for a 
long time. 
The bolded verb has been working is a com-
bination of the active voice work, the progressive 
aspect be+VBG and the perfect aspect has+VBN. 
It is a bit difficult for us to take all cases into 
consideration, so we just apply several simple 
rules and solve a subset of problems for this type. 
Some typical rules are listed below: 
1. The verb that has a dependency relation aux 
to preposition to is modified to its base form. 
2. The verb that has a dependency relation 
pcomp to preposition by is modified to its past 
form. 
3. The verb related to other prepositions (ex-
cept to and by) is modified to ~ing form. 
4. The verb depends on auxiliary do and mod-
al verb (including its inflections and negative 
form) is modified to its base form. 
We have also tried to use SRL and transitivity 
of a verb to determine the active and passive 
voice but it didn?t work well. 
5 Experiments and Analysis 
5.1 Data Description 
The NUCLE corpus introduced by NUS (Nation-
al University of Singapore) contains 1414 essays 
written by L2 students with relatively high profi-
ciency of English in which grammatical errors 
have been well annotated by native tutors. It has 
a small proportion of annotated errors which is 
much lower than other similar corpora (Dahl-
meier et al, 2013). In our experiments, we divide 
the whole corpus into 80%, 10% and 10% for 
training, developing and testing. And we use 90% 
and 10% for training and developing for the final 
test. 
5.2 External tools and corpora 
External tools we used include WordNet (Fell-
baum, 1998) for word base form and noun cate-
gory generation, Morphg (Minnen et al, 2000)3 
to generate inflections of nouns and verbs, mate-
tools (Bohnet, 2010) for SRL, Stanford-ner 
                                                 
3 http://www.informatics.sussex.ac.uk/research/groups/nlp 
/carroll/morph.html 
(Finkel et al, 2005)4 for name entity extraction 
and Longman online dictionary5  for generation 
of noun countability and verb transitivity.  
We didn?t employ any external corpora in our 
system. 
5.3 Experiments 
The performance of each machine learning mod-
ule is affected by the selection of training sam-
ples, features and confidence tuning for the max-
imum entropy classifier. All these factors con-
tribute more or less to the final performance and 
need to be carefully developed. In our experi-
ments, we focus on machine learning based 
modules and make comparisons on sample selec-
tion, confidence tuning and feature selection and 
list a series of results before and after applying 
our strategies.  
In our experiment, the performance is meas-
ured with precision, recall and F1-measure where 
1
2 precision recallF precision recall
? ?? ?
 
Precision is the amount of predicted correc-
tions that are also corrected by the manual anno-
tators divided by the whole amount of predicted 
corrections. Recall has the same numerator as 
precision while its denominator is the amount of 
manually corrected errors. They are in accord-
ance with those measurements generated by the 
official m2scorer (Dahlmeier and Ng, 2012c) to a 
great extent and easily to be integrated in our 
program. 
As we have mentioned in Section 3, we don?t 
employ all samples but make use of all with 
(with errors and annotations) instances and sam-
ple the without ones (without errors) for training. 
And the sampling for without type is totally ran-
dom without loss of generality. We apply the 
same strategy in all of these three error types 
(ArtOrDet, Prep and Nn) and try several ratios of 
with-without to find out whether this ratio has 
great impact on the final result and which ratio 
performs best. We use the 80%-10%-10% data 
(mentioned in ?5.1) for our experiments and 
make comparisons of different ratios on develop-
ing data. The experimental results are described 
in detail in Figure 3. 
Confidence tuning is applied in all these three 
error types which contributes most to the final 
performance in our model. We compare the re-
sults before and after tuning in all sample ratios 
                                                 
4 http://www-nlp.stanford.edu/software/CRF-NER.shtml 
5 http://www.ldoceonline.com/ 
119
that we designed and they are also depicted in 
Figure 3.  
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
precision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-1. Comparisons before and after tuning 
in ArtOrDet. 1:all means to use the whole without 
samples. 
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.1
.2
.3
.4
.5
.6
presision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-2. Comparisons before and after tuning in 
Prep.  
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
1.0
precision before and after tuning
recall before and after tuning
F
1
 before and after tuning
Sample with:without
 
Figure 3-3. Comparisons before and after tuning 
in Nn. 
 
From the three groups of data in Figure 3, we 
notice that the ratio of samples has little impact 
on F1. This phenomenon shows that our conclu-
sion goes against the previous work by Dahl-
meier and Ng (2012a). We believe it is mainly 
due to our confidence tuning which makes the 
parameters vary much under different sample 
ratios, that is, if given the same parameters, the 
effect of sample ratio selection may become ob-
vious. Unfortunately, we didn?t do such a sys-
tematic comparison in our work. The improve-
ment under confidence tuning can be seen clearly 
in all ratios of with-without samples. The confi-
dence tuning algorithm employed in our work is 
better than the traditional tuning methods that 
assign a fixed threshold for each category or for 
all categories (about 1%~2% better measured by 
F1).  
However, although we are able to pick out the 
training data with a high F1 through confidence 
tuning for the developing data, it is difficult for 
us to choose a set of confidence parameters that 
also fits the test data well. Given several close 
F1s, the numerical values of denominators and 
numerators which determine the precision and 
recall can vary a lot. For example, one set that 
has a high precision and low recall may share the 
similar F1 with another set that has a low preci-
sion and high recall. Our work lacked of the de-
velopment on how to control the number of pro-
posed errors to make leverage on the perfor-
mance between developing set and testing set. It 
resulted in that the developing set and the testing 
set were not balanced at all, and our model was 
not able to keep the sample distribution as the 
training set. This is the main factor that leads to a 
low performance in our submitted result which 
can be clearly seen in Table 1. The upper bound 
performance of our system achieves precision of 
34.23%, recall of 25.56% and F1 of 29.27%, in 
which the F1 goes 7% beyond our submitted sys-
tem. We notice that results of all metrics of the 
three error types where machine learning algo-
rithms are applied improve with the simultaneous 
increase of numerators and denominators. This is 
especially noticeable in Prep. 
For the other two types SVA and Vform, we 
just apply several heuristic rules to solve a subset 
of problems and the case of Vform has not been 
solved well such as tense and voice. 
Genetic Algorithm (GA) is applied to process 
feature reduction and subset selection. This is 
done in ArtOrDet type in which we extract as 
many as 350,000 binary features. For error type 
Prep and Nn, the feature dimensionalities we 
constructed were not as high as that in ArtOrDet, 
and the improvements under GA were not obvi-
ous which we would not discuss in this work. 
Through experiments on a few sample ratios, we 
notice that feature selection using genetic algo-
rithm is able to reduce the feature dimensionality 
to about 170,000 which greatly lowers down the 
120
downstream computational complexity. However, 
the improvement contributed by GA after confi-
dence tuning is not obvious as that before confi-
dence tuning. We think it is partly because of the 
bad initialization of GA which is to be improved 
in our future work. The unfixed parameters may 
also lead to such a result which we didn?t discuss 
enough in our work. The comparison before and 
after GA is described in Figure 4. 
 
 Our submission% Upper bound% 
P(Det) 41.38(168/406) 36.44(254/697) 
R(Det) 24.35(168/690) 36.81(254/690) 
F1(Det) 30.66 36.63 
P(Prep) 13.79(4/29) 26.12(35/134) 
R(Prep) 1.29(4/311) 11.25(35/311) 
F1(Prep) 2.35 15.73 
P(Nn) 24.81(65/262) 27.27(102/374) 
R(Nn) 16.41(65/396) 25.76(102/396) 
F1(Nn) 19.76 26.49 
P(SVA) 
R(SVA) 
F1(SVA) 
24.42(21/86) 
16.94(21/124) 
20.00 
24.42(21/86) 
16.94(21/124) 
20.00 
P(Vform) 
R(Vform) 
F1(Vform) 
19.35(6/31) 
4.92(6/122) 
7.84 
19.35(6/31) 
4.92(6/122) 
7.84 
P(all) 35.65(272/763) 34.23(420/1227) 
R(all) 16.56(272/1643) 25.56(420/1643) 
F1(all) 22.61 29.27 
Table 1. Different performances according to dif-
ferent confidence parameters. Det stands for Ar-
tOrDet. 
 
Pre-processing and post-processing we pro-
pose also contribute to some extent which we 
could see from Table 2. Some idiomatic phrases 
are excluded from being corrected in pre-
processing which enhances precision while some 
are being modified in post-processing to improve 
recall. 
 
 Without pre-processing 
and post-processing% 
Final% 
P 
R 
F1 
33.72(265/768) 
16.13(265/1643) 
21.82 
35.65(272/763) 
16.56(272/1643) 
22.61 
Table 2. Comparison with and without pre-
processing and post-processing. 
 
We didn?t do much on the interacting errors 
problem since we didn?t work out perfect plans 
to solve it. So, in the result combination module, 
we just simply combine the result of each part 
together. 
Sample positive:negative
1:1 1:2 1:3 1:6
F
1
0.00
.05
.10
.15
.20
.25
.30
ME
ME+GA
ME+Tuning
ME+GA+Tuning
 
Figure 4. Comparisons before and after Genet-
ic Algorithm on ArtOrDet error type. ME, GA, 
and Tuning stand for Maximum Entropy, Ge-
netic Algorithm and confidence tuning. 
 
In the revised version, under further correc-
tions for the gold annotations, our model 
achieves precision of 41.75%, recall of 20.19% 
and F1 of 27.3%. 
6  Discussion 
Which factor contributes most to the final result 
in the problem of grammatical error correction? 
Since we didn?t include any external corpora, we 
discuss it here only according to the local classi-
fiers and context features.  
Based on our experiments, we find that, in our 
machine learning based modules, a tiny modifi-
cation of confidence parameter setting for each 
category, no matter which type of error, can have 
great impact on the final result. It results in that 
our model is much too sensitive to parameters 
which may easily lead to a poor behavior. Per-
haps a sufficient consideration of how to keep 
the distribution of samples, such as cross-
validation, may be helpful. In addition, the selec-
tion of classifiers, features and training samples 
all have effect on the result more or less, but not 
as obvious as that of the confidence threshold 
setting. 
7 Conclusion 
In this paper, we propose a hybrid model 
combining machine learning based modules and 
rule-based modules to solve the grammatical er-
ror correction task. We are able to solve a subset 
of the correction problems in which ArtOrDet 
and Nn perform better. However, our result in 
the testing data shows that our model is sensitive 
121
to parameters. How to keep the distribution of 
training samples needs to be further developed. 
Acknowledgement 
This work is supported in part by the National 
Natural Science Foundation of China (No. 612-
72383 and 61173075). 
References  
Bernd Bohnet. Top Accuracy and Fast Depend-
ency Parsing is not a Contradiction. In Pro-
ceedings of COLING, 2010. 
C. Fellbaum. WordNet: An Electronic Lexical 
Data-base. MIT Press. 1998. 
Daniel Dahlmeier, and Hwee Tou Ng. Grammat-
ical error correction with alternating structure 
optimization. In Proceedings of ACL. Associa-
tion for Computational Linguistics, 2011. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun 
Feng Ng. NUS at the HOO 2012 Shared Task. 
In Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012a. 
Daniel Dahlmeier and Hwee Tou Ng. A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the EMNLP. Associa-
tion for Computational Linguistics, 2012b. 
Daniel Dahlmeier and Hwee Tou Ng. Better 
Evaluation for Grammatical Error Correction. 
In Proceedings of NAACL, Association for 
Computational Linguistics, 2012c. 
Daniel Dahlmeier, Hwee Tou Ng and Siew Mei 
Wu. Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner 
English. In Proceedings of the 8th Workshop 
on Innovative Use of NLP for Building Educa-
tional Applications (BEA), 2013. 
De Felice, Rachele and Stephen G. Pulman. A 
classifier-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of COLING. Association for 
Computational Linguistics, 2008. 
G. Minnen, J. Carroll and D. Pearce. Robust, 
applied morphological generation. In Proceed-
ings of the 1st International Natural Language 
Generation Conference, 2000. 
Jenny Rose Finkel, Trond Grenager, and Chris-
topher Manning. Incorporating Non-local In-
formation into Information Extraction Systems 
by Gibbs Sampling. In Proceedings of ACL , 
2005. 
Joel R. Tetreault and Martin Chodorow. The ups 
and downs of preposition error detection in 
ESL writing. In Proceedings of COLING, As-
sociation for Computational Linguistics, 2008. 
John Lee and Stephanie Seneff. Correcting mis-
use of verb forms. In Proceedings of ACL: 
HLT, 2008. 
M Anbarasi, E Anupriya, and NC Iyengar. En-
hanced prediction of heart disease with feature 
subset selection using genetic algorithm. In-
ternational Journal of Engineering Science 
and Technology,Vol.2(10),2010: 5370-5376. 
ME ElAlami. A filter model for feature subset 
selection based on genetic algorithm. 
Knowledge-Based Systems,Vol.22(5), 2009: 
356-362. 
Michael Heilman, Aoife Cahill, and Joel 
Tetreault. Precision isn't everything: a hybrid 
approach to grammatical error detection. In 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012. 
Hongsuck Seo et al A meta learning approach to 
grammatical error correction. In Proceedings 
of ACL. Association for Computational Lin-
guistics, 2012. 
N.R. Han, M. Chodorow, and C. Leacock. 2006. 
Detecting errors in English article usage by 
non-native speakers. Natural Language Engi-
neering, Vol.12(02):115-129. 
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-
scale ngram models for lexical disambiguation. 
In Proceedings of IJCAI.2009. 
X. Yi, J. Gao, and W.B. Dolan. 2008. A web-
based English proofing system for English as a 
second language users. In Proceedings of 
IJCNLP.2008. 
122
