Coling 2010: Poster Volume, pages 997?1005,
Beijing, August 2010
An Evaluation Framework for Plagiarism Detection
Martin Potthast Benno Stein
Web Technology & Information Systems
Bauhaus-Universit?t Weimar
{martin.potthast, benno.stein}@uni-weimar.de
Alberto Barr?n-Cede?o Paolo Rosso
Natural Language Engineering Lab?ELiRF
Universidad Polit?cnica de Valencia
{lbarron, prosso}@dsic.upv.es
Abstract
We present an evaluation framework for
plagiarism detection.1 The framework
provides performance measures that ad-
dress the specifics of plagiarism detec-
tion, and the PAN-PC-10 corpus, which
contains 64 558 artificial and 4 000 sim-
ulated plagiarism cases, the latter gener-
ated via Amazon?s Mechanical Turk. We
discuss the construction principles behind
the measures and the corpus, and we com-
pare the quality of our corpus to exist-
ing corpora. Our analysis gives empirical
evidence that the construction of tailored
training corpora for plagiarism detection
can be automated, and hence be done on a
large scale.
1 Introduction
The lack of an evaluation framework is a seri-
ous problem for every empirical research field.
In the case of plagiarism detection this short-
coming has recently been addressed for the first
time in the context of our benchmarking work-
shop PAN [15, 16]. This paper presents the eval-
uation framework developed in the course of the
workshop. But before going into details, we sur-
vey the state of the art in evaluating plagiarism de-
tection, which has not been studied systematically
until now.
1.1 A Survey of Evaluation Methods
We have queried academic databases and search
engines to get an overview of all kinds of con-
tributions to automatic plagiarism detection. Al-
together 275 papers were retrieved, from which
139 deal with plagiarism detection in text,
1The framework is available free of charge at
http://www.webis.de/research/corpora.
Table 1: Summary of the plagiarism detection
evaluations in 205 papers, from which 104 deal
with text and 101 deal with code.
Evaluation Aspect Text Code
Experiment Task
local collection 80% 95%
Web retrieval 15% 0%
other 5% 5%
Performance Measure
precision, recall 43% 18%
manual, similarity 35% 69%
runtime only 15% 1%
other 7% 12%
Comparison
none 46% 51%
parameter settings 19% 9%
other algorithms 35% 40%
Evaluation Aspect Text Code
Corpus Acquisition
existing corpus 20% 18%
homemade corpus 80% 82%
Corpus Size [# documents]
[1, 10) 11% 10%
[10, 102) 19% 30%
[102, 103) 38% 33%
[103, 104) 8% 11%
[104, 105) 16% 4%
[105, 106) 8% 0%
123 deal with plagiarism detection in code, and
13 deal with other media types. From the pa-
pers related to text and code we analyzed the
205 which present evaluations. Our analysis
covers the following aspects: experiment tasks,
performance measures, underlying corpora, and,
whether comparisons to other plagiarism detec-
tion approaches were conducted. Table 1 summa-
rizes our findings.
With respect to the experiment tasks the ma-
jority of the approaches perform overlap detec-
tion by exhaustive comparison against some lo-
cally stored document collection?albeit a Web
retrieval scenario is more realistic. We explain
this shortcoming by the facts that the Web can-
not be utilized easily as a corpus, and, that in the
case of code plagiarism the focus is on collusion
detection in student courseworks. With respect to
performance measures the picture is less clear: a
manual result evaluation based on similarity mea-
sures is used about the same number of times for
text (35%), and even more often for code (69%),
as an automatic computation of precision and re-
call. 21% and 13% of the evaluations on text and
code use custom measures or examine only the de-
997
tection runtime. This indicates that precision and
recall may not be well-defined in the context of
plagiarism detection. Moreover, comparisons to
existing research are conducted in less than half
of the papers, a fact that underlines the lack of an
evaluation framework.
The right-hand side of Table 1 overviews two
corpus-related aspects: the use of existing cor-
pora versus the use of handmade corpora, and the
size distribution of the used corpora. In particu-
lar, we found that researchers follow two strate-
gies to compile a corpus. Small corpora (<1 000
documents) are built from student courseworks or
from arbitrary documents into which plagiarism-
alike overlap is manually inserted. Large corpora
(>1 000 documents) are collected from sources
where overlap occurs more frequently, such as
rewritten versions of news wire articles, or from
consecutive versions of open source software. Al-
together, we see a need for an open, commonly
used plagiarism detection corpus.
1.2 Related Work
There are a few surveys about automatic plagia-
rism detection in text [7, 8, 14] and in code [12,
17, 19, 20]. These papers, as well as nearly all
papers of our survey, omit a discussion of evalua-
tion methodologies; the following 4 papers are an
exception.
In [21] the authors introduce graph-based per-
formance measures for code plagiarism detection
that are intended for unsupervised evaluations.
We argue that evaluations in this field should be
done in a supervised manner. An aside: the pro-
posed measures have not been adopted since their
first publication. In [15] we introduce preliminary
parts of our framework. However, the focus of
that paper is less on methodology but on the com-
parison of the detection approaches that were sub-
mitted to the first PAN benchmarking workshop.
In [9, 10] the authors report on an unnamed cor-
pus that comprises 57 cases of simulated plagia-
rism. We refer to this corpus as the Clough09 cor-
pus; a comparison to our approach is given later
on. Finally, a kind of related corpus is the ME-
TER corpus, which has been the only alternative
for the text domain up to now [11]. It comprises
445 cases of text reuse among 1 716 news articles.
Although the corpus can be used to evaluate pla-
giarism detection its design does not support this
task. This is maybe the reason why it has not been
used more often. Furthermore, it is an open ques-
tion whether or not cases of news reuse differ from
plagiarism cases where the plagiarists strive to re-
main undetected.
1.3 Contributions
Besides the above survey, the contributions of our
paper are threefold: Section 2 presents formal
foundations for the evaluation of plagiarism detec-
tion and introduces three performance measures.
Section 3 introduces methods to create artificial
and simulated plagiarism cases on a large scale,
and the PAN-PC-10 corpus in which these meth-
ods have been operationalized. Section 4 then
compares our corpus with the Clough09 corpus
and the METER corpus. The comparison reveals
important insights for the different kinds of text
reuse in these corpora.
2 Plagiarism Detection Performance
This section introduces measures to quantify the
precision and recall performance of a plagiarism
detection algorithm; we present a micro-averaged
and a macro-averaged variant. Moreover, the so-
called detection granularity is introduced, which
quantifies whether the contiguity between plagia-
rized text passages is properly recognized. This
concept is important: a low granularity simpli-
fies both the human inspection of algorithmically
detected passages as well as an algorithmic style
analysis within a potential post-process. The three
measures can be applied in isolation but also
be combined into a single, overall performance
score. A reference implementation of the perfor-
mance measures is distributed with our corpus.
2.1 Precision, Recall, and Granularity
Let dplg denote a document that contains pla-
giarism. A plagiarism case in dplg is a 4-tuple
s = ?splg, dplg, ssrc, dsrc?, where splg is a plagia-
rized passage in dplg, and ssrc is its original coun-
terpart in some source document dsrc. Likewise,
a plagiarism detection for document dplg is de-
noted as r = ?rplg, dplg, rsrc, d?src?; r associates
an allegedly plagiarized passage rplg in dplg with
998
a passage rsrc in d?src. We say that r detects s iff
rplg ? splg = ?, rsrc ? ssrc = ?, and d?src = dsrc.
With regard to a plagiarized document dplg it is as-
sumed that different plagiarized passages of dplg
do not intersect; with regard to detections for dplg
no such restriction applies. Finally, S and R de-
note sets of plagiarism cases and detections.
While the above 4-tuples resemble an intu-
itive view of plagiarism detection we resort to
an equivalent, more concise view to simplify the
subsequent notations: a document d is repre-
sented as a set of references to its characters d =
{(1, d), . . . , (|d|, d)}, where (i, d) refers to the
i-th character in d. A plagiarism case s can then be
represented as s = splg ? ssrc, where splg ? dplg
and ssrc ? dsrc. The characters referred to in splg
and ssrc form the passages splg and ssrc. Likewise,
a detection r can be represented as r = rplg?rsrc.
It follows that r detects s iff rplg ? splg = ? and
rsrc?ssrc = ?. Based on these representations, the
micro-averaged precision and recall of R under S
are defined as follows:
precmicro(S,R) =
|?(s,r)?(S?R)(s 	 r)|
|?r?R r|
, (1)
recmicro(S,R) =
|?(s,r)?(S?R)(s 	 r)|
|?s?S s|
, (2)
where s 	 r =
{
s ? r if r detects s,
? otherwise.
The macro-averaged precision and recall are
unaffected by the length of a plagiarism case; they
are defined as follows:
precmacro(S,R) =
1
|R|
?
r?R
|?s?S(s 	 r)|
|r| , (3)
recmacro(S,R) =
1
|S|
?
s?S
|?r?R(s 	 r)|
|s| , (4)
Besides precision and recall there is another
concept that characterizes the power of a detec-
tion algorithm, namely, whether a plagiarism case
s ? S is detected as a whole or in several pieces.
The latter can be observed in today?s commercial
plagiarism detectors, and the user is left to com-
bine these pieces to a consistent approximation
of s. Ideally, an algorithm should report detec-
tions R in a one-to-one manner to the true cases S.
To capture this characteristic we define the detec-
tion granularity of R under S:
gran(S,R) = 1|SR|
?
s?SR
|Rs|, (5)
where SR ? S are cases detected by detections
in R, and Rs ? R are the detections of a given s:
SR = {s | s ? S ? ?r ? R : r detects s},
Rs = {r | r ? R ? r detects s}.
The domain of gran(S,R) is [1, |R|], with 1
indicating the desired one-to-one correspondence
and |R| indicating the worst case, where a single
s ? S is detected over and over again.
Precision, recall, and granularity allow for a
partial ordering among plagiarism detection algo-
rithms. To obtain an absolute order they must be
combined to an overall score:
plagdet(S,R) = F?log2(1 + gran(S,R))
, (6)
where F? denotes the F?-Measure, i.e., the
weighted harmonic mean of precision and recall.
We suggest using ? = 1 (precision and recall
equally weighted) since there is currently no indi-
cation that either of the two is more important. We
take the logarithm of the granularity to decrease
its impact on the overall score.
2.2 Discussion
Plagiarism detection is both a retrieval task and
an extraction task. In light of this fact not only
retrieval performance but also extraction accuracy
becomes important, the latter of which being ne-
glected in the literature. Our measures incorpo-
rate both. Another design objective of our mea-
sures is the minimization of restrictions imposed
on plagiarism detectors. The overlap restriction
for plagiarism cases within a document assumes
that a certain plagiarized passage is unlikely to
have more than one source. Imprecision or lack
of evidence, however, may cause humans or algo-
rithms to report overlapping detections, e.g., when
being unsure about the true source of a plagia-
rized passage. The measures (1)-(4) provide for a
sensible treatment of this fact since the set-based
999
passage representations eliminate duplicate detec-
tions of characters. The macro-averaged vari-
ants allot equal weight to each plagiarism case,
regardless of its length. Conversely, the micro-
averaged variants favor the detection of long pla-
giarism passages, which are generally easier to be
detected. Which of both is to be preferred, how-
ever, is still an open question.
3 Plagiarism Corpus Construction
This section organizes and analyzes the practices
that are employed?most of the time implicitly?
for the construction of plagiarism corpora. We
introduce three levels of plagiarism authentic-
ity, namely, real plagiarism, simulated plagiarism,
and artificial plagiarism. It turns out that simu-
lated plagiarism and artificial plagiarism are the
only viable alternatives for corpus construction.
We propose a new approach to scale up the gen-
eration of simulated plagiarism based on crowd-
sourcing, and heuristics to generate artificial pla-
giarism. Moreover, based on these methods, we
compile the PAN plagiarism corpus 2010 (PAN-
PC-10) which is the first corpus of its kind that
contains both a large number and a high diversity
of artificial and simulated plagiarism cases.
3.1 Real, Simulated, and Artificial Plagiarism
Syntactically, a plagiarism case is the result of
copying a passage ssrc from a source document
into another document dplg. Since verbatim
copies can be detected easily, plagiarists often
rewrite ssrc to obfuscate their illegitimate act.
This behavior must be modeled when constructing
a training corpus for plagiarism detection, which
can be done at three levels of authenticity. Ide-
ally, one would secretly observe a large number
of plagiarists and use their real plagiarism cases;
at least, one could resort to plagiarism cases which
have been detected in the past. The following as-
pects object against this approach:
? The distribution of detected real plagiarism
is skewed towards ease of detectability.
? The acquisition of real plagiarism is expen-
sive since it is often concealed.
? Publishing real cases requires the consents
from the plagiarist and the original author.
? A public corpus with real cases is question-
able from an ethical and legal viewpoint.
? The anonymization of real plagiarism is dif-
ficult due to Web search engines and author-
ship attribution technology.
It is hence more practical to let people create
plagiarism cases by ?purposeful? modifications,
or to tap resources that contain similar kinds of
text reuse. We subsume these strategies under the
term simulated plagiarism. The first strategy has
often been applied in the past, though on a small
scale and without a public release of the corpora;
the second strategy comes in the form of the ME-
TER corpus [11]. Note that, from a psycholog-
ical viewpoint, people who simulate plagiarism
act under a different mental attitude than plagia-
rists. From a linguistic viewpoint, however, it is
unclear whether real plagiarism differs from sim-
ulated plagiarism.
A third possibility is to generate plagiarism al-
gorithmically [6, 15, 18], which we call artificial
plagiarism. Generating artificial plagiarism cases
is a non-trivial task if one requires semantic equiv-
alence between a source passage ssrc and the pas-
sage splg that is obtained by an automatic obfus-
cation of ssrc. Such semantics-preserving algo-
rithms are still in their infancy; however, the sim-
ilarity computation between texts is usually done
on the basis of document models like the bag of
words model and not on the basis of the original
text, which makes obfuscation amenable to sim-
pler approaches.
3.2 Creating Simulated Plagiarism
Our approach to scale up the creation of simu-
lated plagiarism is based on Amazon?s Mechani-
cal Turk, AMT, a commercial crowdsourcing ser-
vice [3]. This service has gathered considerable
interest, among others to recreate TREC assess-
ments [1], but also to write and translate texts [2].
We offered the following task on the Mechani-
cal Turk platform: Rewrite the original text found
below [on the task Web page] so that the rewritten
version has the same meaning as the original, but
with a different wording and phrasing. Imagine a
scholar copying a friend?s homework just before
class, or imagine a plagiarist willing to use the
1000
Table 2: Summary of 4 000 Mechanical Turk tasks
completed by 907 workers.
Worker Demographics
Age Education
18, 19 10% HS 11%
20?29 37% College 30%
30?39 16% BSc. 17%
40?49 7% MSc. 11%
50?59 4% Dr. 2%
60?69 1%
n/a 25% n/a 29%
Native Speaker Gender
yes 62% male 37%
no 14% female 39%
n/a 23% n/a 24%
Prof. Writer Plagiarized
yes 10% yes 16%
no 66% no 60%
n/a 24% n/a 25%
Task Statistics
Tasks per Worker
average 15
std. deviation 20
minimum 1
maximum 103
Work Time (minutes)
average 14
std. deviation 21
minimum 1
maximum 180
Compensation
pay per task 0.5 US$
rejected results 25%
original text without proper citation.
Workers were required to be fluent in English
reading and writing, and they were informed that
every result was to be reviewed. A questionnaire
displayed alongside the task description asked
about the worker?s age, education, gender, and na-
tive speaking ability. Further we asked whether
the worker is a professional writer, and whether
he or she has ever plagiarized. Completing the
questionnaire was optional in order to minimize
false answers, but still, these numbers have to
be taken with a grain of salt: the Mechanical
Turk is not the best environment for such sur-
veys. Table 2 overviews the worker demographics
and task statistics. The average worker appears
to be a well-educated male or female in the twen-
ties, whose mother tongue is English. 16% of the
workers claim to have plagiarized at least once,
and if at least the order of magnitude of the lat-
ter number can be taken seriously this shows that
plagiarism is a prevalent problem.
A number of pilot experiments were conducted
to determine the pay per task, depending on the
text length and the task completion time: for
50 US-cents about 500 words get rewritten in
about half an hour. We observed that decreasing
or increasing the pay per task has proportional ef-
fect on the task completion time, but not on the
result quality. This observation is in concordance
with earlier research [13]. Table 3 contrasts a
source passage ssrc and its rewritten, plagiarized
passage splg obtained via the Mechanical Turk.
3.3 Creating Artificial Plagiarism
To create artificial plagiarism, we propose three
obfuscation strategies. Given a source passage
ssrc a plagiarized passage splg can be created as
follows (see Table 4):
? Random Text Operations. splg is created
from ssrc by shuffling, removing, inserting,
or replacing words or short phrases at ran-
dom. Insertions and replacements are taken
from the document dplg where splg is to be
inserted.
? Semantic Word Variation. splg is created
from ssrc by replacing words by one of their
synonyms, antonyms, hyponyms, or hyper-
nyms, chosen at random. A word is kept if
none of them is available.
Table 3: Example of a simulated plagiarism case s, generated with Mechanical Turk.
Source Passage ssrc Plagiarized Passage splg
The emigrants who sailed with Gilbert were better fitted for a
crusade than a colony, and, disappointed at not at once find-
ing mines of gold and silver, many deserted; and soon there
were not enough sailors to man all the four ships. Accord-
ingly, the Swallow was sent back to England with the sick;
and with the remainder of the fleet, well supplied at St. John?s
with fish and other necessaries, Gilbert (August 20) sailed
south as far as forty-four degrees north latitude. Off Sable
Island a storm assailed them, and the largest of the ves-
sels, called the Delight, carrying most of the provisions, was
driven on a rock and went to pieces.
[Excerpt from ?Abraham Lincoln: A History? by John Nicolay and John Hay.]
The people who left their countries and sailed with Gilbert
were more suited for fighting the crusades than for leading a
settled life in the colonies. They were bitterly disappointed as
it was not the America that they had expected. Since they did
not immediately find gold and silver mines, many deserted.
At one stage, there were not even enough man to help sail
the four ships. So the Swallow was sent back to England
carrying the sick. The other fleet was supplied with fish and
the other necessities from St. John. On August 20, Gilbert
had sailed as far as forty-four degrees to the north latitude.
His ship known as the Delight, which bore all the required
supplies, was attacked by a violent storm near Sable Island.
The storm had driven it into a rock shattering it into pieces.
1001
Table 4: Examples of the obfuscation strategies.
Obfuscation Examples
Original Text
The quick brown fox jumps over the lazy dog.
Manual Obfuscation (by a human)
Over the dog which is lazy jumps quickly the fox which is brown.
Dogs are lazy which is why brown foxes quickly jump over them.
A fast auburn vulpine hops over an idle canine.
Random Text Operations
over The. the quick lazy dog <context word> jumps brown fox
over jumps quick brown fox The lazy. the
brown jumps the. quick dog The lazy fox over
Semantic Word Variation
The quick brown dodger leaps over the lazy canine.
The quick brown canine jumps over the lazy canine.
The quick brown vixen leaps over the lazy puppy.
POS-preserving Word Shuffling
The brown lazy fox jumps over the quick dog.
The lazy quick dog jumps over the brown fox.
The brown lazy dog jumps over the quick fox.
? POS-preserving Word Shuffling. The se-
quence of parts of speech in ssrc is deter-
mined and splg is created by shuffling words
at random while retaining the original POS
sequence.
To generate different degrees of obfuscation the
strategies can be adjusted by varying the number
of operations made on ssrc, and by limiting the
range of affected phrases within ssrc. For our cor-
pus, the strategies were combined and adjusted to
match an intuitive understanding of a ?low? and
a ?high? obfuscation. Of course other obfusca-
tion strategies are conceivable, e.g., based on au-
tomatic paraphrasing methods [4], but for perfor-
mance reasons simple strategies are preferred at
the expense of readability of the obfuscated text.
3.4 Overview of the PAN-PC-10
To compile the PAN plagiarism corpus 2010, sev-
eral other parameters besides the above plagiarism
obfuscation methods have been varied. Table 5
gives an overview.
The documents used in the corpus are derived
from books from the Project Gutenberg.2 Every
document in the corpus serves one of two pur-
poses: it is either used as a source for plagiarism
or as a document suspicious of plagiarism. The
latter documents divide into documents that actu-
ally contain plagiarism and documents that don?t.
2http://www.gutenberg.org
Table 5: Corpus statistics of the PAN-PC-10 for
its 27 073 documents and 68 558 plagiarism cases.
Document Statistics
Document Purpose
source documents 50%
suspicious documents
? with plagiarism 25%
? w/o plagiarism 25%
Intended Algorithms
external detection 70%
intrinsic detection 30%
Plagiarism per Document
hardly (5%-20%) 45%
medium (20%-50%) 15%
much (50%-80%) 25%
entirely (>80%) 15%
Document Length
short (1-10 pp.) 50%
medium (10-100 pp.) 35%
long (100-1000 pp.) 15%
Plagiarism Case Statistics
Topic Match
intra-topic cases 50%
inter-topic cases 50%
Obfuscation
none 40%
artificial
? low obfuscation 20%
? high obfuscation 20%
simulated (AMT) 6%
translated ({de,es} to en) 14%
Case Length
short (50-150 words) 34%
medium (300-500 words) 33%
long (3000-5000 words) 33%
The documents without plagiarism allow to deter-
mine whether or not a detector can distinguish pla-
giarism cases from overlaps that occur naturally
between random documents.
The corpus is split into two parts, correspond-
ing to the two paradigms of plagiarism detection,
namely external plagiarism detection and intrinsic
plagiarism detection. Note that in the case of in-
trinsic plagiarism detection the source documents
used to generate the plagiarism cases are omitted:
intrinsic detection algorithms are expected to de-
tect plagiarism in a suspicious document by an-
alyzing the document in isolation. Moreover, the
intrinsic plagiarism cases are not obfuscated in or-
der to preserve the writing style of the original au-
thor; the 40% of unobfuscated plagiarism cases in
the corpus include the 30% of the cases belonging
to the intrinsic part.
The fraction of plagiarism per document, the
lengths of the documents and plagiarism cases,
and the degree of obfuscation per case deter-
mine the difficulty of the cases: the corpus con-
tains short documents with a short, unobfuscated
plagiarism case, resulting in a 5% fraction of
plagiarism, but it also contains large documents
with several obfuscated plagiarism cases of vary-
ing lengths, drawn from different source docu-
ments and resulting in fractions of plagiarism up
to 100%. Since the true distributions of these pa-
rameters in real plagiarism are unknown, sensible
1002
estimations were made for the corpus. E.g., there
are more simple plagiarism cases than complex
ones, where ?simple? refers to short cases, hardly
plagiarism per document, and less obfuscation.
Finally, plagiarism cases were generated be-
tween topically related documents and between
unrelated documents. To this end, the source doc-
uments and the suspicious documents were clus-
tered into k = 30 clusters using bisecting k-
means [22]. Then an equal share of plagiarism
cases were generated for pairs of source docu-
ments and suspicious documents within as well
as between clusters. Presuming the clusters cor-
respond to (broad) topics, we thus obtained intra-
topic plagiarism and inter-topic plagiarism.
4 Corpus Validation
This section reports on validation results about
the ?quality? of the plagiarism cases created for
our corpus. We compare both artificial plagia-
rism cases and simulated plagiarism cases to cases
of the two corpora Clough09 and METER. Pre-
suming that the authors of these corpora put their
best efforts into case construction and annotation,
the comparison gives insights whether our scale-
up strategies are reasonable in terms of case qual-
ity. To foreclose the results, we observe that sim-
ulated plagiarism and, in particular, artificial pla-
giarism behave similar to the two handmade cor-
pora. In the light of the employed strategies to
construct plagiarism this result may or may not
be surprising?however, we argue that it is neces-
sary to run such a comparison in order to provide
a broadly accepted evaluation framework in this
sensitive area.
The experimental setup is as follows: given a
plagiarism case s = ?splg, dplg, ssrc, dsrc?, the pla-
giarized passage splg is compared to the source
passage ssrc using 10 different retrieval models.
Each model is an n-gram vector space model
(VSM) where n ranges from 1 to 10 words,
employing stemming, stop word removal, tf -
weighting, and the cosine similarity. Similarity
values are computed for all cases found in each
corpus, but since the corpora are of different sizes,
100 similarities are sampled from each corpus to
ensure comparability.
The rationale of this setup is as follows: a well-
known fact from near-duplicate detection is that
if two documents share only a few 8-grams?so-
called shingles?it is highly probable that they are
duplicates [5]. Another well-known fact is that
two documents which are longer than a few sen-
tences and which are exactly about the same topic
will, with a high probability, share a considerable
portion of their vocabulary. I.e., they have a high
1
0.8
0.6
0.4
0.2
0
n = 1 2 3 4 5 6 7 8 9 10
Si
m
ila
rit
y
n-gram VSM
Clough09
Artificial
Median
25% Quartile
75% Quartile
Simulated (AMT)
METER
Left to right:
Figure 1: Comparison of four corpora of text reuse and plagiarism: each box plot shows the middle
range of the measured similarities when comparing source passages to their rewritten versions. Basis is
an n-gram VSM, where n ? {1, 2, . . . , 10} words.
1003
similarity under a 1-gram VSM. It follows for pla-
giarism detection that a common shingle between
splg and ssrc pinpoints very accurately an unob-
fuscated portion of splg, while it is inevitable that
even a highly obfuscated splg will share a portion
of its vocabulary with ssrc. The same holds for all
other kinds of text reuse.
Figure 1 shows the obtained similarities, con-
trasting each n-gram VSM and each corpus. The
box plots show the middle 50% of the respective
similarity distributions as well as median similar-
ities. The corpora divide into groups with compa-
rable behavior: in terms of the similarity ranges
covered, the artificial plagiarism compares to the
METER corpus, except for n ? {2, 3}, while the
simulated plagiarism from the Clough09 corpus
behaves like that from our corpus, but with a dif-
ferent amplitude. In terms of median similarity,
METER, Clough09, and our simulated plagiarism
behave almost identical, while the artificial plagia-
rism differs. Also note that our simulated plagia-
rism as well as the Clough09 corpus contain some
cases which are hardly obfuscated.
We interpret these results as follows: (1) Dif-
ferent kinds of plagiarism and text reuse do not
differ very much under n-gram models. (2) Ar-
tificial plagiarism, if carefully generated, is a vi-
able alternative to simulated plagiarism cases and
real text reuse cases. (3) Our strategies to scale-up
the construction of plagiarism corpora works well
compared to existing, handmade corpora.
5 Summary
Current evaluation methodologies in the field
of plagiarism detection research have conceptual
shortcomings and allow only for a limited compa-
rability. Our research contributes right here: we
present tailored performance measures for plagia-
rism detection and the large-scale corpus PAN-
PC-10 for the controlled evaluation of detection
algorithms. The corpus features various kinds
of plagiarism cases, including obfuscated cases
that have been generated automatically and man-
ually. An evaluation of the corpus in relation to
previous corpora reveals a high degree of matu-
rity. Until now, 31 plagiarism detectors have been
compared using our evaluation framework. This
high number of systems has been achieved based
on two benchmarking workshops in which the
framework was employed and developed, namely
PAN?09 [15] and PAN?10 [16]. We hope that our
framework will be beneficial as a challenging and
yet realistic test bed for researchers in order to pin-
point the room for the development of better pla-
giarism detection systems.
Acknowledgements
We thank Andreas Eiselt for his devoted work
on the corpus over the past two years. This
work is partially funded by CONACYT-Mexico
and the MICINN project TEXT-ENTERPRISE
2.0 TIN2009-13391-C04-03 (Plan I+D+i).
Bibliography
[1] Omar Alonso and Stefano Mizzaro. Can
We Get Rid of TREC Assessors? Using
Mechanical Turk for Relevance
Assessment. In SIGIR?09: Proceedings of
the Workshop on The Future of IR
Evaluation, 2009.
[2] Vamshi Ambati, Stephan Vogel, and Jaime
Carbonell. Active learning and
crowd-sourcing for machine translation. In
Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike
Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh conference on
International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may
2010. European Language Resources
Association (ELRA). ISBN 2-9517408-6-7.
[3] Jeff Barr and Luis Felipe Cabrera. AI Gets
a Brain. Queue, 4(4):24?29, 2006. ISSN
1542-7730. doi:
10.1145/1142055.1142067.
[4] Regina Barzilay and Lillian Lee. Learning
to Paraphrase: An Unsupervised Approach
Using Multiple-Sequence Alignment. In
NAACL?03: Proceedings of the 2003
Conference of the North American Chapter
of the Association for Computational
Linguistics on Human Language
Technology, pages 16?23, Morristown, NJ,
USA, 2003. Association for Computational
Linguistics. doi:
10.3115/1073445.1073448.
[5] Andrei Z. Broder. Identifying and Filtering
Near-Duplicate Documents. In COM?00:
Proceedings of the 11th Annual Symposium
on Combinatorial Pattern Matching, pages
1004
1?10, London, UK, 2000. Springer-Verlag.
ISBN 3-540-67633-3.
[6] Manuel Cebrian, Manuel Alfonseca, and
Alfonso Ortega. Towards the Validation of
Plagiarism Detection Tools by Means of
Grammar Evolution. IEEE Transactions on
Evolutionary Computation, 13(3):477?485,
June 2009. ISSN 1089-778X.
[7] Paul Clough. Plagiarism in Natural and
Programming Languages: An Overview of
Current Tools and Technologies. Internal
Report CS-00-05, University of Sheffield,
2000.
[8] Paul Clough. Old and New Challenges in
Automatic Plagiarism Detection. National
UK Plagiarism Advisory Service,
http://ir.shef.ac.uk/cloughie/papers/pas_plagiarism.pdf,
2003.
[9] Paul Clough and Mark Stevenson. Creating
a Corpus of Plagiarised Academic Texts. In
Proceedings of Corpus Linguistics
Conference, CL?09 (to appear), 2009.
[10] Paul Clough and Mark Stevenson.
Developing A Corpus of Plagiarised Short
Answers. Language Resources and
Evaluation: Special Issue on Plagiarism
and Authorship Analysis (in press), 2010.
[11] Paul Clough, Robert Gaizauskas, and S. L.
Piao. Building and Annotating a Corpus for
the Study of Journalistic Text Reuse. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC-02), pages 1678?1691,
2002.
[12] Wiebe Hordijk, Mar?a L. Ponisio, and Roel
Wieringa. Structured Review of Code
Clone Literature. Technical Report
TR-CTIT-08-33, Centre for Telematics and
Information Technology, University of
Twente, Enschede, 2008.
[13] Winter Mason and Duncan J. Watts.
Financial Incentives and the "Performance
of Crowds". In HCOMP?09: Proceedings
of the ACM SIGKDD Workshop on Human
Computation, pages 77?85, New York, NY,
USA, 2009. ACM. ISBN
978-1-60558-672-4. doi:
10.1145/1600150.1600175.
[14] Hermann Maurer, Frank Kappe, and Bilal
Zaka. Plagiarism - A Survey. Journal of
Universal Computer Science, 12(8):
1050?1084, 2006.
[15] Martin Potthast, Benno Stein, Andreas
Eiselt, Alberto Barr?n-Cede?o, and Paolo
Rosso. Overview of the 1st International
Competition on Plagiarism Detection. In
Benno Stein, Paolo Rosso, Efstathios
Stamatatos, Moshe Koppel, and Eneko
Agirre, editors, SEPLN 2009 Workshop on
Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 09), pages
1?9. CEUR-WS.org, September 2009. URL
http://ceur-ws.org/Vol-502.
[16] Martin Potthast, Benno Stein, Andreas
Eiselt, Alberto Barr?n-Cede?o, and Paolo
Rosso. Overview of the 2nd International
Benchmarking Workshop on Plagiarism
Detection. In Benno Stein, Paolo Rosso,
Efstathios Stamatatos, and Moshe Koppel,
editors, Proceedings of PAN at CLEF 2010:
Uncovering Plagiarism, Authorship, and
Social Software Misuse, September 2010.
[17] Chanchal K. Roy and James R. Cordy.
Scenario-Based Comparison of Clone
Detection Techniques. In ICPC ?08:
Proceedings of the 2008 The 16th IEEE
International Conference on Program
Comprehension, pages 153?162,
Washington, DC, USA, 2008. IEEE
Computer Society. ISBN
978-0-7695-3176-2.
[18] Chanchal K. Roy and James R. Cordy.
Towards a Mutation-based Automatic
Framework for Evaluating Code Clone
Detection Tools. In C3S2E ?08:
Proceedings of the 2008 C3S2E conference,
pages 137?140, New York, NY, USA, 2008.
ACM. ISBN 978-1-60558-101-9.
[19] Chanchal K. Roy, James R. Cordy, and
Rainer Koschke. Comparison and
Evaluation of Code Clone Detection
Techniques and Tools: A Qualitative
Approach. Sci. Comput. Program., 74(7):
470?495, 2009. ISSN 0167-6423.
[20] Chanchal K. Roy and James R. Cordy. A
survey on software clone detection
research. Technical Report 2007-541,
School of Computing, Queen?s University
at Kingston, Ontario, Canada, 2007.
[21] Geoffrey R. Whale. Identification of
Program Similarity in Large Populations.
The Computer Journal, 33(2):140?146,
1990. doi: 10.1093/comjnl/33.2.140.
[22] Ying Zhao, George Karypis, and Usama
Fayyad. Hierarchical Clustering Algorithms
for Document Datasets. Data Min. Knowl.
Discov., 10(2):141?168, 2005. ISSN
1384-5810. doi:
10.1007/s10618-005-0361-3.
1005
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 962?973, Dublin, Ireland, August 23-29 2014.
Improving Cloze Test Performance of Language Learners Using Web N-Grams
Martin Potthast Matthias Hagen Anna Beyer Benno Stein
Bauhaus-Universit?t Weimar, Germany
<first name>.<last name>@uni-weimar.de
Abstract
We study the effectiveness of search engines for common usage, a new category of search engines
that exploit n-gram frequencies on the web to measure the commonness of a formulation, and that
allow their users to submit wildcard queries about formulation uncertainties often encountered in
the process of writing. These search engines help to resolve questions on common prepositions
following verbs, common synonyms in given contexts, and word order difficulties, to name only
a few. Until now, however, it has never been shown that search engines for common usage have
a positive impact on writing performance.
Our contribution is a large-scale user study with 121 participants using the Netspeak search
engine to shed light on this issue for the first time. Via carefully designed cloze tests we show
that second language learners who have access to a search engine for common usage significantly
and effectively improve their test performance as opposed to not using them.
1 Introduction
When writing texts in a second language, uncertainties on specific formulations regularly come up. Even
experienced second language writers may sometimes be in doubt about the preposition following a verb
or what word order to choose. In this paper, we study search engines for common usage (usage search
engines, for short) that aim at assisting second language writers to cope with their uncertainties. These
search engines allow for phrasal queries that include wildcards at positions where a user is not sure what
to write. The search results typically consist of a list of phrases matching the query?s expression?the
wildcards filled with formulations. The returned phrases are ranked by their commonness of being used in
everyday writing, where a phrase?s commonness is estimated by its occurrence frequency in a collection
of web n-grams. The occurrence frequencies are usually not hidden from the user but displayed alongside
each phrase, either implicitly or explicitly. This way, the users of usage search engines have a way of
judging whether a phrase is commonly used by others. Figure 1 (left) shows an example search result.
Target audience of usage search engines is language learners who have mastered basic vocabulary
and grammar but whose language proficiency in terms of their feeling for language usage is still worse
than that of a native speaker. Until recently, there has been hardly any technological support for them,
so they could only resort to studying abstract style guides, consuming foreign language media, and
language study travels in order to improve their usage skills. Today, three public usage search engines
are available. The first one, called Netspeak (Stein et al., 2010), is developed at our group since 2008.
It was followed by PhraseUp and Linggle (Boisson et al., 2013), which have been released in 2011
and 2013.
1
Moreover, there is Google?s N-Gram Viewer prototype (Michel et al., 2011), which has a
different purpose and target audience but visualizes n-gram usage over time.
All of these search engines provide a way to quantify the commonness of a phrase and thus have the
potential to become important tools for second language learners. That is, if they work as advertised.
Until now, it has not at all been clear whether writers can actually benefit from the information distilled
from analyzing n-gram occurrence frequencies, or whether they are easily misled, for example, by noisy
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Netspeak is freely available at www.netspeak.org, PhraseUp at www.phraseup.com, and Linggle at www.linggle.com.
962
Figure 1: Netspeak?s two alternative interfaces: search results can either be displayed as textual ranked
list of phrases alongside frequencies (left), or as WordGraph visualization (right) (Riehmann et al., 2011),
where the frequencies determine various aspects of the visualization. The WordGraph is particularly
suited to handling multiple wildcards per query. The participants of our user study used primarily the
textual interface, since they did not require more than one or two wildcards for solving the cloze tests.
data. Our contribution is to shed light on this issue for the first time and to conduct a large-scale user study
with 121 language learners aged 14?18, measuring their performance when using our Netspeak search
engine to solve cloze tests. The study ascertains the positive impact of Netspeak and by extension, usage
search engines in general; moreover, it shows the low barrier to entry of Netspeak?s user interface.
The paper is organized as follows: after a detailed discussion of related work in Section 2, Netspeak?s
retrieval engine is formally described in Section 3 as background for the design of our user study and as
an example of how such search engines work internally. Section 4 reports on our user study and provides
a statistical analysis of our findings. The paper closes with a conclusion and an outlook into future work.
2 Related Work
Carrying out research and development on usage search engines is an interdisciplinary effort that requires
expertise from information retrieval, information visualization and interface design, as well as domain
knowledge from computer linguistics. Therefore, we divide our review of related work into four parts:
(1) existing search engines and web services, (2) retrieval engines and wildcard search from the perspec-
tive of information retrieval, (3) search result visualization, and, (4) writing support systems dedicated
to second language writers.
2.1 Public Search Engines and Web Services
There are currently three public search engines and one public prototype that fall into the category of
search engines for common usage, namely Netspeak (Stein et al., 2010), PhraseUp, Linggle (Boisson
et al., 2013), and the Google N-Gram Viewer (Michel et al., 2011). All of them index large n-gram
corpora, and their search interfaces are primarily dedicated to returning results that allow their users to
judge the commonness of a phrase compared to alternative phrases. We distinguish the former three
search engines from the latter mainly by its target audience. While the former target average web users,
the latter targets professional linguists and humanities researchers. To the best of our knowledge, our
paper is the first to investigate the effectiveness of such search engines for the use case of assisting
writers, thereby underpinning these efforts.
Moreover, a number of other linguistic search engines are available, such as WebCorp Live (Kehoe
and Renouf, 2002), WebAsCorpus (Fletcher, 2007), and the Linguist?s Search Engine (Resnik and Elkiss,
2005). These search engines cannot be readily used for usage search as defined above, since they work
more like concordancers in that they only retrieve usage examples and present them in context, disregard-
ing usage commonness. Again, their target audience is professional linguists rather than laymen users,
let alone second language learners. While they may still be applied in the context of language learning,
the search interfaces of these search engines are not sufficiently tailored to this domain.
963
Another category of related web services that are readily available to second language learners include
style and grammar checkers, such as Grammarly, PaperRater, SlickWrite, AfterTheDeadline (Mudge,
2010), the Hemingway App, GrammarBase, etc. From what can be said by analyzing their features, all
of these services are based on a collection of basic style and grammar rules that can be checked automati-
cally with some degree of confidence in their recommendations. However, none of the services we found
make any recommendations with regard to usage commonness, i.e., they do not identify uncommon
formulations or make recommendations for more common ones.
2.2 Information Retrieval Models and Indexes for Wildcard Search
The retrieval models employed by usage search engines are hardly ever discussed in the literature cited
above. One of the few exceptions is Netspeak (Stein et al., 2010), where the retrieval model has been a
contribution in itself since it is tailored specifically to its application domain. For the lack of discussion
of the finer details of how the above search engines work, it can be assumed that they do not employ a
specifically tailored retrieval approach. Nevertheless, when reviewing the information retrieval literature
for retrieval models that support linguistic queries or wildcard queries, a number of sources can be found.
Cafarella et al. (2005, 2007) study indexing methods that are particularly suited to support queries
comprising parts-of-speech as wildcards. They introduce so-called neighborhood indexes whose disk
accesses required to answer a query are on the order of the number of non-wildcard terms in a query.
Rafiei and Li (2009) develop a wildcard search engine that supports linguistically rich wildcards in
order to support information extraction from the web, which employs a preprocessor for queries, and a
postprocessor for search results on top of a traditional web search engine. The approach does not create
a tailored index but translates the wildcard queries into flat queries that can be answered by traditional
search engines. Sekine (2008) explores the trie data structure as an alternative to inverted indexes when
indexing large-scale n-gram corpora. The approach is limited to short n-grams (n < 10) to be feasible,
which can be a strong point in terms of retrieval speed. Netspeak?s retrieval engine is also intentionally
restricted to small values of n, but uses minimal perfect hash functions instead of tries to maximize
retrieval performance.
While all of the aforementioned approaches support shallow linguistic wildcards, or only basic wild-
cards, Tsang and Chawla (2011) propose a method to support regular expressions. Doing so involves
various trade-offs between retrieval performance and index size. Further, a search engine like this may
be only useful to experts, but not second language learners. Again, all of the aforementioned contri-
butions target either professional linguists or they are meant to facilitate automatic usage, instead of
supporting average writers.
2.3 Visualization of Usage Search Results
An important part of every search engine is its user interface. Since usage search engines are still in their
infancy, their user interfaces have not been studied in-depth, so far. As a first attempt to close this gap,
we developed and analyzed two alternative user interfaces for Netspeak in a previous work, one textual
interface and one using a tailored visualization that was specifically developed for usage search engines,
the so-called WordGraph (Riehmann et al., 2011). Figure 1 shows them side-by-side. The textual inter-
face displays search results in the form of a tabular list, where each row lists an n-gram matching the
wildcard query alongside its absolute and relative occurrence frequency. If a query comprises more than
one wildcard, situations arise where this linear ranking of n-grams is insufficient to grasp the true distri-
bution of formulations that may be used instead of the wildcards. The WordGraph therefore visualizes
the search results as a horizontal graph, so that the i-th word of an n-gram is displayed as a node on
the i-th level of the graph. Paths from left to right through the graph correspond to n-grams found in
the result set returned by Netspeak. A user study that investigated the fitness of the WordGraph to serve
as a user interface for specific search tasks found that study participants prefer the WordGraph over the
textual user interface when the number of wildcards increases (Riehmann et al., 2012). The user study
we report on in this paper is based solely on the textual user interface, since most of our cloze tests can
be solved by using one wildcard.
964
2.4 Writing Support for Second Language Learners
?For writers of English as a Second Language (ESL), useful editorial assistance geared to their needs is
surprisingly hard to come by,? and ?[...] there has been remarkably little progress in this area over the last
decade,? observe Brockett et al. (2006) about the state of the art. This is despite the fact that English is
the second language of most people who speak English today.
2
A recent overview of technology to detect
grammatical errors of language learners is given by Leacock et al. (2010), whereas computer feedback for
second language learners is mostly studied within pedagogical research under the label of computer-aided
language learning (CALL). There, classroom systems are being deployed on a small scale to measure
their effects on student learning performance. The development of usage search engines in general, our
Netspeak engine in particular, and the user study contributed in this paper may be considered first steps
toward the development of new, better technologies that specifically target the needs of second language
learners and writers.
3 Netspeak: A Search Engine for Common Usage Based on Web N-Grams
As a background for our user study and as an example of how usage search engines work internally, this
section briefly describes Netspeak and its retrieval engine.
3
The main building block of Netspeak is a
query processor tailored to the following task: given a wildcard query q and a set D of n-grams, retrieve
those n-grams D
q
? D that match the pattern defined by q. To solve this task, we have developed
an index-based wildcard query processor addressing the three steps indexing, retrieval, and filtering, as
illustrated in Figure 2 (middle).
3.1 Query Language
Netspeak utilizes a query language defined by the EBNF grammar shown in Figure 2 (left). A query is a
sequence of literal words and wildcard operators, wherein the literal words must occur in the expression
sought after, while the wildcard operators allow to specify uncertainties. Currently five operators are
supported:
? the question mark (?), which matches exactly one word;
? the asterisk (*), which matches any sequence of words;
? the tilde sign in front of a word (?<word>), which matches any of the word?s synonyms;
? the multiset operator ({<words>}), which matches any ordering of the enumerated words; and,
? the optionset operator ([<words>]), which matches any one word from a list of options.
The textual interface displays the search results for the given query as a ranked list of phrases, ordered
by decreasing absolute and relative occurrence frequencies. This way, the user can find confidence in
choosing a particular phrase by judging both its absolute and relative frequencies. For example, a phrase
may have a low relative frequency but a high absolute frequency, or vice versa, which in both cases
indicates that the phrase is not the worst of all choices. Furthermore, the textual web interface offers
example sentences for each phrase, which are retrieved on demand when clicking on a plus sign next to
a phrase. This allows users who are still in doubt to get an idea of the larger context of a phrase.
3.2 Retrieval Engine
The indexing step is done offline. Let V denote the set of all words found in the n-grams D, and
let D?denote the set of integer references to the storage positions of the n-grams in D on hard disk.
During indexing, an inverted index ? : V ? P(D )? is built that maps each word w ? V to a sorted
list ?(w) ? D ,? where ?(w) is comprised of exactly all references to the n-grams in D that contain w.
2
http://en.wikipedia.org/wiki/English language#Geographical distribution
3
Extended versions of this section can be found in previous publications on Netspeak?s WordGraph visualization (Riehmann
et al., 2011; Riehmann et al., 2012).
965
EBNF grammar of Netspeak?s query language
query = { word | wildcard }
5
1
word = ( [apostrophe] ( letter { alpha } ) ) | ? , ?
letter = ? a ? | ... | ? z ? | ? A ? | ... | ? Z ?
alpha = letter | ? 0 ? | ... | ? 9 ?
apostrophe = ? ? ?
wildcard = ? ? ? | ? * ? | synonyms | multiset | optionset
synonyms = ? ~ ? word
multiset = ?{ ? word { word } ?} ?
optionset = ? [ ? word { word } ? ] ?
Netspeak's retrieval engine
Retrieval Filtering
Inverted
index ?
Web
n-grams D
?
w?q  ?(w) = ?q DqDq'q
Sequential
access
Random
access
Indexing
online
offline
rotate about
around
once
on
the
axis
y
the z
on its
the
its
an
its own
<empty>
a vertical
Frequency
128,176      63.7%
36,615      18.2%
10,390        5.2%
4,091        2.0%
3,941        2.0%
3,323        1.7%
3,110        1.5%
2,574        1.3%
Phrase
i am waiting for
i am waiting to
i am waiting on
i am waiting.
i am waiting,
i am waiting impatiently
i am waiting ur
i am waiting until
Figure 2: Netspeak at a glance (Riehmann et al., 2012): the left table shows Netspeak?s query language
as an EBNF grammar, the middle figure overviews its retrieval engine, and the right figure shows an
example of search results as shown to its users. Given a query q, the intersection of relevant postlists
yields a tentative postlist ?
q
, which then is filtered and presented as a ranked list. The index ? exploits
essential characteristics that are known a-priori about possible queries and the n-gram set D.
The list ?(w) is referred to as posting list or postlist. Since D is invariant, ? can be implemented as
an external hash table with O(1)-access to ?(w). For ? being space-optimal, a minimal perfect hash
function based on the CHD algorithm is employed (Belazzougui et al., 2009).
The two online steps, retrieval and filtering, are taken successively when answering a query q. Within
the retrieval step, a tentative postlist ?
q
=
?
w?q
?(w) is constructed; ?
q
is the complete set of references
to n-grams in D that contain all words in q. The computation of ?
q
is done in increasing order of postlist
length, whereas each ?(w) is read sequentially from hard disk. Within the filtering step, a pattern matcher
is compiled from q, and D
q
is constructed as the set of those n-grams referenced in ?
q
that are accepted
by the pattern matcher. Constructing D
q
requires random hard disk access. Basically, this approach
corresponds to how web search engines retrieve documents for a given keyword query before ranking
them. In what follows, we briefly outline how the search in D is significantly narrowed down.
With an inverted index that also stores specific n-gram information along with the keywords, the
filtering of ?
q
can be avoided. In this regard, we distinguish the queries that can be formulated with
Netspeak?s query language into two classes: fixed-length queries and variable-length queries. A fixed-
length query contains only wildcard operators that represent an a-priori known number of words, while
a variable-length query contains at least one wildcard operator that expands to a variable number of
words. For example, the query fine ? me is a fixed-length query since only 3-grams in D match this
pattern, while the query fine
*
me is a variable-length query since n-grams of length 2, 3, 4, . . . match.
Obviously, fixed-length queries can be answered with less filtering effort than variable-length queries:
simply checking an n-gram?s length suffices to discard many non-matching queries. The query processor
first reformulates a variable-length query into a set of fixed-length queries, which then are processed in
parallel, merging the results.
Moreover, the retrieval engine employs pruning strategies so that only relevant parts of a postlist
are read during retrieval, presuming sorted postlists. Head pruning means to start reading a postlist at
some entry within, without compromising recall. Given a query q, let ? denote an upper bound for the
frequencies of the n-grams in q?s result set D
q
, i.e., d ? D
q
implies f(d) ? ? . Obviously, in all postlists
that are involved within the construction of D
q
, all entries whose n-gram frequencies are above ? can
safely be skipped, whereas ? is determined in a preprocessing step as the lowest occurrence frequency of
a sub-sequence of q that does not include wildcards. Up to this point, the retrieval of n-grams matching
a query q is exact?but, not all n-grams that match a query are of equal importance. We consider this
fact by applying tail pruning for postlists that are too long to be read at once into main memory. As a
consequence, less frequent n-grams that might match a given query can be missed.
3.3 The Web n-Gram Collection
To provide relevant suggestions, a wide cross-section of written text on the web is required. Currently,
Netspeak indexes the Google n-gram corpus ?Web 1T 5-gram Version 1? (Brants and Franz, 2006),
966
which consists of 42 GB of phrases up to a length of n = 5 words along with their occurrence frequencies
on the web in 2006. This corpus has been compiled from approximately 1 trillion words extracted from
the English portion of the web, totaling more than 3 billion n-grams. Two post-processing steps were
applied: case reduction and vocabulary filtering. For the latter, a white list vocabulary V was compiled
and only n-grams whose words appear in V were retained. The vocabulary V consists of the words
found in the Wiktionary and various other dictionaries, complemented by words from the 1-gram portion
of the Google corpus whose occurrence frequency exceeds 10 000. After post-processing, the size of the
corpus has been reduced by about 54%.
3.4 Retrieval Performance in Practice and Public Availability
In practice, the described techniques enable Netspeak to provide search results at a speed similar to
modern web search engines. Results are usually returned within a couple of milliseconds. Whenever a
user stops typing for more than 300 milliseconds, the current input is submitted as an ?instant? query
without need for a click. That way, the ?search experience? with Netspeak is similar to what users expect
from web search engines.
Netspeak is freely available online and has about 300 distinct users on a working day who submit about
2500 queries (half the workload on weekends). Most of its users are returning users. From their feedback
and from our own experience, we know that Netspeak helps to resolve uncertainties on formulations in
the daily process of writing papers, proposals, etc. However, in the following section we attempt to
capture Netspeak?s effectiveness in a controlled user study.
4 User Study on the Effectiveness of Usage Search Engines
It is generally assumed that usage search engines are useful, say, that they provide valuable feedback that
leads to improved writing. To empirically confirm this ?usefulness? assumption, we conduct systematic
tests with experienced language learners and analyze whether a usage search engine enables them to
improve their writing. We choose Netspeak as a representative of usage search engines for our study.
Our study?s underlying rationale is to model the use case of usage search engines by solving cloze
tests. In a cloze test, a word or a phrase is removed from a sentence and the participant has to replace
the missing words. Although we followed standard procedures on constructing cloze tests (Sachs et al.,
1997), it should be noted that our usage of cloze tests is not as originally intended (Taylor, 1953). We
do not assess a language learner?s reading skills, but use the cloze test to model word choice, which
resembles the use case of usage search engines very well. For each participant, we provide two different
cloze test questionnaires. The first has to be solved without any help, whereas for the second, participants
are allowed to use the search engine. Besides evaluating the answers, we also analyze the submitted
search queries.
4.1 Experiment 1: General Usage, Average Learners
In the first experiment, we examine whether the search engine in general can support users in resolving
uncertainties on formulations modeled by cloze tests. Our hypothesis is that using a usage search engine
helps to improve a human?s performance in such tests.
Experimental Design To test our hypothesis, we conduct an empirical study with a within-subjects
design (Lazar et al., 2010). This means that our participants are exposed to a cloze test without the help
of a search engine and then to another cloze test where our chosen usage search engine is allowed.
The to-be-solved cloze tests are carefully constructed under the guidance of a university-level English
teacher who is a native English speaker. From several language learner textbooks, we selected questions
in order to have an equal mix of two easy, four medium, and three hard questions for two different cloze
test questionnaires A and B (see Appendix A and B).
In order to have objectively comparable test cases, the English teacher provided four possible answers
for each of the nine questions from test A and B, from which participants had to choose one in each case.
This way, the participants do not have to rely on their subjective own vocabulary knowledge.
967
Table 1: Results of our user study on the impact of usage search engines on language learners.
Experiment Question Questions answered
difficulty manually with search engine available
but not used and used
X ? ? sum X ? ? X ? ? sum
easy 17 41 0 58 7 2 1 42 6 0 58
Average medium 61 100 3 164 25 16 1 88 34 0 164
Learners hard 37 72 2 111 4 22 2 18 62 1 111
all 115 213 5 333 36 40 4 149 102 1 333
Highly easy 11 5 0 16 10 1 0 4 1 0 16
Experienced medium 27 17 0 44 24 2 0 14 3 1 44
Learners hard 18 12 0 30 8 8 0 4 10 0 30
all 56 34 0 90 42 11 0 22 14 1 90
easy 147 29 1 177 28 2 1 135 11 0 177
Specific medium 117 57 3 177 20 6 1 123 24 3 177
Operators hard 135 40 2 177 31 5 2 130 18 1 177
all 399 126 6 531 79 13 4 378 53 4 531
Search engine not used Search engine used
Experiment Search engine used vs not used
p-value effect size
Average Learners 0.0000 0.73 large
Highly Exp. Learners 0.7030 0.12 small
Specific Operators 0.0000 0.58 large
In the left table,Xdenotes correct answers,
? denotes wrong answers, and ? denotes
unanswered questions.
To evaluate the statistical significance and the
effect size, we distinguished cloze test answers
for the conditions ?Search engine not used? and
?Search engine used? in the left table.
The brackets below the bottom row of the left
table indicate which cases fall under what
condition.
The English teacher first chose the questions independent of knowing the indexed n-grams of the
search engine. In a ?postprocessing? step, the chosen answers for the questions are checked for existence
in the n-gram vocabulary of the search engine. This always was the case, although sometimes the queries
required to retrieve them were different from the exact context around the cloze test?s missing word. This
check ensured that there was a chance of answering each individual question in the cloze tests with the
search engine.
During the experiment, the use or non-use of the search engine is the independent variable. The
dependent variable is the number of correct answers per questionnaire. There also are confounding
variables like whether our engine really was used when it was allowed, the time needed to type queries,
or the different numbers of answered questions with and without the search engine. We will further
elaborate on how we deal with these variables in the following description of the experimental process.
Experimental Process From three different local high schools, 43 German pupils (23 female, 20 male;
mean age 16.2, SD = 1.2) with five or more years of English courses participated in six groups. None
of the participants had any previous experience with any usage search engine.
When a group arrived in our lab, they were randomly assigned to a lab seat; questionnaire A or B
were distributed ensuring that neighboring participants had a different question set. This way, the test
distribution was random and the participants could not collaborate (which was also ensured by their
accompanying ?watchdog? teachers). After seven minutes, the first questionnaires were collected and
a short five minute introduction to the search engine and its operator set was given. To ensure that
the pupils really followed the introduction, we provided the chance of winning small prices based on
correctly answering a question on the underlying technique of usage search engines?the index?in an
exit questionnaire. After that, each participant had to solve the opposite questionnaire (A when the first
was B, and vice versa) but was allowed to use the search engine this time. In pilot studies, we noticed
that pupils of that age often need a lot of time for typing their search queries on a standard keyboard.
Thus, we allowed 10 minutes for the second questionnaire. This confounding variable of different timing
for the questionnaires could not be avoided. Otherwise, most participants would not have had the chance
to complete all questions. In order to check whether our participants actually used the search engine, we
logged their querying behavior and manually identified the questions which they had answered without
using the search engine.
Results and Discussion Since not all participants answered all questions for both cloze tests, we ex-
cluded the six participants from the following analyses, who had a difference of more than one between
the number of answered questions for either test.
The aggregated numbers on questionnaire performance for the remaining 37 individuals are given in
the first block of rows of Table 1 (?Average Learners?). Note that the ratio of correct vs. incorrect answers
goes up when the search engine was used: on average, an individual answered two more questions
correctly. Especially interesting is that the short five minute introduction was sufficient for that effect
968
which shows the strength of the textual interface. To statistically estimate the per-individual effect, we
compare the ratio of correct answers among all answers when the search engine was used to the ratio
when it was not used (note that this includes the questions where the engine was allowed but was not used;
i.e., columns ?manually? and ?but not used? in Table 1). According to the Shapiro-Wilk test (Razali and
Wah, 2011), the individual participants? ratios are not normally distributed for either condition (engine
used vs. not used) such that we choose a non-parametric significance test (Lazar et al., 2010). For our
within-subjects design with ratio data and two to-be-compared samples, the Wilcoxon signed rank test
is known as a suitable significance test (Lazar et al., 2010). For the 37 participants? ratios we get a p-
value below 0.001 and thus can reject the null hypothesis that the ratios? distributions are equal. Further
estimating the effect size for the Wilcoxon signed rank test, we obtain a value of 0.73 which corresponds
to a large effect (Cohen, 1988; Fritz et al., 2012). This result supports our prediction that the search
engine can help resolve writing uncertainties.
We also studied the query logs of our participants. Per cloze test question, they submitted 4?5 queries
with 2?3 terms on average (a wildcard is counted as a term). The last query in each such ?search session?
for a single question typically was 3?4 terms long. Almost all participants only used the ?-operator and
most participants chose the strategy of querying with context before and after the operator. Having only
context before or only after the operator are less successful strategies with higher error ratios.
4.2 Experiment 2: General Usage, Highly Experienced Learners
In our neighborhood, there also is an international high school, where German pupils have all their
classes taught in English. Obviously, such pupils have a much higher experience speaking and writing
English than our participants from Experiment 1. For a second experiment, we invited pupils from the
international school to our lab. Our hypothesis is that the pupils from the international school will have
to use the search engine less frequently but still can benefit from it for individual questions.
Experimental Design and Process We used the same questionnaires, time constraints, and logging
strategies as in Experiment 1. From the international school, 12 German pupils (7 female, 5 male; mean
age 16.5, SD = 0.7) participated in two groups. These pupils are taught all their courses in English
for five and more years. None of them had any previous experience with usage search engines. The
experimental process was as in Experiment 1.
Results and Discussion Again, not all participants answered all questions for both cloze tests; we ex-
cluded the two participants from the following analyses, who had a difference of more than one between
the number of answered questions for either test.
The aggregated numbers on questionnaire performance for the remaining 10 individuals are given in
the second block of rows of Table 1 (?Highly Experienced Learners?). As expected, the highly experi-
enced pupils used the search engine very rarely. This is not too surprising since our questionnaires were
designed with an average German pupil in mind; many questions seemed too easy to the internationals
which they also indicated in their exit questionnaires. Still, on a per-question basis, for the medium and
difficult questions where the pupils used the search engine, they slightly improved their performance.
However, the sample and the effect size are too small to draw any reliable conclusions.
The experiment shows that the highly experienced pupils indeed did not use our engine often. How-
ever, the predicted benefit for them cannot be confirmed from our small sample. It is thus an interesting
open task to conduct a larger study with highly experienced users and more difficult questions.
4.3 Experiment 3: Specific Operators, Average Learners
Our first experiment revealed that most participants used the ?-operator to solve the tasks. We thus
designed a third experiment specifically targeted at the options, synonyms, and word-order operators of
our Netspeak search engine. Our hypothesis is that each individual operator helps improve a human?s
performance in cloze tests targeted at the individual operator.
Experimental Design As in Experiment 1, we asked the university-level English teacher to design
two cloze test questionnaires (see Appendix C and D); for each operator with an easy, a medium, and
969
a hard question. Here, the questions for the option operator are of a similar kind as the questions from
Experiment 1. Four alternatives are given, but the participants are asked to use the option operator [] and
not the ?-operator. For synonyms, a complete sentence is given and for a specified word, the best among
four given potential synonyms is requested. As for the word order operator, a two-word phrase is missing
from the sentence and the two different word orders are provided as options. Like in Experiment 1 and 2,
the explicit answer options ensure that the test is objective and not subjective. In a second development
step, the questions were checked for solvability using the search engine just like in Experiment 1.
Experimental Process From three different local schools, 66 pupils (45 female, 21 male; mean
age 15.9, SD = 1.4) participated in six groups. None of the pupils participated in Experiment 1 or 2 nor
had they any previous experience with usage search engines. These pupils have learned English in their
schools for at least five years. The schedule was similar to Experiment 1 with an emphasis on the three
tested operators in the introductory explanations on Netspeak. In the questionnaires, the pupils were
asked to use only the specific operator for the respective queries. Logging their queries, we are able to
exclude solutions obtained by using a not-allowed operator.
Results and Discussion Again, not all participants answered all questions for both cloze tests; we
excluded the seven participants from the following analyses, who had a difference of more than one
between the number of answered questions for either test.
The aggregated numbers on questionnaire performance for the remaining 59 individuals are given in
the third block of rows of Table 1 (?Specific Operators?). Note that the ratio of correct vs. incorrect
answers goes up when the search engine was used: one to two more questions correctly answered on
average. As in Experiment 1, the short five minute introduction is sufficient for that effect which shows
the strength of our interface. To statistically estimate the per-individual effect, we compare the ratio
of correct answers among all answers when the search engine was used to the ratio when it was not
used (note that this includes the questions where the engine was allowed but was not used; i.e., columns
?manually? and ?but not used? in Table 1). For the 59 participants? ratios, we get a p-value below 0.001
and thus can reject the null hypothesis that the ratios? distributions are equal. Further, estimating the
effect size for the Wilcoxon signed rank test, we obtain a value of 0.58 which corresponds to a large
effect (Cohen, 1988; Fritz et al., 2012). Again, the result supports our prediction that usage search
engines can help resolve writing uncertainties.
However, a deeper analysis reveals that the large effect is due to the synonym operator. Only for
that operator, a statistically significant performance difference and a large effect size can be shown. For
the other two operators, the null hypothesis of no performance difference cannot be rejected. This is
in line with the exit questionnaire findings, where the pupils reported the synonym operator to be very
helpful while the other questions were perceived as rather easy. In the query log analyses, we found that
context before and after the wildcard had a similarly positive effect as before and was generally better
than adding context only before the wildcard.
5 Conclusion and Future Work
Search engines for common usage have the potential to become an important tool for second language
writers and learners. The possibility to check one?s language against what is commonly written forms a
unique opportunity to improve one?s writing on-the-fly. Such information has not been available at scale
so far. Our user study shows that usage search engines can indeed help second language writers solve
uncertainties about formulations. Modeling writing uncertainties by carefully designed cloze tests, we
are able to show a significant improvement when experienced language learners use the search engine.
Highly experienced language learners represented by our study participants from an international
school, however, did not use the search engine often enough to draw meaningful conclusions. This
can probably be attributed to the fact that the cloze tests were not tailored to their level of language pro-
ficiency. Therefore, the question of whether also highly experienced writers and learners, or even native
speakers, can benefit from such search engines remains open and is left for future work.
Another missing piece in determining the effectiveness of usage search engines is whether their users
970
actually learn something while using them, or whether users frequently submit the same or similar queries
again and again. Our user study was not designed to answer this question, since our participants were
only around for about 30 minutes for organizational reasons. Even measuring effects on short-term
memory is rendered infeasible in this time frame. A longitudinal study would be ideal, in this case, but
we also see an exciting, data-driven way to approach this. By analyzing the query logs of Netspeak,
which is currently being used hundreds of times per day, we can track returning users. We can then study
their online search behavior to determine if and how often they return to submit similar queries, which
allows us to draw conclusions about their learning success. More generally, the query logs of usage
search engines may form a unique opportunity to observe language learners ?in the wild? as opposed to
the laboratory.
Finally, regarding the user interface of usage search engines, our user study has revealed ways to
improve them. For example, the interface must be optimized for faster typing (especially on mobile de-
vices) as we observed that the pupils were not adept to entering special characters on standard keyboards,
which resulted in slow typing speed. Besides this, our user study also showed that the current state of
Netspeak?s textual user interface as well as the simplified wildcard query language is easy enough to
be understood in less than a minute by any newcomer, which demonstrates the low barrier to entry that
search engines for common usage have right now.
Acknowledgements
We thank the anonymous participants of our user study as well as Tim Gollub, Martin Trenkmann,
Michael V?lske, Howard Atkinson, Johannes Kiesel, Matthias Busse, and Alexander Herr for their help
in organizing the user study.
References
Djamal Belazzougui, Fabiano C. Botelho, and Martin Dietzfelbinger. 2009. Hash, Displace, and Compress. In
Proceedings of ESA 2009, pages 682?693.
Joanne Boisson, Ting-Hui Kao, Jian-Cheng Wu, Tzu-Hsi Yen, and Jason S. Chang. 2013. Linggle: A Web-scale
Linguistic Search Engine for Words in Context. In Proceedings of ACL 2013 (Demos), pages 139?144.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium LDC2006T13.
Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL Errors Using Phrasal SMT Tech-
niques. In Proceedings of ACL 2006, pages 249?256.
Michael J. Cafarella and Oren Etzioni. 2005. A Search Engine for Natural Language Applications. In Proceedings
of WWW 2005, pages 442?452.
Michael J. Cafarella, Christopher Re, Dan Suciu, and Oren Etzioni. 2007. Structured Querying of Web Text Data:
A Technical Challenge. In Proceedings of CIDR 2007, pages 225?234.
Jacob Cohen. 1988. Statistical Power Analysis for the Behavioral Sciences. Psychology Press.
William H. Fletcher. 2007. Implementing a BNC-Compare-able Web Corpus. In Proceedings of the 3rd Web as
Corpus Workshop, pages 43?56.
Catherine O. Fritz, Peter E. Morris, and Jennifer J. Richler. 2012. Effect Size Estimates: Current Use, Calculations,
and Interpretation. Journal of Experimental Psychology: General, 141(1):2.
Andrew Kehoe and Antoinette Renouf. 2002. WebCorp: Applying the Web to Linguistics and Linguistics to the
Web. In Proceedings of WWW 2002 (Posters).
Jonathan Lazar, Jinjuan Heidi Feng, and Harry Hochheiser. 2010. Research Methods in Human-Computer Inter-
action. Wiley Publishing.
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error
Detection for Language Learners. Morgan and Claypool Publishers.
Jean-Baptiste Michel, Yuan K. Shen, Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team,
Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak,
and Erez L. Aiden. 2011. Quantitative Analysis of Culture Using Millions of Digitized Books. Science,
331(6014):176?182.
971
Raphael Mudge. 2010. The Design of a Proofreading Software Service. In Proceedings of HLT 2010 Workshop
on Computational Linguistics and Writing, pages 24?32.
Davood Rafiei and Haobin Li. 2009. Data Extraction from the Web Using Wild Card Queries. In Proceedings of
CIKM 2009, pages 1939?1942.
Nornadiah Mohd Razali and Yap Bee Wah. 2011. Power Comparisons of Shapiro-Wilk, Kolmogorov-Smirnov,
Lilliefors and Anderson-Darling Tests. Journal of Statistical Modeling and Analytics, 2(1):21?33.
Philip Resnik and Aaron Elkiss. 2005. The Linguist?s Search Engine: An Overview. In Proceedings of ACL 2005
(Posters and Demos), pages 33?36.
Patrick Riehmann, Henning Gruendl, Bernd Froehlich, Martin Potthast, Martin Trenkmann, and Benno Stein.
2011. The NETSPEAK WORDGRAPH: Visualizing Keywords in Context. In Proceedings of PacificVis 2011,
pages 123?130.
Patrick Riehmann, Henning Gruendl, Martin Potthast, Martin Trenkmann, Benno Stein, and Bernd Froehlich.
2012. WORDGRAPH: Keyword-in-Context Visualization for NETSPEAK?s Wildcard Search. IEEE Transac-
tions on Visualization and Computer Graphics, 18(9):1411?1423.
J. Sachs, P. Tung, and R.Y.H. Lam. 1997. How to Construct a Cloze Test: Lessons from Testing Measurement
Theory Models. Perspectives, 9:145?160.
Satoshi Sekine. 2008. A Linguistic Knowledge Discovery Tool: Very Large N -gram Database Search with
Arbitrary Wildcards. In Proceedings of COLING 2008 (Demos), pages 181?184.
Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving Customary Web Language to Assist
Writers. In Proceedings of ECIR 2010, pages 631?635.
W. L. Taylor. 1953. Cloze Procedure: A New Tool for Measuring Readability. Journalism Quarterly, 30:415?433.
Dominic Tsang and Sanjay Chawla. 2011. A Robust Index for Regular Expression Queries. In Proceedings of
CIKM 2011, pages 2365?2368.
Appendix
A Questionnaire A from Experiments 1 and 2
1. I really prefer just anything watching television.
? against X to ? about ? on
2. Has Tony?s new book yet?
X come out ? published ? developed ? drawn up
3. If this plan off, I promise you you?ll get the credit for it.
? lets ? goes ? gets X comes
4. Helen had great admiration her history teacher.
? in ? to X for ? on
5. I just couldn?t over how well the team played!
X get ? turn ? make ? put
6. The problem stems the government?s lack of action.
? out X from ? under ? for
7. It?s too late to phone Jill at work, at any .
? case ? time ? situation X rate
8. I?m afraid I?m not very good children.
? about ? for X with ? at
9. We are no obligation to change goods which were not purchased here.
? with X under ? to ? at
B Questionnaire B from Experiments 1 and 2
1. Don?t worry about the lunch. I?ll to it.
? look ? prepare ? care X see
2. I am afraid that these regulations have to be with.
? provided X complied ? faced ? met
3. Our thoughts on our four missing colleagues.
? based X centred ? laid ? depended
4. Carol doesn?t have a very good relationship her mother.
X with ? at ? for ? to
972
5. It seems to be your boss who is fault in this case.
? under ? with X at ? for
6. Being rich doesn?t count much on a desert island.
? on ? to ? of X for
7. The policeman me off with a warning as it was Christmas.
? sent ? gave X let ? set
8. Tina is an authority Byzantine architecture.
X on ? for ? with ? in
9. I was the impression that you liked Indian food.
? at ? with ? of X under
C Questionnaire A from Experiment 3
Choose the word which fits best using the options operator [<words>].
1. If you spend so much money every day, you will out of money before the end of the month.
? pay ? use X run ? take
2. You need to take all your other clothes before you put on your swimming costume.
? down ? away ? out X off
3. I?m afraid I?m not very good history.
? about ? for X at ? with
Choose the best synonym for the underlined word using the synonym operator ?<word>.
4. I love studying geometry the most.
? hate ? absent X enjoy ? difficult
5. My ambition is to become a computer scientist.
? thought ? reward ? study X dream
6. Your action will have serious consequences.
X effects ? events ? reasons ? affects
Choose the correct word order using the word order operator {<words>}.
7. The bird! I?m going to help it!
X poor little ? little poor
8. She was wearing a dress.
? green beautiful X beautiful green
9. I plan on wearing my coat.
X long black ? black long
D Questionnaire B from Experiment 3
Choose the word which fits best using the options operator [<words>].
1. Sometimes Julia speaks very quickly so the other students have to ask her to slow .
X down ? up ? out ? off
2. The missing plane has apparently disappeared without a .
? sign ? news ? word X trace
3. When Gabriel?s credit card stopped, he cut it many small pieces.
? out X into ? apart ? in
Choose the best synonym for the underlined word using the synonym operator ?<word>.
4. I choose to study the differences between alligators and crocodiles.
? make ? buy X prefer ? wash
5. I cannot find my money. Can you get me my billfold?
X wallet ? pocket ? watch ? bag
6. This is a very rough environment for elephants to live in.
X harsh ? abrasive ? coarse ? beneficial
Choose the correct word order using the word order operator {<words>}.
7. She sold the chairs at a yard sale.
? wooden old X old wooden
8. The years were fantastic.
? two first X first two
9. It?s close to the building.
X big blue ? blue big
973
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1212?1221,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Crowdsourcing Interaction Logs to Understand Text Reuse from the Web
Martin Potthast Matthias Hagen Michael V?lske Benno Stein
Bauhaus-Universit?t Weimar
99421 Weimar, Germany
<first name>.<last name>@uni-weimar.de
Abstract
We report on the construction of the Webis
text reuse corpus 2012 for advanced re-
search on text reuse. The corpus compiles
manually written documents obtained from
a completely controlled, yet representative
environment that emulates the web. Each
of the 297 documents in the corpus is about
one of the 150 topics used at the TREC
Web Tracks 2009?2011, thus forming a
strong connection with existing evaluation
efforts. Writers, hired at the crowdsourc-
ing platform oDesk, had to retrieve sources
for a given topic and to reuse text from
what they found. Part of the corpus are
detailed interaction logs that consistently
cover the search for sources as well as the
creation of documents. This will allow for
in-depth analyses of how text is composed
if a writer is at liberty to reuse texts from a
third party?a setting which has not been
studied so far. In addition, the corpus pro-
vides an original resource for the evalua-
tion of text reuse and plagiarism detectors,
where currently only less realistic resources
are employed.
1 Introduction
The web has become one of the most common
sources for text reuse. When reusing text from
the web, humans may follow a three step ap-
proach shown in Figure 1: searching for appro-
priate sources on a given topic, copying of text
from selected sources, modification and paraphras-
ing of the copied text. A considerable body of
research deals with the detection of text reuse, and,
in particular, with the detection of cases of plagia-
rism (i.e., the reuse of text with the intent of disguis-
ing the fact that text has been reused). Similarly,
a large number of commercial software systems is
being developed whose purpose is the detection of
plagiarism. Both the developers of these systems as
well as researchers working on the subject matter
frequently claim their approaches to be searching
the entire web or, at least, to be scalable to web
size. However, there is hardly any evidence to
substantiate this claim?rather the opposite can be
observed: commercial plagiarism detectors have
not been found to reliably identify plagiarism from
the web (K?hler and Weber-Wulff, 2010), and the
evaluation of research prototypes even under lab-
oratory conditions shows that there is still a long
way to go (Potthast et al, 2010b). We explain the
disappointing state of the art by the lack of realistic,
large-scale evaluation resources.
With our work, we want to contribute to closing
the gap. In this regard the paper in hand introduces
the Webis text reuse corpus 2012 (Webis-TRC-12),
which, for the first time, emulates the entire process
of reusing text from the web, both at scale and in
a controlled environment. The corpus comprises a
number of features that set it apart from previous
ones: (1) the topic of each document in the corpus
is derived from a topic of the TREC Web Track,
and the sources to copy from have been retrieved
manually from the ClueWeb corpus. (2) The search
for sources is logged, including click-through and
browsing data. (3) A fine-grained edit history has
been recorded for each document. (4) A total of
297 documents were written with an average length
of about 5700 words, whereas diversity is ensured
via crowdsourcing. Altogether, this corpus forms
the current most realistic sample of writers reusing
text. The corpus is publicly available.1
1.1 Related Work
As organizers of the annual PAN plagiarism de-
tection competitions,2 we have introduced the first
standardized evaluation framework for that pur-
1http://www.webis.de/research/corpora
2http://pan.webis.de
1212
Search I?m Feeling Lucky
Search Copy & Paste Modification
Figure 1: The basic steps of reusing text from the web (Potthast, 2011).
pose (Potthast et al, 2010b). Among others, it com-
prises a series of corpora that consist of automat-
ically generated cases of plagiarism, provided in
the form of the PAN plagiarism corpora 2009-2011.
The corpora have been used to evaluate dozens of
plagiarism detection approaches within the respec-
tive competitions in these years;3 but even though
they have been adopted by the community, a num-
ber of shortcomings render them less realistic:
1. All plagiarism cases were generated by ran-
domly selecting text passages from documents
and inserting them at random positions in a
host document. This way, the reused passages
do not match the topic of the host document.
2. The majority of the reused passages were mod-
ified in order to obfuscate the reuse. However,
the applied modification strategies, again, are
basically random: shuffling, replacing, insert-
ing, or deleting words randomly. An effort
was made to avoid non-readable text, yet none
of it bears any semantics.
3. The corpus documents are parts of books from
the Project Gutenberg. Many of these books
are pretty old, whereas today the web is the
predominant source for text reuse.
To overcome the second issue, about 4 000 pas-
sages were rewritten manually via crowdsourcing
on Amazon?s Mechanical Turk for the 2011 cor-
pus. But, because of the first issue (random passage
insertion), a topic drift analysis can spot a reused
passage more easily than a search within the doc-
ument set containing the original source (Potthast
et al, 2011). From these observations it becomes
clear that there are limits for the automatic con-
struction of such kinds of corpora. The Webis text
reuse corpus 2012 addresses all of the mentioned
issues since it has been constructed manually.
3See (Potthast et al, 2009; Potthast et al, 2010a; Potthast
et al, 2011) for overviews of approaches and evaluation results
of each competition.
Besides the PAN corpora, there are two other
corpora that comprise ?genuinely reused? text: the
Clough09 corpus, and the Meter corpus. The for-
mer corpus consists of 57 answers to one of five
computer science questions that were reused from
a respective Wikipedia article (Clough and Steven-
son, 2011). While the text was genuinely written by
a number of volunteer students, the choice of topics
is narrow, and text lengths range from 200 to 300
words, which is hardly more than 2-3 paragraphs.
Also, the sources from which text was reused were
given up front, so that there is no data about their
retrieval. The Meter corpus annotates 445 cases
of text reuse among 1 716 news articles (Clough et
al., 2002). The cases of text reuse in this corpus
are realistic for the news domain; however, they
have not been created by the reuse process outlined
in Figure 1. Note that in the news domain, text is
often reused directly from a news wire without the
need for retrieval. Our new corpus complements
these two resources.
2 Corpus Construction
Two data sets form the basis for constructing our
corpus, namely (1) a set of topics to write about
and (2) a set of web pages to research about a given
topic. With regard to the former, we resort to topics
used at TREC, specifically to those used at the Web
Tracks 2009?2011. With regard to the latter, we em-
ploy the ClueWeb corpus from 20094 (and not the
?web in the wild?). The ClueWeb comprises more
than one billion documents from ten languages and
can be considered as a representative cross-section
of the real web. It is a widely accepted resource
among researchers and became one of the primary
resources to evaluate the retrieval performance of
search engines within several TREC tracks. Our
corpus?s strong connection to TREC will allow for
unforeseen synergies. Based on these decisions our
4http://lemurproject.org/clueweb09
1213
corpus construction steps can be summarized as
follows:
1. Rephrasing of the 150 topics used at the
TREC Web Tracks 2009?2011 so that they
explicitly invite people to write an essay.
2. Indexing of the ClueWeb corpus category A
(the entire English portion with about 0.5 bil-
lion documents) using the BM25F retrieval
model plus additional features.
3. Development of a search interface that allows
for answering queries within milliseconds and
that is designed along the lines of commercial
search interfaces.
4. Development of a browsing API for the
ClueWeb, which serves ClueWeb pages on
demand and which rewrites links of delivered
pages, now pointing to their corresponding
ClueWeb pages on our servers (instead of to
the originally crawled URL).
5. Recruiting 27 writers, 17 of whom with a
professional writing background, hired at the
crowdsourcing platform oDesk from a wide
range of hourly rates for diversity.
6. Instructing the writers to write one essay at
a time of at least 5000 words length (cor-
responding to an average student?s home-
work assignment) about an open topic of
their choice, using our search engine?hence
browsing only ClueWeb pages.
7. Logging all writers? interactions with the
search engine and the ClueWeb on a per-essay
basis at our site.
8. Logging all writers? edits to their essays in a
fine-grained edit log: a snapshot was taken
whenever a writer stopped writing for more
than 300ms.
9. Double-checking all of the essays for quality.
After having deployed the search engine and
completed various usability tests, the actual corpus
construction took nine months, from April 2012
through December 2012.
Obviously, the outlined experimental setup can
serve different lines of research and is publicly
available as well. The remainder of the section
presents elements of our setup in greater detail.
2.1 Topic Preparation
Since the topics used at the TREC Web Tracks were
not amenable for our purpose as is, we rephrased
them so that they ask for writing an essay instead of
searching for facts. Consider for example topic 001
of the TREC Web Track 2009:
Query. obama family tree
Description. Find information on Pres-
ident Barack Obama?s family history,
including genealogy, national origins,
places and dates of birth, etc.
Sub-topic 1. Find the TIME magazine
photo essay ?Barack Obama?s Family
Tree.?
Sub-topic 2. Where did Barack Obama?s
parents and grandparents come from?
Sub-topic 3. Find biographical informa-
tion on Barack Obama?s mother.
This topic is rephrased as follows:
Obama?s family. Write about President
Barack Obama?s family history, includ-
ing genealogy, national origins, places
and dates of birth, etc. Where did Barack
Obama?s parents and grandparents come
from? Also include a brief biography of
Obama?s mother.
In the example, Sub-topic 1 is considered too
specific for our purposes while the other sub-topics
are retained. TREC Web Track topics divide into
faceted and ambiguous topics. While topics of
the first kind can be directly rephrased into essay
topics, from topics of the second kind one of the
available interpretations was chosen.
2.2 A Controlled Web Search Environment
To give the oDesk writers a familiar search experi-
ence while maintaining reproducibility at the same
time, we developed a tailored search engine called
ChatNoir (Potthast et al, 2012b).5 Besides ours,
the only other public search engine for the ClueWeb
is Carnegie Mellon?s Indri,6 which, unfortunately,
is far from our efficiency requirements. Moreover,
its search interface does not follow the standard in
terms of result page design, and it does not give
access to interaction logs. Our search engine is
on the order of milliseconds in terms of retrieval
5http://chatnoir.webis.de
6http://lemurproject.org/clueweb09.php/index.php#Services
1214
time, its interface follows industry standards, and
it features an API that allows for user tracking.
ChatNoir is based on the BM25F retrieval
model (Robertson et al, 2004), uses the anchor
text list provided by (Hiemstra and Hauff, 2010),
the PageRanks provided by the Carnegie Mellon
University alongside the ClueWeb corpus, and the
Spam rank list provided by (Cormack et al, 2011).
ChatNoir comes with a proximity feature with
variable-width buckets as described by (Elsayed
et al, 2011). Our choice of retrieval model and
ranking features is intended to provide a reasonable
baseline performance. However, it is neither near
as mature as those of commercial search engines
nor does it compete with the best-performing mod-
els from TREC. Yet, it is among the most widely
accepted models in information retrieval, which
underlines our goal of reproducibility.
In addition to its retrieval model, ChatNoir im-
plements two search facets: text readability scoring
and long text search. The first facet, similar to that
provided by Google, scores the readability of a text
found on a web page via the well-known Flesch-
Kincaid grade level formula (Kincaid et al, 1975):
it estimates the number of years of education re-
quired in order to understand a given text. This
number is mapped onto the three categories ?Sim-
ple? (up to 5 years), ?Intermediate? (between 5 and
9 years) and ?Expert? (at least 9 years). The ?Long
Text? search facet omits search results which do
not contain at least one continuous paragraph of
text that exceeds 300 words. The two facets can be
combined with each other.
When clicking on a search result, ChatNoir does
not link into the real web but redirects into the
ClueWeb. Though the ClueWeb provides the orig-
inal URLs from which the web pages have been
obtained, many of these pages have gone or been
updated since. We hence set up an API that serves
web pages from the ClueWeb on demand: when
accessing a web page, it is pre-processed before
being shipped, removing automatic referrers and
replacing all links to the real web with links to
their counterpart inside the ClueWeb. This way,
the ClueWeb can be browsed as if surfing the real
web, whereas it becomes possible to track a user.
The ClueWeb is stored in the HDFS of our 40 node
Hadoop cluster, and web pages are fetched directly
from there with latencies of about 200ms. Chat-
Noir?s inverted index has been optimized to guaran-
tee fast response times, and it is deployed alongside
Hadoop on the same cluster.
Table 1: Demographics of the 12 Batch 2 writers.
Writer Demographics
Age Gender Native language(s)
Minimum 24 Female 67% English 67%
Median 37 Male 33% Filipino 25%
Maximum 65 Hindi 17%
Academic degree Country of origin Second language(s)
Postgraduate 41% UK 25% English 33%
Undergraduate 25% Philippines 25% French 17%
None 17% USA 17% Afrikaans, Dutch,
n/a 17% India 17% German, Spanish,
Australia 8% Swedish each 8%
South Africa 8% None 8%
Years of writing Search engines used Search frequency
Minimum 2 Google 92% Daily 83%
Median 8 Bing 33% Weekly 8%
Standard dev. 6 Yahoo 25% n/a 8%
Maximum 20 Others 8%
2.3 Two Batches of Writing
In order to not rely only on the retrieval model
implemented in our controlled web search envi-
ronment, we divided the task into two batches, so
that two essays had to be written for each of the
150 topics, namely one in each batch. In Batch 1,
our writers did not search for sources themselves,
but they were provided up front with an average
of 20 search results to choose from for each topic.
These results were obtained from the TREC Web
Track relevance judgments (so-called ?qrels?): only
documents that were found to be relevant or key
documents for a given topic by manual inspection
of the NIST assessors were provided to our writ-
ers. These documents result from the combined
wisdom of all retrieval models of the TREC Web
Tracks 2009?2011, and hence can be considered
as optimum retrieval results produced by the state
of the art in search engine technology. In Batch 2,
in order to obtain realistic search interaction logs,
our writers were instructed to search for source
documents using ChatNoir.
2.4 Crowdsourcing Writers
Our ideal writer has experience in writing, is ca-
pable of writing about a diversity of topics, can
complete a text in a timely manner, possesses de-
cent English writing skills, and is well-versed in
using the aforementioned technologies. After boot-
strapping our setup with 10 volunteers recruited at
our university, it became clear that, because of the
workload involved, accomplishing our goals would
not be possible with volunteers only. Therefore, we
resorted to hiring (semi-)professional writers and
made use of the crowdsourcing platform oDesk.7
Crowdsourcing has quickly become one of the
7http://www.odesk.com
1215
Table 2: Key figures of the Webis text reuse corpus 2012.
Corpus Distribution Total
characteristic min avg max stdev
Writers (Batch 1+2) 27
Essays (Topics) (Two essays per topic) 297 (150)
Essays / Writer 1 2 66 15.9
Queries (Batch 2) 13 655
Queries / Essay 4 91.0 616 83.1
Clicks (Batch 2) 16 739
Clicks / Essay 12 111.6 443 80.3
Clicks / Query 1 2.3 76 3.3
Irrelevant (Batch 2) 5 962
Irrelevant / Essay 1 39.8 182 28.7
Irrelevant / Query 0 0.5 60 1.4
Relevant (Batch 2) 251
Relevant / Essay 0 1.7 7 1.5
Relevant / Query 0 0.0 4 0.2
Key (Batch 2) 1 937
Key / Essay 1 12.9 46 7.5
Key / Query 0 0.2 22 0.7
Corpus Distribution Total
characteristic min avg max stdev
Search Sessions (Batch 2) 931
Sessions / Essay 1 12.3 149 18.9
Days (Batch 2) 201
Days / Essay 1 4.9 17 2.7
Hours (Batch 2) 2 068
Hours / Writer 3 129.3 679 167.3
Hours / Essay 3 7.5 10 2.5
Edits (Batch 1+2) 633 334
Edits / Essay 45 2 132.4 6 975 1 444.9
Edits / Day 5 2 959.5 8 653 1 762.5
Words (Batch 1+2) 1 704 354
Words / Essay 260 5 738.8 15 851 1 604.3
Words / Writer 2 078 63 124.2 373 975 89 246.7
Sources (Batch 1+2) 4 582
Sources / Essay 0 15.4 69 10.0
Sources / Writer 5 169.7 1 065 269.6
cornerstones for constructing evaluation corpora,
which is especially true for paid crowdsourcing.
Compared to Amazon?s Mechanical Turk (Barr
and Cabrera, 2006), which is used more frequently
than oDesk, there are virtually no workers at oDesk
submitting fake results because of its advanced rat-
ing features for workers and employers. Moreover,
oDesk tracks their workers by randomly taking
screenshots, which are provided to employers in or-
der to check whether the hours logged correspond
to work-related activity. This allowed us to check
whether our writers used our environment instead
of other search engines and editors.
During Batch 2, we have conducted a survey
among the twelve writers who worked for us at
that time. Table 1 gives an overview of the demo-
graphics of these writers, based on a questionnaire
and their resumes at oDesk. Most of them come
from an English-speaking country, and almost all
of them speak more than one language, which sug-
gests a reasonably good education. Two thirds of
the writers are female, and all of them have years
of writing experience. Hourly wages were negoti-
ated individually and range from 3 to 34 US dollars
(dependent on skill and country of residence), with
an average of about 12 US dollars. For ethical rea-
sons, we payed at least the minimum wage of the
respective countries involved. In total, we spent
20 468 US dollars to pay the writers?an amount
that may be considered large compared to other
scientific crowdsourcing efforts from the literature,
but small in terms of the potential of crowdsourcing
to make a difference in empirical science.
3 Corpus Analysis
This section presents selected results of a prelim-
inary corpus analysis. We overview the data and
shed some light onto the search and writing behav-
ior of writers.
3.1 Corpus Statistics
Table 2 shows key figures of the collected inter-
action logs, including the absolute numbers of
queries, relevance judgments, working times, num-
ber of edits, words, and retrieved sources, as well
as their relation to essays, writers, and work time,
where applicable. On average, each writer wrote
2 essays while the standard deviation is 15.9, since
one very prolific writer managed to write 66 essays.
From a total of 13 655 queries submitted by the
writers within Batch 2, each essay got an aver-
age of 91 queries. The average number of results
clicked per query is 2.3. For comparison, we com-
puted the average number of clicks per query in
the AOL query log (Pass et al, 2006), which is 2.0.
In this regard, the behavior of our writers on indi-
vidual queries does not differ much from that of
the average AOL user in 2006. Most of the clicks
that we recorded are search result clicks, whereas
2 457 of them are browsing clicks on web page
links. Among the browsing clicks, 11.3% are clicks
on links that point to the same web page (i.e., an-
chor links using the hash part of a URL). The
longest click trail contains 51 unique web pages,
but most trails are very short. This is a surprising
result, since we expected a larger proportion of
browsing clicks, but it also shows that our writers
1216
relied heavily on the ChatNoir?s ranking. Regard-
ing search facets, we observed that our writers used
them only for about 7% of their queries. In these
cases, the writers used either the ?Long Text? facet,
which retrieves web pages containing at least one
continuous passage of at least 300 words, or set the
desired reading level to ?Expert.?
The query log of each writer in Batch 2 divides
into 931 search sessions with an average of 12.3 ses-
sions per topic. Here, a session is defined as a se-
quence of queries recorded for a given topic which
is not divided by a break longer than 30 minutes.
Despite other claims in the literature (Jones and
Klinkner, 2008; Hagen et al, 2013) we argue that,
in our case, sessions can be reliably identified by
timeouts because we have a priori knowledge about
which query belongs to which essay. Typically,
completing an essay took 4.9 days, which includes
to a long-lasting exploration of the topic at hand.
The 297 essays submitted within the two batches
were written with a total of 633 334 edits. Each
topic was edited 2 132 times on average, whereas
the standard deviation gives an idea about how
diverse the modifications of the reused text were.
Writers were not specifically instructed to modify a
text as much as possible?rather they were encour-
aged to paraphrase in order to foreclose the detec-
tion by an automatic text reuse detector. This way,
our corpus captures each writer?s idea of the nec-
essary modification effort to accomplish this goal.
The average lengths of the essays is 5 739 words,
but there are also some short essays if hardly any
useful information could be found on the respective
topics. About 15 sources have been reused in each
essay, whereas some writers reused text from as
many as 69 unique documents.
3.2 Relevance Judgments
In the essays from Batch 2, writers reused texts
from web pages they found during their search.
This forms an interesting relevance signal which
allows us to separate web pages relevant to a given
topic from those which are irrelevant. Following
the terminology of TREC, we consider web pages
from which text is reused as key documents for
the respective essay?s topic, while web pages that
are on a click trail leading to a key document are
termed relevant. The unusually high number of
key documents compared to relevant documents
is explained by the fact that there are only few
click trails of this kind, whereas most web pages
Table 3: Confusion matrix of TREC judgments
versus writer judgments.
TREC Writer judgment
judgment irrelevant relevant key unjudged
spam (-2) 3 0 1 2 446
spam (-1) 64 4 18 16 657
irrelevant (0) 219 13 73 33 567
relevant (1) 114 8 91 10 676
relevant (2) 44 5 56 3 711
key (3) 12 0 8 526
unjudged 5 506 221 1 690 ?
have been retrieved directly. The remainder of web
pages that were viewed but discarded by our writers
are considered as irrelevant.
Each year, the NIST assessors employed for the
TREC conference manually review hundreds of
web pages that have been retrieved by experimental
retrieval systems that are submitted to the various
TREC tracks. This was also the case for the TREC
Web Tracks from which the topics of our corpus
are derived. We have compared the relevance judg-
ments provided by TREC for these tracks with the
implicit judgments from our writers. Table 3 con-
trasts the two judgment scales in the form of a con-
fusion matrix. TREC uses a six-point Likert scale
ranging from -2 (extreme Spam) to 3 (key docu-
ment). For 733 of the documents visited by our
writers, TREC relevance judgments can be found.
From these, 456 documents (62%) have been con-
sidered irrelevant for the purposes of reuse by our
writers, however, the TREC assessor disagree with
this judgment in 170 cases. Regarding the docu-
ments considered as key documents for reuse by
our writers, the TREC assessors disagree on 92 of
the 247 documents. An explanation for the dis-
agreement can be found in the differences between
the TREC ad hoc search task and our text reuse
task: the information nuggets (small chunks of
text) that satisfy specific factual information needs
from the original TREC topics are not the same as
the information ?ingots? (big chunks of text) that
satisfy our writers? needs.
3.3 Research Behavior
To analyze the writers? search behavior during es-
say writing in Batch 2, we have recorded detailed
search logs of their queries while they used our
search engine. Figure 2 shows for each of the
150 essays of this batch a curve of the percentage
of queries at times between a writer?s first query
and an essay?s completion. We have normalized
the time axis and excluded working breaks of more
1217
1653320161582105870113231811928271962334710924840148153113154319
642630182083524341142844652605248669750138364234703457
1206167410162326910641362810898474610555088481989421848198
11276201471701395610632370607410451423011116944150274489215599
241588418140135461181851429133611723782466803368121626076
6210822424275691814720830241731616522636960864427464
A
F
E
D
C
B
1 5 10 15 20 25
Figure 2: Spectrum of writer search behavior. Each grid cell corresponds to one of the 150 essays of
Batch 2 and shows a curve of the percentage of submitted queries (y-axis) at times between the first query
until the essay was finished (x-axis). The numbers denote the amount of queries submitted. The cells are
sorted by area under the curve, from the smallest area in cell A1 to the largest area in cell F25.
than five minutes. The curves are organized so as
to highlight the spectrum of different search behav-
iors we have observed: in row A, 70-90% of the
queries are submitted toward the end of the writ-
ing task, whereas in row F almost all queries are
submitted at the beginning. In between, however,
sets of queries are often submitted in the form of
?bursts,? followed by extended periods of writing,
which can be inferred from the steps in the curves
(e.g., cell C12). Only in some cases (e.g., cell C10)
a linear increase of queries over time can be ob-
served for a non-trivial amount of queries, which
indicates continuous switching between searching
and writing. From these observations, it can be
inferred that our writers sometimes conducted a
?first fit? search and reused the first texts they found
easily. However, as the essay progressed and the
low hanging fruit in terms of search were used up,
they had to search more intensively in order to com-
plete their essay. More generally, this data gives
an idea of how humans perform exploratory search
in order to learn about a given topic. Our current
research on this aspect focuses on the prediction
of search mission types, since we observe that the
search mission type does not simply depend on the
writer or the perceived topic difficulty.
3.4 Visualizing Edit Histories
To analyze the writers? writing style, that is to
say, how writers reuse texts and how the essay
is completed in both batches, we have recorded
the edit logs of their essays. Whenever a writer
stopped writing for more than 300ms, a new edit
was stored in a version control system at our site.
The edit logs document the entire text evolution,
from first the keystroke until an essay was com-
pleted. We have used the so-called history flow
visualization to analyze the writing process (Vi?-
gas et al, 2004). Figure 3 shows four examples
from the set of 297 essays. Based on these visu-
alizations, a number of observations can be made.
In general, we identify two distinct writing-style
types to perform text reuse, namely to build up an
essay during writing, or, to first gather material and
then to boil down a text until the essay is completed.
Later in this section, we will analyze this observa-
tion in greater detail. Within the plots, a number
of events can be spotted that occurred during writ-
ing: in the top left plot, encircled as area A, the
insertion of a new piece of text can be observed.
Though marked as original text at first, the writer
worked on this passage and then revealed that it
was reused from another source. At area B in the
top right plot, one can observe the reorganization of
two passages as they exchange places from one edit
to another. Area C in the bottom right plot shows
that the writer, shortly before completing this essay,
reorganized substantial parts. Area D in the same
plot shows how the writer went about boiling down
the text by incorporating contents from different
passages that have been collected beforehand and,
then, from one edit to another, discarded most of
the rest. The saw-tooth shaped pattern in area E
in the bottom left plot reveals that, even though
the writer of this essay adopts a build-up style, she
still pastes passages from her sources into the text
one at a time, and then individually boils down
each. Our visualizations also include information
about the text positions where writers have been
working at a given point in time; these positions
are shown as blue dots in the plots. In this regard
distinct writing patterns are discernible of writers
who go through a text linearly versus those who do
not. Future work will include an analysis of these
writing patterns.
1218
AB
C
D
E
Figure 3: Types of text reuse: build-up reuse (left) versus boil-down reuse (right). Each plot shows the text
length at text edit between first keystroke and essay completion; edits have been recorded during writing
whenever a writer stopped for more than 300ms. Colors encode different source documents. Original text
is white; blue dots indicate the text position of the writer?s last edit.
3.5 Build-up Reuse versus Boil-down Reuse
Based on the edit history visualizations, we have
manually classified the 297 essays of both batches
into two categories, corresponding to the two styles
build-up reuse and boil-down reuse. We found
that 40% are instances of build-up reuse, 45% are
instances of boil-down reuse, and 13% fall in be-
tween, excluding 2% of the essays as outliers due
to errors or for being too short. The in-between
cases show that a writer actually started one way
and then switched to the respective other style of
reuse so that the resulting essays could not be at-
tributed to a single category. An important question
that arises out of this observation is whether differ-
ent writers habitually exert different reuse styles
or whether they apply them at random. To obtain
a better overview, we envision the applied reuse
style of an essay by the skyline curve of its edit
history visualization (i.e., by the curve that plots
the length of an essay after each edit). Aggregating
these curves on a per-writer basis reveals distinct
Table 4: Contingency table: writers over reuse style.
Reuse Writer ID
Style A02A05A06A07A10A17A18A19A20A21A24
build-up 4 27 11 4 9 13 12 4 9 18 2
boil-down 52 5 0 14 2 13 11 3 0 0 24
mixed 10 3 0 1 1 7 6 0 0 3 1
patterns. For eight of our writers Figure 4 shows
this characteristic. The plots are ordered by the
shape of the averaged curve, starting from a linear
increase (left) to a compound of steep increase to
a certain length after which the curve levels out
(right). The former shape corresponds to writers
who typically apply build-up reuse, while the lat-
ter can be attributed to writers who typically apply
boil-down reuse.
When comparing the plots we notice a very in-
teresting effect: it appears that writers who conduct
boil-down reuse vary more wildly in their behavior.
The reuse style of some writers, however, falls in
between the two extremes. Besides the visual anal-
ysis, Table 4 shows the distribution of reuse styles
1219
Te
x
t l
en
gt
h 
(%)
Te
x
t l
en
gt
h 
(%)
A10 (12 essays) A18 (32 essays) A24 (27 essays)A21 (21 essays)
A06 (12 essays) A17 (33 essays) A02 (66 essays)A05 (37 essays)
Edits (%)Edits (%) Edits (%)Edits (%)
build up boil down
Text reuse style
Figure 4: Text reuse styles ranging from build-up reuse (left) to boil-down reuse (right). A gray curve
shows the normalized length of an essay over the edits that went into it during writing. Curves are grouped
by writers. The black curve marks the average of all other curves in a plot.
for the eleven writers who contributed at least five
essays. Most writers use one style for about 80%
of their essays, whereas two writers (A17, A18) are
exactly on par between the two styles. Based on
Pearson?s chi-squared test, one can safely reject the
null hypothesis that writers and text reuse styles
are independent: ?2 = 139.0 with p = 7.8 ? 10?20.
Since our sample of authors and essays is sparse,
Pearson?s chi-squared test may not be perfectly
suited which is why we have also applied Fisher?s
exact test, which computes probability p = 0.0005
that the null hypothesis is true.
4 Summary and Outlook
This paper details the construction of the Webis text
reuse corpus 2012 (Webis-TRC-12), a new corpus
for text reuse research that has been created en-
tirely manually on a large scale. We have recorded
consistent interaction logs of human writers with a
search engine as well as with the used text proces-
sor; these logs serve the purpose of studying how
texts from the web are being reused for essay writ-
ing. Our setup is entirely reproducible: we have
built a static web search environment consisting of
a search engine along with a means to browse a
large corpus of web pages as if it were the ?real?
web. Yet, in terms of scale, this environment is rep-
resentative of the real web. Besides our corpus also
this infrastructure is available to other researchers.
The corpus itself goes beyond existing resources in
that it allows for a much more fine-grained analysis
of text reuse, and in that it significantly improves
the realism of the data underlying evaluations of
automatic tools to detect text reuse and plagiarism.
Our analysis gives an overview of selected as-
pects of the new corpus. This includes corpus
statistics about important variables, but also ex-
ploratory studies of search behaviors and strategies
for reusing text. We present new insights about how
text is composed, revealing two types of writers:
those who build up a text as they go, and those who
first collect a lot of material which then is boiled
down until the essay is finished.
Parts of our corpus have been successfully em-
ployed to evaluate plagiarism detectors in the
PAN plagiarism detection competition 2012 (Pot-
thast et al, 2012a). Future work will include analy-
ses that may help to understand the state of mind of
writers when reusing text as well as of plagiarists.
We also expect insights with regard to the develop-
ment of algorithms for detection purposes and for
linguists studying the process of writing.
Acknowledgements
We thank our writers at oDesk and all volunteers
for their contribution. We also thank Jan Gra?egger
and Martin Tippmann who kept the search engine
up and running during corpus construction.
1220
References
Jeff Barr and Luis Felipe Cabrera. 2006. AI gets a
brain. Queue, 4(4):24?29.
Paul Clough and Mark Stevenson. 2011. Develop-
ing a corpus of plagiarised short answers. Language
Resources and Evaluation, 45:5?24.
Paul Clough, Robert Gaizauskas, Scott S. L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt Reuse.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2002),
Philadelphia, PA, USA, July 6?12, 2002, pages 152?
159.
Gordon V. Cormack, Mark D. Smucker, and Charles L.
A. Clarke. 2011. Efficient and effective spam filtering
and re-ranking for large web datasets. Information
Retrieval, 14(5):441?465.
Tamer Elsayed, Jimmy J. Lin, and Donald Metzler.
2011. When close enough is good enough: approxi-
mate positional indexes for efficient ranked retrieval.
In Proceedings of the 20th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2011),
Glasgow, United Kingdom, October 24?28, 2011,
pages 1993?1996.
Matthias Hagen, Jakob Gomoll, Anna Beyer, and
Benno Stein. 2013. From Search Session Detection to
Search Mission Detection. In Proceedings of the 10th
International Conference Open Research Areas in In-
formation Retrieval (OAIR 2013), Lisbon, Portugal,
May 22?24, 2013, to appear.
Djoerd Hiemstra and Claudia Hauff. 2010. MIREX:
MapReduce information retrieval experiments. Tech-
nical Report TR-CTIT-10-15, University of Twente.
Rosie Jones and Kristina Lisa Klinkner. 2008. Be-
yond the session timeout: automatic hierarchical seg-
mentation of search topics in query logs. In Proceed-
ings of the 17th ACM Conference on Information and
Knowledge Management (CIKM 2008), Napa Valley,
California, USA, October 26?30, 2008, pages 699?
708.
J. Peter Kincaid, Robert P. Fishburne, Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation of
new readability formulas (automated readability index,
Fog count and Flesch reading ease formula) for Navy
enlisted personnel. Research Branch Report 8-75,
Naval Air Station Memphis, Millington, TN.
Katrin K?hler and Debora Weber-Wulff. 2010. Pla-
giarism detection test 2010. http://plagiat.
htw-berlin.de/wp-content/uploads/
PlagiarismDetectionTest2010-final.pdf.
Greg Pass, Abdur Chowdhury, and Cayley Torgeson.
2006. A picture of search. In Proceedings of the 1st
International Conference on Scalable Information
Systems (Infoscale 2006), Hong Kong, May 30?June 1,
2006, paper 1.
Martin Potthast, Benno Stein, Andreas Eiselt, Alberto
Barr?n-Cede?o, and Paolo Rosso. 2009. Overview
of the 1st international competition on plagiarism
detection. In SEPLN 2009 Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2009), pages 1?9.
Martin Potthast, Alberto Barr?n-Cede?o, Andreas
Eiselt, Benno Stein, and Paolo Rosso. 2010a.
Overview of the 2nd international competition on
plagiarism detection. In Working Notes Papers of the
CLEF 2010 Evaluation Labs.
Martin Potthast, Benno Stein, Alberto Barr?n-Cede?o,
and Paolo Rosso. 2010b. An evaluation framework
for plagiarism detection. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING 2010), Beijing, China, August 23?27,
2010, pages 997?1005.
Martin Potthast, Andreas Eiselt, Alberto Barr?n-
Cede?o, Benno Stein, and Paolo Rosso. 2011.
Overview of the 3rd international competition on pla-
giarism detection. In Working Notes Papers of the
CLEF 2011 Evaluation Labs.
Martin Potthast, Tim Gollub, Matthias Hagen, Jan
Gra?egger, Johannes Kiesel, Maximilian Michel,
Arnd Oberl?nder, Martin Tippmann, Alberto Barr?n-
Cede?o, Parth Gupta, Paolo Rosso, and Benno Stein.
2012a. Overview of the 4th international competition
on plagiarism detection. In Working Notes Papers of
the CLEF 2012 Evaluation Labs.
Martin Potthast, Matthias Hagen, Benno Stein, Jan
Gra?egger, Maximilian Michel, Martin Tippmann, and
Clement Welsch. 2012b. ChatNoir: a search engine
for the ClueWeb09 corpus. In Proceedings of the
35th International ACM Conference on Research and
Development in Information Retrieval (SIGIR 2012),
Portland, OR, USA, August 12?16, 2012, page 1004.
Martin Potthast. 2011. Technologies for Reusing
Text from the Web. Dissertation, Bauhaus-Universit?t
Weimar.
Stephen E. Robertson, Hugo Zaragoza, and Michael J.
Taylor. 2004. Simple BM25 extension to multiple
weighted fields. In Proceedings of the 13th ACM Con-
ference on Information and Knowledge Management
(CIKM 2004), Washington, DC, USA, November 8?13,
2004, pages 42?49.
Fernanda B. Vi?gas, Martin Wattenberg, and Kushal
Dave. 2004. Studying cooperation and conflict be-
tween authors with history flow visualizations. In Pro-
ceedings of the 2004 Conference on Human Factors
in Computing Systems (CHI 2004), Vienna, Austria,
April 24?29, 2004, pages 575?582.
1221
