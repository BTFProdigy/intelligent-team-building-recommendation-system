Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799?807,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
MINT: A Method for Effective and Scalable Mining of  
Named Entity Transliterations from Large Comparable Corpora 
Raghavendra Udupa         K Saravanan         A Kumaran        Jagadeesh Jagarlamudi*          
Microsoft Research India 
Bangalore 560080 INDIA 
 [raghavu,v-sarak,kumarana,jags}@microsoft.com 
 
Abstract 
In this paper, we address the problem of min-
ing transliterations of Named Entities (NEs) 
from large comparable corpora. We leverage 
the empirical fact that multilingual news ar-
ticles with similar news content are rich in 
Named Entity Transliteration Equivalents 
(NETEs). Our mining algorithm, MINT, uses 
a cross-language document similarity model to 
align multilingual news articles and then 
mines NETEs from the aligned articles using a 
transliteration similarity model. We show that 
our approach is highly effective on 6 different 
comparable corpora between English and 4 
languages from 3 different language families. 
Furthermore, it performs substantially better 
than a state-of-the-art competitor.   
1 Introduction 
Named Entities (NEs) play a critical role in many 
Natural Language Processing and Information 
Retrieval (IR) tasks.  In Cross-Language Infor-
mation Retrieval (CLIR) systems, they play an 
even more important role as the accuracy of their 
transliterations is shown to correlate highly with 
the performance of the CLIR systems (Mandl 
and Womser-Hacker, 2005, Xu and Weischedel, 
2005).  Traditional methods for transliterations 
have not proven to be very effective in CLIR. 
Machine Transliteration systems (AbdulJaleel 
and Larkey, 2003; Al-Onaizan and Knight, 2002; 
Virga and Khudanpur, 2003) usually produce 
incorrect transliterations and translation lexcions 
such as hand-crafted or statistical dictionaries are 
too static to have good coverage of NEs1 occur-
ring in the current news events. Hence, there is a 
critical need for creating and continually updat-
                                                 
* Currently with University of Utah. 
1 New NEs are introduced to the vocabulary of a lan-
guage every day. On an average, 260 and 452 new 
NEs appeared daily in the XIE and AFE segments of 
the LDC English Gigaword corpora respectively. 
ing multilingual Named Entity transliteration 
lexicons. 
The ubiquitous availability of comparable 
news corpora in multiple languages suggests a 
promising alternative to Machine Transliteration, 
namely, the mining of Named Entity Translitera-
tion Equivalents (NETEs) from such corpora. 
News stories are typically rich in NEs and there-
fore, comparable news corpora can be expected 
to contain NETEs (Klementiev and Roth, 2006; 
Tao et al, 2006). The large quantity and the per-
petual availability of news corpora in many of 
the world?s languages, make mining of NETEs a 
viable alternative to traditional approaches. It is 
this opportunity that we address in our work. 
    In this paper, we detail an effective and scala-
ble mining method, called MINT (MIning 
Named-entity Transliteration equivalents), for 
mining of NETEs from large comparable corpo-
ra. MINT addresses several challenges in mining 
NETEs from large comparable corpora: exhaus-
tiveness (in mining sparse NETEs), computa-
tional efficiency (in scaling on corpora size), 
language independence (in being applicable to 
many language pairs) and linguistic frugality (in 
requiring minimal external linguistic resources).   
Our contributions are as follows: 
? We give empirical evidence for the hypo-
thesis that news articles in different languages 
with reasonably similar content are rich sources 
of NETEs (Udupa, et al, 2008).  
? We demonstrate that the above insight can 
be translated into an effective approach for min-
ing NETEs from large comparable corpora even 
when similar articles are not known a priori. 
? We demonstrate MINT?s effectiveness on 
4 language pairs involving 5 languages (English, 
Hindi, Kannada, Russian, and Tamil) from 3 dif-
ferent language families, and its scalability on 
corpora of vastly different sizes (2,000 to 
200,000 articles).  
? We show that MINT?s performance is sig-
nificantly better than a state of the art method 
(Klementiev and Roth, 2006). 
 
799
We discuss the motivation behind our ap-
proach in Section 2 and present the details in 
Section 3.  In Section 4, we describe the evalua-
tion process and in Section 5, we present the re-
sults and analysis.  We discuss related work in 
Section 6.  
2 Motivation 
MINT is based on the hypothesis that news ar-
ticles in different languages with similar content 
contain highly overlapping set of NEs. News 
articles are typically rich in NEs as news is about 
events involving people, locations, organizations, 
etc2. It is reasonable to expect that multilingual 
news articles reporting the same news event 
mention the same NEs in the respective languag-
es. For instance, consider the English and Hindi 
news reports from the New York Times and the 
BBC on the second oath taking of President Ba-
rack Obama (Figure 1). The articles are not pa-
rallel but discuss the same event. Naturally, they 
mention the same NEs (such as Barack Obama, 
John Roberts, White House) in the respective 
languages, and hence, are rich sources of NETEs.    
Our empirical investigation of comparable 
corpora confirmed the above insight. A study of 
                                                 
2 News articles from the BBC corpus had, on an 
average, 12.9 NEs and new articles from the The 
New Indian Express, about 11.8 NEs. 
 
200 pairs of similar news articles published by 
The New Indian Express in 2007 in English and 
Tamil showed that 87% of the single word NEs 
in the English articles had at least one translitera-
tion equivalent in the conjugate Tamil articles.  
The MINT method leverages this empirically 
backed insight to mine NETEs from such compa-
rable corpora.   
However, there are several challenges to the 
mining process: firstly, vast majority of the NEs 
in comparable corpora are very sparse; our anal-
ysis showed that 80% of the NEs in The New 
Indian Express news corpora appear less than 5 
times in the entire corpora.  Hence, any mining 
method that depends mainly on repeated occur-
rences of the NEs in the corpora is likely to miss 
vast majority of the NETEs.  Secondly, the min-
ing method must restrict the candidate NETEs 
that need to be examined for match to a reasona-
bly small number, not only to minimize false 
positives but also to be computationally efficient.  
Thirdly, the use of linguistic tools and resources 
must be kept to a minimum as resources are 
available only in a handful of languages.  Finally, 
it is important to use as little language-specific 
knowledge as possible in order to make the min-
ing method applicable across a vast majority of 
languages of the world.  The MINT method pro-
posed in this paper addresses all the above is-
sues. 
 
800
3 The MINT Mining Method 
MINT has two stages. In the first stage, for 
every document in the source language side, the 
set of documents in the target language side with 
similar news content are found using a cross-
language document similarity model. In the 
second stage, the NEs in the source language 
side are extracted using a Named Entity Recog-
nizer (NER) and, subsequently, for each NE in a 
source language document, its transliterations are 
mined from the corresponding target language 
documents. We present the details of the two 
stages of MINT in the remainder of this section. 
3.1 Finding Similar Document Pairs  
The first stage of MINT method (Figure 2) works 
on the documents from the comparable corpora 
(CS, CT) in languages S and T and produces a col-
lection AS,T  of similar article pairs (DS, DT).  Each 
article pair (DS, DT) in AS,T consists of an article 
(DS) in language S and an article (DT) in language 
T, that have similar content. The cross-language 
similarity between DS and DT, as measured by the 
cross-language similarity model MD, is at least ? 
> 0. 
 
Cross-language Document Similarity Model: 
The cross-language document similarity model 
measures the degree of similarity between a pair 
of documents in source and target languages.  
We use the negative KL-divergence between 
source and target document probability distribu-
tions as the similarity measure. 
  Given two documents DS, DT in source and tar-
get languages respectively, with 
TS VV , denoting 
the vocabulary of source and target languages, 
the similarity between the two documents is giv-
en by the KL-divergence measure, -KL(DS || DT), 
as: 
?
? TTw ST
TT
ST
V Dwp
DwpDwp )|(
)|(log)|(
  
where p(w | D) is the likelihood of word w in D. 
As we are interested in target documents which 
are similar to a given source document, we can 
ignore the numerator as it is independent of the 
target document.  Finally, expanding p(wT | Ds) 
as 
)|()|( SVw TSS wwpDwpSS??
we specify the 
cross-language similarity score as follows: 
 
Cross-language similarity =       
)|(log)|()|( TTSTw w SS DwpwwpDwpTVT SVS? ?? ?
 
 
3.2 Mining NETEs from Document Pairs  
The second stage of the MINT method works on 
each pair of articles (DS, DT) in the collection AS,T  
and produces a set PS,T of NETEs. Each pair (?S, 
?T) in PS,T  consists of an NE ?S in language S, and 
a token ?T in language T, that are transliteration 
equivalents of each other.  Furthermore, the 
transliteration similarity between ?S and ?T, as 
measured by the transliteration similarity model 
MT, is at least ? > 0. Figure 3 outlines this algo-
rithm.  
 
Discriminative Transliteration Similarity 
Model:  
The transliteration similarity model MT measures 
the degree of transliteration equivalence between 
a source language and a target language term.  
Input: Comparable news corpora (CS, CT) in languages (S,T)  
           Crosslanguage Document Similarity Model MD for (S, T) 
           Threshold score ?. 
Output: Set AS,T of pairs of similar articles (DS, DT) from (CS, CT). 
1 AS,T  ? ? ;         // Set of Similar articles (DS, DT) 
2 for each article DS in CS do 
3     XS   ? ? ;       // Set of candidates for DS. 
4      for each article dT  in CT  do 
5         score = CrossLanguageDocumentSimilarity(DS,dT,MD); 
6         if (score ? ?) then XS  ? XS  ? (dT , score) ; 
7      end 
8     DT  = BestScoringCandidate(XS); 
9    if (DT  ? ?) then AS,T  ? AS,T  ? (DS, DT) ; 
10 end 
CrossLanguageSimilarDocumentPairs 
Figure 2. Stage 1 of MINT 
Input:  
      Set AS,T  of similar documents (DS, DT)  in languages  
(S,T),   
      Transliteration Similarity Model MT for (S, T),  
      Threshold score ?. 
Output: Set PS,T  of NETEs (?S, ?T) from  AS,T ; 
1   PS,T  ? ? ;  
2   for each pair of articles (DS, DT) in AS,T  do 
3        for each named entity ?S in DS do  
4            YS ? ? ; // Set of candidates for ?S. 
5            for each candidate eT  in DT  do 
6                 score = TransliterationSimilarity(?S, eT, MT) ; 
7                 if (score ? ?)  then   YS  ?  YS ? (eT , score) ; 
8            end 
9            ?T  = BestScoringCandidate(YS) ;  
10          if (?T  ? null) then PS,T  ?  PS,T  ? (?S, ?T) ; 
11      end 
12 end 
TransliterationEquivalents 
Figure 3. Stage 2 of MINT 
801
We employ a logistic function as our translitera-
tion similarity model MT, as follows: 
 
 TransliterationSimilarity (?S,eT,MT) = 
),( TS1
1
ewte ?????
 
where ? (?S, eT) is the feature vector for the pair 
(?S, eT) and w is the weights vector.  Note that the 
transliteration similarity takes a value in the 
range [0..1]. The weights vector w is learnt dis-
criminatively over a training corpus of known 
transliteration equivalents in the given pair of 
languages. 
 
Features: The features employed by the model 
capture interesting cross-language associations 
observed in (?S, eT): 
 
? All unigrams and bigrams from the 
source and target language strings. 
? Pairs of source string n-grams and target 
string n-grams such that difference in the 
start positions of the source and target n-
grams is at most 2. Here n ? ?2,1? . 
? Difference in the lengths of the two 
strings.  
 
Generative Transliteration Similarity Model: 
We also experimented with an extension of He?s 
W-HMM model (He, 2007). The transition prob-
ability depends on both the jump width and the 
previous source character as in the W-HMM 
model. The emission probability depends on the 
current source character and the previous target 
character unlike the W-HMM model (Udupa et 
al., 2009). Instead of using any single alignment 
of characters in the pair (wS, wT), we marginalize 
over all possible alignments: 
? ? ? ? ? ?11
1
11 ,|,|| 1 ??
?
???? jajajj
A
m
j
nm tstpsaapstP
jj
 
 
Here, 
jt
(and resp. 
is ) denotes the j
th (and resp. 
ith) character in wT (and resp. wS) and maA 1? is 
the hidden alignment between wT and wS where 
jt
is aligned to 
jas
, ,m,j ?1? . We estimate 
the parameters of the model using the EM algo-
rithm. The transliteration similarity score of a 
pair (wS, wT) is log P(wT  | wS) appropriately trans-
formed. 
 
 
4 Experimental Setup 
Our empirical investigation consists of experi-
ments in three data environments, with each en-
vironment providing answer to specific set of 
questions, as listed below: 
 
1. Ideal Environment (IDEAL): Given a collec-
tion AS,T of oracle-aligned article pairs (DS, DT) 
in S and T, how effective is Stage 2 of MINT in 
mining NETE from AS,T? 
2. Near Ideal Environment (NEAR-IDEAL): 
Let AS,T  be a collection of similar article pairs 
(DS, DT) in S and T. Given comparable corpora 
(CS, CT) consisting of only articles from AS,T, but 
without the knowledge of pairings between the 
articles,  
a. How effective is Stage 1 of MINT in re-
covering AS,T  from (CS, CT) ? 
b. What is the effect of Stage 1 on the 
overall effectiveness of MINT? 
3. Real Environment (REAL): Given large 
comparable corpora (CS, CT), how effective is 
MINT, end-to-end? 
 
The IDEAL environment is indeed ideal for 
MINT since every article in the comparable cor-
pora is paired with exactly one similar article in 
the other language and the pairing of articles in 
the comparable corpora is known in advance.  
We want to emphasize here that such corpora are 
indeed available in many domains such as tech-
nical documents and interlinked multilingual 
Wikipedia articles. In the IDEAL environment, 
only Stage 2 of MINT is put to test, as article 
alignments are given.  
In the NEAR-IDEAL data environment, every 
article in the comparable corpora is known to 
have exactly one conjugate article in the other 
language though the pairing itself is not known 
in advance.  In such a setting, MINT needs to 
discover the article pairing before mining NETEs 
and therefore, both stages of MINT are put to 
test.  The best performance possible in this envi-
ronment should ideally be the same as that of 
IDEAL, and any degradation points to the short-
coming of the Stage 1 of MINT.  These two en-
vironments quantify the stage-wise performance 
of the MINT method.    
Finally, in the data environment REAL, we 
test MINT on large comparable corpora, where 
even the existence of a conjugate article in the 
target side for a given article in the source side of 
the comparable corpora is not guaranteed, as in 
802
any normal large multilingual news corpora. In 
this scenario both the stages of MINT are put to 
test.  This is the toughest, and perhaps the typical 
setting in which MINT would be used.  
4.1 Comparable Corpora 
In our experiments, the source language is Eng-
lish whereas the 4 target languages are from 
three different language families (Hindi from the 
Indo-Aryan family, Russian from the Slavic fam-
ily, Kannada and Tamil from the Dravidian fami-
ly). Note that none of the five languages use a 
common script and hence identification of cog-
nates, spelling variations, suffix transformations, 
and other techniques commonly used for closely 
related languages that have a common script are 
not applicable for mining NETEs.  Table 1 sum-
marizes the 6 different comparable corpora that 
were used for the empirical investigation; 4 for 
the IDEAL and NEAR-IDEAL environments (in 
4 language pairs), and 2 for the REAL environ-
ment (in 2 language pairs). 
 
Cor-
pus 
Source -
Target 
Data 
Environ-
ment 
Articles (in 
Thousands) 
Words (in 
Millions) 
Src Tgt Src Tgt 
EK-S 
English- 
Kannada 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.34 
ET-S 
English- 
Tamil 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.32 
ER-S 
English- 
Russian 
IDEAL& 
NEAR-IDEAL 
2.30 2.30 1.03 0.40 
EH-S 
English- 
Hindi 
IDEAL& 
NEAR-IDEAL 
11.9 11.9 3.77 3.57 
EK-L 
English- 
Kannada 
REAL 103.8 111.0 27.5 18.2 
ET-L 
English- 
Tamil 
REAL 103.8 144.3 27.5 19.4 
Table 1: Comparable Corpora 
 
The corpora can be categorized into two sepa-
rate groups, group S (for Small) consisting of 
EK-S, ET-S, ER-S, and EH-S and group L (for 
Large) consisting of EK-L and ET-L. Corpora in 
group S are relatively small in size, and contain 
pairs of articles that have been judged by human 
annotators as similar. Corpora in group L are two 
orders of magnitude larger in size than those in 
group S and contain a large number of articles 
that may not have conjugates in the target side. 
In addition the pairings are unknown even for the 
articles that have conjugates. All comparable 
corpora had publication dates, except EH-S, 
which is known to have been published over the 
same year. 
The EK-S, ET-S, EK-L and ET-L corpora are 
from The New Indian Express news paper, whe-
reas the EH-S corpora are from Web Dunia and 
the ER-S corpora are from BBC/Lenta News 
Agency respectively. 
4.2 Cross-language Similarity Model  
The cross-language document similarity model 
requires a bilingual dictionary in the appropriate 
language pair. Therefore, we generated statistical 
dictionaries for 3 language pairs (from parallel 
corpora of the following sizes: 11K sentence 
pairs in English-Kannada, 54K in English-Hindi, 
and 14K in English-Tamil) using the GIZA++ 
statistical alignment tool (Och et al, 2003), with 
5 iterations each of IBM Model 1 and HMM.  
We did not have access to an English-Russian 
parallel corpus and hence could not generate a 
dictionary for this language pair. Hence, the 
NEAR-IDEAL experiments were not run for the 
English-Russian language pair.   
Although the coverage of the dictionaries was 
low, this turned out to be not a serious issue for 
our cross-language document similarity model as 
it might have for topic based CLIR (Ballesteros 
and Croft, 1998). Unlike CLIR, where the query 
is typically smaller in length compared to the 
documents, in our case we are dealing with news 
articles of comparable size in both source and 
target languages.  
When many translations were available for a 
source word, we considered only the top-4 trans-
lations.  Further, we smoothed the document 
probability distributions with collection frequen-
cy as described in (Ponte and Croft, 1998). 
4.3 Transliteration Similarity Model  
The transliteration similarity models for each of 
the 4 language pairs were produced by learning 
over a training corpus consisting of about 16,000 
single word NETEs, in each pair of languages.  
The training corpus in English-Hindi, English-
Kannada and English-Tamil were hand-crafted 
by professionals, the English-Russian name pairs 
were culled from Wikipedia interwiki links and 
were cleaned heuristically.  Equal number of 
negative samples was used for training the mod-
els. To produce the negative samples, we paired 
each source language NE with a random non-
matching target language NE.  No language spe-
cific features were used and the same feature set 
was used in each of the 4 language pairs making 
MINT language neutral.   
In all the experiments, our source side lan-
guage is English, and the Stanford Named Entity 
Recognizer (Finkel et al 2005) was used to ex-
tract NEs from the source side article.  It should 
be noted here that while the precision of the NER 
803
used was consistently high, its recall was low, 
(~40%) especially in the New Indian Express 
corpus, perhaps due to the differences in the data 
used for training the NER and the data on which 
we used it.   
4.4 Performance Measures  
Our intention is to measure the effectiveness of 
MINT by comparing its performance with the 
oracular (human annotator) performance.  As 
transliteration equivalents must exist in the 
paired articles to be found by MINT, we focus 
only on those NEs that actually have at least one 
transliteration equivalent in the conjugate article. 
Three performance measures are of interest to 
us: the fraction of distinct NEs from source lan-
guage for which we found at least one translitera-
tion in the target side (Recall on distinct NEs), 
the fraction of distinct NETEs (Recall on distinct 
NETEs) and the Mean Reciprocal Rank (MRR) 
of the NETEs mined.  Since we are interested in 
mining not only the highly frequent but also the 
infrequent NETEs, recall metrics measure how 
effective our method is in mining NETEs ex-
haustively. The MRR score indicates how effec-
tive our method is in preferring the correct ones 
among candidates. 
To measure the performance of MINT, we 
created a test bed for each of the language pairs. 
The test beds are summarized in Table 2.  
The test beds consist of pairs of similar ar-
ticles in each of the language pairs. It should be 
noted here that as transliteration equivalents must 
exist in the paired articles to be found by MINT, 
we focus only on those NEs that actually have at 
least one transliteration equivalent in the conju-
gate article. 
5 Results & Analysis 
In this section, we present qualitative and quan-
titative performance of the MINT algorithm, in 
mining NETEs from comparable news corpora. 
All the results in Sections 5.1 to 5.3 were ob-
tained using the discriminative transliteration 
similarity model described in Section 3.2. The 
results using the generative transliteration simi-
larity model are discussed in Section 5.4. 
5.1 IDEAL Environment 
Our first set of experiments investigated the ef-
fectiveness of Stage 2 of MINT, namely the min-
ing of NETEs in an IDEAL environment. As 
MINT is provided with paired articles in this ex-
periment, all experiments for this environment 
were run on test beds created from group S cor-
pora (Table 2).  
 
 
Results in the IDEAL Environment:  
The recall measures for distinct NEs and distinct 
NETEs for the IDEAL environment are reported 
in Table 3.  
 
Test 
Bed 
Recall (%) 
Distinct NEs Distinct NETEs 
EK-ST 97.30 95.07 
ET-ST 99.11 98.06 
EH-ST 98.55 98.66 
ER-ST 93.33 85.88 
 Table 3: Recall of MINT in IDEAL 
 
Note that in the first 3 language pairs MINT was 
able to mine a transliteration equivalent for al-
most all the distinct NEs. The performance in 
English-Russian pair was relatively worse, per-
haps due to the noisy training data.   
In order to compare the effectiveness of 
MINT with a state-of-the-art NETE mining ap-
proach, we implemented the time series based 
Co-Ranking algorithm based on (Klementiev and 
Roth, 2006).  
 
Table 4 shows the MRR results in the IDEAL 
environment ? both for MINT and the Co-
Ranking baseline: MINT outperformed Co-
Ranking on all the language pairs, despite not 
using time series similarity in the mining 
process.  The high MRRs (@1 and @5) indicate 
that in almost all the cases, the top-ranked candi-
date is a correct NETE.  Note that Co-Ranking 
could not be run on the EH-ST test bed as the 
articles did not have a date stamp. Co-Ranking is 
crucially dependent on time series and hence re-
quires date stamps for the articles. 
 
Test Bed 
Comparable 
Corpora 
Article 
Pairs 
Distinct 
NEs 
Distinct 
NETEs 
EK-ST EK-S 200 481 710 
ET-ST ET-S 200 449 672 
EH-ST EH-S 200 347 373 
ER-ST ER-S 100 195 347 
Table 2: Test Beds for IDEAL & NEAR-IDEAL 
Test 
Bed 
MRR@1 MRR@5 
MINT CoRanking MINT CoRanking 
EK-ST 0.94 0.26 0.95 0.29 
ET-ST 0.91 0.26 0.94 0.29 
EH-ST 0.93 - 0.95 - 
ER-ST 0.80 0.38 0.85 0.43 
Table 4: MINT & Co-Ranking in IDEAL 
804
5.2 NEAR-IDEAL Environment 
The second set of experiments investigated the 
effectiveness of Stage 1 of MINT on comparable 
corpora that are constituted by pairs of similar 
articles, where the pairing information between 
the articles is with-held.  MINT reconstructed the 
pairings using the cross-language document si-
milarity model and subsequently mined NETEs. 
As in previous experiments, we ran our experi-
ments on test beds described in Section 4.4. 
 
Results in the NEAR-IDEAL Environment: 
There are two parts to this set of experiments. In 
the first part, we investigated the effectiveness of 
the cross-language document similarity model 
described in Section 3.1. Since we know the 
identity of the conjugate article for every article 
in the test bed, and articles can be ranked accord-
ing to the cross-language document similarity 
score, we simply computed the MRR for the 
documents identified in each of the test beds, 
considering only the top-2 results. Further, where 
available, we made use of the publication date of 
articles to restrict the number of target articles 
that are considered in lines 4 and 5 of the MINT 
algorithm in Figure 2.  Table 5 shows the results 
for two date windows ? 3 days and 1 year. 
 
 Test 
Bed 
MRR@1 MRR@2 
3 days 1 year 3 days 1 year 
EK-ST 0.99 0.91 0.99 0.93 
ET-ST 0.96 0.83 0.97 0.87 
EH-ST - 0.81 - 0.82 
Table 5: MRR of Stage 1 in NEAR-IDEAL 
 
Subsequently, the output of the Stage 1 was giv-
en as the input to the Stage 2 of the MINT me-
thod. In Table 6 we report the MRR @1 and @5 
for the second stage, for both time windows (3 
days & 1 year). 
 
It is interesting to compare the results of MINT 
in NEAR-IDEAL data environment (Table 6) 
with MINT?s results in IDEAL environment 
(Table 4). The drop in MRR@1 is small: ~2% 
for EK-ST and ~3% for ET-ST. For EH-ST the 
drop is relatively more (~12%) as may be ex-
pected since the time window (3 days) could not 
be applied for this test bed.  
5.3 REAL Environment 
The third set of experiments investigated the ef-
fectiveness of MINT on large comparable corpo-
ra. We ran the experiments on test beds created 
from group L corpora.   
 
 Test-beds for the REAL Environment: The 
test beds for the REAL environment (Table 7) 
consisted of only English articles since we do not 
know in advance whether these articles have any 
similar articles in the target languages. 
 
 Results in the REAL Environment: In real 
environment, we examined the top 2 articles of 
returned by Stage 1 of MINT, and mined NETEs 
from them. We used a date window of 3 in Stage 
1. Table 8 summarizes the results for the REAL 
environment. 
 
We observe that the performance of MINT is 
impressive, considering the fact that the compa-
rable corpora used in the REAL environment is 
two orders of magnitude larger than those used in 
IDEAL and NEAR-IDEAL environments. This 
implies that MINT is able to effectively mine 
NETEs whenever the Stage 1 algorithm was able 
to find a good conjugate for each of the source 
language articles.  
5.4 Generative Transliteration Similarity 
Model 
We employed the extended W-HMM translitera-
tion similarity model in MINT and used it in the 
IDEAL data environment.  Table 9 shows the 
results. 
Test 
Bed 
MRR@1 MRR@5 
3 days 1 year 3 days 1 year 
EK-ST 0.92 0.87 0.94 0.90 
ET-ST 0.88 0.74 0.91 0.78 
EH-ST - 0.82 - 0.87 
Table 6: MRR of Stage 2 in NEAR-IDEAL 
Test 
Bed 
Comparable 
Corpora 
Articles 
Distinct  
NEs 
EK-LT EK-L 100 306 
ET-LT ET-L 100 228 
Table 7: Test Beds for REAL 
 
Test Bed 
MRR 
@1 @5 
EK-LT 0.86 0.88 
ET-LT 0.82 0.85 
Table 8: MRR of Stage 2 in REAL 
Test Bed 
MRR 
@1 @5 
EK-S 0.85 0.86 
ET-S 0.81 0.82 
EH-S 0.91 0.93 
Table 9:  MRR of Stage 2 in IDEAL using genera-
tive transliteration similarity model 
805
We see that the results for the generative transli-
teration similarity model are good but not as 
good as those for the discriminative translitera-
tion similarity model. As we did not stem either 
the English NEs or the target language words, 
the generative model made more mistakes on 
inflected words compared to the discriminative 
model.   
5.5  Examples of Mined NETEs 
Table 10 gives some examples of the NETEs 
mined from the comparable news corpora.  
 
6  Related Work 
CLIR systems have been studied in several 
works (Ballesteros and Croft, 1998; Kraiij et al 
2003). The limited coverage of dictionaries has 
been recognized as a problem in CLIR and MT 
(Demner-Fushman & Oard, 2002; Mandl & 
Womser-hacker, 2005; Xu &Weischedel, 2005).  
In order to address this problem, different 
kinds of approaches have been taken, from learn-
ing transformation rules from dictionaries and 
applying the rules to find cross-lingual spelling 
variants (Pirkola et al, 2003), to  learning trans-
lation lexicon from monolingual and/or compa-
rable corpora (Fung, 1995; Al-Onaizan and 
Knight, 2002; Koehn and Knight, 2002; Rapp, 
1996). While these works have focused on find-
ing translation equivalents of all class of words, 
we focus specifically on transliteration equiva-
lents of NEs.  (Munteanu and Marcu, 2006; 
Quirk et al, 2007) addresses mining of parallel 
sentences and fragments from nearly parallel 
sentences. In contrast, our approach mines 
NETEs from article pairs that may not even have 
any parallel or nearly parallel sentences.   
NETE discovery from comparable corpora 
using time series and transliteration model was 
proposed in (Klementiev and Roth, 2006), and 
extended for NETE mining for several languages 
in (Saravanan and Kumaran, 2007).  However, 
such methods miss vast majority of the NETEs 
due to their dependency on frequency signatures.   
In addition, (Klementiev and Roth, 2006) may 
not scale for large corpora, as they examine 
every word in the target side as a potential trans-
literation equivalent. NETE mining from compa-
rable corpora using phonetic mappings was pro-
posed in (Tao et al, 2006), but the need for lan-
guage specific knowledge restricts its applicabili-
ty across languages.  We proposed the idea of 
mining NETEs from multilingual articles with 
similar content in (Udupa, et al, 2008). In this 
work, we extend the approach and provide a de-
tailed description of the empirical studies. 
7  Conclusion 
In this paper, we showed that MINT, a simple 
and intuitive technique employing cross-
language document similarity and transliteration 
similarity models, is capable of mining NETEs 
effectively from large comparable news corpora. 
Our three stage empirical investigation showed 
that MINT performed close to optimal on com-
parable corpora consisting of pairs of similar ar-
ticles when the pairings are known in advance. 
MINT induced fairly good pairings and performs 
exceedingly well even when the pairings are not 
known in advance. Further, MINT outperformed 
a state-of-the-art baseline and scaled to large 
comparable corpora.  Finally, we demonstrated 
the language neutrality of MINT, by mining 
NETEs from 4 language pairs (between English 
and one of Russian, Hindi, Kannada or Tamil) 
from 3 vastly different linguistic families. 
As a future work, we plan to use the ex-
tended W-HMM model to get features for the 
discriminative transliteration similarity model. 
We also want to use a combination of the cross-
language document similarity score and the 
transliteration similarity score for scoring the 
NETEs. Finally, we would like to use the mined 
NETEs to improve the performance of the first 
stage of MINT. 
Acknowledgments 
We thank Abhijit Bhole for his help and Chris 
Quirk for valuable comments. 
 
Language 
Pair 
Source NE Transliteration 
English-
Kannada 
Woolmer ??????? 
Kafeel ????? 
Baghdad ???????? 
English-Tamil Lloyd ??????  
Mumbai ?????????? 
Manchester ??????????? 
English-Hindi Vanhanen ??????? 
Trinidad ???????????  
Ibuprofen ?????????? 
English-
Russian 
Kreuzberg ?????????? 
Gaddafi ??????? 
Karadzic ???????? 
Table 10: Examples of Mined NETEs 
806
References 
AbdulJaleel, N. and Larkey, L.S. 2003. Statistical translite-
ration for English-Arabic cross language information re-
trieval. Proceedings of CIKM 2003.  
Al-Onaizan, Y. and Knight, K. 2002. Translating named 
entities using monolingual and bilingual resources. Pro-
ceedings of the 40th Annual Meeting of ACL. 
Ballesteros, L. and Croft, B. 1998. Dictionary Methods for 
Cross-Lingual Information Retrieval. Proceedings of 
DEXA?96.  
Chen, H., et al 1998. Proper Name Translation in Cross-
Language Information Retrieval. Proceedings of the 36th 
Annual Meeting of the ACL. 
Demner-Fushman, D., and Oard, D. W. 2002. The effect of 
bilingual term list size on dictionary-based cross-
language information retrieval. Proceedings of the 36th 
Hawaii International Conference on System Sciences.  
Finkel, J. Trond Grenager, and Christopher Manning. 2005. 
Incorporating Non-local Information into Information 
Extraction Systems by Gibbs Sampling. Proceedings of 
the 43nd Annual Meeting of the ACL. 
Fung, P. 1995. Compiling bilingual lexicon entries from a 
non-parallel English-Chinese corpus. Proceedings of the 
3rd Workshop on Very Large Corpora. 
Fung, P. 1995. A pattern matching method for finding noun 
and proper noun translations from noisy parallel corpora.  
Proceedings of ACL 1995.  
He. X. 2007: Using word dependent transition models in 
HMM based word alignment for statistical machine 
translation. In Proceedings of 2nd ACL Workshop on Sta-
tistical Machine Translation . 
Hermjakob, U., Knight, K., and Daume, H. 2008. Name 
translation in statistical machine translation: knowing 
when to transliterate. Proceedings ACL 2008. 
Klementiev, A. and Roth, D. 2006. Weakly supervised 
named entity transliteration and discovery from multilin-
gual comparable corpora. Proceedings of the 44th Annual 
Meeting of the ACL.  
Knight, K. and Graehl, J. 1998. Machine Transliteration. 
Computational Linguistics.  
Koehn, P. and Knight, K. 2002. Learning a translation lex-
icon from monolingual corpora. Proceedings of Unsu-
pervised Lexical Acquisition. 
Kraiij, W., Nie, J-Y. and  Simard, M. 2003. Emebdding 
Web-based Statistical Translation Models in Cross-
Language Information Retrieval. Computational Linguis-
tics., 29(3):381-419. 
Mandl, T., and Womser-Hacker, C.  2004. How do named 
entities contribute to retrieval effectiveness? Proceedings 
of the 2004 Cross Language Evaluation Forum Cam-
paign 2004. 
Mandl, T., and Womser-Hacker, C.  2005. The Effect of 
named entities on effectiveness in cross-language infor-
mation retrieval evaluation. ACM Symposium on Applied 
Computing.  
Munteanu, D. and Marcu D. 2006. Extracting parallel sub-
sentential fragments from non-parallel corpora. Proceed-
ings of the ACL 2006. 
Och, F. and Ney, H. 2003. A systematic comparison of var-
ious statistical alignment models. Computational Lin-
guistics. 
Pirkola, A., Toivonen, J., Keskustalo, H., Visala, K. and 
Jarvelin, K. 2003. Fuzzy translation of cross-lingual 
spelling variants. Proceedings of SIGIR 2003.  
Ponte, J. M. and Croft, B. 1998. A Language Modeling 
Approach to Information Retrieval. Proceedings of ACM 
SIGIR 1998.  
Quirk, C., Udupa, R. and Menezes, A. 2007. Generative 
models of noisy translations with applications to parallel 
fragments extraction. Proceedings of the 11th MT Sum-
mit. 
Rapp, R. 1996. Automatic identification of word transla-
tions from unrelated English and German corpora. Pro-
ceedings of ACL?99 
Saravanan, K. and Kumaran, A. 2007. Some experiments in 
mining named entity transliteration pairs from compara-
ble corpora. Proceedings of the 2nd International Work-
shop on Cross Lingual Information Access. 
Tao, T., Yoon, S., Fister, A., Sproat, R. and Zhai, C. 2006. 
Unsupervised named entity transliteration using temporal 
and phonetic correlation. Proceedings of EMNLP 2006.  
Udupa, R., Saravanan, K., Kumaran, A. and Jagarlamudi, J.  
2008.  Mining Named Entity Transliteration Equivalents 
from Comparable Corpora. Proceedings of the CIKM 
2008. 
Udupa, R., Saravanan, K., Bakalov, A. and Bhole, A.  2009.  
?They are out there if you know where to look?: Mining 
transliterations of OOV terms in cross-language informa-
tion retrieval. Proceedings of the ECIR 2009. 
Virga, P. and Khudanpur, S. 2003. Transliteration of proper 
names in cross-lingual information retrieval. Proceedings 
of the ACL Workshop on Multilingual and Mixed Lan-
guage Named Entity Recognition.  
Xu, J. and Weischedel, R. 2005. Empirical studies on the 
impact of lexical resources on CLIR performance. In-
formation Processing and Management. 
807
Some Experiments in Mining Named Entity Transliteration Pairs from 
Comparable Corpora 
K Saravanan 
Microsoft Research India 
Bangalore, India 
v-sarak@microsoft.com 
A Kumaran 
Microsoft Research India 
Bangalore, India 
kumarana@microsoft.com 
 
  
Abstract 
Parallel Named Entity pairs are important 
resources in several NLP tasks, such as, 
CLIR and MT systems.  Further, such pairs 
may also be used for training transliteration 
systems, if they are transliterations of each 
other.  In this paper, we profile the perfor-
mance of a mining methodology in mining 
parallel named entity transliteration pairs in  
English and an Indian language, Tamil,   
leveraging linguistic tools in English, and 
article-aligned comparable corpora in  the 
two languages.  We adopt a methodology 
parallel to that of [Klementiev and Roth, 
2006], but we focus instead on mining    
parallel named entity transliteration pairs,   
using a well-trained linear classifier to 
identify transliteration pairs.  We profile 
the performance at several operating para-
meters of our algorithm and present the   
results that show the potential of the       
approach in mining transliterations pairs; in 
addition, we uncover a host of issues that 
need to be resolved, for effective mining of  
parallel named entity transliteration pairs. 
1 Introduction & Motivation 
Parallel Named Entity (NE) pairs are important 
resources in several NLP tasks, from supporting 
Cross-Lingual Information Retrieval (CLIR)     
systems, to improving Machine Translation (MT) 
systems.  In addition, such pairs may also be used 
for developing transliteration systems, if they are 
transliterations of each other.  Transliteration of a 
name, for the purpose of this work, is defined as its 
transcription in a different language, preserving the 
phonetics, perhaps in a different orthography 
[Knight and Graehl, 1997] 1 .  While traditional 
transliteration systems have relied on hand-crafted 
linguistic rules, more recently, statistical machine 
learning techniques have been shown to be effec-
tive in transliteration tasks [Jung et al, 2000] [Ab-
dulJaleel and Larkey, 2003] [Virga  and Kudhan-
pur , 2003] [Haizhou et al, 2004].  However, such 
data-driven approaches require significant amounts 
of training data, namely pairs of names in two dif-
ferent languages, possibly in different orthography, 
referred to as transliteration pairs, which are not 
readily available in many resource-poor languages.  
It is important to note at this point, that NEs are 
found typically in news corpora in any given     
language.  In addition, news articles covering the 
same event in two different languages may reason-
ably be expected to contain the same NEs in the 
respective languages.  The perpetual availability of 
news corpora in the world?s languages, points to 
the promise of  mining transliteration pairs        
endlessly, provided an effective identification of 
such NEs in specific languages and pairing them 
appropriately, could be devised.  
 
Recently, [Klementiev and Roth, 2006] outlined an 
approach by leveraging the availability of article-
aligned news corpora between English and Rus-
sian, and tools in English, for discovering translite-
ration pairs between the two languages, and pro-
gressively refining the discovery process.  In this 
paper, we adopt their basic methodology, but we 
focus on 3 different issues:  
                                                 
1 London rewritten as ?????? in Tamil, or ???? in Arabic (both 
pronounced as London), are considered as transliterations, but 
not the rewriting of New Delhi as ???? ?????? (puthu thilli) in 
Tamil.   
1. mining comparable corpora for NE pairs, leve-
raging a well trained classifier, 
2. calibrating the performance of this mining 
framework, systematically under different pa-
rameters for mining, and,  
3. uncovering further research issues in mining NE 
pairs between English and an Indian language, 
Tamil. 
While our analysis points to a promising approach 
for mining transliteration pairs, it also uncovers 
several issues that may need to be resolved, to 
make this process highly effective. As in [Klemen-
tiev and Roth, 2006] no language specific know-
ledge was used to refine our mining process, mak-
ing the approach broadly applicable. 
2 Transliteration Pairs Discovery 
In this section, we outline briefly the methodology 
presented in [Klementiev and Roth, 2006], and 
refer interested readers to the source for details. 
 
They present a methodology to automatically 
discover parallel NE transliteration pairs between 
English and Russian, leveraging the availability of 
a good-quality Named Entity Recognizer (NER) in 
English, and article-aligned bilingual comparable 
corpora, in English and Russian.  The key idea of 
their approach is to extract all NEs in English, and 
identify a set of potential transliteration pairs in 
Russian for these NEs using a simple classifier 
trained on a small seed corpus, and re-ranking the 
identified pairs using the similarity between the 
frequency distributions of the NEs in the 
comparable corpora.  Once re-ranked, the 
candidate pairs, whose scores are above a threshold 
are used to re-train the classifier, and the process is 
repeated to make the discovery process more 
effective. 
 
To discriminate transliteration pairs from other 
content words, a simple perceptron-based linear 
classifier, which is trained on n-gram features 
extracted from a small seed list of NE pairs, is 
employed leveraging the fact that transliteration 
relies on approximately monotonic alignment 
between the names in two languages.  The 
potential transliteration pairs identified by this 
classifier are subsequently re-ranked using a 
Discrete Fourier Transform based similarity 
metric, computed based on the frequency of words 
of the candidate pair, found in the article-aligned 
comparable corpora.  For the frequency analysis, 
equivalence classes of the words are formed, using 
a common prefix of 5 characters, to account for the 
rich morphology of Russian language.  The 
representative prefix of each of the classes are used 
for classification. 
 
Finally, the high scoring pairs of words are used to 
re-train the perceptron-based linear classifier, to 
improve the quality of the subsequent rounds.  The 
quality of the extracted NE pairs is shown to 
improve, demonstrating viability of such an 
approach for successful discovery of NE pairs 
between English and Russian. 
3 Adoption for Transliteration Pairs 
Mining  
We adopt the basic methodology presented in 
[Klementiev and Roth, 2006], but we focus on 
three specific issues described in the introduction.   
3.1 Mining of Transliteration Pairs  
We start with comparable corpora in English and 
Tamil, similar in size to that used in [Klementiev 
and Roth, 2006], and using the English side of this 
corpora, first, we extract all the NEs that occur 
more than a given threshold parameter, FE, using a 
standard NER tool.  The higher the threshold is, 
the more will be the evidence for legitimate transli-
teration pairs, in the comparable corpora, which 
may be captured by the mining methodology. The 
extracted list of NEs provides the set of NEs in 
English, for which we mine for transliteration pairs 
from the Tamil side of the comparable corpora.   
 
We need to identify all NEs in the Tamil side of 
the corpora, in order to appropriately pair-up with 
English NEs.  However, given that there is no pub-
licly available NER tool in Tamil (as the case may 
be in many resource-poor languages) we start with 
an assumption that all words found in the Tamil 
corpus are potentially NEs.  However, since Tamil 
is a highly morphologically inflected language, the 
same NE may occur in its various inflected forms 
in the Tamil side of the corpora; hence, we collect 
those words with the same prefix (of fixed size) 
into a single bucket, called equivalence class, and 
consider a representative prefix, referred to as sig-
nature of the collection for comparison.  The     
assumption here is that the common prefix would 
stand for a Tamil NE, and all the members of the 
equivalence class are the various inflected forms of 
the NE. We use such a signature to classify a Ta-
mil word as potential transliteration of an English 
word. Again, we consider only those signatures 
that have occurred more than a threshold parame-
ter, FT, in the Tamil side of the comparable corpora, 
in order to strengthen support for a meaningful  
similarity in their frequency of occurrence. 
 
We used a linear Support Vector Machine classifi-
er (details given in a later section) trained on a   
sizable seed corpus of transliterations between 
English and Tamil, and use it to identify potential   
Tamil signatures with any of the NEs extracted 
from the English side.  We try to match each of the 
NEs extracted from the English side, to every sig-
nature from the Tamil side, and produce an ordered 
list of Tamil signatures that may be potential trans-
literations for a given English NE.  Every Tamil 
signature, thus, would get a score, which is used to 
rank the signatures in the decreasing order of simi-
larity.  Subsequently, we consider only those above 
a certain threshold for analysis, and in addition, 
consider only the top-n candidates. 
3.2 Quality Refinement 
Since a number of such transliteration candidates 
are culled from the Tamil corpus for a given NE in 
English, we further cull out unlikely candidates, by 
re-ranking them using frequency cues from the 
aligned comparable corpora.  For this, we start 
with the hypothesis, that the NEs will have similar 
normalized frequency distributions with respect to 
time, in the two corpora.  Given that the news cor-
pora are expected to contain same names in similar 
time periods in the two different languages, the 
frequency distribution of words in the two         
languages provides a strong clue about possible 
transliteration pairs; however, such potential pairs 
might also include other content words, such as, 
? ???????? (soshaliSt), ?????? (kavanamaa-
ka), ??????? (keetpathu), etc., which are common 
nouns, adjectives or even adverbs and verbs.  On 
the other hand, function words are expected to be 
uniformly distributed in the corpus, and hence may 
not have high variability like content words.   Note 
that the NEs in English are not usually inflected. 
Since Tamil NEs usually have inflections, the   
frequency of occurrence of a NE in Tamil must be 
normalized across all forms, to make it reasonably 
comparable to the frequency of the corresponding 
English NE. This was taken care of by considering 
the signature and its equivalence class. Hence the 
frequency of occurrence of a NE (i.e., its signature) 
in Tamil is the sum of frequencies of all members 
in its equivalence class.    
 
For identifying the names between the languages, 
we first create a frequency distribution of every 
word in English and Tamil, by creating temporal 
bins of specific duration, covering the entire time-
line of the corpus.  The frequency is calculated as 
the number of occurrences of each signature in the 
bin interval.  Once the frequency distributions are 
formed, they are normalized for every signature.  
Given the normalized frequencies, two words are 
considered to have same (or, similar) pattern of 
occurrence in the corpus, if the normalized        
frequency vectors of the two words are the same 
(or, close within a threshold).  Figure 1 shows the 
frequency of the word Abishek, and its Tamil ver-
sion, ??????? (apishek) as a frequency plot, 
where a high correlation between the frequencies 
can be observed. 
 
 
 
Figure 1: Names Frequency Plot in Comparable Corpora 
 
Hence, to refine the quality of the classifier output, 
we re-rank the list of candidates, using the distance 
between the frequency vectors of the English NE, 
and the Tamil candidate signature.   This step 
moves up those signatures that have similar pat-
terns of occurrence, and moves down those that do 
not.  It is likely that such frequency cues from the 
comparable corpora will make the quality of 
matched transliteration pairs better, yielding better 
mined data. 
4 Experimental Setup & Results 
In this section, we present the experimental setup 
and the data that we used for mining transliteration 
pairs from comparable corpora in two languages: 
English and the Indian language, Tamil.  We eva-
luate and present the effectiveness of the metho-
dology in extracting NE pairs, between these lan-
guages, under various parameters. 
4.1 Comparable Corpora 
We used a set of news articles from the New     
Indian Express (in English) and Dinamani (in   
Tamil) roughly covering similar events in English 
and Tamil respective, and covering a period of 
about 8 months, between January and August of 
2007.  The articles were verified to contain similar 
set of NEs, though only a fraction of them are   
expected to be legitimate transliteration pairs.  
Others related NEs could be translations,  for    
example, chief minister in English vs ???????? 
(muthalvar) in Tamil, abbreviation which are not 
usually transliterated but spelled out , for example, 
ICC in English, and ? ? ? (aicici) in Tamil, or      
co-references , for example, New Delhi in English, 
and ?????????? (puthu thilli) in Tamil.  While the 
number of      articles used were roughly the same 
(~2,400), the number of words in Tamil were only 
about 70% of that in English.  This is partially due 
to the fact Tamil is a highly agglutinative lan-
guage, where various affixes (prefixes and suffixes 
of other content words) stand for function words 
and prepositions in English, thus do not contribute 
to the word count.  Further, since our focus is on 
mining names, we expect the same NEs to be cov-
ered in both the corpora, and hence we do not   
expect a severe impact on mining. 
 
Corpus Time  
Period 
Size  
Articles Words 
New Indian  
Express  
(English) 
2007.01.01 to 
2007.08.31 
2,359 347,050 
Dinamani 
(Tamil) 
2007.01.01 to 
2007.08.31 
2,359 256,456 
Table 1: Statistics on Comparable Corpora 
 
From the above corpora, we first extracted all the 
NEs from the English side, using the Stanford 
NER tool [Finkel et al 2005].  No multiword    
expressions were considered for this experiment.  
Also, only those NEs that have a frequency count 
of more than a threshold value of FE were consi-
dered, in order to avoid unusual names that are 
hard to identify in the comparable corpora.  Thus, 
we extracted from the above corpora, only a subset 
of NEs found in the English side to be matched 
with their potential transliteration pairs; for exam-
ple, for a parameter setting of FE to 10, we extract 
only 274 legitimate NEs.   
 
From the Tamil side of the corpora, we extracted 
all words, and grouped them in to equivalence 
classes, by considering a prefix of 5 characters.  
That is, all words that share the same 5 characters 
were considered to be morphological variations of 
the same root word or NE in Tamil.  After they 
were grouped, the longest common prefix of the 
group is extracted, and is used as the signature of 
the equivalence class.  It should be noted here that 
though the number of unique words in the corpus 
is about 46,503, the number of equivalence classes 
to be considered changes depending on the filter-
ing threshold that we use in the Tamil side.  For 
example, at a threshold (FT) value of 1, the number 
of equivalence classes is 14,101.  It changes to 
4,612 at a threshold (FT) value of 5, to 2,888 at a 
threshold (FT) value of 10 and to 1779 at a thre-
shold (FT) value of 20.  However, their signature 
(i.e., longest common prefix) sizes ranged from 5 
to 13 characters.  Thus, we had about 14,101 equi-
valence classes, covering all the words from the 
Tamil corpus.  The equivalence classes thus 
formed were as shown in Figure 2: 
 
Tamil  
Signature 
Tamil  
Equiv. Class 
???????? 
(aiSvaryaa) 
???????? (aiSvaryaa),  
???????????? (aiSvaryaavin),  
?????????????? (aiSvaryaavukku), 
?????????? (aiSvaryaavai),  
???????????????? (aiSvaryaaviRkum),                           
????????????? (aiSvaryaavutan) 
?????    
(piram) 
?????????????? (pirammapuththiraa), 
????????????? (pirammaaNdamaana),         
??????? (pirampu), ??????? (pirammaa) 
?????? 
(kaaveeri) 
?????? (kaaveeri) 
? ? ?  
(aicici) 
? ? ? (aicici), ? ? ????? (aicicyin),                
? ? ???? (aicici kku), ? ? ????? (aicicithaan),         
? ? ?????? (aiciciyidam) 
Figure 2: Signatures and Equivalence Classes 
 
As can be seen in the table, all elements of an 
equivalence class share the same signature (by  
definition). However, some signatures, such as 
???????? (aiSvaryaa), correspond to an equiva-
lence class in which every element is a morpholog-
ical variation of the signature.  Such equivalence 
classes, we name them pure.  Some signatures 
represent only a subset of the members, as this set 
includes some members unrelated to this stem; for 
example, the signature ????? (piram), correctly   
corresponds to ??????? (pirammaa), and incorrect-
ly to the noun ??????? (pirambu), as well as incor-
rectly to the adjective ????????????? (piram-
maandamaana).  We name such equivalence 
classes fuzzy.  Some are well formed, but may not 
ultimately contribute to our mining, being an ab-
breviation, such as ICC (in Tamil, ? ? ?), even 
though they are used similar to any NE in Tamil. 
While most equivalence classes contained inflec-
tions of single stems, we also found morphological 
variations of several compound names in the same 
equivalence class such as, ????????? (akamath?a-
kar), ????????? (akamathaapaath), with ????? 
(akamath). 
4.2 Classifier for Transliteration Pair Identi-
fication 
We used SVM-light [Joachims, 1999], a Support-
vector Machine (SVM) from Cornell University, to 
identify near transliterations between English and 
Tamil.   We used a seed corpus consisting of 5000 
transliteration pair samples collected from a differ-
ent resource, unrelated to the experimental compa-
rable corpora. In addition to the 5000 positive   
examples from this seed corpus, 5000 negative   
examples were extracted randomly, but incorrectly, 
aligned names from this same seed corpus and 
used for the classifier. 
 
The features used for the classification are binary 
features based on the length of the pair of strings 
and all aligned unigram and bigram pairs, in each 
direction, between the two strings in the seed cor-
pus in English and Tamil.  The length features in-
clude the difference in lengths between them (up to 
3), and a separate binary feature if they differ by 
more than 3.  For unigram pairs, the ith character in  
a language string is matched to (i-1)st,  ith and  
(i+1)st characters of the other language string.  
Each string is padded with special characters at the 
beginning and the end, for appropriately forming 
the unigrams for the first and the last characters of 
the string.  In the same manner, for binary features, 
every bigram extracted with a sliding window of 
size 2 from a language string, is matched with 
those extracted from the other language string.  
After the classifier is trained on the seed corpus of 
hand crafted transliteration pairs, during the min-
ing phase, it compares every English NE extracted 
from the English corpus, to every signature from 
the Tamil corpus. 
 
While classifier provided ranked list of all the sig-
natures from Tamil side, we consider only the top-
30 signatures (and the words in the equivalence 
classes) for subsequent steps of our methodology.  
We hand-verified a random sample of about 100 
NEs from English side, and report in Table 5, the 
fraction of the English NEs for which we found at 
least one legitimate transliteration in the top-30 
candidates (for example, the  recall of the classifier 
is 0.56, in identifying a right signature in the top-
30 candidates, when the threshold FE is 10 & FT is 
1).   
 
It is interesting to note that as the two threshold 
factors are increased, the number of NEs extracted 
from the English side decreases (as expected), and 
the average number of positive classifications per 
English NE reduces (as shown in Table 2), consi-
dering all NEs.  This makes sense as the classifier 
for identifying potential transliterations is trained 
with sizable corpora and is hence accurate; but, as 
the thresholds increase, it has less data to work 
with, and possibly a fraction of legitimate translite-
rations also gets filtered with noise. 
 
Parameters Extracted 
English NEs 
Ave. Positive 
Classifications/ 
English NE 
FE: 10, FT: 1 274 79.34 
FE: 5, FT: 5 588 29.50 
FE: 10, FT: 10 274 17.49 
FE: 20, FT: 20 125 10.55 
Table 2: Threshold Parameters vs Mining Quantity 
 
Table 3 shows some sample results after the classi-
fication step with parameter values as (FE: 10, FT: 1). 
Right signature for Aishwarya (corresponding to 
all correct transliterations) has been ranked 10 and 
Gandhi (with only a subset of the equivalence class 
corresponding to the right transliterations) has been 
ranked at 8.  Three different variations of Argenti-
na can be found, ranked 2nd, 3rd and 13th.  While, in 
general no abbreviations are found (usually their 
Tamil equivalents are spelled out), a rare case of 
abbreviation (SAARC) and its right transliteration is 
ranked 1st.   
 
 
English 
Named Entity 
Tamil Equivalence Class  
Signature 
Precision Rank 
aishwarya ???????? (aiSvaryaa) 1 10 
argentina ?????????????    
(arjantinaavila) 
1 2 
argentina ?????????????    
(aarjantinaavi) 
1 3 
argentina ??????????????    
(aarjantinaavil) 
1 13 
gandhi ????? (kaa?tha) 0.2121 8 
saarc  ????? (saark) 1 1 
Table 3: Ranked List after Classification Step 
 
4.3 Enhancing the Quality of Transliteration-
Pairs 
For the frequency analysis, we use the frequency 
distribution of the words in English and Tamil side 
of the comparable corpora, counting the number of 
occurrences of NEs in English and the Tamil    
signatures in each temporal bin spanning the entire 
corpus. We consider one temporal bin to be equal 
to two successive days. Thus, each of the English 
NEs and the Tamil signatures is represented by a 
vector of dimension approximately 120. We com-
pute the distance between the two vectors, and  
hypothesize that they may represent the same (or, 
similar) name, if the difference between them is 
zero (or, small).  Note that, as mentioned earlier,  
the frequency vector of the Tamil signature will 
contain the sum of individual frequencies of the 
elements in the equivalence class corresponding to 
it.  Given that the classifier step outputs a list of 
English NEs, and associated with each entry, a 
ranked list of Tamil signatures that are identified as 
potential transliteration by the classifier, we com-
pute the distance between the frequency vector of 
every English NE, with each of the top-30 signa-
tures in the ranked list.  We re-rank the top-30 
candidate strings, using this distance measure.  The 
output is similar to that shown in Table 4, but with 
possibly a different rank order. 
 
 
English 
Named Entity 
Tamil Equivalence Class  
Signature 
Precision Rank 
aishwarya ???????? (aiSvaryaa) 1 1 
argentina ?????????????           
(arjantinaavila) 
1 1 
argentina ?????????????          
(aarjantinaavi) 
1 3 
argentina ??????????????          
(aarjantinaavil) 
1 14 
gandhi ????? (kaa?tha) 0.2121 16 
saarc  ????? (saark) 1 1 
Table 4: Ranked List after Frequency Analysis Step 
 
On comparing Table 3 and 4, we observe that some 
of the ranks have moved for the better, and some 
of them for the worse.  It is interesting to note that 
the ranking of different stems corresponding to 
Argentina has moved differently.  It is quite likely 
that merging these three equivalence classes cor-
responding to the English NE Argentina might re-
sult in a frequency profile that is more closely 
aligned to that of the English NE.   
4.4 Overall Performance of Transliteration 
Pairs Mining 
To find the effectiveness of each step of the mining 
process in identifying the right signatures (and 
hence, the equivalence classes) for a given English 
NE, we computed the Mean Reciprocal Rank 
(MRR) of the random sample of 100 transliteration 
pairs mined, in two different ways:  First, we com-
puted MRRpure, which corresponded to the first oc-
currence of a pure equivalence class, and MRRfuzzy, 
which corresponded to the first occurrence of a 
fuzzy equivalence class in the random samples.  
MRRfuzzy captures how successful the mining was 
in identifying one possible transliteration, MRRpure, 
captures how successful we were in identifying an 
equivalence class that contains only right translite-
rations2.  In addition, these metrics were computed, 
corresponding to different frequency thresholds for 
the occurrence of a English NE (FE) and a Tamil 
signature (FT).  The overall quality profile of the 
mining framework in mining the NE transliteration 
pairs in English and Tamil is shown in Table 5.  
Additionally, we also report the recall metric (the 
fraction of English NEs, for which at least one le-
                                                 
2 However, it should be noted that the current metrics 
neither capture how pure an equivalence class is (frac-
tion of the set that are correct transliterations), nor the 
size of the equivalence class.  We hope to specify these 
as part of quality of mining, in our subsequent work.  
gitimate Tamil signature was identified) computed 
on a randomly chosen 100 entity pairs. 
 
Parameters 
Classification 
Step 
Frequency 
Analysis Step Re-
call MRR 
fuzzy  
MRR 
pure 
MRR 
fuzzy  
MRR 
pure 
FE: 10, FT: 1 0.3579 0.2831 0.3990 0.3145 0.56 
FE: 5, FT: 5 0.4490 0.3305 0.5064 0.3529 0.61 
FE: 10, FT: 10 0.4081 0.2731 0.4930 0.3494 0.57 
FE: 20, FT: 20 0.3489 0.2381 0.4190 0.2779 0.47 
Table 5: Quality Profile of NE Pairs Extraction 
 
First, it should be noted that the recalls are the 
same for both the steps, since Frequency Analysis 
step merely re-arranges the output of the Classifi-
cation step.  Second, the recall figures drop, as 
more filtering is applied to the NEs on both sides.  
This trend makes sense, since the classifier gets 
less data to work with, as more legitimate words 
are filtered out with noise.  Third, as can be ex-
pected, MRRpure is less than the MRRfuzzy at every 
step of the mining process.  Fourth, we see that the 
MRRpure and the MRRfuzzy improve between the two 
mining steps, indicating that the time-series analy-
sis has, in general, made the output better.   
 
Finally, we find that the MRRpure and the MRRfuzzy 
keep dropping with increased filtering of English 
NEs and Tamil signatures based on their frequen-
cy, in both the classification and frequency analy-
sis steps. The fall of the MRRs after the classifica-
tion steps is due to the fact that the classifier has 
less and less data with the increasing threshold, 
and hence some legitimate transliterations may be 
filtered out as noise.  However, the frequency 
analysis step critically depends on availability of 
sufficient words from the Tamil side for similarity 
testing.  In frequency analysis step, the fall of 
MRRs from threshold 5 to 10 is 0.0134 on MRRfuzzy 
and 0.0035 on MRRpure. This fall is comparatively 
less to the fall of MRRs from threshold 10 to 20 
which is 0.074 on MRRfuzzy and 0.0715 on MRRpure. 
This may be due to the fact that the number of legi-
timate transliterations filtered out from threshold 5 
to 10 is less when compared to the number of legi-
timate transliterations filtered out from threshold 
10 to 20. These results show that with less number 
of words filtered, it can get reasonable recall and 
MRR values. More profiling experiments may be 
needed to  validate this claim.      
5 Open Issues in NE pair Mining 
In this paper, we outline our experience in mining 
parallel NEs between English and Tamil, in an  
approach similar to the one discussed in [Klemen-
tiev and Roth, 2006].  Over and above, we made 
parameter choices, and some procedural modifica-
tions to bridge the underspecified methodology 
given in the above work.  While the results are 
promising, we find several issues that need further 
research.  We outline some of them below: 
5.1 Indistinguishable Signatures 
Table 7 shows a signature that offers little help in 
distinguishing a set of words.  Both the words, 
? ???? (cennai) and morphological variations of 
? ?? (cen), share the same 5-character signature, 
namely, ? ???  (cenna), affecting the frequency 
distribution of the signature adversely. 
 
English 
Named 
Entity 
Tamil 
Named 
Entity 
Tamil  
Equivalent Class 
chennai 
? ???? 
(cennai) 
? ???? (cennai), ? ???????? (cennaiyil), 
? ?????????????? (cennaiyilirunthu), 
? ?????? (cennin), ? ????????           (cen-
nukku),? ?????? (cennaiyai) 
Table 7: Multiple-Entity Equivalence Class 
5.2 Abbreviations 
Table 8 shows a set of abbreviations, that are not 
identified well in our NE pair mining. Between the 
two languages, the abbreviations may be either 
expanded, as BJP expanded to (the equivalent 
translation for Bharatiya Janatha Party in Tamil), 
or spelled out, as in BSNL referred to as 
??????????? (pieSenel).  The last example is very 
interesting, as each W in English is written out as 
??????? (tapiLyuu).  All these are hard to capture 
by a simple classifier that is trained on well-formed 
transliteration pairs.  
 
English 
Named 
Entity 
Tamil  
Named Entity 
BJP 
???? (paajaka), ??.?.?. (paa. ja. ka.), ?????? 
???? ??? ? (paarathiiya janathaa katci) 
BSNL 
??????????? (pieSenel), ???????????????     
(pieSenellin), ????????????? (piesenellai) 
WWW 
?????????????????????  
(tapiLyuutapiLyuutapiLyuu) 
Table 8: Multiple-Entity Equivalence Class 
5.3 Multiword Expressions 
This methodology is currently designed for mining 
only single word expressions.  It may be an inter-
esting line of research to mine multiword expres-
sions automatically. 
6 Related Work 
Our work essentially follows a similar procedure 
as reported in [Klementiev and Roth, 2006] paper, 
but applied to English-Tamil language pair.  Earli-
er works, such as [Cucerzan and Yarowsky, 1999] 
and [Collins and Singer, 1999] addressed identifi-
cation of NEs from untagged corpora. They relied 
on significant contextual and morphological clues.  
[Hetland, 2004] outlined methodologies based on 
time distribution of terms in a corpus to identify 
NEs, but only in English.  While a large body of 
literature exists on transliteration, we merely point 
out that the focus of this work (based on [Klemen-
tiev and Roth, 2006]) is not on transliteration, but 
mining transliteration pairs, which may be used for 
developing a transliteration system.   
7 Conclusions 
In this paper, we focused on mining NE transliteration 
pairs in two different languages, namely English and an 
Indian language, Tamil.  While we adopted a methodol-
ogy similar to that in [Klementiev and Roth, 2006], our 
focus was on mining parallel NE transliteration pairs, 
leveraging the availability of comparable corpora and a 
well-trained linear classifier to identify transliteration 
pairs.  We profiled the performance of our mining 
framework on several parameters, and presented the 
results.  Our experiment results are inline with those 
reported by [Klementiev and Roth, 2006]. Given that 
the NE pairs are an important resource for several NLP 
tasks, we hope that such a methodology to mine the 
comparable corpora may be fruitful, as comparable   
corpora may be freely available in perpetuity in several 
of the world?s languages.  
8 Acknowledgements 
We would like to thank Raghavendra Udupa, 
Chris Quirk, Aasish Pappu, Baskaran Sankaran,  
Jagadeesh Jagarlamudi and Debapratim De for 
their help. 
References 
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical 
transliteration for English-Arabic cross language informa-
tion retrieval. In Proceedings of CIKM, pages 139?146, 
New York, NY, USA. 
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Information into 
Information Extraction Systems by Gibbs Sampling. In 
Proceedings of the 43nd Annual Meeting of the Association 
for Computational Linguistics (ACL 2005), pp. 363-370. 
L Haizhou, Z Min and S Jian. 2004. A Joint Source-Channel 
Model for Machine Transliteration. In Proceedings of 42nd 
Meeting of Assoc. of Computational Linguistics. 
 
Magnus Lie Hetland. 2004. Data Mining in Time Series Data-
bases, a chapter in A Survey of Recent Methods for Effi-
cient Retrieval of Similar Time Sequences. World Scientif-
ic.  
T. Joachims. 1999. 11 in: Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. Burges 
and A. Smola (ed.), MIT Press. 
Sung Young Jung, SungLim Hong, and Eunok Paek. 2000. An 
English to Korean transliteration model of extended mar-
kov window. In Proceedings of the International Confe-
rence on Computational Linguistics (COLING), pages 
383?389.  
Alexandre Klementiev and Dan Roth. 2006. Named Entity 
Transliteration and Discovery from Multilingual Compara-
ble Corpora. In Proceedings of the Human Language 
Technology Conference of the North American Chapter of 
the ACL, pages 82?88. 
Kevin Knight and Jonathan Graehl. 1997. Machine translite-
ration. In Proceedings of the Meeting of the European As-
sociation of Computational Linguistics, pages 128?135.  
Yusuke Shinyama and Satoshi Sekine. 2004. Named entity 
discovery using comparable news articles. In Proceedings 
the International Conference on Computational Linguistics 
(COLING), pages 848?853. 
Richard Sproat, Tao Tao, ChengXiang Zhai. 2006. Named 
Entity Transliteration with Comparable Corpora. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the ACL, 
pages 73?80, Sydney. 
Tao Tao and ChengXiang Zhai. 2005. Mining comparable 
bilingual text corpora for cross-language information inte-
gration. In KDD?05, pages 691?696. 
 
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat, and 
ChengXiang Zhai. 2006. Unsupervised named entity transli-
teration using temporal and phonetic correlation. In EMNLP 
2006, Sydney, July. 
Paula Virga and Sanjeev Khudanpur. 2003. Transliteration of 
Proper Names in Cross-Lingual Information Retrieval.  In 
Proceedings of Workshop on Multilingual and Mixed-
Language Named Entity Recognition. 
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 29?32,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
WikiBABEL: A Wiki-style Platform for Creation of Parallel Data 
 A Kumaran?      K Saravanan?      Naren Datha*      B Ashok*      Vikram Dendi? 
 
?Multilingual Systems  
Research 
Microsoft Research India 
 
*Advanced Development & 
Prototyping 
Microsoft Research India 
 
?Machine Translation  
Incubation   
Microsoft Research 
 
Abstract 
In this demo, we present a wiki-style platform ? 
WikiBABEL ? that enables easy collaborative 
creation of multilingual content in many non-
English Wikipedias, by leveraging the relatively 
larger and more stable content in the English 
Wikipedia.  The platform provides an intuitive 
user interface that maintains the user focus on 
the multilingual Wikipedia content creation, by 
engaging search tools for easy discoverability of 
related English source material, and a set of lin-
guistic and collaborative tools to make the con-
tent translation simple.  We present two different 
usage scenarios and discuss our experience in 
testing them with real users.  Such integrated 
content creation platform in Wikipedia may yield 
as a by-product, parallel corpora that are critical 
for research in statistical machine translation sys-
tems in many languages of the world.   
1 Introduction 
Parallel corpora are critical for research in many 
natural language processing systems, especially, 
the Statistical Machine Translation (SMT) and 
Crosslingual Information Retrieval (CLIR) sys-
tems, as the state-of-the-art systems are based on 
statistical learning principles; a typical SMT sys-
tem in a pair of language requires large parallel 
corpora, in the order of a few million parallel 
sentences.  Parallel corpora are traditionally 
created by professionals (in most cases, for busi-
ness or governmental needs) and are available 
only in a few languages of the world.  The prohi-
bitive cost associated with creating new parallel 
data implied that the SMT research was re-
stricted to only a handful of languages of the 
world.  To make such research possible widely, it 
is important that innovative and inexpensive 
ways of creating parallel corpora are found.  Our 
research explores such an avenue: by involving 
the user community in creation of parallel data. 
In this demo, we present a community colla-
boration platform ? WikiBABEL ? which 
enables the creation of multilingual content in 
Wikipedia.  WikiBABEL leverages two signifi-
cant facts with respect to Wikipedia data: First, 
there is a large skew between the content of Eng-
lish and non-English Wikipedias.  Second, while 
the original content creation requires subject 
matter experts, subsequent translations may be 
effectively created by people who are fluent in 
English and the target language.  In general, we 
do expect the large English Wikipedia to provide 
source material for multilingual Wikipedias; 
however on specific topics specific multilingual 
Wikipedia may provide the source material 
(http://ja.wikipedia.org/wiki/?? may be better 
than http://en.wikipedia.org/wiki/haiku).   We 
leverage these facts in the WikiBABEL frame-
work, enabling a community of interested native 
speakers of a language, to create content in their 
respective language Wikipedias.  We make such 
content creation easy by integrating linguistic 
tools and resources for translation, and collabora-
tive mechanism for storing and sharing know-
ledge among the users.  Such methodology is 
expected to generate comparable data (similar, 
but not the same content), from which parallel 
data may be mined subsequently (Munteanu et 
al, 2005) (Quirk et al 2007).   
We present here the WikiBABEL platform, 
and trace its evolution through two distinct usage 
versions: First, as a standalone deployment pro-
viding a community of users a translation plat-
form on hosted Wikipedia data to generate paral-
lel corpora, and second, as a transparent edit 
layer on top of Wikipedias to generate compara-
ble corpora.  Both paradigms were used for user 
testing, to gauge the usability of the tool and the 
viability of the approach for content creation in 
multilingual Wikipedias.  We discuss the imple-
mentations and our experience with each of the 
above scenarios.  Such experience may be very 
valuable in fine-tuning methodologies for com-
munity creation of various types of linguistic 
data.  Community contributed efforts may per-
haps be the only way to collect sufficient corpora 
effectively and economically, to enable research 
in many resource-poor languages of the world. 
29
2 Architecture of WikiBABEL 
The architecture of WikiBABEL is as illustrated 
in Figure 1: Central to the architecture is the Wi-
kiBABEL component that coordinates the interac-
tion between its linguistic and collaboration 
components, and the users and the Wikipedia 
system.  WikiBABEL architecture is designed to 
support a host of linguistic tools and resources 
that may be helpful in the content creation 
process: Bilingual dictionaries for providing for 
word-level translations, allowing user customiza-
tion of domain-specific, or even, user-specific 
bilingual dictionaries.  Also available are ma-
chine translation and transliteration systems for 
rough initial translation [or transliteration] of a 
source language string at sentential/phrasal levels 
[or names] to the intended target language.  As 
the quality of automatic translations are rarely 
close to human quality translations, the user may 
need to correct any such automatically translated 
or transliterated content, and an intuitive edit 
framework provides tools for such corrections.  
A collaborative translation memory component 
stores all the user corrections (or, sometimes, 
their selection from a set of alternatives) of ma-
chine translations, and makes them available to 
the community as a translation help (?tribe know-
ledge?).  Voting mechanisms are available that 
may prioritize more frequently chosen alterna-
tives as preferred suggestions for subsequent us-
ers.  The user-management tracks the user de-
mographic information, and their contributions 
(its quality and quantity) for possible recogni-
tion.  The user interface features are imple-
mented as light-weight components, requiring 
minimal server-side interaction.  Finally, the ar-
chitecture is designed open, to integrate any user-
developed tools and resources easily.  
 
 
 
3 WikiBABEL on Wikipedia 
IN this section we discuss Wikipedia content and 
user characteristics and outline our experience 
with the two versions on Wikipedia.   
3.1 Wikipedia: User & Data Characteristics 
Wikipedia content is acknowledged to be on par 
with the best of the professionally created re-
sources (Giles, 2005) and is used regularly as 
academic reference (Rainie et al, 2007).  How-
ever, there is a large disparity in content between 
English and other language Wikipedias. English 
Wikipedia - the largest - has about 3.5 Million 
topics, but with an exception of a dozen or so 
Western European and East Asian languages, 
most of the 250-odd languages have less than 1% 
of English Wikipedia content (Wikipedia, 2009).  
Such skew, despite the size of the respective user 
population, indicates a large room for growth in 
many multilingual Wikipedias.  On the contribu-
tion side, Wikipedia has about 200,000 contribu-
tors (> 10 total contributions); but only about 4% 
of them are very active (> 100 contributions per 
month).  The general perception that a few very 
active users contributed to the bulk of Wikipedia 
was disputed in a study (Swartz, 2006) that 
claims that large fraction of the content were 
created by those who made very few or occa-
sional contributions that are primarily editorial in 
nature.  It is our strategy to provide a platform 
for easy multilingual Wikipedia content creation 
that may be harvested for parallel data.   
3.2 Version 1: A Hosted Portal 
In our first version, a set of English Wikipedia 
topics (stable non-controversial articles, typically 
from Medicine, Healthcare, Science & Technol-
ogy, Literature, etc.) were chosen and hosted in 
our WikiBABEL portal.  Such set of articles is 
already available as Featured Articles in most 
Wikipedias.  English Wikipedia has a set of 
~1500 articles that are voted by the community 
as stable and well written, spanning many do-
mains, such as, Literature, Philosophy, History, 
Science, Art, etc.  The user can choose any of 
these Wikipedia topics to translate to the target 
language and correct the machine translation er-
rors.  Once a topic is chosen, a two-pane window 
is presented to the user, as shown in Figure 2, in 
which the original English Wikipedia article is 
shown in the left panel and a rough translation of 
the same article in the user-chosen target lan-
guage is presented in the right panel.  The right 
panel has the same look and feel as the original 
30
English Wikipedia article, and is editable, while 
the left panel is primarily intended for providing 
source material for reference and context, for the 
translation correction.  On mouse-over the paral-
lel sentences are highlighted, linking visually the 
related text on both panels.  On a mouse-click, an 
edit-box is opened in-place in the right panel, 
and the current content may be edited.  As men-
tioned earlier, integrated linguistic tools and re-
sources may be invoked during edit process, to 
help the user.  Once the article reaches sufficient 
quality as judged by the users, the content may 
be transferred to target language Wikipedia, ef-
fectively creating a new topic in the target lan-
guage Wikipedia. 
User Feedback: We field tested our first ver-
sion with a set of Wikipedia users, and a host of 
amateur and professional translators.  The prima-
ry feedback we got was that such efforts to create 
content in multilingual Wikipedia was well ap-
preciated.  The testing provided much quantita-
tive (in terms of translation time, effort, etc.) and 
qualitative (user experience) measures and feed-
back.  The details are available in (Kumaran et 
al., 2008), and here we provide highlights only: 
? Integrated linguistic resources (e.g., bilingual 
dictionaries, transliteration systems, etc.) 
were appreciated by all users. 
? Amateur users used the automatic translations 
(in direct correlation with its quality), and 
improved their throughput up to 40%.  
? In contrast, those who were very fluent in 
both the languages were distracted by the 
quality of translations, and were slowed by 
30%.  In most cases, they preferred to redo 
the entire translations, rather than considering 
and correcting the rough translation.  
? One qualitative feedback from the Wikipedia 
community is that the sentence-by-sentence 
translation enforced by the portal is not in 
tune with their philosophy of user-decided 
content for the target topic.  
We used the feedback from the version 1, to re-
design WikiBABEL in version 2. 
3.3 Version 2: As a Transparent Edit Layer 
In our second version, we implemented the 
significant feedback from Wikipedians, pertain-
ing to source content selection and the user con-
tribution.  In this version, we delivered the Wi-
kiBABEL experience as an add-on to Wikipedia, 
as a semi-transparent overlay that augments the 
basic Wikipedia edit capabilities without taking 
the contributor away from the site. Capable of 
being launched with one click (via a bookmark-
let, or a browser plug-in, or as a potential server 
side integration with Wikipedia), the new version 
offered a more seamless workflow and integrated 
linguistic and collaborative components.  This 
add-on may be invoked on Wikipedia itself, pro-
viding all WikiBABEL functionalities.  In a typi-
cal WikiBABEL usage scenario, a Wikipedia 
31
content creator may be at an English Wikipedia 
article for which no corresponding article exists 
in the target language, or at target language Wi-
kipedia article which has much less content 
compared to the corresponding English article.  
The WikiBABEL user interface in this version 
is as shown in Figure 3.  The source English Wi-
kipedia article is shown in the left panel tabs, and 
may be toggled between English and the target 
language; also it may be viewed in HTML or in 
Wiki-markup.  The right panel shows the target 
language Wikipedia article (if it exists), or a 
newly created stub (otherwise); either case, the 
right panel presents a native target language Wi-
kipedia edit page, for the chosen topic.  The left 
panel content is used as a reference for content 
creation in target language Wikipedia in the right 
panel.  The user may compose the target lan-
guage Wikipedia article, either by dragging-and-
dropping translated content from the left to the 
right panel (into the target language Wikipedia 
editor), or add new content as a typical Wikipe-
dia user would.  To enable the user to stay within 
WikiBABEL for their content research, we have 
provided the capability to search through other 
Wikipedia articles in the left panel.  All linguistic 
and collaborative features are available to the 
users in the right panel, as in the previous ver-
sion.  The default target language Wikipedia pre-
view is at any time.  While the user testing of this 
implementation is still in the preliminary stages, 
we wish to make the following observations on 
the methodology: 
? There is a marked shift of focus from 
?translation from English Wikipedia article? 
to ?content creation in target Wikipedia?. 
? The user is never taken away from Wiki-
pedia site, requiring optionally only Wikipe-
dia credentials. The content is created direct-
ly in the target Wikipedia. 
 
The WikiBABEL Version 2 prototype will be 
made available externally in the future.   
References  
Kumaran, A, Saravanan, K and Maurice, S. WikiBA-
BEL: Community Creation of Multilingual Data.  
WikiSYM 2008 Conference, 2008.  
Munteanu, D. and Marcu, D. Improving the MT per-
formance by exploiting non-parallel corpora. 
Computational Linguistics. 2005. 
Giles, J. Internet encyclopaedias go head to head.  
Nature. 2005. doi:10.1038/438900a.  
Quirk, C., Udupa, R. U. and Menezes, A. Generative 
models of noisy translations with app. to parallel 
fragment extraction.  MT Summit XI, 2007.   
Rainie, L. and Tancer, B. Pew Internet and American 
Life.  http://www.pewinternet.org/.   
Swartz, A. Raw thought: Who writes Wikipedia? 
2006. http://www.aaronsw.com/.  
Wikipedia Statistics, 2009.http://stats.wikimedia.org/.  
32
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 1?18,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Report of NEWS 2009 Machine Transliteration Shared Task
Haizhou Li?, A Kumaran?, Vladimir Pervouchine? and Min Zhang?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,vpervouchine,mzhang}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the details of the
Machine Transliteration Shared Task con-
ducted as a part of the Named Enti-
ties Workshop (NEWS), an ACL-IJCNLP
2009 workshop. The shared task features
machine transliteration of proper names
from English to a set of languages. This
shared task has witnessed enthusiastic par-
ticipation of 31 teams from all over the
world, with diversity of participation for
a given system and wide coverage for a
given language pair (more than a dozen
participants per language pair). Diverse
transliteration methodologies are repre-
sented adequately in the shared task for a
given language pair, thus underscoring the
fact that the workshop may truly indicate
the state of the art in machine transliter-
ation in these language pairs. We mea-
sure and report 6 performance metrics on
the submitted results. We believe that the
shared task has successfully achieved the
following objectives: (i) bringing together
the community of researchers in the area
of Machine Transliteration to focus on var-
ious research avenues, (ii) Calibrating sys-
tems on common corpora, using common
metrics, thus creating a reasonable base-
line for the state-of-the-art of translitera-
tion systems, and (iii) providing a quan-
titative basis for meaningful comparison
and analysis between various algorithmic
approaches used in machine translitera-
tion. We believe that the results of this
shared task would uncover a host of inter-
esting research problems, giving impetus
to research in this significant research area.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They have a critical role
in Cross Language Information Retrieval (CLIR)
and Machine Translation (MT) systems as the sys-
tems? performances are shown to positively cor-
relate with the correct conversion of names be-
tween the languages in several studies (Demner-
Fushman and Oard, 2002; Mandl and Womser-
Hacker, 2005; Hermjakob et al, 2008; Udupa et
al., 2009). The traditional source for name equiva-
lence, the bilingual dictionaries ? whether hand-
crafted or statistical ? offer only limited support
as they do not have sufficient coverage of names.
New names are introduced to the vocabulary of a
language every day.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. This has attracted attention from the re-
search community. Over the last decade scores of
papers on Machine Transliteration have appeared
in the top Computational Linguistics, Information
Retrieval and Data Management conferences, ex-
ploring diverse algorithmic approaches in a wide
variety of different languages (Knight and Graehl,
1998; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Goldwasser and Roth,
2008; Goldberg and Elhadad, 2008; Klementiev
and Roth, 2006). However, there has not been
any coordinated effort in calibrating the state-of-
the-art technical capabilities of machine translit-
eration: the studies explore different algorithmic
approaches in different language pairs and report
their performance in different metrics and tested
on different corpora.
The overarching objective of this shared task
is to drive the machine transliteration technology
forward, to measure and baseline the state-of-the-
1
art and to provide a meaningful comparison be-
tween the most promising algorithmic approaches
in order to stimulate the discussions among the re-
searchers. The NLP community in Asia is espe-
cially interested in transliteration as several major
Asian languages do not use Latin script in their na-
tive writing systems. The Named Entity Workshop
(NEWS 2009) in ACL-IJCNLP 2009 in Singapore
provides an ideal platform for the shared task to
take off. This is precisely what we address in this
shared task on machine transliteration that is con-
ducted as a part of the Named Entity Workshop
(NEWS-2009), an ACL-IJCNLP 2009 workshop.
The shared task aims at achieving the following
objectives:
? Providing a forum to bring together the com-
munity of researchers in the area of Machine
Transliteration to focus on various research
avenues in this important research area.
? Calibrating systems on common hand-crafted
corpora, using common metrics, in many dif-
ferent languages, thus creating a reasonable
baseline for the state-of-the-art of translitera-
tion systems.
? Analysing the results so that a reason-
able comparison of different algorithmic
approaches and their trade-offs (such as,
transliteration quality vs. generality of ap-
proach across languages vs. training data
size, etc.) may be explored.
We believe that a substantial part of what we have
set out to achieve has been accomplished, and we
present this report as a record of the task pro-
cess, system participation and results and our find-
ings. It is our hope that this reporting will generate
lively discussions during the NEWS workshop and
subsequent research in this important area.
This introduction outlines the purpose of the
transliteration shared task conducted as a part of
the NEWS workshop. Section 2 outlines the ma-
chine transliteration task and the corpora used and
Section 3 discusses the metrics chosen for evalua-
tion, along with the rationale for choosing them.
Section 4 sketches the participation. Section 5
presents the results of the shared task and the anal-
ysis of the results. Section 6, summarises the
queries and feedback we have received from the
participants and Section 7 concludes, presenting
some lessons learnt from the current edition of the
shared task, and some ideas we want to pursue
in the future plan for the Machine Transliteration
tasks.
2 Transliteration Shared Task
In this section, we outline the definition of the task,
the process followed and the rationale for the de-
cisions.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target language,
but also at its ultimate utility for downstream ap-
plications, such as CLIR and MT. We have nar-
rowed down to three specific requirements for the
task, as follows: ?Transliteration is the conver-
sion of a given name in the source language (a
text string in the source writing system or orthog-
raphy) to a name in the target language (another
text string in the target writing system or orthog-
raphy), such that the target language name is:
(i) phonemically equivalent to the source name
(ii) conforms to the phonology of the target lan-
guage and (iii) matches the user intuition of the
equivalent of the source language name in the tar-
get language.?
Given that the phoneme set of languages may
not be exactly the same, the first requirement must
be diluted to ?close to?, instead of ?equivalent?.
The second requirement is needed to ensure that
the target string is a valid string as per the target
language phonology. The third requirement is in-
troduced to produce what a normal user would ex-
pect (at least for the popular names), and in or-
der to make it useful for downstream applications
like MT or CLIR systems. Though the third re-
quirement make systems produce target language
strings that marginally violate the first or second
requirements, it ensures that such transliteration
system is of value to downstream systems. All the
above requirements are implicitly enforced by the
choice of name pairs used to define the training
and test corpora in a given language pair. In cases
where multiple equivalent target language names
are possible for a source language name, we in-
2
clude all of them.
After much debate, we have also retained the
task name as ?transliteration?, though our defi-
nition may be closest to the ?popular transcrip-
tion? (Halpern, 2007), due to the popularity of
term ?Machine Transliteration? among the lan-
guage technology researchers.
2.2 Shared Task Description
The shared task is specified as development of ma-
chine transliteration systems in one or more of the
specified language pairs. Each language pair of
the shared task consists of a source and a target
language, implicitly specifying the transliteration
direction. Training and development data in each
of the language pairs have been made available to
all registered participants for developing a translit-
eration system for that specific language pair using
any approach that they find appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 1,000 and 3,000
source names (approximately 10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair every participant is re-
quired to submit one run (designated as a ?stan-
dard? run) that uses only the data provided by the
NEWS workshop organisers in that language pair,
and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more runs (designated as ?non-standard?) for ev-
ery language pair using either data beyond that
provided by the shared task organisers or linguis-
tic resources in a specific language, or both. This
essentially may enable any participant to demon-
strate the limits of performance of their system in
a given language pair.
The shared task timelines provide adequate time
for development, testing (approximately 2 months
after the release of the training data) and the final
result submission (5 days after the release of the
test data).
2.3 Shared Task Corpora
We have had two specific constraints in selecting
languages for the shared task: language diversity
and data availability. To make the shared task in-
teresting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 7 languages shown in Table 1 for the
task (Li et al, 2004; Kumaran and Kellner, 2007;
MSRI, 2009; CJKI, 2009).
For all of the languages chosen, we have been
able to procure paired names data between En-
glish and the respective languages and were able
to make them available to the participants. In ad-
dition, we have been able to procure a specific
corpus of about 40K Romanised Japanese names
and their Kanji counterparts, and the correspond-
ing language pair (Japanese names from their Ro-
manised form to Kanji) has been included as one
of the task language pair.
It should be noted here that each corpus has a
definite skew in its characteristics: the names in
the Chinese, Japanese and Korean (CJK) language
corpora are Western names; the Indic languages
(Hindi, Kannada and Tamil) corpora consists of a
mix of Indian and Western names. The Roman-
ised Kanji to Kanji corpus consists only of native
Japanese names. While such characteristics may
have provided us an opportunity to specifically
measure the performance for forward translitera-
tions (in CJK) and backward transliterations (in
Romanised Kanji), we do not highlight such fine
distinctions in this edition.
Finally, it should be noted here that the corpora
procured and released for NEWS 2009 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit re-
sults of one standard and up to four non-standard
3
Source language Target language Data Source Data Size (No. source names) Task IDTraining Development Testing
English Hindi Microsoft Research India 9,975 974 1,000 EnHi
English Tamil Microsoft Research India 7,974 987 1,000 EnTa
English Kannada Microsoft Research India 7,990 968 1,000 EnKa
English Russian Microsoft Research India 5,977 943 1,000 EnRu
English Chinese Institute for Infocomm Research 31,961 2,896 2,896 EnCh
English Korean Hangul CJK Institute 4,785 987 989 EnKo
English Japanese Katakana CJK Institute 23,225 1,492 1,489 EnJa
Japanese name (in English) Japanese Kanji CJK Institute 6,785 1,500 1,500 JnJk
Table 1: Source and target languages for the shared task on transliteration.
runs. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 6 evaluation
metrics capturing different aspects of translitera-
tion performance. Since a name may have mul-
tiple correct transliterations, all these alternatives
are treated equally in the evaluation, that is, any
of these alternatives is considered as a correct
transliteration, and all candidates matching any of
the reference transliterations are accepted as cor-
rect ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
4
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
3.5 MAP10
MAP10 measures the precision in the 10-best can-
didates for i-th source name provided by the can-
didate system. In general, the higher MAP10 is,
the better is the quality of the transliteration sys-
tem in capturing the multiple references.
MAP10 =
1
N
N?
i=1
1
10
(
10?
k=1
num(i, k)
)
(10)
3.6 MAPsys
MAPsys measures the precision in the top Ki-best
candidates produced by the system for i-th source
name, for which ni reference transliterations are
available. This measure allows the systems to pro-
duce variable number of transliterations, based on
their confidence in identifying and producing cor-
rect transliterations.
MAPsys =
1
N
N?
i=1
1
Ki
(
Ki?
k=1
num(i, k)
)
(11)
4 Participation in Shared Task
There have been 31 systems from around the
world that participated in the shared task and sub-
mitted the transliteration results for a common test
data, produced by their systems trained on the
common training corpora.
A few teams have participated in all or almost
all tasks (that is, language pairs); most others par-
ticipated in 3 tasks on average. Each language pair
has attracted on average around 13 teams. The par-
ticipation details are shown in Table 3 and the de-
mographics of the participating teams by country
is shown in Figure 1.
? ? ? ? ? ? ? ? ? ? ??
??
???
???
?????????
????
?????
???
????
????
????????
0 1 2 3 4 5 6 7 8 9 10
Figure 1: Participation by country.
Teams are required to submit at least one stan-
dard run for every task they participated in. In total
104 standard and 86 non-standard runs have been
submitted. Table 2 shows the number of standard
and non-standard runs submitted for each task. It
is clear that the most ?popular? tasks are translit-
eration from English to Hindi and from English to
Chinese, attempted by 21 and 18 participants re-
spectively. Overall, as can be noted from the re-
sults, each task has received significant participa-
tion.
5 Task Results and Analysis
5.1 Standard runs
The 8 individual plots in Figure 2 summarise (for
each task) the results of standard runs via 3 mea-
sured metrics concerning output of at least one
correct candidate per source word, namely, ac-
curacy in top-1, F -score and Mean Reciprocal
Rank (MRR). The plots in Figure 3 summarise (for
each task) the results for 3 metrics on ranked or-
dered transliteration output of the systems, namely
MAPref , MAP10 and MAPsys metrics. All the
results are presented numerically in Tables 8?11,
for all evaluation metrics. These are the official
5
English
to Hindi
English
to Tamil
English
to Kan-
nada
English
to Rus-
sian
English
to Chi-
nese
English
to Ko-
rean
English
to
Japanese
Katakana
Japanese
translit-
erated to
Japanese
Kanji
Language pair code EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
Standard runs 21 13 14 13 18 8 10 7
Non-standard runs 18 5 5 16 20 9 5 8
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
evaluation results published for this edition of the
transliteration shared task. Note that two teams
have updated their results (after fixing bugs in their
systems) after the deadline; their results are iden-
tified specifically.
We find that two approaches to transliteration
are most popular in the shared task submissions.
One of these approaches is Phrase-based statis-
tical machine transliteration (Finch and Sumita,
2008), an approach initially developed for ma-
chine translation (Koehn et al, 2003). Systems
that adopted this approach are (Song, 2009; Haque
et al, 2009; Noeman, 2009; Rama and Gali, 2009;
Chinnakotla and Damani, 2009).1 The other is
Conditional Random Fields(Lafferty et al, 2001)
(CRF), adopted by (Aramaki and Abekawa, 2009;
Shishtla et al, 2009). With only a few exceptions,
most implementations are based on approaches
that are language-independent. Indeed, many of
the participants fielded their systems on multiple
languages, as can be seen from Table 3.
We also note that combination of several differ-
ent models via re-ranking of their outputs (CRF,
Maximum Entropy Model, Margin Infused Re-
laxed Algorithm) proves to be very successful (Oh
et al, 2009); their system (reported as Team ID
6) produced the best or second-best transliteration
performance consistently across all metrics, in all
tasks, except Japanese back-transliteration. Exam-
ples of other model combinations are (Das et al,
2009).
At least two teams (reported as Team IDs 14
and 27) incorporate language origin detection in
their system (Bose and Sarkar, 2009; Khapra and
Bhattacharyya, 2009). The Indian language cor-
pora contains names of both English and Indic ori-
gin. Khapra and Bhattacharyya (2009) demon-
strate how much the transliteration performance
can be improved when language of origin detec-
1To maintain anonymity, papers of the teams that submit-
ted anonymous results are not cited in this report.
tion is employed, followed by a language-specific
transliteration model for decoding.
Some systems merit specific mention as they
adopt are rather unique approaches. Jiampoja-
marn et al (2009) propose DirectTL discrimina-
tive sequence prediction model that is language-
independent (reported as Team ID 7). Their
transliteration accuracy is among the highest in
several tasks (EnCh, EnHi and EnRu). Zelenko
(2009) present an approach to the transliteration
problem based on Minimum Description Length
(MDL) principle. Freitag and Wang (2009) ap-
proach the problem of transliteration with bidirec-
tional perceptron edit models.
Finally, in Figure 4 we present a plot where
each point represents a standard run by a system,
with different tasks marked with specific shape
and colour. This plot gives a bird-eye-view of
the system performances across two most uncorre-
lated evaluation metrics, namely accuracy in top-1
(ACC) and Mean F -score. Not surprisingly, we
notice very high performance in terms of F -score
for English to Russian transliteration task, likely
because Russian orthography follows pronuncia-
tion very closely, except for characters like soft
and hard signs that can hardly be recovered from
English words.
We also observe that Japanese back-
transliteration has proven to be much harder
than other (forward-transliteration) tasks. In
general, we note that a well-performing translit-
eration system performs well across all metrics.
We are curious about the correlation between
different metrics, and the results (specifically,
the Spearman?s rank correlation coefficient) are
presented below:
? Accuracy in top-1 vs. F -score: 0.40
? Accuracy in top-1 vs. MRR: 0.97
? Accuracy in top-1 vs. MAPref : 0.997
6
? Accuracy in top-1 vs. MAP10: 0.89
? Accuracy in top-1 vs. MAPsys: 0.80
We find that F -score is the most uncorrelated met-
ric: the Spearman?s rank correlation coefficient
between F -score and accuracy in top-1 is 0.40 and
between F -score and MRR it is 0.44. This is likely
because all metrics, except for F -score, are based
on word accuracy, while F -score is based on word
similarity allowing non-matching words to have
scores well above 0.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Accuracy in top-10
0.10.2
0.30.4
0.50.6
0.70.8
0.91
F-score English to ChineseEnglish to HindiEnglish to TamilEnglish to KannadaEnglish to RussianEnglish to KoreanEnglish to JapaneseJapanese transliterated to Japanese Kanji
Figure 4: Accuracy in top-1 vs. F -score for dif-
ferent tasks.
5.2 Non-standard runs
For the non-standard runs there exist no restric-
tions for the teams on the use of more data or other
linguistic resources. The purpose of non-standard
runs is to see how accurate personal name translit-
eration can be, for a given language pair. The ap-
proaches used in non-standard runs are typical and
may be summarised as follows:
? Dictionary lookup.
? Pronunciation dictionaries to convert words
to their phonetic transcription.
? Additional corpora for training and dictio-
nary lookup, such as LDC English-Chinese
named entity list LDC2005T34 (Linguistic
Data Consortium, 2005).
? Web search, and in particular, Wikipedia
search. First, transliteration candidates are
generated. Then a Web search is performed
to see if any of the candidates appear in the
search results. Based on the results, the can-
didates are re-ranked.
The results are shown in Tables 16?19. For En-
glish to Chinese and English to Russian transliter-
ation tasks the accuracy in top-1 can go as high as
0.909 and 0.955 respectively when Web search is
used to aid transliteration.
5.3 Post-evaluation
Two participants have found a bug in their system
implementation and re-evaluated the results after
the deadline. Their results are marked specifically
in Tables 4?8 and 16.
6 Process Analysis and Fine-tuning
In this section we highlight some of the sugges-
tions and feedback that we have received from the
participants during the course of this shared task.
While a few of them have been implemented in the
current edition, many of these may be considered
in the future editions of the shared task.
More or different languages There is quite a
bit of interest in enhancing the list of language
pairs short-listed. While we are constrained (in
this edition) due to the availability of manually
verified data, certainly more languages will be in-
cluded in the future editions, as some specific data
have already been promised for future editions.
Bidirectional transliteration Many partic-
ipants express interest in transliterations into
English; and this reflexive task will be added in
the future editions. We believe it will encourage
more participation as it will be easy to read and
verify system output in English for those teams
not familiar with the non-English side of the
language.
Forward vs. backward transliteration There
is quite a bit of interest expressed in specifically
separating forward and backward transliteration
tasks. However, such separation requires specific
corpora with known origin for each name pair, and
clearly we are constrained by the availability of
corpora. When corpora is available, the task may
be designated explicitly in future editions.
Number of standard runs The number of stan-
dard runs that may be submitted may be increased
in the future editions, as many participants would
like to submit many standard runs, trained with
different parameters.
7
Errors in training and development corpora
While we have taken all precautions in acquiring
and creating the corpora, some errors still remain.
We thank those who have sent us the errata. How-
ever, since the affected part is less than 0.5% of
the data, we believe that the effect on final results
is minimal. The errata will be made available to
all participants.
7 Conclusions and Future Plans
We are pleased to report a comprehensive cal-
ibration and baselining of machine translitera-
tion apporaches as most state-of-the-art machine
transliteration techniques are represented in the
shared task. The most popular techniques such as
Phrase-Based Machine Transliteration (Koehn et
al., 2003), and Conditional Random Fields (Laf-
ferty et al, 2001) are inspired by recent progress in
machine translation. As the standard runs are lim-
ited by the use of corpus, most of the systems are
implemented under the direct orthographic map-
ping (DOM) framework (Li et al, 2004). While
the standard runs allow us to conduct meaning-
ful comparison across different algorithms, we
recognise that the non-standard runs open up more
opportunities for exploiting larger linguistic cor-
pora. It is also noted that several systems have re-
ported improved performance over any previously
reported results on similar corpora.
NEWS 2009 Shared Task represents a suc-
cessful debut of a community effort in driving
machine transliteration techniques forward. The
overwhelming responses in the first shared task
also warrant continuation of such an effort in fu-
ture ACL or IJCNLP events.
Acknowledgements
The organisers of the NEWS 2009 Shared Task
would like to thank the Institute for Infocomm Re-
search (Singapore), Microsoft Research India and
CJK Institute (Japan) for providing the corpora
and technical support. Without those, the Shared
Task would not be possible. We thank those par-
ticipants who identified errors in the data and sent
us the errata. We want to thank Monojit Choud-
hury for his contribution to metrics defined for the
shared task. We also want to thank the members
of programme committee for their invaluable com-
ments that improve the quality of the shared task
papers. Finally, we wish to thank all the partici-
pants for their active participation that have made
this first machine transliteration shared task a com-
prehensive one.
8
References
Eiji Aramaki and Takeshi Abekawa. 2009. Fast de-
coding and easy implementation: Transliteration as
a sequential labeling. In Proc. ACL/IJCNLP Named
Entities Workshop Shared Task.
Dipankar Bose and Sudeshna Sarkar. 2009. Learn-
ing multi character alignment rules and classifica-
tion of training data for transliteration. In Proc.
ACL/IJCNLP Named Entities Workshop Shared
Task.
Manoj Kumar Chinnakotla and Om P. Damani. 2009.
Experiences with English-Hindi, English-Tamil and
English-Kannada transliteration tasks at NEWS
2009. In Proc. ACL/IJCNLP Named Entities Work-
shop Shared Task.
CJKI. 2009. CJK Institute. http://www.cjk.org/.
Amitava Das, Asif Ekbal, Tapabrata Mondal, and
Sivaji Bandyopadhyay. 2009. English to Hindi
machine transliteration system at NEWS 2009.
In Proc. ACL/IJCNLP Named Entities Workshop
Shared Task.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Dayne Freitag and Zhiqiang Wang. 2009. Name
transliteration with bidirectional perceptron edit
models. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Yoav Goldberg and Michael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar
Srivastava, Sudip Kumar Naskar, and Andy Way.
2009. English-Hindi transliteration using context-
informed PB-SMT. In Proc. ACL/IJCNLP Named
Entities Workshop Shared Task.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Mitesh Khapra and Pushpak Bhattacharyya. 2009. Im-
proving transliteration accuracy using word-origin
detection and lexicon lookup. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Int?l.
Conf. Machine Learning, pages 282?289.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Sara Noeman. 2009. Language independent translit-
eration system using phrase based SMT approach
on substring. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine transliteration with target-
language grapheme and phoneme: Multi-engine
transliteration approach. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
Taraka Rama and Karthik Gali. 2009. Modeling ma-
chine transliteration as a phrase based statistical ma-
chine translation problem. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
9
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
Praneeth Shishtla, V Surya Ganesh, S Sethurama-
lingam, and Vasudeva Varma. 2009. A language-
independent transliteration schema using character
aligned models. In Proc. ACL/IJCNLP Named Enti-
ties Workshop Shared Task.
Yan Song. 2009. Name entities transliteration via
improved statistical translation on character-level
chunks. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
Dmitry Zelenko. 2009. Combining MDL translitera-
tion training with discriminative modeling. In Proc.
ACL/IJCNLP Named Entities Workshop Shared
Task.
10
Team ID Organisation English to
Hindi
English to
Tamil
English to
Kannada
English to
Russian
English to
Chinese
English
to Ko-
rean
English
to
Japanese
Katakana
Japanese
translit-
erated to
Japanese
Kanji
EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
1 IIT Bombay x x x
2 Institution of Computational
Linguistics Peking Univer-
sity
x
3 University of Tokyo x x x x x x x
4? University of Illinois,
Urbana-Champaign
x x
5 IIT Bombay x x
6 NICT x x x x x x x x
7 University of Alberta x x x x x x
8 x x x x x x x x
9 x x x x x x x x
10 Johns Hopkins University x x x x x
11 x x x
12 x x
13 Jadavpur University x
14 IIIT Hyderabad x
15 x x x
16? ARL-CACI x
17 x x x x x x x x
18 x
19? Chaoyang University of
Technology
x
20 Pondicherry University x x x
21 Microsoft Research x x
22 SRI International x x x x x
23 IBM Cairo TDC x x
24 SRA x x x x x x x x
25 IIT Kharagpur x x x
26 Institute of Software Chinese
Academy of Sciences
x
27 x
28 George Washington Univer-
sity
x
29? x
30 Dublin City University x
31 IIIT x x x x x
Table 3: Participation of teams in different tasks. ?Participants without a system paper.
11
?
??
??
??
??
??
??
??
??
??
?
? ? ?? ?? ? ? ?? ?? ?? ?? ? ?? ?? ?? ?? ?? ? ? ?? ?? ??
???
????
??
Site?ID
(a) English to Hindi
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ? ?? ?? ? ? ?? ? ?? ? ?? ??
???
????
??
Site?ID
(b) English to Kannada
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ?? ? ?? ?? ? ? ?? ? ?? ??
???
????
??
Site?ID
(c) English to Tamil
???
????
?? ??
????
????
?
? ? ?? ?? ? ?? ?? ? ?? ? ? ?? ??
???
????
??
Site?ID
(d) English to Russian
???
????
?? ??
????
????
?
? ? ?? ? ? ?? ? ?? ?? ? ? ?? ?? ?? ?? ?? ?? ??
???
????
??
Site?ID
(e) English to Chinese
?
??
??
??
??
??
??
??
??
?? ? ?? ?? ? ? ? ?
???
????
??
Site?ID
(f) English to Korean
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ? ?? ? ? ?? ?? ?
???
????
??
Site?ID
(g) English to Japanese Katakana
?
??
??
??
??
??
??
??
??
??
?? ?? ? ? ? ? ??
???
????
??
Site?ID
(h) Japanese transliterated to Japanese Kanji
Figure 2: Accuracy in top-1, F -score and MRR for standard runs.
12
?
??
??
??
??
??
??
? ? ?? ?? ? ? ?? ?? ?? ?? ? ?? ?? ?? ?? ?? ? ? ?? ?? ??
?????
????
?????
Site?ID
(a) English to Hindi
?
??
??
??
??
??
??
? ?? ?? ? ?? ?? ? ? ?? ? ?? ? ?? ??
?????
????
?????
Site?ID
(b) English to Kannada
??
??
??
??
??
??
??
? ?? ?? ?? ? ?? ?? ? ? ?? ? ?? ??
?????
????
?????
Site?ID
(c) English to Tamil
?
??
??
??
??
??
??
? ? ?? ?? ? ?? ?? ? ?? ? ? ?? ??
?????
????
?????
Site?ID
(d) English to Russian
?
??
??
??
??
??
??
??
??
? ? ?? ? ? ?? ? ?? ?? ? ? ?? ?? ?? ?? ?? ?? ??
?????
????
?????
Site?ID
(e) English to Chinese
?
??
??
??
??
??
??
?? ? ?? ?? ? ? ? ?
?????
????
?????
Site?ID
(f) English to Korean
?
??
??
??
??
??
??
? ?? ?? ? ?? ? ? ?? ?? ?
?????
????
?????
Site?ID
(g) English to Japanese Katakana
?
??
??
??
??
??
??
?? ?? ? ? ? ? ??
?????
????
?????
Site?ID
(h) Japanese transliterated to Japanese Kanji
Figure 3: MAPref , MAP10 and MAPsys scores for standard runs.
13
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.498 0.890 0.603 0.488 0.195 0.195 University of Alberta
6 0.483 0.892 0.607 0.477 0.202 0.202 NICT
13 0.471 0.861 0.519 0.463 0.162 0.383 Jadavpur University
14 0.463 0.876 0.573 0.454 0.201 0.201 IIIT Hyderabad
8 0.462 0.876 0.576 0.454 0.189 0.189
1 0.423 0.863 0.544 0.417 0.179 0.202 IIT Bombay
11 0.418 0.879 0.546 0.412 0.183 0.240
21 0.418 0.864 0.522 0.409 0.170 0.170 Microsoft Research
17 0.415 0.858 0.505 0.406 0.164 0.168
24 0.409 0.864 0.527 0.402 0.174 0.176 SRA
5 0.409 0.881 0.546 0.400 0.184 0.184 IIT Bombay
31 0.407 0.877 0.544 0.402 0.195 0.195 IIIT
16 0.406 0.863 0.514 0.397 0.170 0.280 ARL-CACI
30 0.399 0.863 0.488 0.392 0.157 0.157 Dublin City University
10 0.398 0.855 0.515 0.389 0.170 0.170 Johns Hopkins University
25 0.366 0.854 0.493 0.360 0.164 0.164 IIT Kharagpur
3 0.363 0.864 0.503 0.360 0.170 0.170 University of Tokyo
9 0.349 0.829 0.455 0.341 0.151 0.151
22 0.212 0.788 0.317 0.207 0.106 0.106 SRI International
29 0.053 0.664 0.089 0.053 0.037 0.037
20 0.004 0.012 0.004 0.004 0.001 0.004 Pondicherry University
21 0.466 0.881 0.567 0.457 0.183 0.183 Microsoft Research (post-evaluation)
22 0.465 0.886 0.567 0.458 0.185 0.185 SRI International (post-evaluation)
Table 4: Standard runs for English to Hindi task.
TeamID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.474 0.910 0.608 0.465 0.204 0.204 NICT
17 0.436 0.894 0.551 0.427 0.184 0.189
11 0.435 0.902 0.572 0.430 0.195 0.265
31 0.406 0.894 0.542 0.399 0.193 0.193 IIIT
1 0.405 0.892 0.542 0.397 0.181 0.184 IIT Bombay
25 0.404 0.883 0.539 0.398 0.182 0.182 IIT Kharagpur
24 0.374 0.880 0.512 0.369 0.174 0.174 SRA
3 0.365 0.884 0.504 0.360 0.172 0.172 University of Tokyo
8 0.361 0.883 0.510 0.354 0.174 0.174
10 0.327 0.870 0.458 0.317 0.156 0.156 Johns Hopkins University
9 0.316 0.848 0.451 0.307 0.154 0.154
22 0.141 0.760 0.256 0.139 0.090 0.090 SRI International
20 0.061 0.131 0.068 0.059 0.021 0.056 Pondicherry University
22 0.475 0.909 0.581 0.466 0.193 0.193 SRI International (post-evaluation)
Table 5: Standard runs for English to Tamil task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.398 0.880 0.526 0.391 0.178 0.178 NICT
17 0.370 0.867 0.499 0.362 0.170 0.175
11 0.363 0.870 0.482 0.355 0.164 0.218
1 0.360 0.861 0.479 0.351 0.161 0.164 IIT Bombay
31 0.350 0.864 0.482 0.344 0.175 0.175 IIIT
24 0.345 0.854 0.462 0.336 0.157 0.157 SRA
8 0.343 0.855 0.458 0.334 0.155 0.155
5 0.335 0.859 0.453 0.327 0.154 0.154 IIT Bombay
25 0.335 0.856 0.457 0.328 0.154 0.154 IIT Kharagpur
3 0.324 0.856 0.438 0.315 0.148 0.148 University of Tokyo
10 0.235 0.817 0.353 0.229 0.121 0.121 Johns Hopkins University
9 0.177 0.799 0.307 0.178 0.109 0.109
22 0.091 0.735 0.180 0.090 0.064 0.064 SRI International
20 0.004 0.009 0.004 0.004 0.001 0.004 Pondicherry University
22 0.396 0.874 0.494 0.385 0.161 0.161 SRI International (post-evaluation)
Table 6: Standard runs for English to Kannada task.
14
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.613 0.928 0.696 0.613 0.212 0.212 University of Alberta
6 0.605 0.926 0.701 0.605 0.215 0.215 NICT
17 0.597 0.925 0.691 0.597 0.212 0.255
24 0.566 0.919 0.662 0.566 0.203 0.216 SRA
8 0.564 0.917 0.677 0.564 0.210 0.210
31 0.548 0.916 0.640 0.548 0.210 0.210 IIIT
23 0.545 0.917 0.596 0.545 0.286 0.299 IBM Cairo TDC
3 0.531 0.912 0.635 0.531 0.219 0.219 University of Tokyo
10 0.506 0.901 0.609 0.506 0.204 0.204 Johns Hopkins University
4 0.504 0.909 0.618 0.504 0.193 0.193 University of Illinois, Urbana-Champaign
9 0.500 0.906 0.613 0.500 0.192 0.192
22 0.364 0.876 0.440 0.364 0.136 0.136 SRI International
27 0.354 0.869 0.394 0.354 0.134 0.134
22 0.609 0.928 0.686 0.609 0.209 0.209 SRI International (post-evaluation)
Table 7: Standard runs for English to Russian task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.731 0.895 0.812 0.731 0.246 0.246 NICT
7 0.717 0.890 0.785 0.717 0.237 0.237 University of Alberta
15 0.713 0.883 0.794 0.713 0.241 0.241
8 0.666 0.864 0.765 0.666 0.234 0.234
2 0.652 0.858 0.755 0.652 0.232 0.232 Institution of Computational Linguistics Peking
University China
17 0.646 0.867 0.747 0.646 0.229 0.229
9 0.643 0.854 0.745 0.643 0.228 0.229
18 0.621 0.852 0.718 0.621 0.220 0.222
24 0.619 0.847 0.711 0.619 0.217 0.217 SRA
4 0.607 0.840 0.695 0.607 0.213 0.213 University of Illinois, Urbana-Champaign
3 0.580 0.826 0.653 0.580 0.199 0.199 University of Tokyo
26 0.498 0.786 0.603 0.498 0.187 0.189 Institute of Software Chinese Academy of Sci-
ences
31 0.493 0.804 0.600 0.493 0.192 0.192 IIIT
22 0.468 0.768 0.546 0.468 0.168 0.168 SRI International
28 0.456 0.763 0.587 0.456 0.185 0.185 George Washington University
10 0.450 0.755 0.514 0.450 0.157 0.166 Johns Hopkins University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.199 0.606 0.229 0.199 0.070 0.070 Chaoyang University of Technology
22 0.671 0.872 0.725 0.672 0.218 0.218 SRI International (post-evaluation)
Table 8: Standard runs for English to Chinese task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.476 0.742 0.596 0.477 0.187 0.199
6 0.473 0.740 0.584 0.473 0.182 0.182 NICT
12 0.451 0.720 0.576 0.451 0.181 0.181
24 0.413 0.702 0.524 0.412 0.165 0.165 SRA
7 0.387 0.693 0.469 0.387 0.146 0.146 University of Alberta
8 0.362 0.662 0.460 0.362 0.144 0.144
9 0.332 0.648 0.425 0.331 0.134 0.135
3 0.170 0.512 0.218 0.170 0.069 0.069 University of Tokyo
Table 9: Standard runs for English to Korean task.
15
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.537 0.858 0.657 0.529 0.223 0.223 NICT
15 0.510 0.838 0.624 0.498 0.209 0.209
17 0.503 0.843 0.627 0.491 0.212 0.212
7 0.500 0.847 0.604 0.487 0.199 0.199 University of Alberta
21 0.465 0.827 0.559 0.454 0.183 0.183 Microsoft Research
3 0.457 0.828 0.576 0.445 0.194 0.194 University of Tokyo
8 0.449 0.816 0.571 0.436 0.192 0.192
24 0.420 0.807 0.541 0.410 0.182 0.184 SRA
12 0.408 0.808 0.537 0.398 0.182 0.182
9 0.406 0.800 0.529 0.393 0.180 0.180
21 0.469 0.834 0.567 0.454 0.186 0.186 Microsoft Research (post-evaluation)
Table 10: Standard runs for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
15 0.627 0.763 0.706 0.605 0.292 0.292
17 0.606 0.749 0.695 0.586 0.287 0.288
8 0.596 0.741 0.687 0.575 0.282 0.282
7 0.560 0.730 0.644 0.525 0.244 0.244 University of Alberta
9 0.555 0.708 0.653 0.538 0.261 0.261
6 0.532 0.716 0.583 0.485 0.214 0.218 NICT
24 0.509 0.675 0.600 0.491 0.226 0.226 SRA
Table 11: Standard runs for Japanese Transliterated to Japanese Kanji task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.509 0.893 0.610 0.498 0.198 0.198 University of Alberta
1 0.487 0.873 0.594 0.481 0.195 0.229 IIT Bombay
6 0.475 0.893 0.601 0.469 0.200 0.200 NICT
6 0.469 0.884 0.581 0.464 0.192 0.193 NICT
6 0.455 0.888 0.575 0.448 0.191 0.191 NICT
5 0.448 0.885 0.570 0.439 0.190 0.190 IIT Bombay
6 0.443 0.879 0.555 0.437 0.184 0.191 NICT
17 0.424 0.862 0.513 0.415 0.166 0.174
30 0.421 0.864 0.519 0.415 0.171 0.171 Dublin City University
30 0.420 0.867 0.519 0.413 0.170 0.170 Dublin City University
30 0.419 0.868 0.464 0.419 0.338 0.338 Dublin City University
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
30 0.407 0.856 0.507 0.399 0.168 0.168 Dublin City University
16 0.400 0.864 0.516 0.391 0.171 0.212 ARL-CACI
13 0.389 0.831 0.487 0.385 0.160 0.328 Jadavpur University
13 0.384 0.828 0.485 0.380 0.160 0.325 Jadavpur University
16 0.273 0.796 0.358 0.266 0.119 0.193 ARL-CACI
Table 12: Non-standard runs for English to Hindi task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.478 0.910 0.606 0.472 0.203 0.203 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.453 0.907 0.584 0.446 0.196 0.196 NICT
17 0.437 0.894 0.555 0.426 0.185 0.193
Table 13: Non-standard runs for English to Tamil task.
16
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.399 0.881 0.522 0.391 0.176 0.176 NICT
6 0.386 0.877 0.503 0.379 0.169 0.169 NICT
6 0.380 0.869 0.488 0.370 0.163 0.163 NICT
17 0.374 0.868 0.502 0.366 0.170 0.176
6 0.373 0.869 0.485 0.362 0.162 0.168 NICT
Table 14: Non-standard runs for English to Kannada task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.955 0.989 0.966 0.955 0.284 0.504
17 0.609 0.928 0.701 0.609 0.214 0.263
7 0.608 0.927 0.694 0.608 0.212 0.212 University of Alberta
7 0.607 0.927 0.690 0.607 0.211 0.211 University of Alberta
6 0.600 0.927 0.634 0.600 0.189 0.189 NICT
6 0.600 0.926 0.699 0.600 0.214 0.214 NICT
7 0.591 0.928 0.679 0.591 0.208 0.208 University of Alberta
6 0.561 0.918 0.595 0.561 0.178 0.182 NICT
6 0.557 0.920 0.596 0.557 0.179 0.233 NICT
23 0.545 0.917 0.618 0.545 0.188 0.206 IBM Cairo TDC
23 0.524 0.913 0.602 0.524 0.184 0.203 IBM Cairo TDC
23 0.524 0.913 0.579 0.524 0.277 0.291 IBM Cairo TDC
4 0.496 0.908 0.613 0.496 0.191 0.191 University of Illinois, Urbana-Champaign
27 0.338 0.872 0.408 0.338 0.128 0.128
27 0.293 0.845 0.325 0.293 0.099 0.099
27 0.162 0.849 0.298 0.162 0.188 0.188
Table 15: Non-standard runs for English to Russian task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.909 0.960 0.933 0.909 0.276 0.276
7 0.746 0.900 0.814 0.746 0.245 0.245 University of Alberta
7 0.734 0.895 0.807 0.734 0.244 0.244 University of Alberta
7 0.732 0.895 0.803 0.732 0.242 0.242 University of Alberta
6 0.731 0.894 0.812 0.731 0.246 0.246 NICT
6 0.715 0.890 0.741 0.715 0.220 0.231 NICT
6 0.699 0.884 0.729 0.699 0.216 0.232 NICT
6 0.684 0.873 0.711 0.684 0.211 0.211 NICT
22 0.663 0.867 0.754 0.663 0.230 0.230 SRI International
17 0.658 0.865 0.752 0.658 0.230 0.230
18 0.587 0.834 0.665 0.587 0.203 0.330
26 0.500 0.786 0.607 0.500 0.189 0.191 Institute of Software Chinese Academy of Sciences
22 0.487 0.787 0.622 0.487 0.196 0.196 SRI International
28 0.462 0.764 0.564 0.462 0.175 0.175 George Washington University
28 0.458 0.763 0.602 0.458 0.191 0.191 George Washington University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.279 0.668 0.351 0.279 0.110 0.110 Chaoyang University of Technology
28 0.058 0.353 0.269 0.058 0.101 0.101 George Washington University
28 0.050 0.359 0.260 0.050 0.098 0.098 George Washington University
4 0.001 0.249 0.001 0.001 0.000 0.000 University of Illinois, Urbana-Champaign
22 0.674 0.873 0.763 0.674 0.232 0.232 SRI International (post-evaluation)
22 0.500 0.793 0.636 0.500 0.200 0.200 SRI International (post-evaluation)
Table 16: Non-standard runs for English to Chinese task.
17
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.794 0.894 0.836 0.793 0.249 0.323
12 0.785 0.887 0.840 0.785 0.252 0.441
12 0.784 0.889 0.840 0.784 0.252 0.484
12 0.781 0.885 0.839 0.781 0.252 0.460
12 0.740 0.868 0.806 0.740 0.243 0.243
6 0.461 0.737 0.576 0.461 0.180 0.180 NICT
6 0.457 0.734 0.506 0.457 0.153 0.153 NICT
6 0.447 0.718 0.493 0.447 0.149 0.149 NICT
6 0.369 0.679 0.406 0.369 0.123 0.123 NICT
Table 17: Non-standard runs for English to Korean task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.535 0.858 0.656 0.526 0.222 0.222 NICT
6 0.517 0.850 0.567 0.495 0.177 0.188 NICT
6 0.513 0.854 0.567 0.495 0.178 0.178 NICT
7 0.510 0.848 0.614 0.496 0.202 0.202 University of Alberta
6 0.500 0.842 0.547 0.480 0.170 0.196 NICT
Table 18: Non-standard runs for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.717 0.818 0.784 0.691 0.319 0.319
17 0.703 0.805 0.768 0.673 0.311 0.311
17 0.698 0.805 0.774 0.676 0.317 0.317
17 0.681 0.790 0.755 0.657 0.308 0.309
6 0.525 0.713 0.607 0.503 0.248 0.249 NICT
6 0.525 0.712 0.606 0.502 0.248 0.248 NICT
6 0.523 0.712 0.572 0.479 0.211 0.213 NICT
6 0.517 0.705 0.603 0.496 0.248 0.249 NICT
Table 19: Non-standard runs for Japanese Transliterated to Japanese Kanji task.
18
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 19?26,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Whitepaper of NEWS 2009 Machine Transliteration Shared Task?
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of the
shared task in the NEWS 2009 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be re-
leased, on which the participants are expected to
produce a ranked list of transliteration candidates
in another language (i.e. n-best transliterations),
and this will be evaluated using common metrics.
For every language pair the participants must sub-
mit one run that uses only the data provided by the
NEWS workshop organisers in a given language
pair (designated as ?standard? runs). Users may
submit more runs (?non-standard?) for each lan-
guage pair that uses other data than those provided
by the NEWS 2009 workshop; such runs would be
evaluated and reported separately.
?http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
2 Important Dates
Research paper submission deadline 1 May 2009
Shared task
Registration opens 16 Feb 2009
Registration closes 9 Apr 2009
Release Training/Development Data 16 Feb 2009
Release Test Data 10 Apr 2009
Results Submission Due 14 Apr 2009
Results Announcement 29 Apr 2009
Task (short) Papers Due 3 May 2009
For all submissions
Acceptance Notification 1 Jun 2009
Camera-Ready Copy Deadline 7 Jun 2009
Workshop Date 7 Aug 2009
3 Participation
1. Registration (16 Feb 2009)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (16 Feb 2009)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
3. Evaluation Script (16 Mar 2009)
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
19
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (10 April 2009)
(a) The test data would be released on 10
Apr 2009, and the participants have a
maximum of 4 days to submit their re-
sults in the expected format.
(b) Only 1 ?standard? run must be submit-
ted from every group on a given lan-
guage pair; more ?non-standard? runs (0
to 4) may be submitted. In total, maxi-
mum 5 runs (1 ?standard? run plus up to
4 ?non-standard? runs) can be submit-
ted from each group on a registered lan-
guage pair.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (29 April 2009)
(a) On 29 April 2009, the evaluation results
would be announced and will be made
available on the Workshop website.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Further, all participants should agree not
to reveal identities of other participants
in any of their publications unless you
get permission from the other respective
participants. If the participants want
to remain anonymous in published
results, they should inform the or-
ganisers (mzhang@i2r.a-star.edu.sg,
a.kumaran@microsoft.com), at the time
of registration. Note that the results of
their systems would still be published,
but with the participant identities
masked. As a result, in this case, your
organisation name will still appear in
the web site as one of participants, but it
is not linked explicitly with your results.
6. Short Papers on Task (3 May 2009)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) All system short papers will be included
in the proceedings. Selected short pa-
pers will be presented orally in the
NEWS 2009 workshop. Reviewers?
comments for all system short papers
and the acceptance notification for the
system short papers for oral presentation
would be announced on 1 June 2009 to-
gether with that of other papers.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review
will be managed electronically
through https://www.softconf.com/acl-
ijcnlp09/NEWS/.
4 Languages Involved
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1.
20
Source language Target language Data Owner Approx. Data Size Task ID
English Chinese Institute for Infocomm Research 30K EnCh
English Japanese Katakana CJK Institute 25K EnJa
English Korean Hangul CJK Institute 7K EnKo
Japanese name (in English) Japanese Kanji CJK Institute 20K JnJk
English Hindi Microsoft Research India 15K EnHi
English Tamil Microsoft Research India 15K EnTa
English Kannada Microsoft Research India 15K EnKa
English Russian Microsoft Research India 10K EnRu
Table 1: Source and target languages for the shared task on transliteration.
The names given in the training sets for Chi-
nese, Japanese and Korean languages are Western
names and their CJK transliterations; the Japanese
Name (in English)? Japanese Kanji data set con-
sists only of native Japanese names. The Indic data
set (Hindi, Tamil, Kannada) consists of a mix of
Indian and Western names.
English? Chinese
Timothy????
English? Japanese Katakana
Harrington??????
English? Korean Hangul
Bennett ? ??
Japanese name in English? Japanese Kanji
Akihiro???
English? Hindi
San Francisco ? ????????????????
English? Tamil
London ? ??????
English? Kannada
Tokyo ? ??????
English? Russian
Moscow ? ??????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 5K ? 40K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K ? 2K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 1K ? 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; Kumaran and Kellner, 2007; MSRI,
2009; CJKI, 2009). NEWS 2009 will pro-
vide the contact details of each individual
database. The data would be provided in Uni-
code UTF-8 encoding, in XML format; the
results are expected to be submitted in XML
format. The XML formats will be announced
at the workshop website.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
21
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. We expect that the participants to use only the
data (parallel names) provided by the Shared
Task for transliteration task for a ?standard?
run to ensure a fair evaluation. One such run
(using only the data provided by the shared
task) is mandatory for all participants for a
given language pair that they participate in.
5. If more data (either parallel names data or
monolingual data) were used, then all such
runs using extra data must be marked as
?non-standard?. For such ?non-standard?
runs, it is required to disclose the size and
characteristics of the data used in the system
paper.
6. A participant may submit a maximum of 5
runs for a given language pair (including the
mandatory 1 ?standard? run).
6 Paper Format
Paper submissions to NEWS 2009 should follow
the ACL-IJCNLP-2009 paper submission policy,
including paper format, blind review policy and ti-
tle and author format convention. Full papers (re-
search paper) are in two-column format without
exceeding eight (8) pages of content plus one extra
page for references and short papers (task paper)
are also in two-column format without exceeding
four (4) pages, including references. Submission
must conform to the official ACL-IJCNLP-2009
style guidelines. For details, please refer to the
website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 6 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
2http://www.acl-ijcnlp-2009.org/main/authors/stylefiles/index.html
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
22
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
5. MAP10 measures the precision in the 10-best
candidates for i-th source name provided by the
candidate system. In general, the higher MAP10
is, the better is the quality of the transliteration
system in capturing the multiple references. Note
that the number of reference transliterations may
be more or less than 10. If the number of refer-
ence transliterations is below 10, then MAP10 can
never be equal to 1. Only if the number of ref-
erence transliterations for every source word is at
least 10, then MAP10 could possibly be equal to 1.
MAP10 =
1
N
N?
i=1
1
10
(
10?
k=1
num(i, k)
)
(10)
Note that in general MAPm measures the ?good-
ness in m-best? candidate list. We use m = 10
because we have asked the systems to produce up
to 10 candidates for every source name in the test
set.
6. MAPsys Measures the precision in the top
Ki-best candidates produced by the system for i-
th source name, for which ni reference translit-
erations are available. This measure allows the
systems to produce variable number of translitera-
tions, based on their confidence in identifying and
producing correct transliterations. If all of the ni
references are produced in the top-ni candidates
(that is, Ki = ni, and all of them are correct), then
the MAPsys is 1.
MAPsys =
1
N
N?
i=1
1
Ki
(
Ki?
k=1
num(i, k)
)
(11)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
Dr. A. Kumaran
Microsoft Research India
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
Mr. Kurt Easterwood
The CJK Dictionary Institute (CJK Data)
Komine Building (3rd & 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
akurt@cjki.org
23
References
CJKI. 2009. CJK Institute. http://www.cjk.org/.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Appendix A: Training/Development Data
? File Naming Conventions:
NEWS09 train XXYY nnnn.xml
NEWS09 dev XXYY nnnn.xml
NEWS09 test XXYY nnnn.xml
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
Appendix B: Submission of Results
? File Naming Conventions:
NEWS09 result XXYY gggg nn descr.xml
? XX: Source Language
? YY: Target Language
? gggg: Group ID
? nn: run ID. Note that run ID ?1? stands for ?stan-
dard? run where only the provided data are al-
lowed to be used. Run ID ?2?5? means ?non-
standard? run where additional data can be used.
? descr: Description of the run.
? File formats:
All data will be made available in XML formats (Fig-
ure 2).
? Data Encoding Formats:
The results are expected to be submitted in UTF-8 en-
coded files without byte-order mark only, and in the
XML format specified.
24
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2009-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=?1?>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=?2?>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2009 Train EnHi 25K.xml
25
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2009 EnHi TUniv 01 StdRunHMMBased.xml
26
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1238?1247, Dublin, Ireland, August 23-29 2014.
Online Gaming for Crowd-sourcing Phrase-equivalents 
A. Kumaran 
Microsoft Research 
Bangalore, India 
a.kumaran@microsoft.com 
Melissa Densmore 
University of Cape Town 
Cape Town, South Africa 
mdensmore@acm.org 
Shaishav Kumar 
Microsoft Research 
Bangalore, India 
v-shaisk@microsoft.com 
  
Abstract 
We propose the use of a game with a purpose (GWAP) to facilitate crowd-sourcing of phrase-
equivalents, as an alternative to expert or paid crowd-sourcing. Doodling is an online multi-
player game, in which one player (drawer), draws pictures on a shared board to get the other 
players (guessers) to guess the meaning behind an assigned phrase.  In this paper we describe 
the system and results from several experiments intended to improve the quality of information 
generated by the play. In addition, we describe the mechanism by which we take candidate 
phrases generated during the games and filter out true phrase equivalents. We expect that, at 
scale, this game will be more cost-efficient than paid mechanisms for a similar task, and demon-
strate this by comparing the productivity of an hour of game play to an equivalent crowd-sourced 
Amazon Mechanical Turk task to produce phrase-equivalents over one week.  
1  Introduction 
While it is fairly well known when individual words have the same meaning, it is far more difficult to 
determine when phrases or even sentences carry the same basic idea.  While it might be possible to 
address this task with machine learning techniques, building a corpus of sentences from which to seed 
a database requires human intelligence.  We suggest a game with a purpose (GWAP) that will serve to 
generate phrases with similar meanings, while simultaneously providing meta-information about the 
quality of the match.  In this drawing game, called Doodling, individuals compete in groups to guess the 
meaning behind a given drawing that is being drawn by one designated drawer trying to convey a given 
phrase or a short sentence.  The designated drawer decides when a guessed phrase matches the source 
phrase. For example ?How far is the airport?? might match semantically ?What is the distance to the 
airport??  In addition, the drawer can indicate for each partial guess how close it is on a scale of 1-3 to 
help the guessers converge on phrases that will match the given phrase or sentence.  We then pass all of 
the guesses and annotations through an SVM classifier to automatically identify potential phrase-equiv-
alents. In this study we examine several techniques for using this system to generate high quality data 
while also making the game more enjoyable.  We measure the efficacy of each technique by comparing 
our results to a gold standard: using human evaluators to rate the phrase matches generated through the 
game manually.  We also compare Doodling to a paid crowdsourcing paradigm ? Amazon?s Mechanical 
Turk ? to source phrase equivalents for the same set of phrases, and we show that our approach might 
be cost effective for large scale sourcing of paraphrases of equivalent quality. 
 
2 Background 
In this section we define the problem we are trying to address, and discuss the various ways it has been 
approached in the past. 
2.1 Phrase-equivalents & Evaluation of Quality 
In this paper, we define phrase equivalents (PEs) as text elements ? phrases or short sentences ? that 
have same or similar semantic content, but with surface structure different from each other. PEs are 
similar to paraphrases, but broader in scope, inclusive of partial matches in meaning as well as complete 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
1238
paraphrases. PEs are useful for many NLP systems from simple language modelling and smoothing, to 
complex Machine Translation technology for generation of a surface form in the target language.  Most 
existing corpora are hand-created, and hence they tend to be small in size, and available only in limited 
languages and domains.  Other data driven approaches ? such as, creation of paraphrases using mono-
lingual machine translation (Quirk et al., 2004), mining inference rules from text corpora (Lin & Pantel, 
2001), or paraphrase extraction from parallel corpora (Dolan et al., 2004) (Barzilay & McKeown, 2001) 
? were shown to be effective, but such approaches require significant seed corpora which are available 
only in limited domains and languages.  In addition, the (Lin & Pantel, 2001) approach can generate 
equivalents using user defined patterns, and may not be appropriate for generating loosely related con-
ceptual paraphrases like the human generated ones that Doodling may generate.   
The criteria used for evaluating phrase equivalents differ vastly in research literature, ranging from 
conceptual equivalence (Barzilay & McKeown, 2001), to interchangeability (Ibrahim et al, 2003), to 
preservation of grammatical correctness and semantic equivalence (Callison-Burch, 2005), and the 
standard metric of BLEU score (Callison-Burch, 2005; Papineni et al., 2002).  In general, there is no 
accepted standard model for measuring quality, hence we adopted manual annotation by experts.   
2.2 Crowdsourcing & Games with a Purpose (GWAP) for Computational Linguistics  
Many flavors of crowdsourcing paradigms exist for the creation of language data.  From the for-pay 
model where the contribution is for monetary rewards (Callison-Burch, 2009; Irvine & Klementiev, 
2010; Chen & Dolan, 2011), to the for-recognition model, where the contribution is made for individu-
als? visibility in a community (e.g., SourceForge), and the common-good model, value is produced for 
the benefit of some community (Kumaran et al., 2009).  In this paper, we explore the for-fun model 
(Cooper et al., 2010; Law et al., 2007; Von Ahn & Dabbish, 2004; Von Ahn et al., 2006), in which data 
is a by-product of some gameplay, often referred to as ?Games with a Purpose? (Von Ahn & Dabbish, 
2008), which have been shown to be very successful in many domains.  
Specifically with respect to generation of paraphrases or phrase equivalents, (Chen & Dolan, 2011) 
present their paraphrase collection using video annotations, focusing primarily on viability of establish-
ing Mechanical Turk for providing paraphrases in a productive way. (Barzilay & McKeown, 2003) pos-
ited that multiple translations of a foreign text may be a naturally occurring source for paraphrases as 
each is authored by a different translator; our approach is analogous to this approach, though our source 
phrases/sentences are not from a foreign language. (Chklovski, 2005) presents an online paraphrase 
collection tool and studies the incentive model for responsible contributions by volunteers. Paraphrases 
generated by Doodling would be similar to paraphrases labelled under class ?Phrasal? and to a lesser 
extent class ?Elaboration? in (Chen & Dolan, 2011).     In our earlier work (Kumaran et al., 2012) we 
focused on a proof-of-concept methodology using a Pictionary-based approach for generation of para-
phrases.  In this paper, we expand our concept for generating phrase equivalents in scale inexpensively, 
using several game and UI/UX features, and also compare it with a realistic for-pay baseline using Me-
chanical Turk.  The power of our methodology is its self-verification mechanisms (by drawer annotating 
the response for convergence, and the final acceptance) that validates the generated paraphrases. 
 
3 Doodling as a Game 
In this section, we present the design elements and the game flow of the Doodling game. 
3.1 Game Design 
In the Doodling game, the games are played in rooms with one player (designated as the Drawer) 
sketches an assigned concept - as phrase or sentence - while other players in the game room (Guessers) 
attempt to guess the assigned concept from the drawing that is being replicated to all screens. The 
Guessers typically start guessing the words first (based on the concept that the Drawer starts sketching 
on the screen); while the game will automatically indicate exact partial matches (for example, ?Taxi? as 
a guess for the given phrase, ?Taxi Driver?), the drawer also has the ability to provide feedback using 
annotations.  The Drawer may annotate partial guesses as incorrect (red), on the right track (orange), or 
partially correct (green), to guide the convergence. All the guessers? guesses and the drawer?s annotation 
are broadcast to all the players in the room.  Such broadcasting provides a mechanism in which players 
1239
can build on the top of other?s guesses, gradually building up the phrase or the sentence.  At some point, 
if one of the guessers guess the right phrase exactly, the game is closed automatically.  In addition, if 
the drawer judges the guess as having the same meaning as the assigned concept (for example, ?Cabbie? 
for ?Taxi Driver?), he/she can end the round by marking the guess as correct, rewarding the guesser 
with game points.  If the timer runs out before a correct guess happens, then the game times out.  Figure 
1 shows the UI during the progress of a game (the given text element being ?taxi driver?). 
 
Figure 1: Doodling Game 
 
Our primary intuition is that the sketches provide a language-independent means of communication 
of concepts that is effectively employed for the generation of phrase equivalents. Thus, we leverage a 
fun drawing-guessing game to fulfill the linguistic purpose of generating phrase-equivalents.  An im-
portant aspect of making Doodling effective was to make it engaging to play. We underwent multiple 
user studies followed by changes to the game?s UI/UX. Earlier trials had revealed the need for additional 
feedback from the drawer, leading to the introduction of 3-stage annotations of guessed phrases.  From 
a usability standpoint, the UI and gestures were optimized for use with touchscreen capable devices, 
including of the use of swipe gestures for annotating incoming guesses. 
The Doodling game subscribes to the Inversion Problem (Von Ahn & Dabbish, 2008), where one of 
the players produces an output in the form of a sketch for a given input phrase. The other players attempt 
to guess the given input. The game may produce multiple surface forms of a single semantic intent that 
have a relationship similar to that of the input-output pair in the ?noisy-channel? model. 
3.2 Game Elements  
While the game dynamics promote the resolution of the underlying computational problem (i.e., the 
generation of phrase equivalents), we made certain modifications to the basic sketch-and-convey meta-
phor ? in the formation and constitution of the game rooms, in the assignment of roles to players in a 
round-robin fashion, and the drawer?s feedback using annotation, in exposing every player?s guess to 
the entire game room, and the winning strategy that encourages building on each other?s guesses ? in 
order to help the rounds finish successfully, converge faster, and be more competitive.  Above all, the 
game dynamics and the UI were designed to make Doodling enjoyable as a game.  
Roles: Users may join existing game rooms, or can create a new private game room after logging in 
to the Doodling portal.  In a game room, one of the users is assigned ? randomly ? the role of Drawer 
(D), and the others the role of Guessers (G).  At the end of a given game round, the role of drawer cycles 
among the game room participants.  All G?s both compete (the first guesser to guess right ? either fully 
or partially ? is rewarded), as well as collaborate (each builds on other?s guesses to build longer phrases 
for bigger rewards) in guessing the text element being conveyed by the D. 
1240
Game Round: Like the sketches, the individual guesses of a given G are broadcast to the entire room, 
along with any annotation from D on each of the guesses (red/orange/green). While the right guesses 
(either lexicographic match, or as judged by D) gives the game point to the specific G, the broadcast of 
guesses and feedbacks from D to the entire game room provides a transparent mechanism to help each 
player build on the guesses of the others.  The game round closes with exact reproduction of the source 
phrase by one of the G?s, or by D accepting a full semantic equivalent by double tapping a tile.  As an 
incentive for the role of the drawer, the D is also rewarded with some game points. 
Data: In our current experiments, we used standard phrases from a generic WikiTravel (http://wik-
itravel.org/en/wikitravel:phrasebook_template) tourism phrase book as input elements. The authors 
subjectively classified each text element as Easy or Hard, depending on the potential difficulty to express 
it as a sketch; though such annotation implies additional preparatory work, it may be well worth the 
investment as such tagged corpora forms the seed for many variations. We plan to add text elements in 
many domains (Celebrities, Movies and Idioms), to provide diversity to the players.   
 
Text Element Diff. Granularity 
Cheese Omelette Easy Phrase 
Museum of Modern Art Hard Phrase 
I would like a bowl of soup. Easy Sentence 
I am not feeling well! Hard Sentence 
Table 1: Sample of text elements used in the initial seed corpus 
In order to understand the dynamics of the game, and to improve the quality and quantity of the phrase 
equivalents generated in Doodling, we incorporated many features.   
Number of Players: The application supports 2-4 players per game room, to measure the effect of 
room size on convergence rate and the player enjoyment.  We hypothesize that those game rooms with 
more players will lead to better completion primarily due to higher productivity in phrase generation.   
Hints & Reminders: we provided hints to all guessers at the beginning of the game to prime then on 
what to expect about the guess phrase.  Hints are simple text elements, such as ?Short Phrase? or ?Hard 
Sentence?, etc. In addition, we also provided some reminders periodically for improving the game dy-
namics, especially for the new players, including a reminder to the drawer that they can accept non-
exact phrases with the same meaning by double-tapping on the guess tile.  Reminders appear on the 
screen, and fade away unobtrusively.  Some game rooms were provided the hints, while others are not, 
in order to measure how helpful the hints are for game completion.   
Soft Matches:  Exact lexicographic guesses (full or partial) are automatically rewarded by the game 
engine. However, as the primary mechanism for gathering paraphrases, soft matches were allowed and 
rewarded at the discretion of the drawer (either by the double-tap action that accepts a guess as a correct 
phrase equivalent, or by the swipe-right action that which indicates a potential partial match).  Yet, to 
discourage collusion or cheating, a reporting mechanism is provided: The final accepted guess along 
with the input text element are shown to all participants, to report any unsatisfactory acceptance.  
Metrics: For measuring the effectiveness of the Doodling game, we define many metrics ranging 
from completion statistics (completion rate and completion time), to quality by comparison with gold 
data (true positives as compared with user-annotated data, precision and accuracy of automatically clas-
sified data), to qualitative user feedback (fun factor).   
4 Doodling: Experimental Evaluation 
Doodling is an HTML5 app that is accessible from most devices - touchscreen laptop or tablets - and 
deployed in the cloud (http://doodle1.cloudapp.net/).  After deployment, we recruited volunteers (pri-
marily graduate students) to log in and play the game for one hour.  As the volunteers entered the game 
server, they were assigned to different game rooms; each room was instrumented for a specific config-
uration (game room size - between 2 and 4 players, and availability of hints and reminders). Each room 
was given the same set of 38 phrases in the same sequence, to keep the variability to a minimum.  After 
an hour, the games were closed and the players asked to fill in an online questionnaire.   
In these trials, the 14 volunteers played a total of 112 games, in different game rooms. Most players 
had previously been exposed to the sketch-and-convey metaphor through Pictionary-type games.   
1241
4.1 Quality of the Generated Data 
Basis for evaluation: We first extracted all of the text elements annotated as a potential match (green, 
orange, or winning) by the Drawer. Each of the three authors then independently classified according to 
the relevance of the match. The following five classes were used for annotating every annotated text 
element: EF (Exact Full Match), EP (Exact Partial Match), TF (True Full Match), TP (True Partial 
Match) or NM (Not a Match). Partial matches entailed guesses which captured some sub-element of the 
seed text, but not the entire meaning.  We then measured inter-annotator agreement of author?s annota-
tions using a Fleiss Kappa measure (Fleiss, 1971), which stood at 0.7424, indicating substantial agree-
ment among our annotations.  Hence, we used our annotation (using majority voting for resolving any 
conflicts) as the gold data set for validating automatically the user generated paraphrases, in subsequent 
sections.  
Quality of the generated data:  Of the 112 games played, 98 of them completed successfully. Games 
were considered incomplete if the timer expired before successful completion. Of the 98 completed 
games, 15 of the final guesses were false positives (i.e. NM, wrong answers accepted erroneously), 42 
games closed with guessers reproducing the exact text element given to the drawer (i.e. EF), and 19 
games closed with Drawer correctly accepting a guess that is semantically equivalent to the given text 
element (i.e. TF, a true phrase-equivalent), and the remaining producing various degrees partial semantic 
matches (i.e. TP, true partial phrase-equivalents).  The average time of completion for successfully com-
pleted games was 160 seconds.  
In addition, most of the games, irrespective of whether closed correctly or not, produced partial equiv-
alents to the given text element as intermediate guesses, thus providing valuable data for research. These 
include all the potential matches which were not accepted as the final answer for a game, but were 
marked as green or orange via the drawers? swipe-based annotation. Table 3 shows the breakdown of 
the gold classification of all of the potential matches. 
4.2 From Game to Corpus  
Once assured the quality of the generated data, we devised a methodology for automatically detecting 
phrase equivalents (full or partial) from the user generated data, so that the game would be able to scale 
without the need for human annotators to verify individual guesses.  We designed a classifier for auto-
matically validating phrase equivalents (partial or full), based only on the game meta data, and very 
shallow text level features, and not based on any linguistic (such as, dictionaries, thesauri, etc.) or other 
specialized corpora (such as, parallel or paraphrase corpora). Our basic premise is that if such a classifier 
can identify good paraphrases with simple features, then we will be able to identify the phrase equiva-
lents automatically, in new domains or languages. 
Our classifier uses only simple game and text-level features: hardness of the input text element 
(easy/medium/hard), status of completion flag and cheating flag at completion, order and time of the 
guess, drawer?s annotation (green/orange/red), cross-game evidence, substring similarities to the input 
text element and orthographic overlap with the input text.  First, we extracted any exact matches (EF or 
EP) by removing any text elements that were a substring of the original guess, leaving us with a training 
corpus of 122 potential phrase-equivalents. We trained the classifier using a 5-fold cross-validation this 
corpus.  Some paraphrases thus extracted are shown in Table 2. 
 
Source phrase Paraphrases extracted 
Police Officer Policeman, Police Inspector, Police Superintend 
I lost my luggage. I need to find my bag at the lost-and-found counter, Lost-and-found luggage counter. 
School Teacher Class Teacher, Teacher teaching in school. 
Railway Station  Railroad Station, Railway Platform 
Table 2: Automatically Extracted Paraphrases 
 Doodling Doodling  + SVM MTurk 
 Raw Corpus Training Corpus SVM = NM SVM = TF|TP Corpus 
Size 234 122 73 49 92 
Exact Full (EF) 42 EF and EP Data automatically removed from  
Corpus using String and Substring Match 
0 
Exact Partial (EP) 71 21 
1242
True Full (TF) 30 30 2 28 53 
True Partial (TP) 11 11 5 6 13 
Not a Match (NM) 81 81 66 15 5 
Precision (TF+TP/Size) 17% 34% 10% 69% 72% 
Table 3: Comparison of corpora produced by Doodling and MTurk to gold data. SVM numbers are an average of the results 
generated during the 5-fold cross-validation. 
The classifier reduces the burden on expert hand-annotators, by automatically filtering out text ele-
ments that are likely to not be a match. As can be seen in Table 3, only 17% of the raw corpus constitutes 
useful data. Removing exact and substring matches (EF and EP) increases the precision to 34%. The 
usable corpus produced by the classifier (SVM=TF|TP) has a precision of 69%, with only 10% of the 
remaining corpus (SVM=NM) constituting false negative, or ?lost? data. The overall accuracy of the 
classifier (% of true positives + true negatives) is 82%. 
This methodology provides a viable means of generating paraphrase corpora, with a small amount of 
hand-crafted corpus in a new domain.  The classifier can be fine-tuned either for accuracy of prediction 
(precision) or productivity (recall); in our experiments we fine-tuned it for precision.  Also, we believe 
that given that these features used are devoid of linguistic or domain information, our results may pro-
vide a lower bound on the quality of automatic identification of phrase equivalents; this may be im-
proved substantially by use of appropriate linguistic resources or specialized corpora.  
In addition to phrase-equivalent data, many of the guesses relate semantically to the input text ele-
ment, in varying degrees.  Using similar features as used in the classifier, the annotation data can be 
used for identifying sets of related words for given input text elements, creating valuable resources for 
search query expansion.   
5 Mechanical Turk Experiments 
To understand the quantitative difference between Doodling and a paid crowdsourcing model for gen-
erating paraphrases we designed a ?Data Collection? Mechanical Turk task using the same phrases that 
were used in our user experiments.  Based on previous work relating to designing of Turk experiments 
and accepted best practices, we kept the task description simple: Each task asked a respondent to gen-
erate five unique and semantically equivalent phrases for a given source phrase. The respondents were 
chosen based on their familiarity with English as their first language, and each phrase was to be anno-
tated by 20 respondents over one week duration; this duration was chosen to keep the respondent popu-
lation size roughly equal to that of our user experiment. Reward for completing the generation of five 
phrase-equivalents for a single given phrase was fixed at $0.10USD, in line with the rewards given out 
for tasks with similar levels of difficulty as cited in published literature (Callison-Burch et al. 2009; 
Dolan et al, 2011).  Though the time frame was a larger than the duration of our experiments (one hour) 
significantly, the overall time taken for task is comparable to the time spent in gameplay.  
At the end of the one-week duration of the experiment, 14 out of 38 phrases got at least one set of 
valid paraphrases, leading to a completion percentage of 37%.  Most of the submitted phrases were 
annotated only by one respondent; the average number of respondents per phrase was 1.23. The anno-
tation data was judged by the authors in the same scale as outlined in Section 5.1, and the Fleiss Kappa 
measure for the annotation was 0.74, signifying significant agreement between their judgments.  Overall, 
72% of the MTurker generated paraphrases were accepted as full or partial alternatives (See Table 3).  
While the quality of data is very good, any misunderstanding of the task generated results that are sig-
nificantly off the mark:  For example, ?How do I get to the nearest international airport?? was generated 
for ?International Airport? as the source phrase.  Since the participation and completion was low, we 
extended the duration of the task by another week, but the second week yielded only 2 additional com-
pleted tasks indicating that the duration of the experiment was not the sole factor in the relative low rate 
of task completion; perhaps it is the nature of the task that did not attract significant participation.   
1243
6 Discussions  
6.1 Viability of Doodling as a Game 
The 85% successful completion (98 out of 112) of the games is encouraging, and indicates the viability 
of the game to complete successfully.  At the end of the experimental session (wherein 30 rounds of the 
game had been completed by each player on an average), the players were asked to fill in an online 
survey to measure various qualitative metrics on effectiveness of Doodling as a game.  A wide variety 
of questions were asked, ranging from specific input (How did [a specific feature] affect your ability to 
guess the right phrase?) to generic qualitative measures (Would you play this game again?).  Among 
the questions were three specific questions on how much the players enjoyed the game as a drawer, as 
a guesser and overall, in a scale of 1 (Hated it.) to 5 (Loved it!).  From the 10 respondents, the enjoyment 
factor averaged at 4.7 overall.  Such high score validates the game design and UX as a viable mechanism 
for an enjoyable game.  Further, 9 out of 10 respondents said that they would definitely play the game 
again, with comments such as ?It was very interesting and fun? and ?This game is kind of addictive?, 
indicating attraction of the game for subsequent engagement 
6.2 Use of Hints & Reminders 
We find no evidence for the hints or reminders to be valuable either in improving the quality of the 
result, or helping the time for convergence/completion.  We note that several gamers resorted to other 
means of indicating the structure of the guess phrases, such as drawing out a number of dashes to indicate 
the size of the guess phrase, with some of them requesting us to do the same.   
6.3 Scaling Up: Comparison with Mechanical Turk for crowd-sourcing phrase equivalents 
GWAPs have been criticized for their complexity, long time-to-market, and hidden running costs (Wang 
et al., 2012).  Paid crowd-sourcing methods, by comparison, are simpler to set up, and have lower initial 
costs.  While a concrete, direct comparison is not possible, Table 3 lays out some of the differences 
between the two methods, especially with reference to our metrics. 
 
 
 Mechanical Turk (MT) Doodling 
Experimental Operating Costs US$82 US$90 
Ongoing Costs US$0.10/source phrase US$90/month 
Setup Costs Minimal 3 man-months 
Players/Workers 9 14 
Time 2 weeks  1 hour 
Completion (Games with ? 1 TF generated) 14/38 (37%) 38/38 (100%) 
Quantity (# of Unique TFs) 53 28 
Precision (% of usable data) 72% 69% (with Classifier) 
Table 4: Comparison of MTurk and Doodling experiments for generation of phrase-equivalents 
 
In the case of the Doodling game, the development of the game took 3 man-months, while Mechanical 
Turk?s (MT) setup time was minimal. Both the Doodling and MT experiments had similar operational 
costs, at US$90 and US$82 respectively. This cost of $90 for Doodling consists of hosting and band-
width charges incurred for two virtual servers running on a commercial cloud platform.  However, once 
we scale Doodling up to permit more users and higher productivity, we expect the costs to remain fixed, 
whereas MT costs will scale proportionally to the productivity at US$0.10 per source. In addition, even 
with approximately equal investment, one hour of Doodling game play is more productive than the two 
weeks of MT task. As discussed in Section 5, we encountered a significant limitation of paid crowd-
sourcing: workers may not choose to do tasks they consider uninteresting.  While it is possible to in-
crease the pay rate to increase the completion rate, this entails additional costs, with deteriorating com-
pletion rates.  While we expect the productivity of Doodling to scale with the number of users, MT?s 
productivity is low even for our limited experiment, and may not scale at all.   
1244
To put this in perspective, the time taken to generate useful data using Mechanical Turk varies highly 
depending on the task: (Chen and Dolan, 2011) reported a duration of 2 months, whereas (Callison-
Burch et al., 2009) reported 2 days for their experiments.  In our Doodling experiment, the task comple-
tion rate for the game (one hour, 14 players) is faster than the equivalent Mechanical Turk task (two 
weeks, 9 workers). We argue that for scalable data collection, a fixed recurring cost for a reliable com-
pletion rate may be preferable over a variable recurring cost. Furthermore, the Doodling game setup is 
easily scalable to large user base with little marginal cost, and hence we hypothesize that the economy 
of scale will make Doodling cheaper than MT for diverse domains. Finally, while MT workers tend to 
be transient, gamers tend to be loyal, particularly if the game is perceived to be interesting.  Such a user 
base may be likely to participate and be productive in other (perhaps related) GWAPs for the generation 
of useful language data. 
6.4 Cheating 
Doodling depends on fair gameplay in order to generate reliable phrase-equivalent. Although we did not 
have many cases of cheating during the trials, cases of cheating will be unavoidable as the game scales 
to more users. The drawer scribbling answers to the canvas is a most obvious form of cheating, which 
may require sophisticated image recognition algorithms to weed out automatically. However, we opted 
for a low-cost approach of allowing any guesser to mark a certain game round as cheating, if they find 
the drawer scribbling on the canvas. Any guesser can also mark a game round as cheating, if he/she 
finds the drawer concluding a game round with guesses that are not equivalent phrases. All guessers in 
a room other than the guesser who provided the accepted guess, are given three seconds to report cheat-
ing in case the guess was not found as a suitable equivalent phrase. While this methodology may not 
work in a two player room, we expect that in larger rooms the competitive nature of the players will 
keep a game honest.  Frequent offenders may be penalized. Proposed penalties would be banning from 
game rooms, disabling certain roles or introducing harder authentication protocol to prune out offending 
players.   
Along the same lines, we intend to introduce an ?inappropriate or offending? flag, to be flagged for a 
drawing or a guess, by any of the players in the room.  Such flags, once set, may need to be investigated 
offline, and the players penalized in order to discourage misuse or abuse of the game environment. 
7 Conclusions and Future Work 
In this paper, we explored gaming as a methodology for generating paraphrase data that is useful for 
NLP or IR research and development of practical systems.  Specifically, we outlined a game-with-a-
purpose ? Doodling ? that is based on sketch-and-convey metaphor, where a sketch by a Drawer was 
used as a mechanism for abstracting a concept (the source phrase) which was then surfaced by different 
guessers in the game room, potentially producing paraphrases.   We showed that our online multiplayer 
game was effective in generating paraphrase data, by mining user guesses in the familiar sketch-and-
convey paradigm, and rewarding phrase-equivalents in addition to exact phrase guesses. Our experi-
ments for just one hour with volunteers have shown that this game can generate high quality data in 
scale. Most importantly, our volunteers rated the game ?very enjoyable?, even after an hour of continual 
play.  In addition, we presented a classification mechanism to automatically identify good partial or full 
phrase-equivalents from the user guesses, using only the meta-level features of the game and shallow 
text features, opening an avenue for data generation in diverse domains, with a small seed corpora.  We 
believe the quality of such identification may be improved significantly with addition of linguistic re-
sources, such as, dictionaries or thesauri.  Finally, our experiments with Amazon?s Mechanical Turk 
indicated that our game is comparable to and potentially more scalable than paid crowd-sourcing.  We 
believe such a game may be a viable mechanism for generating paraphrase data in diverse domains and 
languages, cheaply. 
7.1 Future Work 
Currently, we are in the process of developing and releasing Doodling as a multiplayer game app, 
providing a potential opportunity to study its uptake in the Internet, and the quality of data generated.  
In our experiments we measured, through a post-game survey, the potential for Doodling being a fun 
game, and we obtained a score of 4.7 out of 5 for ?fun-factor?, in addition to many verbal comments on 
1245
how enjoyable the game was.  Such user feedback amply indicate Doodling?s potential for scaling well 
as a game in diverse domains, such as sports, entertainment and idioms.  Also, while the current imple-
mentation of Doodling game works well for phrases, we have ample evidence that it works for short 
sentences (such as, ?My luggage is lost?, ?Where is the nearest post office?? etc.).  We hope to extend 
it to complex sentences as future work. 
One of our goals long term is to explore the game?s potential for generating parallel data ? perhaps 
through a game being played between two players conversant in two different languages.  While this 
multi- and cross-lingual game poses significant challenges, it provides for an interesting exploration into 
generation of parallel data through games.  Significantly, it may also provide opportunities for language 
learning and/or cross-cultural awareness, as many of the idioms and culture-specific phrases are not 
readily conveyed by the surface forms in one language or another.  If successful, this may pave way for 
cost-effective generation of parallel data between many languages of the world.  
1246
Reference 
Barzilay, R. 2003. Information Fusion for Mutli-document summarization: Paraphrasing and Generation. Ph.D. 
thesis @ Columbia University. 
Barzilay, R., and McKeown, K. 2001. Extracting paraphrases from a parallel corpus. 39th Annual Meeting of the 
Association for Computational Linguistics. 
Callison-Burch, C. 2009. Fast, cheap, and creative: evaluating translation quality using Amazon?s Mechanical 
Turk. EMNLP?09. 
Callison-Burch, C., Cohn, T., and Lapata, M.  2008. ParaMetric: An Automatic Evaluation Metric for Paraphras-
ing, International Conference on Computational Linguistics, 2008. 
Chen, D.L. and Dolan, W. 2011. Collecting highly parallel data for paraphrase evaluation. 49th Annual Meeting 
of the Association for Computational Linguistics. 
Lin, D., and Pantel, P. DIRT - Discovery of inference rules from text. Proceedings of the seventh ACM SIGKDD 
International conference on Knowledge discovery and data mining. ACM, 2001. 
Chklovski, T. Collecting paraphrase corpora from volunteer contributors. Proceedings of the 3rd K-CAP. Interna-
tional conference on Knowledge Capture, ACM, 2005. 
Cooper, S., Khatib, F., Treuille, A., Barbero, J., Lee, J., Beenen, M., Leaver-Fey, A., Baker, D., Popovic, Z. and 
Foldit Players. 2010. Predicting protein structures with a multiplayer online game. Nature (466), Aug 2010. 
Dolan, W., Quirk, C., and Brockett, C. 2004. Unsupervised construction of large paraphrase corpora: Exploiting 
massively parallel news sources. 20th International Conference on Computational Linguistics. 
Fleiss, J. L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76, 378?382. 
Ibrahim, A., Katz, B., and Lin, J. 2003. Extracting structural paraphrases from aligned monolingual corpora. Sec-
ond International Workshop on Paraphrasing (collocated with ACL 2003). 
Irvine, A. and Klementiev, A. 2010. Using Mechanical Turk to annotate lexicons for less commonly used lan-
guages. NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk. 
Kumaran, A., Jauhar, S. K., and Basu, S. 2012. Doodling: A Gaming Paradigm for Generating Language Data. 
Human Computation Workshop 2012. 
Kumaran, A., Saravanan, K., Datha, N., Ashok, B. and Dendi, V. 2009. WikiBABEL: a wiki-style platform for 
creation of parallel data. ACL-IJCNLP 2009. 
Law, E.L.M., Von Ahn, L., Dannenberg, R. B. and Crawford, M. 2007. Tagatune: A game for music and sound 
annotation. ISMIR?07. 
Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. 2002. Bleu: A method for automatic evaluation of machine 
translation. 40th Annual Meeting of the Association for Computational Linguistics. 
Quirk, C., Brockett, C., and Dolan, W. 2004. Monolingual machine translation for paraphrase generation. Empir-
ical Methods in Natural Language Processing (EMNLP-2004). 
Von Ahn, L. and Dabbish, L. 2004. Labeling images with a computer game. CHI?04. 
Von Ahn, L., Kedia, M. and Blum, M. 2006. Verbosity: a game for collecting common-sense facts. CHI?06.  
Von Ahn, L. and Dabbish, L. 2008. Designing games with a purpose. Communications of the ACM, Vol 51.  
Wang, A., Hoang, C. D. V. and Kan, M.  2012. Perspectives on crowdsourcing annotations for natural language 
processing.  Language Resources & Evaluation Conference, 2012. 
1247
 
Cross-Lingual Information Retrieval System for Indian 
Languages 
 
Jagadeesh Jagarlamudi and A Kumaran 
 
Abstract 
 
This paper describes our first participation in the Indian language sub-task of 
the main Adhoc monolingual and bilingual track in CLEF competition. In 
this track, the task is to retrieve relevant documents from an English corpus 
in response to a query expressed in different Indian languages including 
Hindi, Tamil, Telugu, Bengali and Marathi. Groups participating in this 
track are required to submit a English to English monolingual run and a 
Hindi to English bilingual run with optional runs in rest of the languages. 
We had submitted a monolingual English run and a Hindi to English cross-
lingual run. 
 
We used a word alignment table that was learnt by a Statistical Machine 
Translation (SMT) system trained on aligned parallel sentences, to map a 
query in source language into an equivalent query in the language of the 
target document collection. The relevant documents are then retrieved using 
a Language Modeling based retrieval algorithm. On CLEF 2007 data set, our 
official cross-lingual performance was 54.4% of the monolingual 
performance and in the post submission experiments we found that it can be 
significantly improved up to 73.4%. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 420?428,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Everybody loves a rich cousin: An empirical study of transliteration through
bridge languages
Mitesh M. Khapra
Indian Institute of Technology
Bombay,
Powai, Mumbai 400076,
India
miteshk@cse.iitb.ac.in
A Kumaran
Microsoft Research India,
Bangalore,
India
a.kumaran@microsoft.com
Pushpak Bhattacharyya
Indian Institute of Technology
Bombay,
Powai, Mumbai 400076,
India
pb@cse.iitb.ac.in
Abstract
Most state of the art approaches for machine
transliteration are data driven and require sig-
nificant parallel names corpora between lan-
guages. As a result, developing translitera-
tion functionality among n languages could
be a resource intensive task requiring paral-
lel names corpora in the order of nC2. In this
paper, we explore ways of reducing this high
resource requirement by leveraging the avail-
able parallel data between subsets of the n lan-
guages, transitively. We propose, and show
empirically, that reasonable quality transliter-
ation engines may be developed between two
languages, X and Y , even when no direct par-
allel names data exists between them, but only
transitively through language Z . Such sys-
tems alleviate the need for O(nC2) corpora,
significantly. In addition we show that the per-
formance of such transitive transliteration sys-
tems is in par with direct transliteration sys-
tems, in practical applications, such as CLIR
systems.
1 Introduction
Names and Out Of Vocabulary (OOV) terms appear
very frequently in written and spoken text and hence
play a very important role in several Natural Lan-
guage Processing applications. Several studies have
shown that handling names correctly across lan-
guages can significantly improve the performance of
CLIR Systems (Mandl and Womser-Hacker, 2004)
and the utility of machine translation systems. The
fact that translation lexicons or even statistical dic-
tionaries derived from parallel data do not provide a
good coverage of name and OOV translations, un-
derscores the need for good transliteration engines
to transform them between the language.
The importance of machine transliteration, in the
above context, is well realized by the research com-
munity and several approaches have been proposed
to solve the problem. However, most state of the art
approaches are data driven and require significant
parallel names corpora between languages. Such
data may not always be available between every pair
of languages, thereby limiting our ability to support
transliteration functionality between many language
pairs, and subsequently information access between
languages. For example, let us consider a practi-
cal scenario where we have six languages from four
different language families as shown in Figure 1.
The nodes in the graph represent languages and the
edges indicate the availability of data between that
language pair and thus the availability of a Machine
Transliteration system for that pair. It is easy to see
the underlying characteristics of the graph. Data is
available between a language pair due to one of the
following three reasons:
Politically related languages: Due to the political
dominance of English it is easy to obtain parallel
names data between English and most languages.
Genealogically related languages: Arabic and He-
brew share a common origin and there is a signifi-
cant overlap between their phoneme and grapheme
inventory. It is easy to obtain parallel names data
between these two languages.
Demographically related languages: Hindi and
Kannada are languages spoken in the Indian sub-
continent, though they are from different language
families. However, due to the shared culture and de-
mographics, it is easy to create parallel names data
between these two languages.
420
Figure 1: A connected graph of languages
On the other hand, for politically, demographi-
cally and genealogically unrelated languages such
as, say, Hindi and Hebrew, parallel data is not readily
available, either due to the unavailability of skilled
bilingual speakers. Even the argument of using
Wikipedia resources for such creation of such par-
allel data does not hold good, as the amount of inter-
linking may be very small to yield data. For exam-
ple, only 800 name pairs between Hindi and Hebrew
were mined using a state of the art mining algorithm
(Udupa et al, 2009), from Wikipedia interwiki links.
We propose a methodology to develop a practi-
cal Machine Transliteration system between any two
nodes of the above graph, provided a two-step path
exists between them. That is, even when no parallel
data exists between X & Y but sufficient data exists
between X & Z and Z & Y it is still possible to de-
velop transliteration functionality between X & Y
by combining a X ? Z system with a Z ? Y
system. For example, given the graph of Figure 1,
we explore the possibility of developing translitera-
tion functionality between Hindi and Russian even
though no direct data is available between these two
languages. Further, we show that in many cases the
bridge language can be suitably selected to ensure
optimal MT accuracy.
To establish the practicality and utility of our ap-
proach we integrated such a bridge transliteration
system with a standard CLIR system and compared
its performance with that of a direct transliteration
system. We observed that such a bridge system
performs well in practice and in specific instances
results in improvement in CLIR performance over
a baseline system further strengthening our claims
that such bridge systems are good practical solutions
for alleviating the resource scarcity problem.
To summarize, our main contributions in this pa-
per are:
1. Constructing bridge transliteration systems and
establishing empirically their quality.
2. Demonstrating their utility in providing prac-
tical transliteration functionality between two
languages X & Y with no direct parallel data
between them.
3. Demonstrating that in specific cases it is pos-
sible to select the bridge language so that op-
timal Machine Transliteration accuracy is en-
sured while stepping through the bridge lan-
guage.
1.1 Organization of the Paper
This paper is organized in the following manner. In
section 2 we present the related work and highlight
the lack of work on transliteration in resource scarce
scenarios. In section 3 we discuss the methodology
of bridge transliteration. Section 4 discusses the ex-
periments and datasets used. Section 4.3 discusses
the results and error analysis. Section 5 discusses or-
thographic characteristics to be considered while se-
lecting the bridge language. Section 6 demonstrates
the effectiveness of such bridge systems in a practi-
cal scenario, viz., Cross Language Information Re-
trieval. Section 7 concludes the paper, highlighting
future research issues.
2 Related Work
Current models for transliteration can be classi-
fied as grapheme-based, phoneme-based and hy-
brid models. Grapheme-based models, such as,
Source Channel Model (Lee and Choi, 1998), Max-
imum Entropy Model (Goto et al, 2003), Condi-
tional Random Fields (Veeravalli et al, 2008) and
Decision Trees (Kang and Choi, 2000) treat translit-
eration as an orthographic process and try to map
the source language graphemes directly to the tar-
get language graphemes. Phoneme based models,
such as, the ones based on Weighted Finite State
421
Transducers (WFST) (Knight and Graehl, 1997)
and extended Markov window (Jung et al, 2000)
treat transliteration as a phonetic process rather than
an orthographic process. Under such frameworks,
transliteration is treated as a conversion from source
grapheme to source phoneme followed by a conver-
sion from source phoneme to target grapheme. Hy-
brid models either use a combination of a grapheme
based model and a phoneme based model (Stalls
and Knight, 1998) or capture the correspondence be-
tween source graphemes and source phonemes to
produce target language graphemes (Oh and Choi,
2002).
A significant shortcoming of all the previous
works was that none of them addressed the issue of
performing transliteration in a resource scarce sce-
nario, as there was always an implicit assumption
of availability of data between a pair of languages.
In particular, none of the above approaches address
the problem of developing transliteration functional-
ity between a pair of languages when no direct data
exists between them but sufficient data is available
between each of these languages and an intermedi-
ate language. Some work on similar lines has been
done in Machine Translation (Wu and Wang, 2007)
wherein an intermediate bridge language (say, Z) is
used to fill the data void that exists between a given
language pair (say, X and Y ). In fact, recently it has
been shown that the accuracy of a X ? Z Machine
Translation system can be improved by using addi-
tional X ? Y data provided Z and Y share some
common vocabulary and cognates (Nakov and Ng,
2009). However, no such effort has been made in the
area of Machine Transliteration. To the best of our
knowledge, this work is the first attempt at providing
a practical solution to the problem of transliteration
in the face of resource scarcity.
3 Bridge Transliteration Systems
In this section, we explore the salient question ?Is
it possible to develop a practical machine transliter-
ation system between X and Y , by composing two
intermediate X ? Z and Z ? Y transliteration
systems?? We use a standard transliteration method-
ology based on orthography for all experiments (as
outlined in section 3.1), to ensure the applicability
of the methodology to a variety of languages.
3.1 CRF based transliteration engine
Conditional Random Fields ((Lafferty et al, 2001))
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of the target word given the
source word is given by,
P (Y |X;?) = 1
N(X)
? e
PT
t=1
PK
k=1 ?kfk(Yt?1,Yt,X,t)
(1)
where,
X = source word
Y = target word
T = length of source word
K = number of features
?k = feature weight
N(X) = normalization constant
CRF++ 1, an open source implementation of CRF
was used for training and decoding (i.e. transliter-
ating the names). GIZA++ (Och and Ney, 2003),
a freely available implementation of the IBM align-
ment models (Brown et al, 1993) was used to get
character level alignments for the name pairs in the
parallel names training corpora. Under this align-
ment, each character in the source word is aligned to
zero or more characters in the corresponding target
word. The following features are then generated us-
ing this character-aligned data (here ei and hi form
the i-th pair of aligned characters in the source word
and target word respectively):
? hi and ej such that i? 2 ? j ? i + 2
? hi and source character bigrams ( {ei?1, ei} or
{ei, ei+1})
? hi and source character trigrams ( {ei?2, ei?1,
ei} or {ei?1, ei, ei+1} or {ei, ei+1, ei+2})
? hi, hi?1 and ej such that i? 2 ? j ? i + 2
? hi, hi?1 and source character bigrams
? hi, hi?1 and source character trigrams
1http://crfpp.sourceforge.net/
422
3.2 Bridge Transliteration Methodology
In this section, we outline our methodology for com-
posing transitive transliteration systems between X
and Y , using a bridge language Z , by chaining indi-
vidual direct transliteration systems. Our approach
of using bridge transliteration for finding the best
target string (Y ?), given the input string X can be
represented by the following probabilistic expres-
sion:
Y ? = arg max
Y
P (Y |X)
=
?
Z
P (Y,Z|X)
=
?
Z
P (Y |Z,X) ? P (Z|X) (2)
We simplify the above expression, by assuming that
Y is independent of X given Z; the linguistic intu-
ition behind this assumption is that the top-k outputs
of the X ? Z system corresponding to a string in
X, capture all the transliteration information neces-
sary for transliterating to Y . Subsequently, in sec-
tion 5 we discuss the characteristics of the effective
bridge languages to maximize the capture of neces-
sary information for the second stage of the translit-
eration, namely for generating correct strings of Z .
Thus,
Y ? =
?
Z
P (Y |Z) ? P (Z|X) (3)
The probabilities P (Y |Z) and P (Z|X) in Equation
(3) are derived from the two stages of the bridge sys-
tem. Specifically, we assume that the parallel names
corpora are available between the language pair, X
and Z , and the language pair, Z and Y . We train two
baseline CRF based transliteration systems (as out-
lined in Section 3.1), between the language X and
Z , and Z and Y . Each name in language X was
provided as an input into X ? Z transliteration sys-
tem, and the top-10 candidate strings in language Z
produced by this first stage system were given as an
input into the second stage system Z ? Y . The re-
sults were merged using Equation (2). Finally, the
top-10 outputs of this system were selected as the
output of the bridge system.
4 Experiments
It is a well known fact that transliteration is lossy,
and hence the transitive systems may be expected to
suffer from the accumulation of errors in each stage,
resulting in a system that is of much poorer quality
than a direct transliteration system. In this section,
we set out to quantify this expected loss in accuracy,
by a series of experiments in a set of languages us-
ing bridge transliteration systems and a baseline di-
rect systems. We conducted a comprehensive set of
experiments in a diverse set of languages, as shown
in Figure 1, that include English, Indic (Hindi and
Kannada), Slavic (Russian) and Semitic (Arabic and
Hebrew) languages. The datasets and results are de-
scribed in the following subsections.
4.1 Datasets
To be consistent, for training each of these systems,
we used approximately 15K name pairs corpora (as
this was the maximum data available for some lan-
guage pairs). While we used the NEWS 2009 train-
ing corpus (Li et al, 2009) as a part of our train-
ing data, we enhanced the data set to about 15K by
adding more data of similar characteristics (such as,
name origin, domain, length of the name strings,
etc.), taken from the same source as the original
NEWS 2009 data. For languages such as Arabic
and Hebrew which were not part of the NEWS 2009
shared task, the data was created along the same
lines. All results are reported on the standard NEWS
2009 test set, wherever applicable. The test set con-
sists of about 1,000 name pairs in languages X and
Y ; to avoid any bias, it was made sure that there is
no overlap between the test set with the training sets
of both the X ? Z and Z ? Y systems. To estab-
lish a baseline, the same CRF based transliteration
system (outlined in Section 3.1) was trained with a
15K name pairs corpora between the languages X
? Y . The same test set used for testing the transi-
tive systems was used for testing the direct system
as well. As before, to avoid any bias, we made sure
that there is no overlap between the test set and the
training set for the direct system as well.
4.2 Results
We produce top-10 outputs from the bridge system
as well from the direct system and compare their
performance. The performance is measured using
the following standard measures, viz., top-1 accu-
racy (ACC-1) and Mean F-score. These measures
are described in detail in (Li et al, 2009). Table 1
423
Language
Pair
ACC-1 Relative change in
ACC-1 Mean F-score
Relative change in
Mean F-score
Hin-Rus 0.507 0.903
Hin-Eng-Rus 0.466 -8.08% 0.886 -1.88%
Hin-Ara 0.458 0.897
Hin-Eng-Ara 0.420 -8.29% 0.876 -2.34%
Eng-Heb 0.544 0.917
Eng-Ara-Heb 0.544 0% 0.917 0%
Hin-Eng 0.422 0.884
Hin-Kan-Eng 0.382 -9.51% 0.871 -1.47%
Table 1: Stepping through an intermediate language
presents the performance measures, both for a di-
rect system (say, Hin-Rus), and a transitional sys-
tem (say, Hin-Eng-Rus), in 4 different transitional
systems, between English, Indic, Semitic and Slavic
languages. In each case, we observe that the transi-
tional systems have a slightly lower quality, with an
absolute drop in accuracy (ACC-1) of less than 0.05
(relative drop under 10%), and an absolute drop in
Mean F-Score of 0.02 (relative drop under 3%).
4.3 Analysis of Results
Intuitively, one would expect that the errors of the
two stages of the transitive transliteration system
(i.e., X ? Z , and Z ? Y ) to compound, leading
to a considerable loss in the overall performance of
the system. Given that the accuracies of the direct
transliteration systems are as given in Table 2, the
transitive systems are expected to have accuracies
close to the product of the accuracies of the individ-
ual stages, for independent systems.
Language Pair ACC-1 Mean F-Score
Hin-Eng 0.422 0.884
Eng-Rus 0.672 0.935
Eng-Ara 0.514 0.905
Ara-Heb 1.000 1.000
Hin-Kan 0.433 0.879
Kan-Eng 0.434 0.886
Table 2: Performance of Direct Transliteration Systems
However, as we observe in Table 1, the relative
drop in the accuracy (ACC-1) is less than 10% from
that of the direct system, which goes against our in-
tuition. To identify the reasons for the better than
expected performance, we performed a detailed er-
ror analysis of each stage of the bridge translitera-
tion systems, and the results are reported in Tables 3
? 5. We draw attention to two interesting facts which
account for the better than expected performance of
the bridge system:
Improved 2nd stage performance on correct
inputs: In each one of the cases, as expected, the
ACC-1 of the first stage is same as the ACC-1 of the
X ? Z system. However, we notice that the ACC-1
of the second stage on the correct strings output
in the first stage, is significantly better than the the
ACC-1 of the Z ? Y system! For example, the
ACC-1 of the Eng-Rus system is 67.2% (see Table
2), but, that of the 2nd stage Eng-Rus system is
77.8%, namely, on the strings that are transliterated
correctly by the first stage. Our analysis indicate
that there are two reasons for such improvement:
First, the strings that get transliterated correctly in
the first stage are typically shorter or less ambigu-
ous and hence have a better probability of correct
transliterations in the both stages. This phenomenon
could be verified empirically: Names like gopAl
{Gopal}, rm? {Ramesh}, rAm {Ram} are
shorter and in general have less ambiguity on target
orthography. Second, also significantly, the use of
top-10 outputs from the first stage as input to the
second stage provides a better opportunity for the
second stage to produce correct string in Z . Again,
this phenomenon is verified by providing increasing
number of top-n results to the 2nd stage.
424
Hi?En?Ru En ? Ru(Stage-2)
Stage-2
Acc.
Correct Error
Hi?En Correct 263 75 77.81%
(Stage-1) Error 119 362 24.74%
Table 3: Error Analysis for Hi?En?Ru
Hi?En?Ar En ? Ar(Stage-2)
Stage-2
Acc.
Correct Error
Hi?En Correct 221 127 63.50%
(Stage-1) Error 119 340 25.70%
Table 4: Error Analysis for Hi?En?Ar
2nd stage error correction on incorrect inputs:
The last rows in each of the above tables 3 ? 5 re-
port the performance of the second stage system on
strings that were transliterated incorrectly by the first
stage. While we expected the second row to pro-
duce incorrect transliterations nearly for all inputs
(as the input themselves were incorrect in Z), we
find to our surprise that upto 25% of the erroneous
strings in Z were getting transliterated correctly in
Y ! This provides credence to our hypothesis that
sufficient transliteration information is captured in
the 1st stage output (even when incorrect) that may
be exploited in the 2nd stage. Empirically, we veri-
fied that in most cases (nearly 60%) the errors were
due to the incorrectly transliterated vowels, and in
many cases, they get corrected in the second stage,
and re-ranked higher in the output. Figure 2 shows a
few examples of such error corrections in the second
stage.
Figure 2: Examples of error corrections
Hi?Ka?En Ka ? En(Stage-2)
Stage-2
Acc.
Correct Error
Hi?Ka Correct 225 196 53.44%
(Stage-1) Error 151 400 27.40%
Table 5: Error Analysis for Hi?Ka?En
5 Characteristics of the bridge language
An interesting question that we explore in this sec-
tion is ?how the choice of bridge language influence
the performance of the bridge system??. The under-
lying assumption in transitive transliteration systems
(as expressed in Equation 3), is that ?Y is indepen-
dent of X given Z?. In other words, we assume that
the representations in the language will Z ?capture
sufficient transliteration information from X to pro-
duce correct strings in Y ?. We hypothesize that two
parameters of the bridge language, namely, the or-
thography inventory and the phoneme-to-grapheme
entropy, that has most influence on the quality of the
transitional systems, and provide empirical evidence
for this hypothesis.
5.1 Richer Orthographic Inventory
In each of the successful bridge systems (that is,
those with a relative performance drop of less than
10%), presented in Table 1, namely, Hin-Eng-Ara,
Eng-Ara-Heb and Hin-Kan-Eng, the bridge lan-
guage has, in general, richer orthographic inven-
tory than the target language. Arabic has a reduced
set of vowels, and hence poorer orthographic inven-
tory compared with English. Similarly, between the
closely related Semitic languages Arabic-Hebrew,
there is a many-to-one mapping from Arabic to He-
brew, and between Kannada-English, Kannada has
nearly a superset of vowels and consonants as com-
pared to English or Hindi.
As an example for a poor choice of Z , we present
a transitional system, Hindi ? Arabic ? English, in
Table 6, in which the transitional language z (Ara-
bic) has smaller orthographic inventory than Y (En-
glish).
Arabic has a reduced set of vowels and, unlike En-
glish, in most contexts short vowels are optional. As
a result, when Arabic is used as the bridge language
the loss of information (in terms of vowels) is large
425
Language
Pair
ACC-1 Relative change in
ACC-1
Hin-Eng 0.422
Hin-Ara-Eng 0.155 -64.28%
Table 6: Incorrect choice of bridge language
and the second stage system has no possibility of re-
covering from such a loss. The performance of the
bridge system confirms such a drastic drop in ACC-
1 of nearly 64% compared with the direct system.
5.2 Higher Phoneme-Grapheme Entropy
We also find that the entropy in phoneme - grapheme
mapping of a language indicate a good correlation
with a good choice for a transition language. In
a good transitional system (say, Hin-Eng-Rus), En-
glish has a more ambiguous phoneme-to-grapheme
mapping than Russian; for example, in English the
phoneme ?s? as in Sam or Cecilia can be repre-
sented by the graphemes ?c? and ?s?, whereas Rus-
sian uses only a single character to represent this
phoneme. In such cases, the ambiguity introduced
by the bridge language helps in recovering from er-
rors in the X ? Z system. The relative loss of
ACC-1 for this transitional system is only about 8%.
The Table 7 shows another transitional system, in
which a poor choice was for the transitional lan-
guage was made.
Language
Pair
ACC-1 Relative change in
ACC-1
Hin-Eng 0.422
Hin-Tam-Eng 0.231 -45.26%
Table 7: Incorrect choice of bridge language
Tamil has a reduced set of consonants compared
with Hindi or English. For example, the Hindi con-
sonants (k, kh, g, gh) are represented by a sin-
gle character in Tamil. As a result, when Tamil is
used as the bridge language it looses information (in
terms of consonants) and results in a significant drop
in performance (nearly a 45% drop in ACC-1) for
the bridge system.
6 Effectiveness of Bridge Transliteration
on CLIR System
In this section, we demonstrate the effectiveness of
our bridge transliteration system on a downstream
application, namely, a Crosslingual Information Re-
trieval system. We used the standard document col-
lections from CLEF 2006 (Nardi and Peters, 2006),
CLEF 2007 (Nardi and Peters, 2007) and FIRE 2008
(FIRE, 2008). We used Hindi as the query language.
All the three fields (title, description and narration)
of the topics were used for the retrieval. Since the
collection and topics are from the previous years,
their relevance judgments were also available as a
reference for automatic evaluation.
6.1 Experimental Setup
We used primarily the statistical dictionaries gen-
erated by training statistical word alignment mod-
els on an existing Hindi-English parallel corpora.
As with any CLIR system that uses translation lex-
icon, we faced the problem of out-of-vocabulary
(OOV) query terms that need to be transliterated,
as they are typically proper names in the target lan-
guage. First, for comparison, we used the above
mentioned CLIR system with no transliteration en-
gine (Basic), and measured the crosslingual retrieval
performance. Clearly, the OOV terms would not be
converted into target language, and hence contribute
nothing to the retrieval performance. Second, we in-
tegrated a direct machine transliteration system be-
tween Hindi and English (D-HiEn), and calibrated
the improvement in performance. Third, we inte-
grate, instead of a direct system, a bridge transliter-
ation system between Hindi and English, transition-
ing through Kannada (B-HiKaEn). For both, direct
as well as bridge transliteration, we retained the top-
5 transliterations generated by the appropriate sys-
tem, for retrieval.
6.2 Results and Discussion
The results of the above experiments are given in
Table 7. The current focus of these experiments is
to answer the question of whether the bridge ma-
chine transliteration systems used to transliterate
the OOV words in Hindi queries to English (by step-
ping through Kannada) performs at par with a di-
rect transliteration system. As expected, enhancing
the CLIR system with a machine transliteration sys-
426
Collection CLIR System MAP Relative MAP change
from Basic
Recall Relative Recall change
from Basic
Basic 0.1463 - 0.4952 -
CLEF 2006 D-HiEn 0.1536 +4.98% 0.5151 +4.01%
B-HiKaEn 0.1529 +4.51% 0.5302 +7.06%
Basic 0.2521 - 0.7156 -
CLEF 2007 D-HiEn 0.2556 +1.38% 0.7170 + 0.19%
B-HiKaEn 0.2748 +9.00% 0.7174 + 0.25%
Basic 0.4361 - 0.8457 -
FIRE 2008 D-HiEn 0.4505 +3.30% 0.8506 +0.57%
B-HiKaEn 0.4573 +4.86% 0.8621 +1.93%
Table 8: CLIR Experiments with bridge transliteration systems
tem (D-HiEn) gives better results over a CLIR sys-
tem with no transliteration functionality (Basic). On
the standard test collections, the bridge translitera-
tion system performs in par or better than the di-
rect transliteration system in terms of MAP as well
as recall. Even though, the bridged system is of
slightly lesser quality in ACC-1 in Hi-Ka-En, com-
pared to Hi-En (see Table 1), the top-5 results had
captured the correct transliteration, as shown in our
analysis. A detailed analysis of the query transla-
tions produced by the above systems showed that in
some cases the bridge systems does produce a bet-
ter transliteration thereby leading to a better MAP.
As an illustration, consider the OOV terms vEV?n
{Vatican} and n-l {Nestle} and the corre-
sponding transliterations generated by the different
systems. The Direct-HiEn system was unable to
OOV term D-HiEn B-HiKaEn
vetican vetican
veticon vettican
vEV?n vettican vatican
(vatican) vetticon watican
wetican wetican
nesle nestle
nesly nesle
n-l nesley nesley
(nestle) nessle nestley
nesey nesly
Table 9: Sample output in direct and bridge systems
generate the correct transliteration in the top-5 re-
sults whereas the B-HiKaEn was able to produce the
correct transliteration in the top-5 results thereby re-
sulting in an improvement in MAP for these queries.
7 Conclusions
In this paper, we introduced the idea of bridge
transliteration systems that were developed employ-
ing well-studied orthographic approaches between
constituent languages. We empirically established
the quality of such bridge transliteration systems
and showed that quite contrary to our expectations,
the quality of such systems does not degrade dras-
tically as compared to the direct systems. Our er-
ror analysis showed that these better-than-expected
results can be attributed to (i) Better performance
(?10-12%) of the second stage system on the strings
transliterated correctly by the first stage system and
(ii) Significant (?25%) error correction in the sec-
ond stage. Next, we highlighted that the perfor-
mance of such bridge systems will be satisfactory as
long as the orthographic inventory of the bridge lan-
guage is either richer or more ambiguous as com-
pared to the target language. We showed that our
results are consistent with this hypothesis and pro-
vided two examples where there is a significant drop
in the accuracy when the bridge language violates
the above constraints. Finally, we showed that a
state of the art CLIR system integrated with a bridge
transliteration system performs in par with the same
CLIR system integrated with a direct translitera-
tion system, vindicating our claim that such bridge
transliteration systems can be use in real-world ap-
plications to alleviate the resource requirement of
nC2 parallel names corpora.
427
References
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263?311.
FIRE. 2008. Forum for information retrieval evaluation.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context in-
formation based on the maximum entropy method. In
Proceedings of MT-Summit IX, pages 125?132.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An english to korean transliteration model of
extended markov window. In Proceedings of the 18th
conference on Computational linguistics, pages 383?
389.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
transliteration and back-transliteration by decision tree
learning. In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation, pages
1135?1411.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Computational Linguistics, pages
128?135.
John D. Lafferty, Andrew Mccallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ?01: Proceedings of the Eighteenth Interna-
tional Conference on Machine Learning, pages 282?
289, San Francisco, CA, USA.
Jae Sung Lee and Key-Sun Choi. 1998. English to ko-
rean statistical transliteration for information retrieval.
In Computer Processing of Oriental Languages, pages
17?37.
Haizhou Li, A Kumaran, , Min Zhang, and Vladimir Per-
vouvhine. 2009. Whitepaper of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Transliter-
ation (NEWS 2009), pages 19?26, Suntec, Singapore,
August. Association for Computational Linguistics.
Thomas Mandl and Christa Womser-Hacker. 2004. How
do named entities contribute to retrieval effectiveness?
In CLEF, pages 833?842.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1358?1367, Singa-
pore, August. Association for Computational Linguis-
tics.
A Nardi and C Peters. 2006. Working notes for the clef
2006 workshop.
A Nardi and C Peters. 2007. Working notes for the clef
2007 workshop.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Jong-hoon Oh and Key-Sun Choi. 2002. An english-
korean transliteration model using pronunciation and
contextual rules. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics
(COLING), pages 758?764.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in arabic text. In
Proceedings of COLING/ACL Workshop on Computa-
tional Approaches to Semitic Languages, pages 34?41.
Raghavendra Udupa, K Saravanan, Anton Bakalov, and
Abhijit Bhole. 2009. ?they are out there, if you know
where to look: Mining transliterations of oov query
terms for cross language information retrieval?. In
ECIR?09: Proceedings of the 31st European Confer-
ence on IR research on Advances in Information Re-
trieval, pages 437?448, Toulouse, France.
Suryaganesh Veeravalli, Sreeharsha Yella, Prasad Pin-
gali, and Vasudeva Varma. 2008. Statistical translit-
eration for cross language information retrieval using
hmm alignment model and crf. In Proceedings of the
2nd workshop on Cross Lingual Information Access
(CLIA) Addressing the Information Need of Multilin-
gual Societies.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
428
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 1?11,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Report of NEWS 2010 Transliteration Generation Shared Task
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the Translitera-
tion Generation Shared Task conducted as
a part of the Named Entities Workshop
(NEWS 2010), an ACL 2010 workshop.
The shared task features machine translit-
eration of proper names from English to
9 languages and from 3 languages to En-
glish. In total, 12 tasks are provided. 7
teams from 5 different countries partici-
pated in the evaluations. Finally, 33 stan-
dard and 8 non-standard runs are submit-
ted, where diverse transliteration method-
ologies are explored and reported on the
evaluation data. We report the results with
4 performance metrics. We believe that the
shared task has successfully achieved its
objective by providing a common bench-
marking platform for the research commu-
nity to evaluate the state-of-the-art tech-
nologies that benefit the future research
and development.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They are important in Cross
Lingual Information Retrieval (CLIR) and Ma-
chine Translation (MT) as the system performance
has been shown to positively correlate with the
correct conversion of names between the lan-
guages in several studies (Demner-Fushman and
Oard, 2002; Mandl and Womser-Hacker, 2005;
Hermjakob et al, 2008; Udupa et al, 2009). The
traditional source for name equivalence, the bilin-
gual dictionaries ? whether handcrafted or sta-
tistical ? offer only limited support because new
names always emerge.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. Much research effort has been made to ad-
dress the transliteration issue in the research com-
munity (Knight and Graehl, 1998; Meng et al,
2001; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Al-Onaizan and Knight,
2002; Goldwasser and Roth, 2008; Goldberg and
Elhadad, 2008; Klementiev and Roth, 2006; Oh
and Choi, 2002; Virga and Khudanpur, 2003; Wan
and Verspoor, 1998; Kang and Choi, 2000; Gao
et al, 2004; Zelenko and Aone, 2006; Li et al,
2009b; Li et al, 2009a). These previous work
fall into three categories, i.e., grapheme-based,
phoneme-based and hybrid methods. Grapheme-
based method (Li et al, 2004) treats translitera-
tion as a direct orthographic mapping and only
uses orthography-related features while phoneme-
based method (Knight and Graehl, 1998) makes
use of phonetic correspondence to generate the
transliteration. Hybrid method refers to the com-
bination of several different models or knowledge
sources to support the transliteration generation.
The first machine transliteration shared task (Li
et al, 2009b; Li et al, 2009a) was held in NEWS
2009 at ACL-IJCNLP 2009. It was the first time
to provide common benchmarking data in diverse
language pairs for evaluation of state-of-the-art
techniques. NEWS 2010 is a continued effort of
NEWS 2009. It builds on the foundations estab-
lished in the first transliteration shared task and
extends the scope to include new language pairs.
The rest of the report is organised as follows.
Section 2 outlines the machine transliteration task
and the corpora used and Section 3 discusses the
metrics chosen for evaluation, along with the ratio-
nale for choosing them. Sections 4 and 5 present
the participation in the shared task and the results
with their analysis, respectively. Section 6 con-
cludes the report.
1
2 Transliteration Shared Task
In this section, we outline the definition and the
description of the shared task.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target lan-
guage, but also at its practical utility for down-
stream applications, such as CLIR and MT. There-
fore, we adopted the same definition of translit-
eration as during the NEWS 2009 workshop (Li
et al, 2009a) to narrow down ?transliteration? to
three specific requirements for the task, as fol-
lows:?Transliteration is the conversion of a given
name in the source language (a text string in the
source writing system or orthography) to a name
in the target language (another text string in the
target writing system or orthography), such that
the target language name is: (i) phonemically
equivalent to the source name (ii) conforms to the
phonology of the target language and (iii) matches
the user intuition of the equivalent of the source
language name in the target language, consider-
ing the culture and orthographic character usage
in the target language.?
In NEWS 2010, we introduce three
back-transliteration tasks. We define back-
transliteration as a process of restoring translit-
erated words to their original languages. For
example, NEWS 2010 offers the tasks to convert
western names written in Chinese and Thai into
their original English spellings, or romanized
Japanese names into their original Kanji writings.
2.2 Shared Task Description
Following the tradition in NEWS 2009, the shared
task at NEWS 2010 is specified as development of
machine transliteration systems in one or more of
the specified language pairs. Each language pair
of the shared task consists of a source and a target
language, implicitly specifying the transliteration
direction. Training and development data in each
of the language pairs have been made available to
all registered participants for developing a translit-
eration system for that specific language pair using
any approach that they find appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 1,000 and 3,000
source names (approximately 10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair every participant is re-
quired to submit at least one run (designated as a
?standard? run) that uses only the data provided by
the NEWS workshop organisers in that language
pair, and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more ?standard? runs, up to 4 in total. If more than
one ?standard? runs is submitted, it is required to
name one of them as a ?primary? run, which is
used to compare results across different systems.
In addition, up to 4 ?non-standard? runs could be
submitted for every language pair using either data
beyond that provided by the shared task organisers
or linguistic resources in a specific language, or
both. This essentially may enable any participant
to demonstrate the limits of performance of their
system in a given language pair.
The shared task timelines provide adequate time
for development, testing (approximately 1 month
after the release of the training data) and the final
result submission (7 days after the release of the
test data).
2.3 Shared Task Corpora
We considered two specific constraints in select-
ing languages for the shared task: language diver-
sity and data availability. To make the shared task
interesting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
2
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 12 tasks shown in Table 1 (Li et al,
2004; Kumaran and Kellner, 2007; MSRI, 2009;
CJKI, 2010).
NEWS 2010 leverages on the success of NEWS
2009 by utilizing the training and dev data of
NEWS 2009 as the training data of NEWS 2010
and the test data of NEWS 2009 as the dev data
of NEWS 2010. NEWS 2010 provides totally new
test data across all 12 tasks for evaluation. In ad-
dition to the 7 tasks inherited from NEWS 2009,
NEWS 2010 is enhanced with 5 new tasks, three
new languages (Arabic, Bangla and Thai) and two
back-transliteration (Chinese to English and Thai
to English).
The names given in the training sets for Chi-
nese, Japanese, Korean and Thai languages are
Western names and their respective translitera-
tions; the Japanese Name (in English)? Japanese
Kanji data set consists only of native Japanese
names; the Arabic data set consists only of native
Arabic names. The Indic data set (Hindi, Tamil,
Kannada, Bangla) consists of a mix of Indian and
Western names.
For all of the tasks chosen, we have been
able to procure paired names data between the
source and the target scripts and were able to
make them available to the participants. For
some language pairs, such as English-Chinese and
English-Thai, there are both transliteration and
back-transliteration tasks. Most of the task are just
one-way transliteration, although Indian data sets
contained mixture of names of both Indian and
Western origins. The language of origin of the
names for each task is indicated in the first column
of Table 1.
Finally, it should be noted here that the corpora
procured and released for NEWS 2010 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit results
of up to four standard and four non-standard runs.
One standard run must be named as the primary
submission and is used for the performance sum-
mary. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 4 evaluation
metrics capturing different aspects of translitera-
tion performance. We have dropped two MAP
metrics used in NEWS 2009 because they don?t
offer additional information to MAPref . Since a
name may have multiple correct transliterations,
all these alternatives are treated equally in the eval-
uation, that is, any of these alternatives is consid-
ered as a correct transliteration, and all candidates
matching any of the reference transliterations are
accepted as correct ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC = 1
N
N
?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) = 1
2
(|c|+ |r| ? ED(c, r)) (2)
3
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh
Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn
Western English Korean Hangul CJK Institute 5K 2K 2K EnKo
Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa
Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk
Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe
Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi
Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa
Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa
Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa
Western English Thai NECTEC 26K 2K 2K EnTh
Western Thai English NECTEC 24K 2K 2K ThEn
Table 1: Source and target languages for the shared task on transliteration.
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR = 1
N
N
?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N
?
i
1
ni
( ni
?
k=1
num(i, k)
)
(9)
4 Participation in Shared Task
7 teams from 5 countries and regions (Canada,
Hong Kong, India, Japan, Thailand) submitted
their transliteration results.
Two teams have participated in all or almost all
tasks while others participated in 1 to 4 tasks. Each
language pair has attracted on average around 4
teams. The details are shown in Table 3.
Teams are required to submit at least one stan-
dard run for every task they participated in. In
total, we receive 33 standard and 8. Table 2
shows the number of standard and non-standard
runs submitted for each task. It is clear that the
most ?popular? task is transliteration from English
to Hindi attempted by 5 participants. The next
most popular are other Indic scripts (Tamil, Kan-
nada, Bangla) and Thai, attempted by 3 partici-
pants. This is somewhat different from NEWS
2009, where the two most popular tasks were En-
glish to Hindi and English to Chinese translitera-
tion.
4
English to
Chinese
Chinese to
English
English to
Thai
Thai to En-
glish
English to
Hindi
English to
Tamil
Language pair code EnCh ChEn EnTh ThEn EnHi EnTa
Standard runs 5 2 2 2 7 3
Non-standard runs 0 0 1 1 2 1
English to
Kannada
English to
Japanese
Katakana
English
to Korean
Hangul
English to
Japanese
Kanji
Arabic to
English
English to
Bengali
(Bangla)
Language pair code EnKa EnJa EnKo JnJk ArAe EnBa
Standard runs 3 2 1 1 2 3
Non-standard runs 1 0 0 0 0 2
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
Team
ID
Organisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArAe EnBa
1? IIT, Bombay x
2 University of Alberta x x x x x x x x x x x x
3 x
4 City University of
Hong Kong
x x
5 NICT x x x x x x x x
6 x x
7 Jadavpur University x x x x
Table 3: Participation of teams in different tasks. ?Participation without a system paper.
5 Task Results and Analysis
5.1 Standard runs
All the results are presented numerically in Ta-
bles 4?15, for all evaluation metrics. These are the
official evaluation results published for this edition
of the transliteration shared task.
Among the four submitted system papers1,
Song et al (2010) and Finch and Sumita (2010)
adopt the approach of phrase-based statistical ma-
chine transliteration (Finch and Sumita, 2008),
an approach initially developed for machine trans-
lation (Koehn et al, 2003) while Das et al
(2010) adopts the approach of Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001). Jiampo-
jamarn et al (2010) further develop DirectTL ap-
proach presented at the previous NEWS work-
shop (Jiampojamarn et al, 2009), achieving very
good performance in the NEWS 2010.
An example of a completely language-
1To maintain anonymity, papers of the teams that submit-
ted anonymous results are not cited in this report.
independent approach is (Finch and Sumita,
2010). Other participants used language-
independent approach but added language-
specific pre- or post-processing (Jiampojamarn
et al, 2010; Das et al, 2010; Song et al, 2010),
including name origin recognition for English to
Hindi task (Jiampojamarn et al, 2010).
Combination of different models via re-ranking
of their outputs has been used in most of the sys-
tems (Das et al, 2010; Song et al, 2010; Finch and
Sumita, 2010). In fact, one system (Song et al,
2010) is mostly devoted to re-ranking of the sys-
tem output to achieve significant improvement of
the ACC (accuracy in top-1) results compared to
the same system in NEWS 2009 workshop (Song,
2009).
Compared the same seven tasks among the
NEWS 2009 and the NEWS 2010 (almost same
training sets, but different test sets), we can see
that the performance in the NEWS 2010 drops ex-
cept the English to Korean task. This could be due
to the fact that NEWS 2010 introduces a entirely
5
new test set, which come from different sources
than the train and dev sets, while NEWS 2009
have all train, dev and test sets from the same
sources.
As far as back-transliteration is concerned, we
can see that English-to-Thai and Thai-to-English
have the similar performance. However, Chinese-
to-English back transliteration performs much
worse than English-to-Chinese forward transliter-
ation. This could be due to the fact that Thai
and English are alphabet languages in nature while
Chinese is not. As a result, Chinese have much
fewer transliteration units than English and Thai.
In other words, Chinese to English translitera-
tion is a one-to-many mapping while English-to-
Chinese is a many-to-one mapping. The later one
has fewer mapping ambiguities.
5.2 Non-standard runs
For the non-standard runs there exist no restric-
tions on the use of data or other linguistic re-
sources. The purpose of non-standard runs is to
see how best personal name transliteration can be,
for a given language pair. In NEWS 2010, the ap-
proaches used in non-standard runs are typical and
may be summarised as follows:
? Pronunciation dictionaries to convert words
to their phonetic transcription (Jiampojamarn
et al, 2010).
? Web search. First, transliteration candidates
are generated. A Web search is then per-
formed to re-affirm or re-rank the candi-
dacy (Das et al, 2010).
Unfortunately, these additional knowledge used
in the non-standard runs is not helpful since all
non-standard runs perform worse than their cor-
responding standard runs. This would be an inter-
esting issue to look into.
6 Conclusions and Future Plans
The Transliteration Generation Shared Task in
NEWS 2010 shows that the community has a
continued interest in this area. This report sum-
marizes the results of the shared task. Again,
we are pleased to report a comprehensive cal-
ibration and baselining of machine translitera-
tion approaches as most state-of-the-art machine
transliteration techniques are represented in the
shared task. The most popular techniques such
as Phrase-Based Machine Transliteration (Koehn
et al, 2003), system combination and re-ranking,
are inspired by recent progress in statistical ma-
chine translation. As the standard runs are lim-
ited by the use of corpus, most of the systems are
implemented under the direct orthographic map-
ping (DOM) framework (Li et al, 2004). While
the standard runs allow us to conduct meaningful
comparison across different algorithms, we recog-
nise that the non-standard runs open up more op-
portunities for exploiting larger linguistic corpora.
It is also noted that two systems have reported
significant performance improvement over their
NEWS 2009 systems.
NEWS 2010 Shared Task represents a success-
ful debut of a community effort in driving machine
transliteration techniques forward. We would like
to continue this event in the future conference to
promote the machine transliteration research and
development.
Acknowledgements
The organisers of the NEWS 2010 Shared Task
would like to thank the Institute for Infocomm
Research (Singapore), Microsoft Research India,
CJK Institute (Japan) and National Electronics and
Computer Technology Center (Thailand) for pro-
viding the corpora and technical support. Without
those, the Shared Task would not be possible. We
thank those participants who identified errors in
the data and sent us the errata. We also want to
thank the members of programme committee for
their invaluable comments that improve the qual-
ity of the shared task papers. Finally, we wish to
thank all the participants for their active participa-
tion that have made this first machine translitera-
tion shared task a comprehensive one.
6
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc.
ACL-2002Workshop: Computational Apporaches to
Semitic Languages, Philadelphia, PA, USA.
CJKI. 2010. CJK Institute. http://www.cjk.org/.
Amitava Das, Tanik Saikh, Tapabrata Mondal, Asif Ek-
bal, and Sivaji Bandyopadhyay. 2010. English to
Indian languages machine transliteration system at
NEWS 2010. In Proc. ACL Named Entities Work-
shop Shared Task.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Andrew Finch and Eiichiro Sumita. 2010. Transliter-
ation using a phrase-based statistical machine trans-
lation system to re-score the output of a joint multi-
gram model. In Proc. ACL Named Entities Work-
shop Shared Task.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381,
Sanya, Hainan, China.
Yoav Goldberg andMichael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane
Bergsma, Aditya Bhargava, Qing Dou, Mi-Young
Kim, and Grzegorz Kondrak. 2010. Translitera-
tion generation and mining with limited training re-
sources. In Proc. ACL Named Entities Workshop
Shared Task.
Byung-Ju Kang and Key-Sun Choi. 2000.
English-Korean automatic transliteration/back-
transliteration system and character alignment. In
Proc. ACL, pages 17?18, Hong Kong.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Int?l.
Conf. Machine Learning, pages 282?289.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of NEWS 2009 machine
transliteration shared task. In Proc. Named Entities
Workshop at ACL 2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. ACL-IJCNLP 2009 Named
Entities Workshop ? Shared Task on Translitera-
tion. In Proc. Named Entities Workshop at ACL
2009.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002,
Taipei, Taiwan.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
7
Yan Song, Chunyu Kit, and Hai Zhao. 2010. Rerank-
ing with multiple features for better transliteration.
In Proc. ACL Named Entities Workshop Shared
Task.
Yan Song. 2009. Name entities transliteration via
improved statistical translation on character-level
chunks. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER, Sapporo, Japan.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
8
Team ID ACC F -score MRR MAPref Organisation
Primary runs
4 0.477333 0.740494 0.506209 0.455491 City University of Hong Kong
2 0.363333 0.707435 0.430168 0.347701 University of Alberta
Non-primary standard runs
2 0.362667 0.704284 0.428854 0.347500 University of Alberta
2 0.360333 0.706765 0.428990 0.345215 University of Alberta
2 0.357000 0.702902 0.419415 0.341567 University of Alberta
Table 4: Runs submitted for English to Chinese task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
4 0.226766 0.749237 0.268557 0.226090 City University of Hong Kong
2 0.137209 0.740364 0.197665 0.136702 University of Alberta
Table 5: Runs submitted for Chinese to English back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.391000 0.872526 0.505264 0.391000 NICT
2 0.377500 0.866254 0.467328 0.377500 University of Alberta
Non-standard runs
6 0.247000 0.842063 0.366959 0.247000
Table 6: Runs submitted for English to Thai task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.396690 0.872642 0.524511 0.396690 NICT
2 0.352056 0.861207 0.450472 0.352056 University of Alberta
Non-standard runs
6 0.092778 0.706995 0.131779 0.092778
Table 7: Runs submitted for Thai to English back-transliteration task.
9
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.456456 0.884199 0.559212 0.456456 University of Alberta
5 0.445445 0.883841 0.574195 0.445445 NICT
3 0.381381 0.860320 0.403172 0.381381
1 0.158158 0.810309 0.231594 0.158158 IIT, Bombay
7 0.150150 0.714490 0.307674 0.150150 Jadavpur University
Non-primary standard runs
2 0.456456 0.885122 0.558203 0.456456 University of Alberta
1 0.142142 0.799092 0.205945 0.142142 IIT, Bombay
Non-standard runs
7 0.254254 0.751766 0.369072 0.254254 Jadavpur University
7 0.170170 0.738777 0.314335 0.170170 Jadavpur University
Table 8: Runs submitted for English to Hindi task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.390000 0.890692 0.515298 0.390000 University of Alberta
5 0.390000 0.886560 0.522088 0.390000 NICT
7 0.013000 0.562917 0.121233 0.013000 Jadavpur University
Non-standard runs
7 0.082000 0.759856 0.142317 0.082000 Jadavpur University
Table 9: Runs submitted for English to Tamil task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.371000 0.871131 0.506010 0.371000 NICT
2 0.341000 0.867133 0.460189 0.341000 University of Alberta
7 0.056000 0.663196 0.111500 0.056000 Jadavpur University
Non-standard runs
7 0.055000 0.662106 0.168750 0.055000 Jadavpur University
Table 10: Runs submitted for English to Kannada task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.397933 0.791233 0.507828 0.398062 University of Alberta
5 0.378295 0.782682 0.510096 0.377778 NICT
Table 11: Runs submitted for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.553604 0.770168 0.672665 0.553835 University of Alberta
Table 12: Runs submitted for English to Korean task.
10
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.125937 0.426349 0.201497 0.127339 University of Alberta
Table 13: Runs submitted for English to Japanese Kanji back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.463679 0.923826 0.535097 0.265379 University of Alberta
5 0.403014 0.891443 0.512337 0.327418 NICT
Table 14: Runs submitted for Arabic to English task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.411705 0.882858 0.549913 0.411705 NICT
2 0.394551 0.876947 0.511876 0.394551 University of Alberta
7 0.232089 0.818470 0.325345 0.232089 Jadavpur University
Non-standard runs
7 0.429869 0.875349 0.526152 0.429869 Jadavpur University
7 0.369324 0.845273 0.450589 0.369324 Jadavpur University
Table 15: Runs submitted for English to Bengali (Bangla) task.
11
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 12?20,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Whitepaper of NEWS 2010 Shared Task on Transliteration Generation?
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2010 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be
released, on which the participants are expected
to produce a ranked list of transliteration candi-
dates in another language (i.e. n-best translitera-
tions), and this will be evaluated using common
metrics. For every language pair the participants
must submit at least one run that uses only the
data provided by the NEWS workshop organisers
in a given language pair (designated as ?standard?
run, primary submission). Users may submit more
?stanrard? runs. They may also submit several
?non-standard? runs for each language pair that
?http://translit.i2r.a-star.edu.sg/news2010/
use other data than those provided by the NEWS
2010 workshop; such runs would be evaluated and
reported separately.
2 Important Dates
Research paper submission deadline 1 May 2010
Shared task
Registration opens 1 Feb 2010
Registration closes 13 Mar 2010
Training/Development data release 19 Feb 2010
Test data release 13 Mar 2010
Results Submission Due 20 Mar 2010
Results Announcement 27 Mar 2010
Task (short) Papers Due 5 Apr 2010
For all submissions
Acceptance Notification 6 May 2010
Workshop Date 16 Jul 2010
3 Participation
1. Registration (1 Feb 2010)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (19 Feb 2010)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL 2010.
3. Evaluation Script (19 Feb 2010)
12
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (13 Mar 2010)
(a) The test data would be released on 13
March 2010, and the participants have a
maximum of 7 days to submit their re-
sults in the expected format.
(b) One ?standard? run must be submit-
ted from every group on a given lan-
guage pair. Additional ?standard? runs
may be submitted, up to 4 ?standard?
runs in total. However, the partici-
pants must indicate one of the submit-
ted ?standard? runs as the ?primary sub-
mission?. The primary submission will
be used for the performance summary.
In addition to the ?standard? runs, more
?non-standard? runs may be submitted.
In total, maximum 8 runs (up to 4 ?stan-
dard? runs plus up to 4 ?non-standard?
runs) can be submitted from each group
on a registered language pair. The defi-
nition of ?standard? and ?non-standard?
runs is in Section 5.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (27 Mar 2010)
(a) On 27 March 2010, the evaluation re-
sults would be announced and will be
made available on the Workshop web-
site.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Furthermore, all participants should
agree not to reveal identities of other
participants in any of their publications
unless you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.
6. Short Papers on Task (5 Apr 2010)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors? ideas
13
and methods can be understood by the
workshop participants. We are aiming
at accepting all system papers, and se-
lected ones will be presented orally in
the NEWS 2010 workshop.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2010/NEWS.
4 Language Pairs
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2010 Shared Task
offers 12 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2010 releases
training, development and testing data for each of
the language pairs. NEWS 2010 continues some
language pairs that were evaluated in NEWS 2009.
In such cases, the training and development data in
the release of NEWS 2010 may overlap with those
in NEWS 2009. However, the test data in NEWS
2010 are entirely new.
The names given in the training sets for Chi-
nese, Japanese, Korean and Thai languages are
Western names and their respective translitera-
tions; the Japanese Name (in English)? Japanese
Kanji data set consists only of native Japanese
names; the Arabic data set consists only of native
Arabic names. The Indic data set (Hindi, Tamil,
Kannada, Bangla) consists of a mix of Indian and
Western names.
Examples of transliteration:
English? Chinese
Timothy????
English? Japanese Katakana
Harrington??????
English? Korean Hangul
Bennett ? ??
Japanese name in English? Japanese Kanji
Akihiro???
English? Hindi
English? Tamil
English? Kannada
Arabic? Arabic name in English
? 
Khalid
????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 5K ? 32K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 2K ? 6K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 2K ? 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; MSRI, 2010; CJKI, 2010). NEWS
2010 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
14
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh
Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn
Western English Korean Hangul CJK Institute 5K 2K 2K EnKo
Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa
Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk
Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe
Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi
Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa
Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa
Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa
Western English Thai NECTEC 26K 2K 2K EnTh
Western Thai English NECTEC 24K 2K 2K ThEn
Table 1: Source and target languages for the shared task on transliteration.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a ?standard? run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.
5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must
be marked as ?non-standard?. For such ?non-
standard? runs, it is required to disclose the
size and characteristics of the data used in the
system paper.
6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 ?standard? run marked as ?pri-
mary submission?).
6 Paper Format
Paper submissions to NEWS 2010 should follow
the ACL 2010 paper submission policy, includ-
ing paper format, blind review policy and title and
author format convention. Full papers (research
paper) are in two-column format without exceed-
ing eight (8) pages of content plus one extra page
for references and short papers (task paper) are
also in two-column format without exceeding four
(4) pages, including references. Submission must
conform to the official ACL 2010 style guidelines.
For details, please refer to the ACL 2010 website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
2http://acl2010.org/authors.html
15
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ? ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
16
Dr. A. Kumaran
Microsoft Research India
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
Mr. Jack Halpern
CEO, The CJK Dictionary Institute, Inc.
Komine Building (3rd & 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
jack@cjki.org
References
[CJKI2010] CJKI. 2010. CJK Institute.
http://www.cjk.org/.
[Li et al2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159?166, Barcelona, Spain.
[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.
17
A Training/Development Data
? File Naming Conventions:
NEWS10 train XXYY nnnn.xml
NEWS10 dev XXYY nnnn.xml
NEWS10 test XXYY nnnn.xml
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
B Submission of Results
? File Naming Conventions:
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a ?standard? or ?non-standard? run, and if it
is a ?standard? run, whether it is the primary
submission.
? File formats:
All data will be made available in XML for-
mats (Figure 2).
? Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.
18
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2010-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=?1?>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=?2?>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2010 Train EnHi 25K.xml
19
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2010 EnHi TUniv 01 StdRunHMMBased.xml
20
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 21?28,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Report of NEWS 2010 Transliteration Mining Shared Task 
 
A Kumaran Mitesh M. Khapra Haizhou Li 
Microsoft Research India 
Bangalore, India 
Indian Institute of Technology Bombay 
Mumbai, India 
Institute for Infocomm  
Research, Singapore 
 
Abstract 
This report documents the details of the Trans-
literation Mining Shared Task that was run as 
a part of the Named Entities Workshop 
(NEWS 2010), an ACL 2010 workshop.  The 
shared task featured mining of name translite-
rations from the paired Wikipedia titles in 5 
different language pairs, specifically, between 
English and one of Arabic, Chinese, Hindi 
Russian and Tamil.  Totally 5 groups took part 
in this shared task, participating in multiple 
mining tasks in different languages pairs.  The 
methodology and the data sets used in this 
shared task are published in the Shared Task 
White Paper [Kumaran et al 2010]. We meas-
ure and report 3 metrics on the submitted re-
sults to calibrate the performance of individual 
systems on a commonly available Wikipedia 
dataset.  We believe that the significant contri-
bution of this shared task is in (i) assembling a 
diverse set of participants working in the area 
of transliteration mining, (ii) creating a base-
line performance of transliteration mining sys-
tems in a set of diverse languages using com-
monly available Wikipedia data, and (iii) pro-
viding a basis for meaningful comparison and 
analysis of trade-offs between various algo-
rithmic approaches used in mining.  We be-
lieve that this shared task would complement 
the NEWS 2010 transliteration generation 
shared task, in enabling development of prac-
tical systems with a small amount of seed data 
in a given pair of languages. 
1 Introduction  
Proper names play a significant role in Machine 
Translation (MT) and Information Retrieval (IR) 
systems.  When the systems involve multiple 
languages, The MT and IR system rely on Ma-
chine Transliteration systems, as the proper 
names are not usually available in standard trans-
lation lexicons. The quality of the Machine 
Transliteration systems plays a significant part in 
determining the overall quality of the system, 
and hence, they are critical for most multilingual 
application systems.  The importance of Machine 
Transliteration systems has been well understood 
by the community, as evidenced by significant 
publication in this important area. 
While research over the last two decades has 
shown that reasonably good quality Machine 
Transliteration systems may be developed easily, 
they critically rely on parallel names corpora for 
their development.  The Machine Transliteration 
Shared Task of the NEWS 2009 workshop 
(NEWS 2009) has shown that many interesting 
approaches exist for Machine Transliteration, 
and about 10-25K parallel names is sufficient for 
most state of the art systems to provide a practic-
al solution for the critical need.  The traditional 
source for crosslingual parallel data ? the bilin-
gual dictionaries ? offer only limited support as 
they do not include proper names (other than 
ones of historical importance).  The statistical 
dictionaries, though they contain parallel names, 
do not have sufficient coverage, as they depend 
on some threshold statistical evidence 1 . New 
names and many variations of them are intro-
duced to the vocabulary of a language every day 
that need to be captured for any good quality 
end-to-end system such as MT or CLIR.   So 
there is a perennial need for harvesting parallel 
names data, to support end-user applications and 
systems well and accurately. 
This is the specific focus of the Transliteration 
Mining Shared Task in NEWS 2010 workshop 
(an ACL 2010 Workshop): To mine accurately 
parallel names from a popular, ubiquitous source, 
the Wikipedia.  Wikipedia exists in more than 
250 languages, and every Wikipedia article has a 
link to an equivalent article in other languages2.  
We focused on this specific resource ? the Wiki-
pedia titles in multiple languages and the inter-
linking between them ? as the source of parallel 
names.  Any successful mining of parallel names 
from title would signal copious availability of 
parallel names data, enabling transliteration gen-
eration systems in many languages of the world. 
                                                 
1 In our experiments with Indian Express news corpo-
ra over 2 years shows that 80% of the names occur 
less than 5 times in the entire corpora. 
2 Note that the titles contain concepts, events, dates, 
etc., in addition to names.  Even when the titles are 
names, parts of them may not be transliterations. 
21
2 Transliteration Mining Shared Task 
In this section, we provide details of the shared 
task, and the datasets used for the task and results 
evaluation.  
2.1 Shared Task: Task Details 
The task featured in this shared task was to de-
velop a mining system for identifying single 
word transliteration pairs from the standard inter-
linked Wikipedia topics (aka, Wikipedia Inter-
Language Links, or WIL3) in one or more of the 
specified language pairs. The WIL?s link articles 
on the same topic in multiple languages, and are 
traditionally used as a parallel language resource 
for many natural language processing applica-
tions, such as Machine Translation, Crosslingual 
Search, etc.  Specific WIL?s of interest for our 
task were those that contained proper names ? 
either wholly or partly ? which can yield rich 
transliteration data.   
The task involved transliteration mining in the 
language pairs summarized in Table 1.  
 
Source 
Language 
Target Lan-
guage 
Track ID 
English  Chinese  WM-EnCn 
English  Hindi  WM-EnHi 
English  Tamil WM-EnTa 
English  Russian  WM-EnRu 
English Arabic WM-EnAr 
Table 1: Language Pairs in the shared task 
 
Each WIL consisted of a topic in the source 
and target language pair, and the task was to 
identify parts of the topic (in the respective lan-
guage titles) that are transliterations of each oth-
er. A seed data set (of about 1K transliteration 
pairs) was provided for each language pair, and 
was the only resource to be used for developing a 
mining system.  The participants were expected 
to produce a paired list of source-target single 
word named entities, for every WIL provided. At 
the evaluation time, a random subset of WIL?s 
(about 1K WIL?s) in each language pair were 
hand labeled, and used to test the results pro-
duced by the participants.  
Participants were allowed to use only the 1K 
seed data provided by the organizers to produce 
?standard? results; this restriction is imposed to 
provide a meaningful way of comparing the ef-
                                                 
3 Wikipedia?s Interlanguage Links: 
http://en.wikipedia.org/wiki/Help:Interlanguage_links
.  
fective methods and approaches.  However, 
?non-standard? runs were permitted where par-
ticipants were allowed to use more seed data or 
any language-specific resource available to them. 
2.2 Data Sets for the Task  
The following datasets were used for each lan-
guage pair, for this task.   
 
Training Data  Size Remarks 
Seed Data  
(Parallel 
names) 
~1K Paired names be-
tween source and 
target languages. 
To-be-mined 
Wikipedia In-
ter-Wiki-Link 
Data (Noisy) 
Vari-
able 
Paired named entities 
between source and 
target languages ob-
tained directly from 
Wikipedia 
Test Data 
 
~1K This was a subset of 
Wikipedia Inter-
Wiki-Link data, 
which was hand la-
beled for evaluation. 
Table 2: Datasets created for the shared task 
 
The first two sets were provided by the orga-
nizers to the participants, and the third was used 
for evaluation. 
 
Seed transliteration data:  In addition we pro-
vided approximately 1K parallel names in each 
language pair as seed data to develop any metho-
dology to identify transliterations.  For standard 
run results, only this seed data was to be used, 
though for non-standard runs, more data or other 
linguistics resources were allowed. 
 
English Names Hindi Names 
village ????? 
linden ?????? 
market ??????? 
mysore ????? 
Table 3: Sample English-Hindi seed data 
 
English Names Russian Names 
gregory ???????? 
hudson ?????? 
victor ?????? 
baranowski ??????????? 
Table 4: Sample English-Russian seed data 
 
To-Mine-Data WIL data:  All WIL?s were ex-
tracted from the Wikipedia around January 2010, 
22
and provided to the participants.  The extracted 
names were provided as-is, with no hand verifi-
cation about their correctness, completeness or 
consistency.  As sample of the WIL data for Eng-
lish-Hindi and English-Russian is shown in 
Tables 5 and 6 respectively.  Note that there are 
0, 1 or more single-word transliterations from 
each WIL. 
 
# English Wikipedia  
Title 
Hindi Wikipedia 
Title 
1 Indian National Congress ?????? ????????? ????????? 
2 University of Oxford ????????? ????????????? 
3 Indian Institute of Science 
?????? ??????? 
???????? 
4 Jawaharlal Nehru University 
???????? ????? 
?????????????  
Table 5: English-Hindi Wikipedia title pairs 
 
# English Wikipedia  
Title 
Russian Wikipedia 
Title 
1 Mikhail Gorbachev 
????????, ?????? 
????????? 
2 George Washington ?????????, ??????  
3 Treaty of Versailles ??????????? ??????? 
4 French Republic ??????? 
Table 6: English-Russian Wikipedia title pairs 
Test set:  We randomly selected ~1000 wikipe-
dia links (from the large noisy Inter-wiki-links) 
as test-set, and manually extracted the single 
word transliteration pairs associated with each of 
these WILs.  Please note that a given WIL can 
provide 0, 1 or more single-word transliteration 
pairs.  To keep the task simple, it was specified 
that only those transliterations would be consi-
dered correct that were clear transliterations 
word-per-word (morphological variations one or 
both sides are not considered transliterations) 
These 1K test set was be a subset of Wikipedia 
data provided to the user.  The gold dataset is as 
shown in Tables 7 and 8. 
 
WIL# English Names Hindi Names 
1 Congress ????????? 
2 Oxford ????????? 
3 <Null> <Null> 
4 Jawaharlal ???????? 
4 Nehru ????? 
  Table 7: Sample English-Hindi transliteration 
pairs mined from Wikipedia title pairs 
WIL# English Names Russian Names 
1 Mikhail ?????? 
1 Gorbachev ???????? 
2 George ?????? 
2 Washington ????????? 
3 Versailles ??????????? 
4 <Null> <Null> 
  Table 8: Sample English-Russian translitera-
tion pairs mined from Wikipedia title pairs 
2.3 Evaluation: 
The participants were expected to mine such sin-
gle-word transliteration data for every specific 
WIL, though the evaluation was done only 
against the randomly selected, hand-labeled test 
set.  A participant may submit a maximum of 10 
runs for a given language pair (including a min-
imum of one mandatory ?standard? run).  There 
could be more standard runs, without exceeding 
10 (including the non-standard runs). 
At evaluation time, the task organizers 
checked every WIL in test set from among the 
user-provided results, to evaluate the quality of 
the submission on the 3 metrics described later.  
3 Evaluation Metrics 
We measured the quality of the mining task us-
ing the following measures:  
1. PrecisionCorrectTransliterations(PTrans) 
2. RecallCorrectTransliteration  (RTrans) 
3. F-ScoreCorrectTransliteration (FTrans).   
Please refer to the following figures for the ex-
planations: 
 
A = True Positives (TP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant and were indeed "Correct Transliterations" 
as per the gold standard 
B = False Positives (FP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant but they were "Incorrect Transliterations" as 
per the gold standard. 
C = False Negatives (FN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant but were actually "Correct Translitera-
tions" as per the gold standard. 
D = True Negatives (TN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant and were indeed "Incorrect Translitera-
tions" as per the gold standard.  
 
23
Figure 1: Overview of the mining task and evaluation 
 
1. RecallCorrectTransliteration  (RTrans) 
The recall was computed using the sample as 
follows: 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
 
 
2. PrecisionCorrectTransliteration  (PTrans) 
The precision was computed using the sample as 
follows: 
?????? =
??
?? + ??
=  
?
? + ?
 
 
3. F-Score (F) 
? =
2 ? ?????? ? ??????
?????? + ??????
 
4 Participants & Approaches 
The following 5 teams participated in the Trans-
literation Mining Task?: 
 
# Team Organization 
1   Alberta University of Alberta, Canada 
2   CMIC Cairo Microsoft Innovation  
Centre, Egypt 
3   Groningen University of Groningen,  
Netherlands 
4   IBM Egypt IBM Egypt, Cairo, Egypt 
5   MINT? Microsoft Research India, India 
                                                 
? Non-participating system, included for reference.  
  Table 9: Participants in the Shared Task  
The approaches used by the 4 participating 
groups can be broadly classified as discrimina-
tive and generation based approaches. Discri-
minative approaches treat the mining task as a 
binary classification problem where the goal is to 
build a classifier that identifies whether a given 
pair is a valid transliteration pair or not. Genera-
tion based approaches on the other hand generate 
transliterations for each word in the source title 
and measure their similarity with the candidate 
words in the target title. Below, we give a sum-
mary of the various participating systems. 
The CMIC team (Darwish et. al., 2010) used a 
generative transliteration model (HMM) to trans-
literate each word in the source title and com-
pared the transliterations with the words appear-
ing in the target title. For example, for a given 
word Ei in the source title if the model generates 
a transliteration Fj which appears in the target 
title then (Ei, Fj) are considered as transliteration 
pairs. The results are further improved by using 
phonetic conflation (PC) and iteratively training 
(IterT) the generative model using the mined 
transliteration pairs. For phonetic conflation a 
modified SOUNDEX scheme is used wherein 
vowels are discarded and phonetically similar 
characters are conflated. Both, phonetic confla-
tion and iterative training, led to an increase in 
24
recall which was better than the corresponding 
decline in precision. 
The Alberta team (Jiampojamarn et. al., 2010) 
fielded 5 different systems in the shared task. 
The first system uses a simple edit distance based 
method where a pair of strings is classified as a 
transliteration pair if the Normalized Edit Dis-
tance (NED) between them is above a certain 
threshold. To calculate the NED, the target lan-
guage string is first Romanized by replacing each 
target grapheme by the source grapheme having 
the highest conditional probability. These condi-
tional probabilities are obtained by aligning the 
seed set of transliteration pairs using an M2M-
aligner approach (Jiampojamarn et. al., 2007). 
The second system uses a SVM based discrimin-
ative classifier trained using an improved feature 
representation (BK 2007) (Bergsma and Kon-
drak, 2007). These features include all substring 
pairs up to a maximum length of three as ex-
tracted from the aligned word pairs. The transli-
teration pairs in the seed data provided for the 
shared task were used as positive examples. The 
negative examples were obtained by generating 
all possible source-target pairs in the seed data 
and taking those pairs which are not translitera-
tions but have a longest common subsequence 
ratio above a certain threshold. One drawback of 
this system is that longer substrings cannot be 
used due to the combinatorial explosion in the 
number of unique features as the substring length 
increases. To overcome this problem they pro-
pose a third system which uses a standard n-gram 
string kernel (StringKernel) that implicitly em-
beds a string in a feature space that has one co-
ordinate for each unique n-gram (Shawe-Taylor 
and Cristianini, 2004). The above 3 systems are 
essentially discriminative systems. In addition, 
they propose a generation based approach (DI-
RECTL+) which determines whether the gener-
ated transliteration pairs of a source word and 
target word are similar to a given candidate pair. 
They use a state-of-the-art online discriminative 
sequence prediction model based on many-to-
many alignments, further augmented by the in-
corporation of joint n-gram features (Jiampoja-
marn et. al., 2010). Apart from the four systems 
described above, they propose an additional sys-
tem for English Chinese, wherein they formulate 
the mining task as a matching problem (Match-
ing) and greedily extract the pairs with highest 
similarity. The similarity is calculated using the 
alignments obtained by training a generation 
model (Jiampojamarn et. al., 2007) using the 
seed data. 
The IBM Cairo team (Noemans et. al., 2010) 
proposed a generation based approach which 
takes inspiration from Phrase Based Statistical 
Machine Translation (PBSMT) and learns a cha-
racter-to-character alignment model between the 
source and target language using GIZA++. This 
alignment table is then represented using a finite 
state automaton (FSA) where the input is the 
source character and the output is the target cha-
racter. For a given word in the source title, can-
didate transliterations are generated using this 
FST and are compared with the words in the tar-
get title. In addition they also submitted a base-
line run which used phonetic edit distance. 
The Groningen (Nabende et. al., 2010) team 
used a generation based approach that uses pair 
HMMs (P-HMM) to find the similarity between 
a given pair of source and target strings. The 
proposed variant of pair HMM uses transition 
parameters that are distinct between each of the 
edit states and emission parameters that are also 
distinct. The three edits states are substitution 
state, deletion state and insertion state. The pa-
rameters of the pair HMM are estimated using 
the Baum-Welch Expectation Maximization al-
gorithm (Baum et. al. 1970).  
Finally, as a reference, results of a previously 
published system ? MINT (Udupa et. al., 2009) ? 
were also included in this report as a reference.  
MINT is a large scalable mining system for min-
ing transliterations from comparable corpora, 
essentially multilingual news articles in the same 
timeline.  While MINT takes a two step approach 
? first aligning documents based on content simi-
larity, and subsequently mining transliterations 
based on a name similarity model ? for this task, 
only the transliteration mining step is employed. 
For mining transliterations a logistic function 
based similarity model (LFS) trained discrimina-
tively with the seed parallel names data was em-
ployed.  It should be noted here that the MINT 
algorithm was used as-is for mining translitera-
tions from Wikipedia paired titles, with no fine-
tuning.  While the standard runs used only the 
data provided by the organizers, the non-standard 
runs used about 15K (Seed+) parallel names be-
tween the languages. 
5 Results & Analysis 
The results for EnAr, EnCh, EnHi, EnRu and 
EnTa are summarized in Tables 10, 11, 12, 13 
and 14 respectively. The results clearly indicate 
that there is no single approach which performs 
well across all languages. In fact, there is even 
25
no single genre (discriminative v/s generation 
based) which performs well across all languages. 
We, therefore, do a case by case analysis of the 
results and highlight some important observa-
tions. 
? The discriminative classifier using string 
kernels proposed by Jiampojamarn et. al. 
(2010) consistently performed well in all the 
4 languages that it was tested on. Specifical-
ly, it gave the best performance for EnHi and 
EnTa. 
? The simple discriminative approach based on 
Normalized Edit Distance (NED) gave the 
best result for EnRu. Further, the authors re-
port that the results of StringKernel and BK-
2007 were not significantly better than NED. 
? The use of phonetic conflation consistently 
performed better than the case when phonet-
ic conflation was not used.  
? The results for EnCh are significantly lower 
when compared to the results for other lana-
guge pairs. This shows that mining translite-
ration pairs between alphabetic languages 
(EnRu, EnAr, EnHi, EnTa) is relatively easi-
er as compared to the case when one of the 
languages is non-alphabetic (EnCh) 
6 Plans for the Future Editions 
This shared task was designed as a comple-
mentary shared task to the popular NEWS 
Shared Tasks on Transliteration Generation; suc-
cessful mining of transliteration pairs demon-
strated in this shared task would be a viable 
source for generating data for developing a state 
of the art transliteration generation system.    
We intend to extend the scope of the mining in 
3 different ways: (i) extend mining to more lan-
guage pairs, (ii) allow identification of near 
transliterations where there may be changes do to 
the morphology of the target (or the source) lan-
guages, and, (iii) demonstrate an end-to-end 
transliteration system that may be developed 
starting with a small seed corpora of, say, 1000 
paired names. 
 
 
References  
Baum, L., Petrie, T., Soules, G. and Weiss, N. 1970. A 
Maximization Technique Occurring in the Statis-
tical Analysis of Probabilistic Functions of Markov 
Chains. In The Annals of Mathematical Statistics, 
41 (1): 164-171. 
Bergsma, S. and Kondrak, G. 2007. Alignment Based 
Discriminative String Similarity. In Proceedings of 
the 45th Annual Meeting of the ACL, 2007.  
Darwish, K. 2010. Transliteration Mining with Pho-
netic Conflation and Iterative Training. Proceed-
ings of the 2010 Named Entities Workshop: Shared 
Task on Transliteration Mining, 2010.  
Jiampojamarn, S., Dwyer, K., Bergsma, S., Bhargava, 
A., Dou, Q., Kim, M. Y. and Kondrak, G. 2010. 
Transliteration generation and mining with limited 
training resources. Proceedings of the 2010 
Named Entities Workshop: Shared Task on Trans-
literation Mining, 2010. 
Shawe-Taylor, J and Cristianini, N. 2004. Kernel Me-
thods for Pattern Analysis. Cambridge University 
Press. 
Klementiev, A. and Roth, D. 2006. Weakly supervised 
named entity transliteration and discovery from 
multilingual comparable corpora. Proceedings of 
the 44th Annual Meeting of the ACL, 2006.  
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion. Computational Linguistics.  
Kumaran, A., Khapra, M. and Li, Haizhou. 2010. 
Whitepaper on NEWS 2010 Shared Task on Trans-
literation Mining. Proceedings of the 2010 Named 
Entities Workshop: Shared Task on Transliteration 
Mining, 2010. 
Nabende, P. 2010. Mining Transliterations from Wi-
kipedia using Pair HMMs. Proceedings of the 2010 
Named Entities Workshop: Shared Task on Trans-
literation Mining, 2010. 
Noeman, S. and Madkour, A. 2010. Language inde-
pendent Transliteration mining system using Finite 
State Automata framework. Proceedings of the 
2010 Named Entities Workshop: Shared Task on 
Transliteration Mining, 2010.  
Udupa, R., Saravanan, K., Kumaran, A. and Jagarla-
mudi, J. 2009. MINT: A Method for Effective and 
Scalable Mining of Named Entity Transliterations 
from Large Comparable Corpora. Proceedings of 
the 12th Conference of the European Chapter of 
Association for Computational Linguistics, 2009.  
  
26
Participant Run Type Description Precision Recall F-Score 
IBM Egypt 
 
Standard 
FST, edit distance 2 with nor-
malized characters 0.887 0.945 0.915 
IBM Egypt 
 
Standard 
FST, edit distance 1 with nor-
malized characters 0.859 0.952 0.903 
IBM Egypt 
 
Standard 
Phonetic distance, with norma-
lized characters 0.923 0.830 0.874 
CMIC Standard HMM + IterT 0.886 0.817 0.850 
CMIC Standard HMM + PC 0.900 0.796 0.845 
CMIC Standard (HMM + ItertT) + PC 0.818 0.827 0.822 
Alberta Non- Standard  0.850 0.780 0.820 
Alberta Standard BK-2007 0.834 0.798 0.816 
Alberta Standard NED+ 0.818 0.783 0.800 
CMIC Standard (HMM + PC + ItertT) + PC 0.895 0.678 0.771 
Alberta Standard DirecTL+ 0.861 0.652 0.742 
CMIC Standard HMM 0.966 0.587 0.730 
CMIC Standard HMM + PC + IterT 0.952 0.588 0.727 
IBM Egypt 
 
Standard 
FST, edit distance 2 without 
normalized characters 0.701 0.747 0.723 
IBM Egypt 
 
Standard 
FST, edit distance 1 without 
normalized characters 0.681 0.755 0.716 
IBM Egypt 
 
Standard 
Phonetic distance, without 
normalized characters 0.741 0.666 0.702 
Table 10: Results of the English Arabic task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard Matching 0.698 0.427 0.530 
Alberta Non-Standard  0.700 0.430 0.530 
CMIC Standard (HMM + IterT) + PC 1 0.030 0.059 
CMIC Standard HMM + IterT 1 0.026 0.05 
CMIC Standard HMM + PC 1 0.024 0.047 
CMIC Standard (HMM + PC + IterT) + PC 1 0.022 0.044 
CMIC Standard HMM 1 0.016 0.032 
CMIC Standard HMM + PC + IterT 1 0.016 0.032 
Alberta Standard DirecTL+ 0.045 0.005 0.009 
Table 11: Results of the English Chinese task 
 
Participant Run Type Description Precision Recall F-Score 
MINT? Non-Standard LFS + Seed+ 0.967 0.923 0.944 
Alberta  Standard StringKernel 0.954 0.895 0.924 
Alberta Standard NED+ 0.875 0.941 0.907 
Alberta Standard DirecTL+ 0.945 0.866 0.904 
CMIC Standard (HMM + PC + IterT) + PC 0.953 0.855 0.902 
Alberta Standard BK-2007 0.883 0.880 0.882 
CMIC Standard (HMM + IterT) + PC  0.951 0.812 0.876 
CMIC Standard HMM + PC 0.959 0.786 0.864 
Alberta Non-Standard  0.890 0.820 0.860 
MINT? Standard LFS 0.943 0.780 0.854 
MINT? Standard LFS 0.946 0.773 0.851 
                                                 
? Non-participating system 
27
CMIC Standard HMM + PC + IterT 0.981 0.687 0.808 
CMIC Standard HMM + IterT 0.984 0.569 0.721 
CMIC Standard HMM 0.987 0.559 0.714 
Table 10: Results of the English Hindi task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard NED+ 0.880 0.869 0.875 
CMIC Standard HMM + PC 0.813 0.839 0.826 
MINT? Non-Standard LFS + Seed+ 0.797 0.853 0.824 
Groningen? Standard P-HMM 0.780 0.834 0.806 
Alberta Standard StringKernel 0.746 0.889 0.811 
CMIC Standard HMM 0.868 0.748 0.804 
CMIC Standard HMM + PC + IterT 0.843 0.747 0.792 
Alberta Non-Standard  0.730 0.870 0.790 
Alberta Standard DirecTL+ 0.778 0.795 0.786 
CMIC Standard HMM + IterT 0.716 0.868 0.785 
MINT? Standard LFS 0.822 0.752 0.785 
CMIC Standard (HMM + PC + IterT) + PC 0.771 0.794 0.782 
Alberta Standard BK-2007 0.684 0.902 0.778 
CMIC Standard (HMM + IterT) + PC 0.673 0.881 0.763 
Groningen Standard P-HMM 0.658 0.334 0.444 
Table 11: Results of the English Russian task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard StringKernel 0.923 0.906 0.914 
MINT? Non-Standard LFS + Seed+ 0.910 0.897 0.904 
MINT? Standard LFS 0.899 0.814 0.855 
MINT? Standard LFS 0.913 0.790 0.847 
Alberta Standard BK-2007 0.808 0.852 0.829 
CMIC Standard (HMM + IterT) + PC 0.939 0.741 0.828 
Alberta Non-Standard  0.820 0.820 0.820 
Alberta Standard DirectL+ 0.919 0.710 0.801 
Alberta Standard NED+ 0.916 0.696 0.791 
CMIC Standard HMM + IterT 0.952 0.668 0.785 
CMIC Standard HMM + PC 0.963 0.604 0.743 
CMIC Standard (HMM + PC + IterT) + PC 0.968 0.567 0.715 
CMIC Standard HMM + PC + IterT 0.975 0.446 0.612 
CMIC Standard HMM 0.976 0.407 0.575 
Table 12: Results of the English Tamil task 
 
                                                 
? Non-participating system 
? Post-deadline submission of the participating system 
28
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 29?38,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Whitepaper of NEWS 2010 Shared Task on  
Transliteration Mining 
A Kumaran Mitesh M. Khapra Haizhou Li 
Microsoft Research India 
Bangalore, India 
Indian Institute of Technology-Bombay 
Mumbai, India 
Institute for Infocomm  
Research, Singapore 
 
 
 
Abstract 
Transliteration is generally defined as phonetic 
translation of names across languages. Ma-
chine Transliteration is a critical technology in 
many domains, such as machine translation, 
cross-language information retriev-
al/extraction, etc. Recent research has shown 
that high quality machine transliteration sys-
tems may be developed in a language-neutral 
manner, using a reasonably sized good quality 
corpus (~15-25K parallel names) between a 
given pair of languages.  In this shared task, 
we focus on acquisition of such good quality 
names corpora in many languages, thus com-
plementing the machine transliteration shared 
task that is concurrently conducted in the same 
NEWS 2010 workshop.  Specifically, this task 
focuses on mining the Wikipedia paired enti-
ties data (aka, inter-wiki-links) to produce 
high-quality transliteration data that may be 
used for transliteration tasks. 
1 Task Description  
The task is to develop a system for mining single 
word transliteration pairs from the standard Wi-
kipedia paired topics (aka, Wikipedia Inter-
Language Links, or WIL1) in one or more of the 
specified language pairs. The WIL?s link articles 
on the same topic in multiple languages, and are 
traditionally used as a parallel language resource 
for many NLP applications, such as Machine 
Translation, Crosslingual Search, etc.  Specific 
WIL?s of interest for our task are those that con-
tain proper names ? either wholly or partly ? 
which can yield rich transliteration data.   
Each WIL consists of a topic in the source and 
the language pair, and the task is to identify parts 
of the topic (in the respective language titles) that 
are transliterations of each other. A seed data set 
(of about 1K transliteration pairs) would be pro-
vided for each language pair, and are the only 
resource to be used for developing a mining sys-
tem.  The participants are expected to produce a 
                                                 
1
 Wikipedia?s Interlanguage Links:  
http://en.wikipedia.org/wiki/Help:Interlanguage_links.  
paired list of source-target single word named 
entities, for every WIL provided. At the evalua-
tion time, a random subset of WIL?s (about 1K 
WIL?s) in each language pair that are hand la-
beled would be used to test the results produced 
by the participants.  
Participants may use only the 1K seed data 
provided by the organizers to produce ?standard? 
results; this restriction is imposed to provide a 
meaningful way of comparing the effective me-
thods and approaches.  However, ?non-standard? 
runs would be permitted where participants may 
use more seed data or any language-specific re-
source available to them. 
2 Important Dates  
SHARED TASK SCHEDULES 
Registration Opens  1-Feb-2010 
Registration Closes   13-Mar-2010 
Training Data Release  26 -Feb-2010 
Test Data Release  13-Mar-2010 
Results Submission Due  20-Mar-2010 
Evaluation Results An-
nouncement 27-Mar-2010 
Short Papers Due  5-Apr-2010 
Workshop Paper Sub-
mission Closes  5-Apr-2010 
Workshop & Task Pa-
pers Acceptance  6-May-2010 
CRC Due  15-May-2010 
Workshop Date   16-Jul-2010 
3 Participation 
1. Registration (1 Feb 2010) 
a. Prospective participants are to register to 
the NEWS-2010 Workshop homepage, for 
this specific task. 
2. Training Data Release (26 Feb 2010) 
a. Registered participants are to obtain seed 
and Wikipedia data from the Shared Task 
organizers. 
 
29
3. Evaluation Script (1 March 2010) 
a. A sample submission and an evaluation 
script will be released in due course. 
b. The participants must make sure that their 
output is produced in a way that the evalua-
tion script may run and produce the ex-
pected output. 
c. The same script (with held out test data and 
the user outputs) would be used for final 
evaluation. 
 
4. Testing data (13 March 2010) 
a. The test data would be a held out data of 
approximately 1K ?gold-standard? mined 
data. 
b. The submissions (up to 10) would be tested 
against the test data, and the results pub-
lished. 
 
5. Results (27 March 2010) 
a. On the results announcement date, the 
evaluation results would be published on 
the Workshop website. 
b. Note that only the scores (in respective me-
trics) of the participating systems on each 
language pairs would be published, but no 
explicit ranking of the participating sys-
tems.   
c. Note that this is a shared evaluation task 
and not a competition; the results are meant 
to be used to evaluate systems on common 
data set with common metrics, and not to 
rank the participating systems.  While the 
participants can cite the performance of 
their systems (scores on metrics) from the 
workshop report, they should not use any 
ranking information in their publications. 
d. Further, all participants should agree not to 
reveal identities of other participants in any 
of their publications unless you get permis-
sion from the other respective participants. 
If the participants want to remain anonym-
ous in published results, they should inform 
the organizers at the time of registration.  
Note that the results of their systems would 
still be published, but with the participant 
identities masked. As a result, in this case, 
your organization name will still appear in 
the web site as one of participants, but it is 
not linked explicitly with your results. 
 
6. Short Papers on Task (5 April 2010) 
a. Each submitting site is required to submit a 
4-page system paper (short paper) for its 
submissions, including their approach, data 
used and the results. 
b. All system short papers will be included in 
the proceedings. Selected short papers will 
be presented in the NEWS 2010 workshop.  
Acceptance of the system short-papers 
would be announced together with that of 
other papers. 
4 Languages Involved  
The task involves transliteration mining in the 
language pairs summarized in the following ta-
ble.   
   
Source Lan-
guage 
Target Lan-
guage 
Track ID 
English  Chinese  WM-EnCn 
English  Hindi  WM-EnHi 
English  Tamil WM-EnTa 
English  Russian  WM-EnRu 
English Arabic WM-EnAr 
Table 1: Language Pairs in the shared task 
5 Data Sets for the Task  
The following datasets are used for each lan-
guage pair, for this task.   
 
Training Data  Size Remarks 
Seed Data (Pa-
rallel) 
~1K Paired names be-
tween source and 
target languages. 
To-be-mined 
Wikipedia Inter-
Wiki-Link Data 
(Noisy) 
Vari-
able 
Paired named entities 
between source and 
target languages ob-
tained directly from 
Wikipedia 
Test Data 
 
~1K This is a subset of 
Wikipedia Inter-
Wiki-Link data, 
which will be hand 
labeled. 
Table 2: Datasets for the shared task 
The first two sets would be provided by the or-
ganizers to the participants, and the third will be 
used for evaluation. 
 
To-Mine-Data WIL data:  All WIL?s from an 
appropriate download from Wikipedia would be 
provided.  The WIL data might look like the 
samples shown in Tables 3 and 4, with the sin-
30
gle-word transliterations highlighted.  Note that 
there could be 0, 1 or more single-word translite-
rations from each WIL. 
 
# English Wikipedia  
Title 
Hindi Wikipedia 
Title 
1 Indian National Congress ?????? ????????? ????????? 
2 University of Oxford ????????? 
????????????? 
3 Indian Institute of Science ?????? ??????? 
???????? 
4 Jawaharlal Nehru Univer-
sity 
???????? ????? 
?????????????  
Table 3: Sample English-Hindi Wikipedia title 
pairs 
 
# English Wikipedia  
Title 
Russian Wikipedia 
Title 
1 Mikhail Gorbachev ????????, ?????? 
????????? 
2 George Washington ?????????, ??????  
3 Treaty of Versailles ??????????? ??????? 
4 French Republic ??????? 
Table 4: Sample English-Russian Wikipedia title 
pairs 
Seed transliteration data:  In addition we pro-
vide approximately 1K parallel names in each 
language pair as seed data to develop any metho-
dology to identify transliterations.  For standard 
run results, only this seed data could be used, 
though for non-standard runs, more data or other 
linguistics resources may be used. 
English Names Hindi Names 
Village ????? 
Linden ??????? 
Market ????? 
Mysore ????? 
Table 5: Sample English-Hindi seed data 
 
English Names Russian Names 
Gregory ???????? 
Hudson ?????? 
Victor ?????? 
baranowski ??????????? 
Table 6: Sample English-Russian seed data 
 
Test set:  We plan to randomly select ~1000 wi-
kipedia links (from the large noisy Inter-wiki-
links) as test-set, and manually extract the single 
word transliteration pairs associated with each of 
these WILs.  Please note that a given WIL can 
provide 0, 1 or more single-word transliteration 
pairs.  To keep the task simple, we consider as 
correct transliterations only those that are clear 
transliterations word-per-word (morphological 
variations one or both sides are not considered 
transliterations) These 1K test set will be a subset 
of Wikipedia data provided to the user.  The gold 
dataset might look like the following (assuming 
the items 1, 2, 3 and 4 in Tables 3 and 4 were 
among the randomly selected WIL?s from To-
Mine-Data).   
 
WIL# English Names Hindi Names 
1 Congress ????????? 
2 Oxford ????????? 
3 <Null> <Null> 
4 Jawaharlal ???????? 
4 Nehru ????? 
  Table 7: Sample English-Hindi transliteration 
pairs mined from Wikipedia title pairs 
 
WIL# English Names Russian Names 
1 Mikhail ?????? 
1 Gorbachev ???????? 
2 George ?????? 
2 Washington ????????? 
3 Versailles ??????????? 
4 <Null> <Null> 
  Table 8: Sample English-Russian translitera-
tion pairs mined from Wikipedia title pairs 
 
Evaluation: The participants are expected to 
mine such single-word transliteration data for 
every specific WIL, though the evaluation would 
be done only against the randomly selected, 
hand-labeled test set.  At evaluation time, the 
task organizers check every WIL in test set from 
among the user-provided results, to evaluate the 
quality of the submission on the 3 metrics de-
scribed later.  
Additional information on data use: 
1. Seed data may have ownership and appropri-
ate licenses may need to be procured for use.  
2. To-be-mined Wikipedia data is extracted 
from Wikipedia (in Jan/Feb 2010), and dis-
tributed as-is.  No assurances that they are 
correct, complete or consistent. 
 
31
Figure 1: Overview of the mining task and evaluation 
 
3. The hand-labeled test set is created by 
NEWS shared task organizers, and will be 
used for computing the metrics for a given 
submission. 
4. We expect that the participants to use only 
the seed data (parallel names) provided by 
the Shared Task for a standard run to ensure 
a fair evaluation and a meaningful compari-
son between the effectiveness of approaches 
taken by various systems.  At least one such 
run (using only the data provided by the 
shared task) is mandatory for all participants 
for a given task that they participate in.   
5. If more data (either parallel names data or 
monolingual data), or any language-specific 
modules were used, then all such runs using 
extra data or resources must be marked as 
?Non-standard?.  For such non-standard 
runs, it is required to disclose the size and 
characteristics of the data or the nature of 
languages resources used, in their paper. 
6. A participant may submit a maximum of 10 
runs for a given language pair (including one 
or more ?standard? run).  There could be 
more standard runs, without exceeding 10 
(including the non-standard runs). 
6 Paper Format 
All paper submissions to NEWS 2010 should 
follow the ACL 2010 paper submission policy 
(http://acl2010.org/papers.html), including paper 
format, blind review policy and title and author 
format convention. Shared task system short pa-
pers are also in two-column format without ex-
ceeding four (4) pages plus any extra page for 
references. However, there is no need for double-
blind requirements, as the users may refer to 
their runs and metrics in the published results.   
7 Evaluation Metrics 
We plan to measure the quality of the mining 
task using the following measures:  
 
1. PrecisionCorrectTransliterations (PTrans) 
2. RecallCorrectTransliteration (RTrans) 
3. F-ScoreCorrectTransliteration (FTrans).   
 
Please refer to the following figures for the ex-
planations: 
 
A = True Positives (TP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant and were indeed "Correct Transliterations" 
as per the gold standard 
B = False Positives (FP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant but they were "Incorrect Transliterations" as 
per the gold standard. 
C = False Negatives (FN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant but were actually "Correct Translitera-
tions" as per the gold standard. 
32
D = True Negatives (TN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant and were indeed "Incorrect Translitera-
tions" as per the gold standard. 
 
1. RecallCorrectTransliteration  (RTrans) 
The recall is going to be computed using the 
sample as follows: 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
 
 
2. PrecisionCorrectTransliteration  (PTrans) 
The precision is going to be computed using the 
sample as follows: 
?????? =
??
?? + ??
=  
?
? + ?
 
3. F-Score (F) 
? =
2 ? ?????? ? ??????
?????? + ??????
 
8 Contact Us 
If you have any questions about this share task 
and the database, please contact one of the orga-
nizers below: 
 
Dr. A. Kumaran 
 Microsoft Research India 
Bangalore 560080 INDIA 
a.kumaran@microsoft.com 
 
Mitesh Khapra  
 Indian Institute of Technology-Bombay 
 Mumbai, INDIA 
MKhapra@cse.iitb.ac.in.  
 
Dr Haizhou Li 
 Institute for Infocomm Research 
 Singapore, SINGAPORE 138632 
hli@i2r.a-star.edu.sg.  
  
33
Appendix A: Seed Parallel Names Data  
 
? File Naming Conventions: 
o NEWS09_Seed_XXYY_1K.xml,  
? XX: Source Language 
? YY: Target Language 
? 1K: number of parallel names  
 
? File Formats:  
o All data would be made available in XML formats (Appendix A). 
 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only, and in the XML format specified. 
 
File: NEWS2009_Seed_EnHi_1000.xml 
 
<?xml version="1.0" encoding="UTF-8"?> 
 <SeedCorpus 
      CorpusID = "NEWS2009-Seed-EnHi-1K" 
     SourceLang = "English" 
     TargetLang = "Hindi" 
     CorpusType = "Seed" 
     CorpusSize = "1000" 
     CorpusFormat = "UTF8"> 
  <Name ID=?1?> 
   <SourceName>eeeeee1</SourceName> 
   <TargetName ID="1">hhhhhh1_1</TargetName> 
   <TargetName ID="2">hhhhhh1_2</TargetName> 
   ... 
   <TargetName ID="n">hhhhhh1_n</TargetName> 
  </Name> 
  <Name ID=?2?> 
   <SourceName>eeeeee2</SourceName> 
   <TargetName ID="1">hhhhhh2_1</TargetName> 
   <TargetName ID="2">hhhhhh2_2</TargetName> 
   ... 
   <TargetName ID="m">hhhhhh2_m</TargetName> 
  </Name> 
... 
  <!-- rest of the names to follow --> 
  ... 
 </SeedCorpus> 
 
 
Appendix B: Wikipedia InterwikiLinks Data  
 
? File Naming Conventions: 
o NEWS09_Wiki_XXYY_nnnn.xml,  
? XX: Source Language 
? YY: Target Language 
? nnnn: size of paired entities culled from Wikipedia (?25K?, ?10000?, etc.) 
? File Formats:  
o All data would be made available in XML formats (Appendix A). 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only, and in the XML format specified. 
 
 
34
File: NEWS2009_Wiki_EnHi_10K.xml 
<?xml version="1.0" encoding="UTF-8"?> 
 <WikipediaCorpus 
      CorpusID = "NEWS2009-Wiki-EnHi-10K" 
     SourceLang = "English" 
     TargetLang = "Hindi" 
     CorpusType = "Wiki" 
     CorpusSize = "10000" 
     CorpusFormat = "UTF8"> 
  <Title ID=?1?> 
   <SourceEntity>e1 e2 ? en</SourceEntity> 
   <TargetEntity>h1 h2 ? hm</TargetEntity> 
  </Title> 
  <Title ID=?2?> 
   <SourceEntity>e1 e2 ? ei</SourceEntity> 
   <TargetEntity>h1 h2 ? hj</TargetEntity> 
  </Title> 
... 
  <!-- rest of the titles to follow --> 
  ... 
 </ WikipediaCorpus> 
 
 
Appendix C: Results Submission - Format 
 
? File Naming Conventions: 
o NEWS09_Result_XXYY_gggg_nn_description.xml 
? XX: Source 
? YY: Target 
? gggg: Group ID 
? nn: run ID.  
? description: Description of the run 
? File Formats:  
o All results would be submitted in XML formats (Appendix B). 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only. 
Example: NEWS2009_EnHi_TUniv_01_HMMBased.xml 
 
<?xml version="1.0" encoding="UTF-8"?> 
 <WikipediaMiningTaskResults 
      SourceLang = "English" 
     TargetLang = "Hindi" 
     GroupID = "Trans University" 
     RunID = "1" 
     RunType = "Standard" 
    Comments = "SVD Run with params: alpha=xxx beta=yyy"> 
  <Title ID="1"> 
   <MinedPair ID="1"> 
<SourceName>e1</SourceName> 
    <TargetName>h1</TargetName> 
</MinedPair> 
   <MinedPair ID="2"> 
<SourceName>e2</SourceName> 
    <TargetName>h2</TargetName> 
</MinedPair> 
    <!?followed by other pairs mined from this title--> 
  </Title> 
  <Title ID="2"> 
   <MinedPair ID="1"> 
<SourceName>e1</SourceName> 
    <TargetName>h1</TargetName> 
</MinedPair> 
35
   <MinedPair ID="2"> 
<SourceName>e2</SourceName> 
    <TargetName>h2</TargetName> 
</MinedPair> 
   <!?followed by other pairs mined from this title--> 
  </Title> 
... 
  <!-- All titles in the culled data to follow --> 
  ... 
 </WikipediaMiningTaskResults> 
 
 
Appendix D: Sample Eng-Hindi Interwikilink Data 
 
<?xml version="1.0" encoding="UTF-8"?>  
<WikipediaCorpus CorpusID = "NEWS2009-Wiki-EnHi-Sample"  
SourceLang = "English"  
TargetLang = "Hindi"  
CorpusType = "Wiki" CorpusSize = "3" 
 CorpusFormat = "UTF8"> 
  <Title ID="1"> 
  <SourceEntity>Indian National Congress</SourceEntity> 
  <TargetEntity>?????? ????????? ?????????</TargetEntity> 
 </Title> 
<!-- {Congress, ?????????} should be identified by the paricipants--> 
 <Title ID="2"> 
  <SourceEntity>University of Oxford</SourceEntity> 
  <TargetEntity>????????? ?????????????</TargetEntity> 
 </Title> 
<!-- {Oxford, ?????????} should be identified by the paricipants--> 
 <Title ID="3"> 
  <SourceEntity>Jawaharlal Nehru University</SourceEntity> 
  <TargetEntity>???????? ????? ?????????????</TargetEntity> 
 </Title> 
<!-- {Jawaharlal, ????????} and {Nehru, ?????} should be  
identified by the paricipants--> 
 <Title ID="4"> 
  <SourceEntity>Indian Institute Of Science</SourceEntity> 
  <TargetEntity>?????? ??????? ????????</TargetEntity> 
 </Title> 
<!--There are no transliteration pairs here --> 
</WikipediaCorpus> 
 
 
Appendix E: Eng-Hindi Gold Mined Data (wrt the above WIL Data) 
 
<?xml version="1.0" encoding="UTF-8"?> 
<WikipediaMiningTaskResults 
 SourceLang = "English" 
 TargetLang = "Hindi" 
 GroupID = "Gold-Standard" 
 RunID = "" 
 RunType = "" 
Comments = ""> 
 <Title ID="1"> 
  <MinedPair ID="1"> 
   <SourceName>Congress</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="2"> 
  <MinedPair ID="1"> 
36
   <SourceName>Oxford</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="3"> 
  <MinedPair ID="1"> 
   <SourceName>Jawaharlal</SourceName> 
   <TargetName> ????????</TargetName> 
  </MinedPair> 
  <MinedPair ID="2"> 
   <SourceName>Nehru</SourceName> 
   <TargetName> ?????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="4"> 
 </Title> 
</WikipediaMiningTaskResults> 
 
 
Appendix F: English-Hindi Sample Submission and Evaluation 
 
<?xml version="1.0" encoding="UTF-8"?> 
<WikipediaMiningTaskResults 
 SourceLang = "English" 
 TargetLang = "Hindi" 
 GroupID = "Gold-Standard" 
 RunID = "" 
 RunType = "" 
 <Title ID="1"> 
  <MinedPair ID="1"> 
   <SourceName>Congress</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
The participant mined all correct transliteration pairs  
 </Title> 
 <Title ID="2"> 
  <MinedPair ID="1"> 
   <SourceName>Oxford</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
  <MinedPair ID="1"> 
   <SourceName>University</SourceName> 
   <TargetName>?????????????</TargetName> 
  </MinedPair> 
The participant mined an incorrect transliteration pair {University,?????????????} 
 </Title> 
 <Title ID="3"> 
  <MinedPair ID="1"> 
   <SourceName>Jawaharlal</SourceName> 
   <TargetName> ????????</TargetName> 
  </MinedPair> 
The participant missed the correct transliteration pair {Nehru, ?????} 
 </Title> 
 <Title ID="4"> 
  <MinedPair ID="1"> 
   <SourceName>Indian</SourceName> 
   <TargetName>??????</TargetName> 
  </MinedPair> 
The participant mined an incorrect transliteration pair {Indian, ??????} 
 </Title> 
</WikipediaMiningTaskResults> 
 
37
Sample Evaluation 
T = |{(Congress, ?????????), (Oxford, ?????????), (Jawaharlal, ????????),(Nehru, ?????)} | = 4 
A = TP = | {(Congress, ?????????), (Oxford, ?????????), (Jawaharlal, ????????)}| = 3 
B = FP = |{(Indian, ??????), (University, ?????????????) }| = 2 
C = FN = |{(Nehru, ?????)}| = 1 
 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
=  
3
4
= 0.75 
 
?????? =
??
??+??
=  
?
?+?
=  
3
5
= 0.60 
 
? =
2 ? ?????? ? ??????
?????? + ??????
=  
2 ? 0.6 ? 0.75
0.6 + 0.75
=  0.67 
 
38
