Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 2?7,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Automatic extraction of NV expressions in Basque: basic issues on
cooccurrence techniques
Antton Gurrutxaga
Elhuyar Foundation
a.gurrutxaga@elhuyar.com
In?aki Alegria
IXA group/Univ. of the Basque Country
i.alegria@ehu.es
Abstract
Taking as a starting-point the development
on cooccurrence techniques for several lan-
guages, we focus on the aspects that should
be considered in a NV extraction task for
Basque. In Basque, NV expressions are con-
sidered those combinations in which a noun,
inflected or not, is co-occurring with a verb, as
erabakia hartu (?to make a decision?), kontuan
hartu (?to take into account?) and buruz jakin
(?to know by heart?). A basic extraction sys-
tem has been developed and evaluated against
two references: a) a reference which includes
NV entries from several lexicographic works;
and b) a manual evaluation by three experts of
a random sample from the n-best lists.
1 Introduction
The last decade has witnessed great advances in the
automatic identification and processing of MWEs.
In the case of Basque, advances are limited to termi-
nology extraction and the tagging in corpora of the
MWEs represented in lexical databases.
Furthermore, the work on both theoretical and
practical phraseology in Basque has been mainly fo-
cused on idiomatic expressions, leaving aside col-
locations (Pe?rez Gaztelu et al, 2004). As a con-
sequence, Basque NLP and lexicography have not
benefited from the approach that emphasized the im-
portance of such units, and very important areas are
underdeveloped.
With the aim of taking steps to turn this situa-
tion, we undertake the task of extracting NV com-
binations from corpora. As a preliminary step, we
must face the morphosyntactic aspects of Basque
that might influence the efficiency of the process.
2 MWE: basic definition and extraction
techniques
As a basis for our work, we take idiomaticity as
the key feature for the definition and classifica-
tion of MWE. Idiomaticity could be described as a
non-discrete magnitude, whose ?value?, according
to recent investigations (Baldwin and Kim, 2010;
Fazly and Stevenson, 2007; Granger and Paquot,
2008), has turned to depend on a complex combi-
nation of features such as institutionalization, non-
compositionality and lexico-syntactic fixedness.
The idiomaticity of MWEs appears rather as a
continuum than as a scale of discrete values (Sin-
clair, 1996; Wulff, 2010). Thus, the classifica-
tion of MWEs into discrete categories is a difficult
task. Taking Cowie?s classification as an initial basis
(Cowie, 1998), our work is focused on phrase-like
units, aiming, at this stage, to differentiate MWEs
(idioms and collocations) from free combinations.
Specifically, NV combinations with the following
characteristics are considered as MWEs:
? Idioms: non-compositional combinations, as
opaque idioms (adarra jo: ?to pull somebody?s
leg?; lit: ?to play the horn?) and figurative id-
ioms (burua hautsi: ?to rack one?s brain?; lit:
?to break one?s head?).
? Collocations:
? Semicompositional combinations, in
which the noun keeps its literal meaning,
2
whereas the verb acts as a support verb
(lan egin: ?to work?; lit. ?to do work?),
or has a meaning which is specific to that
combination (atentzioa eman: ?to catch
someone?s eye?; lit. ?to give attention?
(sth to sb)); legea urratu: ?to break the
law?; lit. ?to tear the law?).
? Compositional combinations with lexical
restriction, in which it is not possible to
substitute the verb with its synonyms, or
that present a clear statistical idiosyncrasy
in favor of a given synonym choice (elka-
rtasuna adierazi: ?to express solidarity?;
konpromisoa berretsi: ?to confirm a com-
mitment?).
Among the different techniques that have been
proposed to extract and characterize MWEs, the
cooccurrence of the components is the most used
heuristic of institutionalization, and the use of asso-
ciation measures (AM) goes back to early research
on this field (Church and Hanks, 1990; Smadja,
1993). In recent years, the comparative analysis of
AMs has aroused considerable interest, as well as
the possibility of obtaining better results by com-
bining them (Pearce, 2002; Pecina, 2005). Cooc-
currence techniques are usually used in combination
with linguistic techniques, which allow the use of
lemmatized and POS-tagged corpora, or even syn-
tactic dependencies (Seretan, 2008).
3 Special features of Basque NV
combinations
These are some characteristics of the NV combina-
tions in Basque to be considered in order to design
the extraction process efficiently:
? Basque being an agglutinative language, MWE
extraction must work on tagged texts, in order
to identify different surface forms with their
corresponding lemma. Thus, pure statistical
methods working with raw text are not ex-
pected to yield acceptable results.
? Some combinations with a noun as first lemma
do not correspond to NV combinations in the
sense that is usually understood in English. For
example, the expression kontuan hartu can be
translated as take into account, where kontu is
a noun in the inessive case. We are interested in
all types of combinations that a noun can form
with verbs.
? Representing NV combinations as lemma-
lemma pairs is by no means satisfactory; we
would not be able to differentiate the aforemen-
tioned kontuan hartu from kontu hartu (?to ask
for an explanation?). So it is necessary to deal
with the form or type of every noun.
? In order to propose canonical forms for NV
combinations, we need case and number an-
notations for nouns in bigram data. The next
examples are different forms of the canoni-
cal erabakia hartu (?to make a decision?): ez
zuen erabakirik hartu (?he did not make any
decision?), zenbait erabaki hartu behar ditugu
(?we have to make some decisions?). Canonical
forms can be formulated by bigram normaliza-
tion (see section 4.5 for details).
4 Experimental setup
4.1 Corpora resources
In our experiments, we use a journalistic cor-
pus from two sources: (1) Issues published be-
tween 2001-2002 by the newspaper Euskaldunon
Egunkaria (28 Mw); and (2) Issues published be-
tween 2006-2010 by the newspaper Berria (47 Mw).
So, the overall size of the corpus is 75 Mw.
4.2 Corpus-processing
For linguistic tagging, we use EUSTAGGER by the
IXA group of the University of the Basque Country
(Aduriz et al, 1996). After linguistic processing, we
obtain information about the lemma, part-of-speech,
subcategory, case, number and other morphosyntac-
tic features.
We used EUSTAGGER without the module to de-
tect and annotate MWEs in order to evaluate the au-
tomatic extraction, regardless of wheter the candi-
dates are in the lexical database.
4.3 Preparing tagged corpora for bigram
generation
For bigram generation, we use the Ngram Statistics
Package-NSP (Banerjee and Pedersen, 2010). In
3
order to retain in the text sent to NSP the linguis-
tic information needed according to section 3, we
add different types of linguistic information to the
tokens, depending on the POS of the components of
the combination we are dealing with. In the case of
NV combinations, the nouns are represented in the
following form:
token lemma POS subcategory case number
In the case of verbs, only lemma and POS are
used, as verb inflection has no influence on the
canonical form of the expression. In future work,
verb inflection will be one of the parameters to mea-
sure syntactical flexibility. All other types of tokens
are discarded and considered as ?non-token? for NSP
processing.
Before this step, some surface-grammar rules are
defined to detect and filter the participle forms that
are not part of a NV combination, but must be ana-
lyzed as adjectives or nouns (eg. herrialde aurrerat-
uak ?developed countries?, and gobernuaren aliat-
uak, ?government?s allies?).
4.4 Bigram generation
We generated bigram sets for two different window
spans: ?1 and ?5. In both sets, the frequency cri-
terion for a bigram to be generated is f > 30. Also,
the following punctuation marks are interpreted as
a boundary for bigram generation: period, colon,
semicolon, and question and exclamation marks.
Then, all counts of bigrams in NV and VN order are
combined using NSP, and reordered in NV order.
Additionally, a heuristic is used to filter some
combinations. The first member of many ?com-
pound verbs? like nahi izan (?to want?), is a noun,
and some of them combine usually with a verb, in
VN order: ikusi nahi (zuen) (?he wanted to see?). In
order to reduce this noise, the combinations occur-
ring mostly in VN order are removed. The combi-
nations generated from passive constructions (hartu-
tako erabakien ondorioak, ?the consequences of the
decisions made?) are not affected by this filtering.
4.5 Bigram normalization
In order to get more representative statistics, and
to get information that would enable us to propose
a canonical form for each MWE candidate, differ-
ent inflection forms of the same case in nouns are
normalized to the most frequent form, and bigram
counts are recalculated. I.e. [ erabakia / erabakiak
/ erabakiok / erabakirik / erabaki ] hartu are col-
lapsed to erabakia hartu (?to make a decision?), be-
cause all the mentioned forms of the lemma erabaki
appear in the absolutive case. In contrast, the com-
binations kontu hartu (?to ask for an explanation?)
and kontuan hartu (?take into account?) are not nor-
malized, as their noun forms correspond to differ-
ent cases, namely, absolutive (kontu) and inessive
(kontuan). A Perl script detects in the dataset the
bigrams to be normalized, using the combined key
noun lemma/noun case+verb lemma, creates a sin-
gle bigram with the most frequent form, and sums
the frequencies of bigrams and those of the noun un-
igrams.
As an example, this is normalization data for
kalean ibili (?to walk on the street?):
kalean kale IZE ARR INE NUMS<>ibili ADI<>223 3354 10880
kaleetan kale IZE ARR INE NUMP<>ibili ADI<>119 243 10880
?
kalean kale IZE ARR INE NUMS<>ibili ADI<>342 3597 10880
Besides, ergative-singular ? absolutive-plural
normalization is carried out when the ratio is greater
than 1:5. This heuristic is used in order to repair
some mistakes from the tagger. Finally, partitive
case (PAR) is assimilated to absolutive (ABS) for bi-
gram normalization; partitive is a case used in neg-
ative, interrogative and conditional sentences with
subjects of intransitive verbs and objects of transi-
tive verbs. I.e. ez zuen erabakirik hartu (?he did not
make any decision?).
Thus, this is the normalization of erabakia hartu:
erabakia erabaki IZE ARR ABS NUMS<>hartu ADI<>2658 6329 88447
erabakiak erabaki IZE ARR ABS NUMP<>hartu ADI<>1632 2397 88447
erabakiak erabaki IZE ARR ERG NUMP<>hartu ADI<>88 141 88447
erabakirik erabaki IZE ARR PAR MG<>hartu ADI<>211 211 88447
?
erabakia erabaki IZE ARR ABS NUMS<>hartu ADI<>4589 9361 88447
4.6 AM calculation
The statistical analysis of cooccurrence data is car-
ried out using Stefan Evert?s UCS toolkit (Evert,
2005). The most common association measures are
calculated for each bigram: f , t-score (also t-test),
log-likelihood ratio, MI, MI3, and chi-square (?2).
4.7 Evaluation
In order to evaluate the results of the bigram extrac-
tion process, we use as a reference a collection of
4
NV expressions published in five Basque resources:
a) The Unified Basque Dictionary, b) Euskal Hizte-
gia (Sarasola, 1996); c) Elhuyar Hiztegia; d) Intza
project; and e) EDBL (Aldezabal et al, 2001).
The total number for NV expressions is 3,742.
Despite the small size of the reference, we believe
that it may be valid for a comparison of the perfor-
mance of different AMs. Nevertheless, even a su-
perficial analysis reveals that the reference is mostly
made up of two kinds of combinations, idioms and
typical ?compound verbs?1.
Every evaluation against a dictionary depends
largely on its recall and quality, and we envisage,
as recommended by Krenn (1999), to build a hand-
made gold standard. To this end, we extract an eval-
uation sample merging the 2,000-best candidates of
each AM ranking from the w = ?1 extraction set.
There are 4,334 different bigrams in this set. This
manual evaluation is an ongoing work by a group of
three experts (one of them is an author of this paper).
Annotators were provided with an evaluation man-
ual, with explanatory information about the evalua-
tion task and the guidelines that must be followed to
differentiate MWEs from free combinations, based
on the criteria mentioned in section 2. Illustrative
examples are included.
At present, a random sample of 600 has been eval-
uated (13.8%), with a Fleiss kappa of 0.46. Even
though some authors have reported lower agree-
ments on this task (Street et al, 2010), this level of
agreement is comparatively low (Fazly and Steven-
son, 2007; Krenn et al, 2004), and by no means sat-
isfactory. It is necessary to make further efforts to
improve the discriminatory criteria, and achieve a
better ?tuning? between the annotators.
5 Results
Figure 1 shows the precision curves obtained for
each AM in the automatic evaluation. Frequency
yields the best precision, followed by t-score, log-
likelihood and MI3. MI and ?2 have a very low
performance, even below the baseline2. These re-
1Support verbs with syntactic idiosyncrasy (anomalous use
of the indefinite noun), as lan egin (?to work?) and min hartu
(?to get hurt?).
2Following Evert (2005), our baseline corresponds to the
precision yielded by a random ranking of the n candidates from
thedata set?; and our topline is ?the precision achieved by an
sults are consistent with those reported by Krenn and
Evert (2001) for support-verbs (FVG). Accordingly,
this is the type of combination which is very much
present in our dictionary reference.
Figure 1: Precision results for the extraction set with w =
?1 and f > 30.
Figure 2 offers an evaluation of the influence of
window span and bigram normalization. The best
results are obtained by the f ranking with a narrow
window and without bigram normalization. Regard-
ing bigram normalization, it could be concluded, at
first sight, that the canonical forms included in the
dictionary are not the most frequent forms of their
corresponding MWEs. Thus, the frequency criteria
used to normalize different forms of the same case
and assign canonical forms must be reviewed. As for
window span, the hypothesis that, since Basque is
largely a free-word-order language, a wider window
would yield more significant cooccurrence statistics,
is not confirmed at the moment. Further analysis is
needed to interpret these results from a deeper lin-
guistic point of view.
Even though the manually evaluated random sam-
ple is small (600 combinations), some provisional
conclusions can be drawn from the results. The
amount of candidates validated by at least two of the
three evaluators is 153, whereas only 29 of them are
included in the dictionary reference. Even though
MWE classification has not yet been undertaken by
the annotator?s team, a first analysis by the authors
shows that most of the manually validated combina-
?ideal? measure that ranks all TPs at the top of the list?.
5
Figure 2: Precision results of f and t-score for three dif-
ferent extraction sets (f > 30): a) w = ?1 with bigram
normalization; b) w = ?1 without bigram normalization;
and c) w = ?5 with bigram normalization.
tions not included in the dictionary (108 out of 124)
are restricted collocations (mainly support-verb con-
structions that are not ?compound verbs?) or statis-
tically idiosyncratic units. This is the first clue that
confirms our suspicions about the limited coverage
and representativeness of the reference. At the same
time, it could be one of the possible explanations for
the low inter-annotator agreement achieved, as far as
those types of MWEs are the most difficult to differ-
entiate from free combinations.
Figure 3 presents the precision curves for the
complete evaluation set estimated from the manu-
ally evaluated random sample using the technique
proposed by Evert and Krenn (2005). As expected,
precision results increase compared with the evalu-
ation against the dictionary. Frequency and t-score
outperform the other AMs, but frequency is not the
best measure in the whole range, as it is overtaken
by t-score in the first 1,200 candidates.
6 Conclusions and Future work
The first results for the extraction of NV expressions
in Basque are similar to the figures in Krenn and
Evert (2001). Frequency and t-score are good mea-
sures and it seems difficult to improve upon them.
Nevertheless, in light of the results, it is essential to
complete the manual evaluation and build a repre-
sentative gold standard in order to have a more pre-
cise idea of the coverage of the reference, and get
Figure 3: Precision results estimated from a 13.8% ran-
don sample manually evaluated (600 conbinations).
a more accurate view of the behaviour of AMs in
function of several factors such as the type of combi-
nation, corpus size, frequency range, window span,
etc. Bigram normalization is, in principle, a reason-
able procedure to formulate representative canoni-
cal forms, but requires a deeper analysis of the si-
lence that it seems to generate in the results. Finally,
the first evaluation using a small gold-standard is en-
couraging, because it suggests that using AMs it is
possible to find new expressions that are not pub-
lished in Basque dictionaries.
In the near future, we want to carry out a more
comprehensive evaluation of the AMs, and study
how to combine them in order to improve the re-
sults (Pecina and Schlesinger, 2006). In addition of
this, we want to detect lexical, syntactic and seman-
tic features of the expressions, and use this informa-
tion to characterize them (Fazly et al, 2009).
Acknowledgments
This research was supported in part by the Span-
ish Ministry of Education and Science (OpenMT-2,
TIN2009-14675-C03-01) and by the Basque Gov-
ernment (Berbatek: Tools and Technologies to pro-
mote Language Industry. Etortek - IE09-262). Our
colleagues Ainara Estarrona and Larraitz Uria are
kindly acknowledged for providing their expertise as
linguists in the manual evaluation process.
6
References
Aduriz, I., I. Aldezabal, I. Alegria, X. Artola,
N. Ezeiza, and R. Urizar (1996). EUSLEM:
A lemmatiser/tagger for Basque. Proc. of EU-
RALEX?96, 17?26.
Aldezabal, I., O. Ansa, B. Arrieta, X. Artola,
A. Ezeiza, G. Herna?ndez, and M. Lersundi
(2001). EDBL: A general lexical basis for the au-
tomatic processing of Basque. In IRCS Workshop
on linguistic databases, pp. 1?10.
Baldwin, T. and S. Kim (2010). Multiword expres-
sions. Handbook of Natural Language Process-
ing, second edition. Morgan and Claypool.
Banerjee, S. and T. Pedersen (2010). The design,
implementation, and use of the Ngram Statistics
Package. Computational Linguistics and Intelli-
gent Text Processing, 370?381.
Church, K. and P. Hanks (1990). Word associa-
tion norms, mutual information, and lexicogra-
phy. Computational linguistics 16(1), 22?29.
Cowie, A. (1998). Phraseology: Theory, analysis,
and applications. Oxford University Press, USA.
Evert, S. (2005). The statistics of word cooccur-
rences: Word pairs and collocations. Ph. D. the-
sis, University of Stuttgart.
Evert, S. and B. Krenn (2005). Using small ran-
dom samples for the manual evaluation of statis-
tical association measures. Computer Speech &
Language 19(4), 450?466.
Fazly, A., P. Cook, and S. Stevenson (2009). Un-
supervised type and token identification of id-
iomatic expressions. Computational Linguis-
tics 35(1), 61?103.
Fazly, A. and S. Stevenson (2007). Distinguish-
ing subtypes of multiword expressions using
linguistically-motivated statistical measures. In
Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pp. 9?16. As-
sociation for Computational Linguistics.
Granger, S. and M. Paquot (2008). Disentangling
the phraseological web. Phraseology. An inter-
disciplinary perspective, 27?50.
Krenn, B. (1999). The usual suspects: Data-
oriented models for identification and represen-
tation of lexical collocations. German Research
Center for Artificial Intelligence.
Krenn, B. and S. Evert (2001). Can we do better than
frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Work-
shop on Collocations, pp. 39?46.
Krenn, B., S. Evert, and H. Zinsmeister (2004). De-
termining intercoder agreement for a collocation
identification task. In Proceedings of KONVENS,
pp. 89?96.
Pearce, D. (2002). A comparative evaluation of col-
location extraction techniques. In Proc. of LREC
2002, pp. 1530?1536.
Pecina, P. (2005). An extensive empirical study of
collocation extraction methods. In Proceedings of
the ACL Student Research Workshop, pp. 13?18.
Association for Computational Linguistics.
Pecina, P. and P. Schlesinger (2006). Combining as-
sociation measures for collocation extraction. pp.
651?658.
Pe?rez Gaztelu, E., I. Zabala, and L. Gra?cia (2004).
Las fronteras de la composicio?n en lenguas
roma?nicas y en vasco. San Sebastia?n: Universi-
dad de Deusto.
Sarasola, I. (1996). Euskal Hiztegia. Kutxa Fun-
dazioa / Fundacio?n Kutxa.
Seretan, V. (2008). Collocation extraction based on
syntactic parsing. Ph. D. thesis, University of
Geneva.
Sinclair, J. (1996). The search for units of meaning.
Textus 9(1), 75?106.
Smadja, F. (1993). Retrieving collocations from
text: Xtract. Computational linguistics 19(1),
143?177.
Street, L., N. Michalov, R. Silverstein, M. Reynolds,
L. Ruela, F. Flowers, A. Talucci, P. Pereira,
G. Morgon, S. Siegel, M. Barousse, A. Anderson,
T. Carroll, and A. Feldman (2010). Like finding a
needle in a haystack: Annotating the american na-
tional corpus for idiomatic expressions. In Proc.
of LREC 2010, Valletta, Malta.
Wulff, S. (2010). Rethinking Idiomaticity. Corpus
and Discourse. New York: Continuum Interna-
tional Publishing Group Ltd.
7
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 116?125,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Combining Different Features of Idiomaticity for the Automatic
Classification of Noun+Verb Expressions in Basque
Antton Gurrutxaga
Elhuyar Foundation
Zelai Haudi 3, Osinalde industrialdea
Usurbil 20170. Basque Country
a.gurrutxaga@elhuyar.com
In?aki Alegria
IXA group, Univ. of the Basque Country
Manuel Lardizabal 1
Donostia 20018. Basque Country
i.alegria@ehu.es
Abstract
We present an experimental study of how dif-
ferent features help measuring the idiomatic-
ity of noun+verb (NV) expressions in Basque.
After testing several techniques for quantify-
ing the four basic properties of multiword ex-
pressions or MWEs (institutionalization, se-
mantic non-compositionality, morphosyntac-
tic fixedness and lexical fixedness), we test
different combinations of them for classifica-
tion into idioms and collocations, using Ma-
chine Learning (ML) and feature selection.
The results show the major role of distribu-
tional similarity, which measures composi-
tionality, in the extraction and classification
of MWEs, especially, as expected, in the case
of idioms. Even though cooccurrence and
some aspects of morphosyntactic flexibility
contribute to this task in a more limited mea-
sure, ML experiments make benefit of these
sources of knowledge, allowing to improve
the results obtained using exclusively distribu-
tional similarity features.
1 Introduction
Idiomaticity is considered the defining feature of the
concept of multiword expressions (MWE). It is de-
scribed as a non-discrete magnitude, whose ?value?
depends on a combination of features like in-
stitutionalization, non-compositionality and lexico-
syntactic fixedness (Granger and Paquot, 2008).
Idiomaticity appears as a continuum rather than as
a series of discrete values. Thus, the classification of
MWEs into discrete categories is a difficult task. A
very schematic classification that has achieved a fair
degree of general acceptance among experts distin-
guishes two main types of MWEs at phrase-level:
idioms and collocations.
This complexity of the concept of idiomaticity has
posed a challenge to the development of methods
addressing the measurement of the aforementioned
four properties. Recent research has resulted in
this issue nowadays being usually addressed through
measuring the following phenomena: (i) cooccur-
rence, for institutionalization; (ii) distributional sim-
ilarity, for non-compositionality; (iii) deviation from
the behavior of free combinations, for morphosyn-
tactic fixedness; and (iv) substitutability, for lexical
fixedness. This is the broad context of our experi-
mental work on the automatic classification of NV
expressions in Basque.
2 Related Work
2.1 Statistical Idiosyncrasy or
Institutionalization
Using the cooccurrence of the components of a com-
bination as a heuristic of its institutionalization goes
back to early research on this field (Church and
Hanks, 1990), and is computed using association
measures (AM), usually in combination with lin-
guistic techniques, which allows the use of lemma-
tized and POS-tagged corpora, or the use of syntac-
tic dependencies (Seretan, 2011). In recent years,
the comparative analysis of AMs (Evert, 2005) and
the combination of them (Lin et al, 2008; Pecina,
2010) have aroused considerable interest.
This approach has been recently explored in
Basque (Gurrutxaga and Alegria, 2011).
116
2.2 Compositionality
The central concept in characterizing compositional-
ity is the hypothesis of distributional similarity (DS)
As proposed by Baldwin and Kim (2010), ?the un-
derlying hypothesis is that semantically idiomatic
MWEs will occur in markedly different lexical con-
texts to their component words.?
Berry-Rogghe (1974) proposed R-value to mea-
sure the compositionality of verb-particle construc-
tions (VPCs), by dividing the overlap between the
sets of collocates associated with the particle by
the total number of collocates of the VPC. Wulff
(2010) proposes two extensions to the R-value
in her research on verb-preposition-noun construc-
tions, combining and weighting in different ways in-
dividual R-values of each component.
The Vector Space Model (VSM) is applied,
among others, by Fazly and Stevenson (2007), who
use the cosine as a similarity measure. The shared
task Distributional Semantics and Compositionality
(DiSCo) at ACL-HLT 2011 shows a variety of tech-
niques for this task, mainly association measures
and VSM (Biemann and Giesbrecht, 2011). LSA
(Latent Semantic Analysis) is used in several stud-
ies (Baldwin et al, 2003; Katz and Giesbrecht, 2006;
Schone and Jurafsky, 2001).
Those approaches have been applied recently to
Basque (Gurrutxaga and Alegria, 2012)
2.3 Morphosyntactic Flexibility (MSFlex)
Morphosyntactic fixedness is usually computed in
terms of relative flexibility, as the statistical dis-
tance between the behavior of the combination and
(i) the average behavior of the combinations with
equal POS composition (Fazly and Stevenson, 2007;
Wulff, 2010), or (ii) the average behavior of the
combinations containing each one of the compo-
nents of the combination (Bannard, 2007).
Fazly and Stevenson (2007) use Kullback-
Leibler divergence (KL-div) to compute this dis-
tance. They analyze a set of patterns: determination
(a/the), demonstratives, possessives, singular/plural
and passive. They compute two additional measure-
ments (dominant pattern and presence of absence of
adjectival modifiers preceding the noun).
Wulff (2010) considers (i) tree-syntactic, (ii)
lexico-syntactic and (iii) morphological flexibilities,
and implements two metrics for these features: (i) an
extension of Barkema proposal (NSSD, normalized
sum of squared deviations), (ii) a special conception
of ?relative entropy? (Hrel).
Bannard (2007), using CPMI (conditional point-
wise mutual information), analyses these variants:
(i) variation, addition or dropping of a determiner;
(ii) internal modification of the noun phrase; and (iii)
verb passivation.
2.4 Lexical Flexibility (LFlex)
The usual procedure for measuring lexical flexibility
is to compute the substitutability of each component
of the combination using as substitutes its synony-
mous, quasi-synonyms, related words, etc.
The pioneering work in this field is Lin (1999),
who uses a thesaurus automatically built from text.
This resource is used in recent research (Fazly and
Stevenson, 2007). They assume that the target pair
is lexically fixed to the extent that its PMI deviates
from the average PMI of its variants generated by
lexical substitution. They compute flexibility using
the z-score.
In Van de Cruys and Moiro?n (2007), a technique
based on KL-div is used for Duch. They define Rnv
as the ratio of noun preference for a particular verb
(its KL-div), compared to the other nouns that are
present in the cluster of substitutes. Similarly for
Rvn. The substitute candidates are obtained from
the corpus using standard distributional similarity
techniques.
2.5 Other Methods
Fazly and Stevenson (2007) consider two other fea-
tures: (i) the verb itself; and (ii) the semantic cate-
gory of the noun according to WordNet.
2.6 Combined Systems
In order to combine several sources of knowledge,
several studies have experimented with using Ma-
chine Learning methods (ML).
For Czech, Pecina (2010) combines only AMs us-
ing neural networks, logistic regression and SVM
(Support Vector Machine). Lin et al (2008) employ
logistic linear regression model (LLRM) to combine
scores of AMs.
Venkatapathy and Joshi (2005) propose a mini-
mally supervised classification scheme that incorpo-
117
rates a variety of features to group verb-noun combi-
nations. Their features drawn from AM and DS, but
some of each type are tested and combined. They
compute ranking correlation using SVM, achieving
results of about 0.45.
Fazly and Stevenson (2007) use all the types of
knowledge, and decision trees (C5.0) as a learning
method, and achieve average results (F-score) near
to 0.60 for 4 classes (literal, abstract, light verbs and
idioms). The authors claim that the syntactic and
combined fixedness measures substantially outper-
form measures of collocation extraction.
3 Experimental Setup
3.1 Corpus and Preprocessing
We use a journalistic corpus of 75 million words
(MW) from two sources: (1) Issues published
in 2001-2002 by the newspaper Euskaldunon
Egunkaria (28 MW); and (2) Issues published in
2006-2010 by the newspaper Berria (47 MW).
The corpus is annotated with lemma, POS, fine
grained POS (subPOS), case and number informa-
tion using Eustagger developed by the IXA group of
the University of the Basque Country. A precision of
95.42% is reported for POS + subPOS + case analy-
sis (Oronoz et al, 2010).
3.2 Extraction of Bigram Candidates
The key data for defining a Basque NV bigram are
lemma and case for the noun, and lemma for the
verb. Case data is needed to differentiate, for exam-
ple, kontu hartu (?to ask for an explanation?) from
kontuan hartu (?to take into account?), where kontu
is a noun lemma in the inessive case.
In order to propose canonical forms, we need, for
nouns, token, case and number annotations in bi-
gram data. Those canonical forms can be formulated
using number normalization, as described in Gur-
rutxaga and Alegria (2011). Bigrams belonging to
the same key noun lemma/noun case+verb lemma
are normalized; a single bigram with the most fre-
quent form is created, and the frequencies of bi-
grams and those of the noun unigrams summed.
We use the Ngram Statistics Package-
NSP (Banerjee and Pedersen, 2010) to generate NV
bigrams from a corpus generated from the output of
Eustagger. Taking into account our previous results
(Gurrutxaga and Alegria, 2011), we use a window
span of ?1 and a frequency threshold of f > 30.
Before generation, some surface-grammar rules are
applied to correct annotations that produce noise.
For example, in most Basque AdjN combinations,
the adjetive is a verb in a participe form (eg. indar
armatuak, ?armed forces?). Similarly, those kind
of participles can function as nouns (gobernuaren
aliatuak, ?the allies of the government?). Not
tagging those participles properly would introduce
noise in the extraction of NV combinations.
3.3 Experiments Using Single Knowledge
Sources
3.3.1 Cooccurrence
The cooccurrence data provided by NSP in the bi-
gram extraction step is processed to calculate AMs.
To accomplish this, we use Stefan Evert?s UCS
toolkit (Evert, 2005). The most common AMs are
calculated: f , t-score, log-likelihood ratio, MI, MI3,
and chi-square (?2).
3.3.2 Distributional Similarity
The idea is to compare the contexts of each NV
bigram with the contexts of its corresponding com-
ponents, by means of different techniques. The
more similar the contexts, the more compositional
the combination.
Context Generation We extract the context words
of each bigram from the sentences with contiguous
cooccurrences of the components. The noun has to
occur in the grammatical case in which it has been
defined after bigram normalization.
The contexts of the corresponding noun and verb
are extracted separately from sentences where they
did not occur together. Only content-bearing lem-
mas are included in the contexts (nouns, verbs and
adjectives).
Context Comparison We process the contexts in
two different ways:
First, we construct a VSM model, representing
the contexts as vectors. As similarity measures, we
use Berry-Roghe?s R-value (RBR) and the two ex-
tensions to it proposed by Wulff (RW1 and RW2),
Jaccard index and cosine. For the cosine, different
AMs have been tested for vector weights (f , t-score,
118
LLR and PMI). We experiment with different per-
centages of the vector and different numbers of col-
locates, using the aforementioned measures to rank
the collocates. The 100 most frequent words in the
corpus are stopped.
Second, we represent the same contexts as doc-
uments, and compare them by means of differ-
ent indexes using the Lemur Toolkit (Allan et al,
2003). The contexts of the bigrams are used as
queries against a document collection containing the
context-documents of all the members of the bi-
grams. This can be implemented in different ways;
the best results were obtained using the following:
? Lemur 1 (L1): As with vectors, the contexts of
a bigram are included in a single query docu-
ment, and the same is done for the contexts of
its members
? Lemur 2 (L2): The context sentences of bi-
grams are treated as individual documents, but
the contexts of each one of its members are rep-
resented in two separate documents
Due to processing reasons, the number of context
sentences used in Lemur to generate documents is
limited to 2,000 (randomly selected from the whole
set of contexts).
We further tested LSA (using Infomap1), but the
above methods yielded better results.
3.3.3 Morphosyntactic Flexibility
We focus on the variation of the N slot, dis-
tinguishing the main type of extensions and num-
ber inflections. Among left-extensions, we take
into account relative clauses. In addition, we con-
sider the order of components as a parameter. We
present some examples of the free combination libu-
rua irakurri (?to read a book?)
? Determiner: liburu bat irakurri dut (?I have
read one book?), zenbat liburu irakurri dituzu?
(?how many books have you read??)
? Postnominal adjective: liburu interesgarria
irakurri nuen (?I read an interesting book?)
? Prenominal adjective: italierazko liburua
irakurri (?to read a book in Italian?)
1http://infomap-nlp.sourceforge.net/
? Relative clause: irakurri dudan liburua (?the
book I have read?), anaiak irakurritako liburu
batzuk (?some books read by my brother?)
? Number inflection: liburua/liburuak/
liburu/liburuok irakurri (?to read
a/some/?/these book(s)?)
? Order of components (NV / VN): liburua
irakurri dut / irakurri dut liburua (?I have read
a book?)
We count the number of variations for each bi-
gram, for all NV bigrams, and for each combination
of the type bigram component+POS of the other
component (e.g, for liburua irakurri, the variations
of all the combinations liburua+V and N+irakurri).
To calculate flexibility, we experiment with all the
measures described in section 2.3: Fazly?s KL-div,
Wulff?s NSSD and Hrel (relative entropy), and Ban-
nard?s CPMI.
3.3.4 Lexical Flexibility
In order to test the substitutability of the compo-
nents of bigrams, we use two resources: (i) ELH:
Sinonimoen Kutxa, a Basque dictionary of syn-
onyms, published by the Elhuyar Foundation (for
nouns and verbs, 40,146 word-synomyn pairs); (ii)
WN: the Basque version of WordNet2(68,217 word-
synomyn pairs). First, we experimented with both
resources on their own, but the results show that
in many cases there either was no substitute candi-
date, or the corpus lacked combinations containing
a substitute. In order to ensure a broader coverage,
we combined both resources (ELHWN), and we ex-
panded the set of substitutes including the siblings
retrieved from Basque WordNet (ELHWNexpand).
To calculate flexibility, we experiment with the
two measures described in section 2.4: z-score and
KL-div based R.
3.4 Combining Knowledge Sources Using
Machine Learning
We use some ML methods included in the Weka
toolkit (Hall et al, 2009) in order to combine re-
sults obtained in experiments using single knowl-
edge sources (described in section 3.3). The values
2http://ixa2.si.ehu.es/cgi-bin/mcr/public/wei.consult.perl
119
of the different measures obtained in those experi-
ments were set as features.
We have selected five methods corresponding to
different kind of techniques which have been used
successfully in this field: Naive Bayes, C4.5 deci-
sion tree (j48), Random Forest, SVM (SMO algo-
rithm) and Logistic Regression. Test were carried
out using either all features, the features from each
type of knowledge, and some subsets, obtained af-
ter manual and automatic selection. Following Fa-
zly and Stevenson (2007), verbs are also included as
features.
Since, as we will see in section 3.5, the amount of
instances in the evaluation dataset is not very high
(1,145), cross-validation is used in the experiments
for model validation (5 folds). In the case of auto-
matic attribute selection, we use AttributeSelected-
Classifier, which encapsulates the attribute selection
process with the classifier itself, so the attribute se-
lection method and the classifier only see the data in
the training set of each fold.
3.5 Evaluation
3.5.1 Reference Dataset and Human
Judgments
As an evaluation reference, we use a subset of
1,200 combinations selected randomly from a ex-
tracted set of 4,334 bigrams, that is the result of
merging the 2,000-best candidates of each AM rank-
ing from the w = ?1 and f > 30 extraction set.
The subset has been manually classified by three
lexicographers into idioms, collocations and free
combinations. Annotators were provided with an
evaluation manual, containing the guidelines for
classification and illustrative examples.
The agreement among evaluators was calculated
using Fleiss? ?. We obtained a value of 0.58, which
can be considered moderate, close to fair, agree-
ment. Although this level of agreement is relatively
low when compared to Krenn et al (2004), it is
comparable to the one reported by Pecina (2010),
who attributed his ?relatively low? value to the fact
that ?the notion of collocation is very subjective,
domain-specific, and also somewhat vague.? Street
et al (2010) obtain quite low inter-annotator agree-
ment for annotation of idioms in the ANC (Ameri-
can National Corpus). Hence, we consider that the
level of agreement we have achieved is acceptable.
For the final classification of the evaluation set,
cases where agreement was two or higher were au-
tomatically adopted, and the remaining cases were
classified after discussion. We removed 55 combina-
tions that did not belong to the NV category, or that
were part of larger MWEs. The final set included
1,145 items, out of which 80 were idioms 268 collo-
cations, and 797 free combinations.
3.5.2 Procedure
In order to compare the results of the individual
techniques, we based our evaluation on the rank-
ings provided by each measure. If we were to have
an ideal measure, the set of bigram categories (?id?,
?col? and ?free?) would be an ordered set, with ?id?
values on top of the ranking, ?col? in the middle, and
?free? at the bottom. Thus, the idea is to compute
the distance between a rank derived from the ideally
ordered set, which contains a high number of ties,
and the rank yielded by each measure. To this end,
we use Kendall?s ?B as a rank-correlation measure.
Statistical significance of the Kendall?s ?B correla-
tion coefficient is tested with the Z-test. The realistic
topline, yielded by a measure that ranks candidates
ideally, but without ties, would be 0.68.
In addition, average precision values (AP) were
calculated for each ranking.
In the case of association measures, similarity
measures applied to VSM, and measures of flexibil-
ity, the bigrams were ranked by means of the val-
ues of the corresponding measure. In the case of ex-
periments with Lemur, the information used to rank
the bigrams consisted of the positions of the docu-
ments corresponding to each member of the bigram
in the document list retrieved (?rank? in Table 1). For
the experiments in which the context sentences have
been distributed in different documents, average po-
sitions were calculated and weighted, in relation to
the amount of documents for each bigram analysis
(?rank weight?). The total number of documents in
the list (or ?hits?) is weighted in the same manner
(?hit rel?).
When using ML techniques, several measures
provided by Weka were analyzed: percentage of
Correctly Classified Instances (CCI), F-measures
for each class (id, col, free), Weighted Average F-
measure and Average F-measure.
120
measure ?B AP MWE AP id AP col
random rank (-0.02542) 0.30879 0.0787 0.23358
AM
f 0.18853 0.43573 0.07391 0.37851
t-score 0.19673 0.45461 0.08442 0.38312
log-likelihood 0.15604 0.42666 0.10019 0.33480
PMI (-0.12090) 0.25732 0.08648 0.18234
chi-squared (-0.03699) 0.30227 0.11853 0.20645
DS
RBR NV (MI -50%) 0.27034 0.47343 0.21738 0.30519
RW1(2000 MI f3 50%) 0.26206 0.47152 0.19664 0.30967
L1 Indri rankNV 0.31438 0.53536 0.22785 0.35299
L1 KL rankNV 0.29559 0.51694 0.23558 0.33607
L2 Indri hit rel NV 0.32156 0.56612 0.29416 0.35389
L2 KL hit rel NV 0.30848 0.55146 0.31977 0.33241
L2 Indri rankN weight 0.21387 0.45567 0.26148 0.28025
L2 Indri rankV weight 0.31398 0.55208 0.12837 0.43143
MSFlex
Hrel Det 0.07295 0.38995 0.12749 0.27704
Hrel PostAdj (-0.05617) 0.31673 0.04401 0.29597
Hrel PreAdj 0.11459 0.38561 0.09897 0.29223
Hrel Rel 0.09115 0.40502 0.12913 0.29012
Hrel Num 0.11861 0.43381 0.13387 0.31318
Hrel ord (0.02319) 0.31661 0.08124 0.24052
CPMI (components) 0.05785 0.41917 0.12630 0.30831
LFlex
Rnv ELHWN (0.08998) 0.36717 0.07521 0.29896
Rvn ELHWN (0.03306) 0.31752 0.08689 0.24369
z-score V ELHWNexpand 0.10079 0.35687 0.12232 0.25019
z-score N ELHWNexpand 0.08412 0.35534 0.07245 0.29005
Table 1: Kendall?s ?B rank-correlations relative to an ideal idiomaticity ranking, obtained by different idiomaticity
measures. Non-significant values of ?B in parentheses (p > 0.05). Average precisions for MWEs in general, and
specific values for idioms and collocations.
4 Experimental Results
4.1 Single Knowledge Experiments
The results for Kendall?s ?B and AP for MWEs and
separate AP values for idioms and collocations are
summarized in Table 1 (only the experiments with
the most noteworthy results are included).
The best results are obtained in the Lemur exper-
iments, most notably in the Lemur 2 type, using ei-
ther Indri or KL-div indexes. In the MWE rankings,
measures of the R-value type only slightly outper-
form AMs.
In the case of idioms, DS measures obtain signif-
icantly better ranks than the other measures. Idioms
being the least compositional expressions, his result
is expected, and supports the hypothesis that seman-
tic compositionality can better be characterized us-
ing measures of DS than using AMs.
Regarding collocations, no such claim can be
made, as the AP values for t-score and f outper-
form DS values, with a remarkable exception: the
best AP is obtained by an Indri index that com-
pares the semantic similarity between the verb in
combination with the noun and the verb in contexts
without the noun (L2 Indri rankV weight), accord-
ingly with the claim that the semantics of the verb
contribute to the semicompositionality of colloca-
tions. By contrast, the corresponding measure for
the noun (L2 Indri rankN weight) works quite a bit
better with idioms than the previous verb measure.
Figure 1 shows the precision curves for the extrac-
tion of MWEs by the best measure of each compo-
nent of idiomaticity.
In Figure 2 and 3, we present separately the preci-
121
Figure 1: Precision results for the compositionality rank-
ings of MWEs.
sion curves for idioms and collocations. We plot the
measures with the best precision values.
Figure 2: Precision results for the compositionality rank-
ings of idioms.
Regarding the precision for collocations in Fig-
ure 3, the differences are not obviously significant.
Even though the DS measure has the better perfor-
mance, precision values for the t-score are not too
much lower, and the t-score has a similar perfor-
mance at the beginning of the ranking (n < 150).
4.2 Machine Learning Experiments
We report only the results of the three methods with
the best overall performance: Logistic Regression
(LR), SMO and RandomForest (RF).
In Table 2, we present the results obtained with
datasets containing only DS attributes (the source
of knowledge with the best results in single ex-
Figure 3: Precision results for the compositionality rank-
ings of collocations.
periments); datasets containing all features corre-
sponding to the four properties of idiomaticity; and
datasets obtained adding the verb of the bigram as a
string-type attribute.
As the figures show, it is difficult to improve the
results obtained using only DS. The results of SMO
are better when the features of the four components
of idiomaticity are used, and even better when the
verb is added, especially for idioms. The verb causes
the performance of RF be slightly worse; in the case
of LR, it generates considerable noise.
It can be observed that the figures for LR are
more unstable. Using SMO and RF, convergence
does not depend on how many noisy variables are
present (Biau, 2012). Thus, feature selection could
improve the results when LR is used.
In a complementary experiment, we observed the
impact of removing the attributes of each source of
knowledge (without including verbs). The most ev-
ident result was that the exclusion of LFlex features
contributes the most to improving F. This was an ex-
pected effect, considering the poor results for LFlex
measures described in section 4.1. More interest-
ing is the fact that removing MSFlex features had a
higher negative impact on F than not taking AMs as
features.
Table 3 shows the results for two datasets gener-
ated through two manual selection of attributes: (1)
manual 1: the 20 attributes with best AP average re-
sults; and (2) manual 2: a manual selection of the
attributes from each knowledge source with the best
AP MWE, best AP id and best AP col. The third
122
Features Method CCI F id F col F free F W.Av. F Av.
DS
LR 72.489 0.261 0.453 0.838 0.707 0.517
SMO 74.061 0.130 0.387 0.824 0.575 0.447
RF 71.441 0.295 0.440 0.821 0.695 0.519
all idiom. properties
LR 71.703 0.339 0.514 0.821 0.716 0.558
SMO 76.507 0.367 0.505 0.857 0.740 0.576
RF 74.498 0.323 0.486 0.844 0.724 0.551
all + verb
LR 60.000 0.240 0.449 0.726 0.627 0.472
SMO 75.808 0.400 0.540 0.848 0.744 0.596
RF 74.061 0.243 0.459 0.846 0.713 0.516
Table 2: Results of Machine Learning experiments combining knowledge sources in three ways: (i) DS: distributional
similarity features; (ii) knowledge related to the four components of idiomaticity (AM+DS+MSFlex+LFlex); (iii)
previous features+verb components of bigrams.
section presents the results obtained with AttributeS-
electedClassifier using CfsSubsetEval (CS) as evalu-
ator3 and BestFirst (BS) as search method. Looking
at the results of the selection process in each fold, we
saw that the attributes selected in more than 2 folds
are 36: 1 AM, 20 from DS, 7 from MSFlex, 1 from
LFlex and 7 verbs.
Features Method F W.Av. F Av.
manual 1
LR 0.709 0.525
SMO 0.585 0.304
RF 0.680 0.485
manual 2
LR 0.696 0.518
SMO 0.581 0.286
RF 0.688 0.519
CS-BF
LR 0.727 0.559
SMO 0.693 0.485
RF 0.704 0.531
Table 3: F Weighted average and F average results for ex-
periments using: (1) the 20 attributes with best AP aver-
age results; (2) a manual selection of the 3 best attributes
from each knowledge source; and (3) AttributeSelected-
Classifier with automatic attribute selection using Cfs-
SubsetEval as evaluator and BestFirst as search method
The results show that, for each method, auto-
matic selection outperforms the two manual selec-
tions. Most of the attributes automatically selected
are DS measures, but it is interesting to observe that
MSFlex and the verb slot contribute to improving
the results. Using automatic attribute selection and
3http://wiki.pentaho.com/display/
DATAMINING/CfsSubsetEval
LR, the results are close to the best figure of F W.Av.
using SMO and all the features (0.727 vs 0.744).
5 Discussion
The most important conclusions from our experi-
ments are the following:
? In the task of ranking the candidates, the best
results are obtained using DS measures, and,
in particular, Indri and KL-div in L2 experi-
ments. This is true for both type of MWEs, and
is ratified in ML experiments when automatic
attribute filtering is carried out. It is, however,
particularly notable with regard to idioms; in
the case of collocations, the difference between
the performance of DS and that of and MS and
AM were not that significant.
? MSFlex contributes to the classification task
when used in combination with DS, but get
poor results by themselves. The most relevant
parameter MSFlex is number inflection.
? SMO is the most precise method when a high
amount of features is used. It gets the best over-
all F-score. The other methods need feature se-
lection to obtain similar results.
? Automatic attribute selection using CS-BF fil-
ter yields better results than manual selections.
The method that takes the most advantage is
LR, whose scores are little bit worse than those
of SMO using the whole set of attributes.
123
Some of these conclusions differ from those reached
by earlier works. In particular, the claims in Fazly
and Stevenson (2007) and Van de Cruys and Moiro?n
(2007) that syntactic as well as lexical flexibility out-
perform other techniques of MWE characterization
are not confirmed in this work for Basque. Some
hypothesis could be formulated to explain those
differences: (1) Basque idioms could be syntacti-
cally more flexible, whereas some free combinations
could present a non-negligible level of fixedness; (2)
Basque, especially in the journalistic register, could
be sociolinguistically less fixed than, say, English
or Spanish; thus, the lexical choice of the collocate
could be not so clearly established; (3) the Basque
lexical resources to test substitutability could have
insufficient coverage; and (4) Fazly and Stevenson
(2007) use the cosine for DS, a measure which in our
experiments is clearly below other measures. Those
hypotheses require experimental testing and deeper
linguistic analysis.
6 Conclusions and Future Work
We have presented an in-depth analysis of the per-
formance of different features of idiomaticity in the
characterization of NV expressions, and the results
obtained combining them using ML methods. The
results confirm the major role of DS, especially, as
expected, in the case of idioms. It is remarkable that
the best results have been obtained using Lemur, an
IR tool. ML experiments show that other features
contribute to improve the results, especially some
aspects of MSFlex, the verb of the bigram and, to
a more limited extent, AMs. The performance of
DS being the best one for idioms confirm previous
research on other languages, but MSFlex and LFlex
behave below the expected. The explanations pro-
posed for this issue require further verification.
We are planning experiments using these tech-
niques for discriminating between literal and id-
iomatic occurrences of MWEs in context. Work on
parallel corpora is planned for the future.
Acknowledgments
This research was supported in part by the Span-
ish Ministry of Education and Science (TACARDI-
TIN2012-38523-C02-011) and by the Basque
Government (Berbatek project, Etortek-IE09-262;
KONBITZ project, Saiotek 2012). Ainara Estar-
rona and Larraitz Uria (IXA group) and Ainara On-
darra and Nerea Areta (Elhuyar) are acknowledged
for their work as linguists in the manual evaluation.
Maddalen Lopez de la Calle and In?aki San Vicente
(Elhuyar) and Oier Lopez de la Calle (IXA group)
have contributed with their expertise to the design of
the experiments with Lemur and Infomap. Finally,
special thanks goes to Olatz Arregi (IXA group)
for having guided us in the experiments with Weka,
and to Yosu Yurramendi from the University of the
Basque Country, for his advice on the statistics in
the evaluation step.
References
Allan, J., J. Callan, K. Collins-Thompson, B. Croft,
F. Feng, D. Fisher, J. Lafferty, L. Larkey,
T. Truong, P. Ogilvie, et al (2003). The Lemur
Toolkit for language modeling and information
retrieval.
Baldwin, T., C. Bannard, T. Tanaka, and D. Wid-
dows (2003). An empirical model of multi-
word expression decomposability. In Proceed-
ings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pp. 96.
Baldwin, T. and S. Kim (2010). Multiword expres-
sions. Handbook of Natural Language Process-
ing, second edition. Morgan and Claypool.
Banerjee, S. and T. Pedersen (2010). The design,
implementation, and use of the Ngram Statistics
Package. Computational Linguistics and Intelli-
gent Text Processing, 370?381.
Bannard, C. (2007). A measure of syntactic flexi-
bility for automatically identifying multiword ex-
pressions in corpora. In Proceedings of the Work-
shop on a Broader Perspective on Multiword Ex-
pressions, pp. 1?8.
Berry-Rogghe, G. (1974). Automatic identification
of phrasal verbs. Computers in the Humanities,
16?26.
Biau, G. (2012). Analysis of a random forests
model. The Journal of Machine Learning Re-
search 98888, 1063?1095.
Biemann, C. and E. Giesbrecht (2011). Distri-
butional semantics and compositionality 2011:
124
Shared task description and results. Workshop
on Distributional semantics and compositionality
2011. ACL HLT 2011, 21.
Church, K. and P. Hanks (1990). Word associa-
tion norms, mutual information, and lexicogra-
phy. Computational linguistics 16(1), 22?29.
Evert, S. (2005). The statistics of word cooccur-
rences: Word pairs and collocations. Ph. D. the-
sis, University of Stuttgart.
Fazly, A. and S. Stevenson (2007). Distinguish-
ing subtypes of multiword expressions using
linguistically-motivated statistical measures. In
Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pp. 9?16. As-
sociation for Computational Linguistics.
Granger, S. and M. Paquot (2008). Disentangling
the phraseological web. Phraseology. An inter-
disciplinary perspective, 27?50.
Gurrutxaga, A. and I. Alegria (2011). Automatic
extraction of NV expressions in Basque: basic
issues on cooccurrence techniques. Proc. of the
Workshop on Multiword Expressions. ACL HLT
2011, 2?7.
Gurrutxaga, A. and I. Alegria (2012). Measuring
the compositionality of nv expressions in basque
by means of distributional similarity techniques.
LREC2012.
Hall, M., E. Frank, G. Holmes, B. Pfahringer,
P. Reutemann, and I. H. Witten (2009). The weka
data mining software: an update. Volume 11, pp.
10?18. ACM.
Katz, G. and E. Giesbrecht (2006). Automatic
identification of non-compositional multi-word
expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying
Properties, pp. 12?19. Association for Computa-
tional Linguistics.
Krenn, B., S. Evert, and H. Zinsmeister (2004). De-
termining intercoder agreement for a collocation
identification task. In Proceedings of KONVENS,
pp. 89?96.
Lin, D. (1999). Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the ACL, pp. 317?324. Associ-
ation for Computational Linguistics.
Lin, J., S. Li, and Y. Cai (2008). A new colloca-
tion extraction method combining multiple asso-
ciation measures. In Machine Learning and Cy-
bernetics, 2008 International Conference on, Vol-
ume 1, pp. 12?17. IEEE.
Oronoz, M., A. D. de Ilarraza, and K. Gojenola
(2010). Design and evaluation of an agreement er-
ror detection system: testing the effect of ambigu-
ity, parser and corpus type. In Advances in Natu-
ral Language Processing, pp. 281?292. Springer.
Pecina, P. (2010). Lexical association measures and
collocation extraction. Language resources and
evaluation 44(1), 137?158.
Schone, P. and D. Jurafsky (2001). Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem. In Proc. of the 6th
EMNLP, pp. 100?108. Citeseer.
Seretan, V. (2011). Syntax-Based Collocation Ex-
traction. Text, Speech and Language Technology.
Dordrecht: Springer.
Street, L., N. Michalov, R. Silverstein, M. Reynolds,
L. Ruela, F. Flowers, A. Talucci, P. Pereira,
G. Morgon, S. Siegel, et al (2010). Like finding a
needle in a haystack: Annotating the american na-
tional corpus for idiomatic expressions. In Proc.
of LREC?2010.
Van de Cruys, T. and B. Moiro?n (2007). Semantics-
based multiword expression extraction. In Pro-
ceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pp. 25?32. Asso-
ciation for Computational Linguistics.
Venkatapathy, S. and A. Joshi (2005). Measuring the
relative compositionality of verb-noun (vn) collo-
cations by integrating features. In Proceedings of
HLT/EMNLP, pp. 899?906. Association for Com-
putational Linguistics.
Wulff, S. (2010). Rethinking Idiomaticity. Corpus
and Discourse. New York: Continuum Interna-
tional Publishing Group Ltd.
125
