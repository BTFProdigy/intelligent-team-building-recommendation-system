Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 721?731, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generalizing Sub-sentential Paraphrase Acquisition
across Original Signal Type of Text Pairs
Aure?lien Max Houda Bouamor
LIMSI-CNRS & Univ. Paris Sud
Orsay, France
firstname.lastname@limsi.fr
Anne Vilnat
Abstract
This paper describes a study on the impact of
the original signal (text, speech, visual scene,
event) of a text pair on the task of both man-
ual and automatic sub-sentential paraphrase
acquisition. A corpus of 2,500 annotated sen-
tences in English and French is described, and
performance on this corpus is reported for
an efficient system combination exploiting a
large set of features for paraphrase recogni-
tion. A detailed quantified typology of sub-
sentential paraphrases found in our corpus
types is given.
1 Introduction
Sub-sentential paraphrases can be acquired from text
pairs expressing the same meaning (Madnani and
Dorr, 2010). If the semantic similarity of a text
pair has a direct impact on the quality of the ac-
quired paraphrases, it has, to our knowledge, never
been shown what impact the type of original sig-
nal has on paraphrase acquisition. In this work,
we consider four types of corpora, which we think
are representative of the main types of original
semantic signals: text pairs (roughly, sentences)
originating a) from independent translations of a
text (TEXT), b) from independent translations of a
speech (SPEECH), c) from independent descriptions
of a visual scene (SCENE), and d) from independent
descriptions of some event (EVENT). We will report
the results of experiments on sub-sentential para-
phrase acquisition on all these corpus types in two
languages, English and French, and provide some
answers to the following questions: What types of
paraphrases can be found by human annotators, with
what confidence and in which quantities? How well
can representative paraphrase acquisition systems
perform on each corpus type, and how performance
can be improved through combination? On what
corpus types can performance be improved by using
training material from other corpus types? Our ex-
perimental results will provide several indications of
the differences and complementarities of the corpus
types under study, and will notably show that perfor-
mance on the most readily available corpus type can
be improved by using training data from the set of
all other corpus types.
We will first describe the building procedures
and characteristics of our corpora (section 2), and
then describe our experimental settings for evalu-
ating paraphrase acquisition (section 3.1). Our ex-
periments will first consist of the description (sec-
tion 3.2) and evaluation (section 3.3) of a system
combination on each corpus type and then of our
system provided with additional training data from
the other corpus types (section 3.4). We will finally
briefly review related work (section 4) and discuss
our main findings and future work (section 5).
2 Collection of sentence pair corpora
In this study, we will focus on paraphrase acquisition
from related sentence pairs characteristic of 4 corpus
types, which correspond to different original signal
types of text pairs illustrated by the word alignment
matrices on Figure 1. A corpus for each type has
been collected for 2 languages, English and French,
and comprises 625 sentence pairs per language. We
now briefly describe how each corpus was built.
721
It
is
estimated
that
the
total
annual
volume
of
import
and
export
will
exceed
9
billion
US
dollars.
It is an
tic
ipa
ted
tha
t
the an
nu
al
tot
al
fo
rei
gn
tra
de
vo
lum
e
wi
ll
ex
ce
ed
US
$9
bil
lio
n
.
So
he
uses
the
photo
booths
to
remind
people
what
he
looks
like
.
He us
es
th
os
e
m
ac
hi
ne
s
to re
m
in
d
th
e
liv
in
g
of hi
s
fa
ce
.
a
boy
is
riding
on
a
bicycle
fast
.
a bo
y
rid
es
a bi
ke
on a di
rt
ro
ad
.
Pigeons
have
an
understanding
of
numbers
on
par
with
primates
Pi
ge
on
s
ha
ve
nu
m
er
ica
l
ab
ili
tie
s
ju
st
lik
e
pr
im
ate
s
Figure 1: Example reference alignment matrices for
(from top to bottom) TEXT, SPEECH, SCENE and
EVENT. Sure alignments appear in green or gray (identi-
ties) and possible alignments in yellow.
TEXT For English, we used the MTC corpus1 (de-
scribed in (Cohn et al2008)) consisting of sets
of news article translations from Chinese, and for
French the CESTA corpus2 consisting of sets of
news article translations from English. For each
sentence cluster, we selected sentence pairs with
minimal edit distance above an empirically-selected
threshold, covering all clusters first and then select-
ing from already used clusters to reach the target
number of sentence pairs.
e.g. It is estimated that the total annual volume of import
and export will exceed 9 billion US dollars. ? It is an-
ticipated that the annual total foreign trade volume will
exceed US$9 billion.
SPEECH For English, we used two freely avail-
able subtitle files3 of the French movies Le Fabuleux
Destin d?Ame?lie Poulain and Les Choristes, and for
French we used two subtitle files from the Desperate
Housewives TV series. We first aligned each paral-
lel corpus using the algorithm described in (Tiede-
mann, 2007), based on time frames and developed
for bilingual subtitles, we then filtered out sentence
pairs below a minimal edit distance threshold, and
manually removed obvious errors made by the algo-
rithm.
e.g. So he uses the photo booths to remind people what
he looks like. ? He uses those machines to remind the
living of his face.
SCENE We used the Multiple Video Description
Corpus (Chen and Dolan, 2011) obtained from mul-
tiple descriptions of short videos. Similarly to what
we did for TEXT, we selected sentence pairs from
clusters by minimal edit distance above a threshold.
An important fact is that for English we were able
to use what is described as ?verified? descriptions.
There were, however, far fewer descriptions avail-
able for French, and none had the ?verified? status.
We decided to use this corpus nonetheless, but with
the knowledge that this source for French is of a sub-
stantially lower quality (this corpus type will there-
fore appear as ?(SCENE)? in all tables to reflect this).
e.g. a boy is riding on a bicycle fast. ? a boy rides a bike
on a dirt road.
1http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2002T01
2http://www.elda.org/article125.html
3http://www.opensubtitles.org
722
Corpus statistics Annotator agreements Tokens in paraphrase statistics
500 sentence pairs 50 sentence pairs not considering identity paraphrases
sure para. possible para.
# tokens # tokens per sent. sure para. possible para. % tokens # tokens % tokens # tokens
ENGLISH
TEXT 21,473 21.0 66.1 20.4 18.6 4004 12.3 2651
SPEECH 11,049 10.5 79.1 10.9 17.5 1942 31.6 3500
SCENE 7,783 7.5 80.5 35.2 10.9 851 14.0 1094
EVENT 8,609 8.0 65.3 20.5 17.5 1506 14.5 1251
FRENCH
TEXT 24,641 24.0 64.6 16.6 29.2 7218 6.2 1527
SPEECH 11,850 11.5 82.7 20.8 22.5 2667 16.7 1981
(SCENE) 7,012 6.5 42.8 9.3 3.9 275 9.4 664
EVENT 9,121 9.1 67.8 3.8 19.6 1793 9.6 876
Table 1: Description of all corpora and paraphrase reference sets for English (top) and French (bottom). Note that
SCENE for French appears within parentheses as we do not consider it of the same quality as the other corpora.
EVENT We used titles of news article clusters
from the Google News4 news aggregation service.
We further refined the clustering algorithm by filter-
ing out article pairs whose publication dates differed
from more than one day. We repeated the same se-
lection procedure as for TEXT and SCENE to have
a maximal cluster coverage and select more similar
pairs first.
e.g. Pigeons Have an Understanding of Numbers on Par
With Primates ? Pigeons Have Numerical Abilities Just
Like Primates
Table 1 provides various statistics for these cor-
pora. The first observation is that TEXT contains sig-
nificantly larger sentences than the other types, more
than twice as long as those of SPEECH. Annotation
was performed following the guidelines proposed by
Cohn et al2008)5 using the YAWAT tool (Germann,
2008), except that alignments where not initially ob-
tained automatically so as not to bias our annota-
tors? work (there were two annotators per language).
The main guidelines that they had to follow were
that sure and possible paraphrases must be distin-
guished, smaller alignments were to be prefered but
any-to-any alignments may be used, and sentences
should be aligned as much as possible. Henceforth,
we will only consider for all reported statistics and
experiments those paraphrases that are not identity
pairs (e.g. (a nice day ? a nice day)), as they are
4http://news.google.com
5See http://staffwww.dcs.shef.ac.uk/
people/T.Cohn/paraphrase_guidelines.pdf
considered trivial as far as acquisition is concerned.
Table 1 also reports inter-annotator agreement6
values computed on sets of 50 sentence pairs. We
find that acceptable values are obtained for sure
paraphrases, but that low values are obtained for
possible paraphrases. This was somehow expected,
given the many possible interpretations of possible
paraphrases, but was not a problem for our experi-
ments: as we will describe in section 3.1, the evalua-
tion metrics we use will not count them as expected
solutions, but will simply not count them as false
when proposed as candidates.
Table 1 finally shows proportions and absolute
numbers of paraphrases of each type for all corpora.
We find that there are approximately the same to-
tal number of paraphrases for English (16,799) and
French (17,001), but that English corpora collec-
tively have an equivalent number of sure and pos-
sible paraphrases (8,303 vs. 8,496) and French have
more sure paraphrases (11,953 vs. 5,048). This may
be explained by the fact that our annotators worked
independently and that the corpora used have dif-
ferences by nature, as our experiments will show.
Other salient results include the fact that TEXT con-
tains more sure paraphrases in number than the other
corpora, that SPEECH contains relatively more pos-
sible paraphrases than the other corpora, and that
SCENE has significantly fewer paraphrases, both in
proportion and number. In Figure 2 various mea-
6For each paraphrase type, we used the average of recall
values obtained for each annotator set as the reference .
723
synonymy typography tense inclusion pragmatics syntax morphology number
ENGLISH
TEXT 51.2 7.6 5.1 12.1 0.6 4.4 12.1 6.4
SPEECH 39.8 25.6 3.5 12.3 1.7 3.5 3.5 9.7
SCENE 50.0 1.3 13.5 21.6 0.0 1.3 5.4 6.7
EVENT 36.9 15.0 8.2 19.1 1.3 6.8 6.8 5.4
FRENCH
TEXT 46.9 9.0 8.7 2.1 3.6 6.6 3.0 19.8
SPEECH 45.5 14.2 8.0 8.0 2.6 11.6 3.5 6.2
(SCENE) 46.4 5.3 3.5 8.9 0.0 5.3 0.0 30.3
EVENT 28.3 19.7 6.1 16.0 7.4 8.6 7.4 6.1
Table 2: Percentages of paraphrase classes in 50 randomly selected sentence pairs for reference paraphrases for English
(top) and French (bottom). Classes are illustrated by the following examples: (mutual understanding ? consensus)
(synonymy), (California ? CA) (typography), (letting ? having let) (tense), (Asian Development Bank ? Asian
Bank) (inclusion), (police dispatcher ? woman) (pragmatics), (grief-stricken ? struck with grief ) (syntactic), (Viet-
name ? Vietnam) (morphology), (mortgage ? mortgages) (number).
sures of sentence pair similarities are given. TEXT
contains the most similar sentence pairs according to
all metrics, with EVENT at a similar level on French.
SCENE has sentence pairs that are more similar than
those in SPEECH for English, but this is not the case
for French. While the metrics used can only provide
a crude account of semantic equivalence at the sen-
tence level, these results clearly indicate that trans-
lating from text yields more similar sentences than
translating from speech.
Table 2 provides a typology of paraphrases found
in all our corpora and two languages, where each
class has been quantified with respect to the refer-
ence alignments.7 The main observation here is that
phrasal synonymy (e.g. mutual understanding ?
consensus) is the most present phenomenon. It is
also interesting to note that the EVENT corpus type,
which is easy to collect on a daily basis, contains ref-
erence paraphrases spread over all classes. Lastly, it
is expected that paraphrases in the pragmatics class
(e.g. police dispatcher ? woman) would be diffi-
cult to acquire, as this would often rely on document
context and costly world knowledge.8
7Note that typologies of paraphrases have already been pro-
posed in the literature (e.g. (Culicover, 1968; Vila et al2011)),
but that the choice of our classes has been primarily moti-
vated by potential subsequent uses of the acquired paraphrases
(paraphrases could be annotated as belonging to more than one
class). Note also that our experiments will also include results
focused on the synonymy class only (cf. Table 5).
8Reusing such types of paraphrases into applications would
however often be too strongly context-dependent.
  COSINE*100 BLEU 1-TER METEOR010
203040
506070 TEXT SPEECH SCENE EVENT
  COSINE*100 BLEU 1-TER METEOR010
203040
506070
Figure 2: Sentence pair average similarities for all cor-
pora for English (left) and French (right) using the co-
sine of token vectors, BLEU (Papineni et al2002),
TER (Snover et al2006) and METEOR (Lavie and
Agarwal, 2007).
3 Bilingual experiments across corpus
types
3.1 Evaluation of paraphrase acquisition
We followed the PARAMETRIC methodology de-
scribed in (Callison-Burch et al2008) for assess-
ing the performance of systems on the task of sub-
sentential paraphrase acquisition. In this methodol-
ogy, a set of paraphrase candidates extracted from
a sentence pair is compared with a set of reference
paraphrases, obtained through human annotation, by
computing usual measures of precision (P ) and re-
call (R). The first value corresponds to the propor-
tion of paraphrase candidates, denoted H, produced
by a system and that are correct relative to the ref-
erence set containing sure and possible paraphrases,
denoted Rall. Recall is obtained by measuring the
proportion of the reference set of sure paraphrases,
724
  TEXT      SPEECH     SCENE       EVENT
GIZA
FASTR
TERp
PIVOT
biphrases
biphrases
biphrases
biphrases
combinationsystem
paraphrases
training and test instances of sentencepairs
union of biphraseswith features
Original signal type of text pairs
Figure 3: Architecture of our combination system for
paraphrase identification.
denoted Rsure, that are found by a system. We also
computed an F-measure value (F1), which consid-
ers recall and precision as equally important. These
values are thus given by the following formulae:
P =
|H ? Rall|
|H|
R =
|H ? Rsure|
|Rsure|
F1 =
2PR
P +R
Note that the way the sets Rall and Rsure of refer-
ence paraphrase pairs are defined ensures that para-
phrase pair candidates that include possible refer-
ence paraphrases will not penalize precision while
not increasing recall.
All performance values reported in the follow-
ing sections will be obtained using 10-fold cross-
validation and averaging the results on each sub-test.
All data sets of cross-validation contain 500 sen-
tence pairs per corpus type, and 125 pairs are kept
for development.
3.2 A framework for sub-sentential paraphrase
identification
We now describe the systems that will be tested
on the various corpora described in section 2 using
the methodology described in section 3.1. Follow-
ing (Bouamor et al2012), a combination system
is used to automatically weight paraphrase pair can-
didates produced by individual systems using a set
of features aiming at recognizing paraphrases, as il-
lustrated on Figure 3. Four individual systems have
been used and are described below: the reasons for
considering those systems include their free avail-
ability, the possibility of using comparable resources
when relevant for our two languages, and the spe-
cific characteristics of the techniques used.
Statistical learning of word alignments (GIZA)
The GIZA++ tool (Och and Ney, 2004) com-
putes statistical word alignment models of increas-
ing complexity from parallel corpora. It was run
on each monolingual corpus of sentence pairs in
both directions, symmetrized alignments were kept
and classical phrase extraction heuristics were ap-
plied (Koehn et al2003), without growing phrases
with unaligned tokens.
Linguistic knowledge on term variation (FASTR)
The FASTR tool (Jacquemin, 1999) spots term vari-
ants in large corpora, where variants are described
through metarules expressing how the morphosyn-
tactic structure of a term variant can be derived
from a given term by means of regular expressions
on morphosyntactic categories. Paradigmatic varia-
tion can also be expressed with constraints between
words, imposing that they be of the same morpho-
logical or semantic family using existing resources
available in our two languages. Variants for all
phrases from one sentence of a pair are extracted
from the other sentence, and the intersection of the
sets for both directions is kept.
Edit rate on word sequences (TERp) The TERp
tool (Snover et al2010) can be used to compute an
optimal set of word and phrase edits that can trans-
form one sentence into another one.9 Edit types are
parameterized by one or more weights which were
optimized towards F-measure by hill climbing with
100 random restarts using the held-out data set con-
sisting of 125 sentence pairs for each corpus type.
Translational equivalence (PIVOT) We exploited
the paraphrase probability defined by Bannard and
Callison-Burch (2005) on bilingual parallel corpora.
We used the Europarl corpus10 of parliamentary de-
bates in English and French, consisting of approx-
imately 1.7 million parallel sentences, using each
language as source and pivot in turn. GIZA++
9Note that contrarily to what TERp allows, we did not used
the possibility of using word or phrase equivalents as those are
only made available for English. This type of knowledge is
however captured in part by the FASTR and PIVOT systems.
10http://statmt.org/europarl
725
Phrase pair features ? edit distance between paraphrases, stem identity, bag-of-tokens similarity, phrase
length ratio
Sentence pair features ? sentence pair similarity (cosine, BLEU, TER, METEOR), relative position of
paraphrases, presence of common tokens at paraphrase boundaries, presence of another paraphrase pair
from each system at paraphrase boundaries, presence of a paraphrase at a different position in the other
sentence
Distributional features ? similarity of token context vectors for each phrase of a paraphrase (derived
from counts in the large English-French parallel corpus from WMT?11 (http://www.statmt.org/
wmt11/translation-task.html) (approx. 30 million parallel sentences)
System features ? combination of the individual systems that proposed the paraphrase pair
Table 3: Features used by our classifiers. Discretized intervals based on median values are used for real values, and
binarized values are used for combinations.
was used for word alignment and phrase transla-
tion probabilities were estimated from them by the
MOSES system (Koehn et al2007). For each
phrase of a sentence pair, we built its set of para-
phrases, and extracted its paraphrase from the other
sentence with highest probability. We repeated this
process in both directions, and finally kept for each
phrase its paraphrase pair from any direction with
highest probability.
Automatic validation of candidate paraphrases
Taking the union of all paraphrase pair candidates
from all the above systems for each sentence pair, we
perform a Maximum Entropy two-class classifica-
tion11, which allows us to include features that were
not necessarily exploited or straightforward to ex-
ploit by individual systems to determine the proba-
bility that each candidate is a good paraphrase. More
generally, this allows us to attempt to learn a more
generic characterization of paraphrases, which could
trivially accept any number of systems as inputs.
Positive examples for the classifier are those from
the union of candidates that are also in the reference
set Rsure, while negative examples are the remaining
ones from the union. The features that we used are
summarized in Table 3.
3.3 Experimental results
Results for individual systems, their union and our
validation system trained on each corpus type are
given on Table 4. First, we find that all individual
systems fare better on TEXT, for which more train-
ing data were available and where semantic equiv-
11Using the implementation at: http://homepages.
inf.ed.ac.uk/lzhang10/maxent_toolkit.html
alence of sentence pairs is most likely. EVENT ap-
pears to be the most difficult corpus type, whereas
one could say that being the most readily data source
this is a disapointing result: we will return to this in
section 3.4. In terms of performance on F-measure
per corpus type, GIZA performs best for TEXT and
SPEECH, containing long sentences with possible
repetitions, while TERp performs on par with GIZA
for SCENE and best for EVENT, where equivalences
that are rare at the corpus level are more present.
FASTR achieves a very low recall, showing that the
encoded definitions of term variants do not cover all
types of paraphrases, and also possibly that the lex-
ical resource that it uses has incomplete coverage.
It nonetheless obtains high precision values, most
notably on TEXT. One last comment regarding in-
dividual systems is that PIVOT is by far the most
precise of all the techniques used, but with a recall
much lower than those of GIZA and TERp: as is
the case for FASTR, which makes use of manually-
encoded lexical resources, PIVOT encodes in some
sense some kind of semantic knowledge.12
In all cases, our combination system manages
to increase F-measure substantially over the best
individual system for a corpus type and the sim-
ple union. Improvements are strong on TEXT
(resp. +12.5 and +11.6 on English and French)
and on SPEECH (+11.7 and +11.1) and quite good
on SCENE (+3.2 and +6.4) and on EVENT (+5.4
12Note that the fact that English and French were used as the
pivot for one another may have had some positive effect here,
but, incidentally, the two corpora obtained by translating from
the other language (TEXT and SPEECH) are not those where
PIVOT fares better. The difference observed may however lie in
the higher complexity of the sentences in these corpus types.
726
Individual systems Combination systems
GIZA FASTR TERp?F PIVOT union validation
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
ENGLISH
TEXT 48.2 58.9 53.0 63.1 5.9 10.7 41.2 66.4 50.9 73.4 25.8 38.2 20.8 80.8 33.1 68.4 62.8 65.5
SPEECH 39.7 44.2 41.8 27.1 3.5 6.3 25.0 50.3 33.4 79.2 15.3 25.7 25.5 71.4 37.6 51.0 56.3 53.5
SCENE 44.8 57.7 50.5 47.4 5.2 9.5 40.1 67.9 50.4 84.6 14.6 25.0 36.2 83.4 50.5 44.9 66.8 53.7
EVENT 19.0 33.9 24.3 62.9 3.1 6.0 28.8 68.7 40.6 97.4 11.2 20.1 20.8 75.5 32.7 35.0 67.1 46.0
FRENCH
TEXT 52.5 58.9 55.5 56.9 4.9 9.1 46.4 61.4 52.8 64.5 30.3 41.2 41.5 77.9 54.1 74.7 61.0 67.1
SPEECH 44.0 54.9 48.9 30.7 4.3 7.6 34.8 60.2 44.1 75.5 19.0 30.4 31.4 76.2 44.5 60.2 59.7 60.0
(SCENE) 14.4 43.6 21.7 53.0 4.0 7.4 13.8 75.3 23.4 94.6 5.21 9.8 12.7 86.4 22.2 19.9 59.8 29.8
EVENT 28.7 44.2 34.8 34.4 2.3 4.3 29.9 58.9 39.7 79.5 15.0 25.2 25.2 72.5 37.4 40.0 56.3 46.8
Table 4: Evaluation results for individual systems (left) and combination systems (right) on all corpus types for English
(top) and French (bottom). Values in bold are for highest values for a given metric for each corpus type and language.
and +6.1). Recall from Table 1 that TEXT and
SPEECH were the two corpus types with the highest
number of sure paraphrase examples for both lan-
guages: results show that our classifier was able to
efficiently use them.
Recall values for the union are quite strong for
all corpus types, ranging from 71.4 (SPEECH in En-
glish) to 83.4 (SCENE in English). There is, how-
ever, a substantial decrease between the unions and
the results of our combination systems, although
recall values for our systems are roughly between
56 and 67, which may be considered an acceptable
range on such a task. Further study of false neg-
atives should help with engineering new features to
improve paraphrase recognition. Lastly, we note that
precision is in general highest for a specific system
(PIVOT), and reaches high values for our validation
system on TEXT, where we have the most examples
(resp. 68.4 and 74.7 for English and French).
As seen in Table 2, synonymy is the most present
phenomenon in all our corpora; it is also proba-
bly one of the most useful type of knowledge for
many applications. We now therefore focus on this
class, for which all the sure paraphrases in our cor-
pora falling in this class have been annotated. Ta-
ble 5 shows F-measure values for the individual
techniques and our combination systems on all cor-
pus types. We first observe that our combination sys-
tem also always improves here over the best individ-
ual system, albeit not by a large margin on EVENT.
GIZA FASTR TERp PIVOT validation
ENGLISH
TEXT 52.2 6.1 47.3 47.1 68.1
SPEECH 42.6 5.0 30.3 39.5 54.9
SCENE 51.8 6.0 48.0 26.0 56.3
EVENT 22.5 2.1 34.8 24.7 35.5
FRENCH
TEXT 55.3 3.9 50.7 50.5 70.3
SPEECH 49.8 1.6 40.9 36.2 57.2
(SCENE) 19.6 4.2 23.1 0.0 24.7
EVENT 36.8 3.5 35.3 25.6 39.9
Table 5: F-measure values for test instances in the syn-
onymy class (see Table 2) for all individual systems and
our validation system for English (top) and French (bot-
tom).
Also, we find that PIVOT performs relatively closer
to GIZA and TERp on TEXT and SPEECH than for
the full set of classes, confirming the intuition that
translational equivalence may be appropriate to rec-
ognize synonymy.
3.4 Experiments across corpus types
To test how different the corpora under study are as
regards paraphrase identification, we now consider
using as additional training data for our classifiers
corpora of the other types, both individually and col-
lectively. Results are given on Table 6.13
13Note that our results are still given by performing cross-
validation averaging over 10 test sets for each tested corpus
type.
727
+TEXT +SPEECH +SCENE +EVENT +All
ENGLISH
# ex+ 7,342 2,296 1,784 1,171 12,593
TEXT 65.5 66.2 65.1 66.2 65.1
SPEECH 56.0 53.5 52.8 54.8 56.6
SCENE 49.7 54.3 53.7 53.8 42.7
EVENT 51.1 45.3 42.5 46.0 56.2
FRENCH
# ex+ 12,961 3,340 966 2,160 19,427
TEXT 67.1 67.2 66.7 67.0 66.6
SPEECH 57.6 60.0 56.4 59.6 57.9
(SCENE) 23.7 22.0 29.8 23.9 21.1
EVENT 45.2 45.6 44.3 46.8 49.3
Table 6: Evaluation results (F1 scores) for all corpus
types for English (top) and French (bottom) when adding
training material from other corpus types (values with
gray background on the diagonal are when no additional
training data are used). ?#ex+? rows indicate numbers of
positive paraphrase examples for each additional corpus
type.
The most notable observation is that EVENT is
substantially improved by using all available addi-
tional training data for English (+10.2), and to a
lesser extent for French (+2.5) . It should be noted
that no individual corpus type, save TEXT, individu-
ally improves results on EVENT, and that results are
yet substantially improved over the use of training
data from TEXT when using all available data, re-
vealing a collective contribution of all corpus types.
The second major observation is that all other cor-
pus types seem to be quite specific in nature, as no
addition of training data from other types yields any
improvement (with the exception of SPEECH on En-
glish), but they often in fact decrease performance.
For instance, SCENE in English is substantially neg-
atively impacted by the use of the numerous exam-
ples of TEXT (-4 in F-measure) and even more when
using all other training data (-9). This underlines
the specific nature of this corpus type: independent
descriptions of the same scene in a video may be
worded with much variation that mostly differ from
that present in other corpus types.
Our main conclusion here is therefore that all our
corpora under study are quite specific in nature, but
that EVENT can benefit from all training data from
the other corpus types. We can further note that the
fact that TEXT is almost not impacted by additional
data may also be explained by the fact that this cor-
pus type contains more than half of the total number
of examples for the two languages. Finally, there are
substantially more positive paraphrase examples for
French (19,427) than for English (12,593).
4 Related work
Over the years, paraphrase acquisition and genera-
tion have attracted a wealth of research works that
are too many to adequatly summarize here: (Mad-
nani and Dorr, 2010) presents a complete and up-
to-date review of the main approaches. Sentential
paraphrase collection has been tackled from specific
resources increasing the probability of sentences be-
ing paraphrases (Dolan et al2004; Bernhard and
Gurevych, 2008; Wubben et al2009), from com-
parable monolingual corpora (Barzilay and Elhadad,
2003; Fung and Cheung, 2004; Nelken and Shieber,
2006), and even at web scale (Pasc?a and Dienes,
2005; Bhagat and Ravichandran, 2008).
Various techniques have been proposed for para-
phrase acquisition from related sentence pairs
(Barzilay and McKeown, 2001; Pang et al2003)
and from bilingual parallel corpora (Bannard and
Callison-Burch, 2005; Kok and Brockett, 2010).
The issue of corpus construction for developing and
evaluating paraphrase acquisition techniques are ad-
dressed in (Cohn et al2008; Callison-Burch et al
2008). To the best of our knowledge, this is the first
time that a study in paraphrase acquisition is con-
ducted on several corpus types and for 2 languages.
Faruqui and Pado? (2011) study the acquisition of en-
tailment pairs (premise and hypothesis), with ex-
periments in 3 languages and various domains of
newspaper corpora for one language. Although their
work is not directly comparable to ours, they report
that robustness across domains is difficult to achieve.
Laslty, the evaluation of automatically generated
paraphrases has recently received some attention
(Liu et al2010; Chen and Dolan, 2011; Met-
zler et al2011) although it remains a difficult is-
sue. Application-driven paraphrase generation pro-
vides indirect means of evaluating paraphrase gen-
eration (Zhao et al2009). For instance, the field of
Statistical Machine Translation has produced works
showing both the usefulness of human-produced
728
(Schroeder et al2009; Resnik et al2010) and au-
tomatically produced paraphrases (Madnani et al
2008; Marton et al2009; Max, 2010; He et al
2011) for improving translation performance.
5 Discussion and future work
This work has addressed the issue of sub-sentential
paraphrase acquisition from text pairs. Analogu-
ously to bilingual parallel corpora, which are still
to date the most reliable resources for automatic ac-
quisition of sub-sentential translations, monolingual
parallel corpora are generally regarded as very ap-
propriate for paraphrase acquisition. However, their
low availability makes searching for less parallel
corpora a necessity. In this study, we have attempted
to identify corpora of various degrees of semantic
textual similarity by considering text pairs originat-
ing from various signal types. These signal types
allow various degrees of freedom as to how to for-
mulate a text: a text is read and translated into a dif-
ferent language (TEXT); some speech is listened to
in the context of a visual story and translated into a
different language (SPEECH); some action is looked
at and described (SCENE); and some event that took
place is concisely reported (EVENT).
The results presented in this paper have shown
how these corpora differed in various aspects. First,
they contain varying quantities of paraphrases that
are differently distributed into paraphrase classes.
Individual acquisition techniques, based on statis-
tical learning of word alignments (GIZA), linguis-
tic knowledge on term variation (FASTR), edit rate
on word sequence (TERp), and translational equiv-
alence (PIVOT), for which different performances
were observed among them on the same corpus
type, were shown to achieve different performances
across corpus types. An efficient combination of
candidate paraphrases from these individual tech-
niques exploiting additional features to character-
ize paraphrases has yielded substantial increases in
performance on all corpus types; however, it is in-
teresting to note that the highest amplitude in per-
formance across corpus types was not so much on
recall (amplitude of 10.5 on English and 4.7 on
French) than on precision (amplitude of 33.4 on En-
glish and 34.714 on French). This, some other fac-
14Not considering (SCENE) for French.
tors aside, emphasizes the fact that the correct idenfi-
cation of paraphrases is facilitated when equivalence
of semantic content is more probable. Many works
have accordingly attempted to identify text units that
are as parallel as possible from large corpora, and
the task of measuring semantic textual similarity,
which can find many uses, has received some atten-
tion lately (Agirre et al2012). However, it itself
relies on some knowledge on paraphrasing.
Our avenues for future work lie in three main ar-
eas. The first one is to continue our current line of
work and study the impact of additional individual
acquisition techniques and better characterizations
of paraphrases in context, in tandem with working
on identifying parallel text pairs in large corpora.
Another avenue is to start from the output of high
recall techniques and to attempt to characterize the
contexts of possible substitution for candidate para-
phrases from large corpora as a means to acquire
precise paraphrases. As the examples from Table 7
show, some classes of paraphrases, and in particular
in the continuum from our synonymy to pragmat-
ics classes, require the joint acquisition of contextual
information that license substitution. Lastly, we plan
to apply such knowledge in text-to-text applications.
synonymy
TEXT take part in ? participate in
great assistance ? enormous help
SPEECH make a deal ? come to an agreement
I don?t care ? I don?t give a damn
SCENE riding a bicycle ? cycling
lady ? woman
EVENT jail escapee ? prison fugitive
apologizes ? expresses regret
pragmatics
TEXT flew in ? arrived in
flood-control materials ? needed supplies
SPEECH face ? picture
want to sleep ? dream about sleeping
SCENE a man ? someone
bento ? food
EVENT violence ? bloodshed
anger ? emotion
Table 7: Examples in English for the synonymy and
pragmatics classes.
729
Acknowledgements
The authors would like to thank the reviewers for
their comments and suggestions. This work was
partly funded by ANR project Edylex (ANR-09-
CORD-008).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
SemEval, Montre?al, Canada.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP, Sapporo, Japan.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL, Toulouse, France.
Delphine Bernhard and Iryna Gurevych. 2008. Answer-
ing Learners? Questions by Retrieving Question Para-
phrases from Social Q&A Sites. In Proceedings of the
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, Columbus, USA.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2012.
Validation of sub-sentential paraphrases acquired from
parallel monolingual corpora. In EACL, Avignon,
France.
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
2008. Parametric: An automatic evaluation metric for
paraphrasing. In Proceedings of COLING, Manch-
ester, UK.
David Chen and William Dolan. 2011. Collecting highly
parallel data for paraphrase evaluation. In Proceedings
of ACL, Portland, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4).
P. W. Culicover. 1968. Paraphrase Generation and Infor-
mation Retrieval from Stored Text. Mechanical Trans-
lation and Computational Linguistics, 11:78?88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING, Geneva, Switzerland.
Manaal Faruqui and Sebastian Pado?. 2011. Acquiring
entailment pairs across languages and domains: A data
analysis. In Proceedings of the International Con-
ference on Computational Semantics (IWCS), Oxford,
UK.
Pascale Fung and Percy Cheung. 2004. Multi-level
bootstrapping for extracting parallel sentences from a
quasi-comparable corpus. In Proceedings of COLING,
Geneva, Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-HLT,
demo session, Columbus, USA.
Wei He, Shiqi Zhao, Haifeng Wang, and Ting Liu. 2011.
Enriching SMT Training Data via Paraphrasing. In
Proceedings of IJCNLP, Chiang Mai, Thailand.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, College Park, USA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of NAACL-HLT, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL, demo session, Prague, Czech Repub-
lic.
Stanley Kok and Chris Brockett. 2010. Hitting the Right
Paraphrases in Good Time. In Proceedings of NAACL,
Los Angeles, USA.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the ACL Workshop on Statistical Machine Translation,
Prague, Czech Republic.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
PEM: A paraphrase evaluation metric exploiting par-
allel texts. In Proceedings of EMNLP, Cambridge,
USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
Driven Methods . Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are multiple reference
translations necessary? investigating the value of
paraphrased reference translations in parameter opti-
mization. In Proceedings of AMTA, Waikiki, USA.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceedings
of EMNLP, Singapore.
730
Aure?lien Max. 2010. Example-Based Paraphrasing for
Improved Phrase-Based Statistical Machine Transla-
tion. In Proceedings of EMNLP, Cambridge, USA.
Donald Metzler, Eduard Hovy, and Chunliang Zhang.
2011. An empirical evaluation of data-driven para-
phrase generation techniques. In Proceedings of ACL-
HLT, Portland, USA.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of EACL, Trento, Italy.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
Philadelphia, USA.
Marius Pasc?a and Peter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across the
Web. In Proceedings of IJCNLP, Jeju Island, South
Korea.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin B. Bederson. 2010. Im-
proving translation via targeted paraphrasing. In Pro-
ceedings of EMNLP, Cambridge, USA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word Lattices for Multi-Source Translation. In Pro-
ceedings of EACL, Athens, Greece.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of AMTA, Boston, USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Jo?rg Tiedemann. 2007. Building a Multilingual Paral-
lel Subtitle Corpus. In Proceedings of the Conference
on Computational Linguistics in the Netherlands, Leu-
ven, Belgium.
Marta Vila, M. Anto`nia Mart??, and Horacio Rodr??guez.
2011. Paraphrase Concept and Typology. A Linguisti-
cally Based and Computationally Oriented Approach.
Procesamiento del Lenguaje Natural, (462-3).
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and maching
headlines for automatic paraphrase acquisition. In
Proceedings of the European Workshop on Natural
Language Generation, Athens, Greece.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven Statistical Paraphrase Generation.
In Proceedings of ACL-IJCNLP, Singapore.
731
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207?213,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Human Judgment Corpus and a Metric for Arabic MT Evaluation
Houda Bouamor, Hanan Alshikhabobakr, Behrang Mohit and Kemal Oflazer
Carnegie Mellon University in Qatar
{hbouamor,halshikh,behrang,ko}@cmu.edu
Abstract
We present a human judgments dataset
and an adapted metric for evaluation of
Arabic machine translation. Our medium-
scale dataset is the first of its kind for Ara-
bic with high annotation quality. We use
the dataset to adapt the BLEU score for
Arabic. Our score (AL-BLEU) provides
partial credits for stem and morphologi-
cal matchings of hypothesis and reference
words. We evaluate BLEU, METEOR and
AL-BLEU on our human judgments cor-
pus and show that AL-BLEU has the high-
est correlation with human judgments. We
are releasing the dataset and software to
the research community.
1 Introduction
Evaluation of Machine Translation (MT) contin-
ues to be a challenging research problem. There
is an ongoing effort in finding simple and scal-
able metrics with rich linguistic analysis. A wide
range of metrics have been proposed and evaluated
mostly for European target languages (Callison-
Burch et al., 2011; Mach?a?cek and Bojar, 2013).
These metrics are usually evaluated based on their
correlation with human judgments on a set of MT
output. While there has been growing interest in
building systems for translating into Arabic, the
evaluation of Arabic MT is still an under-studied
problem. Standard MT metrics such as BLEU (Pa-
pineni et al., 2002) or TER (Snover et al., 2006)
have been widely used for evaluating Arabic MT
(El Kholy and Habash, 2012). These metrics use
strict word and phrase matching between the MT
output and reference translations. For morpholog-
ically rich target languages such as Arabic, such
criteria are too simplistic and inadequate. In this
paper, we present: (a) the first human judgment
dataset for Arabic MT (b) the Arabic Language
BLEU (AL-BLEU), an extension of the BLEU
score for Arabic MT evaluation.
Our annotated dataset is composed of the output
of six MT systems with texts from a diverse set of
topics. A group of ten native Arabic speakers an-
notated this corpus with high-levels of inter- and
intra-annotator agreements. Our AL-BLEU met-
ric uses a rich set of morphological, syntactic and
lexical features to extend the evaluation beyond
the exact matching. We conduct different exper-
iments on the newly built dataset and demonstrate
that AL-BLEU shows a stronger average correla-
tion with human judgments than the BLEU and
METEOR scores. Our dataset and our AL-BLEU
metric provide useful testbeds for further research
on Arabic MT and its evaluation.
1
2 Related Work
Several studies on MT evaluation have pointed out
the inadequacy of the standard n-gram based eval-
uation metrics for various languages (Callison-
Burch et al., 2006). For morphologically complex
languages and those without word delimiters, sev-
eral studies have attempted to improve upon them
and suggest more reliable metrics that correlate
better with human judgments (Denoual and Lep-
age, 2005; Homola et al., 2009).
A common approach to the problem of mor-
phologically complex words is to integrate some
linguistic knowledge in the metric. ME-
TEOR (Denkowski and Lavie, 2011), TER-
Plus (Snover et al., 2010) incorporate limited lin-
guistic resources. Popovi?c and Ney (2009) showed
that n-gram based evaluation metrics calculated on
POS sequences correlate well with human judg-
ments, and recently designed and evaluated MPF,
a BLEU-style metric based on morphemes and
POS tags (Popovi?c, 2011). In the same direc-
1
The dataset and the software are available at:
http://nlp.qatar.cmu.edu/resources/
AL-BLEU
207
tion, Chen and Kuhn (2011) proposed AMBER,
a modified version of BLEU incorporating re-
call, extra penalties, and light linguistic knowl-
edge about English morphology. Liu et al. (2010)
propose TESLA-M, a variant of a metric based
on n-gram matching that utilizes light-weight lin-
guistic analysis including lemmatization, POS tag-
ging, and WordNet synonym relations. This met-
ric was then extended to TESLA-B to model
phrase synonyms by exploiting bilingual phrase
tables (Dahlmeier et al., 2011). Tantug et al.
(2008) presented BLEU+, a tool that implements
various extension to BLEU computation to allow
for a better evaluation of the translation perfor-
mance for Turkish.
To the best of our knowledge the only human
judgment dataset for Arabic MT is the small cor-
pus which was used to tune parameters of the ME-
TEOR metric for Arabic (Denkowski and Lavie,
2011). Due to the shortage of Arabic human judg-
ment dataset, studies on the performance of eval-
uation metrics have been constrained and limited.
A relevant effort in this area is the upper-bound es-
timation of BLEU and METEOR scores for Ara-
bic MT output (El Kholy and Habash, 2011). As
part of its extensive functionality, the AMEANA
system provides the upper-bound estimate by an
exhaustive matching of morphological and lexical
features between the hypothesis and the reference
translations. Our use of morphological and lex-
ical features overlaps with the AMEANA frame-
work. However, we extend our partial matching
to a supervised tuning framework for estimating
the value of partial credits. Moreover, our human
judgment dataset allows us to validate our frame-
work with a large-scale gold-standard data.
3 Human judgment dataset
We describe here our procedure for compiling a
diverse Arabic MT dataset and annotating it with
human judgments.
3.1 Data and systems
We annotate a corpus composed of three datasets:
(1) the standard English-Arabic NIST 2005 cor-
pus, commonly used for MT evaluations and com-
posed of news stories. We use the first English
translation as the source and the single corre-
sponding Arabic sentence as the reference. (2) the
MEDAR corpus (Maegaard et al., 2010) that con-
sists of texts related to the climate change with
four Arabic reference translations. We only use
the first reference in this study. (3) a small dataset
of Wikipedia articles (WIKI) to extend our cor-
pus and metric evaluation to topics beyond the
commonly-used news topics. This sub-corpus
consists of our in-house Arabic translations of
seven English Wikipedia articles. The articles are:
Earl Francis Lloyd, Western Europe, Citizenship,
Marcus Garvey, Middle Age translation, Acadian,
NBA. The English articles which do not exist in
the Arabic Wikipedia were manually translated by
a bilingual linguist.
Table 1 gives an overview of these sub-corpora
characteristics.
NIST MEDAR WIKI
# of Documents 100 4 7
# of Sentences 1056 509 327
Table 1: Statistics on the datasets.
We use six state-of-the-art English-to-Arabic
MT systems. These include four research-oriented
phrase-based systems with various morphological
and syntactic features and different Arabic tok-
enization schemes and also two commercial off-
the-shelf systems.
3.2 Annotation of human judgments
In order conduct a manual evaluation of the six
MT systems, we formulated it as a ranking prob-
lem. We adapt the framework used in the WMT
2011 shared task for evaluating MT metrics on
European language pairs (Callison-Burch et al.,
2011) for Arabic MT. We gather human ranking
judgments by asking ten annotators (each native
speaker of Arabic with English as a second lan-
guage) to assess the quality of the English-Arabic
systems, by ranking sentences relative to each
other, from the best to the worst (ties are allowed).
We use the Appraise toolkit (Federmann, 2012)
designed for manual MT evaluation. The tool dis-
plays to the annotator, the source sentence and
translations produced by various MT systems. The
annotators received initial training on the tool and
the task with ten sentences. They were presented
with a brief guideline indicating the purpose of the
task and the main criteria of MT output evaluation.
Each annotator was assigned to 22 ranking
tasks. Each task included ten screens. Each screen
involveed ranking translations of ten sentences. In
total, we collected 22, 000 rankings for 1892 sen-
208
tences (22 tasks?10 screens?10 judges). In each
annotation screen, the annotator was shown the
source-language (English) sentences, as well as
five translations to be ranked. We did not provide
annotators with the reference to avoid any bias in
the annotation process. Each source sentence was
presented with its direct context. Rather than at-
tempting to get a complete ordering over the sys-
tems, we instead relied on random selection and a
reasonably large sample size to make the compar-
isons fair (Callison-Burch et al., 2011).
An example of a source sentence and its five
translations to be ranked is given in Table 2.
3.3 Annotation quality and analysis
In order to ensure the validity of any evaluation
setup, a reasonable of inter- and intra-annotator
agreement rates in ranking should exist. To mea-
sure these agreements, we deliberately reassigned
10% of the tasks to second annotators. More-
over, we ensured that 10% of the screens are re-
displayed to the same annotator within the same
task. This procedure allowed us to collect reliable
quality control measure for our dataset.
?
inter
?
intra
EN-AR 0.57 0.62
Average EN-EU 0.41 0.57
EN-CZ 0.40 0.54
Table 3: Inter- and intra-annotator agreement
scores for our annotation compared to the aver-
age scores for five English to five European lan-
guages and also English-Czech (Callison-Burch et
al., 2011).
We measured head-to-head pairwise agreement
among annotators using Cohen?s kappa (?) (Co-
hen, 1968), defined as follows:
? =
P (A)? P (E)
1? P (E)
where P(A) is the proportion of times annotators
agree and P(E) is the proportion of agreement by
chance.
Table 3 gives average values obtained for inter-
annotator and intra-annotator agreement and com-
pare our results to similar annotation efforts in
WMT-13 on different European languages. Here
we compare against the average agreement for En-
glish to five languages and also from English to
one morphologically rich language (Czech).
4
Based on Landis and Koch (1977) ? interpre-
tation, the ?
inter
value (57%) and also compar-
ing our agreement scores with WMT-13 annota-
tions, we believe that we have reached a reliable
and consistent annotation quality.
4 AL-BLEU
Despite its well-known shortcomings (Callison-
Burch et al., 2006), BLEU continues to be the
de-facto MT evaluation metric. BLEU uses an
exact n-gram matching criterion that is too strict
for a morphologically rich language like Arabic.
The system outputs in Table 2 are examples of
how BLEU heavily penalizes Arabic. Based on
BLEU, the best hypothesis is from Sys
5
which has
three unigram and one bigram exact matches with
the reference. However, the sentence is the 4
th
ranked by annotators. In contrast, the output of
Sys
3
(ranked 1
st
by annotators) has only one ex-
act match, but several partial matches when mor-
phological and lexical information are taken into
consideration.
We propose the Arabic Language BLEU (AL-
BLEU) metric which extends BLEU to deal with
Arabic rich morphology. We extend the matching
to morphological, syntactic and lexical levels with
an optimized partial credit. AL-BLEU starts with
the exact matching of hypothesis tokens against
the reference tokens. Furthermore, it considers the
following: (a) morphological and syntactic feature
matching, (b) stem matching. Based on Arabic lin-
guistic intuition, we check the matching of a sub-
set of 5 morphological features: (i) POS tag, (ii)
gender (iii) number (iv) person (v) definiteness.
We use the MADA package (Habash et al., 2009)
to collect the stem and the morphological features
of the hypothesis and reference translation.
Figure 1 summarizes the function in which we
consider partial matching (m(t
h
, t
r
)) of a hypoth-
esis token (t
h
) and its associated reference token
(t
r
). Starting with the BLEU criterion, we first
check if the hypothesis token is same as the ref-
erence one and provide the full credit for it. If
the exact matching fails, we provide partial credit
for matching at the stem and morphological level.
The value of the partial credits are the sum of
the stem weight (w
s
) and the morphological fea-
4
We compare against the agreement score for annotations
performed by WMT researchers which are higher than the
WMT annotations on Mechanical Turk.
209
Source France plans to attend ASEAN emergency summit.
Reference .

?

KPA??@
	
?AJ


?B@

??

?
P?
	
?k ?
	
Q

?

K A?
	
Q
	
?
frnsaA tEtzm HDwr qmp AaAlaAsyaAn AaAlTaAr}ip
Hypothesis
Systems Rank
Annot
BLEU Rank
BLEU
AL-BLEU Rank
AL?BLEU
Sys
1
2 0.0047 2 0.4816 1

?

KPA??@
	
?AJ


?

B@

??

?
P?
	
?m
?
A?
	
Q
	
? ??
	
m
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 716?725,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Validation of sub-sentential paraphrases acquired
from parallel monolingual corpora
Houda Bouamor Aure?lien Max
LIMSI-CNRS & Univ. Paris Sud
Orsay, France
firstname.lastname@limsi.fr
Anne Vilnat
Abstract
The task of paraphrase acquisition from re-
lated sentences can be tackled by a variety
of techniques making use of various types
of knowledge. In this work, we make the
hypothesis that their performance can be
increased if candidate paraphrases can be
validated using information that character-
izes paraphrases independently of the set of
techniques that proposed them. We imple-
ment this as a bi-class classification prob-
lem (i.e. paraphrase vs. not paraphrase),
allowing any paraphrase acquisition tech-
nique to be easily integrated into the com-
bination system. We report experiments on
two languages, English and French, with
5 individual techniques on parallel mono-
lingual parallel corpora obtained via multi-
ple translation, and a large set of classifi-
cation features including surface to contex-
tual similarity measures. Relative improve-
ments in F-measure close to 18% are ob-
tained on both languages over the best per-
forming techniques.
1 Introduction
The fact that natural language allows messages
to be conveyed in a great variety of ways consti-
tutes an important difficulty for NLP, with appli-
cations in both text analysis and generation. The
term paraphrase is now commonly used in the
NLP litterature to refer to textual units of equiva-
lent meaning at the phrasal level (including single
words). For instance, the phrases six months and
half a year form a paraphrase pair applicable in
many different contexts, as they would appropri-
ately denote the same concept. Although one can
envisage to manually build high-coverage lists of
synonyms, enumerating meaning equivalences at
the level of phrases is too daunting a task for hu-
mans. Because this type of knowledge can how-
ever greatly benefit many NLP applications, au-
tomatic acquisition of such paraphrases has at-
tracted a lot of attention (Androutsopoulos and
Malakasiotis, 2010; Madnani and Dorr, 2010),
and significant research efforts have been devoted
to this objective (Callison-Burch, 2007; Bhagat,
2009; Madnani, 2010).
Central to acquiring paraphrases is the need of
assessing the quality of the candidate paraphrases
produced by a given technique. Most works to
date have resorted to human evaluation of para-
phrases on the levels of grammaticality and mean-
ing equivalence. Human evaluation is however
often criticized as being both costly and non re-
producible, and the situation is even more compli-
cated by the inherent complexity of the task that
can produce low inter-judge agreement. Task-
based evaluation involving the use of paraphras-
ing into some application thus seem an acceptable
solution, provided the evaluation methodologies
for the given task are deemed acceptable. This,
in turn, puts the emphasis on observing the im-
pact of paraphrasing on the targeted application
and is rarely accompanied by a study of the intrin-
sic limitations of the paraphrase acquisition tech-
nique used.
The present work is concerned with the task of
sub-sentential paraphrase acquisition from pairs
of related sentences. A large variety of tech-
niques have been proposed that can be applied
to this task. They typically make use of differ-
ent kinds of automatically or manually acquired
knowledge. We make the hypothesis that their
performance can be increased if candidate para-
716
phrases can be validated using information that
characterize paraphrases in complement to the set
of techniques that proposed them. We propose to
implement this as a bi-class classification problem
(i.e. paraphrase vs. not paraphrase), allowing
any paraphrase acquisition technique to be easily
integrated into the combination system. In this
article, we report experiments on two languages,
English and French, with 5 individual techniques
based on a) statistical word alignment models,
b) translational equivalence, c) handcoded rules of
term variation, d) syntactic similarity, and e) edit
distance on word sequences. We used parallel
monolingual parallel corpora obtained via mul-
tiple translation from a single language as our
sources of related sentences, and a large set of
features including surface to contextual similarity
measures. Relative improvements in F-measure
close to 18% are obtained on both languages over
the best performing techniques.
The remainder of this article is organized as
follows. We first briefly review previous work
on sub-sentential paraphrase acquisition in sec-
tion 2. We then describe our experimental setting
in section 3 and the individual techniques that we
have studied in section 4. Section 5 is devoted to
our approach for validating paraphrases proposed
by individual techniques. Finally, section 6 con-
cludes the article and presents some of our future
work in the area of paraphrase acquisition.
2 Related work
The hypothesis that if two words or, by exten-
sion, two phrases, occur in similar contexts then
they may be interchangeable has been extensively
tested. The distributional hypothesis, attributed to
Zellig Harris, was for example applied to syntac-
tic dependency paths in the work of Lin and Pan-
tel (2001). Their results take the form of equiva-
lence patterns with two arguments such as {X asks
for Y, X requests Y, X?s request for Y, X wants Y,
Y is requested by X, . . .}.
Using comparable corpora, where the same in-
formation probably exists under various linguis-
tic forms, increases the likelihood of finding very
close contexts for sub-sentential units. Barzilay
and Lee (2003) proposed a multi-sequence align-
ment algorithm that takes structurally similar sen-
tences and builds a compact lattice representation
that encodes local variations. The work by Bhagat
and Ravichandran (2008) describes an application
of a similar technique on a very large scale.
The hypothesis that two words or phrases are
interchangeable if they share a common trans-
lation into one or more other languages has
also been extensively studied in works on sub-
sentential paraphrase acquisition. Bannard and
Callison-Burch (2005) described a pivoting ap-
proach that can exploit bilingual parallel corpora
in several languages. The same technique has
been applied to the acquisition of local paraphras-
ing patterns in Zhao et al(2008). The work of
Callison-Burch (2008) has shown how the mono-
lingual context of a sentence to paraphrase can be
used to improve the quality of the acquired para-
phrases.
Another approach consists in modelling local
paraphrasing identification rules. The work of
Jacquemin (1999) on the identification of term
variants, which exploits rewriting morphosyntac-
tic rules and descriptions of morphological and
semantic lexical families, can be extended to ex-
tract the various forms corresponding to input pat-
terns from large monolingual corpora.
When parallel monolingual corpora aligned at
the sentence level are available (e.g. multiple
translations into the same language), the task of
sub-sentential paraphrase acquisition can be cast
as one of word alignment between two aligned
sentences (Cohn et al 2008). Barzilay and
McKeown (2001) applied the distributionality hy-
pothesis on such parallel sentences, and Pang et
al. (2003) proposed an algorithm to align sen-
tences by recursive fusion of their common syn-
tactic constituants.
Finally, they has been a recent interest in auto-
matic evaluation of paraphrases (Callison-Burch
et al 2008; Liu et al 2010; Chen and Dolan,
2011; Metzler et al 2011).
3 Experimental setting
We used the main aspects of the methodology
described by Cohn et al(2008) for constructing
evaluation corpora and assessing the performance
of techniques on the task of sub-sentential para-
phrase acquisition. Pairs of related sentences are
hand-aligned to define a set of reference atomic
paraphrase pairs at the level of words or phrases,
denoted asRatom1.
1Note that in this study we do not distinguish between
?Sure? and ?Possible? alignments, and when reusing anno-
717
single language multiple language video descriptions multiply-translated news headlines
translation translation subtitles
# tokens 4,476 4,630 1,452 2,721 1,908
# unique tokens 656 795 357 830 716
% aligned tokens (excluding identities) 60.58 48.80 23.82 29.76 14.46
lexical overlap (tokens) 77.21 61.03 59.50 32.51 39.63
lexical overlap (lemmas content words) 83.77 71.04 64.83 39.54 45.31
translation edit rate (TER) 0.32 0.55 0.76 0.68 0.62
penalized n-gram prec. (BLEU) 0.33 0.15 0.13 0.14 0.39
Table 1: Various indicators of sentence pair comparability for different corpus types. Statistics are reported for
French on sets of 100 sentence pairs.
We conducted a small-scale study to assess dif-
ferent types of corpora of related sentences:
1. single language translation Corpora ob-
tained by several independent human trans-
lation of the same sentences (e.g. (Barzilay
and McKeown, 2001)).
2. multiple language translation Same as
above, but where a sentence is translated
from 4 different languages into the same lan-
guage (Bouamor et al 2010).
3. video descriptions Descriptions of short
YouTube videos obtained via Mechanical
Turk (Chen and Dolan, 2011).
4. multiply-translated subtitles Aligned mul-
tiple translations of contributed movie subti-
tles (Tiedemann, 2007).
5. comparable news headlines News head-
lines collected from Google News clusters
(e.g. (Dolan et al 2004)).
We collected 100 sentence pairs of each type
in French, for which various comparability mea-
sures are reported on Table 1. In particular, the
?% aligned tokens? row indicates the propor-
tion of tokens from the sentence pairs that could
be manually aligned by a native-speaker annota-
tor.2 Obviously, the more common tokens two
sentences from a pair contain, the fewer sub-
sentential paraphrases may be extracted from that
pair. However, high lexical overlap increases the
probability that two sentences be indeed para-
phrases, and in turn the probability that some of
their phrases be paraphrases. Furthermore, the
tated corpora using them we considered all alignments as be-
ing correct.
2The same annotator hand-aligned the 5*100=500 para-
phrase pairs using the YAWAT (Germann, 2008) manual
alignment tool.
presence of common token may serve as useful
clues to guide paraphrase extraction.
For our experiments, we chose to use parallel
monolingual corpora obtained by single language
translation, the most direct resource type for ac-
quiring sub-sentential paraphrase pairs. This al-
lows us to define acceptable references for the
task and resort to the most consensual evaluation
technique for paraphrase acquisition to date. Us-
ing such corpora, we expect to be able to extract
precise paraphrases (see Table 1), which will be
natural candidates for further validation, which
will be addressed in section 5.3.
Figure 1 illustrates a reference alignment ob-
tained on a pair of English sentential paraphrases
and the list of atomic paraphrase pairs that can be
extracted from it, against which acquisition tech-
niques will be evaluated. Note that we do not con-
sider pairs of identical units during evaluation, so
we filter them out from the list of reference para-
phrase pairs.
The example in Figure 1 shows different cases
that point to the inherent complexity of this task,
even for human annotators: it could be argued,
for instance, that a correct atomic paraphrase
pair should be reached ? amounted to rather
than reached ? amounted. Also, aligning in-
dependently 260 ? 0.26 and million ? billion
is assuredly an error, while the pair 260 mil-
lion? 0.26 billion would have been appropriate.
A case of alignment that seems non trivial can be
observed in the provided example (during the en-
tire year ? annual). The abovementioned rea-
sons will explain in part the difficulties in reach-
ing high performance values using such gold stan-
dards.
Reference composite paraphrase pairs (denoted
as R), obtained by joining adjacent atomic para-
phrase pairs from Ratom up to 6 tokens3, will
3We used standard biphrase extraction heuristics (Koehn
718
the
amount
of
foreign
capital
actually
utilized
during
the
entire
year
reached
260
million
us
dollars
.
th
e
an
nu
al
fo
re
ig
n
in
ve
stm
en
t
ac
tu
al
ly
us
ed
am
ou
nt
ed
to us
$
0.
26
bi
lli
on
capital ? investment
utilized ? used
during the entire year ? annual
reached ? amounted
260 ? 0.26
million ? billion
us dollars ? us$
Figure 1: Reference alignments for a pair of English
sentential paraphrases from the annotation corpus of
Cohn et al(2008) (note that possible and sure align-
ments are not distinguished here) and the list of atomic
paraphrase pairs extracted from these alignments.
also be considered when measuring performance.
Evaluated techniques have to output atomic can-
didate paraphrase pairs (denoted as Hatom) from
which composite paraphrase pairs (denoted as
H) are computed. The usual measures of pre-
cision (P ), recall (R) and F-measure (F1) can
then be defined in the following way (Cohn et al
2008):
P =
|Hatom ?R|
|Hatom|
R =
|H ? Ratom|
|Ratom|
F1 =
2pr
p+ r
We conducted experiments using two different
corpora in English and French. In each case,
a held-out development corpus of 150 sentential
paraphrase pairs was used for development and
tuning, and all techniques were evaluated on the
same test set consisting of 375 sentential para-
phrase pairs. For English, we used the MTC
et al 2007) : all words from a phrase must be aligned to at
least one word from the other and not to words outside, but
unaligned words at phrase boundaries are not used.
corpus described in (Cohn et al 2008), consist-
ing of multiply-translated Chinese sentences into
English, and used as our gold standard both the
alignments marked as ?Sure? and ?Possible?. For
French, we used the CESTA corpus of news ar-
ticles4 obtained by translating into French from
English.
We used the YAWAT (Germann, 2008) manual
alignment tool. Inter-annotator agreement val-
ues (averaging with each annotation set as the
gold standard) are 66.1 for English and 64.6 for
French, which we interpret as acceptable val-
ues. Manual inspection of the two corpora reveals
that the French corpus tends to contain more lit-
eral translations, possibly due to the original lan-
guages of the sentences, which are closer to the
target language than Chinese is to English.
4 Individual techniques for paraphrase
acquisition
As discussed in section 2, the acquisition of sub-
sentential paraphrases is a challenging task that
has previously attracted a lot of work. In this
work, we consider the scenario where sentential
paraphrases are available and words and phrases
from one sentence can be aligned to words and
phrases from the other sentence to form atomic
paraphrase pairs. We now describe several tech-
niques that perform the task of sub-sentential unit
alignment. We have selected and implemented
five techniques which we believe are representa-
tive of the type of knowledge that these techniques
use, and have reused existing tools, initially devel-
oped for other tasks, when possible.
4.1 Statistical learning of word alignments
(Giza)
The GIZA++ tool (Och and Ney, 2004) computes
statistical word alignment models of increasing
complexity from parallel corpora. While origi-
nally developed in the bilingual context of Statis-
tical Machine Translation, nothing prevents build-
ing such models on monolingual corpora. How-
ever, in order to build reliable models, it is nec-
essary to use enough training material includ-
ing minimal redundancy of words. To this end,
we provided GIZA++ with all possible sentence
pairs from our mutiply-translated corpus to im-
prove the quality of its word alignments (note that
4http://www.elda.org/article125.html
719
we used symmetrized alignments from the align-
ments in both directions). This constitutes a sig-
nificant advantage for this technique that tech-
niques working on each sentence pair indepen-
dently do not have.
4.2 Translational equivalence (Pivot)
Translational equivalence can be exploited to de-
termine that two phrases may be paraphrases.
Bannard and Callison-Burch (2005) defined a
paraphrasing probability between two phrases
based on their translation probability through all
possible pivot phrases as:
Ppara(p1, p2) =
?
piv
Pt(piv|p1)Pt(p2|piv)
where Pt denotes translation probabilies. We used
the Europarl corpus5 of parliamentary debates in
English and French, consisting of approximately
1.7 million parallel sentences : this allowed us
to use the same resource to build paraphrases for
English, using French as the pivot language, and
for French, using English as the pivot language.
The GIZA++ tool was used for word alignment
and the MOSES Statistical Machine Translation
toolkit (Koehn et al 2007) was used to com-
pute phrase translation probabilities from these
word alignments. For each sentential paraphrase
pair, we applied the following algorithm: for each
phrase, we build the entire set of paraphrases us-
ing the previous definition. We then extract its
best paraphrase as the one exactly appearing in the
other sentence with maximum paraphrase proba-
bility, using a minimal threshold value of 10?4.
4.3 Linguistic knowledge on term variation
(Fastr)
The FASTR tool (Jacquemin, 1999) was designed
to spot term/phrase variants in large corpora.
Variants are described through metarules express-
ing how the morphosyntactic structure of a term
variant can be derived from a given term by means
of regular expressions on word morphosyntactic
categories. Paradigmatic variation can also be ex-
pressed by expressing constraints between words,
imposing that they be of the same morphologi-
cal or semantic family. Both constraints rely on
preexisting repertoires available for English and
French. To compute candidate paraphrase pairs
using FASTR, we first consider all phrases from
5http://statmt.org/europarl
the first sentence and search for variants in the
other sentence, then do the reverse process and
finally take the intersection of the two sets.
4.4 Syntactic similarity (Synt)
The algorithm introduced by Pang et al(2003)
takes two sentences as input and merges them by
top-down syntactic fusion guided by compatible
syntactic substructure. A lexical blocking mecha-
nism prevents constituents from fusionning when
there is evidence of the presence of a word in an-
other constituent of one of the sentence. We use
the Berkeley Probabilistic parser (Klein and Man-
ning, 2003) to obtain syntactic trees for English
and its adapted version for French (Candito et al
2010). Because this process is highly sensitive to
syntactic parse errors, we use in our implemen-
tation k-best parses and retain the most compact
fusion from any pair of candidate parses.
4.5 Edit rate on word sequences (TERp)
TERp (Translation Edit Rate Plus) (Snover et al
2010) is a score designed for the evaluation of
Machine Translation output. Its typical use takes
a system hypothesis to compute an optimal set of
word edits that can transform it into some exist-
ing reference translation. Edit types include ex-
act word matching, word insertion and deletion,
block movement of contiguous words (computed
as an approximation), as well as optionally vari-
ants substitution through stemming, synonym or
paraphrase matching.6 Each edit type is parame-
terized by at least one weight which can be opti-
mized using e.g. hill climbing. TERp being a tun-
able metric, our experiments will include tuning
TERp systems towards either precision (? P ),
recall (? R), or F-measure (? F1).7
4.6 Evaluation of individual techniques
Results for the 5 individual techniques are given
on the left part of Table 2. It is first apparent
that all techniques but TERp fared better on the
French corpus than on the English corpus. This
can certainly be explained by the fact that the for-
mer results from more literal translations (from
6Note that for these experiments we did not use the stem-
ming module, the interface to WordNet for synonym match-
ing and the provided paraphrase table for English, due to the
fact that these resources were available for English only.
7Hill climbing was used for all tunings as done by Snover
et al(2010), and we used one iteration starting with uniform
weights and 100 random restarts.
720
Individual techniques Combinations
GIZA PIVOT FASTR SYNT TERp union validation? P ? R ? F1
English
P 31.01 31.78 37.38 52.17 50.00 29.15 33.37 21.44 50.51
R 38.30 18.50 6.71 2.53 5.83 45.19 45.37 60.87 41.19
F1 34.27 23.39 11.38 4.83 10.44 35.44 38.46 31.71 45.37
French
P 28.99 29.53 52.48 62.50 31.35 30.26 31.43 17.58 40.77
R 45.98 26.66 8.59 8.65 44.22 44.60 44.10 63.36 45.85
F1 35.56 28.02 14.77 15.20 36.69 36.05 36.70 27.53 43.16
Table 2: Results on the test set on English and French for the 5 individual paraphrase acquisition techniques (left
part) and for the 2 combination techniques (right part).
English to French, compared with from Chinese
to English), which should be consequently eas-
ier to word-align. This is for example clearly
shown by the results of the statistical aligner
GIZA, which obtains a 7.68 advantage on recall
for French over English.
The two linguistically-aware techniques,
FASTR and SYNT, have a very strong precision
on the more parallel French corpus, but fail to
achieve an acceptable recall on their own. This
is not surprising : FASTR metarules are focussed
on term variant extraction, and SYNT requires
two syntactic trees to be highly comparable
to extract sub-sentential paraphrases. When
these constrained conditions are met, these two
techniques appear to perform quite well in terms
of precision.
GIZA and TERp perform roughly in the same
range on French, with acceptable precision and
recall, TERp performing overall better, with e.g.
a 1.14 advantage on F-measure on French and
4.19 on English. The fact that TERp performs
comparatively better on English than on French8,
with a 1.76 advantage on F-measure, is not con-
tradictory: the implemented edit distance makes
it possible to align reasonably distant words and
phrases independently from syntax, and to find
alignments for close remaining words, so the dif-
ferences of performance between the two lan-
guages are not necessarily expected to be com-
parable with the results of a statistical alignment
technique. English being a poorly-inflected lan-
guage, alignment clues between two sentential
paraphrases are expected to be more numerous
8Recall that all specific linguistic modules for English
only from TERp had been disabled, so the better perfor-
mance on English cannot be explained by a difference in
terms of resources used.
than for highly-inflected French.
PIVOT is on par with GIZA as regards preci-
sion, but obtains a comparatively much lower re-
call (differences of 19.32 and 19.80 on recall on
French and English respectively). This may first
be due in part to the paraphrasing score threshold
used for PIVOT, but most certainly to the use of
a bilingual corpus from the domain of parliamen-
tary debates to extract paraphrases when our test
sets are from the news domain: we may be ob-
serving differences inherent to the domain, and
possibly facing the issue of numerous ?out-of-
vocabulary? phrases, in particular for named en-
tities which frequently occur in the news domain.
Importantly, we can note that we obtain at best
a recall of 45.98 on French (GIZA) and of 45.37
on English (TERp). This may come as a disap-
pointment but, given the broad set of techniques
evaluated, this should rather underline the inher-
ent complexity of the task. Also, recall that the
metrics used do not consider identity paraphrases
(e.g. at the same time ? at the same time), as
well as the fact that gold standard alignment is
a very difficult process as shown by interjudge
agreement values and our example from section 3.
This, again, confirms that the task that is ad-
dressed is indeed a difficult one, and provides fur-
ther justification for initially focussing on parallel
monolingual corpora, albeit scarce, for conduct-
ing fine-grained studies on sub-sentential para-
phrasing.
Lastly, we can also note that precision is not
very high, with (at best, using TERp?P ) average
values for all techniques of 40.97 and 40.46 on
French and English, respectively. Several facts
may provide explanations for this observation.
First, it should be noted that none of those tech-
niques, except SYNT, was originally developed
721
for the task of sub-sentential paraphrase acqui-
sition from monolingual parallel corpora. This
results in definitions that are at best closely re-
lated to this task.9 Designing new techniques
was not one of the objectives of our study, so we
have reused existing techniques, originally devel-
oped with different aims (bilingual parallel cor-
pora word alignment (GIZA), term variant recog-
nition (FASTR), Machine Translation evaluation
(TERp)). Also, techniques such as GIZA and
TERp attempt to align as many words as possi-
ble in a sentence pair, when gold standard align-
ments sometimes contain gaps.10 Finally, the met-
rics used will count as false small variations of
gold standard paraphrases (e.g. missing function
word): the acceptability or not of such candi-
dates could be either evaluated in a scenario where
such ?acceptable? variants would be taken into
account, and could be considered in the context
of some actual use of the acquired paraphrases
in some application. Nonetheless, on average the
techniques in our study produce more candidates
that are not in the gold standard: this will be an
important fact to keep in mind when tackling the
task of combining their outputs. In particular, we
will investigate the use of features indicating the
combination of techniques that predicted a given
paraphrase pair, aiming to capture consensus in-
formation.
5 Paraphrase validation
5.1 Technique complementarity
Before considering combining and validating the
outputs of individual techniques, it is informative
to look at some notion of ?complementarity? be-
tween techniques, in terms of how many correct
paraphrases a technique would add to a combined
set. The following formula was used to account
for the complementarity between the set of can-
didates from some technique i, ti, and the set for
some technique j, tj :
C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj))
9Recall, however, that our best performing technique on
F-measure, TERp, was optimized to our task using a held
out development set.
10It is arguable whether such cases should happen in sen-
tence pairs obtained by translating the same original sentence
into the same language, but this clearly depends on the inter-
pretation of the expected level of annotation by the annota-
tors.
Results on the test set for the two languages
are given in Table 3. A number of pairs of tech-
niques have strong complementarity values, the
strongest one being for GIZA and TERp for both
languages. According to these figures, PIVOT
identify paraphrases which are slightly more sim-
ilar to those of TERp than those of GIZA. Inter-
estingly, FASTR and SYNT exhibit a strong com-
plementarity, where in French, for instance, they
only have a very small proportion of paraphrases
in common. Considering the set of all other tech-
niques, GIZA provides the more new paraphrases
on French and TERp on English.
GIZA PIVOT FASTR SYNT TERp?R all others
English
GIZA - 4.65 2.83 0.59 10.31 8.31
PIVOT 4.65 - 2.30 1.88 3.12 3.72
FASTR 2.83 2.30 - 2.42 1.71 0.53
SYNT 0.59 1.88 2.42 - 0.59 0.00
TERp?R 10.31 3.12 1.71 0.59 - 12.20
French
GIZA - 9.79 3.64 2.20 10.73 8.91
PIVOT 9.79 - 2.26 5.22 7.84 3.39
FASTR 3.64 2.26 - 7.28 3.01 0.19
SYNT 2.20 5.22 7.28 - 1.76 0.44
TERp?R 10.73 7.84 3.01 1.76 - 5.65
Table 3: Values of complementarity on the test set for
both languages, where the following formula was used
for the set of technique outputs T = {t1, t2, ..., tn} :
C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj)).
Complementarity values are computed between all
pairs of individual techniques, and each individual
technique and the set of all other techniques. Values in
bold indicate highest values for the technique of each
row.
5.2 Naive combination by union
We first implemented a naive combination ob-
tained by taking the union of all techniques. Re-
sults are given in the first column of the right part
of Table 2. The first result is quite encouraging:
in both languages, more than 6 paraphrases from
the gold standard out of 10 are found by at least
one of the techniques, which, given our previous
discussion, constitutes a good result and provide
a clear justification for combining different tech-
niques for improving performance on this task.
Precision is mechanically lowered to account for
roughly 1 correct paraphrase over 5 candidates
for both languages. F-measure values are much
lower than those of TERp and GIZA, showing
that the union of all techniques is only interest-
ing for recall-oriented paraphrase acquisition. In
722
the next section, we will show how the results of
the union can be validated using machine learning
to improve these figures.
5.3 Paraphrase validation via automatic
classification
A natural improvement to the naive combination
of paraphrase candidates from all techniques can
consist in validating candidate paraphrases by us-
ing several models that may be good indicators of
their paraphrasing status. We can therefore cast
our problem as one of biclass classification (i.e.
?paraphrase? vs. ?not paraphrase?).
We have used a maximum entropy classifier11
with the following features, aiming at capturing
information on the paraphrase status of a candi-
date pair:
Morphosyntactic equivalence (POS) It may
be the case that some sequences of part-of-speech
can be rewritten as different sequences, e.g. as
a result of verb nominalization. We therefore
use features to indicate the sequences of part-of-
speech for a pair of candidate paraphrases. We
used the preterminal symbols of the syntactic
trees of the parser used for SYNT.
Character-based distance (CAR) Morpholog-
ical variants often have close word forms, and
more generally close word forms in sentential
paraphase pairs may indicate related words. We
used features for discretized values of the edit
distance between the two phrases of a candidate
paraphrase pair as measured by the Levenshtein
distance.
Stem similarity (STEM) Inflectional morphol-
ogy, which is quite productive in languages such
as French, can increase vocabulary size signifi-
cantly, while in sentential paraphrases common
stems may indicate related words. We used a
binary feature indicating whether the stemmed
phrases of a candidate paraphrase pair match.12
Token set identity (BOW) Syntactic rearrange-
ments may involve the same sets of words in var-
ious orders. We used discretized features indicat-
ing the proportion of common tokens in the set
11We used the implementation available at:
http://homepages.inf.ed.ac.uk/lzhang10/
maxent_toolkit.html
12We use the implementations of the Snowball stem-
mer from English and French available from: http://
snowball.tartarus.org
of tokens for the two phrases of a candidate para-
phrase pair.
Context similarity (CTXT) It can be derived
from the distributionality hypothesis that the more
two phrases will be seen in similar contexts, the
more they are likely to be paraphrases. We used
discretized features indicating how similar the
contexts of occurrences of two paraphrases are.
For this, we used the full set of bilingual English-
French data available for the translation task of
the Workshop on Statistical Machine Transla-
tion13, totalling roughly 30 million parallel sen-
tences: this again ensures that the same resources
are used for experiments in the two languages. We
collect all occurrences for the phrases in a pair,
and build a vector of content words cooccurring
within a distance of 10 words from each phrase.
We finally compute the cosine between the vec-
tors of the two phrases of a candidate paraphrase
pair.
Relative position in a sentence (REL) De-
pending on the language in which parallel sen-
tences are analyzed, it may be the case that sub-
sentential paraphrases occur at close locations in
their respective sentence. We used a discretized
feature indicating the relative position of the two
phrases in their original sentence.
Identity check (COOC) We used a binary fea-
ture indicating whether one of the two phrases
from a candidate pair, or the two, occurred at
some other location in the other sentence.
Phrase length ratio (LEN) We used a dis-
cretized feature indicating phrase length ratio.
Source techniques (SRC) Finally, as our set-
ting validates paraphrase candidates produced by
a set of techniques, we used features indicat-
ing which combination of techniques predicted a
paraphrase candidate. This can allow learning that
paraphrases in the intersection of the predicted
sets for some techniques may produce good re-
sults.
We used a held out training set consisting of
150 sentential paraphrase pairs from the same cor-
pora as our previous developement and test sets
for both languages. Positive examples were taken
from the candidate paraphrase pairs from any of
13http://www.statmt.org/wmt11/
translation-task.html
723
the 5 techniques in our study which belong to
the gold standard, and we used a corresponding
number of negative examples (randomly selected)
from candidate pairs not in the gold standard. The
right part of Table 2 provides the results for our
validation experiments of the union set for all pre-
vious techniques.
We obtain our best results for this study using
the output of our validation classifier over the set
of all candidate paraphrase pairs. On French, it
yields an improvement in F-measure (43.16) of
+6.46 over the best individual technique (TERp)
and of +15.63 over the naive union from all indi-
vidual techniques. On English, the improvement
in F-measure (45.37) is for the same conditions of
respectively +6.91 (over TERp) and +13.66. We
unfortunately observe an important decrease in re-
call over the naive union, of respectively -17.54
and -19.68 for French and English. Increasing our
amount of training data to better represent the full
range of paraphrase types may certainly overcome
this in part. This would indeed be sensible, as bet-
ter covering the variety of paraphrase types as a
one-time effort would help all subsequent valida-
tions. Figure 2 shows how performance varies on
French with number of training examples for var-
ious feature configurations. However, some para-
phrase types will require integration of more com-
plex knowledge, as is the case, for instance, for
paraphrase pairs involving some anaphora and its
antecedent (e.g. China? it).
While these results, which are very comparable
for the two languages studied, are already satisfy-
ing given the complexity of our task, further in-
spection of false positives and negatives may help
us to develop additional models that will help us
obtain a better classification performance.
6 Conclusions and future work
In this article, we have addressed the task of com-
bining the results of sub-sentential paraphrase ac-
quition from parallel monolingual corpora using a
large variety of techniques. We have provided jus-
tifications for using highly parallel corpora con-
sisting of multiply translated sentences from a
single language. All our experiments were con-
ducted on both English and French using com-
parable resources, so although the results cannot
be directly compared they give some acceptable
comparison points. The best recall of any indi-
vidual technique is around 45 for both language,
10 20 30 40 50 60 70 80 90 10031
33
35
37
39
41
43
All\POS\SRC\CTXT\STEM\LEN\COOC
F-mea
sure
% of examples from training corpus
Figure 2: Learning curves obtained on French by re-
moving features individually.
and F-measure in the range 36-38, indicating that
the task under study is a very challenging one.
Our validation strategy based on bi-class classi-
fication using a broad set of features applicable to
all candidate paraphrase pairs allowed us to obtain
a 18% relative improvement in F-measure over
the best individual technique for both languages.
Our future work include performing a deeper
error analysis of our current results, to better com-
prehend what characteristics of paraphrase still
defy current validation. Also, we want to inves-
tigate adding new individual techniques to pro-
vide so far unseen candidates. Another possible
approach would be to submit all pairs of sub-
sentential paraphrase pairs from a sentence pair
to our validation process, which would obviously
require some optimization and devising sensible
heuristics to limit time complexity. We also in-
tend to collect larger corpora for all other corpus
types appearing in Table 1 and conducting anew
our acquisition and validation tasks.
Acknowledgements
The authors would like to thank the reviewers for
their comments and suggestions, as well as Guil-
laume Wisniewski for helpful discussions. This
work was partly funded by ANR project Edylex
(ANR-09-CORD-008).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
724
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, Ann Arbor, USA.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of NAACL-HLT, Edmonton, Canada.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL, Toulouse, France.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Rahul Bhagat. 2009. Learning Paraphrases from Text.
Ph.D. thesis, University of Southern California.
Houda Bouamor, Aure?lien Max, and Anne Vilnat.
2010. Comparison of Paraphrase Acquisition Tech-
niques on Sentential Paraphrases. In Proceedings of
IceTAL, Rejkavik, Iceland.
Chris Callison-Burch, Trevor Cohn, and Mirella La-
pata. 2008. Parametric: An automatic evaluation
metric for paraphrasing. In Proceedings of COL-
ING, Manchester, UK.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP, Hawai, USA.
Marie Candito, Beno??t Crabbe?, and Pascal Denis.
2010. Statistical French dependency parsing: tree-
bank conversion and first results. In Proceedings of
LREC, Valletta, Malta.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of ACL, Portland, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4).
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of COLING, Geneva, Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-HLT,
demo session, Columbus, USA.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, College Park, USA.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
Sapporo, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Proceedings of ACL, demo session,
Prague, Czech Republic.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. PEM: A paraphrase evaluation metric ex-
ploiting parallel texts. In Proceedings of EMNLP,
Cambridge, USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generat-
ing Phrasal and Sentential Paraphrases: A Survey
of Data-Driven Methods . Computational Linguis-
tics, 36(3).
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, University of Maryland College Park.
Donald Metzler, Eduard Hovy, and Chunliang Zhang.
2011. An empirical evaluation of data-driven para-
phrase generation techniques. In Proceedings of
ACL-HLT, Portland, USA.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of NAACL-HLT, Edmonton,
Canada.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase,
semantic, and alignment enhancements to Transla-
tion Edit Rate. Machine Translation, 23(2-3).
Jo?rg Tiedemann. 2007. Building a Multilingual Paral-
lel Subtitle Corpus. In Proceedings of the Confer-
ence on Computational Linguistics in the Nether-
lands, Leuven, Belgium.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings
of ACL-HLT, Columbus, USA.
725
Proceedings of NAACL-HLT 2013, pages 439?444,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Dudley North visits North London:
Learning When to Transliterate to Arabic
Mahmoud Azab Houda Bouamor
Carnegie Mellon University
P.O. Box 24866, Doha, Qatar
{mazab, hbouamor, behrang, ko}@qatar.cmu.edu
Behrang Mohit Kemal Oflazer
Abstract
We report the results of our work on automat-
ing the transliteration decision of named en-
tities for English to Arabic machine trans-
lation. We construct a classification-based
framework to automate this decision, evalu-
ate our classifier both in the limited news and
the diverse Wikipedia domains, and achieve
promising accuracy. Moreover, we demon-
strate a reduction of translation error and
an improvement in the performance of an
English-to-Arabic machine translation sys-
tem.
1 Introduction
Translation of named entities (NEs) is important
for NLP applications such as Machine Translation
(MT) and Cross-lingual Information Retrieval. For
MT, NEs are major subset of the out-of-vocabulary
terms (OOVs). Due to their diversity, they cannot
always be found in parallel corpora, dictionaries or
gazetteers. Thus, state-of-the-art of MT needs to
handle NEs in specific ways. For instance, in the
English-Arabic automatic translation example given
in Figure 1, the noun ?North? has been erroneously
translated to ? ?J
?A?
??@ /Al$mAlyp ? (indicating the
north direction in English) instead of being translit-
erated to ? HP?	K / nwrv?.
As shown in Figure 1, direct translation of in-
vocabulary terms could degrade translation quality.
Also blind transliteration of OOVs does not neces-
sarily contribute to translation adequacy and may ac-
tually create noisy contexts for the language model
and the decoder.
English Input: Dudley North was an English merchant.
SMT output: . ?K

	Q
?m.Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 395?400,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Monolingual Alignment by Edit Rate Computation
on Sentential Paraphrase Pairs
Houda Bouamor Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
Orsay, France
{firstname.lastname}@limsi.fr
Anne Vilnat
Abstract
In this paper, we present a novel way of tack-
ling the monolingual alignment problem on
pairs of sentential paraphrases by means of
edit rate computation. In order to inform the
edit rate, information in the form of subsenten-
tial paraphrases is provided by a range of tech-
niques built for different purposes. We show
that the tunable TER-PLUS metric from Ma-
chine Translation evaluation can achieve good
performance on this task and that it can effec-
tively exploit information coming from com-
plementary sources.
1 Introduction
The acquisition of subsentential paraphrases has at-
tracted a lot of attention recently (Madnani and Dorr,
2010). Techniques are usually developed for extract-
ing paraphrase candidates from specific types of cor-
pora, including monolingual parallel corpora (Barzi-
lay and McKeown, 2001), monolingual comparable
corpora (Dele?ger and Zweigenbaum, 2009), bilin-
gual parallel corpora (Bannard and Callison-Burch,
2005), and edit histories of multi-authored text (Max
and Wisniewski, 2010). These approaches face two
main issues, which correspond to the typical mea-
sures of precision, or how appropriate the extracted
paraphrases are, and of recall, or how many of the
paraphrases present in a given corpus can be found
effectively. To start with, both measures are often
hard to compute in practice, as 1) the definition of
what makes an acceptable paraphrase pair is still
a research question, and 2) it is often impractical
to extract a complete set of acceptable paraphrases
from most resources. Second, as regards the pre-
cision of paraphrase acquisition techniques in par-
ticular, it is notable that most works on paraphrase
acquisition are not based on direct observation of
larger paraphrase pairs. Even monolingual corpora
obtained by pairing very closely related texts such as
news headlines on the same topic and from the same
time frame (Dolan et al, 2004) often contain unre-
lated segments that should not be aligned to form a
subsentential paraphrase pair. Using bilingual cor-
pora to acquire paraphrases indirectly by pivoting
through other languages is faced, in particular, with
the issue of phrase polysemy, both in the source and
in the pivot languages.
It has previously been noted that highly parallel
monolingual corpora, typically obtained via mul-
tiple translation into the same language, consti-
tute the most appropriate type of corpus for ex-
tracting high quality paraphrases, in spite of their
rareness (Barzilay and McKeown, 2001; Cohn et
al., 2008; Bouamor et al, 2010). We build on this
claim here to propose an original approach for the
task of subsentential alignment based on the compu-
tation of a minimum edit rate between two sentential
paraphrases. More precisely, we concentrate on the
alignment of atomic paraphrase pairs (Cohn et al,
2008), where the words from both paraphrases are
aligned as a whole to the words of the other para-
phrase, as opposed to composite paraphrase pairs
obtained by joining together adjacent paraphrase
pairs or possibly adding unaligned words. Figure 1
provides examples of atomic paraphrase pairs de-
rived from a word alignment between two English
sentential paraphrases.
395
China
will
continue continue?carry on
implementing
the
financial financial opening
up?open financialopening
up
policy
Ch
ina
wi
ll
ca
rry
on op
en
fin
an
cia
l
po
lic
y
Figure 1: Reference alignments for a pair of English
sentential paraphrases and their associated list of atomic
paraphrase pairs extracted from them. Note that identity
pairs (e.g. China ? China) will never be considered in
this work and will not be taken into account for evalua-
tion.
The remainder of this paper is organized as fol-
lows. We first briefly describe in section 2 how we
apply edit rate computation to the task of atomic
paraphrase alignment, and we explain in section 3
how we can inform such a technique with paraphrase
candidates extracted by additional techniques. We
present our experiments and discuss their results in
section 4 and conclude in section 5.
2 Edit rate for paraphrase alignment
TER-PLUS (Translation Edit Rate Plus) (Snover et
al., 2010) is a score designed for evaluation of Ma-
chine Translation (MT) output. Its typical use takes
a system hypothesis to compute an optimal set of
word edits that can transform it into some existing
reference translation. Edit types include exact word
matching, word insertion and deletion, block move-
ment of contiguous words (computed as an approx-
imation), as well as variants substitution through
stemming, synonym or paraphrase matching. Each
edit type is parameterized by at least one weight
which can be optimized using e.g. hill climbing.
TER-PLUS is therefore a tunable metric. We will
henceforth design as TERMT the TER metric (basi-
cally, without variants matching) optimized for cor-
relation with human judgment of accuracy in MT
evaluation, which is to date one of the most used
metrics for this task.
While this metric was not designed explicitely for
the acquisition of word alignments, it produces as a
by-product of its approximate search a list of align-
ments involving either individual words or phrases,
potentially fitting with the previous definition of
atomic paraphrase pairs. When applying it on a
MT system hypothesis and a reference translation,
it computes how much effort would be needed to
obtain the reference from the hypothesis, possibly
independently of the appropriateness of the align-
ments produced. However, if we consider instead
a pair of sentential paraphrases, it can be used to
reveal what subsentential units can be aligned. Of
course, this relies on information that will often go
beyond simple exact word matching. This is where
the capability of exploiting paraphrase matching can
come into play: TER-PLUS can exploit a table of
paraphrase pairs, and defines the cost of a phrase
substitution as ?a function of the probability of the
paraphrase and the number of edits needed to align
the two phrases without the use of phrase substitu-
tions?. Intuitively, the more parallel two sentential
paraphrases are, the more atomic paraphrase pairs
will be reliably found, and the easier it will be for
TER-PLUS to correctly identify the remaining pairs.
But in the general case, and considering less appar-
ently parallel sentence pairs, its work can be facil-
itated by the incorporation of candidate paraphrase
pairs in its paraphrase table. We consider this possi-
ble type of hybridation in the next section.
3 Informing edit rate computation with
other techniques
In this article, we use three baseline techniques
for paraphrase pair acquisition, which we will only
briefly introduce (see (Bouamor et al, 2010) for
more details). As explained previously, we want to
evaluate whether and how their candidate paraphrase
pairs can be used to improve paraphrase acquisition
on sentential paraphrases using TER-PLUS. We se-
lected these three techniques for the complementar-
ity of types of information that they use: statistical
word alignment without a priori linguistic knowl-
edge, symbolic expression of linguistic variation ex-
ploiting a priori linguistic knowledge, and syntactic
similarity.
396
Statistical Word Alignment The GIZA++
tool (Och and Ney, 2004) computes statistical word
alignment models of increasing complexity from
parallel corpora. While originally developped in the
bilingual context of Machine Translation, nothing
prevents building such models on monolingual
corpora. However, in order to build reliable models
it is necessary to use enough training material
including minimal redundancy of words. To this
end, we will be using monolingual corpora made
up of multiply-translated sentences, allowing us to
provide GIZA++ with all possible sentence pairs
to improve the quality of its word alignments (note
that following common practice we used symetrized
alignments from the alignments in both directions).
This constitutes an advantage for this technique that
the following techniques working on each sentence
pair independently do not have.
Symbolic expression of linguistic variation The
FASTR tool (Jacquemin, 1999) was designed to spot
term variants in large corpora. Variants are de-
scribed through metarules expressing how the mor-
phosyntactic structure of a term variant can be de-
rived from a given term by means of regular ex-
pressions on word categories. Paradigmatic varia-
tion can also be expressed by defining constraints
between words to force them to belong to the same
morphological or semantic family, both constraints
relying on preexisting repertoires available for En-
glish and French. To compute candidate paraphrase
pairs using FASTR, we first consider all the phrases
from the first sentence and search for variants in the
other sentence, do the reverse process and take the
intersection of the two sets.
Syntactic similarity The algorithm introduced
by Pang et al (2003) takes two sentences as in-
put and merges them by top-down syntactic fusion
guided by compatible syntactic substructure. A
lexical blocking mechanism prevents sentence con-
stituents from fusionning when there is evidence of
the presence of a word in another constituent of one
of the sentence. We use the Berkeley Probabilistic
parser (Petrov and Klein, 2007) to obtain syntac-
tic trees for English and its Bonsai adaptation for
French (Candito et al, 2010). Because this process
is highly sensitive to syntactic parse errors, we use
k-best parses (with k = 3 in our experiments) and
retain the most compact fusion from any pair of can-
didate parses.
4 Experiments and discussion
We used the methodology described by Cohn et al
(2008) for constructing evaluation corpora and as-
sessing the performance of various techniques on the
task of paraphrase acquisition. In a nutshell, pairs of
sentential paraphrases are hand-aligned and define a
set of reference atomic paraphrase pairs at the level
of words or blocks or words, denoted as Ratom, and
also a set of reference composite paraphrase pairs
obtained by joining adjacent atomic paraphrase pairs
(up to a given length), denoted as R. Techniques
output word alignments from which atomic candi-
date paraphrase pairs, denoted as Hatom, as well as
composite paraphrase pairs, denoted as H, can be
extracted. The usual measures of precision, recall
and f-measure can then be defined in the following
way:
p =
|Hatom ?R|
|Hatom|
r =
|H ? Ratom|
|Ratom|
f1 =
2pr
p + r
To evaluate our individual techniques and their
use by the tunable TER-PLUS technique (hence-
forth TERP), we measured results on two different
corpora in French and English. In each case, a held-
out development corpus of 150 paraphrase pairs was
used for tuning the TERP hybrid systems towards
precision (? p), recall (? r), or F-measure (?
f1).1 All techniques were evaluated on the same test
set consisting of 375 paraphrase pairs. For English,
we used the MTC corpus described in (Cohn et al,
2008), which consists of multiply-translated Chi-
nese sentences into English, with an average lexical
overlap2 of 65.91% (all tokens) and 63.95% (content
words only). We used as our reference set both the
alignments marked as ?Sure? and ?Possible?. For
French, we used the CESTA corpus of news articles3
obtained by translating into French from various lan-
guages with an average lexical overlap of 79.63%
(all tokens) and 78.19% (content words only). These
1Hill climbing was used for tuning as in (Snover et al,
2010), with uniform weights and 100 random restarts.
2We compute the percentage of lexical overlap be-
tween the vocabularies of two sentences S1 and S2 as :
|S1 ? S2|/min(|S1|, |S2|)
3http://www.elda.org/article125.html
397
Individual techniques Hybrid systems (TERPpara+X)
Giza++ Fastr Pang TMT TERPpara +G +F +P +G + F + P
G F P ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1
French French
p 28.99 52.48 62.50 25.66 31.35 30.26 31.43 41.99 30.55 41.14 36.74 29.65 34.84 54.49 20.94 33.89 42.27 27.06 42.80
r 45.98 8.59 8.65 41.15 44.22 44.60 44.10 35.88 45.67 35.25 40.96 43.85 44.41 13.61 40.40 40.46 31.36 44.10 31.61
f1 35.56 14.77 15.20 25.66 36.69 36.05 36.70 38.70 36.61 37.97 38.74 35.38 39.05 21.78 27.58 36.88 36.01 33.54 36.37
English English
p 18.28 33.02 36.66 20.41 31.19 19.14 19.35 26.89 19.85 21.25 41.57 20.81 22.51 31.32 18.02 18.92 29.45 16.81 29.42
r 14.63 5.41 2.23 17.37 2.31 19.38 19.69 11.92 18.47 17.10 6.94 21.02 20.28 3.41 18.94 16.44 13.57 19.30 16.35
f1 16.25 9.30 4.21 18.77 4.31 19.26 19.52 16.52 19.14 18.95 11.91 20.92 21.33 6.15 18.47 17.59 18.58 17.96 21.02
Figure 2: Results on the test set on French and English for the individual techniques and TERP hybrid systems.
Column headers of the form ?? c? indicate that TERP was tuned on criterion c.
figures reveal that the French corpus tends to contain
more literal translations, possibly due to the original
languages of the sentences, which are closer to the
target language than Chinese is to English. We used
the YAWAT (Germann, 2008) interactive alignment
tool and measure inter-annotator agreement over a
subset and found it to be similar to the value reported
by Cohn et al (2008) for English.
Results for all individual techniques in the two
languages are given on Figure 2. We first note that
all techniques fared better on the French corpus than
on the English corpus. This can certainly be ex-
plained by the fact that the former results from more
literal translations, which are consequently easier to
word-align.
TERMT (i.e. TER tuned for Machine Transla-
tion evaluation) performs significantly worse on all
metrics for both languages than our tuned TERP ex-
periments, revealing that the two tasks have differ-
ent objectives. The two linguistically-aware tech-
niques, FASTR and PANG, have a very strong pre-
cision on the more parallel French corpus, and also
on the English corpus to a lesser extent, but fail to
achieve a high recall (note, in particular, that they
do not attempt to report preferentially atomic para-
phrase pairs). GIZA++ and TERPpara perform in
the same range, with acceptable precision and re-
call, TERPpara performing overall better, with e.g. a
1.14 advantage on f-measure on French and 3.27 on
English. Recall that TERP works independently on
each paraphrase pair, while GIZA++ makes use of
artificial repetitions of paraphrases of the same sen-
tence.
Figure 3 gives an indication of how well each
technique performs depending on the difficulty of
the task, which we estimate here as the value
(1? TER(para1, para2)), whose low values cor-
respond to sentences which are costly to trans-
form into the other using TER. Not surprisingly,
TERPpara and GIZA++, and PANG to a lesser ex-
tent, perform better on ?more parallel? sentential
paraphrase pairs. Conversely, FASTR is not affected
by the degree of parallelism between sentences, and
manages to extract synonyms and more generally
term variants, at any level of difficulty.
We have further tested 4 hybrid configurations
by providing TERPpara with the output of the other
individual techniques and of their union, the latter
simply obtained by taking paraphrase pairs output
by at least one of these techniques. On French,
where individual techniques achieve good perfor-
mance, any hybridation improves the F-measure
over both TERPpara and the technique used, the best
performance, using FASTR, corresponding to an im-
provement of respectively +2.35 and +24.28 over
TERPpara and FASTR. Taking the union of all tech-
niques does not yield additional gains: this might
be explained by the fact that incorrect predictions
are proportionnally more present and consequently
have a greater impact when combining techniques
without weighting them, possibly at the level of each
398
  <0.1 <0.2 <0.3 <0.4 <0.5 <0.6 <0.7 <0.8 <0.90
1020
3040
5060
7080
90100 TERpParaF1Giza++FastrPang
Difficulty (1-TER)
F-measu
re
  <0.1 <0.2 <0.3 <0.4 <0.5 <0.6 <0.7 <0.8 <0.90
1020
3040
5060
7080
90100 TERpParaF1Giza++FastrPang
Difficulty (1-TER)
F-measu
re
(a) French (b) English
Figure 3: F-measure values for our 4 individual techniques on French and English depending on the complexity of
paraphrase pairs measured with the (1-TER) formula. Note that each value corresponds to the average of F-measure
values for test examples falling in a given difficulty range, and that all ranges do not necessarily contain the same
number of examples.
prediction.4 Successful hybridation on English seem
harder to obtain, which may be partly attributed to
the poor quality of the individual techniques relative
to TERPpara. We however note anew an improve-
ment over TERPpara of +1.81 when using FASTR.
This confirms that some types of linguistic equiva-
lences cannot be captured using edit rate computa-
tion alone, even on this type of corpus.
5 Conclusion and future work
In this article, we have described the use of edit rate
computation for paraphrase alignment at the sub-
sentential level from sentential paraphrases and the
possibility of informing this search with paraphrase
candidates coming from other techniques. Our ex-
periments have shown that in some circumstances
some techniques have a good complementarity and
manage to improve results significantly. We are
currently studying hard-to-align subsentential para-
phrases from the type of corpora we used in order to
get a better understanding of the types of knowledge
required to improve automatic acquisition of these
units.
4Indeed, measuring the precision on the union yields a poor
performance of 23.96, but with the highest achievable value of
50.56 for recall. Similarly, the maximum value for precision
with a good recall can be obtained by taking the intersection of
the results of TERPpara and GIZA++, which yields a value of
60.39.
Our future work also includes the acquisition of
paraphrase patterns (e.g. (Zhao et al, 2008)) to gen-
eralize the acquired equivalence units to more con-
texts, which could be both used in applications and
to attempt improving further paraphrase acquisition
techniques. Integrating the use of patterns within an
edit rate computation technique will however raise
new difficulties.
We are finally also in the process of conducting
a careful study of the characteristics of the para-
phrase pairs that each technique can extract with
high confidence, so that we can improve our hybri-
dation experiments by considering confidence val-
ues at the paraphrase level using Machine Learning.
This way, we may be able to use an edit rate com-
putation algorithm such as TER-PLUS as a more
efficient system combiner for paraphrase extraction
methods than what was proposed here. A poten-
tial application of this would be an alternative pro-
posal to the paraphrase evaluation metric PARAMET-
RIC (Callison-Burch et al, 2008), where individual
techniques, outputing word alignments or not, could
be evaluated from the ability of the informated edit
rate technique to use correct equivalence units.
Acknowledgments
This work was partly funded by a grant from LIMSI.
The authors wish to thank the anonymous reviewers
for their useful comments and suggestions.
399
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL, Toulouse, France.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2010.
Comparison of Paraphrase Acquisition Techniques on
Sentential Paraphrases. In Proceedings of IceTAL, Re-
jkavik, Iceland.
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
2008. Parametric: An automatic evaluation metric for
paraphrasing. In Proceedings of COLING, Manch-
ester, UK.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical French dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4).
Louise Dele?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of Coling 2004, pages 350?356, Geneva,
Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-08: HLT
Demo Session, Columbus, USA.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, pages 341?348, College Park, USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
Driven Methods . Computational Linguistics, 36(3).
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia?s Revision History. In Proceedings of
LREC, Valletta, Malta.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase Pat-
terns from Bilingual Corpora. In Proceedings of ACL-
HLT, Columbus, USA.
400
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 181?185,
Dublin, Ireland, August 23-24, 2014.
CMUQ-Hybrid: Sentiment Classification
By Feature Engineering and Parameter Tuning
Kamla Al-Mannai
1
, Hanan Alshikhabobakr
2
,
Sabih Bin Wasi
2
, Rukhsar Neyaz
2
, Houda Bouamor
2
, Behrang Mohit
2
Texas A&M University in Qatar
1
, Carnegie Mellon University in Qatar
2
almannaika@hotmail.com
1
{halshikh, sabih, rukhsar, hbouamor, behrang}@cmu.edu
Abstract
This paper describes the system we sub-
mitted to the SemEval-2014 shared task
on sentiment analysis in Twitter. Our sys-
tem is a hybrid combination of two system
developed for a course project at CMU-
Qatar. We use an SVM classifier and cou-
ple a set of features from one system with
feature and parameter optimization frame-
work from the second system. Most of the
tuning and feature selection efforts were
originally aimed at task-A of the shared
task. We achieve an F-score of 84.4% for
task-A and 62.71% for task-B and the sys-
tems are ranked 3rd and 29th respectively.
1 Introduction
With the proliferation of Web2.0, people increas-
ingly express and share their opinion through so-
cial media. For instance, microblogging websites
such as Twitter
1
are becoming a very popular com-
munication tool. An analysis of this platform re-
veals a large amount of community messages ex-
pressing their opinions and sentiments on differ-
ent topics and aspects of life. This makes Twit-
ter a valuable source of subjective and opinionated
text that could be used in several NLP research
works on sentiment analysis. Many approaches
for detecting subjectivity and determining polarity
of opinions in Twitter have been proposed (Pang
and Lee, 2008; Davidov et al., 2010; Pak and
Paroubek, 2010; Tang et al., 2014). For instance,
the Twitter sentiment analysis shared task (Nakov
et al., 2013) is an interesting testbed to develop
and evaluate sentiment analysis systems on social
media text. Participants are asked to implement
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
a system capable of determining whether a given
tweet expresses positive, negative or neutral sen-
timent. In this paper, we describe the CMUQ-
Hybrid system we developed to participate in the
two subtasks of SemEval 2014 Task 9 (Rosenthal
et al., 2014). Our system uses an SVM classifier
with a rich set of features and a parameter opti-
mization framework.
2 Data Preprocessing
Working with tweets presents several challenges
for NLP, different from those encountered when
dealing with more traditional texts, such as
newswire data. Tweet messages usually contain
different kinds of orthographic and typographical
errors such as the use of special and decorative
characters, letter duplication used generally for
emphasis, word duplication, creative spelling and
punctuation, URLs, #hashtags as well as the use
of slangs and special abbreviations. Hence, before
building our classifier, we start with a preprocess-
ing step on the data, in order to normalize it. All
letters are converted to lower case and all words
are reduced to their root form using the WordNet
Lemmatizer in NLTK
2
(Bird et al., 2009). We kept
only some punctuation marks: periods, commas,
semi-colons, and question and exclamation marks.
The excluded characters were identified to be per-
formance boosters using the best-first branch and
bound technique described in Section 3.
3 Feature Extraction
Out of a wide variety of features, we selected the
most effective features using the best-first branch
and bound method (Neapolitan, 2014), a search
tree technique for solving optimization problems.
We used this technique to determine which punc-
tuation marks to keep in the preprocessing step and
2
http://www.nltk.org/api/nltk.stem.
html
181
in selecting features as well. In the feature selec-
tion step, the root node is represented by a bag of
words feature, referred as textual tokens.
At each level of the tree, we consider a set of
different features, and iteratively we carry out the
following steps: we process the current feature by
generating its successors, which are all the other
features. Then, we rank features according to the
f-score and we only process the best feature and
prune the rest. We pass all the current pruned fea-
tures as successors to the next level of the tree. The
process iterates until all partial solutions in the tree
are processed or terminated. The selected features
are the following:
Sentiment lexicons : we used the Bing Liu Lex-
icon (Hu and Liu, 2004), the MPQA Subjectivity
Lexicon (Wilson et al., 2005), and NRC Hashtag
Sentiment Lexicon (Mohammad et al., 2013). We
count the number of words in each class, result-
ing in three features: (a) positive words count, (b)
negative words count and (c) neutral words count.
Negative presence: presence of negative words
in a term/tweet using a list of negative words. The
list used is built from the Bing Liu Lexicon (Hu
and Liu, 2004).
Textual tokens: the target term/tweet is seg-
mented into tokens based on space. Token identity
features are created and assigned the value of 1.
Overall polarity score: we determine the polar-
ity scores of words in a target term/tweet using the
Sentiment140 Lexicon (Mohammad et al., 2013)
and the SentiWordNet lexicon (Baccianella et al.,
2010). The overall score is computed by adding
up all word scores.
Level of association: indicates whether the
overall polarity score of a term is greater than 0.2
or not. The threshold value was optimized on the
development set.
Sentiment frequency: indicates the most fre-
quent word sentiment in the tweet. We determine
the sentiment of words using an automatically
generated lexicon. The lexicon comprises 3,247
words and their sentiments. Words were obtained
from the provided training set for task-A and sen-
timents were generated using our expression-level
classifier.
We used slightly different features for Task-A
and Task-B. The features extracted for each task
are summarized in Table 1.
Feature Task A Task B
Positive words count X
Negative words count X
Neutral words count X
Negative presence X X
Textual tokens X X
Overall polarity score X X
Level of association X
Sentiment frequency X
Table 1: Feature summary for each task.
4 Modeling Kernel Functions
Initially we experimented with both logistic
regression and the Support Vector Machine
(SVM) (Fan et al., 2008), using the Stochastic
Gradient Descent (SGD) algorithm for parame-
ter optimization. In our development experiments,
SVM outperformed and became our single classi-
fier. We used the LIBSVM package (Chang and
Lin, 2011) to train and test our classifier.
An SVM kernel function and associated param-
eters were optimized for best F-score on the de-
velopment set. In order to avoid the model over-
fitting the data, we select the optimal parameter
value only if there are smooth gaps between the
near neighbors of the corresponded F-score. Oth-
erwise, the search will continue to the second op-
timal value.
In machine learning, the difference between the
number of training samples, m, and the number
of features, n, is crucial in the selection process
of SVM kernel functions. The Gaussian kernel is
suggested when m is slightly larger than n. Other-
wise, the linear kernel is recommended. In Task-
B, the n : m ratio was 1 : 3 indicating a large
difference between the two numbers. Whereas in
Task-A, a ratio of 5 : 2 indicated a small differ-
ence between the two numbers. We selected the
theoretical types, after conducting an experimen-
tal verification to identify the best kernel function
according to the f-score.
We used a radical basis function kernel for the
expression-level task and the value of its gamma
parameter was adjusted to 0.319. Whereas, we
used a linear function kernel for the message-level
task and the value of its cost parameter was ad-
justed to 0.053.
182
5 Experiments and Results
In this section, we describe the data and the sev-
eral experiments we conducted for both tasks. We
train and evaluate our classifier with the training,
development and testing datasets provided for the
SemEval 2014 shared task. A short summary of
the data distribution is shown in Table 2.
Dataset Postive Negative Neutral
Task-A:
Train (9,451) 62% 33% 5%
Dev (1,135) 57% 38% 5%
Test (10,681) 60% 35% 5%
Task-B:
Train (9,684) 38% 15% 47%
Dev (1,654) 35% 21% 44%
Test (5,754) 45% 15% 40%
Table 2: Datasets distribution percentage per class.
Our test dataset is composed of five different
sets: The test dataset is composed of five dif-
ferent sets: Twitter2013 a set of tweets collected
for the SemEval2013 test set, Twitter2014, tweets
collected for this years version, LiveJournal2014
consisting of formal tweets, SMS2013, a collection
of sms messages, TwitterSarcasm, a collection of
sarcastic tweets.
5.1 Task-A
For this task, we train our classifier on 10,586
terms (9,451 terms in the training set and 1,135
in the development set), tune it on 4,435 terms,
and evaluate it using 10,681 terms. The average
F-score of the positive and negative classes for
each dataset is given in the first part of Table 3.
The best F-score value of 88.94 is achieved on the
Twitter2013.
We conducted an ablation study illustrated in
the second part of Table 3 shows that all the se-
lected features contribute well in our system per-
formance. Other than the textual tokens feature,
which refers to a bag of preprocessed tokens, the
study highlights the role of the term polarity score
feature: ?4.20 in the F-score, when this feature is
not considered on the TwitterSarcasm dataset.
Another study conducted is a feature correlation
analysis, in which we grouped features with sim-
ilar intuitions. Namely the two features negative
presence and negative words count are grouped
as ?negative features?, and the features positive
words count and negative words count are grouped
as ?words count?. We show in Table 4 the effect
on f-score after removing each group from the fea-
tures set. Also we show the f-score after remov-
ing each individual feature within the group. This
helps us see whether features within a group are
redundant or not. For the Twitter2014 dataset, we
notice that excluding one of the features in any of
the two groups leads to a significant drop, in com-
parison to the total drop by its group. The uncor-
related contributions of features within the same
group indicate that features are not redundant to
each other and that they are indeed capturing dif-
ferent information. However, in the case of the
TwitterSarcasm dataset, we observe that the neg-
ative presence feature is not only not contributing
to the system performance but also adding noise
to the feature space, specifically, to the negative
words count feature.
5.2 Task-B
For this task, we trained our classifier on 11,338
tweets (9,684 terms in the training set and 1,654
in the development set), tuned it on 3,813 tweets,
and evaluated it using 8,987 tweets. Results for
different feature configurations are reported in Ta-
ble 5.
It is important to note that if we exclude the tex-
tual tokens feature, all datasets benefit the most
from the polarity score feature. It is interesting to
note that the bag of words, referred to as textual
tokens, is not helping in one of the datasets, the
TwitterSarcasm set. For all datasets, performance
could be improved by removing different features.
In Table 5, we observe that the Negative pres-
ence feature decreases the F-score on the Twitter-
Sarcasm dataset. This could be explained by the
fact that negative words do not usually appear in
a negative implication in sarcastic messages. For
example, this tweet: Such a fun Saturday catch-
ing up on hw. which has a negative sentiment, is
classified positive because of the absence of neg-
ative words. Table 5 shows that the textual tokens
feature increases the classifier?s performance up to
+21.07 for some datasets. However, using a large
number of features in comparison to the number
of training samples could increase data sparseness
and lower the classifier?s performance.
We conducted a post-competition experiment to
examine the relationship between the number of
features and the number of training samples. We
183
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative presence -0.45 0.00 -0.45 -0.23 +0.30
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Polarity score -1.83 -4.20 -0.23 -2.14 -3.00
Level of association -0.18 0.00 -0.18 -0.07 +0.57
Textual tokens -8.74 -2.40 -3.02 -4.37 -6.06
Table 3: Task-A feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative features -1.53 -0.84 -3.05 -1.88 -0.67
Negative presence -0.45 0.00 -0.45 -0.23 +0.3
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Words count -1.07 -2.2 -0.79 -0.62 -2.01
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Table 4: Task-A features correlation analysis. We grouped features with similar intuitions and we calcu-
lated F-scores on each set along with the effect when removing one feature at a time.
fixed the size of our training dataset. Then, we
compared the performance of our classifier using
only the bag of tokens feature, in two different
sizes. In the first experiment, we included all to-
kens collected from all tweets. In the second, we
only considered the top 20 ranked tokens from
each tweet. Tokens were ranked according to the
difference between their highest level of associa-
tion into one of the sentiments and the sum of the
rest. The level of associations for tokens were de-
termined using the Sentiment140 and SentiWord-
Net lexicons. The threshold number of tokens was
identified empirically for best performance. We
found that the classifier?s performance has been
improved by 2 f-score points when the size of to-
kens bag is smaller. The experiment indicates that
the contribution of the bag of words feature can be
increased by reducing the size of vocabulary list.
6 Error Analysis
Our efforts are mostly tuned towards task-A,
hence our inspection and analysis is focused on
task-A. The error rate calculated per sentiment
class: positive, negative and neutral are 6.8%,
14.9% and 93.8%, respectively. The highest error
rate in the neutral class, 93.8%, is mainly due to
the few neutral examples in the training data (only
5% of the data). Hence the system could not learn
from such a small set of neutral class examples.
In the case of negative class error rate, 14.9%,
most of which were classified as positive. An ex-
ample of such classification: I knew it was too
good to be true OTL. Since our system highly re-
lies on lexicon, hence looking at lexicon assigned
polarity to the phrase too good to be true which is
positive, happens because the positive words good
and true has dominating positive polarity.
Lastly for the positive error rate, which is rel-
atively lower, 6%, most of which were classified
negative instead of positive. An example of such
classification: Looks like we?re getting the heavi-
est snowfall in five years tomorrow. Awesome. I?ll
never get tired of winter. Although the phrase car-
ries a positive sentiment, the individual negative
words of the phrase never and tired again domi-
nates over the phrase.
7 Conclusion
We described our systems for Twitter Sentiment
Analysis shared task. We participated in both
tasks, but were mostly focused on task-A. Our hy-
brid system was assembled by integrating a rich
set of lexical features into a framework of fea-
ture selection and parameter tuning, The polarity
184
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 62.71 40.95 65.14 63.22 61.75
Negative presence -1.65 +1.26 -3.37 -3.66 -0.95
Neutral words count +0.05 0.00 -0.72 -0.57 -0.54
Polarity score -4.03 -6.92 -3.82 -3.83 -4.84
Sentiment frequency +0.10 0.00 +0.18 -0.12 -0.05
textual tokens -17.91 +6.5 -21.07 -19.97 -15.8
Table 5: Task B feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
score feature was the most important feature for
our model in both tasks. The F-score results were
consistent across all datasets, except the Twitter-
Sarcasm dataset. It indicates that feature selection
and parameter tuning steps were effective in gen-
eralizing the model to unseen data.
Acknowledgment
We would like to thank Kemal Oflazer and also the
shared task organizers for their support throughout
this work.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC?10), pages 2200?2204, Valletta, Malta.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology, 2:27:1?27:27.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 241?249, Uppsala, Sweden.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 168?
177, Seattle, WA, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA.
Richard E. Neapolitan, 2014. Foundations of Algo-
rithms, pages 257?262. Jones & Bartlett Learning.
Alexander Pak and Patrick Paroubek. 2010. Twitter
Based System: Using Twitter for Disambiguating
Sentiment Ambiguous Adjectives. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 436?439, Uppsala, Sweden.
Bo Pang and Lillian Lee, 2008. Opinion Mining and
Sentiment Analysis, volume 2, pages 1?135. Now
Publishers Inc.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14), Dublin, Ireland.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1555?1565, Baltimore, Maryland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Vancouver, B.C.,
Canada.
185
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 186?191,
Dublin, Ireland, August 23-24, 2014.
CMUQ@Qatar:Using Rich Lexical Features for
Sentiment Analysis on Twitter
Sabih Bin Wasi, Rukhsar Neyaz, Houda Bouamor, Behrang Mohit
Carnegie Mellon University in Qatar
{sabih, rukhsar, hbouamor, behrang}@cmu.edu
Abstract
In this paper, we describe our system for
the Sentiment Analysis of Twitter shared
task in SemEval 2014. Our system uses
an SVM classifier along with rich set of
lexical features to detect the sentiment of
a phrase within a tweet (Task-A) and also
the sentiment of the whole tweet (Task-
B). We start from the lexical features that
were used in the 2013 shared tasks, we en-
hance the underlying lexicon and also in-
troduce new features. We focus our fea-
ture engineering effort mainly on Task-
A. Moreover, we adapt our initial frame-
work and introduce new features for Task-
B. Our system reaches weighted score of
87.11% in Task-A and 64.52% in Task-B.
This places us in the 4th rank in the Task-
A and 15th in the Task-B.
1 Introduction
With more than 500 million tweets sent per day,
containing opinions and messages, Twitter
1
has
become a gold-mine for organizations to monitor
their brand reputation. As more and more users
post about products and services they use, Twit-
ter becomes a valuable source of people?s opin-
ions and sentiments: what people can think about
a product or a service, how positive they can be
about it or what would people prefer the product to
be like. Such data can be efficiently used for mar-
keting. However, with the increasing amount of
tweets posted on a daily basis, it is challenging and
expensive to manually analyze them and locate the
meaningful ones. There has been a body of re-
cent work to automatically learn the public sen-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
timents from tweets using natural language pro-
cessing techniques (Pang and Lee, 2008; Jansen
et al., 2009; Pak and Paroubek, 2010; Tang et al.,
2014). However, the task of sentiment analysis of
tweets in their free format is harder than that of any
well-structured document. Tweet messages usu-
ally contain different kinds of orthographic errors
such as the use of special and decorative charac-
ters, letter or word duplication, extra punctuation,
as well as the use of special abbreviations.
In this paper, we present our machine learn-
ing based system for sentiment analysis of Twitter
shared task in SemEval 2014. Our system takes
as input an arbitrary tweet and assigns it to one
of the following classes that best reflects its sen-
timent: positive, negative or neutral. While pos-
itive and negative tweets are subjective, neutral
class encompasses not only objective tweets but
also subjective tweets that does not contain any
?polar? emotion. Our classifier was developed as
an undergrad course project but later pursued as
a research topic. Our training, development and
testing experiments were performed on data sets
published in SemEval 2013 (Nakov et al., 2013).
Motivated with its performance, we participated
in SemEval 2014 Task 9 (Rosenthal et al., 2014).
Our approach includes an extensive usage of off-
the-shelf resources that have been developed for
conducting NLP on social media text. Our orig-
inal aim was enhancement of the task-A. More-
over, we adapted our framework and introduced
new features for task-B and participated in both
shared tasks. We reached an F-score of 83.3% in
Task-A and an F-score of 65.57% in Task-B. That
placed us in the 4th rank in the task-A and 15th
rank in the task-B.
Our approach includes an extensive usage of
off-the-shelf resources that have been developed
for conducting NLP on social media text. That
includes the Twitter Tokenizer and also the Twit-
ter POS tagger, several sentiment analysis lexica
186
and finally our own enhanced resources for spe-
cial handling of Twitter-specific text. Our origi-
nal aim in introducing and evaluating many of the
features was enhancement of the task-A. More-
over, we adapted our framework and introduced
new features for task-B and participated in both
shared tasks. We reached an F-score of 83.3% in
Task-A and an F-score of 65.57% in Task-B. That
placed us in the 4th rank in the task-A and 15th
rank in the task-B.
2 System Overview
We participate in tasks A and B. We use three-
way classification framework in which we design
and use a rich feature representation of the Twitter
text. In order to process the tweets, we start with
a pre-processing step, followed by feature extrac-
tion and classifier training.
2.1 Data Pre-processing
Before the tweet is fed to the system, it goes
through pre-processing phase that breaks tweet
string into words (tokenization), attaches more in-
formation to each word (POS tagging), and other
treatments.
Tokenization: We use CMU ARK Tok-
enizer (Owoputi et al., 2013) to tokenize each
tweet. This tokenizer is developed to tokenize
not only space-separated text but also tokens that
need to be analyzed separately.
POS tagging: We use CMU ARK POS Tag-
ger (Owoputi et al., 2013) to assign POS tags to
the different tokens. In addition to the grammat-
ical tags, this tagger assigns also twitter-specific
tags like @ mentions, hash tags, etc. This infor-
mation is used later for feature extraction.
Other processing: In order to normalize the dif-
ferent tokens and convert them into a correct En-
glish, we find acronyms in the text and add their
expanded forms at the end of the list. We decide
to keep both the acronym and the new word to en-
sure that if the token without its expansion was
the word the user meant, then we are not losing
any information by getting its acronym. We ex-
tend the NetLingo
2
top 50 Internet acronym list to
add some missing acronyms. In order to reduce in-
flectional forms of a word to a common base form
we use WordNetlemmatizer in NLTK (Bird et al.,
2
http://www.netlingo.com/top50/
popular-text-terms.php
Tweet ?This is so awesome
@henry:D! #excited?
Bag of Words ?This?:1, ?is?:1, ?so?:1,
?awesome?:1, ?@henry?:1,
?:D?:1, ?!,?:1, #excited?:1
POS features numHashTags:1, numAd-
verb:1, numAdjective:1
Polarity features positiveWords:1, negWords:0,
avgScore: -0.113
Task-B specific
features
numCapsWords:0, numEmo-
ticons:1, numUrls:0
Table 1: Set of Features demonstrated on a sample
tweet for Task-B.
2009)
3
. This could be useful for the feature extrac-
tion, to get as much matches as possible between
the train and test data (e.g., for bag-of-words fea-
ture).
2.2 Feature Extraction
Assigning a sentiment to a single word, phrase or
a full tweet message requires a rich set of fea-
tures. For this, we adopt a forward selection ap-
proach (Ladha and Deepa, 2011) to select the fea-
tures that characterize to the best the different sen-
timents and help distinguishing them. In this ap-
proach, we incrementally add the features one by
one and test whether this boosts the development
results. We heavily rely on a binary feature rep-
resentation (Heinly et al., 2012) to ensure the ef-
ficiency and robustness of our classifier. The dif-
ferent features used are illustrated in the example
given in Table 1.
Bag-of-words feature: indicates whether a
given token is present in the phrase.
Morpho-syntactic feature: we use the POS and
twitter-specific tags extracted for each token. We
count the number of adjectives, adverbs and hash-
tags present in the focused part of the tweet mes-
sage (entire tweet or phrase). We tried adding
other POS based features (e.g., number of posses-
sive pronouns, etc.), but only the aforementioned
tags increased the result figures for both tasks.
Polarity-based features: we use freely avail-
able sentiment resources to explicitly define the
polarity at a token-level. We define three feature
categories, based on the lexicon used:
3
http://www.nltk.org/api/nltk.stem.
html
187
Task-A Task-B
Dev Train Test Dev Train Test
Positive 57.09 % 62.06% 59.49% 34.76% 37.59% 39.01%
Negative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15%
Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84%
All 1,135 9,451 10,681 1,654 9,684 8,987
Table 2: Class size distribution for all the three sets for both Task-A and Task-B.
? Subjectivity: number of words mapped to
?positive? from the MPQA Subjectivity lexi-
con (Wilson et al., 2005).
? Hybrid Lexicon: We combine the Senti-
ment140 lexicon (Mohammad et al., 2013)
with the Bing Liu?s bag of positive and neg-
ative words (Hu and Liu, 2004) to create a
dictionary in which each token is assigned a
sentiment.
? Token weight: we use the SentiWordNet
lexicon (Baccianella et al., 2010) to define
this feature. SentiWordNet contains positive,
negative and objective scores between 0 and
1 for all senses in WordNet. Based on this
sense level annotation, we first map each to-
ken to its weight in this lexicon and then the
sum of all these weights was used as the tweet
weight.
Furthermore, in order to take into account the
presence of negative words, which modify the po-
larity of the context within which they are invoked,
we reverse the polarity score of adjectives or ad-
verbs that come within 1-2 token distance after a
negative word.
Task specific features: In addition to the fea-
tures described above, we also define some task-
specific ones. For example, we indicate the num-
ber of capital letters in the phrase as a feature in
Task-A. This could help in this task, since we are
focusing on short text. For Task-B we indicate
instead the number of capital words. This relies
on the intuition that polarized tweets would carry
more (sometimes all) capital words than the neu-
tral or objective ones. We also added the number
of emoticons and number of URL links as fea-
tures for Task-B. Here, the goal is to segregate
fact-containing objective tweets from emotion-
containing subjective tweets.
2.3 Classifier
We use a Support Vector Machine (SVM) classi-
fier (Chang and Lin, 2011) to which we provide
the rich set of features described in the previous
section. We use a linear kernel and tune its param-
eter C separately for the two tasks. Task-A sys-
tem was bound tight to the development set with
C=0.18 whereas in Task-B the system was given
freedom by setting C=0.55. These values were
optimized during the development using a brute-
force mechanism.
Task-A Task-B
LiveJournal 2014 83.89 65.63
SMS 2013 88.08 62.95
Twitter 2013 89.85 65.11
Twitter 2014 83.45 65.53
Sarcasm 78.07 40.52
Weighted average 87.11 64.52
Table 3: F1 measures and final results of the sys-
tem for Task-A and Task-B for all the data sets
including the weighted average of the sets.
3 Experiments and Results
In this section, we explain details of the data and
the general settings for the different experiments
we conducted. We train and evaluate our classifier
for both tasks with the training, development and
testing datasets provided for the SemEval 2014
shared task. The size of the three datasets we
use as well as their class distributions are illus-
trated in Table 2 . It is important to note that
the total dataset size for training and development
set (10,586) is about the same as test set mak-
ing the learning considerably challenging for cor-
rect predictions. Positive instances covered more
than half of each dataset for Task-A while Neutral
were the most popular class for Task-B. The class
distribution of training set is the same as the test
set.
188
Task-A Task-B
all features 87.11 64.52
all-preprocessing 80.79(-6.32) 59.20(-5.32)
all-ARK tokenization 83.69(-3.42) 60.61(-3.91)
all-other treatments 85.06(-2.05) 62.19(-2.33)
only BOW 81.69(-5.42) 57.85(-6.67)
all-bow 82.05(-5.06) 52.04(-12.48)
all-pos 86.92(-0.19) 64.31(-0.21)
all-polarity based features 81.80(-5.31) 57.95(-6.57)
all-SVM tuning 80.82(-6.29) 21.41(-43.11)
all-SVM c=0.01 84.20(-2.91) 59.87(-4.65)
all-SVM c=selected 87.11(0.00) 64.52(0.00)
all-SVM c=1 86.39(-0.72) 62.51(-2.01)
Table 4: F-scores obtained on the test sets with the specific feature removed.
The test dataset is composed of five differ-
ent sets: Twitter2013 a set of tweets collected
for the SemEval2013 test set, Twitter2014, tweets
collected for this years version, LiveJournal2014
consisting of formal tweets, SMS2013, a collec-
tion of sms messages,TwitterSarcasm, a collection
of sarcastic tweets. The results of our system are
shown in Table 3. The top five rows shows the
results by the SemEval scorer for all the data sets
used by them. This scorer took the average of F1-
score of only positive and negative classes. The
last row shows the weighted average score of all
the scores for Task A and B from the different data
sets.
Our scores for Task-A and Task-B were 83.45
and 65.53 respectively for Twitter 2014.
Our system performed better on Twitter and
SMS test sets from 2013. This was reasonable
since we tuned our system on these datasets. On
the other hand, the system performed worst on sar-
casm test set. This drop is extremely evident in
Task-B where the results were dropped by 25%.
To analyze the effects of each step of our sys-
tem, we experimented with our system using dif-
ferent configurations. The results are shown in Ta-
ble 4 and our analysis is described in the following
subsections. The results were scored by SemEval
2014 scorer and we took the weighted average of
all data sets to accurately reflect the performance
of our system.
We show the polarities values assigned to each
token of a tweet by our classifier, in Table 5.
Tokens POS Tags Sentiments Polarity
This O Neutral -0.194
Is V Neutral -0.115
So R Neutral -0.253
Awesome A Positive 2.351
@Henry @ - -
#excited # Positive 1.84
Table 5: Polarity assigned using our classifier to
each word of a Tweet message.
3.1 Preprocessing Effects
We compared the effects of basic tokenization
(based on white space) against the richer ARK
Twitter tokenizer. The scores dropped by 3.42%
and 3.91% for Task-A and Task-B, respectively.
Other preprocessing enhancements like lemmati-
zation and acronym additions also gave our sys-
tem performance a boost. Again, the effects were
more visible for Task-B than for Task-A. Over-
all, the system performance was boosted by 6.32%
for Task-A and 5.32% for Task-B. Considering
the overall score for Task-B, this is a significant
change.
3.2 Feature Engineering Effects
To analyze the effect of feature extraction pro-
cess, we ran our system with different kind of
features disabled - one at a time. For Task-A,
unigram model and polarity based features were
equally important. For Task-B, bag of words fea-
ture easily outperformed the effects of any other
feature. However, polarity based features were
second important class of features for our system.
These suggest that if more accurate, exhaustive
189
and social media representative lexicons are made,
it would help both tasks significantly. POS based
features were not directly influential in our system.
However, these tags helped us find better matches
in lexicons where words are further identified with
their POS tag.
3.3 Classifier Tuning
We also analyzed the significance of SVM tuning
to our system. Without setting any parameter to
SVMutil library (Chang and Lin, 2011), we no-
ticed a drop of 6.29% to scores of Task-A and a
significant drop of 43.11% to scores of Task-B.
Since the library use poly kernel by default, the
results were drastically worse for Task-B due to
large feature set. We also compared the perfor-
mance with SVM kernel set to C=1. In this re-
stricted setting, the results were slightly lower than
the result obtained for our final system.
4 Discussion
During this work, we found that two improve-
ments to our system would have yielded better
scores. The first would be lexicons: Since the
lexicons like Sentiment140 Lexicon are automati-
cally generated, we found that they contain some
noise. As we noticed a drop of that our results
were critically dependent on these lexicons, this
noise would have resulted in incorrect predictions.
Hence, more accurate and larger lexicons are re-
quired for better classification, especially for the
tweet-level task. Unlike SentiWordNet these lexi-
cons should contain more informal words that are
common in social media. Additionally, as we can
see our system was not able to confidently predict
sarcasm tweets on both expression and message
level, special attention is required to analyze the
nature of sarcasm on Twitter and build a feature
set that can capture the true sentiment of the tweet.
5 Conclusion
We demonstrated our classification system that
could predict sentiment of an input tweet. Our
system performed more accurately in expression-
level prediction than on entire tweet-level predic-
tion. Our system relied heavily on bag-of-words
feature and polarity based features which in turn
relied on correct part-of-speech tagging and third-
party lexicons. With this system, we ranked 4th
in SemEval 2014 expression-level prediction task
and 15th in tweet-level prediction task.
Acknowledgment
We would like to thank Kemal Oflazer and the
shared task organizers for their support through-
out this work.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC?10), pages 2200?2204, Valletta, Malta.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology, 2:27:1?27:27.
Jared Heinly, Enrique Dunn, and Jan-Michael Frahm.
2012. Comparative Evaluation of Binary Features.
In Proceedings of the 12th European Conference on
Computer Vision, pages 759?773, Firenze, Italy.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 168?
177, Seattle, WA, USA.
Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter Power: Tweets as
Electronic Word of Mouth. Journal of the Ameri-
can society for information science and technology,
60(11):2169?2188.
L. Ladha and T. Deepa. 2011. Feature Selection Meth-
ods and Algorithms. International Journal on Com-
puter Science and Engineering (IJCSE), 3:1787?
1797.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA.
190
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved Part-of-Speech Tagging for
Online Conversational Text with Word Clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390, Atlanta, Georgia.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a Corpus for Sentiment Analysis and Opinion Min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), pages 1320?1326, Valletta, Malta.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis, volume 2. Now Publishers Inc.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14), Dublin, Ireland.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1555?1565, Baltimore, Maryland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347?354.
191
Workshop on Monolingual Text-To-Text Generation, pages 10?19,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 10?19,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Web-based validation for contextual targeted paraphrasing
Houda Bouamor
LIMSI-CNRS
Univ. Paris Sud
hbouamor@limsi.fr
Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
amax@limsi.fr
Gabriel Illouz
LIMSI-CNRS
Univ. Paris Sud
gabrieli@limsi.fr
Anne Vilnat
LIMSI-CNRS
Univ. Paris Sud
anne@limsi.fr
Abstract
In this work, we present a scenario where con-
textual targeted paraphrasing of sub-sentential
phrases is performed automatically to support
the task of text revision. Candidate para-
phrases are obtained from a preexisting reper-
toire and validated in the context of the orig-
inal sentence using information derived from
the Web. We report on experiments on French,
where the original sentences to be rewrit-
ten are taken from a rewriting memory au-
tomatically extracted from the edit history of
Wikipedia.
1 Introduction
There are many instances where it is reasonable to
expect machines to produce text automatically. Tra-
ditionally, this was tackled as a concept-to-text real-
ization problem. However, such needs apply some-
times to cases where a new text should be derived
from some existing texts, an instance of text-to-text
generation. The general idea is not anymore to pro-
duce a text from data, but to transform a text so as to
ensure that it has desirable properties appropriate for
some intended application (Zhao et al, 2009). For
example, one may want a text to be shorter (Cohn
and Lapata, 2008), tailored to some reader pro-
file (Zhu et al, 2010), compliant with some spe-
cific norms (Max, 2004), or more adapted for sub-
sequent machine processing tasks (Chandrasekar et
al., 1996). The generation process must produce
a text having a meaning which is compatible with
the definition of the task at hand (e.g. strict para-
phrasing for document normalization, relaxed para-
phrasing for text simplification), while ensuring that
it remains grammatically correct. Its complexity,
compared with concept-to-text generation, mostly
stems from the fact that the semantic relationship
between the original text and the new one is more
difficult to control, as the mapping from one text to
another is very dependent on the rewriting context.
The wide variety of techniques for acquiring phrasal
paraphrases, which can subsequently be used by text
paraphrasing techniques (Madnani and Dorr, 2010),
the inherent polysemy of such linguistic units and
the pragmatic constraints on their uses make it im-
possible to ensure that potential paraphrase pairs
will be substitutable in any context, an observation
which was already made at a lexical level (Zhao et
al., 2007). Hence, automatic contextual validation of
candidate rewritings is a fundamental issue for text
paraphrasing with phrasal units.
In this article, we tackle the problem of what we
call targeted paraphrasing, defined as the rewriting
of a subpart of a sentence, as in e.g. (Resnik et al,
2010) where it is applied to making parts of sen-
tences easier to translate automatically. While this
problem is simpler than full sentence rewriting, its
study is justified as it should be handled correctly
for the more complex task to be successful. More-
over, being simpler, it offers evaluation scenarios
which make the performance on the task easier to
assess. Our particular experiments here aim to as-
sist a Wikipedia contributor in revising a text to im-
prove its quality. For this, we use a collection of
phrases that have been rewritten in Wikipedia, and
test the substitutability of paraphrases coming from
a repertoire of sub-sentential paraphrases acquired
10
from different sources. We thus consider that preex-
isting repertoires of sub-sentential paraphrase pairs
are available, and that each potential candidate has to
be tested in the specific context of the desired rewrit-
ing. Due to the large variety of potential phrases
and their associated known paraphrases, we do not
rely on precomputed models of substitutability, but
rather build them on-the-fly using information de-
rived from web queries.1
This article is organized as follows. In section 2,
we first describe the task of text revision, where a
subpart of a sentence is rewritten, as an instance
of targeted paraphrasing. Section 3 presents previ-
ous works on the acquisition of sub-sentential para-
phrases and describes the knowledge sources that we
have used in this work. We then describe in section 4
how we estimate models of phrase substitution in
context by exploiting information coming from the
web. We present our experiments and their results in
section 5, and finally discuss our current results and
future work in section 6.
2 Targeted paraphrasing for text revision
One of the important processes of text revision is
the rewording of parts of sentences. Some reword-
ings are not intended to alter meaning significantly,
but rather to make text more coherent and easier to
comprehend. Those instances which express close
meanings are sub-sentential paraphrases: in their
simpler form, they can involve synonym substitu-
tion, but they can involve more complex deeper
lexical-syntactic transformations.
Such rephrasings are commonly found in record-
ings of text revisions, which now exist in large
quantities in the collaborative editing model of
Wikipedia. In fact, revision histories of the encyclo-
pedia contain a significant amount of sub-sentential
paraphrases, as shown by the study of (Dutrey et al,
2011). This study also reports that there is an impor-
tant variety of rephrasing phenomena, as illustrated
by the difficulty of reaching a good identification
coverage using a rule-based term variant identifica-
tion engine.
1Note that using the web may not always be appropriate, or
that at least it should be used in a different way than what we
propose in this article, in particular in cases where the desired
properties of the rewritten text are better described in controlled
corpora.
The use of automatic targeted paraphrasing as an
authoring aid has been illustrated by the work of
Max and Zock (2008), in which writers are pre-
sented with potential paraphrases of sub-sentential
fragments that they wish to reword. The automatic
paraphrasing technique used is a contextual vari-
ant of bilingual translation pivoting (Bannard and
Callison-Burch, 2005). It has also been proposed
to externalize various text editing tasks, including
proofreading, by having crowdsourcing functions on
text directly from word processors (Bernstein et al,
2010).
Text improvements may also be more specifi-
cally targeted for automatic applications. In the
work by Resnik et al (2010), rephrasings for spe-
cific phrases are acquired through crowdsourcing.
Difficult-to-translate phrases in the source text are
first identified, and monolingual contributors are
asked to provide rephrasings in context. Collected
rephrasings can then be used as input for a Ma-
chine Translation system, which can positively ex-
ploit the increased variety in expression to pro-
duce more confident translations for better estimated
source units (Schroeder et al, 2009).2 For instance,
the phrase in bold in the sentence The number of
people known to have died has now reached 358
can be rewritten as 1) who died, 2) identified to
have died and 3) known to have passed away. All
such rephrasings are grammatically correct, the first
one being significantly shorter, and they all convey
a meaning which is reasonably close to the original
wording.
The task of rewriting complete sentences has also
been addressed in various works (e.g. (Barzilay and
Lee, 2003; Quirk et al, 2004; Zhao et al, 2010)). It
poses, however, numerous other challenges, in par-
ticular regarding how it could be correctly evalu-
ated. Human judgments of whole sentence trans-
formations are complex and intra- and inter-judge
coherence is difficult to attain with hypotheses of
comparable quality. Using sentential paraphrases
to support a given task (e.g. providing alternative
reference translations for optimizing Statistical Ma-
chine Translation systems (Madnani et al, 2008))
2It is to be noted that, in the scenario presented in (Resnik et
al., 2010), monolingual contributors cannot predict how useful
their rewritings will be to the underlying Machine Translation
engine used.
11
can be seen as a proxy for extrinsic evaluation of
the quality of paraphrases, but it is not clear from
published results that improvements on the task are
clearly correlated with the quality of the produced
paraphrases. Lastly, automatic metrics have been
proposed for evaluating the grammaticality of sen-
tences (e.g. (Mutton et al, 2007)). Automatic evalu-
ation of sentential paraphrases has not produced any
consensual results so far, as they do not integrate
task-specific considerations and can be strongly bi-
ased towards some paraphrasing techniques.
In this work, we tackle the comparatively more
modest task of sub-sentential paraphrasing applied
to text revision. In order to use an unbiased
task, we use a corpus of naturally-occurring rewrit-
ings from an authoring memory of Wikipedia ar-
ticles. We use the WICOPACO corpus (Max and
Wisniewski, 2010), a collection of local rephras-
ings from the edit history of Wikipedia which con-
tains instances of lexical, syntactical and semantic
rephrasings (Dutrey et al, 2011), the latter type be-
ing illustrated by the following example:
Ce vers de Nuit rhe?nane d?Apollinaire [qui para??t
presque sans structure rythmique? dont la ce?sure
est comme masque?e]. . . 3
The appropriateness of this corpus for our work
is twofold: first, the fact that it contains naturally-
occurring rewritings provides us with an interest-
ing source of text spans in context which have been
rewritten. Moreover, for those instances where the
meaning after rewriting was not significantly al-
tered, it provides us with at least one candidate
rewriting that should be considered as a correct para-
phrase, which can be useful for training validation
algorithms.
3 Automatic sub-sentential paraphrase
acquisition and generation
The acquisition of paraphrases, and in particular
of sub-sentential paraphrases and paraphrase pat-
terns, has attracted a lot of works with the advent of
data-intensive Natural Language Processing (Mad-
nani and Dorr, 2010). The techniques proposed have
a strong relationship to the type of text corpus used
3This verse from Apollinaire?s Nuit Rhe?nane [which seems
almost without rhythmic structure ? whose cesura is as if
hidden]. . .
for acquisition, mainly:
? pairs of sentential paraphrases (monolingual
parallel corpora) allow for a good precision
but evidently a low recall (e.g. (Barzilay and
McKeown, 2001; Pang et al, 2003; Cohn et
al., 2008; Bouamor et al, 2011))
? pairs of bilingual sentences (bilingual parallel
corpora) allow for a comparatively better re-
call (e.g. (Bannard and Callison-Burch, 2005;
Kok and Brockett, 2010))
? pairs of related sentences (monolingual com-
parable corpora) allow for even higher recall
but possibly lower precision (e.g. (Barzilay
and Lee, 2003; Li et al, 2005; Bhagat and
Ravichandran, 2008; Dele?ger and Zweigen-
baum, 2009)
Although the precision of such techniques can in
some cases be formulated with regards to a prede-
fined reference set (Cohn et al, 2008), it should
more generally be assessed in the specific context
of some use of the paraphrase pair. This refers to
the problem of substituability in context (e.g. (Con-
nor and Roth, 2007; Zhao et al, 2007)), which is a
well studied field at the lexical level and the object of
evaluation campains (McCarthy and Navigli, 2009).
Contextual phrase substitution poses the additional
challenge that phrases are rarer than words, so that
building contextual and grammatical models to en-
sure that the generated rephrasings are both seman-
tically compatible and grammatical is more compli-
cated (e.g. (Callison-Burch, 2008)).
The present work does not aim to present any
original technique for paraphrase acquisition, but
rather focusses on the task of sub-sentential para-
phrase validation in context. We thus resort to some
existing repertoire of phrasal paraphrase pairs. As
explained in section 2, we use the WICOPACO cor-
pus as a source of sub-sentential paraphrases: the
phrase after rewriting can thus be used as a potential
paraphrase in context.4 To obtain other candidates
of various quality, we used two knowledge sources.
The first uses automatic pivot translation (Bannard
and Callison-Burch, 2005), where a state-of-the-art
4Note, however, that in our experiments we will ask our hu-
man judges to assess anew its paraphrasing status in context.
12
general-purpose Statistical Machine Translation sys-
tem is used in a two-way translation. The second
uses manual acquisition of paraphrase candidates.
Web-based acquisition of this type of knowledge has
already been done before (Chklovski, 2005; Espan?a
Bonet et al, 2009), and could be done by crowd-
sourcing, a technique growing in popularity in recent
years. We have instead formulated manual acquisi-
tion as a web-based game. Players can take parts in
two parts of the game, illustrated on Figure 3.
First, players propose sub-sentential paraphrases
in context for selected text spans in web documents
(top of Figure 3), and then raters can take part in as-
sessing paraphrases proposed by other players (bot-
tom of Figure 3). In order to avoid any bias, players
cannot evaluate games in which they played. Eval-
uation is sped up by using a compact word lattice
view for eliciting human judgments, built using the
syntactic fusion algorithm of (Pang et al, 2003).
Data acquisition was done in French to remain co-
herent with our experiments on the French corpus
of WICOPACO, and both players and raters were
native speakers. An important point is that in our
experiments the context of acquisition and of evalu-
ation were different: players were asked to generate
paraphrases in contexts that are different from those
of the WICOPACO corpus used for evaluation. To
this end, web snippets were automatically retrieved
for the various phrases of our dataset without con-
texts, so that sentences from the Web (but not from
Wikipedia) were used for manual paraphrase acqui-
sition. This allows us to simulate the availability of a
preexisting repertoire of (contextless) sub-sentential
paraphrases, and to assess the performance of our
contextual validation techniques on a possibly in-
compatible context.
4 Web-based contextual validation
Given a repertoire of potential phrasal paraphrases
and a context for a naturally-occurring rewriting, our
task consists in deciding automatically which poten-
tial paraphrases can be substituted with good confi-
dence for the original phrase. A concrete instantia-
tion of it could correspond to the proposal of Max
and Zock (2008), where such candidate rephrasings
could be presented in order of decreasing suitability
to a word processor user, possibly during the revi-
sion of a Wikipedia article.
The specific nature of the text units that we are
dealing with calls for a careful treatment: in the
general scenario, it is unlikely that any supervised
corpus would contain enough information for ap-
propriate modeling of the substituability in context
decision. It is therefore tempting to consider using
the Web as the largest available information source,
in spite of several of its known limitations, includ-
ing that data can be of varying quality. It has how-
ever been shown that a large range of NLP applica-
tions can be improved by exploiting n-gram counts
from the Web (using Web document counts as a
proxy) (Lapata and Keller, 2005).
Paraphrase identification has been addressed pre-
viously, both using features computed from an of-
fline corpus (Brockett and Dolan, 2005) and fea-
tures computed from Web queries (Zhao et al,
2007). However, to our knowledge previous work
exploiting information from the Web was limited to
the identification of lexical paraphrases. Although
the probability of finding phrase occurrences sig-
nificantly increases by considering the Web, some
phrases are still very rare or not present in search
engine indexes.
As in (Brockett and Dolan, 2005), we tackle our
paraphrase identification task as one of monolingual
classification. More precisely, considering an orig-
inal phrase p within the context of sentence s, we
seek to determine whether a candidate paraphrase p?
would be a grammatical paraphrase of p within the
context of s. We make use of a Support Vector Ma-
chine (SVM) classifier which exploits the features
described in the remainder of this section.
Edit distance model score Surface similarity on
phrase pairs can be a good indicator that they share
semantic content. In order to account for the cost
of transforming one string into the other, rather
than simply counting common words, we use the
score produced by the Translation Edit Rate met-
ric (Snover et al, 2010). Furthermore, we perform
this computation on strings of lemmas rather than
surface forms:5
5Note that because we computed the TER metric on French
strings, stemming and semantic matching through WordNet
were not activated.
13
Figure 1: Interface of our web-based game for paraphrase acquisition and evaluation. On the top, players reformulate
all text spans highlighted by the game creator on any webpage (a Wikipedia article on the example). On the bottom,
raters evaluate paraphrases proposed by sets of players using a compact word-lattice view. Note that in its standard
definition, the game attributes higher scores to paraphrase candidates that are highly rated and rarer.
hedit = TER(Lemorig, Lempara) (1)
Note that this model is not derived from informa-
tion from the Web, in contrast to all the models de-
scribed next.
Language model score The likelihood of a sen-
tence can be a good indicator of its grammatical-
ity (Mutton, 2006). Language model probabilities
can now be obtained from Web counts. In our ex-
periments, we used the Microsoft Web N-gram Ser-
vice6 for research (Wang et al, 2010) to obtain log
likelihood scores for text units.7 However, this score
is certainly not sufficient as it does not take the orig-
inal wording into account. We therefore used a ratio
of the language model score of the paraphrased sen-
tence with the language model score of the original
6http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
7Note that in order to query on French text, we had to re-
move all diacritics for the service to behave correctly, indepen-
dently of encodings: careful examination of ranked hypotheses
showed that this trick allowed us to obtain results coherent with
expectations.
sentence, after normalization by sentence length of
the language model scores (Onishi et al, 2010):
hLM ratio =
LM(para)
LM(orig)
=
lm(para)1/length(para)
lm(orig)1/length(orig)
(2)
Contextless thematic model scores Cooccurring
words are used in distributional semantics to account
for common meanings of words. We build vector
representations of cooccurrences for both the origi-
nal phrase p and its paraphrase p?. Our contextless
thematic model is built in the following fashion: we
query a search engine to retrieve the top N docu-
ment snippets for phrase p. We then count frequen-
cies for all content words in these snippets, and keep
the set W of words appearing more than a fraction
of N . We then build a vector T (thematic profile)
of dimension |W | where values are computed by the
following formula:
Tnocontorig [w] =
count(p, w)
count(p)
(3)
14
where count(x) correspond to the number of docu-
ments containing a given exact phrase or word ac-
cording to the search engine used and count(x, y)
correspond to the number of documents containing
simultaneously both. We then compute the same
thematic profile for the paraphrase p?, using only the
subset of words W :
Tnocontpara [w] =
count(p?, w)
count(p)
(4)
Finally, we compute a similarity between the two
profiles by taking the cosinus between their two vec-
tors:
hnocontthem =
Tnocontorig ? T
nocont
para
||Tnocontorig || ? ||T
nocont
para ||
(5)
In all our experiments, we used the Yahoo! Search
BOSS8 Web service for obtaining Web counts and
retrieving snippets. Assuming that the distribution
of words in W is not biased by the result ordering
of the search engine, our model measures some sim-
ilarity between the most cooccurring content words
with p and the same words with p?.
Context-aware thematic model scores Our
context-aware thematic model takes into account
the words of sentence s in which the substitution
of p with p? is attempted. We now consider the set
of content words from s (s being the part of the
sentence without phrase p) in lieu of the previous
set of cooccurring words W , and compute the
same profile vectors and similarity between that of
the original sentence and that of the paraphrased
sentence:
hcontthem =
T contorig ? T
cont
para
||T contorig || ? ||T
cont
para||
(6)
However, words from s might not be strongly
cooccurring with p. In order to increase the likeli-
hood of finding thematically related words, we also
build an extended context model, hextcontthem where
content words from s are supplemented with their
most cooccurring words. This is done using the
same procedure as that previously used for finding
content words cooccurring with p.
8http://developer.yahoo.com/search/boss/
5 Experiments
In this section we report on experiments conducted
to assess the performance of our proposed approach
for validating candidate sub-sentential paraphrases
using information from the Web.
5.1 Data used
We randomly extracted 150 original sentences in
French and their rewritings from the WICOPACO
corpus which were marked as paraphrases. Of those,
we kept 100 for our training corpus and the remain-
ing 50 for testing. The number of original phrases of
each length is reported on Figure 2.
phrase length 1 2 3 4 5 6 7 8
original phrases 0 3 29 8 6 2 2 0
paraphrases 39 64 74 36 21 10 5 1
Figure 2: Distribution of number of phrases per phrase
length in tokens for the test corpus
For each original sentence, we collected 5 candi-
date paraphrases to simulate the fact that we had a
repertoire of paraphrases with the required entries:9
? WICOPACO: the original paraphrase from the
WICOPACO corpus;
? GAME: two candidate paraphrases from users
of our Web-based game;
? PIVOTES and PIVOTZH: two candidate para-
phrases obtained by translation by pivot, using
the Google Translate10 online SMT system and
one language close to French as pivot (Span-
ish), and another one more distant (Chinese).
We then presented the original sentence and its 5
paraphrases (in random order) to two judges. Four
native speakers took part in our experiments: they
all took part in the data collection for one half of
the sentences of the training and test corpora and to
the evaluation of paraphrases for the other half. For
the annotation with two classes (paraphrase vs. not
paraphrase), we obtain as inter-judge agreement11 a
9Note that, as a consequence, we did not carry any experi-
ment related to the recall of any technique here.
10http://translate.google.com
11We used R (http://www.r-project.org) to com-
pute this Cohen?s ? value.
15
Figure 3: Example of an original sentence and its 5 associated candidate paraphrases. The phrase in bold from the
original sentence (The brand is at the origin of many concepts that have revolutionized computing.) is paraphrased
as est le promoteur (is the promoter), a popularise? (popularized), origine (origin), est a` la source (is the source), and
l?origine (the origin).
value of ? = 0.65, corresponding to a substantial
agreement according to the literature. An example
of the interface used is provided in Figure 3.
We considered that our technique could not pro-
pose reliable results when web phrase counts were
too low. From the distribution of counts of phrases
and paraphrases from our training set (see Figure 4),
we empirically chose a threshold of 10 for the min-
imum count of any phrase. Our corpus was conse-
quently reduced from 750=150*5 to 434 examples
for the training corpus, and from 250=50*5 to 215
for the test corpus.
  <10 <100 <1000 <10000 <100000 <1000000 >10000000
1020
3040
5060
7080
90100 # of original phrases# of paraphrases
Range of number of counts
Figure 4: Number of phrases and paraphrases per web
count range
Results will be reported for three conditions:
? Possible: the gold standard for instances where
at least one of the judges indicated ?para-
phrases? records the pair as a paraphrase. In
this condition, the test set has 116 instances that
are paraphrases and 99 that are not.
? Sure: the gold standard for instances where not
all judges indicated ?paraphrases? records the
pair as not paraphrase. In this condition, the
test set has 76 instances that are paraphrases
and 139 that are not.
? Surer: only those instances where both judges
agree are recorded. This reduces our training
and test set to respectively 287 and 175 exam-
ples. Thus, results on this subcorpora will not
be directly comparable with the other results.
In this condition, the test set has 76 instances
that are paraphrases and 99 that are not.
5.2 Baseline techniques
Web-count based baselines We used two base-
lines based on simple Web counts. The first one,
WEBLM, considers a candidate sentence a para-
phrase of the original sentence whenever its Web
language model score is higher than that of the orig-
inal phrase. The second one, BOUNDLM, considers
a sentence as a paraphrase whenever the counts for
the bigrams crossing the left and right boundary of
the sub-sentential paraphrase is higher than 10.
Syntactic dependency baseline When rewriting a
subpart of a sentence, the fact that syntactic depen-
dencies between the rewritten phrase and its con-
text are the same than those of the original phrase
and the same context can provide some information
16
about the grammatical and semantic substituability
of the two phrases (Zhao et al, 2007; Max and Zock,
2008). We thus build syntactic dependencies for
both the original and rewritten sentence, using the
French version (Candito et al, 2010) of the Berkeley
probabilistic parser (Petrov and Klein, 2007), and
consider the subset of dependencies for the two sen-
tences that exist between a word inside the phrase
under focus and a word outside it (Deporig and
Deppara). Our CONTDEP baseline considers a sen-
tence as a paraphrase iff Deppara = Deporig.
5.3 Evaluation results
We used the models described in Section 4 to build
a SVM classifier using the LIBSVM package (Chang
and Lin, 2001). Accuracy results are reported on
Figure 5.
WEBLM BOUNDLM CONTDEP CLASSIFIER
POSSIBLE 62.79 54.88 48.53 57.67
SURE 68.37 36.27 51.90 70.69
SURER 56.79 51.41 42.69 62.85
Figure 5: Accuracy results for the three baselines and our
classifier on the test set for the three conditions. Note that
the SURER condition cannot be directly compared with
the other two as the number of training and test examples
are not the same.
The first notable observation is that our task is not
surprisingly a difficult one. The best performance
achieved is an accuracy of 70.69 with our system in
the SURE condition. There are, however, some im-
portant variations across conditions, with a result as
low as 57.67 for our system in the POSSIBLE condi-
tion (recall that in this condition candidates are con-
sidered paraphrases when only one of the two judges
considered it a paraphrase, i.e. when the two judges
disagreed).
Overall, the WEBLM baseline and our system ap-
pear as stronger than the two other baselines. The
two lower baselines, BOUNDLM and CONTDEP, at-
tempt to model local grammatical constraints, which
are not surprisingly not sufficient for paraphrase
identification. WEBLM is comparatively a much
more competitive baseline, but its accuracy in the
SURER condition is not very strong. As this latter
condition considers only consensual judgements for
the two judges, we can hypothesize that the interpre-
tation of its results is more reliable. In this condi-
WICOPACO GAMERS PIVOTES PIVOTZH
POSSIBLE 89.33 67.00 47.33 20.66
SURE 64.00 44.50 31.33 10.66
SURER 86.03 57.34 37.71 12.60
Figure 6: Paraphrase accuracy of our different paraphrase
acquisition methods for the three conditions.
tion, our system obtains the best performance, with
a +6.06 advantage over WEBLM. As found in other
works (e.g. (Bannard and Callison-Burch, 2005)),
using language models for paraphrase validation is
not sufficient as it cannot model meaning preserva-
tion, and our results show that this is also true even
when counts are estimated from the Web. Using a
ratio of normalized LM scores may have improved
the situation a bit.12
Lastly, we report in Figure 6 the paraphrase
accuracy of each individual acquisition technique
(i.e. source of paraphrases from the preexisting
repertoire). The original rewritting from WICO-
PACO obtains not surprisingly a very high para-
phrase accuracy, in particular in the POSSIBLE and
SURER conditions. Paraphrases obtained through
our Web-based game have an acceptable accuracy:
the numbers confirm that paraphrase pairs are highly
context-dependent, because the pairs which were
likely to be paraphrases in the context of the game
are not necessarily so in a different context. This,
of course, may be due to a number of reasons that
we will have to investigate. Lastly, there is a signif-
icant drop in accuracy for the automatic pivot para-
phrasers, but pivoting through Spanish obtained, not
suprisingly again, a much better performance than
pivoting through Chinese.
6 Discussion and future work
We have presented an approach to the task of
targeted paraphrasing in the context of text revi-
sion, a scenario which was supported by naturally-
occurring data from the rephrasing memory of
Wikipedia. Our framework takes a repertoire of ex-
isting sub-sentential paraphrases, coming from pos-
12A possible explanation for the relative good performance of
WEBLM may lie in the fact that our two automatic paraphrasers
using Google Translate as a pivot translation engine tend to pro-
duce strings that are very likely according to the language mod-
els used by the translation system, which we assume to be very
comparable to those that were used in our experiments.
17
sibly any source including manual acquisition, and
validates all candidate paraphrases using informa-
tion from the Web. Our experiments have shown
that the current version of our classifier outperforms
several baselines when considering paraphrases with
consensual judgements in the gold standard refer-
ence.
Although our initial experiments are positive, we
believe that they can be improved in a number of
ways. We intend to broaden our exploration of the
various characteristics at play. We will try more fea-
tures, including e.g. a model of syntactic depen-
dencies derived from the Web, and extend our work
to new languages. We will also attempt to analyze
more precisely our results to identify problematic
cases, some of which could turn to be almost im-
possible to model without resorting to world knowl-
edge, which was beyond our attempted modeling.
Finally, we will also be interested in considering the
applicability of this approach as a framework for the
evaluation of paraphrase acquisition techniques.
Acknowledgments
This work was partly supported by ANR project
Trace (ANR-09-CORD-023). The authors would
like to thank the anonymous reviewers for their help-
ful questions and comments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT,
Edmonton, Canada.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, Toulouse, France.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceedings
of the ACM symposium on User interface software and
technology.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2011.
Monolingual alignment by edit rate computation on
sentential paraphrase pairs. In Proceedings of ACL,
Short Papers session, Portland, USA.
Chris Brockett and William B. Dolan. 2005. Support
vector machines for paraphrase identification and cor-
pus construction. In Proceedings of The 3rd Inter-
national Workshop on Paraphrasing IWP, Jeju Island,
South Korea.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, Hawai, USA.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
R. Chandrasekar, Christine Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of COLING, Copenhagen, Denmark.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Timothy Chklovski. 2005. Collecting paraphrase cor-
pora from volunteer contributors. In Proceedings of
KCAP 2005, Banff, Canada.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, Manchester, UK.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Comput. Linguist.,
34(4):597?614.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of ECML, Warsaw, Poland.
Louise Dele?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in wikipedia?s revision history. SEPLN
journal, 46:51?58.
Cristina Espan?a Bonet, Marta Vila, M. Anto`nia Mart??,
and Horacio Rodr??guez. 2009. Coco, a web interface
for corpora compilation. SEPLN journal, 43.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of NAACL-
HLT, Los Angeles, USA.
18
Mirella Lapata and Frank Keller. 2005. Web-based Mod-
els for Natural Language Processing. ACM Transac-
tions on Speech and Language Processing, 2(1):1?31.
Weigang Li, Ting Liu, Yu Zhang, Sheng Li, and Wei
He. 2005. Automated generalization of phrasal para-
phrases from the web. In Proceedings of the IJCNLP
Workshop on Paraphrasing, Jeju Island, South Korea.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are multiple reference
translations necessary? investigating the value of
paraphrased reference translations in parameter opti-
mization. In Proceedings of AMTA, Waikiki, USA.
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia?s Revision History. In Proceedings of
LREC 2010, Valletta, Malta.
Aure?lien Max and Michael Zock. 2008. Looking up
phrase rephrasings via a pivot language. In Proceed-
ings of the COLING Workshop on Cognitive Aspects
of the Lexicon, Manchester, United Kingdom.
Aure?lien Max. 2004. From controlled document au-
thoring to interactive document normalization. In Pro-
ceedings of COLING, Geneva, Switzerland.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2).
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In Proceedings of ACL, Prague, Czech
Republic.
Andrew Mutton. 2006. Evaluation of sentence grammat-
icality using Parsers and a Support Vector Machine.
Ph.D. thesis, Macquarie University.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of ACL, Short Papers ses-
sion, Uppsala, Sweden.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP, Barcelona,
Spain.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin B. Bederson. 2010. Im-
proving translation via targeted paraphrasing. In Pro-
ceedings of EMNLP, Cambridge, MA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word lattices for multi-source translation. In Proceed-
ings of EACL, Athens, Greece.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Kuansan Wang, Chris Thrasher, Evelyne Viegas, Xiao-
long Li, and Bo-june (Paul) Hsu. 2010. An Overview
of Microsoft Web N-gram Corpus and Applications.
In Proceedings of the NAACL-HLT Demonstration
Session, Los Angeles, USA.
Shiqi Zhao, Ting Liu, Xincheng Yuan, Sheng Li, and
Yu Zhang. 2007. Automatic acquisition of context-
specific lexical paraphrases. In Proceedings of IJCAI
2007, Hyderabad, India.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint ACL-IJCNLP, Singapore.
Shiqi Zhao, Haifeng Wang, Ting Liu, , and Sheng Li.
2010. Leveraging multiple mt engines for paraphrase
generation. In Proceedings of COLING, Beijing,
China.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of COLING,
Beijing, China.
19
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137?142,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
CMUQ@QALB-2014: An SMT-based System
for Automatic Arabic Error Correction
Serena Jeblee
1
, Houda Bouamor
2
, Wajdi Zaghouani
2
and Kemal Oflazer
2
1
Carnegie Mellon University
sjeblee@cs.cmu.edu
2
Carnegie Mellon University in Qatar
{hbouamor,wajdiz}@qatar.cmu.edu, ko@cs.cmu.edu
Abstract
In this paper, we describe the CMUQ sys-
tem we submitted to The ANLP-QALB 2014
Shared Task on Automatic Text Correction
for Arabic. Our system combines rule-based
linguistic techniques with statistical language
modeling techniques and machine translation-
based methods. Our system outperforms the
baseline and reaches an F-score of 65.42% on
the test set of QALB corpus. This ranks us 3rd
in the competition.
1 Introduction
The business of text creation and editing represents a
large market where NLP technologies might be applied
naturally (Dale, 1997). Today?s users of word proces-
sors get surprisingly little help in checking spelling,
and a small number of them use more sophisticated
tools such as grammar checkers, to provide help in en-
suring that a text remains grammatically accurate after
modification. For instance, in the Arabic version of Mi-
crosoft Word, the spelling checker for Arabic, does not
give reasonable and natural proposals for many real-
word errors and even for simple probable errors (Had-
dad and Yaseen, 2007).
With the increased usage of computers in the pro-
cessing of natural languages comes the need for cor-
recting errors introduced at different stages. Natu-
ral language errors are not only made by human op-
erators at the input stage but also by NLP systems
that produce natural language output. Machine trans-
lation (MT), or optical character recognition (OCR),
often produce incorrect output riddled with odd lexi-
cal choices, grammar errors, or incorrectly recognized
characters. Correcting human/machine-produced er-
rors, or post-editing, can be manual or automated. For
morphologically and syntactically complex languages,
such as Modern Standard Arabic (MSA), correcting
texts automatically requires complex human and ma-
chine processing which makes generation of correct
candidates a challenging task.
For instance, the Automatic Arabic Text Correction
Shared Task is an interesting testbed to develop and
evaluate spelling correction systems for Arabic trained
either on naturally occurring errors in texts written by
humans (e.g., non-native speakers), or machines (e.g.,
MT output). In such tasks, participants are asked to
implement a system that takes as input Modern Stan-
dard Arabic texts with various spelling errors and au-
tomatically correct them. In this paper, we describe
the CMUQ system we developed to participate in the
The First Shared Task on Automatic Text Correction
for Arabic (Mohit et al., 2014). Our system combines
rule-based linguistic techniques with statistical lan-
guage modeling techniques and machine translation-
based methods. Our system outperforms the baseline,
achieves a better correction quality and reaches an F-
score of 62.96% on the development set of QALB cor-
pus (Zaghouani et al., 2014) and 65.42% on the test set.
The remainder of this paper is organized as follows.
First, we review the main previous efforts for automatic
spelling correction, in Section 2. In Section 3, we de-
scribe our system, which consists of several modules.
We continue with our experiments on the shared task
2014 dev set (Section 4). Then, we give an analysis of
our system output in Section 5. Finally, we conclude
and hint towards future improvement of the system, in
Section 6.
2 Related Work
Automatic error detection and correction include auto-
matic spelling checking, grammar checking and post-
editing. Numerous approaches (both supervised and
unsupervised) have been explored to improve the flu-
ency of the text and reduce the percentage of out-
of-vocabulary words using NLP tools, resources, and
heuristics, e.g., morphological analyzers, language
models, and edit-distance measure (Kukich, 1992;
Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan
et al., 2003; Haddad and Yaseen, 2007; Hassan et al.,
2008; Habash, 2008; Shaalan et al., 2010). There has
been a lot of work on error correction for English (e.g.,
(Golding and Roth, 1999)). Other approaches learn
models of correction by training on paired examples
of errors and their corrections, which is the main goal
of this work.
For Arabic, this issue was studied in various direc-
tions and in different research work. In 2003, Shaalan
et al. (2003) presented work on the specification and
classification of spelling errors in Arabic. Later on,
Haddad and Yaseen (2007) presented a hybrid ap-
proach using morphological features and rules to fine
137
tune the word recognition and non-word correction
method. In order to build an Arabic spelling checker,
Attia et al. (2012) developed semi-automatically, a dic-
tionary of 9 million fully inflected Arabic words us-
ing a morphological transducer and a large corpus.
They then created an error model by analyzing error
types and by creating an edit distance ranker. Finally,
they analyzed the level of noise in different sources of
data and selected the optimal subset to train their sys-
tem. Alkanhal et al. (2012) presented a stochastic ap-
proach for spelling correction of Arabic text. They used
a context-based system to automatically correct mis-
spelled words. First of all, a list is generated with pos-
sible alternatives for each misspelled word using the
Damerau-Levenshtein edit distance, then the right al-
ternative for each misspelled word is selected stochas-
tically using a lattice search, and an n-gram method.
Shaalan et al. (2012) trained a Noisy Channel Model
on word-based unigrams to detect and correct spelling
errors. Dahlmeier and Ng (2012a) built specialized de-
coders for English grammatical error correction. More
recently, (Pasha et al., 2014) created MADAMIRA,
a system for morphological analysis and disambigua-
tion of Arabic, this system can be used to improve the
accuracy of spelling checking system especially with
Hamza spelling correction.
In contrast to the approaches described above, we
use a machine translation (MT) based method to train
an error correction system. To the best of our knowl-
edge, this is the first error correction system for Arabic
using an MT approach.
3 Our System
Our system is a pipeline that consists of several dif-
ferent modules. The baseline system uses a spelling
checking module, and the final system uses a phrase-
based statistical machine translation system. To
preproces the text, we use the provided output of
MADAMIRA (Pasha et al., 2014) and a rule-based
correction. We then do a rule-based post-processing
to fix the punctuation.
3.1 Baseline Systems
For the baseline system, we try a common spelling
checking approach. We first pre-process the data us-
ing the features from MADAMIRA (see Feature 14
Replacement), then we use a noisy channel model for
spelling checking.
Feature 14 Replacement
The first step in the pipeline is to extract
MADAMIRA?s 14th feature from the .column file
and replace each word in the input text with this form.
MADAMIRA uses morphological disambiguation and
SVM analysis to select the most likely fully diacritized
Arabic word for the input word. The 14th feature
represents the undiacritized form of the most likely
word. This step corrects many Hamza placement or
omission errors, which makes a good base for other
correction modules.
Spelling Correction
The spelling checker is based on a noisy channel model
- we use a word list and language model to determine
the most probable correct Arabic word that could have
generated the incorrect form that we have in the text.
For detecting spelling errors we use the AraComLex
word list for spelling checking (Attia et al., 2012),
which contains about 9 million Arabic words.
1
We
look up the word from the input sentence in this list,
and attempt to correct those that are not found in the
list. We also train a mapping of incorrect words and
possible corrections from the edits in the training data.
If the word is in this map, the list of possible correc-
tions from the training data becomes the candidate list.
If the word is not in the trained map, the candidate list
is created by generating a list of words with common
insertions, substitutions, and deletions, according to the
list in (Attia et al., 2012). Each candidate is generated
by performing these edits and has a weight according to
the edit distance weights in the list. We then prune the
candidate list by keeping only the lowest weight words,
and removing candidates that are not found in the word
list. The resulting sentence is scored with a 3-gram lan-
guage model built with KenLM (Heafield et al., 2013)
on the correct side of the training data. The top one
sentence is then kept and considerd as the ?corrected?
one.
This module handles spelling errors of individual
words; it does not handle split/merge errors or word
reordering. The spelling checker sometimes attempts
to correct words that were already correct, because
the list does not contain named entities or translitera-
tions, and it does not contain all possible correct Arabic
words. Because the spelling checker module decreased
the overall performance, it is not included in our final
system.
3.2 Final System
Feature 14 Replacement
The first step in our final system is Feature 14 Replace-
ment, as described above.
Rule-based Clitic Correction
With the resulting data, we apply a set of rules to reat-
tach clitics that may have been split apart from the base
word. After examining the train dataset, we realized
that 95% of word merging cases involve ??? attach-
ment. When found by themselves, the clitics are at-
tached to either the previous word or next word, based
on whether they generally appear as prefixes or suf-
fixes. The clitics handled by this module are specified
in Table 2.
We also remove extra characters by replacing a se-
quence of 3 or more of the same character with a single
1
http://sourceforge.net/projects/
arabic-wordlist/
138
Dev
Exact Match No Punct
Precision Recall F1 Precision Recall F1
Feature 14 0.7746 0.3210 0.4539 0.8100 0.5190 0.6326
Feature 14 + Spelling checker (baseline) 0.4241 0.3458 0.3810 0.4057 0.4765 0.4382
Feature 14 + Clitic Rules 0.7884 0.3642 0.4983 0.8149 0.5894 0.6841
Feature 14 + Phrase-based MT 0.7296 0.5043 0.5964 0.7797 0.6397 0.7028
Feature 14 + Clitic Rules + Phrase-based MT 0.7571 0.5389 0.6296 0.8220 0.6850 0.7473
Test
Feature 14 + Clitic Rules + Phrase-based MT 0.7797 0.5635 0.6542 0.7438 0.6855 0.7135
Table 1: System results on the dev set (upper part) and on the test set (lower part).
Attach clitic to... Clitics
Beginning of next word {?, ?@, H
.
,
	
?, ?}
End of previous word {?, A?, A
	
K, ?


	
G, ?


, ??, @}
Table 2: Clitics handled by the rule-based module.
instance of that character (e.g. !!!!!!! would be replaced
with !).
Statistical Phrase-based Model
We use the Moses toolkit (Koehn et al., 2007) to
create a statistical phrase-based machine translation
model built on the best pre-processed data, as described
above. We treat this last step as a translation prob-
lem, where the source language is pre-processed in-
correct Arabic text, and the reference is correct Ara-
bic. Feature 14 extraction, rule-based correction, and
character de-duplication are applied to both the train
and dev sets. All but the last 1,000 sentences of the
train data are used at the training set for the phrase-
based model, the last 1,000 sentences of the train data
are used as a tuning set, and the dev set is used for
testing and evaluation. We use fast align, the aligner
included with the cdec decoder (Dyer et al., 2010) as
the word aligner with grow-diag as the symmetrization
heuristic (Och and Ney, 2003), and build a 5-gram lan-
guage model from the correct Arabic training data with
KenLM (Heafield et al., 2013). The system is evaluated
with BLEU (Papineni et al., 2002) and then scored for
precision, recall, and F1 measure against the dev set
reference.
We tested several different reordering window sizes
since this is not a standard translation task, so we may
want shorter distance reordering. Although 7 is the de-
fault size, we tested 7, 5, 4, 3, and 0, and found that a
window of size 4 produces the best result according to
BLEU score and F1 measure.
4 Experiments and Results
We train and evaluate our system with the train-
ing and development datasets provided for the shared
task and the m2Scorer (Dahlmeier and Ng, 2012b).
These datasets are extracted from the QALB corpus
of human-edited Arabic text produced by native speak-
ers, non-native speakers and machines (Zaghouani et
al., 2014).
We conducted a small scale statistical study on the
950K tokens training set used to build our system. We
realized that 306K tokens are affected by a correction
action which could be a word edit, insertion, deletion,
split or merge. 169K tokens were edited to correct the
spelling errors and 99K tokens were inserted (mostly
punctuation marks). Furthermore, there is a total of
6,7K non necessary tokens deleted and 10.6K attached
tokens split and 18.2 tokens merged. Finally, there are
only 427 tokens moved in the sentence and 1563 mul-
tiple correction action.
We experiment with different configurations and
reach the sweet spot of performance when combining
the different modules.
4.1 Results
To evaluate the performance of our system on the de-
velopment data, we compare its output to the reference
(gold annotation). We then compute the usual mea-
sures of precision, recall and f-measure. Results for
various system configurations on the dev and test sets
are given in Table 1. Using the baseline system con-
sisting in replacing words by their non diacritized form
(Feature 14), we could correct 51.9% of the errors oc-
curring in the dev set, when punctuation is not consid-
ered. This result drops when we consider the punctua-
tion errors which seem to be more complex to correct:
Only 32.1% of the errors are corrected in the dev set. It
is important to notice that adding the clitic rules to the
Feature 14 baseline yields an improvement of + 5.15 in
F-measure. We reach the best F-measure value when
using the phrase-based MT system after pre-processing
the data and applying the Feature 14 and clitic rules.
Using this combination we were able to correct 68.5%
of the errors (excluding punctuation) on the develop-
ment set with a precision of 82.2% and 74.38% on the
test set. When we consider the punctuation, 53.89%
of the errors of different types were corrected on the
dev set and 56.35% on the test set with a precision of
75.71% and 77.97%, respectively.
139
5 Error Analysis and Discussion
When building error correction systems, minimizing
the number of cases where correct words are marked
as incorrect is often regarded as more important than
covering a high number of errors. Therefore, a higher
precision is often preferred over higher recall. In order
to understand what was affecting the performance, we
took a closer look at our system output and translation
tables to present some samples of errors that our system
makes on development set.
5.1 Out-of-vocabulary Words
This category includes words that are not seen by our
system during the training which is a common problem
in machine translation systems. In our system, most of
out-of-vocabulary words were directly transferred un-
changed from source to target. For example the word

?J


??

????
	
? @ was not corrected to

?J


??

???
?
@.
5.2 Unnecessary Edits
In some cases, our system made some superfluous edits
such as adding the definite article in cases where it is
not required such as :
Source

?
	
JK


Y?
?
@
	
?AJ


?

@
Hypothesis

?
	
JK


Y?
?
@
	
?AJ


?

B@
Reference (unchanged)

?
	
JK


Y?
?
@
	
?AJ


?

@
Table 3: An example of an unnecessary addition of the
definite article.
5.3 Number Normalization
We observed that in some cases, the system did not nor-
malize the numbers such as in the following case which
requires some knowledge of the real context to under-
stand that these numbers require normalization.
Source

H@?A
	
?J


? 450000
Hypothesis

H@?A
	
?J


? 450000
Reference

H@?A
	
?J


? 450
Table 4: An example of number normalization.
5.4 Hamza Spelling
Even though our system corrected most of the Hamza
spelling errors, we noticed that in certain cases they
were not corrected, especially when the words without
the Hamza were valid entries in the dictionary. These
cases are not always easy to handle since only context
and semantic rules can handle them.
5.5 Grammatical Errors
In our error analysis we encountered many cases of un-
corrected grammatical errors. The most frequent type
Source

?J


	
J???@ X@?
Hypothesis

?J


	
J???@ X@?
Reference

?J


	
J???@ X

@?
Table 5: A sentence where the Hamza was not added
above the Alif in the first word because both versions
are valid dictionary entries.
is the case endings correction such as correcting the
verbs in jussive mode when there is a prohibition par-
ticle (negative imperative) like the (B) in the following
examples :
Source ??E


XAK



@
??
? @?K
.
Q?
	
?


B
Hypothesis ??E


XAK



@
??
? @?K
.
Q?
	
?


B
Reference ??E


XAK



@
??
?
	
??K
.
Q?
	
?


B
Table 6: An example of a grammatical error.
5.6 Unnecessary Word Deletion
According to the QALB annotation guidelines, ex-
tra words causing semantic ambiguity in the sentence
should be deleted. The decision to delete a given word
is usually based on the meaning and the understanding
of the human annotator, unfortunately this kind of er-
rors is very hard to process and our system was not able
to delete most of the unnecessary words.
Source Q
	
k

@ A
	
J



?? A??E


YK



@ A?
	
?? Y?D

?
	
J? ??
Hypothesis Q
	
k

@ A
	
J



?? A??E


YK



@ A?
	
?? Y?D

?
	
J? ??
Reference Q
	
k

@ A
	
J



?? A?
	
?? Y?D

?
	
J? ??
Table 7: An example of word deletion.
5.7 Adding Extra Words
Our analysis revealed cases of extra words introduced
to some sentences, despite the fact that the words added
are coherent with the context and could even improve
the overall readability of the sentence, they are uncred-
ited correction since they are not included in the gold
standard. For example :
Source ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Hypothesis Qm
?
'@ ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Reference ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Table 8: An example of the addition of extra words.
5.8 Merge and Split Errors
In this category, we show some sample errors of neces-
sary word splits and merge not done by our system. The
140
word Y?K
.
A???
	
k should have been split as Y?K
.
A???
	
k
and the word YK
.
B should have been merged to appear
as one word as in YK
.
B.
5.9 Dialectal Correction Errors
Dialectal words are usually converted to their Modern
Standard Arabic (MSA) equivalent in the QALB cor-
pus, since dialectal words are rare, our system is unable
to detect and translate the dialectal words to the MSA
as in the expression
	
?K


	
P I
.
? that is translated in the
gold standard to
	
?K


	
P
Q



	
?.
6 Conclusion
We presented our CMUQ system for automatic Ara-
bic text correction. Our system combines rule-based
linguistic techniques with statistical language model-
ing techniques and a phrase-based machine transla-
tion method. We experiment with different configu-
rations. Our experiments have shown that the system
we submitted outperforms the baseline and we reach
an F-score of 74.73% on the development set from
the QALB corpus when punctuation is excluded, and
65.42% on the test set when we consider the punctu-
ation errors . This placed us in the 3rd rank. We be-
lieve that our system could be improved in numerous
ways. In the future, we plan to finalize a current mod-
ule that we are developing to deal with merge and split
errors in a more specific way. We also want to focus in
a deeper way on the word movement as well as punc-
tuation problems, which can produce a more accurate
system. We will focus as well on learning further error
correction models from Arabic Wikipedia revision his-
tory, as it contains natural rewritings including spelling
corrections and other local text transformations.
Acknowledgements
This publication was made possible by grants NPRP-
09-1140-1-177 and NPRP-4-1058- 1-168 from the
Qatar National Research Fund (a member of the Qatar
Foundation). The statements made herein are solely the
responsibility of the authors.
References
Mohamed I. Alkanhal, Mohamed Al-Badrashiny, Man-
sour M. Alghamdi, and Abdulaziz O. Al-Qabbany.
2012. Automatic Stochastic Arabic Spelling Correc-
tion With Emphasis on Space Insertions and Dele-
tions. IEEE Transactions on Audio, Speech & Lan-
guage Processing, 20(7):2111?2122.
Mohammed Attia, Pavel Pecina, Younes Samih,
Khaled Shaalan, and Josef van Genabith. 2012. Im-
proved Spelling Error Detection and Correction for
Arabic. In Proceedings of COLING 2012: Posters,
pages 103?112, Mumbai, India.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A Beam-
Search Decoder for Grammatical Error Correction.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 568?578, Jeju Island, Korea.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Bet-
ter Evaluation for Grammatical Error Correction. In
NAACL HLT ?12 Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 568?572.
Robert Dale. 1997. Computer Assistance in Text Cre-
ation and Editing. In Survey of the state of the art
in Human Language Technology, chapter 7, pages
235?237. Cambridge University Press.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A Decoder, Alignment, and Learning Frame-
work for Finite-state and Context-free Translation
Models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7?12, Uppsala, Sweden.
A. R. Golding and D. Roth. 1999. A Winnow Based
Approach to Context-Sensitive Spelling Correction.
Machine Learning, 34(1-3):107?130.
Nizar Habash. 2008. Four Techniques for Online Han-
dling of Out-of-Vocabulary Words in Arabic-English
Statistical Machine Translation. In Proceedings of
ACL-08: HLT, Short Papers, pages 57?60, Colum-
bus, Ohio.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-words in Arabic: a Hybrid
Approach. International Journal of Computer Pro-
cessing of Oriental Languages, 20(04):237?257.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction using
Finite State Automata. In Proceedings of the Third
International Joint Conference on Natural Language
Processing (IJCNLP 2008), pages 913?918, Hyder-
abad, India.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modfied
Kneser-Ney Language Model Estimation. In In Pro-
ceedings of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
141
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Surveys
(CSUR), 24(4):377?439.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. In Computational Linguistics, page 1951.
Kemal Oflazer. 1996. Error-Tolerant Finite-State
Recognition with Applications to Morphological
Analysis and Spelling Correction. Computational
Linguistics, 22(1):73?89.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the Association for Computational Linguis-
tics, Philadelphia, Pennsylvania.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC?14), pages 1094?1101, Reykjavik, Iceland.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Proceedings of the 4th Conference on Lan-
guage Engineering, Egyptian Society of Language
Engineering (ELSE), Cairo, Egypt.
Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An
Approach for Analyzing and Correcting Spelling Er-
rors for Non-native Arabic Learners. In Proceedings
of The 7th International Conference on Informatics
and Systems, INFOS2010, the special track on Nat-
ural Language Processing and Knowledge Mining,
pages 28?30, Cairo, Egypt.
Khaled Shaalan, Mohammed Attia, Pavel Pecina,
Younes Samih, and Josef van Genabith. 2012.
Arabic Word Generation and Modelling for Spell
Checking. In Proceedings of the Eighth Inter-
national Conference on Language Resources and
Evaluation (LREC-2012), pages 719?725, Istanbul,
Turkey.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines
and Framework. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC?14), Reykjavik, Iceland.
Chiraz Zribi and Mohammed Ben Ahmed. 2003. Ef-
ficient Automatic Correction of Misspelled Arabic
Words Based on Contextual Information. In Pro-
ceedings of the Knowledge-Based Intelligent Infor-
mation and Engineering Systems Conference, pages
770?777, Oxford, UK.
142
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196?206,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Domain and Dialect Adaptation
for Machine Translation into Egyptian Arabic
Serena Jeblee
1
, Weston Feely
1
, Houda Bouamor
2
Alon Lavie
1
, Nizar Habash
3
and Kemal Oflazer
2
1
Carnegie Mellon University
{sjeblee, wfeely, alavie}@cs.cmu.edu
2
Carnegie Mellon University in Qatar
hbouamor@qatar.cmu.edu, ko@cs.cmu.edu
3
New York University Abu Dhabi
nizar.habash@nyu.edu
Abstract
In this paper, we present a statistical ma-
chine translation system for English to Di-
alectal Arabic (DA), using Modern Stan-
dard Arabic (MSA) as a pivot. We cre-
ate a core system to translate from En-
glish to MSA using a large bilingual par-
allel corpus. Then, we design two separate
pathways for translation from MSA into
DA: a two-step domain and dialect adap-
tation system and a one-step simultane-
ous domain and dialect adaptation system.
Both variants of the adaptation systems are
trained on a 100k sentence tri-parallel cor-
pus of English, MSA, and Egyptian Arabic
generated by a rule-based transformation.
We test our systems on a held-out Egyp-
tian Arabic test set from the 100k sen-
tence corpus and we achieve our best per-
formance using the two-step domain and
dialect adaptation system with a BLEU
score of 42.9.
1 Introduction
While MSA is the shared official language of cul-
ture, media and education in the Arab world, it is
not the native language of any speakers of Ara-
bic. Most native speakers are unable to produce
sustained spontaneous discourse in MSA - they
usually resort to repeated code-switching between
their dialect and MSA (Abu-Melhim, 1991). Ara-
bic speakers are quite aware of the contextual fac-
tors and the differences between their dialects and
MSA, although they may not always be able to
pinpoint exact linguistic differences. In the con-
text of natural language processing (NLP), some
Arabic dialects have started receiving increas-
ing attention, particularly in the context of ma-
chine translation (Zbib et al., 2012; Salloum and
Habash, 2013; Salloum et al., 2014; Al-Mannai
et al., 2014) and in terms of data collection (Cot-
terell and Callison-Burch, 2014; Bouamor et al.,
2014; Salama et al., 2014) and basic enabling
technologies (Habash et al., 2012; Pasha et al.,
2014). However, the focus is on a small number
of iconic dialects, (e.g., Egyptian). The Egyptian
media industry has traditionally played a dominant
role in the Arab world, making the Egyptian di-
alect the most widely understood and used dialect.
DA is now emerging as the language of informal
communication online. DA differs phonologically,
lexically, morphologically, and syntactically from
MSA. And while MSA has an established stan-
dard orthography, the dialects do not: people write
words reflecting their phonology and sometimes
use roman script. Thus, MSA tools cannot ef-
fectively model DA; for instance, over one-third
of Levantine verbs cannot be analyzed using an
MSA morphological analyzer (Habash and Ram-
bow, 2006). These differences make the direct use
of MSA NLP tools and applications for handling
dialects impractical.
In this work, we design an MT system for En-
glish to Egyptian Arabic translation by using MSA
as an intermediary step. This includes different
challenges from those faced when translating into
English. Because MSA is the formal written va-
riety of Arabic, there is an abundance of written
data, including parallel corpora from sources like
the United Nations and newspapers, as well as var-
ious treebanks. Using these resources, many re-
searchers have created fairly reliable MSA trans-
lation systems. However, these systems are not
designed to deal with the other Arabic variants.
Egyptian Arabic is much closer to MSA than
it is to English, so one can get a system bet-
196
ter performance by translating first into MSA and
then translating from MSA to Egyptian Arabic,
which are far more similar. Our approach consists
of a core MT system trained on a large amount
of out-of-domain English-MSA parallel data, fol-
lowed by an adaptation system. We design and im-
plement two adaptation systems: a two-step sys-
tem first adapts to in-domain MSA and then sep-
arately adapts from MSA to Egyptian Arabic, and
a one-step system that adapts directly from out-of-
domain MSA to in-domain Egyptian Arabic.
Our research contributions are summarized as
follows:
(a) We build a machine translation system to
translate into, rather than out of, dialectal Ara-
bic (from English), using MSA as a pivot
point.
(b) We apply a domain adaptation technique to
improve the MSA results on our in-domain
dataset.
(c) We automatically generate the Egyptian side
of a 100k tri-parallel corpus covering MSA,
English and Egyptian.
(d) We use this domain adaptation technique to
adapt MSA into dialectal Arabic.
The remainder of this paper is structured as fol-
lows. We first review the main previous efforts
for dealing with DA in NLP, in Section 2. In Sec-
tion 3,we give a general description about using
phrase-based MT as an adaptation system. Sec-
tion 4 presents the dataset used in the different ex-
periments. Our approach for translating English
text into Egyptian Arabic is explained in Section 5.
Section 6 presents our experimental setup and the
results obtained. Then, we give an analysis of our
system output in Section 7. Finally, we conclude
and describe our future work in Section 8.
2 Related work
Machine translation (MT) for dialectal Arabic
(DA) is quite challenging given the limited re-
sources to build rule-based models or train statis-
tical models for MT. While there has been a con-
siderable amount of work in the context of stan-
dard Arabic NLP (Habash, 2010), DA is impov-
erished in terms of available tools and resources
compared to MSA, e.g., there are few parallel DA-
English corpora (Zbib et al., 2012; Bouamor et
al., 2014). The majority of DA resources are for
speech recognition, although more and more re-
sources for machine translation and enabling tech-
nologies such as morphological analyzers are be-
coming available for specific dialects (Habash et
al., 2012; Habash et al., 2013).
For Arabic and its dialects, several researchers
have explored the idea of exploiting existing MSA
rich resources to build tools for DA NLP. Differ-
ent research work successfully translated DA to
MSA as a bridge to translate to English (Sawaf,
2010; Salloum and Habash, 2013), or to enhance
the performance of Arabic-based information re-
trieval systems (Shatnawi et al., 2012). Among the
efforts on translation from DA to MSA, Abo Bakr
et al. (2008) introduced a hybrid approach to trans-
fer a sentence from Egyptian Arabic to MSA.
Sajjad et al. (2013) used a dictionary of Egyp-
tian/MSA words to transform Egyptian to MSA
and showed improvement in the quality of ma-
chine translation. A similar but rule-based work
was done by Mohamed et al. (2012). Boujel-
bane et al. (2013) and Hamdi et al. (2014) built
a bilingual dictionary using explicit knowledge
about the relation between Tunisian Arabic and
MSA. These works are limited to a dictionary or
rules that are not available for all dialects. Zbib
et al. (2012) used crowdsourcing to translate sen-
tences from Egyptian and Levantine into English,
and thus built two bilingual corpora. The dialec-
tal sentences were selected from a large corpus
of Arabic web text. Then, they explored several
methods for dialect/English MT. Their best Egyp-
tian/English system was trained on dialect/English
parallel data. They argued that differences in genre
between MSA and DA make bridging through
MSA of limited value. For this reason, while piv-
oting through MSA, it is important to consider the
domain and add an additional step: domain adap-
tation.
The majority of previous efforts in DA MT has
been focusing on translating from dialectal Arabic
into other languages (mainly MSA or English). In
contrast, in this work we focus on building a sys-
tem to translate from English and MSA into DA.
Furthermore, to the best of our knowledge, this is
the first work in which we adapt the domain in ad-
dition to the dialect (Egyptian specifically).
3 Using Phrase-Based MT as an
Adaptation System
For commercial use, MT output is usually post-
edited by a human translator in order to fix the er-
rors generated by the MT system. This is often
faster and cheaper than having a human translate
197
the document from scratch. However, we can ap-
ply statistical phrase-based MT to create an auto-
matic machine post-editor (what we refer to in this
paper as an adaptation system) to improve the out-
put of an MT system, and make it more closely
resemble the references. Simard et al. (2007) used
a phrase-based MT system as an automatic post-
editor for the output of a commercial rule-based
MT system, showing that it produced better results
than both the rule-based system alone and a sin-
gle pass phrase-based MT system. This technique
is also useful for adapting to a specific domain or
dataset. Isabelle et al. (2007) used a statistical MT
system to automatically post-edit the output of a
generic rule-based MT system, to avoid manually
customizing a system dictionary and to reduce the
amount of manual post-editing required.
For our adaptation systems, we build a core
phrase-based MT system with a large amount of
out-of-domain data, which allows us to have better
coverage of the target language. For an adaptation
system, we then build a second phrase-based MT
system by translating the in-domain train, tune,
and test sets through the core translation system,
then using that data to build the second system.
This system uses only in-domain data: parallel
MT output from the core and the references. In
this system, instead of learning to translate one
language into another, the model learns to trans-
late erroneous MT output into more fluent output
of the same language, which more closely resem-
bles the references.
In this work, we apply this technique for
domain and dialect adaptation, treating Egyp-
tian Arabic as the target language, and the MT-
generated MSA as the erroneous MT output. We
use this approach to adapt to the domain of the
MSA data, and also to adapt to the Egyptian di-
alect. What we refer to as the ?one-step? system is
a core system plus one adaptation system, whereas
the ?two-step? system consists of the core plus two
subsequent adaptation systems. We describe the
systems in more detail in Section 5.
4 Data
For the core English to MSA system, we use
the 5 million parallel sentences of English and
MSA from NIST 2012 as the training set. The
tuning set consists of 1,356 sentences from the
NIST 2008 Open Machine Translation Evalua-
tion (MT08) data (NIST Multimodal Information
Group, 2010a), and the test set consists of 1,313
sentences from NIST MT09 (NIST Multimodal
Information Group, 2010b).
We use a 5-gram MSA language model built
using the SRILM toolkit (Stolcke, 2002) on 260
million words of MSA from the Arabic Gigaword
(Parker et al., 2011). All our MSA parallel data
and monolingual MSA language modeling data
were tokenized with MADA v3.1 (Habash and
Rambow, 2005) using the ATB (Arabic Treebank)
tokenization scheme.
For the adaptation systems, we build a 100k
tri-parallel corpus Egyptian-MSA-English corpus.
The MSA and English parts are extracted from
the NIST corpus distributed by the Linguistic Data
Consortium. The Egyptian sentences are obtained
automatically by extending Mohamed et al. (2012)
method for generating Egyptian Arabic from mor-
phologically disambiguated MSA sentences. This
rule-based method relies on 103 transformation
rules covering essentially nouns, verbs and pro-
nouns as well as certain lexical items. For each
MSA sentence, this method provides more than
one possible candidate, in its original version, the
Egyptian sentence kept was chosen randomly. We
extend the selection algorithm by scoring the dif-
ferent sentences using a language model. For
this, we use SRILM with modified Kneser-Ney
smoothing to build a 5-gram language model. The
model is trained on a corpus including articles ex-
tracted from the Egyptian version of Wikipedia
1
and the Egyptian side of the AOC corpus (Zaidan
and Callison-Burch, 2011). We chose to include
Egyptian Wikipedia for the formal level of sen-
tences in it different from the regular DA written
in blogs or microblogging websites (e.g., Twitter)
and closer to the ones generated by our system.
We split this data into train, tune, and test sets
of 98,027, 960, and 961 sentences respectively,
after removing duplicates across sets. The MSA
corpus was tokenized using MADA and the Egyp-
tian Arabic data was tokenized with MADA-ARZ
v0.4 (Habash et al., 2013), both using the ATB to-
kenization scheme, with alif/ya normalization.
5 System Design
Figure 1 shows a diagram of our three English to
Egyptian Arabic MT systems: (1) the baseline MT
system, (2) the one-step adaptation MT system,
and (3) the two-step adaptation MT system. We
describe each system below.
1
http://arz.wikipedia.org/
198
System Design
Egyptian ArabicEnglish
English
English Egyptian Arabic
Egyptian Arabic
MSA
Translation
Translation
Translation
Domain & Dialect Adaptation
Domain Adaptation Dialect AdaptationIn-domain MSAMSA
100K sent.
100K sent. 100K sent.
100K sent.5M sent.
5M sent.
Baseline MT System
One-Step Adaptation MT System
Two-Step Adaptation MT System
Figure 1: An overview of the different system architectures.
Baseline System
Our baseline system is a single phrase-based En-
glish to Egyptian Arabic MT system, built using
Moses (Koehn et al., 2007) on the 100k corpus de-
scribed in Section 4. This system does not include
any MSA data, nor does it have an adaptation sys-
tem; it is a typical, one-pass MT system that trans-
lates English directly into Egyptian Arabic. We
will show that using adaptation systems improves
the results significantly.
Core System
We base our systems on a core system built us-
ing Moses with the NIST data, a large amount of
parallel English-MSA data from different sources
than our in-domain data (the 100k dataset). Our
core system is also built using Moses. We use
this core system to translate the English side of our
100k train, tune, and test sets into MSA, the output
of which we refer to as MSA?. This MSA? data is
what we use as the source side for the adaptation
systems.
One-Step Adaptation System
To adapt to the domain and dialect of the 100k cor-
pus, we first build a single adaptation system that
translates the MSA? output of the core directly into
Egyptian Arabic using the 100k corpus. The train-
ing data consists of parallel MSA? (the output of
the core) and the Egyptian Arabic from the 100k
dataset. With this system, we can take an English
test set, translate it through the core to get MSA?
output, which we can translate through the adap-
tation system to get Egyptian Arabic.
Two-Step Adaptation System
We also build a two-step adaptation system that
consists of two adaptation steps: one to adapt the
MSA output of the core system to the domain of
the MSA in the 100k corpus, and a second system
to translate the MSA output of the domain adap-
tation system into Egyptian Arabic. We use the
first adaptation system to translate the MSA? train,
tune, and test sets (the output of the core, which is
out-of-domain MSA), into in-domain MSA. This
system is trained on the MSA? output parallel with
the MSA references from the 100k dataset. We
refer to the output of this system as MSA?, be-
cause it has been translated from English into out-
of-domain MSA (MSA?), and then from out-of-
domain MSA to in-domain MSA.
The first adaptation system is used to translate
the MSA? train, tune, and test sets into MSA?.
Then we use these MSA? sets with their parallel
Egyptian Arabic from the 100k dataset to build the
second adaptation system from in-domain MSA to
Egyptian Arabic. We do not use the dialect trans-
formation from (Mohamed et al., 2012) because it
is designed to work with gold-standard annotation
of the MSA text, which we do not have.
System Variants
Since MSA and Egyptian are more similar to each
other than they are to English, we tried several dif-
ferent reordering window sizes to find the optimal
reordering distance for adapting MSA to Egyptian
Arabic, including the typical reordering window
of length 7, a smaller window of length 4, and no
reordering at all. We found a reordering window
199
size of 7 to work best for all our systems, except
for the one-step adaptation system, where no re-
ordering produced the best result.
We also tested two different heuristics for sym-
metrizing the word alignments: grow-diag and
grow-diag-final-and (Och and Ney, 2003). We
found that using grow-diag as our symmetriza-
tion heuristic produced slightly better scores on
the 100k datasets. For the baseline and adaptation
systems we built 5-gram language models with
KenLM (Heafield et al., 2013) using the target side
of the training set, and for the core system we used
the large MSA language model described in sec-
tion 4. We use KenLM because it has been shown
(Heafield, 2011) to be faster and use less memory
than SRILM (Stolcke, 2002) and IRSTLM (Fed-
erico et al., 2008).
6 Evaluation and Results
For evaluation we use multeval (Clark et al.,
2011) to calculate BLEU (Papineni et al.,
2002), METEOR (Denkowski and Lavie, 2011),
TER (Snover et al., 2006), and length of the test set
for each system. We evaluate the core and adap-
tation systems on the MSA and Egyptian sides of
the test set drawn from the 100k corpus, which we
refer to as the 100k sets. The data used for evalua-
tion is a genuine Egyptian Arabic generated from
MSA, just like the data the systems were trained
on. It is not practical to evaluate on naturally-
generated Egyptian Arabic in this case because the
domain of our datasets is very formal, since most
of the text comes from news sources, and dialectal
Arabic is generally used in informal situations.
2
Below we report BLEU scores from our evalua-
tion using tokenized and detokenized system out-
put. We separate our results into the baseline sys-
tem results, the results of the core, the results of
the adaptation systems, and a comparison section.
We specify scores of intermediate system output,
such as MSA, as BLEU (A), and the scores of fi-
nal system output as BLEU (B). For error analysis,
we use METEOR X-ray (Denkowski and Lavie,
2011) to visualize the alignments of our system
results with the references and each other.
For all MT systems we used grow-diag as our
symmetrization heuristic. For each system, we re-
port only the BLEU score of the best reordering
window variant, which is specified in the caption
2
It is important to note that the Egyptian Arabic data we
use is more MSA-like than typical Egyptian because it was
generated directly from MSA.
below each table. The difference in scores be-
tween the different reordering window sizes (7, 4,
and 0) we tried for the adaptation systems was not
large (between 0 and 0.7 BLEU). In the following
tables we present the best results for each adapta-
tion system, which is a reordering window size of
7 for all systems, except for the phrase-based one-
step domain and dialect adaptation system, which
performs better with no reordering (0.2 BLEU bet-
ter than a window of 7, 0.6 BLEU better than a
window of 4), but these small differences in BLEU
scores are within noise. The greatest difference
in scores from the reordering windows was in the
two-step systems domain adaptation step (MSA to
MSA) on top of the phrase-based core, where a re-
ordering window of 7 was 0.7 BLEU better than a
window of 0.
6.1 Baseline System Results
BLEU (B)
Tokenized Detokenized
100k EGY Tune 22.6 22.3
100k EGY Test 21.5 21.1
Table 1: Baseline results (English ? EGY) with a
reordering window size of 7.
The baseline system demonstrates the results of
building a basic MT system directly from English
to Egyptian Arabic. The goal of the core and adap-
tation systems is to achieve better scores than this
initial approach.
6.2 Core System Results
In Table 2, we report BLEU scores for our core
system on its own tuning set, NIST MT08, and
NIST MT09 as a held-out MSA test set. We
also report scores on the tune and test sets used
to build our adaptation systems, both MSA and
Egyptian Arabic. This is not the final system out-
put, but rather these scores are for intermediate
output only, which becomes the input for our ada-
patation systems.
We notice that unsurprisingly the core system
performs much better on the 100k MSA test set
than on the 100k Egyptian Arabic test set, which
is to be expected because the core system is not
trained on any Egyptian Arabic data. This shows
the impact that the dialectal differences make on
MT output. The results on the Egyptian test
set here are the result of evaluating MSA output
against Egyptian Arabic references.
200
BLEU (A)
Tokenized Detokenized
NIST MT08 (Tune) 23.6 22.8
NIST MT09 (Test) 29.3 28.5
100k MSA Tune 39.8 39.3
100k MSA Test 39.4 39.0
100k EGY Tune 28.1 28.1
100k EGY Test 27.7 27.7
Table 2: Core system (English ? MSA) results
using a reordering window size of 7.
6.3 Adaptation System Results
The adaptation systems take as input the MSA out-
put of the core and attempt to improve the scores
on the Egyptian test set by adapting to the domain
of the 100k dataset, as well as to Egyptian Arabic,
in either one or two steps.
BLEU (B)
Tokenized Detokenized
100k EGY Tune 40.8 40.5
100k EGY Test 40.3 40.1
Table 3: One-Step Adaptation system (MSA? ?
Egyptian Arabic) results using a reordering win-
dow size of 0.
Table 3 shows the results of the single adapta-
tion system, which adapts directly from the MSA
output of the core to Egyptian Arabic. These
BLEU scores are already much better than the core
systems performance on the same test sets, im-
proving from 28.1 BLEU to 40.5 BLEU on the
Egyptian Arabic tuning set (a difference of 12.4
BLEU) and improving from 22.7 BLEU to 40.1
BLEU on the Egyptian Arabic test set (a differ-
ence of 17.4 BLEU).
Tables 4 and 5 below illustrate the results of the
first and second steps of the two-step adaptation
system: Table 4 contains the results of the first do-
main adaptation step from out-of-domain MSA to
in-domain MSA and Table 5 contains the results of
the second dialect adaptation step from in-domain
MSA to Egyptian Arabic.
An example of our system output for an English
sentence is given in Table 6. Its METEOR X-ray
alignment is illustrated in Figure 2.
6.4 System Comparisons on 100k Test Sets
In Table 7, we compare the results from the core
and the results from the first step of the two-step
BLEU (A)
Tokenized Detokenized
100k MSA Tune 45.2 44.6
100k MSA Test 44.8 44.2
100k EGY Tune 32.2 32.2
100k EGY Test 32.0 32.0
Table 4: Domain Adaptation system (MSA? ?
MSA?) for Two-Step Adaptation System Results
using a reordering window size of 7.
BLEU (B)
Tokenized Detokenized
100k EGY Tune 43.3 43.2
100k EGY Test 43.1 42.9
Table 5: Dialect Adaptation system (MSA? ?
Egyptian) for Two-Step Adaptation System Re-
sults using a reordering window size of 7.
? ? ????? ??????
?
????? ?????? ?????? ?? ??????? ??????
??
????? ????? ??????? ? ???????????? ? ???????????? ? ????????? ? ????????????? ????????? ? ???? ????? ? ?? ????????????? ? ????????????? ? ?????????? ? ?????Segment 314
P: 0.700 vs 0.900 : 0.200R: 0.700 vs 0.900 : 0.200Frag: 0.214 vs 0.085 : 0.129-Score: 0.550 vs 0.823 : 0.273
Figure 2: METEOR X-ray alignment of the sen-
tence in table 6. The left side is the output of the
one-step system, the right side is the output of the
two-step system, and the top is the reference. The
shaded cells represent matches between the refer-
ence and the one-step system, and the dots repre-
sent matches between the reference and the two-
step system.
adaptation system on the MSA test set and we
see that adapting to the domain improves BLEU
scores on MSA.
Since our goal is to improve the output for
1
One-Step System: Core + Domain and Dialect Adapta-
tion (MSA? ? EGY)
2
Two Step Adaptation System (Step 1): Core + Domain
Adaptation (MSA? ? MSA?)
3
Two Step Adaptation System (Step 2): Core + Domain
Adaptation (MSA??MSA?) + Dialect Adaptation (MSA??
EGY)
201
English UN closes old office in Liberia in preparation for new mission
Egyptian Reference

?YK


Yg
.

????
?
@X @Y?

J?@

?K


Q



J
.
J


? ?


	
?

?K
.
A??@ A?D
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktbhA AAlsAbq fy lybyryp AAstEdAdA lmhmp jdydp
1-Step System

?YK


Yg
.

????
?
@X @Y?

J?@ AK


Q



J
.
J


? ?


	
?

??
?

Y

??@ I
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktb AAlqdymp fy lybyryA AAstEdAdA lmhmp jdydp
2-Step System (step2)

?YK


Yg
.

????
?
@X @Y?

J?@

?K


Q



J
.
J


? ?


	
?

??
?

Y

??@ A?D
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktbhA AAlqdymp fy lybyryp AAstEdAdA lmhmp jdydp
Table 6: An example of system output from the Egyptian test set.
BLEU (A)
Tokenized Detokenized
Core (English ? MSA?) 39.4 39.0
Core + Domain Adaptation (MSA? ? MSA?) 44.8 44.2
Table 7: Comparison of results on 100k MSA test set.
BLEU (A/B)
Tokenized Detokenized
Baseline (English ? EGY) 21.5 (B) 21.1
Core (English ?MSA
?
) 27.7 (A) 27.7
One-Step Adaptation System
1
40.3 (B) 40.1
Two-Step Adaptation System (Step 1)
2
32.0 (A) 32.0
Two-Step Adaptation System (Step 2)
3
43.1 (B) 42.9
Table 8: Comparison of results on 100k EGY test data.
BLEU (B) METEOR TER Length
Baseline System 21.1 38.5 66.1 102.7
One-Step System 40.1 53.4 51.3 100.0
Two-Step System: Step 2 (Dialect) 42.9 55.2 50.4 100.1
Table 9: Detokenized BLEU, METEOR, TER, and length scores for the best system results.
Egyptian Arabic, we examine the improvement of
scores through different steps of the system in Ta-
ble 8. These scores are all based on the same
Egyptian Arabic references, even though some of
the systems are designed to produce MSA output.
It is important to note that although the first step
of the two-step adaptation system (domain adap-
tation) is still producing MSA output, it performs
better on the Egyptian test set than the out-of-
domain MSA core. The domain adaptation sys-
tem built on top of the core performs better than
the core alone on the 100k corpus MSA test set
(+5.2 BLEU), as well as the 100k corpus Egyptian
Arabic test set (+4.3 BLEU). The best score we
achieve on the 100k corpus MSA test set is 44.2
BLEU, from the core plus the domain adaptation
system.
Table 9 shows the other detokenized scores
from multeval (Clark et al., 2011) from the final
output on the EGY test set from each system, and
Table 10 shows BLEU-1 through BLEU-4 scores
on the same detokenized results, which shows an
improvement at different n-gram levels in unigram
coverage from the baseline system to the adapta-
tion systems.
Overall, the two-step adaptation system built on
top of the core performs 15.2 BLEU better than
the core alone on the 100k corpus Egyptian Ara-
bic test set and the one-step adaptation system per-
forms 12.4 BLEU better than the core on the same
test set. The best score on the 100k EGY test set
is from the two-step adaptation system with 42.9
BLEU, which outperforms the one-step adaptation
system by 2.8 BLEU points. We consider possible
causes of these results in section 7.
202
BLEU-1 BLEU-2 BLEU-3 BLEU-4
Baseline System 53.4 26.6 15.3 9.1
One-Step System 64.3 43.5 33.5 27.1
Two-Step System: Step 2 (Dialect) 65.2 46.0 36.8 30.7
Table 10: Detokenized BLEU (B) scores on the 100k EGY test set at different n-gram levels.
English US , Indonesia commit to closer trade , investment ties
Egyptian Reference

?

K?@

?K


PA?

J

?@?

?K


PAm
.
