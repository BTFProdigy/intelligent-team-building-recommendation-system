Detecting Segmentation Errors in Chinese Annotated Corpus 
  Chengjie Sun   Chang-Ning Huang   Xiaolong Wang     Mu Li
Harbin Institute of  Microsoft Research,  Harbin Institute of  Microsoft Research, 
Technology, Harbin,   Asia, Beijing,     Technology, Harbin,    Asia, Beijing, 
150001, China     100080, China       150001, China      100080, China 
{cjsun, wangxl}@insun.hit.edu.cn     cnhuang@msrchina.research.microsoft.com
muli@microsoft.com
                      
Abstract
          
This paper proposes a 
semi-automatic method to detect 
segmentation errors in a manually 
annotated Chinese corpus in order 
to improve its quality further. A 
particular Chinese character string 
occurring more than once in a 
corpus may be assigned different 
segmentations during a 
segmentation process. Based on 
these differences our approach 
outputs the segmentation error 
candidates found in a segmented 
corpus and then on which the 
segmentation errors are identified 
manually. Segmentation error rate 
of a gold standard corpus can be 
given using our method. In Peking 
University (PK) and Academic 
Sinica (AS) test corpora of Special 
Interest Group for Chinese 
Language Processing (SIGHAN) 
Bakeoff1, 1.29% and 2.26% 
segmentation error rates are 
detected by our method. These 
errors decrease the F-measure of 
SIGHAN Bakeoff1 baseline test by 
1.36% in PK test data and 1.93% in 
AS test data respectively.  
                                                          
                                                           This work was done while Chengjie Sun was visiting 
Microsoft Research Asia. 
1 Introduction 
SIGHAN Bakeoff11 proposed an automatic 
method to evaluate the performance of 
different Chinese word segmentation 
systems on four distinct data sets. This 
method makes the performance of different 
Chinese word segmentation systems 
comparable and greatly promotes the 
technology of Chinese Word Segmentation. 
However, the quality of the reference 
corpora in the evaluation should be paid 
more attention because they provide training 
material for participants and they serve as a 
gold standard for evaluating the 
performance of participant systems.  
This paper presents a semi-automatic 
method to detect segmentation errors in a 
manually annotated Chinese corpus in order 
to improve its quality further. Especially a 
segmentation error rate of a gold standard 
corpus could be obtained with our approach. 
As we know a particular Chinese character 
string occurring more than once in a corpus 
may be assigned different segmentations. 
Those differences are considered as   
segmentation inconsistencies by some 
researchers (Wu, 2003; Chen, 2003).  
Segmentation consistency is also considered 
as one of the quality criteria of an annotated 
Chinese corpus (Sun, 1999). But in order to 
provide a more clearer description of those 
segmentation differences we define a new 
1 http://www.sighan.org/bakeoff2003/ 
1
term, segmentation variation, to replace the 
original one, segmentation inconsistency.
Our approach of spotting segmentation 
errors within an annotated corpus consists of 
two steps: (1) automatically listing the 
segmentation error candidates with 
segmentation variations found in an 
annotated corpus, (2) spotting segmentation 
errors within those candidates manually. 
The target of this approach is to count the 
number of error tokens in the corpus and 
give the segmentation error rate of the 
corpus, which is not given for any gold 
standard corpus in Bakeoff1. 
The remainder of this paper is structured 
as follows. In section 2, we discriminate the 
kinds of segmentation inconsistencies in test 
sets of SIGHAN Bakeoff1. In section 3, 
segmentation variation is defined and our 
approach to detect segmentation errors in a 
manually annotated corpus is proposed. In 
section 4 we conduct baseline experiments 
of PK and AS corpora with revised test sets 
in order to show exactly the impact of 
segmentation errors in the test sets of 
Bakeoff1. Section 5 is a brief conclusion. 
2 Segmentation inconsistency
In the close test of Bakeoff1, participants 
could only use training material from the 
training data for the particular corpus being 
testing on. No other material was allowed 
(Sproat and Emerson, 2003). As we know 
that the test data should be consistent with 
the training data based on a general 
definition of Chinese words. That is if we 
collect all words seen in the training data 
and store them into a lexicon, then each 
word in a test set is either a lexicon word or 
an OOV (out of vocabulary) word (Gao et 
al., 2003). In another word, if a character 
string has been treated as one word, i.e. a 
lexicon word, in the training data, the same 
occurrence should be taken in the 
corresponding test data unless it is a CAS 
(combination ambiguity string) and vice 
versa.
As we all know that a CAS like "??
[cai2-neng2]" may be segmented into one 
word or two words depending on different 
contexts. Thus segmentation inconsistency 
like "??" (talent) and "??" (only can) 
could both be correct segmentations in a text. 
Therefore ?segmentation inconsistency? 
should not be regarded as incorrect 
segmentations in general and should be 
clarified further. If one wants to discuss the 
segmentation errors based on segmentation 
inconsistencies, then from which those CAS 
instances should be excluded. 
If we exclude CAS words in our 
investigation temporary then for a non-CAS 
character string, there are four kinds of 
situations violating the general definition of 
Chinese word, also called lexicon driven 
principle in automatic word segmentation 
technology: 
S1. A character string is segmented 
inconsistently within a training data; 
S2. A character string is segmented 
inconsistently within a test data; 
S3. A character string is segmented 
inconsistently between a test data and its training 
data. This situation could be divided into the 
following two cases further: 
S3.1 A word identified in a training data has 
been segmented into multiple words in 
corresponding test data; 
S3.2 A word identified in a test data has been 
segmented into multiple words in 
corresponding training data. 
Chen (2003) describes inconsistency 
problem found in cases S1, S2 and S3.1 of 
PK corpora. For example, he gives the 
amount of unique text fragments that have 
two or more segmentations within PK 
training data, within PK test data and also 
between PK training data and PK test data. 
2
But those CAS words have not been 
excluded in his description. Ignoring the 
content of inconsistencies the influence 
about the number of segmentation 
inconsistencies of a particular corpus will be 
exaggerated greatly. In addition, Chen didn?t 
consider the case of S3.2 which could also 
affect the evaluation significantly according 
to the lexicon driven principle. 53 word 
types found in case 3.2 (refer to Appendix 
part 2) were totally treated as OOV words in 
Bakeoff1 which impacts the identification of 
those authentic new words in the task. So 
the issue of segmentation inconsistency in 
reference corpora needs further 
investigation.
As mentioned before, in common 
knowledge "segmentation inconsistency" is 
a derogatory term. But our investigation 
shows that most of segmentation 
inconsistencies found in an annotated corpus 
turned out to be correct segmentations of 
CASs. Therefore it is not an appropriate 
technique term to assess the quality of an 
annotated corpus. Besides, with the concept 
of "segmentation inconsistency" it is hard to 
distinguish the different inconsistent 
components within an annotated corpus and 
finally count up the number of segmentation 
errors exactly.  In the next section we 
propose a new term "segmentation 
variation" to replace the original one, 
"segmentation inconsistency". 
3 Segmentation variation  
3.1 Definition
Definition 1: In annotated corpora C, a set 
of f(W, C) is defined as: f(W, C) = {all 
possible segmentations that word W has in 
corpora C}.
Definition 2: W is a segmentation 
variation type (segmentation variation
in short, hereafter) with respect to C iff 
|f(W, C)|>1.
Definition 3: An instance of element in f(W,
C) is called a variation instance. Thus a 
segmentation variation (type) consists of 
more than one variation instances in 
corpora C. And a variation instance may 
include one or more than one tokens.
Definition 4: If a variation instance is an 
incorrect segmentation, it is called an 
error instance (EI).
The definitions of segmentation variation, 
variation instance and error instance (EI) 
clearly distinguish those inconsistent 
components, so we can count the number of 
segmentation errors (in tokens) exactly.  
The term variation is also used to express 
other annotation inconsistency in a corpus 
by other researchers. For example, 
Dickinson and Meurers (2003) used 
variation to describe POS (Part-of-Speech) 
inconsistency in an annotated corpus. 
Example 1: Segmentation variations 
(Bakeoff1 PK corpus):  
Word "??[deng3-tong2]" is segmented as 
"??" (equal) and "??" (et al with).  
Word "???[huang2-jin1-zhou1]" is 
segmented as "???" (golden week) and "
???" (gold week). 
Word "????[bing1-qing1-yu4-jie2]" is 
segmented as "????" (pure and noble) 
and "????" (ice clear jade clean). 
In example 1, Words like ????, ???
? ? and ????? ? are segmentation 
variation types. Segmentations ???? and 
??? ? are two variation instances of 
segmentation variation ????. Besides, the 
variation instance ???? consists of two 
tokens ??? and ???. While the variation 
instance "? ? ? ?" consists of four 
tokens "?", "?", "?" and "?".
The existence of segmentation variations 
in corpora lies in two reasons: 1) ambiguity: 
variation type W has multiple possible 
segmentations in different contexts, or 2) 
3
error: W has been wrongly segmented which 
could be judged by a given lexicon. 
Example 2: A segmentation variation 
caused by ambiguity (Bakeoff1 PK corpus): 
Segmentation variation: "??[guo2-du1]" 
Variation instances: "??" (capital) and "?
[guo2] ?[dou1]" (countries all). They are 
both correct segmentations in following 
sentences:
?????????????
(Constantinople became the capital of 
Byzantium.) 
??????????
????????
(Both countries all advocate solving 
disagreements by conversation and 
negotiation.) 
Example 3: Segmentation variations caused 
by error (Bakeoff1 PK corpus): 
Segmentation variation: "????
[jin4-guan3-ru2-ci4]" 
Variation instances: "????" (still) and "
????" (despite so). 
Segmentation variation: ??????
Variation instances: ?????? and ??
????
In the rest of the paper, a segmentation 
variation caused by ambiguity is called a 
CAS variation and a segmentation variation 
caused by error is called a non-CAS 
variation. Each kind of segmentation 
variations may include error instances (EIs). 
*: The number in the bracket is the amount caused by CAS.
Table 1 segmentation variations types, instances and EIs in PK test data 
3.2 Finding error instances (EIs) 
How to find the segmentation variations in 
corpora? Following is the algorithm of 
finding segmentation variations. According 
to our definition, the algorithm is quite 
straightforward. It takes two segmented 
Chinese corpora (reference corpus and 
corpus to be checked) and outputs a list of 
segmentation variation instances between 
the two corpora2.
Algorithm steps:
                                                          
2 These two corpora could be also regarded as one 
unique corpus: the corpus to be checked. A large scale 
reference corpus is always helpful in spotting more 
variations in the corpus to be checked. 
1. Extract all the multi-character words 
in reference corpus and store their positions 
in reference corpus respectively; 
2. Find the words that be segmented into 
N parts (N is from 2 to the length of current 
word) in the corpus to be checked. Store the 
positions of those segmentations found in 
the corpus to be checked; 
3. Output a list of variation instances 
with their contexts between two corpora. 
We use ?AutoCheck? to stand for the 
processing using the algorithm above. In 
order to find the segmentation variations 
within one corpus, we can also make the 
reference corpus and the corpus to be 
checked be the same corpus. Data in Table 1 
are obtained through ?AutoCheck + manual 
Situation
Within test 
data
Between:
One-to-Mult
Between:
Mult-to-One
# of variation type 21 92 228
# of variation instances 87 129 506
# of EIs* 12(3) 68(4) 77
# of error tokens* 28(6) 142(8) 77
4
checking?. That is firstly running 
?AutoCheck? 3 times as shown in Table 2 to 
get the list of variation types and instances 
in each situation respectively, and then EIs 
are found through manual checking.  
In Table 1, situations ?within test data?, 
?Between: One-to-Mult? and ?Between: 
Mult-to-One? correspond to the Situations 
S2, S3.1 and S3.2 described in Section 2. 
Here we still include CAS segmentations in 
order to take a close look at the distribution 
of EIs in each kind of segmentation 
variation. We can see that in situation 
?Between: One-to-Mult?, there are only 4 
EIs caused by CAS among 68 EIs. It is a 
very small fraction, so most of CAS 
variation instances are correct segmentations 
in a manually checked corpus. 
Situation Reference 
corpus
Corpus to be 
checked
Within test 
data 
PK test data PK test data 
Between: 
One-to-Mult 
PK training 
data 
PK test data 
Between: 
Mult-to-One 
PK test data PK training 
data 
Table 2 Inputs of different AutoCheck runs 
Except ??? ?? (gold week) most of 
the EIs in S2: ?within test data? are also 
found in S3.1: ?Between Rne-to-Pult?3. This 
is because in S3.1 the size of the reference 
corpus (training set) is much greater than the 
corpus to be checked (test set) so variations 
found in this case almost cover all of those 
found in S2 (test set only). EIs in S3.2: 
?between: Pult-to-Rne? are such strings that 
they are never considered as one word in PK 
training data while always identified as one 
                                                          
3 ??? ?? is considered as a segmentation 
error according to its variation instance ?????
(golden week). 
word in PK test data. For example, the 
segmentation variation (type) " ? ?
[shang4-tu2]" occurs four times as one word 
???? (above picture) in test data, but three 
of its variation instances "? ?" (upper 
picture) have been found in the training set. 
Thus, variation type "?? " should be 
identified as a segmentation error rather than 
an OOV word as in Bakeoff1. From Table 1, 
we can find 221 error tokens in all error 
instances (EIs) after removing the 26 
redundant ones in PK test data (17194 
tokens). So, the error rate of PK test data is 
1.29%. 
Using the same method, we also find out 
the 139 error instances (271 error tokens) in 
AS test data. The error rate of AS test data is 
2.26% as shown in table 3. 
Table 3 shows the error rate of AS test set 
is 2.26% and it is higher than PK test data 
which is 1.29%. So we believe that the 
reason why the evaluation result on AS 
corpus are higher than those on PK corpus 
of Bakeoff1 is not due to the segmentation 
quality of AS test data but because RI the 
OOV rate (0.022) in AS test data ZKLFK is 
much lower than PK test data (0.069). 
data PK test data AS test data 
Total tokens 17194 11985 
Error tokens 221 271 
Error rate 1.29% 2.26% 
Table 3 Segmentation errors in PK and AS 
test data 
?AutoCheck? outputs a list of all 
variation instances found in the corpus but it 
can not judge whether a variation instance is 
EI or not. Besides, the output of 
?AutoCheck? doesn?t include those 
segmentation errors which are not instances 
of any segmentation variation in a corpus. 
Two examples are given in Example 4. It 
5
means that ?AutoCheck+manual checking?4
can not spot all segmentation errors in a 
corpus. Despite of these disadvantages of 
?AutoCheck?, it is still a necessary assistant 
to find out almost all of the segmentation 
errors in an annotated corpus for its effective 
in finding segmentation error candidates. 
Example 4: Segmentation errors which are 
not instances of any segmentation variation 
(Bakeoff1 PK corpus): 
??????g?????
?g????????????
(Archon Marino Zanotti held a ceremony on 
the morning of 16th) 
????????????
?????
(?has become the largest scale agency 
system in the world) 
 AutoCheck has been applied in 
preparing the MSRA (Microsoft Research 
Asia) annotated corpora of Chinese word 
segmentation (MS corpora, hereafter) that 
were submitted to SIGHAN Bakeoff2 as one 
of the data sets. "AutoCheck+manual 
checking" is applied as the principal way of 
quality control on MS corpora. Even only 
taking a manual check on those variations 
output by the AutoCheck could provide an 
approximate assessment about the quality of 
the annotated corpus. The lower the number 
of error instances (EIs) found in the output 
list the lower the segmentation error rate the 
annotated corpus reaches. For example, 
there are 37 variation instances output by 
AutoCheck in an annotated document #25 
with 26K tokens in MS Corpora, in which 
no EIs has been found manually. Then the 
whole document was reviewed thoroughly 
by a person in which only two segmentation 
errors (shown in Example 5) have been 
found. Our practice shows that with the 
                                                          
4 ?manual checking? is restricted on the output list 
only. Therefore it is a very effective way to assess 
approximately the quality of an annotated corpus. 
quality control method above the 
segmentation error rate of MS corpora 
reaches 0.1% in average at the worst cases.  
Example 5: Segmentation errors in #25 
Error 1: ? ? ?????? ?? ?
???? ?? ?? ? ?? ?
(There are more than 360 leaders of 
corps and division working in grass roots of 
army and college.) 
The string "??[jun1-shi1]" (military 
counselor) should be corrected as "? ?"
(corps and division).
Error 2: ? ? ?? ? ?? ??? ?
?? ? ?? ?
(It is like a prism reflecting the style and 
features of the age) 
The string "??[yi1-mian4] (at the same 
time) should be corrected as "? ?" (a).
4 The impact to the evaluation 
caused by segmentation errors in 
corpora of Bakeoff1 
In order to show the impact to the 
evaluation result caused by EIs existing in 
test data of Bakeoff1, we conduct the 
baseline close test with PK and AS corpora, 
i.e. we compile lexicons only containing 
words in their training data and then use the 
lexicons with a forward maximum matching 
algorithm to segment their test data 
respectively (Sproat and Emerson, 2003). 
Original and modified test data are used as 
gold standard in our baseline test.  
In table 4, reference data PK1 and AS1 
are the original PK test data and AS test data. 
Reference data PK2 and AS2 are obtained 
after correcting all segmentation errors 
found in their original data (Table 3). 
Results in Table 4 are the output of 
Bakeoff1 evaluation program. Word count is 
the number of tokens in reference data and 
the change in word count is caused by our 
modification. Table 4 shows the impact of 
EIs in test data to the evaluation results. We 
6
can see the F measure increase to 0.879 
(0.933) from 0.867 (0.915) and the OOV 
ratio is decrease to 0.065 (0.020) from 0.069 
(0.022) when all EIs are corrected in PK 
(AS) test data. 
Table 4 Baseline test results with original and revised PK and AS test data 
5 Conclusion
A semi-automatic method to detect 
segmentation errors in a manually annotated 
Chinese corpus is presented in this paper. 
The main contributions of this research are:  
? Offer an effective way to spot the 
segmentation errors in a manually 
annotated corpus and give the 
segmentation error rate of the 
corpus.
? Point out that segmentation 
inconsistency is not an appropriate 
technique term to assess the 
segmentation quality of an 
annotated corpus and define the 
concept of segmentation variation 
instead to get the segmentation error 
rate of a gold standard corpus. 
? Show the influence to the evaluation 
result caused by the segmentation 
errors in a gold standard corpus. 
1.29% error rate of PK test data and 
2.26% error rate of AS test data 
decrease the F-measure of the 
SIGHAN Bakeoff1 baseline test by 
1.36% and 1.93% respectively. 
Acknowledgements 
We would like to thank the members of the 
Natural Language Computing Group at 
Microsoft Research Asia, especially to 
acknowledge Jianfeng Gao, John Chen, and 
the two anonymous reviewers for their 
insightful comments and suggestions. 
References 
Aitao Chen. 2003. Chinese word segmentation 
using minimal linguistic knowledge. In 
Proceedings of the Second SIGHAN 
Workshop on Chinese Language Processing, 
July 11-12, 2003, Sapporo, Japan. 
Andi Wu. 2003. Chinese word segmentation in 
MSR-NLP. In Proceedings of the Second 
SIGHAN Workshop on Chinese Language 
Processing, July 11-12, 2003, Sapporo, Japan. 
Jianfeng Gao, Mu Li and Chang-Ning Huang. 
2003. Improved source-channel models for 
Chinese word segmentation. In Proceedings 
of ACL-2003. July 7-12, 2003. Sapporo, 
Japan. 
Markus Dickinson, W. Detmar Meurers. 2003. 
Detecting errors in Part-of-Speech annotation. 
In Proceedings of the 11th Conference of the 
European Chapter of the Association for 
Referen
ce data 
Word
count
R P F OOV ROOV RIV
PK1 17,194 0.909 0.829 0.867 0.069 0.050 0.972 
PK2 17,200 0.920 0.841 0.879 0.065 0.053 0.980 
AS1 11,985 0.917 0.912 0.915 0.022 0.000 0.938 
AS2 11.886 0.939 0.926 0.933 0.020 0.000 0.958 
7
Computational Linguistics (EACL-03), 2003, 
Budapest, Hungary 
Richard Sproat, Thomas Emerson. 2003. The 
first international Chinese word Segmentation 
Bakeoff. In Proceedings of the Second 
SIGHAN Workshop on Chinese Language 
Processing, July 11-12, 2003, Sapporo, Japan. 
 Sun Maosong. 1999. On the consistency of 
word-segmented Chinese corpus. (In Chinese) 
Applied Linguistics, (2):88-91, 1999.  
Appendix: Modified EIs in PK test data 
1) EI found in CAS variations: 
Original: ??????????
????
Modified: ?????????
?????
2) Some EIs in S3.2 which are considered as 
new words in Bakeoff1: 
??? ???? ?? ??? ?? ?
? ?? ?? ??? ??? ?? ?
?? ??? ???? ?? ?? ??
? ?? ?? ?? ?? ?? ?? ?
? ?? ?? ?? ?? ?? ?? ?
? ?? ??? ?? ?? ????
?? ?? ?? ?? ?? ??? ?
? ?? ??? ?? ??? ????
?? ?? ?? ?? ??
3) Some EIs in S3.1 in which their variation 
types should be lexicon words 
Original Modified
?? ? ? ????
???? ? ?
?
???????
? ?? ???
?? ?? ????
?? ?? ????
?? ?? ????
? ? ? ? ????
?? ?? ????
?? ?? ????
? ? ??
? ? ??
? ? ??
? ? ??
8
  
Name Origin Recognition Using Maximum Entropy Model  
and Diverse Features 
Min Zhang1, Chengjie Sun2, Haizhou Li1, Aiti Aw1, Chew Lim Tan3, Xiaolong Wang2 
1Institute for Infocomm 
Research, Singapore 
{mzhang,hli,aaiti} 
@i2r.a-star.edu.sg 
2Harbin Institute of 
Technology, China 
{cjsun,wangxl} 
@insun.hit.edu.cn 
 
3National University of 
Singapore, Singapore 
tancl@comp.
nus.edu.sg 
Abstract 
Name origin recognition is to identify the 
source language of a personal or location 
name.  Some early work used either rule-
based or statistical methods with single 
knowledge source. In this paper, we cast the 
name origin recognition as a multi-class 
classification problem and approach the 
problem using Maximum Entropy method. 
In doing so, we investigate the use of differ-
ent features, including phonetic rules, n-
gram statistics and character position infor-
mation for name origin recognition. Ex-
periments on a publicly available personal 
name database show that the proposed ap-
proach achieves an overall accuracy of 
98.44% for names written in English and 
98.10% for names written in Chinese, which 
are significantly and consistently better than 
those in reported work.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another with 
approximate phonetic equivalents. The phonetic 
translation practice is referred to as transliteration; 
conversely, the process of recovering a word in its 
native language from a transliteration is called as 
back-transliteration (Zhang et al 2004; Knight 
and Graehl, 1998).  For example, English name 
?Smith? and ????  (Pinyin 1 : Shi-Mi-Si)? in 
                                                 
1 Hanyu Pinyin, or Pinyin in short, is the standard romaniza-
tion system of Chinese. In this paper, Pinyin is given next to 
Chinese form a pair of transliteration and back-
transliteration. In many natural language process-
ing tasks, such as machine translation and cross-
lingual information retrieval, automatic name 
transliteration has become an indispensable com-
ponent.  
Name origin refers to the source language of a 
name where it originates from. For example, the 
origin of the English name ?Smith? and its Chi-
nese transliteration ???? (Shi-Mi-Si)? is Eng-
lish, while both ?Tokyo? and ??? (Dong-Jing)? 
are of Japanese origin. Following are examples of 
different origins of a collection of English-Chinese 
transliterations. 
 
English: Richard-??? (Li-Cha-De) 
Hackensack-????(Ha-Ken-
Sa-Ke) 
Chinese: Wen JiaBao-???(Wen-Jia-
Bao) 
ShenZhen???(Shen-Zhen) 
Japanese: Matsumoto-?? (Song-Ben) 
Hokkaido-???(Bei-Hai-Dao) 
Korean: Roh MooHyun-???(Lu-Wu-
Xuan) 
Taejon-??(Da-Tian) 
Vietnamese: Phan Van Khai-???(Pan-
Wen-Kai) 
Hanoi-??(He-Nei) 
 
In the case of machine transliteration, the name 
origins dictate the way we re-write a foreign word. 
For example, given a name written in English or 
Chinese for which we do not have a translation in 
                                                                            
Chinese characters in round brackets for ease of reading. 
56
  
a English-Chinese dictionary, we first have to de-
cide whether the name is of Chinese, Japanese, 
Korean or some European/English origins. Then 
we follow the transliteration rules implied by the 
origin of the source name. Although all English 
personal names are rendered in 26 letters, they 
may come from different romanization systems. 
Each romanization system has its own rewriting 
rules. English name ?Smith? could be directly 
transliterated into Chinese as ????(Shi-Mi-Si)? 
since it follows the English phonetic rules, while 
the Chinese translation of Japanese name ?Koi-
zumi? becomes ???(Xiao-Quan)? following the 
Japanese phonetic rules. The name origins are 
equally important in back-transliteration practice. 
Li et al (2007) incorporated name origin recogni-
tion to improve the performance of personal name 
transliteration. Besides multilingual processing, 
the name origin also provides useful semantic in-
formation (regional and language information) for 
common NLP tasks, such as co-reference resolu-
tion and name entity recognition. 
Unfortunately, little attention has been given to 
name origin recognition (NOR) so far in the litera-
ture. In this paper, we are interested in two kinds 
of name origin recognition: the origin of names 
written in English (ENOR) and the origin of 
names written in Chinese (CNOR). For ENOR, 
the origins include English (Eng), Japanese (Jap), 
Chinese Mandarin Pinyin (Man) and Chinese Can-
tonese Jyutping (Can). For CNOR, they include 
three origins: Chinese (Chi, for both Mandarin and 
Cantonese), Japanese and English (refer to Latin-
scripted language). 
Unlike previous work (Qu and Grefenstette, 
2004; Li et al, 2006; Li et al, 2007) where NOR 
was formulated with a generative model, we re-
gard the NOR task as a classification problem. We 
further propose using a discriminative learning 
algorithm (Maximum Entropy model: MaxEnt) to 
solve the problem. To draw direct comparison, we 
conduct experiments on the same personal name 
corpora as that in the previous work by Li et al 
(2006). We show that the MaxEnt method effec-
tively incorporates diverse features and outper-
forms previous methods consistently across all test 
cases. 
The rest of the paper is organized as follows: in 
section 2, we review the previous work. Section 3 
elaborates our proposed approach and the features. 
Section 4 presents our experimental setup and re-
ports our experimental results. Finally, we con-
clude the work in section 5. 
2 Related Work 
Most of previous work focuses mainly on ENOR 
although same methods can be extended to CNOR. 
We notice that there are two informative clues that 
used in previous work in ENOR. One is the lexical 
structure of a romanization system, for example, 
Hanyu Pinyin, Mandarin Wade-Giles, Japanese 
Hepbrun or Korean Yale, each has a finite set of 
syllable inventory (Li et al, 2006). Another is the 
phonetic and phonotactic structure of a language, 
such as phonetic composition, syllable structure. 
For example, English has unique consonant 
clusters such as /str/ and /ks/ which Chinese, 
Japanese and Korean (CJK) do not have. 
Considering the NOR solutions by the use of these 
two clues, we can roughly group them into two 
categories: rule-based methods (for solutions 
based on lexical structures) and statistical methods 
(for solutions based on phonotactic structures). 
Rule-based Method  
Kuo and Yang (2004) proposed using a rule-
based method to recognize different romanization 
system for Chinese only. The left-to-right longest 
match-based lexical segmentation was used to 
parse a test word. The romanization system is con-
firmed if it gives rise to a successful parse of the 
test word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanization systems 
that have a finite set of discriminative syllable in-
ventory, such as Pinyin for Chinese Mandarin. For 
the general tasks of identifying the language origin 
and romanization system, rule based approach 
sounds less attractive because not all languages 
have a finite set of discriminative syllable inven-
tory. 
Statistical Method 
1) N-gram Sum Method (SUM): Qu and Gre-
fenstette (2004) proposed a NOR identifier using a 
trigram language model (Cavnar and Trenkle, 
1994) to distinguish personal names of three lan-
guage origins, namely Chinese, Japanese and Eng-
lish. In their work, the training set includes 11,416 
Chinese name entries, 83,295 Japanese name en-
tries and 88,000 English name entries. However, 
the trigram is defined as the joint probabil-
57
  
ity 1 2( )i i ip c c c? ? for 3-character 1 2i i ic c c? ?  rather than 
the commonly used conditional probabil-
ity 1 2( | )i i ip c c c? ? . Therefore, the so-called trigram 
in Qu and Grefenstette (2004) is basically a sub-
string unigram probability, which we refer to as 
the n-gram (n-character) sum model (SUM) in this 
paper. Suppose that we have the unigram count 
1 2( )i i iC c c c? ? for character substring 1 2i i ic c c? ? , the 
unigram is then computed as: 
1 2
1 2
1 2
1 2,
( )
( )
( )
i i i
i i i
i i i
i i ii c c c
C c c c
p c c c
C c c c
? ?
? ?
? ?
? ?
= ?           (1) 
which is the count of character substring 1 2i i ic c c? ?  
normalized by the sum of all 3-character string 
counts in the name list for the language of interest.  
For origin recognition of Japanese names, this 
method works well with an accuracy of 92%. 
However, for English and Chinese, the results are 
far behind with a reported accuracy of 87% and 
70% respectively. 
2) N-gram Perplexity Method (PP): Li et al 
(2006) proposed using n-gram character perplexity 
cPP  to identify the origin of a Latin-scripted name. 
Using bigram, the cPP is defined as: 
1
1 log ( | )
2
Nc
i i 1ic
p c cN
cPP
?
=
? ?
=   (2) 
where cN is the total number of characters in the 
test name, ic is the i
th character in the test name. 
1( | )i ip c c ? is the bigram probability which is 
learned from each name list respectively. As a 
function of model, cPP  measures how good the 
model matches the test data. Therefore, cPP can be 
used to measure how good a test name matches a 
training set. A test name is identified to belong to 
a language if the language model gives rise to the 
minimum perplexity. Li et al (2006) shown that 
the PP method gives much better performance 
than the SUM method. This may be due to the fact 
that the PP measures the normalized conditional 
probability rather than the sum of joint probability. 
Thus, the PP method has a clearer mathematical 
interpretation than the SUM method. 
The statistical methods attempt to overcome the 
shortcoming of rule-based method, but they suffer 
from data sparseness, especially when dealing 
with a large character set, such as in Chinese (our 
experiments will demonstrate this point empiri-
cally). In this paper, we propose using Maximum 
Entropy (MaxEnt) model as a general framework 
for both ENOR and CNOR. We explore and inte-
grate multiple features into the discriminative clas-
sifier and use a common dataset for benchmarking. 
Experimental results show that the MaxEnt model 
effectively incorporates diverse features to demon-
strate competitive performance.   
3 MaxEnt Model and Features 
3.1 MaxEnt Model for NOR 
The principle of maximum entropy (MaxEnt) 
model is that given a collection of facts, choose a 
model consistent with all the facts, but otherwise 
as uniform as possible (Berger et al, 1996). Max-
Ent model is known to easily combine diverse fea-
tures. For this reason, it has been widely adopted 
in many natural language processing tasks. The 
MaxEnt model is defined as: 
( , )
1
1
( | ) j i
K
f c x
i j
j
p c x
Z
?
=
= ?           (3) 
      ( , )
1 1 1
( | ) j i
KN N
f c x
i j
i i j
Z p c x ?
= = =
= =? ??          (4) 
where ic is the outcome label, x is the given obser-
vation, also referred to as an instance. Z is a nor-
malization factor. N  is the number of outcome 
labels, the number of language origins  in our case. 
1 2, , , Kf f fL are feature functions and 
1 2, , , K? ? ?L are the model parameters. Each pa-
rameter corresponds to exactly one feature and can 
be viewed as a ?weight? for the corresponding fea-
ture.  
In the NOR task, c is the name origin label; x is 
a personal name, if is a feature function. All fea-
tures used in the MaxEnt model in this paper are 
binary. For example: 
 
1,    " "& (" ")
( , )
0,  j
if c Eng x contains str
f c x
otherwise
=?
= ??
 
In our implementation, we used Zhang?s maxi-
mum entropy package2. 
3.2 Features 
Let us use English name ?Smith? to illustrate the 
features that we define. All characters in a name 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
58
  
are first converted into upper case for ENOR be-
fore feature extraction. 
N-gram Features: N-gram features are de-
signed to capture both phonetic and orthographic 
structure information for ENOR and orthographic 
information only for CNOR. This is motivated by 
the facts that: 1) names written in English but from 
non-English origins follow different phonetic rules 
from the English one; they also manifest different 
character usage in orthographic form; 2) names 
written in Chinese follows the same pronunciation 
rules (Pinyin), but the usage of Chinese characters 
is distinguishable between different language ori-
gins as reported in Table 2 of (Li et al, 2007).  
The N-gram related features include: 
1) FUni: character unigram <S, M, I, T, H> 
2) FBi: character bigram <SM, MI, IT, TH> 
3) FTri: character trigram <SMI, MIT, ITH > 
Position Specific n-gram Features: We in-
clude position information into the n-gram fea-
tures. This is mainly to differentiate surname from 
given name in recognizing the origin of CJK per-
sonal names written in Chinese. For example, the 
position specific n-gram features of a Chinese 
name ????(Wen-Jia-Bao)? are as follows: 
1) FPUni: position specific unigram  
<0?(Wen), 1?(Jia), 2?(Bao)> 
2) FPBi: position specific bigram  
<0??(Wen-Jia), 1??(Jia-Bao)> 
3) FPTri: position specific trigram  
<0???(Wen-Jia-Bao)> 
Phonetic Rule-based Features: These features 
are inspired by the rule-based methods (Kuo and 
Yang, 2004; Qu and Grefenstette, 2004) that check 
whether an English name is a sequence of sylla-
bles of CJK languages in ENOR task. We use the 
following two features in ENOR task as well. 
1) FMan: a Boolean feature to indicate 
whether a name is a sequence of Chinese 
Mandarin Pinyin.   
2) FCan: a Boolean feature to indicate whether 
a name is a sequence of Cantonese Jyutping. 
Other Features:  
1) FLen: the number of Chinese characters in a 
given name. This feature is for CNOR only.  
The numbers of Chinese characters in per-
sonal names vary with their origins. For ex-
ample, Chinese and Korean names usually 
consist of 2 to 3 Chinese characters while 
Japanese names can have up to 4 or 5 Chi-
nese characters 
2) FFre: the frequency of n-gram in a given 
name. This feature is for ENOR only. In 
CJK names, some consonants or vowels 
usually repeat in a name as the result of the 
regular syllable structure. For example, in 
the Chinese name ?Zhang Wanxiang?, the 
bigram ?an? appears three times 
Please note that the trigram and position spe-
cific trigram features are not used in CNOR due to 
anticipated data sparseness in CNOR3.  
4 Experiments 
We conduct the experiments to validate the effec-
tiveness of the proposed method for both ENOR 
and CNOR tasks. 
4.1 Experimental Setting 
 
Origin #  entries Romanization System 
Eng4 88,799 English 
Man5 115,879 Pinyin 
Can 115,739 Jyutping 
Jap6 123,239 Hepburn 
 
Table 1: DE: Latin-scripted personal name corpus for 
ENOR 
 
 
Origin #  entries 
Eng7 37,644 
Chi8 29,795 
Jap9 33,897 
 
Table 2: DC: Personal name corpus written in Chinese 
characters for CNOR 
 
                                                 
3 In the test set of CNOR, 1080 out of 2980 names of Chinese 
origin do not consist of any bigrams learnt from training data, 
while 2888 out of 2980 names do not consist of any learnt 
trigrams. This is not surprising as most of Chinese names only 
have two or three Chinese characters and in our open testing, 
the train set is exclusive of all entries in the test set.  
4 http://www.census.gov/genealogy/names/ 
5 http://technology.chtsai.org/namelist/  
6 http://www.csse.monash.edu.au/~jwb/enamdict_doc.html 
7 Xinhua News Agency (1992)  
8 http://www.ldc.upenn.edu LDC2005T34 
9 www.cjk.org 
59
  
Datasets: We prepare two data sets which are col-
lected from publicly accessible sources: DE and DC 
for the ENOR and CNOR experiment respectively. 
DE is the one used in (Li et al, 2006), consisting of 
personal names of Japanese (Jap), Chinese (Man), 
Cantonese (Can) and English (Eng) origins. DC 
consists of personal names of Japanese (Jap), Chi-
nese (Chi, including both Mandarin and Canton-
ese) and English (Eng) origins. Table 1 and Table 
2 list their details. In the experiments, 90% of en-
tries in Table 1 (DE) and Table 2 (DC) are ran-
domly selected for training and the remaining 10% 
are kept for testing for each language origin. Col-
umns 2 and 3 in Tables 7 and 8 list the numbers of 
entries in the training and test sets.  
 
Evaluation Methods: Accuracy is usually used to 
evaluate the recognition performance (Qu and 
Gregory, 2004; Li et al, 2006; Li et al, 2007). 
However, as we know, the individual accuracy 
used before only reflects the performance of recall 
and does not give a whole picture about a multi-
class classification task. Instead, we use precision 
(P), recall (R) and F-measure (F) to evaluate the 
performance of each origin. In addition, an overall 
accuracy (Acc) is also given to describe the whole 
performance. The P, R, F and Acc are calculated 
as following: 
 
#        
#          
correctly recognized entries of the given origin
P
entries recognized as the given origin by the system
=
 
 
#        
#      
correctly recognized entries of the given origin
R
entries of the given origin
=
 
 
2PR
F
P R
=
+
     #     
#   
all correctly recognized entries
Acc
all entries
=
 
4.2 Experimental Results and Analysis 
Table 3 reports the experimental results of ENOR. 
It shows that the MaxEnt approach achieves the 
best result of 98.44% in overall accuracy when 
combining all the diverse features as listed in Sub-
section 3.2. Table 3 also measures the contribu-
tions of different features for ENOR by gradually 
incorporating the feature set. It shows that:  
1) All individual features are useful since the 
performance increases consistently when 
more features are being introduced. 
2) Bigram feature presents the most informa-
tive feature that gives rise to the highest 
performance gain, while the trigram feature  
further boosts performance too. 
3) MaxEnt method can integrate the advan-
tages of previous rule-based and statistical 
methods and easily integrate other features. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
    
R(
%)
 
F 
Ac
c(%
) 
Eng 91.40 80.76 85.75
Man 83.05 81.90 82.47
Can 81.13 82.76 81.94
FUni 
Jap 87.31 94.11 90.58
85.29
Eng 97.54 91.10 94.21
Man 97.51 98.10 97.81
Can 97.68 98.05 97.86
+FBi 
Jap 94.62 98.24 96.39
96.72
Eng 97.71 93.79 95.71
Man 98.94 99.37 99.16
Can 99.12 99.19 99.15
+FTri 
Jap 96.19 98.52 97.34
97.97
Eng 97.53 94.64 96.06
Man 99.21 99.43 99.32
Can 99.41 99.24 99.33
+FPUni 
Jap 96.48 98.49 97.47
98.16
Eng 97.68 94.98 96.31
Man 99.32 99.50 99.41
Can 99.53 99.34 99.44
+FPBi 
Jap 96.59 98.52 97.55
98.28
Eng 97.62 94.97 96.27
Man 99.34 99.58 99.46
Can 99.63 99.37 99.50
+FPTri 
Jap 96.61 98.45 97.52
98.30
Eng 97.74 95.06 96.38
Man 99.37 99.59 99.48
Can 99.61 99.41 99.51
+FFre 
Jap 96.66 98.56 97.60
98.35
Eng 97.82 95.11 96.45
Man 99.52 99.68 99.60
Can 99.71 99.59 99.65
 + FMan 
+ FCan 
Jap 96.69 98.59 97.63
98.44
 
Table 3: Contribution of each feature for ENOR 
 
 
60
  
Features Eng Jap Man Can 
FMan -0.357 0.069 0.072 -0.709 
FCan -0.424 -0.062 -0.775 0.066 
 
Table 4: Features weights in ENOR task. 
 
F
ea
tu
re
 
O
ri
gi
n 
P(
%
) 
R(
%
) 
F 
   A
cc(
%
) 
Eng 97.89 98.43 98.16
Chi 95.80 95.03 95.42FUni 
Jap 96.96 97.05 97.00
96.97 
Eng 96.99 98.27 97.63
Chi 96.86 92.11 94.43+FBi 
Jap 95.04 97.73 96.36
96.28 
Eng 97.35 98.38 97.86
Chi 97.29 95.00 96.13+FLen 
Jap 96.78 97.64 97.21
97.14 
Eng 97.74 98.65 98.19
Chi 97.65 96.34 96.99+FPUni 
Jap 97.91 98.05 97.98
97.77 
Eng 97.50 98.43 97.96
Chi 97.61 96.04 96.82+FPBi 
Jap 97.59 97.94 97.76
97.56 
Eng 98.08 99.04 98.56
Chi 97.57 96.88 97.22
FUni 
+FLen 
+ 
FPUni Jap 98.58 98.11 98.34
98.10 
 
Table 5: Contribution of each feature for CNOR 
 
Table 4 reports the feature weights of two fea-
tures ?FMan? and ?FCan? with regard to different 
origins in ENOR task. It shows that ?FCan? has 
positive weight only for origin ?Can? while 
?FMan? has positive weights for both origins 
?Man? and ?Jap?, although the weight for ?Man? 
is higher. This agrees with our observation that the 
two features favor origins ?Man? or ?Can?. The 
feature weights also reflect the fact that some 
Japanese names can be successfully parsed by the 
Chinese Mandarin Pinyin system due to their simi-
lar syllable structure. For example, the Japanese 
name ?Tanaka Miho? is also a sequence of Chi-
nese Pinyin: ?Ta-na-ka Mi-ho?.  
Table 5 reports the contributions of different 
features in CNOR task by gradually incorporating 
the feature set. It shows that:  
1) Unigram features are the most informative 
2) Bigram features degrade performance. This 
is largely due to the data sparseness prob-
lem as discussed in Section 3.2.   
3) FLen is also useful that confirms our intui-
tion about name length. 
Finally the combination of the above three use-
ful features achieves the best performance of 
98.10% in overall accuracy for CNOR as in the 
last row of Table 5. 
In Tables 3 and 5, the effectiveness of each fea-
ture may be affected by the order in which the fea-
tures are incorporated, i.e., the features that are 
added at a later stage may be underestimated. 
Thus, we conduct another experiment using "all-
but-one" strategy to further examine the effective-
ness of each kind of features. Each time, one type 
of the n-gram (n=1, 2, 3) features (including or-
thographic n-gram, position-specific and n-gram 
frequency features) is removed from the whole 
feature set. The results are shown in Table 6. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
 
R(
%)
 
F 
Ac
c(%
) 
Eng 97.81 95.01 96.39
Man 99.41 99.58 99.49
Can 99.53 99.48 99.50
w/o 
Uni-
gram 
Jap 96.63 98.52 97.57
98.34
Eng 97.34 95.17 96.24
Man 99.30 99.48 99.39
Can 99.54 99.33 99.43
w/o Bi-
gram 
Jap 96.73 98.32 97.52
98.26
Eng 97.57 94.10 95.80
Man 98.98 99.23 99.10
Can 99.20 99.08 99.14
w/o 
Tri-
gram 
Jap 96.06 98.42 97.23
97.94
 
Table 6: Effect of n-gram feature for ENOR 
 
Table 6 reveals that removing trigram features 
affects the performance most. This suggests that 
trigram features are much more effective for 
ENOR than other two types of features. It also 
shows that trigram features in ENOR does not suf-
fer from the data sparseness issue. 
As observed in Table 5, in CNOR task, 93.96% 
61
  
accuracy is obtained when removing unigram fea-
tures, which is much lower than 98.10% when bi-
gram features are removed. This suggests that uni-
gram features are very useful in CNOR, which is 
mainly due to the data sparseness problem that 
bigram features may have encountered. 
4.3 Model Complexity and Data Sparseness 
Table 7 (ENOR) and Table 8 (CNOR) compare 
our MaxEnt model with the SUM model (Qu and 
Gregory, 2004) and the PP model (Li et al, 2006). 
All the experiments are conducted on the same 
data sets as described in section 4.1. Tables 7 and 
8 show that the proposed MaxEnt model outper-
forms other models. The results are statistically 
significant ( 2? test with p<0.01) and consistent 
across all tests. 
Model Complexity: 
We look into the complexity of the models and 
their effects. Tables 7 and 8 summarize the overall 
accuracy of three models. Table 9 reports the 
numbers of parameters in each of the models. We 
are especially interested in a comparison between 
the MaxEnt and PP models because their perform-
ance is close.  We observe that, using trigram fea-
tures, the MaxEnt model has many more parame-
ters than the PP model does. Therefore, it is not 
surprising if the MaxEnt model outperforms when 
more training data are available. However, the ex-
periment results also show that the MaxEnt model 
consistently outperforms the PP model even with 
the same size of training data. This is largely at-
tributed to the fact that MaxEnt incorporates more 
robust features than the PP model does, such as 
rule-based, length of names features.  
One also notices that PP clearly outperforms 
SUM by using the same number of parameters in 
ENOR and shows comparable performance in 
CNOR tasks. Note that SUM and PP are different 
in two areas: one is the PP model employs word 
length normalization while SUM doesn?t; another 
that the PP model uses n-gram conditional prob-
ability while SUM uses n-character joint probabil-
ity. We believe that the improved performance of 
PP model can be attributed to the effect of usage 
of conditional probability, rather than length nor-
malization since length normalization does not 
change the order of probabilities. 
Data Sparesness: 
We understand that we can only assess the ef-
fectiveness of a feature when sufficient statistics is 
available. In CNOR (see Table 8), we note that the 
Chinese transliterations of English origin use only 
377 Chinese characters, so data sparseness is not a 
big issue. Therefore, bigram SUM and bigram PP 
methods easily achieve good performance for Eng-
lish origin. However, for Japanese origin (repre-
sented by 1413 Chinese characters) and Chinese 
origin (represented by 2319 Chinese characters), 
the data sparseness becomes acute and causes per-
formance degradation in SUM and PP models. We 
are glad to find that MaxEnt still maintains a good 
performance benefiting from other robust features. 
Table 10 compares the overall accuracy of the 
three methods using unigram and bigram features 
in CNOR task, respectively. It shows that the 
MaxEnt method achieves best performance. An-
other interesting finding is that unigram features 
perform better than bigram features for PP and  
MaxEnt models, which shows that  data sparseness 
remains an issue even for MaxEnt model.  
5 Conclusion 
We propose using MaxEnt model to explore di-
verse features for name origin recognition. Ex-
periment results show that our method is more ef-
fective than previously reported methods. Our 
contributions include: 
1) Cast the name origin recognition problem as 
a multi-class classification task and propose 
a MaxEnt solution to it; 
2) Explore and integrate diverse features for 
name origin recognition and propose the 
most effective feature sets for ENOR and 
for CNOR 
In the future, we hope to integrate our name 
origin recognition method with a machine translit-
eration engine to further improve transliteration 
performance. We also hope to study the issue of 
name origin recognition in context of sentence and 
use contextual words as additional features. 
References 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational Lin-
guistics. 22(1):39?71. 
William B. Cavnar and John M. Trenkle. 1994. Ngram 
based text categorization. In 3rd Annual Symposium 
62
  
on Document Analysis and Information Retrieval, 
275?282. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics. 24(4), 
599-612. 
Jin-Shea Kuo and Ying-Kuei Yan. 2004. Generating 
Paired Transliterated-Cognates Using Multiple Pro-
nunciation Characteristics from Web Corpora. PA-
CLIC 18, December 8th-10th, Waseda University, 
Tokyo, Japan, 275?282. 
Haizhou Li, Shuanhu Bai and Jin-Shea Kuo. 2006. 
Transliteration. Advances in Chinese Spoken Lan-
guage Processing. World Scientific Publishing Com-
pany, USA, 341?364. 
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui 
Dong. 2007. Semantic Transliteration of Personal 
Names. ACL-2007. 120?127. 
Xinhua News Agency. 1992. Chinese Transliteration of 
Foreign Personal Names. The Commercial Press  
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in 
Latin script via language identification and corpus 
validation. ACL-2004. 183?190. 
Min Zhang, Jian Su and Haizhou Li. 2004. Direct Or-
thographical Mapping for Machine Translation. 
COLING-2004. 716-722. 
 
Trigram SUM Trigram PP MaxEnt Origin # training 
entries 
# test 
entries P (%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 79,920 8,879 94.66 72.50 82.11 95.84 94.72 95.28 97.82 95.11 96.45
Man 104,291 11,588 86.79 94.87 90.65 98.99 98.33 98.66 99.52 99.68 99.60
Can 104,165 11,574 90.03 93.87 91.91 96.17 99.67 97.89 99.71 99.59 99.65
Jap 110,951 12,324 89.17 92.84 90.96 98.20 96.29 97.24 96.69 98.59 97.63
Overall Acc (%) 89.57 97.39 98.44 
Table 7: Benchmarking different methods in ENOR task 
Bigram SUM  Bigram PP  MaxEnt Origin # training 
entries 
# test 
entries P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 37,644 3,765 95.94 98.65 97.28 97.58 97.61 97.60 98.08 99.04 98.56 
Chi 29,795 2,980 96.26 87.35 91.59 95.10 87.35 91.06 97.57 96.88 97.22 
Jap 33,897 3,390 93.01 97.67 95.28 90.94 97.43 94.07 98.58 98.11 98.34 
Overall Acc (%) 95.00 94.53 98.10 
Table 8: Benchmarking different methods in CNOR task 
# of parameters for ENOR # of parameters for CNOR 
Methods 
Trigram Unigram Bigram 
MaxEnt  124,692 13,496  182,116 
PP 16,851 4,045 86,490 
SUM  16,851 4,045 86,490 
 
Table 9: Numbers of parameters used in different methods 
 
 SUM PP MaxEnt 
Unigram Features 90.55 97.09 98.10 
Bigram Features 95.00 94.53 97.56 
 
Table 10: Overall accuracy using unigram and bigram features in CNOR task 
63
A Study of Chinese Lexical Analysis Based on Discriminative Models 
Guang-Lu Sun  Cheng-Jie Sun  Ke Sun and Xiao-Long Wang 
Intelligent Technology & Natural Language Processing Laboratory, School of Computer 
Science and Technology, Harbin Institute of Technology, 150001, Harbin, China 
{glsun, cjsun, ksun, wangxl}@insun.hit.edu.cn 
 
Abstract 
This paper briefly describes our system in 
The Fourth SIGHAN Bakeoff. 
Discriminative models including maximum 
entropy model and conditional random 
fields are utilized in Chinese word 
segmentation and named entity recognition 
with different tag sets and features. 
Transformation-based learning model is 
used in part-of-speech tagging. Evaluation 
shows that our system achieves the 
F-scores: 92.64% and 92.73% in NCC 
Word Segmentation close and open tests, 
89.11% in MSRA name entity recognition 
open test, 91.13% and 91.97% in PKU 
part-of-speech tagging close and open tests. 
All the results get medium performances 
on the bakeoff tracks. 
1 Introduction 
Lexical analysis is the basic step in natural 
language processing. It is prerequisite to many 
further applications, such as question answer 
system, information retrieval and machine 
translation. Chinese lexical analysis chiefly 
consists of word segmentation (WS), name entity 
recognition (NER) and part-of-speech (POS) 
tagging. Because Chinese does not have explicit 
word delimiters to mark word boundaries like 
English, WS is essential process for Chinese. POS 
tagging and NER are just like those of English.  
Our system participated in The Fourth SIGHAN 
Bakeoff which held in 2007. Different approaches 
are applied to solve all the three tasks which are 
integrated into a unified system (ITNLP-IsLex). 
For WS task, conditional random fields (CRF) are 
used. For NER, maximum entropy model (MEM) 
is applied. And transformation-based learning 
(TBL) algorithm is utilized to solve POS tagging 
problem. The reasons using different models are 
listed in the rest sections of this paper. We give a 
brief introduction to our system sequentially. 
Section 2 describes WS. Section 3 and section 4 
introduce NER and POS tagging respectively. We 
give some experimental results in section 5. Finally 
we draw some conclusions. 
2 Chinese word segmentation 
For WS task, NCC corpus is chosen both in close 
test and open test.  
2.1 Conditional random fields 
Conditional random fields are undirected graphical 
models defined by Lafferty (2001). There are two 
advantages of CRF. One is their great flexibility to 
incorporate various types of arbitrary, 
non-independent features of the input, the other is 
their ability to overcome the label bias problem.  
Given the observation sequence X, on the basis 
of CRF, the conditional probability of the state 
sequence Y is: 
( ) (k k i-1 i
k
1
p Y X = exp l f y , y ,X,i
Z(X)
)? ?? ?? ??      (1)
(k k i-1 i
y Y k
)Z(X)= exp l f y , y ,X,i
?
? ?? ?? ?? ?          (2) 
Z(x) is the normalization factor. ( )1, , ,k i if y y X i?  
is the universal definition of features in CRF. 
2.2 Word segmentation based on CRF 
Inspired by Zhao (2006), the Chinese WS task is 
considered as a sequential labeling problem, i.e., 
assigning a label to each character in a sentence 
given its contexts. CRF model is adopted to do 
labeling. 
6 tags are utilized in this work: B, B1, B2, I, E, S. 
The meaning of each tag is listed in Table 1. The 
147
Sixth SIGHAN Workshop on Chinese Language Processing
raw training file format from NCC can be easily to 
convert to this 6 tags format. 
? ? ? ? ? ?An example: /S /B /B1 /B2 /I /I 
? ? ? ? ? ? ? ? ?/I /I /I /E /B /E /B /E /S.  
 
Table 1 Tags of character-based labeling 
Tag Meaning 
B The 1st character of a multi-character word
B1 The 2nd character of a multi-character word 
B2 The 3rd character of a multi-character word 
I Other than B, B1, B2 and last character in a multi-character word 
E The last character of a multi-character word 
S Single character word 
 
The contexts window size for each character is 5: 
C-2, C-1, C0, C1, and C2. There are 10 feature 
templates used to generate features for CRF model 
including uni-gram, bi-gram and tri-gram: C-2, C-1, 
C0, C1, C2, C-1C0, C0C1, C-2C-1C0, C-1C0C1, and 
C0C1C2. 
For the parameters in CRF model, we only do 
work to choose cut-off value for features. Our 
experiments show that the best performance can be 
achieved when cut-off value is set to 2. 
Maximum likelihood estimation and L-BFGS 
algorithm is used to estimate the weight of 
parameters in the training module. Baum-Welch 
algorithm is used to search the best sequence of 
test data. 
For close test, we only used CRF to do 
segmentation, no more post-processing, such as 
time and date finding, was done. So the 
performance could be further improved. 
For open test, we just use our NER system to tag 
the output of our close segmentation result, no more 
other resources were involved. 
3 Chinese name entity recognition 
For NER task, MSRA is chosen in open test. 
Chinese name dictionary, foreign name dictionary, 
Chinese place dictionary and organization 
dictionary are used in the model. 
3.1 Maximum entropy model 
Maximum entropy model is an exponential 
model that offers the flexibility of integrating 
multiple sources of knowledge into a model 
(Berger, 1996). It focuses on the modeling of 
tagging sequence, replacing the modeling of 
observation sequence. 
Given the observations sequence X, on the basis 
of MEM, the conditional probability of the state 
sequence Y is: 
1
( | ) exp ( , )
( ) j jj
p Y X f Y X
Z X
?? ?= ??? ?
? ??         (3) 
( ) exp ( , )j j
Y j
Z X f?? ?= ??? ?
? ? Y X ??              (4) 
 
Table 2 Feature templates of NER 
Feature 
template Description 
Ci 
The word tokens in the 
window 
i =-2, -1, 0, 1, 2 
Ti 
The NE tags 
i = -1  
CiCi-1 
The bigram of Ci 
i = -1, 1 
Pi 
The POS tags of word 
tokens 
i = -1, 0, 1 
P-1P1 
The combination of POS 
tags 
T-1C0 
The previous tag and the 
current word token 
B Ci is Chinese family name 
C Ci is part of Chinese first name 
W Ci is Chinese whole name 
F Ci is foreign name 
S Ci is Chinese first name 
W(Ci) 
O other 
W(Ci-1)W(Ci) 
The bigram of W(Ci) 
i = -1, 1 
IsInOrgDict(C0)
The current word token is in 
organization dictionary 
IsInPlaceDict(C0)
The current word token is in 
place dictionary 
148
Sixth SIGHAN Workshop on Chinese Language Processing
Being Similar to the definition of CRF, Z(x) is 
the normalization factor. ( ),jf Y X is the universal 
definition of features. 
3.2 Name entity recognition based on MEM 
Firstly, we use a segmentation tool to split both 
training and test corpus into word-token-based 
texts. Characters that are not in the dictionary are 
scattered in the texts. NE tags using in the model 
follow the tags in training corpus. Other word 
tokens that do not belong to NE are tagged as O. 
Based on the segmented text, the context window 
is also set as 5. Inspired by Zhang?s (2006) work, 
there are 10 types of feature templates for 
generating features for NER model in Table 2. 
When training our ME Model, the best 
performance can be achieved when cut-off value is 
set to 1. 
Maximum likelihood estimation and GIS 
algorithm is used to estimate the weight of 
parameters in the model. The iteration time is 500.  
4 Chinese part-of-speech tagging 
For POS tagging task, NCC corpus and PKU 
corpus are chosen both in the close test and open 
test. 
4.1 Transformation-based learning 
The formalism of Transformation-based learning is 
first introduced in 1992. It starts with the correctly 
tagged training corpus. A baseline heuristic for 
initial tag and a set of rule templates that specify the 
transformation rules match the context of a word. 
By transformating the error initial tags to the correct 
ones, a set of candidate rules are built to be the 
conditional pattern based on which the 
transformation is applied. Then, the candidate rule 
which has the best transformation effect is selected 
and stored as the first transformation rules in the 
TBL model. The training process is repeated until 
no more candidate rule has the positive effect. The 
selected rules are stored in the learned rule sequence 
in turn for the purpose of template correction 
learning. 
4.2 Part-of-speech tagging based on TBL 
POS tagging is a standard sequential labeling 
problem. CRF has some advantages to solve it. 
Because both corpora have relative many POS tags, 
our computational ability can not afford the CRF 
model in condition of these tags. TBL model is 
utilized to replace with CRF. 
We compute the max probability of current 
word?s POS tag in training corpus. The POS tag 
which has max occurrence probability for each 
word is used to tag its word token. By this method, 
we got the initial POS tag for each word.  
The rule templates which are formed from 
conjunctions of words match to particular 
combinations in the histories of the current 
position. 40 types of rule templates are built using 
the patterns. The cut-off value of the 
transformation rules is set to 3 (Sun, 2007). 
For open test, our NER system is used to tag the 
output of our POS tagging result. Parts of NE tags 
are corrected. 
5 Evaluation 
Following the measurement approach adopted in 
SIGHAN, we measure the performance of the three 
tasks in terms of the precision (P), recall (R), and 
F-score (F). 
5.1 Word segmentation results 
Table 3 Word segmentation results on NCC corpus 
NCC close test open test 
R .9268 .9268 
Cr .00133447 .00133458 
P .926 .928 
Cp .00134119 .00132534 
F .9264 .9273 
Roov .6094 .6265 
Poov .4948 .5032 
Foov .5462 .5581 
Riv .9426 .9417 
Piv .9527 .9546 
Fiv .9476 9481 
 
The WS results are listed on the Table 3. Some 
errors could be caused by the annotation 
differences between the training data and test data.  
For example, ???? (A Zhen) was considered as a 
whole word in training data, while ???? (A Lan)  
was annotated as two separate word ??? (A) and 
??? (Lan) in the test data. Some post-processing 
rules for English words, money unit and 
morphology can improve the performance further, 
Following are such errors in our results: ?vid eo?, 
149
Sixth SIGHAN Workshop on Chinese Language Processing
?? ?? (Japan yen), ?? ? ? ?? (not three 
not four). 
For open test, we hoped to use NER module to 
increase the OOV recall. But the NER module 
didn?t prompt the performance very much because 
it was trained by the MSRA NER data in Bakeoff3. 
The difference between two corpora may depress 
the NER modules effect. Also, the open test was 
done on the output of close test and all the errors 
were passed. 
5.2 Name entity recognition results 
The official results of our NER system on MSRA 
corpus for open track are showed in Table 4. As it 
shows, our system achieves a relatively high score 
on both PER and LOC task, but the performance of 
ORG is not so good, and the Avg1 performance is 
decreased by it. The reasons are: (1) The ORG 
sequences are often very long and our system is 
unable to deal with the long term, a MEMM or 
CRF model may perform better. (2) The resource 
for LOC and ORG are much smaller than that of 
PER. More sophisticated features such like 
?W(Ci)? may provide more useful information for 
the system. 
 
Table 4 NER results on MSRA corpus 
MSRA P R F 
PER .9498 .9549 .9524 
LOC .9129 .9194 .9161 
ORG .8408 .7469 .7911 
Avg1 .9035 .8791 .8911 
5.3 Part-of-speech tagging results 
We evaluate our POS tagging model on the PKU 
corpus for close and open track and NCC corpus 
for close track based on TBL. Table 5 is the 
official result of our system. In PKU open test, 
NER is used to recognize name entity of text, so its 
result is better than that of close test. The IV-R 
result is relative good, but the OOV-R is not so 
good, which drops the total performance. The 
reasons lie in: (1) TBL model is not good at 
tagging out of vocabulary words. CRF model may 
be a better selection if our computer can meet its 
huge memory requirements. (2) Our NER system 
is trained by MSRA corpus. It does not fit the PKU 
and NCC corpus. 
 
Table 5 POS results on PKU and NCC corpus 
Corpus Total-A IV-R OOV-R MT-R
PKU close 
test .9113 .9518 .2708 .8958
PKU open 
test .9197 .9512 .4222 .899 
NCC close 
test .9277 .9664 .2329 .9 
6 Conclusions 
Chinese lexical analysis system is built for the 
SIGHAN tracks which consists of Chinese word 
segmentation, name entity recognition and 
part-of-speech tagging. Conditional random fields, 
maximum entropy model and transformation-based 
learning model are utilized respectively. Our 
system achieves the medium results in all the three 
tasks. 
References 
A. Berger, S. A. Della Pietra and V. J. Della Pietra. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, 1996. 22(1), 
pages 39-71. 
 
G. Sun, Y. Guan and X. Wang. A Maximum Entropy 
Chunking Model With N-fold Template Correction. 
Journal of Electronics, 2007. 24(5), pages 690-695. 
 
J. Lafferty, A. McCallum and F. Pereira. Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data. In Proceedings of 
ICML-2001, Williams College, Massachusetts, USA. 
2001. pages 282-289. 
 
S. Zhang, Y. Qin, J. Wen, X. Wang. Word 
Segmentation and Named Entity Recognition for 
SIGHAN Bakeoff3. In Proceedings of the Fifth 
SIGHAN Workshop on Chinese Language Processing. 
Sydney, Australia. 2006. pages 158?161. 
 
H. Zhao, C. Huang, and M. Li. An improved Chinese 
word segmentation system with conditional random 
field. In Proceedings of the Fifth SIGHAN Workshop 
on Chinese Language Processing, Sydney, Australia. 
2006. pages 162?165.  
 
150
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230?1238,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Modeling Semantic Relevance for Question-Answer Pairs
in Web Social Communities
Baoxun Wang, Xiaolong Wang, Chengjie Sun, Bingquan Liu, Lin Sun
School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China
{bxwang, wangxl, cjsun, liubq, lsun}@insun.hit.edu.cn
Abstract
Quantifying the semantic relevance be-
tween questions and their candidate an-
swers is essential to answer detection in
social media corpora. In this paper, a deep
belief network is proposed to model the
semantic relevance for question-answer
pairs. Observing the textual similarity
between the community-driven question-
answering (cQA) dataset and the forum
dataset, we present a novel learning strat-
egy to promote the performance of our
method on the social community datasets
without hand-annotating work. The ex-
perimental results show that our method
outperforms the traditional approaches on
both the cQA and the forum corpora.
1 Introduction
In natural language processing (NLP) and infor-
mation retrieval (IR) fields, question answering
(QA) problem has attracted much attention over
the past few years. Nevertheless, most of the QA
researches mainly focus on locating the exact an-
swer to a given factoid question in the related doc-
uments. The most well known international evalu-
ation on the factoid QA task is the Text REtrieval
Conference (TREC)1, and the annotated questions
and answers released by TREC have become im-
portant resources for the researchers. However,
when facing a non-factoid question such as why,
how, or what about, however, almost no automatic
QA systems work very well.
The user-generated question-answer pairs are
definitely of great importance to solve the non-
factoid questions. Obviously, these natural QA
pairs are usually created during people?s com-
munication via Internet social media, among
which we are interested in the community-driven
1http://trec.nist.gov
question-answering (cQA) sites and online fo-
rums. The cQA sites (or systems) provide plat-
forms where users can either ask questions or de-
liver answers, and best answers are selected man-
ually (e.g., Baidu Zhidao2 and Yahoo! Answers3).
Comparing with cQA sites, online forums have
more virtual society characteristics, where people
hold discussions in certain domains, such as tech-
niques, travel, sports, etc. Online forums contain
a huge number of QA pairs, and much noise infor-
mation is involved.
To make use of the QA pairs in cQA sites and
online forums, one has to face the challenging
problem of distinguishing the questions and their
answers from the noise. According to our investi-
gation, the data in the community based sites, es-
pecially for the forums, have two obvious charac-
teristics: (a) a post usually includes a very short
content, and when a person is initializing or re-
plying a post, an informal tone tends to be used;
(b) most of the posts are useless, which makes
the community become a noisy environment for
question-answer detection.
In this paper, a novel approach for modeling the
semantic relevance for QA pairs in the social me-
dia sites is proposed. We concentrate on the fol-
lowing two problems:
1. How to model the semantic relationship be-
tween two short texts using simple textual fea-
tures? As mentioned above, the user generated
questions and their answers via social media are
always short texts. The limitation of length leads
to the sparsity of the word features. In addition,
the word frequency is usually either 0 or 1, that is,
the frequency offers little information except the
occurrence of a word. Because of this situation,
the traditional relevance computing methods based
on word co-occurrence, such as Cosine similarity
and KL-divergence, are not effective for question-
2http://zhidao.baidu.com
3http://answers.yahoo.com
1230
answer semantic modeling. Most researchers try
to introduce structural features or users? behavior
to improve the models performance, by contrast,
the effect of textual features is not obvious.
2. How to train a model so that it has good per-
formance on both cQA and forum datasets? So
far, people have been doing QA researches on the
cQA and the forum datasets separately (Ding et
al., 2008; Surdeanu et al, 2008), and no one has
noticed the relationship between the two kinds of
data. Since both the cQA systems and the online
forums are open platforms for people to commu-
nicate, the QA pairs in the cQA systems have sim-
ilarity with those in the forums. In this case, it is
highly valuable and desirable to propose a train-
ing strategy to improve the model?s performance
on both of the two kinds of datasets. In addition,
it is possible to avoid the expensive and arduous
hand-annotating work by introducing the method.
To solve the first problem, we present a deep
belief network (DBN) to model the semantic rel-
evance between questions and their answers. The
network establishes the semantic relationship for
QA pairs by minimizing the answer-to-question
reconstructing error. Using only word features,
our model outperforms the traditional methods on
question-answer relevance calculating.
For the second problem, we make our model
to learn the semantic knowledge from the solved
question threads in the cQA system. Instead of
mining the structure based features from cQA
pages and forum threads individually, we con-
sider the textual similarity between the two kinds
of data. The semantic information learned from
cQA corpus is helpful to detect answers in forums,
which makes our model show good performance
on social media corpora. Thanks to the labels for
the best answers existing in the threads, no manual
work is needed in our strategy.
The rest of this paper is organized as follows:
Section 2 surveys the related work. Section 3 in-
troduces the deep belief network for answer de-
tection. In Section 4, the homogenous data based
learning strategy is described. Experimental result
is given in Section 5. Finally, conclusions and fu-
ture directions are drawn in Section 6.
2 Related Work
The value of the naturally generated question-
answer pairs has not been recognized until recent
years. Early studies mainly focus on extracting
QA pairs from frequently asked questions (FAQ)
pages (Jijkoun and de Rijke, 2005; Riezler et al,
2007) or service call-center dialogues (Berger et
al., 2000).
Judging whether a candidate answer is seman-
tically related to the question in the cQA page
automatically is a challenging task. A frame-
work for predicting the quality of answers has
been presented in (Jeon et al, 2006). Bernhard
and Gurevych (2009) have developed a transla-
tion based method to find answers. Surdeanu et
al. (2008) propose an approach to rank the an-
swers retrieved by Yahoo! Answers. Our work is
partly similar to Surdeanu et al (2008), for we also
aim to rank the candidate answers reasonably, but
our ranking algorithm needs only word informa-
tion, instead of the combination of different kinds
of features.
Because people have considerable freedom to
post on forums, there are a great number of irrel-
evant posts for answering questions, which makes
it more difficult to detect answers in the forums.
In this field, exploratory studies have been done by
Feng et al (2006) and Huang et al (2007), who ex-
tract input-reply pairs for the discussion-bot. Ding
et al(2008) and Cong et al(2008) have also pre-
sented outstanding research works on forum QA
extraction. Ding et al (2008) detect question con-
texts and answers using the conditional random
fields, and a ranking algorithm based on the au-
thority of forum users is proposed by Cong et al
(2008). Treating answer detection as a binary clas-
sification problem is an intuitive idea, thus there
are some studies trying to solve it from this view
(Hong and Davison, 2009; Wang et al, 2009). Es-
pecially Hong and Davison (2009) have achieved
a rather high precision on the corpora with less
noise, which also shows the importance of ?social?
features.
In order to select the answers for a given ques-
tion, one has to face the problem of lexical gap.
One of the problems with lexical gap embedding
is to find similar questions in QA achieves (Jeon et
al., 2005). Recently, the statistical machine trans-
lation (SMT) strategy has become popular. Lee et
al. (2008) use translate models to bridge the lexi-
cal gap between queries and questions in QA col-
lections. The SMT based methods are effective on
modeling the semantic relationship between ques-
tions and answers and expending users? queries in
answer retrieval (Riezler et al, 2007; Berger et al,
1231
2000; Bernhard and Gurevych, 2009). In (Sur-
deanu et al, 2008), the translation model is used
to provide features for answer ranking.
The structural features (e.g., authorship, ac-
knowledgement, post position, etc), also called
non-textual features, play an important role in an-
swer extraction. Such features are used in (Ding
et al, 2008; Cong et al, 2008), and have signifi-
cantly improved the performance. The studies of
Jeon et al (2006) and Hong et al (2009) show that
the structural features have even more contribution
than the textual features. In this case, the mining
of textual features tends to be ignored.
There are also some other research topics in this
field. Cong et al (2008) and Wang et al (2009)
both propose the strategies to detect questions in
the social media corpus, which is proved to be a
non-trivial task. The deep research on question
detection has been taken by Duan et al (2008).
A graph based algorithm is presented to answer
opinion questions (Li et al, 2009). In email sum-
marization field, the QA pairs are also extracted
from email contents as the main elements of email
summarization (Shrestha and McKeown, 2004).
3 The Deep Belief Network for QA pairs
Due to the feature sparsity and the low word fre-
quency of the social media corpus, it is difficult
to model the semantic relevance between ques-
tions and answers using only co-occurrence fea-
tures. It is clear that the semantic link exists be-
tween the question and its answers, even though
they have totally different lexical representations.
Thus a specially designed model may learn se-
mantic knowledge by reconstructing a great num-
ber of questions using the information in the cor-
responding answers. In this section, we propose
a deep belief network for modeling the seman-
tic relationship between questions and their an-
swers. Our model is able to map the QA data into
a low-dimensional semantic-feature space, where
a question is close to its answers.
3.1 The Restricted Boltzmann Machine
An ensemble of binary vectors can be modeled us-
ing a two-layer network called a ?restricted Boltz-
mann machine? (RBM) (Hinton, 2002). The di-
mension reducing approach based on RBM ini-
tially shows good performance on image process-
ing (Hinton and Salakhutdinov, 2006). Salakhut-
dinov and Hinton (2009) propose a deep graphical
model composed of RBMs into the information re-
trieval field, which shows that this model is able to
obtain semantic information hidden in the word-
count vectors.
As shown in Figure 1, the RBM is a two-layer
network. The bottom layer represents a visible
vector v and the top layer represents a latent fea-
ture h. The matrix W contains the symmetric in-
teraction terms between the visible units and the
hidden units. Given an input vector v, the trained
Figure 1: Restricted Boltzmann machine
RBM model provides a hidden feature h, which
can be used to reconstruct v with a minimum er-
ror. The training algorithm for this paper will be
described in the next subsection. The ability of the
RBM suggests us to build a deep belief network
based on RBM so that the semantic relevance be-
tween questions and answers can be modeled.
3.2 Pretraining a Deep Belief Network
In the social media corpora, the answers are al-
ways descriptive, containing one or several sen-
tences. Noticing that an answer has strong seman-
tic association with the question and involves more
information than the question, we propose to train
a deep belief network by reconstructing the ques-
tion using its answers. The training object is to
minimize the error of reconstruction, and after the
pretraining process, a point that lies in a good re-
gion of parameter space can be achieved.
Firstly, the illustration of the DBN model is
given in Figure 2. This model is composed of
three layers, and here each layer stands for the
RBM or its variant. The bottom layer is a variant
form of RBM?s designed for the QA pairs. This
layer we design is a little different from the classi-
cal RBM?s, so that the bottom layer can generate
the hidden features according to the visible answer
vector and reconstruct the question vector using
the hidden features. The pre-training procedure of
this architecture is practically convergent. In the
bottom layer, the binary feature vectors based on
the statistics of the word occurrence in the answers
are used to compute the ?hidden features? in the
1232
Figure 2: The Deep Belief Network for QA Pairs
hidden units. The model can reconstruct the ques-
tions using the hidden features. The processes can
be modeled as follows:
p(h j = 1|a) = ?(b j +
?
i
wi jai) (1)
p(qi = 1|h) = ?(bi +
?
j
wi jh j) (2)
where ?(x) = 1/(1 + e?x), a denotes the visible
feature vector of the answer, qi is the ith element
of the question vector, and h stands for the hid-
den feature vector for reconstructing the questions.
wi j is a symmetric interaction term between word
i and hidden feature j, bi stands for the bias of the
model for word i, and b j denotes the bias of hidden
feature j.
Given the training set of answer vectors, the bot-
tom layer generates the corresponding hidden fea-
tures using Equation 1. Equation 2 is used to re-
construct the Bernoulli rates for each word in the
question vectors after stochastically activating the
hidden features. Then Equation 1 is taken again
to make the hidden features active. We use 1-step
Contrastive Divergence (Hinton, 2002) to update
the parameters by performing gradient ascent:
?wi j = (< qih j >qData ? < qih j >qRecon) (3)
where < qih j >qData denotes the expectation of
the frequency with which the word i in a ques-
tion and the feature j are on together when the
hidden features are driven by the question data.
< qih j >qRecon defines the corresponding expec-
tation when the hidden features are driven by the
reconstructed question data.  is the learning rate.
The classical RBM structure is taken to build
the middle layer and the top layer of the network.
The training method for the higher two layer is
similar to that of the bottom one, and we only have
to make each RBM to reconstruct the input data
using its hidden features. The parameter updates
still obeying the rule defined by gradient ascent,
which is quite similar to Equation 3. After train-
ing one layer, the h vectors are then sent to the
higher-level layer as its ?training data?.
3.3 Fine-tuning the Weights
Notice that a greedy strategy is taken to train each
layer individually during the pre-training proce-
dure, it is necessary to fine-tune the weights of the
entire network for optimal reconstruction. To fine-
tune the weights, the network is unrolled, taking
the answers as the input data to generate the corre-
sponding questions at the output units. Using the
cross-entropy error function, we can then tune the
network by performing backpropagation through
it. The experiment results in section 5.2 will show
fine-tuning makes the network performs better for
answer detection.
3.4 Best answer detection
After pre-training and fine-tuning, a deep belief
network for QA pairs is established. To detect the
best answer to a given question, we just have to
send the vectors of the question and its candidate
answers into the input units of the network and
perform a level-by-level calculation to obtain the
corresponding feature vectors. Then we calculate
the distance between the mapped question vector
and each candidate answer vector. We consider the
candidate answer with the smallest distance as the
best one.
4 Learning with Homogenous Data
In this section, we propose our strategy to make
our DBN model to detect answers in both cQA and
forum datasets, while the existing studies focus on
one single dataset.
4.1 Homogenous QA Corpora from Different
Sources
Our motivation of finding the homogenous
question-answer corpora from different kind of so-
cial media is to guarantee the model?s performance
and avoid hand-annotating work.
In this paper, we get the ?solved question? pages
in the computer technology domain from Baidu
Zhidao as the cQA corpus, and the threads of
1233
Figure 3: Comparison of the post content lengths in the cQA and the forum datasets
ComputerFansClub Forum4 as the online forum
corpus. The domains of the corpora are the same.
To further explain that the two corpora are ho-
mogenous, we will give the detail comparison on
text style and word distribution.
As shown in Figure 3, we have compared the
post content lengths of the cQA and the forum
in our corpora. For the comparison, 5,000 posts
from the cQA corpus and 5,000 posts from the fo-
rum corpus are randomly selected. The left panel
shows the statistical result on the Baidu Zhidao
data, and the right panel shows the one on the fo-
rum data. The number i on the horizontal axis de-
notes the post contents whose lengths range from
10(i? 1) + 1 to 10i bytes, and the vertical axis rep-
resents the counts of the post contents. From Fig-
ure 3 we observe that the contents of most posts
in both the cQA corpus and the forum corpus are
short, with the lengths not exceeding 400 bytes.
The content length reflects the text style of the
posts in cQA systems and online forums. From
Figure 3 it can be also seen that the distributions
of the content lengths in the two figures are very
similar. It shows that the contents in the two cor-
pora are both mainly short texts.
Figure 4 shows the percentage of the concurrent
words in the top-ranked content words with high
frequency. In detail, we firstly rank the words by
frequency in the two corpora. The words are cho-
sen based on a professional dictionary to guarantee
that they are meaningful in the computer knowl-
edge field. The number k on the horizontal axis in
Figure 4 represents the top k content words in the
4http://bbs.cfanclub.net/
corpora, and the vertical axis stands for the per-
centage of the words shared by the two corpora in
the top k words.
Figure 4: Distribution of concurrent content words
Figure 4 shows that a large number of meaning-
ful words appear in both of the two corpora with
high frequencies. The percentage of the concur-
rent words maintains above 64% in the top 1,400
words. It indicates that the word distributions of
the two corpora are quite similar, although they
come from different social media sites.
Because the cQA corpus and the forum corpus
used in this study have homogenous characteris-
tics for answer detecting task, a simple strategy
may be used to avoid the hand-annotating work.
Apparently, in every ?solved question? page of
Baidu Zhidao, the best answer is selected by the
user who asks this question. We can easily extract
the QA pairs from the cQA corpus as the training
1234
set. Because the two corpora are similar, we can
apply the deep belief network trained by the cQA
corpus to detect answers on both the cQA data and
the forum data.
4.2 Features
The task of detecting answers in social media cor-
pora suffers from the problem of feature sparsity
seriously. High-dimensional feature vectors with
only several non-zero dimensions bring large time
consumption to our model. Thus it is necessary to
reduce the dimension of the feature vectors.
In this paper, we adopt two kinds of word fea-
tures. Firstly, we consider the 1,300 most fre-
quent words in the training set as Salakhutdinov
and Hinton (2009) did. According to our statis-
tics, the frequencies of the rest words are all less
then 10, which are not statistically significant and
may introduce much noise.
We take the occurrence of some function words
as another kind of features. The function words
are quite meaningful for judging whether a short
text is an answer or not, especially for the non-
factoid questions. For example, in the answers to
the causation questions, the words such as because
and so are more likely to appear; and the words
such as firstly, then, and should may suggest the
answers to the manner questions. We give an ex-
ample for function word selection in Figure 5.
Figure 5: An example for function word selection
For this reason, we collect 200 most frequent
function words in the answers of the training set.
Then for every short text, either a question or an
answer, a 1,500-dimensional vector can be gener-
ated. Specifically, all the features we have adopted
are binary, for they only have to denote whether
the corresponding word appears in the text or not.
5 Experiments
To evaluate our question-answer semantic rele-
vance computing method, we compare our ap-
proach with the popular methods on the answer
detecting task.
5.1 Experiment Setup
Architecture of the Network: To build the deep
belief network, we use a 1500-1500-1000-600 ar-
chitecture, which means the three layers of the net-
work have individually 1,500?1,500, 1,500?1,000
and 1,000?600 units. Using the network, a 1,500-
dimensional binary vector is finally mapped to a
600-dimensional real-value vector.
During the pretraining stage, the bottom layer
is greedily pretrained for 200 passes through the
entire training set, and each of the rest two layers is
greedily pretrained for 50 passes. For fine-tuning
we apply the method of conjugate gradients5, with
three line searches performed in each pass. This
algorithm is performed for 50 passes to fine-tune
the network.
Dataset: we have crawled 20,000 pages of
?solved question? from the computer and network
category of Baidu Zhidao as the cQA corpus. Cor-
respondingly we obtain 90,000 threads from Com-
puterFansClub, which is an online forum on com-
puter knowledge. We take the forum threads as
our forum corpus.
From the cQA corpus, we extract 12,600 human
generated QA pairs as the training set without any
manual work to label the best answers. We get the
contents from another 2,000 cQA pages to form
a testing set, each content of which includes one
question and 4.5 candidate answers on average,
with one best answer among them. To get another
testing dataset, we randomly select 2,000 threads
from the forum corpus. For this training set, hu-
man work are necessary to label the best answers
in the posts of the threads. There are 7 posts in-
cluded in each thread on average, among which
one question and at least one answer exist.
Baseline: To show the performance of our
method, three main popular relevance computing
methods for ranking candidate answers are con-
sidered as our baselines. We will briefly introduce
them:
Cosine Similarity. Given a question q and its
candidate answer a, their cosine similarity can be
computed as follows:
cos(q, a) =
?n
k=1 wqk ? wak??n
k=1 w2qk ?
??n
k=1 w2ak
(4)
where wqk and wak stand for the weight of the kth
word in the question and the answer respectively.
5Code is available at
http://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/
1235
The weights can be get by computing the product
of term frequency (tf ) and inverse document fre-
quency (idf )
HowNet based Similarity. HowNet6 is an elec-
tronic world knowledge system, which serves as
a powerful tool for meaning computation in hu-
man language technology. Normally the similar-
ity between two passages can be calculated by
two steps: (1) matching the most semantic-similar
words in each passages greedily using the API?s
provided by HowNet; (2) computing the weighted
average similarities of the word pairs. This strat-
egy is taken as a baseline method for computing
the relevance between questions and answers.
KL-divergence Language Model. Given a ques-
tion q and its candidate answer a, we can con-
struct unigram language model Mq and unigram
language model Ma. Then we compute KL-
divergence between Mq and Ma as below:
KL(Ma||Mq) =
?
w
p(w|Ma) log(p(w|Ma)/p(w|Mq))
(5)
5.2 Results and Analysis
We evaluate the performance of our approach for
answer detection using two metrics: Precision@1
(P@1) and Mean Reciprocal Rank (MRR). Ap-
plying the two metrics, we perform the baseline
methods and our DBN based methods on the two
testing set above.
Table 1 lists the results achieved on the forum
data using the baseline methods and ours. The ad-
ditional ?Nearest Answer? stands for the method
without any ranking strategies, which returns the
nearest candidate answer from the question by po-
sition. To illustrate the effect of the fine-tuning for
our model, we list the results of our method with-
out fine-tuning and the results with fine-tuning.
As shown in Table 1, our deep belief network
based methods outperform the baseline methods
as expected. The main reason for the improve-
ments is that the DBN based approach is able to
learn semantic relationship between the words in
QA pairs from the training set. Although the train-
ing set we offer to the network comes from a dif-
ferent source (the cQA corpus), it still provide
enough knowledge to the network to perform bet-
ter than the baseline methods. This phenomena in-
dicates that the homogenous corpora for training is
6Detail information can be found in:
http://www.keenage.com/
effective and meaningful.
Method P@1 (%) MRR (%)
Nearest Answer 21.25 38.72
Cosine Similarity 23.15 43.50
HowNet 22.55 41.63
KL divergence 25.30 51.40
DBN (without FT) 41.45 59.64
DBN (with FT) 45.00 62.03
Table 1: Results on Forum Dataset
We have also investigated the reasons for the un-
satisfying performance of the baseline approaches.
Basically, the low precision is ascribable to the
forum corpus we have obtained. As mentioned
in Section 1, the contents of the forum posts are
short, which leads to the sparsity of the features.
Besides, when users post messages in the online
forums, they are accustomed to be casual and use
some synonymous words interchangeably in the
posts, which is believed to be a significant situ-
ation in Chinese forums especially. Because the
features for QA pairs are quite sparse and the con-
tent words in the questions are usually morpholog-
ically different from the ones with the same mean-
ing in the answers, the Cosine Similarity method
become less powerful. For HowNet based ap-
proaches, there are a large number of words not
included by HowNet, thus it fails to compute the
similarity between questions and answers. KL-
divergence suffers from the same problems with
the Cosine Similarity method. Compared with
the Cosine Similarity method, this approach has
achieved the improvement of 9.3% in P@1, but
it performs much better than the other baseline
methods in MRR.
The baseline results indicate that the online fo-
rum is a complex environment with large amount
of noise for answer detection. Traditional IR
methods using pure textual features can hardly
achieve good results. The similar baseline results
for forum answer ranking are also achieved by
Hong and Davison (2009), which takes some non-
textual features to improve the algorithm?s perfor-
mance. We also notice that, however, the baseline
methods have obtained better results on forum cor-
pus (Cong et al, 2008). One possible reason is that
the baseline approaches are suitable for their data,
since we observe that the ?nearest answer? strat-
egy has obtained a 73.5% precision in their work.
Our model has achieved the precision of
1236
45.00% in P@1 and 62.03% in MRR for answer
detecting on forum data after fine-tuning, while
some related works have reported the results with
the precision over 90% (Cong et al, 2008; Hong
and Davison, 2009). There are mainly two rea-
sons for this phenomena: Firstly, both of the pre-
vious works have adopt non-textual features based
on the forum structure, such as authorship, po-
sition and quotes, etc. The non-textual (or so-
cial based) features have played a significant role
in improving the algorithms? performance. Sec-
ondly, the quality of corpora influences the results
of the ranking strategies significantly, and even
the same algorithm may perform differently when
the dataset is changed (Hong and Davison, 2009).
For the experiments of this paper, large amount of
noise is involved in the forum corpus and we have
done nothing extra to filter it.
Table 2 shows the experimental results on the
cQA dataset. In this experiment, each sample is
composed of one question and its following sev-
eral candidate answers. We delete the ones with
only one answer to confirm there are at least two
candidate answers for each question. The candi-
date answers are rearranged by post time, so that
the real answers do not always appear next to the
questions. In this group of experiment, no hand-
annotating work is needed because the real an-
swers have been labeled by cQA users.
Method P@1 (%) MRR (%)
Nearest Answer 36.05 56.33
Cosine Similarity 44.05 62.84
HowNet 41.10 58.75
KL divergence 43.75 63.10
DBN (without FT) 56.20 70.56
DBN (with FT) 58.15 72.74
Table 2: Results on cQA Dataset
From Table 2 we observe that all the approaches
perform much better on this dataset. We attribute
the improvements to the high quality QA corpus
Baidu Zhidao offers: the candidate answers tend to
be more formal than the ones in the forums, with
less noise information included. In addition, the
?Nearest Answer? strategy has reached 36.05% in
P@1 on this dataset, which indicates quite a num-
ber of askers receive the real answers at the first
answer post. This result has supported the idea of
introducing position features. What?s more, if the
best answer appear immediately, the asker tends
to lock down the question thread, which helps to
reduce the noise information in the cQA corpus.
Despite the baseline methods? performances
have been improved, our approaches still outper-
form them, with a 32.0% improvement in P@1
and a 15.3% improvement in MRR at least. On
the cQA dataset, our model shows better perfor-
mance than the previous experiment, which is ex-
pected because the training set and the testing set
come from the same corpus, and the DBN model
is more adaptive to the cQA data.
We have observed that, from both of the two
groups of experiments, fine-tuning is effective for
enhancing the performance of our model. On the
forum data, the results have been improved by
8.6% in P@1 and 4.0% in MRR, and the improve-
ments are 3.5% and 3.1% individually.
6 Conclusions
In this paper, we have proposed a deep belief net-
work based approach to model the semantic rel-
evance for the question answering pairs in social
community corpora.
The contributions of this paper can be summa-
rized as follows: (1) The deep belief network we
present shows good performance on modeling the
QA pairs? semantic relevance using only word fea-
tures. As a data driven approach, our model learns
semantic knowledge from large amount of QA
pairs to represent the semantic relevance between
questions and their answers. (2) We have stud-
ied the textual similarity between the cQA and the
forum datasets for QA pair extraction, and intro-
duce a novel learning strategy to make our method
show good performance on both cQA and forum
datasets. The experimental results show that our
method outperforms the traditional approaches on
both the cQA and the forum corpora.
Our future work will be carried out along two
directions. Firstly, we will further improve the
performance of our method by adopting the non-
textual features. Secondly, more research will be
taken to put forward other architectures of the deep
networks for QA detection.
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. Special
thanks to Deyuan Zhang, Bin Liu, Beidong Liu
and Ke Sun for insightful suggestions. This work
is supported by NSFC (60973076).
1237
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In In Proceedings of the 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 192?199.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining lexical semantic resources with question &
answer archives for translation-based answer find-
ing. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 728?736, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ?08: Proceed-
ings of the 31st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 467?474, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL-08: HLT, pages
710?718, Columbus, Ohio, June. Association for
Computational Linguistics.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of
ACL-08: HLT, pages 156?164, Columbus, Ohio,
June. Association for Computational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Ccile Paris and Candace L. Sidner, editors, IUI,
pages 171?177. ACM.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Georey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering
in discussion boards. In SIGIR ?09: Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 171?178, New York, NY, USA. ACM.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence,
pages 423?428, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM ?05, pages 84?90, New
York, NY, USA. ACM.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In SIGIR ?06,
pages 228?235, New York, NY, USA. ACM.
Valentin Jijkoun and Maarten de Rijke. 2005. Retriev-
ing answers from frequently asked questions pages
on the web. In CIKM ?05, pages 76?83, New York,
NY, USA. ACM.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging lexical gaps be-
tween queries and questions on large online q&a
collections with compact translation models. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 410?418, Morristown, NJ, USA. Association
for Computational Linguistics.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
737?745, Suntec, Singapore, August. Association
for Computational Linguistics.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 464?471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Semantic hashing. Int. J. Approx. Reasoning,
50(7):969?978.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of Coling 2004, pages 889?
895, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online QA collections. In Proceedings of ACL-08:
HLT, pages 719?727, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiao-
long Wang, and Lin Sun. 2009. Extracting chinese
question-answer pairs from online forums. In SMC
2009: Proceedings of the IEEE International Con-
ference on Systems, Man and Cybernetics, 2009.,
pages 1159?1164.
1238
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 100?105,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Detect Hedges and their Scope Using CRF 
Qi Zhao, Chengjie Sun, Bingquan Liu, Yong Cheng 
Harbin Institute of Technology, HIT 
Harbin, PR China 
{qzhao, cjsun, liubq, ycheng}@insun.hit.edu.cn 
 
Abstract 
Detecting speculative assertions is essential 
to distinguish the facts from uncertain 
information for biomedical text. This paper 
describes a system to detect hedge cues and 
their scope using CRF model. HCDic feature 
is presented to improve the system perfor-
mance of detecting hedge cues on BioScope 
corpus. The feature can make use of cross-
domain resources.  
1 Introduction 
George Lakoff (1972) first introduced linguistic 
hedges which indicate that speakers do not back 
up their opinions with facts. Later other linguists 
followed the social functions of hedges closely. 
Interestingly, Robin Lakoff (1975) introduces 
that hedges might be one of the ?women?s 
language features? as they have higher frequency 
in women?s languages than in men?s. 
In the natural language processing domain, 
hedges are very important, too. Along with the 
rapid development of computational and 
biological technology, information extraction 
from huge amount of biomedical resource 
becomes more and more important. While the 
uncertain information can be a noisy factor 
sometimes, affecting the performance of 
information extraction. Biomedical articles are 
rich in speculative, while 17.70% of the 
sentences in the abstracts section of the 
BioScope corpus and 19.44% of the sentences in 
the full papers section contain hedge cues 
(Vincze et al, 2008). In order to distinguish facts 
from uncertain information, detecting speculative 
assertions is essential in biomedical text.  
Hedge detection is paid attention to in the 
biomedical NLP field. Some researchers regard 
the problem as a text classification problem (a 
sentence is speculative or not) using simple 
machine learning techniques. Light et al (2004) 
use substring matching to annotate speculation in 
biomedical text. Medlock and Briscoe (2007) 
create a hedging dataset and use an SVM 
classifier and get to a recall/precision Break-
Even Point (BEP) of 0.76. They report that the 
POS feature performs badly, while lemma 
feature works well. Szarvas (2008) extends the 
work of Medlock and Briscoe with feature 
selection, and further improves the result to a 
BEP of 0.85 by using an external dictionary. 
Szarvas concludes that scientific articles contain 
multiword hedging cues more commonly, and 
the portability of hedge classifiers is limited. 
Halil Kilicoglu and Sabine Bergler (2008) 
propose an algorithm to weight hedge cues, 
which are used to evaluate the speculative 
strength of sentences. Roser Morante and Walter 
Daelemans (2009) introduce a metalearning 
approach to process the scope of negation, and 
they identify the hedge cues and their scope with 
a CRF classifier based on the original work. 
They extract a hedge cues dictionary as well, but 
do not combine it with the CRF model. 
In the CoNLL-2010 shared task (Farkas et al, 
2010), there are two subtasks for worldwide 
participants to choose: 
? Task 1: learning to detect sentences 
contain-ing uncertainty.  
? Task 2: learning to resolve the in-
sentence scope of hedge cues.  
This paper describes a system using CRF 
model for the task, which is partly based on 
Roser Morante and Walter Daelemans? work. 
2 Hedges in the training dataset of 
BioScope and Wikipedia Corpus 
Two training datasets, the BioScope and Wiki-
pedia corpus are provided in the CoNLL-2010 
shared task. BioScope consists of two parts, full 
articles and abstracts collected from biomedical 
papers. The latter is analyzed for having larger 
scale and more information of hedges.  
In Table 1, the percentage of the speculative 
sentences in the abstracts section of BioScope 
corpus is the same as Vincze et al (2008) 
reported. We can estimate 1.28 cue words per 
sentence, meaning that each sentence usually just 
has one hedge cue. The statistics in Table 1 also 
100
indicate that a hedge cue appears 26.7 times on 
average. 
 
Dataset ITEM # 
Sentences 11871 
Certain sentences 9770 
Uncertain 
sentences 
2101 
(17.7%) 
Hedge cues 2694 
cues# per sentence 1.28 
Different hedge 
cues 
143 
Abstracts 
of 
BioScope 
Max length of the 
cues 
4 
Sentences 11111 
Certain sentences 8627 
Uncertain 
sentences 
2484 
(22.4%) 
weasel cues 3133 
Different weasel 
cues 
1984 
Wikipedia 
Max length of the 
cues 
13 words 
 
Table 1: Statistics about the abstracts section of 
the BioScope corpus and Wikipedia corpus. 
 
We extract all the hedge cues from the 
abstracts section of BioScope corpus, getting 143 
different hedge cues and 101 cues with ignoring 
morphological changes. The maximum length of 
the cues is 4, with 1.44 words per hedge cue. 
This suggests that most hedge cues happen to be 
a single word. We assume that hedge cues set is 
a limited one in BioScope corpus. Most hedge 
cues could be identified if the known dataset of 
hedge cues is large enough. The cue words 
collected from the BioScope corpus play an 
important role in the speculative sentences 
detection. 
In contrast to the biomedical abstracts, the 
weasel cues on Wikipedia corpus make a little 
difference. Most weasel cues consist of more 
than one word, and usually appear once. This 
leads to different results in our test. 
A hedge cue word may appear in the non-
speculative sentences. Occurrences of the four 
typical words in speculative and non-speculative 
sentences are counted. 
As shown in Table 2, the cue words can be 
divided into two classes generally. The hedge 
cue words ?feel? and ?suggesting?, which are 
grouped as one class, only act as hedge cues with 
never appearing in the non-speculative sentences. 
While ?may? and ?or? appear both in the 
speculative and non-speculative sentences, which 
are regard as the other one. Moreover, we treat 
the words ?may? and ?or? in the same class 
differently, while ?may? is more likely to be a 
hedge cue than ?or?. The treatment is also 
unequal between ?feel? and ?suggesting?. In the 
training datasets, the non-S#/S# ratio can give a 
weight to distinguish the words in each class. 
After all, we can divide the hedge cues into 4 
groups. 
 
word S# non-S# 
feel 1 0 
suggesting 150 0 
may 516 1 
or 118 6218 
 
Table 2: Statistics of cue words. (S# short for the 
occurrence times in speculative sentences, non-
S# for the count in non-speculative ones) 
3 Methods 
Conditional random fields (CRF) model was 
firstly introduced by Lafferty et al (2001). CRF 
model can avoid the label bias problem of 
HMMs and other learning approaches. It was 
applied to solve sequence-labeling problems, and 
has shown good performance in NER task. We 
consider hedge cues detection as some kind of 
sequence-labeling problem, and the model will 
contribute to a good result.  
We use CRF++ (version 0.51) to implement 
the CRF model. Cheng Yong, one of our team 
members has evaluated the several widespread 
used CRF tool kits, and he points out that 
CRF++ has better precision and recall but longer 
training time. Fortunately, the training time cost 
of BioScope corpus is acceptable. In our system, 
all the data training and testing processing step 
can be completed within 8 minutes (Intel Xeon 
2.0GHz CPU, 6GB RAM). It is likely due to the 
small scale of the training dataset and the limited 
types of the annotation. 
To identify sentences in the biomedical texts 
that contain unreliable or uncertain information 
(CoNLL-2010 shared task1), we start with hedge 
cues detection: 
? If one or more than one hedge cues are 
detected in the sentence, then it will be 
annotated ?uncertain? 
? If not, the sentence will be tagged as 
?certain?. 
101
3.1 Detecting hedge cues 
The BioScope corpus annotation guidelines 1 
show that most typical instances of keywords can 
be grouped into 4 types as Auxiliaries, Verbs of 
hedging or verbs with speculative content, 
Adjectives or adverbs, and Conjunctions. So the 
POS (part-of-speech) is thought to be the feature 
reasonably. Lemma feature of the word and 
chunk features are also considered to improve 
system performance. Chunk features may help to 
the recognition of biomedical entity boundaries. 
GENIA Tagger (Tsuruoka et al, 2005) is em-
ployed to obtain part-of-speech (POS) features, 
chunk features and lemma features. It works well 
for biomedical documents. 
In the biomedical abstracts section of Bio-
Scope corpus, the hedge cues are collected into a 
dictionary (HCDic, short for the Hedge Cues 
Dictionary). As mentioned in section 2, one 
hedge cue appears 26.7 times on average, and we 
assume the set of hedge cues is limited. The 
HCDic consist of 143 different hedge cues 
extracted from the abstracts. The dictionary 
(HCDic) extracted from the corpus is very 
valuable for the system. We can focus on 
whether the word such as ?or? listed in table 2 is 
a hedge cue or not. The cue words in HCDic are 
divided into 4 different levels with the non-S#/S# 
ratio. 
The four types are described as ?L?, ?H?, 
?FL? and ?FH?. ?L? shows low confidence of 
the cue word being a hedge cue, while ?H? 
indicates high confidence about it. The prefix ?F? 
for ?FL?/?FH? shows false negatives may 
happen to the cue word in HCDic. The threshold 
for the non-S#/S# ratio to distinguish ?FL? type 
from ?FH? is set 1.0. As the non-S#/S# ratio of 
?L? and ?H? is always zero, we set the hedge cue 
whose S# is more than 5 as ?H? type as shown in 
table 3. The four types are added into the HCDic 
along with the hedge cues,  
In our experiment, HCDic types of word 
sequence are tagged as follows: 
? If words are found in HCDic using 
maximum matching method, label them 
with their types in HCDic. For hedges of 
multi-word, label them with BI scheme 
which will be described later. 
? If not, tag the words as ?O? type.  
                                                 
1
 http://www.inf.u-szeged.hu/rgai/bioscope 
The processing assigns each token of a 
sentence with an HCDic type. The BIO types for 
each token are involved as features for the CRF. 
The HCDic can be expanded to a larger scale. 
Hedge cues extracted from different corpora can 
be added into HCDic, and regular expression of 
hedge cues can be used, too. This will be helpful 
to the usage of cross-domain resources. 
 
word S# non-S# type  
feel 1 0 L 
suggesting 150 0 H 
may 516 1 FH 
or 118 6218 FL 
 
Table 3: Types of the HCDic words. (S# and 
non-S# have the same meaning as in Table 2) 
 
The features F (F stands for all the Features) 
including unigram, bigram, and trigram types is 
used for CRF as follows: 
 
F(n)(n=-2,-1,0,+1,+2) 
F(n-1)F(n)(n=-1,0,+1,+2) 
F(n-2)F(n-1)F(n) (n=0,+1,+2) 
Where F(0) is the current feature, F(-1) is the 
previous one, F(1) is the following one, etc. 
 
We regard each word in a sentence as a token 
and each token is tagged with a cue-label. The 
BIO scheme is used for tagging multiword hedge 
cues, such as ?whether or not? in our HCDic. 
where B-cue (tag for ?whether?) represents that 
the token is the start of a hedge cue, I-cue (tag 
for ?or?, ?not?) stands for the inside of a hedge 
cue, and O (tag for the other words in the 
sentence) indicates that the token does not 
belong to any hedge cue. 
We also have the method tested on Wikipedia 
corpus with a preprocessing of the HCDic. 
Section 2 reports that most weasel cues in 
Wikipedia corpus are multiword, and usually 
appear once. Different from our assumption in 
BioScope corpus, the set of weasel cues seems 
numerous. The HCDic of Wikipedia would be 
not so valuable if it tags few tokens for a new 
given text. To prevent these from happening, a 
preprocessing of the HCDic is taken. 
Most of the hedge cues in Wikipedia corpus 
accord with the structure of ?adjective + noun? 
e.g. ?many persons?. Although most cue words 
appear just once, the adjective usually happens to 
be the same, and we call them core words. 
Therefore, the hedge cue dictionary (HCDic) can 
be simplified with the core words. It helps to 
102
reduce the scale of the hedges cues from 1984 
cues down to 170. Then, we process the 
Wikipedia text the same way as the BioScope 
corpus. 
3.2 Detecting scope of hedge cues  
This phase (for CoNLL-2010 shared task 2) is 
based on Roser Morante and Walter Daelemans? 
scope detection system. 
CRF model is applied in this part, too. The 
word, POS, lemma, chunk and HCDic tags are 
also applied to be the features as in the step of 
hedge cues detection. In section 3.1, we can 
obtain the hedge cues in a sentence. The scope 
relies on its cue vary much. We make the BIO 
schema of detected hedge cues to be the 
important features of this part. Besides, the 
sentences tagged as ?certain? type are neglected 
in this step. 
Here is an example of golden standard of 
scope label.  
 
<sentence id="S5.149"> We <xcope id="X5.149. 
3"><cue ref="X5.149.3" type= "specula-tion"> 
propose </cue> that IL-10-producing Th1 cells 
<xcope id="X5.149.2"> <cue ref="X5.149.2" 
type= "speculation" >may</cue> be the essential 
regulators of acute infection-induced inflammation 
</xcope> and that such ?self-regulating? Th1 cells 
<xcope id= "X5.149.1"> <cue ref= "X5.149.1" 
type= "speculation" >may</cue> be essential for 
the infection to be cleared without inducing 
immune-mediated pathology </xcope> </xcope>. 
 
As shown, each scope is a block with a 
beginning and an end, and we refer to the 
beginning of scope as scope head (<xcope?>), 
and the end of the scope as scope tail 
(</xcope>). 
The types of the scope are labeled as: 
 
1. Label the token next to scope head as 
?xcope-H? ( e.g. propose, may ) 
2. Tag the token before scope tail as ?xcope-
T?(e.g. pathology for both scopes)  
3. The other words tag ?O? , including the 
words inside the scope and out of it. This 
is very different from the BIO scheme. 
 
The template for each feature is the same as in 
section 3.1. 
Following are our rules to form the scope of a 
hedge: 
 
1. Most hedge cues have only one scope tag, 
meaning there is one-to-one relationship 
between hedge cue and its scope. 
2. The scope labels may be nested. 
3. The scope head of the cue words appears 
nearest before hedge cue. 
4. The scope tail appears far from the cue 
word. 
5. The most frequent head/tail positions of the 
scope are shown in Table 4. 
a) The scope head usually is just before 
the cue words. 
b) The scope tail appears in the end of the 
sentence frequently. 
 
Scopes of hedge cues in BioScope corpus 
should be found for the shared task. The training 
dataset of abstract part is analyzed for its larger 
scale  
 
item Following strings  
with high frequency % 
1 
scope 
head 
<cue...>(cue words) 0.861 
?.?(sentence end) 0.695 
</xcope> 
(another scope tail) 0.144 
2 
scope 
tail 
?,?  ?;?  ?:? 0.078 
 
Table 4: Statistics of the strings nearby the scope 
head and tail. Item 1 shows the word follow 
scope head, and item 2 shows the frequent words 
next to the scope tail. 
 
We analyze the words around the scope head 
and the scope tail. The item 1 in Table 4 shows 
that 86.1% of the following words of the scope 
head are hedge cues. Other following words not 
listed are less than 1%, according to our 
statistics. The item 2 lists the strings with high 
frequency next to the scope tail as well. The first 
2 words in item 2 can be combined sometimes, 
so the percentage of scope tail at the end of the 
sentence can be more than 80%. The strings 
ahead of scope head and tail not listed are also 
counted, but they do not give such valuable 
information as the two items listed in Table 4. 
Therefore, when the CRF model gives low 
confidence, we just set the most probable 
positions of scope head and tail. 
For the one-to-one relationship between hedge 
cues and their scopes, we make rules to insure 
each cue has only one scope, including the scope 
head and scope tail. 
103
Rule 1: if more than one scope heads or tails 
are predicted, we get rid of the farther head or 
nearer tail. 
Rule 2: if none of scope head or tail is pre-
dicted, the head is set to the word just before the 
cue words; the tail is set at the end of the 
sentence. 
Rule 3: if one scope head and one tail are 
predicted, we consider them the result of scope 
detection. 
4 Results 
Our experiments are based on the CoNLL-2010 
shared task?s datasets, including BioScope and 
Wikipedia corpus. All the experiments for 
BioScope use abstracts and full papers for 
training data and the provided evaluation for 
testing. 
We employ CRF model to detect the hedge 
cues in the BioScope. The experiments are 
carried out on different feature sets: words 
sequence with the chunk feature only, lemma 
feature only and POS feature only. The effect of 
the HCDic feature is also evaluated. 
 
Features prec. recall F-score 
Chunk only 0.7236 0.6275 0.6721 
Lemma only 0.7278 0.6103 0.6639 
POS only 0.7320 0.6208 0.6718 
Without 
HCDic 
0.7150 0.6447 0.6781 
ALL 0.7671 0.7393 0.7529 
 
Table 5: Results at hedge cue-level 
 
As described in section 1 of this paper, the 
feature of POS may be not so significant as the 
lemma, but we do not agree with this point of 
view for given POS feature's better performance 
in F-score (in Table 5). The interesting cue-level 
result does not go into for time limitations. The 
F-score of the three features, chunk, lemma and 
POS are approximately equal. When all of the 
three features are used for CRF model, the 
performance is not improved so significantly. 
The recall rate is a bit low in the experiment 
without HCDic features. As shown in Table 5, 
the feature of HCDic is effective to get a better 
score both in precision rate and in recall rate. As 
our assumption, hedges in the evaluation dataset 
are limited, too. Most of them along with some 
non-hedges can be tagged with HCDic. Then the 
tag could contribute to a good recall. It also helps 
the classifier to focus on whether the words with 
?L?, ?FL?, and ?FH? are hedge cues or not, 
which will be good for a better precision. 
With detected hedge cues, we can get senten-
ces containing uncertainty for the shared task 1. 
A sentence is tagged as ?uncertain? type if any 
hedge cue is found in it.  
 
 precision recall F-score 
Without 
HCDic 0.8965 0.7898 0.8398 
ALL  0.8344 0.8481 0.8412 
 
Table 6: Evaluation result of task 1 
 
Statistics in Table 6 show that even poor 
performance in cue-level test can get a 
satisfactory F-score of speculative sentences 
detection as well. It seems that hedges detection 
at cue-level is not proportionate to the sentence-
level. Think about instance of more than one 
cues in a sentence such as the example of golden 
standard in section 3.2, the sentence will be 
tagged even if only one hedge cue has been 
identified (lower recall at cue-level). Moreover, 
in the speculative sentence with one hedge cue, 
false positives (lower precision at cue-level) can 
also lead to the correct result at sentence-level. 
The method is also tested on Wikipedia corpus, 
using provided training dataset and evaluation 
data. The method has a bad performance in our 
close test. The results are listed in Table 7. 
As talked in section 2, hedges in Wikipedia 
corpus are very different from in BioScope 
corpus. Besides, the string matching method for 
simplified HCDic is not so effective. The useful-
ness of HCDic is not so significant for a good 
recall in Wikipedia corpus.  
 
dataset precision recall F-score 
Wikipedia 0.7075 0.2001 0.3120 
BioScope 0.7671 0.7393 0.7529 
 
Table 7: Results of weasel/hedge detection in 
Wikipedia and BioScope corpus. 
 
In CoNLL-2010 shared task 2, the evaluation 
result shows our precision, recall and F-score are 
34.8%, 41% and 37.6%. The performance of 
identifying the scope relies on the cue-level 
detection. Therefore, the false positive and false 
negatives of hedge cues can lead to recognition 
errors. The result shows that our lexical-level 
method for the semantic problem is limited. For 
the time constraints, we do not probe deeply. 
104
5 Conclusions 
This paper presents an approach for extracting 
the hedge cues and their scopes in BioScope 
corpus using two CRF models for CoNLL-2010 
shared task. In the first task, the HCDic feature is 
proposed to improve the system performances, 
getting better performance (84.1% in F-score) 
than the baseline. The HCDic feature is also 
helpful to make use of cross-domain resources. 
The comparison of our methods based on 
between BioScope and Wikipedia corpus is 
given, which shows that ours are good at hedge 
cues detection in BioScope corpus but short at 
the in Wikipedia corpus. To detect the scope of 
hedge cues, we make rules to post process the 
text. For future work, we will look forward to 
constructing regulations for the HCDic to 
improve our system.  
References 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of the Fourteenth Conference 
on Computational Natural Language Learning 
(CoNLL-2010): Shared Task, pages 1?12, 
Uppsala, Sweden, July. Association for 
Computational Linguistics. 
Halil Kilicoglu, and Sabine Bergler. 2008. 
Recognizing speculative language in biomedical 
research articles: a linguistically motivated 
perspective. BMC Bioinformatics, 9(Suppl 
11):S10. 
John Lafferty, Andrew K. McCallum, and Fernando 
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling 
sequence data. In ICML, pages 282?289. 
George Lakoff. 1972. Hedges: a study in meaning 
criteria and the logic of fuzzy concepts. Chicago 
Linguistics Society Papers, 8:183?228. 
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 
2004 The language of bioscience: 
facts,speculations, and statements in between. In 
BioLINK 2004: Linking Biological Literature, 
Ontologies and Databases, pages 17?24. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proceedings of ACL 
2007, pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the BioNLP 2009 
Workshop, pages 28-36, Boulder, Colorado, June 
2009. Association for Computational Linguistics. 
Roser Morante, and Walter Daelemans. 2009. A 
metalearning approach to processing the scope of 
negation. In Proceedings of CoNLL-2009. 
Boulder, Colorado. 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised 
selection of keywords. In Proceedings of ACL 
2008, pages 281?289, Columbus, Ohio, USA. 
ACL. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun?ichi Tsujii. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In: 
Advances in Informatics, PCI 2005, pages 382?
392. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The 
BioScope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9(Suppl 11):S9. 
105
CRF tagging for head recognition based on Stanford parser 
 
Yong Cheng, Chengjie Sun, Bingquan Liu, Lei Lin 
Harbin Institute of Technology  
{ycheng, cjsun, linl,liubq}@insun.hit.edu.cn 
 
   
Abstract 
Chinese parsing has received more and 
more attention, and in this paper, we use 
toolkit to perform parsing on the data of 
Tsinghua Chinese Treebank (TCT) used in 
CIPS, and we use Conditional Random 
Fields (CRFs) to train specific model for the 
head recognition. At last, we compare 
different results on different POS results. 
1 Introduction 
    In the past decade, Chinese parsing has 
received more and more attention, it is the 
core of Chinese information processing 
technology, and it is also the cornerstone for 
deep understanding of Chinese.  
    Parsing is to identify automatically 
syntactic units in the sentence and give the 
relationship between these units. It is based 
on a given grammar. The results of parsing 
are usually structured syntax tree. For 
example, the parsing result of sentence "?
???????" is as following. 
                           (ROOT 
(dj (nS ??) 
(vp (v ?) 
(np 
                              (np (m ?) (n ??)) 
                           (n ??))))) 
With the development of Chinese 
economy, Chinese information processing 
has become a worldwide hot spot, and 
parsing is an essential task. However, 
parsing is a recognized research problem, 
and it is so difficult to meet the urgent needs 
of industrial applications in accuracy, 
robustness, speed. So the study of Chinese 
grammar and syntax analysis algorithm are 
still the focus of Chinese information 
processing.  
In all the parsing technology research, 
English parsing research is the most in-depth, 
and there are three main aspects of research 
in statistical parsing, they are  parsing model, 
parsing algorithm, and corpus construction. 
As for the parsing model, currently there are 
four commonly used parsing models, PCFG 
model [1], the model based on historical, 
Hierarchical model of progressive, head-
driven model [2]. 
 Since parsing is mostly a data driven 
process, its performance is determined by 
the amount of data in a Treebank on which a 
parser is trained. Much more data for 
English than for any other languages have 
been available so far. Thus most researches 
on parsing are concentrated on English. It is 
unrealistic to directly apply any existing 
parser trained on an English Treebank for 
Chinese sentences. But the methodology is, 
without doubt, highly applicable. Even for 
those corpora with special format and 
information integrated some modification 
and enhancement on a well-performed parser 
to fit the special structure for the data could 
help to obtain a good performance.  
    This paper presents our solution for the 
shared Task 2 of CIPS2010-Chinese Parsing. 
We exploit an existing powerful parser, 
Stanford parser, which has showed its 
effectiveness on English, with necessary 
modifications for parsing Chinese for the 
shared task. Since the corpus used in CIPS is 
from TCT, and the sentence contains the 
head-word information, but for the Stanford 
parser, it can't recognize the head 
constituents. So we apply a sequence tagging 
method to label head constituents based on 
the data extracted from the TCT corpus, In 
section 2 and section 3, we will present the  
Table 1. Training data with different formats 
 
details of our approach, and In section 4, we 
present the details of experiment. 
 
2 Parsing 
    Since English parsing has made many 
achievements, so we investigated some 
statistical parsing models designed for 
English. There are three open source 
constituent parsers, Stanford parser [3], 
Berkeley parser [4] and Bikel's parser [5]. 
Bikel's parser is an implementation of 
Collins' head-driven statistical model [6]. 
The Stanford parser is based on the factored 
model described in [7]. Berkeley parser is 
based on unlexicalized parsing model, as 
described in [8]. 
All the three parsers are claimed to be 
multilingual parsers but only accept training 
data in UPenn Treebank format. To adapt 
these parsers to Tsinghua Chinese Treebank 
(TCT) used in CIP, we firstly transform the 
TCT training data into UPenn format. Then, 
some slight modifications have been made to 
the three parsers. So that they could fulfill 
the needs in our task. 
In our work, we use Stanford parser to 
train our model by change the training data 
to three parts with different formats, one for 
training parsing model, one for training POS 
model, and the last for training head-
recognition model. Table 1 shows the three 
different forms. 
 
3 Head recognition 
    Head recognition is to find the head 
word in a clause, for example, 'np-1' express 
that in the clause, the word with index '1' is 
the key word. 
    To recognize the head constituents, and 
extra step is needed since Stanford parsing 
could not provide a straight forward way for 
this. Consider that head constituents are 
always determined by their syntactic symbol 
and their neighbors, whose order and 
relations strongly affects the head labeling. 
Like chunking [9], it is natural to apply a 
sequence labeling strategy to tackle this 
problem. We adopt the linear-chain CRF 
[10], one of the most successful sequence 
labeling framework so far, for the head 
recognition is this stage.  
    
4 Experiment 
4.1 Data 
    The training data is from Tsinghua 
Chinese Treebank (TCT), and our task is to 
perform full parsing on them. There are 
37218 lines in official released training data, 
As the Table 1 show; we change the data 
into three parts for different models. 
The testing data doesn?t contain POS 
labels, and there are 1000 lines in official 
released testing data. 
 
 
Parsing model 
1.(ROOT (np-0-2 (n ?
???) (cC ??) (np-
0-1 (n ? ? ) (n ?
?) ) ) ) 
2.(ROOT (vp-1 (pp-1 (p 
?) (np-0-2 (np-1 (n ?
?) (n ??) ) (cC ?
? ) (np-2 (a ? ? ) 
(uJDE ?) (np-1 (n ?
?) (np-1 (n ??) (n ?
?) ) ) ) ) ) (vp-1 (d ?
?) (vp-1 (d ??) (v ?
?) ) ) ) ) 
POS model 
1. ??/nS  ??/a  ?
?/n 
2.??/nS  ?/vC  ?/a  
??/n  ??/n  ?/wP  
??/nR  ??/n  ?/vC  
??/m  ?/m  ?/qN  
??/n  ?/uJDE  ??
/n  ?/wE   
Head-recognition 
model 
a O n np 0 
n a O np 1 
 
nS O np np 0 
np nS O np 1 
Table 2. Different POS tagging results 
 original new 
pos accuracy 80.40 94.82 
 
4.2 Models training 
4.2.1 Parsing model training 
    As for training parsing model with 
Stanford parser, since there are little 
parameters need to set, so we directly use the 
Stanford parser to train a model without any 
parameter setting. 
4.2.2 POS model training 
    In this session of the evaluation, POS 
tagging is no longer as a separate task, so we 
have to train our own POS tagging model. In 
the evaluation process, we didn't fully 
consider the POS tagging results' impact on 
the overall results, so we didn't train the POS 
model specially, we directly use the POS 
function in Stanford parser toolkit. This has 
led to relatively poor results in POS tagging, 
and it also affects the overall parsing result. 
After the evaluation, we train a specific 
model to improve the POS tagging results. 
As the table 1 shows, we extract training 
data from the original corpus and adopt the 
linear-chain CRF to train a POS tagging 
model. Table 2 shows the original POS 
tagging results and new results.  
4.2.3 Head recognition model training 
As the table 1 shows, we extract specific 
training data from original corpus.  
Table 3.  Training data formats for Head-
recognition 
original corpus 1.[vp-0 ??/v  [np-1 
??/n  ??/n  ] ]  
temp corpus 1.[np-1 ??/n  ??
/n  ] 
2.[vp-0 ??/v  [np-1 
??/n  ??/n  ] ] 
final corpus n O n np 0 
n n O np 1 
 
v O np vp 1 
np v O vp 0 
Table 4. Statistics the frequency of the words in 
each clause 
number of word statistics number 
< 1 160 
2 50834 
3 12592 
4 56 
5 664 
>5 360 
 
And for head-word recognition, since the 
adjacent clause has little effect on the 
recognition of head-word, so we set the 
clause as the smallest unit. We chose CRF to 
train our model. However, for getting the 
proper format of data for training in CRF, 
We have to do further processing on the data. 
As the table 3 shows, the final data set word 
as the unit. 
For example, the line 'n O np vp 1?, the 
meaning from beginning to end is POS or 
clause mark of current word or clause, POS 
or clause mark of previous word, POS or 
clause mark of latter word, the clause mark 
of current word, and the last mean that if 
current word or clause is headword 1 
represents YES, 0 represents NO. 
4.4 Result and Conclusion 
As we mention before, in evaluation, we 
didn't train specific POS tagging model, So 
we re-train our pos model, and the new 
results is shown in table 6, it can be seen that, 
with the increase of POS result, there is a 
corresponding increase in the overall results. 
Table 5. Performance of head recognition and 
the template for model training 
Boundary + 
Constituent 70.58 
 Boundary + 
Constituent + Head 66.97 
template 
U00:%x[0,0] 
U01:%x[-1,0] 
U02:%x[1,0] 
U04:%x[0,0]/%x[-1,0]
U05:%x[0,0]/%x[1,0]
U06:%x[-1,0]/%x[1,0]
 
 
Table 6. Overall results on different POS results 
 POS Boundary + 
Constituent 
original 80.40 67.00 
new 94.82 74.28 
 
Through our evaluation results, we can 
see that it is not appropriate to directly use 
English parser toolkit to process Chinese. 
And it is urgent to development parsing 
model based on the characteristics of 
Chinese. 
 
 
 
References 
[1] T. L. Booth and R. A. Thompson. Applying 
Probability Measures  to Abstract Languages. 
IEEE Transactions on Computers, 1973, C-
22(5):422-450. 
[2] M. Collins. Three Generative, Lexicalised 
Models for Statistical Parsing. In Proceedings 
of the 35th annual meeting of the association 
for computational linguistics. 
[3] http://nlp.stanford.edu/software/lex-parser.html 
[4] http://code.google.com/p/berkeleyparser 
[5] http://www.cis.upenn.edu/~dbikel/download 
[6] Michael Collins. 1999. Head-Driven Statistical Models for  
     Natural Language Parsing. Ph.D. thesis. 
University of Pennsylvania. 
[7] Dan Klein and Christopher D. Manning 
Accurate unlixicalized parsing. In Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics. 
[8] S Petrov and D Klein. Improved inference for 
unlexicalized parsing. In Proceedings of 
NAACL HLT 2007. 
[9] Fei Sha and Fernando Pereira. 2003. Shallow 
parsing with conditional random fields. In 
Proceedings of HLT-NAACL 2003, pages 
213-220, Edmonton. Canada. 
[10] John Lafferty. Andrew McCallum. And 
Fernando Pereira. 2001. Conditional random 
fields: Probabilistic models for segmenting and 
labeling sequence data. In Proceedings of 
ICML 2001, pages 282-289, Williams College, 
Williamstown, MA, USA. 
