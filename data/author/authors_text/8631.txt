Minimizing Word Error Rate in 
Textual Summaries of Spoken Language 
Klaus Zechner and A lex Waibel 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213, USA 
(zechner ,waibel}@cs. cmu. edu 
Abstract 
Automatic generation of text summaries for spoken 
language faces the problem of containing incorrect 
words and passages due to speech recognition er- 
rors. This paper describes comparative experiments 
where passages with higher speech recognizer confi- 
dence scores are favored in the ranking process. Re- 
sults show that a relative word error rate reduction 
of over 10% can be achieved while at the same time 
the accuracy of the summary improves markedly. 
1 Introduction 
The amount of audio data on-line has been grow- 
ing rapidly in recent years, and so methods for ef- 
ficiently indexing and retrieving non-textual infor- 
mation have become increasingly important (see, 
e.g., the TREC-7 branch for "Spoken Document Re- 
trieval" (Garofolo et al, 1999)). 
One way of compressing audio information is the  
automatic creation of textual summaries which can 
be skimmed much faster and stored much more effi- 
ciently than the audio itself. There has been plenty 
of research in the area of summarizing written lan- 
guage (see (Mani and Maybury, 1999) for a compre- 
hensive overview). So far, however, very little atten- 
tion has been given to the question how to create 
and evaluate a summary of spoken audio based on 
automatically generated transcripts from a speech 
recognizer. One fundamental problem with those 
summaries i that they contain incorrectly recog- 
nized words, i.e., the original text is to some extent 
"distorted". 
Several research groups have developed interac- 
tive "browsing" tools, where audio (and possibly 
video) can be accessed together with various types 
of textual information (transcripts, ummaries) via a 
graphical user interface (Waibel et al, 1998; Valenza 
et al, 1999; Hirschberg et al, 1999). With these 
tools, the problem of misrecognitions is alleviated 
in the sense that the user can always easily listen 
to the audio recording corresponding to a passage 
in a textual summary. In some instances, however, 
this approach may not be feasible or too expensive 
to pursue, and a short, stand-alone textual repre- 
sentation of the spoken audio may be preferred or 
even required. This paper addresses in particular 
this latter case and (a) explores means of making 
textual summaries less distorted (i.e., reducing their 
word error rate (WElt)), and (b) assesses how the 
accuracy of the summaries changes when methods 
for word error rate reduction ar e applied. Summary 
accuracy will be a function of how much relevant 
information is present in the sun'mary. 
Our results from experiments on four television 
shows with multiple speakers how that it is possi- 
ble to reduce word error rate while at the same time 
also improving the accuracy of the summary. Fur- 
thermore, this paper presents a novel method for 
evaluation of textual summaries from spoken lan- 
guage data. . . . .  
The paper is organized as follows: In the next : 
section, we review related work on spoken language 
summarization. In section 3 we describe our sum: 
marizer. Next, we present and discuss our proposal 
for an audio summarization evaluation metric (sec- 
tion 4). In section 5 we describe the Corpus that we 
use for our experiments and how i t  was annotated. 
Sections 6 and 7 describe xperixnents on both hu .... 
man and machine generated transcripts of the audio 
data. Finally, we discuss and summarize the results 
in sections 8 and 9. 
2 Related work ? " 
(Waibel et al, 1998) report results of their sum- 
marization system on automatically transcribed 
SWITCHBOARD data (Godfrey et al, 1992), the word 
error rate being about 30%. In a question-answer 
test with summaries offive dialogues, ubjects could 
identify most of the key concepts using a summary 
size of only five turns. However, the results vary 
widely across five different dialogues tested in this 
experiment (between 20% and 90% accuracy). 
(Valenza et al, 1999) went one step further and 
report that they were able to reduce the word error 
rate in summaries (as opposed to full texts) by using 
speech recognizer confidence scores. They combined 
inverse frequency weights with confidence scores for 
each recognized word. Using summaries composed of 
186 
one 30-gram per minute (approximately 15% length 
of the full text), the WER dropped from 25% for 
the full text to 10% for these summaries. They also 
conducted a qualitative study where human subjects 
were given summaries of n-grams of different length 
and also summaries with speaker utterances as min- 
imal units, either giving a high weight o the inverse 
frequency scores or to the confidence scores. The 
utterance summaries were considered best, followed 
closely by 30-gram summaries, both using high con- 
fidence score weights. This suggests that not only 
does the WER drop by extracting passages that are 
more likely to be correctly recognized but also do 
summaries seem to be "better" which are generated 
that way. 
While the results of (Valenza et al, 1999) are in- 
dicative for their approach, we want to investigate 
the benefits of using speech recognizer confidence 
scores in more detail and particularly find out about 
the trade-off between WER and summarization ac- 
curacy when we vary the influence of the confidence 
scores. To our knowledge, this paper addresses this 
trade-off for the first time in a clear, numerically de- 
scribable way. To be able to obtain numerical values 
for summary accuracy, we had our corpus annotated 
for relevance (section 5) and devised an evaluation 
scheme that allows the calculation of summary ac- 
curacy for both human and machine generated tran- 
scripts (section 4). 
3 Summar izat ion  sys tem 
Prior to summarizing, the input text is cleaned up 
for disfluencies, such as hesitations, filled pauses, 
and repetitions. I In the context of multi-topical 
recordings we use for our experiments, summaries 
are generated for each topical segment separately. 
The segment boundaries were determined to be at 
those places where the majority Cat least half) of the 
human annotators agreed (see section 5). Intercoder 
agreement for topical boundaries i fairly good (and 
higher than the agreement on relevant words or pas- 
sages).2 
To determine the content of the summaries, we 
use a "maximal marginal relevance" (MMR) based 
summarizer with speaker turns as minimal units (cf. 
(Carbonell and Goldstein, 1998)). 
The MMR formula is given in equation 1. It gen- 
erates a list of turns ranked by their relevance and 
states that the next turn to be put in this ranked 
list will be taken from the turns which were not yet 
ranked (tar) and has the following properties: it is 
(a) maximally similar to a "query" and (b) max- 
imally dissimilar to the turns which were already 
1 More details about this component and other parts of the 
summarization system can be found in (Zechner and Walbei, 
20oo). 
2For details ee (Zechner, 2000).  
ranked (tr). As "query" we use a frequency vector 
for all content words within a topical segment. The 
A-parameter (0.0 < A < 1.0) is used to trade off the 
influence of C a) vs. (b). 
Both similarity metrics (sire1, sire2) are inner 
vector products of (_stemmed) term frequencies (see 
equations 2 to 4); tft is a vector of stem frequencies 
in a turn; f ,  are in-segment frequencies of a stem; 
f ,  rna= are maximal segment frequencies of any stem 
in the topical segment, sirnl can be normalized or 
not. The formulae for tfa (equation 4) are inspired 
from Cornell's SMART system (Salton, 1971); we 
will call these parameters "smax', "log", and ?'freq", 
respectively. 
neztturn = argmax(Asima(tn,j,query) 
tnr,~ 
- (1 - A) maxsim2 (tnrd, tr,t~)) (1) 
tr ,  k 
siml : tf~tft or \[tf=tit~ (2) 
I / I  -I 
tftztft~ 
I I I  I 
tfi,, = 0.5 + 0.5 /~" or 1 + logfi., 
or ~,, (4) 
Using the MMR algorithm, we obtain a list of 
ranked turns for each topical segment. We com- 
pute this both for human and machine generated 
transcripts of the audio files ("reference text" vs. 
"hypothesis text") .3 
4 Eva luat ion  metr i cs  
The challenge of devising a meaningful evaluation 
metric for the task of audio summarization is that 
it has to be applicable to both the reference (hu- 
man transcript) and the hypothesis transcripts (au- 
tomatic speech recognizer (ASR) transcripts). We 
want to be able to assess the quality of the sum- 
mary with respect to the relevance markings of the 
human annotators (see section 5), as well as to re- 
late this "summary accuracy" to the word error rate 
present in the ASR transcripts. 
The approach we take is to align the words in the 
summary with the words in the reference transcript 
(wa). For ASR transcripts, word substitutions are 
aligned with their "true original" and word inser- 
tions are aligned with a NIL dummy. That way, 
3The human reference is cons idered  to be an "optimal" or 
"ideal" rendering of the words which were actually said in a 
conversation. Human transcription errors do occur, but are 
marginal and hence ignored in the context of this paper. 
187 
we can determine for each individual word wa in 
the summary (a) whether it occurs in a "relevant 
phrase" and (b) whether it is correctly recognized 
or a recognition error (for ASR transcripts). 
We define word error rate as WER = (S + 
I + D) / (S  + I + C) (I=insertion, D=deletion, 
S=substitution, C=correct). 
Each word's relevance score r is the average num- 
ber it occurs in the human annotators' relevant 
phrases (0.0 < r <_. 1.0). Relevance scores for in- 
sertions and substitutions are always 0.0. 
We choose to define the summary accuracy sa 
("relevance") as the sum of relevance scores of all 
n aligned words ~--~? r~, divided by the maximum 
achievable relevance score with the same number of 
n words somewhere in the text (i.e., 0.0 < sa <_ 1.0). 
Word deletions obviously do not show up in the sum- 
mary, but are accounted for, as well, to make the 
WER computation sound. 
To better illustrate how these metrics work, we 
demonstrate them on a simplified example of only 
two speaker turns (Figure 1). The first line repre- 
sents the relevance score r for each word (the number 
this word was within a "relevant phrase" divided by 
the number of annotators for that text). In turn 1, 
"this is to illustrate" was only marked relevant by 
two annotators, whereas "the idea" by 3 out of 4 
annotators. The second line provides the reference 
transcript, the third line the ASB. transcript. Line 
4 gives the type of word error, and line 5 the con- 
fidence score of the speech recognizer (between 0.0 
and 1.0, 1.0 meaning maximal confidence). 
Now let us assume that turn 2 shows up in the 
summary. The scores are computed as follows: 
? When summarizing the reference: Here, the 
word error rate is trivially 0.0; the summary 
accuracy sa is the sum of all relevance scores 
(-6.0) divided by the maximal achievable score 
with the same number of words (n = 7). l"hrn 2 
has 6 words which were marked relevant by all 
coders (r -- 1.0), turn l 's highest score is r = 
0.75. Therefore: sa2 = 6.0/(6.0 + 0.75) = 0.89. 
This is higher than the summary accuracy for 
turn 1: sal = 3.5/6.0 = 0.58(n = 6). 
? When summarizing the ASR transcript ("hy- 
pothesis"): Selecting turn 2 will give sa2 = 
0.02.25 = 0.0 (n = 5) .  For turn 1, sal = 
2.25/(0.75 + 0.5 + 0.5 + 0.5 + 0.0 + 0.0) = 1.0 
(n = 6; the sum in the denominator can only use 
relevance scores based on the aligned words wa 
which were correctly recognized, therefore the 
1.0-scores in turn 2 cannot be used). Turn 2 has 
WER=6\ [5=l .2 ,  turn 1 has WER=3/6=0.5 .  
Obviously, when summarizing the ASB. output, we 
would rather have turn 1 showing up in the summary 
than turn 2, because turn 2 is completely off from 
the truth and turn 1 only partially. The fact that 
turn 2 was considered to be more relevant by human 
coders cannot, in our opinion, be used to favor its 
inclusion in the summary. An exception would be 
a situation where the user has immediate access to 
the audio as well and is able to listen to selected 
passages from the summary (see section 1). In our 
case, where we focus on text-only summaries to be 
used stand-alone, we have to minimize their word 
error rate. 
Given that, turn 1 has to be favored over turn 2, 
both because of its lower WEB, and because of its 
higher accuracy with respect o the relevance anno- 
tations. 
In order to increase the likelihood that turns 
with lower WEB, are selected over turns with higher 
WEB., we make use of the speech recognizer's con- 
fidence scores which are attached to every word hy- 
pothesis and can be viewed as probabilities: they are 
in \[0.0,1.0\], high values reflecting a high confidence 
in the correctness of the respective word. 4 Follow- 
ing (Valenza et al, 1999) we conjecture that we can 
use these confidence scores to increase the probabil- 
ity of passages with lower WEB, to show up in the 
summary. To test how far this assumption is justi- 
fied, we correlated the WEB. with various metrics of 
confidence scores: (i) sum of scores, (ii) average of 
scores, (iii) number of scores above a threshold, (iv) 
the latter normalized by the number of all scores, 
and (v) the geometric mean of scores. Table 1 shows 
the correlation coefficients (Pearson r) for the four 
ASK transcripts we used in our experiments ( ee sec- 
tion 5). To prevent he influence of large differences 
in turn length, those computations were done for 
subsequent "buckets" of 50 words each. 
Since in most cases we achieve the highest corre- 
lation coefficient (absolute value) for method (iv = 
avgth) (average number of words whose confidence 
score is greater than a threshold of 0.95), we apply 
this metric to the computation ofturn-query similar- 
ities (sire1 in equation 1). We use the two following 
formulae to adjust the similarity,scores. (We shall 
call these adjustments MULT and EXP in the follow- 
ins.) 
\[mutt\] sirn~ = Siml (1 + aavgth) (5) 
\[ezp\] sim'l' = s imlavgth ~ (6) 
For both equations it holds that if a = 0.0, the 
scores don't change, whereas if c~ > 0.0, we en- 
hance the weights of turns with many high confi- 
dence scores ("boosting") and hence increase their 
likelihood of showing up earlier in the summary. 5 
Even though our evaluation method looks like it 
would "guarantee" an increase in summary accu- 
4The speech recognizer computes  these scores based on the 
acoustic stability of words during lattice rescoring. 
5For EXP, we define 0 ? ---- O. 
188 
TURN 
re1: 
REF: 
HYP: 
e r r :  
con:  
TURN 
re l  : 
REF: 
HYP: 
err: 
con:  
1: 
0.5 0.5 0.5 0.5 0.75 0.75 *** 
this is to illustrate the idea *** 
this is to  ILLUMINATE *** idea 
C C C S D C I 
1 1 1 0 .9  - 0 .8  0 .8  
2: 
0 1 1 1 1 1 1 
and here we have very  re levant  in fo rmat ion  
and HE ** BEHAVES **** IRREVERENT FORMATION 
C S D S D S S 
0.8  0 .7  - 0 .8  - 0 .8  0 .9  
Figure 1: Simplified example of two turns (for score computation) 
BACK 
(i} sum -0.43 
(ii) average -0.53 
Off) scores > 0.95 -0.55 
( iv) normal i zed  (ii i) -0.58 
(v) geometric mean -0.53 
19CENT BUCHANAN 
-0.51 -0.12 
-0.52 -0.43 
-0.48 -0.35 
-0.48 -0.48 
-0.53 -0.42 
GRAY 
-0.03 
-0.42 
-0.25 
-0.44 
-0.38 
Table 1: Pearson r correlation between WER and confidence scores 
racy when the word error rate is reduced, this is 
not necessarily the case. For example, it could turn 
out that while we can reduce WER by "boosting" 
passages with higher confidence scores, those pas- 
sages might have (much) fewer words marked rele- 
vant than those being present in the summary with- ~ 
out boosting. This way, it would be conceivable to 
create low word error summaries that contain also 
very few relevant pieces of information. However, as 
we will see later, WER reduction goes hand in hand 
with an increase of summary accuracy. 
5 Data  character i s t i cs  and  
annotat ion  
Table 2 describes the main features of the corpus we 
used for our experiments: we selected four audio ex- 
cerpts from four television shows, together with hu- 
man generated textual transcripts. All these shows 
are conversations between multiple speakers. The 
audio was sampled at 16kHz and then also automat- 
ically transcribed using a gender independent, vo- 
cal tract length normalized, large vocabulary speech 
recognizer which was trained on about 80 hours of 
Broadcast News data (Yu et al, 1999). The average 
word error rates for our 4 recordings ranged from 
25% to 50%. 
The reference transcripts of the four recordings 
were given to six human annotators who had to seg- 
ment them into topically coherent regions and to de- 
cide on the "most relevant phrases" to be included 
in a summary for each topical region. Those phrases 
usually do not coincide exactly with speaker turns 
and the annotators were encouraged to mark sec- 
tions of text freely such that they would form mean- 
ingful, concise, and informative phrases. Three an- ? 
notators could listen to the audio while annotat- 
ing the corpus, the other three only had the hu- 
man generated transcripts available. 2 of the 6 an- 
notators only finished the NewsHour data, so we 
have the opinion of 4 annotators for the recordings 
BUCHANAN and GRAY and of 6 annotators for BACK 
and 19CENT. 
6 Exper iments  on  human generated  
t ranscr ip ts  
We created summaries of the reference transcripts 
using different parameters for the MMR computa- 
tion: For tf  we used "freq", "log", and "smax"; fur- 
ther, we did or did not normalize these weights; fi- 
nally, we varied the MMR-A from 0.85 to 1.0. Sum- 
marization accuracy was determined at 5%, 10%, 
15%, 20%, and 25% of the text length of each sum- 
marized topical segment and then averaged over all 
sample points in all segments. Since these were 
word-based lengths, words were added incrementally 
to the summary in the order of the turns ranked via 
MMR; turns were cut off when the length limit was 
reached. As explained in the example in section 4, 
the accuracy score is defined as the fraction of the 
sum of all individual word relevance scores (as de- 
189 
TV show 
number of speakers 
speaker turns 
words in transcript 
length in minutes 
topical segments 
word error rate (in %) 
BACK 19CENT BUCHANAN GR.AY 
NewsHour 
5 
24 
1216 
8.6 
4 
25.6 
NewsHour 
2 
27 
1281 
8.6 
4 
32.6 
Crossfire 
4 
69 
3252 
17.3 
4 
32.5 
Table 2: Characteristics of the corpus 
Crossfire 
5 
7O 
2205 
11.9 
3 
49.8 
i 119CE T I BUCH*NAN I O A* I av0*a'? I 0,  
0.533 0.596 0.513 0.443 0.522 
Table 3: Reference summarization accuracy of MMR o~ 
summaries 
termined by human annotators) over the maximum 
possible score given the current number of words in 
the summary. 
Table 3 shows the summary accuracy results for 
the best parameter setting (if=log, no normaliza- 
tion) ~. 
7 Exper iments  on automat ica l ly  
generated  t ranscr ip ts  
Using the same summarizer as before, we now cre- 
ated summaries from ASR transcripts. Addition- 
ally to the summary accuracy, we evaluate now also 
the WER for each evaluation point. Again, we ran 
a series of experiments for different parameters of 
the MMR formula (if=log, smax, freq; with/without 
normalization). As before, we achieved the best re- 
sults for non normalized scores and tf=log. We var- 
ied a from 0.0 to 10.0 to see how much of an effect we 
would get from the "boosting" of turns with many 
high confidence scores (see equations 5 and 6). 
The ExP formula yielded better esults than MULl? 
(Table 4), the optimum for ExP was reached for 
= 3.0 with a WER of 26.6%, an absolute improve- 
ment of over 8% over the average of WER=35.1% 
for the complete ASR transcripts (non-summarized). 
The summarization accuracy peaks at 0.47, a 9% 
absolute improvement over the a = 0.0-baseline and 
only about 5% absolute lower than for reference sum- 
maries (Table 4 and Figure 2). 
When we compare the baseline of ~ = 0.0 (i.e., no 
"boosting" of high confidence turns) to the best re- 
sult (a = 3.0), we see that the WER drops markedly 
by about 12% relative from 30.1 to 26.6%. At the 
same time, the summarization accuracy increases by 
about 18% relative form 0.401 to 0.472. 
? I f  we use non-normal ized scores, the value of the MMR-X 
does not  have any measurab le  effect; we assigned it to be 0.95 
for all subsequent  experiments. 
0.$ 
0.25 
Summary accuracy vL Word mror rate 
FgXP Ioenula 
' ' i 0"2.35 0.4 0.45 0 5 0.65 
=unvnan/accuracy 
Figure 2: Summary accuracy vs. word error rates 
with ~.xP boosting (0 < a < 7) 
Results for the MULT formula confirm this trend, 
but it is considerably weaker: approximately 6% 
WER reduction and 14% accuracy improvement for 
c~ = 10.0 over the c~ = 0.0 baseline. 
An appendix (section 11) provides an example of 
actual summaries generated by our system for the 
first topical segment of the BACK conversation. It 
illustrates how WER reduction and summary ac- 
curacy improvement can be achieved by using our 
confidence boosting method. 
8 D iscuss ion  
The most significant result of our experiments i , 
in our opinion, the fact that the trade-off between 
word and summary accuracy indeed leads to an op- 
timal parameter setting for the creation of textual 
summaries for spoken language (Figure 2). Using 
a formula which emphasizes turns containing many 
high confidence scores leads to an average WER re- 
duction of over 10% and to an average improvement 
in summary accuracy of over 15%, compared to the 
baseline of a standard MMR-based summary. 
Comparing our results to those reported in 
(Valenza et al, 1999), we find that their relative 
190 
a = 0~0 
P.xP (o = 3.0) 
MOLT (0 = 10.0) 
BACK 19CENT 
acc I WER acc I WER 
0.411 26 .2  0.501 26.7 
0.648 18.8 0.501 26.7 
0.575 21.5 0.501 26.7 
BUCHANAN G RAY average  
acc WER acc WER ace \] WER 
0.412 30 .6  0.280 36.9 0.401 30.1 
0.444 26.9 0.296 34.0 0.472 26.6 
0.429 29.6 0.317 35.7 0.456 28.3 
Table 4: Effect of a on summary accuracy vs. WER (in %) transcripts with ExP and MULl" boosting methods 
I IBm?K\[ 19C NT BUOHA ANIO AYI 
avgth -0.79 -0.11 -0.43 -0.03 
Table 5: Correlation between WER and confidence 
scores on a turn basis 
WER reduction for summaries over full texts was 
considerably arger than ours (60% vs. 24%). We 
conjecture that reasons for this may be due to the 
different nature and quality of the confidence scores, 
and (not unrelated), to the different absolute WER 
of the two corpora (25% vs. 35%): in transcripts 
with higher WER, the confidence scores are usually 
less reliable (eft Table 1). 
Looking at the four audio recordings individually, 
we see that the improvements vary strongly across 
different recordings. We conjecture that one reason 
for this fact may be due to the high variation in 
the correlation between WER and confidence scores 
on a turn basis (Table 5). This would explain why, 
e.g., BACK'S improvements are much stronger than 
those of the BUCHANAN recording or why there are 
no improvements for the 19CENT recording. How- 
ever, GRAY does improve despite its very low abso- 
lute correlation. 
9 Summary  
In this paper, we presented experiments on sum- 
maries of both human and machine generated tran- 
scripts from four recordings of spoken language. We 
explored the trade-off of word accuracy vs. summary 
accuracy (relevance) using speech recognizer confi- 
dence scores to rank passages with lower word error 
rate higher in the summarization process. 
Results comparing our approach to a simple MMR 
ranking show that while the WER can be reduced by 
over 10%, summarization accuracy improves by over 
15% as measured against ranscripts with relevance 
annotations. 
10 Acknowledgements  
We thank the six human annotators for their tedious 
work of annotating the corpus with topical segment 
boundaries and relevance information. We also want 
to thank Alon Lavie and the three anonymous re- 
viewers for useful feedback and comments on earlier 
drafts of this paper. 
This work was funded in part by ATR - Inter- 
preting Telecommunications Re earch Laboratories 
of Japan, and the US Department ofDefense. 
Re ferences  
Jaime Carbonell and Jade Goldstein. 1998. The use 
of MMR, diversity-based reranking for reordering 
documents and producing summaries. In Proceed- 
ings of the ~Ist A CM-SIGIR International Con- 
ference on Research and Development in Informa- 
tion Retrieval, Melbourne, Australia. 
John S. Garofolo, Ellen M. Voorhees, Cedric G. P. 
Auzanue, and Vincent M. Stanford. 1999. Spoken 
document retrieval: 1998 evaluation and investi- 
gation of new metrics. In Proceedings of the ESCA 
workshop: Accessing information in spoken audio, 
pages 1-7. Cambridge, OK, April. 
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 
1992. SWITCHBOARD: telephone speech corpus 
for research and development. In Proceedings of 
the ICASSP-9~, volume 1, pages 517-520. 
Julia Hirschberg, Steve Whittaker, Don Hindle, Fer- 
nando Pereira, and Amit Singhal. 1999. Finding 
information i  audio: A new paradigm for audio 
browsing/retrieval. In Proceedings of the ESCA 
workshop: Accessing information in spoken audio, 
pages 117-122. Cambridge, OK, April. 
Inderjeet Mani and Mark T. Maybury, editors. 1999. 
Advances in automatic text summarization. MIT 
Press, Cambridge, MA. 
Gerard Salton, editor. 1971. The SMART Retrieval 
System -- Experiments in Automatic Text Pro- 
cessing. Prentice Hall, Englewood Cliffs, New Jer- 
sey. 
Robin Valenza, Tony Robinson, Marianne Hickey, 
and Roger Tucker. 1999. Summarisation f spo- 
ken audio through information extraction. In Pro- 
ceedings of the ESCA workshop: Accessing in- 
formation in spoken audio, pages 111-116. Cam- 
bridge, OK, April. 
Alex Waibel, Michael Bett, and Michael Finke. 
1998. Meeting browser: Tracking and summa- 
rizing meetings. In Proceedings of the DARPA 
Broadcast News Workshop. 
Hua Yu, Michael Finke, and Alex Waibel. 1999. 
Progress in automatic meeting transcription. 
In Proceedings of EUROSPEECH-99, Budapest, 
Hungary, September. 
191 
a relative sa WEB. in % turns in summary 
0.0 0.428 29.2 2, 1 \[beginning\] 
3.0 0.885 11.8 1, 5\[beginning\] 
Table 6: Relative summary accuracy, WER, and se- 
lected turns by the summarizer for (a) no boosting 
and (b) P.XP boosting. 
higher WER scores, case (b) (0 = 3.0) successfully 
ranks turn 1 first due to its higher confidence scores 
and hence both summary accuracy and WElt scores 
improve. 
turn avg. relevance score 
1 0.663 
2 0.369 
3 0.149 
4 0.212 
5 0.274 
WERin % avgth. 
9.5 0.84 
27.5 0.40 
26.9 0.39 
11.1 0.08 
27.7 0.17 
Table 7: Average relevance scores, WER, and confi- 
dence values for the five turns of BACK'S first topical 
segment. 
Klaus Zechner and Alex Waibel. 2000. Dia- 
summ: Flexible summarization of spontaneous 
dialogues in unrestricted domains. Available from 
http://www.cs.cmu.edu/?zechner/publications.html. 
Klaus Zechner. 2000. A word-based annota- 
tion and evaluation scheme for summariza- 
tion of spontaneous speech. Available from 
http://www .cs.cmu.edu/~zechner/publications.html. 
11 Append ix :  Example  summar ies  
This appendix provides summaries for the first topi- 
cal segment of the BACK conversation. The contents 
of this conversation revolves around former Illinois 
congressman Dan Rostenkowski who had been re- 
leased from prison and was ready to re-enter public 
life. 
Figure 3 shows the human transcript of this seg- 
ment which is about two minutes long and con- 
sists of 5 speaker turns. Figure 4 contrasts the 
machine generated summaries for this segment (a) 
without confidence boosting (a -- 0.0) and (b) using 
the optimal confidence boosting (c~ = 3.0, method 
ExP). Insertions and substitutions are capitalized 
and marked with I- or S- prefixes. Table 6 compares 
the relative summary accuracies ( a) and word error 
rates (WER in %) for these two summaries (aver- 
age over the 5 sample points from 5% to 25% sum- 
mary length). Additionally, the turns that show up 
in the summaries are listed in their ranking order. 
Table 7 provides the average relevance scores, word 
error rates, and confidence scores ("avgth") for each 
turn of this topical segment. 
We observe that the most relevant urn is turn 
1 which has, incidentally, also the lowest WER. 
Whereas in case (a) (o = 0.0), turn 2 is ranked 
first and therefore dominates the lower relevance and 
192 
1 e l i zabeth :  i t  has been e ight  months s ince  dan roetenkowsk i  ua lked  out  o f  a u i scons in  federa l  
p r i son  n ix  months s ince  he le f t  a ha l fuay  house in  ch icago  the  fo rmer  chai rman of  the  house ways and means 
committee i s  ready  to  s tep  back in to  the  pub l i c  eye 
2 e l i zabeth :  the  recept ion  was-warm the  banquet  ha l l  packed w i th  the  c i ty ' s  movers and shakers  
the  th i r ty  f i ve  do l la rs  a p la te  inv i ta t ion  re fer red  to  ros tenkowsk i  an mr. chai rman ros tenkowsk i  
made no re ference  to h i s  conv ic t ion  fo r  misus ing  federa l  funds  on ly  a br ie f  re fe rence  to  h i s  f i f teen  
months of  p r i son  t ime 
3 dan:  i g raduated  from oxford  and i rea l ly  had a rhodes scho larsh ip  the  past  th ree  years  have been a 
constant ly  cha l leng ing  t ime fo r  me change never  comes eas i ly  and g iven  the  c i rcumstances  
o f  my s i tuat ion  that  has par t i cu le~r ly  t rue  fo r  me at  t imes  th ings  have been dosnr ight  b leak  and i 
uou ldn ' t  want to  wish my exper ience  on my uors t  enemy but  there  were some s i l ver  l in ings  i ' ve  had an 
oppor tun i ty  to  read and re f lec t  in  a bay that  uasn ' t  poss ib le  when i was 
in  constant  moment in  these  
remarks  today  i 'd  l i ke  to  share  some of  my conc lus ions  
4 e l i zabeth :  the  conc lus ions  d id 'not  due l l  on the  demise of  dan ros tenkuwsk i ' s  career  but  
the  demise o f  par ty  po l i t i cs  
5 dan:  those  who say that  the  pres ident ' s  po l i t i ca l  poser  has been ueakened by scanda l  have t ru ly  shor t  
memories the  sad fac t  i s  that  p res ident  c l in ton  has never  had a democrat i c  base in  congress  a group 
of  peop le  uhom one cou ld  suppor t  the  wh i te  house on any g iven  i ssue  are  not  there  
Figure 3: Human transcript of first topical segment (BACK) 
1 e l i zabeth :  hen been e ight  months s ince  dan ros tenkoesk i  ba lked  out  o f  
u i scons in  federa l  p r i son  I -~YBE 
2 e l i zabeth :  wan S-ALARMED the  banquet  ha l l  packed s i th  the  c i ty ' s  
S-CDM~O~S S-IH S-CHAHBERSS-UHICH th i r ty  f i ve  S-DOLLAR a p la te  
S - IN ITAT IO| re fer red  to  routenkoeek i  an S-MR. chai rman I-LET 1-HE I-ASK 
ros tenkousk imade no re ference  to  h i s  conv ic t ion  fo r  I -HIS S-USIHG federa l  
funds  on ly  a br ie f  re fe rence  to  S- IS  f i f teen  months o f  p r i son  t ime 
1 e l i zabeth :  has been e ight  months s ince  dan routenkoesk i  
ea lked  out  o f  s i scons in  federa l  p r i son  I-SAYBE s ix  months s ince  he le f t  
S-THE ha l fuay  house in  ch icago  the  fo rmer  chairman of  the  house says  and 
means committee ready  to  s tep  back in to  the  pub l i c  eye 
5 dan:  S-ALSO say  that  the  pres ident ' s  po l i t i ca lpower  has been eeakened by 
scanda l  S-RIGHT S-ESPECIALLY shor t  S-MEMORY S-THAT S-DISSATISFACTI0| that  
p res ident  c l in ton  has never  
Figure 4: Machine generated summaries for (a) ~ = 0.0 and (b) a = 3.0 (25% of text length) 
193 
DIASUMM: Flexible Summarizat ion of 
Spontaneous Dialogues in Unrestricted Domains 
Klaus Zechner and Alex Waibel 
LaJlguage 'l~chnologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsbm:gh, PA 115213, USA 
{zechner., waibel}@cs, cmu. edu 
Abstract 
In this paper, we present a summa.rization system 
for spontaneous dialogues which consists of a novel 
multi-stage architectm'e. It is specifically aimed at 
addressing issues related to tlle nature of the l;exts 
being spoken vs. written and being diMogical vs. 
monologica.l. The system is embedded in a. graph- 
ical user interface ~md was developed and tested on 
transcripts of recorded telephone conversations in 
English and Spanish (CAI,LHOMI,;). 
1 Introduct ion 
Summa.rization of written docmnents has recently 
O' been a. focus for much research in NI,t ~ ( ~.o., (Mani 
and 1Vlasq~ury , 1997; AAAI, 1998; Mani el. al., 1998; 
ACL, 2000), to nanle some of tile Inajol: events in 
this field ill the past few years). Ilowever, very lit- 
tle a.ttention has been given so far to the summa- 
riza.tion of spol, r('~n language, even less of conversa- 
lions vs. monologic'al texts. We believe tha.t sum- 
mariza.tion of speech will bccoJne increasingly more 
important, a.s the ~ml(mnt of online audio daLa. grows 
and demand for r~tl)id browsing, skimming, a.nd a.e- 
cess of speech data increases. Another application 
which particulm:ly pertains to our interest in spo-- 
ken dialogue summarization would be the generation 
of meeting minutes for archival purposes a.nd/or to 
update l)a.rticil)a.nts .joining a.t la.ter stages on qm' 
progress of the conversa.tion so far. 
Sunmmrization of dialogues within l imilcd do- 
mains ha.s been attempted within the context of 
the VERBMOBII, pl:ojcct ("protocol generation", 
(Alexandersson and Poller, 1998)) or by SRI's MIMI 
summarizer (Kameyama et ~d., 1996). l{ecent work 
on spoken language summarization i unrestricted 
domains has focused ahnost exclusively on Broad- 
cast News, mostly due to the spoken hmguage track 
of recent TREC evaluations (Oarofolo et al, 1997; 
Garotblo et al, 1999). (Waibel et a.1., 1(.)98) describe 
a Meeting Browser where summaries earl be gener- 
ated using technology established for written texts. 
(Va.lenza. el. M., 1999) go one step further and incof  
pora.te knowledge from the speech recognizer (con- 
fidence scores) into their summarization system, as 
well. 
We a.rgue that the nature of spoken dialogues, to- 
gether with their textual representations a speech 
recognizer hypotheses, requires a. set of specific al> 
proa.ches to make summarization feasible for this 
text genre. 
As a demonstrable proof of concept, we present 
the multi-stage a.rchitecture of the summa.rization 
system I)IASUMM which can flexibly deal with spo- 
ken di,dogues in English and Spa.nish, without any 
restrictions of domahl. Since it cannot rely on a.ny 
domain specific knowledge base, it uses shallow sta- 
tisticaJ approaches and presents (possibly modified) 
ca:lracts from the original text. as summa.ry. 
We. present results of several evaluations of our 
system using human transcripts of spontaneous tele- 
phone conversations in English and Spanish from the 
(~,AI,LIIOME corl)/ls ((LI)C), 1996), in particular the 
accura.cy of the topic segmentation and in\[brmat.ion 
condensing components (sections (5 and 7). Also, Ibr 
I.he purpose of a global evaluation, a user study was 
l~ei:%i:med which a.ddresscd in\[or\]nation access t.inJe 
a.nd a.ccura.ey of retaine.d information eompa.ring dif- 
ferent versions of summaries (section 10). 
This paper is organized as follows: In the next sec- 
tion, we provide, a.n ow;rview a.bout t}ie in,till issues 
Ibr summa.rization of Sl)oken dialogues and indicate 
I;hc "~l)l)roaches we, are taking in our system. We 
then present he system a.rchitecture (section 3), fol- 
lowed by a. detailed description of the readier building 
blocks (sections <1 to 8). After a. brief elmra.cteriza- 
tion of the (2 UI (section 9) we describe a user study 
for global system evaluation in section 10. We con- 
clude the pa.per with a smmnary and a brief outlook 
in section 11. 
2 Issues and Approaches: Overview 
In this section, we give a,n overview about the main 
issues that a.ny sunmmrizat;ion system for spoken di- 
a.logues has to address mid indica.te the approach we 
are taking for each of these in I)IASUMM. 
In a generM sense, when dealing with written 
texts, usually there is plenty of information avail- 
able which can be used lbr the purpose of summa- 
968 
rization, such as capitalization, i)un(-tuation ~narks, 
t,itles, passage head(rs, i)aragral)h boundaries, or 
other ,nark-ul)S. (hfforl.mud.ely, however, ,,onc (.)f 
this holds for :q)ccch data whh:h arrives as a stream 
of word l,ok('w; from ;I recognizer, (:ut iuto "utt(.q'- 
antes" by using a silence heuristi('. 
2.1. Lack of  clause. 1)Oulldaries 
One of the mosl. serious issues is the lack el senten(:e 
or clause boundaries in spoken dialogues whi(:h ix 
particularly problemati(: .;in(:e scnten(:es, clauses, or 
l)aragral)hs a.re (.:onsidercd the "minimal re,its" in 
virtually all existil,g summarizat ion systcu,s. \'Vheu 
humans speak, they so,lletillles pause durinq a 
(:\]a.use, and not always at. l.he eml of a claus(', whi(:h 
means that the outl)ut of a r(;coguizer (whi(:h us,t- 
ally uses some silelme-heuristics to cut the segments) 
frequently does nol real,eli Iogi(:al sep, l,en(:e or clause 
boundaries, l,ooking at five I';nglish (~A,,I,HOM,,: (li- 
alogues with an average ii/11111)(".1' of :{20 iltl\[.('3'a,l('.c~.q 
eat.h, we find on average 30 such "(:ontinuations" of 
logical clauses over automa.ti(:ally detcrmiued a(:ous- 
tit" segment I)ounda.ries. lu a smmnary,  this can 
cause a. r(;du(:tion in coh(,,ren(:c and r<~dability of 
the outlmt. 
We address this issue I)y linking adjac(;nt tm'ns 
of th(; smue sl)eaker together if the silence between 
them ix less than a given col,sl.\[/llt (se(;tioll d). 
2.2 Distr i lml ;c .d int ' (n 'mat io l l  
Siuce we have multi-pari,y conversations as o\])l)oscd 
to Inonologi('al texts, sonmtimcs the cru(:ial in\['or- 
matiou is found in a question-auswer-l)air , i.e., it 
involv('s more than oue Sl)eaker; extracting ouly the 
question or only the auswer wo,ld be meaningless 
in ma.ny cases. We found that on average about 
10% el' the speaker turns belong to such question- 
answer l)airs in five examined English (~AIA,IIOME 
dialogues. Often, either the question or the answer 
ix very shoI:t and does not contain any words with 
high relevan(:c. In order not to "lose" these short 
tutus at a later stage, when only the n~ost, relevant 
turns are extracted, we link them to the matching 
question/answer ahead of/. ime, using two different 
methods to detect questions aud their answers (sec- 
tion 4). 
2.3 D is t luent  speech 
Speech disfluencies in spontaneous convers,ttions - -  
such as fillers, repetitions, repairs, or unfinished 
clauses -- can make transcril)ts (and summary  ex- 
tracts) quite ha.rd to read and also introduce all tin- 
wanted bias to relevance computat ions (e.g., word 
repetitions would cause a higher word count tbr the 
repeated content words; words in untinished clauses 
would be included in the word count.) 
'l'o alleviate this problem, we employ a clean-up 
tilter pipeline, which eliminates liller words and ,:el)- 
el.it.ions, and segments the tm'ns into short clauses 
(sectiou 5). \Ve also remove incomplete clauses, typ- 
ically sentem:c-iuitial repairs, at this stage of our 
'.syst?lu. This "clea.niug-up" serves two main pur- 
1)oscs: (i) it. im:rea~cs tim readabilit3~ (for the fiually 
(;xtracl.cd segments); and (ii)it. ~nakcs the text more 
tractable by subsequent modules. 
The following exalnl)le com\])arcs a turn before and 
after t.he clean-up component:  
before: I MEAN WE LOSE WE LOSE I CAN'T I 
CAN'T DO ANYTHING ABOUT IT SO 
after: we lose / i can't do anything 
about it 
2.4 Lack of tel)i(" l)oundaries 
(;AI,I,IIOME s\])c'e(;h data is lll/llti-to\])ica\] I)tlt does 
uot include mark<q) \['or pa.ragral)hs, nor al,y tolfie- 
inforlJ,ative headers. Tyl)ically, we lind about 5 I0 
(.lilt'erent opics within a 10-mimd;e segment of a di-- 
ah)gue, i.e., the. topic changes about every 1 2 min- 
utes in these conversations. To facilitate browsing 
and smHtlmrization, we thus have to discover topi- 
(:ally coherent, segl,lents automatical ly.  This is done 
using a TextTi l ing approach, adapted t'ron~ (l\]earst, 
\]997) (section (i). 
2.5 Speech. reeog l f i zer  e r rors  
Imst but not least, we face t.he l)roblcm of iml)er- 
t'e(:t word a(:cura(:y of sl)eech recognizers, l)articu- 
larly when (h'.a~ling with Sl)OUl.a\]mous t)eech over a 
large vo(:al)uhu'y aud over a low I);mdwi(Ith (:hamJe\], 
SIIC\]I \[~S l,h(~ (',AI,I,IIOME ({at;tl)asc's which we Juainly 
used for develol)lnent , testing, and evaluatiou of our 
syste/n. (hu'r(mt recognizers tyl)ically exhibit word 
error rates \['or l,hese (:orl)ora ill the order of 50%. In 
I)IASUMM's hfl'ormation condensation component,  
the relevaucc weights of speaker ttlr,ls (:all be ad- 
justed to take into acc.omd, their word confidence 
scores from 1.111; sl)eech recognizer. That  way we can 
reduce the likelihood of extra.eting passages with a 
larger amount of word lnisreeognitions (Zeclmer and 
\Vaibel, 201111). lu this 1)aper, however, the focus will 
be exclusively on results of our evaluations on hu- 
man generated transcripts. No information from the 
speech recognizer nor from the acoustic signal (other 
than inter-utterance pause durations) are used. We 
are aware that in particular prosodic information 
may be of help for tasks such as the detection of 
sentence boundaries, speech acts, or topic bound- 
aries (l\]irschberg ~md Nakatani, 1998; Shriberg et 
al., 1998; Stolcke et al, 2000), but the investigation 
of the integration of this additional source of i n fer  
marion is beyond the scope of this pal)er and lel't tbr 
future work. 
3 System Arch i tec ture  
The global system architecture of I)IASUMM is a 
1)ipeline of the tbllowing lbur major components: 
969 
inputtor \] 
CLEAN ~ Turn Linking 
and TELE ! 
i 
\] Clean-up Filter 
! 
I i 
\] 
J 
input fo r .  Topic Segmentation 
TRANS 
i l 
Information Condensation ~ TRANS 
i 
L 
1 71-  - - - \]7 7 -  ~ CLEAN 
Telegraphic Reduction TELE 
Fignre 1: System architecture 
turn linking; clean-up filter; topic segmentation; and 
information condensation. A. fifth component is 
added a.t the end for the purpose of telegraphic re- 
duction, so that we can maximize the information 
content in a given amount of space. The system ar- 
chitecture is shown in Figure 1. It also indicates the 
three major types of smnmaries which can be gener- 
ated by l)Ia SUMM: 'P\]~ANS ("transcript"): not using 
the linking and clean-up components; CLEAN: ris- 
ing the main four components; 'I'EI,E ("telegraphic" 
summary): additionally, using the telegraphic reduc- 
tion component. 
The following sections describe the components of 
DIASUMM ill more detail. 
4 Turn  L ink ing  
The two main objectives of this component are: (i) 
to form turns which contain a set of full (and not 
partial) clauses; and (ii) to forln turn-pairs in cases 
where we have a question-answer pair in the dia- 
logue. 
To achieve the first objective, we scan the input for 
adjacent turns of one speaker and link them together 
if their time-stamp distance is below a pre-specified 
threshold 0. If the threshold is too small, we don't 
get most of the (logical) turn continuations across 
utterance boundaries, if it is too large, we run the 
risk of "skipping" over short but potentiMly relevant 
Daglnents of the speaker on the other channel. We 
experimented with thresholds between 0.0 and 2.0 
seconds and determined a local performance maxi- 
mum around 0 = 1..0. 
For the second objective, to form turn-pairs which 
comprise a question-answer information exchange 
between two dialogue participants, we need to detect 
wh- and yes-uo-questions i  the dialogue. We tested 
\] English \] Spanish 
Annotated l)ata 
turns 1603 1185 
Wh-questions /12 78 
yes-no-questions /t3 98 
questions total 85 (5.3%) 176 (14.9%) 
Automatic Detection Results (F1) 
SA classifier 
POS rules 
raudom baseline 
0.24 0.22 
0.22 0.37 
0.02 0.13 
Tahle 1: Q-A-pair distribution in the data and ex- 
pel'imental results for automatic Q-A-detection 
two approa.ches: (a) a I tMM based speech a.ct (SA) 
classifier (\]/Jes, \] 999) and (b) a set of part-of-speech 
(POS) based rules. The SA classifier was trained oll 
dialogues which were manually annotated for speech 
acts, using parts of the SWITCIIBOARI) corpus (God- 
frey et al, 1992) for Fmglish and CALLIIOMF, for 
Spanish. The corresponding answers for the de- 
tected questions were hypothesized in the first turn 
with a. different sl)eaker , following the question-turn. 
Table 1 shows the results of these experiments for 5 
English and 5 Spanish CAI,L\]IOME dialogues, corn- 
payed to a baseline of randomly assigning n question 
speech acts, n being the number of question-turns 
marked by human a.nnotal~ors. We report Fl-seores, 
where F1 - ~ with P=preeision and /g--recall. 
We note that while the results \[br the SA-classifier 
and the rule-based approach are very similar for En- 
glish, the rule-based apl~roach yields better results 
tbr Spanish. The much higher random baseline for 
Spanish can be explained by the higher incidence of 
questions in the Spanish data (14.9?/(, vs. 5.3% for 
English). 
5 C lean-up  F i l te r  
The clean-up component is a sequence of modules 
which serve the purposes of (a) rendering the tran- 
scripts more readable, (b) simplifying the input for 
subsequent components, and (c) avoiding unwanted 
bias for relevance computations ( ee section 2). All 
this has to happen without losing essential informa- 
tion that could be relevant in a summary. While 
other work (\]\]eeman et al, 1996; Stolcke et al, 1998) 
was concerned with building classifiers that can de- 
tect and possibly correct wn:ious speech disfluencies, 
our implementntion is of a much simpler design. It 
does not require as much lnanual annota.ted train- 
ing data and uses individual components for every 
major category of disfluency.1 
t While we have not yet numerical ly evaluated the per fo f  
mance of this component,  its output  is deemed very natura l  to 
read by system users. Since the focus and goals of this contpo- 
nent are somewhat  different han l)reviotts work in that  area, 
meaningful  compar isons are hard to make. 
970 
Single or multiple word repetitions, fillers (e.g., 
"uhm"), and discourse markers without semantic 
content (e.g., "you know") a.re removed fl:om the in- 
put, some short forms axe expanded (e.g., "we'll" 
-+ "we will"), a.nd fl'cquent word sequences are 
combined into a single token (e.g., % lot of" -+ 
"a_lot_of"). 
Longer tm'ns are segmented into shorl clauses, 
which are defined a.s consisting of at least a. sub- 
ject and a.n inIlectcd verbal form. While (Stolcke 
and Shriberg, 1996) use n-gram models for this task, 
and (C~awald~t et al, 1997) use neura.l networks, we 
decided to use a. rule-based approach (using word 
a,nd POS information), whose performa.nce proved 
to be compat'able with the results in the cited \])~- 
pets (1,'~ > 0.85, error < 0.05). ~ 
leo, . several of tile clea.n-up filter's components, we 
ina.ke use of Brill's POS ta.gger (Ih:ill, I,(),qd). For 
Fmglish, we use ~t modified version of Brilt's original 
t~g set, and the tagger was adapted and retra.ined for 
Sl)oken langua.ge orl)ora, (CAIAAIOME a.lKl SWITCll-  
tlOalU)) (Zechner, 1997). For S1)anish, we crea.ted 
our own tag set., derived from the l,l)C lexicon and 
front the CI{ATEI/. project (LeOn, 1994), and trained 
the tagger on ma.nua.lly annotated (~;AI,I,IIOME dia- 
logues, l!'urthernlore, a. POS based sha.lk)w chunk 
parser (Zechner a.nd Wa.ibel, 1998) is used to fill.('.,' 
(,tit. likely ca.ndidates for incomplete, clauses dne to 
speech repair or interrul)tion by the other Slleaker. 
6 Topic Segmentation 
,~illce CAI,I,IIOME dialogues are a.lways multi-topica.I, 
segmenting them into tOl)ical units is an important 
:;tel) in our summa.riza.tion system. '.l'his allows us 
to l)rovi(le "signature?' information (frcqllenl; coil- 
tent words) about every topic to the user as a. hell) 
for faster 1)rowsing and accessing the dat.a., l,'ur- 
thel:more, the subsequent informa.tio, condensation 
COI\]l\])Ollent ca.ll ~,VolYk on smaller parts of the diaJogue 
a.nd thus opera.re more ellieiently. 
Following (l{oguraev and Ii{cnnedy, 1997; Ba.rzi- 
la.y and Elhadad, 1997) who use 'l'extTiling (llcarst, 
1997) for their summa.riza.tion systems of written 
text,  we adapted this algorithm (it.s block compar- 
ison version) R)r sl)eech data: we choose turns to 
be minimal units a.nd compute block simila.rity be- 
tween l)locl(s of k turns every d turns. We use 9 
English and 15 Spanish @ALI,tIOMI,; dialogues, man- 
ually annota.ted for topic bounda.ries, to determine 
the optinmm wdues for a set of TextTiling pm:am- 
eters and ~t. the same time to eva.lua.te the accu- 
racy of this algorithm. '.re do this, we ran a.n n-R)ld 
cross-wdidation (".jack-l~nifing") where ~dl dia.logues 
but one are used to determine the 1)est parameters 
"train set") m,d the remaining dia.logue is used as 
2'\]'lie COIIIIIDA'isoII W~:tS (\[OllC OI1 t.he S~-tllle <latat set as  used  
m (Gav;ddh ctal. ,  1997). 
English Spanish 
blocksize k 25 15 
sample distance d 2 2 
rounds of smoothing r 2 l 
smoothing width s 2 \] 
'l.'able 2: OptimM 'l>xt'.l.'iling pa.rameters for English 
and Spanish CAI,IAIOME dialogues 
nmnber of dbdogues 
r~mdom baseline 
test set avg. (%nseen data") 
train set a~vg. ("seen dat?') 
English Spanish 
9 15 
0.34 0.35 
0.58 0.53 
0.69 0.58 
'l'~d)le 3: Topic segmenta.tion results for English and 
Spa.nish CAI,IAIOMI,: dialogues (Fl-Scores) 
a held-out d~ta. set for eva.luation ("test set"). This 
process is rcpea.ted n times and average results are 
reported. Ta.ble 2 shows the set of p~u:ameters which 
worked best for most diak)gues ~md 'Fable 3 shows 
tile eva.hm.tion results of the cross-validation exper- 
iment. /,'~-scores improve I)y 18-2d% absohtte over 
the random baseline for unseen a.nd by 23 35% for 
seen data., the performance for E\]@ish being better 
than for Spanish. 'l'hese results, albeit achieved on 
a. quite different ext genre, are well in line with the 
results in (llea.rst, 1997) who reports a.n absolute im- 
provement of a, bout :20% over a, random baseline for 
seen data. 
7 Information Condensation 
The informa,tion condensa, tion COml)onent is the core 
o\[' our sysl,en:~, lilts pUrl)OSe is to determine weights 
for terms and turns (or linked turn-i)airs ) and then 
to rank the turns a.ccording to their relewmce within 
each topical segment of the dialogue. 
For term-weighting, lf*idf-insl)ired formula.e 
(Sa.lton and Buckley, 1990) are used to empha.size 
words which are in the "middle range" of fl:equency 
in the dialogue a.nd do not a.pl)eat: in a. stop list. :~ 
For turn--ranking, we use a version of the "maximal 
n,argina.l relevance" (MMI{) algorithm (Ca.rbonell 
and Goldstein, 1998), where emphasis is given to 
liurns which conta.in ma.ny highly weighted terms tot" 
the current segment ("sa.lience") a.nd are sutficiently 
dissimila.r to previously ranked turns (to minimize 
redunda.ncy). 
For 9 English and l d Spanish dialogues, the "most 
relevant" turns were nmrl~ed lay hmnan coders. We 
ran a. series of cross-validation experiments o (a,) op- 
timize the parameters of this component related to 
tJ'*idf a.nd MMR computa,tion and to (b) deterlnine 
31,'or l,;nglish, our stop list comprises 557 words, for Span- 
ish, 831 words. 
971 
how well this information condensing component can 
match tile human relewmce annotations. 
Summarization results are comlmted using 1 l-pt- 
avg precision scores t`or ranked turn lists where the 
maximum precision of the list of retrieved turns 
is averaged in the 11 evenly spaced intervals be- 
tween recall=\[0,0.1),\[0.1,0.2), . . \[1.0,1.:1)(Salton 
and McGill, 1.983). 4 Table 4 shows the results from 
these experiments. Similar to other experiments in
the summarization literature (Ma.ni et a.l., 1998), we 
find a wide performance variation across different 
texts. 
8 Telegraphic Reduction 
The purpose of this component is to maximize infor- 
mation in a tixed amount of space. We shorten the 
OUClmt of the summarizer to a "telegraphic style"; 
that way, more inrorma.tion can be included in a 
summary of k words (02: n bytes). Since we only 
use shallow methods for textual analysis that do 
not generate a. dependency structure, we cannot use 
complex methods for text reduction as described, 
e.g., in (Jing, 2000). Our method simply excludes 
words occurring in the stop list fl:om the summary, 
except for some highly inforlnative words such as 'T' 
or  ~11ot ~ . 
9 User  In ter face  and  System 
Per fo r lnance  
Since we want to enable interactive summarization 
which a.llows ~ user to browse through a dialogue 
qnickly Co search for information he is interested 
in, we have integrated our summarization system 
into a 3AVA-based graphical user interface ("Meet- 
ing Browser") (Bert et al, 2000). This interface also 
integrates the output of a speech recognizer (Yu et 
al., 1.999), and can display a wide variety of infer  
1nation about a conversation, including speech acts, 
dialogue games, and emotions. 
For sumlnarization, the user can determine the 
size of the summary and which topical segments 
he wants to have displayed. Ite can also rocus 
the summary on particular content words ("query- 
based summary")  or exclude words from considera- 
tion ("dynamic stop list expansion"). 
Smmnarizing a 10 minute segment of a CALL- 
hOME dialogue with our system takes on average less 
than 30 seconds on a 167 MHz 320 MB Sun Ultral 
workstation.S 
4 We are aware that  this annotat ion and evaluat ion scheme 
is far fl'om opt lmah it does neither reflect the fact that  turns 
are not necessari ly the best units for extract ion or that  the 
11-pt-avg precision score is not optimal ly suited for the sum- 
mar izat ion task. We thus have recently developed a new 
word-based method  for annotat ion  and evaluat ion of spon- 
taneous peech (Zechner, 2000). 
5The average was computed  over five English dialogues. 
10 Human Study  
1(1.1 Exper iment  Set;up 
Ill order to ewduate the system as a. whole, we con- 
ducted a study with humans in the loop to 1)e able Co 
colnpare three types of summaries (TITANS, CLEAN, 
TELE, see section 3) with the fllll original transcript. 
We address these two main questions in this study: 
(i) how fast can information be identified using dif- 
ferent types of summaries? (ii) how accurately is the 
information preserved, comparing different types of 
summaries? 
We did not only ask the user "narrow" questions 
for a specific piece of information - -  along the lines 
of the Q-A-evaluation part. of the SUMMAC confer- 
ence (Mani eC a.l., 1998) -- but also very "global", 
non-specific questions, tied Co a. parCicular (topical) 
segment of the dialogue. 
The experiment was conducted as follows: Sub- 
jeers were given 24 texts each, aceompa.nied by either 
a generic question ("What is the topic of the discus- 
sion in this text segment?") or three specitic ques- 
tions (e.g., "Which clothes did speaker A buy.'?"). 
The texts were drawn from five topical segments 
each rrom five English CAIAAIOME dialogues. (; They 
have four difl>rent formats: (a) fldl transcripts (i.e., 
the transcript of the whole segment) (FULL); (b) 
summa.ry of the raw transcripts (without linking and 
clea.n--up) ('rll.aNS); (c) cleaned-up summary (using 
all four major components of our  sys ten l )  (C,I,I,;AN); 
and (d) telegram suln21\]a, ry (der ived  r ron \ ]  (c),  us ing  
also Cite Celegraphic reduct.ion component) (TI';LE). 
'l'he texts or for,,,a.t,, (b), (c), a.nd (d) were gener- 
ated 1;o have the saaue length: 40% of (a), i.e., we 
use a 60% reduction rate. All these formats can 
be accotnpanied by either a. generic or three specitic 
questions: hence there are eight types of tasks for 
each of the 24: texts. 
We divided the subjects in eight groups such that 
no subject had to l)erform more than one task on 
the same text and we distributed the different Casks 
evenly \['or each group. Thus we cau make unbiased 
comparisons across texts and tasks. 
The answer accuracy vs. a pre-defined answer key 
was manually assessed on a 6 point discrete scale 
between 0.0 and 1.0. 
10.2 ll,esults and Discussion 
Of the 27 subjects taking part in this experiment, 
we included 24 subjects iu the evaluation; 3 sub- 
jects were excluded who were extreme outliers with 
respect o average answer time or score (not within 
/* + -2sCddev). 
From the results in Table 5 we observe the fol- 
lowing trends with respect to answer accuracy and 
response time: 
SOne of the 25 segments  was set aside for demonst rat ion  
purposes. 
972 
English Spanish 
nun+her of dialogues 9 14 
turns t)er dialogue ma.rked ;ts relevant I)y human coders 12% 25% 
I l-pt-a.vg precision (average over t.ol)i(:a.l segnlent.s) 0.45 0.5.0 
score variation between (liak)gues 0.2 0.49 0.15 0.8 
TM)Ie 4: Smmnarizat ion result;s for English and S1)anisll (I~AI,I,IIOME 
I,'ornmt tra ns (:lea.n \] tele 
'\]'ime vs. A(:c. T in . :  \] Ae(.  'l'ime \[ A( C. I T ime \[ Ac( 
generic (q = 72) 
specific (q = 216) 
L full T ime~ Ace. 
I 0._(,. 1D.): s 
, -~ec .  
-%~ \[ 07739 
'l'M)le 5: Average a.nsw('r times (i,, sect a.nd a.ccuracy scores (\[0.0-1.0\]) over eight dilferent tasks (number of 
subjects=2d; q:=mmd)er of questions l)er task type). 
summary  l,ype 
generic / indicative 
speci\[ic / informative 
\[ !)/).s I    wci.,l \] 
Lr Ls 1 ? t  .0 
' l 'able 6: Ilela.tive answer accuracies in % for dill'~,rent 
Sl)l\]llll~/ri(~S 
* ge~w'ric questions ("indicative summarie,s", the 
task being to identi\[y the topic o\[' a text): The 
tWO c leaned u D StlllnFla,ries tool(  M)out, the same 
Lime to in;ocess I)ui. had lower a eeura('y scores 
than tim v(;rsion directly u:dug the trans(:ril)l.. 
* spcc~/ir quest.ions ("ilfl'orlnal.ive sunllllaries", 
the (.ask being Io lilM Sl)ecilie intLrllml ion in t\]l(~ 
re?t): (I) The accuracy advant, age of the raw 
I,ranscripl, sun lmaries ('I'R, A NS) over  the c leal led 
u\]) versions (CLlCAN) is only small (,oZ :;Latis- 
tica.lly signitieant: L:-0.748) 7. (2) 'l'her(" is a 
sui)eriority of the 'l'l,;lA,,-StllnlHary to t)o(;h otJmr 
kinds ('rFLI.: is significa.nlJy more ;,iCCtllX/|l(2 (h~-/.ll 
CLEAN \[()r 1) "~ 0.0~r)). 
l,'rom this w(; conjecture thai. our methods for (:us- 
tomizaJ.ion of the summaries to spoken dialogues is 
mostly relewmt for inJ'ormativc, but llot so tUll(;h 
for indi,:,tivc smmmu'ization. We drink that el.her 
methods, such as lists of signature l)hrases would l)e 
n tor0 effective to use lbr the \]al;tcr \[mrl)ose. 
'l)dtle 6 shows the answer accuracy for the three 
different smmnary  tyl)es relative 1;o the accuracy of 
tile fldl transcripl, texts of l, he sa.me segmenl,s (':rela- 
tive ~mswer a.ccm:acy"). We, observe that; tit(: r('l~d;ive 
accuracy reduction for all smnn\]aries i markedly 
lower than the reduction of tc'xt size: all sunmmries 
were reduced from the full transcripts l)y 60%, 
whereas tile answer a(:(:uracy only drops between 9% 
(TITANS) a,tld 24% (CI,EAN) l()l" the generic quest, ions, 
7111 \['DA;\[,, ill 2, of 5 dialogues. I,\]m CI,I.1AN SIIllllllD, l'y scores 
m:e higher tllall th<>se of the 'I'IIANS summaries. 
and between 20% ('rF, l,l~,) and 29% (CI,F, AN) fOl: the 
speci\[ic questions. This proves that our systeln is 
able to retain most of the relevant information in 
tim summaries.  
As for average' answer times, we see a. ma.rked re- 
duction (3()0{,) of all sunmm.ries coulparcd to the full 
texts in l,hc .qcneric case; for the SlmCific ease, the 
t ime reduction is sonlewhat sma.ller (l 5% 25%). 
One shortcoming of the current, system is thai; it 
oper~d;es on turns (or \[;tlrll-pa.irs) as minimal units 
\['or extraction, tn \[Stture work, we will investigate 
possil)ilities to reduce the minimal units ot7 extrac-- 
l.ion l.o tim level of chmses or sent.<m<:es, wilhoul, giv 
like; Ul) the idea of linking cross-slxmker information. 
1 1 Summary  and  l g l tu re  Work  
\Ve have presented a sunmmrizat ion sysl,e~ for six) 
ken dialogues which is constructed to address key 
difl)renees of spolcen vs. written langua.ge, dia.logues 
vs. monologues, and inul|.i-topical vs. mono-topical  
texts. The system cleans up the input for speech 
disfluencies, links t.urns together into coherent in- 
formation units, determines tOlfica.l segments, and 
extracts the most relevant pieces of informal, ion in 
a user-customiza.ble way. I~;vahml,ions of major sys- 
tem (:Oral)Orients and of t.he systeJn as a. whole were 
1)erfornmd. 'l'hc results of a user sl, udy show that 
with a. sutmna ry size of d0%, between 71% and 911% 
of the inlbrma.tion of the fill\] text is ret.a.ined in the 
summary,  depending on tile type of summary  and 
tim Lyl)('s of quest, ions being asked. 
\?c' are currently extending the system to be able 
to ha.ndle different levels of granularity for extract;ion 
(clauses, sentences, turns), leurthermore, we plan to 
investigate the, integration of l)rosodic information 
into several (-onq)onents of our system. 
12 Acknowledgements  
We wa.nt, l,o tha.nk the almotators for their ell'errs aim 
Klaus Hies for providing l.he automatic speech a(:t 
973 
tagger. We appreciate comments and suggestions 
t?om Alon Lavie, Marsal Gawtld~/, Jade Goldstein, 
Thomas MacCracken, and the &llonymotls l:eviewers 
on earlier drafts of this paper. 
This work was funded in part by the VEf{BMOBI1, 
project of the Federal Republic of Oerma,ny, ATR - 
Interpreting Telecommunications Research L~l)ora- 
tories of Japan, and the US l)epartment of l)efense. 
Re ferences  
AAAI, editor. 1998. Proceedin9s of the AAAI-98 Spring 
Symposium on Intelligent Te.vt Summarization, Stan\]ord, 
CA. 
ACL. 2000. Proceedings of thc ANLP/NAACL-2000 Work- 
shop on Automatic Summarization, Seattle, WA, May. 
Jan Alexaudersson and Peter Poller. 1998. Towards mul- 
tilingual protocol generation for spontaneous speech dia- 
logues. In Proceedings of the INLG-98, Niagara-on-the- 
lahc, Canada, ilugust. 
f{cgina Barzilay and Michael Elhadad. 1997. Using lexical 
chains for text summarization. In ilCL/EACL-97 Work- 
shop on Intelligent and Scalable Te.vt Summarization. 
Michael Bert, l{alph Gross, llua Yu, Xiaojin Zhu, Yue Pan, 
Jie Yang, and Alex Waibel. 2000. Multimodal meeting 
tracker. In Proceedings o\] the Conference on Content- 
Based Multimedia Information Access, IHAO-2000, Paris, 
l<7'ance, April. 
Braniinir Boguraev and Chrlstol)hcr I(cnnedy. 1997. 
Salience-based characterisation of text documents. In 
A CID/EA CL- 97 Workshop on Intelligent and Scalable Text 
Summarization. 
Eric Brill. 1994. Some advances in transforlnation-I)~ed part 
of speech tagging. In Proceeedings o.f AAAI-9/~. 
Jaime Carbonell mid Jade Goldstein. 1998. The use of MMR, 
diversity-based reranking for reordering docunlents and 
producing summaries. In Proceedings o.f the 21st ACM- 
SIGIJg International Co,florence on Research and Devel- 
opment in lnJormation ll.ctrieval, Melbour~;c, Australia. 
Johl\] S. Garofolo, Ellen M. Voorhees, Vincent M. Stanford, 
and l(aren Sparck .\]ones. \]997. TI{I\]C-6 1997 spoken doc- 
IllllellL retriewfl track overview and results. In Proceed- 
in9s o.\[ the 1997 "17H?C-6 Conference, Gaithe'rsburg, MI), 
November, pages 83 -91. 
John S. Garofolo, Ellen M. Voorhees, Cedric G. P. Auzanne, 
and Vincent M. Stratford. 1999. Spoken doculnent re- 
trieval: 1998 evaluation aud investigation of new inetrics. 
In Proceedings of the ESCA workshop: Accessing informa- 
tion in spoken audio, pages 1-7. Camloridge, UK, April. 
Morsel Gawddh, Klaus Zechner, and Gregory Aist. 1997. 
Iligh perforlnauce s gnlentation f spontaneous speech us- 
ing part of speech and trigger word infornmtion. In Pro- 
eeedin9 s of the 5th ANLP Conference, Washington DO, 
pages 12-15. 
J. J. Godfrey, E. C. ltolliman, and J. Mcl)mfiel. 1992. 
SWITCttBOARD: telephone speech corpus for research mid 
development. In Proceedings of the IUASSP-92, vohnne 1, 
pages 517-520. 
Martl A. IIearst. 1997. TextTiling: Segmenting text into 
multi-paragraph subtopic passages. Computational Lin.- 
guistics, 2311):33-64, March. 
Peter A. IIeeman, Ieyung he Loken-Khn, and James 1:. Allen. 
1996. Oombining the detection and correction of speech 
repairs. In Proceedin9s of ICSLP-96. 
Julia Ilirsehberg mid Christine Nakatmfi. 1998. Acoustic 
indicators of topic segmentation. In Proceedings o.f the 
ICSLP-98, Sydney, Australia. 
IIongyan Jing. 2000. Sentence reduction for automatic text 
sum,narlzation. In Proceedings of ANIH~-NAA CL-2000, 
Seattle, WA, May, pages 310-;315. 
Megumi Kameyama, Goh Kawai, and isao Arima. 1996. A 
real-tinie systcni for summarizing human-human sponta- 
neous spoken dialogues. Ill Proceedings of the ICSLP-96, 
pages 681-684. 
Linguistic Data Consortium (LDC). 1996. CallHome alld 
CallFriend LVCSR databases. 
Fernando S~nchez \[,edn. 1994. Spanish l.agset for tile 
CI~.ATIBR project, http://xxx.lanl.gov/cinp-lg/9406023. 
lndetjeet Mani and Mark Maybury, editors. 1997. Proceed- 
in gs of the A CL/ICA CL '97 Workshop on Intelligent Scal- 
able Text Summarization, Madrid, Spain. 
\]ndet:ieet Mani, I)avid ltouse, Gary Klein, l,ynette 
Hirschman, Leo Obrst, Therese Firmin, Michael 
Chrzanowsld, and lJeth Sundheim. 1998. The 'I'\]P- 
STER SUMMAC text summarization evaluation. Mitre 
Technical Report MTIi 98W0000138, October 1998. 
Klaus liles. 1999. ItMM and neural network based speech 
act detectiou. \]n Proceedings o\] the ICASSP-99, Phoenix, 
Arizona, March. 
Gerard Salton and Chris Buckley. 1990. \]?lexlble text match- 
ing for information retrieval. 'Pcchnical report, Cornell 
University, Department ofComputer Science, TR. 90-1158, 
September. 
Germ'd Salton and Michael J. McGill. 1983. Introduction to 
Modern Information ltetrieval. McO,'aw IIill, q\~kyo etc. 
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke, Paul 
q)*ylor, Daniel aurafsky, Klaus f{ies, Noah Coccaro, l{achel 
Martin, Marie Meteer, and Carol Van Ess-Dykema. 1998. 
(Jan prosody aid the automatic classification ofdialog acts 
in conversational speech? Lan9aa9 e and Speech, ,1113- 
4):439 487. 
Andrew,s Stolcke and l~lizabeth Shriberg. 1996. Automatic 
linguistic segmentation f conversational speech. In Pro- 
ceedings o\] the I6'SL\]~-96, pages 1005-1008. 
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Marl 
Ostendorf, Dilek IIakkani, Madelei,m Plauche, (JSkhan 
Tfir, and Yu tin. 1998. Automatic detection of sentence 
1ooundm:ies and disfluencies based on recognized words. In 
Proceedings of the ICSLP-98, Sydney, Australia, Decen> 
bet, volunm 5, pages 2247--2250. 
Andreas 8tolcke, ISlizabeth Shriberg, l)ilek IIakkani-Tfir, and 
GSkhan q'fir. 2000. Prosody-based automatic segmenta- 
tion of speech into sentences and topics. Speech Comn~u- 
"nhcatio'a., 32(1-2). 
l/obin Valenza, 3kmy l~obinson, Marianne l\]ickcy, and l{oger 
Tucker. 199,(/. Sunnnarisation of Sl)oken audio through in- 
forniatiou extraction, tn Proceedings o,f the /'TSCA work- 
shop: Aceessin.9 i~fformatio'n i~ spoken audio, pages 111 
116. C.2ambridge, UK, April. 
Alex Waibel, Michael Belt, and Michael Finke. 1998. Meet- 
ing browser: Tracking and summarizillg meetings, in Pro- 
ceedings of the DARPA Broadcast News l/Vo'rkshop. 
Hue Yu, Michael Finke, and Alex Waibel. 1999. Progress 
ill atltonlatic meeting transcril)tion. \]n Proceedings qf 
EUI~OSI'EECI1-99, Budapest, lhm9ary, September. 
Klaus Zeehner and Alex \?aibel. 1998. Using chunk based 
partial parsing of spontaneous speech in unrestricted do- 
mains for reducing word error rate in speech recognition. 
In Proceedings of COLING-A CL 98, \]WIontreal, Canada. 
Klaus Zechner and Alex Waibel. 2000. Minimizing word error 
rate in textual suinnlaries of spoken lmiguage. \]u Procced- 
ings o\] the First Meeting o.f the North American Chapter o.f 
the Association for Computational Linguistics, NAACL- 
2000, Seattle, WA, April/May, pages 186-193. 
Klaus Zechner. 1997. Building chunk level represen- 
tations for spontmmous peech in unrestricted do- 
mains: The CHUNI';Y system and its al)plication to 
reranking N-best lists of a speech recognizer. Mas- 
ter's thesis (project report), Oh/I_U, available fl'om: 
http  : / /wuu.  es .  emu. edu/-zechner/publ icat  ons. html. 
Klaus Zechner. 2000. A word-based annota- 
tion and evaluation scheme for summariza- 
tion of Sl)ontancoIJs speech. Awfilablc fi'oni 
http://www.cs.?,,,,.eduFzechner/pubiications.i,1:ml. 
974 
Improving Statistical Machine Translation in the Medical Domain  
using the Unified Medical Language System 
Matthias Eck 
 
 
 
matteck@cs.cmu.edu 
Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
vogel+@cs.cmu.edu 
Alex Waibel 
 
 
 
ahw@cs.cmu.edu 
 
Abstract 
Texts from the medical domain are an 
important task for natural language 
processing. This paper investigates the 
usefulness of a large medical database (the 
Unified Medical Language System) for the 
translation of dialogues between doctors and 
patients using a statistical machine translation 
system. We are able to show that the 
extraction of a large dictionary and the usage 
of semantic type information to generalize the 
training data significantly improves the 
translation performance. 
1 Introduction 
Hospitals in the United States have to deal with 
an increasing number of patients who have no 
knowledge of the English language. It is not 
surprising that in this area translation errors can 
lead to severe problems (Neergard, 2003; Flores et 
al. 2003). This is one of the main reasons why the 
medical domain plays an important role in many of 
the current projects involving natural language 
processing. Especially many text or speech 
translation projects include tasks to translate texts 
or dialogues with medical topics.  
The goal of this research was the improvement 
of translation quality in the medical domain using 
a statistical machine translation system. A 
statistical machine translation system deduces 
translation rules from large amounts of parallel 
texts in the source and target language.  
The general approach to gather as much training 
data as possible is usually complicated and 
expensive. So it is necessary to make use of 
already available data and databases and it is 
reasonable to hope that some ideas and special 
methods could actually improve the performance 
in limited domains, like the medical domain.  
The Internet and especially the WWW offers a 
lot of data related to medical topics. Especially 
interesting and promising for us was the Unified 
Medical Language System? (UMLS, 1986-2004) 
available from the US National Library of 
Medicine. It provides a vast amount of information 
concerning medical terms and we extracted 
information from this database to improve an 
existent translation system.  
The paper will first give an introduction into the 
Unified Medical Language system. We will then 
point out which parts could be useful for statistical 
machine translation and later show how the 
baseline system was actually significantly 
improved using this data. 
2 The Unified Medical Language System 
2.1 Introduction 
The Unified Medical Language System (UMLS, 
1986-2004) project was initiated in 1986 by the 
U.S. National Library of Medicine. It integrates 
different knowledge sources into one database (e.g. 
biomedical vocabularies, dictionaries). 
The goal is to help health professionals and 
researchers to use biomedical information from 
these different sources. It is usually updated about 
3 or 4 times per year. 
It consists of three main knowledge repositories, 
the UMLS Metathesaurus, the UMLS Semantic 
Network and the SPECIALIST lexicon. 
Interesting facts about the UMLS, related work 
and further information can be found in 
(Lindbergh, 1990; Kashyap, 2003; Brown et al, 
2003; Friedman et al, 2001; Zweigenbaum et al, 
2003). 
2.2 The UMLS Metathesaurus 
The UMLS Metathesaurus provides a common 
structure for approximately 100 source biomedical 
vocabularies.  
The 2003AB1 version of the Metathesaurus 
contains exactly 900,551 concepts named by 
2,247,457 terms.  It is organized by concept, which 
is a cluster of terms (i.e. synonyms, lexical variants 
                                                    
1
 2003AB was the actual release when the 
experiments described in this paper were executed. The 
most recent version now is 2004AA, which contains 
certain additional and updated information. All numbers 
given in this paper are according to the 2003AB 
version. 
and translations) with the same meaning. 
Translations are present for up to 14 additional 
languages besides English. It is very likely that 
other languages will be added in later releases. 
 
Table 1 shows the distribution of the terms 
according to the 15 different languages. 
 
Language Number of Terms 
English 1860683 
Spanish 73136 
German 71316 
Portuguese 69127 
Russian 44907 
Dutch  38600 
French 38249 
Italian 24992 
Finnish 22382 
Danish 723 
Swedish 723 
Norwegian 722 
Hungarian 718 
Basque 695 
Hebrew 484 
Table 1: Languages in the UMLS 
 
For example the concept ?arm? includes the 
English lexical variant, its plural form, ?arms? and 
with ?bras?, ?arm?, ?braccio?, ?braco?, ?ruka? and 
?brazo? the French, German, Italian, Portuguese, 
Russian and Spanish translations.  
 
Some entries contain case information, too, and 
the entries are not limited to words but some terms 
are also longer phrases like  ?third degree burn of 
lower leg? or ?loss of consciousness?. 
 
It also includes inter-concept relationships 
across the multiple vocabularies. The main 
relationship types are shown in Table 2: 
 
Relationship types 
broader  
narrower 
other related 
like 
parent 
child 
sibling 
is allowed qualifier 
can be qualified by 
is co-occurring with 
Table 2: Relationship types 
 
 
The synonym-relationship is implicitly realized 
by different terms that are affiliated with the same 
concept. 
The co-occurrence relationship refers to 
concepts co-occurring in the MEDLINE-
publications. 
 
In addition each concept is categorized into 
semantic types according to the UMLS Semantic 
Network. 
2.3 The UMLS Semantic Network 
The UMLS Semantic Network categorizes the 
concepts of the UMLS Metathesaurus through 
semantic types and relationships.  
Every concept in the Metathesaurus is part of 
one or more semantic types. 
There are 135 semantic types arranged in a 
generalization hierarchy with the two roots 
?Entity? and ?Event?. This hierarchy is still rather 
abstract (e.g. not deeper than six).  
A more detailed generalization hierarchy is 
realized with the child, parent and sibling 
relationships of the UMLS Metathesaurus. 
 
Figure 1 shows some examples for semantic types. 
 
Entity 
    Physical Object 
        Organism 
        Anatomical Structure 
            Fully Formed Anatomical Structure 
                Body Part, Organ or Organ Component 
        Manufactured Object 
            Medical Device 
                Drug Delivery Device 
            Clinical Drug 
Event 
    Activity 
        Behavior 
            Social Behavior 
        Occupational Activity 
            Health Care Activity 
                Laboratory Procedure 
    Phenomenon or Process 
        Human caused Phenomenon or Process 
Figure 1: Some semantic types 
2.4 The SPECIALIST lexicon 
The SPECIALIST lexicon contains over 30,000 
English words. It is intended to be a general 
English lexicon including many biomedical terms.  
The lexicon entry for each word or term records 
the syntactic, morphological and orthographic 
information. 
 
{base=anesthetic 
spelling_variant=anaesthetic 
entry=E0330018 
        cat=noun 
        variants=reg 
        variants=uncount 
} 
Figure 2: Example entry from the  
Specialist Lexicon 
 
Figure 2 shows the entry for ?anesthetic?. There 
is a spelling variant ?anaesthetic? and an entry 
number. The category in this case is noun (there is 
another entry for ?anesthetic? as an adjective). The 
variants-slot contains a code indicating the 
inflectional morphology of the entry. ?anesthetic? 
can either be a regular count noun (with regular 
plural ?anesthetics?) or an uncountable noun. 
 
3 Machine Translation Experiments 
3.1 The Baseline System 
The Baseline system, which we used to test 
different approaches to improve the translation 
performance, is a statistical machine translation 
system. The task was to facilitate doctor-patient 
dialogues across languages. In this case we chose 
translation from Spanish to English. 
 
The Baseline system was trained using 9,227 
lines of training data (90,012 English words, 
89,432 Spanish words). 3,227 lines of this data are 
?in-domain? data. We collected doctor patient 
dialogues during ongoing research projects in our 
group and used this data as training data. The 
6,000 other lines of training data are out of domain 
data from the C-Star Project. This data also 
consists of dialogues but not from the medical 
domain.  
 
The test data consists of 500 lines with 6,886 
words. The test data was also taken from medical 
dialogues between a doctor and a patient and 
contains a reasonable number of medical terms but 
the language is not very complex. Figure 3 shows 
some example test sentences (from the reference 
data). 
  
 (?) 
Doctor: The symptoms you are describing and 
given your recent change in diet, I believe 
you may be anemic. 
Patient: Anemic? Really? Is that serious? 
Doctor: Anemia can be very serious if left 
untreated. Being anemic means your body 
lacks a sufficient amount of red blood cells 
to carry oxygen through your body. 
 (?) 
Figure 3: Example test sentences (reference) 
 
The Baseline system uses IBM1 lexicon 
transducers and different types of phrase 
transducers (Zhang et al 2003, Vogel et al 1996, 
Vogel et al 2003). The Language model is a 
trigram language model with Good-Turing-
Smoothing built with the SRI-Toolkit (SRI, 1995-
2004) using only the English part of the training 
data. 
 
The Baseline system scores a 0.171 BLEU and 
4.72 NIST. [BLEU and NIST are well known 
scoring methods for measuring machine translation 
quality. Both calculate the precision of a 
translation by comparing it to a reference 
translation and incorporating a length penalty 
(Doddington, 2001; Papineni et al, 2002).] 
3.2 Extracting dictionaries from the UMLS 
The first way to exploit the UMLS database for a 
statistical machine translation system naturally is 
to extract additional Spanish-English lexicons or 
phrasebooks.  
The UMLS Metathesaurus provides translation 
information as we can assume that Spanish and 
English terms that are associated with the same 
concept are respective translations.  For example 
as the English term ?arm? is associated with the 
same concept as the Spanish term ?brazo? we can 
deduce that ?arm? is the English translation of 
?brazo?.  
Unfortunately the UMLS does not contain 
morphological information about languages other 
than English. This means it cannot be 
automatically detected that ?brazo? is the singular 
form and thus the translation of ?arm? and not the 
translation of ?arms?. 
As most of the entries are in singular form we 
just extracted every possible combination of 
Spanish and English terms regardless of possible 
errors like combining the singular ?brazo? and the 
plural ?arms?. 
The resulting (lower-cased) Spanish-English 
lexicon/phrasebook contains 495,248 pairs of 
words and phrases. This means each Spanish term 
is combined with seven English terms on average. 
This seems to be an extremely huge amount but 
it has to be considered that there are terms in the 
UMLS and the resulting lexicon that are probably 
too special to be really useful for the translation of 
dialogues  (e.g. ?1,1,1-trichloropropene-2,3-oxide? 
translating to ?oxido de tricloropropeno?). 
Nevertheless there are lots of meaningful entries 
as the following experiments show.  
 
Applying the dictionaries to the Baseline system 
In the first step we just added this 
lexicon/phrasebook as an additional transducer and 
did not change the language model.  
The experiment showed a nice increase in BLEU 
and NIST performances and scored at 0.180 BLEU 
and 4.86 NIST. 
This system especially has a higher coverage, as 
only 302 words (types) are not covered by the 
training data compared to 411 for the baseline 
system. 
 
Adding the English side to the Language Model 
As the extracted dictionary contained many 
phrases it seemed reasonable to add the English 
side to the language modeling data. This also 
prevents words from the extracted dictionary to be 
treated as ?unknown? by the language model if 
they were not in the language model training data. 
This further improved the BLEU and NIST scores 
to 0.182 BLEU and 4.92 NIST. 
 
It should not be surprising to get an improvement 
in these first two experiments because basically 
just more data was used to train the systems. The 
really interesting ideas will be presented in the 
next sections. 
3.3 Using the Semantic Type Information 
The overall idea to use the semantic type 
information is to generalize the training data. 
The training data contains for example sentence 
pairs like: 
 
Necesito examinar su cabeza. 
I need to examine your head. 
Necesito examinar su brazo. 
I need to examine your arm. 
Necesito examinar su rodilla. 
I need to examine your knee. 
 
If we could generalize these sentences by 
replacing the special body parts like ?head?, ?arm? 
and ?knee? with a general tag e.g. 
?@BODYPART? and especially treat this tag we 
could use one sentence of training data for every 
body part imaginable in this sentence.  
We would just need an additional lexicon that just 
translates body parts. 
 
Necesito examinar su @BODYPART. 
I need to examine your @BODYPART. 
 
 
We could additionally correctly translate possibly 
unseen sentences like ?Necesito examinar su 
antebrazo? (?I need to examine your forearm?) if 
we could automatically deduce that 
?antebrazo/forearm? is a body part and if we just 
knew this translation pair. 
 
Some additional similar sentences in which we 
could apply the same ideas are: 
 
Enseneme que @BODYPART es. 
Show me which @BODYPART. 
?Que @BODYPART le/la duele? 
Which @BODYPART hurts? 
 
(In the last sentence it actually depends on the 
gender of the body part on the Spanish side if the 
sentence is ??Que @BODYPART la duele?? or 
??Que @BODYPART le duele??. But as we are 
translating from Spanish to English this did not 
seem to be a big problem.) 
 
As stated before every concept in the UMLS 
Metathesaurus is categorized into one or more 
semantic types defined in the UMLS Semantic 
Network. 
The two semantic types ?Body Part, Organ, or 
Organ Component? and ?Body Location or 
Region? from the UMLS Semantic Network cover 
pretty closely what we usually affiliate with the 
colloquial meaning of body part. 
[The terminological difference is that the 
semantic type ?Body Part, Organ, or Organ 
Component? is defined by a certain function. For 
example ?liver? and ?eye? are part of this semantic 
type, whereas the semantic type ?Body Location or 
Region? is defined by the topographical location of 
the respective body part. Examples are ?head? and 
?arm?. The function in this case is not as clearly 
defined as the function of a ?liver?.] 
 
This information was used in the next 
experiment. We first filtered the general Spanish-
English dictionary, we had extracted from the 
UMLS, to contain only words and phrases from the 
two semantic types ?Body Part, Organ, or Organ 
Component? and ?Body Location or Region?. This 
gave a dictionary of 11,260 translation entries for 
body parts. Again each Spanish term is combined 
with about seven English terms on average. 
In the next step we replaced every occurrence of a 
word or phrase pair from this new dictionary in the 
training data (i.e. if it occurred on the Spanish and 
English side) with a general body-part-tag. 
527 sentence pairs of the original 9,227 sentence 
pairs contained a word or phrase pair from this 
dictionary. 
 A retraining of the translation system with this 
changed training data resulted in transducer rules 
containing this body-part-tag. 
By using cascaded transducers (Vogel and Ney, 
2000) in the actual translation the first transducer, 
that is applied (in this case the body-part 
dictionary) replaces the Spanish body part with its 
translation pair and the body-part tag.  
The following transducers can apply their 
generalized rules containing the body-part-tag 
instead of the real body part. 
 
E.g. translation of the sentence:  
 
Necesito examinar su antebrazo. 
 
First step apply body-part dictionary rule  
(antebrazo?forearm) 
 
Necesito examinar su @BODYPART(antebrazo?forearm). 
 
Apply generalized transducer rule: (a rule could 
be: Necesito examinar su @BODYPART ? I need 
to examine your @BODYPART) 
 
I need to examine your @BODYPART(antebrazo?forearm). 
 
Resolve tags: 
 
I need to examine your forearm. 
 
By applying this to the whole translation system 
the score improved to 0.188 BLEU/4.94 NIST. 
Using other semantic types  
As the body-part lexicon and the replacement of 
body-parts proved to be helpful we applied two 
more of these replacement strategies. Consider the 
following 4 sentence pairs from the training data. 
 
?Siente dolor cuando respira? 
Do you feel pain when you breathe? 
?Cuando le empezo la fiebre? 
When did the fever start? 
?Podria ser artritis? 
Could this be arthritis? 
?Es grave la anemia, doctor? 
Is anemia serious, doctor? 
The first two sentences contain findings or 
symptoms with the terms ?dolor/pain? and 
?fiebre/fever?. The second two sentences contain 
diseases with ?artritis/arthritis? and 
?anemia/anemia?. The appropriate semantic types 
from the UMLS Semantic Network for these terms 
are ?Finding? and ?Sign or Symptom? for ?pain? 
and ?fever? and ?Disease or Syndrome? for 
?arthritis? and ?anemia? 
Filtering the Spanish-English dictionary resulted 
in 25,987 ?Finding/Sign or Symptom? translation 
pairs (approximately three English terms per 
Spanish term) and 116,793 ?Disease or Syndrome? 
translation pairs (approximately five English terms 
per Spanish term). 
198 sentence pairs from the training data 
contained a ?Finding/Sign or Symptom?-pair and 
127 sentence pairs contained a ?Disease or 
Syndrome?-pair from these dictionaries. 
 
The final translation with those three semantic 
types replaced in the training data and using the 
three filtered dictionaries with the cascaded 
transducer application gave a translation 
performance of 0.190 BLEU/5.02 NIST.  
This shows that although less than 10% of the 
sentences were affected by the replacement with 
the appropriate tags we could nicely improve the 
overall translation performance. 
 Example translations 
Some example translations comparing the baseline 
and the best system with the reference are listed in 
table 3. 
 
1. Sentence  
Reference 
the condition is called tenosynovitis, which 
is an inflammation of the tendon sheath. 
 
Baseline 
this condici?n diagnostic, which is a 
inflammation from the of the tendon. 
 
Best System 
this condition is called tenosynovitis, which 
is a inflammation of tendon sheath. 
2. Sentence  
Reference 
i guess your work involves a lot of repetitive 
movement, huh? 
 
Baseline 
do you I guess your work require plenty 
baby?s, no? 
 
Best System 
i guess you your work require plenty 
repetitive movements, not? 
3. Sentence 
Reference 
you need vitamin c and iron in your blood 
to help your body 
 
Baseline 
you need vitamin c and iron in your blood 
help rescue to  
 
Best System 
you need vitamin c and iron in your blood 
to help the body 
4. Sentence  
Reference 
did you take anything for the pain? 
 
Baseline 
did you sleep taken anything for the pain? 
 
Best System 
did you taken anything for the pain? 
5. Sentence  
Reference 
i can feel it here, behind my breastbone. 
 
Baseline 
i here, behind of the estern?n. 
 
Best System 
i here, behind of sternum. 
Table 3: Example translations 
 
The last example sentence is an interesting case. 
The best system does not get more words right 
compared to the baseline system and so the 
BLEU/NIST-score does not improve. But 
?sternum? is a synonym of the correct 
?breastbone? and a more technical term. This 
supports the claim that the UMLS tends to contain 
more technical terms (like ?tenosynovitis? in the 
first sentence).  
4 Future work 
It is surely possible to use every semantic type 
from the semantic network in the same way like 
the overall five semantic types, which were used in 
the experiments. We did not do this here because 
further semantic types occurred extremely rarely in 
the test and training data. But this could easily be 
done for other test and training data and it is 
reasonable to expect similar improvements. 
Another idea is to use a more specialized 
approach and to make use of the relationships in 
the UMLS Metathesaurus. Each concept could be 
generalized by its parent-concepts instead of its 
semantic type. The generalization hierarchy for the 
concept ?leg? is for example: leg ? lower extremity 
? extremity ? body region ? anatomy. 
This could be especially helpful when translating 
to morphologically richer languages than English 
because the usage of extremities could differ from 
other body parts for example. 
 
In the extracted dictionaries every translation 
pair was given the same translation probability. It 
might be helpful to re-score these probabilities by 
using information from bilingual or monolingual 
texts to improve the translation probabilities for 
usually frequently used terms compared to rarely 
used terms. 
 
As the example translations showed, the 
extracted dictionaries from the UMLS tend to 
contain technical terms instead of colloquial terms 
(translation ?sternum? instead of ?breastbone?). 
We can further assume that a doctor prefers to use 
the more technical terms and a patient prefers the 
more colloquial terms.  Therefore it could be 
interesting to examine if having two different 
translation systems for sentences uttered by a 
doctor and a patient would improve the overall 
translation performance. 
5 Conclusion 
We carried out four different experiments in order 
to improve a Spanish-English medical domain 
translation system. After sequentially applying 
different ideas the final system shows an 11% 
improvement in BLEU and 6% improvement in 
NIST score.  
Table 4 compares the different experiments and 
scores (the 500k dictionary refers to the dictionary 
that was first extracted from the UMLS with 
495,248 word pairs). 
 
System BLEU NIST 
Baseline system 0.171 4.72 
+500k dictionary 0.180 4.86 
+LM improvement 0.182 4.92 
+body part-tags 0.188 4.94 
+sign/symptom/finding 
+disease/syndrome 
0.190 5.02 
Table 4: Experiments and improvements 
 
With more investigation and the ongoing effort 
of the National Library of Medicine to extent the 
UMLS databases it will hopefully be possible to 
further improve the translation performance.  
 
References 
Allen C. Browne, Guy Divita, Alan R. Aronson, 
Alexa T. McGray, 2003.  UMLS Language and 
Vocabulary Tools, Proceedings of the American 
Medical Informatics Association (AMIA) 2003 
Symposium, Washington, DC, USA. 
George Doddington. 2001. Automatic Evaluation 
of Machine Translation Quality using n-Gram 
Cooccurrence Statistics. NIST Washington, DC, 
USA. 
Glenn Flores, M. Barton Laws, Sandra J. Mayo, 
Barry Zuckerman, Milagros Abreu, Leonardo 
Medina,  
Eric J. Hardt, 2003. Errors in medical 
interpretation and their potential clinical 
consequences in pediatric encounters, Pediatrics, 
Jan 2003. 
Carol Friedman, Hongfang Liu, Lyuda Shagina, 
Stephen Johnson, George Hripcsak, 2001. 
Evaluating the UMLS as a Source of Lexical 
Knowledge for Medical Language Processing, 
Proceedings of the AMIA 2001 Symposium, 
Washington, DC, USA. 
Vipul Kashyap, 2003. The UMLS semantic 
network and the semantic web, Proceedings of 
the AMIA 2003 Symposium,  Washington, DC, 
USA. 
C. Lindberg, 1990. The Unified Medical Language 
System (UMLS) of the National Library of 
Medicine, Journal of the American Medical 
Record Association, 1990;61(5):40-42. 
Lauren Neergard, 2003. Hospitals struggle with 
growing language barrier, Associated Press, The 
Charlotte Observer Sept. 2, 2003 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu, 2002. BLEU: a Method for 
Automatic Evaluation of Machine Translation, 
Proceedings of the ACL 2002, Philadelphia, 
USA. 
SRI Speech Technology and Research Laboratory, 
SRI Language Modeling Toolkit, 1995-2004 
(ongoing) 
http://www.speech.sri.com/projects/srilm/ 
UMLS Unified Medical Language System, 
National Library of Medicine, 1986-2004 
(ongoing) 
http://www.nlm.nih.gov/research/umls/ 
Stephan Vogel and Hermann Ney, 2000. 
Translation with Cascaded Finite State 
Transducers. Proceedings of the 38th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2000), pp. 23-30. Hongkong, 
China, October 2000. 
Stephan Vogel, Hermann Ney, and Christoph Till-
mann, 1996. HMM-based Word Alignment in 
Statistical Translation, Proceedings of COLING 
1996: The 16th International Conference on 
Computational Linguistics, pp. 836-841. 
Copenhagen, August 1996. 
Stephan Vogel, Ying Zhang, Fei Huang, Alicia 
Tribble, Ashish Venogupal, Bing Zhao, Alex 
Waibel, 2003. The CMU Statistical Translation 
System, Proceedings of MT-Summit IX. New 
Orleans, LA. Sep 2003. 
Ying Zhang, Stephan Vogel, Alex Waibel, 2003.  
Integrated Phrase Segmentation and Alignment 
Algorithm for Statistical Machine Translation, 
Proceedings of International Conference on 
Natural Language Processing and Knowledge 
Engineering 2003, Beijing, China, Oct 2003. 
Pierre Zweigenbaum, Robert Baud, Anita Burgun, 
Fiammetta Namer, ?ric Jarrousse, Natalia 
Grabar, Patrick Ruch, Franck Le Duff, Beno?t 
Thirion, St?fan Darmoni, 2003. UMLF: a 
Unified Medical Lexicon for French, 
Proceedings of the AMIA 2003 Symposium, 
Washington, DC, USA. 
 
 
 
 
Architecture and Design Considerations in NESPOLE!:
a Speech Translation System for E-commerce Applications
Alon Lavie,
Chad Langley,
Alex Waibel
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
Fabio Pianesi,
Gianni Lazzari,
Paolo Coletti
ITC-irst
Trento, Italy
Loredana Taddei,
Franco Balducci
AETHRA
Ancona, Italy
1. INTRODUCTION
NESPOLE! 1 is a speech-to-speech machine translation research
project funded jointly by the European Commission and the US
NSF. The main goal of the NESPOLE! project is to advance the
state-of-the-art of speech-to-speech translation in a real-world set-
ting of common users involved in e-commerce applications. The
project is a collaboration between three European research labs
(IRST in Trento Italy, ISL at University of Karlsruhe in Germany,
CLIPS at UJF in Grenoble France), a US research group (ISL at
Carnegie Mellon in Pittsburgh) and two industrial partners (APT
- the Trentino provincial tourism bureau, and Aethra - an Italian
tele-communications commercial company). The speech-to-speech
translation approach taken by the project builds upon previous work
that the research partners conducted within the context of the C-
STAR consortium (see http://www.c-star.org). The pro-
totype system developed in NESPOLE! is intended to provide ef-
fective multi-lingual speech-to-speech communication between all
pairs of four languages (Italian, German, French and English) within
broad, but yet restricted domains. The first showcase currently un-
der development is in the domain of tourism and travel information.
The NESPOLE! speech translation system is designed to be an
integral part of advanced e-commerce technology of the next gener-
ation. We envision a technological scenario in which multi-modal
(speech, video and gesture) interaction plays a significant role, in
addition to the passive browsing of pre-designed web pages as is
common in e-commerce today. The interaction between client and
provider will need to support online communication with agents
(both real and artificial) on the provider side. The language barrier
then becomes a significant obstacle for such online communica-
tion between the two parties, when they do not speak a common
language. Within the tourism and travel domain, one can imagine
a scenario in which users (the clients) are planning a recreational
trip and are searching for specific detailed information about the
1NESPOLE! - NEgotiating through SPOken Lan-
guage in E-commerce. See the project website at
http://nespole.itc.it/
.
regions they wish to visit. Initial general information is obtained
from a web site of a tourism information provider. When more
detailed or special information is required, the customer has the
option of opening an online video-conferencing connection with a
human agent of the tourism information provider. Speech transla-
tion is integrated within the video-conference connection; the two
parties each speak in their native language and hear the synthesized
translation of the speech of the other participant. Text translation
(in the form of subtitles) can also be provided. Some multi-modal
communication between the parties is also available. The provider
agent can send web pages to the display of the customer, and both
sides can annotate and refer to pictures and diagrams presented on
a shared whiteboard application.
In this paper we describe the design considerations behind the ar-
chitecture that we have developed for the NESPOLE! speech trans-
lation system in the scenario described above. In order to make the
developed prototype as realistic as possible for use by a common
user, we assume only minimal hardware and software is available
on the customer side. This does include a PC-type video camera,
commercially available internet video-conferencing software (such
as Microsoft Netmeeting), standard audio and video hardware and
a standard web browser. However, no speech recognition and/or
translation software is assumed to reside locally on the PC of the
customer. This implies a server-type architecture in which speech
recognition and translation are accomplished via interaction with
a dedicated server. The extent to which this server is centralized
or distributed is one of the major design considerations taken into
account in our system.
2. NESPOLE! INTERLINGUA-BASED
TRANSLATION APPROACH
Our translation approach builds upon previous work that we have
conducted within the context of the C-STAR consortium. We use
an interlingua-based approach with a relatively shallow task-oriented
interlingua representation [2] [1], that was initially designed for the
C-STAR consortium and has been significantly extended for the
NESPOLE! project. Interlingual machine translation is convenient
when more than two languages are involved because it does not re-
quire each language to be connected by a set of transfer rules to
each other language in each direction [3]. Adding a new language
that has all-ways translation with existing languages requires only
writing one analyzer that maps utterances into the interlingua and
one generator that maps interlingua representations into sentences.
The interlingua approach also allows each partner group to imple-
ment an analyzer and generator for its home language only. A fur-
Figure 1: General Architecture of NESPOLE! System
ther advantage is that it supports a paraphrase generation back into
the language of the speaker. This provides the user with some con-
trol in case the analysis of an utterance failed to produce a correct
interlingua. The following are three examples of utterances tagged
with their corresponding interlingua representation:
Thank you very much
c:thank
And we?ll see you on February twelfth.
a:closing (time=(february, md12))
On the twelfth we have a single and a double
available.
a:give-information+availability+room
(room-type=(single & double),time=(md12))
3. NESPOLE! SYSTEM ARCHITECTURE
DESIGN
Several main considerations were taken into account in the de-
sign of the NESPOLE! Human Language Technology (HLT) server
architecture: (1) The desire to cleanly separate the actual HLT
system from the communication channel between the two parties,
which makes use of the speech translation capabilities provided by
the HLT system; (2) The desire to allow each research site to in-
dependently develop its language specific analysis and generation
modules, and to allow each site to easily integrate new and im-
proved components into the global NESPOLE! HLT system; and
(3) The desire of the research partners to build to whatever ex-
tent possible upon software components previously developed in
the context of the C-STAR consortium. We will discuss the ex-
tent to which the designed architecture achieves these goals after
presenting an overview of the architecture itself.
Figure 1 shows the general architecture of the current NESPOLE!
system. Communication between the client and agent is facilitated
by a dedicated module - the Mediator. This module is designed to
control the video-conferencing connection between the client and
the agent, and to integrate the speech translation services into the
communication. The mediator handles audio and video data as-
sociated with the video-conferencing application and binary data
associated with a shared whiteboard application. Standard H.323
data formats are used for these three types of data transfer. Speech-
to-speech translation of the utterances captured by the mediator is
accomplished through communication with the NESPOLE! global
HLT server. This is accomplished via socket connections with
language-specific HLT servers. The communication between the
mediator and each HLT server consists mainly of linear PCM au-
dio packets (some text and control messages are also supported and
are described later in this section).
Communication with Mediator
Speech
Recognizer
Module
Parser/Analysis
IF
text
Analysis Chain
Speech
Synthsizer
Generation
Module
IF
text
.
Generation
Chain
Communication with CommSwitch
audio audio
Language X HLT Server
Figure 2: Architecture of NESPOLE! Language-specific HLT Servers
The global NESPOLE! HLT server comprises four separate lang-
uage-specific servers. Additional language-specific HLT servers
can easily be integrated in the future. The internal architecture
of each language-specific HLT server is shown in figure 2. Each
language-specific HLT server consists of an analysis chain and a
generation chain. The analysis chain receives an audio stream cor-
responding to a single utterance and performs speech recognition
followed by parsing and analysis of the input utterance into the in-
terlingua representation (IF). The interlingua is then transmitted to
a central HLT communication switch (the CS), that forwards it to
the HLT servers for the other languagesas appropriate. IF messages
received from the central communication switch are processed by
the generation chain. A generation module first generates text in
the target language from the IF. The text utterance is then sent to
a speech synthesis module that produces an audio stream for the
utterance. The audio is then communicated externally to the me-
diator, in order to be integrated back into the video-conferencing
stream between the two parties.
The mediator can, in principle, support multiple one-to-one com-
munication sessions between client and agent. However, the de-
sign supports multiple mediators, which, for example, could each
be dedicated to a different provider application. Communication
with the mediator is initiated by the client by an explicit action
via the web browser. This opens a communication channel to the
mediator, which contacts the agent station, establishes the video-
conferencing connection between client and agent, and starts the
whiteboard application. The specific pair of languages for a dia-
logue is determined in advance from the web page from which the
client initiates the communication. The mediator then establishes a
socket communication channel with the two appropriate language
specific HLT servers. Communication between the two language
specific HLT servers, in the form of IF messages, is facilitated by
the NESPOLE! global communication switch (the CS). The lan-
guage specific HLT servers may in fact be physically distributed
over the internet. Each language specific HLT server is set to ser-
vice analysis requests coming from the mediator side, and genera-
tion requests arriving from the CS.
Some further functionality beyond that described above is also
supported. As described earlier, the ability to produce a textual
paraphrase of an input utterance and to display it back to the orig-
inal speaker provides useful user control in the case of translation
failures. This is supported in our system in the following way. In
addition to the translated audio, each HLT server also forwards the
generated text in the output language to the mediator, which then
displays the text on a dedicated application window on the PC of
the target user. Additionally, at the end of the processing of an in-
put utterance by the analysis chain of an HLT server, the resulting
IF is passed internally to the generation chain, which produces a
text generation from the IF. The result is a textual paraphrase of the
input utterance in the source language. This text is then sent back
to the mediator, which forwards it to the party from which the ut-
terance originated. The paraphrase is then displayed to the original
speaker in the dedicated application window. If the paraphrase is
wrong, it is likely that the produced IF was incorrect, and thus the
translation would also be wrong. The user may then use a button
on the application interface to signal that the last displayed para-
phrase was wrong. This action triggers a message that is forwarded
by the mediator to the other party, indicating that the last displayed
translation should be ignored. Further functionality is planned to
support synchronization between multi-modal events on the white-
board and their corresponding speech actions. As these are in very
preliminary stages of planning we do not describe them here.
4. DISCUSSION AND CONCLUSIONS
We believe that the architectural design described above has sev-
eral strengths and advantages. The clean separation of the HLT
server dedicated to the speech translation services from the exter-
nal communication modules between the two parties allows the re-
search partners to develop the HLT modules with a large degree
of independence. Furthermore, this separation will allow us in the
future to explore other types of mediators for different types of ap-
plications. One such application being proposed for development
within the C-STAR consortium is a speech-to-speech translation
service over mobile phones. The HLT server architecture described
here would be able to generally support such alternative external
communication modalities as well.
The physical distribution of the individual language specific HLT
servers allows each site to independently develop, integrate and
test its own analysis and generation modules. The organization of
each language specific HLT server as an independent module al-
lows each of the research sites to develop its unique approaches to
analysis and generation, while adhering to a simple communication
protocol between the HLT servers and externally with the mediator.
This allowed the research partners to ?jump-start? the project with
analysis and generation modules previously developed for the C-
STAR consortium, and incrementally develop these modules over
time. Furthermore, the global NESPOLE! communication switch
(the CS) supports testing of analysis and generation among the four
languages in isolation from the external parts of the system. Cur-
rently, requests for analysis of a textual utterance can be transmitted
to the HLT servers via the CS, with the resulting IF sent (via the CS)
to all HLT servers for generation. This gives us great flexibility in
developing and testing our translation system. The functionality of
the CS was originally developed for our previous C-STAR project,
and was reused with little modification.
Support for additional languages is also very easy to incorpo-
rate into the system by adding new language-specific HLT servers.
Any new language specific HLT server needs only to adhere to the
communication protocols with both the global NESPOLE! commu-
nication switch (the CS) and the external mediator. The C-STAR
consortium plans to use the general architecture described here for
its next phase of collaboration, with support for at least three asian
languages (Japanese, Korean and Chinese) in addition to the lan-
guages currently covered by the NESPOLE! project.
The first prototype of the NESPOLE! speech translation system
is currently in advanced stages of full integration. A showcase
demonstration of the prototype system to the European Commis-
sion is currently scheduled for late April 2001.
5. ACKNOWLEDGMENTS
The research work reported here was supported in part by the
National Science Foundation under Grant number 9982227. Any
opinions, findings and conclusions or recomendations expressed in
this material are those of the author(s) and do not necessarily reflect
the views of the National Science Foundation (NSF).
6. REFERENCES
[1] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[2] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[3] S. Nirenburg, J. Carbonell, M. Tomita, and K. Goodman.
Machine Translation: A Knowledge-Based Approach. Morgan
Kaufmann, San Mateo, California, 1992.
 
	 1  	Towards Automatic Sign Translation
Jie Yang, Jiang Gao, Ying Zhang, Alex Waibel 
Interactive Systems Laboratory 
Carnegie Mellon University 
Pittsburgh, PA 15213 USA 
{yang+,jgao,joy,waibel}@cs.cmu.edu
ABSTRACT 
Signs are everywhere in our lives. They make our lives easier 
when we are familiar with them. But sometimes they also pose 
problems. For example, a tourist might not be able to understand 
signs in a foreign country. In this paper, we present our efforts 
towards automatic sign translation. We discuss methods for 
automatic sign detection. We describe sign translation using 
example based machine translation technology. We use a user-
centered approach in developing an automatic sign translation 
system. The approach takes advantage of human intelligence in 
selecting an area of interest and domain for translation if needed. 
A user can determine which sign is to be translated if multiple 
signs have been detected within the image. The selected part of 
the image is then processed, recognized, and translated. We have 
developed a prototype system that can recognize Chinese signs 
input from a video camera which is a common gadget for a tourist, 
and translate them into English text or voice stream. 
Keywords 
Sign, sign detection, sign recognition, sign translation. 
1. INTRODUCTION 
Languages play an important role in human communication. 
We communicate with people and information systems 
through diverse media in increasingly varied environments. 
One of those media is a sign. A sign is something that 
suggests the presence of a fact, condition, or quality. Signs 
are everywhere in our lives. They make our lives easier 
when we are familiar with them.  But sometimes they also 
pose problems. For example, a tourist might not be able to 
understand signs in a foreign country. Unfamiliar language 
and environment make it difficult for international tourists 
to read signs, take a taxi, order food, and understand the 
comments of passersby.  
At the Interactive Systems Lab of Carnegie Mellon 
University, we are developing technologies for tourist 
applications [12]. The systems are equipped with a unique 
combination of sensors and software. The hardware 
includes computers, GPS receivers, lapel microphones and 
earphones, video cameras and head-mounted displays. This 
combination enables a multimodal interface to take 
advantage of speech and gesture inputs to provide 
assistance for tourists. The software supports natural 
language processing, speech recognition, machine 
translation, handwriting recognition and multimodal fusion. 
A vision module is trained to locate and read written 
language, is able to adapt to new environments, and is able 
to interpret intentions offered by the user, such as a spoken 
clarification or pointing gesture.  
In this paper, we present our efforts towards automatic sign 
translation. A system capable of sign detection and 
translation would benefit three types of individuals: tourists, 
the visually handicapped and military intelligence.  Sign 
translation, in conjunction with spoken language 
translation, can help international tourists to overcome these 
barriers. Automatic sign recognition can help us to increase 
environmental awareness by effectively increasing our field 
of vision. It can also help blind people to extract 
information. A successful sign translation system relies on 
three key technologies: sign extraction, optical character 
recognition (OCR), and language translation. Although 
much research has been directed to automatic speech 
recognition, handwriting recognition, OCR, speech and text 
translation, little attention has been paid to automatic sign 
recognition and translation in the past. Our current research 
is focused on automatic sign detection and translation while 
taking advantage of OCR technology available. We have 
developed robust automatic sign detection algorithms. We 
have applied Example Based Machine Translation (EBMT) 
technology [1] in sign translation.  
Fully automatic extraction of signs from the environment is 
a challenging problem because signs are usually embedded 
in the environment. Sign translation has some special 
problems compared to a traditional language translation 
task. They can be location dependent. The same text on 
different signs can be treated differently. For example, it is 
not necessary to translate the meanings for names, such as 
street names or company names, in most cases. In the 
system development, we use a user-centered approach. The 
 
 
 
approach takes advantage of human intelligence in selecting 
an area of interest and domain for translation if needed. For 
example, a user can determine which sign is to be translated 
if multiple signs have been detected within the image. The 
selected part of the image is then processed, recognized, 
and translated, with the translation displayed on a hand-held 
wearable display, or a head mounted display, or synthesized 
as a voice output message over the earphones. By focusing 
only on the information of interest and providing domain 
knowledge, the approach provides a flexible method for 
sign translation. It can enhance the robustness of sign 
recognition and translation, and speed up the recognition 
and translation process. We have developed a prototype 
system that can recognize Chinese sign input from a video 
camera which is a common gadget for a tourist, and 
translate the signs into English text or voice stream.  
The organization of this paper is as follows: Section 2 
describes challenges in sign recognition and translation. 
Section 3 discusses methods for sign detection. Section 4 
addresses the application of EBMT technology into sign 
translation. Section 5 introduces a prototype system for 
Chinese sign translation. Section 6 gives experimental 
results. Section 7concludes the paper. 
2. PROBLEM DESCRIPTION  
A sign can be a displayed structure bearing letters or 
symbols, used to identify or advertise a place of business. It 
can also be a posted notice bearing a designation, direction, 
or command. Figure 1 and Figure 2 illustrate two examples 
of signs. Figure 1 shows a Russian sign completely 
embedded in the background. Figure 2 is a sign that 
contains German text with no verb and article. In this 
research, we are interested in translating signs that have 
direct influence upon a tourist from a different country or 
culture. These signs, at least, include the following 
categories: 
? Names: street, building, company, etc. 
? Information: designation, direction, safety 
advisory, warning, notice, etc. 
? Commercial: announcement, advertisement, etc. 
? Traffic: warning, limitation, etc. 
? Conventional symbol: especially those are 
confusable to a foreign tourist, e.g., some symbols 
are not international. 
Fully automatic extraction of signs from the environment is 
a challenging problem because signs are usually embedded 
in the environment. The related work includes video OCR 
and automatic text detection. Video OCR is used to capture 
text in the video images and recognize the text.  Many 
video images contain text contents. Such text can come 
from computer-generated text that is overlaid on the 
imagery (e.g., captions in broadcast news programs) or text 
that appears as a part of the video scene itself (e.g., a sign 
outside a place of business, or a post). Location and 
recognition of text in video imagery is challenging due to 
low resolution of characters and complexity of background. 
Research in video OCR has mainly focused on locating the 
text in the image and preprocessing the text area for OCR 
[4][6][7][9][10]. Applications of the research include 
automatically identifying the contents of video imagery for 
video index [7][9], and capturing documents from paper 
source during reading and writing [10]. Compared to other 
video OCR tasks, sign extraction takes place in a more 
dynamic environment. The user?s movement can cause 
unstable input images. Non-professional equipment can 
make the video input poorer than that of other video OCR 
tasks, such as detecting captions in broadcast news 
programs. In addition, sign extraction has to be 
implemented in real time using limited resources. 
 
Figure 1 A sign embedded in the background 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 A German sign 
Sign translation requires sign recognition. A straightforward 
idea is to use advanced OCR technology. Although OCR 
technology works well in many applications, it requires 
some improvements before it can be applied to sign 
recognition. At current stage of the research, we will focus 
our research on sign detection and translation while taking 
advantage of state-of-the-art OCR technologies. 
Sign translation has some special problems compared to a 
traditional language translation task. The function of signs 
lead to the characteristic of the text used in the sign: it has 
to be short and concise. The lexical mismatch and structural 
mismatch problems become more severe in sign translation 
because shorter words/phrases are more likely to be 
ambiguous and insufficient information from the text to 
resolve the ambiguities which are related to the 
environment of the sign.  
We assume that a tourist has a video camera to capture 
signs into a wearable or portable computer. The procedure 
of sign translation is as follows: capturing the image with 
signs, detecting signs in the image, recognizing signs, and 
translating results of sign recognition into target language.  
3. AUTOMATIC SIGN DETECTION  
Fully automatic extraction of signs from the environment is 
very difficult, because signs are usually embedded in the 
environment. There are many challenges in sign detection, 
such as variation, motion and occlusion. We have no 
control in font, size, orientation, and position of sign texts. 
Originating in 3-D space, text on signs in scene images can 
be distorted by slant, tilt, and shape of objects on which 
they are found [8]. In addition to the horizontal left-to-right 
orientation, other orientations include vertical, circularly 
wrapped around another object, slanted, sometimes with the 
characters tapering (as in a sign angled away from the 
camera), and even mixed orientations within the same text 
area (as would be found on text on a T-shirt or wrinkled 
sign). Unlike other text detection and video OCR tasks, sign 
extraction is in a more dynamic environment. The user?s 
movement can cause unstable input images. Furthermore, 
the quality of the video input is poorer than that of other 
video OCR tasks, such as detecting captions in broadcast 
news programs, because of low quality of equipment. 
Moreover, sign detection has to be real-time using a limited 
resource. Though automatic sign detection is a difficult 
task, it is crucial for a sign translation system.  
We use a hierarchical approach to address these challenges. 
We detect signs at three different levels. At the first level, 
the system performs coarse detection by extracting features 
from edges, textures, colors/intensities. The system 
emphasizes robust detection at this level and tries to 
effectively deal with the different conditions such as 
lighting, noise, and low resolution. A multi-resolution 
detection algorithm is used to compensate different lighting 
and low contrasts. The algorithm provides hypotheses of 
sign regions for a variety of scenes with large variations in 
both lighting condition and contrast. At the second level, 
the system refines the initial detection by employing various 
adaptive algorithms. The system focuses on each detected 
area and makes elaborate analysis to guarantee reliable and 
complete detection. In most cases, the adaptive algorithms 
can lead to finding the regions without missing any sign 
region. At the third level, the system performs layout 
analysis based on the outcome from the previous levels. 
The design and layout of signs are language and culture 
dependent. For example, many Asia languages, such as 
Chinese and Japanese, have two types of layout: the 
horizontal and the vertical. The system provides 
considerable flexibility to allow the detection of slanted 
signs and signs with non-uniform character sizes.  
4. SIGN TRANSLATION  
Sign translation has some special problems compared to a 
traditional language translation task. Sign translation 
depends not only on domain but also on functionality of the 
sign. The same text on different signs can be treated 
differently. In general, the text used in the sign is short and 
concise. For example, the average length of each sign in our 
Chinese sign database is 6.02 Chinese characters. The 
lexical mismatch and structural mismatch problems become 
more severe for sign translation because shorter 
words/phrases are more likely to be ambiguous and there 
isn?t sufficient information from the text to resolve the 
ambiguities which are related to the environment of the 
sign. For example, in order to make signs short, 
abbreviations are widely used in signs, e.g.,  (/ji 
yan suo/) is the abbreviation for ,(/ji 
sheng chong yan jiu suo/ institute of parasites), such 
abbreviations are difficult, if not impossible, even for a 
human to understand without knowledge of the context of 
the sign. Since designers of signs always assume that 
readers can use the information from other sources to 
understand the meaning of the sign, they tend to use short 
words. e.g.  in sign (/man xing/, drive slowly), the 
word (/xing/, walk, drive) is ambiguous, it can mean 
  (/xing zou/ ?move of human,? walk) or  
?move of a car,? drive). The human reader can understand 
the meaning if he knows it is a traffic sign for cars, but 
without this information, MT system cannot select the 
correct translation for this word. Another problem in sign is 
structural mismatch. Although this is one of the basic 
problems for all MT systems, it is more serious in sign 
translation: some grammatical functions are omitted to 
make signs concise. Examples include: (1) the subject ?we? 
is omitted in (/li mao dai ke/, treat customers 
politely); (2) the sentence is reordered to emphasize the 
topic: rather than saying 
(/qing jiang bao zhuang zhi 
tou ru la ji xiang/, please throw wrapping paper into the 
garbage can), using (/bao 
zhuang zhi qing tou ru la ji xiang/, wrapping paper, please 
throw it into the garbage can) to highlight the ?wrapping 
paper.? With these special features, sign translation is not a 
trivial problem of just using existing MT technologies to 
translate the text recognized by OCR module.  
 
Although a knowledge-based MT system works well with 
grammatical sentences, it requires a great amount of human 
effort to construct its knowledge base, and it is difficult for 
such a system to handle ungrammatical text that appears 
frequently in signs.  
We can use a database search method to deal with names, 
phrases, and symbols related to tourists. Names are usually 
location dependent, but they can be easily obtained from 
many information sources such as maps and phone books. 
Phrases and symbols related to tourists are relative fixed for 
a certain country. The database of phrases and symbols is 
relatively stable once it is built 
We propose to apply Generalized Example Based Machine 
Translation (GEBMT) [1][2] enhanced with domain 
detection to a sign translation task. This is a data-driven 
approach. What EBMT needs are a set of bilingual corpora 
each for one domain and a bilingual dictionary where the 
latter can be constructed statistically from the corpora. 
Matched from the corpus, EBMT can give the same style of 
translations as the corpus. The domain detection can be 
achieved from other sources. For example, shape/color of 
the sign and semantics of the text can be used to choose the 
domain of the sign. 
We will start with the EBMT software [1]. The system will 
be used as a shallow system that can function using nothing 
more than sentence-aligned plain text and a bilingual 
dictionary; and given sufficient parallel text, the dictionary 
can be extracted statistically from the corpus.  In a 
translation process, the system looks up all matching 
phrases in the source-language half of the parallel corpus 
and performs a word-level alignment on the entries 
containing matches to determine a (usually partial) 
translation. Portions of the input for which there are no 
matches in the corpus do not generate a translation. 
Because the EBMT system does not generate translations 
for 100% of its input text, a bilingual dictionary and phrasal 
glossary are used to fill any gaps.  Selection of the ?best? 
translation is guided by a trigram model of the target 
language and a chart table [3]. 
5. A PROTOTYPE SYSTEM  
We have developed a prototype system for Chinese sign 
recognition and translation. Figure 3 shows the architecture 
of the prototype system. A user can interactively involve 
sign recognition and translation process when needed. For 
example, a user can select the area of interest, or indicate 
that the sign is a street name. The system works as follows. 
The system captures the sign in a natural background using 
a video camera. The system then automatically detects or 
interactively selects the sign region. The system performs 
sign recognition and translation within the detected/selected 
region. It first preprocesses the selected region, binarizes 
the image to get text or symbol, and feeds the binary image 
into the sign recognizer. OCR software from a third party is 
used for text recognition. The recognized text is then 
translated into English. The output of the translation is fed 
to the user by display on screen or synthesized speech. 
Festival, a general purpose multi-lingual text-to-speech 
(TTS) system is used for speech synthesis.  
 
 
 
Figure 3 Architecture of the prototype system 
 
 
 
Figure 4  The interface of the prototype system 
An efficient user interface is important to a user-centered 
system. Use of interaction is not only necessary for an 
interactive system, but also useful for an automatic system. 
A user can select a sign from multiple detected signs for 
translation, and get involved when automatic sign detection 
is wrong. Figure 4 is the interface of the system. The 
window of the interface displays the image from a video 
camera. The translation result is overlaid on the location of 
the sign. A user can select the sign text using pen or mouse 
anywhere in the window. 
6. EXPERIMENTAL RESULTS  
We have evaluated the prototype system for automatic sign 
detection and translation. We have built a Chinese sign 
database with about 800 images taken from China and 
Singapore. We have tested the automatic detection module 
using 50 images randomly selected from the database. 
Table 1 shows the test result of automatic sign detection. 
Figure 5 and Figure 6 show examples of automatic sign 
detection with white rectangles indicating the sign regions. 
Figure 5 shows correct detection after layout analysis. 
Figure 6 illustrates a result with a false detection (Note the 
small detection box below and to the left of the larger 
detection).  
 
Table 1 Test Results of Automatic Detection on 50 
Chinese Signs 
Detection 
without missing 
characters 
Detection 
with false 
alarm 
Detection with 
missing characters 
43 12 5 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5 An example of automatic sign detection 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6 An example of false detection 
 
Figure 7 illustrates two difficult examples of sign detection. 
The text in Figure 7(a) is easily confused with the reflective 
background. The sign in Figure 7(b) is embedded in the 
background  
 
 
 
 
 
 
 
 
 
                     (a)                                                (b) 
Figure 7 Difficult examples of sign detection 
 
We have also tested the EBMT based method. We assume 
perfect sign recognition in our test. We randomly selected 
50 signs from our database. We first tested the system 
includes a Chinese-English dictionary from the Linguistic 
Data Consortium, and a statistical dictionary built from the 
HKLC (Hong Kong Legal Code) corpus. As a result, we 
only got about 30% reasonable translations. We then 
trained with a small corpus of 670 pairs of bilingual 
sentences [7], The accuracy is improved from 30% to 52% 
on 50 test signs. Some examples of errors are illustrated 
below: 
Mis-segmentaion: 
Chinese with wrong segmentation: 
  
/ge zhong che liang qing rao xing/ 
Translation from MT: 
 All vehicles are please wind profession  
Correct segmentation: 
   
Translation if segmentation is correct: 
 All vehicles please use detour 
Lack-domain information: 
Chinese with segmentation: 
  
 /qing wu dong shou/ 
 Please don?t touch it 
Translation from MT: 
 Please do not get to work 
Domain knowledge needed to translate : 
?start to work? in domain such as work plan and 
?don?t touch? in domains like tourism, exhibition 
etc. 
Proper Name: 
Chinese with segmentation: 
  
 /bei jing tong ren yi yuan/ 
 Beijing Tongren Hospital 
Translation from MT: 
 Beijing similar humane hospital 
is translated to the meaning of each 
character because it is not identified as a proper 
name which then should only be represented by its 
pronunciation. 
Figure 8 illustrates error analysis of the translation module. 
It is interesting to note that 40% of errors come from mis-
segmentation of words. There is a big room for 
improvement in proper word segmentation.  In addition, we 
can take advantage of the contextual information provided 
by the OCR module to further improve the translation 
quality.  
Error source
41.11%
31.11%
10.00%
14.44%
3.33%
Mis-segmentation
Lack domain knowledge
Mis-detection of proper
name
Corpus/dict not large enough
Other
 
Figure 8 Error analysis of the translation module 
7. CONCLUSION 
We have reported progress on automatic sign translation in 
this paper. Sign translation, in conjunction with spoken 
language translation, can help international tourists to 
overcome language barriers. A successful sign translation 
system relies on three key technologies: sign extraction, 
OCR, and language translation. We have developed 
algorithms for robust sign detection. We have applied 
EBMT technology for sign translation. We have employed 
a user-centered approach in developing an automatic sign 
translation system. The approach takes advantage of human 
intelligence in selecting an area of interest and domain for 
translation if needed. We have developed a prototype 
system that can recognize Chinese signs input from a video 
camera which is a common gadget for a tourist, and 
translate them into English text or voice stream. 
ACKNOWLEDGMENTS 
We would like to thank Dr. Ralf Brown and Dr. Robert 
Frederking for providing initial EBMT software and 
William Kunz for developing the interface for the prototype 
system. We would also like to thank other members in the 
Interactive Systems Labs for their inspiring discussions and 
support. This research is partially supported by DARPA 
under TIDES project. 
 
REFERENCES 
[1] R.D. Brown. Example-based machine translation in the 
pangloss system. Proceedings of the 16th International 
Conference on Computational Linguistics, pp. 169-
174, 1996. 
[2] R.D. Brown. Automated generalization of translation 
examples". In Proceedings of the Eighteenth 
International Conference on Computational Linguistics 
(COLING-2000), p. 125-131. Saarbr?cken, Germany, 
August 2000. 
[3] C. Hogan and R.E. Frederking. An evaluation of the 
multi-engine MT architecture. Machine Translation 
and the Information Soup: Proceedings of the Third 
Conference of the Association for Machine Translation 
in the Americas (AMTA ?98), vol. 1529 of Lecture 
Notes in Artificial Intelligence, pp. 113-123. Springer-
Verlag, Berlin, October. 
[4] A.K. Jain and B. Yu. Automatic text location in images 
and video frames. Pattern Recognition, vol. 31, no. 12, 
pp. 2055--2076, 1998. 
[5] C. C. Kubler. "Read Chinese Signs". Published by 
Chheng & Tsui Company, 1993. 
[6] H. Li and D. Doermann, Automatic Identification of 
Text in Digital Video Key Frames, Proceedings of 
IEEE International Conference of Pattern Recognition, 
pp. 129-132, 1998. 
[7] R. Lienhart, Automatic Text Recognition for Video 
Indexing, Proceedings of ACM Multimedia 96, pp. 11-
20, 1996. 
[8] J. Ohya, A. Shio, and S. Akamatsu. Recognition 
characters in scene images. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 16, no. 
2, pp. 214--220, 1994. 
[9] T. Sato, T. Kanade, E.K. Hughes, and M.A. Smith. 
Video ocr for digital news archives. IEEE Int. 
Workshop on Content-Based Access of Image and 
Video Database, 1998. 
[10] M.J. Taylor, A. Zappala, W.M. Newman, and C.R. 
Dance, Documents through cameras, Image and Vision 
Computing, vol. 17, no. 11, pp. 831-844, 1999. 
[11] V. Wu, R. Manmatha, and E.M. Riseman, Textfinder: 
an automatic system to detect and recognize text in 
images. IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 21, no. 11, pp. 1224-1229, 
1999. 
[12] J. Yang, W. Yang, M. Denecke, and A. Waibel. Smart 
sight: a tourist assistant system.  Proceedings of Third 
International Symposium on Wearable Computers, pp. 
73--78. 1999. 
 
79
80
81
82
83
84
85
86
,PSURYLQJ1DPHG(QWLW\7UDQVODWLRQ&RPELQLQJ3KRQHWLF
DQG6HPDQWLF6LPLODULWLHV
)HL+XDQJ6WHSKDQ9RJHODQG$OH[:DLEHO/DQJXDJH7HFKQRORJLHV,QVWLWXWH6FKRRORI&RPSXWHU6FLHQFHV&DUQHJLH0HOORQ8QLYHUVLW\^IKXDQJYRJHODKZ`#FVFPXHGX
$EVWUDFW
7KLVSDSHUGHVFULEHV DQ DSSURDFK WR WUDQVODWHUDUHO\RFFXUULQJQDPHGHQWLWLHV1(E\FRPELQLQJSKRQHWLFDQGVHPDQWLFVLPLODULWLHV7KHSKRQHWLFVLPLODULW\LVHVWLPDWHGIURPDVXUIDFHVWULQJ WUDQVOLWHUDWLRQPRGHO DQG WKH VHPDQWLFVLPLODULW\ LV FDOFXODWHG IURP D FRQWH[W YHFWRUVHPDQWLFPRGHO*LYHQDVRXUFH&KLQHVH1(DQG LWV FRQWH[W WKLV DSSURDFK ILUVW JHQHUDWHVTXHULHV LQ WKH WDUJHW (QJOLVK ODQJXDJH DFFRUGLQJ WR WKHFRQWH[W WUDQVODWLRQK\SRWKHVHVWKHQ VHDUFKHV IRU UHOHYDQW GRFXPHQWV IURP DWDUJHW ODQJXDJH FRUSXV 7DUJHW 1(V LQ UHWULHYHG GRFXPHQWV DUH FRPSDUHG ZLWK WKHVRXUFH1(EDVHGRQWKHLUSKRQHWLFDQGFRQWH[WXDO VHPDQWLF VLPLODULWLHV DQG WKH EHVWPDWFKHGRQHLVVHOHFWHGDVWKHFRUUHFWWUDQVODWLRQ ([SHULPHQWV VKRZ WKDW WKLV DSSURDFKDFKLHYHV A THAI SPEECH TRANSLATION SYSTEM  FOR MEDICAL DIALOGS
Tanja Schultz, Dorcas Alexander, Alan W Black, Kay Peterson, Sinaporn Suebvisai, Alex Waibel
Language Technologies Institute, Carnegie Mellon University
E-mail: tanja@cs.cmu.edu
1. Introduction
In this paper we present our activities towards a Thai
Speech-to-Speech translation system. We investigated in
the design and implementation of a prototype system. For
this purpose we carried out research on bootstrapping a
Thai speech recognition system, developing a translation
component, and building an initial Thai synthesis system
using our existing tools.
2. Speech Recognition
The language adaptation techniques developed in our lab
[5] enables us to rapidly bootstrap a speech recognition
system in a new target language given very limited amount
of training data. The Thailand?s National Electronics and
Technology Center gave us the permission to use their
Thai speech data collected in the hotel reservation domain.
They provided us with a 6 hours text and speech database
recorded from native Thai speakers. We divided the data
into three speaker disjoint sets, 34 speakers were used for
training, 4 speakers for development, and another 4
speakers for evaluation. The provided transcriptions were
manually pre-segmented and given in Thai script. We
transformed the Thai script into a Roman script
representation by concatenating the phoneme
representation of the Thai word given in the pronunciation
dictionary. The motivation for this romanization step was
threefold: (1) it makes it easier for non-Thai researchers to
work with the Roman representation like in the grammar
development, (2) the romanized output basically provides
the pronunciation which makes things easier for the speech
synthesis component, and (3) our speech engine currently
does not handle Thai characters.
In our first Thai speech engine we decided to disregard the
tone information. Since tone is a distinctive feature in the
Thai language, disregarding the tone increases the number
of homographs. In order to limit this number, we
distinguished those word candidates by adding a tag that
represents the tone. The resulting dictionary consists of
734 words which cover the given 6-hours database.
Building on our earlier studies which showed that
multilingual seed models outperform monolingual ones
[5], we applied phonemes taken from seven languages,
namely Chinese, Croatian, French, German, Japanese,
Spanish, and Turkish as seed models for the Thai phone
set. Table 1 describes the performance of the Thai speech
recognition component for different acoustic model sizes
(context-independent vs. 500 and 1000 tri-phone models).
The results indicate that a Thai speech recognition engine
can be built by using the bootstrapping approach with a
reasonable amount of speech data. Even the very initial
system bootstrapped from multilingual seed models gives
a performance above 80% word accuracy. The good
performance might be an artifact from the very limited
domain with a compact and closed vocabulary.
System Dev Test Eval Test
Context-Independent 85.62% 83.63%
Context-Dependent (500) 86.99% 84.44%
Context-Dependent (1000) 84.63% 82.71%
Table1: Word accuracy [%] in Thai language
3. Machine Translation
The Machine Translation (MT) component of our current
Thai system is based on an interlingua called the
Interchange Format (IF). The IF developed by CMU has
been expanded and now encompasses concepts in both the
travel and medical domains, as well as many general-use
or cross-domain concepts in many different languages [4].
Interlingua-based MT has several advantages, namely: (1)
it abstracts away from variations in syntax across
languages, providing potentially deep analysis of meaning
without relying on information pertinent only to one
particular language pair, (2) modules for analysis and
generation can be developed monolingually, with
additional reference only to the second "language" of the
interlingua, (3) the speaker can be given a paraphrase in
his or her own language, which can help verify the
accuracy of the analysis and be used to alert the listener to
inaccurate translations, and (4) translation systems can be
extended to new languages simply by hooking up new
monolingual modules for analysis and/or generation,
eliminating the need to develop a completely new system
for each new language pair.
Thai has some particular characteristics which we
addressed in IF and appear in the grammars as follows:
1) The use of a term to indicate the gender of the person:
Thai: zookhee kha1
Eng: okay (ending)
s[acknowledge] (zookhee *[speaker=])
2) An affirmation that means more than simply "yes."
Thai: saap khrap
Eng: know (ending)
s[affirm+knowledge](saap *[speaker=])
3) The separation from the main verb of terms for
feasibility and other modalities.
Thai: rvv khun ca paj dooj thxksii
kyydaaj
Eng: or you will go by taxi [can too]
s[give-information+feasibility+trip]
(*DISC-RHET [who=] ca paj
[locomotion=] [feasibility=])
4. Language Generation
For natural language generation from interlingua for Thai
and English, we are currently investigating two options: a
knowledge-based generation with the pseudo-unification
based GenKit generator developed at CMU, which
employs manually written semantic/syntactic grammars
and lexicons, and a statistical generation operating on a
training corpus of aligned interlingua and natural language
correspondences. Performance tests as well as the amount
and quality of training data will decide which approach
will be pursued in the future.
5. Speech Synthesis
First, we built a limited domain Thai voice in the Festival
Speech Synthesis System [1]. Limited Domain voices can
achieve very high quality voice output [2], and can be easy
to construct if the domain is constrained. Our initial voice
targeted the Hotel Reservation domain and we constructed
235 sentence that covered the aspects of our immediate
interest. Using the tools provided in FestVox [1], we
recorded, auto-labeled, and built a synthetic voice.
In supporting any new language in synthesis, a number of
language specific issues first had to be addressed. As with
our other speech-to-speech translation projects we share
the phoneme set between the recognizer and the
synthesizer. The second important component is the
lexicon. The pronunciation of Thai words from Thai script
is not straightforward, but there is a stronger relationship
between the orthography and pronunciation than in
English. For this small set of initial words we constructed
an explicit lexicon by hand with the output vocabulary of
522 words. The complete Thai limited domain voice uses
unit selection concatenative synthesis. Unlike our other
limited domain synthesizers, where they have a limited
vocabulary, we tag each phone with syllable and tone
information in selection making the result more fluent, and
a little more general.
Building on our previous Thai work in pronunciation of
Thai words [3], we have used the lexicon and statistically
trained letter to sound rules to bootstrap the required word
coverage. With a pronunciation model we can select
suitable phonetically balanced text (both general and in-
domain) from which we are able to record and build a
more general voice.
6. Demonstration Prototype System
Our current version is a two-way speech-to-speech
translation system between Thai and English for dialogs in
the medical domain where the English speaker is a doctor
and the Thai speaker is a patient. The translated speech
input will be spoken using the built voice. At the moment,
the coverage is very limited due to the simplicity of the
used grammars. The figure shows the interface of our
prototype system.
Acknowledgements
This work was partly funded by LASER-ACTD. The
authors thank Thailand?s National Electronics and
Computer Technology Center for giving the permission to
use their database and dictionary for this task.
References
[1] Black, A. and Lenzo, K. (2000) "Building Voices in the
Festival Speech Synthesis System", http://festvox.org
[2] Black, A. and Lenzo, K. (2000) "Limited Domain Synthesis",
ICSLP2000, Beijing, China.
[3] Chotmongkol, A. and Black, A. (2000) "Statistically trained
orthographic to sound models for Thai", ICSLP2000,
Beijing, China.
[4] Lavie A. and Levin L. and Schultz T. and Langley C. and
Han B., Tribble, A., Gates D., Wallace D. and Peterson K.
(2001) ?Domain Portability in Speech-to-speech
Translation?,  HLT, San Diego, March 2001.
[5] Schultz, T. and Waibel, A. (2001) ?Language Independent
and Language Adaptive Acoustic Modeling for Speech
Recognition?, Speech Communication, Volume 35, Issue 1-
2, pp. 31-51, August 2001.
Proceedings of NAACL HLT 2007, Companion Volume, pages 21?24,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Translation Model Pruning via Usage Statistics  
for Statistical Machine Translation 
 
 
Matthias Eck, Stephan Vogel, and Alex Waibel 
InterACT Research 
Carnegie Mellon University, Pittsburgh, USA 
matteck@cs.cmu.edu, vogel+@cs.cmu.edu, ahw@cs.cmu.edu 
 
 
 
Abstract 
We describe a new pruning approach to 
remove phrase pairs from translation mod-
els of statistical machine translation sys-
tems. The approach applies the original 
translation system to a large amount of text 
and calculates usage statistics for the 
phrase pairs. Using these statistics the rele-
vance of each phrase pair can be estimated. 
The approach is tested against a strong 
baseline based on previous work and shows 
significant improvements.  
1 Introduction 
A relatively new device for translation systems are 
small portable devices like cell phones, PDAs and 
handheld game consoles. The idea here is to have a 
lightweight and convenient translation device e.g. 
for tourists that can be easily carried. Other appli-
cations include medical, relief, and military scenar-
ios.  
Preferably such a device will offer speech-to-
speech translation for both (or multiple) translation 
directions. These devices have been researched and 
are starting to become commercially available (e.g. 
Isotani et al, 2003). The main challenges here are 
the severe restrictions regarding both memory and 
computing power on such a small portable device. 
1.1 Statistical Machine Translation  
Generally statistical machine translation systems 
have recently outperformed other translation ap-
proaches so it seems natural to also apply them in 
these scenarios.  
A main component of every statistical machine 
translation system is the translation model. The 
translation model assigns translation probabilities 
to phrase1 pairs of source and target phrases ex-
tracted from a parallel bilingual text. These phrase 
pairs are applied during the decoding process and 
their target sides are combined to form the final 
translation. A variety of algorithms to extract 
phrase pairs has been proposed. (e.g. Och and Ney, 
2000 and Vogel, 2005). 
Our proposed approach now tries to remove 
phrase pairs, which have little influence on the fi-
nal translation performance, from a translation sys-
tem (pruning of the translation model2). The goal 
is to reduce the number of phrase pairs and in turn 
the memory requirement of the whole translation 
system, while not impacting the translation per-
formance too heavily.  
The approach does not depend on the actual al-
gorithm used to extract the phrase pairs and can be 
applied to every imaginable method that assigns 
probabilities to phrase pairs. We assume that the 
phrase pairs were pre-extracted before decoding. 
(in contrast to the proposed approaches to ?online 
phrase extraction? (Zhang and Vogel, 2005; Calli-
son-Burch et al, 2005)). 
The task now is to remove enough pre-extracted 
phrase pairs in order to accommodate the possibly 
strict memory limitations of a portable device 
while restricting performance degradation as much 
as possible.  
We will not specifically address the computing 
power limitations of the portable devices in this 
paper.  
                                                          
1
 A ?phrase? here can also refer to a single word. 
2
 Small language models are also desirable and the approaches 
could be applied as well but this was not investigated yet. 
21
2 Previous work 
Previous work mainly introduced two natural ideas 
to prune phrase pairs. Both are for example di-
rectly available in the Pharaoh decoder (Koehn, 
2004). 
Probability threshold 
A very simple way to prune phrase pairs from a 
translation model is to use a probability threshold 
and remove all pairs for which the translation 
probability is below the threshold. The reasoning 
for this is that it is very unlikely that a translation 
with a very low probability will be chosen (over 
another translation candidate with a higher prob-
ability).  
Translation variety threshold 
Another way to prune phrase pairs is to impose a 
limit on the number of translation candidates for a 
certain phrase. That means the pruned translation 
model can only have equal or fewer possible trans-
lations for a given source phrase than the thresh-
old. This is accomplished by sorting the phrase 
pairs for each source phrase according to their 
probability and eliminating low probability ones 
until the threshold is reached. 
3 Pruning via Usage Statistics  
The approach presented here uses a different idea 
inspired by the Optimal Brain Damage algorithm 
for neural networks (Le Cun et al, 1990). 
The Optimal Brain Damage algorithm for neural 
networks computes a saliency for each network 
element. The saliency is the relevance for the per-
formance of the network. In each pruning step the 
element with the smallest saliency is removed, and 
the network is re-trained and all saliencies are re-
calculated etc. 
We can analogously view each phrase pair in the 
translation system as such a network element. The 
question is of course how to calculate the relevance 
for the performance for each phrase pair.  
A simple approximation was already done in the 
previous work using a probability or variety 
threshold. Here the relevance is estimated using the 
phrase pair probability or the phrase pair rank as 
relevance indicators.  
But these are not the only factors that influence 
the final selection of a phrase pair and most of 
these factors are not established during the training 
and phrase extraction process. Especially the fol-
lowing two additional factors play a major role in 
the importance of a phrase pair. 
Frequency of the source phrase  
We can clearly say that a phrase pair with a very 
common source phrase will be much more impor-
tant than a phrase pair where the source phrase oc-
curs only very rarely. 
Actual use of the phrase-pair 
But even phrase-pairs with very common source 
phrases might not be used for the final translation 
hypothesis. It is for example possible that it is part 
of a longer phrase pair that gets a higher probabil-
ity so that the shorter phrase pair is not used.  
 
Generally there are a lot of different factors influ-
encing the estimated importance of a phrase pair 
and it seems hard to consider every influence sepa-
rately. Hence the proposed idea does not use a 
combination of features to estimate the phrase pair 
importance. Instead the idea is to just apply the 
translation system to a large amount of text and see 
how often a phrase pair is actually used (i.e. influ-
ences the translation performance). If the translated 
text is large enough this will give a good statistics 
of the relevance of this respective phrase pair. This 
leads to the following algorithm: 
Algorithm 
Translate a large amount of (in-domain) data with 
the translation system (tuned on a development set) 
and collect the following two statistics for each 
phrase pair in the translation model. 
? c(phrase pair) = Count how often a phrase pair 
was considered during decoding (i.e. was 
added to the translation lattice) 
? u(phrase pair) = Count how often a phrase pair 
was used in the final translation (i.e. in the 
chosen path through the lattice). 
The overall score for a phrase pair with simple 
smoothing (+1) is calculated as:  
 
[ ] [ ]1)(*)1)(log(
)(
pair phrasepair phrase
pair phrase
++
=
uc
score
 
 
We use the logarithm function to limit the influ-
ence of the c value. The u value is more important 
as this measures how often a phrase was actually 
used in a translation hypothesis. This scoring func-
22
tion was empirically found after experimenting 
with a variety of possible scoring terms. 
The phrase pairs can then be sorted according to 
this score and the top n phrase pairs can be selected 
for a smaller phrase translation model. 
4 Data and Experiments 
4.1 Experimental Setup & Baseline 
Translation system 
The translation system that was used for the ex-
periments is a state-of-the-art statistical machine 
translation system (Eck et al 2006). The system 
uses a phrase extraction method described in Vogel 
(2005) and a 6-gram language model.  
Training and testing data 
The training data for all experiments consisted of 
the BTEC corpus (Takezawa et al, 2002) with 
162,318 lines of parallel Japanese-English text. All 
translations were done from Japanese to English. 
The language model was trained on the English 
part of the training data.   
The test set from the evaluation campaign of 
IWSLT 2004 (Akiba et al, 2004) was used as test-
ing data. This data consists of 500 lines of tourism 
data. 16 reference translations to English were 
available.  
Extracted phrases 
Phrase pairs for n-grams up to length 10 were ex-
tracted (with low frequency thresholds for higher 
n-grams). This gave 4,684,044 phrase pairs 
(273,459 distinct source phrases). The baseline 
score using all phrase pairs was 59.11 (BLEU, 
Papineni et al, 2002) with a 95% confidence inter-
val of [57.13, 61.09].  
Baseline pruning 
The approaches presented in previous work served 
as a baseline. The probability threshold was tested 
for 8 values (0 (no pruning), 0.0001, 0.0005, 0.001, 
0.005, 0.01, 0.05, 0.1) while the variety threshold 
tested for 14 values (1, 2, 3, 4, 5, 6, 8, 10, 15, 20, 
50, 100, 200, 500 (no pruning in this case)) and all 
combinations thereof. The final translation scores 
for different settings are very fluctuating. For that 
reason we defined the baseline score for each pos-
sible size as the best score that was reached with 
equal or less phrase pairs than the given size in any 
of the tested combinations.  
4.2 Results for  
Pruning via Usage Statistics 
For the proposed approach ?Pruning via Usage 
Statistics?, the translation system was applied to 
the 162,318 lines of Japanese training data. 
As explained in section 3 it was now counted for 
each phrase pair how often it occurred in a transla-
tion lattice and how often it was used for the final 
translation. The phrase pairs were then sorted ac-
cording to their relevance estimation and the top n 
phrase pairs were chosen for different values of n. 
The pruned phrase table was then used to translate 
the IWSLT 2004 test set. Table 1 shows the results 
comparing the baseline scores with the results us-
ing the described pruning. Figure 1 illustrates the 
scores. The plateaus in the baseline graph are due 
to the baseline definition as stated above. 
 
 BLEU scores  
# of Phrase  
Pairs (n) 
Baseline 
 
Pruning 
 
Relative score  
improvement 
100,000 - 0.4735 - 
200,000 0.3162 0.5008 58.38% 
300,000 0.4235 0.5154 21.70% 
400,000 0.4743 0.5241 10.50% 
500,000 0.4743 0.5269 11.09% 
600,000 0.4890 0.5359 9.59% 
800,000 0.5194 0.5394 3.85% 
1,000,000 0.5355 0.5442 1.62% 
1,500,000 0.5413 0.5523 2.03% 
2,000,000 0.5630 0.5749 2.11% 
3,000,000 0.5778 0.5798 0.35% 
4,000,000 0.5855 0.5865 0.17% 
4,684,044 0.5911 0.5911 0.00% 
Table 1: BLEU scores at different levels of pruning 
(Baseline: Best score with equal or less phrase 
pairs) 
 
For more than 1 million phrase pairs the differ-
ences are not very pronounced. However the trans-
lation score for the proposed pruning algorithm is 
still not significantly lower than the 59.11 score at 
2 million phrase pairs while the baseline drops 
slightly faster. For less than 1 million phrase pairs 
the differences become much more pronounced 
with relative improvements of up to 58% at 
200,000 phrase pairs. It is interesting to note that 
the improved pruning removes infrequent source 
23
phrases and to a lesser extent source vocabulary 
even for larger numbers of phrase pairs. 
 
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0 1,000,000 2,000,000 3,000,000 4,000,000
phrase pairs
B
LE
U 
sc
o
re
Baseline Pruning
 
Figure 1: Pruning and baseline comparison 
5 Conclusions and Future Work 
The proposed pruning algorithm is able to outper-
form a strong baseline based on previously intro-
duced threshold pruning ideas. Over 50% of phrase 
pairs can be pruned without a significant loss of 
performance. Even for very low memory situations 
the improved pruning remains a viable option 
while the baseline pruning performance drops 
heavily.  
One idea to improve this new pruning approach 
is to exchange the used count with the count of the 
phrase occurring in the best path of the lattice ac-
cording to a scoring metric. This would require 
having a reference translation available to be able 
to tell which path is the actual best one (metric-
best path). It would be interesting to compare the 
performance if the statistics is done using the met-
ric-best path on a smaller amount of data to the 
performance if the statistics is done using the 
model-best path on a larger amount (as there is no 
reference translation necessary). 
The Optimal Brain Damage algorithm recalcu-
lates the saliency after removing each network 
element. It could also be beneficial to sequentially 
prune the phrase pairs and always re-calculate the 
statistics after removing a certain number of phrase 
pairs. 
6 Acknowledgements 
This work was partly supported by the US DARPA 
under the programs GALE and TRANSTAC. 
7 References  
Yasuhiro Akiba, Marcello Federico, Noriko Kando, 
Hiromi Nakaiwa, Michael Paul, and Jun'ichi Tsujii}. 
2004. Overview of the IWSLT04 Evaluation Cam-
paign. Proceedings of IWSLT 2004, Kyoto, Japan. 
Chris Callison-Burch, Colin Bannard, and Josh Schroe-
der. 2005. Scaling Phrase-Based Statistical Machine 
Translation to Larger Corpora and Longer Phrases. 
Proceedings of ACL 2005, Ann Arbor, MI, USA. 
Yann Le Cun, John S. Denker, and Sara A. Solla. 1990. 
Optimal brain damage. In Advances in Neural In-
formation Processing Systems 2, pages 598-605. 
Morgan Kaufmann, 1990. 
Matthias Eck, Ian Lane, Nguyen Bach, Sanjika He-
wavitharana, Muntsin Kolss, Bing Zhao, Almut Silja 
Hildebrand, Stephan Vogel, and Alex Waibel. 2006. 
The UKA/CMU Statistical Machine Translation Sys-
tem for IWSLT 2006. Proceedings of IWSLT 2006, 
Kyoto, Japan.  
Ryosuke Isotani, Kyoshi Yamabana, Shinichi Ando, 
Ken Hanazawa, Shin-ya Ishikawa and Ken.ichi Iso. 
2003. Speech-to-speech translation software on 
PDAs for travel conversation. NEC research & de-
velopment, Tokyo, Japan. 
Philipp Koehn. 2004. A Beam Search Decoder for Sta-
tistical Machine Translation Models. Proceedings of 
AMTA 2004, Baltimore, MD, USA. 
Franz Josef Och and Hermann Ney, 2000. Improved 
statistical alignment models, Proceedings of ACL 
2000, Hongkong, China. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. Proceedings of 
ACL 2002, Philadelphia, PA, USA. 
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, 
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. 
Toward a Broad-coverage Bilingual Corpus for 
Speech Translation of Travel Conversation in the 
Real World. Proceedings of LREC 2002, Las Palmas, 
Spain. 
Stephan Vogel. 2005. PESA: Phrase Pair Extraction as 
Sentence Splitting. Proceedings of MTSummit X, 
Phuket, Thailand. 
Ying Zhang and Stephan Vogel. 2005. An Efficient 
Phrase-to-Phrase Alignment Model for Arbitrarily 
Long Phrases and Large Corpora. Proceedings of 
EAMT 2005, Budapest, Hungary. 
 
24
Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incremental Adaptation of Speech-to-Speech Translation
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
Abstract
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
1 Introduction
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al, 2006;
Bach et al, 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
W i i c o n t r o l l e r
M i c & L i g h t
L o u d  s p e a k e r
Figure 1: The users interact with the system
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
2 Data Scenario
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
149
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
3 ASR LM Adaptation
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al, 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
Table 1: Iraqi ASR?s WER on day 2 using different adaptation
schemes for day 1 data
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
Table 2: Impact of ASR adaptation to SMT
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al, 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
4 SMT Adaptation
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al, 2007).
Related work including (Eck et al, 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
150
language models. This approach is similar to the work
in (Chen et al, 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
Table 3: Performance in BLEU of unsupervised adaptation.
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
Table 4: Performance in BLEU of supervised adaptation.
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
5 Joint Adaptation
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
Table 5: Performance in BLEU of joint adaptation.
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
6 Selective Adaptation
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
151
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
No. sents Day 1 Day 2
Baseline 29.39 27.41
? 0 864 30.27 28.29
? 10 797 31.15 28.27
? 20 747 30.81 28.24
? 30 585 30.04 27.71
? 40 416 29.72 27.65
? 50 296 30.06 27.04
Correct 98 29.18 27.19
Table 6: Performance in BLEU of selective adaptation
W i c o n t r o l e M & L g
h r n u
d
r c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  
h r n u c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  Effective Phrase Translation Extraction from Alignment Models
Ashish Venugopal
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
ashishv@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
vogel+@cs.cmu.edu
Alex Waibel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
ahw@cs.cmu.edu
Abstract
Phrase level translation models are ef-
fective in improving translation qual-
ity by addressing the problem of local
re-ordering across language boundaries.
Methods that attempt to fundamentally
modify the traditional IBM translation
model to incorporate phrases typically do
so at a prohibitive computational cost. We
present a technique that begins with im-
proved IBM models to create phrase level
knowledge sources that effectively repre-
sent local as well as global phrasal con-
text. Our method is robust to noisy align-
ments at both the sentence and corpus
level, delivering high quality phrase level
translation pairs that contribute to signif-
icant improvements in translation quality
(as measured by the BLEU metric) over
word based lexica as well as a competing
alignment based method.
1 Introduction
Statistical Machine Translation defines the task
of translating a source language sentence
 
		

into a target language sentence
 


		
. The traditional framework presented in
(Brown et al, 1993) assumes a generative process
where the source sentence is passed through a noisy
stochastic process to produce the target sentence.
The task can be formally stated as finding the 

s.t 

= IMPROVEMENTS IN NON-VERBAL CUE IDENTIFICATION USING MULTILINGUAL
PHONE STRINGS
Tanja Schultz, Qin Jin, Kornel Laskowski, Alicia Tribble, Alex Waibel
Interactive Systems Laboratories
Carnegie Mellon University
E-mail:
 
tanja,qjin,kornel,atribble,ahw  @cs.cmu.edu
1. INTRODUCTION
Today?s state-of-the-art front-ends for multilingual speech-
to-speech translation systems apply monolingual speech
recognizers trained for a single language and/or accent.
The monolingual speech engine is usually adaptable to an
unknown speaker over time using unsupervised training
methods; however, if the speaker was seen during training,
their specialized acoustic model will be applied, since it
achieves better performance. In order to make full use of
specialized acoustic models in this proposed scenario, it is
necessary to automatically identify the speaker with high
accuracy. Furthermore, monolingual speech recognizers
currently rely on the fact that language and/or accent will
be selected beforehand by the user. This requires the user?s
cooperation and an interface which easily allows for such
selection. Both requirements are awkward and error-prone,
especially when translation services are provided for many
languages using small devices like PDAs or telephones. For
these scenarios, front-ends are desired which automatically
identify the spoken language or accent. We believe that
the automatic identification of an utterance?s non-verbal
cues, such as language, accent and speaker, are necessary to
the successful deployment of speech-to-speech translation
systems.
Currently, approaches based on Gaussian Mixture Models
(GMMs) [1] are the most widely and successfully used
methods for speaker identification. Although GMMs have
been applied successfully to close-speaking microphone
scenarios under matched training and testing conditions,
their performance degrades dramatically under mismatched
conditions. For language and accent identification, phone
recognition together with phone N-gram modeling has been
the most successful approach in the past [2]. More recently,
Kohler introduced an approach for speaker recognition
where a phonotactic N-gram model is used [3].
In [4], we extended Kohler?s approach to accent and lan-
guage identification as well as to speaker identification un-
der mismatched conditions. The term ?mismatched condi-
tion? describes a situation in which the testing conditions,
e.g. microphone distance, are quite different from what had
been seen during training. In that work, we explored a com-
mon framework for the identification of language, accent
and speaker using multilingual phone strings produced by
phone recognizers trained on data from different languages.
In this paper, we propose and evaluate some improvements,
comparing classification accuracy as well as realtime per-
formance in our framework. Furthermore, we investigate
the benefits that are to be drawn from additional phone rec-
ognizers.
2. THE MULTILINGUAL PHONE STRING
APPROACH
The basic idea of the multilingual phone string approach
is to use phone strings produced by different context-
independent phone recognizers instead of traditional
short-term acoustic vectors [6]. For the classification of an
audio segment into one of  classes of a specific non-verbal
cue,  such phone recognizers together with 
phonotactic N-gram models produce an  matrix of
features. A best class estimate is made based solely on this
feature matrix. The process relies on the availability of
 phone recognizers, and the training of  N-gram
models on their output.
By using information derived from phonotactics rather than
directly from acoustics, we expect to cover speaker idiosyn-
crasy and accent-specific pronunciations. Since this infor-
mation is provided from complementary phone recognizers,
we anticipate greater robustness under mismatched condi-
tions. Furthermore, the approach is somewhat language in-
dependent since the recognizers are trained on data from
different languages.
2.1. Phone Recognition
The experiments presented here were conducted using
two versions of phone recognizers borrowed without
modification from the GlobalPhone project [5]. All were
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 101-108.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
trained using our Janus Recognition Toolkit (JRTk).
25
30
35
40
45
50
25 30 35 40 45
Ph
on
em
e 
Er
ro
r R
at
e 
[%
]
Number of Phonemes
CHDE
FR
JA
KR
PO
SPTU
50
Fig. 1. Error rate vs number of phones for the baseline
GlobalPhone phone recognizer set
The first set of phone recognizers, which we refer to as
our baseline, includes recognizers for: Mandarin Chinese
(CH), German (DE), French (FR), Japanese (JA), Croatian
(KR), Portuguese (PO), Spanish (SP) and Turkish (TU).
For each language, the acoustic model consists of a context-
independent 3-state HMM system with 128 Gaussians per
state. The Gaussians are on 13 Mel-scale cepstral coeffi-
cients with first and second order derivatives and power.
Following cepstral mean subtraction, linear discriminant
analysis reduces the input vector to 32 dimensions.
The second set consists of extended phone recognizers,
available in 12 languages. Arabic (AR), Korean (KO),
Russian (RU) and Swedish (SW) are available in this set
in addition to the languages named above for the baseline
set. The 12 new phone recognizers were derived from
an improved generation of context dependent LVCSR
systems which also include vocal tract normalization
(VTLN) for speaker normalization. For decoding, we
used an unsupervised scheme to find the best warp fac-
tor for a test speaker and calculate a viterbi alignment
based on that speaker?s best warp factor. To improve
system speed, we reduced the number of Gaussians per
state from 128 to 16; in addition, the feature dimension
was halved from 32 to 16 using linear discriminant analysis.
Figure 1 shows the phone error rates in relation to the num-
ber of modeled phones for eight languages. The error rate
correlates with the number of phones used to model this lan-
guage. Turkish seems to be an exception to this finding. The
error analysis showed that this is due to a very high substi-
audio
phone string
phone string
	

	

 
	 
  
  
  

Efficient Optimization for Bilingual Sentence Alignment  
Based on Linear Regression 
 
Bing Zhao 
 
Language Technologies 
Institute 
Carnegie Mellon University 
bzhao@cs.cmu.edu 
Klaus Zechner 
 
Educational Testing Service 
Rosedale Road, Princeton, 
NJ 08541 
kzechner@ets.org 
Stephan Vogel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
vogel+@cs.cmu.edu 
Alex Waibel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
ahw@cs.cmu.edu 
 
Abstract 
This paper presents a study on optimizing sen-
tence pair alignment scores of a bilingual sen-
tence alignment module. Five candidate 
scores based on perplexity and sentence 
length are introduced and tested. Then a linear 
regression model based on those candidates is 
proposed and trained to predict sentence pairs? 
alignment quality scores solicited from human 
subjects. Experiments are carried out on data 
automatically collected from Internet. The 
correlation between the scores generated by 
the linear regression model and the scores 
from human subjects is in the range of the in-
ter-subject agreement score correlations. Pear-
son's correlation ranges from 0.53 up to 0.72 
in our experiments.  
1 Introduction 
In many instances, multilingual natural language 
systems like machine translation systems are developed 
and trained on parallel corpora.  When faced with a dif-
ferent, unseen text genre, however, translation perform-
ance usually drops noticeably.  One way to remedy this 
situation is to adapt and retrain the system parameters 
based on bilingual data from the same source or at least 
a closely related source.  A bilingual sentence alignment 
program (Gale and Church, 1991, and Brown et al, 
1991) is the crucial part in this adaptation procedure, in 
that it collects bilingual document pairs from the Inter-
net, and identifies sentence pairs, which should have a 
high likelihood of being correct translations of each 
other.  The set of identified bilingual parallel sentence 
pairs is then added to the training set for parameter re-
estimation. 
As is well known, text mined from the Internet is 
very noisy.  Even after careful html parsing and filtering 
for text size and language, the text from comparable 
html-page pairs still contains mismatches of content or 
non-parallel junk text, and the sentence order can be too 
different to be aligned.  Together with a large mismatch 
of vocabulary, the aligned sentence pairs, which are 
extracted from these collected comparable html-page 
pairs, contain a number of low translation quality 
alignments.  These need to be removed before the re-
training of the MT system. 
In this paper, we present an approach to automati-
cally optimizing the alignment scores of such a bilingual 
sentence alignment program.  The alignment score is a 
combination (by linear regression) of two word transla-
tion lexicon scores and three sentence length scores and 
predicts the translation quality scores from a set of hu-
man annotators.  We also present experiments analyzing 
how many different human scorers are needed for good 
prediction and also how many sentence pairs should be 
scored per human annotator. 
The paper is structured as follows: in section 2, the 
text mining system is briefly described.  In section 3, 
five sentence alignment models based on lexical infor-
mation and sentence length are explained. In section 4, a 
regression model is proposed to combine the five mod-
els to get further improvement in predicting alignment 
quality.  We describe alignment experiments in section 
5, focusing on the correlation between the alignment 
scores predicted by the sentence alignment models and 
by humans.  Conclusions are given in section 6. 
2 System of Mining Parallel Text 
One crucial component of statistical machine trans-
lation (SMT) system is the parallel text mining from 
Internet. Several processing modules are applied to col-
lect, extract, convert, and clean the text from Internet.  
The components in our system include: 
? A web crawler, which collects potential parallel 
html documents based on link information follow-
ing (Philip Resnik 1999); 
? A bilingual html parser (based on flex for effi-
ciency), which is designed for both Chinese and 
English html documents.  The paragraphs? bounda-
ries within the html structure are kept.  
? A character encoding detector, which judges if the 
Chinese html document is GB2312 encoding or 
BIG5 encoding.  
? An encoding converter, which converts the BIG5 
documents to GB2312 encoding.  
? A language identifier to ensure that source and tar-
get documents are both of the proper language. 
(Noord?s Implementation).  
? A Chinese word segmenter, which parses the Chi-
nese strings into Chinese words.   
? A document alignment program, which judges if 
the document pair is close translation candidates, 
and filters out those non-translation pairs. 
? A sentence boundary detector, which is based on 
punctuation and capitalized characters; 
? And the key component, a sentence alignment pro-
gram, which aligns and extracts potential parallel 
sentence pairs from the candidate document pairs. 
   
After sentence alignment, each candidate of a par-
allel sentence pair is then re-scored by the regression 
models (to be described in section 5). These scores are 
used to judge the quality of the aligned sentences.  Thus 
one can select the aligned sentence pairs, which have 
high alignment quality scores, to re-estimate the sys-
tem?s parameters.  
2.1 Sentence Alignment 
Our sentence alignment program uses IBM Model-1 
based perplexity (section 2.2) to calculate the similarity 
of each sentence pair. Dynamic programming is applied 
to find Viterbi path for sentence alignments of the bilin-
gual comparable document pair. In our dynamic pro-
gramming implementation, we allow for seven 
alignment types between English and Chinese sentences: 
 
? 1:1 ? exact match, where one sentence is the trans-
lation of the other one; 
? 2:2 ? the break point between two sentences in the 
source document is different from the segmentation 
in the target document.  E.g. part of sentence one in 
the source might be translated as part of the second 
sentence in the target; 
? 2:1, 1:2, and 3:1 ? these cases are similar to the 
case before: they handle differences in how a text is 
split into sentences. The case 1:3 has not been used 
in the final configuration of the system, as this type 
did not occur in any significant number; 
? 1:0 (deletion) and (0:1) insertion ? a sentence in the 
source document is missing in the translation or 
vice versa. 
 
The deletion and insertion types are discarded, and 
the remaining types are extracted to be used as potential 
parallel data. In general, one Chinese sentence corre-
sponds to several English sentences. In (Bing and 
Stephan, 2002), experiments on a 10-year XinHua news 
story collection from the Linguistic Data Consortium 
(LDC) show that alignment types like (2:1) and (3:1) 
are common, and this 7-type alignment is shown to be 
reliable for English-Chinese sentence alignment.  How-
ever, only a small part of the whole 10-year collection 
was pre-aligned (Xiaoyi, 1999) and extracted for sen-
tence alignment.   
The picture can be very different when directly min-
ing the data from Internet. Due to the mismatch between 
the training data and the data collected from Internet, 
the vocabulary coverage can be very low; the data is 
very noisy; and the data aligned is not strictly parallel. 
The percentage of alignment types of insertion (0:1) and 
deletion (1:0) become very high as shown in section 5. 
The aligned sentence pairs are subject to many align-
ment errors. The alignment errors are not desired in the 
re-training of the system, and need to be removed.  
Though the sentence alignment outputs a score from 
Viterbi path for each of the aligned sentence pairs, this 
score is only a rough estimation of the alignment quality. 
A more reliable re-scoring of the data is desirable to 
estimate the alignment quality as a post processing step 
to filter out the errors and noise from the aligned data.  
2.2 Statistical Translation Lexicon 
We use a statistical translation lexicon known as IBM 
Model-1 in (Brown et al, 1993) for both efficiency and 
simplicity.  
In our approach, Model-1 is the conditional probabil-
ity that a word f in the source language is translated 
given word e in the target language, t(f|e). This prob-
ability can be reliably estimated using the expectation-
maximization (EM) algorithm (Cavnar, W. B. and J. M. 
Trenkle, 1994). 
Given training data consisting of parallel sen-
tences: }..1),,{( )()( Sief ii = , our Model-1 training for 
t(f|e) is as follows: 
?
=
?
=
S
s
ss
e efefceft
1
)()(1 ),;|()|( ?  
Where 1?
e? is a normalization factor such that 
0.1)|( =
?
j
j eft  
),;|( )()( ss efefc denotes the expected number of times 
that word e connects to word f.  
??
?
==
=
=
l
i
i
m
j
jl
k
k
ss eeff
eft
eft
efefc
11
1
)()( ),(),(
)|(
)|(),;|( ??  
With the conditional probability t(f|e), the probability 
for an alignment of foreign string F given English string 
E is in (1): 
??
= =
+
=
m
j
n
i
ijm eftlEFP 1 0
)|()1(
1)|(  (1) 
The probability of alignment F given E: )|( EFP is 
shown to achieve the global maximum under this EM 
framework as stated in (Brown et al,1993).  
In our approach, equation (1) is further normalized 
so that the probability for different lengths of F is com-
parable at the word level: 
m
m
j
n
i
ijm eftlEFP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (2) 
 
The alignment models described in (Brown et al, 
1993) are all based on the notion that an alignment 
aligns each source word to exactly one target word.  
This makes this type of alignment models asymmetric.  
Thus by using the conditional probability t(e|f) trans-
lation lexicon trained from English (source) to Chinese 
(target), different aspects of the bilingual lexical 
information can be captured. A similar probability to (2) 
can be defined based on this reverse translation lexicon: 
n
m
i
n
j
jim fetlFEP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (3) 
 
Starting from the Hong Kong news corpora provided 
by LDC, we trained the translation lexicons to be used 
in the parallel sentence alignment.  Each sentence pair 
has a perplexity, which is calculated using the minus log 
of the probability eg. equation (2).  
3 Alignment Models 
The alignment model is aimed at automatically pre-
dicting the alignment scores of a bilingual sentence 
alignment program. By scoring the alignment quality of 
the sentence pairs, we can filter out those mis-aligned 
sentence pairs, and save our SMT system from being 
corrupted by mis-aligned data. 
3.1 Lexicon Based Models 
It is necessary to include lexical features in the 
aligned quality evaluation. One way is to use the trans-
lation lexicon based perplexity as in our sentence 
alignment program.  
For each of the aligned sentence pairs, the sentence 
alignment generated a score, which is solely based on 
equation (2). Using this score only, we can do a simple 
filtering by setting a threshold of perplexity. The sen-
tence pairs which have a higher perplexity than the 
threshold will be removed. However the perplexity 
based on (2) is definitely not discriminative enough to 
evaluate the quality of aligned sentence pairs.  
In our experiment, it showed that perplexity (3) has 
more discriminative power in judging the quality of the 
aligned sentence pairs for Chinese-English sentence 
alignment. It is also possible that equation (2) is more 
suitable for other language pairs.  Both (2) and (3) are 
applied in our sentence alignment quality judgment, 
which is to be explained in section 4.  
3.2 Sentence Length Models 
As was shown in the sentence alignment literature 
(Church, K.W. 1993), the sentence length ratio is also a 
very good indication of the alignment of a sentence pair 
for languages from a similar family such as French and 
English.  For language pairs from very different families 
such as Chinese and English, the sentence length ratio is 
also a good indication of alignment quality as shown in 
our experiments.  
For the language pair of Chinese and English, the 
sentence length can be defined in several different ways.  
3.2.1 Sentence Length 
In general, a Chinese sentence does not have word 
boundary information; so one way to define Chinese 
sentence length is to count the number of bytes of the 
sentence. Another way is to first segment the Chinese 
sentence into words (section 3.2.2) and count how many 
words are in the sentence. For English sentences, we 
can similarly define the length in bytes and in words.  
The length ratio is assumed to be a Gaussian distri-
bution. The mean and variance are calculated from the 
parallel training corpus, which, in our case, is the Hong 
Kong parallel corpus with 290K parallel sentence pairs.  
3.2.2 A Chinese Word Segmenter 
The word segmenter for Chinese is to parse the Chi-
nese string into words. Different word segmenters can 
generate different numbers of words for the same Chi-
nese sentence.  
There are many word segmenters publicly available. 
In our experiments, we applied a two-pass strategy to 
segment the word according to the dictionary of the 
LDC bilingual dictionary of Chinese-English. The two-
pass started first from left to right, and then from right 
back to left, to calculate the maximum word frequency 
and select one best path to segment the words.  
In general, the sentence length is not sensitive to the 
segmenters used. But for reliability, we want each seg-
mented word can have an English translation, thus we 
used the LDC bilingual dictionary as a reference word 
list for segmentation.  
3.2.3 Sentence Length Model 
Assume the alignment probability of ),|( tsAP  is 
only related to the length of source sentence s and target 
sentence t: 
|))||,(|(~
|)||(|~
||)||,|||||(|~
),||||(|),|(
tsP
tsP
tstsP
tstsPtsAP
?=
?=
?=
?=
 
where || s and || t are the sentence lengths of s and t.  
The difference of the length |)||,(| ts? is assumed 
to be a Gaussian distribution (Church, K.W. 1993) and 
can be normalized as follows: 
)1,0(~
)1|(|
||||
2
N
s
cst
?
?
+
?
=  (4) 
where c is a constant indicating the mean length ratios 
between source and target sentences and 2? is the vari-
ance of the length ratios.  
In our case, we applied three length models de-
scribed in the following Table 1: 
 
Table 1. Three Length Models description 
L-1 Both English and Chinese sentence are meas-
ured in bytes 
L-2 Both English and Chinese sentence are meas-
ured in words 
L-3 English sentence is measured in words and 
Chinese sentence is measured in bytes 
 
The means and 2? of the length ratios for each of the 
length models are calculated from Hong Kong news 
parallel corpus. The statistics of the three sentence 
length models are shown in Table 2. 
 
Table 2. Sentence length ratio statistics 
  L-1  L-2 L-3: 
Mean 1.59 1.01 0.33 
Var 3.82 0.79 0.71 
 
In general, the smaller the variance, the better the 
sentence length model can be. From Table 2 we observe 
that the bytes based length ratio model has significantly 
larger variance (3.82) than the other two models (L-2: 
0.79, L-3: 0.71).  This means L1 is not as reliable as L2 
and L3. Both L2 and L3 have similar variance, which 
indicates measuring English sentences in words will 
entail smaller variance in length model; measuring Chi-
nese sentences in bytes or words entails only a slight 
difference in variance. This also indicates that the length 
model is not so sensitive to the Chinese word segmenter 
applied. L-1, L-2 and L-3 capture the length relationship 
of parallel sentence in different views. Their modeling 
power has overlap, but they also compensate each other 
in capturing the parallel characteristics of good transla-
tion quality. A combination of these models can poten-
tially bring further improvement, which is shown in our 
experiment in section 6.  
4 Regression Model 
Rather than doing a binary decision (classification) that 
the aligned sentence pair is either good or not, the re-
gression can give a confidence score indicating how 
good the alignment can be, thus offering more flexibil-
ity in decisions.   Predicting the alignment quality using 
the candidate models is considered as a regression prob-
lem in that different scores are combined together.   
There are many ways such as genetic programming, 
to combine the candidate models, and regression is one 
of the straight forward and efficient ones.  So in this 
work, we explored linear regression. 
4.1 Candidate Models 
We have five candidate models described in section 
3. They are: PP1, the perplexity based on the word pair 
conditional probability p(f|e) in equation (2); PP2, the 
perplexity based on the reverse word pair conditional 
probability p(e|f) in equation (3); L-1, Length ratio 
model measured in bytes (mean=1.59, var=3.82); L-2, 
length ratio model measured in words (mean=1.01, 
var=0.79); L-3, length ratio model, where the English 
sentence is measured in words and the Chinese sentence 
is measured in bytes (mean=0.33, var=0.71).  These five 
models capture different aspects of the aligned quality 
of the sentence pair. The idea is to combine these five 
models together to get better prediction of the aligned 
quality. 
Linear regression is applied to combine these five 
models. It is trained from the observation of the five 
models together with the label of human judgment on a 
training set. 
4.2 Regression Model Training 
The linear regression model tries to discover the 
equation for a line that most nearly fits the given data 
(Trevor Hastie et al 2001). That linear equation is then 
used to predict values for the data.  
Now given human subject judgment of the aligned 
translation quality of sentence pairs, we can train a re-
gression model based on the five models we described 
in section 4.1 under the objective of least square errors.  
The human evaluation is measures translation qual-
ity of aligned pairs on a discrete 6-point scale between 1 
(very bad) and 5 (perfect translation). The score 0 was 
used for alignments that were not genuine translation 
e.g., both sentences were from the same language. We 
will use n for the number of total sentence pairs labeled 
by humans and used in training.  
Let A= [PP1, PP2, L-1, L-2, L-3] be the machine-
generated scores for each of the sentence pairs. In our 
case, A is a 5?n  matrix.  
Let H= [Human-Judgment-Score] be the human 
evaluation of the sentence pairs on a 6-point scale. In 
our case, H is a 1?n  matrix. 
In linear regression modeling, a linear transforma-
tion matrix W should satisfy the least square error crite-
rion: 
||}{||min* HAWW
w
?=  (5) 
where W is in fact a 5x1 weight matrix. The equation 
can be solved as:  
HAAAW TT 1* )( ?=  (6) 
The inverse of matrix AAT  is usually calculated using 
singular vector decomposition (SVD). After W is calcu-
lated, the predicted score from the regression model is: 
*
' AWH =  (7) 
where 'H  is the final predicted alignment quality score 
of the regression model. We can also view 'H  as a 
weighted sum of the five models shown in section 4.1. 
The calculation of 'H  reduces to a linear weighted 
summation, which is very efficient to compute.  
5 Experiments 
1500 pairs of comparable html document pairs were 
obtained from bilingual web pages crawled from Inter-
net. After preprocessing, filtering, and sentence align-
ment, the alignment types were distributed as shown in 
Table 3. Ignoring the alignment type of insertion (0:1) 
and deletion (1:0), we extracted around 5941 parallel 
sentences.  
 
Table 3. Alignment types? distribution of mined 
data from noisy web data crawled 
 1:0 0:1 1:1 2:1 1:2 2:2 3:1 
% 23.7 41.9 29.4 1.99 0.01 0.02 2.79 
 
From Table 3, we see the data is very noisy, con-
taining a large portion of insertions (23.7%) and dele-
tions (41.9%).  This is very different from the LDC 
XinHua pre-aligned collection provided by LDC, which 
is relatively clean.  
For this set of English-Chinese bilingual sentences, 
we randomly selected 200 sentence pairs, focusing on 
Viterbi alignment scores below 12.0 from sentence 
alignment, which was an empirically determined 
threshold (The alignment scores here were purely re-
flecting the Model-1 parameters using equation (2)).  
Three human subjects then had to score the 'translation 
quality' of every sentence pair, using a 6 point scale 
described in section 4.2. We further excluded very short 
sentences from consideration and evaluated 168 remain-
ing sentences. 
Pearson R correlation is applied to calculate the mag-
nitude of the association between two variables (human-
human or human-machine in our case) that are on an 
interval or ratio scale. The correlation coefficients 
(Pearson R) between human subjects were in Table 4 
(all are statistically significant): 
 
Table 4. Correlation between Human Subjects 
 H2 H3 
H1 0.786 0.615 
H2 ---- 0.568 
 
Overall, more than 2/3 of the human scores are identical 
or differ by only 1 (between subjects). 
For the automatic score prediction, the five compo-
nent scores described in section 4.1 are used, which are 
then combined using a standard Linear Regression as 
described in section 4.2. Table 5 shows the correlation 
between alignment scores based on Model X and human 
subjects' predicted quality scores: 
 
Table 5. Correlation between optimization models 
and human subjects 
Model human-1 human -2 human -3 
PP-1 .57 .53 .32 
PP-2 .60 .58 .46 
L-1 .42 .41 .30 
L-2 .46 .41 .40 
L-3 .40 .38 .29 
Na?ve .58 .56 .38 
Regression  .72 .68 .53 
 
The data we used in our training of the lexicon is Hong 
Kong news parallel data from LDC. There are 290K 
parallel sentence pairs, with 7 million words of English 
and 7.3 million Chinese words after segmentation. The 
IBM Model-1 for PP-1 and PP-2 are both trained using 
5 EM iterations. The other three length models are also 
calculated from the same 290K sentence pairs. Punctua-
tion is removed before the calculation of all automatic 
score prediction models. 
The regression model here is the standard linear re-
gression using the observations from three human sub-
jects as described in section 4.1. The average 
performance of the regression model is shown in the 
bottom line of the above Table 5. The average correla-
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Average of 3-human judgement 5-scale score (5-best, 0-non)
Pearson r Correlation
tion varies from 0.53 upto 0.72, which shows that the 
regression model has a very strong positive correlation 
with the human judgment.  
Also from Table 5, we see both lexicon based mod-
els: PP-1 and PP-2 are better than the length models in 
term of correlation with human scorer. Model PP-2 has 
the largest correlation, and is slightly better than PP-1. 
PP-2 is based on the conditional probability of p(e|f), 
which models the generation of an English word from a 
Chinese word. The vocabulary size of Chinese is usu-
ally smaller than English vocabulary size, so this model 
can be more reliably estimated than the reverse direction 
of p(f|e). This explains why PP-2 is slightly better than 
PP-1.  
For sentence length models, we see L-2, for which 
the lengths of both the English sentence and the Chinese 
sentence are measured in words, has the best perform-
ance among the three settings of a sentence length 
model. This indicates that the length model measured in 
words is more reliable.  
Also shown in Table 5, the na?ve interpolation of 
these different models, i.e. just using each model with 
equal weight, resulted in lower correlation than the best 
single alignment model. 
We also performed correlation experiments with 
varied numbers of training sentences from either Hu-
man-1/Human-2/Human-3 or from all of the three hu-
man subjects.  We picked the first 30/60/90/120 labeled 
sentence pairs for training and saved the last 48 sen-
tence pairs for testing.  The average performance of the 
regression model is as follows: 
 
Table 6. Correlation between different training set 
sizes and human scorers. 
Training  
set size 
Human-1 Human -2 Human ?3 
30 .686 .639 .447 
60 .750 .707 .452 
90 .765 .721 .456 
120 .760 .721 .464 
 
The average correlation of the regression models 
showed here increased noticeably when the training set 
was increased from 30 sentence pairs to 90 sentence 
pairs. More sentence pairs caused no or only marginal 
improvements (esp. for the third human subject).  
Figure 1 shows a scatter plot, which illustrates a 
good correlation (here: Pearson R=0.74) between our 
regression model predictors and the human scorers. 
6 Conclusion 
In this paper, we have demonstrated ways to effi-
ciently optimize a sentence alignment module, such that 
it is able to select aligned sentence pairs of high transla-
tion quality automatically. This procedure of alignment 
score optimization requires (a) a small number of hu-
man subjects who annotate a set of about 100 sentence 
pairs each for translation quality; and (b) a set of align-
ment scores, based on perplexity and sentence length 
ratio, to be able to learn to predict the human scores. 
Based on the learned predictions, by means of linear 
regression, the alignment program can choose the best  
sentence pair candidates to be included in the training 
data for the SMT system re-estimation. 
Our experiments showed that, for Chinese-English 
language pair, perplexity based on the reverse word pair 
conditional probability p(e|f) (PP-2) gives the most reli-
able prediction among the five models proposed in this 
paper; the regression model, which combines those five 
models, give the best correlation between human score 
and automatic predictions. Our approach needs only a 
fairly limited number of human labeled sentences pairs, 
and is an efficient optimization of the sentence 
alignment system. 
 
Figure 1. Correlation between regression model and 
human scorers, Pearson R=0.74. 
 
References 
Bing Zhao, Stephan Vogel. 2002. Adaptive Parallel 
Sentences Mining from Web Bilingual News Collec-
tion. IEEE International Conference on Data Mining 
(ICDM 02) , pp. 745-748. Japan. 
Brown, P., Lai, J. C., and Mercer, R. 1991. Aligning 
Sentences in Parallel Corpora. In Proceedings of 
ACL-91, Berkeley CA. 1991 
Cavnar, W. B. and J. M. Trenkle. 1994.  N-Gram-Based 
Text Categorization.  Proceedings of Third Annual 
Symposium on Document Analysis and Information 
Retrieval, Las Vegas, NV, UNLV Publica-
tions/Reprographics, pp. 161-175, 11-13. 
Stanley Chen. 1993. Aligning sentences in Bilingual 
corpora using lexical information. In proceedings of 
the 31st Annual Conference of the Association for 
computational linguistics, pages 9-16, Columbus, 
Ohio, June 1993 
Church, K. W. 1993. Char_align: A Program for Align-
ing Parallel Texts at the Character Level. Proceed-
ings of ACL-93, Columbus OH. 
Gale, W. A. and Church, K. W.  1991. A Program for 
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-91, Berkeley CA. 1991. 
Melamed, I.D. 1996. A Geometric Approach to Map-
ping Bitext Correspondence. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA. 1996 
Noord?s Implementation of Textcat: http://odur.let. 
rug.nl/~vannoord/TextCat/index.html 
Peter F. Brown, Stephan A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer.  1993. The 
Mathmatics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics, vol 
19, no.2 , pp.263-311. 
Philip Resnik. 1999. Mining the Web for Bilingual Text. 
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL'99), University of Maryland, 
College Park, Maryland. 
Trevor Hastie, Robert Tibshirani, Jerome Friedman. 
2001. The Elements of Statistical Learning: Data 
Mining, Inference and Prediction. Springer Publisher. 
Xiaoyi Ma, Mark Y. Liberman, ?BITS: A Method for 
Bilingual Text Search over the Web?. Machine 
Translation Summit VII, 1999 
 
$XWRPDWLF([WUDFWLRQRI1DPHG(QWLW\7UDQVOLQJXDO(TXLYDOHQFH%DVHGRQ0XOWL)HDWXUH&RVW0LQLPL]DWLRQ)HL+XDQJ6WHSKDQ9RJHODQG$OH[:DLEHO/DQJXDJH7HFKQRORJLHV,QVWLWXWH&DUQHJLH0HOORQ8QLYHUVLW\3LWWVEXUJK3$^IKXDQJYRJHODKZ`#FVFPXHGX $EVWUDFW7UDQVOLQJXDO HTXLYDOHQFH UHIHUV WR WKH UHODWLRQVKLSEHWZHHQH[SUHVVLRQVRIWKHVDPHPHDQLQJIURPGLIIHUHQWODQJXDJHV ,GHQWLI\LQJ WUDQVOLQJXDO HTXLYDOHQFH RIQDPHG HQWLWLHV 1( FDQ VLJQLILFDQWO\ FRQWULEXWH WRPXOWLOLQJXDO QDWXUDO ODQJXDJH SURFHVVLQJ VXFK DVFURVVOLQJXDO LQIRUPDWLRQ UHWULHYDO FURVVOLQJXDOLQIRUPDWLRQ H[WUDFWLRQ DQG VWDWLVWLFDO PDFKLQHWUDQVODWLRQ ,Q WKLV SDSHU ZH SUHVHQW DQ LQWHJUDWHGDSSURDFKWRH[WUDFW1(WUDQVOLQJXDOHTXLYDOHQFHIURPDSDUDOOHO&KLQHVH(QJOLVKFRUSXV6WDUWLQJ IURP D ELOLQJXDO FRUSXV ZKHUH 1(V DUHDXWRPDWLFDOO\ WDJJHG IRU HDFK ODQJXDJH 1( SDLUV DUHDOLJQHG LQ RUGHU WR PLQLPL]H WKH RYHUDOO PXOWLIHDWXUHDOLJQPHQW FRVW  $Q 1( WUDQVOLWHUDWLRQ PRGHO LVSUHVHQWHG DQG LWHUDWLYHO\ WUDLQHG XVLQJ QDPHG HQWLW\SDLUV H[WUDFWHG IURP D ELOLQJXDO GLFWLRQDU\ 7KHWUDQVOLWHUDWLRQ FRVW FRPELQHG ZLWK WKH QDPHG HQWLW\WDJJLQJFRVWDQGZRUGEDVHGWUDQVODWLRQFRVWFRQVWLWXWHWKH PXOWLIHDWXUH DOLJQPHQW FRVW 7KHVH IHDWXUHV DUHGHULYHG IURP VHYHUDO LQIRUPDWLRQ VRXUFHV XVLQJXQVXSHUYLVHGDQGSDUWO\VXSHUYLVHGPHWKRGV $JUHHG\VHDUFK DOJRULWKP LV DSSOLHG WR PLQLPL]H WKH DOLJQPHQWFRVW ([SHULPHQWV VKRZ WKDW WKH SURSRVHG DSSURDFKH[WUDFWV1(WUDQVOLQJXDOHTXLYDOHQFHZLWK)VFRUHDQGLPSURYHVWKHWUDQVODWLRQVFRUHIURPProceedings of the ACL Workshop on Building and Using Parallel Texts, pages 25?32,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Bilingual Word Spectral Clustering for Statistical Machine Translation
Bing Zhao? Eric P. Xing? ? Alex Waibel?
?Language Technologies Institute
?Center for Automated Learning and Discovery
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213
{bzhao,epxing,ahw}@cs.cmu.edu
Abstract
In this paper, a variant of a spectral clus-
tering algorithm is proposed for bilingual
word clustering. The proposed algorithm
generates the two sets of clusters for both
languages efficiently with high seman-
tic correlation within monolingual clus-
ters, and high translation quality across
the clusters between two languages. Each
cluster level translation is considered as
a bilingual concept, which generalizes
words in bilingual clusters. This scheme
improves the robustness for statistical ma-
chine translation models. Two HMM-
based translation models are tested to use
these bilingual clusters. Improved per-
plexity, word alignment accuracy, and
translation quality are observed in our ex-
periments.
1 Introduction
Statistical natural language processing usually suf-
fers from the sparse data problem. Comparing to
the available monolingual data, we have much less
training data especially for statistical machine trans-
lation (SMT). For example, in language modelling,
there are more than 1.7 billion words corpora avail-
able: English Gigaword by (Graff, 2003). However,
for machine translation tasks, there are typically less
than 10 million words of training data.
Bilingual word clustering is a process of form-
ing corresponding word clusters suitable for ma-
chine translation. Previous work from (Wang et al,
1996) showed improvements in perplexity-oriented
measures using mixture-based translation lexicon
(Brown et al, 1993). A later study by (Och,
1999) showed improvements on perplexity of bilin-
gual corpus, and word translation accuracy using a
template-based translation model. Both approaches
are optimizing the maximum likelihood of parallel
corpus, in which a data point is a sentence pair: an
English sentence and its translation in another lan-
guage such as French. These algorithms are es-
sentially the same as monolingual word clusterings
(Kneser and Ney, 1993)?an iterative local search.
In each iteration, a two-level loop over every possi-
ble word-cluster assignment is tested for better like-
lihood change. This kind of approach has two draw-
backs: first it is easily to get stuck in local op-
tima; second, the clustering of English and the other
language are basically two separated optimization
processes, and cluster-level translation is modelled
loosely. These drawbacks make their approaches
generally not very effective in improving translation
models.
In this paper, we propose a variant of the spec-
tral clustering algorithm (Ng et al, 2001) for bilin-
gual word clustering. Given parallel corpus, first, the
word?s bilingual context is used directly as features
- for instance, each English word is represented by
its bilingual word translation candidates. Second,
latent eigenstructure analysis is carried out in this
bilingual feature space, which leads to clusters of
words with similar translations. Essentially an affin-
ity matrix is computed using these cross-lingual fea-
tures. It is then decomposed into two sub-spaces,
which are meaningful for translation tasks: the left
subspace corresponds to the representation of words
in English vocabulary, and the right sub-space cor-
responds to words in French. Each eigenvector is
considered as one bilingual concept, and the bilin-
gual clusters are considered to be its realizations in
two languages. Finally, a general K-means cluster-
25
ing algorithm is used to find out word clusters in the
two sub-spaces.
The remainder of the paper is structured as fol-
lows: in section 2, concepts of translation models
are introduced together with two extended HMMs;
in section 3, our proposed bilingual word cluster-
ing algorithm is explained in detail, and the related
works are analyzed; in section 4, evaluation metrics
are defined and the experimental results are given;
in section 5, the discussions and conclusions.
2 Statistical Machine Translation
The task of translation is to translate one sentence
in some source language F into a target language E.
For example, given a French sentence with J words
denoted as fJ1 = f1f2...fJ , an SMT system auto-
matically translates it into an English sentence with
I words denoted by eI1 = e1e2...eI . The SMT sys-
tem first proposes multiple English hypotheses in its
model space. Among all the hypotheses, the system
selects the one with the highest conditional proba-
bility according to Bayes?s decision rule:
e?I1 = argmax
{eI1}
P (eI1|fJ1 ) = argmax
{eI1}
P (fJ1 |eI1)P (eI1),
(1)
where P (fJ1 |eI1) is called translation model, and
P (eI1) is called language model. The translation
model is the key component, which is the focus in
this paper.
2.1 HMM-based Translation Model
HMM is one of the effective translation models (Vo-
gel et al, 1996), which is easily scalable to very
large training corpus.
To model word-to-word translation, we introduce
the mapping j ? aj , which assigns a French word
fj in position j to a English word ei in position
i = aj denoted as eaj . Each French word fj is
an observation, and it is generated by a HMM state
defined as [eaj , aj], where the alignment aj for po-
sition j is considered to have a dependency on the
previous alignment aj?1. Thus the first-order HMM
is defined as follows:
P (fJ1 |eI1) =
?
aJ1
J?
j=1
P (fj |eaj )P (aj |aj?1), (2)
where P (aj |aj?1) is the transition probability. This
model captures the assumption that words close in
the source sentence are aligned to words close in
the target sentence. An additional pseudo word of
?NULL? is used as the beginning of English sen-
tence for HMM to start with. The (Och and Ney,
2003) model includes other refinements such as spe-
cial treatment of a jump to a Null word, and a uni-
form smoothing prior. The HMM with these refine-
ments is used as our baseline. Motivated by the work
in both (Och and Ney, 2000) and (Toutanova et al,
2002), we propose the two following simplest ver-
sions of extended HMMs to utilize bilingual word
clusters.
2.2 Extensions to HMM with word clusters
Let F denote the cluster mapping fj ? F(fj), which
assigns French word fj to its cluster ID Fj = F(fj).
Similarly E maps English word ei to its cluster ID
of Ei = E(ei). In this paper, we assume each word
belongs to one cluster only.
With bilingual word clusters, we can extend the
HMM model in Eqn. 1 in the following two ways:
P (fJ1 |eI1) =
?
aJ1
?J
j=1 P (fj |eaj )?
P (aj |aj?1,E(eaj?1),F(fj?1)),
(3)
where E(eaj?1) and F(fj?1) are non overlapping
word clusters (Eaj?1 , Fj?1)for English and French
respectively.
Another explicit way of utilizing bilingual word
clusters can be considered as a two-stream HMM as
follows:
P (fJ1 , F J1 |eI1, EI1) =?
aJ1
?J
j=1 P (fj |eaj )P (Fj |Eaj )P (aj |aj?1).
(4)
This model introduces the translation of bilingual
word clusters directly as an extra factor to Eqn. 2.
Intuitively, the role of this factor is to boost the trans-
lation probabilities for words sharing the same con-
cept. This is a more expressive model because it
models both word and the cluster level translation
equivalence. Also, compared with the model in Eqn.
3, this model is easier to train, as it uses a two-
dimension table instead of a four-dimension table.
However, we do not want this P (Fj |Eaj ) to dom-
inate the HMM transition structure, and the obser-
26
vation probability of P (fj |eaj ) during the EM itera-
tions. Thus a uniform prior P (Fj) = 1/|F | is intro-
duced as a smoothing factor for P (Fj |Eaj ):
P (Fj |Eaj ) = ?P (Fj |Eaj ) + (1? ?)P (Fj), (5)
where |F | is the total number of word clusters in
French (we use the same number of clusters for both
languages). ? can be chosen to get optimal perfor-
mance on a development set. In our case, we fix it to
be 0.5 in all our experiments.
3 Bilingual Word Clustering
In bilingual word clustering, the task is to build word
clusters F and E to form partitions of the vocabular-
ies of the two languages respectively. The two par-
titions for the vocabularies of F and E are aimed to
be suitable for machine translation in the sense that
the cluster/partition level translation equivalence is
reliable and focused to handle data sparseness; the
translation model using these clusters explains the
parallel corpus {(fJ1 , eI1)} better in terms of perplex-
ity or joint likelihood.
3.1 From Monolingual to Bilingual
To infer bilingual word clusters of (F,E), one can
optimize the joint probability of the parallel corpus
{(fJ1 , eI1)} using the clusters as follows:
(F?, E?) = argmax
(F,E)
P (fJ1 , eI1|F,E)
= argmax
(F,E)
P (eI1|E)P (fJ1 |eI1, F, E).(6)
Eqn. 6 separates the optimization process into two
parts: the monolingual part for E, and the bilingual
part for F given fixed E. The monolingual part is
considered as a prior probability:P (eI1|E), and E can
be inferred using corpus bigram statistics in the fol-
lowing equation:
E? = argmax
{E}
P (eI1|E)
= argmax
{E}
I?
i=1
P (Ei|Ei?1)P (ei|Ei). (7)
We need to fix the number of clusters beforehand,
otherwise the optimum is reached when each word
is a class of its own. There exists efficient leave-one-
out style algorithm (Kneser and Ney, 1993), which
can automatically determine the number of clusters.
For the bilingual part P (fJ1 |eI1, F, E), we can
slightly modify the same algorithm as in (Kneser
and Ney, 1993). Given the word alignment {aJ1}
between fJ1 and eI1 collected from the Viterbi path
in HMM-based translation model, we can infer F? as
follows:
F? = argmax
{F}
P (fJ1 |eI1, F,E)
= argmax
{F}
J?
j=1
P (Fj |Eaj )P (fj |Fj). (8)
Overall, this bilingual word clustering algorithm is
essentially a two-step approach. In the first step, E
is inferred by optimizing the monolingual likelihood
of English data, and secondly F is inferred by op-
timizing the bilingual part without changing E. In
this way, the algorithm is easy to implement without
much change from the monolingual correspondent.
This approach was shown to give the best results
in (Och, 1999). We use it as our baseline to compare
with.
3.2 Bilingual Word Spectral Clustering
Instead of using word alignment to bridge the par-
allel sentence pair, and optimize the likelihood in
two separate steps, we develop an alignment-free al-
gorithm using a variant of spectral clustering algo-
rithm. The goal is to build high cluster-level trans-
lation quality suitable for translation modelling, and
at the same time maintain high intra-cluster similar-
ity , and low inter-cluster similarity for monolingual
clusters.
3.2.1 Notations
We define the vocabulary VF as the French vo-
cabulary with a size of |VF |; VE as the English vo-
cabulary with size of |VE |. A co-occurrence matrix
C{F,E} is built with |VF | rows and |VE | columns;
each element represents the co-occurrence counts of
the corresponding French word fj and English word
ei. In this way, each French word forms a row vec-
tor with a dimension of |VE |, and each dimensional-
ity is a co-occurring English word. The elements in
the vector are the co-occurrence counts. We can also
27
view each column as a vector for English word, and
we?ll have similar interpretations as above.
3.2.2 Algorithm
With C{F,E}, we can infer two affinity matrixes
as follows:
AE = CT{F,E}C{F,E}
AF = C{F,E}CT{F,E},
where AE is an |VE | ? |VE | affinity matrix for En-
glish words, with rows and columns representing
English words and each element the inner product
between two English words column vectors. Corre-
spondingly, AF is an affinity matrix of size |VF | ?
|VF | for French words with similar definitions. Both
AE and AF are symmetric and non-negative. Now
we can compute the eigenstructure for both AE and
AF . In fact, the eigen vectors of the two are corre-
spondingly the right and left sub-spaces of the orig-
inal co-occurrence matrix of C{F,E} respectively.
This can be computed using singular value decom-
position (SVD): C{F,E} = USV T , AE = V S2V T ,
and AF = US2UT , where U is the left sub-space,
and V the right sub-space of the co-occurrence ma-
trix C{F,E}. S is a diagonal matrix, with the singular
values ranked from large to small along the diagonal.
Obviously, the left sub-space U is the eigenstructure
for AF ; the right sub-space V is the eigenstructure
for AE .
By choosing the top K singular values (the square
root of the eigen values for both AE and AF ), the
sub-spaces will be reduced to: U|VF |?K and V|VE |?K
respectively. Based on these subspaces, we can carry
out K-means or other clustering algorithms to in-
fer word clusters for both languages. Our algorithm
goes as follows:
? Initialize bilingual co-occurrence matrix
C{F,E} with rows representing French words,
and columns English words. Cji is the co-
occurrence raw counts of French word fj and
English word ei;
? Form the affinity matrix AE = CT{F,E}C{F,E}
and AF = CT{F,E}C{F,E}. Kernels can also be
applied here such as AE = exp(
C{F,E}CT{F,E}
?2 )
for English words. Set AEii = 0 and AF ii = 0,
and normalize each row to be unit length;
? Compute the eigen structure of the normalized
matrix AE , and find the k largest eigen vectors:
v1, v2, ..., vk; Similarly, find the k largest eigen
vectors of AF : u1, u2, ..., uk;
? Stack the k eigenvectors of v1, v2, ..., vk in
the columns of YE , and stack the eigenvectors
u1, u2, ..., uk in the columns for YF ; Normalize
rows of both YE and YF to have unit length. YE
is size of |VE | ? k and YF is size of |VF | ? k;
? Treat each row of YE as a point in R|VE |?k, and
cluster them into K English word clusters us-
ing K-means. Treat each row of YF as a point in
R|VF |?k, and cluster them into K French word
clusters.
? Finally, assign original word ei to cluster Ek
if row i of the matrix YE is clustered as Ek;
similar assignments are for French words.
Here AE and AF are affinity matrixes of pair-wise
inner products between the monolingual words. The
more similar the two words, the larger the value.
In our implementations, we did not apply a kernel
function like the algorithm in (Ng et al, 2001). But
the kernel function such as the exponential func-
tion mentioned above can be applied here to control
how rapidly the similarity falls, using some carefully
chosen scaling parameter.
3.2.3 Related Clustering Algorithms
The above algorithm is very close to the variants
of a big family of the spectral clustering algorithms
introduced in (Meila and Shi, 2000) and studied in
(Ng et al, 2001). Spectral clustering refers to a class
of techniques which rely on the eigenstructure of
a similarity matrix to partition points into disjoint
clusters with high intra-cluster similarity and low
inter-cluster similarity. It?s shown to be computing
the k-way normalized cut: K ? trY TD? 12AD? 12Y
for any matrix Y ? RM?N . A is the affinity matrix,
and Y in our algorithm corresponds to the subspaces
of U and V .
Experimentally, it has been observed that using
more eigenvectors and directly computing a k-way
partitioning usually gives better performance. In our
implementations, we used the top 500 eigen vectors
to construct the subspaces of U and V for K-means
clustering.
28
3.2.4 K-means
The K-means here can be considered as a post-
processing step in our proposed bilingual word clus-
tering. For initial centroids, we first compute the
center of the whole data set. The farthest centroid
from the center is then chosen to be the first initial
centroid; and after that, the other K-1 centroids are
chosen one by one to well separate all the previous
chosen centroids.
The stopping criterion is: if the maximal change
of the clusters? centroids is less than the threshold of
1e-3 between two iterations, the clustering algorithm
then stops.
4 Experiments
To test our algorithm, we applied it to the TIDES
Chinese-English small data track evaluation test set.
After preprocessing, such as English tokenization,
Chinese word segmentation, and parallel sentence
splitting, there are in total 4172 parallel sentence
pairs for training. We manually labeled word align-
ments for 627 test sentence pairs randomly sampled
from the dry-run test data in 2001, which has four
human translations for each Chinese sentence. The
preprocessing for the test data is different from the
above, as it is designed for humans to label word
alignments correctly by removing ambiguities from
tokenization and word segmentation as much as pos-
sible. The data statistics are shown in Table 1.
English Chinese
Train
Sent. Pairs 4172
Words 133598 105331
Voc Size 8359 7984
Test
Sent. Pairs 627
Words 25500 19726
Voc Size 4084 4827
Unseen Voc Size 1278 1888
Alignment Links 14769
Table 1: Training and Test data statistics
4.1 Building Co-occurrence Matrix
Bilingual word co-occurrence counts are collected
from the training data for constructing the matrix
of C{F,E}. Raw counts are collected without word
alignment between the parallel sentences. Practi-
cally, we can use word alignment as used in (Och,
1999). Given an initial word alignment inferred by
HMM, the counts are collected from the aligned
word pair. If the counts are L-1 normalized, then
the co-occurrence matrix is essentially the bilingual
word-to-word translation lexicon such as P (fj |eaj ).
We can remove very small entries (P (f |e) ? 1e?7),
so that the matrix of C{F,E} is more sparse for eigen-
structure computation. The proposed algorithm is
then carried out to generate the bilingual word clus-
ters for both English and Chinese.
Figure 1 shows the ranked Eigen values for the
co-occurrence matrix of C{F,E}.
0 100 200 300 400 500 600 700 800 900 10000.5
1
1.5
2
2.5
3
3.5 Eigen values of affinity matrices
Top 1000 Eigen Values
Eige
n Va
lues
(a) co?occur counts from init word alignment(b) raw co?occur counts from data
Figure 1: Top-1000 Eigen Values of Co-occurrence
Matrix
It is clear, that using the initial HMM word align-
ment for co-occurrence matrix makes a difference.
The top Eigen value using word alignment in plot a.
(the deep blue curve) is 3.1946. The two plateaus
indicate how many top K eigen vectors to choose to
reduce the feature space. The first one indicates that
K is in the range of 50 to 120, and the second plateau
indicates K is in the range of 500 to 800. Plot b. is
inferred from the raw co-occurrence counts with the
top eigen value of 2.7148. There is no clear plateau,
which indicates that the feature space is less struc-
tured than the one built with initial word alignment.
We find 500 top eigen vectors are good enough
for bilingual clustering in terms of efficiency and ef-
fectiveness.
29
4.2 Clustering Results
Clusters built via the two described methods are
compared. The first method bil1 is the two-step op-
timization approach: first optimizing the monolin-
gual clusters for target language (English), and af-
terwards optimizing clusters for the source language
(Chinese). The second method bil2 is our proposed
algorithm to compute the eigenstructure of the co-
occurrence matrix, which builds the left and right
subspaces, and finds clusters in such spaces. Top
500 eigen vectors are used to construct these sub-
spaces. For both methods, 1000 clusters are inferred
for English and Chinese respectively. The number
of clusters is chosen in a way that the final word
alignment accuracy was optimal. Table 2 provides
the clustering examples using the two algorithms.
settings cluster examples
mono-E1 entirely,mainly,merely
mono-E2
10th,13th,14th,16th,17th,18th,19th
20th,21st,23rd,24th,26th
mono-E3 drink,anglophobia,carota,giant,gymnasium
bil1-C3 ?,d,?,?,??,yQ,y
bil2-E1 alcoholic cognac distilled drinkscotch spirits whiskey
bil2-C1 ??,?,,??,2,?y,
?h,7,??},6,?,,k
bil2-E2 evrec harmony luxury people sedan sedanstour tourism tourist toward travel
bil2-C2 ??,s?,?,?(,ff?,u?,
@q,@?,|,|?,-|
Table 2: Bilingual Cluster Examples
The monolingual word clusters often contain
words with similar syntax functions. This hap-
pens with esp. frequent words (eg. mono-E1 and
mono-E2). The algorithm tends to put rare words
such as ?carota, anglophobia? into a very big cluster
(eg. mono-E3). In addition, the words within these
monolingual clusters rarely share similar transla-
tions such as the typical cluster of ?week, month,
year?. This indicates that the corresponding Chi-
nese clusters inferred by optimizing Eqn. 7 are not
close in terms of translational similarity. Overall, the
method of bil1 does not give us a good translational
correspondence between clusters of two languages.
The English cluster of mono-E3 and its best aligned
candidate of bil1-C3 are not well correlated either.
Our proposed bilingual cluster algorithm bil2
generates the clusters with stronger semantic mean-
ing within a cluster. The cluster of bil2-E1 relates
to the concept of ?wine? in English. The mono-
lingual word clustering tends to scatter those words
into several big noisy clusters. This cluster also has a
good translational correspondent in bil2-C1 in Chi-
nese. The clusters of bil2-E2 and bil2-C2 are also
correlated very well. We noticed that the Chinese
clusters are slightly more noisy than their English
corresponding ones. This comes from the noise in
the parallel corpus, and sometimes from ambiguities
of the word segmentation in the preprocessing steps.
To measure the quality of the bilingual clusters,
we can use the following two kind of metrics:
? Average ?-mirror (Wang et al, 1996): The ?-
mirror of a class Ei is the set of clusters in
Chinese which have a translation probability
greater than ?. In our case, ? is 0.05, the same
value used in (Och, 1999).
? Perplexity: The perplexity is defined as pro-
portional to the negative log likelihood of the
HMM model Viterbi alignment path for each
sentence pair. We use the bilingual word clus-
ters in two extended HMM models, and mea-
sure the perplexities of the unseen test data af-
ter seven forward-backward training iterations.
The two perplexities are defined as PP1 =
exp(??Jj=1 log(P (fj |eaj )P (aj |aj?1, Eaj?1 ,
Fj?1))/J) and PP2 = exp(?J?1
?J
j=1 log(
P (fj |eaj )P (aj |aj?1)P (Fj?1|Eaj?1))) for the
two extended HMM models in Eqn 3 and 4.
Both metrics measure the extent to which the trans-
lation probability is spread out. The smaller the bet-
ter. The following table summarizes the results on
?-mirror and perplexity using different methods on
the unseen test data.
algorithms ?-mirror HMM-1 Perp HMM-2 Perp
baseline - 1717.82
bil1 3.97 1810.55 352.28
bil2 2.54 1610.86 343.64
The baseline uses no word clusters. bil1 and bil2
are defined as above. It is clear that our proposed
method gives overall lower perplexity: 1611 from
the baseline of 1717 using the extended HMM-1.
If we use HMM-2, the perplexity goes down even
more using bilingual clusters: 352.28 using bil1, and
343.64 using bil2. As stated, the four-dimensional
30
table of P (aj |aj?1, E(eaj?1), F (fj?1)) is easily
subject to overfitting, and usually gives worse per-
plexities.
Average ?-mirror for the two-step bilingual clus-
tering algorithm is 3.97, and for spectral cluster-
ing algorithm is 2.54. This means our proposed al-
gorithm generates more focused clusters of transla-
tional equivalence. Figure 2 shows the histogram for
the cluster pairs (Fj , Ei), of which the cluster level
translation probabilities P (Fj |Ei) ? [0.05, 1]. The
interval [0.05, 1] is divided into 10 bins, with first bin
[0.05, 0.1], and 9 bins divides[0.1, 1] equally. The
percentage for clusters pairs with P (Fj |Ei) falling
in each bin is drawn.
Histogram of (F,E) pairs with P(F|E) > 0.05 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Ten bins for P(F|E) ranging from [0.05, 1.0] 
spec-bi-clustering
two-step-bi-clustering
Figure 2: Histogram of cluster pairs (Fj , Ei)
Our algorithm generates much better aligned clus-
ter pairs than the two-step optimization algorithm.
There are 120 cluster pairs aligned with P (Fj |Ei) ?
0.9 using clusters from our algorithm, while there
are only 8 such cluster pairs using the two-step ap-
proach. Figure 3 compares the ?-mirror at different
numbers of clusters using the two approaches. Our
algorithm has a much better ?-mirror than the two-
step approach over different number of clusters.
Overall, the extended HMM-2 is better than
HMM-1 in terms of perplexity, and is easier to train.
4.3 Applications in Word Alignment
We also applied our bilingual word clustering in a
word alignment setting. The training data is the
TIDES small data track. The word alignments are
manually labeled for 627 sentences sampled from
the dryrun test data in 2001. In this manually
aligned data, we include one-to-one, one-to-many,
and many-to-many word alignments. Figure 4 sum-
marizes the word alignment accuracy for different
e-mirror over different settings
00.5
11.5
22.5
33.5
44.5
0 200 400 600 800 1000 1200 1400 1600 1800 2000
number of clusters
e-m
irro
r
BIL2: Co-occur raw countsBIL2: Co-occur counts from init word-alignBIL1: Two-step optimization
Figure 3: ?-mirror with different settings
methods. The baseline is the standard HMM trans-
lation model defined in Eqn. 2; the HMM1 is de-
fined in Eqn 3, and HMM2 is defined in Eqn 4. The
algorithm is applying our proposed bilingual word
clustering algorithm to infer 1000 clusters for both
languages. As expected, Figure 4 shows that using
F-measure of word alignment
38.00%
39.00%
40.00%
41.00%
42.00%
43.00%
44.00%
45.00%
1 2 3 4 5 6 7HMM Viterbi Iterations
F-m
ea
su
re
Baseline HMM
Extended HMM-1
Extended HMM-2
Figure 4: Word Alignment Over Iterations
word clusters is helpful for word alignment. HMM2
gives the best performance in terms of F-measure of
word alignment. One quarter of the words in the test
vocabulary are unseen as shown in Table 1. These
unseen words related alignment links (4778 out of
14769) will be left unaligned by translation models.
Thus the oracle (best possible) recall we could get
is 67.65%. Our standard t-test showed that signifi-
cant interval is 0.82% at the 95% confidence level.
The improvement at the last iteration of HMM is
marginally significant.
4.4 Applications in Phrase-based Translations
Our pilot word alignment on unseen data showed
improvements. However, we find it more effective
in our phrase extraction, in which three key scores
31
are computed: phrase level fertilities, distortions,
and lexicon scores. These scores are used in a lo-
cal greedy search to extract phrase pairs (Zhao and
Vogel, 2005). This phrase extraction is more sen-
sitive to the differences in P (fj |ei) than the HMM
Viterbi word aligner.
The evaluation conditions are defined in NIST
2003 Small track. Around 247K test set (919 Chi-
nese sentences) specific phrase pairs are extracted
with up to 7-gram in source phrase. A trigram
language model is trained using Gigaword XinHua
news part. With a monotone phrase-based decoder,
the translation results are reported in Table 3. The
Eval. Baseline Bil1 Bil2
NIST 6.417 6.507 6.582
BLEU 0.1558 0.1575 0.1644
Table 3: NIST?03 C-E Small Data Track Evaluation
baseline is using the lexicon P (fj |ei) trained from
standard HMM in Eqn. 2, which gives a BLEU
score of 0.1558 +/- 0.0113. Bil1 and Bil2 are using
P (fj |ei) from HMM in Eqn. 4 with 1000 bilingual
word clusters inferred from the two-step algorithm
and the proposed one respectively. Using the clus-
ters from the two-step algorithm gives a BLEU score
of 0.1575, which is close to the baseline. Using clus-
ters from our algorithm, we observe more improve-
ments with BLEU score of 0.1644 and a NIST score
of 6.582.
5 Discussions and Conclusions
In this paper, a new approach for bilingual word
clustering using eigenstructure in bilingual feature
space is proposed. Eigenvectors from this feature
space are considered as bilingual concepts. Bilin-
gual clusters from the subspaces expanded by these
concepts are inferred with high semantic correla-
tions within each cluster, and high translation quali-
ties across clusters from the two languages.
Our empirical study also showed effectiveness of
using bilingual word clusters in extended HMMs for
statistical machine translation. The K-means based
clustering algorithm can be easily extended to do hi-
erarchical clustering. However, extensions of trans-
lation models are needed to leverage the hierarchical
clusters appropriately.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. In Computational Linguistics, volume 19(2),
pages 263?331.
David Graff. 2003. Ldc gigaword corpora: English gi-
gaword (ldc catalog no: Ldc2003t05). In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
R. Kneser and Hermann Ney. 1993. Improved clus-
tering techniques for class-based statistical language
modelling. In European Conference on Speech Com-
munication and Technology, pages 973?976.
Marina Meila and Jianbo Shi. 2000. Learning segmenta-
tion by random walks. In Advances in Neural Informa-
tion Processing Systems. (NIPS2000), pages 873?879.
A. Ng, M. Jordan, and Y. Weiss. 2001. On spectral
clustering: Analysis and an algorithm. In Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2001.
Franz J. Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation.
In COLING?00: The 18th Int. Conf. on Computational
Linguistics, pages 1086?1090, Saarbrucken, Germany,
July.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 29, pages 19?51.
Franz J. Och. 1999. An efficient method for determin-
ing bilingal word classes. In Ninth Conf. of the Europ.
Chapter of the Association for Computational Linguis-
tics (EACL?99), pages 71?76.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proc. of the Conference on
Empirical Methods in Natural Language Processing.
S. Vogel, Hermann Ney, and C. Tillmann. 1996. Hmm
based word alignment in statistical machine transla-
tion. In Proc. The 16th Int. Conf. on Computational
Lingustics, (Coling?96), pages 836?841.
Yeyi Wang, John Lafferty, and Alex Waibel. 1996.
Word clustering with parallel spoken language cor-
pora. In proceedings of the 4th International Con-
ference on Spoken Language Processing (ICSLP?96),
pages 2364?2367.
Bing Zhao and Stephan Vogel. 2005. A generalized
alignment-free phrase extraction algorithm. In ACL
2005 Workshop: Building and Using Parallel Cor-
pora: Data-driven Machine Translation and Beyond,
Ann Arbor, Michigan.
32
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 208?215,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Training and Evaluating Error Minimization Rules for Statistical Machine
Translation
Ashish Venugopal
School of Computer Science
Carnegie Mellon University
arv@andrew.cmu.edu
Andreas Zollmann
School of Computer Science
Carnegie Mellon University
zollmann@cs.cmu.edu
Alex Waibel
School of Computer Science
Carnegie Mellon University
waibel@cs.cmu.edu
Abstract
Decision rules that explicitly account for
non-probabilistic evaluation metrics in
machine translation typically require spe-
cial training, often to estimate parame-
ters in exponential models that govern the
search space and the selection of candi-
date translations. While the traditional
Maximum A Posteriori (MAP) decision
rule can be optimized as a piecewise lin-
ear function in a greedy search of the pa-
rameter space, the Minimum Bayes Risk
(MBR) decision rule is not well suited to
this technique, a condition that makes past
results difficult to compare. We present a
novel training approach for non-tractable
decision rules, allowing us to compare and
evaluate these and other decision rules on
a large scale translation task, taking ad-
vantage of the high dimensional parame-
ter space available to the phrase based
Pharaoh decoder. This comparison is
timely, and important, as decoders evolve
to represent more complex search space
decisions and are evaluated against in-
novative evaluation metrics of translation
quality.
1 Introduction
State of the art statistical machine translation takes
advantage of exponential models to incorporate a
large set of potentially overlapping features to se-
lect translations from a set of potential candidates.
As discussed in (Och, 2003), the direct translation
model represents the probability of target sentence
?English? e = e1 . . . eI being the translation for a
source sentence ?French? f = f1 . . . fJ through an
exponential, or log-linear model
p?(e|f) =
exp(
?m
k=1 ?k ? hk(e, f))?
e??E exp(
?m
k=1 ?k ? hk(e
?, f))
(1)
where e is a single candidate translation for f
from the set of all English translations E, ? is the
parameter vector for the model, and each hk is a
feature function of e and f . In practice, we restrict
E to the set Gen(f) which is a set of highly likely
translations discovered by a decoder (Vogel et al,
2003). Selecting a translation from this model under
the Maximum A Posteriori (MAP) criteria yields
transl?(f) = argmax
e
p?(e|f) . (2)
This decision rule is optimal under the zero-
one loss function, minimizing the Sentence Error
Rate (Mangu et al, 2000). Using the log-linear
form to model p?(e|f) gives us the flexibility to
introduce overlapping features that can represent
global context while decoding (searching the space
of candidate translations) and rescoring (ranking a
set of candidate translations before performing the
argmax operation), albeit at the cost of the tradi-
tional source-channel generative model of transla-
tion proposed in (Brown et al, 1993).
A significant impact of this paradigm shift, how-
ever, has been the movement to leverage the flex-
ibility of the exponential model to maximize per-
formance with respect to automatic evaluation met-
208
rics. Each evaluation metric considers different as-
pects of translation quality, both at the sentence and
corpus level, often achieving high correlation to hu-
man evaluation (Doddington, 2002). It is clear that
the decision rule stated in (1) does not reflect the
choice of evaluation metric, and substantial work
has been done to correct this mismatch in crite-
ria. Approaches include integrating the metric into
the decision rule, and learning ? to optimize the
performance of the decision rule. In this paper
we will compare and evaluate several aspects of
these techniques, focusing on Minimum Error Rate
(MER) training (Och, 2003) and Minimum Bayes
Risk (MBR) decision rules, within a novel training
environment that isolates the impact of each compo-
nent of these methods.
2 Addressing Evaluation Metrics
We now describe competing strategies to address the
problem of modeling the evaluation metric within
the decoding and rescoring process, and introduce
our contribution towards training non-tractable error
surfaces. The methods discussed below make use
of Gen(f), the approximation to the complete can-
didate translation space E, referred to as an n-best
list. Details regarding n-best list generation from
decoder output can be found in (Ueffing et al, 2002).
2.1 Minimum Error Rate Training
The predominant approach to reconciling the mis-
match between the MAP decision rule and the eval-
uation metric has been to train the parameters ? of
the exponential model to correlate the MAP choice
with the maximum score as indicated by the evalu-
ation metric on a development set with known ref-
erences (Och, 2003). We differentiate between the
decision rule
transl?(f) = argmax
e?Gen(f)
p?(e|f) (3a)
and the training criterion
?? = argmin
?
Loss(transl?(~f),~r) (3b)
where the Loss function returns an evaluation re-
sult quantifying the difference between the English
candidate translation transl?(f) and its correspond-
ing reference r for a source sentence f . We indicate
that this loss function is operating on a sequence of
sentences with the vector notation. To avoid overfit-
ting, and since MT researchers are generally blessed
with an abundance of data, these sentences are from
a separate development set.
The optimization problem (3b) is hard since the
argmax of (3a) causes the error surface to change
in steps in Rm, precluding the use of gradient based
optimization methods. Smoothed error counts can
be used to approximate the argmax operator, but the
resulting function still contains local minima. Grid-
based line search approaches like Powell?s algorithm
could be applied but we can expect difficultly when
choosing the appropriate grid size and starting pa-
rameters. In the following, we summarize the opti-
mization algorithm for the unsmoothed error counts
presented in (Och, 2003) and the implementation de-
tailed in (Venugopal and Vogel, 2005).
? Regard Loss(transl?(~f),~r) as defined in (3b)
as a function of the parameter vector ? to
optimize and take the argmax to compute
transl?(~f) over the translations Gen(f) accord-
ing to the n-best list generated with an initial
estimate ?0.
? The error surface defined by Loss (as a func-
tion of ?) is piecewise linear with respect to a
single model parameter ?k, hence we can deter-
mine exactly where it would be useful (values
that change the result of the argmax) to evalu-
ate ?k for a given sentence using a simple line
intersection method.
? Merge the list of useful evaluation points
for ?k and evaluate the corpus level
Loss(transl?(~f),~r) at each one.
? Select the model parameter that represents the
lowest Loss as k varies, set ?k and consider the
parameter ?j for another dimension j.
This training algorithm, referred to as minimum er-
ror rate (MER) training, is a greedy search in each
dimension of ?, made efficient by realizing that
within each dimension, we can compute the points
at which changes in ? actually have an impact on
Loss. The appropriate considerations for termina-
tion and initial starting points relevant to any greedy
search procedure must be accounted for. From the
209
nature of the training procedure and the MAP de-
cision rule, we can expect that the parameters se-
lected by MER training will strongly favor a few
translations in the n-best list, namely for each source
sentence the one resulting in the best score, moving
most of the probability mass towards the translation
that it believes should be selected. This is due to the
decision rule, rather than the training procedure, as
we will see when we consider alternative decision
rules.
2.2 The Minimum Bayes Risk Decision Rule
The Minimum Bayes Risk Decision Rule as pro-
posed by (Mangu et al, 2000) for the Word Error
Rate Metric in speech recognition, and (Kumar and
Byrne, 2004) when applied to translation, changes
the decision rule in (2) to select the translation that
has the lowest expected loss E[Loss(e, r)], which
can be estimated by considering a weighted Loss
between e and the elements of the n-best list, the
approximation to E, as described in (Mangu et al,
2000). The resulting decision rule is:
transl?(f) = argmin
e?Gen(f)
?
e??Gen(f)
Loss(e, e?)p?(e
?|f) .
(4)
(Kumar and Byrne, 2004) explicitly consider select-
ing both e and a, an alignment between the Eng-
lish and French sentences. Under a phrase based
translation model (Koehn et al, 2003; Marcu and
Wong, 2002), this distinction is important and will
be discussed in more detail. The representation of
the evaluation metric or the Loss function is in the
decision rule, rather than in the training criterion for
the exponential model. This criterion is hard to op-
timize for the same reason as the criterion in (3b):
the objective function is not continuous in ?. To
make things worse, it is more expensive to evalu-
ate the function at a given ?, since the decision rule
involves a sum over all translations.
2.3 MBR and the Exponential Model
Previous work has reported the success of the MBR
decision rule with fixed parameters relating indepen-
dent underlying models, typically including only the
language model and the translation model as fea-
tures in the exponential model.
We extend the MBR approach by developing a
training method to optimize the parameters ? in the
exponential model as an explicit form for the condi-
tional distribution in equation (1). The training task
under the MBR criterion is
?? = argmin
?
Loss(transl?(~f),~r) (5a)
where
transl?(f) = argmin
e?Gen(f)
?
e??Gen(f)
Loss(e, e?)p?(e
?|f) .
(5b)
We begin with several observations about this opti-
mization criterion.
? The MAP optimal ?? are not the optimal para-
meters for this training criterion.
? We can expect the error surface of the MBR
training criterion to contain larger sections of
similar altitude, since the decision rule empha-
sizes consensus.
? The piecewise linearity observation made in
(Papineni et al, 2002) is no longer applicable
since we cannot move the log operation into the
expected value.
3 Score Sampling
Motivated by the challenges that the MBR training
criterion presents, we present a training method that
is based on the assumption that the error surface is
locally non-smooth but consists of local regions of
similar Loss values. We would like to focus the
search within regions of the parameter space that re-
sult in low Loss values, simulating the effect that
the MER training process achieves when it deter-
mines the merged error boundaries across a set of
sentences.
Let Score(?) be some function of
Loss(transl?(~f),~r) that is greater or equal
zero, decreases monotonically with Loss, and for
which
?
(Score(?) ? min?? Score(??))d? is finite;
e.g., 1 ? Loss(transl?(~f),~r) for the word-error
rate (WER) loss and a bounded parameter space.
While sampling parameter vectors ? and estimating
Loss in these points, we will constantly refine
our estimate of the error surface and thereby of
the Score function. The main idea in our score
210
sampling algorithm is to make use of this Score
estimate by constructing a probability distribution
over the parameter space that depends on the Score
estimate in the current iteration step i and sample
the parameter vector ?i+1 for the next iteration from
that distribution. More precisely, let S?c
(i)
be the
estimate of Score in iteration i (we will explain how
to obtain this estimate below). Then the probability
distribution from which we sample the parameter
vector to test in the next iteration is given by:
p(?) =
S?c
(i)
(?) ? min?? S?c
(i)
(??)
?
(S?c
(i)
(?) ? min?? S?c
(i)
(??)) d?
. (6)
This distribution produces a sequence ?1, . . . , ?n of
parameter vectors that are more concentrated in ar-
eas that result in a high Score. We can select the
value from this sequence that generates the highest
Score, just as in the MER training process.
The exact method of obtaining the Score estimate
S?c is crucial: If we are not careful enough and guess
too low values of S?c(?) for parameter regions that
are still unknown to us, the resulting sampling dis-
tribution p might be zero in those regions and thus
potentially optimal parameters might never be sam-
pled. Rather than aiming for a consistent estimator
of Score (i.e., an estimator that converges to Score
when the sample size goes to infinity), we design S?c
with regard to yielding a suitable sampling distribu-
tion p.
Assume that the parameter space is bounded such
that mink ? ?k ? maxk for each dimension k,
We then define a set of pivots P , forming a grid of
points in Rm that are evenly spaced between mink
and maxk for each dimension k. Each pivot repre-
sents a region of the parameter space where we ex-
pect generally consistent values of Score. We do not
restrict the values of ?m to be at these pivot points
as a grid search would do, rather we treat the pivots
as landmarks within the search space.
We approximate the distribution p(?) with the
discrete distribution p(? ? P), leaving the problem
of estimating |P| parameters. Initially, we set p to
be uniform, i.e., p(0)(?) = 1/|P|. For subsequent
iterations, we now need an estimate of Score(?) for
each pivot ? ? P in the discrete version of equation
(6) to obtain the new sampling distribution p. Each
iteration i proceeds as follows.
? Sample ??i from the discrete distribution
p(i?1)(? ? P) obtained by the previous it-
eration.
? Sample the new parameter vector ?i by choos-
ing for each k ? {1, . . . ,m}, ?ik := ??ik + ?k,
where ?k is sampled uniformly from the inter-
val (?dk/2, dk/2) and dk is the distance be-
tween neighboring pivot points along dimen-
sion k. Thus, ?i is sampled from a region
around the sampled pivot.
? Evaluate Score(?i) and distribute this score to
obtain new estimates S?c
(i)
(?) for all pivots ? ?
P as described below.
? Use the updated estimates S?c
(i)
to generate the
sampling distribution p(i) for the next iteration
according to
p(i)(?) =
S?c
(i)
(?) ? min?? S?c
(i)
(??)
?
??P(S?c
(i)
(?) ? min?? S?c
(i)
(??))
.
The score Score(?i) of the currently evaluated pa-
rameter vector does not only influence the score es-
timate at the pivot point of the respective region, but
the estimates at all pivot points. The closest piv-
ots are influenced most strongly. More precisely, for
each pivot ? ? P , S?c
(i)
(?) is a weighted average
of Score(?1), . . . , Score(?i), where the weights
w(i)(?) are chosen according to
w(i)(?) = infl(i)(?) ? corr(i)(?) with
infl(i)(?) = mvnpdf(?, ?i,?) and
corr(i)(?) = 1/p(i?1)(?) .
Here, mvnpdf(x, ?,?) denotes the m-dimensional
multivariate-normal probability density function
with mean ? and covariance matrix ?, evaluated
at point x. We chose the covariance matrix ? =
diag(d21, . . . , d
2
m), where again dk is the distance be-
tween neighboring grid points along dimension k.
The term infl(i)(?) quantifies the influence of the
evaluated point ?i on the pivot ?, while corr(i)(?)
is a correction term for the bias introduced by hav-
ing sampled ?i from p(i?1).
211
Smoothing uncertain regions In the beginning of
the optimization process, there will be pivot regions
that have not yet been sampled from and for which
not even close-by regions have been sampled yet.
This will be reflected in the low sum of influence
terms infl(1)(?) + ? ? ? + infl(i)(?) of the respective
pivot points ?. It is therefore advisable to discount
some probability mass from p(i) and distribute it
over pivots with low influence sums (reflecting low
confidence in the respective score estimates) accord-
ing to some smoothing procedure.
4 N-Best lists in Phrase Based Decoding
The methods described above make extensive use of
n-best lists to approximate the search space of can-
didate translations. In phrase based decoding we of-
ten interpret the MAP decision rule to select the top
scoring path in the translation lattice. Selecting a
particular path means in fact selecting the pair ?e, s?,
where s is a segmentation of the the source sentence
f into phrases and alignments onto their translations
in e. Kumar and Byrne (2004) represent this deci-
sion explicitly, since the Loss metrics considered in
their work evaluate alignment information as well as
lexical (word) level output. When considering lexi-
cal scores as we do here, the decision rule minimiz-
ing 0/1 loss actually needs to take the sum over all
potential segmentations that can generate the same
word sequence. In practice, we only consider the
high probability segmentation decisions, namely the
ones that were found in the n-best list. This gives
the 0/1 loss criterion shown below.
transl?(f) = argmax
e
?
s
p?(e, s|f) (7)
The 0/1 loss criterion favors translations that are
supported by several segmentation decisions. In the
context of phrase-based translations, this is a useful
criterion, since a given lexical target word sequence
can be correctly segmented in several different ways,
all of which would be scored equally by an evalua-
tion metric that only considers the word sequence.
5 Experimental Framework
Our goal is to evaluate the impact of the three de-
cision rules discussed above on a large scale trans-
lation task that takes advantage of multidimensional
features in the exponential model. In this section
we describe the experimental framework used in this
evaluation.
5.1 Data Sets and Resources
We perform our analysis on the data provided by the
2005 ACL Workshop in Exploiting Parallel Texts for
Statistical Machine Translation, working with the
French-English Europarl corpus. This corpus con-
sists of 688031 sentence pairs, with approximately
156 million words on the French side, and 138 mil-
lion words on the English side. We use the data as
provided by the workshop and run lower casing as
our only preprocessing step. We use the 15.5 mil-
lion entry phrase translation table as provided for the
shared workshop task for the French-English data
set. Each translation pair has a set of 5 associated
phrase translation scores that represent the maxi-
mum likelihood estimate of the phrase as well as in-
ternal alignment probabilities. We also use the Eng-
lish language model as provided for the shared task.
Since each of these decision rules has its respective
training process, we split the workshop test set of
2000 sentences into a development and test set using
random splitting. We tried two decoders for trans-
lating these sets. The first system is the Pharaoh de-
coder provided by (Koehn et al, 2003) for the shared
data task. The Pharaoh decoder has support for mul-
tiple translation and language model scores as well
as simple phrase distortion and word length models.
The pruning and distortion limit parameters remain
the same as in the provided initialization scripts,
i.e., DistortionLimit = 4, BeamThreshold =
0.1, Stack = 100. For further information on
these parameter settings, confer (Koehn et al, 2003).
Pharaoh is interesting for our optimization task be-
cause its eight different models lead to a search
space with seven free parameters. Here, a princi-
pled optimization procedure is crucial. The second
decoder we tried is the CMU Statistical Translation
System (Vogel et al, 2003) augmented with the four
translation models provided by the Pharaoh system,
in the following called CMU-Pharaoh. This system
also leads to a search space with seven free parame-
ters.
212
5.2 N-Best lists
As mentioned earlier, the model parameters ? play
a large role in the search space explored by a prun-
ing beam search decoder. These parameters affect
the histogram and beam pruning as well as the fu-
ture cost estimation used in the Pharaoh and CMU
decoders. The initial parameter file for Pharaoh pro-
vided by the workshop provided a very poor esti-
mate of ?, resulting in an n-best list of limited po-
tential. To account for this condition, we ran Min-
imum Error Rate training on the development data
to determine scaling factors that can generate a n-
best list with high quality translations. We realize
that this step biases the n-best list towards the MAP
criteria, since its parameters will likely cause more
aggressive pruning. However, since we have cho-
sen a large N=1000, and retrain the MBR, MAP, and
0/1 loss parameters separately, we do not feel that
the bias has a strong impact on the evaluation.
5.3 Evaluation Metric
This paper focuses on the BLEU metric as presented
in (Papineni et al, 2002). The BLEU metric is de-
fined on a corpus level as follows.
Score(~e,~r) = BP (~e,~r) ? exp(
1
N
N?
1
(log pn))
where pn represent the precision of n-grams sug-
gested in ~e and BP is a brevity penalty measur-
ing the relative shortness of ~e over the whole cor-
pus. To use the BLEU metric in the candidate pair-
wise loss calculation in (4), we need to make a de-
cision regarding cases where higher order n-grams
matches are not found between two candidates. Ku-
mar and Byrne (2004) suggest that if any n-grams
are not matched then the pairwise BLEU score is set
to zero. As an alternative we first estimate corpus-
wide n-gram counts on the development set. When
the pairwise counts are collected between sentences
pairs, they are added onto the baseline corpus counts
to and scored by BLEU. This scoring simulates the
process of scoring additional sentences after seeing
a whole corpus.
5.4 Training Environment
It is important to separate the impact of the decision
rule from the success of the training procedure. To
appropriately compare the MAP, 0/1 loss and MBR
decisions rules, they must all be trained with the
same training method, here we use the Score Sam-
pling training method described above. We also re-
port MAP scores using the MER training described
above to determine the impact of the training algo-
rithm for MAP. Note that the MER training approach
cannot be performed on the MBR decision rule, as
explained in Section 2.3. MER training is initialized
at random values of ? and run (successive greedy
search over the parameters) until there is no change
in the error for three complete cycles through the pa-
rameter set. This process is repeated with new start-
ing parameters as well as permutations of the para-
meter search order to ensure that there is no bias in
the search towards a particular parameter. To im-
prove efficiency, pairwise scores are cached across
requests for the score at different values of ?, and
for MBR only the E[Loss(e, r)] for the top twenty
hypotheses as ranked by the model are computed.
6 Results
The results in Table 1 compare the BLEU score
achieved by each training method on the develop-
ment and test data for both Pharaoh and CMU-
Pharaoh. Score-sampling training was run for 150
iterations to find ? for each decision rule. The MAP-
MER training was performed to evaluate the effect
of the greedy search method on the generalization
of the development set results. Each row represents
an alternative training method described in this pa-
per, while the test set columns indicate the criteria
used to select the final translation output ~e. The
bold face scores are the scores for matching train-
ing and testing methods. The underlined score is
the highest test set score, achieved by MBR decod-
ing using the CMU-Pharaoh system trained for the
MBR decision rule with the score-sampling algo-
rithm. When comparing MER training for MAP-
decoding with score-sampling training for MAP-
decoding, score-sampling surprisingly outperforms
MER training for both Pharaoh and CMU-Pharaoh,
although MER training is specifically tailored to
the MAP metric. Note, however, that our score-
sampling algorithm has a considerably longer run-
ning time (several hours) than the MER algorithm
(several minutes). Interestingly, within MER train-
213
training method Dev. set sc. test set sc. MAP test set sc. 0/1 loss test set sc. MBR
MAP MER (Pharaoh) 29.08 29.30 29.42 29.36
MAP score-sampl. (Pharaoh) 29.08 29.41 29.24 29.30
0/1 loss sc.-s. (Pharaoh) 29.08 29.16 29.28 29.30
MBR sc.-s. (Pharaoh) 29.00 29.11 29.08 29.17
MAP MER (CMU-Pharaoh) 28.80 29.02 29.41 29.60
MAP sc.-s. (CMU-Ph.) 29.10 29.85 29.75 29.55
0/1 loss sc.-s. (CMU-Ph.) 28.36 29.97 29.91 29.72
MBR sc.-s. (CMU-Ph.) 28.36 30.18 30.16 30.28
Table 1. Comparing BLEU scores generated by alternative training methods and decision rules
ing for Pharaoh, the 0/1 loss metric is the top per-
former; we believe the reason for this disparity be-
tween training and test methods is the impact of
phrasal consistency as a valuable measure within the
n-best list.
The relative performance of MBR score-sampling
w.r.t. MAP and 0/1-loss score sampling is quite dif-
ferent between Pharaoh and CMU-Pharaoh: While
MBR score-sampling performs worse than MAP
and 0/1-loss score sampling for Pharaoh, it yields the
best test scores across the board for CMU-Pharaoh.
A possible reason is that the n-best lists generated by
Pharaoh have a large percentage of lexically iden-
tical translations, differing only in their segmenta-
tions. As a result, the 1000-best lists generated by
Pharaoh contain only a small percentage of unique
translations, a condition that reduces the potential
of the Minimum Bayes Risk methods. The CMU
decoder, contrariwise, prunes away alternatives be-
low a certain score-threshold during decoding and
does not recover them when generating the n-best
list. The n-best lists of this system are therefore typi-
cally more diverse and in particular contain far more
unique translations.
7 Conclusions and Further Work
This work describes a general algorithm for the ef-
ficient optimization of error counts for an arbitrary
Loss function, allowing us to compare and evalu-
ate the impact of alternative decision rules for sta-
tistical machine translation. Our results suggest
the value and sensitivity of the translation process
to the Loss function at the decoding and reorder-
ing stages of the process. As phrase-based trans-
lation and reordering models begin to dominate
the state of the art in machine translation, it will
become increasingly important to understand the
nature and consistency of n-best list training ap-
proaches. Our results are reported on a complete
package of translation tools and resources, allow-
ing the reader to easily recreate and build upon our
framework. Further research might lie in finding
efficient representations of Bayes Risk loss func-
tions within the decoding process (rather than just
using MBR to rescore n-best lists), as well as
analyses on different language pairs from the avail-
able Europarl data. We have shown score sam-
pling to be an effective training method to con-
duct these experiments and we hope to establish its
use in the changing landscape of automatic trans-
lation evaluation. The source code is available at:
www.cs.cmu.edu/?zollmann/scoresampling/
8 Acknowledgments
We thank Stephan Vogel, Ying Zhang, and the
anonymous reviewers for their valuable comments
and suggestions.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In In Proc. ARPA Workshop on Human Lan-
guage Technology.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
214
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT/NAACL), Boston,MA,
May 27-June 1.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. CoRR, cs.CL/0010012.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing, Philadephia, PA,
July 6-7.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Associ-
ation for Computational Linguistics, Sapporo, Japan,
July 6-7.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Association of Computational Linguistics, pages 311?
318.
Nicola Ueffing, Franz Josef Och, and Hermann Ney.
2002. Generation of word graphs in statistical ma-
chine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing,
Philadephia, PA, July 6-7.
Ashish Venugopal and Stephan Vogel. 2005. Consider-
ations in mce and mmi training for statistical machine
translation. In Proceedings of the Tenth Conference
of the European Association for Machine Translation
(EAMT-05), Budapest, Hungary, May. The European
Association for Machine Translation.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical translation system. In Pro-
ceedings of MT Summit IX, New Orleans, LA, Septem-
ber.
215
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80?84,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The Universita?t Karlsruhe Translation System for the EACL-WMT 2009
Jan Niehues, Teresa Herrmann, Muntsin Kolss and Alex Waibel
Universita?t Karlsruhe (TH)
Karlsruhe, Germany
{jniehues,therrman,kolss,waibel}@ira.uka.de
Abstract
In this paper we describe the statistical
machine translation system of the Univer-
sita?t Karlsruhe developed for the transla-
tion task of the Fourth Workshop on Sta-
tistical Machine Translation. The state-of-
the-art phrase-based SMT system is aug-
mented with alternative word reordering
and alignment mechanisms as well as op-
tional phrase table modifications. We par-
ticipate in the constrained condition of
German-English and English-German as
well as in the constrained condition of
French-English and English-French.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT?09 Shared
Translation Task and the particular language-pair-
dependent variations of the system. We use stan-
dard alignment and training tools and a phrase-
based SMT decoder for creating state-of-the-art
MT systems for our contribution in the transla-
tion directions English-German, German-English,
English-French and French-English.
Depending on the language pair, the baseline
system is augmented with part-of-speech (POS)-
based short-range and long-range word reordering
models, discriminative word alignment (DWA)
and several modifications of the phrase table. Ex-
periments with different system variants were con-
ducted including some of those additional system
components. Significantly better translation re-
sults could be achieved compared to the baseline
results.
An overview of the system will follow in Sec-
tion 2, which describes the baseline architecture,
followed by descriptions of the additional system
components. Translation results for the different
languages and system variants are presented in
Section 5.
2 Baseline System
The core of our system is the STTK decoder (Vo-
gel, 2003), a phrase-based SMT decoder with a
local reordering window of 2 words. The de-
coder generates a translation for the input text
or word lattice by searching translation model
and language model for the hypothesis that max-
imizes phrase translation probabilities and target
language probabilities. The translation model, i.e.
the SMT phrase table is created during the training
phase by a modified version of the Moses Toolkit
(Koehn et al, 2007) applying GIZA++ for word
alignment. Language models are built using the
SRILM Toolkit. The POS-tags for the reorder-
ing models were generated with the TreeTagger
(Schmid, 1994) for all languages.
2.1 Training, Development and Test Data
We submitted translations for the English-
German, German-English, English-French and
French-English tasks. All systems were trained
on the Europarl and News Commentary corpora
using the Moses Toolkit and apply 4-gram lan-
guage models created from the respective mono-
lingual News corpora. All feature weights are au-
tomatically determined and optimized with respect
to BLEU via MERT (Venugopal et al, 2005).
For development and testing we used data pro-
vided by the WMT?09, news-dev2009a and news-
dev2009b, consisting of 1026 sentences each.
3 Word Reordering Model
One part of our system that differs from the base-
line system is the reordering model. To account
for the different word orders in the languages, we
used the POS-based reordering model presented in
Rottmann and Vogel (2007). This model learns
rules from a parallel text to reorder the source side.
The aim is to generate a reordered source side that
can be translated in a more monotone way.
80
In this framework, first, reordering rules are
extracted from an aligned parallel corpus and
POS information is added to the source side.
These rules are of the form VVIMP VMFIN PPER
? PPER VMFIN VVIMP and describe how the
source side has to be reordered to match the tar-
get side. Then the rules are scored according to
their relative frequencies.
In a preprocessing step to the actual decoding
different reorderings of the source sentences are
encoded in a word lattice. Therefore, for all re-
ordering rules that can be applied to a sentence the
resulting reorderings are added to the lattice if the
score is better than a given threshold. The decod-
ing is then performed on the resulting word lattice.
This approach does model the reordering well
if only short-range reorderings occur. But espe-
cially when translating from and to German, there
are also long-range reorderings that require the
verb to be shifted nearly across the whole sen-
tence. During this shift of the verb, the rest of
the sentence remains mainly unchanged. It does
not matter which words are in between, since they
are moved as a whole. Furthermore, rules in-
cluding an explicit sequence of POS-tags spanning
the whole sentence would be too specific. A lot
more rules would be needed to cover long-range
reorderings with each rule being applicable only
very sparsely. Therefore, we model long-range re-
ordering by generalizing over the unaffected se-
quences and introduce rules with gaps. (For more
details see Niehues and Kolss (2009)). These are
learned in a way similar to the other type of re-
ordering rules described above, but contain a gap
representing one or several arbitrary words. It is,
for example, possible to have the following rule
VAFIN * VVPP ? VAFIN VVPP *, which puts
both parts of the German verb next to each other.
4 Translation Model
The translation models of all systems we submit-
ted differ in some parts from the baseline system.
The main changes done will be described in this
section.
4.1 Word Alignment
The baseline method for creating the word align-
ment is to create the GIZA++ alignments in both
directions and then to combine both alignments
using a heuristic, e.g. grow-diag-final-and heuris-
tic, as provided by the Moses Toolkit. In some
of the submitted systems we used a discrimina-
tive word alignment model (DWA) to generate
the alignments as described in Niehues and Vogel
(2008) instead. This model is trained on a small
amount of hand-aligned data and uses the lexical
probability as well as the fertilities generated by
the GIZA++ Toolkit and POS information. We
used all local features, the GIZA and indicator fer-
tility features as well as first order features for 6
directions. The model was trained in three steps,
first using the maximum likelihood optimization
and afterwards it was optimized towards the align-
ment error rate. For more details see Niehues and
Vogel (2008).
4.2 Phrase Table Smoothing
The relative frequencies of the phrase pairs are a
very important feature of the translation model,
but they often overestimate rare phrase pairs.
Therefore, the raw relative frequency estimates
found in the phrase translation tables are smoothed
by applying modified Kneser-Ney discounting as
described in Foster et al (2006).
4.3 Lattice Phrase Extraction
For the test sentences the POS-based reordering
allows us to change the word order in the source
sentence, so that the sentence can be translated
more easily. But this approach does not reorder
the training sentences. This may cause problems
for phrase extraction, especially for long-range re-
orderings. For example, if the English verb is
aligned to both parts of the German verb, this
phrase can not be extracted, since it is not contin-
uous on the German side. In the case of German
as source language, the phrase could be extracted
if we also reorder the training corpus.
Therefore, we build lattices that encode the
different reorderings for every training sentence.
Then we can not only extract phrase pairs from the
monotone source path, but also from the reordered
paths. So it would be possible to extract the ex-
ample mentioned before, if both parts of the verb
were put together by a reordering rule. To limit
the number of extracted phrase pairs, we extract
a source phrase only once per sentence even if it
may be found on different paths. Furthermore, we
do not use the weights in the lattice.
If we use the same rules as for the test sets,
the lattice would be so big that the number of ex-
tracted phrase pairs would be still too high. As
mentioned before, the word reordering is mainly
81
a problem at the phrase extraction stage if one
word is aligned to two words which are far away
from each other in the sentence. Therefore, the
short-range reordering rules do not help much in
this case. So, only the long-range reordering rules
were used to generate the lattice for the training
corpus. This already leads to an increase of the
number of source phrases in the filtered phrase ta-
ble from 724K to 971K. The number of phrase
pairs grows from 5.1M to 6.7M.
4.4 Phrase Table Adaption
For most of the different tasks there was a huge
amount of parallel out-of-domain training data
available, but only a much smaller amount of in-
domain training data. Therefore, we tried to adapt
our system to the in-domain data. We want to
make use of the big out-of-domain data, but do
not want to lose the information encoded in the in-
domain data.
To achieve this, we built an additional phrase
table trained only on the in-domain data. Since
the word alignment does not depend heavily on the
domain we used the same word alignment. Then
we combined both phrase tables in the following
way. A phrase pair with features ? from the first
phrase table is added to the combined one with
features < ?, 1 >, where 1 is a vector of ones with
length equal to the number of features in the other
phrase table. The phrase pairs of the other phrase
table were added with the features < 1, ? >.
5 Results
We submitted system translations for the English-
German, German-English, English-French and
French-English task. Their performance is mea-
sured applying the BLEU metric. All BLEU
scores are computed on the lower-cased transla-
tions.
5.1 English-German
The system translating from English to German
was trained on the data described in Section 2.1.
The first system already uses the POS-based re-
ordering model for short-range reorderings. The
results of the different systems are shown in Ta-
ble 1.
We could improve the translation quality on the
test set by using the smoothed relative frequen-
cies in the phrase table as described before and
by adapting the phrase table. Then we used the
discriminative word alignment to generate a new
word alignment. For the training of the model
we used 500 hand-aligned sentences from the Eu-
roparl corpus. By training a translation model
based on this word alignment we could improve
the translation quality further. At last we added
the model for long-range reorderings, which per-
forms best on the test set.
The improvement achieved by smoothing is sig-
nificant at a level of 5%, the remaining changes are
not significant on their own. In all language pairs,
the problem occurs that some features do not lead
to an improvement on the development set, but on
the test set. One reason for this may be that the
development set is quite small.
Table 1: Translation results for English-German
(BLEU Score)
System Dev Test
Short-range 13.96 14.99
+ Smoothing 14.36 15.38
+ Adaptation 13.96 15.44
+ Discrim. WA 14.45 15.61
+ Long-range reordering 14.58 15.70
5.2 German-English
The German-English system was trained on the
same data as the English-German except that we
perform compound splitting as an additional pre-
processing step. The compound splitting was
done with the frequency-based method described
in Koehn et al (2003). For this language di-
rection, the initial system already uses phrase ta-
ble smoothing, adaptation and discriminative word
alignment, in addition to the techniques of the
English-German baseline system. The results are
shown in Table 2.
For this language pair, we could improve the
translation quality, first, by adding the long-range
reordering model. Further improvements could be
achieved by using lattice phrase extraction as de-
scribed before.
5.3 English-French
For creating the English-French translations, first,
the baseline system as described in Section 2
was used. This baseline was then augmented
with phrase table smoothing, short-range word re-
ordering and phrase table adaptation as described
above. In addition, the adapted phrase table was
82
Table 2: Translation results for German-English
(BLEU Score)
System Dev Test
Initial System 20.52 22.01
+ Long-range reordering 21.04 22.36
+ Lattice phrase extraction 20.69 22.64
postprocessed such that phrase table entries in-
clude the same amount of punctuation marks, es-
pecially quotation marks, in both source and tar-
get phrase. In contrast to the English?German
language pairs, the word reordering required
in English?French translations are restricted to
rather local word shifts which can be covered by
the short-range reordering feature. Applying addi-
tional long-range reordering is scarcely expected
to yield further improvements for these language
pairs and was not applied specifically in this task.
Table 3 shows the results of the system variants.
Table 3: Translation results for English-French
(BLEU Score)
System Dev Test
Baseline 20.97 20.87
+ Smoothing 21.42 21.32
+ Short-range reordering 20.79 22.26
+ Adaptation 21.05 21.97
+ cleanPT 21.50 21.98
Both on development and test set, smoothing
the probabilities in the phrase table resulted in an
increase of nearly 0.5 BLEU points. Applying
short-range word reordering did not lead to an im-
provement on the development set. However, the
increase in BLEU on the test set is substantial. The
opposite is the case when adapting the phrase ta-
ble: While phrase table adaptation improves the
translation quality on the development set, adapta-
tion leads to lower scores on the test set.
Thus, the system configuration that performed
best on the test set applies phrase table smoothing
and short-range word reordering. For creating the
translations for our submission, this configuration
was used.
5.4 French-English
For the French-English task, similar experiments
have been conducted. With respect to the base-
line system, improvements in translation quality
could be measured when applying phrase table
smoothing. An increase of 0.43 BLEU points was
achieved using short-range word reordering. Ad-
ditional experiments with adapting the phrase ta-
ble to the domain of the test set led to further im-
provement. Submissions for the shared task were
created using the system including all mentioned
features.
Table 4: Translation results for French-English
(BLEU Score)
System Dev Test
Baseline 21.29 22.41
+ Smoothing 21.55 22.59
+ Short-range reordering 22.55 23.02
+ Adaptation 21.72 23.20
+ cleanPT 22.60 23.21
6 Conclusions
We have presented our system for the WMT?09
Shared Translation Task. The submissions for the
language pairs English-German, German-English,
English-French and French-English have been
created by the STTK decoder applying different
additional methods for each individual language
pair to enhance translation quality.
Word reordering models covering short-
range reordering for the English?French and
English?German and long-range reordering for
English?German respectively proved to result in
better translations.
Smoothing the phrase probabilities in the phrase
table also increased the scores in all cases, while
adapting the phrase table to the test domain only
showed a positive influence on translation quality
in some of our experiments. Further tuning of the
adaptation procedure could help to clarify the ben-
efit of this method.
Using discriminative word alignment as an
alternative to performing word alignment with
GIZA++ did also improve the systems translating
between English and German. Future experiments
will be conducted applying discriminative word
alignment also in the English?French systems.
Acknowledgments
This work was partly supported by Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
83
References
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proc. of Empirical Methods in
Natural Language Processing. Sydney, Australia.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
HLT/NAACL 2003. Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of Second ACL Workshop on Statistical Ma-
chine Translation. Prague, Czech Republic.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation. Columbus, OH, USA.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proc. of Forth ACL Workshop on Statistical Machine
Translation. Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI. Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing. Manchester, UK.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimiza-
tion Rules for Statistical Machine Translation. In
Proc. of ACL 2005, Workshop on Data-drive Ma-
chine Translation and Beyond (WPT-05). Ann Ar-
bor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In NLP-KE?03. Beijing, China.
84
Activity detection for information access
to oral communication
Klaus Ries and Alex Waibel?
{ries|ahw}@cs.cmu.edu
Interactive Systems Labs, Carnegie Mellon University, Pittsburgh, PA, 15213, USA
Interactive Systems Labs, Universita?t Karlsruhe, Fakulta?t fu?r Informatik, 76128 Karlsruhe, Germany
http://www.is.cs.cmu.edu/ http://werner.ira.uka.de
ABSTRACT
Oral communication is ubiquitous and carries important in-
formation yet it is also time consuming to document. Given
the development of storage media and networks one could
just record and store a conversation for documentation. The
question is, however, how an interesting information piece
would be found in a large database. Traditional informa-
tion retrieval techniques use a histogram of keywords as the
document representation but oral communication may offer
additional indices such as the time and place of the rejoinder
and the attendance. An alternative index could be the ac-
tivity such as discussing, planning, informing, story-telling,
etc. This paper addresses the problem of the automatic de-
tection of those activities in meeting situation and everyday
rejoinders. Several extensions of this basic idea are being
discussed and/or evaluated: Similar to activities one can
define subsets of larger database and detect those automati-
cally which is shown on a large database of TV shows. Emo-
tions and other indices such as the dominance distribution
of speakers might be available on the surface and could be
used directly. Despite the small size of the databases used
some results about the effectiveness of these indices can be
obtained.
Keywords
activity, dialogue processing, oral communication, speech,
information access
?We would like to thank our lab, especially Klaus Zechner,
Alon Lavie and Lori Levin for their discussions and support.
We would also like to thank our sponsors at DARPA. Any
opinions, findings and conclusions expressed in this material
are those of the authors and may not reflect the views of
DARPA, or any other party.
.
1. INTRODUCTION
Information access to oral communication is becoming an
interesting research area since recording, storing and trans-
mitting large amounts of audio (and video) data is feasible
today. While written information is often available elec-
tronically (especially since it is typically entered on com-
puters) oral communication is usually only documented by
constructing a new document in written form such as a tran-
script (court proceedings) or minutes (meetings). Oral com-
munications are therefore a large untapped resource, espe-
cially if no corresponding written documents are available
and the cost of documentation using traditional techniques
is considered high: Tutorial introductions by a senior staff
member might be worthwhile to attend by many newcomers,
office meetings may contain informations relevant for oth-
ers and should be reproducable, informal and formal group
meetings may be interesting but not fully documented. In
essence the written form is already a reinterpretation of the
original rejoinder. Such a reinterpretation are used to
? extract and condense information
? add or delete information
? change the meaning
? cite the rejoinder
? relate rejoinders to each other
Reinterpretation is a time consuming, expensive and op-
tional step and written documentation is combining reinter-
pretation and documentation step in one 1. If however rein-
terpretation is not necessary or unwanted a system which
is producing audiovisual records is superior. If reinterpreta-
tion is wanted or needed a system using audiovisual records
may be used to improve the reinterpretation by adding all
audiovisual data and the option to go back to the unaltered
original. Whether reinterpretation is done or not it is cru-
cial to be able to navigate effectively within an audiovisual
document and to find a specific document.
1The most important exception is the literal courtroom
transcript, however one could argue that even transcripts
are reinterpretations since they do not contain a number of
informations present in the audio channel such as emotions,
hesitations, the use of slang and certain types of hetereglos-
sia, accents and so forth. This is specifically true if tran-
scription machines are used which restrict the transcriber
to standard orthography.
Willie is sickNeed new coding schemePersonal stuff
Setting up the new hard drive
Data collection, last WednesdayDialogue detection, with HansLanguage modeling tutorial for Tim
MeetingsTV showsLecturesSpeechesMeeting
Database
Segment
Figure 1: Information access hierarchy: Oral com-
munications take place in very different formats and
the first step in the search is to determine the
database (or sub-database) of the rejoinder. The
next step is to find the specific rejoinder. Since re-
joinders can be very long the rejoinder has to seg-
mented and a segment has to be selected.
While keywords are commonly used in information ac-
cess to written information the use of other indices such as
style is still uncommon (but see Kessler et al (1997); van
Bretan et al (1998)). Oral communication is richer than
written communication since it is an interactive real time
accomplishment between participants, may involve speech
gestures such as the display of emotion and is situated in
space and time. Bahktin (1986) characterizes a conversa-
tion by topic, situation and style. Information access to
oral communication can therefore make use of indices that
pertain to the oral nature of the discourse (Fig. 2). In-
dices other than topic (represented by keywords) increase
in importance since browsing audio documents is cumber-
some which makes the common interactive retrieval strategy
?query, browse, reformulate? less effective. Finally the topic
may not be known at all or may not be that relevant for the
query formulation, for example if one just wants to be re-
minded what was being discussed last time a person was
met. Activities are suggested as an alternative index and
are a description of the type of interaction. It is common
to use ?action-verbs? such as story-telling, discussing, plan-
ning, informing, etc. to describe activities 2. Items similar
to activities have been shown to be directly retrievable from
autobiographic memory (Herrmann, 1993) and are therefore
indices that are available to participants of the conversation.
Other indices may be very effective but not available: The
frequency of the word ?I? in the conversation, the histogram
of word lengths or the histogram of pitch per participant.
In Fig. 1 the information access hierarchy is being intro-
duced which allows to understand the problem of informa-
tion access to oral communication at different levels. In
Ries (1999) we have shown that the detection of general di-
2 The definition of activities such as planning may vary
vastly across general dialogue genres, for example compare
a military combat situation with a mother child interaction.
However it is often possible to develop activities and dia-
logue typologies for a specific dialogue genre. The related
problem of general typologies of dialogues is still far from
being settled and action-verbs are just one potential catego-
rization (Fritz and Hundschnur, 1994).
SpeakerTimeLocation
Related rejonders
Parts of speechEmotion
Overlap between speakers
Semantics / pragmaticsKeywords
SituationStyle
Topic
Figure 2: Bahktin?s characterization of dialogue:
Bahktin (1986) describes a discourse along the three
major properties style, situation and topic. Current
information retrieval systems focus on the topical as-
pect which might be crucial in written documents.
Furthermore, since throughout text analysis is still a
hard problem, information retrieval has mostly used
keywords to characterize topic. Many features that
could be extracted are therefore ignored in a tradi-
tional keyword based approach.
alogue genre (database level in Fig. 1) can be done with
high accuracy if a number of different example types have
been annotated; in Ries et al (2000) we have shown that it
is hard but not impossible to distinguish activities in per-
sonal phone calls (segment level in Fig. 1) . In this paper
we will address activities in meetings and other types of di-
alogues and show that these activities can be distinguished
using certain features and a neural network based classifier
(Sec. 2, segment level in Fig. 1). The concept of information
retrieval assessment using information theoretic measures is
applied to this task (Sec. 3). Additionally we will introduce
a level somewhat below the database level in Fig. 1 that we
call ?sub-genre? and we have collected a large database of
TV-shows that are automatically classified for their show-
type (Sec. 4). We also explore whether there are other in-
dices similar to activities that could be used and we are
presenting results on emotions in meetings (Sec. 5).
2. ACTIVITY DETECTION
We are interested in the detection of activities that are
described by action verbs and have annotated those in two
databases:
meetings have been collected at Interactive Systems Labs at
CMU (Waibel et al, 1998) and a subset of 8 meetings
has been annotated. Most of the meetings are by the
data annotation group itself and are fairly informal in
style. The participants are often well acquainted and
meet each other a lot besides their meetings.
Santa Barbara (SBC) is a corpus released by the LDC
and 7 out of 12 rejoinders have been annotated.
The annotator has been instructed to segment the rejoin-
ders into units that are coherent with respect to their topic
Activity SBC Meeting
Discussion 35 58
Information 25 23
Story-telling 24 10
Planning 7 19
Undetermined 5 8
Advising 5 17
Not meeting 3 2
Interrogation 2 1
Evaluation 1 0
Introduction 0 1
Closing 0 1
Table 1: Distribution of activity types: Both
databases contain a lot of discussing, informing and
story-telling activities however the meeting data
contains a lot more planning and advising.
and activity and annotate them with an activity which fol-
lows the intuitive definition of the action-verb such as dis-
cussing, planning, etc. Additionally an activity annotation
manual containing more specific instructions has been avail-
able (Ries et al, 2000; Thyme?-Gobbel et al, 2001) 3. The
list of tags and the distribution can be seen in Tab. 1. The
set of activities can be clustered into ?interactive? activities
of equal contribution rights (discussion,planning), one per-
son being active (advising, information giving, story-telling),
interrogations and all others.
Measure Meeting SBC CallHome
all inter all inter Spanish
? 0.41 0.51 0.49 0.56 0.59
Mutual inf. 0.35 0.25 0.65 0.32 0.61
Table 2: Intercoder agreement for activities: The
meeting dialogues and Santa Barbara corpus have
been annotated by a semi-naive coder and the first
author of the paper. The ?-coefficient is determined
as in Carletta et al (1997) and mutual information
measures how much one label ?informs? the other
(see Sec. 3). For CallHome Spanish 3 dialogues were
coded for activities by two coders and the result
seems to indicate that the task was easier.
Both datasets have been annotated not only by a semi-
naive annotator but also by the first author of the paper.
The results for ?-statistics (Carletta et al, 1997) and mu-
tual information between the coders can be seen in Tab. 2.
The intercoder agreement would be considered moderate but
compares approximately to Carletta et al (1997) agreement
on transactions (? = 0.59), especially for the interactive ac-
tivities and CallHome Spanish.
For classification a neural network was trained that uses
the softmax function as its output and KL-divergence as
3 In contrast to (Ries et al, 2000; Thyme?-Gobbel et al,
2001) the ?consoling? activity has been eliminated and an
?informing? activity has been introduced for segments where
one or more than one member of the rejoinder give informa-
tion to the others. Additionally an ?introducing? activity
was added to account for a introduction of people or topics
at the beginning of meetings.
Feature all interactive
SBC meet SBC meet
baseline 32.7 41.1 50.5 54.6
dialogue acts per channel 28.1 37.6 47.7 56.7
dialogue acts 28.0 36.2 46.7 65.3
words 38.3 39.7 53.3 54.6
dominance 32.7 44.7 64.5 58.2
style 24.3 35.5 53.3 58.9
style + words 42.1 38.3 52.3 57.5
dominance + words 41.1 41.1 52.3 58.9
dominance + style + words 42.1 39.7 53.3 60.3
dialogue acts + words 42.1 37.6 57.0 61.0
dialogue acts + style + words 39.3 40.4 57.9 61.0
Wordnet 37.4 37.6 46.7 52.5
Wordnet + words 49.5 39.0 53.3 57.5
first author 59.8 57.9 73.8 72.7
Table 3: Activity detection: Activities are detected
on the Santa Barbara Corpus (SBC) and the meet-
ing database (meet) either without clustering the
activities (all) or clustering them according to their
interactivity (interactive) (see Sec. 2 for details).
the error function. The network connects the input di-
rectly to the output units. Hidden units have not been used
since they did not yield improvements on this task. The
network was trained using RPROP with momentum (Ried-
miller and Braun, 1993) and corresponds to an exponen-
tial model (Nigam et al, 1999). The momentum term can
be interpreted as a Gaussian prior with zero mean on the
network weights. It is the same architecture that we used
previously (Ries et al, 2000) for the detection of activities
on CallHome Spanish. Although some feature sets could be
trained using the iterative scaling algorithm if no hidden
units are being used the training times weren?t high enough
to justify the use of the less flexible iterative scaling algo-
rithm. The features used for classification are
words the 50 most frequent words / part of speech pairs
are used directly, all other pairs are replaced by their
part of speech 4.
stylistic features adapted from Biber (1988) and contain
mostly syntactic constructions and some word classes.
Wordnet a total of 40 verb and noun classes (so called lex-
icographers classes (Fellbaum, 1998)) are defined and
a word is replaced by the most frequent class over all
possible meanings of the word.
dialogue acts such as statements, questions, backchannels,
. . . are detected using a language model based detec-
tor trained on Switchboard similar to Stolcke et al
(2000) 5
4Klaus Zechner trained an English part of speech tagger
tagger on Switchboard that has been used. The tagger uses
the code by Brill (1994).
5The model was trained to be very portable and therefore
the following choices were taken: (a) the dialogue model
is context-independent and (b) only the part of speech are
taken as the input to the model plus the 50 most likely
word/part of speech types.
dominance is described as the distribution of the speaker
dominance in a conversation. The distribution is rep-
resented as a histogram and speaker dominance is mea-
sured as the average dominance of the dialogue acts (Linell
et al, 1988) of each speaker. The dialogue acts are de-
tected and the dominance is a numeric value assigned
for each dialogue act type. Dialogue act types that
restrict the options of the conversation partners have
high dominance (questions), dialogue acts that signal
understanding (backchannels) carry low dominance.
First author The activities used for classification are those
of the semi-naive coder. The ?first author? column de-
scribes the ?accuracy? of the first author with respect
to the naive coder.
The detection of interactive activities works fairly well
using the dominance feature on SBC which is also natu-
ral since the relative dominance of speakers should describe
what kind of interaction is exhibited. The dialogue act dis-
tribution on the other hand works fairly well on the more
homogeneous meeting database were there is a better chance
to see generalizations from more specific dialogue based in-
formation. Overall the combination of more than one feature
is really important since word level, Wordnet and stylistic
information, while sometimes successful, seem to be able to
improve the result while they don?t provide good features by
themselves. The meeting data is also more difficult which
might be due to its informal style.
3. INFORMATION ACCESS ASSESSMENT
Assuming a probabilistic information retrieval model a
query r ? in our example an activity ? predicts a docu-
ment d with the probability q(d|r) = q(r|d)q(d)q(r) . Let p(d, r)
be the real probability mass distribution of these quanti-
ties. The probability mass function q(r|d) is estimated on
a separate training set by a neural network based classi-
fier 6. The quantity we are interested in is the reduction
in expected coding length of the document using the neural
network based detector 7:
?Eplog
q(D)
q(D|R)
? H(R)? Ep log
1
q(R|D)
The two expectations correspond exactly to the measures in
Tab. 5, the first represents the baseline, the second the one
for the respective classifier. In more standard information
theoretic notation this quantity may be written as:
H(R)? (Hp(R|D) +D(p(r|d)||q(r|d)))
This equivalence is not extremely useful though since the
quantities in parenthesis can?t be estimated separately. For
the small meeting database and SBC however no entropy
reductions could be obtained. On the larger databases, on
the other hand, entropy reductions could be obtained (?
0.5bit on the CallHome Spanish database Ries et al (2000),
? 1bit for the sub-database detection problem in Sec. 4).
6All quantities involving the neural net q(r|d) have been
determined using a round robin approach such that network
is trained on a separate training set.
7Since estimating q(d) is simple we may assume that q(d) ??
r p(d, r).
Another option is to assume that the labels of one coder
are part of D. If the query by the other coder is R we are in-
terested in the reduction of the document entropy given the
query. If we furthermore assume that H(R|D) = H(R|R?)
where R? is the activity label embedded in D:
H(D)?H(D|R) = H(R)?H(R|D) = MI(R,R?)
Tab. 2 shows that the labels of the semi-naive coder and the
first author only inform each other by 0.25?0.65 bits. How-
ever, since all constraints are important to apply, it might be
important to include manual annotations to be matched by
a query or in a graphical presentation of the output results.
Another interesting question to consider is whether the
activity is correlated with the rejoinder or not. This ques-
tion is important since a correlation of the activity with the
rejoinder would mean that the indexing performance of ac-
tivities needs to be compared to other indices that apply to
rejoinders such as attendance, time and place (for results on
the correlation with rejoinders see Waibel et al (2001)). The
correlation can be measured using the mutual information
between the activity and the meeting identity. The mutual
information is moderate for SBC (? 0.67 bit) and much
lower for the meetings (? 0.20 bit). This also corresponds
to our intuition since some of the rejoinders in SBC belong
to very distinct dialogue genre while the meeting database
is homogeneous. The conclusion is that activities are useful
for navigation in a rejoinder if the database is homogeneous
and they might be useful for finding conversations in a more
heterogeneous database.
# # #
Talk 344 Edu 25 Finance 8
News 217 Scifi 24 Religious 5
Sitcom 97 Series 24 Series-Old 3
Soap 87 Cartoon 23 Infotain 3
Game 46 Movies 22 Music 2
Law 32 Crafts 17 Horror 1
Sports 32 Specials 15
Drama 31 Comedy 9
Table 4: TV show types: The distribution of show
types in a large database of TV shows (1067 shows)
that has been recorded over the period of a couple
of months until April 2000 in Pittsburgh, PA
4. DETECTION OF SUB-DATABASES
We set up an environment for TV shows that records the
subtitles with timestamps continuously from one TV chan-
nel and the channel was switched every other day. At the
same time the TV program was downloaded from http:
//tv.yahoo.com/ to obtain programming information in-
cluding the genre of the show. Yahoo assigns primary and
secondary show types and unless the combination of pri-
mary/secondary show-type is frequent enough the primary
showtype is used (Tab. 4). The TV show database has the
advantage that we were able to collect a large and varied
database with little effort. The same classifier as in Sec. 2
has been used however dialogue acts have not been detected
since the data contains a lot of noise, is not necessarily con-
versational and speaker identities can?t be determined easily.
Detection results for TV shows can be seen in Tab. 5. It may
be noted that adding a lot of keywords does improve the de-
tection result but not so much the entropy. It may therefore
be assume that there is a limited dependence between topic
and genre which isn?t really a surprise since there are many
shows with weekly sequels and there may be some true re-
peats.
Feature accuracy entropy
Wordnet stylistic words
baseline 32.2 3.31
? 50.9 2.73
? 50 62.2 2.33
? ? 50 60.0 2.29
? ? 61.2 2.28
? 56.9 2.41
? 50 61.5 2.25
50 61.3 2.35
250 62.7 2.17
500 66.0 2.14
? ? 500 64.9 2.13
5000 67.2 2.08
Table 5: Show type detection: Using the neural net-
work described in Sec. 2 the show type was detected.
If there is a number in the word column the word
feature is being used. The number indicates how
many word/part of speech pairs are in the vocabu-
lary additionally to the parts of speech.
5. EMOTION AND DOMINANCE
Emotions are displayed in a variety of gestures, some of
which are oral and may be detected via automated methods
from the audio channel (Polzin, 1999). Using only verbal
information the emotions happy, excited and neutral can
be detected on the meeting database with 88.1% accuracy
while always picking neutral yields 83.6%. This result can be
improved to 88.6% by adding pitch and power information.
While these experiments were conducted at the utterance
level emotions can be extended to topical segments. For
that purpose the emotions of the individual utterances are
entered in a histogram over the segment and the vectors
are clustered automatically. The resulting clusters roughly
correspond to a ?neutral?, ?a little happy? and ?somewhat
excited? segment. Using the classifier for emotions on the
word level the segment can be classified automatically into
categories with a 83.3% accuracy while the baseline is 68.9%.
The entropy reduction by automatically detected emotional
activities is ? 0.3bit 8. A similar attempt can be made for
dominance (Linell et al, 1988) distributions: Dominance is
easy to understand for the user of an information access
system and it can be determined automatically with high
accuracy.
8 A similar classification result for emotions on the utter-
ance level has been obtained by just using the laughter vs.
non-laughter tokens of the transcript as the input. This
may indicate that (a) the index should really be the amount
of laughter in the conversational segment and that (b) emo-
tions might not be displayed very overtly in meetings. These
results however would require a wider sampling of meeting
types to be generally acceptable.
6. CONCLUSION AND FUTURE WORK
It has been shown that activities can be detected and that
they may be efficient indices for access to oral communica-
tion. Overall it is easy to make high level distinctions with
automated methods while fine-grained distinctions are even
hard to make for humans ? on the other hand automatic
methods are still able to model some aspect of it (Fig. 3).
To obtain an reduction in entropy a relatively large database
such as CallHome Spanish is required (120 dialogues). Al-
ternatives to activities might be emotional and dominance
distributions that are easier to detect and that may be nat-
ural to understand for users. If activities are only used for
local navigation support within a rejoinder one could also
visualize by displaying the dialogue act patterns for each
channel on a time line.
The author has also observed that topic clusters and activ-
ities are largely independent in the meeting domain result-
ing in orthogonal indices. Since activities have intuitions for
naive users and they may be remembered it can be assumed
that users would be able to make use of these constraints.
Ongoing work includes the use of speaker activity for dia-
logue segmentation and further assessment of features for
information access. Overall the methods presented here and
the ongoing work are improving the ability to index oral
communication. It should be noted that some of the tech-
niques presented lend themselves to implementations that
don?t require (full) speech recognition: Speaker identifica-
tion and dialogue act identification may be done without
an LVCSR system which would allow to lower the compu-
tational requirements as well as to a more robust system.
Figure 3: Detection accuracy summary: The detec-
tion of high-level genre as exemplified by the differ-
entiation of corpora can be done with high accuracy
using simple features (Ries, 1999). Similar it was
fairly easy to discriminate between male and female
speakers on Switchboard (Ries, 1999). Discrimi-
nating between sub-genre such as TV-show types
(Sec. 4) can be done with reasonable accuracy. How-
ever it is a lot harder to discriminate between ac-
tivities within one conversation for personal phone
calls (CallHome) (Ries et al, 2000) or for general
rejoinders (Santa) and meetings (Sec. 2).
References
M. M. Bahktin. Speech Genres and other late Essays, chap-
ter Speech Genres. University of Texas Press, Austin,
1986.
D. Biber. Variation across speech and writing. Cambridge
University Press, 1988.
E. Brill. A report on recent progress in transformation based
error-driven learning. In DARPA Workshop, 1994.
J. Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-
Sneddon, and A. H. Anderson. The reliability of a dia-
logue structure coding scheme. Computational Linguis-
tics, 23(1):13?31, March 1997.
C. Fellbaum, editor. WordNet ? An Electronic Lexical
Database. MIT press, 1998.
G. Fritz and F. Hundschnur. Handbuch der Dialoganalyse.
Niemeyer, Tuebingen, 1994.
D. J. Herrmann. Autobiographical memory and the validity
of retrospective reports, chapter The validity of retrospec-
tive reports as a function of the directness of retrieval
processes, pages 21?31. Springer, 1993.
B. Kessler, G. Nunberg, and H. Schu?tze. Automatic detec-
tion of genre. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguistics and
the 8th Meeting of the European Chapter of the Associ-
ation for Computational Linguistics, pages 32?38. Mor-
gan Kaufmann Publishers, San Francisco CA, 1997. URL
http://xxx.lanl.gov/abs/cmp-lg/9707002.
P. Linell, L. Gustavsson, and P. Juvonen. Interactional
dominance in dyadic communication: a presentation of
initiative-response analysis. Linguistics, 26:415?442, 1988.
K. Nigam, J. Lafferty, and A. McCallum. Using maxi-
mum entropy for text classification. In Proceedings of
the IJCAI-99 Workshop on Machine Learning for Infor-
mation Filtering, 1999. URL http://www.cs.cmu.edu/
~lafferty/.
T. Polzin. Detecting Verbal and Non-Verbal Cues in the
Communication of Emotion. PhD thesis, Carnegie Mellon
University, November 1999.
M. Riedmiller and H. Braun. A direct adaptive method for
faster backpropagation learning: The RPROP algorithm.
In Proc. of the IEEE Int. Conf. on Neural Networks, pages
586?591, 1993.
K. Ries. Towards the detection and description of textual
meaning indicators in spontaneous conversations. In Pro-
ceedings of the Eurospeech, volume 3, pages 1415?1418,
Budapest, Hungary, September 1999.
K. Ries, L. Levin, L. Valle, A. Lavie, and A. Waibel.
Shallow discourse genre annotation in callhome spanish.
In Proceecings of the International Conference on Lan-
guage Ressources and Evaluation (LREC-2000), Athens,
Greece, May 2000.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. V. Ess-Dykema, and
M. Meteer. Dialogue act modeling for automatic tagging
and recognition of conversational speech. Computational
Linguistics, 26(3), September 2000.
A. Thyme?-Gobbel, L. Levin, K. Ries, and L. Valle. Dia-
logue act, dialogue game, and activity tagging manual for
spanish conversational speech. Technical report, Carnegie
Mellon University, 2001. in preperation.
van Bretan, J. Dewe, A. Hallberg, J. Karlgren, and N. Wolk-
ert. Genres defined for a purpose, fast clustering, and an
iterative information retrieval interface. In Eighth DE-
LOS Workshop on User Interfaces in Digital Libraries
L?angholmen, pages 60?66, October 1998.
A. Waibel, M. Bett, and M. Finke. Meeting browser: Track-
ing and summarising meetings. In Proceedings of the
DARPA Broadcast News Workshop, 1998.
A. Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf, T. Schultz,
H. Soltau, H. Yu, and K. Zechner. Advances in automatic
meeting record creation and access. In ICASSP, Salt Lake
City, Utah, USA, 2001. to appear.
Advances in Meeting Recognition
Alex Waibel    , Hua Yu   , Martin Westphal  , Hagen Soltau  ,
Tanja Schultz    , Thomas Schaaf  , Yue Pan   , Florian Metze  , Michael Bett  
Interactive Systems Laboratories
  Carnegie Mellon University, Pittsburgh, PA, USA
 Universita?t Karlsruhe, Fakulta?t fu?r Informatik, Karlsruhe, Germany
http://www.is.cs.cmu.edu/
tanja@cs.cmu.edu
1. INTRODUCTION
Speech recognition has advanced considerably, but has been lim-
ited almost entirely either to situations in which close speaking mi-
crophones are natural and acceptable (telephone, dictation, com-
mand&control, etc.) or in which high-quality recordings are en-
sured. Furthermore, most recognition applications involve con-
trolled recording environments, in which the user turns the recog-
nition event on and off and speaks cooperatively for the purpose of
being recognized.
Unfortunately, the majority of situations in which humans speak
with each other fall outside of these limitations. When we meet
with others, we speak without turning on or off equipment, or we
don?t require precise positioning vis a vis the listener. Recogni-
tion of speech during human encounters, or ?meeting recognition?,
therefore represents the ultimate frontier for speech recognition, as
it forces robustness, knowledge of context, and integration in an
environment and/or human experience.
2. CHALLENGES
Over the last three years we have explored meeting recogni-
tion at the Interactive Systems Laboratories [5, 6, 7]. Meeting
recognition is performed as one of the components of a ?meeting
browser?; a search retrieval and summarization tool that provides
information access to unrestricted human interactions and encoun-
ters. The system is capable of automatically constructing a search-
able and browsable audiovisual database of meetings. The meet-
ings can be described and indexed in somewhat unorthodox ways,
including by what has been said (speech), but also by who said
it (speaker&face ID), where (face, pose, gaze, and sound source
tracking), how (emotion tracking), and why, and other meta-level
descriptions such as the purpose and style of the interaction, the fo-
cus of attention, the relationships between the participants, to name
a few (see [1, 2, 3, 4]).
The problem of speech recognition in unrestricted human meet-
ings is formidable. Error rates for standard recognizers are 5-10
times higher than for dictation tasks. Our explorations based on
LVCSR systems trained on BN, reveal that several types of mis-
.
matches are to blame [6]:
 Mismatched and/or degraded recording conditions (remote,
different microphone types),
 Mismatched dictionaries and language models (typically ideo-
synchratic discussions highly specialized on a topic of inter-
est for a small group and therefore very different from other
existing tasks),
 Mismatched speaking-style (informal, sloppy, multiple speak-
ers talking in a conversational style instead of single speakers
reading prepared text).
In the following sections, we describe experiments and improve-
ments based on our Janus Speech Recognition Toolkit JRTk [8]
applied to transcribing meeting speech robustly.
3. EXPERIMENTAL SETUP
As a first step towards unrestricted human meetings each speaker
is equipped with a clip-on lapel microphone for recording. By this
choice interferences can be reduced but are not ruled out com-
pletely. Compared to a close-talking headset, there is significant
channel cross-talk. Quite often one can hear multiple speakers on
a single channel. Since meetings consist of highly specialized top-
ics, we face the problem of a lack of training data. Large databases
are hard to collect and can not be provided on demand. As a conse-
quence we have focused on building LVCSR systems that are robust
against mismatched conditions as described above. For the purpose
of building a speech recognition engine on the meeting task, we
combined a limited set of meeting data with English speech and text
data from various sources, namely Wall Street Journal (WSJ), En-
glish Spontaneous Scheduling Task (ESST), Broadcast News (BN),
Crossfire and Newshour TV news shows. The meeting data con-
sists of a number of internal group meeting recordings (about one
hour long each), of which fourteen are used for experiments in this
paper. A subset of three meetings were chosen as the test set.
4. SPEECH RECOGNITION ENGINE
To achieve robust performance over a range of different tasks, we
trained our baseline system on Broadcast News (BN). The system
deploys a quinphone model with 6000 distributions sharing 2000
codebooks. There are about 105K Gaussians in the system. Vocal
Tract Length Normalization and cluster-based Cepstral Mean Nor-
malization are used to compensate for speaker and channel varia-
tions. Linear Discriminant Analysis is applied to reduce feature di-
mensionality to 42, followed by a diagonalization transform (Maxi-
mum Likelihood Linear Transform). A 40k vocabulary and trigram
System WER on Different Tasks [%]
BN (h4e98 1) F0-condition 9.6
BN (h4e98 1) all F-conditions 18.5
BN+ESST (h4e98 1) all F-conditions 18.4
Newshour 20.8
Crossfire 25.6
Improvements on Meeting Recognition
Baseline ESST system 54.1
Baseline BN system 44.2
+ acoustic training BN+ESST 42.2
+ language model interpolation (14 meetings) 39.0
Baseline BN system
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (14 meetings) 38.7
Table 1: Recognition Results on BN and Meeting Task
language model are used. The baseline language model is trained
on the BN corpus.
Our baseline system has been evaluated across the above men-
tioned tasks resulting in the word error rates shown in Table 1.
While we achieve a first pass WER of 18.5% on all F-conditions
and 9.6% on the F0-conditions in the Broadcast News task, the
word error rate of 44.2% on meeting data is quite high, reflecting
the challenges of this task. Results on the ESST system [9] are even
worse with a WER of 54.1% which results from the fact that ESST
is a highly specialized system trained on noise-free but spontaneous
speech in the travel domain.
4.1 Acoustic and Language Model Adaptation
The BN acoustic models have been adapted to the meeting data
thru Viterbi training, MLLR (Maximum Likelihood Linear Regres-
sion), and MAP (Maximum A Posteriori) adaptation. To improve
the robustness towards the unseen channel conditions, speaking
mode and training/test mismatch, we trained a system ?BN+ESST?
using a mixed training corpus. The comparison of the results in-
dicate that the mixed system is more robust (44.2%  42.2%),
without loosing the good performance on the original BN test set
(18.5% vs. 18.4%).
To tackle the lack of training corpus, we investigated linear inter-
polation of the BN and the meeting (MT) language model. Based
on a cross-validation test we calculated the optimal interpolation
weight and achieved a perplexity reduction of 21.5% relative com-
pared to the MT-LM and more than 50% relative compared to the
BN-LM. The new language model gave a significant improvement
decreasing the word error rate to 38.7%. Overall the error rate was
reduced by 	
  relative (44.2%  38.7%) compared to the BN
baseline system.
4.2 Model Combination based Acoustic Map-
ping (MAM)
For the experiments on meeting data reported above we have
used comparable recording conditions as each speaker in the meet-
ing has been wearing his or her own lapel microphone. Frequently
however this assumption does not apply. We have also carried out
experiments aimed at producing robust recognition when micro-
phones are positioned at varying distances from the speaker. In this
case data, specific for the microphone distance and SNR found in
the test condition is unavailable. We therefore apply a new method,
Model Combination based Acoustic Mapping (MAM) to the recog-
nition of speech at different distances. MAM was originally pro-
posed for recognition in different car noise environments, please
refer to [10, 11] for details.
MAM estimates an acoustic mapping on the log-spectral domain
in order to compensate for noise condition mismatches between
training and test. During training, the generic acoustic models  




SPEECHALATOR: TWO-WAY SPEECH-TO-SPEECH TRANSLATION IN YOUR HAND
Alex Waibel
 
, Ahmed Badran
 
, Alan W Black
 
, Robert Frederking
 
, Donna Gates
 
Alon Lavie
 
, Lori Levin
 
, Kevin Lenzo

, Laura Mayfield Tomokiyo
Juergen Reichert

, Tanja Schultz   , Dorcas Wallace   , Monika Woszczyna , Jing Zhang
 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA

Cepstral, LLC,

Multimodal Technologies Inc,

Mobile Technologies Inc.
speechalator@speechinfo.org
ABSTRACT
This demonstration involves two-way automatic speech-
to-speech translation on a consumer off-the-shelf PDA. This
work was done as part of the DARPA-funded Babylon project,
investigating better speech-to-speech translation systems for
communication in the field. The development of the Speecha-
lator software-based translation system required addressing
a number of hard issues, including a new language for the
team (Egyptian Arabic), close integration on a small device,
computational efficiency on a limited platform, and scalable
coverage for the domain.
1. BACKGROUND
The Speechalator was developed in part as the next genera-
tion of automatic voice translation systems. The Phrasalator
is a one-way device that can recognize a set of pre-defined
phrases and play a recorded translation, [1]. This device
can be ported easily to new languages, requiring only a
hand translation of the phrases and a set of recorded sen-
tences. However, such a system severely limits communica-
tion as the translation is one way, thus reducing one party?s
responses to simple pointing and perhaps yes and no.
The Babylon project addresses the issues of two-way
communication where either party can use the device for
conversation. A number of different groups throughout the
US were asked to address specific aspects of the task, such
as different languages, translation techniques and platform
specifications. The Pittsburgh group was presented with
three challenges. First, we were to work with Arabic, a lan-
guage with which the group had little experience, to test our
capabilities in moving to new languages quickly. Second,
we were instructed to use an interlingua approach to trans-
lation, where the source language is translated into an in-
termediate form that is shared between all languages. This
step streamlines expansion to new languages, and CMU has
a long history in working with interlingua based translation
systems. Third, we were constrained to one portable PDA-
class device to host the entire two-way system: two recog-
nizers, two translation engines, and two synthesizers.
2. RECOGNITION
We used an HMM-based recognizer, developed by Multi-
modal Technologies Inc, which has been specifically tuned
for PDAs. The recognizer allows a grammar to be tightly
coupled with the recognizer, which offers important effi-
ciencies considering the limited computational power of the
device. With only minor modification we were able to gen-
erate our interlingua interchange format (IF) representation
directly as output from the recognizer, removing one mod-
ule from the process.
MTI?s recognizer requires under 1M of memory with
acoustic models of around 3M per language. Special op-
timizations deal with the slow processor and ensure low
use of memory during decoding. The Arabic models were
bootstrapped from the GlobalPhone [2] Arabic collections
as well as data collected as part of this project.
3. TRANSLATION
As part of this work we investigated two different tech-
niques for translation, both interlingua based. The first was
purely knowledge-based, following our previous work [3].
The engine developed for this was too large to run on the
device, although we were able to run the generation part off-
line seamlessly connected by a wireless link from the hand-
held device. The second technique we investigated used
a statistical training method to build a model to translate
structured interlingua IF to text in the target language. Be-
cause this approach was developed with the handheld in
mind, it is efficient enough to run directly on the device,
and is used in this demo.
4. SYNTHESIS
The synthesis engine is Cepstral?s Theta system. As the
Speechalator runs on very small hardware devices (at least
small compared to standard desktops), it was important that
the synthesis footprint remained as small as possible.
The speechalator is to be used for people with little ex-
posure to synthetic speech, and the output quality must be
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 29-30
                                                         Proceedings of HLT-NAACL 2003
very high. Cepstral?s unit selection voices, tailored to the
domain, meet the requirements for both quality and size.
Normal unit selection voices may take hundreds of megabytes,
but the 11KHz voices developed by Cepstral were around 9
megabytes each.
5. ARABIC
The Arabic language poses a number of challenges for any
speech translation system. The first problem is the wide
range of dialects of the language. Just as Jamaican and
Glaswegian speakers may find it difficult to understand each
other?s dialect of English, Arabic speakers of different di-
alects may find it impossible to communicate.
Modern Standard Arabic (MSA) is well-defined and widely
understood by educated speakers across the Arab world.
MSA is principally a written language and not a spoken lan-
guage, however. Our interest was in dealing with a normal
spoken dialect, and we chose Egyptian Arabic; speakers of
that dialect were readily accessible to us, and media influ-
ences have made it perhaps the most broadly understood of
the regional dialects.
Another feature of Arabic is that the written form, ex-
cept in specific rare cases, does not include vowels. For
speech recognition and synthesis, this makes pronunciations
hard. Solutions have been tested for recognition where the
vowels are not explicitly modeled, but implicitly modeled
by context. This would not work well for synthesis; we have
defined an internal romanization, based on the CallHome
[4] romanization, from which full phonetic forms can easily
be derived. This romanization is suitable for both recog-
nizer and synthesis systems, and can easily be transformed
into the Arabic script for display.
6. SYSTEM
The end-to-end system runs on a standard Pocket PC de-
vice. We have tested it on a number of different machines,
including various HP (Compaq) iPaq machines (38xx 39xx)
and Dell Axims. It can run on 32M machines, but runs best
on a 64M machine with about 40M made available for pro-
gram space. Time from the end of spoken input to start of
translated speech is around 2-4 seconds depending on the
length of the sentence and the actual processor. We have
found StrongARM 206MHz processors, found on the older
Pocket PCs, slightly faster than XScale 400MHz, though no
optimization for the newer processors has been attempted.
Upon startup, the user is presented with the screen as
shown in Figure 1. A push-to-talk button is used and the
speaker speaks in his language. The recognized utterance
is first displayed, with the translation following, and the ut-
terance is then spoken in the target language. Buttons are
provided for replaying the output and for switching the in-
put to the other language.
7. DISCUSSION
The current demonstration is designed for the medical inter-
view domain, with the doctor speaking English and the pa-
tient speaking Arabic. At this point in the project no formal
evaluation has taken place. However, informally, in office-
like acoustic environments, accuracy within domain is well
over 80%.
Arabic input Screen
Speechalator snapshot
8. REFERENCES
[1] Sarich, A., ?Phraselator, one-way speech translation
system,? http://www.sarich.com/translator/, 2001.
[2] T. Schultz and A. Waibel, ?The globalphone project:
Multilingual lvcsr with janus-3,? in Multilingual Infor-
mation Retrieval Dialogs: 2nd SQEL Workshop, Plzen,
Czech Republic, 1997, pp. 20?27.
[3] A. Lavie, et al ?A multi-perspective evaluation of
the NESPOLE! speech-to-speech translation system,?
in Proceedings of ACL 2002 workshop on Speech-to-
speech Translation: Algorithms and Systems, Philadel-
phia, PA., 2002.
[4] Linguistic Data Consortium, ?Callhome egyptian ara-
bic speech,? 1997.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 345?353,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
End-to-End Evaluation in Simultaneous Translation
Olivier Hamon1,2, Christian F?gen3, Djamel Mostefa1, Victoria Arranz1,
Muntsin Kolss3, Alex Waibel3,4 and Khalid Choukri1
1Evaluations and Language Resources Distribution Agency (ELDA), Paris, France
2 LIPN (UMR 7030) ? Universit? Paris 13 & CNRS, Villetaneuse, France
3 Univerit?t Karlsruhe (TH), Germany
4 Carnegie Mellon University, Pittsburgh, USA
{hamon|mostefa|arranz|choukri}@elda.org,
{fuegen|kolss|waibel}@ira.uka.de
Abstract
This paper presents the end-to-end evalu-
ation of an automatic simultaneous trans-
lation system, built with state-of-the-art
components. It shows whether, and for
which situations, such a system might be
advantageous when compared to a human
interpreter. Using speeches in English
translated into Spanish, we present the
evaluation procedure and we discuss the
results both for the recognition and trans-
lation components as well as for the over-
all system. Even if the translation process
remains the Achilles? heel of the system,
the results show that the system can keep
at least half of the information, becoming
potentially useful for final users.
1 Introduction
Anyone speaking at least two different languages
knows that translation and especially simultaneous
interpretation are very challenging tasks. A human
translator has to cope with the special nature of
different languages, comprising phenomena like
terminology, compound words, idioms, dialect
terms or neologisms, unexplained acronyms or ab-
breviations, proper names, as well as stylistic and
punctuation differences. Further, translation or in-
terpretation are not a word-by-word rendition of
what was said or written in a source language. In-
stead, the meaning and intention of a given sen-
tence have to be reexpressed in a natural and fluent
way in another language.
Most professional full-time conference inter-
preters work for international organizations like
the United Nations, the European Union, or the
African Union, whereas the world?s largest em-
ployer of translators and interpreters is currently
the European Commission. In 2006, the European
Parliament spent about 300 million Euros, 30% of
its budget, on the interpretation and translation of
the parliament speeches and EU documents. Gen-
erally, about 1.1 billion Euros are spent per year
on the translating and interpreting services within
the European Union, which is around 1% of the
total EU-Budget (Volker Steinbiss, 2006).
This paper presents the end-to-end evaluation
of an automatic simultaneous translation system,
built with state-of-the-art components. It shows
whether, and in which cases, such a system might
be advantageous compared to human interpreters.
2 Challenges in Human Interpretation
According to Al-Khanji et al (2000), researchers
in the field of psychology, linguistics and interpre-
tation seem to agree that simultaneous interpre-
tation (SI) is a highly demanding cognitive task
involving a basic psycholinguistic process. This
process requires the interpreter to monitor, store
and retrieve the input of the source language in
a continuous manner in order to produce the oral
rendition of this input in the target language. It is
clear that this type of difficult linguistic and cog-
nitive operation will force even professional in-
terpreters to elaborate lexical or synthetic search
strategies.
Fatigue and stress have a negative effect on the
interpreter, leading to a decrease in simultaneous
interpretation quality. In a study by Moser-Mercer
et al (1998), in which professional speakers were
asked to work until they could no longer provide
acceptable quality, it was shown that (1) during
the first 20 minutes the frequency of errors rose
steadily, (2) the interpreters, however, seemed to
be unaware of this decline in quality, (3) after 60
minutes, all subjects made a total of 32.5 mean-
ing errors, and (4) in the category of nonsense the
number of errors almost doubled after 30 minutes
on the task.
Since the audience is only able to evaluate the
simultaneously interpreted discourse by its form,
345
the fluency of an interpretation is of utmost im-
portance. According to a study by Kopczynski
(1994), fluency and style were third on a list of
priorities (after content and terminology) of el-
ements rated by speakers and attendees as con-
tributing to quality. Following the overview in
(Yagi, 2000), an interpretation should be as natu-
ral and as authentic as possible, which means that
artificial pauses in the middle of a sentence, hes-
itations, and false-starts should be avoided, and
tempo and intensity of the speaker?s voice should
be imitated.
Another point to mention is the time span be-
tween a source language chunk and its target lan-
guage chunk, which is often referred to as ear-
voice-span. Following the summary in (Yagi,
2000), the ear-voice-span is variable in duration
depending on some source and target language
variables, like speech delivery rate, information
density, redundancy, word order, syntactic charac-
teristics, etc. Short delays are usually preferred for
several reasons. For example, the audience is irri-
tated when the delay is too large and is soon asking
whether there is a problem with the interpretation.
3 Automatic Simultaneous Translation
Given the explanations above on human interpre-
tation, one has to weigh two factors when consid-
ering the use of simultaneous translation systems:
translation quality and cost.
The major disadvantage of an automatic system
compared to human interpretation is its translation
quality, as we will see in the following sections.
Current state-of-the-art systems may reach satis-
factory quality for people not understanding the
lecturer at all, but are still worse than human inter-
pretation. Nevertheless, an automatic system may
have considerable advantages.
One such advantage is its considerable short-
term memory: storing long sequences of words is
not a problem for a computer system. Therefore,
compensatory strategies are not necessary, regard-
less of the speaking rate of the speaker. However,
depending on the system?s translation speed, la-
tency may increase. While it is possible for hu-
mans to compress the length of an utterance with-
out changing its meaning (summarization), it is
still a challenging task for automatic systems.
Human simultaneous interpretation is quite ex-
pensive, especially due to the fact that usually two
interpreters are necessary. In addition, human in-
terpreters require preparation time to become fa-
miliar with the topic. Moreover, simultaneous in-
terpretation requires a soundproof booth with au-
dio equipment, which adds an overall cost that is
unacceptable for all but the most elaborate multi-
lingual events. On the other hand, a simultaneous
translation system also needs time and effort for
preparation and adaptation towards the target ap-
plication, language and domain. However, once
adapted, it can be easily re-used in the same do-
main, language, etc. Another advantage is that the
transcript of a speech or lecture is produced for
free by using an automatic system in the source
and target languages.
3.1 The Simultaneous Translation System
Figure 1 shows a schematic overview of the si-
multaneous translation system developed at Uni-
versit?t Karlsruhe (TH) (F?gen et al, 2006b). The
speech of the lecturer is recorded with the help
of a close-talk microphone and processed by the
speech recognition component (ASR). The par-
tial hypotheses produced by the ASR module are
collected in the resegmentation component, for
merging and re-splitting at appropriate ?seman-
tic? boundaries. The resegmented hypotheses are
then transferred to one or more machine transla-
tion components (MT), at least one per language
pair. Different output technologies may be used
for presenting the translations to the audience. For
a detailed description of the components as well
as the client-server framework used for connect-
ing the components please refer to (F?gen et al,
2006b; F?gen et al, 2006a; Kolss et al, 2006; F?-
gen and Kolss, 2007; F?gen et al, 2001).
3.2 End-to-End Evaluation
The evaluation in speech-to-speech translation
jeopardises many concepts and implies a lot of
subjectivity. Three components are involved and
an overall system may grow the difficulty of esti-
mating the output quality. However, two criteria
are mainly accepted in the community: measuring
the information preservation and determining how
much of the translation is understandable.
Several end-to-end evaluations in speech-to-
speech translation have been carried out in the last
few years, in projects such as JANUS (Gates et
al., 1996), Verbmobil (N?bel, 1997) or TC-STAR
(Hamon et al, 2007). Those projects use the
main criteria depicted above, and protocols differ
in terms of data preparation, rating, procedure, etc.
346
Dictionary
Source
Hypothesis Translatable
Segment
Model
Source Boundary
Resegmen?
tationRecognition
Speech
Translation
Model Model
Target Language
Machine
Translation
Model
Source Acoustic
Model
Source Language
Output
Translated
Translation
Vocabulary
Audio Stream
Text
Output
(Subtitles)(Synthesis)
Spoken
Figure 1: Schematic overview and information flow of the simultaneous translation system. The main
components of the system are represented by cornered boxes and the models used for theses components
by ellipses. The different output forms are represented by rounded boxes.
To our opinion, to evaluate the performance of a
complete speech-to-speech translation system, we
need to compare the source speech used as input to
the translated output speech in the target language.
To that aim, we reused a large part of the evalua-
tion protocol from the TC-STAR project(Hamon
et al, 2007).
4 Evaluation Tasks
The evaluation is carried out on the simultaneously
translated speech of a single speaker?s talks and
lectures in the field of speech processing, given in
English, and translated into Spanish.
4.1 Data used
Two data sets were selected from the talks and
lectures. Each set contained three excerpts, no
longer than 6 minutes each and focusing on dif-
ferent topics. The former set deals with speech
recognition and the latter with the descriptions of
European speech research projects, both from the
same speaker. This represents around 7,200 En-
glish words. The excerpts were manually tran-
scribed to produce the reference for the ASR eval-
uation. Then, these transcriptions were manually
translated into Spanish by two different transla-
tors. Two reference translations were thus avail-
able for the spoken language translation (SLT)
evaluation. Finally, one human interpretation was
produced from the excerpts as reference for the
end-to-end evaluation. It should be noted that for
the translation system, speech synthesis was used
to produce the spoken output.
4.2 Evaluation Protocol
The system is evaluated as a whole (black box
evaluation) and component by component (glass
box evaluation):
ASR evaluation. The ASR module is evaluated
by computing the Word Error Rate (WER) in case
insensitive mode.
SLT evaluation. For the SLT evaluation, the au-
tomatically translated text from the ASR output is
compared with two manual reference translations
by means of automatic and human metrics. Two
automatic metrics are used: BLEU (Papineni et
al., 2001) and mWER (Niessen et al, 2000).
For the human evaluation, each segment is eval-
uated in relation to adequacy and fluency (White
and O?Connell, 1994). For the evaluation of ad-
equacy, the target segment is compared to a ref-
erence segment. For the evaluation of fluency,
the quality of the language is evaluated. The two
types of evaluation are done independently, but
each evaluator did both evaluations (first that of
fluency, then that of adequacy) for a certain num-
ber of segments. For the evaluation of fluency,
evaluators had to answer the question: ?Is the text
written in good Spanish??. For the evaluation of
adequacy, evaluators had to answer the question:
?How much of the meaning expressed in the ref-
erence translation is also expressed in the target
translation??.
For both evaluations, a five-point scale is pro-
posed to the evaluators, where only extreme val-
ues are explicitly defined. Three evaluations are
carried out per segment, done by three different
evaluators, and segments are divided randomly,
because evaluators must not recreate a ?story?
347
and thus be influenced by the context. The total
number of judges was 10, with around 100 seg-
ments per judge. Furthermore, the same number
of judges was recruited for both categories: ex-
perts, from the domain with a knowledge of the
technology, and non-experts, without that knowl-
edge.
End-to-End evaluation. The End-to-End eval-
uation consists in comparing the speech in the
source language to the output speech in the tar-
get language. Two important aspects should be
taken into account when assessing the quality of
a speech-to-speech system.
First, the information preservation is measured
by using ?comprehension questionnaires?. Ques-
tions are created from the source texts (the En-
glish excerpts), then questions and answers are
translated into Spanish by professional translators.
These questions are asked to human judges after
they have listened to the output speech in the tar-
get language (Spanish). At a second stage, the an-
swers are analysed: for each answer a Spanish val-
idator gives a score according to a binary scale (the
information is either correct or incorrect). This al-
lows us to measure the information preservation.
Three types of questions are used in order to di-
versify the difficulty of the questions and test the
system at different levels: simple Factual (70%),
yes/no (20%) and list (10%) questions. For in-
stance, questions were: What is the larynx respon-
sible for?, Have all sites participating in CHIL
built a CHIL room?, Which types of knowledge
sources are used by the decoder?, respectively.
The second important aspect of a speech-to-
speech system is the quality of the speech output
(hereafter quality evaluation). For assessing the
quality of the speech output one question is asked
to the judges at the end of each comprehension
questionnaire: ?Rate the overall quality of this au-
dio sample?, and values go from 1 (?1: Very bad,
unusable?) to 5 (?It is very useful?). Both auto-
matic system and interpreter outputs were evalu-
ated with the same methodology.
Human judges are real users and native Span-
ish speakers, experts and non-experts, but different
from those of the SLT evaluation. Twenty judges
were involved (12 excerpts, 10 evaluations per ex-
cerpt and 6 evaluations per judge) and each judge
evaluated both automatic and human excerpts on a
50/50 percent basis.
5 Components Results
5.1 Automatic Speech Recognition
The ASR output has been evaluated using the
manual transcriptions of the excerpts. The overall
Word Error Rate (WER) is 11.9%. Table 1 shows
the WER level for each excerpt.
Excerpts WER [%]
L043-1 14.5
L043-2 14.5
L043-3 9.6
T036-1 11.3
T036-2 11.7
T036-3 9.2
Overall 11.9
Table 1: Evaluation results for ASR.
T036 excerpts seem to be easier to recognize au-
tomatically than L043 ones, probably due to the
more general language of the former.
5.2 Machine Translation
5.2.1 Human Evaluation
Each segment within the human evaluation is eval-
uated 4 times, each by a different judge. This aims
at having a significant number of judgments and
measuring the consistency of the human evalua-
tions. The consistency is measured by computing
the Cohen?s Kappa coefficient (Cohen, 1960).
Results show a substantial agreement for flu-
ency (kappa of 0.64) and a moderate agreement
for adequacy (0.52).The overall results of the hu-
man evaluation are presented in Table 2. Regard-
ing both experts? and non-experts? details, agree-
ment is very similar (0.30 and 0.28, respectively).
All judges Experts Non experts
Fluency 3.13 2.84 3.42
Adequacy 3.26 3.21 3.31
Table 2: Average rating of human evalua-
tions [1<5].
Both fluency and adequacy results are over the
mean. They are lower for experts than for non-
experts. This may be due to the fact that experts
are more familiar with the domain and therefore
more demanding than non experts. Regarding the
detailed evaluation per judge, scores are generally
lower for non-experts than for experts.
348
5.2.2 Automatic Evaluation
Scores are computed using case-sensitive metrics.
Table 3 shows the detailed results per excerpt.
Excerpts BLEU [%] mWER [%]
L043-1 25.62 58.46
L043-2 22.60 62.47
L043-3 28.73 62.64
T036-1 34.46 55.13
T036-2 29.41 59.91
T036-3 35.17 50.77
Overall 28.94 58.66
Table 3: Automatic Evaluation results for SLT.
Scores are rather low, with a mWER of 58.66%,
meaning that more than half of the translation is
correct. According to the scoring, the T036 ex-
cerpts seem to be easier to translate than the L043
ones, the latter being of a more technical nature.
6 End-to-End Results
6.1 Evaluators Agreement
In this study, ten judges carried out the evaluation
for each excerpt. In order to observe the inter-
judges agreement, the global Fleiss?s Kappa co-
efficient was computed, which allows to measure
the agreement between m judges with r criteria of
judgment. This coefficient shows a global agree-
ment between all the judges, which goes beyond
Cohen?s Kappa coefficient. However, a low co-
efficient requires a more detailed analysis, for in-
stance, by using Kappa for each pair of judges.
Indeed, this allows to see how deviant judges are
from the typical judge behaviour. For m judges,
n evaluations and r criteria, the global Kappa is
defined as follows:
? = 1 ?
nm2 ?
?n
i=1
?r
j=1 X
2
ij
nm(m? 1) ?rj=1 Pj(1 ? Pj)
where:
Pj =
?n
i=1 Xij
nm
and: Xij is the number of judgments for the ith
evaluation and the jth criteria.
Regarding quality evaluation (n = 6, m = 10,
r = 5), Kappa values are low for both human in-
terpreters (? = 0.07) and the automatic system
(? = 0.01), meaning that judges agree poorly
(Landis and Koch, 1977). This is explained by
the extreme subjectivity of the evaluation and the
small number of evaluated excerpts. Looking at
each pair of judges and the Kappa coefficients
themselves, there is no real agreement, since most
of the Kappa values are around zero. However,
some judge pairs show fair agreement, and some
others show moderate or substantial agreement. It
is observed, though, that some criteria are not fre-
quently selected by the judges, which limits the
statistical significance of the Kappa coefficient.
The limitations are not the same for the com-
prehension evaluation (n = 60, m = 10, r = 2),
since the criteria are binary (i.e. true or false). Re-
garding the evaluated excerpts, Kappa values are
0.28 for the automatic system and 0.30 for the in-
terpreter. According to Landis and Koch (1977),
those values mean that judges agree fairly. In
order to go further, the Kappa coefficients were
computed for each pair of judges. Results were
slightly better for the interpreter than for the au-
tomatic system. Most of them were between 0.20
and 0.40, implying a fair agreement. Some judges
agreed moderately.
Furthermore, it was also observed that for the
120 available questions, 20 had been answered
correctly by all the judges (16 for the interpreter
evaluation and 4 for the automatic system one)
and 6 had been answered wrongly by all judges (1
for the former and 5 for the latter). That shows a
trend where the interpreter comprehension would
be easier than that of the automatic system, or at
least where the judgements are less questionable.
6.2 Quality Evaluation
Table 4 compares the quality evaluation results of
the interpreter to those of the automatic system.
Samples Interpreter Automatic system
L043-1 3.1 1.6
L043-2 2.9 2.3
L043-3 2.4 2.1
T036-1 3.6 3.1
T036-2 2.7 2.5
T036-3 3.5 2.5
Mean 3.03 2.35
Table 4: Quality evaluation results for the inter-
preter and the automatic system [1<5].
As can be seen, with a mean score of 3.03 even
for the interpreter, the excerpts were difficult to
interpret and translate. This is particularly so for
349
L043, which is more technical than T036. The
L043-3 excerpt is particularly technical, with for-
mulae and algorithm descriptions, and even a com-
plex description of the human articulatory system.
In fact, L043 provides a typical presentation with
an introduction, followed by a deeper description
of the topic. This increasing complexity is re-
flected on the quality scores of the three excerpts,
going from 3.1 to 2.4.
T036 is more fluent due to the less technical na-
ture of the speech and the more general vocabu-
lary used. However, the T036-2 and T036-3 ex-
cerpts get a lower quality score, due to the descrip-
tion of data collections or institutions, and thus the
use of named entities. The interpreter does not
seem to be at ease with them and is mispronounc-
ing some of them, such as ?Grenoble? pronounced
like in English instead of in Spanish. The inter-
preter seems to be influenced by the speaker, as
can also be seen in his use of the neologism ?el ce-
nario? (?the scenario?) instead of ?el escenario?.
Likewise, ?Karlsruhe? is pronounced three times
differently, showing some inconsistency of the in-
terpreter.
The general trend in quality errors is similar to
those of previous evaluations: lengthening words
(?seeee?ales?), hesitations, pauses between syl-
lables and catching breath (?caracter?s...ticas?),
careless mistakes (?probibilidad? instead of ?prob-
abilidad?), self-correction of wrong interpreting
(?reconocien-/reconocimiento?), etc.
An important issue concerns gender and num-
ber agreement. Those errors are explained by
the presence of morphological gender in Spanish,
like in ?estos se?ales? instead of ?estas se?ales?
(?these signals?) together with the speaker?s speed
of speech. The speaker seems to start by default
with a masculine determiner (which has no gen-
der in English), adjusting the gender afterward de-
pending on the noun following. A quick transla-
tion may also be the cause for this kind of errors,
like ?del se?al acustico? (?of the acoustic signal?)
with a masculine determiner, a feminine substan-
tive and ending in a masculine adjective. Some
translation errors are also present, for instance
?computerizar? instead of ?calcular? (?compute?).
The errors made by the interpreter help to un-
derstand how difficult oral translation is. This
should be taken into account for the evaluation of
the automatic system.
The automatic system results, like those of
the interpreter, are higher for T036 than for L043.
However, scores are lower, especially for the
L043-1 excerpt. This seems to be due to the
type of lexicon used by the speaker for this ex-
cerpt, more medical, since the speaker describes
the articulatory system. Moreover, his description
is sometimes metaphorical and uses a rather col-
loquial register. Therefore, while the interpreter
finds it easier to deal with these excerpts (known
vocabulary among others) and L043-3 seems to be
more complicated (domain-specific, technical as-
pect), the automatic system finds it more compli-
cated with the former and less with the latter. In
other words, the interpreter has to ?understand?
what is said in L043-3, contrary to the automatic
system, in order to translate.
Scores are higher for the T036 excerpts. In-
deed, there is a high lexical repetition, a large
number of named entities, and the quality of the
excerpt is very training-dependant. However, the
system runs into trouble to process foreign names,
which are very often not understandable. Differ-
ences between T036-1 and the other T036 excerpts
are mainly due to the change in topic. While the
former deals with a general vocabulary (i.e. de-
scription of projects), the other two excerpts de-
scribe the data collection, the evaluation metrics,
etc., thus increasing the complexity of translation.
Generally speaking, quality scores of the au-
tomatic system are mainly due to the transla-
tion component, and to a lesser extent to the
recognition component. Many English words are
not translated (?bush?, ?keyboards?, ?squeaking?,
etc.), and word ordering is not always correct.
This is the case for the sentence ?how we solve
it?, translated into ?c?mo nos resolvers lo? instead
of ?c?mo lo resolvemos?. Funnily enough, the
problems of gender (?maravillosos aplicaciones?
- masc. vs fem.) and number (?pueden real-
mente ser aplicado? - plu. vs sing.) the in-
terpreter has, are also found for the automatic
system. Moreover, the translation of compound
nouns often shows wrong word ordering, in partic-
ular when they are long, i.e. up to three words (e.g.
?reconocimiento de habla sistemas? for ?speech
recognition system? instead of ?sistemas de re-
conocimiento de habla?).
Finally, some error combinations result in fully
non-understandable sentences, such as:
?usted tramo se en emacs es squeaking
ruido y dries todos demencial?
350
where the following errors take place:
? tramo: this translation of ?stretch? results
from the choice of a substantive instead of a
verb, giving rise to two choices due to the lex-
ical ambiguity: ?estiramiento? and ?tramo?,
which is more a linear distance than a stretch
in that context;
? se: the pronoun ?it? becomes the reflexive
?se? instead of the personal pronoun ?lo?;
? emacs: the recognition module transcribed
the couple of words ?it makes? into ?emacs?,
not translated by the translation module;
? squeaking: the word is not translated by the
translation module;
? dries: again, two successive errors are made:
the word ?drives? is transcribed into ?dries?
by the recognition module, which is then left
untranslated.
The TTS component also contributes to decreas-
ing the output quality. The prosody module finds it
hard to make the sentences sound natural. Pauses
between words are not very frequent, but they do
not sound natural (i.e. like catching breath) and
they are not placed at specific points, as it would
be done by a human. For instance, the prosody
module does not link the noun and its determiner
(e.g. ?otros aplicaciones?). Finally, a not user-
friendly aspect of the TTS component is the rep-
etition of the same words always pronounced in
the same manner, what is quite disturbing for the
listener.
6.3 Comprehension Evaluation
Tables 5 and 6 present the results of the compre-
hension evaluation, for the interpreter and for the
automatic system, respectively. They provide the
following information:
identifiers of the excerpt: Source data are the
same for the interpreter and the automatic
system, namely the English speech;
subj. E2E: The subjective results of the end-to-
end evaluation are done by the same assessors
who did the quality evaluation. This shows
the percentage of good answers;
fair E2E: The objective verification of the an-
swers. The audio files are validated to check
whether they contain the answers to the ques-
tions or not (as the questions were created
from the English source). This shows the
maximum percentage of answers an evalua-
tor managed to find from either the interpreter
(speaker audio) or the automatic system out-
put (TTS) in Spanish. For instance, informa-
tion in English could have been missed by
the interpreter because he/she felt that this in-
formation was meaningless and could be dis-
carded. We consider those results as an ob-
jective evaluation.
SLT, ASR: Verification of the answers in each
component of the end-to-end process. In or-
der to determine where the information for
the automatic system is lost, files from each
component (recognised files for ASR, trans-
lated files for SLT, and synthesised files for
TTS in the ?fair E2E? column) are checked.
Excerpts subj. E2E fair E2E
L043-1 69 90
L043-2 75 80
L043-3 72 60
T036-1 80 100
T036-2 73 80
T036-3 76 100
Mean 74 85
Table 5: Comprehension evaluation results for the
interpreter [%].
Regarding Table 5, the interpreter loses 15%
of the information (i.e. 15% of the answers were
incorrect or not present in the interpreter?s trans-
lation) and judges correctly answered 74% of the
questions. Five documents get above 80% of cor-
rect results, while judges find almost above 70%
of the answers for the six documents.
Regarding the automatic system results (Table
6), the information rate found by judges is just
above 50% since, by extension, more than half the
questions were correctly answered. The lowest
excerpt, L043-1, gets a rate of 25%, the highest,
T036-1, a rate of 76%, which is in agreement with
the observation for the quality evaluation. Infor-
mation loss can be found in each component, es-
pecially for the SLT module (35% of the informa-
tion is lost here). It should be noticed that the TTS
module made also errors which prevented judges
351
Excerpts subj. E2E fair E2E SLT ASR
L043-1 25 30 30 70
L043-2 62 70 80 70
L043-3 43 40 60 100
T036-1 76 80 90 100
T036-2 61 70 60 80
T036-3 47 60 70 80
Mean 52 58 65 83
Table 6: Comprehension evaluation results for the
automatic system [%].
from answering related questions. Moreover, the
ASR module loses 17% of the information. Those
results are certainly due to the specific vocabulary
used in this experiment.
So as to objectively compare the interpreter with
the automatic system, we selected the questions
for which the answers were included in the inter-
preter files (i.e. those in the ?fair E2E? column
of Table 5). The goal was to compare the overall
quality of the speech-to-speech translation to in-
terpreters? quality, without the noise factor of the
information missing. The assumption is that the
interpreter translates the ?important information?
and skips the useless parts of the original speech.
This experiment is to measure the level of this in-
formation that is preserved by the automatic sys-
tem. So a new subset of results was obtained, on
the information kept by the interpreter. The same
study was repeated for the three components and
the results are shown in Tables 7 and 8.
Excerpts subj. E2E fair E2E SLT ASR
L043-1 27 33 33 78
L043-2 65 75 88 75
L043-3 37 67 83 100
T036-1 76 80 90 100
T036-2 69 88 75 100
T036-3 47 60 70 80
Mean 53 60 70 80
Table 7: Evaluation results for the automatic sys-
tem restricted to the questions for which answers
can be found in the interpreter speech [%].
Comparing the automatic system to the inter-
preter, the automatic system keeps 40% of the in-
formation where the interpreter translates the doc-
uments correctly. Those results confirm that ASR
loses a lot of information (20%), while SLT loses
10% further, and so does the TTS. Judges are quite
close to the objective validation and found most of
the answers they could possibly do.
Excerpts subj. E2E
L043-1 66
L043-2 90
L043-3 88
T036-1 80
T036-2 81
T036-3 76
Mean 80
Table 8: Evaluation results for interpreter, re-
stricted to the questions for which answers can be
found in the interpreter speech [%].
Subjective results for the restricted evaluation
are similar to the previous results, on the full data
(80% vs 74% of the information found by the
judges). Performance is good for the interpreter:
98% of the information correctly translated by the
automatic system is also correctly interpreted by
the human. Although we can not compare the
performance of the restricted automatic system to
that of the restricted interpreter (since data sets of
questions are different), it seems that of the inter-
preter is better. However, the loss due to subjective
evaluation seems to be higher for the interpreter
than for the automatic system.
7 Conclusions
Regarding the SLT evaluation, the results achieved
with the simultaneous translation system are still
rather low compared to the results achieved with
offline systems for translating European parlia-
ment speeches in TC-STAR. However, the offline
systems had almost no latency constraints, and
parliament speeches are much easier to recognize
and translate when compared to the more spon-
taneous talks and lectures focused in this paper.
This clearly shows the difficulty of the whole task.
However, the human end-to-end evaluation of the
system in which the system is compared with hu-
man interpretation shows that the current transla-
tion quality allows for understanding of at least
half of the content, and therefore, may be already
quite helpful for people not understanding the lan-
guage of the lecturer at all.
352
References
Rajai Al-Khanji, Said El-Shiyab, and Riyadh Hussein.
2000. On the Use of Compensatory Strategies in Si-
multaneous Interpretation. Meta : Journal des tra-
ducteurs, 45(3):544?557.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. In Educational and Psychological
Measurement, volume 20, pages 37?46.
Christian F?gen and Muntsin Kolss. 2007. The influ-
ence of utterance chunking on machine translation
performance. In Proc. of the European Conference
on Speech Communication and Technology (INTER-
SPEECH), Antwerp, Belgium, August. ISCA.
Christian F?gen, Martin Westphal, Mike Schneider,
Tanja Schultz, and Alex Waibel. 2001. LingWear:
A Mobile Tourist Information System. In Proc. of
the Human Language Technology Conf. (HLT), San
Diego, California, March. NIST.
Christian F?gen, Shajith Ikbal, Florian Kraft, Kenichi
Kumatani, Kornel Laskowski, John W. McDonough,
Mari Ostendorf, Sebastian St?ker, and Matthias
W?lfel. 2006a. The isl rt-06s speech-to-text system.
In Steve Renals, Samy Bengio, and Jonathan Fiskus,
editors, Machine Learning for Multimodal Interac-
tion: Third International Workshop, MLMI 2006,
Bethesda, MD, USA, volume 4299 of Lecture Notes
in Computer Science, pages 407?418. Springer Ver-
lag Berlin/ Heidelberg.
Christian F?gen, Muntsin Kolss, Matthias Paulik, and
Alex Waibel. 2006b. Open Domain Speech Trans-
lation: From Seminars and Speeches to Lectures.
In TC-Star Speech to Speech Translation Workshop,
Barcelona, Spain, June.
Donna Gates, Alon Lavie, Lori Levin, Alex. Waibel,
Marsal Gavalda, Laura Mayfield, and Monika Wosz-
cyna. 1996. End-to-end evaluation in janus: A
speech-to-speech translation system. In Proceed-
ings of the 6th ECAI, Budapest.
Olivier Hamon, Djamel Mostefa, and Khalid Choukri.
2007. End-to-end evaluation of a speech-to-speech
translation system in tc-star. In Proceedings of the
MT Summit XI, Copenhagen, Denmark, September.
Muntsin Kolss, Bing Zhao, Stephan Vogel, Ashish
Venugopal, and Ying Zhang. 2006. The ISL Statis-
tical Machine Translation System for the TC-STAR
Spring 2006 Evaluations. In TC-Star Workshop
on Speech-to-Speech Translation, Barcelona, Spain,
December.
Andrzej Kopczynski, 1994. Bridging the Gap: Empiri-
cal Research in Simultaneous Interpretation, chapter
Quality in Conference Interpreting: Some Pragmatic
Problems, pages 87?100. John Benjamins, Amster-
dam/ Philadelphia.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
In Biometrics, Vol. 33, No. 1 (Mar., 1977), pp. 159-
174.
Barbara Moser-Mercer, Alexander Kunzli, and Ma-
rina Korac. 1998. Prolonged turns in interpreting:
Effects on quality, physiological and psychological
stress (pilot study). Interpreting: International jour-
nal of research and practice in interpreting, 3(1):47?
64.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation, Athens,
Greece.
Rita N?bel. 1997. End-to-end Evaluation in Verb-
mobil I. In Proceedings of the MT Summit VI, San
Diego.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), Research Report, Com-
puter Science IBM Research Division, T.J.Watson
Research Center.
Accipio Consulting Volker Steinbiss. 2006.
Sprachtechnologien f?r Europa. www.tc-star.
org/pubblicazioni/D17_HLT_DE.pdf.
John S. White and Theresa A. O?Connell. 1994.
Evaluation in the arpa machine translation program:
1993 methodology. In HLT ?94: Proceedings of the
workshop on Human Language Technology, pages
135?140, Morristown, NJ, USA. Association for
Computational Linguistics.
Sane M. Yagi. 2000. Studying Style in Simultane-
ous Interpretation. Meta : Journal des traducteurs,
45(3):520?547.
353
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 43?47,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Tight Integration of Speech Disfluency Removal into SMT
Eunah Cho Jan Niehues
Interactive Systems Lab
Institute of Anthropomatics
Karlsruhe Institute of Technology, Germany
{eunah.cho,jan.niehues,alex.waibel}@kit.edu
Alex Waibel
Abstract
Speech disfluencies are one of the main
challenges of spoken language processing.
Conventional disfluency detection systems
deploy a hard decision, which can have
a negative influence on subsequent appli-
cations such as machine translation. In
this paper we suggest a novel approach
in which disfluency detection is integrated
into the translation process.
We train a CRF model to obtain a disflu-
ency probability for each word. The SMT
decoder will then skip the potentially dis-
fluent word based on its disfluency prob-
ability. Using the suggested scheme, the
translation score of both the manual tran-
script and ASR output is improved by
around 0.35 BLEU points compared to the
CRF hard decision system.
1 Introduction
Disfluencies arise due to the spontaneous nature
of speech. There has been a great deal of effort to
detect disfluent words, remove them (Johnson and
Charniak, 2004; Fitzgerald et al., 2009) and use
the cleaned text for subsequent applications such
as machine translation (MT) (Wang et al., 2010;
Cho et al., 2013).
One potential drawback of conventional ap-
proaches is that the decision whether a token is
a disfluency or not is a hard decision. For an
MT system, this can pose a severe problem if the
removed token was not in fact a disfluency and
should have been kept for the correct translation.
Therefore, we pass the decision whether a word is
part of a disfluency or not on to the translation sys-
tem, so that we can use the additional knowledge
available in the translation system to make a more
reliable decision. In order to limit the complexity,
the search space is pruned prior to decoding and
represented in a word lattice.
2 Related Work
Disfluencies in spontaneous speech have been
studied from various points of view. In the noisy
channel model (Honal and Schultz, 2003), it is
assumed that clean text without any disfluencies
has passed through a noisy channel. The clean
string is retrieved based on language model (LM)
scores and five additional models. Another noisy
channel approach involves a phrase-level statisti-
cal MT system, where noisy tokens are translated
into clean tokens (Maskey et al., 2006). A tree ad-
joining grammar is combined with this noisy chan-
nel model in (Johnson and Charniak, 2004), using
a syntactic parser to build an LM.
Fitzgerald et al. (2009) present a method to de-
tect speech disfluencies using a conditional ran-
dom field (CRF) with lexical, LM, and parser
information features. While previous work has
been limited to the postprocessing step of the au-
tomatic speech recogition (ASR) system, further
approaches (Wang et al., 2010; Cho et al., 2013)
use extended CRF features or additional models
to clean manual speech transcripts and use them
as input for an MT system.
While ASR systems use lattices to encode hy-
potheses, lattices have been used for MT systems
with various purposes. Herrmann et al. (2013)
use lattices to encode different reordering variants.
Lattices have also been used as a segmentation tac-
tic for compound words (Dyer, 2009), where the
segmentation is encoded as input in the lattice.
One of the differences between our work and
previous work is that we integrate the disfluency
removal into an MT system. Our work is not lim-
ited to the preprocessing step of MT, instead we
use the translation model to detect and remove dis-
fluencies. Contrary to other systems where detec-
tion is limited on manual transcripts only, our sys-
43
tem shows translation performance improvements
on the ASR output as well.
3 Tight Integration using Lattices
In this chapter, we explain how the disfluency re-
moval is integrated into the MT process.
3.1 Model
The conventional translation of texts from sponta-
neous speech can be formulated as
e? = argmax
e
p(e| argmax
f
c
p(f
c
|f)) (1)
with
p(f
c
|f) =
I
?
i=1
p(c
i
|f
i
) (2)
where f
c
denotes the clean string
f
c
= {f
1
, . . . , f
I
| c
i
= clean} (3)
for the disfluency decision class c of each token.
c ?
{
clean
disfluent
(4)
Thus, using the conventional models, disfluency
removal is applied to the original, potentially noisy
string in order to obtain the cleaned string first.
This clean string is then translated.
The potential drawback of a conventional
speech translation system is caused by the rough
estimation in Equation 1, as disfluency removal
does not depend on maximizing the translation
quality itself. For example, we can consider the
sentence Use what you build, build what you use.
Due to its repetitive pattern in words and structure,
the first clause is often detected as a disfluency us-
ing automatic means. To avoid this, we can change
the scheme how the clean string is chosen as fol-
lows:
e? = argmax
e,f
c
(p(e|f
c
) ? p(f
c
|f)) (5)
This way a clean string which maximizes the
translation quality is chosen. Thus, no instant de-
cision is made whether a token is a disfluency or
not. Instead, the disfluency probability of the to-
ken will be passed on to the MT process, using
the log linear combination of the probabilities as
shown in Equation 5.
In this work, we use a CRF (Lafferty et al.,
2001) model to obtain the disfluency probability
of each token.
Since there are two possible classes for each to-
ken, the number of possible clean sentences is ex-
ponential with regard to the sentence length. Thus,
we restrict the search space by representing only
the most probable clean source sentences in a word
lattice.
3.2 CRF Model Training
In order to build the CRF model, we used the
open source toolkit CRF++ (Kudoh, 2007). As
unigram features, we use lexical and LM features
adopted from Fitzgerald et al. (2009), and addi-
tional semantics-based features discussed in (Cho
et al., 2013). In addition to the unigram features,
we also use a bigram feature to model first-order
dependencies between labels.
We train the CRF with four classes; FL for filler
words, RC for (rough) copy, NC for non-copy and
0 for clean tokens. The class FL includes obvious
filler words (e.g. uh, uhm) as well as other dis-
course markers (e.g. you know, well in English).
The RC class covers identical or roughly simi-
lar repetitions as well as lexically different words
with the same meaning. The NC class represents
the case where the speaker changes what to speak
about or reformulates the sentence and restarts the
speech fragments. The disfluency probability P
d
of each token is calculated as the sum of probabil-
ities of each class.
3.3 Lattice Implementation
We construct a word lattice which encodes long-
range reordering variants (Rottmann and Vogel,
2007; Niehues and Kolss, 2009). For translation
we extend this so that potentially disfluent words
can be skipped. A reordering lattice of the ex-
ample sentence Das sind die Vorteile, die sie uh
die sie haben. (En.gls: These are the advantages,
that you uh that you have.) is shown in Figure 1,
where words representing a disfluency are marked
in bold letters. In this sentence, the part die sie
uh was manually annotated as a disfluency, due to
repetition and usage of a filler word.
Table 1 shows the P
d
obtained from the CRF
model for each token. As expected, the words die
sie uh obtain a high P
d
from the CRF model.
In order to provide an option to avoid translating
a disfluent word, a new edge which skips the word
is introduced into the lattice when the word has a
higher P
d
than a threshold ?. During decoding the
importance of this newly introduced edge is opti-
mized by weights based on the disfluency proba-
44
0 1 das 2 sie 3 sie 4 sind 5 das 6 das 7 die 8 sind 9 sind 10 Vorteile 11 die 12 die 13 , 14 Vorteile 15 Vorteile 16 die 17 , 18 , 19 sie 20
 haben  die 21 die 22 uh 23
 sie 
24 sie 25 die 26
 uh 
27 uh 28 sie 29 haben 30
 die  die 31 haben  sie  sie 32 . 
Figure 1: Reordering lattice before adding alternative clean paths for an exemplary sentence
0 1 das 2 sie 3 sie 5 das 4
 sind  das 6 das 7
 die 
8 sind 9 sind 
10 Vorteile 
11 die 12 die 
13 , 
14 Vorteile 15 Vorteile 
16 die 19 haben 26 die 17 , 18 , 
 haben 20 sie  die  die  die 21 die 30 die 
22 sie 28 die 23 uh  die 
24 sie  die 
25 uh  die  die 
27 uh  die 
 die 
29 haben  sie  die 
31
 sie  sie  haben 32 . 
Figure 2: Extended lattice with alternative clean paths for an exemplary sentence
das 0.000732 sie 0.953126
sind 0.004445 uh 0.999579
die 0.013451 die 0.029010
Vorteile 0.008183 sie 0.001426
, 0.035408 haben 0.000108
die 0.651642 . 0.000033
Table 1: Disfluency probability of each word
bility and transition probability. The extended lat-
tice for the given sentence with ? = 0.5 is shown
in Figure 2, with alternative paths marked by a
dotted line. The optimal value of ? was manually
tuned on the development set.
4 System Description
The training data for our MT system consists of
1.76 million sentences of German-English paral-
lel data. Parallel TED talks
1
are used as in-domain
data and our translation models are adapted to the
domain. Before training, we apply preprocess-
ing such as text normalization, tokenization, and
smartcasing. Additionally, German compound
words are split.
To build the phrase table we use the Moses
package (Koehn et al., 2007). An LM is trained
on 462 million words in English using the SRILM
Toolkit (Stolcke, 2002). In order to extend source
word context, we use a bilingual LM (Niehues et
al., 2011). We use an in-house decoder (Vogel,
2003) with minimum error rate training (Venu-
gopal et al., 2005) for optimization.
For training and testing the CRF model, we use
61k annotated words of manual transcripts of uni-
1
http://www.ted.com
versity lectures in German. For tuning and testing
the MT system, the same data is used along with
its English reference translation. In order to make
the best use of the data, we split it into three parts
and perform three-fold cross validation. There-
fore, the train/development data consists of around
40k words, or 2k sentences, while the test data
consists of around 20k words, or 1k sentences.
5 Experiments
In order to compare the effect of the tight inte-
gration with other disfluency removal strategies,
we conduct different experiments on manual tran-
scripts as well as on the ASR output.
5.1 Manual Transcripts
As a baseline for manual transcripts, we use
the whole uncleaned data for development and
test. For ?No uh?, we remove the obvious filler
words uh and uhm manually. In the CRF-hard
experiment, the token is removed if the label
output of the CRF model is a disfluency class.
The fourth experiment uses the tight integration
scheme, where new source paths which jump over
the potentially noisy words are inserted based on
the disfluency probabilities assigned by the CRF
model. In the next experiments, this method is
combined with other aforementioned approaches.
First, we apply the tight integration scheme after
we remove all obvious filler words. In the next
experiment, we first remove all words whose P
d
is higher than 0.9 as early pruning and then apply
the tight integration scheme. In a final experiment,
we conduct an oracle experiment, where all words
annotated as a disfluency are removed.
45
5.2 ASR Output
The same experiments are applied to the ASR out-
put. Since the ASR output does not contain re-
liable punctuation marks, there is a mismatch be-
tween the training data of the CRF model, which is
manual transcripts with all punctuation marks, and
the test data. Thus, we insert punctuation marks
and augment sentence boundaries in the ASR out-
put using the monolingual translation system (Cho
et al., 2012). As the sentence boundaries differ
from the reference translation, we use the Leven-
shtein minimum edit distance algorithm (Matusov
et al., 2005) to align hypothesis for evaluation.
No optimization is conducted, but the scaling fac-
tors obtained when using the correponding setup
of manual transcripts are used for testing.
5.3 Results
Table 2 shows the results of our experiments. The
scores are reported in case-sensitive BLEU (Pap-
ineni et al., 2002).
System Dev Text ASR
Baseline 23.45 22.70 14.50
No uh 25.09 24.04 15.10
CRF-hard 25.32 24.50 15.15
Tight int. 25.30 24.59 15.19
No uh + Tight int. 25.41 24.68 15.33
Pruning + Tight int. 25.38 24.84 15.51
Oracle 25.57 24.87 -
Table 2: Translation results for the investigated
disfluency removal strategies
Compared to the baseline where all disfluen-
cies are kept, the translation quality is improved
by 1.34 BLEU points for manual transcripts by
simply removing all obvious filler words. When
we take the output of the CRF as a hard deci-
sion, the performance is further improved by 0.46
BLEU points. When using the tight integration
scheme, we improve the translation quality around
0.1 BLEU points compared to the CRF-hard deci-
sion. The performance is further improved by re-
moving uh and uhm before applying the tight inte-
gration scheme. Finally the best score is achieved
by using the early pruning coupled with the tight
integration scheme. The translation score is 0.34
BLEU points higher than the CRF-hard decision.
This score is only 0.03 BLEU points less than the
oracle case, without all disfluencies.
Experiments on the ASR output also showed a
considerable improvement despite word errors and
consequently decreased accuracy of the CRF de-
tection. Compared to using only the CRF-hard de-
cision, using the coupled approach improved the
performance by 0.36 BLEU points, which is 1.0
BLEU point higher than the baseline.
System Precision Recall
CRF-hard 0.898 0.544
Pruning + Tight int. 0.937 0.521
Table 3: Detection performance comparison
Table 3 shows a comparison of the disfluency
detection performance on word tokens. While re-
call is slightly worse for the coupled approach,
precision is improved by 4% over the hard deci-
sion, indicating that the tight integration scheme
decides more accurately. Since deletions made by
a hard decision can not be recovered and losing a
meaningful word on the source side can be very
critical, we believe that precision is more impor-
tant for this task. Consequently we retain more
words on the source side with the tight integration
scheme, but the numbers of word tokens on the
translated target side are similar. The translation
model is able to leave out unnecessary words dur-
ing translation.
6 Conclusion
We presented a novel scheme to integrate disflu-
ency removal into the MT process. Using this
scheme, it is possible to consider disfluency prob-
abilities during decoding and therefore to choose
words which can lead to better translation perfor-
mance. The disfluency probability of each token
is obtained from a CRF model, and is encoded in
the word lattice. Additional edges are added in the
word lattice, to bypass the words potentially rep-
resenting speech disfluencies.
We achieve the best performance using the tight
integration method coupled with early pruning.
This method yields an improvement of 2.1 BLEU
points for manual transcripts and 1.0 BLEU point
improvement over the baseline for ASR output.
Although the translation of ASR output is im-
proved using the suggested scheme, there is still
room to improve. In future work, we would like to
improve performance of disfluency detection for
ASR output by including acoustic features in the
model.
46
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658.
References
Eunah Cho, Jan Niehues, and Alex Waibel. 2012.
Segmentation and Punctuation Prediction in Speech
Language Translation using a Monolingual Trans-
lation System. In Proceedings of the Interna-
tional Workshop for Spoken Language Translation
(IWSLT), Hong Kong, China.
Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2013.
CRF-based Disfluency Detection using Seman-
tic Features for German to English Spoken Lan-
guage Translation. In Proceedings of the Interna-
tional Workshop for Spoken Language Translation
(IWSLT), Heidelberg, Germany.
Chris Dyer. 2009. Using a Maximum Entropy Model
to Build Segmentation Lattices for MT. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Boulder, Colorado, USA, June. Association for
Computational Linguistics.
Erin Fitzgerald, Kieth Hall, and Frederick Jelinek.
2009. Reconstructing False Start Errors in Sponta-
neous Speech Text. In Proceedings of the European
Association for Computational Linguistics (EACL),
Athens, Greece.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Matthias Honal and Tanja Schultz. 2003. Correction of
Disfluencies in Spontaneous Speech using a Noisy-
Channel Approach. In Eurospeech, Geneva.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based Noisy Channel Model of Speech Repairs. In
Proceedings of the Association for Computational
Linguistics (ACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the Association for Computational
Linguistics (ACL), Demonstration Session, Prague,
Czech Republic, June.
Taku Kudoh. 2007. CRF++: Yet Another CRF
Toolkit.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilitic Models for Segmenting and Labeling Se-
quence Data. In ICML, Massachusetts, USA.
Sameer Maskey, Bowen Zhou, and Yuqing Gao. 2006.
A Phrase-Level Machine Translation Approach for
Disfluency Detection using Weighted Finite State
Tranducers. In Interspeech, Pittsburgh, PA.
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Herrmann Ney. 2005. Evaluating Machine Trans-
lation Output with Automatic Sentence Segmenta-
tion. In Proceedings of the International Workshop
on Spoken Language Translation (IWSLT), Boulder,
Colorado, USA, October.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the 4th Workshop on Statistical Ma-
chine Translation, Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sk?ovde,
Sweden.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. Denver, Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In WPT-
05, Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Wen Wang, Gokhan Tur, Jing Zheng, and Necip Fazil
Ayan. 2010. Automatic Disfluency Removal for Im-
proving Spoken Language Translation. In Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
47
Proceedings of NAACL-HLT 2013, pages 783?788,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Measuring the Structural Importance through Rhetorical Structure Index
Narine Kokhlikyan?, Alex Waibel? ?, Yuqi Zhang?, Joy Ying Zhang?
?Karlsruhe Institute of Technology
Adenauerring 2
76131 Karlsruhe, Germany
? Carnegie Mellon University
NASA Research Park, Bldg. 23
Moffett Field, CA 94035
narine.kokhlikyan@student.kit.edu, waibel@cs.cmu.edu,
yuqi.zhang@kit.edu, joy.zhang@sv.cmu.edu
Abstract
In this paper, we propose a novel Rhetorical
Structure Index (RSI) to measure the struc-
tural importance of a word or a phrase. Un-
like TF-IDF and other content-driven mea-
surements, RSI identifies words or phrases
that are structural cues in an unstructured doc-
ument. We show structurally motivated fea-
tures with high RSI values are more useful
than content-driven features for applications
such as segmenting unstructured lecture tran-
scripts into meaningful segments. Experi-
ments show that using RSI significantly im-
proves the segmentation accuracy compared
to TF-IDF, a traditional content-based feature
weighting scheme.
1 Introduction
Online learning, a new trend in distance learning,
provides numerous lectures to students all over the
world. More than 19,000 colleges offer thousands
of free online lectures1. Starting from video record-
ings of lectures which sometimes also come with the
presentation material, a set of processes can be ap-
plied to extract information from the unstructured
data to assist students in browsing, searching and
understanding the content of the lecture. These pro-
cesses include automatic speech recognition (ASR)
which converts the audio to text, lecture segmen-
tation which inserts paragraph boundaries and adds
section titles to the lecture transcriptions, automatic
summarization that generates a short summary from
1http://www.thebestcolleges.org/
free-online-classes-and-course-lectures/
the full lecture, and lecture translation that translates
the lecture from the original language to the native
language of the student.
The transcription of a lecture generated by the
ASR system is a sequence of words which does
not contain any structural information such as para-
graph, section boundaries and section titles. Zhang
et al (2007; 2008; 2010) used acoustic and lin-
guistic features for rhetorical structure detection and
summarization. They showed that linguistic features
such as TF-IDF are the most influential in segmenta-
tion and summarization and that knowing the struc-
ture of a lecture can significantly improve the perfor-
mance of lecture summarization. Our experiments
with a real-time lecture translation system also show
that displaying the rolling translation results of a live
lecture with proper paragraphing and inserted sec-
tion titles makes it easier for students to grasp the
key points during a lecture.
In this paper, we apply existing algorithms,
namely the Hidden Markov Model (HMM) (Gales
and Young, 2007) to unstructured lecture transcrip-
tion to infer the underlying structure for better lec-
ture segmentation and summarization. HMM has
been successfully applied in early works (van Mul-
bregt et al, 1998; Sherman and Liu, 2008) for text
segmentation, event tracking and boundary detec-
tion. The focus of this work is to identify cue
words and phrases that are good indicators of lec-
ture structure. Intuitively, words and phrases such
as ?last week we talked about?, ?this is an out-
line of my talk?, ?now I am going to talk about?,
?in conclusion?, and ?any questions? should be
important features to recognize lecture structure.
783
These words/phrases, however, may not be so im-
portant content-wise. Thus, content-driven met-
rics such as the TF-IDF score usually do not as-
sign higher weights to these structurally impor-
tant words/phrases. We propose a novel metric
called Rhetorical Structural Index (RSI) to weigh
words/phrases based on their structural importance.
2 Rhetorical Structural Index
RSI incorporates both frequency of occurrences and,
more importantly, the position distribution of occur-
rences of a word/phrase. The intuition is that if a
term is a structural marker, it usually occurs at a cer-
tain position in a lecture. Because the term is mainly
about the structure rather than the content of a lec-
ture, it can appear with high frequency in lectures
that are of different topics. For example, ?today we?
occurs at the beginning of a lecture and ?thank you?
usually appears towards the end (Figure 1) no mat-
ter whether the lecture is about history or computer
science.
We define the RSI of a word w as:
RSI(w) =
1
?Var(Lw) + (1? ?)idf(w,D)
(1)
where Lw is the random variable of ?normalized po-
sitions? of a word w in a lecture. For each occur-
rence of w in a particular lecture d, we divide its
position by the length of the lecture |d| to estimate
its ?normalized position?. Lw takes a value between
[0, 1]. A value close to 0 indicates this word occurs
at the beginning and close to 1 means w is close to
the end of the lecture. Var(Lw) is the variance of the
normalized position of a word w. A small Var(Lw)
indicates that w always occurs at certain positions
of a typical lecture (e.g., ?bye?) while a large value
means w can occur at any position (e.g., function
words ?of? and ?the?).
The second part of RSI is the inverse document
frequency (idf), or effectively the document fre-
quency since RSI is proportional to the 1/idf term.
Lectures, such as different research talks, can vary
in content but usually have a very similar structure
and share some common structural cues. A good
structural cue word should be common to many lec-
tures. idf has been widely used in information re-
trieval research to assign higher weights to words
that occur in just a few documents as compared to
Table 1: Examples of n-grams with high RSI values
which are likely to be structural cues.
n-gram Var(Lw) idf RSI
now 0.0004 0.60 1.04
here 0.0004 0.62 1.03
class 0.0001 2.12 0.90
week 0.0001 2.23 0.89
goodbye 0.0001 3.62 0.80
thank you 0.0003 1.53 0.95
talk about 0.0003 1.90 0.92
dealing with 0.0002 2.00 0.91
today we 0.0003 2.51 0.87
see how 0.0009 2.69 0.85
ladies and gentlemen 0.0008 1.35 0.96
last time we 0.0004 2.22 0.89
here we have 0.0005 2.35 0.88
next time we 0.0002 2.51 0.86
common words that occur in all documents. Define
the idf of a word w given a collection of lectures D
as:
idf(w,D) = log
|D|
|{d ? D|w ? d}|
(2)
|D| is the number of all lectures in the collection and
|{d ? D|w ? d}| is the number of lectures where w
appears. A low idf (w,D) value indicates that word
w occurs in many documents and thus is more likely
to be a common structural cue.
Combining the variance of normalized position
and idf by scaling factor ?, we define RSI as in equa-
tion 1. We found 0.9 as an optimal value of ? accord-
ing our experiments over all data sets. A word w
with high RSI value is more likely to be structurally
important. Similarly, we can calculate the RSI val-
ues for phrases (n-grams) such as ?I would like to
talk about?, ?I will switch gear to? and ?thank you
for your attention?.
Table 1 shows examples of n-grams and the cal-
culated variance, idf-scores and RSI values from a
collection of lectures.
3 Incorporating RSI in Lecture
Segmentation
Several algorithms have been developed for text seg-
mentation including the Naive Bayes classifier for
keyword extraction (Balagopalan et al, 2012), the
784
Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams ?today we? and ?thank you?.
?today we? appears more frequently at the beginning of a lecture, whereas ?thank you? more in the concluding part
of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word at
a position.
Hidden Markov Model (Gales and Young, 2007),
the Maximum entropy Markov model (McCallum et
al., 2000), the Conditional Random Field (Lafferty
et al, 2001) and the Latent Content Analysis (Ponte
and Croft, 1997). In this paper, we evaluate the ef-
fectiveness of the proposed RSI feature on lecture
segmentation using an HMM.
We represent each segment in a lecture as a state
in the Markov model and use the EM algorithm
to learn HMM parameters from unlabeled lecture
data. We use a fully connected HMM with five
states. Typical state labels for lecture are: ?Introduc-
tion?, ?Background?, ?Main Topic?, ?Questions?
and ?Conclusion? as shown in Figure 2. HMM states
emit word tokens. Instead of considering the full
vocabulary as the possible emission alphabet, which
usually leads to model over-fitting, we only consider
terms with high RSI values and high TF-IDF* scores
for comparison. For a word w, define its TF-IDF*
score as:
TF-IDF*(w) = max
d
TF-IDF(w, d), (3)
which is the highest TF-IDF score of a word in any
document in the collection. Our experiments try to
answer the question that ?if HMM is meant to cap-
ture the underlying structure of lectures no matter
which topic the lecture is about, what kind of fea-
tures should be emitted from each state to reflect
such structural patterns among lectures??
The learned HMM model is then applied to un-
seen lecture data to label each sentence to be ?In-
troduction?, ?Background?, ?Main Topic?, ?Ques-
tions? or ?Conclusion? and, based on the label, we
segment the lecture to different sections for evalu-
ation. Segment boundaries are defined in the posi-
tions where sentence labels change.
3.0.1 Bootstrap HMM from K-Means
Clustering Segmentation
Initial HMM parameters are bootstrapped using
results from K-means clustering where we cluster a
sequence of sentences to form a ?segment?. K cor-
responds to the number of desired segments of a lec-
ture. Similarities are computed based on the content
similarity (using n-gram matches) and the relative
785
sentence position defined as:
Sim(Si, Cj) = ?M(Si, Cj) + (1? ?)P (Si, Cj),
(4)
where Si is the i-th sentence, Cj is the centroid of
the j-th cluster. M(Si, Cj) is the content similarity
between sentence Si and centroid Cj and P(Si, Cj)
is the position similarity (distance). ? is a scaling
factor (set to optimal value 0.2 based on all data sets
in our experiments).
Content similarity is based on the number of com-
mon words between two sentences, or between a
sentence and the centroid vector of a cluster. Denote
the binary word frequency vector (bag of words) in
sentence Si as ~Si and similarly ~Cj for cluster cen-
troid Cj , define:
M(Si, Cj) =
~Si? ~Cj
? ~Si ?? ~Cj ?
. (5)
P(Si, Cj) measures the position similarity of two
sentences. Position similarity is based on the rela-
tive position distance between the sentence and the
cluster:Define
P (Si, Cj) =
L
|Pos(Si)? Pos(Cj)|+ 
, (6)
where Si is the i-th sentence, Cj is the j-th cluster.
Pos(Si) is the position of sentence Si. Pos(Cj) is
the average sentence position of all members belong
to cluster Cj and L is the total number of sentences
in a lecture.  is a small constant to avoid division
by zeros.
4 Experiments and Evaluation
We evaluated segmentation on three different data
sets: college lectures recorded by Karlsruhe In-
stitute of Technology (KIT), Microsoft research
(MSR) lectures2 and scientific papers3. Both col-
lege and Microsoft research lectures are manually
transcribed. The reason why we do not include ex-
periments on ASR output is that current ASR quality
of lecture data is still quite poor. Word-Error-Rates
(WER) of ASR output range from 24.37 to 30.80 for
KIT lectures. Roughly speaking, every one out of 3
or 4 words is mis-recognized.
2http://research.microsoft.com/apps/catalog/
3http://aclweb.org/anthology-new/
For evaluation, human annotators annotated a few
lectures to create test/reference sets. The test data
from KIT is annotated by one human annotator and
MSR lectures are annotated by four annotators. The
segmentation gold standard is created based on the
agreed annotations. Since the number of annotated
lectures is small and human annotation is subjective,
we also used ACL papers as an additional data set.
ACL papers are in a way ?lectures in written form?
and have titles for section and subsections which can
be used to identify the segments and annotate the
data set automatically. The statistics of each data set
are listed in Table 2.
Table 2: Statistics of three data sets used in the exper-
iments: our own lecture data (KIT), Microsoft research
talks (MSR) and conference proceedings from ACL an-
thology archive. We removed equations, short titles such
as ?Abstract? and ?Conclusion?, when extracting text
from PDF files from the ACL anthology, which results
in a relatively small number of words per paper. Words
are simply tokenized without case normalization or stem-
ming, which results in relatively large vocabulary sizes.
Properties KIT MSR ACL
Num. 74 1,182 3,583
Avg. Num. of Sent. 484 655 212
Avg. Num. of Words 10,078 10,225 3,896
Avg. Duration (Min.) 43.57 39.15 -
Vocabulary Size 1.3K 22K 24K
First, we calculate the RSI and TF-IDF* scores
for each word in the dataset and choose the top N
words as the HMM emission vocabulary. To avoid
over-fitting, we choose N that is much smaller than
the full vocabulary size of the data set. In our ex-
periments, we set N=300 for KIT, N=5000 for MSR
and N=5400 for ACL. The top 5 words with the
highest TF-IDF* scores from the MSR data set are:
?RFID?, ?Cherokee?, ?tree-to-string?, ?GPU?, and
?data-triggered?, whereas the top 5 words selected
by RSI are ?today?, ?work?, ?question?, ?now?, and
?thank?, which are more structurally informative.
To estimate the accuracy of the segmentation
module, we used Recall, Precision, F-Measure and
Pk (Beeferman et al, 1999) as evaluation metric.
We used an error window of length 6 to calculate
Precision, Recall and F-Measure and a sliding win-
dow with a length equal to half of the average seg-
786
Introduction Main Topic Conclusion Background Questions 
Introduction 
word tokens 
Background 
word tokens 
Content 
word tokens 
Questions 
word tokens 
Conclusion 
word tokens 
Figure 2: Fully connected 5-state HMM representing Introduction, Background, Main Topic, Questions, Conclusion
in a typical lecture.
ment length to estimate the Pk score. With error
window we mean that hypothesis boundaries do not
have to be exactly the same as the reference segment
boundaries. Hypothesis boundaries are acceptable
if they are close enough to reference boundaries in
that window. The Pk score indicates the probability
of segmentation inconsistency. Therefore, the lower
the Pk score the better the segmentation is.
Table 3: Segmentation results measured by Pk (the
smaller the better), Precision, Recall and F-Measure
scores (the higher the better) for three data sets compar-
ing HMM using TF-IDF*-filtered word tokens as emis-
sion and RSI-filtered words as emissions.
Evaluation Score KIT MSR ACL
Pk
HMM + TF-IDF* 0.06 0.06 0.05
HMM + RSI 0.01 0.02 0.01
Precision
HMM + TF-IDF* 32.01 30.47 32.85
HMM + RSI 41.10 41.01 42.70
Recall
HMM + TF-IDF* 39.32 36.09 38.08
HMM + RSI 47.38 46.39 48.95
F-Measure
HMM + TF-IDF* 35.29 33.04 35.27
HMM + RSI 44.01 43.53 45.61
The evaluation results on all data sets listed in
Table 3 show that according F-Measure and Pk
scores, considering words with high RSI values as
HMM emission significantly improve over the base-
line method of choosing word tokens with high TF-
IDF* scores.
5 Conclusions
In this work we propose the Rhetorical Structure In-
dex (RSI), a method to identify structurally impor-
tant terms in lectures. Experiments show that terms
with high RSI values are better candidates than those
with high TF-IDF values when used by an HMM-
based segmenter as state emissions. In other words,
terms with high RSI values are more likely to be
structural cues in lectures independent of the lecture
topic. In the future we will run experiments on ASR
output and incorporate other prosodic features such
as pitch, intensity, duration into the RSI to improve
this metric for structural analysis of lectures and ap-
ply the RSI to other structure discovery applications
such as dialogue segmentation.
Acknowledgments
The authors gratefully acknowledge the support by
an interACT student exchange scholarship. The re-
search leading to these results has received funding
from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement no
287658.
We would like to thank Jan Niehues and Teresa
Herrmann for their suggestions and help.
787
References
A. Balagopalan, L.L. Balasubramanian, V. Balasubrama-
nian, N. Chandrasekharan, and A. Damodar. 2012.
Automatic keyphrase extraction and segmentation of
video lectures. In Technology Enhanced Education
(ICTEE), 2012 IEEE International Conference on,
pages 1?10.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Mach.
Learn., 34(1-3):177?210, February.
Mark J. F. Gales and Steve J. Young. 2007. The ap-
plication of hidden markov models in speech recog-
nition. Foundations and Trends in Signal Processing,
1(3):195?304.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Andrew McCallum, Dayne Freitag, and Fernando C. N.
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In Proceed-
ings of the Seventeenth International Conference on
Machine Learning, ICML ?00, pages 591?598, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmenta-
tion by topic. In ECDL, pages 113?125.
Melissa Sherman and Yang Liu. 2008. Using hid-
den markov models for topic segmentation of meeting
transcripts. In SLT, pages 185?188.
Paul van Mulbregt, Ira Carp, Lawrence Gillick, Steve
Lowe, and Jon Yamron. 1998. Text segmentation and
topic tracking on broadcast news via a hidden markov
model approach. In ICSLP.
Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.
2007. Improving lecture speech summarization using
rhetorical information. In ASRU, pages 195?200.
Justin Jian Zhang, Shilei Huang, and Pascale Fung. 2008.
Rshmm++ for extractive lecture speech summariza-
tion. In SLT, pages 161?164.
Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.
2010. Extractive speech summarization using shallow
rhetorical structure modeling. IEEE Transactions on
Audio, Speech & Language Processing, 18(6):1147?
1157.
788
Segmentation for Efficient Supervised Language Annotation with an
Explicit Cost-Utility Tradeoff
Matthias Sperber1, Mirjam Simantzik2, Graham Neubig3, Satoshi Nakamura3, Alex Waibel1
1Karlsruhe Institute of Technology, Institute for Anthropomatics, Germany
2Mobile Technologies GmbH, Germany
3Nara Institute of Science and Technology, AHC Laboratory, Japan
matthias.sperber@kit.edu, mirjam.simantzik@jibbigo.com, neubig@is.naist.jp
s-nakamura@is.naist.jp, waibel@kit.edu
Abstract
In this paper, we study the problem of manu-
ally correcting automatic annotations of natu-
ral language in as efficient a manner as pos-
sible. We introduce a method for automati-
cally segmenting a corpus into chunks such
that many uncertain labels are grouped into
the same chunk, while human supervision
can be omitted altogether for other segments.
A tradeoff must be found for segment sizes.
Choosing short segments allows us to reduce
the number of highly confident labels that are
supervised by the annotator, which is useful
because these labels are often already correct
and supervising correct labels is a waste of
effort. In contrast, long segments reduce the
cognitive effort due to context switches. Our
method helps find the segmentation that opti-
mizes supervision efficiency by defining user
models to predict the cost and utility of su-
pervising each segment and solving a con-
strained optimization problem balancing these
contradictory objectives. A user study demon-
strates noticeable gains over pre-segmented,
confidence-ordered baselines on two natural
language processing tasks: speech transcrip-
tion and word segmentation.
1 Introduction
Many natural language processing (NLP) tasks re-
quire human supervision to be useful in practice,
be it to collect suitable training material or to meet
some desired output quality. Given the high cost of
human intervention, how to minimize the supervi-
sion effort is an important research problem. Previ-
ous works in areas such as active learning, post edit-
(a) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
(b) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
(c) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
Figure 1: Three automatic transcripts of the sentence ?It
was a bright cold day in April, and the clocks were strik-
ing thirteen?, with recognition errors in parentheses. The
underlined parts are to be corrected by a human for (a)
sentences, (b) words, or (c) the proposed segmentation.
ing, and interactive pattern recognition have inves-
tigated this question with notable success (Settles,
2008; Specia, 2011; Gonza?lez-Rubio et al., 2010).
The most common framework for efficient anno-
tation in the NLP context consists of training an NLP
system on a small amount of baseline data, and then
running the system on unannotated data to estimate
confidence scores of the system?s predictions (Set-
tles, 2008). Sentences with the lowest confidence
are then used as the data to be annotated (Figure 1
(a)). However, it has been noted that when the NLP
system in question already has relatively high accu-
racy, annotating entire sentences can be wasteful, as
most words will already be correct (Tomanek and
Hahn, 2009; Neubig et al., 2011). In these cases, it
is possible to achieve much higher benefit per anno-
tated word by annotating sub-sentential units (Fig-
ure 1 (b)).
However, as Settles et al. (2008) point out, sim-
ply maximizing the benefit per annotated instance
is not enough, as the real supervision effort varies
169
Transactions of the Association for Computational Linguistics, 2 (2014) 169?180. Action Editor: Eric Fosler-Lussier.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
1 3 5 7 9 11 13 15 17 19
0
2
4
6
Segment length
A
vg
. t
im
e 
/ i
ns
ta
nc
e 
[s
ec
]
 
 
Transcription task
Word segmentation task
Figure 2: Average annotation time per instance, plotted
over different segment lengths. For both tasks, the effort
clearly increases for short segments.
greatly across instances. This is particularly impor-
tant in the context of choosing segments to annotate,
as human annotators heavily rely on semantics and
context information to process language, and intu-
itively, a consecutive sequence of words can be su-
pervised faster and more accurately than the same
number of words spread out over several locations in
a text. This intuition can also be seen in our empiri-
cal data in Figure 2, which shows that for the speech
transcription and word segmentation tasks described
later in Section 5, short segments had a longer anno-
tation time per word. Based on this fact, we argue
it would be desirable to present the annotator with
a segmentation of the data into easily supervisable
chunks that are both large enough to reduce the num-
ber of context switches, and small enough to prevent
unnecessary annotation (Figure 1 (c)).
In this paper, we introduce a new strategy for nat-
ural language supervision tasks that attempts to op-
timize supervision efficiency by choosing an appro-
priate segmentation. It relies on a user model that,
given a specific segment, predicts the cost and the
utility of supervising that segment. Given this user
model, the goal is to find a segmentation that mini-
mizes the total predicted cost while maximizing the
utility. We balance these two criteria by defining a
constrained optimization problem in which one cri-
terion is the optimization objective, while the other
criterion is used as a constraint. Doing so allows
specifying practical optimization goals such as ?re-
move as many errors as possible given a limited time
budget,? or ?annotate data to obtain some required
classifier accuracy in as little time as possible.?
Solving this optimization task is computationally
difficult, an NP-hard problem. Nevertheless, we
demonstrate that by making realistic assumptions
about the segment length, an optimal solution can
be found using an integer linear programming for-
mulation for mid-sized corpora, as are common for
supervised annotation tasks. For larger corpora, we
provide simple heuristics to obtain an approximate
solution in a reasonable amount of time.
Experiments over two example scenarios demon-
strate the usefulness of our method: Post editing
for speech transcription, and active learning for
Japanese word segmentation. Our model predicts
noticeable efficiency gains, which are confirmed in
experiments with human annotators.
2 Problem Definition
The goal of our method is to find a segmentation
over a corpus of word tokens wN1 that optimizes
supervision efficiency according to some predictive
user model. The user model is denoted as a set of
functions ul,k(wba) that evaluate any possible sub-
sequence wba of tokens in the corpus according to
criteria l2L, and supervision modes k2K.
Let us illustrate this with an example. Sperber et
al. (2013) defined a framework for speech transcrip-
tion in which an initial, erroneous transcript is cre-
ated using automatic speech recognition (ASR), and
an annotator corrects the transcript either by correct-
ing the words by keyboard, by respeaking the con-
tent, or by leaving the words as is. In this case,
we could define K={TYPE, RESPEAK, SKIP}, each
constant representing one of these three supervision
modes. Our method will automatically determine
the appropriate supervision mode for each segment.
The user model in this example might evaluate ev-
ery segment according to two criteria L, a cost crite-
rion (in terms of supervision time) and a utility cri-
terion (in terms of number of removed errors), when
using each mode. Intuitively, respeaking should be
assigned both lower cost (because speaking is faster
than typing), but also lower utility than typing on a
keyboard (because respeaking recognition errors can
occur). The SKIP mode denotes the special, unsuper-
vised mode that always returns 0 cost and 0 utility.
Other possible supervision modes include mul-
tiple input modalities (Suhm et al., 2001), several
human annotators with different expertise and cost
170
(Donmez and Carbonell, 2008), and correction vs.
translation from scratch in machine translation (Spe-
cia, 2011). Similarly, cost could instead be ex-
pressed in monetary terms, or the utility function
could predict the improvement of a classifier when
the resulting annotation is not intended for direct hu-
man consumption, but as training data for a classifier
in an active learning framework.
3 Optimization Framework
Given this setting, we are interested in simulta-
neously finding optimal locations and supervision
modes for all segments, according to the given cri-
teria. Each resulting segment will be assigned ex-
actly one of these supervision modes. We de-
note a segmentation of the N tokens of corpus wN1
into M?N segments by specifying segment bound-
ary markers sM+11 =(s1=1, s2, . . . , sM+1=N+1).
Setting a boundary marker si=a means that we
put a segment boundary before the a-th word to-
ken (or the end-of-corpus marker for a=N+1).
Thus our corpus is segmented into token sequences
[(wsj , . . . , wsj+1 1)]Mj=1. The supervision modes
assigned to each segment are denoted by mj . We
favor those segmentations that minimize the cumu-
lative valuePMj=1[ul,mj (wsj+1sj )] for each criterion l.
For any criterion where larger values are intuitively
better, we flip the sign before defining ul,mj (wsj+1sj )
to maintain consistency (e.g. negative number of er-
rors removed).
3.1 Multiple Criteria Optimization
In the case of a single criterion (|L|=1), we obtain
a simple, single-objective unconstrained linear opti-
mization problem, efficiently solvable via dynamic
programming (Terzi and Tsaparas, 2006). However,
in practice one usually encounters several compet-
ing criteria, such as cost and utility, and here we
will focus on this more realistic setting. We balance
competing criteria by using one as an optimization
objective, and the others as constraints.1 Let crite-
1This approach is known as the bounded objective function
method in multi-objective optimization literature (Marler and
Arora, 2004). The very popular weighted sum method merges
criteria into a single efficiency measure, but is problematic in
our case because the number of supervised tokens is unspec-
ified. Unless the weights are carefully chosen, the algorithm
might find, e.g., the completely unsupervised or completely su-
(at)% (what?s)% a% bright% ?%
[RESPEAK:1.5/2]/
[SKIP:0/0]/
1/ cold%2/ 3/ 4/ 5/ 6/
[TYPE:2/5]/[TYPE:1/4]/
[TYPE:1/4]/
[RESPEAK:0/3]/[SKIP:0/0]/
Figure 3: Excerpt of a segmentation graph for an ex-
ample transcription task similar to Figure 1 (some edges
are omitted for readability). Edges are labeled with their
mode, predicted number of errors that can be removed,
and necessary supervision time. A segmentation scheme
might prefer solid edges over dashed ones in this exam-
ple.
rion l0 be the optimization objective criterion, and
let Cl denote the constraining constants for the cri-
teria l 2 L l0 = L \ {l0}. We state the optimization
problem:
min
M ;sM+11 ;mM1
MX
j=1
?
ul0,mj
 
wsj+1sj
 ?
s.t.
MX
j=1
?
ul,mj
 
wsj+1sj
 ?
? Cl (8l 2 L l0)
This constrained optimization problem is difficult
to solve. In fact, the NP-hard multiple-choice knap-
sack problem (Pisinger, 1994) corresponds to a spe-
cial case of our problem in which the number of seg-
ments is equal to the number of tokens, implying
that our more general problem is NP-hard as well.
In order to overcome this problem, we refor-
mulate search for the optimal segmentation as a
resource-constrained shortest path problem in a di-
rected, acyclic multigraph. While still not efficiently
solvable in theory, this problem is well studied in
domains such as vehicle routing and crew schedul-
ing (Irnich and Desaulniers, 2005), and it is known
that in many practical situations the problem can
be solved reasonably efficiently using integer linear
programming relaxations (Toth and Vigo, 2001).
In our formalism, the set of nodes V represents
the spaces between neighboring tokens, at which the
algorithm may insert segment boundaries. A node
with index i represents a segment break before the
i-th token, and thus the sequence of the indices in
a path directly corresponds to sM+11 . Edges E de-
note the grouping of tokens between the respective
pervised segmentation to be most ?efficient.?
171
nodes into one segment. Edges are always directed
from left to right, and labeled with a supervision
mode. In addition, each edge between nodes i and j
is assigned ul,k(wj 1i ), the corresponding predicted
value for each criterion l 2 L and supervision mode
k 2 K, indicating that the supervision mode of the
j-th segment in a path directly corresponds to mj .
Figure 3 shows an example of what the result-
ing graph may look like. Our original optimization
problem is now equivalent to finding the shortest
path between the first and last nodes according to
criterion l0, while obeying the given resource con-
straints. According to a widely used formulation for
the resource constrained shortest path problem, we
can defineEij as the set of competing edges between
i and j, and express this optimization problem with
the following integer linear program (ILP):
min
x
X
i,j2V
X
k2Eij
xijkul0,k(sj 1i ) (1)
s.t.
X
i,j2V
X
k2Eij
xijkul,k(sj 1i ) ? Cl
(8l 2 L l0)
(2)
X
i2V
k2Eij
xijk =
X
i2V
k2Eij
xjik
(8j 2 V \{1, n})
(3)
X
j2V
k2E1j
x1jk = 1 (4)
X
i2V
k2Ein
xink = 1 (5)
xijk 2 {0, 1} (8xijk 2 x) (6)
The variables x={xijk|i, j 2 V , k 2 Eij} denote
the activation of the k?th edge between nodes i and
j. The shortest path according to the minimization
objective (1), that still meets the resource constraints
for the specified criteria (2), is to be computed. The
degree constraints (3,4,5) specify that all but the first
and last nodes must have as many incoming as out-
going edges, while the first node must have exactly
one outgoing, and the last node exactly one incom-
ing edge. Finally, the integrality condition (6) forces
all edges to be either fully activated or fully deacti-
vated. The outlined problem formulation can solved
directly by using off-the-shelf ILP solvers, here we
employ GUROBI (Gurobi Optimization, 2012).
3.2 Heuristics for Approximation
In general, edges are inserted for every supervision
mode between every combination of two nodes. The
search space can be constrained by removing some
of these edges to increase efficiency. In this study,
we only consider edges spanning at most 20 tokens.
For cases in which larger corpora are to be anno-
tated, or when the acceptable delay for delivering re-
sults is small, a suitable segmentation can be found
approximately. The easiest way would be to parti-
tion the corpus, e.g. according to its individual doc-
uments, divide the budget constraints evenly across
all partitions, and then segment each partition inde-
pendently. More sophisticated methods might ap-
proximate the Pareto front for each partition, and
distribute the budgets in an intelligent way.
4 User Modeling
While the proposed framework is able to optimize
the segmentation with respect to each criterion, it
also rests upon the assumption that we can provide
user models ul,k(wj 1i ) that accurately evaluate ev-
ery segment according to the specified criteria and
supervision modes. In this section, we discuss our
strategies for estimating three conceivable criteria:
annotation cost, correction of errors, and improve-
ment of a classifier.
4.1 Annotation Cost Modeling
Modeling cost requires solving a regression prob-
lem from features of a candidate segment to annota-
tion cost, for example in terms of supervision time.
Appropriate input features depend on the task, but
should include notions of complexity (e.g. a confi-
dence measure) and length of the segment, as both
are expected to strongly influence supervision time.
We propose using Gaussian process (GP) regres-
sion for cost prediction, a start-of-the-art nonpara-
metric Bayesian regression technique (Rasmussen
and Williams, 2006)2. As reported on a similar
task by Cohn and Specia (2013), and confirmed by
our preliminary experiments, GP regression signifi-
cantly outperforms popular techniques such as sup-
2Code available at http://www.gaussianprocess.org/gpml/
172
port vector regression and least-squares linear re-
gression. We also follow their settings for GP, em-
ploying GP regression with a squared exponential
kernel with automatic relevance determination. De-
pending on the number of users and amount of train-
ing data available for each user, models may be
trained separately for each user (as we do here), or
in a combined fashion via multi-task learning as pro-
posed by Cohn and Specia (2013).
It is also crucial for the predictions to be reliable
throughout the whole relevant space of segments.
If the cost of certain types of segments is system-
atically underpredicted, the segmentation algorithm
might be misled to prefer these, possibly a large
number of times.3 An effective trick to prevent such
underpredictions is to predict the log time instead of
the actual time. In this way, errors in the critical low
end are penalized more strongly, and the time can
never become negative.
4.2 Error Correction Modeling
As one utility measure, we can use the number of
errors corrected, a useful measure for post editing
tasks over automatically produced annotations. In
order to measure how many errors can be removed
by supervising a particular segment, we must es-
timate both how many errors are in the automatic
annotation, and how reliably a human can remove
these for a given supervision mode.
Most machine learning techniques can estimate
confidence scores in the form of posterior probabil-
ities. To estimate the number of errors, we can sum
over one minus the posterior for all tokens, which
estimates the Hamming distance from the reference
annotation. This measure is appropriate for tasks in
which the number of tokens is fixed in advance (e.g.
a part-of-speech estimation task), and a reasonable
approximation for tasks in which the number of to-
kens is not known in advance (e.g. speech transcrip-
tion, cf. Section 5.1.1).
Predicting the particular tokens at which a human
will make a mistake is known to be a difficult task
(Olson and Olson, 1990), but a simplifying constant
3For instance, consider a model that predicts well for seg-
ments of medium size or longer, but underpredicts the supervi-
sion time of single-token segments. This may lead the segmen-
tation algorithm to put every token into its own segment, which
is clearly undesirable.
human error rate can still be useful. For example,
in the task from Section 2, we may suspect a certain
number of errors in a transcript segment, and predict,
say, 95% of those errors to be removed via typing,
but only 85% via respeaking.
4.3 Classifier Improvement Modeling
Another reasonable utility measure is accuracy of a
classifier trained on the data we choose to annotate
in an active learning framework. Confidence scores
have been found useful for ranking particular tokens
with regards to how much they will improve a clas-
sifier (Settles, 2008). Here, we may similarly score
segment utility as the sum of its token confidences,
although care must be taken to normalize and cali-
brate the token confidences to be linearly compara-
ble before doing so. While the resulting utility score
has no interpretation in absolute terms, it can still be
used as an optimization objective (cf. Section 5.2.1).
5 Experiments
In this section, we present experimental results ex-
amining the effectiveness of the proposed method
over two tasks: speech transcription and Japanese
word segmentation.4
5.1 Speech Transcription Experiments
Accurate speech transcripts are a much-demanded
NLP product, useful by themselves, as training ma-
terial for ASR, or as input for follow-up tasks like
speech translation. With recognition accuracies
plateauing, manually correcting (post editing) auto-
matic speech transcripts has become popular. Com-
mon approaches are to identify words (Sanchez-
Cortina et al., 2012) or (sub-)sentences (Sperber et
al., 2013) of low confidence, and have a human edi-
tor correct these.
5.1.1 Experimental Setup
We conduct a user study in which participants
post-edited speech transcripts, given a fixed goal
word error rate. The transcription setup was such
that the transcriber could see the ASR transcript of
parts before and after the segment that he was edit-
ing, providing context if needed. When imprecise
time alignment resulted in segment breaks that were
4Software and experimental data can be downloaded from
http://www.msperber.com/research/tacl-segmentation/
173
slightly ?off,? as happened occasionally, that context
helped guess what was said. The segment itself was
transcribed from scratch, as opposed to editing the
ASR transcript; besides being arguably more effi-
cient when the ASR transcript contains many mis-
takes (Nanjo et al., 2006; Akita et al., 2009), prelim-
inary experiments also showed that supervision time
is far easier to predict this way. Figure 4 illustrates
what the setup looked like.
We used a self-developed transcription tool to
conduct experiments. It presents our computed seg-
ments one by one, allows convenient input and play-
back via keyboard shortcuts, and logs user interac-
tions with their time stamps. A selection of TED
talks5 (English talks on technology, entertainment,
and design) served as experimental data. While
some of these talks contain jargon such as medi-
cal terms, they are presented by skilled speakers,
making them comparably easy to understand. Initial
transcripts were created using the Janus recognition
toolkit (Soltau et al., 2001) with a standard, TED-
optimized setup. We used confusion networks for
decoding and obtaining confidence scores.
For reasons of simplicity, and better compara-
bility to our baseline, we restricted our experiment
to two supervision modes: TYPE and SKIP. We
conducted experiments with 3 participants, 1 with
several years of experience in transcription, 2 with
none. Each participant received an explanation on
the transcription guidelines, and a short hands-on
training to learn to use our tool. Next, they tran-
scribed a balanced selection of 200 segments of
varying length and quality in random order. This
data was used to train the user models.
Finally, each participant transcribed another 2
TED talks, with word error rate (WER) 19.96%
(predicted: 22.33%). We set a target (predicted)
WER of 15% as our optimization constraint,6 and
minimize the predicted supervision time as our ob-
jective function. Both TED talks were transcribed
once using the baseline strategy, and once using the
proposed strategy. The order of both strategies was
reversed between talks, to minimize learning bias
due to transcribing each talk twice.
The baseline strategy was adopted according to
5www.ted.com
6Depending on the level of accuracy required by our final
application, this target may be set lower or higher.
Sperber et al. (2013): We segmented the talk into
natural, subsentential units, using Matusov et al.
(2006)?s segmenter, which we tuned to reproduce
the TED subtitle segmentation, producing a mean
segment length of 8.6 words. Segments were added
in order of increasing average word confidence, until
the user model predicted a WER<15%. The second
segmentation strategy was the proposed method,
similarly with a resource constraint of WER<15%.
Supervision time was predicted via GP regres-
sion (cf. Section 4.1), using segment length, au-
dio duration, and mean confidence as input features.
The output variable was assumed subject to addi-
tive Gaussian noise with zero mean, a variance of
5 seconds was chosen empirically to minimize the
mean squared error. Utility prediction (cf. Section
4.2) was based on posterior scores obtained from
the confusion networks. We found it important to
calibrate them, as the posteriors were overconfident
especially in the upper range. To do so, we automat-
ically transcribed a development set of TED data,
grouped the recognized words into buckets accord-
ing to their posteriors, and determined the average
number of errors per word in each bucket from an
alignment with the reference transcript. The map-
ping from average posterior to average number of
errors was estimated via GP regression. The result
was summed over all tokens, and multiplied by a
constant human confidence, separately determined
for each participant.7
5.1.2 Simulation Results
To convey a better understanding of the poten-
tial gains afforded by our method, we first present a
simulated experiment. We assume a transcriber who
makes no mistakes, and needs exactly the amount of
time predicted by a user model trained on the data of
a randomly selected participant. We compare three
scenarios: A baseline simulation, in which the base-
line segments are transcribed in ascending order of
confidence; a simulation using the proposed method,
in which we change the WER constraint in small in-
crements; finally, an oracle simulation, which uses
7More elaborate methods for WER estimation exist, such as
by Ogawa et al. (2013), but if our method achieves improve-
ments using simple Hamming distance, incorporating more so-
phisticated measures will likely achieve similar, or even better
accuracy.
174
(3) SKIP: ?nineteen forty six until today you see the green?
(4) TYPE: <annotator types: ?is the traditional?>
(5) SKIP: ?Interstate conflict?
(6) TYPE: <annotator types: ?the ones we used to?>
(7) SKIP: . . .
Figure 4: Result of our segmentation method (excerpt).
TYPE segments are displayed empty and should be tran-
scribed from scratch. For SKIP segments, the ASR tran-
script is displayed to provide context. When annotating a
segment, the corresponding audio is played back.
0 10 20 30 40 50 60
0
5
10
15
20
25
Post editing time [min]
R
es
ul
tin
g 
W
ER
 [%
]
 
 
Baseline
Proposed
Oracle
Figure 5: Simulation of post editing on example TED
talk. The proposed method reduces the WER consider-
ably faster than the baseline at first, later both converge.
The much superior oracle simulation indicates room for
further improvement.
the proposed method, but uses a utility model that
knows the actual number of errors in each segment.
For each supervised segment, we simply replace the
ASR output with the reference, and measure the re-
sulting WER.
Figure 5 shows the simulation on an example
TED talk, based on an initial transcript with 21.9%
WER. The proposed method is able to reduce the
WER faster than the baseline, up to a certain point
where they converge. The oracle simulation is even
faster, indicating room for improvement through
better confidence scores.
5.1.3 User Study Results
Table 1 shows the results of the user study. First,
we note that the WER estimation by our utility
model was off by about 2.5%: While the predicted
improvement in WER was from 22.33% to 15.0%,
the actual improvement was from 19.96% to about
12.5%. The actual resulting WER was consistent
Participant Baseline ProposedWER Time WER Time
P1 12.26 44:05 12.18 33:01
P2 12.75 36:19 12.77 29:54
P3 12.70 52:42 12.50 37:57
AVG 12.57 44:22 12.48 33:37
Table 1: Transcription task results. For each user, the
resultingWER [%] after supervision is shown, along with
the time [min] they needed. The unsupervised WER was
19.96%.
across all users, and we observe strong, consistent
reductions in supervision time for all participants.
Prediction of the necessary supervision time was ac-
curate: Averaged over participants, 45:41 minutes
were predicted for the baseline, 44:22 minutes mea-
sured. For the proposed method, 32:11 minutes were
predicted, 33:37 minutes measured. On average,
participants removed 6.68 errors per minute using
the baseline, and 8.93 errors per minute using the
proposed method, a speed-up of 25.2%.
Note that predicted and measured values are not
strictly comparable: In the experiments, to provide
a fair comparison participants transcribed the same
talks twice (once using baseline, once the proposed
method, in alternating order), resulting in a notice-
able learning effect. The user model, on the other
hand, is trained to predict the case in which a tran-
scriber conducts only one transcription pass.
As an interesting finding, without being informed
about the order of baseline and proposed method,
participants reported that transcribing according to
the proposed segmentation seemed harder, as they
found the baseline segmentation more linguistically
reasonable. However, this perceived increase in dif-
ficulty did not show in efficiency numbers.
5.2 Japanese Word Segmentation Experiments
Word segmentation is the first step in NLP for lan-
guages that are commonly written without word
boundaries, such as Japanese and Chinese. We ap-
ply our method to a task in which we domain-adapt a
word segmentation classifier via active learning. In
this experiment, participants annotated whether or
not a word boundary occurred at certain positions in
a Japanese sentence. The tokens to be grouped into
segments are positions between adjacent characters.
175
5.2.1 Experimental Setup
Neubig et al. (2011) have proposed a pointwise
method for Japanese word segmentation that can be
trained using partially annotated sentences, which
makes it attractive in combination with active learn-
ing, as well as our segmentation method. The
authors released their method as a software pack-
age ?KyTea? that we employed in this user study.
We used KyTea?s active learning domain adaptation
toolkit8 as a baseline.
For data, we used the Balanced Corpus of Con-
temporary Written Japanese (BCCWJ), created by
Maekawa (2008), with the internet Q&A subcor-
pus as in-domain data, and the whitepaper subcor-
pus as background data, a domain adaptation sce-
nario. Sentences were drawn from the in-domain
corpus, and the manually annotated data was then
used to train KyTea, along with the pre-annotated
background data. The goal (objective function) was
to improve KyTea?s classification accuracy on an in-
domain test set, given a constrained time budget of
30 minutes. There were again 2 supervision modes:
ANNOTATE and SKIP. Note that this is essentially a
batch active learning setup with only one iteration.
We conducted experiments with one expert with
several years of experience with Japanese word seg-
mentation annotation, and three non-expert native
speakers with no prior experience. Japanese word
segmentation is not a trivial task, so we provided
non-experts with training, including explanation of
the segmentation standard, a supervised test with
immediate feedback and explanations, and hands-on
training to get used to the annotation software.
Supervision time was predicted via GP regression
(cf. Section 4.1), using the segment length and mean
confidence as input features. As before, the output
variable was assumed subject to additive Gaussian
noise with zero mean and 5 seconds variance. To ob-
tain training data for these models, each participant
annotated about 500 example instances, drawn from
the adaptation corpus, grouped into segments and
balanced regarding segment length and difficulty.
For utility modeling (cf. Section 4.3), we first nor-
malized KyTea?s confidence scores, which are given
in terms of SVM margin, using a sigmoid function
(Platt, 1999). The normalization parameter was se-
8http://www.phontron.com/kytea/active.html
lected so that the mean confidence on a development
set corresponded to the actual classifier accuracy.
We derive our measure of classifier improvement for
correcting a segment by summing over one minus
the calibrated confidence for each of its tokens. To
analyze how well this measure describes the actual
training utility, we trained KyTea using the back-
ground data plus disjoint groups of 100 in-domain
instances with similar probabilities and measured
the achieved reduction of prediction errors. The cor-
relation between each group?s mean utility and the
achieved error reduction was 0.87. Note that we ig-
nore the decaying returns usually observed as more
data is added to the training set. Also, we did not
attempt to model user errors. Employing a con-
stant base error rate, as in the transcription scenario,
would change segment utilities only by a constant
factor, without changing the resulting segmentation.
After creating the user models, we conducted the
main experiment, in which each participant anno-
tated data that was selected from a pool of 1000
in-domain sentences using two strategies. The first,
baseline strategy was as proposed by Neubig et al.
(2011). Queries are those instances with the low-
est confidence scores. Each query is then extended
to the left and right, until a word boundary is pre-
dicted. This strategy follows similar reasoning as
was the premise to this paper: To decide whether or
not a position in a text corresponds to a word bound-
ary, the annotator has to acquire surrounding context
information. This context acquisition is relatively
time consuming, so he might as well label the sur-
rounding instances with little additional effort. The
second strategy was our proposed, more principled
approach. Queries of both methods were shuffled
to minimize bias due to learning effects. Finally, we
trained KyTea using the results of both methods, and
compared the achieved classifier improvement and
supervision times.
5.2.2 User Study Results
Table 2 summarizes the results of our experi-
ment. It shows that the annotations by each partic-
ipant resulted in a better classifier for the proposed
method than the baseline, but also took up consider-
ably more time, a less clear improvement than for
the transcription task. In fact, the total error for
time predictions was as high as 12.5% on average,
176
Participant Baseline ProposedTime Acc. Time Acc.
Expert 25:50 96.17 32:45 96.55
NonExp1 22:05 95.79 26:44 95.98
NonExp2 23:37 96.15 31:28 96.21
NonExp3 25:23 96.38 33:36 96.45
Table 2: Word segmentation task results, for our ex-
pert and 3 non-expert participants. For each participant,
the resulting classifier accuracy [%] after supervision is
shown, along with the time [min] they needed. The unsu-
pervised accuracy was 95.14%.
where the baseline method tended take less time than
predicted, the proposed method more time. This is
in contrast to a much lower total error (within 1%)
when cross-validating our user model training data.
This is likely due to the fact that the data for train-
ing the user model was selected in a balanced man-
ner, as opposed to selecting difficult examples, as
our method is prone to do. Thus, we may expect
much better predictions when selecting user model
training data that is more similar to the test case.
Plotting classifier accuracy over annotation time
draws a clearer picture. Let us first analyze the re-
sults for the expert annotator. Figure 6 (E.1) shows
that the proposed method resulted in consistently
better results, indicating that time predictions were
still effective. Note that this comparison may put the
proposed method at a slight disadvantage by com-
paring intermediate results despite optimizing glob-
ally.
For the non-experts, the improvement over the
baseline is less consistent, as can be seen in Fig-
ure 6 (N.1) for one representative. According to
our analysis, this can be explained by two factors:
(1) The non-experts? annotation error (6.5% on av-
erage) was much higher than the expert?s (2.7%),
resulting in a somewhat irregular classifier learn-
ing curve. (2) The variance in annotation time
per segment was consistently higher for the non-
experts than the expert, indicated by an average
per-segment prediction error of 71% vs. 58% rela-
tive to the mean actual value, respectively. Infor-
mally speaking, non-experts made more mistakes,
and were more strongly influenced by the difficulty
of a particular segment (which was higher on av-
erage with the proposed method, as indicated by a
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
 
 
Prop.
Basel
N.1E.1 N.2E.2 N.3E.3 N.4E.4
Annotation time [min.]
Classifier Accu
racy
.
Figure 6: Classifier improvement over time, depicted for
the expert (E) and a non-expert (N). The graphs show
numbers based on (1) actual annotations and user mod-
els as in Sections 4.1 and 4.3, (2) error-free annotations,
(3) measured times replaced by predicted times, and (4)
both reference annotations and replaced time predictions.
lower average confidence).9
In Figures 6 (2-4) we present a simulation experi-
ment in which we first pretend as if annotators made
no mistakes, then as if they needed exactly as much
time as predicted for each segment, and then both.
This cheating experiment works in favor of the pro-
posed method, especially for the non-expert. We
may conclude that our segmentation approach is ef-
fective for the word segmentation task, but requires
more accurate time predictions. Better user models
will certainly help, although for the presented sce-
nario our method may be most useful for an expert
annotator.
9Note that the non-expert in the figure annotated much faster
than the expert, which explains the comparable classification
result despite making more annotation errors. This is in contrast
to the other non-experts, who were slower.
177
5.3 Computational Efficiency
Since our segmentation algorithm does not guar-
antee polynomial runtime, computational efficiency
was a concern, but did not turn out problematic.
On a consumer laptop, the solver produced seg-
mentations within a few seconds for a single docu-
ment containing several thousand tokens, and within
hours for corpora consisting of several dozen doc-
uments. Runtime increased roughly quadratically
with respect to the number of segmented tokens. We
feel that this is acceptable, considering that the time
needed for human supervision will likely dominate
the computation time, and reasonable approxima-
tions can be made as noted in Section 3.2.
6 Relation to Prior Work
Efficient supervision strategies have been studied
across a variety of NLP-related research areas, and
received increasing attention in recent years. Ex-
amples include post editing for speech recogni-
tion (Sanchez-Cortina et al., 2012), interactive ma-
chine translation (Gonza?lez-Rubio et al., 2010), ac-
tive learning for machine translation (Haffari et al.,
2009; Gonza?lez-Rubio et al., 2011) and many other
NLP tasks (Olsson, 2009), to name but a few studies.
It has also been recognized by the active learn-
ing community that correcting the most useful parts
first is often not optimal in terms of efficiency, since
these parts tend to be the most difficult to manually
annotate (Settles et al., 2008). The authors advocate
the use of a user model to predict the supervision ef-
fort, and select the instances with best ?bang-for-the-
buck.? This prediction of supervision effort was suc-
cessful, and was further refined in other NLP-related
studies (Tomanek et al., 2010; Specia, 2011; Cohn
and Specia, 2013). Our approach to user modeling
using GP regression is inspired by the latter.
Most studies on user models consider only super-
vision effort, while neglecting the accuracy of hu-
man annotations. The view on humans as a perfect
oracle has been criticized (Donmez and Carbonell,
2008), since human errors are common and can
negatively affect supervision utility. Research on
human-computer-interaction has identified the mod-
eling of human errors as very difficult (Olson and
Olson, 1990), depending on factors such as user ex-
perience, cognitive load, user interface design, and
fatigue. Nevertheless, even the simple error model
used in our post editing task was effective.
The active learning community has addressed the
problem of balancing utility and cost in some more
detail. The previously reported ?bang-for-the-buck?
approach is a very simple, greedy approach to com-
bine both into one measure. A more theoretically
founded scalar optimization objective is the net ben-
efit (utility minus costs) as proposed by Vijaya-
narasimhan and Grauman (2009), but unfortunately
is restricted to applications where both can be ex-
pressed in terms of the same monetary unit. Vijaya-
narasimhan et al. (2010) and Donmez and Carbonell
(2008) use a more practical approach that specifies a
constrained optimization problem by allowing only
a limited time budget for supervision. Our approach
is a generalization thereof and allows either specify-
ing an upper bound on the predicted cost, or a lower
bound on the predicted utility.
The main novelty of our presented approach is
the explicit modeling and selection of segments of
various sizes, such that annotation efficiency is opti-
mized according to the specified constraints. While
some works (Sassano and Kurohashi, 2010; Neubig
et al., 2011) have proposed using subsentential seg-
ments, we are not aware of any previous work that
explicitly optimizes that segmentation.
7 Conclusion
We presented a method that can effectively choose
a segmentation of a language corpus that optimizes
supervision efficiency, considering not only the ac-
tual usefulness of each segment, but also the anno-
tation cost. We reported noticeable improvements
over strong baselines in two user studies. Future user
experiments with more participants would be desir-
able to verify our observations, and allow further
analysis of different factors such as annotator ex-
pertise. Also, future research may improve the user
modeling, which will be beneficial for our method.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n 287658 Bridges Across the Language
Divide (EU-BRIDGE).
178
References
Yuya Akita, Masato Mimura, and Tatsuya Kawahara.
2009. Automatic Transcription System for Meetings
of the Japanese National Congress. In Interspeech,
pages 84?87, Brighton, UK.
Trevor Cohn and Lucia Specia. 2013. Modelling Anno-
tator Bias with Multi-task Gaussian Processes: An Ap-
plication to Machine Translation Quality Estimation.
In Association for Computational Linguistics Confer-
ence (ACL), Sofia, Bulgaria.
Pinar Donmez and Jaime Carbonell. 2008. Proactive
Learning : Cost-Sensitive Active Learning with Mul-
tiple Imperfect Oracles. In Conference on Information
and Knowledge Management (CIKM), pages 619?628,
Napa Valley, CA, USA.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2010. Balancing User Effort and
Translation Error in Interactive Machine Translation
Via Confidence Measures. In Association for Compu-
tational Linguistics Conference (ACL), Short Papers
Track, pages 173?177, Uppsala, Sweden.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2011. An active learning scenario
for interactive machine translation. In International
Conference on Multimodal Interfaces (ICMI), pages
197?200, Alicante, Spain.
Gurobi Optimization. 2012. Gurobi Optimizer Refer-
ence Manual.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active Learning for Statistical Phrase-based
Machine Translation. In North American Chapter
of the Association for Computational Linguistics -
Human Language Technologies Conference (NAACL-
HLT), pages 415?423, Boulder, CO, USA.
Stefan Irnich and Guy Desaulniers. 2005. Shortest Path
Problems with Resource Constraints. In Column Gen-
eration, pages 33?65. Springer US.
Kikuo Maekawa. 2008. Balanced Corpus of Contem-
porary Written Japanese. In International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 101?102, Hyderabad, India.
R. Timothy Marler and Jasbir S. Arora. 2004. Survey
of multi-objective optimization methods for engineer-
ing. Structural and Multidisciplinary Optimization,
26(6):369?395, April.
EvgenyMatusov, ArneMauser, and Hermann Ney. 2006.
Automatic Sentence Segmentation and Punctuation
Prediction for Spoken Language Translation. In Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 158?165, Kyoto, Japan.
Hiroaki Nanjo, Yuya Akita, and Tatsuya Kawahara.
2006. Computer Assisted Speech Transcription Sys-
tem for Efficient Speech Archive. In Western Pacific
Acoustics Conference (WESPAC), Seoul, Korea.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise Prediction for Robust , Adapt-
able Japanese Morphological Analysis. In Associa-
tion for Computational Linguistics: Human Language
Technologies Conference (ACL-HLT), pages 529?533,
Portland, OR, USA.
Atsunori Ogawa, Takaaki Hori, and Atsushi Naka-
mura. 2013. Discriminative Recognition Rate Esti-
mation For N-Best List and Its Application To N-Best
Rescoring. In International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pages 6832?
6836, Vancouver, Canada.
Judith Reitman Olson and Gary Olson. 1990. The
Growth of Cognitive Modeling in Human-Computer
Interaction Since GOMS. Human-Computer Interac-
tion, 5(2):221?265, June.
Fredrik Olsson. 2009. A literature survey of active ma-
chine learning in the context of natural language pro-
cessing. Technical report, SICS Sweden.
David Pisinger. 1994. A Minimal Algorithm for the
Multiple-Choice Knapsack Problem. European Jour-
nal of Operational Research, 83(2):394?410.
John C. Platt. 1999. Probabilistic Outputs for Sup-
port Vector Machines and Comparisons to Regularized
Likelihood Methods. In Advances in Large Margin
Classifiers, pages 61?74. MIT Press.
Carl E. Rasmussen and Christopher K.I. Williams. 2006.
Gaussian Processes for Machine Learning. MIT
Press, Cambridge, MA, USA.
Isaias Sanchez-Cortina, Nicolas Serrano, Alberto San-
chis, and Alfons Juan. 2012. A prototype for Inter-
active Speech Transcription Balancing Error and Su-
pervision Effort. In International Conference on Intel-
ligent User Interfaces (IUI), pages 325?326, Lisbon,
Portugal.
Manabu Sassano and Sadao Kurohashi. 2010. Using
Smaller Constituents Rather Than Sentences in Ac-
tive Learning for Japanese Dependency Parsing. In
Association for Computational Linguistics Conference
(ACL), pages 356?365, Uppsala, Sweden.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active Learning with Real Annotation Costs. In
Neural Information Processing Systems Conference
(NIPS) - Workshop on Cost-Sensitive Learning, Lake
Tahoe, NV, United States.
Burr Settles. 2008. An Analysis of Active Learning
Strategies for Sequence Labeling Tasks. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1070?1079, Honolulu, USA.
Hagen Soltau, Florian Metze, Christian Fu?gen, and Alex
Waibel. 2001. A One-Pass Decoder Based on Poly-
morphic Linguistic Context Assignment. In Auto-
matic Speech Recognition and Understanding Work-
179
shop (ASRU), pages 214?217, Madonna di Campiglio,
Italy.
Lucia Specia. 2011. Exploiting Objective Annota-
tions for Measuring Translation Post-editing Effort. In
Conference of the European Association for Machine
Translation (EAMT), pages 73?80, Nice, France.
Matthias Sperber, Graham Neubig, Christian Fu?gen,
Satoshi Nakamura, and Alex Waibel. 2013. Efficient
Speech Transcription Through Respeaking. In Inter-
speech, pages 1087?1091, Lyon, France.
Bernhard Suhm, Brad Myers, and Alex Waibel. 2001.
Multimodal error correction for speech user inter-
faces. Transactions on Computer-Human Interaction,
8(1):60?98.
Evimaria Terzi and Panayiotis Tsaparas. 2006. Efficient
algorithms for sequence segmentation. In SIAM Con-
ference on Data Mining (SDM), Bethesda, MD, USA.
Katrin Tomanek and Udo Hahn. 2009. Semi-Supervised
Active Learning for Sequence Labeling. In Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP), pages 1039?1047, Singapore.
Katrin Tomanek, Udo Hahn, and Steffen Lohmann.
2010. A Cognitive Cost Model of Annotations Based
on Eye-Tracking Data. In Association for Compu-
tational Linguistics Conference (ACL), pages 1158?
1167, Uppsala, Sweden.
Paolo Toth and Daniele Vigo. 2001. The Vehicle Routing
Problem. Society for Industrial & Applied Mathemat-
ics (SIAM), Philadelphia.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2009. Whats It Going to Cost You?: Predicting Ef-
fort vs. Informativeness for Multi-Label Image Anno-
tations. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 2262?2269, Miami
Beach, FL, USA.
Sudheendra Vijayanarasimhan, Prateek Jain, and Kristen
Grauman. 2010. Far-sighted active learning on a bud-
get for image and video recognition. In Conference
on Computer Vision and Pattern Recognition (CVPR),
pages 3035?3042, San Francisco, CA, USA, June.
180
Phrase Pair Rescoring with Term Weightings for  
Statistical Machine Translation 
Bing Zhao   Stephan Vogel   Alex Waibel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
{bzhao, vogel+, ahw}@cs.cmu.edu 
 
Abstract 
We propose to score phrase translation 
pairs for statistical machine translation 
using term weight based models.  These 
models employ tf.idf to encode the 
weights of content and non-content words 
in phrase translation pairs.  The transla-
tion probability is then modeled by simi-
larity functions defined in a vector space.  
Two similarity functions are compared.  
Using these models in a statistical ma-
chine translation task shows significant 
improvements. 
1 Introduction 
Words can be classified as content and func-
tional words.  Content words like verbs and 
proper nouns are more informative than function 
words like "to'' and "the''.  In machine translation, 
intuitively, the informative content words should 
be emphasized more for better adequacy of the 
translation quality.  However, the standard statis-
tical translation approach does not take account 
how informative and thereby, how important a 
word is, in its translation model.  One reason is 
the difficulty to measure how informative a word 
is. Another problem is to integrate it naturally 
into the existing statistical machine translation 
framework, which typically is built on word 
alignment models, like the well-known IBM 
alignment models (Brown et al1993).  
In recent years there has been a strong ten-
dency to incorporate phrasal translation into sta-
tistical machine translation.  It directly translates 
an n-gram from the source language into an m-
gram in the target language.  The advantages are 
obvious:  It has built-in local context modeling, 
and provides reliable local word reordering.  It 
has multi-word translations, and models a word?s 
conditional fertility given a local context.  It cap-
tures idiomatic phrase translations and can be 
easily enriched with bilingual dictionaries. In 
addition, it can compensate for the segmentation 
errors made during preprocessing, i.e. word seg-
mentation errors of Chinese.  The advantage of 
using phrase-based translation in a statistical 
framework has been shown in many studies such 
as (Koehn et al 2003; Vogel et al 2003; Zens et 
al. 2002; Marcu and Wong, 2002).  However, the 
phrase translation pairs are typically extracted 
from a parallel corpus based on the Viterbi align-
ment of some word alignment models.  The leads 
to the question what probability should be as-
signed to those phrase translations.  Different 
approaches have been suggested as using relative 
frequencies (Zens et al 2002), calculate prob-
abilities based on a statistical word-to-word dic-
tionary (Vogel et al 2003) or use a linear 
interpolation of these scores (Koehn et al 2003).  
In this paper we investigate a different ap-
proach with takes the information content of 
words better into account.  Term weighting based 
vector models are proposed to encode the transla-
tion quality.  The advantage is that term weights, 
such as tf.idf, are useful to model the informa-
tiveness of words.  Highly informative content 
words usually have high tf.idf scores.  In informa-
tion retrieval this has been successfully applied to 
capture the relevance of a document to a query, 
by representing both query and documents as 
term weight vectors and use for example the 
cosine distance to calculate the similarity be-
tween query vector and document vector.  The 
idea now is to consider the source phrase as a 
?query?, and the different target phrases ex-
tracted from the bilingual corpus as translation 
candidates as a relevant ?documents?.  The co-
sine distance is then a natural choice to model the 
translation probability.  
Our approach is to apply term weighting 
schemes to transform source and target phrases 
into term vectors.  Usually content words in both 
source and target languages will be emphasized 
by large term weights.  Thus, good phrase trans-
lation pairs will share similar contours, or, to ex-
press it in a different way, will be close to each 
other in the term weight vector space.  A similar-
ity function is then defined to approximate trans-
lation probability in the vector space. 
The paper is structured as follows:  in Section 
2, our phrase-based statistical machine translation 
system is introduced; in Section 3, a phrase trans-
lation score function based on word translation 
probabilities is explained, as this will be used as a 
baseline system;  in Section 4, a vector model 
based on tf.idf is proposed together with two 
similarity functions;  in Section 5, length regu-
larization and smoothing schemes are explained 
briefly;  in Section 6, the translation experiments 
are presented; and Section 7 concludes with a 
discussion.  
2 Phrase-based Machine Translation 
In this section, the phrase-based machine transla-
tion system used in the experiments is briefly 
described: the phrase based translation models 
and the decoding algorithm, which allows for 
local word reordering.  
2.1 Translation Model 
The phrase-based statistical translation systems 
use not only word-to-word translation, extracted 
from bilingual data, but also phrase-to phrase 
translations. . Different types of extraction ap-
proaches have been described in the literature: 
syntax-based, word-alignment-based, and genu-
ine phrase alignment models.  The syntax-based 
approach has the advantage to model the gram-
mar structures using models of more or less 
structural richness, such as the syntax-based 
alignment model in (Yamada and Knight, 2001) 
or the Bilingual Bracketing in (Wu, 1997).  Popu-
lar word-alignment-based approaches usually 
rely on initial word alignments from the IBM and 
HMM alignment models (Och and Ney, 2000), 
from which the phrase pairs are then extracted.  
(Marcu and Wong 2002) and (Zhang et al 2003) 
do not rely on word alignment but model directly 
the phrase alignment. 
Because all statistical machine translation sys-
tems search for a globally optimal translation 
using the language and translation model, a trans-
lation probability has to be assigned to each 
phrase translation pair.  This score should be 
meaningful in that better translations have a 
higher probability assigned to them, and balanced 
with respect to word translations.  Bad phrase 
translations should not win over better word for 
word translations, only because they are phrases. 
Our focus here is not phrase extraction, but 
how to estimate a reasonable probability (or 
score) to better represent the translation quality 
of the extracted phrase pairs.  One major problem 
is that most phrase pairs are seen only several 
times, even in a very large corpus.  A reliable and 
effective estimation approach is explained in sec-
tion 3, and the proposed models are introduced in 
section 4.   
In our system, a collection of phrase transla-
tions is called a transducer.  Different phrase ex-
traction methods result in different transducers.  
A manual dictionary can be added to the system 
as just another transducer.  Typically, one source 
phrase is aligned with several candidate target 
phrases, with a score attached to each candidate 
representing the translation quality.  
2.2 Decoding Algorithm 
Given a set of transducers as the translation 
model (i.e. phrase translation pairs together with 
the scores of their translation quality), decoding 
is divided into several steps. 
The first step is to build a lattice by applying 
the transducers to the input source sentence.  We 
start from a lattice, which has as its only path the 
source sentence.  Then for each word or sequence 
of words in the source sentence for which we 
have an entry in the transducer new edges are 
generated and inserted into the lattice, spanning 
over the source phrase.  One new edge is created 
for each translation candidate, and the translation 
score is assigned to this edge.  The resulting lat-
tice has then all the information available from 
the translation model. 
The second step is search for a best path 
through this lattice, but not only based on the 
translation model scores but applying also the 
language model.  We start with an initial special 
sentence begin hypothesis at the first node in the 
lattice.  Hypotheses are then expanded over the 
edges, applying the language model to the partial 
translations attached to the edges.  The following 
algorithm summarizes the decoding process 
when not considering word reordering:  
 
Current node n, previous node n?; edge e 
Language model state L, L? 
Hypothesis h, h? 
Foreach node n in the lattice 
  Foreach incoming edge e in n 
      phrase = word sequence at e 
      n?     = FromNode(e) 
      foreach L in n? 
         foreach h with LMstate L 
           LMcost = 0.0 
           foreach word w in phrase 
             LMcost += -log p(w|L) 
             L? = NewState(L,w) 
             L  = L? 
           end 
           Cost= LMcost+TMcost(e) 
           TotalCost=TotalCost(h)+Cost 
          h? = (L,e,h,TotalCost) 
          store h?in Hypotheses(n,L) 
 
The updated hypothesis h? at the current node 
stores the pointer to the previous hypothesis and 
the edge (labeled with the target phrase) over 
which it was expanded.  Thus, at the final step, 
one can trace back to get the path associated with 
the minimum cost, i.e. the best hypothesis. 
Other operators such as local word reordering 
are incorporated into this dynamic programming 
search (Vogel, 2003). 
3 Phrase Pair Translation Probability  
As stated in the previous section, one of the 
major problems is how to assign a reasonable 
probability for the extracted phrase pair to repre-
sent the translation quality. 
Most of the phrase pairs are seen only once or 
twice in the training data.  This is especially true 
for longer phrases.  Therefore, phrase pair co-
occurrence counts collected from the training 
corpus are not reliable and have little discrimina-
tive power.  In (Vogel et al 2003) a different es-
timation approach was proposed.  Similar as in 
the IBM models, it is assumed that each source 
word si in the source phrase ),,( 21 Issss L
v =  is 
aligned to every target word tj in the target phrase 
),,( 21 Jtttt L
v =  with probability )|Pr( ij st .  The 
total phrase translation probability is then calcu-
lated according to the following generative 
model:  
? ?
= =
=
I
i
J
j
ji tsts
1 1
))|Pr(()|Pr(
vv  (1) 
 
This is essentially the lexical probability as 
calculated in the IBM1 alignment model, without 
considering position alignment probabilities.  
Any statistical translation can be used in (1) to 
calculate the phrase translation probability.  
However, in our experiment we typically see now 
significant difference in translation results when 
using lexicons trained from different alignment 
models. 
Also Equation (1) was confirmed to be robust 
and effective in parallel sentence mining from a 
very large and noisy comparable corpus (Zhao 
and Vogel, 2002).  
Equation (1) does not explicitly discriminate 
content words from non-content words.  As non-
content words such as high frequency functional 
words tend to occur in nearly every parallel sen-
tence pair, they co-occur with most of the source 
words in the vocabulary with non-trivial transla-
tion probabilities.  This noise propagates via (1) 
into the phrase translations probabilities, increas-
ing the chance that non-optimal phrase transla-
tion candidates get high probabilities and better 
translations are often not in the top ranks.  
We propose a vector model to better distin-
guish between content words and non-content 
words with the goal to emphasize content words 
in the translation.  This model will be used to 
rescore the phrase translation pairs, and to get a 
normalized score representing the translation 
probability.  
4 Vector Model for Phrase Translation 
Probability 
Term weighting models such as tf.idf are ap-
plied successfully in information retrieval.  The 
duality of term frequency (tf) and inverse docu-
ment frequency (idf), document space and collec-
tion space respectively, can smoothly predict the 
probability of terms being informative (Roelleke, 
2003).  Naturally, tf.idf is suitable to model con-
tent words as these words in general have large 
tf.idf weights. 
4.1 Phrase Pair as Bag-of-Words 
Our translation model: (transducer, as defined 
in 2.1), is a collection of phrase translation pairs 
together with scores representing the translation 
quality.  Each phrase translation pair, which can 
be represented as a triple },{ pts vv? , is now con-
verted into a ?Bag-of-Words? D consisting of a 
collection of both source and target words ap-
pearing in the phrase pair, as shown in (2):  
},,,,,{},{ 2121 JI tttsssDpts LL
vv =??  (2) 
 
Given each phrase pair as one document, the 
whole transducer is a collection of such docu-
ments.  We can calculate tf.idf for each is  and jt , 
and represent source and target phrases by vec-
tors of sv
v  and tv
v   as in Equation (3):  
},,,{
21 Issss
wwwv Lv =  
},,,{
21 Jtttt
wwwv Lv =  (3) 
where
is
w and
jt
w are tf.idf for is or jt respectively.  
This vector representation can be justified by 
word co-occurrence considerations.  As the 
phrase translation pairs are extracted from paral-
lel sentences, the source words is  and target 
words jt  in the source and target phrases must 
co-occur in the training data.  The co-occurring 
words should share similar term frequency and 
document frequency statistics.  Therefore, the 
vectors  sv
v and tv
v  have similar term weight con-
tours corresponding to the co-occurring word 
pairs.  So the vector representations of a phrase 
translation pair can reflect the translation quality.  
In addition, the content words and non-content 
words are modeled explicitly by using term 
weights.  An over-simplified example would be 
that a rare word in the source language usually 
translates into a rare word in the target language. 
4.2 Term Weighting Schemes 
Given the transducer, it is straightforward to 
calculate term weights for source and target 
words.  There are several versions of tf.idf.  The 
smooth ones are preferred, because phrase trans-
lation pairs are rare events collected from train-
ing data.  
The idf model selected is as in Equation (4): 
)
5.0
5.0log( +
+?=
df
dfNidf  (4)
where N is the total number of documents in the 
transducer, i.e. the total number of translation 
pairs, and df is the document frequency, i.e. in 
how many phrase pairs a given word occurs.  The 
constant of 0.5 acts as smoothing. 
Because most of the phrases are short, such as 
2 to 8 words, the term frequency in the bag of 
words representation is usually 1, and some times 
2.  This, in general, does not bring much dis-
crimination in representing translation quality.  
The following version of tf is chosen, so that 
longer target phrases with more words than aver-
age will be slightly down-weighted: 
)(/)(5.15.0
'
vavglenvlentf
tftf vv?++=  (5)
where tf is the term frequency, )(vlen v  is the 
length in words of the phrase vv , and )(vavglen v  is 
the average length of source or target phrase cal-
culated from the transducer.  Again, the values of 
0.5 and 1.5 are constants used in IR tasks acting 
as smoothing.  
Thus after a transducer is extracted from a par-
allel corpus, tf and df are counted from the collec-
tion of the ?bag-of-words'' phrase alignment 
representations.  For each word in the phrase pair 
translation its tf.idf weight is assigned and the 
source and target phrase are transformed into 
vectors as shown in Equation (3).  These vectors 
reserve the translation quality information and 
also model the content and non-content words by 
the term weighting model of tf.idf. 
4.3 Vector Space Alignment 
Given the vector representations in Equation 
(3), a similarity between the two vectors can not 
directly be calculated.  The dimensions I and J 
are not guaranteed to be the same.  The goal is to 
transform the source vector into a vector having 
the same dimensions as the target vector, i.e. to 
map the source vector into the space of the target 
vector, so that a similarity distance can be calcu-
lated.  Using the same reasoning as used to moti-
vate Equation (1), it is assumed that every source 
word is  contributes some probability mass to 
each target word jt .  That is to say, given a term 
weight for jt , all source term weights are aligned 
to it with some probability.  So we can calculate 
a transformed vector from the source vectors by 
calculating weights jtaw  using a translation lexi-
con )|Pr( st  as in Equation (6): 
?
=
?=
I
i
sij
t
a i
j wstw
1
)|Pr(  (6) 
 
Now the target vector and the mapped vector 
av
v  have the same dimensions as shown in (7):  
},,,{ 21 Jta
t
a
t
aa wwwv L
v =  
},,,{
21 Jtttt
wwwv Lv =  (7) 
 
4.4 Similarity Functions 
As explained in section 4.1, intuitively, if sv  
and t
v  is a good translation pair, then the corre-
sponding vectors of av
v  and tv
v  should be similar 
to each other in the vector space.   
Cosine distance 
The standard cosine distance is defined as the 
inner product of the two vectors av
v  and tv
v  nor-
malized by their norms.  Based on Equation (6), 
it is easy to derive the similarity as follows:  
)()(
)|(
)|(1
1),(),(
1
2
1
2
1 1
1 1
1
cos
??
? ?
? ?
?
==
= =
= =
=
=
=
==
J
j
t
a
J
j
t
J
j
I
i
sijt
J
j
I
i
sijt
t
t
a
J
j
t
t
a
t
t
at
t
a
t
t
a
t
t
a
j
j
ij
ij
j
j
wsqrtwsqrt
wstPw
wstPw
vv
ww
vvvv
vvvvd
 
(8)
where I and J are the length of the source and 
target phrases; 
is
w  and 
jt
w  are term weights for 
source word and target words;  jtaw  is the trans-
formed weight mapped from all source words to 
the target dimension at word jt .   
BM25 distance 
TREC tests show that bm25 (Robertson and 
Walker, 1997) is one of the best-known distance 
schemes.  This distance metric is given in Equa-
tion (9). The constants of 31 ,, kbk are set to be 1, 1 
and 1000 respectively.  
)(
)1(
)(
)1(
3
3
1
1
25 j
j
j
j
t
a
t
a
J
j t
t
bm wk
wk
wK
wk
wd +
+
+
+=?
=
 
)5.0/()5.0( ++?==
jjj ttt
dfdfNidfw  
))(/)1((1 lavgJbkK +?=  
(9)
where avg(l) is the average target phrase length 
in words given the same source phrase. 
Our experiments confirmed the bm25 distance 
is slightly better than the cosine distance, though 
the difference is not really significant.  One ad-
vantage of bm25 distance is that the set of free 
parameters 31 ,, kbk can be tuned to get better per-
formance e.g. via n-fold cross validation.  
4.5 Integrated Translation Score 
Our goal is to rescore the phrase translation 
pairs by using additional evidence of the transla-
tion quality in the vector space.   
The vector based scores (8) & (9) provide a 
distinct view of the translation quality in the vec-
tor space.  Equation (1) provides a evidence of 
the translation quality based on the word align-
ment probability, and can be assumed to be dif-
ferent from the evidences in vector space.  Thus, 
a natural way of integrating them together is a 
geometric interpolation shown in (10) or equiva-
lently a linear interpolation in the log domain.  
)|(Pr),( 1int tsstdd vec
vvvv ?? ??=  (10)
where ),( stdvec
vv is the score from the cosine or 
bm25 vector distance, normalized within [0, 1], 
like a probability. 
0.1),( =?
t
vec stdv
vv   
The parameter ? can be tuned using held-out 
data.  In our cross validation experiments 5.0=?  
gave the best performance in most cases.  There-
fore, Equation (10) can be simplified into: 
)|Pr(),(int tsstdd vec
vvvv ?=  (11)
 
The phrase translation score functions in (1) 
and (11) are non-symmetric.  This is because the 
statistical lexicon Pr(s|t) is non-symmetric.  One 
can easily re-write all the distances by using 
Pr(t|s).  But in our experiments this reverse di-
rection of using Pr(t|s) gives trivially difference.  
So in all the experimental results reported in this 
paper, the distances defined in (1) and (11) are 
used. 
5 Length Regularization  
Phrase pair extraction does not work perfectly 
and sometimes a short source phrase is aligned to 
a long target phrase or vice versa.  Length regu-
larization can be applied to penalize too long or 
too short candidate translations.  Similar to the 
sentence alignment work in (Gale and Church, 
1991), the phrase length ratio is assumed to be a 
Gaussian distribution as given in Equation (12):  
)))(/)((5.0exp(),( 2
2
?
????? sltlstl
vvvv  (12)
where l(t) is the target sentence  length.  Mean ?  
and variance ?  can be estimated using a parallel 
corpus using a Maximum Likelihood criteria. 
The regularized score is the product of (11) and 
(12).  
6 Experiments  
Experiments were carried out on the so-called 
large data track Chinese-English TIDES transla-
tion task, using the June 2002 test data.  The 
training data used to train the statistical lexicon 
and to extract the phrase translation pairs was 
selected from a 120 million word parallel corpus 
in such a way as to cover the phrases in test sen-
tences.  The restricted training corpus contained 
then approximately 10 million words..  A trigram 
model was built on 20 million words of general 
newswire text, using the SRILM toolkit (Stolcke, 
2002).  Decoding was carried out as described in 
section 2.2.  The test data consists of 878 Chinese 
sentences or 24,337 words after word segmenta-
tion.  There are four human translations per Chi-
nese sentence as references.  Both NIST score 
and Bleu score (in percentage) are reported for 
adequacy and fluency aspects of the translation 
quality. 
6.1 Transducers 
Four transducers were used in our experi-
ments: LDC, BiBr, HMM, and ISA.  
LDC was built from the LDC Chinese-English 
dictionary in two steps: first, morphological 
variations are created.  For nouns and noun 
phrases plural forms and entries with definite and 
indefinite determiners were generated.  For verbs 
additional word forms with -s -ed and -ing were 
generated, and the infinitive form with 'to'.  Sec-
ond, a large monolingual English corpus was 
used to filter out the new word forms.  If they did 
not appear in the corpus, the new entries were not 
added to the transducer (Vogel, 2004). 
BiBr extracts sub-tree mappings from Bilin-
gual Bracketing alignments (Wu, 1997);  HMM 
extracts partial path mappings from the Viterbi 
path in the Hidden Markov Model alignments 
(Vogel et. al., 1996).  ISA is an integrated seg-
mentation and alignment for phrases (Zhang et.al, 
2003), which is an extension of (Marcu and 
Wong, 2002).  
 LDC BiBr HMM ISA 
)(KN  425K 137K 349K 263K 
)/( srctgt llavg  1.80 1.11 1.09 1.20 
Table-1 statistics of transducers 
 
Table-1 shows some statistics of the four 
transducers extracted for the translation task. N  
is the total number of phrase pairs in the trans-
ducer.  LDC is the largest one having 425K en-
tries, as the other transducers are restricted to 
?useful? entries, i.e. those translation pairs where 
the source phrase matches a sequence of words in 
one of the test sentence.  Notice that the LDC 
dictionary has a large number of long transla-
tions, leading to a high source to target length 
ratio. 
6.2 Cosine vs BM25 
The normalized cosine and bm25 distances de-
fined in (8) and (9) respectively, are plugged into 
(11) to calculate the translation probabilities.  
Initial experiments are reported on the LDC 
transducer, which gives already a good transla-
tion, and therefore allows for fast and yet mean-
ingful experimentation.  
Four baselines (Uniform, Base-m1, Base-m4, 
and Base-m4S) are presented in Table-2.   
 
NIST Bleu 
Uniform 6.69 13.82 
Base-m1 7.08 14.84 
Base-m4 7.04 14.91 
Base-m4S 6.91 14.44 
cosine 7.17 15.30 
bm25 7.19 15.51 
bm25-len 7.21 15.64 
Table-2 Comparisons of different score functions 
 
In the first uniform probabilities are assigned 
to each phrase pair in the transducer.  The second 
one (Base-m1) is using Equation (1) with a statis-
tical lexicon trained using IBM Model-1, and 
Base-m4 is using the lexicon from IBM Model-4.  
Base-m4S is using IBM Model-4, but we skipped 
194 high frequency English stop words in the 
calculation of Equation (1). 
Table-2 shows that the translation score de-
fined by Equation (1) is much better than a uni-
form model, as expected.  Base-m4 is slightly 
worse than Base-m1.on NIST score, but slightly 
better using the Bleu metric.  Both differences 
are not statistically significant.  The result for 
Base-m4S shows that skipping English stop 
words in Equation (1) gives a disadvantage.  One 
reason is that skipping ignores too much non-
trivial statistics from parallel corpus especially 
for short phrases.  These high frequency words 
actually account already for more than 40% of 
the tokens in the corpus.  
Using the vector model, both with the cosine 
cosd  and the bm25 25bmd  distance, is significantly 
better than Base-m1 and Base-m4 models, which 
confirms our intuition of the vector model as an 
additional useful evidence for translation quality. 
The length regularization (12) helps only slightly 
for LDC.  Since bm25?s parameters could be 
tuned for potentially better performance, we se-
lected bm25 with length regularization as the 
model tested in further experiments.  
A full-loaded system is tested using the 
LM020 with and without word-reordering in de-
coding.  The results are presented in Table-3.  
Table-3 shows consistent improvements on all 
configurations: the individual transducers, com-
binations of transducers, and different decoder 
settings of word-reordering. Because each phrase 
pair is treated as a ?bag-of-words?, the grammar 
structure is not well represented in the vector 
model.  Thus our model is more tuned towards 
the adequacy aspect, corresponding to NIST 
score improvement. 
Because the transducers of BiBr, HMM, and 
ISA are extracted from the same training data, 
they have significant overlaps with each other.  
This is why we observe only small improvements 
when adding more transducers.   
The final NIST score of the full system is 8.24, 
and the Bleu score is 22.37.  This corresponds to 
3.1% and 11.8% relative improvements over the 
baseline.  These improvements are statistically 
significant according to a previous study (Zhang 
et.al., 2004), which shows that a 2% improve-
ment in NIST score and a 5% improvement in 
Bleu score is significant for our translation sys-
tem on the June 2002 test data. 
6.3 Mean Reciprocal Rank 
To further investigate the effects of the rescor-
ing function in (11), Mean Reciprocal Rank 
(MRR) experiments were carried out.  MRR for a 
labeled set is the mean of the reciprocal rank of 
the individual phrase pair, at which the best can-
didate translation is found (Kantor and Voorhees, 
1996).  
Totally 9,641 phrase pairs were selected con-
taining 216 distinct source phrases.  Each source 
phrase was labeled with its best translation can-
didate without ambiguity.  The rank of the la-
beled candidate is calculated according to 
translation scores. The results are shown in Ta-
ble-4. 
 baseline cosine bm25 
MRR 0.40 0.58 0.75 
Table-4 Mean Reciprocal Rank 
 
The rescore functions improve the MRR from 
0.40 to 0.58 using cosine distance, and to 0.75 
using bm25.  This confirms our intuitions that 
good translation candidates move up in the rank 
after the rescoring.  
Decoder settings without word reordering with word reordering 
baseline bm25 baseline bm25 Scores (%) NIST Bleu NIST Bleu NIST Bleu NIST Bleu 
LDC 7.08 14.84 7.21 15.64 7.13 15.10 7.26 15.98 
LDC+ISA 7.73 19.60 7.99 19.58 7.86 20.80 8.13 20.93 
LDC+ISA+HMM 7.86 19.08 8.14 20.70 7.95 19.84 8.19 21.60 
LDC+ISA+HMM+BiBr 7.87 19.23 8.14 21.48 7.99 20.01 8.24 22.37 
Table-3 Translation using bm25 rescore function with different decoder settings 
 
7 Conclusion and Discussion  
In this work, we proposed a way of using term 
weight based models in a vector space as addi-
tional evidences for translation quality, and inte-
grated the model into an existing phrase-based 
statistical machine translation system.  The mod-
el shows significant improvements when using it 
to score a manual dictionary as well as when us-
ing different phrase transducers or a combination 
of all available translation information.  Addi-
tional experiments also confirmed the effective-
ness of the proposed model in terms of of 
improved Mean Reciprocal Rank of good transla-
tions. 
Our future work is to explore alternatives such 
as the reranking work in (Collins, 2002) and in-
clude more knowledge such as syntax informa-
tion in rescoring the phrase translation pairs.  
References 
A. Stolcke. 2002. SRILM -- An Extensible Language 
Modeling Toolkit. In 2002 Proc. Intl. Conf. on 
Spoken Language Processing, Denver. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
J.Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation, Computational Linguistics, 
vol. 19, no. 2, pp. 263?311. 
Michael Collins. 2000. Discriminative Reranking for 
Natural Language Parsing. Proc. 17th International 
Conf. on Machine Learning. pp. 175-182. 
William A. Gale and Kenneth W. Church. 1991. A 
Program for Aligning Sentences in Bilingual Cor-
pora.  In Computational Linguistics, vol.19 pp. 75-
102.  
Paul B. Kantor, Ellen Voorhees. 1996. Report on the 
TREC-5 Confusion Track. The Fifth Text Retrieval 
Conference. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. Pro-
ceedings of HLT-NAACL. Edmonton, Canada.  
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. Proceedings of EMNLP-2002, 
Philadelphia, PA. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. Proceedings of ACL-
00, pp. 440-447, Hongkong, China. 
Thomas Roelleke. 2003. A Frequency-based and a 
Poisson-based Definition of the Probability of Be-
ing Informative. Proceedings of the 26th annual in-
ternational ACM SIGIR. pp. 227-234.  
S.E. Robertson, and S. Walker. 1997. On relevance 
weights with little relevance information. In 1997 
Proceedings of ACM SIGIR. pp. 16-24. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora, 
Computational Linguistics 23(3): pp.377-404.  
K. Yamada and K. Knight. 2001. A Syntax-Based 
Statistical Translation Model. Proceedings of the 
39th Annual Meeting of the Association for Compu-
tational Linguistics. pp. 523-529. Toulouse, France.  
Richard Zens, Franz Josef Och and Hermann Ney. 
2002. Phrase-Based Statistical Machine Transla-
tion. Proceedings of the 25th Annual German Con-
ference on AI: Advances in Artificial Intelligence. 
pp. 18-22. 
Stephan Vogel, Hermann Ney, Christian Tillmann. 
1996. HMM-based word alignment in statistical 
translation. In: COLING '96: The 16th Int. Conf. on 
Computational Linguistics, Copenhagen, Denmark 
(1996) pp. 836-841. 
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, Alex Waibel. 
2003. The CMU Statistical Translation System, 
Proceedings of MT-Summit IX. New Orleans, LA. 
Stephan Vogel. 2003. SMT decoder dissected: word 
reordering, In 2003 Proceedings of Natural Lan-
guage Processing and Knowledge Engineering, 
(NLP-KE'03) Beijing, China.  
Stephan Vogel. 2004. Augmenting Manual Dictionar-
ies for Statistical Machine Translation Systems, In 
2003 Proceedings of LREC, Lisbon, Portugal.  pp. 
1593-1596. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST Scores : How much Im-
provement Do We Need to Have a Better System? 
In 2004 Proceedings of LREC, Lisbon, Portugal. 
pp. 2051-2054. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2003. "In-
tegrated Phrase Segmentation and Alignment Algo-
rithm for Statistical Machine Translation," in the 
Proceedings of NLP-KE'03, Beijing, China. 
Bing Zhao, Stephan Vogel. 2002. Adaptative Parallel 
Sentences Mining from web bilingual news collec-
tion. In 2002 IEEE International Conference on 
Data Mining, Maebashi City, Japan.  
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 184?187,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Tools for Collecting Speech Corpora via Mechanical-Turk 
  
Ian Lane1,2, Alex Waibel1,2 
1Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
{ianlane,ahw}@cs.cmu.edu 
Matthias Eck2, Kay Rottmann2 
2Mobile Technologies LLC 
Pittsburgh, PA, USA 
matthias.eck@jibbigo.com 
kay.rottmann@jibbigo.com
 
 
Abstract 
To rapidly port speech applications to 
new languages one of the most difficult 
tasks is the initial collection of sufficient 
speech corpora. State-of-the-art automatic 
speech recognition systems are typical 
trained on hundreds of hours of speech 
data. While pre-existing corpora do exist 
for major languages, a sufficient amount 
of quality speech data is not available for 
most world languages. While previous 
works have focused on the collection of 
translations and the transcription of audio 
via Mechanical-Turk mechanisms, in this 
paper we introduce two tools which ena-
ble the collection of speech data remotely. 
We then compare the quality of audio col-
lected from paid part-time staff and unsu-
pervised volunteers, and determine that 
basic user training is critical to obtain us-
able data.  
1 Introduction 
In order to port a spoken language application to a 
new language, first an automatic speech recogni-
tion (ASR) system must be developed. For many 
languages pre-existing corpora do not exist and 
thus speech data must be collected before devel-
opment can begin. The collection of speech corpo-
ra is an expensive undertaking and obtaining this 
data rapidly, for example in response to a disaster, 
cannot be done using the typical methodology in 
which corpora are collected in controlled environ-
ments. 
 
To build an ASR system for a new language, two 
sets of data are required; first, a text corpus con-
sisting of written transcriptions of utterances users 
are likely to speak to the system, this is used to 
train the language model (LM) applied during 
ASR; and second, a corpora of recordings of 
speech, which are used to train an acoustic model 
(AM). Text corpora for a new language can be 
created by manually translating a pre-existing cor-
pus (or a sub-set of that corpus) into the new lan-
guage and crowd-sourcing methodologies can be 
used to rapidly perform this task. Rapidly creating 
corpora of speech data, however, is not trivial. 
Generally speech corpora are collected in con-
trolled environments where speakers are super-
vised by experts to ensure the equipment is setup 
correctly and recordings are performed adequately. 
However, for most languages performing this task 
on-site, where developers are located, is impractic-
al as there may not be a local community of speak-
ers of the required language. An alternative is to 
perform the data collection remotely, allowing 
speakers to record speech on their own PCs or mo-
bile devices in their home country or wherever 
they are located. While previous works have fo-
cused on the generation of translations (Razavian, 
2009) and transcribing of audio (Marge, 2010) via 
Mechanical-Turk, in this paper we focus on the 
collection of speech corpora using a Mechanical-
Turk type framework. 
 
Previous works (Voxforge), (Gruenstein, 2009), 
(Schultz, 2007) have developed solutions for col-
lecting speech data remotely via web-based inter-
faces. A web-based system for the collection of 
open-source speech corpora has been developed by 
the group at www.voxforge.org. Speech recordings 
are collected for ten major European languages and 
speakers can either record audio directly on the 
website or they can call in on a dedicated phone 
line. In (Gruenstein, 2009) spontaneous speech 
(US English) was collected via a web-based mem-
ory game. In this system speech prompts were not 
provided, but rather a voice-based memory game 
was used to gather and partially annotate 
184
             
Figure 1: Screenshots from Speech Collection iPhone App 
 
spontaneous speech. In comparison to the above 
works which focus on the collection of data for 
major languages, the SPICE project (Schultz, 
2007) provides a set of web-based tools to enable 
developers to create voice-based applications for 
less-common languages. In addition to tools for 
defining the phonetic units of a language and creat-
ing pronunciation dictionaries, this system also 
includes tools to create prompts and collect speech 
data from volunteers over the web. 
 
In this paper, we describe two tools we have de-
veloped to collect speech corpora remotely. The 
first, a Mobile smart-phone based system which 
allows speakers to record prompted speech directly 
on their phones and second, a web-based system 
which allows recordings to be collected remotely 
on PCs. We compare the quality of audio collected 
from paid part-time staff and unsupervised volun-
teers and determine that basic user training and 
automatic feedback mechanisms are required to 
obtain usable data. 
2 Collection of Speech on Mobile Devices 
Today?s smart-phones are able to record quality 
audio onboard and generally have the ability to 
connect to the internet via a fast wifi-connection. 
This makes them an ideal platform for collecting 
speech data in the field. Speech data can be col-
lected by a user at any time in any location, and the 
data can be uploaded at a later time when a wire-
less connection is available. At Mobile Technolo-
gies we have developed an iPhone application to 
perform this task. 
 
The collection procedure consists of three steps. 
First, on start-up a small amount of personal in-
formation, namely, gender and age, are requested 
from the user. They then select the language for 
which they intend to provide speech data. The mo-
bile-device ID, personal information and language 
selected is used as an identifier for individual 
speakers. Next, collection of speech data is per-
formed. Collection is performed offline, enabling 
data to be collected in the field where there may 
not be a persistent internet connection. A prompt is 
randomly selected from an onboard database of 
sentences and is presented to the user, who reads 
the sentence aloud holding down a push-to-talk 
button while speaking. During the speech collec-
tion stage, the system automatically proceeds to the 
following prompt when the current recording is 
complete. The user however has the ability to go 
back to previous recordings, listen to it and re-
speak the sentence if any issues are found. Finally, 
the speech data is uploaded using a wireless collec-
tion. Data is uploaded one utterance at a time to an 
FTP server. Uploading each utterance individually 
allows the user to halt the upload and continue it at 
a later time if required. 
  
185
 
Figure 2: Java applet for Web-based recording 
3 Collection via Web-based Recording 
One of the most popular websites for crowd-
sourcing is Amazon Mechanical Turk (AMT). 
?Requesters? post Human Intelligence Tasks 
(HITs) to this website and ?Workers? browse the 
HITs, perform tasks and get paid a predefined 
amount after submitting their work. It has been 
reported that over 100,000 workers from 100 coun-
tries are using AMT (Pontin, 2007). 
 
AMT allows two general types of HITs. A Ques-
tion Form HIT is based on a provided XML tem-
plate and only allows certain elements in the HIT. 
However, it is possible to integrate an external 
JAVA applet within a Question Form HIT which 
allows for some flexibility. Questions can also be 
hosted on an external website which increases flex-
ibility for the HIT developer while remaining 
tightly integrated in the AMT environment. 
 
For collection of audio data Amazon does not offer 
any integrated tools. We thus designed and imple-
mented a Java applet for web based speech collec-
tion. The Java applet can easily be incorporated in 
the AMT Question-Form mechanism and could 
also be used as part of an External-Question HIT.  
Currently the Java applet provides the same basic 
functionality as outlined for the iPhone application. 
The applet sequentially shows a number of 
prompts to record. The user can skip a sentence, 
playback a recording to check the quality and also 
redo the recording for the current sentence (see 
screenshot in Figure 2).  
 
After the user is finished, the recorded sentences 
are uploaded to a web-server using an HTTP Post 
request. An important difference is the necessity to 
be online during the speech recordings. 
4 Evaluation of Recorded Audio 
One issue when collecting speech data remotely is 
the quality of the resulting audio. When collection  
Table 1: Details of Evaluated Corpora 
 
Table 2: Annotations used to label poor quality 
recordings 
is performed in a controlled environment, the de-
veloper can ensure that the recording equipment is 
setup correctly, background noise is kept to a min-
imum and the speaker is adequately trained to use 
the recording equipment. However, the same is not 
guaranteed when collecting speech remotely via 
mechanical-turk frameworks.  
When recording prompted speech there are three 
types of issues that result in unsuitable data: 
? Garbage Audio: recordings that are emp-
ty, clipped, have insufficient power, or are 
incorrectly segmented. 
? Low quality recordings: low Signal-to-
Noise recordings due to poor equipment or 
large background noise 
? Speaker errors: Misspeaking of prompts, 
both accidental and malicious 
To verify the quality of audio recorded in unsuper-
vised environments we compared two sets of 
speech data. First, in an earlier data collection task 
we collected 445 prompted utterances from 10 US-
English speakers. This data collection was per-
formed in a quiet office environment with technic-
al supervision. Speakers were paid a fee for their 
time. As a comparison a similar collection of Hai-
tian Creole was performed. In this case data was 
collected on a volunteer basis and supervision was 
limited. Details of the collected data are shown in 
Table 1.  
  
Paid Employees 
Language English 
Number of Speakers 10  
Utterances Evaluated 445 
  
Volunteers 
Language Haitian Creole 
Number of Speakers 3 
Utterances Evaluated 167 
1 Recorded utterance is empty 
2 Utterance is not segmented correctly 
3 Recording is clipped 
4 Recording contains audible echo 
5 Recording contains audible noise 
186
 Figure 3: Percentage of recorded utterances de-
termined to be inadequate for acoustic model 
training. Annotations limited to five issues 
listed in Table 1. 
To determine the frequency of the quality issues 
listed above, we manually verified the two sets of 
collected speech. The recording of each utterance 
was listened to and if the audio file was determined 
to be of low quality it was annotated with one of 
the tags listed in Table 2. The percentage of utter-
ances labeled with each annotation is shown for the 
English and volunteer Haitian Creole cases in Fig-
ure 3. 
 
Around 10% of the English recordings were found 
to have issues. Clipping occurred in approximately 
5% and a distinct echo was present in the record-
ings for one speaker. For the Haitian Creole case 
the yield of useable audio was significantly lower 
than that obtained for English. For all three speak-
ers clipping was more prevalent and the level of 
background noise was higher. We discovered that 
due to lack of training, one of the volunteers had 
significant issues with the push-to-talk interface in 
our system. This led to many empty or incorrectly 
segmented recordings. In both cases, prompts were 
generally spoken accurately and technical prob-
lems caused poor quality recordings. 
 
We believe the large difference in the yield of high 
quality recordings, 90% for English compared to 
65% for Haitian Creole case, is directly due to the 
lack of training speakers received and the volun-
teer nature of the Haitian Creole task. By incorpo-
rating a basic tutorial when users first start our 
tools and an explicit feedback mechanism which 
automatically detects quality issues and prompts 
users to correct them we expect the yield of high 
quality recordings to increase significantly. In the 
near future we plan to use the tools to collect data 
from large communities of remote users.  
5 Conclusions and Future Work 
In this work, we have described two applications 
that allow speech corpora to be collected remotely, 
either directly on Mobile smart-phones or on a PC 
via a web-based interface. We also investigated the 
quality of recordings made by unsupervised volun-
teers and found that although prompts were gener-
ally read accurately, lack of training led to a 
significantly lower yield of high quality record-
ings. 
 
In the near future we plan to use the tools to collect 
data from large communities of remote users. We 
will also investigate the user of tutorials and feed-
back to improve the yield of high quality data. 
 
Acknowledgements 
 
We would like to thank the Haitian volunteers who 
gave their time to help with this data collection. 
References  
N. S. Razavian, S Vogel, "The Web as a Platform 
to Build Machine Translation Resources", 
IWIC2009 
M. Marge, S. Banerjee and A. Rudnicky, "Using 
the Amazon Mechanical Turk for Transcription 
of Spoken Language", IEEE-ICASSP, 2010 
Voxforge, www.voxforge.org 
A. Gruenstein, I. McGraw, and A. Sutherland, "A 
self-transcribing speech corpus: collecting con-
tinuous speech with an online educational 
game," Submitted to the Speech and Language 
Technology in Education (SLaTE) Workshop, 
2009. 
T. Schultz, et. al, "SPICE: Web-based Tools for 
Rapid Language Adaptation in Speech 
Processing Systems", In the Proceedings of 
INTERSPEECH, Antwerp, Belgium, 2007. 
J. Pontin, ?Artificial Intelligence, With Help From 
the Humans?, The New York Times, 25 March 
2007 
187
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 138?142,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Karlsruhe Institute for Technology Translation System for the
ACL-WMT 2010
Jan Niehues, Teresa Herrmann, Mohammed Mediani and Alex Waibel
Karlsruhe Instiute of Technolgy
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes our phrase-based Sta-
tistical Machine Translation (SMT) sys-
tem for the WMT10 Translation Task. We
submitted translations for the German to
English and English to German transla-
tion tasks. Compared to state-of-the-art
phrase-based systems we preformed addi-
tional preprocessing and used a discrim-
inative word alignment approach. The
word reordering was modeled using POS
information and we extended the transla-
tion model with additional features.
1 Introduction
In this paper we describe the systems that we
built for our participation in the Shared Trans-
lation Task of the ACL 2010 Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR. Our translations are generated using
a state-of-the-art phrase-based translation system
and applying different extensions and modifica-
tions including Discriminative Word Alignment,
a POS-based reordering model and bilingual lan-
guage models using POS and stem information.
Depending on the source and target languages,
the proposed models differ in their benefit for the
translation task and also expose different correl-
ative effects. The Sections 2 to 4 introduce the
characteristics of the baseline system and the sup-
plementary models. In Section 5 we present the
performance of the system variants applying the
different models and chose the systems used for
creating the submissions for the English-German
and German-English translation task. Section 6
draws conclusions and suggests directions for fu-
ture work.
2 Baseline System
The baseline systems for the translation directions
German-English and English-German are both de-
veloped using Discriminative Word Alignment
(Niehues and Vogel, 2008) and the Moses Toolkit
(Koehn et al, 2007) for extracting phrase pairs
and generating the phrase table from the discrimi-
native word alignments. The difficult reordering
between German and English was modeled us-
ing POS-based reordering rules. These rules were
learned using a word-aligned parallel corpus. The
POS tags for the reordering models are generated
using the TreeTagger (Schmid, 1994) for all lan-
guages.
Translation is performed by the STTK Decoder
(Vogel, 2003) and all systems are optimized to-
wards BLEU using Minimum Error Rate Training
as proposed in Venugopal et al (2005).
2.1 Training, Development and Test Data
We used the data provided for the WMT for train-
ing, optimizing and testing our systems: Our
training corpus consists of Europarl and News
Commentary data, for optimization we use new-
stest2008 as development set and newstest2009 as
test set.
The baseline language models are trained on
the target language part of the Europarl and News
Commentary corpora. Additional, bigger lan-
guage models were trained on monolingual cor-
pora. For both systems the News corpus was used
while an English language model was also trained
on the even bigger Gigaword corpus.
2.2 Preprocessing
The training data was preprocessed before used for
training. In this step different normalizations were
done like mapping different types of quotes. In
the end the first word of every sentence was smart-
cased.
138
For the German text, additional preprocessing
steps were applied. First, the older German data
uses the old German orthography whereas the
newer parts of the corpus use the new German
orthography. We tried to normalize the text by
converting the whole text to the new German or-
thography. In a first step, we search for words that
are only correct according to the old writing rules.
Therefore, we selected all words in the corpus, that
are correct according to the hunspell lexicon1 us-
ing the old rules, but not correct according to the
hunspell lexicon using the new rules. In a second
step we tried to find the correct spelling according
to the new rules. We first applied rules describing
how words changed from one spelling system to
the other, for example replacing ??? by ?ss?. If the
new word is a correct word according to the hun-
spell lexicon using the new spelling rules, we map
the words.
When translating from German to English, we
apply compound splitting as described in Koehn
and Knight (2003) to the German corpus.
As a last preprocessing step we remove sen-
tences that are too long and empty lines to obtain
the final training corpus.
3 Word Reordering Model
Reordering was applied on the source side prior
to decoding through the generation of lattices en-
coding possible reorderings of each source sen-
tence that better match the word sequence in the
target language. These possible reorderings were
learned based on the POS of the source language
words in the training corpus and the information
about alignments between source and target lan-
guage words in the corpus. For short-range re-
orderings, continuous reordering rules were ap-
plied to the test sentences (Rottmann and Vogel,
2007). To model the long-range reorderings be-
tween German and English, different types of non-
continuous reordering rules were applied depend-
ing on the translation direction. (Niehues and
Kolss, 2009). When translating from English to
German, most of the changes in word order con-
sist in a shift to the right while typical word shifts
in German to English translations take place in the
reverse direction.
1http://hunspell.sourceforge.net/
4 Translation Model
The translation model was trained on the parallel
corpus and the word alignment was generated by
a discriminative word alignment model, which is
described below. The phrase table was trained us-
ing the Moses training scripts, but for the German
to English system we used a different phrase ex-
traction method described in detail in Section 4.2.
In addition, we applied phrase table smoothing as
described in Foster et al (2006). Furthermore, we
extended the translation model by additional fea-
tures for unaligned words and introduced bilingual
language models.
4.1 Word Alignment
In most phrase-based SMT systems the heuristic
grow-diag-final-and is used to combine the align-
ments generated by GIZA++ from both direc-
tions. Then these alignments are used to extract
the phrase pairs.
We used a discriminative word alignment model
(DWA) to generate the alignments as described in
Niehues and Vogel (2008) instead. This model is
trained on a small amount of hand-aligned data
and uses the lexical probability as well as the fer-
tilities generated by the PGIZA++2 Toolkit and
POS information. We used all local features, the
GIZA and indicator fertility features as well as
first order features for 6 directions. The model was
trained in three steps, first using maximum likeli-
hood optimization and afterwards it was optimized
towards the alignment error rate. For more details
see Niehues and Vogel (2008).
4.2 Lattice Phrase Extraction
In translations from German to English, we often
have the case that the English verb is aligned to
both parts of the German verb. Since this phrase
pair is not continuous on the German side, it can-
not be extracted. The phrase could be extracted, if
we also reorder the training corpus.
For the test sentences the POS-based reordering
allows us to change the word order in the source
sentence so that the sentence can be translated
more easily. If we apply this also to the train-
ing sentences, we would be able to extract the
phrase pairs for originally discontinuous phrases
and could apply them during translation of the re-
ordered test sentences.
2http://www.cs.cmu.edu/?qing/
139
Therefore, we build lattices that encode the dif-
ferent reorderings for every training sentence, as
described in Niehues et al (2009). Then we can
not only extract phrase pairs from the monotone
source path, but also from the reordered paths. So
it would be possible to extract the example men-
tioned before, if both parts of the verb were put
together by a reordering rule. To limit the num-
ber of extracted phrase pairs, we extract a source
phrase only once per sentence even if it may be
found on different paths. Furthermore, we do not
use the weights in the lattice.
If we used the same rules as for reordering the
test sets, the lattice would be so big that the num-
ber of extracted phrase pairs would be still too
high. As mentioned before, the word reordering
is mainly a problem at the phrase extraction stage
if one word is aligned to two words which are
far away from each other in the sentence. There-
fore, the short-range reordering rules do not help
much in this case. So, only the long-range reorder-
ing rules were used to generate the lattices for the
training corpus.
4.3 Unaligned Word Feature
Guzman et al (2009) analyzed the role of the word
alignment in the phrase extraction process. To bet-
ter model the relation between word alignment and
the phrase extraction process, they introduced two
new features into the log-linear model. One fea-
ture counts the number of unaligned words on the
source side and the other one does the same for the
target side. Using these additional features they
showed improvements on the Chinese to English
translation task. In order to investigate the impact
on closer related languages like English and Ger-
man, we incorporated those two features into our
systems.
4.4 Bilingual Word language model
Motivated by the improvements in translation
quality that could be achieved by using the n-gram
based approach to statistical machine translation,
for example by Allauzen et al (2009), we tried
to integrate a bilingual language model into our
phrase-based translation system.
To be able to integrate the approach easily into a
standard phrase-based SMT system, a token in the
bilingual language model is defined to consist of
a target word and all source words it is aligned to.
The tokens are ordered according to the target lan-
guage word order. Then the additional tokens can
be introduced into the decoder as an additional tar-
get factor. Consequently, no additional implemen-
tation work is needed to integrate this feature.
If we have the German sentence Ich bin nach
Hause gegangen with the English translation I
went home, the resulting bilingual text would look
like this: I Ich went bin gegangen home Hause.
As shown in the example, one problem with this
approach is that unaligned source words are ig-
nored in the model. One solution could be to have
a second bilingual text ordered according to the
source side. But since the target sentence and not
the source sentence is generated from left to right
during decoding, the integration of a source side
language model is more complex. Therefore, as
a first approach we only used a language model
based on the target word order.
4.5 Bilingual POS language model
The main advantage of POS-based information
is that there are less data sparsity problems and
therefore a longer context can be considered. Con-
sequently, if we want to use this information in the
translation model of a phrase-based SMT system,
the POS-based phrase pairs should be longer than
the word-based ones. But this is not possible in
many decoders or it leads to additional computa-
tion overhead.
If we instead use a bilingual POS-based lan-
guage model, the context length of the language
model is independent from the other models. Con-
sequently, a longer context can be considered for
the POS-based language model than for the word-
based bilingual language model or the phrase
pairs.
Instead of using POS-based information, this
approach can also be applied with other additional
linguistic word-level information like word stems.
5 Results
We submitted translations for English-German
and German-English for the Shared Translation
Task. In the following we present the experiments
we conducted for both translation directions ap-
plying the aforementioned models and extensions
to the baseline systems. The performance of each
individual system configuration was measured ap-
plying the BLEU metric. All BLEU scores are cal-
culated on the lower-cased translation hypotheses.
The individual systems that were used to create the
submission are indicated in bold.
140
5.1 English-German
The baseline system for English-German applies
short-range reordering rules and discriminative
word alignment. The language model is trained
on the News corpus. By expanding the coverage
of the rules to enable long-range reordering, the
score on the test set could be slightly improved.
We then combined the target language part of the
Europarl and News Commentary corpora with the
News corpus to build a bigger language model
which resulted in an increase of 0.11 BLEU points
on the development set and an increase of 0.25
points on the test set. Applying the bilingual lan-
guage model as described above led to 0.04 points
improvement on the test set.
Table 1: Translation results for English-German
(BLEU Score)
System Dev Test
Baseline 15.30 15.40
+ Long-range Reordering 15.25 15.44
+ EPNC LM 15.36 15.69
+ bilingual Word LM 15.37 15.73
+ bilingual POS LM 15.42 15.67
+ unaligned Word Feature 15.65 15.66
+ bilingual Stem LM 15.57 15.74
This system was used to create the submis-
sion to the Shared Translation Task of the WMT
2010. After submission we performed additional
experiments which only led to inconclusive re-
sults. Adding the bilingual POS language model
and introducing the unaligned word feature to the
phrase table only improved on the development
set, while the scores on the test set decreased. A
third bilingual language model based on stem in-
formation again only showed noteworthy effects
on the development set.
5.2 German-English
For the German to English translation system,
the baseline system already uses short-range re-
ordering rules and the discriminative word align-
ment. This system applies only the language
model trained on the News corpus. By adding the
possibility to model long-range reorderings with
POS-based rules, we could improve the system by
0.6 BLEU points. Adding the big language model
using also the English Gigaword corpus we could
improve by 0.3 BLEU points. We got an addi-
tional improvement by 0.1 BLEU points by adding
lattice phrase extraction.
Both the word-based and POS-based bilingual
language model could improve the translation
quality measured in BLEU. Together they im-
proved the system performance by 0.2 BLEU
points.
The best results could be achieved by using also
the unaligned word feature for source and target
words leading to the best performance on the test
set (22.09).
Table 2: Translation results for German-English
(BLEU Score)
System Dev Test
Baseline 20.94 20.83
+ Long-range Reordering 21.52 21.43
+ Gigaword LM 21.90 21.71
+ Lattice Phrase Extraction 21.94 21.81
+ bilingual Word LM 21.94 21.87
+ bilingual POS LM 22.02 22.05
+ unaligned Word Feature 22.09 22.09
6 Conclusions
For our participation in the WMT 2010 we built
translation systems for German to English and En-
glish to German. We addressed to the difficult
word reordering when translating from or to Ger-
man by using POS-based reordering rules during
decoding and by using lattice-based phrase extrac-
tion during training. By applying those methods
we achieved substantially better results for both
translation directions.
Furthermore, we tried to improve the translation
quality by introducing additional features to the
translation model. On the one hand we included
bilingual language models based on different word
factors into the log-linear model. This led to very
slight improvements which differed also with re-
spect to language and data set. We will investigate
in the future whether further improvements are
achievable with this approach. On the other hand
we included the unaligned word feature which has
been applied successfully for other language pairs.
The improvements we could gain with this method
are not as big as the ones reported for other lan-
guages, but still the performance of our systems
could be improved using this feature.
141
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Franc?ois Yvon. 2009. LIMSI?s statistical trans-
lation system for WMT?09. In Fourth Workshop
on Statistical Machine Translation (WMT 2009),
Athens, Greece.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP
2006), Sydney, Australia.
Francisco Guzman, Qin Gao, and Stephan Vogel.
2009. Reassessment of the Role of Phrase Extrac-
tion in PBSMT. In MT Summit XII, Ottawa, Ontario,
Canada.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In ACL 2007, Demonstration Session,
Prague, Czech Republic, June 23.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
142
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 198?206,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Wider Context by Using Bilingual Language Models in Machine Translation
Jan Niehues1, Teresa Herrmann1, Stephan Vogel2 and Alex Waibel1,2
1Institute for Anthropomatics, KIT - Karlsruhe Institute of Technology, Germany
2 Language Techonolgies Institute, Carnegie Mellon University, USA
1firstname.lastname@kit.edu 2lastname@cs.cmu.edu
Abstract
In past Evaluations for Machine Translation of
European Languages, it could be shown that
the translation performance of SMT systems
can be increased by integrating a bilingual lan-
guage model into a phrase-based SMT system.
In the bilingual language model, target words
with their aligned source words build the to-
kens of an n-gram based language model. We
analyzed the effect of bilingual language mod-
els and show where they could help to bet-
ter model the translation process. We could
show improvements of translation quality on
German-to-English and Arabic-to-English. In
addition, for the Arabic-to-English task, train-
ing an extra bilingual language model on the
POS tags instead of the surface word forms
led to further improvements.
1 Introduction
In many state-of-the art SMT systems, the phrase-
based (Koehn et al, 2003) approach is used. In
this approach, instead of building the translation by
translating word by word, sequences of source and
target words, so-called phrase pairs, are used as the
basic translation unit. A table of correspondences
between source and target phrases forms the transla-
tion model in this approach. Target language fluency
is modeled by a language model storing monolin-
gual n-gram occurrences. A log-linear combination
of these main models as well as additional features
is used to score the different translation hypotheses.
Then the decoder searches for the translation with
the highest score.
A different approach to SMT is to use a stochas-
tic finite state transducer based on bilingual n-
grams (Casacuberta and Vidal, 2004). This ap-
proach was for example successfully applied by Al-
lauzen et al (2010) on the French-English trans-
lation task. In this so-called n-gram approach the
translation model is trained by using an n-gram lan-
guage model of pairs of source and target words,
called tuples. While the phrase-based approach cap-
tures only bilingual context within the phrase pairs,
in the n-gram approach the n-gram model trained on
the tuples is used to capture bilingual context be-
tween the tuples. As in the phrase-based approach,
the translation model can also be combined with ad-
ditional models like, for example, language models
using log-linear combination.
Inspired by the n-gram-based approach, we in-
troduce a bilingual language model that extends
the translation model of the phrase-based SMT ap-
proach by providing bilingual word context. In ad-
dition to the bilingual word context, this approach
enables us also to integrate a bilingual context based
on part of speech (POS) into the translation model.
When using phrase pairs it is complicated to use
different kinds of bilingual contexts, since the con-
text of the POS-based phrase pairs should be bigger
than the word-based ones to make the most use of
them. But there is no straightforward way to inte-
grate phrase pairs of different lengths into the trans-
lation model in the phrase-based approach, while it
is quite easy to use n-gram models with different
context lengths on the tuples. We show how we can
use bilingual POS-based language models to capture
longer bilingual context in phrase-based translation
198
systems.
This paper is structured in the following way: In
the next section, we will present some related work.
Afterwards, in Section 3, a motivation for using the
bilingual language model will be given. In the fol-
lowing section the bilingual language model is de-
scribed in detail. In Section 5, the results and an
analysis of the translation results is given, followed
by a conclusion.
2 Related Work
The n-gram approach presented in Mari?o et al
(2006) has been derived from the work of Casacu-
berta and Vidal (2004), which used finite state trans-
ducers for statistical machine translation. In this ap-
proach, units of source and target words are used as
basic translation units. Then the translation model is
implemented as an n-gram model over the tuples. As
it is also done in phrase-based translations, the dif-
ferent translations are scored by a log-linear combi-
nation of the translation model and additional mod-
els.
Crego and Yvon (2010) extended the approach to
be able to handle different word factors. They used
factored language models introduced by Bilmes and
Kirchhoff (2003) to integrate different word factors
into the translation process. In contrast, we use a
log-linear combination of language models on dif-
ferent factors in our approach.
A first approach of integrating the idea presented
in the n-gram approach into phrase-based machine
translation was described in Matusov et al (2006).
In contrast to our work, they used the bilingual units
as defined in the original approach and they did not
use additional word factors.
Hasan et al (2008) used lexicalized triplets to in-
troduce bilingual context into the translation pro-
cess. These triplets include source words from out-
side the phrase and form and additional probability
p(f |e, e?) that modifies the conventional word prob-
ability of f given e depending on trigger words e? in
the sentence enabling a context-based translation of
ambiguous phrases.
Other approaches address this problem by inte-
grating word sense disambiguation engines into a
phrase-based SMT system. In Chan and Ng (2007)
a classifier exploits information such as local col-
locations, parts-of-speech or surrounding words to
determine the lexical choice of target words, while
Carpuat and Wu (2007) use rich context features
based on position, syntax and local collocations to
dynamically adapt the lexicons for each sentence
and facilitate the choice of longer phrases.
In this work we present a method to extend the
locally limited context of phrase pairs and n-grams
by using bilingual language models. We keep the
phrase-based approach as the main SMT framework
and introduce an n-gram language model trained in a
similar way as the one used in the finite state trans-
ducer approach as an additional feature in the log-
linear model.
3 Motivation
To motivate the introduction of the bilingual lan-
guage model, we will analyze the bilingual context
that is used when selecting the target words. In a
phrase-based system, this context is limited by the
phrase boundaries. No bilingual information outside
the phrase pair is used for selecting the target word.
The effect can be shown in the following example
sentence:
Ein gemeinsames Merkmal aller extremen
Rechten in Europa ist ihr Rassismus
und die Tatsache, dass sie das Einwan-
derungsproblem als politischen Hebel be-
nutzen.
Using our phrase-based SMT system, we get the
following segmentation into phrases on the source
side: ein gemeinsames, Merkmal, aller, extremen
Rechten. That means, that the translation of Merk-
mal is not influenced by the source words gemein-
sames or aller.
However, apart from this segmentation, other
phrases could have been conceivable for building a
translation:
ein, ein gemeinsames, ein gemeinsames Merk-
mal, gemeinsames, gemeinsames Merkmal, Merk-
mal aller, aller, extremen, extremen Rechten and
Rechten.
As shown in Figure 1 the translation for the
first three words ein gemeinsames Merkmal into a
common feature can be created by segmenting it
into ein gemeinsames and Merkmal as done by the
199
Figure 1: Alternative Segmentations
phrase-based system or by segmenting it into ein and
gemeinsames Merkmal. In the phrase-based system,
the decoder cannot make use of the fact that both
segmentation variants lead to the same translation,
but has to select one and use only this information
for scoring the hypothesis.
Consequently, if the first segmentation is cho-
sen, the fact that gemeinsames is translated to com-
mon does effect the translation of Merkmal only by
means of the language model, but no bilingual con-
text can be carried over the segmentation bound-
aries.
To overcome this drawback of the phrase-based
approach, we introduce a bilingual language model
into the phrase-based SMT system. Table 1 shows
the source and target words and demonstrates how
the bilingual phrases are constructed and how the
source context stays available over segment bound-
aries in the calculation of the language model score
for the sentence. For example, when calculating the
language model score for the word feature P ( fea-
ture_Merkmal | common_gemeinsames) we can see
that through the bilingual tokens not only the previ-
ous target word but also the previous source word is
known and can influence the translation even though
it is in a different segment.
4 Bilingual Language Model
The bilingual language model is a standard n-gram-
based language model trained on bilingual tokens in-
stead of simple words. These bilingual tokens are
motivated by the tuples used in n-gram approaches
to machine translation. We use different basic units
for the n-gram model compared to the n-gram ap-
proach, in order to be able to integrate them into a
phrase-based translation system.
In this context, a bilingual token consists of a tar-
get word and all source words that it is aligned to.
More formally, given a sentence pair eI1 = e1...eI
and fJ1 = f1...fJ and the corresponding word align-
ment A = {(i, j)} the following tokens are created:
tj = {fj} ? {ei|(i, j) ? A} (1)
Therefore, the number of bilingual tokens in a
sentence equals the number of target words. If a
source word is aligned to two target words like the
word aller in the example sentence, two bilingual to-
kens are created: all_aller and the_aller. If, in con-
trast, a target word is aligned to two source words,
only one bilingual token is created consisting of the
target word and both source words.
The existence of unaligned words is handled in
the following way. If a target word is not aligned
to any source word, the corresponding bilingual to-
ken consists only of the target word. In contrast, if a
source word is not aligned to any word in the target
language sentence, this word is ignored in the bilin-
gual language model.
Using this definition of bilingual tokens the trans-
lation probability of source and target sentence and
the word alignment is then defined by:
p(eI1, f
J
1 , A) =
J?
j=1
P (tj |tj?1...tj?n) (2)
This probability is then used in the log-linear com-
bination of a phrase-based translation system as an
additional feature. It is worth mentioning that al-
though it is modeled like a conventional language
model, the bilingual language model is an extension
to the translation model, since the translation for the
source words is modeled and not the fluency of the
target text.
To train the model a corpus of bilingual tokens can
be created in a straightforward way. In the genera-
tion of this corpus the order of the target words de-
fines the order of the bilingual tokens. Then we can
use the common language modeling tools to train
the bilingual language model. As it was done for
the normal language model, we used Kneser-Ney
smoothing.
4.1 Comparison to Tuples
While the bilingual tokens are motivated by the tu-
ples in the n-gram approach, there are quite some
differences. They are mainly due to the fact that the
200
Source Target Bi-word LM Prob
ein a a_ein P(a_ein | <s>)
gemeinsames common common_gemeinsames P(common_gemeinsames | a_ein, <s>)
Merkmal feature feature_Merkmal P(feature_Merkmal | common_gemeinsames)
of of_ P(of_ | feature_Merkmal)
aller all all_aller P(all_aller | of_)
aller the the_aller P(the_aller | all_aller, of_)
extremen extreme extreme_extremen P(extreme_extremen)
Rechten right right_Rechten P(right_Rechten | extreme_extremen)
Table 1: Example Sentence: Segmentation and Bilingual Tokens
tuples are also used to guide the search in the n-gram
approach, while the search in the phrase-based ap-
proach is guided by the phrase pairs and the bilin-
gual tokens are only used as an additional feature in
scoring.
While no word inside a tuple can be aligned to
a word outside the tuple, the bilingual tokens are
created based on the target words. Consequently,
source words of one bilingual token can also be
aligned to target words inside another bilingual to-
ken. Therefore, we do not have the problems of em-
bedded words, where there is no independent trans-
lation probability.
Since we do not create a a monotonic segmenta-
tion of the bilingual sentence, but only use the seg-
mentation according to the target word order, it is
not clear where to put source words, which have no
correspondence on the target side. As mentioned be-
fore, they are ignored in the model.
But an advantage of this approach is that we have
no problem handling unaligned target words. We
just create bilingual tokens with an empty source
side. Here, the placing order of the unaligned tar-
get words is guided by the segmentation into phrase
pairs.
Furthermore, we need no additional pruning of
the vocabulary due to computation cost, since this is
already done by the pruning of the phrase pairs. In
our phrase-based system, we allow only for twenty
translations of one source phrase.
4.2 Comparison to Phrase Pairs
Using the definition of the bilingual language model,
we can again have a look at the introductory example
sentence. We saw that when translating the phrase
ein gemeinsames Merkmal using a phrase-based sys-
tem, the translation of gemeinsames into common
can only be influenced by either the preceeding ein
# a or by the succeeding Merkmal # feature, but
not by both of them at the same time, since either
the phrase ein gemeinsames or the phrase gemein-
sames Merkmal has to be chosen when segmenting
the source sentence for translation. If we now look
at the context that can be used when translating this
segment applying the bilingual language model, we
see that the translation of gemeinsames into com-
mon is on the one hand influenced by the translation
of the token ein # a within the bilingual language
model probability P (common_gemeinsames | a_ein,
<s>).
On the other hand, it is also influenced by the
translation of the word Merkmal into feature en-
coded into the probability P (feature_Merkmal |
common_gemeinsames). In contrast to the phrase-
based translation model, this additional model is ca-
pable of using context information from both sides
to score the translation hypothesis. In this way,
when building the target sentence, the information
of aligned source words can be considered even be-
yond phrase boundaries.
4.3 POS-based Bilingual Language Models
When translating with the phrase-based approach,
the decoder evaluates different hypotheses with dif-
ferent segmentations of the source sentence into
phrases. The segmentation depends on available
phrase pair combinations but for one hypothesis
translation the segmentation into phrases is fixed.
This leads to problems, when integrating parallel
POS-based information. Since the amount of differ-
201
ent POS tags in a language is very small compared to
the number of words in a language, we could man-
age much longer phrase pairs based on POS tags
compared to the possible length of phrase pairs on
the word level.
In a phrase-based translation system the average
phrase length is often around two words. For POS
sequences, in contrast, sequences of 4 tokens can
often be matched. Consequently, this information
can only help, if a different segmentation could be
chosen for POS-based phrases and for word-based
phrases. Unfortunately, there is no straightforward
way to integrate this into the decoder.
If we now look at how the bilingual language
model is applied, it is much easier to integrate the
POS-based information. In addition to the bilin-
gual token for every target word we can generate a
bilingual token based on the POS information of the
source and target words. Using this bilingual POS
token, we can train an additional bilingual POS-
based language model and apply it during transla-
tion. In this case it is no longer problematic if the
context of the POS-based bilingual language model
is longer than the one based on the word informa-
tion, because word and POS sequences are scored
separately by two different language models which
cover different n-gram lengths.
The training of the bilingual POS language model
is straightforward. We can build the corpus of bilin-
gual POS tokens based on the parallel corpus of
POS tags generated by running a POS tagger over
both source and target side of the initial parallel cor-
pus and the alignment information for the respective
words in the text corpora.
During decoding, we then also need to know the
POS tag for every source and target word. Since
we build the sentence incrementally, we cannot use
the tagger directly. Instead, we store also the POS
source and target sequences during the phrase ex-
traction. When creating the bilingual phrase pair
with POS information, there might be different pos-
sibilities of POS sequences for the source and target
phrases. But we keep only the most probable one for
each phrase pair. For the Arabic-to-English trans-
lation task, we compared the generated target tags
with the tags created by the tagger on the automatic
translations. They are different on less than 5% of
the words.
Using the alignment information as well as the
source and target POS sequences we can then create
the POS-based bilingual tokens for every phrase pair
and store it in addition to the normal phrase pairs.
At decoding time, the most frequent POS tags in the
bilingual phrases are used as tags for the input sen-
tence and the translation is done based on the bilin-
gual POS tokens built from these tags together with
their alignment information.
5 Results
We evaluated and analyzed the influence of the bilin-
gual language model on different languages. On
the one hand, we measured the performance of the
bilingual language model on German-to-English on
the News translation task. On the other hand, we
evaluated the approach on the Arabic-to-English di-
rection on News and Web data. Additionally, we
present the impact of the bilingual language model
on the English-to-German, German-to-English and
French-to-English systems with which we partici-
pated in the WMT 2011.
5.1 System Description
The German-to-English translation system was
trained on the European Parliament corpus, News
Commentary corpus and small amounts of addi-
tional Web data. The data was preprocessed and
compound splitting was applied. Afterwards the dis-
criminative word alignment approach as described
in (Niehues and Vogel, 2008) was applied to gener-
ate the alignments between source and target words.
The phrase table was built using the scripts from the
Moses package (Koehn et al, 2007). The language
model was trained on the target side of the paral-
lel data as well as on additional monolingual News
data. The translation model as well as the language
model was adapted towards the target domain in a
log-linear way.
The Arabic-to-English system was trained us-
ing GALE Arabic data, which contains 6.1M sen-
tences. The word alignment is generated using
EMDC, which is a combination of a discriminative
approach and the IBM Models as described in Gao
et al (2010). The phrase table is generated using
Chaski as described in Gao and Vogel (2010). The
language model data we trained on the GIGAWord
202
V3 data plus BBN English data. After splitting the
corpus according to sources, individual models were
trained. Then the individual models were interpo-
lated to minimize the perplexity on the MT03/MT04
data.
For both tasks the reordering was performed as a
preprocessing step using POS information from the
TreeTagger (Schmid, 1994) for German and using
the Amira Tagger (Diab, 2009) for Arabic. For Ara-
bic the approach described in Rottmann and Vogel
(2007) was used covering short-range reorderings.
For the German-to-English translation task the ex-
tended approach described in Niehues et al (2009)
was used to cover also the long-range reorderings
typical when translating between German and En-
glish.
For both directions an in-house phrase-based de-
coder (Vogel, 2003) was used to generate the transla-
tion hypotheses and the optimization was performed
using MER training. The performance on the test-
sets were measured in case-insensitive BLEU and
TER scores.
5.2 German to English
We evaluated the approach on two different test sets
from the News Commentary domain. The first con-
sists of 2000 sentences with one reference. It will
be referred to as Test 1. The second test set consists
of 1000 sentences with two references and will be
called Test 2.
5.2.1 Translation Quality
In Tables 2 and 3 the results for translation per-
formance on the German-to-English translation task
are summarized.
As it can been seen, the improvements of transla-
tion quality vary considerably between the two dif-
ferent test sets. While using the bilingual language
model improves the translation by only 0.15 BLEU
and 0.21 TER points on Test 1, the improvement on
Test 2 is nearly 1 BLEU point and 0.5 TER points.
5.2.2 Context Length
One intention of using the bilingual language
model is its capability to capture the bilingual con-
texts in a different way. To see, whether additional
bilingual context is used during decoding, we ana-
lyzed the context used by the phrase pairs and by
the n-gram bilingual language model.
However, a comparison of the different context
lengths is not straightforward. The context of an n-
gram language model is normally described by the
average length of applied n-grams. For phrase pairs,
normally the average target phrase pair length (avg.
Target PL) is used as an indicator for the size of the
context. And these two numbers cannot be com-
pared directly.
To be able to compare the context used by the
phrase pairs to the context used in the n-gram lan-
guage model, we calculated the average left context
that is used for every target word where the word
itself is included, i.e. the context of a single word
is 1. In case of the bilingual language model the
score for the average left context is exactly the aver-
age length of applied n-grams in a given translation.
For phrase pairs the average left context can be cal-
culated in the following way: A phrase pair of length
1 gets a left context score of 1. In a phrase pair of
length 2, the first word has a left context score of 1,
since it is not influenced by any target word to the
left. The second word in that phrase pair gets a left
context count of 2, because it is influenced by the
first word in the phrase. Correspondingly, the left
context score of a phrase pair of length 3 is 6 (com-
posed of the score 1 for the first word, score 2 for
the second word and score 3 for the third word). To
get the average left context for the whole translation,
the context scores of all phrases are summed up and
divided by the number of words in the translation.
The scores for the average left contexts for the two
test sets are shown in Tables 2 and 3. They are called
avg. PP Left Context. As it can be seen, the con-
text used by the bilingual n-gram language model is
longer than the one by the phrase pairs. The average
n-gram length increases from 1.58 and 1.57, respec-
tively to 2.21 and 2.18 for the two given test sets.
If we compare the average n-gram length of the
bilingual language model to the one of the target
language model, the n-gram length of the first is of
course smaller, since the number of possible bilin-
gual tokens is higher than the number of possible
monolingual words. This can also be seen when
looking at the perplexities of the two language mod-
els on the generated translations. While the perplex-
ity of the target language model is 99 and 101 on
Test 1 and 2, respectively, the perplexity of the bilin-
203
gual language model is 512 and 538.
Metric No BiLM BiLM
BLEU 30.37 30.52
TER 50.27 50.06
avg. Target PL 1.66 1.66
avg. PP Left Context 1.57 1.58
avg. Target LM N-Gram 3.28 3.27
avg. BiLM N-Gram 2.21
Table 2: German-to-English results (Test 1)
Metric No BiLM BiLM
BLEU 44.16 45.09
TER 41.02 40.52
avg. Target PL 1.65 1.65
avg. PP Left Context 1.56 1.57
avg. Target LM N-Gram 3.25 3.23
avg. BiLM N-Gram 2.18
Table 3: German-to-English results (Test 2)
5.2.3 Overlapping Context
An additional advantage of the n-gram-based ap-
proach is the possibility to have overlapping con-
text. If we would always use phrase pairs of length
2 only half of the adjacent words would influence
each other in the translation. The others are only
influenced by the other target words through the lan-
guage model. If we in contrast would have a bilin-
gual language model which uses an n-gram length
of 2, this means that every choice of word influences
the previous and the following word.
To analyze this influence, we counted how many
borders of phrase pairs are covered by a bilingual
n-gram. For Test 1, 16783 of the 27785 borders
between phrase pairs are covered by a bilingual n-
gram. For Test 2, 9995 of 16735 borders are cov-
ered. Consequently, in both cases at around 60 per-
cent of the borders additional information can be
used by the bilingual n-gram language model.
5.2.4 Bilingual N-Gram Length
For the German-to-English translation task we
performed an additional experiment comparing dif-
ferent n-gram lengths of the bilingual language
BiLM Length aNGL BLEU TER
No 30.37 50.27
1 1 29.67 49.73
2 1.78 30.36 50.05
3 2.11 30.47 50.08
4 2.21 30.52 50.06
5 2.23 30.52 50.07
6 2.24 30.52 50.07
Table 4: Different N-Gram Lengths (Test 1)
BiLM Length aNGL BLEU TER
No 44.16 41.02
1 1 44.22 40.53
2 1.78 45.11 40.38
3 2.09 45.18 40.51
4 2.18 45.09 40.52
5 2.21 45.10 40.52
6 2.21 45.10 40.52
Table 5: Different N-Gram Lengths (Test 2)
model. To ensure comparability between the exper-
iments and avoid additional noise due to different
optimization results, we did not perform separate
optimization runs for for each of the system vari-
ants with different n-gram length, but used the same
scaling factors for all of them. Of course, the sys-
tem using no bilingual language model was trained
independently. In Tables 4 and 5 we can see that the
length of the actually applied n-grams as well as the
BLEU score increased until the bilingual language
model reaches an order of 4. For higher order bilin-
gual language models, nearly no additional n-grams
can be found in the language models. Also the trans-
lation quality does not increase further when using
longer n-grams.
5.3 Arabic to English
The Arabic-to-English system was optimized on the
MT06 data. As test set the Rosetta in-house test set
DEV07-nw (News) and wb (Web Data) was used.
The results for the Arabic-to-English translation
task are summarized in Tables 6 and 7. The perfor-
mance was tested on two different domains, transla-
tion of News and Web documents. On both tasks,
the translation could be improved by more than 1
204
BLEU point. Measuring the performance in TER
also shows an improvement by 0.7 and 0.5 points.
By adding a POS-based bilingual language
model, the performance could be improved further.
An additional gain of 0.2 BLEU points and decrease
of 0.3 points in TER could be reached. Conse-
quently, an overall improvement of up to 1.7 BLEU
points could be achieved by integrating two bilin-
gual language models, one based on surface word
forms and one based on parts-of-speech.
System
Dev Test
BLEU TER BLEU
NoBiLM 48.42 40.77 52.05
+ BiLM 49.29 40.04 53.51
+ POS BiLM 49.56 39.85 53.71
Table 6: Results on Arabic to English: Translation of
News
System
Dev Test
BLEU TER BLEU
NoBiLM 48.42 47.14 41.90
+ BiLM 49.29 46.66 43.12
+ POS BiLM 49.56 46.40 43.28
Table 7: Results on Arabic to English: Translation of
Web documents
As it was done for the German-to-English system,
we also compared the context used by the different
models for this translation direction. The results are
summarized in Table 8 for the News test set and in
Table 9 for the translation of Web data. It can be seen
like it was for the other language pair that the context
used in the bilingual language model is bigger than
the one used by the phrase-based translation model.
Furthermore, it is worth mentioning that shorter
phrase pairs are used, when using the POS-based
bilingual language model. Both bilingual language
models seem to model the context quite good, so that
less long phrase pairs are needed to build the trans-
lation. Instead, the more frequent short phrases can
be used to generate the translation.
5.4 Shared Translation Task @ WMT2011
The bilingual language model was included in 3
systems built for the WMT2011 Shared Translation
Metric No BiLM POS BiLM
BLEU 52.05 53.51 53.71
avg. Target PL 2.12 2.03 1.79
avg. PP Left Context 1.92 1.85 1.69
avg. BiLM N-Gram 2.66 2.65
avg. POS BiLM 4.91
Table 8: Bilingual Context in Arabic-to-English results
(News)
Metric No BiLM POS BiLM
BLEU 41.90 43.12 43.28
avg. Target PL 1.82 1.80 1.57
avg. PP Left Context 1.72 1.69 1.53
avg. BiLM N-Gram 2.33 2.31
avg. POS BiLM 4.49
Table 9: Bilingual Context in Arabic-to-English results
(Web data)
Task evaluation. A phrase-based system similar to
the one described before for the German-to-English
results was used. A detailed system description can
be found in Herrmann et al (2011). The results are
summarized in Table 10. The performance of com-
petitive systems could be improved in all three lan-
guages by up to 0.4 BLEU points.
Language Pair No BiLM BiLM
German-English 24.12 24.52
English-German 16.89 17.01
French-English 28.17 28.34
Table 10: Preformance of Bilingual language model at
WMT2011
6 Conclusion
In this work we showed how a feature of the n-gram-
based approach can be integrated into a phrase-
based statistical translation system. We performed
a detailed analysis on how this influences the scor-
ing of the translation system. We could show im-
provements on a variety of translation tasks cover-
ing different languages and domains. Furthermore,
we could show that additional bilingual context in-
formation is used.
Furthermore, the additional feature can easily be
205
extended to additional word factors such as part-of-
speech, which showed improvements for the Arabic-
to-English translation task.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Fran?ois Yvon. 2010. LIMSI?s Statisti-
cal Translation Systems for WMT?10. In Fifth Work-
shop on Statistical Machine Translation (WMT 2010),
Uppsala, Sweden.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Fac-
tored language models and generalized parallel back-
off. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 4?6, Stroudsburg, PA, USA.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation using Word Sense Disam-
biguation. In In The 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine Translation with Inferred Stochastic Finite-State
Transducers. Comput. Linguist., 30:205?225, June.
Yee Seng Chan and Hwee Tou Ng. 2007. Word Sense
Disambiguation improves Statistical Machine Trans-
lation. In In 45th Annual Meeting of the Association
for Computational Linguistics (ACL-07, pages 33?40.
Josep M. Crego and Fran?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24, June.
Mona Diab. 2009. Second Generation Tools (AMIRA
2.0): Fast and Robust Tokenization, POS tagging, and
Base Phrase Chunking. In Proc. of the Second Interna-
tional Conference on Arabic Language Resources and
Tools, Cairo, Egypt, April.
Qin Gao and Stephan Vogel. 2010. Training Phrase-
Based Machine Translation Models on the Cloud:
Open Source Machine Translation Toolkit Chaski. In
The Prague Bulletin of Mathematical Linguistics No.
93.
Qin Gao, Francisco Guzman, and Stephan Vogel.
2010. EMDC: A Semi-supervised Approach for Word
Alignment. In Proc. of the 23rd International Confer-
ence on Computational Linguistics, Beijing, China.
Sa?a Hasan, Juri Ganitkevitch, Hermann Ney, and Jes?s
Andr?s-Ferrer. 2008. Triplet Lexicon Models for Sta-
tistical Machine Translation. In Proc. of Conference
on Empirical Methods in NLP, Honolulu, USA.
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The Karlsruhe Institute of
Technology Translation Systems for the WMT 2011.
In Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinbugh, U.K.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Demonstration Session, Prague, Czech Repub-
lic, June 23.
Jos? B. Mari?o, Rafael E. Banchs, Josep M. Crego, Adri?
de Gispert, Patrik Lambert, Jos? A. R. Fonollosa, and
Marta R. Costa-juss?. 2006. N-gram-based machine
translation. Comput. Linguist., 32, December.
Evgeny Matusov, Richard Zens, David Vilar, Arne
Mauser, Maja Popovic?, Sa?a Hasan, and Hermann
Ney. 2006. The rwth machine translation system. In
TC-STAR Workshop on Speech-to-Speech Translation,
pages 31?36, Barcelona, Spain, June.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and Alex
Waibel. 2009. The Universit?t Karlsruhe Translation
System for the EACL-WMT 2009. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sk?vde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
206
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 379?385,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2011
Teresa Herrmann, Mohammed Mediani, Jan Niehues and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT11 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual and fine-grained POS language
models, POS-based reordering, lattice phrase
extraction and discriminative word alignment.
Furthermore, we present a special filtering
method for the English-French Giga corpus
and the phrase scoring step in the training is
parallelized.
1 Introduction
In this paper we describe our systems for the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. These include advanced reordering models and
corresponding adaptations to the phrase extraction
process as well as extension to the translation and
language model in form of discriminative word
alignment and a bilingual language model to ex-
tend source word context. For English-German, lan-
guage models based on fine-grained part-of-speech
tags were used to address the difficult target lan-
guage generation due to the rich morphology of Ger-
man.
We also present a filtering method directly ad-
dressing the problems of web-crawled corpora,
which enabled us to make use of the French-English
Giga corpus. Another novelty in our systems this
year is the parallel phrase scoring method that re-
duces the time needed for training which is espe-
cially convenient for such big corpora as the Giga
corpus.
2 System Description
The baseline systems for all languages use a trans-
lation model that is trained on EPPS and the News
Commentary corpus and the phrase table is based
on a GIZA++ word alignment. The language model
was trained on the monolingual parts of the same
corpora by the SRILM Toolkit (Stolcke, 2002). It
is a 4-gram SRI language model using Kneser-Ney
smoothing.
The problem of word reordering is addressed us-
ing the POS-based reordering model as described
in Section 2.4. The part-of-speech tags for the re-
ordering model are obtained using the TreeTagger
(Schmid, 1994).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation and optimization with
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 20 translation
options for every source phrase were considered.
2.1 Data
We trained all systems using the parallel EPPS and
News Commentary corpora. In addition, the UN
corpus and the Giga corpus were used for training
379
the French-English systems.
Optimization was done for most languages using
the news-test2008 data set and news-test2010 was
used as test set. The only exception is German-
English, where news-test2009 was used for opti-
mization due to system combination arrangements.
The language models for the baseline systems were
trained on the monolingual versions of the training
corpora. Later on, we used the News Shuffle and the
Gigaword corpus to train bigger language models.
For training a discriminative word alignment model,
a small amount of hand-aligned data was used.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first words of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus
we use the hunspell1 lexicon to map words writ-
ten according to old German spelling to new Ger-
man spelling, to obtain a corpus with homogenous
spelling.
Compound splitting as described in Koehn and
Knight (2003) is applied to the German part of the
corpus for the German-to-English system to reduce
the out-of-vocabulary problem for German com-
pound words.
2.3 Special filtering of the Giga parallel Corpus
The Giga corpus incorporates non-neglegible
amounts of noise even after our usual preprocess-
ing. This noise may be due to different causes.
For instance: non-standard HTML characters,
meaningless parts composed of only hypertext
codes, sentences which are only partial translation
of the source, or eventually not a correct translation
at all.
Such noisy pairs potentially degrade the transla-
tion model quality, therefore it seemed more conve-
nient to eliminate them.
Given the size of the corpus, this task could not be
performed manually. Consequently, we used an au-
tomatic classifier inspired by the work of Munteanu
and Marcu (2005) on comparable corpora. This clas-
1http://hunspell.sourceforge.net/
sifier should be able to filter out the pairs which
likely are not beneficial for the translation model.
In order to reliably decide about the classifier to
use, we evaluated several techniques. The training
and test sets for this evaluation were built respec-
tively from nc-dev2007 and nc-devtest2007. In each
set, about 30% randomly selected source sentences
switch positions with the immediate following so
that they form negative examples. We also used lex-
ical dictionaries in both directions based on EPPS
and UN corpora.
We relied on seven features in our classifiers:
IBM1 score in both directions, number of unaligned
source words, the difference in number of words be-
tween source and target, the maximum source word
fertility, number of unaligned target words, and the
maximum target word fertility. It is noteworthy
that all the features requiring alignment information
(such as the unaligned source words) were computed
on the basis of the Viterbi path of the IBM1 align-
ment. The following classifiers were used:
Regression Choose either class based on a
weighted linear combination of the features
and a fixed threshold of 0.5.
Logistic regression The probability of the class is
expressed as a sigmoid of a linear combination
of the different features. Then the class with
the highest probability is picked.
Maximum entropy classifier We used the same set
of features to train a maximum entropy classi-
fier using the Megam package2.
Support vector machines classifier An SVM clas-
sifier was trained using the SVM-light pack-
age3.
Results of these experiments are summarized in
Table 1.
The regression weights were estimated so that to
minimize the squared error. This gave us a pretty
poor F-measure score of 90.42%. Given that the lo-
gistic regression is more suited for binary classifica-
tion in our case than the normal regression, it led to
significant increase in the performance. The training
2http://www.cs.utah.edu/?hal/megam/
3http://svmlight.joachims.org/
380
Approach Precision Recall F-measure
Regression 93.81 87.27 90.42
LogReg 93.43 94.84 94.13
MaxEnt 93.69 94.54 94.11
SVM 98.20 96.87 97.53
Table 1: Results of the filtering experiments
was held by maximizing the likelihood to the data
with L2 regularization (with ? = 0.1). This gave an
F-measure score of 94.78%.
The maximum entropy classifier performed better
than the logistic regression in terms of precision but
however it had worse F-measure.
Significant improvements could be noticed us-
ing the SVM classifier in both precision and recall:
98.20% precision, 96.87% recall, and thus 97.53%
F-measure.
As a result, we used the SVM classifier to filter
the Giga parallel corpus. The corpus contained orig-
inally around 22.52 million pairs. After preprocess-
ing and filtering it was reduced to 16.7 million pairs.
Thus throwing around 6 million pairs.
2.4 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
tion model, we use a different approach that relies
on part-of-speech (POS) sequences. By abstracting
from surface words to parts-of-speech, we expect to
model the reordering more accurately.
2.4.1 POS-based Reordering Model
To model reordering we first learn probabilistic
rules from the POS tags of the words in the train-
ing corpus and the alignment information. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009). The reorder-
ing rules are applied to the source text and the orig-
inal order of words and the reordered sentence vari-
ants generated by the rules are encoded in a word
lattice which is used as input to the decoder.
2.4.2 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract the phrase pairs for orig-
inally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths.
To limit the number of extracted phrase pairs, we
extract a source phrase only once per sentence even
if it is found in different paths.
2.5 Translation and Language Models
In addition to the models used in the baseline sys-
tem described above we conducted experiments in-
cluding additional models that enhance translation
quality by introducing alternative or additional in-
formation into the translation or language modelling
process.
2.5.1 Discriminative Word Alignment
In most of our systems we use the PGIZA++
Toolkit4 to generate alignments between words in
the training corpora. The word alignments are gen-
erated in both directions and the grow-diag-final-and
heuristic is used to combine them. The phrase ex-
traction is then done based on this word alignment.
In the English-German system we applied the
Discriminative Word Alignment approach as de-
scribed in Niehues and Vogel (2008) instead. This
alignment model is trained on a small corpus of
hand-aligned data and uses the lexical probability
as well as the fertilities generated by the PGIZA++
Toolkit and POS information.
2.5.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
4http://www.cs.cmu.edu/?qing/
381
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, an additional lan-
guage model in the phrase-based system in which
each token consist of a target word and all source
words it is aligned to. The bilingual tokens enter
the translation process as an additional target factor
and the bilingual language model is applied to the
additional factor like a normal language model. For
more details see (Niehues et al, 2011).
2.5.3 Parallel phrase scoring
The process of phrase scoring is held in two runs.
The objective of the first run is to compute the nec-
essary counts and to estimate the scores, all based
on the source phrases; while the second run is sim-
ilarly held based on the target phrases. Thus, the
extracted phrases have to be sorted twice: once by
source phrase and once by target phrase. These two
sorting operations are almost always done on an ex-
ternal storage device and hence consume most of the
time spent in this step.
The phrase scoring step was reimplemented in or-
der to exploit the available computation resources
more efficiently and therefore reduce the process-
ing time. It uses optimized sorting algorithms for
large data volumes which cannot fit into memory
(Vitter, 2008). In its core, our implementation re-
lies on STXXL: an extension of the STL library for
external memory (Kettner, 2005) and on OpenMP
for shared memory parallelization (Chapman et al,
2007).
Table 2 shows a comparison between Moses and
our phrase scoring tools. The comparison was held
using sixteen-core 64-bit machines with 128 Gb
RAM, where the files are accessed through NFS on
a RAID disk. The experiments show that the gain
grows linearly with the size of input with an average
of 40% of speed up.
2.5.4 POS Language Models
In addition to surface word language models, we
did experiments with language models based on
part-of-speech for English-German. We expect that
having additional information in form of probabil-
ities of part-of-speech sequences should help espe-
cially in case of the rich morphology of German and
#pairs(G) Moses ?103(s) KIT ?103(s)
0.203 25.99 17.58
1.444 184.19 103.41
1.693 230.97 132.79
Table 2: Comparison of Moses and KIT phrase extraction
systems
therefore the more difficult target language genera-
tion.
The part-of-speeches were generated using the
TreeTagger and the RFTagger (Schmid and Laws,
2008), which produces more fine-grained tags that
include also person, gender and case information.
While the TreeTagger assigns 54 different POS tags
to the 357K German words in the corpus, the RF-
Tagger produces 756 different fine-grained tags on
the same corpus.
We tried n-gram lengths of 4 and 7. While no im-
provement in translation quality could be achieved
using the POS language models based on the normal
POS tags, the 4-gram POS language model based
on fine-grained tags could improve the translation
system by 0.2 BLEU points as shown in Table 3.
Surprisingly, increasing the n-gram length to 7 de-
creased the translation quality again.
To investigate the impact of context length, we
performed an analysis on the outputs of two different
systems, one without a POS language model and one
with the 4-gram fine-grained POS language model.
For each of the translations we calculated the aver-
age length of the n-grams in the translation when
applying one of the two language models using 4-
grams of surface words or parts-of-speech. The re-
sults are also shown in Table 3.
The average n-gram length of surface words on
the translation generated by the system without POS
language model and the one using the 4-gram POS
language model stays practically the same. When
measuring the n-gram length using the 4-gram POS
language model, the context increases to 3.4. This
increase of context is not surprising, since with
the more general POS tags longer contexts can be
matched. Comparing the POS context length for
the two translations, we can see that the context in-
creases from 3.18 to 3.40 due to longer matching
POS sequences. This means that the system using
382
the POS language model actually generates trans-
lations with more probable POS sequences so that
longer matches are possible. Also the perplexity
drops by half since the POS language model helps
constructing sentences that have a better structure.
System BLEU avg. ngram length PPL
Word POS POS
no POS LM 16.64 2.77 3.18 66.78
POS LM 16.88 2.81 3.40 33.36
Table 3: Analysis of context length
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The German-to-English baseline system applies
short-range reordering rules and uses a language
model trained on the EPPS and News Commen-
tary. By exchanging the baseline language model
by one trained on the News Shuffle corpus we im-
prove the translation quality considerably, by more
than 3 BLEU points. When we expand the cov-
erage of the reordering rules to enable long-range
reordering we can improve even further by 0.4 and
adding a second language model trained on the En-
glish Gigaword corpus we gain another 0.3 BLEU
points. To ensure that the phrase table also includes
reordered phrases, we use lattice phrase extraction
and can achieve a small improvement. Finally, a
bilingual language model is added to extend the con-
text of source language words available for transla-
tion, reaching the best score of 23.35 BLEU points.
This system was used for generating the translation
submitted to the German-English Translation Task.
3.2 English-German
The English-to-German baseline system also in-
cludes short-range reordering and uses translation
System Dev Test
Baseline 18.49 19.10
+ NewsShuffle LM 20.63 22.24
+ LongRange Reordering 21.00 22.68
+ Additional Giga LM 21.80 22.92
+ Lattice Phrase Extraction 21.87 22.96
+ Bilingual LM 22.05 23.35
Table 4: Translation results for German-English
and language model based on EPPS and News Com-
mentary. Exchanging the language model by the
News Shuffle language model again yields a big im-
provement by 2.3 BLEU points. Adding long-range
reordering improves a lot on the development set
while the score on the test set remains practically
the same. Replacing the GIZA++ alignments by
alignments generated using the Discriminative Word
Alignment Model again only leads to a small im-
provement. By using the bilingual language model
to increase context we can gain 0.1 BLEU points
and by adding the part-of-speech language model
with rich parts-of-speech including case, number
and gender information for German we achieve the
best score of 16.88. This system was used to gener-
ate the translation used for submission.
System Dev Test
Baseline 13.55 14.19
+ NewsShuffle LM 15.10 16.46
+ LongRange Reordering 15.79 16.46
+ DWA 15.81 16.52
+ Bilingual LM 15.85 16.64
+ POS LM 15.88 16.88
Table 5: Translation results for English-German
3.3 English-French
Table 6 summarizes how our system for English-
French evolved. The baseline system for this direc-
tion was trained on the EPPS and News Commen-
tary corpora, while the language model was trained
on the French part of the EPPS, News Commen-
tary and UN parallel corpora. Some improvement
could be already seen by introducing the short-range
reorderings trained on the baseline parallel corpus.
383
Apparently, the UN data brought only slight im-
provement to the overall performance. On the other
hand, adding bigger language models trained on the
monolingual French version of EPPS, News Com-
mentary and the News Shuffle together with the
French Gigaword corpus introduces an improvement
of 3.7 on test. Using a system trained only on the
Giga corpus data with the same last configuration
shows a significant gain. It showed an improvement
of around 1.0. We were able to obtain some further
improvements by merging the translation models of
the last two systems. i.e. the one system based on
EPPS, UN, and News Commentary and the other on
the Giga corpus. This merging increased our score
by 0.2. Finally, our submitted system for this direc-
tion was obtained by using a single language model
trained on the union of all the French corpora in-
stead of using multiple models. This resulted in an
improvement of 0.1 leading to our best score: 28.28.
System Dev Test
Baseline 20.62 22.36
+ Reordering 21.29 23.11
+ UN 21.27 23.24
+ Big LMs 23.77 26.90
Giga data 24.53 27.94
Merge 24.74 28.14
+ Merged LMs 25.07 28.28
Table 6: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 7. Our sys-
tem for this direction evolved quite similarly to the
opposite direction. The largest improvement accom-
panied the integration of the bigger language mod-
els (trained on the English version of EPPS, News
Commentary, News Shuffle and the Gigaword cor-
pus): 3.3 BLEU points, whereas smaller improve-
ments could be gained by applying the short reorder-
ing rules and almost no change by including the UN
data. Further gains were obtained by training the
system on the Giga corpus added to the previous
parallel data. This increased our performance by
0.6. The submitted system was obtained by aug-
menting the last system with a bilingual language
model adding around 0.2 to the previous score and
thus giving 28.34 as final score.
System Dev Test
Baseline 20.76 23.78
+ Reordering 21.42 24.28
+ UN 21.55 24.21
+ Big LMs 24.16 27.55
+ Giga data 24.86 28.17
+ BiLM 25.01 28.34
Table 7: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2011 Evaluation for English?German
and English?French. For English?French, a spe-
cial filtering method for web-crawled data was de-
veloped. In addition, a parallel phrase scoring tech-
nique was implemented that could speed up the MT
training process tremendously. Using these two fea-
tures, we were able to integrate the huge amounts of
data available in the Giga corpus into our systems
translating between English and French.
We applied POS-based reordering to improve our
translations in all directions, using short-range re-
ordering for English?French and long-range re-
ordering for English?German. For German-
English, reordering also the training corpus lead to
further improvements of the translation quality.
A Discriminative Word Alignment Model led to
an increase in BLEU for English-German. For this
direction we also tried fine-grained POS language
models of different n-gram lengths. The best trans-
lations could be obtained by using 4-grams.
For nearly all experiments, a bilingual language
model was applied that expands the context of
source words that can be considered during decod-
ing. The improvements range from 0.1 to 0.4 in
BLEU score.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
384
References
Barbara Chapman, Gabriele Jost, and Ruud van der Pas.
2007. Using OpenMP: Portable Shared Memory Par-
allel Programming (Scientific and Engineering Com-
putation). The MIT Press.
Roman Dementiev Lutz Kettner. 2005. Stxxl: Stan-
dard template library for xxl data sets. In Proceedings
of ESA 2005. Volume 3669 of LNCS, pages 640?651.
Springer.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Jeffrey Scott Vitter. 2008. Algorithms and Data Struc-
tures for External Memory. now Publishers Inc.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
385
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322?329,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joint WMT 2012 Submission of the QUAERO Project
?Markus Freitag, ?Stephan Peitz, ?Matthias Huck, ?Hermann Ney,
?Jan Niehues, ?Teresa Herrmann, ?Alex Waibel,
?Le Hai-son, ?Thomas Lavergne, ?Alexandre Allauzen,
?Bianka Buschbeck, ?Josep Maria Crego, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2012 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems and finally the RWTH system combi-
nation combined these translations in our final
submission. Experimental results show im-
provements of up to 1.7 points in BLEU and
3.4 points in TER compared to the best single
system.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
This paper is structured as follows. In Section
2, the different engines of all four groups are in-
troduced. In Section 3, the RWTH Aachen system
combination approach is presented. Experiments
with different system selections for system combi-
nation are described in Section 4. Finally in Section
5, we discuss the results.
2 Translation Systems
For WMT 2012 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned
on the newstest2009 or newstest2010 development
set. The newstest2011 dev set was used to train
the system combination parameters. Finally, the
newstest2008-newstest2010 dev sets were used to
compare the results of the different system combina-
tion settings. In this Section all four different system
engines are presented.
2.1 RWTH Aachen Single Systems
For the WMT 2012 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
parallel corpus, the translation probabilities are esti-
mated by relative frequencies. The standard feature
322
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. The model weights are optimized with standard
Mert (Och, 2003) on 200-best lists. The optimiza-
tion criterium is BLEU.
2.1.2 Hierarchical System
For the hierarchical setups (HPBT) described in
this paper, the open source Jane toolkit (Vilar et
al., 2010) is employed. Jane has been developed at
RWTH and implements the hierarchical approach as
introduced by Chiang (2007) with some state-of-the-
art extensions. In hierarchical phrase-based transla-
tion, a weighted synchronous context-free grammar
is induced from parallel text. In addition to contigu-
ous lexical phrases, hierarchical phrases with up to
two gaps are extracted. The search is typically car-
ried out using the cube pruning algorithm (Huang
and Chiang, 2007). The model weights are opti-
mized with standard Mert (Och, 2003) on 100-best
lists. The optimization criterium is 4BLEU ?TER.
2.1.3 Preprocessing
In order to reduce the source vocabulary size
translation, the German text was preprocessed
by splitting German compound words with the
frequency-based method described in (Koehn and
Knight, 2003a). To further reduce translation com-
plexity for the phrase-based approach, we performed
the long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.4 Language Model
For both decoders a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl, the
109 French-English, UN and LDC Gigaword Fourth
Edition corpora. For the 109 French-English, UN
and LDC Gigaword corpora RWTH applied the data
selection technique described in (Moore and Lewis,
2010).
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogenous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003b). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation and op-
timization with regard to the BLEU score is done us-
ing Minimum Error Rate Training as described in
Venugopal et al (2005).
2.2.3 Translation Models
The translation model is trained on the Europarl
and News Commentary Corpus and the phrase ta-
ble is based on a discriminative word alignment
(Niehues and Vogel, 2008).
In addition, the system applies a bilingual lan-
guage model (Niehues et al, 2011) to extend the
context of source language words available for trans-
lation.
Furthermore, we use a discriminative word lexi-
con as introduced in (Mauser et al, 2009). The lex-
icon was trained and integrated into our system as
described in (Mediani et al, 2011).
At last, we tried to find translations for
out-of-vocabulary (OOV) words by using quasi-
morphological operations as described in Niehues
and Waibel (2011). For each OOV word, we try to
find a related word that we can translate. We modify
the ending letters of the OOV word and learn quasi-
morphological operations to be performed on the
known translation of the related word to synthesize
a translation for the OOV word. By this approach
we were for example able to translate Kaminen into
chimneys using the known translation Kamin # chim-
ney.
2.2.4 Language Models
We use two 4-gram SRI language models, one
trained on the News Shuffle corpus and one trained
1http://hunspell.sourceforge.net/
323
on the Gigaword corpus. Furthermore, we use a 5-
gram cluster-based language model trained on the
News Shuffle corpus. The word clusters were cre-
ated using the MKCLS algorithm. We used 100
word clusters.
2.2.5 Reordering Model
Reordering is performed based on part-of-speech
tags obtained using the TreeTagger (Schmid, 1994).
Based on these tags we learn probabilistic continu-
ous (Rottmann and Vogel, 2007) and discontinuous
(Niehues and Kolss, 2009) rules to cover short and
long-range reorderings. The rules are learned from
the training corpus and the alignment. In addition,
we learned tree-based reordering rules. Therefore,
the training corpus was parsed by the Stanford parser
(Rafferty and Manning, 2008). The tree-based rules
consist of the head node of a subtree and all its
children as well as the new order and a probability.
These rules were applied recursively. The reordering
rules are applied to the source sentences and the re-
ordered sentence variants as well as the original se-
quence are encoded in a word lattice which is used
as input to the decoder. For the test sentences, the
reordering based on parts-of-speech and trees allows
us to change the word order in the source sentence
so that the sentence can be translated more easily.
In addition, we build reordering lattices for all train-
ing sentences and then extract phrase pairs from the
monotone source path as well as from the reordered
paths.
2.3 LIMSI-CNRS Single System
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine translation
system based on bilingual n-gram2. In this approach,
the translation model relies on a specific decomposi-
tion of the joint probability of a sentence pair P(s, t)
using the n-gram assumption: a sentence pair is de-
composed into a sequence of bilingual units called
tuples, defining a joint segmentation of the source
and target. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing which ultimately derives from initial word and
phrase alignments.
2http://ncode.limsi.fr/
2.3.1 An Overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using a
n-gram model of (source,target) pairs (Casacuberta
and Vidal, 2004). Training this model requires to
reorder source sentences so as to match the target
word order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a ?weak?
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
used in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 development set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2.3.2 Continuous Space Translation Models
One critical issue with standard n-gram transla-
tion models is that the elementary units are bilingual
pairs, which means that the underlying vocabulary
can be quite large. Unfortunately, the parallel data
available to train these models are typically smaller
than the corresponding monolingual corpora used to
train target language models. It is very likely then,
that such models should face severe estimation prob-
lems. In such setting, using neural network language
3Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
324
model techniques seem all the more appropriate. For
this study, we follow the recommendations of Le et
al. (2012), who propose to factor the joint proba-
bility of a sentence pair by decomposing tuples in
two (source and target) parts, and further each part
in words. This yields a word factored translation
model that can be estimated in a continuous space
using the SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the computa-
tional cost of computing n-gram probabilities. The
solution used here was to resort to a two pass ap-
proach: the first pass uses a conventional back-off
n-gram model to produce a k-best list; in the second
pass, the k-best list is reordered using the probabil-
ities of m-gram SOUL translation models. In the
following experiments, we used a fixed context size
for SOUL of m = 10, and used k = 300.
2.3.3 Corpora and Data Preprocessing
The parallel data is word-aligned using
MGIZA++4 with default settings. For the En-
glish monolingual training data, we used the same
setup as last year5 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we took advantage of our in-house
text processing tools for tokenization and detok-
enization steps (De?chelotte et al, 2008) and our sys-
tem was built in ?true-case?. As German is mor-
phologically more complex than English, the default
policy which consists in treating each word form
independently is plagued with data sparsity, which
is detrimental both at training and decoding time.
Thus, the German side was normalized using a spe-
cific pre-processing scheme (Allauzen et al, 2010;
Durgar El-Kahlout and Yvon, 2010), which notably
aims at reducing the lexical redundancy by (i) nor-
malizing the orthography, (ii) neutralizing most in-
flections and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
a system composed of the standard SYSTRAN MT
engine in combination with a statistical post editing
(SPE) component.
4http://geek.kyloo.net/software
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k -
800k entries per language pair).
The SYSTRAN phrase-based SPE component
views the output of the rule-based system as the
source language, and the (human) reference trans-
lation as the target language, see (L. Dugast and
Koehn, 2007). It performs corrections and adaptions
learned from the 5-gram language model trained on
the parallel target-to-target corpus. Moreover, the
following measures - limiting unwanted statistical
effects - were applied:
? Named entities, time and numeric expressions
are replaced by special tokens on both sides.
This usually improves word alignment, since
the vocabulary size is significantly reduced. In
addition, entity translation is handled more re-
liably by the rule-based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus to help to improve word
alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
The SPE language model was trained on 2M bilin-
gual phrases from the news/Europarl corpora, pro-
vided as training data for WMT 2012. An addi-
tional language model built from 15M phrases of
the English LDC Gigaword corpus using Kneser-
Ney (Kneser and Ney, 1995) smoothing was added.
Weights for these separate models were tuned by
the Mert algorithm provided in the Moses toolkit
(P. Koehn et al, 2007), using the provided news de-
velopment set.
325
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical models
is selected as consensus translation.
4 Experiments
This year, we tried different sets of single systems
for system combination. As RWTH has two dif-
ferent translation systems, we put the output of
both systems into system combination. Although
both systems have the same preprocessing and lan-
guage model, their hypotheses differ because of
their different decoding approach. Compared to
the other systems, the system by SYSTRAN has a
completely different approach (see section 2.4). It
is mainly based on a rule-based system. For the
German?English pair, SYSTRAN achieves a lower
BLEU score in each test set compared to the other
groups. However, since the SYSTRAN system is
very different to the others, we still obtain an im-
provement when we add it also to system combina-
tion.
We did experiments with different optimization
criteria for the system combination optimization.
All results are listed in Table 1 (unoptimized), Table
2 (optimized on BLEU) and Table 3 (optimized on
TER-BLEU). Further, we investigated, whether we
will loose performance, if a single system is dropped
from the system combination. The results show that
for each optimization criteria we need all systems to
achieve the best results.
For the BLEU optimized system combination, we
obtain an improvement compared to the best sin-
gle systems for all dev sets. For newstest2008, we
get an improvement of 1.5 points in BLEU and 1.5
points in TER compared to the best single system of
Karlsruhe Institute of Technology. For newstest2009
we get an improvement of 1.9 points in BLEU and
1.5 points in TER compared to the best single sys-
tem. The system combination of all systems outper-
forms the best single system with 1.9 points in BLEU
and 1.9 points in TER for newstest2010. For new-
stest2011 the improvement is 1.3 points in BLEU
and 2.9 points in TER.
For the TER-BLEU optimized system combina-
tion, we achieved more improvement in TER com-
pared to the BLEU optimized system combination.
For newstest2008, we get an improvement of 0.8
points in BLEU and 3.0 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. The system combinations performs better
on newstest2009 with 1.3 points in BLEU and 2.7
points in TER. For newstest2010, we get an im-
provement of 1.7 points in BLEU and 3.4 points in
TER and for newstest2011 we get an improvement
of 0.7 points in BLEU and 2.5 points in TER.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally, the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU and
a lower TER score compared to each single sys-
tem. For each optimization criteria the system com-
binations using all single systems outperforms the
system combinations using one less single system.
Although the single system of SYSTRAN has the
worst error scores and the RWTH single systems are
similar, we achieved the best result in using all single
systems. For the WMT 12 evaluation, we submitted
the system combination of all systems optimized on
BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
326
Table 1: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are unoptimized.
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
KIT 22.2 61.8 21.3 61.0 24.1 59.0 22.4 60.2 37.9
RWTH.PBT 21.4 62.0 21.3 61.1 23.9 59.1 21.4 61.2 39.7
Limsi 22.2 63.0 22.0 61.8 23.9 59.9 21.8 62.0 40.2
RWTH.HPBT 21.5 62.6 21.5 61.6 23.6 60.2 21.5 61.8 40.4
SYSTRAN 18.3 64.6 17.9 63.4 21.1 60.5 18.3 63.1 44.8
sc-withAllSystems 23.4 59.7 22.9 59.0 26.2 56.5 23.3 58.8 35.5
sc-without-RWTH.PBT 23.2 59.8 22.8 59.0 25.9 56.6 23.1 58.7 35.6
sc-without-RWTH.HPBT 23.2 59.6 22.7 58.9 26.1 56.2 23.1 58.7 35.6
sc-without-Limsi 22.7 60.1 22.4 59.2 25.5 56.7 22.8 58.8 36.0
sc-without-SYSTRAN 23.0 60.3 22.5 59.5 25.7 57.2 23.1 59.2 36.1
sc-without-KIT 23.0 59.9 22.5 59.1 25.9 56.6 22.9 59.1 36.3
Table 2: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.7 60.3 23.2 59.5 26.0 57.1 23.7 59.2 35.6
sc-without-RWTH.PBT 23.4 61.1 23.1 59.8 25.5 57.6 23.5 59.5 36.1
sc-without-SYSTRAN 23.3 61.1 22.6 60.5 25.3 58.1 23.5 60.0 36.5
sc-without-Limsi 23.1 60.7 22.6 59.7 25.4 57.5 23.3 59.4 36.2
sc-without-KIT 23.4 60.7 23.0 59.7 25.6 57.7 23.3 59.8 36.5
sc-without-RWTH.HPBT 23.3 59.4 22.8 58.6 26.1 56.0 23.1 58.4 35.2
Table 3: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on TER-BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.0 58.8 22.4 58.3 25.8 55.6 23.1 57.7 34.6
sc-without-RWTH.PBT 23.0 59.3 22.5 58.5 25.6 56.0 23.1 58.0 34.9
sc-without-RWTH.HPBT 23.1 59.0 22.6 58.3 25.8 55.6 23.0 58.0 35.0
sc-without-SYSTRAN 22.9 59.7 22.4 59.1 25.6 56.7 23.2 58.5 35.3
sc-without-Limsi 22.7 59.4 22.2 58.7 25.3 56.1 22.7 58.1 35.5
sc-without-KIT 22.9 59.3 22.4 58.5 25.7 55.8 22.7 58.1 35.4
327
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003a. Empirical Methods for
Compound Splitting. In EACL, Budapest, Hungary.
P. Koehn and K. Knight. 2003b. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In NAACL ?12: Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology.
Jose? B. Marin?o, R. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4).
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT English-
French Translation Systems for IWSLT 2011. In Pro-
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
J. Niehues and S. Vogel. 2008. Discriminative Word
Alignment via Alignment Matrix Modeling. In Proc.
of Third ACL Workshop on Statistical Machine Trans-
lation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In Pro-
328
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Evelyne
Tzoukermann and SusanEditors Armstrong, editors,
Proceedings of the ACL SIGDATWorkshop, pages 47?
50. Kluwer Academic Publishers.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
329
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2012
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa Herrmann, Eunah Cho and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT12 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual, fine-grained part-of-speech (POS)
and automatic cluster language models and
discriminative word lexica. In addition, we
explicitly handle out-of-vocabulary (OOV)
words in German, if we have translations for
other morphological forms of the same stem.
Furthermore, we extended the POS-based
reordering approach to also use information
from syntactic trees.
1 Introduction
In this paper, we describe our systems for the
NAACL 2012 Seventh Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. In addition to the POS-based reordering model
used in past years, for German-English we extended
it to also use rules learned using syntax trees.
The translation model was extended by the bilin-
gual language model and a discriminative word lex-
icon using a maximum entropy classifier. For the
French-English and English-French translation sys-
tems, we also used phrase table adaptation to avoid
overestimation of the probabilities of the huge, but
noisy Giga corpus. In the German-English system,
we tried to learn translations for OOV words by ex-
ploring different morphological forms of the OOVs
with the same lemma.
Furthermore, we combined different language
models in the log-linear model. We used word-
based language models trained on different parts of
the training corpus as well as POS-based language
models using fine-grained POS information and lan-
guage models trained on automatic word clusters.
The paper is organized as follows: The next sec-
tion gives a detailed description of our systems in-
cluding all the models. The translation results for
all directions are presented afterwards and we close
with a conclusion.
2 System Description
For the French?English systems the phrase table
is based on a GIZA++ word alignment, while the
systems for German?English use a discriminative
word alignment as described in Niehues and Vogel
(2008). The language models are 4-gram SRI lan-
guage models using Kneser-Ney smoothing trained
by the SRILM Toolkit (Stolcke, 2002).
The problem of word reordering is addressed with
POS-based and tree-based reordering models as de-
scribed in Section 2.3. The POS tags used in the
reordering model are obtained using the TreeTagger
(Schmid, 1994). The syntactic parse trees are gen-
erated using the Stanford Parser (Rafferty and Man-
ning, 2008).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation. Optimization with
349
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 10 translation
options for every source phrase are considered.
2.1 Data
Our translation models were trained on the EPPS
and News Commentary (NC) corpora. Furthermore,
the additional available data for French and English
(i.e. UN and Giga corpora) were exploited in the
corresponding systems.
The systems were tuned with the news-test2011
data, while news-test2011 was used for testing in all
our systems. We trained language models for each
language on the monolingual part of the training cor-
pora as well as the News Shuffle and the Gigaword
(version 4) corpora. The discriminative word align-
ment model was trained on 500 hand-aligned sen-
tences selected from the EPPS corpus.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first word of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus, in
order to obtain a homogenous spelling, we use the
hunspell1 lexicon to map words written according to
old German spelling rules to new German spelling
rules.
In order to reduce the OOV problem of German
compound words, Compound splitting as described
in Koehn and Knight (2003) is applied to the Ger-
man part of the corpus for the German-to-English
system.
The Giga corpus received a special preprocessing
by removing noisy pairs using an SVM classifier as
described in Mediani et al (2011). The SVM clas-
sifier training and test sets consist of randomly se-
lected sentence pairs from the corpora of EPPS, NC,
tuning, and test sets. Giving at the end around 16
million sentence pairs.
2.3 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
1http://hunspell.sourceforge.net/
tion model, we use a different approach that relies on
POS sequences. By abstracting from surface words
to POS, we expect to model the reordering more ac-
curately. For German-to-English, we additionally
apply reordering rules learned from syntactic parse
trees.
2.3.1 POS-based Reordering Model
In order to build the POS-based reordering model,
we first learn probabilistic rules from the POS tags
of the training corpus and the alignment. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009).
2.3.2 Tree-based Reordering Model
Word order is quite different between German and
English. And during translation especially verbs or
verb particles need to be shifted over a long dis-
tance in a sentence. Using discontinuous POS rules
already improves the translation tremendously. In
addition, we apply a tree-based reordering model
for the German-English translation. Syntactic parse
trees provide information about the words in a sen-
tence that form constituents and should therefore be
treated as inseparable units by the reordering model.
For the tree-based reordering model, syntactic parse
trees are generated for the whole training corpus.
Then the word alignment between the source and
target language part of the corpus is used to learn
rules on how to reorder the constituents in a Ger-
man source sentence to make it matches the English
target sentence word order better. In order to apply
the rules to the source text, POS tags and a parse
tree are generated for each sentence. Then the POS-
based and tree-based reordering rules are applied.
The original order of words as well as the reordered
sentence variants generated by the rules are encoded
in a word lattice. The lattice is then used as input to
the decoder.
For the test sentences, the reordering based on
POS and trees allows us to change the word order
in the source sentence so that the sentence can be
translated more easily. In addition, we build reorder-
ing lattices for all training sentences and then extract
350
phrase pairs from the monotone source path as well
as from the reordered paths.
2.4 Translation Models
In addition to the models used in the baseline system
described above, we conducted experiments includ-
ing additional models that enhance translation qual-
ity by introducing alternative or additional informa-
tion into the translation modeling process.
2.4.1 Phrase table adaptation
Since the Giga corpus is huge, but noisy, it is
advantageous to also use the translation probabil-
ities of the phrase pair extracted only from the
more reliable EPPS and News commentary cor-
pus. Therefore, we build two phrase tables for the
French?English system. One trained on all data
and the other only trained on the EPPS and News
commentary corpus. The two models are then com-
bined using a log-linear combination to achieve the
adaptation towards the cleaner corpora as described
in (Niehues et al, 2010). The newly created trans-
lation model uses the four scores from the general
model as well as the two smoothed relative frequen-
cies of both directions from the smaller, but cleaner
model. If a phrase pair does not occur in the in-
domain part, a default score is used instead of a rela-
tive frequency. In our case, we used the lowest prob-
ability.
2.4.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, in which each token
consists of a target word and all source words it is
aligned to. The bilingual tokens enter the translation
process as an additional target factor and the bilin-
gual language model is applied to the additional fac-
tor like a normal language model. For more details
see Niehues et al (2011).
2.4.3 Discriminative Word Lexica
Mauser et al (2009) have shown that the use
of discriminative word lexica (DWL) can improve
the translation quality. For every target word, they
trained a maximum entropy model to determine
whether this target word should be in the translated
sentence or not using one feature per one source
word.
When applying DWL in our experiments, we
would like to have the same conditions for the train-
ing and test case. For this we would need to change
the score of the feature only if a new word is added
to the hypothesis. If a word is added the second time,
we do not want to change the feature value. In order
to keep track of this, additional bookkeeping would
be required. Also the other models in our translation
system will prevent us from using a word too often.
Therefore, we ignore this problem and can calcu-
late the score for every phrase pair before starting
with the translation. This leads to the following def-
inition of the model:
p(e|f) =
J?
j=1
p(ej |f) (1)
In this definition, p(ej |f) is calculated using a max-
imum likelihood classifier.
Each classifier is trained independently on the
parallel training data. All sentences pairs where the
target word e occurs in the target sentence are used
as positive examples. We could now use all other
sentences as negative examples. But in many of
these sentences, we would anyway not generate the
target word, since there is no phrase pair that trans-
lates any of the source words into the target word.
Therefore, we build a target vocabulary for every
training sentence. This vocabulary consists of all
target side words of phrase pairs matching a source
phrase in the source part of the training sentence.
Then we use all sentence pairs where e is in the tar-
get vocabulary but not in the target sentences as neg-
ative examples. This has shown to have a postive
influence on the translation quality (Mediani et al,
2011) and also reduces training time.
2.4.4 Quasi-Morphological Operations for
OOV words
Since German is a highly inflected language, there
will be always some word forms of a given Ger-
351
Figure 1: Quasi-morphological operations
man lemma that did not occur in the training data.
In order to be able to also translate unseen word
forms, we try to learn quasi-morphological opera-
tions that change the lexical entry of a known word
form to the unknown word form. These have shown
to be beneficial in Niehues and Waibel (2011) using
Wikipedia2 titles. The idea is illustrated in Figure 1.
If we look at the data, our system is able to trans-
late a German word Kamin (engl. chimney), but not
the dative plural form Kaminen. To address this
problem, we try to automatically learn rules how
words can be modified. If we look at the example,
we would like the system to learn the following rule.
If an ?en? is appended to a German word, as it is
done when creating the dative plural form of Kami-
nen, we need to add an ?s? to the end of the English
word in order to perform the same morphological
word transformation. We use only rules where the
ending of the word has at most 3 letters.
Depending on the POS, number, gender or case of
the involved words, the same operation on the source
side does not necessarily correspond to the same op-
eration on the target side.
To account for this ambiguity, we rank the differ-
ent target operation using the following four features
and use the best ranked one. Firstly, we should not
generate target words that do not exist. Here, we
have an advantage that we can use monolingual data
to determine whether the word exists. In addition,
a target operation that often coincides with a given
source operation should be better than one that is
rarely used together with the source operation. We
therefore look at pairs of entries in the lexicon and
count in how many of them the source operation can
be applied to the source side and the target operation
can be applied to the target side. We then use only
operations that occur at least ten times. Furthermore,
2http://www.wikipedia.org/
we use the ending of the source and target word to
determine which pair of operations should be used.
Integration We only use the proposed method for
OOVs and do not try to improve translations of
words that the baseline system already covers. We
look for phrase pairs, for which a source operation
ops exists that changes one of the source words f1
into the OOV word f2. Since we need to apply a
target operation to one word on the target side of the
phrase pair, we only consider phrase pairs where f1
is aligned to one of the target words of the phrase
containing e1. If a target operation exists given f1
and ops, we select the one with the highest rank.
Then we generate a new phrase pair by applying
ops to f1 and opt to e1 keeping the original scores
from the phrase pairs, since the original and syn-
thesized phrase pair are not directly competing any-
way. We do not add several phrase pairs generated
by different operations, since we would then need to
add the features used for ranking the operations into
the MERT. This is problematic, since the operations
were only used for very few words and therefore a
good estimation of the weights is not possible.
2.5 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language mod-
els for all of our systems. For English-French and
French-English systems, we use a good quality cor-
pus as in-domain data to train in-domain language
models. Additionally, we apply the POS and clus-
ter language models in different systems. All lan-
guage models are integrated into the translation sys-
tem by a log-linear combination and received opti-
mal weights during tuning by the MERT.
2.5.1 POS Language Models
The POS language model is trained on the POS
sequences of the target language. In this evalua-
tion, the POS language model is applied for the
English-German system. We expect that having ad-
ditional information in form of probabilities of POS
sequences should help especially in case of the rich
morphology of German. The POS tags are gener-
ated with the RFTagger (Schmid and Laws, 2008)
for German, which produces fine-grained tags that
include person, gender and case information. We
352
use a 9-gram language model on the News Shuf-
fle corpus and the German side of all parallel cor-
pora. More details and discussions about the POS
language model can be found in Herrmann et al
(2011).
2.5.2 Cluster Language Models
The cluster language model follows a similar idea
as the POS language model. Since there is a data
sparsity problem when we substitute words with the
word classes, it is possible to make use of larger
context information. In the POS language model,
POS tags are the word classes. Here, we generated
word classes in a different way. First, we cluster
the words in the corpus using the MKCLS algorithm
(Och, 1999) given a number of classes. Second, we
replace the words in the corpus by their cluster IDs.
Finally, we train an n-gram language model on this
corpus consisting of cluster IDs. Generally, all clus-
ter language models used in our systems are 5-gram.
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The experiments for the German-English translation
system are summarized in Table 1. The Baseline
system uses POS-based reordering, discriminative
word alignment and a language model trained on the
News Shuffle corpus. By adding lattice phrase ex-
traction small improvements of the translation qual-
ity could be gained.
Further improvements could be gained by adding
a language model trained on the Gigaword corpus
and adding a bilingual and cluster-based language
model. We used 50 word classes and trained a 5-
gram language model. Afterwards, the translation
quality was improved by also using a discriminative
word lexicon. Finally, the best system was achieved
by using Tree-based reordering and using special
treatment for the OOVs. This system generates a
BLEU score of 22.31 on the test data. For the last
two systems, we did not perform new optimization
runs.
System Dev Test
Baseline 23.64 21.32
+ Lattice Phrase Extraction 23.76 21.36
+ Gigaward Language Model 24.01 21.73
+ Bilingual LM 24.19 21.91
+ Cluster LM 24.16 22.09
+ DWL 24.19 22.19
+ Tree-based Reordering - 22.26
+ OOV - 22.31
Table 1: Translation results for German-English
3.2 English-German
The English-German baseline system uses also
POS-based reordering, discriminative word align-
ment and a language model based on EPPS, NC and
News Shuffle. A small gain could be achieved by the
POS-based language model and the bilingual lan-
guage model. Further gain was achieved by using
also a cluster-based language model. For this lan-
guage model, we use 100 word classes and trained
a 5-gram language model. Finally, the best system
uses the discriminative word lexicon.
System Dev Test
Baseline 17.06 15.57
+ POSLM 17.27 15.63
+ Bilingual LM 17.40 15.78
+ Cluster LM 17.77 16.06
+ DWL 17.75 16.28
Table 2: Translation results for English-German
3.3 English-French
Table 3 summarizes how our English-French sys-
tem evolved. The baseline system here was trained
on the EPPS, NC, and UN corpora, while the lan-
guage model was trained on all the French part of
the parallel corpora (including the Giga corpus). It
also uses short-range reordering trained on EPPS
and NC. This system had a BLEU score of around
26.7. The Giga parallel data turned out to be quite
353
beneficial for this task. It improves the scores by
more than 1 BLEU point. More importantly, addi-
tional language models boosted the system quality:
around 1.8 points. In fact, three language models
were log-linearly combined: In addition to the afore-
mentioned, two additional language models were
trained on the monolingual sets (one for News and
one for Gigaword). We could get an improvement
of around 0.2 by retraining the reordering rules on
EPPS and NC only, but using Giza alignment from
the whole data. Adapting the translation model by
using EPPS and NC as in-domain data improves the
BLEU score by only 0.1. This small improvement
might be due to the fact that the news domain is
very broad and that the Giga corpus has already been
carefully cleaned and filtered. Furthermore, using a
bilingual language model enhances the BLEU score
by almost 0.3. Finally, incorporating a cluster lan-
guage model adds an additional 0.1 to the score.
This leads to a system with 30.58.
System Dev Test
Baseline 24.96 26.67
+ GigParData 26.12 28.16
+ Big LMs 29.22 29.92
+ All Reo 29.14 30.10
+ PT Adaptation 29.15 30.22
+ Bilingual LM 29.17 30.49
+ Cluster LM 29.08 30.58
Table 3: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 4. The
baseline system for this direction was trained on the
EPPS, NC, UN and Giga parallel corpora, while the
language model was trained on the French part of the
parallel training corpora. The baseline system in-
cludes the POS-based reordering model with short-
range rules. The largest improvement of 1.7 BLEU
score was achieved by the integration of the bigger
language models which are trained on the English
version of News Shuffle and the Gigaword corpus
(v4). We did not add the language models from the
monolingual English version of EPPS and NC data,
since the experiments have shown that they did not
provide improvement in our system. The second
largest improvement came from the domain adap-
tation that includes an in-domain language model
and adaptations to the phrase extraction. The BLEU
score has improved about 1 BLEU in total. The in-
domain data we used here are parallel EPPS and NC
corpus. Further gains were obtained by augmenting
the system with a bilingual language model adding
around 0.2 BLEU to the previous score. The sub-
mitted system was obtained by adding the cluster
5-gram language model trained on the News Shuf-
fle corpus with 100 clusters and thus giving 30.25 as
the final score.
System Dev Test
Baseline 25.81 27.15
+ Indomain LM 26.17 27.91
+ PT Adaptation 26.33 28.11
+ Big LMs 28.90 29.82
+ Bilingual LM 29.14 30.09
+ Cluster LM 29.31 30.25
Table 4: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2012 Evaluation for English?German
and English?French. In all systems we could im-
prove by using a class-based language model. Fur-
thermore, the translation quality could be improved
by using a discriminative word lexicon. Therefore,
we trained a maximum entropy classifier for ev-
ery target word. For English?French, adapting the
phrase table helps to avoid using wrong parts of the
noisy Giga corpus. For the German-to-English sys-
tem, we could improve the translation quality addi-
tionally by using a tree-based reordering model and
by special handling of OOV words. For the inverse
direction we could improve the translation quality
by using a 9-gram language model trained on the
fine-grained POS tags.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
354
References
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The karlsruhe institute of
technology translation systems for the wmt 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 379?385, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Proceed-
ings of the eight International Workshop on Spoken
Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using wikipedia to
translate domain-specific terms in smt. In Proceedings
of the eight International Workshop on Spoken Lan-
guage Translation (IWSLT).
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, EACL ?99, pages
71?76, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three german treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
355
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39?47,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Combining Word Reordering Methods on different Linguistic Abstraction
Levels for Statistical Machine Translation
Teresa Herrmann, Jan Niehues, Alex Waibel
Institute for Anthropomatics
Karlsruhe Institute of Technology
Karlsruhe, Germany
{teresa.herrmann,jan.niehues,alexander.waibel}@kit.edu
Abstract
We describe a novel approach to combin-
ing lexicalized, POS-based and syntactic tree-
based word reordering in a phrase-based ma-
chine translation system. Our results show
that each of the presented reordering meth-
ods leads to improved translation quality on its
own. The strengths however can be combined
to achieve further improvements. We present
experiments on German-English and German-
French translation. We report improvements
of 0.7 BLEU points by adding tree-based and
lexicalized reordering. Up to 1.1 BLEU points
can be gained by POS and tree-based reorder-
ing over a baseline with lexicalized reorder-
ing. A human analysis, comparing subjec-
tive translation quality as well as a detailed er-
ror analysis show the impact of our presented
tree-based rules in terms of improved sentence
quality and reduction of errors related to miss-
ing verbs and verb positions.
1 Introduction
One of the main difficulties in statistical machine
translation (SMT) is presented by the different word
orders between languages. Most state-of-the-art
phrase-based SMT systems handle it within phrase
pairs or during decoding by allowing words to be
swapped while translation hypotheses are generated.
An additional reordering model might be included in
the log-linear model of translation. However, these
methods can cover reorderings only over a very lim-
ited distance. Recently, reordering as preprocessing
has drawn much attention. The idea is to detach the
reordering problem from the decoding process and
to apply a reordering model prior to translation in
order to facilitate a monotone translation.
Encouraged by the improvements that can be
achieved with part-of-speech (POS) reordering rules
(Niehues and Kolss, 2009; Rottmann and Vogel,
2007), we apply such rules on a different linguis-
tic level. We abstract from the words in the sentence
and learn reordering rules based on syntactic con-
stituents in the source language sentence. Syntac-
tic parse trees represent the sentence structure and
show the relations between constituents in the sen-
tence. Relying on syntactic constituents instead of
POS tags should help to model the reordering task
more reliably, since sentence constituents are moved
as whole blocks of words, thus keeping the sentence
structure intact.
In addition, we combine the POS-based and syn-
tactic tree-based reordering models and also add a
lexicalized reordering model, which is used in many
state-of-the-art phrase-based SMT systems nowa-
days.
2 Related Work
The problem of word reordering has been addressed
by several approaches over the last years.
In a phrase-based SMT system reordering can
be achieved during decoding by allowing swaps of
words within a defined window. Lexicalized re-
ordering models (Koehn et al, 2005; Tillmann,
2004) include information about the orientation of
adjacent phrases that is learned during phrase extrac-
tion. This reordering method, which affects the scor-
ing of translation hypotheses but does not generate
new reorderings, is used e.g. in the open source ma-
39
chine translation system Moses (Koehn et al, 2007).
Syntax-based (Yamada and Knight, 2001) or
syntax-augmented (Zollmann and Venugopal, 2006)
MT systems address the reordering problem by em-
bedding syntactic analysis in the decoding process.
Hierarchical MT systems (Chiang, 2005) construct
a syntactic hierarchy during decoding, which is in-
dependent of linguistic categories.
To our best knowledge Xia and McCord (2004)
were the first to model the word reordering problem
as a preprocessing step. They automatically learn
reordering rules for English-French translation from
source and target language dependency trees. After-
wards, many followed these footsteps. Earlier ap-
proaches craft reordering rules manually based on
syntactic or dependency parse trees or POS tags de-
signed for particular languages (Collins et al, 2005;
Popovic? and Ney, 2006; Habash, 2007; Wang et al,
2007). Later there were more and more approaches
using data-driven methods. Costa-jussa` and Fonol-
losa (2006) frame the word reordering problem as
a translation task and use word class information
to translate the original source sentence into a re-
ordered source sentence that can be translated more
easily. A very popular approach is to automatically
learn reordering rules based on POS tags or syn-
tactic chunks (Popovic? and Ney, 2006; Rottmann
and Vogel, 2007; Zhang et al, 2007; Crego and
Habash, 2008). Khalilov et al (2009) present re-
ordering rules learned from source and target side
syntax trees. More recently, Genzel (2010) proposed
to automatically learn reordering rules from IBM1
alignments and source side dependency trees. In
DeNero and Uszkoreit (2011) no parser is needed,
but the sentence structure used for learning the re-
ordering model is induced automatically from a par-
allel corpus. Among these approaches most are able
to cover short-range reorderings and some store re-
ordering variants in a word lattice leaving the selec-
tion of the path to the decoder. Long-range reorder-
ings are addressed by manual rules (Collins et al,
2005) or using automatically learned rules (Niehues
and Kolss, 2009).
Motivated by the POS-based reordering models
in Niehues and Kolss (2009) and Rottmann and Vo-
gel (2007), we present a reordering model based on
the syntactic structure of the source sentence. We
intend to cover both short-range and long-range re-
ordering more reliably by abstracting to constituents
extracted from syntactic parse trees instead of work-
ing only with morphosyntactic information on the
word level. Furthermore, we combine POS-based
and tree-based models and additionally include a
lexicalized reordering model. Altogether we apply
word reordering on three different levels: lexical-
ized reordering model on the word level, POS-based
reordering on the morphosyntactic level and syntax
tree-based reordering on the constituent level. In
contrast to previous work we use original syntactic
parse trees instead of binarized parse trees or depen-
dency trees. Furthermore, our goal is to address es-
pecially long-range reorderings involving verb con-
structions.
3 Motivation
When translating from German to English different
word order is the most prominent problem. Espe-
cially the verb needs to be shifted over long dis-
tances in the sentence, since the position of the verb
differs in German and English sentences. The finite
verbs in the English language are generally located
at the second position in the sentence. In German
this is only the case in a main clause. In German
subordinate clauses the verb is at the final position
as shown in Example 1.
Example 1:
Source: ..., nachdem ich eine Weile im Inter-
net gesucht habe.
Gloss: ... after I a while in-the internet
searched have.
POS Reord.: ..., nachdem ich habe eine Weile im
Internet gesucht.
POS Transl.: ... as I have for some time on the
Internet.
The example shows first the source sentence and
an English gloss. POS Reord presents the reordered
source sentence as produced by POS rules. This
should be the source sentence according to target
language word order. POS Transl shows the trans-
lation of the reordered sequence. We can see that
some cases remain unresolved. The POS rules suc-
ceed in putting the auxiliary habe/have to the right
position in the sentence. But the participle, carry-
ing the main meaning of the sentence, is not shifted
together with the auxiliary. During translation it is
40
dropped from the sentence, rendering it unintelligi-
ble.
A reason why the POS rules do not shift both
parts of the verb might be that the rules operate on
the word level only and treat every POS tag inde-
pendently of the others. A reordering model based
on syntactic constituents can help with this. Addi-
tional information about the syntactic structure of
the sentence allows to identify which words belong
together and should not be separated, but shifted as
a whole block. Abstracting from the word level to
the constituent level also provides the advantage that
even though reorderings are performed over long
sentence spans, the rules consist of less reordering
units (constituents which themselves consist of con-
stituents or words) and can be learned more reliably.
4 Tree-based Reordering
In order to encourage linguistically meaningful re-
orderings we learn rules based on syntactic tree con-
stituents. While the POS-based rules are flat and
perform the reordering on a sequence of words, the
tree-based rules operate on subtrees in the parse tree
as shown in Figure 1.
VP
VVPPNPPTNEG
?
VP
NPVVPPPTNEG
Figure 1: Example reordering rule based on subtrees
A syntactic parse tree contains both the word-
level categories, i.e. parts-of-speech and higher or-
der categories, i.e. constituents. In this way it pro-
vides information about the building blocks of a sen-
tence that belong together and should not be taken
apart by reordering. Consequently, the tree-based
reordering operates both on the word level and on
the constituent level to make use of all available in-
formation in the parse tree. It is able to handle long-
range reorderings as well as short-range reorder-
ings, depending on how many words the reordered
constituents cover. The tree-based reordering rules
should also be more stable and introduce less ran-
dom word shuffling than the POS-based rules.
The reordering model consists of two stages. First
the rule extraction, where the rules are learned by
searching the training corpus for crossing align-
ments which indicate a reordering between source
and target language. The second is the application
of the learned reordering rules to the input text prior
to translation.
4.1 Rule Extraction
As shown in Figure 4 we learn rules like this:
VP PTNEG NP VVPP? VP PTNEG VVPP NP
where the first item in the rule is the head node of
the subtree and the rest represent the children. In
the second part of the rule the children are indexed
so that children of the same category cannot be con-
fused. Figure 2 shows an example for rule extrac-
tion: a sentence in its syntactic parse tree representa-
tion, the sentence in the target language and an auto-
matically generated alignment. A reordering occurs
between the constituents VVPP and NP.
S
1-n
CS
...
VP
2-5
VVPP
3-3
gewa?hlt
NP
4-5
NN
5-5
Szenarien
ADJA
4-4
ku?nstliche
PTNEG
2-2
nicht
VAFIN
2-2
haben
PPER
1-1
Wir
1
We
2
didn?t
3
choose
4
artificial
5
scenarios
Figure 2: Example training sentence used to extract re-
ordering rules
In a first step the reordering rule has to be found.
We extract the rules from a word aligned corpus
where a syntactic parse tree is provided for each
source side sentence. We traverse the tree top down
and scan each subtree for reorderings, i.e. cross-
ings of alignment links between source and target
sentence. If there is a reordering, we extract a
rule that rearranges the source side constituents ac-
cording to the order of the corresponding words on
41
the target side. Each constituent in a subtree com-
prises one or more words. We determine the lowest
(min) and highest (max) alignment point for each
constituent ck and thus determine the range of the
constituent on the target side. This can be formal-
ized as min(ck) = min{j|fi ? ck; ai = j} and
max(ck) = max{j|fi ? ck; ai = j}. To illustrate
the process, we have annotated the parse tree in Fig-
ure 2 with the alignment points (min-max) for each
constituent.
After defining the range, we check for the follow-
ing conditions in order to determine whether to ex-
tract a reordering rule.
1. all constituents have a non-empty range
2. source and target word order differ
First, for each subtree at least one word in each con-
stituent needs to be aligned. Otherwise it is not pos-
sible to determine a conclusive order. Second, we
check whether there is actually a reordering, i.e. the
target language words are not in the same order as
the constituents in the source language: min(ck) >
min(ck+1) and max(ck) > max(ck+1).
Once we find a reordering rule to extract, we cal-
culate the probability of this rule as the relative fre-
quency with which such a reordering occurred in all
subtrees of the training corpus divided by the num-
ber of total occurrences of this subtree in the corpus.
We only store rules for reorderings that occur more
than 5 times in the corpus.
4.1.1 Partial Rules
The syntactic parse trees of German sentences are
quite flat, i.e. a subtree usually has many children.
When a rule is extracted, it always consists of the
head of the subtree and all its children. The ap-
plication requires that the applicable rule matches
the complete subtree: the head and all its children.
However, most of the time only some of the chil-
dren are actually involved in a reordering. There
are also many different subtree variants that are quite
similar. In verb phrases or noun phrases, for exam-
ple, modifiers such as prepositional phrases or ad-
verbial phrases can be added nearly arbitrarily. In
order to generalize the tree-based reordering rules,
we extend the rule extraction. We do not only extract
the rules from the complete child sequence, but also
from any continuous child sequence in a constituent.
This way, we extract generalized rules which can
be applied more often. Formally, for each subtree
h ? cn1 = c1c2...cn that matches the constraints
presented in Section 4.1, we modify the basic rule
extraction to: ?i, j1 ? i < j ? n : h ? cji . It
could be argued that the partial rules might be not
as reliable as the specific rules. In Section 6 we will
show that such generalizations are meaningful and
can have a positive effect on the translation quality.
4.2 Rule Application
During the training of the system all reordering rules
are extracted from the parallel corpus. Prior to trans-
lation the rules are applied to the original source text.
Each rule is applied independently producing a re-
ordering variant of that sentence. The original sen-
tence and all reordering variants are stored in a word
lattice which is later used as input to the decoder.
The rules may be applied recursively to already re-
ordered paths. If more than one rule can be applied,
all paths are added to the lattice unless the rules gen-
erate the same output. In this case only the rule with
the highest probability is applied.
The edges in a word lattice for one sentence are
assigned transition probabilities as follows. In the
monotone path with original word order all transi-
tion probabilities are initially set to 1. In a reordered
path the first branching transition is assigned the
probability of the rule that generated the path. All
other transition probabilities in this path are set to 1.
Whenever a reordered path branches from the mono-
tone path, the probability of the branching edge is
substracted from the probability of the monotone
edge. However, a minimum probability of 0.05 is
reserved for the monotone edge. The score of the
complete path is computed as the product of the tran-
sition probabilities. During decoding the best path
is searched for by including the score for the cur-
rent path weighted by the weight for the reordering
model in the log-linear model. In order to enable
efficient decoding we limit the lattice size by only
applying rules with a probability higher than a pre-
defined threshold.
4.2.1 Recursive Rule Application
As mentioned above, the tree-based rules may be
applied recursively. That means, after one rule is
applied to the source sentence, a reordered path may
42
SS
aus anderen Bundesla?ndern
PP
VAFIN
habe
VP
VVPP
bekommen
viele Anfragen
NP
ADV
schon
PPER
ich
KOUS
dass
Ich kann Ihnen nur sagen,
...
I may just tell you that I got already lots of requests from other federal states
Figure 3: Example parse tree with separated verb particles
be reordered again. The reason is the structure of
the syntactic parse trees. Verbs and their particles
are typically not located within the same subtree.
Hence, they cannot be covered by one reordering
rule. A separate rule is extracted for each subtree.
Figure 3 demonstrates this in an example. The two
parts that belong to the verb in this German sentence,
namely bekommen and habe, are not located within
the same constituent. The finite verb habe forms a
constituent of its own and the participle bekommen
forms part of the VP constituent. In English the fi-
nite verb and the participle need to be placed next to
each other. In order to rearrange the source language
words according to the target language word order,
the following two reordering movements need to be
performed: the finite verb habe needs to be placed
before the VP constituent and the participle bekom-
men needs to be moved within the VP constituent to
the first position. Only if both movements are per-
formed, the right word order can be generated.
However, the reordering model only considers
one subtree at a time when extracting reordering
rules. In this case two rules are learned, but if they
are applied to the source sentence separately, they
will end up in separate paths in the word lattice. The
decoder then has to choose which path to translate:
the one where the finite verb is placed before the VP
constituent or the path where the participle is at the
first position in the VP constituent.
To counter this drawback the rules may be applied
recursively to the new paths created by our reorder-
ing rules. We use the same rules, but newly created
paths are fed back into the queue of sentences to be
reordered. However, we only apply the rules to parts
of the reordered sentence that are still in the original
word order and restrict the recursion depth.
5 Combining reordering methods
In order to get a deeper insight into their individ-
ual strengths we compare the reordering methods on
different linguistic levels and also combine them to
investigate whether gains can be increased. We ad-
dress the word level using the lexicalized reordering,
the morphosyntactic level by POS-based reordering
and the constituent level by tree-based reordering.
5.1 POS-based and tree-based rules
The training of the POS-based reordering is per-
formed as described in (Rottmann and Vogel,
2007) for short-range reordering rules, such as
VVIMP VMFIN PPER ? PPER VMFIN VVIMP.
Long-range reordering rules trained according
to (Niehues and Kolss, 2009) include gaps match-
ing longer spans of arbitrary POS sequences
(VAFIN * VVPP ? VAFIN VVPP *). The POS-
based reordering used in our experiments always in-
cludes both short and long-range rules.
The tree-based rules are trained separately as de-
scribed above. First the POS-based rules are applied
to the monotone path of the source sentence and then
43
the tree-based rules are applied independently, pro-
ducing separate paths.
5.2 Rule-based and lexicalized reordering
As described in Section 4.2 we create word lattices
that encode the reordering variants. The lexical-
ized reordering model stores for each phrase pair
the probabilities for possible reordering orientations
at the incoming and outgoing phrase boundaries:
monotone, swap and discontinuous. In order to ap-
ply the lexicalized reordering model on lattices the
original position of each word is stored in the lat-
tice. While the translation hypothesis is generated,
the reordering orientation with respect to the origi-
nal position of the words is checked at each phrase
boundary. The probability for the respective orien-
tation is included as an additional score.
6 Results
The tree-based models are applied for German-
English and German-French translation. Results are
measured in case-sensitive BLEU (Papineni et al,
2002).
6.1 General System Description
First we describe the general system architecture
which underlies all the systems used later on. We
use a phrase-based decoder (Vogel, 2003) that takes
word lattices as input. Optimization is performed
using MERT with respect to BLEU. All POS-based
or tree-based systems apply monotone translation
only. Baseline systems without reordering rules use
a distance-based reordering model. In addition, a
lexicalized reordering model as described in (Koehn
et al, 2005) is applied where indicated. POS tags
and parse trees are generated using the Tree Tag-
ger (Schmid, 1994) and the Stanford Parser (Raf-
ferty and Manning, 2008).
6.1.1 Data
The German-English system is trained on the pro-
vided data of the WMT 2012. news-test2010 and
news-test2011 are used for development and test-
ing. The type of data used for training, development
and testing the German-French system is similar to
WMT data, except that 2 references are available.
The training corpus for the reordering models con-
sist of the word-aligned Europarl and News Com-
mentary corpora where POS tags and parse trees are
generated for the source side.
6.2 German-English
We built systems using POS-based and tree-based
reordering and show the impact of the individual
models as well as their combination on the transla-
tion quality. The results are presented in Table 1.
For each system, two different setups were evalu-
ated. First, with a distance-based reordering model
only (noLexRM) and with an additional lexicalized
reordering model (LexRM). The baseline system
which uses no reordering rules at all allows a re-
ordering window of 5 in the decoder for both setups.
For all systems where reordering rules are applied,
monotone translation is performed. Since the rules
take over the main reordering job, only monotone
translation is necessary from the reordered word lat-
tice input. In this experiment, we compare the tree-
based rules with and without recursion, and the par-
tial rules.
Rule Type
System noLexRM LexRM
Dev Test Dev Test
Baseline (no Rules) 22.82 21.06 23.54 21.61
POS 24.33 21.98 24.42 22.15
Tree 24.01 21.92 24.24 22.01
Tree rec. 24.37 21.97 24.53 22.19
Tree rec.+ par. 24.31 22.21 24.65 22.27
POS + Tree 24.57 22.21 24.91 22.47
POS + Tree rec. 24.61 22.39 24.81 22.45
POS + Tree rec.+ par. 24.80 22.45 24.78 22.70
Table 1: German-English
Compared to the baseline system using distance-
based reordering only, 1.4 BLEU points can be
gained by applying combined POS and tree-based
reordering. The tree rules including partial rules and
recursive application alone achieve already a bet-
ter performance than the POS rules, but using them
all in combination leads to an improvement of 0.4
BLEU points over the POS-based reordering alone.
When lexicalized reordering is added, the relative
improvements are similar: 1.1 BLEU points com-
pared to the Baseline and 0.55 BLEU points over the
POS-based reordering. We can therefore argue that
the individual rule types as well as the lexicalized re-
ordering model seem to address complementary re-
ordering issues and can be combined successfully to
44
obtain an even better translation quality.
We applied only tree rules with a probability of
0.1 and higher. Partial rules require a threshold of
0.4 to be applied, since they are less reliable. In or-
der to prevent the lattices from growing too large,
the recursive rule application is restricted to a max-
imum recursion depth of 3. These values were set
according to the results of preliminary experiments
investigating the impact of the rule probabilities on
the translation quality. Normal rules and partial rules
are not mixed during recursive application.
With the best system we performed a final exper-
iment on the official testset of the WMT 2012 and
achieved a score of 23.73 which is 0.4 BLEU points
better than the best constrained submission.
6.3 Translation Examples
Example 2 shows how the translation of the sen-
tence presented above is improved by adding the
tree-based rules. We can see that using tree con-
stituents in the reordering model indeed addresses
the problem of verb particles and especially missing
verb parts in German.
Example 2:
Src: ..., nachdem ich eine Weile im Internet
gesucht habe.
Gloss: ..., after I a while in-the Internet search-
ed have.
POS: ... as I have for some time on the Inter-
net.
+Tree: ... after I have looked for a while on the
Internet.
Example 3 shows another aspect of how the tree-
based rules work. With the help of the tree-based re-
ordering rules, it is possible to relocate the separated
prefix of German verbs and find the correct transla-
tion. The verb vorschlagen consist of the main verb
(MV) schlagen (here conjugated as schla?gt) and the
prefix (PX) vor. Depending on the verb form and
sentence type, the prefix must be separated from the
main verb and is located in a different part of the
sentence. The two parts of the verb can also have
individual meanings. Although the translation of
the verb stem were correct if it were the full verb,
not recognizing the separated prefix and ignoring it
in translation, corrupts the meaning of the sentence.
With the help of the tree-based rules, the dependency
between the main verb and its prefix is resolved and
the correct translation can be chosen.
6.4 German-French
The same experiments were tested on German-
French translation. For this language pair, similar
improvements could be achieved by combining POS
and tree-based reordering rules and applying a lexi-
calized reordering model in addition. Table 2 shows
the results. Up to 0.7 BLEU points could be gained
by adding tree rules and another 0.1 by lexicalized
reordering.
Rule Type
System noLexRM LexRM
Dev Test Dev Test
POS 41.29 38.07 42.04 38.55
POS + Tree 41.94 38.47 42.44 38.57
POS + Tree rec. 42.35 38.66 42.80 38.71
POS + Tree rec.+ par. 42.48 38.79 42.87 38.88
Table 2: German-French
6.5 Binarized Syntactic Trees
Even though related work using syntactic parse trees
in SMT for reordering purposes (Jiang et al, 2010)
have reported an advantage of binarized parse trees
over standard parse trees, our model did not bene-
fit from binarized parse trees. It seems that the flat
hierarchical structure of standard parse trees enables
our reordering model to learn the order of the con-
stituents most efficiently.
7 Human Evaluation
7.1 Sentence-based comparison
In order to have an additional perspective of the
impact of our tree-based reordering, we also pro-
vide a human evaluation of the translation output
of the German-English system without the lexical-
ized reordering model. 250 translation hypotheses
were selected to be annotated. For each input sen-
tence two translations generated by different sys-
tems were presented, one applying POS-based re-
ordering only and the other one applying both POS-
based and tree-based reordering rules. The hypothe-
ses were anonymized and presented in random order.
Table 3 shows the BLEU scores of the analyzed
systems and the manual judgement of comparative,
subjective translation quality. In 50.8% of the sen-
45
Example 3:
Src: Die RPG Byty schla?gt ihnen in den Schreiben eine Mieterho?hung von ca. 15 bis 38 Prozent vor.
Gloss: The RPG Byty proposes-MV them in the letters a rent increase of ca. 15 to 38 percent proposes-PX
POS: The RPG Byty beats them in the letter, a rental increase of around 15 to 38 percent.
+Tree: The RPG Byty proposes them in the letters a rental increase of around 15 to 38 percent.
System BLEU wins %
POS Rules 21.98 58 23.2
POS + Tree Rules rec. par. 22.45 127 50.8
Table 3: Human Evaluation of Translation quality
tences, the translation generated by the system us-
ing tree-based rules was judged to be better, whereas
in 23.2% of the cases the system without tree-based
rules was rated better. For 26% of the sentences the
translation quality was very similar. Consequently,
in 76.8% of the cases the tree-based system pro-
duced a translation that is either better or of the same
quality as the system using POS rules only.
7.2 Analysis of verbs
Since the verbs in German-to-English translation
were one of the issues that the tree-based reorder-
ing model should address, a more detailed analysis
was performed on the first 165 sentences. We espe-
cially investigated the changes regarding the verbs
between the translations stemming from the system
using the POS-based reordering only and the one us-
ing both the POS and the tree-based model. We ex-
amined three aspects of the verbs in the two trans-
lations. Each change introduced by the tree-based
reordering model was first classified either as an im-
provement or a degradation of the translation qual-
ity. Secondly, it was assigned to one of the following
categories: exist, position or form. In case of an im-
provement, exist means a verb existed in the trans-
lation due to the tree-based model, which did not
exist before. A degradation in this category means
that a verb was removed from the translation when
including the tree-based reordering model. An im-
provement or degradation in the category position
or form means that the verb position or verb form
was improved or degraded, respectively.
Table 4 shows the results of this analysis. In total,
48 of the verb changes were identified as improve-
ments, while only 16 were regarded as degradations
of translation quality. Improvements mainly concern
Type all exist position form
Improvements 48 22 21 5
Degradations 16 2 11 3
Table 4: Manual Analysis of verbs
improved verb position in the sentence and verbs
that could be translated with the help of the tree-
based rules that were not there before. Even though
also degradations were introduced by the tree-based
reordering model, the improvements outweigh them.
8 Conclusion
We have presented a reordering method based on
syntactic tree constituents to model long-range re-
orderings in SMT more reliably. Furthermore, we
combined the reordering methods addressing dif-
ferent linguistic abstraction levels. Experiments
on German-English and German-French translation
showed that the best translation quality could be
achieved by combining POS-based and tree-based
rules. Adding a lexicalized reordering model in-
creased the translation quality even further. In total
we could reach up to 0.7 BLEU points of improve-
ment by adding tree-based and lexicalized reorder-
ing compared to only POS-based rules. Up to 1.1
BLEU points were gained over to a baseline system
using a lexicalized reordering model.
A human evaluation showed a preference of the
POS+Tree-based reordering method in most cases.
A detailed analysis of the verbs in the transla-
tion outputs revealed that the tree-based reordering
model indeed addresses verb constructions and im-
proves the translation of German verbs.
Acknowledgments
This work was partly achieved as part of the Quaero
Programme, funded by OSEO, French State agency
for innovation. The research leading to these results
has received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n? 287658.
46
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL 2005, Ann Arbor,
Michigan.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Conference on
Empirical Methods on Natural Language Processing
(EMNLP 2006), Sydney, Australia.
Josep M. Crego and Nizar Habash. 2008. Using Shallow
Syntax Information to Improve Word Alignment and
Reordering for SMT. In ACL-HLT 2008, Columbus,
Ohio, USA.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of EMNLP 2011, pages 193?203, Edin-
burgh, Scotland, UK.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of COLING 2010, Beijing, China.
Nizar Habash. 2007. Syntactic preprocessing for statis-
tical machine translation. Proceedings of the 11th MT
Summit.
Jie Jiang, Jinhua Du, and Andy Way. 2010. Improved
phrase-based smt with syntactic reordering patterns
learned from lattice scoring. In Proceedings of AMTA
2010, Denver, CO, USA.
M. Khalilov, J.A.R. Fonollosa, and M. Dras. 2009.
A new subtree-transfer approach to syntax-based re-
ordering for statistical machine translation. In Proc.
of the 13th Annual Conference of the European As-
sociation for Machine Translation (EAMT?09), pages
198?204, Barcelona, Spain.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description for
the 2005 iwslt speech translation evaluation. In Inter-
national Workshop on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for
statistical machine translation. In Annual meeting-
association for computational linguistics, volume 45,
page 2.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Translation.
In International Conference on Language Resources
and Evaluation (LREC 2006), Genoa, Italy.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German, Columbus, Ohio.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101?104.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of COLING 2004, Geneva,
Switzerland.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings of
the 39th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?01, pages 523?530, Strouds-
burg, PA, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-Level Reordering of Source Language Sen-
tences with Automatically Learned Rules for Statis-
tical Machine Translation. In HLT-NAACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, Rochester, NY, USA.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, StatMT ?06, pages 138?141, Stroudsburg,
PA, USA.
47
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104?108,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2013
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan Niehues,
Teresa Herrmann, Isabel Slawik and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based
SMT systems developed for our partici-
pation in the WMT13 Shared Translation
Task. Translations for English?German
and English?French were generated us-
ing a phrase-based translation system
which is extended by additional models
such as bilingual, fine-grained part-of-
speech (POS) and automatic cluster lan-
guage models and discriminative word
lexica (DWL). In addition, we combined
reordering models on different sentence
abstraction levels.
1 Introduction
In this paper, we describe our systems for the
ACL 2013 Eighth Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French using a
phrase-based decoder with lattice input.
The paper is organized as follows: the next sec-
tion gives a detailed description of our systems
including all the models. The translation results
for all directions are presented afterwards and we
close with a conclusion.
2 System Description
The phrase table is based on a GIZA++ word
alignment for the French?English systems. For
the German?English systems we use a Discrim-
inative Word Alignment (DWA) as described in
Niehues and Vogel (2008). For every source
phrase only the top 10 translation options are con-
sidered during decoding. The SRILM Toolkit
(Stolcke, 2002) is used for training SRI language
models using Kneser-Ney smoothing.
For the word reordering between languages, we
used POS-based reordering models as described in
Section 4. In addition to it, tree-based reordering
model and lexicalized reordering were added for
German?English systems.
An in-house phrase-based decoder (Vogel,
2003) is used to perform translation. The trans-
lation was optimized using Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) towards better BLEU (Papineni et al,
2002) scores.
2.1 Data
The Europarl corpus (EPPS) and News Commen-
tary (NC) corpus were used for training our trans-
lation models. We trained language models for
each language on the monolingual part of the
training corpora as well as the News Shuffle and
the Gigaword corpora. The additional data such as
web-crawled corpus, UN and Giga corpora were
used after filtering. The filtering work for this data
is discussed in Section 3.
For the German?English systems we use the
news-test2010 set for tuning, while the news-
test2011 set is used for the French?English sys-
tems. For testing, news-test2012 set was used for
all systems.
2.2 Preprocessing
The training data is preprocessed prior to train-
ing the system. This includes normalizing special
symbols, smart-casing the first word of each sen-
tence and removing long sentences and sentence
pairs with length mismatch.
Compound splitting is applied to the German
part of the corpus of the German?English system
as described in Koehn and Knight (2003).
3 Filtering of Noisy Pairs
The filtering was applied on the corpora which
are found to be noisy. Namely, the Giga English-
French parallel corpus and the all the new web-
crawled data . The operation was performed using
104
an SVM classifier as in our past systems (Medi-
ani et al, 2011). For each pair, the required lexica
were extracted from Giza alignment of the corre-
sponding EPPS and NC corpora. Furthermore, for
the web-crawled data, higher precision classifiers
were trained by providing a larger number of neg-
ative examples to the classifier.
After filtering, we could still find English sen-
tences in the other part of the corpus. Therefore,
we performed a language identification (LID)-
based filtering afterwards (performed only on the
French-English corpora, in this participation).
4 Word Reordering
Word reordering was modeled based on POS se-
quences. For the German?English system, re-
ordering rules learned from syntactic parse trees
were used in addition.
4.1 POS-based Reordering Model
In order to train the POS-based reordering model,
probabilistic rules were learned based on the POS
tags from the TreeTagger (Schmid and Laws,
2008) of the training corpus and the alignment. As
described in Rottmann and Vogel (2007), continu-
ous reordering rules are extracted. This modeling
of short-range reorderings was extended so that it
can cover also long-range reorderings with non-
continuous rules (Niehues and Kolss, 2009), for
German?English systems.
4.2 Tree-based Reordering Model
In addition to the POS-based reordering, we
apply a tree-based reordering model for the
German?English translation to better address the
differences in word order between German and
English. We use the Stanford Parser (Rafferty and
Manning, 2008) to generate syntactic parse trees
for the source side of the training corpus. Then
we use the word alignment between source and
target language to learn rules on how to reorder
the constituents in a German source sentence to
make it match the English target sentence word or-
der better (Herrmann et al, 2013). The POS-based
and tree-based reordering rules are applied to each
input sentence. The resulting reordered sentence
variants as well as the original sentence order are
encoded in a word lattice. The lattice is then used
as input to the decoder.
4.3 Lexicalized Reordering
The lexicalized reordering model stores the re-
ordering probabilities for each phrase pair. Pos-
sible reordering orientations at the incoming and
outgoing phrase boundaries are monotone, swap
or discontinuous. With the POS- and tree-based
reordering word lattices encode different reorder-
ing variants. In order to apply the lexicalized re-
ordering model, we store the original position of
each word in the lattice. At each phrase boundary
at the end, the reordering orientation with respect
to the original position of the words is checked.
The probability for the respective orientation is in-
cluded as an additional score.
5 Translation Models
In addition to the models used in the baseline sys-
tem described above, we conducted experiments
including additional models that enhance trans-
lation quality by introducing alternative or addi-
tional information into the translation modeling
process.
5.1 Bilingual Language Model
During the decoding the source sentence is seg-
mented so that the best combination of phrases
which maximizes the scores is available. How-
ever, this causes some loss of context information
at the phrase boundaries. In order to make bilin-
gual context available, we use a bilingual language
model (Niehues et al, 2011). In the bilingual lan-
guage model, each token consists of a target word
and all source words it is aligned to.
5.2 Discriminative Word Lexicon
Mauser et al (2009) introduced the Discriminative
Word Lexicon (DWL) into phrase-based machine
translation. In this approach, a maximum entropy
model is used to determine the probability of using
a target word in the translation.
In this evaluation, we used two extensions to
this work as shown in (Niehues and Waibel, 2013).
First, we added additional features to model the
order of the source words better. Instead of rep-
resenting the source sentence as a bag-of-words,
we used a bag-of-n-grams. We used n-grams up to
the order of three and applied count filtering to the
features for higher order n-grams.
Furthermore, we created the training examples
differently in order to focus on addressing errors
of the other models of the phrase-based translation
105
system. We first translated the whole corpus with a
baseline system. Then we only used the words that
occur in the N-Best List and not in the reference as
negative examples instead of using all words that
do not occur in the reference.
5.3 Quasi-Morphological Operations
Because of the inflected characteristic of the
German language, we try to learn quasi-
morphological operations that change the lexi-
cal entry of a known word form to the out-of-
vocabulary (OOV) word form as described in
Niehues and Waibel (2012).
5.4 Phrase Table Adaptation
For the French?English systems, we built two
phrase tables; one trained with all data and the
other trained only with the EPPS and NC cor-
pora. This is due to the fact that Giga corpus is big
but noisy and EPPS and NC corpus are more reli-
able. The two models are combined log-linearly to
achieve the adaptation towards the cleaner corpora
as described in Niehues et al (2010).
6 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language
models for all of our systems. For the
English?French systems, we use a good quality
corpus as in-domain data to train in-domain lan-
guage models. Additionally, we apply the POS
and cluster language models in different systems.
For the German?English system, we build sepa-
rate language models using each corpus and com-
bine them linearly before the decoding by mini-
mizing the perplexity. Language models are inte-
grated into the translation system by a log-linear
combination and receive optimal weights during
tuning by the MERT.
6.1 POS Language Models
For the English?German system, we use the POS
language model, which is trained on the POS se-
quence of the target language. The POS tags are
generated using the RFTagger (Schmid and Laws,
2008) for German. The RFTagger generates fine-
grained tags which include person, gender, and
case information. The language model is trained
with up to 9-gram information, using the German
side of the parallel EPPS and NC corpus, as well
as the News Shuffle corpus.
6.2 Cluster Language Models
In order to use larger context information, we use
a cluster language model for all our systems. The
cluster language model is based on the idea shown
in Och (1999). Using the MKCLS algorithm, we
cluster the words in the corpus, given a number
of classes. Then words in the corpus are replaced
with their cluster IDs. Using these cluster IDs,
we train n-gram language models as well as a
phrase table with this additional factor of cluster
ID. Our submitted systems have diversed range of
the number of clusters as well as n-gram.
7 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to
the workshop. The results are reported as case-
sensitive BLEU scores on one reference transla-
tion.
7.1 German?English
The experiments for the German to English trans-
lation system are summarized in Table 1. The
baseline system uses POS-based reordering, DWA
with lattice phrase extraction and language models
trained on the News Shuffle corpus and Giga cor-
pus separately. Then we added a 5-gram cluster
LM trained with 1,000 word classes. By adding a
language model using the filtered crawled data we
gained 0.3 BLEU on the test set. For this we com-
bined all language models linearly. The filtered
crawled data was also used to generate a phrase
table, which brought another improvement of 0.85
BLEU. Applying tree-based reordering improved
the BLEU score, and the performance had more
gain by adding the extended DWL, namely us-
ing both bag-of-ngrams and n-best lists. While
lexicalized reordering gave us a slight gain, we
added morphological operation and gained more
improvements.
7.2 English?German
The English to German baseline system uses POS-
based reordering and language models using par-
allel data (EPPS and NC) as shown in Table 2.
Gradual gains were achieved by changing align-
ment from GIZA++ to DWA, adding a bilingual
language model as well as a language model based
on the POS tokens. A 9-gram cluster-based lan-
guage model with 100 word classes gave us a
106
System Dev Test
Baseline 24.15 22.79
+ Cluster LM 24.18 22.84
+ Crawled Data LM (Comb.) 24.53 23.14
+ Crawled Data PT 25.38 23.99
+ Tree Rules 25.80 24.16
+ Extended DWL 25.59 24.54
+ Lexicalized Reordering 26.04 24.55
+ Morphological Operation - 24.62
Table 1: Translation results for German?English
small gain. Improving the reordering using lexi-
alized reordering gave us gain on the optimization
set. Using DWL let us have more improvements
on our test set. By using the filtered crawled data,
we gained a big improvement of 0.46 BLEU on
the test set. Then we extended the DWL with bag
of n-grams and n-best lists to achieve additional
improvements. Finally, the best system includes
lattices generated using tree rules.
System Dev Test
Baseline 17.00 16.24
+ DWA 17.27 16.53
+ Bilingual LM 17.27 16.59
+ POS LM 17.46 16.66
+ Cluster LM 17.49 16.68
+ Lexicalized Reordering 17.57 16.68
+ DWL 17.58 16.77
+ Crawled Data 18.43 17.23
+ Extended DWL 18.66 17.57
+ Tree Rules 18.63 17.70
Table 2: Translation results for English?German
7.3 French?English
Table 3 reports some remarkable improvements
as we combined several techniques on the
French?English direction. The baseline system
was trained on parallel corpora such as EPPS, NC
and Giga, while the language model was trained
on the English part of those corpora plus News
Shuffle. The newly presented web-crawled data
helps to achieve almost 0.6 BLEU points more
on test set. Adding bilingual language model and
cluster language model does not show a significant
impact. Further gains were achieved by the adap-
tation of in-domain data into general-theme phrase
table, bringing 0.15 BLEU better on the test set.
When we added the DWL feature, it notably im-
proves the system by 0.25 BLEU points, resulting
in our best system.
System Dev Test
Baseline 30.33 29.35
+ Crawled Data 30.59 29.93
+ Bilingual and Cluster LMs 30.67 30.01
+ In-Domain PT Adaptation 31.17 30.16
+ DWL 31.07 30.40
Table 3: Translation results for French?English
7.4 English?French
In the baseline system, EPPS, NC, Giga and News
Shuffle corpora are used for language modeling.
The big phrase tables tailored EPPC, NC and Giga
data. The system also uses short-range reordering
trained on EPPS and NC. Adding parallel and fil-
tered crawl data improves the system. It was fur-
ther enhanced by the integration of a 4-gram bilin-
gual language model. Moreover, the best config-
uration of 9-gram language model trained on 500
clusters of French texts gains 0.25 BLEU points
improvement. We also conducted phrase-table
adaptation from the general one into the domain
covered by EPPS and NC data and it helps as well.
The initial try-out with lexicalized reordering fea-
ture showed an improvement of 0.23 points on the
development set, but a surprising reduction on the
test set, thus we decided to take the system after
adaptation as our best English?French system.
System Dev Test
Baseline 30.50 27.77
+ Crawled Data 31.05 27.87
+ Bilingual LM 31.23 28.50
+ Cluster LM 31.58 28.75
+ In-Domain PT Adaptation 31.88 29.12
+ Lexicalized Reordering 32.11 28.98
Table 4: Translation results for English?French
8 Conclusions
We have presented the systems for our par-
ticipation in the WMT 2013 Evaluation for
English?German and English?French. All sys-
tems use a class-based language model as well
as a bilingual language model. Using a DWL
with source context improved the translation qual-
ity of English?German systems. Also for these
systems, we could improve even more with a
tree-based reordering model. Special handling
107
of OOV words improved German?English sys-
tem, while for the inverse direction the language
model with fine-grained POS tags was helpful. For
English?French, phrase table adaptation helps to
avoid using wrong parts of the noisy Giga corpus.
9 Acknowledgements
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The research lead-
ing to these results has received funding from
the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
n? 287658.
References
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Pro-
ceedings of the eight International Workshop on
Spoken Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2012. Detailed Analysis
of different Strategies for Phrase Table Adaptation
in SMT. In Proceedings of the American Machine
Translation Association (AMTA), San Diego, Cali-
fornia, October.
Jan Niehues and Alex Waibel. 2013. An MT Error-
driven Discriminative Word Lexicon using Sentence
Structure Features. In Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013), Sofia, Bul-
garia.
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the ninth conference on European chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German, Columbus, Ohio.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, Great Britain.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
108
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512?520,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
An MT Error-driven Discriminative Word Lexicon
using Sentence Structure Features
Jan Niehues and Alex Waibel
Institute for Anthropomatics
Karlsruhe Institute of Technology, Germany
firstname.secondname@kit.edu
Abstract
The Discriminative Word Lexicon (DWL)
is a maximum-entropy model that pre-
dicts the target word probability given the
source sentence words. We present two
ways to extend a DWL to improve its abil-
ity to model the word translation probabil-
ity in a phrase-based machine translation
(PBMT) system. While DWLs are able to
model the global source information, they
ignore the structure of the source and tar-
get sentence. We propose to include this
structure by modeling the source sentence
as a bag-of-n-grams and features depend-
ing on the surrounding target words. Fur-
thermore, as the standard DWL does not
get any feedback from the MT system, we
change the DWL training process to ex-
plicitly focus on addressing MT errors.
By using these methods we are able to im-
prove the translation performance by up
to 0.8 BLEU points compared to a system
that uses a standard DWL.
1 Introduction
In many state-of-the-art SMT systems, the phrase-
based (Koehn et al, 2003) approach is used. In
this approach, instead of building the translation
by translating word by word, sequences of source
and target words, so-called phrase pairs, are used
as the basic translation unit. A table of correspon-
dences between source and target phrases forms
the translation model. Target language fluency is
modeled by a language model storing monolingual
n-gram occurrences. A log-linear combination of
these main models as well as additional features is
used to score the different translation hypotheses.
Then the decoder searches for the translation with
the highest score.
One problem of this approach is that bilingual
context is only modeled within the phrase pairs.
Therefore, different approaches to increase the
context available during decoding have been pre-
sented (Haque et al, 2011; Niehues et al, 2011;
Mauser et al, 2009). One promising approach is
the Discriminative Word Lexicon (DWL). In this
approach, a discriminative model is used to predict
the probability of a target word given the words in
the source sentence.
In contrast to other models in the phrase-based
system, this approach is capable of modeling the
translation probability using information from the
whole sentence. Thus it is possible to model
long-distance dependencies. But the model is not
able to use the structure of the sentence, since
the source sentence is modeled only as a bag-
of-words. Furthermore, the DWL is trained to
discriminate between all translation options with-
out knowledge about the other models used in a
phrase-based machine translation system such as
the translation model, language model etc. In
contrast, we try to feedback information about
possible errors of the MT system into the DWL.
Thereby, the DWLs are able to focus on improving
the errors of the other models of an MT system.
We will introduce features that encode infor-
mation about the source sentence structure. Fur-
thermore, the surrounding target words will also
be used in the model to encode information about
the target sentence structure. Finally, we incor-
porate information from the other models into the
creation of the training examples. We create the
negative training examples using possible errors of
the other models.
2 Related Work
Bangalore et al (2007) presented an approach to
machine translation using discriminative lexical
selection. Motivated by their results, Mauser et
al. (2009) integrated the DWL into the PBMT ap-
512
proach. Thereby, they are able to use global source
information.
This was extended by Huck et al (2010) by a
feature selection strategy in order to reduce the
number of weights. In Mediani et al (2011) a first
approach to use information about MT errors in
the training of DWLs was presented. They select
the training examples by using phrase table infor-
mation also.
The DWLs are related to work that was done
in the area of word sense disambiguation (WSD).
Carpuat and Wu (2007) presented an approach to
disambiguate between different phrases instead of
performing the disambiguation at word level.
A different lexical model that uses target side
information was presented in Jeong et al (2010).
The focus of this work was to model complex mor-
phology on the target language.
3 Discriminative Word Lexicon
The DWL is a maximum entropy model used to
determine the probability of using a target word
in the translation. Therefore, we train individ-
ual models for every target word. Each model is
trained to return the probability of this word given
the input sentence.
The input of the model is the source sentence.
Therefore, we need to represent the input sentence
by features. In this approach this is done by using
binary features. We use an indicator feature for
every input word. Therefore, the sentence is mod-
eled as a bag-of-words and the order of the words
is ignored. More formally, a given source sen-
tence F = f1 . . . fI is represented by the features
I(F ) = {if (F ) : f ? SourceV ocabulary}:
if (F ) =
{
1 : f ? F
0 : f /? F (1)
The models are trained on examples generated
by the parallel training data. The labels for train-
ing the classifier of target word e are defined as
follows:
labele(F,E) =
{
1 : e ? E
0 : e /? E (2)
We used the MegaM Toolkit1 to train the maxi-
mum entropy models. This model approximates
the probability p(ej |F ) of a target word ej given
the source sentence F .
1http://www.umiacs.umd.edu/ hal/megam/index.html
When we have the probability for every word ej
given the source sentence F , we need to combine
these probabilities into a probability of the whole
target sentence E = e1 . . . eJ given F . Making an
assumption of independence on the target side as
well, the models can be combined to the probabil-
ity of E given F :
p(E|F ) =
?
ej?e
p(ej |F ) (3)
In this equation we multiply the probability of
one word only once even if the word occurs sev-
eral times in the sentence. Since we build the tar-
get sentence from left to right during decoding,
we would need to change the score for this fea-
ture only if a new word is added to the hypothesis.
If a word is added second time we do not want
to change the feature value. In order to keep track
of this, additional bookkeeping would be required.
But the other models in our translation system will
prevent us from using a word too often in any case.
Therefore, we approximate the probability of the
sentence differently as defined in Equation 4.
p(E|F ) =
J?
j=1
p(ej |F ) (4)
In this case we multiply the probabilities of all
word occurrences in the target sentence. There-
fore, we can calculate the score for every phrase
pair before starting with the translation.
4 Modeling Sentence Structure
As mentioned before one main drawback of DWLs
is that they do not encode any structural informa-
tion about the source or target sentence. We in-
corporated this information with two types of fea-
tures. First, we tried to encode the information
from the source sentence better by using a bag-of-
n-grams approach. Secondly, we introduced new
features to be able to encode information about the
neighboring target words also.
4.1 Source Sentence Structure
In the default approach the sentence is represented
as a bag-of-words. This has the advantage that
the model can use a quite large context of the
whole sentence. In contrast to the IBM models,
where the translation probability only depends on
the aligned source word, here the translation prob-
ability can be influenced by all words in the sen-
tence.
513
On the other hand, the local context is ignored
by the bag-of-words approach. Information about
the word order get lost. No information about the
previous and next word is available. The problem
is illustrated in the example in Figure 1.
Figure 1: Example for source structural informa-
tion
Source: Die Lehrer wussten nicht, ...
Reference: The teachers didn?t know ...
The German word Lehrer (engl. teacher) is the
same word for singular or plural. It is only pos-
sible to distinguish whether singular or plural is
meant through the context. This can be determined
by the plural article die. If only one teacher would
be meant, the corresponding article would be der.
In order be able to use the DWL to distinguish
between these two translations, we need to im-
prove the representation of the input sentence. As
shown in the example, it would be helpful to know
the order of the words. If we know that the word
die precedes Lehrer, it would be more probable
that the word is translated into teachers rather than
teacher.
Therefore, we propose to use a bag-of-n-grams
instead of a bag-of-words to represent the input
sentence. In this case we will use an indicator fea-
ture for every n-gram occurring in the input sen-
tence and not only for every word. This way we
are also able to encode the sequence of the words.
For the example, we would have the input feature
die Lehrer, which would increase the probability
of using teachers in the translation compared to
teacher.
By increasing the order of the n-grams, we will
also increase the number of features and run into
data sparseness problems. Therefore, we used
count filtering on the features for higher order n-
grams. Furthermore, we combine n-grams of dif-
ferent orders to better handle the data sparseness
problem.
4.2 Target Sentence Structure
In the standard DWL approach, the probability of
the target word depends only on the source words
in the input sentence. But this is a quite rough ap-
proximation. In reality, the probability of a target
word occurring in the sentence also depends on the
other target words in the sentence.
If we look at the word langsam (engl. slow or
slowly) in the example sentence in Figure 2, we
can only determine the correct translation by using
the target context. The word can be translated as
slow or slowly depending on how it is used in the
English sentence.
In order to model the translation probability bet-
ter we need structural information of the target
side. For example, if the preceding word on the
target side is be, the translation will be more prob-
ably slow than slowly.
We encoded the target context of the word by
features indicating the preceding or next word.
Furthermore, we extend the context to up to three
words before and after the word. Therefore the
following target features are added to the set of
features for the classifier of word e:
iTC e? k(E) =
{
1 : ?j : ej = e ? ej+k = e?
0 : else
(5)
where k ? {?1, 1} for a context of one word
before and after.
5 Training
Apart from the missing sentence structure the
DWL is not able to make use of feedback from
the other models in the MT system. We try to in-
corporate information about possible errors intro-
duced by the other models into the training of the
DWL.
The DWL is trained on the paral-
lel data that is available for the task
T = (F1, E1), . . . , (FM , EM ). In order to
train it, we need to create positive and negative
examples from this data. We will present different
approaches to generate the training examples,
which differ in the information used for creating
the negative examples.
In the original approach, one training example
is created for every sentence of the parallel data
and for every DWL classifier. If the target word
occurs in the sentence, we create a positive ex-
ample and if not the source sentence is used as a
negative example as described in Equation 2. For
most words, this results in a very unbalanced set of
training examples. Most words will only occur in
quite few sentences and therefore, we have mostly
negative examples.
Mediani et al (2011) presented an approach
to create the training examples that is driven by
looking at possible errors due to the different
514
Figure 2: Example for target structural information
Source: Die Anerkennung wird langsam sein in den Vereinigten Staaten ...
Reference: The recognition is going to be slow in the United States, ...
translations in the phrase table (Phrase pair ap-
proach). Since a translation is generated always
using phrase pairs (f? , e?) with matching source
side, wrong words can only be generated in the
translation if the word occurs in the target side
words of those matching phrase pairs. There-
fore, we can define the possible target vocabulary
TV (F ) of a source sentence:
TV (F ) = {e|?(f? , e?) : f? ? F ? e ? e?} (6)
As a consequence, we generate a negative train-
ing example for one target word only from those
training sentences where the word is in the target
vocabulary but not in the reference.
labele(F,E) =
{
1 : e ? E
0 : e /? E ? e ? TV (F )
(7)
All training sentences for which the label is not
defined are not used in the training of the model
for word e. Thereby, not only can we focus the
classifiers on improving possible errors made by
the phrase table, but also reduce the amount of
training examples and therefore the time needed
for training dramatically.
In the phrase pair approach we only use in-
formation about possible errors of the translation
model for generating the negative training exam-
ples. But it would be preferable to consider possi-
ble errors of the whole MT system instead of only
using the phrase table. Some of the errors of the
phrase table might already be corrected by the lan-
guage model. The possible errors of the whole
system can be approximated by using the N -Best
list.
We first need to translate the whole cor-
pus and save the N -Best list for all sentences
NBEST (F ) = {E?1 . . . E?N}. Then we can
approximate the possible errors of the MT sys-
tem with the errors that occur in the N -Best list.
Therefore, we create a negative example for a tar-
get word only if it occurs in the N -Best list and
not in the reference. Compared to the phrase pair
approach, the only difference is the definition of
the target vocabulary:
TV (F ) = {e|e ? NBEST (F )} (8)
The disadvantage of the N-Best approach is, of
course, that we need to translate the whole cor-
pus. This is quite time consuming, but it can be
parallelized.
5.1 Training Examples for Target Features
If we use target features, the creation of the train-
ing examples gets more difficult. When using only
source features, we can create one example from
every training sentence. Even if the word occurs
in several phrase pairs or in several entries of the
N -Best list, all of them will create the same train-
ing example, since the features only depend on the
source sentence.
When we use target features, the features of the
training example depend also on the target words
that occur around the word. Therefore, we can
only use the N -Best list approach to create the tar-
get features since previous approaches mentioned
in the last part do not have the target context in-
formation. Furthermore, we can create different
examples from the same sentence. If we have, for
example, the N -Best list entries I think ... and I be-
lieve .., we can use the context think or the context
believe for the model of I.
In the approach using all target features (All
TF), we created one training example for every
sentence where the word occurs. If we see the
word in different target contexts, we create all the
features for these contexts and use them in the
training example.
I(F,E) = max( I(F ); I(E); (9)
I(E?)|E? ? NBEST (F ))
The maximum is defined component-wise. So
all features, which have in I(F ),I(E) or I(E?) the
value one, also have the value one in I(F,E). If
we use the context that was given by the reference,
this might not exist in the phrase-based MT sys-
tem. Therefore, in the next approach (N-Best TF),
we only used target features from the N -Best list.
I(F,E) = max(I(F ); I(E?)|E? ? NBEST (F ))
(10)
In both examples, we still have the problem that
we can use different contexts in one training ex-
515
ample. This condition can not happen when ap-
plying the DWL model. Therefore, we changed
the set of training examples in the separate target
features approach (Separate TF). We no longer
create one training example for every training sen-
tence (F,E), but one for every training sentence
N -Best list translation (F,E,E?). We only con-
sidered the examples for the classifier of target
word e, where e occurs in the N -Best list entry E?.
If the word does not occur in any N -Best list en-
try of a training sentence, but in the reference, we
created an additional example (F,E, ??). The fea-
tures of this examples can then be created straight
forward as:
I((F,E,E?)) = max(I(F ); I(E?)) (11)
If we have seen the word only in the reference,
we create an training example without target fea-
tures. Therefore, we have again a training exam-
ple which can not happen when using the DWL
model. Therefore, we removed these examples in
the last method (Restricted TF).
6 Experiments
After presenting the different approaches to per-
form feature and example selection, we will now
evaluate them. First, we will give a short overview
of the MT system. Then we will give a detailed
evaluation on the task of translating German lec-
tures into English and analyze the influence of the
presented approaches. Afterwards, we will present
overview experiments on the German-to-English
and English-to-German translation task of WMT
13 Shared Translation Task.
6.1 System Description
The translation system was trained on the EPPS
corpus, NC corpus, the BTEC corpus and TED
talks.2 The data was preprocessed and compound
splitting (Koehn and Knight, 2003) was applied
for German. Afterwards the discriminative word
alignment approach as described in Niehues and
Vogel (2008) was applied to generate the align-
ments between source and target words. The
phrase table was built using the scripts from the
Moses package (Koehn et al, 2007). A 4-gram
language model was trained on the target side of
the parallel data using the SRILM toolkit (Stolcke,
2002). In addition we used a bilingual language
model as described in Niehues et al (2011).
2http://www.ted.com
Reordering was performed as a preprocessing
step using part-of-speech information generated
by the TreeTagger (Schmid, 1994). We used
the reordering approach described in Rottmann
and Vogel (2007) and the extensions presented in
Niehues and Kolss (2009) to cover long-range re-
orderings, which are typical when translating be-
tween German and English.
An in-house phrase-based decoder was used to
generate the translation hypotheses and the opti-
mization was performed using MERT (Venugopal
et al, 2005).
We optimized the weights of the log-linear
model on a separate set of TED talks and also
used TED talks for testing. The development set
consists of 1.7k segments containing 16k words.
As test set we used 3.5k segments containing 31k
words. We will refer to this system as System 1.
In order to show the influence of the approaches
better, we evaluated them also in a second system.
In addition to the models used in the first system
we performed a log-linear language model and
phrase table adaptation as described in Niehues
and Waibel (2012). To this system we refer as Sys-
tem 2 in the following experiments.
6.2 German - English TED Experiments
6.2.1 Source Features
In a first set of experiments, we analyzed the dif-
ferent types of source structure features described
in Section 4.1. In all the experiments, we generate
the negative training examples using the candidate
translations generated by the phrase pairs. The re-
sults can be found in Table 1.
First, we added the unigram DWL to the base-
line system. The higher improvements for the Sys-
tem 1 is due to the fact that the DWL is only
trained on the TED corpus and therefore also per-
forms some level of domain adaptation. This is
more important for the System 1, since System 2
is already adapted to the TED domain.
If we use features based on bigrams instead of
unigrams, the number of features increases by a
factor of eight. Furthermore, in both cases the
translation quality drops. Especially for System
1, we have a significant drop in the BLEU score
of the test set by 0.6 BLEU points. One prob-
lem might be that most of the bigrams occur quite
rarely and therefore, we have a problem of data
sparseness and generalization.
If we combine the features of unigram and bi-
516
Table 1: Experiments using different source features
System FeatureSize System 1 System 2
Dev Test Dev Test
Baseline 0 26.32 24.24 28.40 25.89
Unigram 40k 27.46 25.56 28.58 26.15
Bigram 319k 27.34 24.92 28.53 25.82
Uni+bigram 359k 27.69 25.55 28.66 26.51
+ Count filter 2 122k 27.75 25.71 28.75 26.74
+ Count filter 5 63k 27.81 25.67 28.72 26.81
+ Trigram 77k 27.76 25.76 28.82 26.94
gram features, for System 1, we get an improve-
ment of 0.2 BLEU points on the development data
and the same translation quality on the test data
as the baseline DWL system using only unigrams.
For System 2, we can improve by 0.1 on the devel-
opment data and 0.4 on the test data. So we can get
a first improvement using these additional source
features, but the number of features increased by a
factor of nine.
In order to decrease the number of features
again, we applied count filtering to the bigram
features. In a first experiment we only used the
bigram features that occur at least twice. This
reduced the number of features dramatically by
a factor of three. Furthermore, this even im-
proved the translation quality. In both systems we
could improve the translation quality by 0.2 BLEU
points. So it seems to be quite important to add
only the relevant bigram features.
If we use a minimum occurrence of five for the
bigram features, we can even decrease the num-
ber of features further by a factor of two without
losing any translation performance.
Finally, we added the trigram features. For
these features we applied count filtering of five.
For System 1, the translation quality stays the
same, but for System 2 we can improve the trans-
lation quality by additional 0.2 BLEU points.
In summary, we could improve the translation
quality by 0.2 for the System 1 and 0.8 BLEU
points for the System 2 on the test set. Due to the
count filtering, this is achieved by only using less
than twice as many features.
6.3 Training Examples
In a next step we analyzed the different exam-
ple selection approaches. The results are summa-
rized in Table 2. In these experiments we used the
source features using unigrams, bigrams and tri-
grams with count filtering in all experiments.
In the first experiment, we used the original ap-
proach to create the training examples. In this
case, all sentences where the word does not occur
in the reference generate negative examples. In
our setup, we needed 8,461 DWL models to trans-
late the development and test data. These are all
target words that occur in phrase pairs that can be
used to translate the development or test set.
In each of approaches we have 0.75M posi-
tive examples for these models. In the origi-
nal approach, we have 428M negative examples.
So in this case the number of positive and nega-
tive examples is very unbalanced. This training
data leads to models with a total of 659M feature
weights.
If we use the target side of the phrase pairs to
generate our training examples, we dramatically
reduce the number of negative training examples.
In this case only 5M negative training examples
are generated. The size of the models is reduced
dramatically to 38M weights. Furthermore, we
could improve the translation quality by 0.3 BLEU
points on both System 1 and System 2.
If we use the 300-Best lists produced by Sys-
tem 1 to generate the training examples, we can
reduce the model size further. This approach leads
to models only half the size of the phrase pairs ap-
proach using only 1.59M negative examples. Fur-
thermore, for System 1 the translation quality can
be improved further to 25.87 BLEU points. For
System 2 the BLEU score on the development data
increases, but the score on the test sets drops by 0.4
BLEU points.
In the next experiment we used the N -Best lists
generated by System 2. The results are shown in
the line N -Best list 2. In this case, the model size
is slightly reduced further. And on the adapted
system a similar performance is achieved. But for
517
Table 2: Experiments using different methods to create training examples
System #weight #neg. Examples System 1 System 2
Dev Test Dev Test
Original Approach 659 M 428 M 27.39 25.44 28.64 26.63
Phrase pairs 38 M 5.26 M 27.76 25.76 28.82 26.94
N -Best list 1 16 M 1.59 M 27.93 25.87 29.07 26.57
N -Best list 2 11 M 1.22 M 27.46 25.37 28.79 26.59
N -Best list 1 nonUnique 16 M 1.41M 27.99 25.97 29.07 26.65
System 1 the performance of this approach drops.
Consequently, it seems to be fine to use an N -
Best list of a more general system to generate the
negative examples. But the N -Best list should not
stem from an adapted system.
Finally, the phrase table was trained on the same
corpus as the one that was used to generate the N -
Best lists for DWL training. Since we have seen
the data before, longer phrases can be used than
in a real test scenario. To compensate partly for
that, we removed all phrase pairs that occur only
once in the phrase table. The results are shown in
the last line. This approach could slightly improve
the translation quality leading to a BLEU score of
25.97 for System 1 and 26.65 for the System 2.
6.4 Target Features
After evaluating the different approaches to gen-
erate the negative examples, we also evaluated the
different approaches for the target features. The
results are summarized in Table 3. In all these ex-
periments we use the training examples generated
by the N -Best list of System 1 using the phrase
table without unique phrase pairs.
First, we tested the four different methods using
a context of one word before and one word after
the word.
In the experiments the first two methods, All
TF and N-Best TF , perform worse than the last
two approaches, Separate TF and Restricted TF.
So it seems to be important to have realistic exam-
ples and not to mix different target contexts in one
example. The Separate and Restricted approach
perform similarly well. In both cases the perfor-
mance can be improved slightly by using a context
of three words before and after instead of using
only one word.
If we look at the model size, the number of
weights increases from 16M to 17M, when using
a context of one word and to 21M using a context
of three words.
If we compare the results to the systems using
no target features in the first row, no or only slight
improvements can be achieved. One reason might
be that the morphology of English is not very com-
plex and therefore, the target context is not as im-
portant to determine the correct translation.
6.4.1 Overview
In Table 4, we give an overview of the results us-
ing the different extensions to DWLs given in this
paper. The baseline system does not use any DWL
at all. If we use a DWL using only bag-of-words
features and the training examples from the phrase
pairs, we can improve by 1.3 BLEU points on Sys-
tem 1 and 0.3 BLEU points on System 2.
By adding the source-context features, the first
system can be improved by 0.2 BLEU points and
the second one by 0.8 BLEU points. If we use the
training examples from the N -Best list instead of
using the ones from the phrase table, we improve
by 0.2 on System 1, but perform 0.3 worse on Sys-
tem 2. Adding the target context features does not
improve System 1, but System 2 can be improved
by 0.3 BLEU points. This system results in the
best average performance. Compared to the base-
line system with DWLs, we can improve by 0.4
and 0.8 BLEU points, respectively.
Table 4: Overview of results for TED lectures
System System 1 System 2
Dev Test Dev Test
Baseline 26.32 24.24 28.40 25.89
DWL 27.46 25.56 28.58 26.15
sourceContext 27.76 25.76 28.82 26.94
N -Best 27.99 25.97 29.07 26.65
TargetContext 28.15 25.91 29.12 26.90
6.5 German - English WMT 13 Experiments
In addition to the experiments on the TED data,
we also tested the models in the systems for the
518
Table 3: Experiments using different target features
System Context System 1 System 2
Dev Test Dev Test
No Target Features 0-0 27.99 25.97 29.07 26.65
All TF 1-1 27.80 25.48 28.80 26.38
N-Best TF 1-1 27.99 25.74 28.86 26.37
Separate TF 1-1 28.06 25.81 28.98 26.80
Restricted TF 1-1 28.13 25.84 28.94 26.68
Separate TF 3-3 27.87 25.90 28.99 26.75
Restricted TF 3-3 28.15 25.91 29.12 26.90
WMT 2013. The systems are similar to the one
used before, but were trained on all available train-
ing data and use additional models. The systems
were tested on newstest2012. The results for Ger-
man to English are summarized in Table 5. In this
case the DWLs were trained on the EPPS and the
NC corpus. Since the corpora are bigger, we per-
form an additional weight filtering on the models.
The baseline system uses already a DWL
trained with the bag-of-words features and the
training examples were created using the phrase
table. If we add the bag-of-n-grams features up
to a n-gram length of 3, we cannot improve the
translation quality on this task. But by addition-
ally generating the negative training examples us-
ing the 300-Best list, we can improve this system
by 0.2 BLEU points.
Table 5: Experiments on German to English WMT
2013
System Dev Test
Unigram DWL 25.79 24.36
+ Bag-of-n-gram 25.85 24.33
+ N -Best 25.84 24.52
6.6 English - German WMT 13 Experiments
We also tested the approach also on the reverse
direction. Since the German morphology is much
more complex than the English one, we hope that
in this case the target features can help more. The
results for this task are shown in Table 6. Here, the
baseline system again already uses DWLs. If we
add the bag-of-n-grams features and generate the
training examples from the 300-Best list, we can
again slightly improve the translation quality. In
this case we can improve the translation quality by
additional 0.1 BLEU points by adding the target
features. This leads to an overall improvement by
nearly 0.2 BLEU points.
Table 6: Experiments on English to German WMT
2013
System Dev Test
unigram DWL 16.97 17.41
+ Bag-of-n-gram 16.89 17.45
+ N -Best 17.10 17.47
+ Target Features 17.08 17.58
7 Conclusion
Discriminative Word Lexica have been recently
used in several translation systems and have shown
to improve the translation quality. In this work, we
extended the approach to improve its modeling of
the translation process.
First, we added features which represent the
structure of the sentence better. By using bag-of-
n-grams features instead of bag-of-words features,
we are able to encode the order of the source sen-
tence. Furthermore, we use features for the sur-
rounding target words to also model the target con-
text of the word. In addition, we tried to train the
DWLs in a way that they help to address possi-
ble errors of the MT system by feeding informa-
tion from the MT system back into the generation
of the negative training examples. Thereby, we
could reduce the size of the models and improve
the translation quality. Overall, we were able to
improve the translation quality on three different
tasks in two different translation directions. Im-
provements of up to 0.8 BLEU points could be
achieved.
519
8 Acknowledgements
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The research lead-
ing to these results has received funding from
the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
n? 287658.
References
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Sta-
tistical Machine Translation through Global Lexical
Selection and Sentence Reconstruction. In Annual
Meeting-Association for Computational Linguistics,
volume 45, page 152.
M. Carpuat and D. Wu. 2007. Improving Statis-
tical Machine Translation using Word Sense Dis-
ambiguation. In In The 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
R. Haque, S.K. Naskar, A. Bosch, and A. Way.
2011. Integrating source-language context into
phrase-based statistical machine translation. Ma-
chine Translation, 25(3):239?285.
M. Huck, M. Ratajczak, P. Lehnen, and H. Ney. 2010.
A Comparison of Various Types of Extended Lexi-
con Models for Statistical Machine Translation. In
Proc. of the Conf. of the Assoc. for Machine Trans-
lation in the Americas (AMTA).
M. Jeong, K. Toutanova, H. Suzuki, and C. Quirk.
2010. A Discriminative Lexicon Model for Com-
plex Morphology. In Proceedings of the Ninth Con-
ference of the Association for Machine Translation
in the Americas (AMTA 2010).
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In EACL, Budapest, Hungary.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statis-
tical Phrase-Based Translation. In Proceedings of
the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology, pages 48?54, Ed-
monton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In ACL 2007,
Demonstration Session, Prague, Czech Republic.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending
Statistical Machine Translation with Discriminative
and Trigger-based Lexicon Models. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1 ? Volume
1, Emnlp?09, Singapore.
M. Mediani, E. Cho, J. Niehues, T. Herrmann, and
A. Waibel. 2011. The KIT English-French Trans-
lation Systems for IWSLT 2011. Proceedings of the
eight International Workshop on Spoken Language
Translation (IWSLT).
Jan Niehues and Mutsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. Fourth
Workshop on Statistical Machine Translation (WMT
2009), Athens, Greece.
J. Niehues and S. Vogel. 2008. Discriminative Word
Alignment via Alignment Matrix Modeling. Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 18?25.
J. Niehues and A. Waibel. 2012. Detailed Analysis of
different Strategies for Phrase Table Adaptation in
SMT. In Proceedings of the Tenth Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA).
J. Niehues, T. Herrmann, S. Vogel, and A. Waibel.
2011. Wider Context by Using Bilingual Language
Models in Machine Translation. Sixth Workshop on
Statistical Machine Translation (WMT 2011), Edin-
burgh, UK.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based
Distortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
A. Stolcke. 2002. SRILM ? An Extensible Language
Modeling Toolkit. In Icslp, Denver, Colorado, USA.
A. Venugopal, A. Zollman, and A. Waibel. 2005.
Training and Evaluation Error Minimization Rules
for Statistical Machine Translation. In Workshop on
Data-drive Machine Translation and Beyond (WPT-
05), Ann Arbor, MI.
520
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 30?39,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Letter N-Gram-based Input Encoding for Continuous Space Language
Models
Henning Sperr?, Jan Niehues? and Alexander Waibel?
Institute of Anthropomatics
KIT - Karlsruhe Institute of Technology
Karlsruhe, Germany
? firstname.lastname@kit.edu
? henning.sperr@student.kit.edu
Abstract
We present a letter-based encoding for
words in continuous space language mod-
els. We represent the words completely by
letter n-grams instead of using the word
index. This way, similar words will au-
tomatically have a similar representation.
With this we hope to better generalize
to unknown or rare words and to also
capture morphological information. We
show their influence in the task of machine
translation using continuous space lan-
guage models based on restricted Boltz-
mann machines. We evaluate the trans-
lation quality as well as the training time
on a German-to-English translation task of
TED and university lectures as well as on
the news translation task translating from
English to German. Using our new ap-
proach a gain in BLEU score by up to 0.4
points can be achieved.
1 Introduction
Language models play an important role in natural
language processing. The most commonly used
approach is n-gram-based language models (Chen
and Goodman, 1999).
In recent years Continuous Space Language
Models (CSLMs) have gained a lot of atten-
tion. Compared to standard n-gram-based lan-
guage models they promise better generalization
to unknown histories or n-grams with only few
occurrences. Since the words are projected into
a continuous space, true interpolation can be per-
formed when an unseen sample appears. The stan-
dard input layer for CSLMs is a so called 1-of-
n coding where a word is represented as a vector
with a single neuron turned on and the rest turned
off. In the standard approach it is problematic to
infer probabilities for words that are not inside the
vocabulary. Sometimes an extra unknown neu-
ron is used in the input layer to represent these
words (Niehues and Waibel, 2012). Since all un-
seen words get mapped to the same neuron, no real
discrimination between those words can be done.
Furthermore, rare words are also hard to model,
since there is too few training data available to es-
timate their associated parameters.
We try to overcome these shortcomings by
using subword features to cluster similar words
closer together and generalize better over unseen
words. We hope that words containing similar let-
ter n-grams will yield a good indicator for words
that have the same function inside the sentence.
Introducing a method for subword units also has
the advantage that the input layer can be smaller,
while still representing nearly the same vocabulary
with unique feature vectors. By using a smaller in-
put layer, less weights need to be trained and the
training is faster. In this work we present the letter
n-gram approach to represent words in an CSLM,
and compare it to the word-based CSLM presented
in Niehues and Waibel (2012).
The rest of this paper is structured as follows:
First we will give an overview of related work.
After that we give a brief overview of restricted
Boltzmann machines which are the basis of the
letter-based CSLM presented in Section 4. Then
we will present the results of the experiments and
conclude our work.
2 Related Work
First research on neural networks to predict word
categories has been done in Nakamura et al
(1990) where neural networks were used to pre-
dict word categories. Xu and Rudnicky (2000)
proposed a language model that has an input con-
sisting of one word and no hidden units. This
network was limited to infer unigram and bigram
statistics. There has been research on feed for-
ward neural network language models where they
30
achieved a decrease in perplexity compared to
standard n-gram language models (Bengio et al,
2003). In Schwenk and Gauvain (2005) and later
in Schwenk (2007) research was performed on
training large scale neural network language mod-
els on millions of words resulting in a decrease of
the word error rate for continuous speech recog-
nition. In Schwenk et al (2006) they use the
CSLM framework to rescore n-best lists of a ma-
chine translation system during tuning and testing
steps. Usually these networks use short lists to
reduce the size of the output layer and to make
calculation feasible. There have been approaches
to optimize the output layer of such a network,
so that vocabularies of arbitrary size can be used
and there is no need to back off to a smaller n-
gram model (Le et al, 2011). In this Structured
Output Layer (SOUL) neural network language
model a hierarchical output layer was chosen. Re-
current Neural Networks have also been used to
try and improve language model perplexities in
Mikolov et al (2010), concluding that Recurrent
Neural Networks potentially improve over classi-
cal n-gram language models with increasing data
and a big enough hidden unit size of the model.
In the work of Mnih and Hinton (2007) and Mnih
(2010) training factored restricted Boltzmann ma-
chines yielded no gain compared to Kneser-Ney
smoothed n-gram models. But it has been shown
in Niehues and Waibel (2012), that using a re-
stricted Boltzmann machine with a different layout
during decoding can yield an increase in BLEU
score. There has also been a lot of research in
the field of using subword units for language mod-
eling. In Shaik et al (2011) linguistically moti-
vated sub-lexical units were proposed to improve
open vocabulary speech recognition for German.
Research on morphology-based and subword lan-
guage models on a Turkish speech recognition task
has been done by Sak et al (2010). The idea
of Factored Language models in machine transla-
tion has been introduced by Kirchhoff and Yang
(2005). Similar approaches to develop joint lan-
guage models for morphologically rich languages
in machine translation have been presented by
Sarikaya and Deng (2007). In Emami et al (2008)
a factored neural network language model for Ara-
bic was built. They used different features such as
segmentation, part-of-speech and diacritics to en-
rich the information for each word.
3 Restricted Boltzmann Machine-based
Language Model
In this section we will briefly review the con-
tinuous space language models using restricted
Boltzmann machines (RBM). We will focus on
the parts that are important for the implementa-
tion of the input layers described in the next sec-
tion. A restricted Boltzmann machine is a gener-
ative stochastic neural network which consists of
a visible and a hidden layer of neurons that have
unidirectional connections between the layers but
no inner layer connections as shown in Figure 1.
Visible
Hidden
Figure 1: Restricted Boltzmann Machine.
The activation of the visible neurons will be de-
termined by the input data. The standard input
layer for neural network language models uses a
1-of-n coding to insert a word from the vocabulary
into the network. This is a vector, where only the
index of the word in the vocabulary is set to one
and the rest to zero. Sometimes this is also referred
to as a softmax layer of binary units. The activa-
tion of the hidden units is usually binary and will
be inferred from the visible units by using sam-
pling techniques. In Niehues and Waibel (2012)
an n-gram Boltzmann machine language model is
proposed using such a softmax layer for each con-
text. In this work, we want to explore different
ways of encoding the word observations in the in-
put layer. Figure 2 is an example of the original
model with three hidden units, two contexts and
a vocabulary of two words. In this example the
bigram my house is modeled.
To calculate the probability of a visible config-
uration v we will use the definition of the free en-
ergy in a restricted Boltzmann machine with bi-
nary stochastic hidden units, which is
F (v) = ?
?
i
viai ?
?
j
log(1 + exj ) (1)
where ai is the bias of the ith visible neuron vi and
31
Visible
<s> </s> my house <s> </s> my house
Hidden
Figure 2: RBMLM with three hidden units and a
vocabulary size of two words and two word con-
texts, where activated units are marked as black.
xj is the activation of the jth hidden neuron. The
activation xj is defined as
xj = bj +
?
i
viwij (2)
where bj is the bias of the jth hidden neuron and
wij is the weight between visible unit vi and hid-
den unit xj . Using these definitions, the probabil-
ity of our visible configuration v is
p(v) =
1
Z
e?F (v) (3)
with the partition function Z =
?
v e
?F (v) being
the normalization constant. Usually this normal-
ization constant is not easy to compute since it is
a sum over an exponential amount of values. We
know that the free energy will be proportional to
the true probability of our visible vector, this is
the reason for using the free energy as a feature
in our log-linear model instead of the true prob-
ability. In order to use it as a feature inside the
decoder we actually need to be able to compute
the probability for a whole sentence. As shown in
Niehues and Waibel (2012) we can do this by sum-
ming over the free energy of all n-grams contained
in the sentence.
3.1 Training
For training the restricted Boltzmann machine lan-
guage model (RBMLM) we used the Contrastive
Divergence (CD) algorithm as proposed in Hinton
(2010). In order to do this, we need to calculate the
derivation of the probability of the example given
the weights
? log p(v)
?wij
= <vihj>data ?<vihj>model (4)
where <vihj>model is the expected value of vihj
given the distribution of the model. In other
words we calculate the expectation of how often
vi and hj are activated together, given the dis-
tribution of the data, minus the expectation of
them being activated together given the distribu-
tion of the model, which will be calculated us-
ing Gibbs-Sampling techniques. Usually many
steps of Gibbs-Sampling are necessary to get an
unbiased sample from the distribution, but in the
Contrastive Divergence algorithm only one step of
sampling is performed (Hinton, 2002).
4 Letter-based Word Encoding
In this section we will describe the proposed in-
put layers for the RBMLM. Compared to the word
index-based representation explained above, we
try to improve the capability to handle unknown
words and morphology by splitting the word into
subunits.
4.1 Motivation
In the example mentioned above, the word index
model might be able to predict my house but it
will fail on my houses if the word houses is not in
the training vocabulary. In this case, a neuron that
classifies all unknown tokens or some other tech-
niques to handle such a case have to be utilized.
In contrast, a human will look at the single let-
ters and see that these words are quite similar. He
will most probably recognize that the appended s
is used to mark the plural form, but both words re-
fer to the same thing. So he will be able to infer
the meaning although he has never seen it before.
Another example in English are be the words
happy and unhappy. A human speaker who does
not know the word unhappy will be able to know
from the context what unhappy means and he can
guess that both of the words are adjectives, that
have to do with happiness, and that they can be
used in the same way.
In other languages with a richer morphology,
like German, this problem is even more important.
The German word scho?n (engl. beautiful) can have
16 different word forms, depending on case, num-
ber and gender.
Humans are able to share information about
words that are different only in some morphemes
like house and houses. With our letter-based input
encoding we want to generalize over the common
word index model to capture morphological infor-
32
mation about the words to make better predictions
for unknown words.
4.2 Features
In order to model the aforementioned morpholog-
ical word forms, we need to create features for
every word that represent which letters are used
in the word. If we look at the example of house,
we need to model that the first letter is an h, the
second is an o and so on.
If we want to encode a word this way, we have
the problem that we do not have a fixed size of
features, but the feature size depends on the length
of the word. This is not possible in the framework
of continuous space language models. Therefore,
a different way to represent the word is needed.
An approach for having a fixed size of features
is to just model which letters occur in the word.
In this case, every input word is represented by a
vector of dimension n, where n is the size of the
alphabet in the text. Every symbol, that is used
in the word is set to one and all the other features
are zero. By using a sparse representation, which
shows only the features that are activated, the word
house would be represented by
w1 = e h o s u
The main problem of this representation is that
we lose all information about the order of the let-
ters. It is no longer possible to see how the word
ends and how the word starts. Furthermore, many
words will be represented by the same feature vec-
tor. For example, in our case the words house and
houses would be identical. In the case of houses
and house this might not be bad, but the words
shortest and others or follow and wolf will also
map to the same input vector. These words have
no real connection as they are different in mean-
ing and part of speech.
Therefore, we need to improve this approach
to find a better model for input words. N-grams
of words or letters have been successfully used to
model sequences of words or letters in language
models. We extend our approach to model not
only the letters that occur in the in the word, but
the letter n-grams that occur in the word. This
will of course increase the dimension of the fea-
ture space, but then we are able to model the order
of the letters. In the example of my house the fea-
ture vector will look like
w1 = my <w>m y</w>
w2 = ho ou se us <w>h e</w>
We added markers for the beginning and end of
the word because this additional information is im-
portant to distinguish words. Using the example
of the word houses, modeling directly that the last
letter is an s could serve as an indication of a plural
form.
If we use higher order n-grams, this will in-
crease the information about the order of the let-
ters. But these letter n-grams will also occur more
rarely and therefore, the weights of these features
in the RBM can no longer be estimated as reliably.
To overcome this, we did not only use the n-grams
of order n, but all n-grams of order n and smaller.
In the last example, we will not only use the bi-
grams, but also the unigrams.
This means my house is actually represented as
w1 = m y my <w>m y</w>
w2 = e h o s u ho ou se us <w>h e</w>
With this we hope to capture many morpholog-
ical variants of the word house. Now the represen-
tations of the words house and houses differ only
in the ending and in an additional bigram.
houses = ... es s</w>
house = ... se e</w>
The beginning letters of the two words will con-
tribute to the same free energy only leaving the
ending letter n-grams to contribute to the different
usages of houses and house.
The actual layout of the model can be seen in
Figure 3. For the sake of clarity we left out the
unigram letters. In this representation we now do
not use a softmax input layer, but a logistic input
layer defined as
p(vi = on) =
1
1 + e?xi
(5)
where vi is the ith visible neuron and xi is the in-
put from the hidden units for the ith neuron de-
fined as
xi = ai +
?
j
hjwij (6)
with ai being the bias of the visible neuron vi and
wij being the weight between the hidden unit hj
and vi.
33
Visible
<i>>e/
myho ou myhH dssemyho ou umnyh semyhH
?
ds umnyh
?
Figure 3: A bigram letter index RBMLM with
three hidden units and two word contexts, where
activated units are marked as black.
4.3 Additional Information
The letter index approach can be extended by
several features to include additional information
about the words. This could for example be part-
of-speech tags or other morphological informa-
tion. In this work we tried to include a neuron
to capture capital letter information. To do this we
included a neuron that will be turned on if the first
letter was capitalized and another neuron that will
be turned on if the word was written in all capital
letters. The word itself will be lowercased after we
extracted this information.
Using the example of European Union, the new
input vector will look like this
w1 =a e n o p r u an ea eu op pe ro ur
<w>e n</w><CAPS>
w2 =u i n o un io ni on
<w>u n</w><CAPS>
This will lead to a smaller letter n-gram vocab-
ulary since all the letter n-grams get lowercased.
This also means there is more data for each of the
letter n-gram neurons that were treated differently
before. We also introduced an all caps feature
which is turned on if the whole word was written
in capital letters. We hope that this can help detect
abbreviations which are usually written in all cap-
ital letters. For example EU will be represented
as
w1 = e u eu <w>e u</w><ALLCAPS>
5 Evaluation
We evaluated the RBM-based language model
on different statistical machine translation (SMT)
tasks. We will first analyze the letter-based word
representation. Then we will give a brief descrip-
tion of our SMT system. Afterwards, we de-
scribe in detail our experiments on the German-
to-English translation task. We will end with addi-
tional experiments on the task of translating Eng-
lish news documents into German.
5.1 Word Representation
In first experiments we analyzed whether the
letter-based representation is able to distinguish
between different words. In a vocabulary of
27,748 words, we compared for different letter n-
gram sizes how many words are mapped to the
same input feature vector.
Table 1 shows the different models, their input
dimensions and the total number of unique clus-
ters as well as the amount of input vectors con-
taining one, two, three or four or more words that
get mapped to this input vector. In the word index
representation every word has its own feature vec-
tor. In this case the dimension of the input vector
is 27,748 and each word has its own unique input
vector.
If we use only letters, as done in the unigram
model, only 62% of the words have a unique repre-
sentation. Furthermore, there are 606 feature vec-
tors representing 4 or more words. This type of
encoding of the words is not sufficient for the task.
When using a bigram letter context nearly each
of the 27,748 words has a unique input represen-
tation, although the input dimension is only 7%
compared to the word index. With the three let-
ter vocabulary context and higher there is no input
vector that represents more than three words from
the vocabulary. This is good since we want similar
words to be close together but not have exactly the
same input vector. The words that are still clus-
tered to the same input are mostly numbers or typ-
ing mistakes like ?YouTube? and ?Youtube?.
5.2 Translation System Description
The translation system for the German-to-English
task was trained on the European Parliament cor-
pus, News Commentary corpus, the BTEC cor-
pus and TED talks1. The data was preprocessed
and compound splitting was applied for German.
Afterwards the discriminative word alignment ap-
proach as described in Niehues and Vogel (2008)
was applied to generate the alignments between
source and target words. The phrase table was
1http://www.ted.com
34
#Vectors mapping to
Model Caps VocSize TotalVectors 1 Word 2 Words 3 Words 4+ Words
WordIndex - 27,748 27,748 27,748 0 0 0
Letter 1-gram No 107 21,216 17,319 2,559 732 606
Letter 2-gram No 1,879 27,671 27,620 33 10 8
Letter 3-gram No 12,139 27,720 27,701 10 9 0
Letter 3-gram Yes 8,675 27,710 27,681 20 9 0
Letter 4-gram No 43,903 27,737 27,727 9 1 0
Letter 4-gram Yes 25,942 27,728 27,709 18 1 0
Table 1: Comparison of the vocabulary size and the possibility to have a unique representation of each
word in the training corpus.
built using the scripts from the Moses package de-
scribed in Koehn et al (2007). A 4-gram language
model was trained on the target side of the parallel
data using the SRILM toolkit from Stolcke (2002).
In addition, we used a bilingual language model as
described in Niehues et al (2011). Reordering was
performed as a preprocessing step using part-of-
speech (POS) information generated by the Tree-
Tagger (Schmid, 1994). We used the reorder-
ing approach described in Rottmann and Vogel
(2007) and the extensions presented in Niehues et
al. (2009) to cover long-range reorderings, which
are typical when translating between German and
English. An in-house phrase-based decoder was
used to generate the translation hypotheses and
the optimization was performed using the MERT
implementation as presented in Venugopal et al
(2005). All our evaluation scores are measured us-
ing the BLEU metric.
We trained the RBMLM models on 50K sen-
tences from TED talks and optimized the weights
of the log-linear model on a separate set of TED
talks. For all experiments the RBMLMs have been
trained with a context of four words. The devel-
opment set consists of 1.7K segments containing
16K words. We used two different test sets to
evaluate our models. The first test set contains
TED talks with 3.5K segments containing 31K
words. The second task was from an in-house
computer science lecture corpus containing 2.1K
segments and 47K words. For both tasks we used
the weights optimized on TED.
For the task of translating English news texts
into German we used a system developed for the
Workshop on Machine Translation (WMT) eval-
uation. The continuous space language models
were trained on a random subsample of 100K sen-
tences from the monolingual training data used for
this task. The out-of-vocabulary rates for the TED
task are 1.06% while the computer science lec-
tures have 2.73% and nearly 1% on WMT.
5.3 German-to-English TED Task
The results for the translation of German TED lec-
tures into English are shown in Table 2. The base-
line system uses a 4-gram Kneser-Ney smoothed
language model trained on the target side parallel
data. We then added a RBMLM, which was only
trained on the English side of the TED corpus.
If the word index RBMLM trained for one iter-
ation using 32 hidden units is added, an improve-
ment of about 1 BLEU can be achieved. The let-
ter bigram model performs about 0.4 BLEU points
better than no additional model, but significantly
worse then the word index model or the other let-
ter n-gram models. The letter 3- to 5-gram-based
models obtain similar BLEU scores, varying only
by 0.1 BLEU point. They also achieve a 0.8 to
0.9 BLEU points improvement against the base-
line system and a 0.2 to 0.1 BLEU points decrease
than the word index-based encoding.
System Dev Test
Baseline 26.31 23.02
+WordIndex 27.27 24.04
+Letter 2-gram 26.67 23.44
+Letter 3-gram 26.80 23.84
+Letter 4-gram 26.79 23.93
+Letter 5-gram 26.64 23.82
Table 2: Results for German-to-English TED
translation task
Using the word index model with the first base-
line system increases the BLEU score nearly as
much as adding a n-gram-based language model
trained on the TED corpus as done in the base-
35
line of the systems presented in Table 3. In these
experiments all letter-based models outperformed
the baseline system. The bigram-based language
model performs worst and the 3- and 4-gram-
based models perform only slightly worse than the
word index-based model.
System Dev Test
Baseline+ngram 27.45 24.06
+WordIndex 27.70 24.34
+Letter 2-gram 27.45 24.15
+Letter 3-gram 27.52 24.25
+Letter 4-gram 27.60 24.30
Table 3: Results of German-to-English TED trans-
lations using an additional in-domain language
model.
A third experiment is presented in Table 4. Here
we also applied phrase table adaptation as de-
scribed in Niehues et al (2010). In this experiment
the word index model improves the system by 0.4
BLEU points. In this case all letter-based models
perform very similar. They are again performing
slightly worse than the word index-based system,
but better than the baseline system.
To summarize the results, we could always im-
prove the performance of the system by adding
the letter n-gram-based language model. Further-
more, in most cases, the bigram model performs
worse than the higher order models. It seems to be
important for this task to have more context infor-
mation. The 3- and 4-gram-based models perform
almost equal, but slightly worse than the word
index-based model.
System Dev Test
BL+ngram+adaptpt 28.40 24.57
+WordIndex 28.55 24.96
+Letter 2-gram 28.31 24.80
+Letter 3-gram 28.31 24.71
+Letter 4-gram 28.46 24.65
Table 4: Results of German-to-English TED trans-
lations with additional in-domain language model
and adapted phrase table.
5.3.1 Caps Feature
In addition, we evaluated the proposed caps fea-
ture compared to the non-caps letter n-gram model
and the baseline systems. As we can see in Ta-
ble 5, caps sometimes improves and sometimes
decreases the BLEU score by about ?0.2 BLEU
points. One reason for that might be that most
English words are written lowercased, therefore
we do not gain much information.
System Dev Test
Baseline 26.31 23.02
+Letter 3-gram 26.80 23.84
+Letter 3-gram+caps 26.67 23.85
Baseline+ngram 27.45 24.06
+Letter 3-gram 27.52 24.25
+Letter 3-gram+caps 27.60 24.47
BL+ngram+adaptpt 28.40 24.57
+Letter 3-gram 28.31 24.71
+Letter 3-gram+caps 28.43 24.66
Table 5: Difference between caps and non-caps
letter n-gram models.
5.4 German-to-English CSL Task
After that, we evaluated the computer science lec-
ture (CSL) test set. We used the same system as
for the TED translation task. We did not perform
a new optimization, since we wanted so see how
well the models performed on a different task.
The results are summarized in Table 6. In this
case the baseline is outperformed by the word in-
dex approach by approximately 1.1 BLEU points.
Except for the 4-gram model the results are similar
to the result for the TED task. All systems could
again outperform the baseline.
System Test
Baseline 23.60
+WordIndex 24.76
+Letter 2-gram 24.17
+Letter 3-gram 24.36
+Letter 4-gram 23.82
Table 6: Results the baseline of the German-to-
English CSL task.
The system with the additional in-domain lan-
guage model in Table 7 shows that both letter
n-gram language models perform better than the
baseline and the word index model, improving the
baseline by about 0.8 to 1 BLEU. Whereas the
word index model only achieved an improvement
of 0.6 BLEU points.
The results of the system with the additional
phrase table adaption can be seen in Table 8. The
36
System Test
Baseline+ngram 23.81
+WordIndex 24.41
+Letter 2-gram 24.37
+Letter 3-gram 24.66
+Letter 4-gram 24.85
Table 7: Results on German-to-English CSL cor-
pus with additional in-domain language model.
word index model improves the baseline by 0.25
BLEU points. The letter n-gram models improve
the baseline by about 0.3 to 0.4 BLEU points also
improving over the word index model. The letter
bigram model in this case performs worse than the
baseline.
System Test
BL+ngram+adaptpt 25.00
+WordIndex 25.25
+Letter 2-gram 24.68
+Letter 3-gram 25.43
+Letter 4-gram 25.33
Table 8: Results on German-to-English CSL with
additional in-domain language model and adapted
phrase table.
In summary, again the 3- and 4-gram letter mod-
els perform mostly better than the bigram version.
They both perform mostly equal. In contrast to the
TED task, they were even able to outperform the
word index model in some configurations by up to
0.4 BLEU points.
5.5 English-to-German News Task
When translating English-to-German news we
could not improve the performance of the base-
line by using a word index model. In contrast, the
performance dropped by 0.1 BLEU points. If we
use a letter bigram model, we could improve the
translation quality by 0.1 BLEU points over the
baseline system.
System Dev Test
Baseline 16.90 17.36
+WordIndex 16.79 17.29
+Letter 2-gram 16.91 17.48
Table 9: Results for WMT2013 task English-to-
German.
5.6 Model Size and Training Times
In general the letter n-gram models perform al-
most as good as the word index model on English
language tasks. The advantage of the models up to
the letter 3-gram context model is that the training
time is lower compared to the word index model.
All the models were trained using 10 cores and
a batch size of 10 samples per contrastive diver-
gence update. As can be seen in Table 10 the letter
3-gram model needs less than 50% of the weights
and takes around 75% of the training time of the
word index model. The four letter n-gram model
takes longer to train due to more parameters.
Model #Weights Time
WordIndex 3.55 M 20 h 10 min
Letter 2-gram 0.24 M 1h 24 min
Letter 3-gram 1.55 M 15 h 12 min
Letter 4-gram 5.62 M 38 h 59 min
Table 10: Training time and number of parameters
of the RBMLM models.
6 Conclusions
In this work we presented the letter n-gram-based
input layer for continuous space language models.
The proposed input layer enables us to encode the
similarity of unknown words directly in the input
layer as well as to directly model some morpho-
logical word forms.
We evaluated the encoding on different trans-
lation tasks. The RBMLM using this encod-
ing could always improve the translation qual-
ity and perform similar to a RBMLM based on
word indices. Especially in the second configu-
ration which had a higher OOV rate, the letter n-
gram model performed better than the word index
model. Moreover, the model based on letter 3-
grams uses only half the parameters of the word
index model. This reduced the training time of the
continuous space language model by a quarter.
Acknowledgments
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The research lead-
ing to these results has received funding from
the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
n? 287658.
37
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359?393.
Ahmad Emami, Imed Zitouni, and Lidia Mangu. 2008.
Rich morphology based n-gram language models for
arabic. In INTERSPEECH, pages 829?832. ISCA.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Comput., 14(8):1771?1800, August.
Geoffrey Hinton. 2010. A Practical Guide to Training
Restricted Boltzmann Machines. Technical report,
University of Toronto.
Katrin Kirchhoff and Mei Yang. 2005. Improved Lan-
guage Modeling for Statistical Machine Translation.
In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 125?128, Ann Ar-
bor, Michigan, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In ACL 2007, Demonstration Session,
Prague, Czech Republic.
Hai Son Le, Ilya Oparin, Abdelkhalek Messaoudi,
Alexandre Allauzen, Jean-Luc Gauvain, and
Franc?ois Yvon. 2011. Large vocabulary soul neural
network language models. In INTERSPEECH 2011,
12th Annual Conference of the International Speech
Communication Association, Florence, Italy.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine Learning, ICML ?07, pages 641?648,
New York, NY, USA. ACM.
Andriy Mnih. 2010. Learning distributed representa-
tions for statistical language modelling and collabo-
rative filtering. Ph.D. thesis, University of Toronto,
Toronto, Ont., Canada. AAINR73159.
Masami Nakamura, Katsuteru Maruyama, Takeshi
Kawabata, and Kiyohiro Shikano. 1990. Neural
network approach to word category prediction for
english texts. In Proceedings of the 13th conference
on Computational linguistics - Volume 3, COLING
?90, pages 213?218, Stroudsburg, PA, USA.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, StatMT ?08, pages 18?25,
Stroudsburg, PA, USA.
Jan Niehues and Alex Waibel. 2012. Continuous
Space Language Models using Restricted Boltz-
mann Machines. In Proceedings of the International
Workshop for Spoken Language Translation (IWSLT
2012), Hong Kong.
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Proceedings of the Seventh International Work-
shop on Spoken Language Translation (IWSLT),
Paris, France.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Hasim Sak, Murat Saraclar, and Tunga Gungor. 2010.
Morphology-based and sub-word language model-
ing for Turkish speech recognition. In 2010 IEEE
International Conference on Acoustics Speech and
Signal Processing (ICASSP), pages 5402?5405.
Ruhi Sarikaya and Yonggang Deng. 2007. Joint
Morphological-Lexical Language Modeling for Ma-
chine Translation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
145?148, Rochester, New York, USA.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Holger Schwenk and Jean-Luc Gauvain. 2005. Train-
ing neural network language models on very large
corpora. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages
201?208, Stroudsburg, PA, USA.
38
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous Space Language mModels
for Statistical Machine T ranslation. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, COLING-ACL ?06, pages 723?730, Strouds-
burg, PA, USA.
Holger Schwenk. 2007. Continuous Space Language
Models. Comput. Speech Lang., 21(3):492?518,
July.
M. Ali Basha Shaik, Amr El-Desoky Mousa, Ralf
Schlu?ter, and Hermann Ney. 2011. Hybrid language
models using mixed types of sub-lexical units for
open vocabulary german lvcsr. In INTERSPEECH
2011, 12th Annual Conference of the International
Speech Communication Association, Florence, Italy.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In 7th International
Conference on Spoken Language Processing, IC-
SLP2002/INTERSPEECH 2002, Denver, Colorado,
USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-driven Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI, USA.
Wei Xu and Alex Rudnicky. 2000. Can artificial neural
networks learn language models? In Sixth Interna-
tional Conference on Spoken Language Processing,
ICSLP 2000 / INTERSPEECH 2000, Beijing, China,
pages 202?205. ISCA.
39
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84?89,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The KIT-LIMSI Translation System for WMT 2014
?
Quoc Khanh Do,
?
Teresa Herrmann,
??
Jan Niehues,
?
Alexandre Allauzen,
?
Franc?ois Yvon and
?
Alex Waibel
?
LIMSI-CNRS, Orsay, France
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
surname@limsi.fr
?
firstname.surname@kit.edu
Abstract
This paper describes the joined submis-
sion of LIMSI and KIT to the Shared
Translation Task for the German-to-
English direction. The system consists
of a phrase-based translation system us-
ing a pre-reordering approach. The base-
line system already includes several mod-
els like conventional language models on
different word factors and a discriminative
word lexicon. This system is used to gen-
erate a k-best list. In a second step, the
list is reranked using SOUL language and
translation models (Le et al., 2011).
Originally, SOUL translation models were
applied to n-gram-based translation sys-
tems that use tuples as translation units
instead of phrase pairs. In this article,
we describe their integration into the KIT
phrase-based system. Experimental re-
sults show that their use can yield sig-
nificant improvements in terms of BLEU
score.
1 Introduction
This paper describes the KIT-LIMSI system for
the Shared Task of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation. The sys-
tem participates in the German-to-English trans-
lation task. It consists of two main components.
First, a k-best list is generated using a phrase-
based machine translation system. This system
will be described in Section 2. Afterwards, the k-
best list is reranked using SOUL (Structured OUt-
put Layer) models. Thereby, a neural network lan-
guage model (Le et al., 2011), as well as several
translation models (Le et al., 2012a) are used. A
detailed description of these models can be found
in Section 3. While the translation system uses
phrase pairs, the SOUL translation model uses tu-
ples as described in the n-gram approach (Mari?no
et al., 2006). We describe the integration of the
SOUL models into the translation system in Sec-
tion 3.2. Section 4 summarizes the experimen-
tal results and compares two different tuning al-
gorithms: Minimum Error Rate Training (Och,
2003) and k-best Batch Margin Infused Relaxed
Algorithm (Cherry and Foster, 2012).
2 Baseline system
The KIT translation system is an in-house imple-
mentation of the phrase-based approach and in-
cludes a pre-ordering step. This system is fully
described in Vogel (2003).
To train translation models, the provided Eu-
roparl, NC and Common Crawl parallel corpora
are used. The target side of those parallel corpora,
the News Shuffle corpus and the GigaWord cor-
pus are used as monolingual training data for the
different language models. Optimization is done
with Minimum Error Rate Training as described
in Venugopal et al. (2005), using newstest2012
and newstest2013 as development and test data,
respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side (German) of the
corpus before training. Since the web-crawled
Common Crawl corpus is noisy, this corpus is
first filtered using an SVM classifier as described
in Mediani et al. (2011).
The word alignment is generated using the
GIZA++ Toolkit (Och and Ney, 2003). Phrase
extraction and scoring is done using the Moses
toolkit (Koehn et al., 2007). Phrase pair proba-
bilities are computed using modified Kneser-Ney
smoothing (Foster et al., 2006).
We apply short-range reorderings (Rottmann
and Vogel, 2007) and long-range reorder-
ings (Niehues and Kolss, 2009) based on part-of-
speech tags. The POS tags are generated using
the TreeTagger (Schmid, 1994). Rewriting rules
84
based on POS sequences are learnt automatically
to perform source sentence reordering according
to the target language word order. The long-range
reordering rules are further applied to the training
corpus to create reordering lattices to extract the
phrases for the translation model. In addition,
a tree-based reordering model (Herrmann et al.,
2013) trained on syntactic parse trees (Rafferty
and Manning, 2008; Klein and Manning, 2003)
is applied to the source sentence. In addition
to these pre-reordering models, a lexicalized
reordering model (Koehn et al., 2005) is applied
during decoding.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) using modified Kneser-Ney
smoothing (Chen and Goodman, 1996). The sys-
tem uses a 4-gram word-based language model
trained on all monolingual data and an additional
language model trained on automatically selected
data (Moore and Lewis, 2010). The system fur-
ther applies a language model based on 1000 auto-
matically learned word classes using the MKCLS
algorithm (Och, 1999). In addition, a bilingual
language model (Niehues et al., 2011) is used as
well as a discriminative word lexicon (DWL) us-
ing source context to guide the word choices in the
target sentence.
3 SOUL models for statistical machine
translation
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al., 2003; Schwenk,
2007) as a potential means to improve discrete
language models. The SOUL model (Le et al.,
2011) is a specific neural network architecture that
allows us to estimate n-gram models using large
vocabularies, thereby making the training of large
neural network models feasible both for target lan-
guage models and translation models (Le et al.,
2012a).
3.1 SOUL translation models
While the integration of SOUL target language
models is straightforward, SOUL translation mod-
els rely on a specific decomposition of the joint
probability P (s, t) of a sentence pair, where s is a
sequence of I reordered source words (s
1
, ..., s
I
)
1
1
In the context of the n-gram translation model, (s, t) thus
denotes an aligned sentence pair, where the source words are
reordered.
and t contains J target words (t
1
, ..., t
J
). In the
n-gram approach (Mari?no et al., 2006; Crego et
al., 2011), this segmentation is a by-product of
source reordering, and ultimately derives from ini-
tial word and phrase alignments. In this frame-
work, the basic translation units are tuples, which
are analogous to phrase pairs, and represent a
matching u = (s, t) between a source phrase s
and a target phrase t.
Using the n-gram assumption, the joint proba-
bility of a segmented sentence pair using L tupels
decomposes as:
P (s, t) =
L
?
i=1
P (u
i
|u
i?1
, ..., u
i?n+1
) (1)
A first issue with this decomposition is that the
elementary units are bilingual pairs. Therefore,
the underlying vocabulary and hence the number
of parameters can be quite large, even for small
translation tasks. Due to data sparsity issues, such
models are bound to face severe estimation prob-
lems. Another problem with Equation (1) is that
the source and target sides play symmetric roles,
whereas the source side is known, and the tar-
get side must be predicted. To overcome some
of these issues, the n-gram probability in Equa-
tion (1) can be factored by first decomposing tu-
ples in two (source and target) parts, and then de-
composing the source and target parts at the word
level.
Let s
k
i
denote the k
th
word of source part of the
tuple s
i
. Let us consider the example of Figure 1,
s
1
11
corresponds to the source word nobel, s
4
11
to
the source word paix, and similarly t
2
11
is the tar-
get word peace. We finally define h
n?1
(t
k
i
) as the
sequence of the n?1 words preceding t
k
i
in the tar-
get sentence, and h
n?1
(s
k
i
) as the n?1 words pre-
ceding s
k
i
in the reordered source sentence: in Fig-
ure 1, h
3
(t
2
11
) thus refers to the three word context
receive the nobel associated with the target word
peace. Using these notations, Equation 1 can be
rewritten as:
P (s, t) =
L
?
i=1
[
|t
i
|
?
k=1
P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
?
|s
i
|
?
k=1
P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
]
(2)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
85
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
s :   .... 
t :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org)
French sentence appears at the top of the figure, just above the reordered source s and the target t. The
pair (s, t) decomposes into a sequence of L bilingual units (tuples) u
1
, ..., u
L
. Each tuple u
i
contains a
source and a target phrase: s
i
and t
i
.
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one
for each language; however, the moves of these
windows remain synchronized by the tuple seg-
mentation. Moreover, the context is not limited
to the current phrase, and continues to include
words in adjacent phrases. Equation (2) involves
two terms that will be further denoted as TrgSrc
and Src, respectively P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
and P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
. It is worth notic-
ing that the joint probability of a sentence pair
can also be decomposed by considering the fol-
lowing two terms: P
(
s
k
i
|h
n?1
(s
k
i
), h
n?1
(t
1
i+1
)
)
and P
(
t
k
i
|h
n?1
(s
1
i
), h
n?1
(t
k
i
)
)
. These two terms
will be further denoted by SrcTrg and Trg. There-
fore, adding SOUL translation models means that
4 scores are added to the phrase-based systems.
3.2 Integration
During the training step, the SOUL translation
models are trained as described in (Le et al.,
2012a). The main changes concern the inference
step. Given the computational cost of computing
n-gram probabilities with neural network models,
a solution is to resort to a two-pass approach: the
first pass uses a conventional system to produce
a k-best list (the k most likely hypotheses); in
the second pass, probabilities are computed by the
SOUL models for each hypothesis and added as
new features. Then the k-best list is reordered ac-
cording to a combination of all features including
these new features. In the following experiments,
we use 10-gram SOUL models to rescore 300-
best lists. Since the phrase-based system described
in Section 2 uses source reordering, the decoder
was modified in order to generate k-best lists that
contain necessary word alignment information be-
tween the reordered source sentence and its asso-
ciated target hypothesis. The goal is to recover
the information that is illustrated in Figure 1 and
to apply the n-gram decomposition of a sentence
pair.
These (target and bilingual) neural network
models produce scores for each hypothesis in the
k-best list; these new features, along with the fea-
tures from the baseline system, are then provided
to a new phase which runs the traditional Mini-
mum Error Rate Training (MERT ) (Och, 2003), or
a recently proposed k-best Batch Margin Infused
Relaxed Algorithm (KBMIRA ) (Cherry and Fos-
ter, 2012) for tuning purpose. The SOUL mod-
els used for this year?s evaluation are similar to
those described in Allauzen et al. (2013) and Le
et al. (2012b). However, since compared to these
evaluations less parallel data is available for the
German-to-English task, we use smaller vocabu-
laries of about 100K words.
4 Results
We evaluated the SOUL models on the German-
to-English translation task using two systems to
generate the k-best lists. The first system used
all models of the baseline system except the DWL
model and the other one used all models.
Table 1 summarizes experimental results in
terms of BLEU scores when the tuning is per-
formed using KBMIRA. As described in Section
3, the probability of a phrase pair can be decom-
posed into products of words? probabilities in 2
different ways: we can first estimate the probabil-
ity of words in the source phrase given the context,
and then the probability of the target phrase given
its associated source phrase and context words
(see Equation (2)); or inversely we can generate
the target side before the source side. The for-
mer proceeds by adding Src and TrgSrc scores as
86
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.30 27.42 26.43 27.85
Translation st 26.46 27.70 26.66 28.04
Translation ts 26.48 27.41 26.61 28.00
All Translation 26.50 27.86 26.70 28.08
All SOUL models 26.62 27.84 26.75 28.10
Table 1: Results using KBMIRA
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.18 27.09 26.44 27.54
Translation st 26.36 27.59 26.66 27.80
Translation ts 26.44 27.69 26.63 27.94
All Translation 26.53 27.65 26.69 27.99
All SOUL models 26.47 27.68 26.66 28.01
Table 2: Results using MERT. Results in bold correpond to the submitted system.
2 new features into the k-best list, and the latter by
adding Trg and SrcTrg scores. These 2 methods
correspond respectively to the Translation ts and
Translation st lines in the Table 1. The 4 trans-
lation models may also be added simultaneously
(All Translations). The first line gives baseline
results without SOUL models, while the Target
line shows results in adding only SOUL language
model. The last line (All SOUL models) shows
the results for adding all neural network models
into the baseline systems.
As evident in Table 1, using the SOUL trans-
lation models yields generally better results than
using the SOUL target language model, yielding
about 0.2 BLEU point differences on dev and test
sets. We can therefore assume that the SOUL
translation models provide richer information that,
to some extent, covers that contained in the neural
network language model. Indeed, these 4 trans-
lation models take into account not only lexi-
cal probabilities of translating target words given
source words (or in the inverse order), but also the
probabilities of generating words in the target side
(Trg model) as does a language model, with the
same context length over both source and target
sides. It is therefore not surprising that adding the
SOUL language model along with all translation
models (the last line in the table) does not give sig-
nificant improvement compared to the other con-
figurations. The different ways of using the SOUL
translation models perform very similarly.
Table 2 summarizes the results using MERT in-
stead of KBMIRA. We can observe that using KB-
MIRA results in 0.1 to 0.2 BLEU point improve-
ments compared to MERT. Moreover, this impact
becomes more important when more features are
considered (the last line when all 5 neural net-
work models are added into the baseline systems).
In short, the use of neural network models yields
up to 0.6 BLEU improvement on the DWL sys-
tem, and a 0.8 BLEU gain on the system without
DWL. Unfortunately, the experiments with KB-
MIRA were carried out after the the submission
date. Therefore the submitted system corresponds
to the last line of table 2 indicated in bold.
5 Conclusion
We presented a system with two main features: a
phrase-based translation system which uses pre-
reordering and the integration of SOUL target lan-
guage and translation models. Although the trans-
lation performance of the baseline system is al-
ready very competitive, the rescoring by SOUL
models improve the performance significantly. In
the rescoring step, we used a continuous language
model as well as four continuous translation mod-
87
els. When combining the different SOUL models,
the translation models are observed to be more im-
portant in increasing the translation performance
than the language model. Moreover, we observe a
slight benefit to use KBMIRA instead of the stan-
dard MERT tuning algorithm. It is worth noticing
that using KBMIRA improves the performance
but also reduces the variance of the final results.
As future work, the integration of the SOUL
translation models could be improved in differ-
ent ways. For SOUL translation models, there
is a mismatch between translation units used dur-
ing the training step and those used by the de-
coder. The former are derived using the n-gram-
based approach, while the latter use the conven-
tional phrase extraction heuristic. We assume that
reducing this mismatch could improve the overall
performance. This can be achieved for instance
using forced decoding to infer a segmentation of
the training data into translation units. Then the
SOUL translation models can be trained using
this segmentation. For the SOUL target language
model, in these experiments we only used the En-
glish part of the parallel data for training. Results
may be improved by including all the monolingual
data.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658 as well as the French Ar-
maments Procurement Agency (DGA) under the
RAPID Rapmat project.
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-Son Le, and Franc?ois Yvon. 2013.
Limsi@ wmt13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 60?
67.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proceedings of the 34th Annual Meeting on Associa-
tion for Computational Linguistics (ACL ?96), pages
310?318, Santa Cruz, California, USA.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Josep M. Crego, Franois Yvon, and Jos B. Mari?no.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In EMNLP, pages 53?61.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, Prague, Czech Republic.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. pages 39?48, Montr?eal, Canada,
June. Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. Limsi@ wmt?12. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 330?337. Association for
Computational Linguistics.
88
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518, July.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluating Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
89
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130?135,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2014
Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha,
Jan Niehues, Isabel Slawik, Yuqi Zhang and Alex Waibel
Institute for Anthropomatics and Robotics
KIT - Karlsruhe Institute of Technology
firstname.lastname@kit.edu
Abstract
In this paper, we present the KIT
systems participating in the Shared
Translation Task translating between
English?German and English?French.
All translations are generated using
phrase-based translation systems, using
different kinds of word-based, part-of-
speech-based and cluster-based language
models trained on the provided data.
Additional models include bilingual lan-
guage models, reordering models based
on part-of-speech tags and syntactic parse
trees, as well as a lexicalized reordering
model. In order to make use of noisy
web-crawled data, we apply filtering
and data selection methods for language
modeling. A discriminative word lexicon
using source context information proved
beneficial for all translation directions.
1 Introduction
We describe the KIT systems for the Shared Trans-
lation Task of the ACL 2014 Ninth Workshop on
Statistical Machine Translation. We participated
in the English?German and English?French
translation directions, using a phrase-based de-
coder with lattice input.
The paper is organized as follows: the next sec-
tion describes the data used for each translation
direction. Section 3 gives a detailed description of
our systems including all the models. The trans-
lation results for all directions are presented after-
wards and we close with a conclusion.
2 Data
We utilize the provided EPPS, NC and Common
Crawl parallel corpora for English?German and
German?English, plus Giga for English?French
and French?English. The monolingual part
of those parallel corpora, the News Shuffle
corpus for all four directions and additionally
the Gigaword corpus for English?French and
German?English are used as monolingual train-
ing data for the different language models. For
optimizing the system parameters, newstest2012
and newstest2013 are used as development and
test data respectively.
3 System Description
Before training we perform a common preprocess-
ing of the raw data, which includes removing long
sentences and sentences with a length mismatch
exceeding a certain threshold. Afterwards, we nor-
malize special symbols, dates, and numbers. Then
we perform smart-casing of the first letter of every
sentence. Compound splitting (Koehn and Knight,
2003) is performed on the source side of the cor-
pus for German?English translation. In order to
improve the quality of the web-crawled Common
Crawl corpus, we filter out noisy sentence pairs us-
ing an SVM classifier for all four translation tasks
as described in Mediani et al. (2011).
Unless stated otherwise, we use 4-gram lan-
guage models (LM) with modified Kneser-Ney
smoothing, trained with the SRILM toolkit (Stol-
cke, 2002). All translations are generated by
an in-house phrase-based translation system (Vo-
gel, 2003), and we use Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) for optimization. The word align-
ment of the parallel corpora is generated using
the GIZA++ Toolkit (Och and Ney, 2003) for
both directions. Afterwards, the alignments are
combined using the grow-diag-final-and heuris-
tic. For English?German, we use discrimi-
native word alignment trained on hand-aligned
data as described in Niehues and Vogel (2008).
The phrase table (PT) is built using the Moses
toolkit (Koehn et al., 2007). The phrase scoring
for the small data sets (German?English) is also
130
done by the Moses toolkit, whereas the bigger sets
(French?English) are scored by our in-house par-
allel phrase scorer (Mediani et al., 2012a). The
phrase pair probabilities are computed using mod-
ified Kneser-Ney smoothing as described in Foster
et al. (2006).
Since German is a highly inflected language,
we try to alleviate the out-of-vocabulary prob-
lem through quasi-morphological operations that
change the lexical entry of a known word form to
an unknown word form as described in Niehues
and Waibel (2011).
3.1 Word Reordering Models
We apply automatically learned reordering rules
based on part-of-speech (POS) sequences and syn-
tactic parse tree constituents to perform source
sentence reordering according to the target lan-
guage word order. The rules are learned
from a parallel corpus with POS tags (Schmid,
1994) for the source side and a word align-
ment to learn reordering rules that cover short
range (Rottmann and Vogel, 2007) and long
range reorderings (Niehues and Kolss, 2009).
In addition, we apply a tree-based reordering
model (Herrmann et al., 2013) to better address
the differences in word order between German and
English. Here, a word alignment and syntactic
parse trees (Rafferty and Manning, 2008; Klein
and Manning, 2003) for the source side of the
training corpus are required to learn rules on how
to reorder the constituents in the source sentence.
The POS-based and tree-based reordering rules
are applied to each input sentence before transla-
tion. The resulting reordered sentence variants as
well as the original sentence are encoded in a re-
ordering lattice. The lattice, which also includes
the original position of each word, is used as input
to the decoder.
In order to acquire phrase pairs matching the
reordered sentence variants, we perform lattice
phrase extraction (LPE) on the training corpus
where phrase are extracted from the reordered
word lattices instead of the original sentences.
In addition, we use a lexicalized reordering
model (Koehn et al., 2005) which stores reorder-
ing probabilities for each phrase pair. During
decoding the lexicalized reordering model deter-
mines the reordering orientation of each phrase
pair at the phrase boundaries. The probability for
the respective orientation with respect to the orig-
inal position of the words is included as an addi-
tional score in the log-linear model of the transla-
tion system.
3.2 Adaptation
In the French?English and English?French sys-
tems, we perform adaptation for translation mod-
els as well as for language models. The EPPS and
NC corpora are used as in-domain data for the di-
rection English?French, while NC corpus is the
in-domain data for French?English.
Two phrase tables are built: one is the out-
of-domain phrase table, which is trained on all
corpora; the other is the in-domain phrase table,
which is trained on in-domain data. We adapt the
translation model by using the scores from the two
phrase tables with the backoff approach described
in Niehues and Waibel (2012). This results in a
phrase table with six scores, the four scores from
the general phrase table as well as the two condi-
tional probabilities from the in-domain phrase ta-
ble. In addition, we take the union of the candidate
phrase pairs collected from both phrase tables A
detailed description of the union method can be
found in Mediani et al. (2012b).
The language model is adapted by log-linearly
combining the general language model and an in-
domain language model. We train a separate lan-
guage model using only the in-domain data. Then
it is used as an additional language model during
decoding. Optimal weights are set during tuning
by MERT.
3.3 Special Language Models
In addition to word-based language models, we
use different types of non-word language models
for each of the systems. With the help of a bilin-
gual language model (Niehues et al., 2011) we
are able to increase the bilingual context between
source and target words beyond phrase bound-
aries. This language model is trained on bilin-
gual tokens created from a target word and all its
aligned source words. The tokens are ordered ac-
cording to the target language word order.
Furthermore, we use language models based
on fine-grained part-of-speech tags (Schmid and
Laws, 2008) as well as word classes to allevi-
ate the sparsity problem for surface words. The
word classes are automatically learned by clus-
tering the words of the corpus using the MKCLS
algorithm (Och, 1999). These n-gram language
models are trained on the target language corpus,
131
where the words have been replaced either by their
corresponding POS tag or cluster ID. During de-
coding, these language models are used as addi-
tional models in the log-linear combination.
The data selection language model is trained
on data automatically selected using cross-entropy
differences between development sets from pre-
vious WMT workshops and the noisy crawled
data (Moore and Lewis, 2010). We selected the
top 10M sentences to train this language model.
3.4 Discriminative Word Lexicon
A discriminative word lexicon (DWL) models the
probability of a target word appearing in the trans-
lation given the words of the source sentence.
DWLs were first introduced by Mauser et al.
(2009). For every target word, they train a maxi-
mum entropy model to determine whether this tar-
get word should be in the translated sentence or
not using one feature per source word.
We use two simplifications of this model that
have shown beneficial to translation quality and
training time in the past (Mediani et al., 2011).
Firstly, we calculate the score for every phrase pair
before translating. Secondly, we restrict the nega-
tive training examples to words that occur within
matching phrase pairs.
In this evaluation, we extended the DWL
with n-gram source context features proposed
by Niehues and Waibel (2013). Instead of rep-
resenting the source sentence as a bag-of-words,
we model it as a bag-of-n-grams. This allows us
to include information about source word order in
the model. We used one feature per n-gram up to
the order of three and applied count filtering for
bigrams and trigrams.
4 Results
This section presents the participating systems
used for the submissions in the four translation
directions of the evaluation. We describe the in-
dividual components that form part of each of
the systems and report the translation qualities
achieved during system development. The scores
are reported in case-sensitive BLEU (Papineni et
al., 2002).
4.1 English-French
The development of our English?French system
is shown in Table 1.
It is noteworthy that, for this direction, we chose
to tune on a subset of 1,000 pairs from news-
test2012, due to the long time the whole set takes
to be decoded. In a preliminary set of experiments
(not reported here), we found no significant differ-
ences between tuning on the small or the big devel-
opment sets. The translation model of the baseline
system is trained on the whole parallel data after
filtering (EPPS, NC, Common Crawl, Giga). The
same data was also used for language modeling.
We also use POS-based reordering.
The biggest improvement was due to using two
additional language models. One consists of a log-
linear interpolation of individual language models
trained on the target side of the parallel data, the
News shuffle, Gigaword and NC corpora. In ad-
dition, an in-domain language model trained only
on NC data is used. This improves the score by
more than 1.4 points. Adaptation of the translation
model towards a smaller model trained on EPPS
and NC brings an additional 0.3 points.
Another 0.3 BLEU points could be gained by
using other special language models: a bilingual
language model together with a 4-gram cluster
language model (trained on all monolingual data
using the MKCLS tool and 500 clusters). Incor-
porating a lexicalized reordering model into the
system had a very noticeable effect on test namely
more than half a BLEU point.
Finally, using a discriminative word lexicon
with source context has a very small positive ef-
fect on the test score, however more than 0.3 on
dev. This final configuration was the basis of our
submitted official translation.
System Dev Test
Baseline 15.63 27.61
+ Big LMs 16.56 29.02
+ PT Adaptation 16.77 29.32
+ Bilingual + Cluster LM 16.87 29.64
+ Lexicalized Reordering 16.92 30.17
+ Source DWL 17.28 30.19
Table 1: Experiments for English?French
4.2 French-English
Several experiments were conducted for the
French?English translation system. They are
summarized in Table 2.
The baseline system is essentially a phrase-
based translation system with some preprocess-
132
ing steps on the source side and utilizing the
short-range POS-based reordering on all parallel
data and fine-grained monolingual corpora such as
EPPS and NC.
Adapting the translation model using a small in-
domain phrase table trained on NC data only helps
us gain more than 0.4 BLEU points.
Using non-word language models including a
bilingual language model and a 4-gram 50-cluster
language model trained on the whole parallel data
attains 0.24 BLEU points on the test set.
Lexicalized reordering improves our system on
the development set by 0.3 BLEU points but has
less effect on the test set with a minor improve-
ment of around 0.1 BLEU points.
We achieve our best system, which is used for
the evaluation, by adding a DWL with source con-
text yielding 31.54 BLEU points on the test set.
System Dev Test
Baseline 30.16 30.70
+ LM Adaptation 30.58 30.94
+ PT Adaptation 30.69 31.14
+ Bilingual + Cluster LM 30.85 31.38
+ Lexicalized Reordering 31.14 31.46
+ Source DWL 31.19 31.54
Table 2: Experiments for French?English
4.3 English-German
Table 3 presents how the English-German transla-
tion system is improved step by step.
In the baseline system, we used parallel data
which consists of the EPPS and NC corpora. The
phrase table is built using discriminative word
alignment. For word reordering, we use word lat-
tices with long range reordering rules. Five lan-
guage models are used in the baseline system; two
word-based language models, a bilingual language
model, and two 9-gram POS-based language mod-
els. The two word-based language models use 4-
gram context and are trained on the parallel data
and the filtered Common Crawl data separately,
while the bilingual language model is built only
on the Common Crawl corpus. The two POS-
based language models are also based on the paral-
lel data and the filtered crawled data, respectively.
When using a 9-gram cluster language model,
we get a slight improvement. The cluster is trained
with 1,000 classes using EPPS, NC, and Common
Crawl data.
We use the filtered crawled data in addition to
the parallel data in order to build the phrase table;
this gave us 1 BLEU point of improvement.
The system is improved by 0.1 BLEU points
when we use lattice phrase extraction along with
lexicalized reordering rules.
Tree-based reordering rules improved the sys-
tem performance further by another 0.1 BLEU
points.
By reducing the context of the two POS-based
language models from 9-grams to 5-grams and
shortening the context of the language model
trained on word classes to 4-grams, the score on
the development set hardly changes but we can see
a slightly improvement for the test case.
Finally, we use the DWL with source context
and build a big bilingual language model using
both the crawled and parallel data. By doing so,
we improved the translation performance by an-
other 0.3 BLEU points. This system was used for
the translation of the official test set.
System Dev Test
Baseline 16.64 18.60
+ Cluster LM 16.76 18.66
+ Common Crawl Data 17.27 19.66
+ LPE + Lexicalized Reordering 17.45 19.75
+ Tree Rules 17.53 19.85
+ Shorter n-grams 17.55 19.92
+ Source DWL + Big BiLM 17.82 20.21
Table 3: Experiments for English?German
4.4 German-English
Table 4 shows the development steps of the
German-English translation system.
For the baseline system, the training data of the
translation model consists of EPPS, NC and the
filtered parallel crawled data. The phrase table
is built using GIZA++ word alignment and lattice
phrase extraction. All language models are trained
with SRILM and scored in the decoding process
with KenLM (Heafield, 2011). We use word lat-
tices generated by short and long range reordering
rules as input to the decoder. In addition, a bilin-
gual language model and a target language model
trained on word clusters with 1,000 classes are in-
cluded in the system.
Enhancing the word reordering with tree-based
reordering rules and a lexicalized reordering
133
model improved the system performance by 0.6
BLEU points.
Adding a language model trained on selected
data from the monolingual corpora gave another
small improvement.
The DWL with source context increased the
score on the test set by another 0.5 BLEU points
and applying morphological operations to un-
known words reduced the out-of-vocabulary rate,
even though no improvement in BLEU can be ob-
served. This system was used to generate the
translation submitted to the evaluation.
System Dev Test
Baseline 24.40 26.34
+ Tree Rules 24.71 26.86
+ Lexicalized Reordering 24.89 26.93
+ LM Data Selection 24.96 27.03
+ Source DWL 25.32 27.53
+ Morphological Operations - 27.53
Table 4: Experiments for German?English
5 Conclusion
In this paper, we have described the systems
developed for our participation in the Shared
Translation Task of the WMT 2014 evaluation
for English?German and English?French. All
translations were generated using a phrase-based
translation system which was extended by addi-
tional models such as bilingual and fine-grained
part-of-speech language models. Discriminative
word lexica with source context proved beneficial
in all four language directions.
For English-French translation using a smaller
development set performed reasonably well and
reduced development time. The most noticeable
gain comes from log-linear interpolation of multi-
ple language models.
Due to the large amounts and diversity of
the data available for French-English, adapta-
tion methods and non-word language models con-
tribute the major improvements to the system.
For English-German translation, the crawled
data and a DWL using source context to guide
word choice brought most of the improvements.
Enhanced word reordering models, namely
tree-based reordering rules and a lexicalized re-
ordering model as well as the source-side fea-
tures for the discriminative word lexicon helped
improve the system performance for German-
English translation.
In average we achieved an improvement of over
1.5 BLEU over the respective baselines for all our
systems.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658.
References
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proceedings of the 2006 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), Sydney, Australia.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, Scotland, United Kingdom.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), Sapporo, Japan.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of the Eleventh Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), Budapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the Second International Workshop
on Spoken Language Translation (IWSLT 2005),
Pittsburgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL
2007), Demonstration Session, Prague, Czech Re-
public.
134
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Suntec, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight International
Workshop on Spoken Language Translation (IWSLT
2011), San Francisco, CA, USA.
Mohammed Mediani, Jan Niehues, and Alex Waibel.
2012a. Parallel Phrase Scoring for Extra-large Cor-
pora. In The Prague Bulletin of Mathematical Lin-
guistics, number 98.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunach Cho, Teresa Herrmann, Rainer
K?argel, and Alexander Waibel. 2012b. The KIT
Translation Systems for IWSLT 2012. In Proceed-
ings of the Ninth International Workshop on Spoken
Language Translation (IWSLT 2012), Hong Kong,
HK.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In Proceedings
of the ACL 2010 Conference Short Papers, Uppsala,
Sweden.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation (WMT 2008), Columbus, OH,
USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In
Proceedings of the Eight International Workshop on
Spoken Language Translation (IWSLT 2008), San
Francisco, CA, USA.
J. Niehues and A. Waibel. 2012. Detailed Analysis of
Different Strategies for Phrase Table Adaptation in
SMT. In Proceedings of the Tenth Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA 2012), San Diego, CA, USA.
J. Niehues and A. Waibel. 2013. An MT Error-Driven
Discriminative Word Lexicon using Sentence Struc-
ture Features. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, Scotland, United King-
dom.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for Deter-
mining Bilingual Word Classes. In Proceedings of
the Ninth Conference of the European Chapter of the
Association for Computational Linguistics (EACL
1999), Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German, Columbus, OH,
USA.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of the
11th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
2007), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In In-
ternational Conference on Computational Linguis-
tics (COLING 2008), Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
135
