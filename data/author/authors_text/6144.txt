Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 953?960
Manchester, August 2008
Using Syntactic Information for Improving Why-Question Answering
Suzan Verberne, Lou Boves, Nelleke Oostdijk and Peter-Arno Coppen
Department of Linguistics
Radboud University Nijmegen
s.verberne@let.ru.nl
Abstract
In this paper, we extend an existing para-
graph retrieval approach to why-question
answering. The starting-point is a system
that retrieves a relevant answer for 73%
of the test questions. However, in 41%
of these cases, the highest ranked relevant
answer is not ranked in the top-10. We
aim to improve the ranking by adding a re-
ranking module. For re-ranking we con-
sider 31 features pertaining to the syntactic
structure of the question and the candidate
answer. We find a significant improvement
over the baseline for both success@10 and
MRR@150. The most important features
for re-ranking are the baseline score, the
presence of cue words, the question?s main
verb, and the relation between question fo-
cus and document title.
1 Introduction
Recently, some research has been directed at prob-
lems involved in why-question answering (why-
QA). About 5% of all questions asked to QA
systems are why-questions (Hovy et al, 2002).
They need a different approach from factoid ques-
tions, since their answers cannot be stated in a sin-
gle phrase. Instead, a passage retrieval approach
seems more suitable. In (Verberne et al, 2008),
we proposed an approach to why-QA that is based
on paragraph retrieval. We reported mediocre per-
formance and suggested that adding linguistic in-
formation may improve ranking power.
c
?Suzan Verberne, 2008. Licensed under the Creative
Commons Attribution-Noncommercial-Share Alike 3.0 Un-
ported license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
In the present paper, we implement a simi-
lar paragraph retrieval approach and extend it by
adding a re-ranking module based on structural lin-
guistic information. Our aim is to find out whether
syntactic knowledge is relevant for discovering re-
lations between question and answer, and if so,
which type of information is the most beneficial.
In the following sections, we first discuss related
work (section 2). In sections 3 and 4, we introduce
the data that we used for development purposes
and the baseline retrieval and ranking method that
we implemented. In section 5, we present our re-
ranking method and the results obtained, followed
by a discussion in section 6, and directions for fur-
ther research in section 7.
2 Related work
A substantial amount of work has been done in
improving QA by adding syntactic information
(Tiedemann, 2005; Quarteroni et al, 2007; Hi-
gashinaka and Isozaki, 2008). All these studies
show that syntactic information gives a small but
significant improvement on top of the traditional
bag-of-words (BOW) approaches.
The work of (Higashinaka and Isozaki, 2008)
focuses on the problem of ranking candidate an-
swer paragraphs for Japanese why-questions. They
find a success@10 score of 70.3% with an MRR
of 0.328. They conclude that their system for
Japanese is the best-performing fully implemented
why-QA system. In (Tiedemann, 2005), passage
retrieval for Dutch factoid QA is enriched with
syntactic information from dependency structures.
The baseline approach, using only the BOW, re-
sulted in an MRR of 0.342. With the addition of
syntactic structure, MRR improved to 0.406.
The work by (Quarteroni et al, 2007) consid-
ers the problem of answering definition questions.
953
They use predicate-argument structures (PAS) for
improved answer ranking. Their results show that
PAS make a very small contribution compared to
BOW only (F-scores 70.7% vs. 69.3%).
The contribution of this paper is twofold: (1) we
consider the relatively new problem of why-QA for
English and (2) we not only improve a simple pas-
sage retrieval approach by adding syntactic infor-
mation but we also perform extensive feature se-
lection in order to find out which syntactic features
contribute to answer ranking and to what extent.
3 Data
As data for developing and testing our system
for why-QA, we use the Webclopedia question set
by (Hovy et al, 2002). This set contains ques-
tions that were asked to the online QA system
answers.com. 805 of these questions are why-
questions. As answer corpus, we use the off-line
Wikipedia XML corpus, which consists of 659,388
articles (Denoyer and Gallinari, 2006). We manu-
ally inspect a sample of 400 of the Webclopedia
why-questions. Of these, 93 have an answer in the
Wikipedia corpus. Manual extraction of one rele-
vant answer for each of these questions results in a
set of 93 why-questions and their reference answer.
We also save the title of the Wikipedia article in
which each of the answers is embedded, in order
to be able to evaluate document retrieval together
with answer retrieval.
4 Paragraph retrieval for why-QA
4.1 Baseline method
We index the Wikipedia XML corpus using the
Wumpus Search Engine (Buttcher, 2007). In
Wumpus, queries can be formulated in the GCL
format, which is especially geared to retrieving
XML items. Since we consider paragraphs as re-
trieval units, we let the engine retrieve text frag-
ments marked with ?p? as candidate answers.
We implement a baseline method for question
analysis in which first stop words are removed1.
Also, any punctuation is removed from the ques-
tion. What remains is a set of question content
words. Next, we automatically create a query for
each question that retrieves paragraphs containing
(a subset of) these question terms. For ranking
1To this end the stop word list is used that can be found
at http://marlodge.supanet.com/museum/ funcword.html. We
use all categories except the numbers and the word why
the paragraphs retrieved, we use the QAP algo-
rithm created by MultiText, which has been im-
plemented in Wumpus. QAP is a passage scor-
ing algorithm specifically developed for QA tasks
(Buttcher et al, 2004). For each question, we re-
trieve and rank the top 150 of highest scoring an-
swer candidates.
4.2 Evaluation method
For evaluation of the results, we perform manual
assessment of all answers retrieved, starting at the
highest-ranked answer and ending as soon as we
encounter a relevant answer2. Then we count the
proportion of questions that have at least one rele-
vant answer in the top n of the results for n = 10
and n = 150, giving us success@10 and suc-
cess@150. For the highest ranked relevant answer
per question, we determine the reciprocal rank
(RR). If there is no relevant answer retrieved by
the system at n = 150, the RR is 0. Over all ques-
tions, we calculate the Mean RR (MRR@150).
We also measure the performance of our system
for document retrieval: the proportion of questions
for which at least one of the answers in the top 10
comes from the reference document (success@10
for document retrieval) and the MRR@150 for the
highest position of the reference document3.
4.3 Results and discussion
Table 1: Baseline results for the why passage retrieval sys-
tem for answer retrieval and document retrieval in terms of
success@10, success@150 and MRR@150
S@10 S@150 MRR@150
Answer retrieval 43.0% 73.1% 0.260
Document retrieval 61.8% 82.2% 0.365
There are two possible directions for improving
our system: (1) by improving retrieval and (2) by
improving ranking. Since success@150 is 73.1%,
for 68 of the 93 questions in our set at least one
relevant answer is retrieved in the top 150. For the
other 25 questions, the reference answer was not
included in the long list of 150 results.
In the present paper we focus on improving an-
swer ranking. The results show that for 30.1% of
2We don?t need to assess the tail since we are only in-
terested in the highest-ranked relevant answer for calculating
MRR
3Note that we consider as relevant all documents in which
a relevant answer is embedded. So the relevant document with
the highest rank is either the reference document or the doc-
ument in which the relevant answer with the highest rank is
embedded.
954
the questions4, a relevant answer is retrieved but
is not placed in the top 10 by the ranking algo-
rithm. For these 28 questions in our set, re-ranking
may be an option. Since re-ranking will not im-
prove the results for the questions for which there
is no relevant answer in the top-150, the maximum
success@10 that we can achieve by re-ranking is
73.1% for answer paragraphs and 82.8% for docu-
ments.
5 Answer re-ranking
Before we can decide on our re-ranking approach,
we take a closer look at the ranking method that is
applied in the baseline system. The QAP algorithm
includes the following variables: (1) term overlap
between query and passage, (2) passage length and
(3) total corpus frequency for each term (Buttcher
et al, 2004). Let us consider three example ques-
tions from our collection to see the strengths and
weaknesses of these variables.
1. Why do people sneeze?
2. Why do women live longer than men on average?
3. Why are mountain tops cold?
In (1), the corpus frequencies of the question
terms people and sneeze ensure that the relatively
unique term sneeze is weighted heavier for ranking
than the very common noun people. This matches
the goal of the query, which is finding an explana-
tion for sneezing. However, in (2), the frequency
variables used by QAP do not reflect the impor-
tance of the terms. Thus, women, live, longer and
average are considered to be of equal importance,
while obviously the latter term is only peripheral to
the goal of the query. This cannot be derived from
its corpus frequency, but may be inferred from its
syntactic function in the question: an adverbial on
sentence level. In (3), mountain and tops are in-
terpreted as two distinct terms by the baseline sys-
tem, whereas the interpretation of mountain tops
as compound item is more appropriate.
Examples 2 and 3 above show that a question-
answer pair may contain more information than
is represented by the frequency variables imple-
mented in the QAP algorithm. Our aim is to find
out which features from a question-answer pair
constitute the information that discloses a relation
between the question and its answer. Moreover, we
aim at weighting these features in such a way that
we can optimize ranking performance.
4
73.1%? 43.0%
5.1 Features for re-ranking
As explained above, baseline ranking is based on
term overlap. The features that we propose for
re-ranking are also based on term overlap, but in-
stead of considering all question content words in-
discriminately in one overlap function, we select a
subset of question terms for each of the re-ranking
features. By defining different subsets based on
syntactic functions and categories, we can investi-
gate which syntactic features of the question, and
which parts of the answer are most important for
re-ranking.
The following subsections list the syntactic fea-
tures that we consider. Each feature consists of two
item sets: a set of question items and a set of an-
swer items. The value that is assigned to a feature
is a function of the intersection between these two
sets. For a set of question items Q and a set of
answer items A, the proportion P of their intersec-
tion is:
P =
|Q ? A|+ |A ? Q|
|Q|+ |A|
(1)
Our approach to composing the set of features is
described in subsections 5.1.1 to 5.1.4 below. We
label the features using the letter f followed by a
number so that we can back-reference to them.
5.1.1 The syntactic structure of the question
Example 2 in the previous section shows that
some syntactic functions in the question may be
more important than other functions. Since we do
not know as yet which syntactic functions are the
most important, we include both heads (f1) and
modifiers (f2) as item sets. We also include the
four main syntactic constituents for why-questions:
subject (f4), main verb (f6), nominal predicate (f8)
and direct object (f10) to be matched against the
answer terms. For these features, we add a vari-
ant where as answer items only words/phrases with
the same syntactic function are included (f5, f7, f9,
f11).
Example 3 in the previous section exemplifies
the potential relevance of noun phrases (f3).
5.1.2 The semantic structure of the question
The features f12 to f15 come from earlier data
analyses that we performed. We saw that often
there is a link between a specific part of the ques-
tion and the title of the document in which the ref-
erence answer is found. For example, the answer
to the question ?Why did B.B. King name his gui-
tar Lucille?? is in the Wikipedia article with the ti-
955
tle B.B. King. The answer document and the ques-
tion apparently share the same topic (B.B. King).
In analogy to linguistically motivated approaches
to factoid QA (Ferret et al, 2002) we introduce the
term question focus for this topic.
The focus is often the syntactic subject of the
question. From our data, we found the follow-
ing two exceptions to this general rule: (1) If the
subject is semantically poor, the question focus is
the (verbal or nominal) predicate: ?Why do peo-
ple sneeze??, and (2) in case of etymology ques-
tions (which cover about 10% of why-questions),
the focus is the subject complement of the pas-
sive sentence: ?Why are chicken wings called
Buffalo Wings?? In all other cases, the question
focus is the grammatical subject: ?Why do cats
sleep so much??
We include a feature (f13) for matching words
from the question focus to words from the docu-
ment title. We also add a feature (f12) for the re-
lation between all question words and words from
the document title, and a feature (f14) for the rela-
tion between question focus words and all answer
words.
5.1.3 Synonyms
For each of the features f1 to f15, we add an
alternative feature (f16 to f30) covering the set of
all WordNet synonyms for all items in the origi-
nal feature. Note that the original words are no
longer included for these features; we only include
the terms from their synonym sets. For synonyms,
we apply a variant of equation 1 in which |Q ? A|
is interpreted as the number of question items that
have at least one synonym in the set of answer
items and |A ? Q| as the number of answer items
that occur in at least one of the synonym sets of the
question items.
5.1.4 Cue words
Finally, we add a closed set of cue words that
often occur in answers to why-questions5 (f31).
5.2 Extracting feature values from the data
For the majority of features we need the syntactic
structure of the input question, and for some of the
features also of the answer. We experimented with
two different parsers for these tasks: a develop-
5These cue words come from earlier work that we did on
the analysis of why-answers: because, since, therefore, why,
in order to, reason, reasons, due to, cause, caused, causing,
called, named
ment version of the Pelican parser6 and the EP4IR
dependency parser (Koster, 2003).
Given a question-answer pair and the parse trees
of both question and answer, we extract values
from each parser?s output for all features in sec-
tion 5.1 by means of a Perl script.
Our script has access to the following external
components: A stop word list (see section 4.1), a
fixed set of cue words, the CELEX Lemma lexi-
con (Burnage et al, 1990), all WordNet synonym
sets, and a list of pronouns and semantically poor
nouns7.
Given one question-answer pair, the feature
extraction script performs the following actions.
Based on the question?s parse tree, it extracts the
subject, main verb, direct object (if present) and
nominal predicate (if present) from the question.
The script decides on question focus using the
rules suggested in section 5.1.2. For the answer, it
extracts the document title. From the parse trees
created for the answer paragraph, it extracts all
subjects, all verbs, all direct objects, and all nomi-
nal predicates.
For each feature, the script composes the re-
quired sets of question items and answer items. All
items are lowercased and punctuation is removed.
In multi-word items, spaces are replaced by un-
derscores before stop words are removed from the
question and the answer. Then the script calculates
the proportion of the intersection of the two sets for
each feature following equation 18.
Whether or not to lemmatize the items before
matching them is open to debate. In the litera-
ture, there is some discussion on the benefit of
lemmatization for information extraction (Bilotti
et al, 2004). Lemmatization can be problematic
in the case of proper names (which are not always
recognizable by capitalization) and noun phrases
that are fixed expressions such as sailors of old.
Noun phrases are involved not only in the NP fea-
ture (f3), but also in our features involving sub-
ject, direct object, nominal predicate and question
focus. Therefore, we decided only to lemmatize
verbs (for features f6 and f7) in the current version
of our system.
For each question-answer pair in our data set,
we extract all feature values using our script. We
6The Pelican parser is a constituency parser that is cur-
rently being developed at Nijmegen University. See also
http://lands.let.ru.nl/projects/pelican/
7These are the nouns humans and people
8A multi-word term is counted as one item
956
use three different settings for feature extraction:
(1) feature extraction from gold standard con-
stituency parse trees of the questions in accordance
with the descriptive model of the Pelican parser9;
(2) feature extraction from the constituency parse
trees of the questions generated by Pelican10; and
(3) feature extraction from automatically gener-
ated dependency parse trees from EP4IR.
Our training and testing method using the ex-
tracted feature values is explained in the next sec-
tion.
5.3 Re-ranking method
As the starting point for re-ranking we run the
baseline system on the complete set of 93 ques-
tions and retrieve 150 candidate answers per ques-
tion, ranked by the QAP algorithm. As described
in section 5.2, we use two different parsers. Of
these, Pelican has a more detailed descriptive
model and gives better accuracy (see section 6.3 on
parser evaluation) but EP4IR is at present more ro-
bust for parsing long sentences and large amounts
of text. Therefore, we parse all answers (93 times
150 paragraphs) with EP4IR only. The questions
are parsed by both Pelican and EP4IR.
As presented in section 5.1, we have 31 re-
ranking features. To these, we add the score that
was assigned by QAP, which makes 32 features
in total. We aim to weight the feature values in
such a way that their contribution to the overall
system performance is optimal. We set each fea-
ture weight as an integer between 0 and 10, which
makes the number of possible weighting configu-
rations 1132. In order to choose the optimal con-
figuration from this huge set of possible configura-
tions, we use a genetic algorithm11 (Goldberg and
Holland, 1988). The variable that we optimize dur-
ing training is MRR. We tune the feature weights
over 100 generations of 1000 individuals. For eval-
uation, we apply cross valuation on five question
9Pelican aims at producing all possible parse trees for a
given sentence. A linguist can then decide on the correct parse
tree given the context. We created the gold standard for each
question by manually selecting the correct parse tree from the
parse trees generated by the parser.
10For this setting, we run the Pelican parser with the option
of only giving one parse (the most likely according to Pelican)
per question. As opposed to the gold standard setting, we do
not perform manual selection of the correct parse.
11We chose to work with a genetic algorithm because we
are mainly interested in feature selection and ranking. We
are currently experimenting with Support Vector Machines
(SVM) to see whether the results obtained from using the ge-
netic algorithm are good enough for reliable feature selection.
folds: in five turns, we train the feature weights on
four of the five folds and evaluate them on the fifth.
We use the feature values that come from the
gold standard parse trees for training the feature
weights, because the benefit of a syntactic item
type can only be proved if the extraction of that
item from the data is correct. At the testing stage,
we re-rank the 93 questions using all three fea-
ture extraction settings: feature values extracted
from gold standard parse trees, feature values ex-
tracted with Pelican and feature values extracted
with EP4IR. We again regard the distribution of
questions over the five folds: we re-rank the ques-
tions in fold five according to the weights found by
training on folds one to four.
5.4 Results from re-ranking
Table 2 on the next page shows the results for the
three feature extraction settings.
Using the Wilcoxon Signed-Rank Test we find
that all three re-ranking conditions give signifi-
cantly better results than the baseline (Z = ?1.91,
P = 0.0281 for paired reciprocal ranks). The dif-
ferences between the three re-ranking conditions
are, however, not significant12.
5.5 Which features made the improvement?
If we plot the weights that were chosen for the fea-
tures in the five folds, we see that for some features
very different weights were chosen in the different
folds. Apparently, for these features, the weight
values do not generalize over the five folds. In or-
der to only use reliable features, we only consider
features that get similar weights over all five folds:
their weight values have a standard deviation < 2
and an average weight > 0. We find that of the
32 features, 21 are reliable according to this def-
inition. Five of these features make a substantial
contribution to the re-ranking score (table 3). Be-
hind each feature is its reference number from sec-
tion 5.1 and its average weight on a scale of 0 to
10.
Moreover, there are three other features that to a
limited extent contribute to the overall score (table
4).
Thirteen other reliable features get a weight < 1.5
assigned during training and thereby slightly con-
tribute to the re-ranking score.
12The slightly lower success and MRR scores for re-
ranking with gold standard parse trees compared to Pelican
parse trees can be explained by the absence of the gold stan-
dard for one question in our set.
957
Table 2: Re-ranking results for three different parser settings in terms of success@10, success@150 and MRR@150.
Answer/paragraph retrieval Document retrieval
Version S@10 S@150 MRR S@10 S@150 MRR
Baseline 43.0% 73.1% 0.260 61.8% 82.8% 0.365
Re-ranking w/ gold standard parse trees 54.4% 73.1% 0.370 63.1% 82.8% 0.516
Re-ranking w/ Pelican parse trees 54.8% 73.1% 0.380 64.5% 82.8% 0.518
Re-ranking w/ EP4IR parse trees 53.8% 73.1% 0.349 63.4% 82.8% 0.493
Table 3: Features that substantially contribute to the re-
ranking score, with their average weight
Question focus synonyms to doctitle (f28) 9.2
Question verb synonyms to answer verbs (f22) 9
Cue words (f31) 9
QAP 8.8
Question focus to doctitle (f13) 7.8
Table 4: Features that to a limited extent contribute to the
re-ranking score, with their average weight
Question subject to answer subjects (f5) 2.2
Question nominal predicate synonyms (f23) 1.8
Question object synonyms to answer objects (f26) 1.8
6 Discussion
Our re-ranking method scores significantly better
than the baseline, with use of a small subset of
the 32 features. It reaches a success@10 score
of 54.8% with an MRR@150 of 0.380 for answer
retrieval. This compares to the MRR of 0.328
that Higashinaka and Isozaki found for why-QA
and the MRR of 0.406 that Tiedemann reaches
for syntactically enhanced factoid-QA (see sec-
tion 2), showing that our method performs reason-
able well. However, the MRR of 0.380 also shows
that a substantial part of the problem of why-QA is
still to be solved.
6.1 Error analysis
For analysis of our results, we counted for how
many questions the ranking was improved, and for
how many the ranking deteriorated. First of all,
ranking remained equal for 35 questions (37.6%).
25 of these are the questions for which no rele-
vant answer was retrieved by the baseline system
at n = 150 (26.9% of questions). For these ques-
tions the ranking obviously remained equal (RR is
0) after re-ranking. For the other 10 questions for
which ranking did not change, RR was 1 and re-
mained 1. Apparently, re-ranking does not affect
excellent rankings.
For two third (69%) of the remaining questions,
ranking improved and for one third (31%), it dete-
riorated. There are eleven questions for which the
reference answer was ranked in the top 10 by the
baseline system but it drops out of the top 10 by
re-ranking. On the other hand, there are 22 ques-
tions for which the reference answer enters the top
10 by re-ranking the answers, leading to an overall
improvement in success@10.
If we take a look at the eleven questions for
which the reference answer drops out of the top
10 by re-ranking, we see that these are all cases
where there is no lexical overlap between the ques-
tion focus and the document title. The importance
of features 13 and 28 in the re-ranking weights
works against the reference answer for these ques-
tions. Here are three examples (question focus as
detected by the feature extraction script is under-
lined):
1. Why do neutral atoms have the same number of protons
as electrons? (answer in ?Oxidation number?)
2. Why do flies walk on food? (answer in ?Insect Habitat?)
3. Why is Wisconsin called the Badger State? (answer in
?Wisconsin?)
In example 1, the reference answer is outranked
by answer paragraphs from documents with one of
the words neutral and atoms in its title. In example
2, there is actually a semantic relation between the
question focus (flies) and the document title (in-
sect); however, this relation is not synonymy but
hyperonymy and therefore not included in our re-
ranking features. One could dispute the definition
of question focus for etymology questions (exam-
ple 3), but there are simply more cases where the
subject complement of the question leads to doc-
ument title than cases where its subject (such as
Winsconsin) does.
6.2 Feature selection analysis
We think that the outcome of the feature selection
(section 5.5) is very interesting. We are not sur-
prised that the original score assigned by QAP is
still important in the re-ranking module: the fre-
quency variables apparently do provide useful in-
formation on the relevance of a candidate answer.
We also see that the presence of cue words
(f31) gives useful information in re-ranking an-
958
swer paragraphs. In fact, incorporating the pres-
ence of cue words is a first step towards recogniz-
ing that a paragraph is potentially an answer to a
why-question. We feel that identifying a paragraph
as a potential answer is the most salient problem
of why-QA, since answers cannot be recognized
by simple semantic-syntactic units such as named
entities as is the case for factoid QA. The current
results show that surface patterns (the literal pres-
ence of items from a fixed set of cue words) are a
first step in the direction of answer selection.
More interesting than the baseline score and cue
words are the high average weights assigned to
the features f13 and f28. These two features refer
to the relation between question focus and docu-
ment title. As explained in section 5.1.2, we al-
ready had the intuition that there is some relation
between the question focus of a why-question and
the document title. The high weights that are as-
signed to the question focus features show that our
procedure for extracting question focus is reliable.
The importance of question focus for why-QA is
especially interesting because it is a question fea-
ture that is specific to why-questions and does not
similarly apply to factoids or other question types.
Moreover, the link from the question focus to the
document title shows that Wikipedia as an answer
source can provide QA systems with more infor-
mation than a collection of plain texts without doc-
ument structure does.
From the other features discussed in section 5.5,
we learn that all four main question constituents
contribute to the re-ranking score, but that syn-
onyms of the main verb make the highest contri-
bution (f22). Subject (f5), object (f26) and nomi-
nal predicate (f23) make a lower contribution. We
suspect that this may be due to our decision to only
lemmatize verbs, and not nouns (see section 5.2).
It could be that since lemmatization leads to more
matches, a feature can make a higher contribution
if its items are lemmatized.
6.3 The quality of the syntactic descriptions
We already concluded in the previous section that
our feature extraction module is very well capable
of extracting the question focus, since f13 and f28
get assigned high weights by training. However,
in the training stage, we used gold standard parse
trees. In this section we evaluate the two automatic
syntactic parsers Pelican and EP4IR, in order to
be able to come up with fruitful suggestions for
improving our system in the future.
As a measure for parser evaluation, we con-
sider constituent extraction: how well do both
parsers perform in identifying and delimiting the
four main constituents from a why-question: sub-
ject, main verb, direct object and nominal pred-
icate? As the gold standard for this experiment
we use manually verified constituents that were
extracted from the gold standard parse trees. We
adapt our feature extraction script so that it prints
each of the four constituents per question. Then we
calculate the recall score for each parser for each
constituent type.
Recall is the number of correctly identified con-
stituents of a specific type divided by the total
number of constituents of this type in the goldstan-
dard parse tree. This total number is not exactly 93
for all constituent types: only 34 questions have a
direct object in their main clause and 31 questions
have a nominal predicate. The results of this exer-
cise are in Table 5.
Table 5: Recall for constituent extraction (in %)
subjs verbs objs preds all
Pelican 79.6 94.6 64.7 71.0 82.1
EP4IR 63.4 64.5 44.1 48.4 59.4
We find that over all constituent types, Peli-
can reaches significantly better recall scores than
EP4IR (Z = 5.57; P < 0.0001 using the
Wilcoxon Signed-Rank Test).
Although Pelican gives much better results on
constituent extraction than EP4IR, the results on
the re-ranking task do not differ significantly. The
most plausible explanation for this is that the high
accuracy of the Pelican parses is undone by the
poor syntactic analysis on the answer side, which
is in all settings performed by EP4IR.
7 Future directions
In section 4.3, we mentioned two directions for im-
proving our pipeline system: improving retrieval
and improving ranking. Recently we have been
working on optimizing the retrieval module of our
pipeline system by investigating the influence of
different retrieval modules and passage segmenta-
tion strategies on the retrieval performance. This
work has resulted in a better passage retrieval mod-
ule in terms of success@150. Details on these ex-
periments are in (Khalid and Verberne, 2008).
Moreover, we have been collecting a larger data
collection in order to do make feature selection for
959
our re-ranking experiments more reliable and less
depending on specific cases in our dataset. This
work has resulted in a total set of 188 why-question
answer pairs. We are currently using this data
collection for further research into improving our
pipeline system.
In the near future, we aim to investigate what
type of information is needed for further improv-
ing our system for why-QA. With the addition of
syntactic information our system reaches an MRR
score of 0.380. This compares to the MRR scores
reached by other syntactically enhanced QA sys-
tems (see section 2). However, an MRR of 0.380
also shows that a substantial part of the problem
of why-QA is still to be solved. We are currently
investigating what type information is needed for
further system improvement.
Finally, we also plan experiments with a number
of dependency parsers to be used instead of EP4IR
for the syntactic analysis of the answer para-
graphs. Current experiments with Charniak (Char-
niak, 2000) show better constituent extraction than
with EP4IR. It is still to be seen whether this also
influences the overall performance of our system.
8 Conclusion
We added a re-ranking step to an existing para-
graph retrieval method for why-QA. For re-
ranking, we took the score assigned to a question
answer pair by the ranking algorithm QAP in the
baseline system, and weighted it with a number of
syntactic features. We experimented with 31 fea-
tures and trained the feature weights on a set of 93
why-questions with 150 answers provided by the
baseline system for each question. Feature values
for training the weights for the 31 features were
extracted from gold standard parse trees for each
question answer pair.
We evaluated the feature weights on automat-
ically parsed questions and answers, in five folds.
We found a significant improvement over the base-
line for both success@10 and MRR@150. The
most important features were the baseline score,
the presence of cue words, the question?s main
verb, and the relation between question focus and
document title.
We think that, although syntactic information
gives a significant improvement over baseline pas-
sage ranking, more improvement is still to be
gained from other types of information. Investi-
gating the type of information needed is part of our
future directions.
References
Bilotti, M.W., B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or mor-
phological query expansion. Proc. IR4QA at SIGIR
2004.
Burnage, G., R.H. Baayen, R. Piepenbrock, and H. van
Rijn. 1990. CELEX: A Guide for Users.
Buttcher, S., C.L.A. Clarke, and G.V. Cormack. 2004.
Domain-Specific Synonym Expansion and Valida-
tion for Biomedical Information Retrieval.
Buttcher, S. 2007. The Wumpus Search Engine.
http://www.wumpus-search.org/.
Charniak, E. 2000. A maximum-entropy-inspired
parser. ACM International Conference Proceeding
Series, 4:132?139.
Denoyer, L. and P. Gallinari. 2006. The Wikipedia
XML corpus. ACM SIGIR Forum, 40(1):64?69.
Ferret, O., B. Grau, M. Hurault-Plantet, G. Illouz,
L. Monceaux, I. Robba, and A. Vilnat. 2002. Find-
ing an answer based on the recognition of the ques-
tion focus. Proc. of TREC 2001, pages 500?250.
Goldberg, D.E. and J.H. Holland. 1988. Genetic Algo-
rithms and Machine Learning. Machine Learning,
3(2):95?99.
Higashinaka, R. and H. Isozaki. 2008. Corpus-based
Question Answering for why-Questions. In Proc. of
IJCNLP, vol.1, pages 418?425.
Hovy, E.H., U. Hermjakob, and D. Ravichandran.
2002. A Question/Answer Typology with Surface
Text Patterns. In Proc. of HLT 2002.
Khalid, M. and S. Verberne. 2008. Passage Retrieval
for Question Answering using Sliding Windows. In
Proc. of IR4QA at COLING 2008.
Koster, CHA. 2003. Head-modifier frames for every-
one. Proc. of SIGIR 2003, page 466.
Quarteroni, S., A. Moschitti, S. Manandhar, and
R. Basili. 2007. Advanced Structural Represen-
tations for Question Classification and Answer Re-
ranking. In Proc. of ECIR 2007, volume 4425, pages
234?245.
Tiedemann, J. 2005. Improving passage retrieval in
question answering using NLP. In Proc. of EPIA
2005.
Verberne, S., L. Boves, N. Oostdijk, and P.A. Coppen.
2008. Evaluating paragraph retrieval for why-QA.
In Proc. of ECIR 2008.
960
Linguistic profiling of texts for the purpose of language verification 
Hans VAN HALTEREN 
Dept. of Language and Speech, Univ.of 
Nijmegen, The Netherlands 
P.O. Box 9103, 6500 HD Nijmegen 
hvh@let.kun.nl 
Nelleke OOSTDIJK 
Dept. of Language and Speech, Univ. of 
Nijmegen, The Netherlands 
P.O. Box 9103, 6500 HD Nijmegen 
n.oostdijk@let.kun.nl 
 
Abstract 
In order to control the quality of internet-based 
language corpora, we developed a method to 
verify automatically that texts are of (near-) 
native quality. For the LOCNESS and ICLE 
corpora, the method is rather successful in 
separating native and non-native learner texts. 
The Equal Error Rate is about 10%. However, 
for other domains, such as internet texts, 
separate classifiers have to be trained on the 
basis of suitable seed corpora. 
1 Introduction 
Research in linguistics and language engineering 
thrives on the availability of data. Traditionally, 
corpora would be compiled with a specific purpose 
in mind. Such corpora characteristically were well-
balanced collections of data. In the form of 
metadata, record was kept of the design criteria, 
sampling procedures, etc. Thus the researcher 
would have a fair idea of where his data originated 
from. Over past decades, data collection has been 
boosted by technological developments. More and 
more and increasingly large collections of data 
have been and are being compiled. It is tempting to 
think that the problem of data sparseness has been 
solved ? at least for raw data or data without any 
annotation other than can be provided fully 
automatically ? especially now that large amounts 
of data can be accessed through the internet. 
However, with data coming to us from all over the 
world, originating from all sorts of sources, we 
now possibly have a new problem on our hands: 
often the origins of the found data remain obscure. 
It is not always clear what exactly the 
implications for our research are of employing data 
whose origin we do not know. Is it legal to use 
these data, ethical, appropriate, ?? In this paper 
we will focus on the last point: the appropriateness 
of the data in the light of a specific application or 
research goal. More in particular, we will 
investigate to what extent we can devise a 
procedure that will enable us to identify texts 
produced by native speakers of the language (and 
thus by default those produced by non-native 
speakers). The present study is motivated by the 
fact that for many uses the (near-)nativeness of the 
data is a critical factor in the development of 
adequate resources and applications. Thus, for 
example, a style checker or some other writing 
assistant tool which has been based on erroneous 
materials or at least materials deviant from the 
language targeted, will not always respond 
appropriately. 
1.1 Assessing (near-)nativeness 
In the general absence of metadata which attest 
that texts have been produced by native speakers, 
there is one obvious approach that one may 
consider in order to assess the (near-)nativeness of 
texts of unknown origin and that is to exploit their 
specific linguistic characteristics.  
Previous studies investigating language variation 
(eg Biber, 1995, 1998; Biber et al, 1998; Conrad 
and Biber, 2001; Granger, 1998, Granger et al, 
2002) have shown that language use in different 
genres and by different (groups of) speakers 
displays characteristic use of specific linguistic 
features (lexical, morphological, syntactic, 
semantic, discoursal). These studies are all based 
on data of known origin. In the present study, we 
take a somewhat different approach as we aim to 
profile texts of unknown origin and identify native 
vs non-native language use, a task for which we 
coined the term language verification.  
1.2 Non-native language use 
Texts produced by non-native speakers will 
generally pass superficial inspection, i.e. they are 
deemed to be texts in the target language and will 
be treated as such. However, on closer inspection 
there is a wide range of features in the language 
use of non-natives which may have a disruptive 
effect on for instance derived language models. It 
is important to realize that non-native use is the 
complex result of different processes and 
conditions. First of all, there is the level of 
achievement. A non-native user gradually 
developes language skills in the target language. 
As he/she masters certain lexical items or morpho-
syntactic structures and feels confident in using 
them, certain items and structures are bound to be 
overused. At the same time, other items and 
structures remain underused as the user avoids 
them since he is not familiar with them or does not 
(yet) feel confident enough to employ them. 
Moreover, even for speakers who have attained a 
relatively high degree of proficiency, the influence 
of the native language remains. This may lead to 
transfer effects and interference (the effects of 
which are found, for example, in the use of false 
friends and word order deviations).  
 
In the present paper, we report the results 
obtained in some experiments that were carried out 
and which aimed to assess whether texts are of 
(British English) native or non-native origin using 
the method of linguistic profiling. The structure of 
the paper is as follows: In section 2, we describe 
the method of linguistic profiling. Next, in section 
3, its application in establishing the nativeness of 
texts is described, while in section 4 it is 
investigated whether the approach holds up when 
we shift from one domain to another. Finally, 
section 5 presents the conclusions. 
2 Linguistic profiling 
In linguistic profiling, the occurrences in a text 
are counted of a large number of linguistic 
features, either individual items or combinations of 
items. These counts are then normalized for text 
length and it is determined to what extent 
(calculated on the basis of the number of standard 
deviations) they differ from the mean observed in a 
profile reference corpus. For each text, the 
deviation scores are combined into a profile vector, 
on which a variety of distance measures can be 
used to position the text relative to any group of 
other texts. 
2.1 Language verification 
Linguistic profiling makes it possible to identify 
(groups of) texts which are similar, at least similar 
in terms of the profiled features (cf. van Halteren, 
2004). We have found that the recognition process 
can be vastly improved by not only providing 
positive examples (in the present case native texts) 
but also negative examples (here the non-native 
texts). So we expect that, given a seed corpus 
containing both native and non-native texts, 
linguistic profiling should be able to distinguish 
between these two types of texts. 
2.2 Features 
As previous research has shown (see e.g Biber 
1995), there are a great many linguistic features 
that contribute to marked structural differences 
between texts. These features mark ?basic 
grammatical, discourse, and communicative 
functions? (Biber, 1995: 104). They comprise 
features referring to vocabulary, lexical patterning, 
syntax, semantics, pragmatics, information content 
or item distribution through a text. Here we restrict 
ourselves to lexical features. 
Sufficiently frequent tokens, i.e. those that were 
observed to occur with a certain frequency in some 
language reference corpus, are used as features by 
themselves. In the present case these are ?tems that 
occur at least five times in the written texts from 
the BNC Sampler (BNC, 2002). For less frequent 
tokens, we determine a token pattern consisting of 
the sequence of character types. For example, the 
token Uefa-cup is represented by the pattern 
?#L#6+/CL-L?, where the first ?L? indicates low 
frequency, 6+ the size bracket, and the sequence 
?CL-L? a capital letter followed by one or more 
lower case letters followed by a hyphen and again 
one or more lower case letters. For lower case 
words, the final three letters of the word are also 
included in the pattern. For example, the token 
altercation is represented by the pattern 
?#L#6+/L/ion?. These patterns were originally 
designed for English and Dutch and will probably 
have to be extended for use with other languages. 
Furthermore, for this specific task, we wanted to 
avoid recognizing text topics rather than 
nativeness, and decided to mask content words. 
Any high frequency word classified primarily as 
noun, verb or adjective (see below), which had a 
high document bias (cf. van Halteren, 2003) was 
replaced by the marker #HC# followed by the 
same type of pattern we use for low frequency 
words, but always without the final three letters. 
This occludes topical words like brain or injury, 
while leaving more functional words like case or 
times intact. 
In addition to the form of the token, we also use 
the syntactic potential of the token as a feature. We 
apply the first few modules of a morphosyntactic 
tagger (in this case the tagger described by van 
Halteren, 2000) to the text, which determine which 
word class tags could apply to each token. For 
known words, the tags are taken from a lexicon; 
for unknown words, they are estimated on the basis 
of the word patterns described above. The most 
likely tags (with a maximum of three) are 
combined into a single feature. Thus still is 
associated with the feature ?RR-JJ-NN1? and 
forms with the feature ?NN2-VVZ?. Note that the 
most likely tags are determined exclusively on the 
basis of the current token; the context in which the 
token occurs is not taken into account. The 
modules of the tagger which are normally used to 
obtain a context dependent disambiguation are not 
applied. 
On top of the individual token and tag features 
we use all possible bi- and trigrams. For example, 
the token combination an attractive option is 
associated with the complex feature ?wcw= 
#HF#an#HC#JJ#HC#6+/L?. Since the number of 
features quickly grows too big to allow for 
efficient processing, we filter the set of features. 
This done by requiring that a feature occur in a set 
minimum number of texts in the profile reference 
corpus (in the present case a feature must occur in 
at least two texts). A feature which is filtered out 
contributes to a rest category feature. Thus, the 
complex feature above would contribute to 
?wcw=<OTHER>?. 
The lexical features currently also include 
features that relate to utterance length. For each 
utterance two such features are determined, viz. the 
exact length (e.g. ?len=15?) and the length bracket 
(e.g. ?len=10-19?). 
2.3 Classification 
When offered a list of positive and negative texts 
for training, and a list of test texts, the system first 
constructs a featurewise average of the profile 
vectors of all positive texts. It then determines a 
raw score for all text samples in the list. Rather 
than using the normal distance measure, we opted 
for a non-symmetric measure which is a weighted 
combination of two factors: a) the difference 
between text score and average profile score for 
each feature and b) the text score by itself. This 
makes it possible to assign more importance to 
features whose count deviates significantly from 
the norm. The following distance formula is used: 
?T = (? |Ti?Ai| D  |Ti| S) 1/(D+S) 
In this formula, Ti and Ai are the values for the ith 
feature for the text sample profile and the positive 
average profile respectively, and D and S are the 
weighting factors that can be used to assign more 
or less importance to the two factors described. 
The distance measure is then transformed into a 
score by the formula 
ScoreT = (? |Ti|(D+S)) 1/(D+S)   ?  ?T 
The score will grow with the similarity between 
text sample profile and positive average profile. 
The first component serves as a correction factor 
for the length of the text sample profile vector. 
The order of magnitude of the score values 
varies with the setting of D and S, and with the text 
collection. In order to bring the values into a range 
which is suitable for subsequent calculations, we 
express them as the number of standard deviations 
they differ from the mean of the scores of the 
negative example texts. 
3 Language verification 
In order to test the feasibility of language 
verification by way of linguistic profiling, we need 
data which is guaranteed to be written by native 
and non-native speakers respectively. Moreover, 
the texts (native and non-native) should be as 
similar as possible with respect to the genre they 
represent. For the present study, therefore, we 
opted for the student essays in the Louvain Corpus 
of Native English Essays (LOCNESS) and the 
International Corpus of Learner English (ICLE; 
Granger et al, 2002). 
3.1 LOCNESS and ICLE 
ICLE is a collection of mostly argumentative 
essays written by advanced EFL students from 
various mother-tongue backgrounds. The essays 
each are some 500-1000 words long (unabridged) 
and although they ?cover a variety of topics, the 
content is similar in so far as the topics are all non-
technical and argumentative (rather than narrative, 
for instance)? (cf. Granger, 1998:10). The size of 
the national sub-corpora is approx. 200,000 words 
per corpus. With the data metadata are available as 
they have been collected via a learner profile 
questionnaire. 
The LOCNESS in various respects is 
comparable to ICLE. It is a 300,000-word corpus 
mainly of essays written by English and American 
university students. A small part of the corpus 
(60,000 odd words) is constituted by British 
English A-level essays. Topics include transport, 
the parliamentary system, fox hunting, boxing, the 
National Lottery, and genetic engineering. 
3.2 Training and test texts 
In order to be able to control for language 
variation between British and American English, 
we opted for only the British part of LOCNESS. 
Because this totalled only some 155,000 words, we 
decided to hold out about one third as test material 
and use the other two thirds for training. In order to 
have as little overlap as possible in essay type and 
topic between training and test material, we used 
sub-corpora 2, 3 and 8 of the A-level essays and 
sub-corpus 3 of the university student essays for 
testing. 
For the ICLE texts, we chose to use each tenth 
text for training purposes. The remaining texts 
were used for testing.  
3.3 General results 
In the first step of training, we selected the 
features to be profiled. We used all features which 
occurred in more than one training text, i.e. about 
470K features. In the second step, we selected the 
system parameters D and S for two classification 
models: similarity to the native texts (D=1.0, 
S=0.0) and similarity to the non-native texts 
(D=1.2, S=0.2). The selection was based on the 
quality of classifying half of the training texts with 
the system having been trained on the other half. 
The verification results for the test set of A-level 
texts are shown in Figure 1. The further the texts 
are plotted to the right, the more similar their 
profile is to the mean profile for the A-level 
training texts. The further the texts are plotted 
towards the top, the more similar their profile is to 
the mean profile for the ICLE training texts.  
Most of the texts form a central cluster in the 
bottom right quadrant. A small gap separates them 
from a group of five near outliers, while there are 
two far outliers. We decided to use the limits of the 
central cluster as our classification separator, 
accepting that 10% of the LOCNESS texts would 
be rejected. We added the separation line to the 
plot. In order to create a reference frame linking 
this figure to the following ones, we add a second 
line, along the core of the cluster of the LOCNESS 
texts. Even though the core of the clusters in the 
successive figures may shift, this line remains 
constant, as does the plotting area.  
 
 
 
Figure 1. Text classification of the LOCNESS 
test texts in terms of similarity to native texts 
(horizontal axis) and similarity to non-native 
text (vertical axis). The separation line (top 
right to bottom left) divides the plot area in a 
native part (bottom right) and a non-native 
part (top left). The second line (top left to 
bottom right) is a reference line which 
allows comparison between this Figure and 
Figures 2-4. 
Figure 2. Text classification of the ICLE test 
texts 
Figure 2 shows the results for the ICLE test 
texts. 89% of the texts are rejected. The 
verification results differ per nationality. A more 
detailed examination of such variation, however, is 
beyond the scope of the present paper. 
The two dimensions, the degree of similarity to 
native texts and the degree of similarity to non-
native texts, are strongly (negatively) correlated. 
Still, there are also clear differences, so that both 
dimensions contribute substantially to the quality 
of the separation. 
3.4 Distinguishing features 
When examining some of the features that 
emerge from studies reported in the literature as 
salient in describing different language varieties, 
we find that none of these dominates the 
classification. Table 1 shows the influence of each 
feature in terms of its contribution (expressed as 
millionths of the total influence, so e.g. 3173 
corresponds to 0.3% of the total influence) to the 
decision to classify a text as native or non-native. 
The second and third column show the influence of 
the words (or word combinations) by themselves, 
which is extremely low. However, when 
examining all patterns containing these words, the 
fourth and fifth columns, their usefulness becomes 
visible.  
Previous studies into the use of intensifying 
adverbs have shown an overuse of the token very. 
Thus it is a likely candidate to be considered as a 
marker of non-native language use. The second 
column in the Table confirms this, but the 
contribution is a mere 0.001%. The picture 
changes when we consider all patterns in which 
very occurs, it appears that there is indeed a 
difference in use of the token by natives and non-
natives. However, there are as many patterns that 
point to nativeness as there are that point to non-
nativeness. Furthermore, the patterns provide a 
sizeable contribution in the classification either 
way. 
 
Word(s) Sep 
? 
ICLE 
Sep 
? 
LOC
Patterns 
? 
ICLE 
Patterns 
? 
LOCNESS
if 
   If  
   if 
 
13 
 
 
4
3931 4529
because 4 - 3230 2925
very 10 - 2860 3173
however - 1 686 644
therefore - 10 953 734
for instance 4 - 30 32
thus 2 - 411 287
yet 4 - 606 349
Table 1. Relative contribution to the overall 
classification of allegedly salient features 
Although the expected features (or rather 
features related to expected word or word 
combinations) have a visible contribution, their 
influence is still only a small part of the total 
influence. In fact, all features have only very little 
influence. The most influential single feature is 
ccc=#HF#AT--#HF#NN1--#HF#CC?RRx13, one 
of the representations of the, followed by a single 
common noun, followed by and, a pattern unlikely 
to be spotted by humans. It contributes 0.06% of 
the influence classifying texts as non-native. Only 
137 features in total contribute more than 0.01% 
either way. Classification by linguistic profiling is 
a matter of myriads of small hints rather than a few 
pieces of strong evidence. This is probably also 
what makes it robust against high text variability 
and sometimes small text sizes.  
4 Domain Shifts 
Now that we have seen that language 
verification is viable within the restricted domain 
of student essays, we may examine whether it 
survives the shift to a new domain. We tested this 
on two corpora: the FLOB corpus and (small) 
internet corpus that was especially collected for 
this purpose. 
4.1 FLOB 
The Freiburg LOB Corpus, informally known as 
FLOB (Hundt et al, 1998) is a modern counterpart 
to the much used Lancaster-Oslo/Bergen Corpus 
(LOB; Johansson, 1978) It is a one-million word 
corpus of written (educated) Modern British 
English. The composition of FLOB is essentially 
the same as that of LOB: it comprises 500 samples 
of 2,000 words each. In all, 15 text categories (A-
R) are distinguished. These fall into four main 
classes: newspaper text (A-C), miscellaneous 
informative prose (D-H), learned and scientific 
English (J), and fiction (K-R). 
 
 
Figure 3. Text classification of the FLOB 
learned and scientific texts (category J) 
 
Figure 4. Text classification of the FLOB non-
fiction texts (categories A-J) 
Of these texts, the learned and scientific class (J) 
is closest to the ICLE and LOCNESS texts, and we 
should expect that the FLOB texts of this category 
are all accepted. This is indeed the case, as can be 
seen in Figure 3, which shows the classification of 
these texts. Only 1 text is rejected (1.25%). This 
seems to confirm that we are indeed recognizing 
something like ?(near-)native English?.  
 
As soon as we shift the domain of the texts, 
however, the native texts are no longer 
distinguished as clearly. The larger the domain 
shift, the more texts are rejected. Within the non-
fiction portion of FLOB, the system rejects 2.3% 
of the newspaper texts (categories A-C) and 8.7% 
of the miscellaneous and informative prose texts 
(D-H). This leads to an overall reject rate of 5.6% 
for the non-fiction texts (Figure 4), which is still 
reasonably acceptable. When shifting to fiction 
texts (K-R), the reject rate jumps to 39.2% (Figure 
5), indicating that a new classifier would have to 
be trained for a proper handling of fiction texts. 
 
Figure 5. Text classification of the FLOB 
fiction texts (categories K-R) 
4.2 Capital-Born 
Since our original goal was the filtering of 
internet texts, we compiled a small corpus of such 
texts. We chose texts which were present as 
HTML. These, we expected, were likely to be 
rather abundant, while they would have been 
subjected to a relatively low degree of editing. 
Thus they would constitute likely candidates for 
filtering. In order to be able to decide whether the 
texts were native-written or not, we searched 
autobiographical material, as indicated by the 
phrase I was born in CITY, with CITY replaced by 
a name of a capital city. The initial set of 
documents appeared to be of a reasonable size. 
However, after filtering out webpages by multiple 
authors (e.g. guest books), fictional 
autobiographies (e.g. a joke page about Al Gore), 
texts judged likely to be edited possibly with the 
help of a native speaker (e.g. a page advertising 
Russian brides), misclassified city names (e.g. 
authors from Paris, Texas should not be assumed 
to be French) and texts outside the desired length 
of 500-1500 words, we ended up with a mere 20 
native British English texts and 18 non-native 
texts. We nicknamed the corpus ?Capital-Born 
corpus?. 
When classifying these texts with the A-level 
versus ICLE classifier, we see that they cluster 
tightly, outside the area plotted so far, and showing 
no useful separation of native and non-native texts. 
This implies that if we want a filter for such texts, 
we have to train a new classifier. 
 
Figure 6. Text classification of internet texts 
(for a description see section 4.2) 
We did train such a new classifier, using only the 
odd-numbered Capital-Born texts and classified 
the even-numbered ones, using the same 
parameters D and S as above. We repeated the 
process with the two sets switching roles. Figure 6 
shows a superposition of the classifications in the 
two experiments. The native texts appear as plus 
signs (+), the non-native texts as minus signs (?). 
Note that we adjusted the separation and support 
lines in order to bring them in line with the data. 
Only a rough separation is visible, with 2 out of 20 
native texts misclassified and 6 out of 18 non-
native texts. Still, given the extremely small size of 
the training sets and the variety of non-native 
nationalities, these results are rather promising. It 
appears that even internet texts can be filtered for 
nativeness, as long as a restricted, and more 
sizeable, seed corpus can be constructed. 
5 Conclusion 
The results show that language verification is 
indeed possible, as long as we accept that near-
native texts produced by non-natives will not be 
filtered out.  
Furthermore, whenever a verification filter is 
needed, it will be necessary to create a new filter, 
based on a seed corpus which contains both native 
and non-native texts as similar as possible in type 
to the texts which are to be filtered. 
There are now two avenues open for future 
research. First of all, we would like to explore the 
classification procedure linguistically: a) examine 
the distinguishing features in more detail and 
compare our findings with those in the literature, 
and b) examine the correlation of the nativeness 
score of the various texts to extra-linguistic text 
variables such as mother tongue and learner level. 
Secondly, once more insight is gained into the 
linguistic workings of the procedure, the 
classification process can be refined. At this point, 
we would also like to examine the effects of 
domain shift in more detail, and attempt to 
estimate a minimum size for seed corpora for use 
in filtering internet material. 
6 Acknowledgements 
Thanks are due to Sylviane Granger and Sylvie 
De Cock (Centre for English Corpus Linguistics, 
Universit? Catholique de Louvain, Belgium) for 
making the LOCNESS and ICLE data available to 
us. 
References  
Douglas Biber. 1995. Dimensions of register 
variation. A cross-linguistic comparison. 
Cambridge: Cambridge University Press. 
 
Douglas Biber 1998. Variation across Speech and 
Writing. Cambridge: Cambridge University 
Press. 
Douglas Biber, Susan Conrad and Randi Reppen. 
1998. Corpus Linguistics: Investigating 
language structure and use. Cambridge: 
Cambridge University Press. 
BNC. 2002. The BNC sampler. Web page: 
www.natcorp.ox.ac.uk/getting/sampler.html 
Susan Conrad and Douglas Biber (eds.) 2001. 
Variation in English: Multi-dimensional 
studies. Harlow, England: Longman. 
Alan Davies. 2003. The Native Speaker: Myth and 
Reality. Clevedon: Multimingual Matters Ltd. 
Sylviane Granger (ed.) 1998. Learner English on 
Computer. London and New York: Longman. 
Sylviane Granger. 1998. The computer learner 
corpus. In Sylviane Granger (ed.): 3-18. 
Sylviane Granger, Joseph Hung, and Stephanie 
Petch-Tyson (eds.) 2002. Computer Learner 
Corpora, Second Language Acquisition and 
Foreign Language Teaching. Amsterdam: 
Benjamins. 
Sylviane Granger, E. Dagneaux, and Fanny 
Meunier (eds.) 2002. International Corpus of 
Learner English. Louvain: UCL Presses 
Universitaires de Louvain. 
Hans van Halteren. 2000. The detection of 
inconsistencies in manually tagged text. Proc. 
Workshop on Linguistically Interpreted 
Corpora (LINC2000). 48-55. 
Hans van Halteren. 2003. New feature sets for 
summarization by sentence extraction. IEEE 
Intelligent Systems, July/August 2003: 34-42. 
Hans van Halteren. 2004. Linguistic profiling for 
author recognition and verification. Proc. ACL 
2004. 
Marianne Hundt, Andrea Sandt and Rainer 
Siemund. 1998. Flobman. Manual of 
Information to accompany the Freiburg-LOB 
Corpus of British English (?FLOB?). Freiburg: 
Englisches Seminar. 
Stig Johansson with Geoffrey Leech and Helen 
Goodluck. 1978. Manual of Information to 
Accompany the Lancaster-Oslo/Bergen Corpus 
of British English, for Use with Digital 
Computers. Oslo: Dept. of English, University of 
Oslo. 
M. van der Laaken, R. Lankamp, and Michael 
Sharwood Smith. 1997. Writing Better English. 
Bussum: Coutinho. 
LOCNESS. 
http://juppiter.fltr.ucl.ac.be/FLTR/GERM/ETAN
/CECL/Cecl-Projects/Icle/LOCNESS.htm 
What Is Not in the Bag of Words forWhy-QA?
Suzan Verberne?
Radboud University Nijmegen
Lou Boves??
Radboud University Nijmegen
Nelleke Oostdijk?
Radboud University Nijmegen
Peter-Arno Coppen?
Radboud University Nijmegen
While developing an approach towhy-QA, we extended a passage retrieval system that uses off-
the-shelf retrieval technology with a re-ranking step incorporating structural information. We
get significantly higher scores in terms of MRR@150 (from 0.25 to 0.34) and success@10. The
23% improvement that we reach in terms of MRR is comparable to the improvement reached on
different QA tasks by other researchers in the field, although our re-ranking approach is based
on relatively lightweight overlap measures incorporating syntactic constituents, cue words, and
document structure.
1. Introduction
About 5% of all questions asked to QA systems are why-questions (Hovy, Hermjakob,
and Ravichandran 2002). Why-questions need a different approach than factoid ques-
tions, because their answers are explanations that usually cannot be stated in a single
phrase. Recently, research (Verberne 2006; Higashinaka and Isozaki 2008) has been
directed at QA forwhy-questions (why-QA). In earlier work on answeringwhy-questions
on the basis of Wikipedia, we found that the answers to most why-questions are pas-
sages of text that are at least one sentence and at most one paragraph in length (Verberne
et al 2007b). Therefore, we aim at developing a system that takes as input a why-
question and gives as output a ranked list of candidate answer passages.
In the current article, we propose a three-step setup for a why-QA system: (1) a
question-processing module that transforms the input question to a query; (2) an off-
the-shelf retrieval module that retrieves and ranks passages of text that share content
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: s.verberne@let.ru.nl.
?? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: l.boves@let.ru.nl.
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: n.oostdijk@let.ru.nl.
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: p.a.coppen@let.ru.nl.
Submission received: 30 July 2008; revised submission received: 18 February 2009; accepted for publication:
4 September 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
with the input query; and (3) a re-ranking module that adapts the scores of the re-
trieved passages using structural information from the input question and the retrieved
passages.
In the first part of this article, we focus on step 2, namely, passage retrieval. The
classic approach to finding passages in a text collection that share content with an
input query is retrieval using a bag-of-words (BOW) model (Salton and Buckley 1988).
BOWmodels are based on the assumption that text can be represented as an unordered
collection of words, disregarding grammatical structure. Most BOW-based models use
statistical weights based on term frequency, document frequency, passage length, and
term density (Tellex et al 2003).
Because BOW approaches disregard grammatical structure, systems that rely on
a BOW model have their limitations in solving problems where the syntactic relation
betweenwords or word groups is crucial. The importance of syntax for QA is sometimes
illustrated by the sentence Ruby killed Oswald, which is not an answer to the question
Who did Oswald kill? (Bilotti et al 2007). Therefore, a number of researchers in the field
investigated the use of structural information on top of a BOW approach for answer
retrieval and ranking (Tiedemann 2005; Quarteroni et al 2007; Surdeanu, Ciaramita, and
Zaragoza 2008). These studies show that although the BOW model makes the largest
contribution to the QA system results, adding structural (syntactic information) can give
a significant improvement.
In the current article, we hypothesize that for the relatively complex problem of
why-QA, a significant improvement?at least comparable to the improvement gained
for factoidQA?can be gained from the addition of structural information to the ranking
component of the QA system. We first evaluate a passage retrieval system for why-QA
based on standard BOW ranking (step 1 and 2 in our set-up). Then we perform an
analysis of the strengths and weaknesses of the BOW model for retrieving and ranking
candidate answers. In view of the observed weaknesses of the BOW model, we choose
our feature set to be applied to the set of candidate answer passages in the re-ranking
module (step 3 in our set-up).
The structural features that we propose are based on the idea that some parts of the
question and the answer passage are more important for relevance ranking than other
parts. Therefore, our re-ranking features are overlap-based: They tell us which parts of
a why-question and its candidate answers are the most salient for ranking the answers.
We evaluate our initial and adapted ranking strategies using a set of why-questions and
a corpus of Wikipedia documents, and we analyze the contribution of both the BOW
model and the structural features.
The main contributions of this article are: (1) we address the relatively new problem
of why-QA and (2) we analyze the contribution of overlap-based structural information
to the problem of answer ranking.
The paper is organized as follows. In Section 2, related work is discussed. Section 3
presents the BOW-based passage retrieval method forwhy-QA, followed by a discussion
of the strengths andweaknesses of the approach in Section 4. In Section 5, we extend our
system with a re-ranking component based on structural overlap features. A discussion
of the results and our conclusions are presented in Sections 6 and 7, respectively.
2. Related Work
Wedistinguish relatedwork in two directions: research into the development of systems
for why-QA (Section 2.1), and research into combining structural and BOW features for
QA (Section 2.2).
230
Verberne et al What Is Not in the Bag of Words forWhy-QA?
2.1 Research intoWhy-QA
In related work (Verberne et al 2007a), we focused on selecting and ranking explanatory
passages for why-QA with the use of rhetorical structures. We developed a system that
employs the discourse relations in a manually annotated document collection: the RST
Treebank (Carlson, Marcu, and Okurowski 2003). This system matches the input ques-
tion to a text span in the discourse tree of the document and it retrieves as answer the
text span that has a specific discourse relation to this question span. We evaluated our
method on a set of 336 why-questions formulated to seven texts from the WSJ corpus.
We concluded that discourse structure can play an important role in why-QA, but that
systems relying on these structures can only work if candidate answer passages have
been annotated with discourse structure. Automatic parsers for creating full rhetorical
structures are currently unavailable. Therefore, a more practical approach appears to
be necessary for work in why-QA, namely, one which is based on automatically created
annotations.
Higashinaka and Isozaki (2008) focus on the problem of ranking candidate answer
paragraphs for Japanese why-questions. They assume that a document retrieval module
has returned the top 20 documents for a given question. They extract features for content
similarity, causal expressions, and causal relations from two annotated corpora and a
dictionary. Higashinaka and Isozaki evaluate their ranking method using a set of 1,000
why-questions that were formulated to a newspaper corpus by a text analysis expert.
70.3% of the reference answers for these questions are ranked in the top 10 by their
system, and MRR1 was 0.328.
Although the approach of Higashinaka and Isozaki is very interesting, their eval-
uation collection has the same flaw as the one used by Verberne et al (2007a): Both
collections consist of questions formulated to a pre-selected answer text. Questions
elicited in response to newspaper texts tend to be unrepresentative of questions asked
in a real QA setting. In the current work, therefore, we work with a set of questions
formulated by users of an online QA system (see Section 3.1).
2.2 Combining Structural and Bag-of-Words Features for QA
Tiedemann (2005) investigates syntactic information from dependency structures in
passage retrieval for Dutch factoid QA. He indexes his corpus at different text layers
(BOW, part-of-speech, dependency relations) and uses the same layers for question
analysis and query creation. He optimizes the query parameters for the passage retrieval
task by having a genetic algorithm apply the weights to the query terms. Tiedemann
finds that the largest weights are assigned to the keywords from the BOW layer and
to the keywords related to the predicted answer type (such as ?person?). The baseline
approach, using only the BOW layer, gives an MRR of 0.342. Using the optimized IR
settings with additional layers, MRR improves to 0.406.
Quarteroni et al (2007) consider the problem of answering definition questions.
They use predicate?argument structures (PAS) for improved answer ranking. They find
that PAS as a stand-alone representation is inferior to parse tree representations, but
that together with the BOW it yields higher accuracy. Their results show a significant
1 The reciprocal rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevant
answer. The Mean RR is obtained by averaging RR over all questions.
231
Computational Linguistics Volume 36, Number 2
improvement of PAS?BOW compared to parse trees (F-scores 70.7% vs. 59.6%) but PAS
makes only a very small contribution compared to BOW only (which gives an F-score
of 69.3%).
Recent work by Surdeanu, Ciaramita, and Zaragoza (2008) addresses the problem
of answer ranking for how-to-questions. From Yahoo! Answers,2 they extract a corpus
of 140,000 answers with 40,000 questions. They investigate the usefulness of a large
set of question and answer features in the ranking task. They conclude that the linguistic
features ?yield a small, yet statistically significant performance increase on top of the
traditional BOW and n-gram representation (page 726).?
All these authors conclude that the addition of structural information in QA
gives a small but significant improvement compared to using a BOW-model only. For
why-questions, we also expect to gain improvement from the addition of structural
information.
3. Passage Retrieval forWhy-QA Using a BOWModel
As explained in Section 1, our system comprises three modules: question2query, passage
retrieval, and re-ranking. In the current section, we present the first two system mod-
ules, and the re-ranking module, including a description of the structural features that
we consider, is presented in Section 5. First, however, we describe our data collection
and evaluation method.
3.1 Data and Evaluation Set-up
For our experiments, we use the Wikipedia INEX corpus (Denoyer and Gallinari 2006).
This corpus consists of all 659,388 articles from the online Wikipedia in the summer of
2006 in XML format.
For development and testing purposes, we exploit the Webclopedia question
set (Hovy, Hermjakob, and Ravichandran 2002), which contains questions asked to
the online QA system answers.com. Of these questions, 805 (5% of the total set) are
why-questions. For 700 randomly selected why-questions, we manually searched for an
answer in the Wikipedia XML corpus, saving the remaining 105 questions for future
testing purposes. 186 of these 700 questions have an answer in the corpus.3 Extraction
of one relevant answer for each of these questions resulted in a set of 186 why-questions
and their reference answers.4 Two examples illustrate the type of data we are working
with:
1. ?Why didn?t Socrates leave Athens after he was convicted?? ? ?Socrates
considered it hypocrisy to escape the prison: he had knowingly agreed to
live under the city?s laws, and this meant the possibility of being judged
guilty of crimes by a large jury.?
2 See http://answers.yahoo.com/.
3 Thus, about 25% of our questions have an answer in the Wikipedia corpus. The other questions are either
too specific (Why do ceiling fans turn counter-clockwise but table fans turn clockwise?) or too trivial (Why do
hotdogs come in packages of 10 and hotdog buns in packages of 8?) for the coverage of Wikipedia in 2006.
4 Just like factoid questions, most why-questions generally have one correct answer that can be formulated
in different ways.
232
Verberne et al What Is Not in the Bag of Words forWhy-QA?
2. ?Why do most cereals crackle when you add milk?? ? ?They are made of
a sugary rice mixture which is shaped into the form of rice kernels and
toasted. These kernels bubble and rise in a manner which forms very thin
walls. When the cereal is exposed to milk or juices, these walls tend to
collapse suddenly, creating the famous ?Snap, crackle and pop? sounds.?
To be able to do fast evaluation without elaborate manual assessments, we manually
created one answer pattern for each of the questions in our set. The answer pattern is a
regular expression that defines which of the retrieved passages are considered a relevant
answer to the input question. The first version of the answer patterns was directly
based on the corresponding reference answer, but in the course of the development
and evaluation process, we extended the patterns in order to cover as many as possible
of the Wikipedia passages that contain an answer. For example, for question 1, we
developed the following answer pattern based on two variants of the correct answer
that occur in the corpus: /(Socrates.* opportunity.* escape.* Athens.* considered.*
hypocrisy | leave.* run.* away.* community.* reputation)/.5
In fact, answer judgment is a complex task due to the presence of multiple answer
variants in the corpus. It is a time-consuming process because of the large number of
candidate answers that need to be judged when long lists of answers are retrieved per
question. In future work, we will come back to the assessment of relevant and irrelevant
answers.
After applying our answer patterns to the passages retrieved, we count the ques-
tions that have at least one relevant answer in the top n results. This number divided by
the total number of questions in a test set gives the measure success@n. In Section 3.2,
we explain the levels for n that we use for evaluation. For the highest ranked relevant
answer per question, we determine the RR. Questions for which the system did not
retrieve an answer in the list of 150 results get an RR of 0. Over all questions, we calculate
the mean reciprocal rank MRR.
3.2 Method and Results
In the question2query module of our system we convert the input question to a query
by removing stop words6 and punctuation, and simply list the remaining content words
as query terms.
The second module of our system performs passage retrieval using off-the-shelf
retrieval technology. In Khalid and Verberne (2008), we compared a number of settings
for our passage retrieval task. We considered two different retrieval engines (Lemur7
and Wumpus8), four different ranking models, and two types of passage segmentation:
disjoint and sliding passages. In each setting, 150 results were obtained by the retrieval
engine and ranked by the retrieval model. We evaluated all retrieval settings in terms of
5 Note that the vertical bar separates the two alternatives.
6 To this end we use the stop word list that can be found at http://marlodge.supanet.com/museum/
funcword.html.We use all items except the numbers and the word why.
7 Lemur is an open source toolkit for information retrieval that provides flexible support for different types
of retrieval models. See http://www.lemurproject.org.
8 Wumpus is an information retrieval system mainly geared at XML retrieval. See http://www.wumpus-
search.org/.
233
Computational Linguistics Volume 36, Number 2
MRR@n9 and success@n for levels n = 10 and n = 150. For the evaluation of the retrieval
module, we were mainly interested in the scores for success@150 because re-ranking
can only be successful if at least one relevant answer was returned by the retrieval
module.
We found that the best-scoring passage retrieval setting in terms of success@150 is
Lemur on an index of sliding passages with TF-IDF (Zhai 2001) as ranking model. We
obtained the following results with this passage retrieval setting: success@150 is 78.5%,
success@10 is 45.2%, and MRR@150 is 0.25. We do not include the results obtained with
the other retrieval settings here because the differences were small.
The results show that for 21.5% of the questions in our set, no answer was retrieved
in the top-150 results. We attempted to increase this coverage by retrieving 250 or
500 answers per question but this barely increased the success score at maximum
n. The main problems for the questions that we miss are infamous retrieval prob-
lems such as the vocabulary gap between a question and its answer. For example,
the answer to Why do chefs wear funny hats? contains none of the words from the
question.
4. The Strengths and Weaknesses of the BOWModel
In order to understand how answer ranking is executed by the passage retrieval mod-
ule, we first take a closer look at the TF-IDF algorithm as it has been implemented in
Lemur. TF-IDF is a pure BOW model: Both the query and the passages in the corpus
are represented by the term frequencies (numbers of occurrences) for each of the words
they contain. The terms are weighted using their inverse document frequency (IDF),
which puts a higher weight on terms that occur in few passages than on terms that
occur in many passages. The term frequency (TF) functions for the query and the doc-
ument, and the parameter values chosen for these functions in Lemur can be found in
Zhai (2001).
As explained in the previous section, we consider success@150 to be the most
important measure for the retrieval module of our system. However, for the system as a
whole, success@10 is a more important evaluation measure. This is because users tend
to pay much more attention to the top 10 results of a retrieval system than to results that
are ranked lower (Joachims et al 2005). Therefore, it is interesting to investigate which
questions are answered in the top 150 and not in the top 10 by our passage retrieval
module. This is the set of questions for which the BOW model is not effective enough
and additional (more specific) overlap information is needed for ranking a relevant
answer in the top 10.
We analyzed the set of questions that get a relevant answer at a rank between 10 and
150 (62 questions), which belowwewill refer to as our focus set. We compared our focus
set to the questions for which a relevant answer is in the top 10 (84 questions). Although
these numbers are too small to do a quantitative error analysis, a qualitative analysis
provides valuable insights into the strengths and weaknesses of a BOW representation
such as TF-IDF. In Sections 4.1 to 4.4 we discuss four different aspects of why-questions
that present problems for the BOWmodel.
9 Note that MRR is often used without the explicit cut-off point (n). We add it to clarify that RR is 0 for the
questions without a correct answer in the top-n.
234
Verberne et al What Is Not in the Bag of Words forWhy-QA?
4.1 Short Questions
Ten questions in our focus set contain only one or two content words. We can see the
effect of short queries if we compare three questions that contain only one semantically
rich content word.10 The rank of the highest ranked relevant answer is given between
parentheses; the last of these three questions is in our focus set.
1. Why do people hiccup? (2)
2. Why do people sneeze? (4)
3. Why do we dream? (76)
We found that the rank of the relevant answer is related to the corpus frequency of
the single semantically rich word, which is 64 for hiccup, 220 for sneeze, and 13,458 for
dream. This means that many passages are retrieved for question 3, making the chances
for the relevant answer to be ranked in the top 10 smaller. One way to overcome the
problem of long result lists for short queries is by adding words to the query that make
it more specific. In the case of why-QA, we know that we are not simply searching
for information on dreaming but for an explanation for dreaming. Thus, in the ranking
process, we can extend the query with explanatory cue words such as because.11 We
expect that the addition of explanatory cue phrases will give an improvement in ranking
performance.
4.2 The Document Context of the Answer
There are many cases where the context of the candidate answer gives useful infor-
mation. Consider, for example, the question Why does a snake flick out its tongue?, the
correct answer to which was ranked 29. A human searcher expects to find the answer
in a Wikipedia article about snakes. Within the Snake article he or she may search for
the words flick and/or tongue in order to find the answer. This suggests that in some
cases there is a direct relation between a specific part of the question and the context
(document and/or section) of the candidate answer. In cases like this, the answer
document and the question apparently share the same topic (snake). By analogy with
linguistically motivated approaches to factoid QA (Ferret et al 2002), we introduce the
term question focus for this topic.
In the example question flick is the word with the lowest corpus frequency (556),
followed by tongue (4,925) and snake (6,809). Using a BOW approach to document title
matching, candidate answers from documents with flick or tongue in their title would
be ranked higher than answers from documents with snake in their title. Thus, for
questions for which there is overlap between the question focus and the title of the
answer documents (two thirds of the questions in our set), we can improve the ranking
of candidate answers by correctly predicting the question focus. In Section 5.1.2, we
make concrete suggestions for achieving this.
10 The word people in subject position is a semantically poor content word.
11 The addition of cue words can also be considered to be applied in the retrieval step. We come back to this
in Section 6.3.
235
Computational Linguistics Volume 36, Number 2
4.3 Multi-Word Terms
A very important characteristic of the BOWmodel is that words are considered separate
terms. One of the consequences is that multi-word terms such as multi-word noun
phrases (mwNPs) are not treated as a single term. Here, three examples of questions
are shown in which the subject is realized by a mwNP (underlined in the examples; the
rank of the relevant answer is shown between brackets):
1. Why are hush puppies called hush puppies? (1)
2. Why is the coral reef disappearing? (29)
3. Why is a black hole black? (31)
We investigated the corpus frequencies for the separate parts of each mwNP. We found
that these are quite high for coral (3,316) and reef (2,597) compared to the corpus
frequency of the phrase coral reef (365). The numbers are even more extreme for black
(103,550) and hole (9,734) versus black hole (1,913). On the other hand, the answer to
the hush puppies question can more easily be ranked because the corpus frequencies
for the separate terms hush (594) and puppies (361) are relatively low. This shows that
multi-word terms do not necessarily give problems for the BOW model as long as the
document frequencies for the constituent words are relatively low. If (one of) the words
in the phrase is/are frequent, it is very difficult to rank the relevant answer high in the
result list with use of word overlap only.
In our focus set, 36 of the 62 questions contain a mwNP. For these questions, we can
expect improved ranking from the addition of NPs to our feature set.
4.4 Syntactic Structure
The BOW model does not take into account sentence structure. The potential impor-
tance of sentence structure for improved ranking can be exemplified by the following
two questions from our set. Note that both examples contain a subordinate clause (finite
or non-finite):
1. Why do baking soda and vinegar explode when you mix them together? (4)
2. Why are there 72 points to the inch when discussing fonts and printing? (36)
In both cases, the contents of the subordinate clause are less important to the goal of the
question than the contents of themain clause. In the first example, this is (coincidentally)
reflected by the corpus frequencies of the words in both clauses: mix (12,724) and
together (83,677) have high corpus frequencies compared to baking (832), soda (1,620),
vinegar (871), and explode (1,285). As a result, the reference answer containing these
terms is ranked in the top-10 by TF-IDF. In the second example, however, the corpus
frequencies do not reflect the importance of the terms. Fonts and printing have lower
corpus frequencies (1,243 and 6,978, respectively) than points (43,280) and inch (10,046).
Thus, fonts and printing are weighted heavier by TF-IDF although these terms are only
peripheral to the goal of the query, the core of which isWhy are there 72 points to the inch?
This cannot be derived from the corpus frequencies, but can only be inferred from the
syntactic function (adverbial) of when discussing fonts and printing in the question.
Thus, the lack of information about sentence structure in the BOW model does
not necessarily give rise to problems as long as the importance of the question terms
is reflected by their frequency counts. If term importance does not align with corpus
236
Verberne et al What Is Not in the Bag of Words forWhy-QA?
frequency, grammatical structure becomes potentially useful. Therefore, we expect that
syntactic structure can make a contribution to cases where the importance of the terms
is not reflected by their corpus frequencies but can be derived from their syntactic
function.
4.5 What Can We Expect from Structural Information?
In Sections 4.1 to 4.4 we discussed four aspects of why-questions that are problematic
for the BOW model. We expect contributions from the inclusion of information on cue
phrases, question focus and the document context of the answer, noun phrases, and
the syntactic structure of the question. We think that it is possible to achieve improved
ranking performance if features based on structural overlap are taken into account
instead of global overlap information.
5. Adding Overlap-Based Structural Information
From our analyses in Section 4, we found a number of question and answer aspects
that are potentially useful for improving the ranking performance of our system. In
this section, we present the re-ranking module of our system. We define a feature set
that is inspired by the findings from Section 4 and aims to find out which structural
features of a question?answer pair contribute the most to better answer ranking. We
aim to weigh these features in such a way that we can optimize ranking performance.
The input data for our re-ranking experiments is the output of the passage retrieval
module. A success@150 score of 78.5% for passage retrieval (see Section 3.2) means that
the maximum success@10 score that we can achieve by re-ranking is 78.5%.
5.1 Features for Re-ranking
The first feature in our re-ranking method is the score that was assigned to a candidate
answer by Lemur/TF-IDF in the retrieval module (f0). In the following sections we
introduce the other features that we implemented. Each feature represents the overlap
between two item bags:12 a bag of question items (for example: all the question?s noun
phrases, or the question?s main verb) and a bag of answer items (for example: all answer
words, or all verbs in the answer). The value that is assigned to a feature is a function of
the overlap between these two bags. We used the following overlap function:
S(Q,A) =
QA + AQ
Q+ A
(1)
in whichQA is the number of question items that occur at least once in the bag of answer
items, AQ is the number of answer items that occur at least once in the bag of question
items, and Q+ A is the number of items in both bags of items joined together.
5.1.1 The Syntactic Structure of the Question. In Section 4.4, we argued that some syntactic
parts of the question may be more important for answer ranking than others. Because
we have no quantitative evidence yet which syntactic parts of the question are the most
important, we created overlap features for each of the following question parts: phrase
12 Note that a ?bag? is a set in which duplicates are counted as distinct items.
237
Computational Linguistics Volume 36, Number 2
heads (f1), phrase modifiers (f2); the subject (f3), main verb (f4), nominal predicate (f5),
and direct object (f6) of the main clause; and all noun phrases (f11). For each of these
question parts, we calculated its word overlap with the bag of all answer words. For the
features f3?f6, we added a variant where as answer items only words/phrases with the
same syntactic function as the question token were included (f7, f8, f9, f10).
Consider for example question 1 from Section 3.1: Why didn?t Socrates leave Athens
after he was convicted?, and the reference answer as the candidate answer for which we
are determining the feature values: Socrates considered it hypocrisy to escape the prison: he
had knowingly agreed to live under the city?s laws, and this meant the possibility of being judged
guilty of crimes by a large jury.
From the parser output, our feature extraction script extracts Socrates as subject,
leave as main verb, and Athens as direct object. Neither leave nor Athens occur in the
answer passage, thus f4, f6, f8, and f10 are all given a value of 0. So are f5 and f9,
because the question has no nominal predicate. For the subject Socrates, our script finds
that it occurs once in the bag of answer words. The overlap count for the feature f3 is
thus calculated as 1+11+18 = 0.105.
13 For the feature f7, our script extracts the grammatical
subjects Socrates, he, and this from the parser?s representation of the answer passage.
Because the bag of answer subjects for f7 contains three items, the overlap is calculated
as 1+11+3 = 0.5.
5.1.2 The Semantic Structure of the Question. In Section 4.2, we saw that often there is a
link between the question focus and the title of the document in which the reference
answer is found. In those cases, the answer document and the question share the same
topic. For most questions, the focus is the syntactic subject: Why do cats sleep so much?
Judging from our data, there are two exceptions to this general rule: (1) If the subject
is semantically poor, the question focus is the (verbal or nominal) predicate: Why do
people sneeze?, and (2) in case of etymology questions (which cover about 10% of
why-questions), the focus is the subject complement of the passive sentence: Why are
chicken wings called Buffalo Wings?
We included a feature (f12) for matching words from the question focus to words
from the document title and a feature (f13) for the relation between question focuswords
and all answer words. We also include a feature (f14) for the other, non-focus question
words.
5.1.3 The Document Context of the Answer. Not only is the document title in relation to
the question focus potentially useful for answer ranking, but also other aspects of the
answer context. We include four answer context features in our feature set: overlap
between the question words and the title of the Wikipedia document (f15), overlap be-
tween question words and the heading of the answer section (f16), the relative position
of the answer passage in the document (f17), and overlap between a fixed set of words
that we selected as explanatory cues when they occur in a section heading and the set
of words that occur in the section heading of the passage (f18).14
13 The bag of question subjects contains one item (Socrates, the 1 in the denominator) and one item from
this bag occurs in the bag of answer words (the left 1 in the numerator). Without stopwords, the bag of
all answer words contains 18 items, one of which occurs in the bag of question subjects (the right 1 in
the numerator).
14 We found these section heading cues by extracting all section headings from the Wikipedia corpus,
sorting them by frequency, and then manually marking those section heading words that we expect
to occur with explanatory sections. The result is a small set of heading cues (history, origin, origins,
background, etymology, name, source, sources) that is independent of the test set we work with.
238
Verberne et al What Is Not in the Bag of Words forWhy-QA?
5.1.4 Synonyms. For each of the features f1 to f10 and f12 to f16 we add an alternative
feature (f19 to f34) covering the set of all WordNet synonyms for all question terms in
the original feature. For synonyms, we apply a variant of Equation (1) in which QA is
interpreted as the number of question items that have at least one synonym in the bag
of answer items and AQ as the number of answer items that occur in at least one of the
synonym sets of the question items.
5.1.5 WordNet Relatedness. Additionally, we included a feature representing the related-
ness between the question and the candidate answer using the WordNet Relatedness
tool (Pedersen, Patwardhan, and Michelizzi 2004) (f35). As a measure of relatedness,
we choose the Lesk measure, which incorporates information fromWordNet glosses.
5.1.6 Cue Phrases. Finally, as proposed in Section 4.1, we added a closed set of cue phrases
that are used to introduce an explanation (f36). We found these explanatory phrases in a
way that is commonly used for finding answer cues and that is independent of our own
set of question?answer pairs. We queried the key answer words to the most frequent
why-question on the Web Why is the sky blue? (blue sky rayleigh scattering) to the MSN
Search engine15 and crawled the first 250 answer fragments that are retrieved by the
engine. From these, we manually extracted all phrases that introduce the explanation.
This led to a set of 47 cue phrases such as because, as a result of, which explains why,
and so on.
5.2 Extracting Feature Values from the Data
For the majority of features we needed the syntactic structure of the input question,
and for some of the features also of the answer. We experimented with two different
syntactic parsers for these tasks: the Charniak parser (Charniak 2000) and a develop-
ment version of the Pelican parser.16 Of these, Pelican has a more detailed descriptive
model and gives better accuracy but Charniak is at present more robust for parsing
long sentences and large amounts of text. We parsed the questions with Pelican because
we need accurate parsings in order to correctly extract all constituents. We parsed all
answers (186 ? 150 passages) with Charniak because of its speed and robustness.
For feature extraction, we used the following external components: A stop word
list,17 the sets of cue phrases as described in Sections 5.1.3 and 5.1.6, the CELEX Lemma
lexicon (Burnage et al 1990), the WordNet synonym sets, the WordNet Similarity
tool (Pedersen, Patwardhan, and Michelizzi 2004), and a list of pronouns and semanti-
cally poor nouns.18 We used a Perl script for extracting feature values for each question?
answer pair. For each feature, the script composes the required bags of question items
and answer items. All words are lowercased and punctuation is removed. For terms
in the question set that consist of multiple words (for example, a multi-word subject),
spaces are replaced by underscores before stop words are removed from the question
and the answer. Then the script calculates the similarity between the two sets for each
feature following Equation (1).19
15 http://www.live.com.
16 See http://lands.let.ru.nl/projects/pelican/.
17 See Section 3.1.
18 Semantically poor nouns that we came across in our data set are the nouns humans and people.
19 A multi-word term from the question is counted as one item.
239
Computational Linguistics Volume 36, Number 2
Table 1
Results for the why-QA system: the complete system including re-ranking compared against
plain Lemur/TF-IDF for 187 why-questions.
Success@10 Success@150 MRR@150
Lemur/TF-IDF?sliding 45.2% 78.5% 0.25
TF-IDF + Re-ranking using 37 structural features 57.0% 78.5% 0.34
Whether or not to lemmatize the terms before matching them is open to debate.
In the literature, there is some discussion on the benefit of lemmatization for question
answering (Bilotti, Katz, and Lin 2004). Lemmatization can be especially problematic
in the case of proper names (which are not always recognizable by capitalization).
Therefore, we decided only to lemmatize verbs (for features f4 and f8) in the current
version of our system.
5.3 Re-ranking Method
Feature extraction led to a vector consisting of 37 feature values for each of the 27,900
items in the data set. We normalized the feature values over all 150 answer candidates
for the same question to a number between 0 and 1 using the L1 vector norm. Each
instance (representing one question?answer pair) was automatically labeled 1 if the
candidate answer matched the answer pattern for the question and 0 if it did not.
On average, a why-question had 1.6 correct answers among the set of 150 candidate
answers.
In the process of training our re-ranking module, we aim at combining the 37
features in a ranking function that is used for re-ordering the set of candidate answers.
The task of finding the optimal ranking function for ranking a set of items is referred to
as ?learning to rank? in the information retrieval literature (Liu et al 2007). In Verberne
et al (2009), we compared several machine learning techniques20 for our learning-
to-rank problem. We evaluated the results using 5-fold cross validation on the ques-
tion set.
5.4 Results from Re-ranking
The results for the complete system compared with passage retrieval with Lemur/
TF-IDF only are in Table 1.We show the results in terms of success@10, success@150, and
MRR@150. We only present the results obtained using the best-performing learning-
to-rank technique: logistic regression.21 A more detailed description of our machine
learning method and a discussion of the results obtained with other learning techniques
can be found in Verberne et al (2009).
20 Naive Bayes, Support Vector Classification, Support Vector Regression, Logistic regression, Ranking
SVM, and a genetic algorithm, all with several optimization functions.
21 We used the lrm function from the Design package in R (http://cran.r-project.org/web/packages/
Design) for training and evaluating models based on logistic regression.
240
Verberne et al What Is Not in the Bag of Words forWhy-QA?
After applying our re-ranking module, we found a significant improvement over
bare TF-IDF in terms of success@10 and MRR@150 (z = ?4.29, p < 0.0001 using the
Wilcoxon Signed-Rank test for paired reciprocal ranks).
5.5 Which Features Made the Improvement?
In order to evaluate the importance of our features, we rank them according to the
coefficient that was assigned to them in the logistic regression model (See Table 2). We
only consider features that are significant at the p = 0.05 level. We find that all eight
significant features are among the top nine features with the highest coefficient.
The feature ranking is discussed in Section 6.1.
6. Discussion
In the following sections, we discuss the feature ranking (Section 6.1), make a compari-
son to other re-ranking approaches (Section 6.2), and explain the attempts that we made
at solving the remaining problems (Section 6.3).
6.1 Discussion of the Feature Ranking
Table 2 shows that only a small subset (8) of our 37 features significantly contribute to
the re-ranking score. The highest ranked feature is TF-IDF (the bag of words), which is
not surprising since TF-IDF alone already reaches an MRR@150 of 0.25 (see Section 3.2).
In Section 4.5, we predicted a valuable contribution from the addition of cue phrases,
question focus, noun phrases, and the document context of the answer. This is partly
confirmed by Table 2, which shows that among the significant features are the feature
that links question focus to document title and the cue phrases feature. The noun
phrases feature (f11) is actually in the top nine features with the highest coefficient but
its contribution was not significant at the 0.05 level (p = 0.068).
The importance of question focus for why-QA is especially interesting because it
is a question feature that is specific to why-questions and does not similarly apply
Table 2
Features that significantly contribute to the re-ranking score (p < 0.05), ranked by their
coefficient in the logistic regression model (representing their importance).
Feature Coefficient
TF-IDF (f0) 0.39**
Overlap between question focus synonyms and document title (f30) 0.25**
Overlap between question object synonyms and answer words (f28) 0.22
Overlap between question object and answer objects (f10) 0.18*
Overlap between question words and document title synonyms (f33) 0.17
Overlap between question verb synonyms and answer words (f24) 0.16
WordNet Relatedness (f35) 0.16*
Cue phrases (f36) 0.15*
Asterisks on coefficients denote the level of significance for the feature: ** p < 0.001; * 0.001 <
p < 0.01; no asterisk means 0.01 < p < 0.05.
241
Computational Linguistics Volume 36, Number 2
to factoids or other question types. Moreover, the link from the question focus to the
document title shows that Wikipedia as an answer source can provide QA systems with
more information than a collection of plain texts with less discriminative document
titles does.
The significance of cue phrases is also an important finding. In fact, including cue
phrases in the why-QA process is the only feasible way of specifying which passages
are likely to contain an explanation (i.e., an answer to a why-question). In earlier work
(Verberne et al 2007a), we pointed out that higher-level annotation such as discourse
structure can give useful information in the why-answer selection process. However,
the development of systems that incorporate discourse structure suffers from the lack
of tools for automated annotation. The current results show that surface patterns (the
literal presence of items from a fixed set of cue words) are a step in the direction of
answer selection.
The significant features in Table 2 also show us which question constituents are
the most salient for answer ranking: focus, main verb, and direct object. We think that
features incorporating the question?s subject are not found to be significant because, in
a subset of the questions, the subject is semantically poor. Moreover, because for most
questions the subject is the question focus, the subject features and the focus features are
correlated. In our data, the question focus apparently is the more powerful predictor.
6.2 Comparison to Other Approaches
The 23% improvement that we reach in terms of MRR@150 (from 0.25 to 0.34) is com-
parable to that reached by Tiedemann in his work on improving factoid QA with the
use of structural information.
In order to see whether the improvement that we achieved with re-ranking is
on account of structural information or just the benefit of using word sequences, we
experimented with a set of re-ranking features based on sequences of question words
that are not syntactically defined. In this re-ranking experiment, we included TF-IDF,
word bigrams, and word trigrams as features. The resulting performance was around
baseline level (MRR = 0.25), significantly worse than re-ranking with structural overlap
features. This is still true if we add the cue word feature (which, in isolation, only gives
a small improvement to baseline performance) to the n-gram features.
6.3 Solving the Remaining Problems
Although the results in terms of success@10 and MRR@150 are satisfactory, there is still
a substantial proportion of why-questions that is not answered in the top 10 result list.
In this section, we discuss a number of attempts that we made to further improve our
system.
First, after we found that for some question parts synonym expansion leads to
improvement (especially the main verb and direct object), we experimented with the
addition of synonyms for these constituents in the retrieval step of our system (Lemur).
We found, however, that it does not improve the results due to the large synonym sets
of many verbs and nouns which add much noise and lead to very long queries. The
same holds for the addition of cue words in the retrieval step.
Second, although our re-ranking module incorporates expansion to synonym sets,
there are many question?answer pairs where the vocabulary gap between the question
242
Verberne et al What Is Not in the Bag of Words forWhy-QA?
and the answer is still a problem. There are cases where semantically related terms in
the question and the answer are of different word classes (e.g., hibernate?hibernation),
and there are cases of proper nouns that are not covered by WordNet (e.g., B.B. King).
We considered using dynamic stemming for verb?noun relations such as the hibernation
case but research has shown that stemming hurts as many queries as it helps (Bilotti,
Katz, and Lin 2004). Therefore, we experimented with a number of different semantic
resources, namely, the nominalization dictionary Nomlex (Meyers et al 1998) and the
wikiOntology by Ponzetto and Strube (2007). However, in their current state of develop-
ment these semantic resources cannot improve our system because their coverage is too
low to make a contribution to our re-ranking module. Moreover, the present version of
the wikiOntology is very noisy and requires a large amount of cleaning up and filtering.
Third, we considered that the use of cue phrases may not be sophisticated enough
for finding explanatory relations between question and answer. Therefore, we exper-
imented with the addition of cause?effect pairs from the English version of the EDR
Concept Dictionary (Yokoi 1995) ? as suggested by Higashinaka and Isozaki (2008).
Unfortunately, the list appeared to be extremely noisy, proving it not useful as a source
for answer ranking.
7. Conclusions and Directions for Future Research
In the current research, we extended a passage retrieval system for why-QA using off-
the-shelf retrieval technology (Lemur/TF-IDF) with a re-ranking step incorporating
structural information. We get significantly higher scores in terms of MRR@150 (from
0.25 to 0.34) and success@10. The 23% improvement that we reach in terms of MRR
is comparable to that reached on various other QA tasks by other researchers in the
field (see Section 6.3). This confirms our hypothesis in Section 1 that for the relatively
complex problem of why-QA, a significant improvement can be gained by the addition
of structural information to the ranking component of the QA system.
Most of the features that we implemented for answer re-ranking are based on word
overlap between part of the question and part of the answer. As a result of this set-up,
our features identify the parts of why-questions and their candidate answers that are the
most powerful/effective for ranking the answers. The question constituents that appear
to be the most important are the question focus, the main verb, and the direct object. On
the answer side, most important are the title of the document in which the candidate
answer is embedded and knowledge on the presence of cue phrases.
Because our features are overlap-based, they are relatively easy to implement. For
implementation of some of the significant features, a form of syntactic parsing is needed
that can identify subject, verb, and direct object from the question and sentences in
the candidate answers. An additional set of rules is needed for finding the question
focus. Finally, we need a fixed list for identifying cue phrases. Exploiting the title of
answer documents in the feature set is only feasible if the documents that may contain
the answers have titles and section headings similar to Wikipedia.
In conclusion, we developed a method for significantly improving a BOW-based
approach to why-QA that can be implemented without extensive semantic knowledge
sources. Our series of experiments suggest that we have reached the maximum per-
formance that can be obtained using a knowledge-poor approach. Experiments with
more complex types of information (discourse structure, cause?effect relations) show
that these information sources have not as yet developed sufficiently to be exploited in
a QA system.
243
Computational Linguistics Volume 36, Number 2
References
Bilotti, M. W., B. Katz, and J. Lin. 2004.
What works better for question
answering: Stemming or morphological
query expansion. In Proceedings of the
Workshop on Information Retrieval for
Question Answering (IR4QA) at SIGIR 2004,
Sheffield.
Bilotti, M. W., P. Ogilvie, J. Callan, and
E. Nyberg. 2007. Structured retrieval for
question answering. In Proceedings of the
30th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 351?358,
Amsterdam.
Burnage, G., R. H. Baayen, R. Piepenbrock,
and H. van Rijn. 1990. CELEX: A Guide for
Users. CELEX, University of Nijmegen,
the Netherlands.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2003. Building
a discourse-tagged corpus in the
framework of Rhetorical Structure Theory.
In Jan van Kuppevelt and Ronnie Smith,
editors, Current Directions in Discourse and
Dialogue. Kluwer Academic Publishers,
Dordrecht, pages 85?112.
Charniak, E. 2000. A maximum-entropy-
inspired parser. ACM International
Conference Proceeding Series, 4:132?139.
Denoyer, L. and P. Gallinari. 2006. The
Wikipedia XML corpus. ACM SIGIR
Forum, 40(1):64?69.
Ferret, O., B. Grau, M. Hurault-Plantet,
G. Illouz, L. Monceaux, I. Robba, and
A. Vilnat. 2002. Finding an answer
based on the recognition of the question
focus. NIST Special Publication,
pages 362?370.
Higashinaka, R. and H. Isozaki. 2008.
Corpus-based question answering for
why-questions. In Proceedings of IJCNLP,
pages 418?425, Hyderabad.
Hovy, E. H., U. Hermjakob, and
D. Ravichandran. 2002. A question/
answer typology with surface text
patterns. In Proceedings of the Human
Language Technology conference (HLT),
pages 247?251, San Diego, CA.
Joachims, T., L. Granka, B. Pan,
H. Hembrooke, and G. Gay. 2005.
Accurately interpreting clickthrough data
as implicit feedback. In Proceedings of the
28th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 154?161,
Salvador, Brazil.
Khalid, M. and S. Verberne. 2008. Passage
retrieval for question answering using
Sliding Windows. In Proceedings of the
COLING 2008 Workshop IR4QA,
Manchester, UK.
Liu, T. Y., J. Xu, T. Qin, W. Xiong, and H. Li.
2007. Letor: Benchmark dataset for
research on learning to rank for
information retrieval. In Proceedings of
the Workshop on Learning to Rank for
Information Retrieval (LR4IR) at SIGIR 2007,
pages 3?10, Amsterdam.
Meyers, A., C. Macleod, R. Yangarber,
R. Grishman, L. Barrett, and R. Reeves.
1998. Using NOMLEX to produce
nominalization patterns for information
extraction. In Proceedings: The
Computational Treatment of Nominals,
volume 2, pages 25?32, Montreal.
Pedersen, T., S. Patwardhan, and
J. Michelizzi. 2004. WordNet::Similarity ?
measuring the relatedness of concepts. In
Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025,
San Jose, CA.
Ponzetto, S. P. and M. Strube. 2007. Deriving
a large scale taxonomy fromWikipedia.
In Proceedings of the National Conference on
Artificial Intelligence, pages 1440?1445,
Vancouver, BC.
Quarteroni, S., A. Moschitti, S. Manandhar,
and R. Basili. 2007. Advanced structural
representations for question classification
and answer re-ranking. In Proceedings of
ECIR 2007, pages 234?245, Rome.
Salton, G. and C. Buckley. 1988.
Term-weighting approaches in automatic
text retrieval. Information Processing and
Management, 24(5):513?523.
Surdeanu, M., M. Ciaramita, and
H. Zaragoza. 2008. Learning to rank
answers on large online QA collections. In
Proceedings of ACL 2008, pages 719?727,
Columbus, OH.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and
G. Marton. 2003. Quantitative evaluation
of passage retrieval algorithms for
question answering. In Proceedings of the
26th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 41?47,
Toronto.
Tiedemann, J. 2005. Improving passage
retrieval in question answering using
NLP. In Progress in Artificial Intelligence,
volume 3808. Springer, Berlin /
Heidelberg, pages 634?646.
Verberne, S. 2006. Developing an approach
for why-question answering. In Conference
Companion of the 11th Conference of the
European Chapter of the Association for
244
Verberne et al What Is Not in the Bag of Words forWhy-QA?
Computational Linguistics (EACL 2006),
pages 39?46, Trento.
Verberne, S., L. Boves, N. Oostdijk, and
P. A. Coppen. 2007a. Discourse-based
answering of why-questions. Traitement
Automatique des Langues (TAL), special issue
on ?Discours et document: traitements
automatiques?, 47(2):21?41.
Verberne, S., L. Boves, N. Oostdijk, and
P. A. Coppen. 2007b. Evaluating
discourse-based answer extraction for
why-question answering. In Proceedings of
the 30th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 735?736,
Amsterdam.
Verberne, S., H. Van Halteren, D. Theijssen,
S. Raaijmakers, and L. Boves. 2009.
Learning to rank QA data. In Proceedings
of the Workshop on Learning to Rank for
Information Retrieval (LR4IR) at SIGIR 2009,
pages 41?48, Boston, MA.
Yokoi, T. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
Zhai, C. 2001. Notes on the Lemur TFIDF
model. Technical report, School of
Computer Science, Carnegie Mellon
University.
245

Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 8?16,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Estimating Time to Event from Tweets Using Temporal Expressions
Ali H?urriyeto
?
glu, Nelleke Oostdijk, and Antal van den Bosch
Centre for Language Studies
Radboud University Nijmegen
P.O. Box 9103, NL-6500 HD Nijmegen, The Netherlands
{a.hurriyetoglu,n.oostdijk,a.vandenbosch}@let.ru.nl
Abstract
Given a stream of Twitter messages about
an event, we investigate the predictive
power of temporal expressions in the mes-
sages to estimate the time to event (TTE).
From labeled training data we learn av-
erage TTE estimates of temporal expres-
sions and combinations thereof, and de-
fine basic rules to compute the time to
event from temporal expressions, so that
when they occur in a tweet that mentions
an event we can generate a prediction. We
show in a case study on soccer matches
that our estimations are off by about eight
hours on average in terms of mean abso-
lute error.
1 Introduction
Textual information streams such as those pro-
duced by news media and by social media reflect
what is happening in the real world. These streams
often contain explicit pointers to future events that
may interest or concern a potentially large amount
of people. Besides media-specific markers such as
event-specific hashtags in messages on Twitter
1
,
these messages may contain explicit markers of
place and time that help the receivers of the mes-
sage disambiguate and pinpoint the event on the
map and calendar.
The automated analysis of streaming text mes-
sages can play a role in catching these important
events. Part of this analysis may be the identifi-
cation of the future start time of the event, so that
the event can be placed on the calendar and appro-
priate action may be taken by the receiver of the
message, such as ordering tickets, planning a se-
curity operation, or starting a journalistic investi-
gation. The automated identification of the time to
event (TTE) should be as accurate and come early
1
http://twitter.com
as possible. In this paper we explore a hybrid rule-
based and data-driven method that exploits the ex-
plicit mentioning of temporal expressions to arrive
at accurate and early TTE estimations.
The idea of publishing future calendars with po-
tentially interesting events gathered (semi-) auto-
matically for subscribers, possibly with personal-
ization features and the option to harvest both so-
cial media and the general news, has been imple-
mented already and is available through services
such as Zapaday
2
, Daybees
3
, and Songkick
4
. To
our knowledge, based on the public interfaces of
these platforms, these services perform directed
crawls of (structured) information sources, and
identify exact date and time references in posts on
these sources. They also manually curate event in-
formation, or collect this through crowdsourcing.
In this study we do not use a rule-based tempo-
ral tagger such as the HeidelTime tagger (Str?otgen
and Gertz, 2013), which searches for only a lim-
ited set of temporal expressions. Instead, we pro-
pose an approach that uses a large set of temporal
expressions, created by using seed terms and gen-
erative rules, and a training method that automati-
cally determines the TTE estimate to be associated
with each temporal expression sequence in a data-
driven way. Typically, rule-based systems do not
use the implicit information provided by adverbs
(?more? in ?three more days?) and relations be-
tween non-subsequent elements, while machine-
learning-based systems do not make use of the
temporal logic inherent to temporal expressions;
they may identify ?three more days? as a temporal
expression but they lack the logical apparatus to
compute that this implies a TTE of about 3 ? 24
hours. To make use of the best of both worlds
we propose a hybrid system which uses informa-
tion about the distribution of temporal expressions
2
http://www.zapaday.com
3
http://daybees.com/
4
https://www.songkick.com/
8
as they are used in forward-looking social media
messages in a training set of known events, and
combines this estimation method with an exten-
sive set of regular expressions that capture a large
space of possible Dutch temporal expressions.
Thus, our proposed system analyzes social me-
dia text to find information about future events,
and estimates how long it will take before the
event takes place. The service offered by this sys-
tem will be useful only if it generates accurate es-
timations of the time to event. Preferably, these
accurate predictions should come as early as pos-
sible. Moreover, the system should be able, in
the long run, to freely detect relevant future events
that are not yet on any schedule we know in any
language represented on social media. For now,
in this paper we focus on estimating the start-
ing time of scheduled events, and use past and
known events for a controlled experiment involv-
ing Dutch twitter messages.
For our experiment we collected tweets refer-
ring to scheduled Dutch premier league soccer
matches. This type of event generally triggers
many anticipatory discussions on social media
containing many temporal expressions. Given a
held-out soccer match not used during training,
our system predicts the time to the event based on
individual tweets captured in a range from eight
days before the event to the event time itself. Each
estimation is based on the temporal expressions
which occur in a particular twitter message. The
mean absolute error of the predictions for each of
the 60 soccer matches in our data set is off by
about eight hours. The results are generated in a
leave-one-out cross-validation setup
5
.
This paper starts with describing the relation of
our work to earlier research in Section 2. Section 3
describes the overall experimental setup, including
a description of the data, the temporal expressions
that were used, our two baselines, and the evalua-
tion method used. Next, in Section 4 the results are
presented. The results are analyzed and discussed
in Section 5. We conclude with a summary of our
main findings and make suggestions for the direc-
tion future research may take (Section 6).
5
Tweet ID?s, per tweet estimations, occurred time ex-
pressions and rules can be found at http://www.ru.nl/
lst/resources/
2 Related Work
Future-reference analysis in textual data has been
studied from different angles. In the realm of
information retrieval the task is more commonly
defined as seeking future temporal references in
large document collections such as the Web by
means of time queries (Baeza Yates, 2005). Var-
ious studies have used temporal expression ele-
ments as features in an automatic setting to im-
prove the relevance estimation of a web docu-
ment (Dias et al., 2011; Jatowt and Au Yeung,
2011). Information relevant to event times has
been the focus of studies such as those by Becker
et al. (2012) and Kawai et al. (2010).
Our research is aimed at estimating the time to
event of an upcoming event as precisely as possi-
ble. Radinsky et al. (2012) approach this problem
by learning from causality pairs in texts from long-
ranging news articles. Noro et al. (2006) describe
a machine-learning-based system for the identifi-
cation of the time period in which an event will
happen, such as in the morning or at night.
Some case studies are focused on detecting
events as early as possible as their unfolding is
fast. The study by Sakaki et al. (2010) describes a
system which analyzes the flow of tweets in time
and place mentioning an earthquake, to predict the
unfolding quake pattern which may in turn provide
just-in-time alerts to people residing in the loca-
tions that are likely to be struck shortly. Zielinski
et al. (2012) developed an early warning system
to detect natural disasters in a multilingual fash-
ion and thereby support crisis management. The
quick throughput of news in the Twitter network
is the catalyst in these studies focusing on natu-
ral disasters. In our study, we rather rely on the
slower build-up of clues in messages in days be-
fore an event, at a granularity level of hours.
Ritter et al. (2012) aim to create a calendar of
events based on explicit date mentions and words
typical of the event. They train on annotated open
domain event mentions and use a rule-based tem-
poral tagger. We aim to offer a more generic so-
lution that makes use of a wider range of tempo-
ral expressions, including indirect and implicit ex-
pressions.
Weerkamp and De Rijke (2012) study this type
of more generic patterns of anticipation in tweets,
but focus on personal future activities, while we
aim to predict as early as possible the time to
event of events that affect and interest many users.
9
Our estimations do not target time periods such as
mornings or evenings but on the number of hours
remaining to the event.
TTE estimation of soccer matches has been the
topic of several studies. Kunneman and Van den
Bosch (2012) show that machine learning meth-
ods can differentiate between tweets posted be-
fore, during, and after a soccer match. Estimat-
ing the time to event of future matches from tweet
streams has been studied by H?urriyetoglu et al.
(2013), using local regression over word time se-
ries. In a related study, Tops et al. (2013) use sup-
port vector machines to classify the time to event
in automatically discretized categories. At best
these studies are about a day off in their predic-
tions. Both studies investigate the use of temporal
expressions, but fail to leverage the utility of this
information source, most likely because they use
limited sets of less than 20 regular expressions. In
this study we scale up the number of temporal ex-
pressions.
3 Experimental Set-Up
We carried out a controlled case study in which we
focused on Dutch premier league soccer matches
as a type of scheduled event. These types of games
have the advantage that they occur frequently,
have a distinctive hashtag by convention, and often
generate thousands to several tens of thousands of
tweets per match.
Below we first describe the collection and com-
position of our data sets (Subsection 3.1) and the
temporal expressions which were used to base our
predictions upon (Subsection 3.2). Then, in Sub-
section 3.3, we describe our baselines and evalua-
tion method.
3.1 Data Sets
We harvested tweets from twiqs.nl
6
, a database
of Dutch tweets collected from December 2010
onwards. We selected the six best performing
teams of the Dutch premier league in 2011 and
2012
7
, and queried all matches in which these
teams played against each other in the calendar
years 2011 and 2012. The collection procedure re-
sulted in 269,999 tweets referring to 60 individual
matches. The number of tweets per event ranges
from 321 to 35,464, with a median of 2,723 tweets.
6
http://twiqs.nl
7
Ajax, Feyenoord, PSV, FC Twente, AZ Alkmaar, and FC
Utrecht.
Afterwards, we restricted the data to tweets sent
within eight days before the match
8
and elimi-
nated all retweets. This reduced the number of
tweets in our final data set to 138,141 tweets.
In this experiment we are working on the as-
sumption that the presence of a hashtag can be
used as proxy for the topic addressed in a tweet.
Inspecting a sample of tweets referring to recent
soccer games not part of our data set, we devel-
oped the hypothesis that the position of the hash-
tag may have an effect as regards the topicality of
the tweet. Hashtags that occur in final position (i.e.
they are tweet-final or are only followed by one
or more other hashtags) are typically metatags and
therefore possibly more reliable as topic identifiers
than tweet non-final hashtags which behave more
like common content words in context. In order
to be able to investigate the possible effect that the
position of the hashtag might have, we split our
data in the following two subsets:
FIN ? comprising tweets in which the hashtag
occurs in final position (as defined above);
84,533 tweets.
NFI ? comprising tweets in which the hashtag oc-
curs in non-final position; 53,608 tweets.
Each tweet in our data set has a time stamp of
the moment (in seconds) it was posted. Moreover,
for each soccer match we know exactly when it
took place. This information is used to calculate
for each tweet the actual time that remains to the
start of the event and the absolute error in estimat-
ing the time to event.
3.2 Temporal Expressions
In the context of this paper temporal expressions
are considered to be words or phrases which point
to the point in time, the duration, or the frequency
of an event. These may be exact, approximate, or
even right out vague. Although in our current ex-
periment we restrict ourselves to an eight-day pe-
riod prior to an event, we chose to create a gross
list of all possible temporal expressions we could
think of, so that we would not run the risk of over-
looking any items and the list can be used on fu-
ture occasions even when the experimental set-
ting is different. Thus the list also includes tem-
poral expressions that refer to points in time out-
side the time span under investigation here, such
8
An analysis of the tweet distribution shows that the eight-
day window captures about 98% of the tweets in the larger
data set from which it was derived.
10
as gisteren ?yesterday? or over een maand ?in a
month from now?, and items indicating duration
or frequency such as steeds ?continuously?/?time
and again?. No attempt has been made to distin-
guish between items as regards time reference (fu-
ture time, past time) as many items can be used in
both fashions (compare for example vanmiddag in
vanmiddag ga ik naar de wedstrijd ?this afternoon
I?m going to the match? vs ik ben vanmiddag naar
de wedstrijd geweest ?I went to the match this af-
ternoon?.
The list is quite comprehensive. Among the
items included are single words, e.g. adverbs
such as nu ?now?, zometeen ?immediately?, straks
?later on?, vanavond ?this evening?, nouns such as
zondagmiddag ?Sunday afternoon?, and conjunc-
tions such as voordat ?before?), but also word com-
binations and phrases such as komende woensdag
?next Wednesday. Temporal expressions of the lat-
ter type were obtained by means of a set of 615
seed terms and 70 rules, which generated a total of
around 53,000 temporal expressions. In addition,
there are a couple of hundred thousand temporal
expressions relating the number of minutes, hours,
days, or time of day;
9
they include items contain-
ing up to 9 words in a single temporal expression.
Notwithstanding the impressive number of items
included, the list is bound to be incomplete.
We included prepositional phrases rather than
single prepositions so as to avoid generating too
much noise. Many prepositions have several uses:
they can be used to express time, but also for
example location. Compare voor in voor drie
uur ?before three o?clock? and voor het stadion
?in front of the stadium?. Moreover, prepositions
are easily confused with parts of separable verbs
which in Dutch are abundant.
Various items on the list are inherently ambigu-
ous and only in one of their senses can be con-
sidered temporal expressions. Examples are week
?week? but also ?weak? and dag ?day? but also
?goodbye?. For items like these, we found that
the different senses could fairly easily be distin-
guished whenever the item was immediately pre-
ceded by an adjective such as komende and vol-
gende (both meaning ?next?). For a few highly
frequent items this proved impossible. These are
words like zo which can be either a temporal ad-
verb (?in a minute?; cf. zometeen) or an intensi-
fying adverb (?so?), dan ?then? or ?than?, and nog
9
For examples see Table 1 and Section 3.3.
?yet? or ?another?. As we have presently no way
of distinguishing between the different senses and
these items have at best an extremely vague tem-
poral sense so that they cannot be expected to con-
tribute to estimating the time to event, we deciced
to discard these.
10
In order to capture event targeted expressions,
we treated domain terms such as wedstrijd ?soc-
cer match? as parts of temporal expressions in case
they co-occur with a temporal expression.
For the items on the list no provisions were
made for handling any kind of spelling variation,
with the single exception of a small group of
words (including ?s morgens ?in the morning?, ?s
middags ?in the afternoon? and ?s avonds ?in the
evening?) which use in their standard spelling the
archaic ?s and abbreviations. As many authors
of tweets tend to spell these words as smorgens,
smiddags and savonds we decided to include these
forms as well.
The items on the list that were obtained through
generation include temporal expressions such as
over 3 dagen ?in 2 days?, nog 5 minuten ?another
5 minutes?, but also fixed temporal expressions
such as clock times.
11
The rules handle frequently
observed variations in their notation, for example
drie uur ?three o?clock? may be written in full or
as 3:00, 3:00 uur, 3 u, 15.00, etc.
Table 1 shows example temporal expression es-
timates and applicable rules. The median estima-
tions are mostly lower than the mean estimations.
The distribution of the time to event (TTE) for
a single temporal expression often appears to be
skewed towards lower values. The final column
of the table displays the applicable rules. The first
six rules subtract the time the tweet was posted
(TT) from an average marker point, heuristically
determined, such as ?today 20.00? (i.e. 8 pm) for
vanavond ?tonight?. The second and third rules
from below state a TTE directly, again heuristi-
cally set ? over 2 uur ?in 2 hours? is directly trans-
lated to a TTE of 2.
3.3 Evaluation and Baselines
Our approach to TTE estimation makes use of
all temporal expressions in our temporal expres-
sion list that are found to occur in the tweets. A
10
Note that nog does occur on the list as part of various
multiword expressions. Examples are nog twee dagen ?an-
other two days? and nog 10 min ?10 more minutes?.
11
Dates are presently not covered by our rules but will be
added in future.
11
Temporal Expression Gloss Mean TTE Median TTE Rule
vandaag today 5.63 3.09 today 15:00 - TT h
vanavond tonight 8.40 4.78 today 20:00 - TT h
morgen tomorrow 20.35 18.54 tomorrow 15:00 - TT h
zondag Sunday 72.99 67.85 Sunday 15:00 - TT h
vandaag 12.30 today 12.20 2.90 2.75 today 12:30 - TT h
om 16.30 at 16.30 1.28 1.36 today 16:30 - TT h
over 2 uur in 2 hours 6.78 1.97 2 h
nog minder dan 1 u within 1 h 21.43 0.88 1 h
in het weekend during the weekend 90.58 91.70 No Rule
Table 1: Examples of temporal expressions and their mean and median TTE estimation from training
data. The final column lists the applicable rule, if any. Rules make use of the time of posting (Tweet
Time, TT).
match may be for a single item in the list (e.g.
zondag ?Sunday?) or any combination of items
(e.g. zondagmiddag, om 14.30 uur, ?Sunday af-
ternoon?, ?at 2.30 pm?). There can be other words
in between these expressions. We consider the
longest match, from left to right, in case we en-
counter any overlap.
The experiment adopts a leave-one-out cross-
validation setup. Each iteration uses all tweets
from 59 events as training data. All tweets from
the single held-out event are used as test set.
In the FIN data set there are 42,396 tweets with
at least one temporal expression, in the NFI data
set this is the case for 27,610 tweets. The number
of tweets per event ranges from 66 to 7,152 (me-
dian: 402.5; mean 706.6) for the FIN data set and
from 41 to 3,936 (median 258; mean 460.1) for the
NFI data set.
We calculate the TTE estimations for every
tweet that contains at least one of the temporal ex-
pression or a combination in the test set. The esti-
mations for the test set are obtained as follows:
1. For each match (a single temporal expression
or a combination of temporal expressions)
the mean or median value for TTE is used
that was learned from the training set;
2. Temporal expressions that denote an exact
amount of time are interpreted by means of
rules that we henceforth refer to as Exact
rules. This applies for example to tempo-
ral expressions answering to patterns such as
over N {minuut | minuten | kwartier | uur |
uren | dag | dagen | week} ?in N {minute |
minutes | quarter of an hour | hour | hours |
day | days |week}?. Here the TTE is assumed
to be the same as the N minutes, days or
whatever is mentioned. The rules take prece-
dence over the mean estimates learned from
the training set;
3. A second set of rules, referred to as the Dy-
namic rules, is used to calculate the TTE dy-
namically, using the temporal expression and
the tweet?s time stamp. These rules apply
to instances such as zondagmiddag om 3 uur
?Sunday afternoon at 3 p.m.?. Here we as-
sume that this is a future time reference on the
basis of the fact that the tweets were posted
prior to the event. With temporal expressions
that are underspecified in that they do not pro-
vide a specific point in time (hour), we pos-
tulate a particular time of day. For exam-
ple, vandaag ?today? is understood as ?today
at 3 p.m., vanavond ?this evening? as ?this
evening at 8 p.m. and morgenochtend ?to-
morrow morning? as ?tomorrow morning at
10 a.m.?. Again, as was the case with the first
set of rules, these rules take precedence over
the mean or median estimates learned from
the training data.
The results for the estimated TTE are evaluated
in terms of the absolute error, i.e. the absolute dif-
ference in hours between the estimated TTE and
the actual remaining time to the event.
We established two naive baselines: the mean
and median TTE measured over all tweets of FIN
and NFI datasets. These baselines reflect a best
guess when no information is available other than
tweet count and TTE of each tweet. The mean
TTE is 22.82 hours, and the median TTE is 3.63
hours before an event. The low values of the
12
baselines, especially the low median, reveal the
skewedness of the data: most tweets referring to
a soccer event are posted in the hours before the
event.
4 Results
Table 2 lists the overall mean absolute error (in
number of hours) for the different variants. The
results are reported separately for each of the two
data sets (FIN and NFI) and for both sets aggre-
gated (FIN+NFI). For each of these three variants,
the table lists the mean absolute error when only
the basic data-driven TTE estimations are used
(?Basic?), when the Exact rules are added (?+Ex.?),
when the Dynamic rules are added (?+Dyn?), and
when both types of rules are added. The coverage
of the combination (i.e. the number of tweets that
match the expressions and the rules) is listed in the
bottom row of the table.
A number of observations can be made. First,
all training methods perform substantially better
than the two baselines in all conditions. Second,
the TTE training method using the median as esti-
mation produces estimations that are about 1 hour
more accurate than the mean-based estimations.
Third, adding Dynamic rules has a larger pos-
itive effect on prediction error than adding Ex-
act rules. The bottom row in the table indicates
that the rules do not increase the coverage of the
method substantially. When taken together and
added to the basic TTE estimation, the Dynamic
and Exact rules do improve over the Basic estima-
tion by two to three hours.
Finally, although the differences are small, Ta-
ble 2 reveals that training on hashtag-final tweets
(FIN) produces slightly better overall results (7.62
hours off at best) than training on hashtag-non-
final tweets (8.50 hours off) or the combination
(7.99 hours off), despite the fact that the training
set is smaller than that of the combination.
In the remainder of this section we report on
systems that use all expressions and Exact and Dy-
namic rules.
Whereas Table 2 displays the overall mean ab-
solute errors of the different variants, Figure 1 dis-
plays the results in terms of mean absolute error at
different points in time before the event, averaged
over periods of one hour, for the two baselines and
the FIN+NFI variant with the two training meth-
ods (i.e. taking the mean versus the median of the
observed TTEs for a particular temporal expres-
sion). In contrast to Table 2, in which only a mild
difference could be observed between the median
and mean variants of training, the figure shows a
substantial difference. The estimations of the me-
dian training variant are considerably more accu-
rate than the mean variant up to 24 hours before
the event, after which the mean variant scores bet-
ter. By virtue of the fact that the data is skewed
(most tweets are posted within a few hours before
the event) the two methods attain a similar overall
mean absolute error, but it is clear that the median
variant produces considerably more accurate pre-
dictions when the event is still more than a day
away.
While Figure 1 provides insight into the ef-
fect of median versus mean-based training with
the combined FIN+NFI dataset, we do not know
whether training on either of the two subsets is
advantageous at different points in time. Table 3
shows the mean absolute error of systems trained
with the median variant on the two subsets of
tweets, FIN and NFI, as well as the combination
FIN+NFI, split into nine time ranges. Interest-
ingly, the combination does not produce the lowest
errors close to the event. However, when the event
is 24 hours away or more, both the FIN and NFI
systems generate increasingly large errors, while
the FIN+NFI system continues to make quite ac-
curate predictions, remaining under 10 hours off
even for the longest TTEs, confirming what we al-
ready observed in Figure 1.
TTE range (h) FIN NFI FIN+NFI
0 2.58 3.07 8.51
1?4 2.38 2.64 8.71
5?8 3.02 3.08 8.94
9?12 5.20 5.47 6.57
13?24 5.63 5.54 6.09
25?48 13.14 15.59 5.81
49?96 17.20 20.72 6.93
97?144 30.38 41.18 6.97
> 144 55.45 70.08 9.41
Table 3: Mean Absolute Error for the FIN, NFI,
and FIN+NFI systems in different TTE ranges.
5 Analysis
One of the results observed in Table 2 was the
relatively limited role of Exact rules, which were
intended to deal with exact temporal expressions
such as nog 5 minuten ?5 more minutes? and over
13
System FIN NFI FIN+NFI
Basic +Ex. +Dyn. +Both Basic +Ex. +Dyn. +Both Basic +Ex. +Dyn. +Both
Baseline Median 21.09 21.07 21.16 21.14 18.67 18.72 18.79 18.84 20.20 20.20 20.27 20.27
Baseline Mean 27.29 27.29 27.31 27.31 25.49 25.50 25.53 25.55 26.61 26.60 26.63 26.62
Training Median 10.38 10.28 7.68 7.62 11.09 11.04 8.65 8.50 10.61 10.54 8.03 7.99
Training Mean 11.62 11.12 8.73 8.29 12.43 11.99 9.53 9.16 11.95 11.50 9.16 8.76
Coverage 31,221 31,723 32.240 32,740 18,848 19,176 19,734 20,061 52,186 52,919 53,887 54,617
Table 2: Overall Mean Absolute Error for each method: difference in hours between the estimated time
to event and the actual time to event, computed separately for the FIN and NFI subsets, and for the
combination. For all variants a count of the number of matches is listed in the bottom row.
een uur ?in one hour?. This can be explained by
the fact that as long as the temporal expression is
related to the event we are targeting, the point in
time is denoted exactly by the temporal expression
and the estimation obtained from the training data
(the ?Basic? performance) will already be accurate,
leaving no room for the rules to improve on this.
The rules that deal with dynamic temporal expres-
sions, on the other hand, have quite some impact.
As was explained in Section 3.2 our list of tem-
poral expressions was a gross list, including items
that were unlikely to occur in our present data. In
all we observed 770 of the 53,000 items listed,
955 clock time rule matches, and 764 time ex-
pressions which contain number of days, hours,
minutes etc. The temporal expressions observed
most frequently in our data are:
12
vandaag ?today?
(10,037), zondag ?Sunday? (6,840), vanavond
?tonight? (5167), straks ?later on? (5,108), van-
middag ?this afternoon? (4,331), matchday ?match
day? (2,803), volgende week ?next week? (1,480)
and zometeen ?in a minute? (1,405).
Given the skewed distribution of tweets over the
eight days prior to the event, it is not surprising to
find that nearly all of the most frequent items refer
to points in time within close range of the event.
Apart from nu ?now?, all of these are somewhat
vague about the exact point in time. There are,
however, numerous items such as om 12:30 uur
?at half past one? and over ongeveer 45 minuten
?in about 45 minutes?) which are very specific and
therefore tend to appear with middle to low fre-
quencies.
13
And while it is possible to state an
exact point in time even when the event is in the
more distant future, we find that there is a clear
12
The observed frequencies can be found between brack-
ets.
13
While an expression such as om 12:30 uur has a fre-
quency of 116, nog maar 8 uur en 35 minuten ?only 8 hours
and 35 minutes from now? has a frequency of 1.
tendency to use underspecified temporal expres-
sions as the event is still some time away. Thus,
rather than volgende week zondag om 14.30 uur
?next week Sunday at 2.30 p.m.? just volgende
week is used, which makes it harder to estimate
the time to event.
Closer inspection of some of the temporal
expressions which yielded large absolute errors
suggests that these may be items that refer to
subevents rather than the main event (i.e. the
match) we are targeting. Examples are eerst ?first?,
daarna ?then?, vervolgens ?next?, and voordat ?be-
fore?.
6 Conclusions and Future Work
We have presented a method for the estimation of
the TTE from single tweets referring to a future
event. In a case study with Dutch soccer matches,
we showed that estimations can be as accurate as
about eight hours off, averaged over a time win-
dow of eight days. There is some variance in
the 60 events on which we tested in a leave-one-
out validation setup: errors ranged between 4 and
13 hours, plus one exceptionally badly predicted
event with a 34-hour error.
The best system is able to stay within 10 hours
of prediction error in the full eight-day window.
This best system uses a large set of hand-designed
temporal expressions that in a training phase have
each been linked to a median TTE with which
they occur in a training set. Together with these
data-driven TTE estimates, the system uses a set
of rules that match on exact and indirect time ref-
erences. In a comparative experiment we showed
that this combination worked better than only hav-
ing the data-driven estimations.
We then tested whether it was more profitable
to train on tweets that had the event hashtag at the
end, as this is presumed to be more likely a meta-
14
Figure 1: Curves showing the absolute error (in hours) in estimating the time to event over an 8-day
period (-192 to 0 hours) prior to the event. The two baselines are compared to the TTE estimation
methods using the mean and median variant.
tag, and thus a more reliable clue that the tweet
is about the event than when the hashtag is not
in final position. Indeed we find that the overall
predictions are more accurate, but only in the fi-
nal hours before the event (when most tweets are
posted). 24 hours and earlier before the event it
turns out to be better to train both on hashtag-final
and hashtag-non-final tweets.
Finally, we observed that the two variants of
our method of estimating TTEs for single tempo-
ral expressions, taking the mean or the median,
leads to dramatically different results, especially
when the event is still a few days away?when
an accurate time to event is actually desirable.
The median-based estimations, which are gener-
ally smaller than the mean-based estimations, lead
to a system that largely stays under 10 hours of
error.
Our study has a number of logical extensions
into future research. First, our method is not
bound to a single type of event, although we tested
it in a controlled setting. With experiments on
tweet streams related to different types of events
the general applicability of the method could be
tested: can we use the trained TTE estimations
from our current study, or would we need to re-
train per event type?
Second, we hardcoded a limited number of fre-
quent spelling variations, where it would be a
more generic solution to rely on a more system-
atic spelling normalization preprocessing step.
Third, so far we did not focus on determining
the relevance of temporal expressions in case there
are several time expressions in a single message;
we treated all occurred temporal expressions as
equally contributing to the estimation. Identifying
which temporal expressions are relevant in a sin-
gle message is studied by Kanhabua et al. (2012).
Finally, our method is limited to temporal ex-
pressions. For estimating the time to event on
the basis of tweets that do not contain tempo-
ral expressions, we could benefit from term-based
approaches that consider any word or word n-
gram as potentially predictive (H?urriyetoglu et al.,
2013).
Acknowledgment
This research was supported by the Dutch na-
tional programme COMMIT as part of the Infiniti
project.
15
References
Ricardo Baeza Yates. 2005. Searching the future. In
In ACM SIGIR Workshop on Mathematical/Formal
Methods for Information Retrieval (MF/IR 2005).
Hila Becker, Dan Iter, Mor Naaman, and Luis Gravano.
2012. Identifying content for planned events across
social media sites. In Proceedings of the fifth ACM
International Conference on Web Search and Data
Mining, WSDM ?12, pages 533?542, New York,
NY, USA. ACM.
Ga?el Dias, Ricardo Campos, and Al??pio Jorge. 2011.
Future retrieval: What does the future talk about? In
In Proceedings SIGIR2011 Workshop on Enriching
Information Retrieval (ENIR2011).
Ali H?urriyetoglu, Florian Kunneman, and Antal
van den Bosch. 2013. Estimating the time between
twitter messages and future events. In DIR, pages
20?23.
Adam Jatowt and Ching-man Au Yeung. 2011. Ex-
tracting collective expectations about the future from
large text collections. In Proceedings of the 20th
ACM International Conference on Information and
Knowledge Management, CIKM ?11, pages 1259?
1264, New York, NY, USA. ACM.
Nattiya Kanhabua, Sara Romano, and Avar?e Stewart.
2012. Identifying relevant temporal expressions for
real-world events. In Proceedings of The SIGIR
2012 Workshop on Time-aware Information Access,
Portland, OR.
Hideki Kawai, Adam Jatowt, Katsumi Tanaka, Kazuo
Kunieda, and Keiji Yamada. 2010. Chronoseeker:
Search engine for future and past events. In Pro-
ceedings of the 4th International Conference on
Uniquitous Information Management and Commu-
nication, ICUIMC ?10, pages 25:1?25:10, New
York, NY, USA. ACM.
Florian A Kunneman and Antal van den Bosch. 2012.
Leveraging unscheduled event prediction through
mining scheduled event tweets. BNAIC 2012 The
24th Benelux Conference on Artificial Intelligence,
page 147.
Taichi Noro, Takashi Inui, Hiroya Takamura, and Man-
abu Okumura. 2006. Time period identification of
events in text. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 1153?1160,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proceedings of the
21st International Conference on World Wide Web,
WWW ?12, pages 909?918, New York, NY, USA.
ACM.
Alan Ritter, Oren Etzioni Mausam, and Sam Clark.
2012. Open domain event extraction from twitter.
In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data mining, KDD ?12, pages 1104?1112, New
York, NY, USA. ACM.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, pages 851?860. ACM.
Jannik Str?otgen and Michael Gertz. 2013. Multilin-
gual and Cross-domain Temporal Tagging. Lan-
guage Resources and Evaluation, 47(2):269?298,
Jun.
Hannah Tops, Antal van den Bosch, and Florian Kun-
neman. 2013. Predicting time-to-event from twitter
messages. BNAIC 2013 The 24th Benelux Confer-
ence on Artificial Intelligence, pages 207?2014.
Wouter Weerkamp and Maarten De Rijke. 2012. Ac-
tivity prediction: A twitter-based exploration. In
Proceedings of the SIGIR 2012 Workshop on Time-
aware Information Access, TAIA-2012, August.
Andrea Zielinski, Ulrich B?ugel, L. Middleton, S. E.
Middleton, L. Tokarchuk, K. Watson, and F. Chaves.
2012. Multilingual analysis of twitter news in sup-
port of mass emergency events. In A. Abbasi and
N. Giesen, editors, EGU General Assembly Confer-
ence Abstracts, volume 14 of EGU General Assem-
bly Conference Abstracts, pages 8085+, April.
16
