Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
Automatically Inducing Ontologies from Corpora 
Inderjeet Mani 
 
 
Department of Linguistics 
Georgetown University, ICC 452 
37th and O Sts, NW 
Washington, DC 20057, USA 
im5@georgetown.edu 
Ken Samuel, Kris Concepcion and 
David Vogel 
 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102, USA 
{samuel, kjc9, dvogel}@mitre.org 
 
Abstract 
The emergence of vast quantities of on-line 
information has raised the importance of methods 
for automatic cataloguing of information in a 
variety of domains, including electronic commerce 
and bioinformatics. Ontologies can play a critical 
role in such cataloguing. In this paper, we describe 
a system that automatically induces an ontology 
from any large on-line text collection in a specific 
domain. The ontology that is induced consists of 
domain concepts, related by kind-of and part-of 
links. To achieve domain-independence, we use a 
combination of relatively shallow methods along 
with any available repositories of applicable 
background knowledge. We describe our 
evaluation experiences using these methods, and 
provide examples of induced structures.  
1 Introduction 
The emergence of vast quantities of on-line 
information has raised the importance of methods 
for automatic cataloguing of information in a 
variety of domains, including electronic commerce 
and bioinformatics. Ontologies1 can play a critical 
role in such cataloguing. In bioinformatics, for 
example, there is growing recognition that 
common ontologies, e.g., the Gene Ontology2, are 
critical to interoperation and integration of 
biological data, including both structured data as 
found in protein databases, as well as unstructured 
data, as found in on-line biomedical literature.  
Constructing an ontology is an extremely 
laborious effort. Even with some reuse of ?core? 
knowledge from an Upper Model (Cohen et al 
1999), the task of creating an ontology for a 
particular domain and task has a high cost, 
incurred for each new domain. Tools that could 
automate, or semi-automate, the construction of 
                                                     
1 This research was supported by the National Science 
Foundation (ITR-0205470). 
2 www.geneontology.org 
ontologies for different domains could 
dramatically reduce the knowledge creation cost.  
One approach to developing such tools is to rely 
on information implicit in collections of on-line 
text in a particular domain. If it were possible to 
automatically extract terms and their semantic 
relations from the text corpus, the ontology 
developer could build on that knowledge, revising 
it, as needed, etc. This would be more cost-
effective than having a human develop the 
ontology from scratch.  
Our approach is inspired by research on topic-
focused multi-document summarization of large 
text collections, where there is a need to 
characterize the collection content succinctly in a 
hierarchy of topic terms and their relationships. 
Current approaches to multi-document 
summarization combine linguistic analysis, corpus 
statistics, and the use of background semantic 
knowledge from generic thesauri such as WordNet 
to infer semantic information about a person. In 
extending such approaches to ontology induction, 
the hypothesis is that similar hybrid approaches 
can be used to identify technical terms in a 
domain-specific corpus and infer semantic 
relationships among them.  
In this paper, we describe a system that 
automatically induces an ontology from any large 
on-line text collection in a specific domain, to 
support cataloguing in information access and data 
integration tasks. The induced ontology consists of 
domain concepts related by kind-of and part-of 
links, but does not include more specialized 
relations or axioms. The structure of the ontology 
is a directed acyclic graph (DAG). To achieve 
domain-independence, we use a combination of 
relatively shallow methods along with existing 
repositories of applicable background knowledge. 
These are described in Section 2. In Section 3, we 
also introduce a new metric Relation Precision for 
evaluating induced ontologies in comparison with 
reference ontologies. We have applied our system 
to produce ontologies in numerous domains: 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 47
  
 
 
 
 
 
 
 
Figure 1: System Architecture 
 
IRS 
Publication 17 
285 
 
k1 
0 285 
 
n1 
Reuters 
Corpus 
9 
k2 
19,024 19,043 
n2 
Total 294 19,024 19,328 
 
Table 1: Distribution of ?income tax? in domain and background corpora
(i) newswire from the TREC collection (ii) 
taxation information from the IRS (Publication 17, 
from (IRS 2001)), (iii) epidemiological newsgroup 
messages from the Program for Monitoring 
Emerging Diseases (PROMED) from the 
Federation of American Scientists3, (iv) the text of 
a book by the first author called Automatic 
Summarization, and (v) MEDLINE biomedical 
abstracts retrieved from the National Library of 
Medicine?s PubMed system4. In the latter domain, 
we have begun building a large ontology using the 
ontology induction methods along with post-
editing by domain experts in molecular biology at 
Georgetown University 5 . This ontology, called 
PRONTO, involves hundreds of thousands of 
protein names found in MEDLINE abstracts and in 
UNIPROT, the world?s largest protein database6. It 
is therefore infeasible to construct PRONTO by 
hand from scratch. PRONTO is also much larger 
than other ontologies in the biology area; for 
example, the Gene Ontology is rather high-level, 
and contains (as of March 2004) only about 17,000 
terms. 
                                                     
3 www.fas.org/promed/ 
4www4.ncbi.nlm.nih.gov/PubMed/ 
5 complingone.georgetown.edu/~prot/ 
6pir.georgetown.edu 
2 Approach 
2.1 System Architecture 
An overall architecture for domain-independent 
ontology induction is shown in Figure 1. The 
documents are preprocessed to separate out 
headers. Next, terms are extracted using finite-state 
syntactic parsing and scored to discover domain-
relevant terms. The subsequent processing infers 
semantic relations between pairs of terms using the 
?weak? knowledge sources run in the order 
described below. Evidence from multiple 
knowledge sources is then combined to infer the 
resulting relations. The resulting ontologies are 
written out in a standard XML-based format (e.g., 
XOL, RDF, OWL), for use in various information 
access applications.  
While the ontology induction procedure does not 
involve human labor, except for writing the 
preprocessing and term tokenization program for 
specialized technical domains, the human may edit 
the resulting ontology for use in a given 
application. An ontology editor has been 
developed, discussed briefly in Section 3.1. 
2.2 Term Discovery 
The system takes a collection of documents in a 
subject area, and identifies terms characteristic of 
the domain.  In a given domain such as 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology48
bioinformatics, specialized term tokenization (into 
single- and multi-word terms) is required. The 
protein names can be long, e.g., 
?steroid/thyroid/retinoic nuclear hormone receptor 
homolog nhr-35?, and involve specialized patterns. 
In constructing PRONTO, we have used a protein 
name tagger based on an ensemble of statistical 
classifiers to tag protein names in collections of 
MEDLINE abstracts (Anon 2004). Thus, in such a 
domain, a specialized tagger replaces the 
components in the dotted box in Figure 1. 
In other domains, we adopt a generic term-
discovery approach. Here the text is tagged for 
part-of-speech, and single- and multi-word terms 
consisting of minimal NPs are extracted using 
finite-state parsing with CASS (Abney 1996). All 
punctuation except for hyphens are removed from 
the terms, which are then lower-cased. Each word 
in each term is stemmed, with statistics (see below) 
being gathered for each stemmed term. Multi-word 
terms are clustered so that open, closed and 
hyphenated compounds are treated as equivalent, 
with the most frequent term in the collection being 
used as the cluster representative.  
The terms are scored for domain-relevance based 
on the assumption that if a term occurs 
significantly more in a domain corpus than in a 
more diffuse background corpus, then the term is 
clearly domain relevant.  
As an illustration, in Table 1 we compare the 
number of documents containing the term ?income 
tax? (or ?income taxes?) in a long (2.18 Mb) IRS 
publication, Publication 17, from an IRS web site 
(IRS 2001) compared to a larger (27.63 Mb subset 
of the) Reuters 21578 news corpus7. One would 
expect that ?income tax? is much more a 
characteristic of the IRS publication, and this is 
borne out by the document frequencies in the table. 
We use the log likelihood ratio (LLR) (Dunning 
1993) given by 
-2log2(Ho(p;k1,n1,k2,n2)/Ha(p1,p2;n1,k1,n2,k2))  
LLR measures the extent to which a 
hypothesized model of the distribution of cell 
counts, Ha, differs from the null hypothesis, Ho  
(namely, that the percentage of documents 
containing this term is the same in both corpora). 
We used a binomial model for Ho and Ha8.   
2.3 Relationship Discovery 
The main innovation in our approach is to fuse 
together information from multiple knowledge 
                                                     
7 In Publication 17, each ?chapter? is a document. 
8From Table 1, p=294/19238=.015, p1=285/285=1.0, 
p2=9/19043=4.72, k1=285, n1=285, k2=9, n2=19043.  
sources as evidence for particular semantic 
relationships between terms. To infer semantic 
relations such as kind-of and part-of, the system 
uses a bottom-up data-driven approach using a 
combination of evidence from shallow methods. 
2.3.1 Subphrase Relations  
These are based on the presence of common 
syntactic heads, and allow us to infer, for example, 
that ?p68 protein? is a kind-of ?protein?. Likewise, 
in the TREC domain, subphrase analysis tells us 
that ?electric car? is a kind of ?car?, and in the IRS 
domain, that ?federal income tax? is a kind of 
?income tax?.  
2.3.2 Existing Ontology Relations 
These are obtained from a thesaurus. For 
example, the Gene Ontology can be used to infer 
that ?ATP-dependent RNA helicase? is a kind of 
?RNA-helicase?. Likewise, in the TREC domain, 
using WordNet tells us that ?tailpipe? is part of 
?automobile?, and in the IRS domain, that ?spouse? 
is a kind of ?person?.  Synonyms are also merged 
together at this stage. 
2.3.3 Contextual Subsumption Relations 
We also infer hierarchical relations between 
terms, by top-down clustering using a context-
based subsumption (CBS) algorithm. The 
algorithm uses a probabilistic measure of set 
covering to find subsumption relations. For each 
term in the corpus, we note the set of contexts in 
which the term appears. Term1 is said to subsume 
term2 when the conditional probability of term1 
appearing in a context given the presence of term2, 
i.e., P(term1|term2), is greater than some threshold.  
CBS is based on the algorithm of (Lawrie et al 
2001), which used a greedy approximation of the 
Domination Set Problem for graphs to discover 
subsumption relations among terms. Unlike their 
work, we did not seek to minimize the set of 
covering terms; therefore, a subsumed term may 
have multiple parents. The conditional probability 
threshold (0.8) we use to determine subsumption is 
much higher than in their approach. We also 
restrict the height of the hierarchies we build to 
three tiers. Tightening these latter two constraints 
appears to notably improve the quality of our 
subsumption relations.  
The largest corpus against which CBS has run is 
the ProMed corpus where, considering each 
paragraph a distinct context, there were 117,690 
contexts in the 11,198 documents. Here is an 
example from ProMed of a transitive relation that 
spans three tiers: ?mosquito? is a hypernym of 
?mosquito pool?, and ?mosquito? is also a 
hypernym of ?standing water?. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 49
2.3.4 Explicit Patterns Relations 
This knowledge source infers specific relations 
between terms based on characteristic cue-phrases 
which relate them. For example, the cue-phrase 
?such as? (Hearst 1992) (Caraballo 1999) suggest a 
kind-of relation, e.g., ?a ligand such as 
triethylphosphine? tells us that ?triethylphosphene? 
is a kind of ?ligand?. Likewise, in the TREC 
domain, ?air toxics such as benzene? can suggest 
that ?benzene? is a kind of ?air toxic?. However, 
since such cue-phrase patterns tend to be sparse in 
occurrence, we do not use them in the evaluations 
described below.  
2.3.5 Domain-Specific Knowledge Sources 
Although our approach is domain-independent, it 
is possible to factor in domain knowledge sources 
for a given domain. For example, in biology, ?ase? 
is usually a suffix indicating an enzyme.  
Postmodifying PPs (found using a CASS grammar) 
can also be useful in some domains, as shown in  
?tax on investment income of child? in Figure 2. 
We have so far, however, not investigated other 
domain-specific knowledge sources. 
 
2.4 Evidence Combination 
The main point about these and other knowledge 
sources is that each may provide only partial 
information. Combining these knowledge sources 
together, we expect, will lead to superior 
performance compared to just any one of them. 
Not only do inferences from different knowledge 
sources support each other, but they are also 
combined to produce new inferences by transitivity 
relations. For example, since phrase analysis tells 
us that ?pyridine metabolism? is a kind-of 
?metabolism?, and Gene Ontology tells us that 
?metabolism? is a kind-of ?biological process?, it 
follows that ?pyridine metabolism? is a kind-of 
?biological process?. The evidence combination, in 
addition to computing transitive closure of these 
relations, also detects inconsistencies, querying the 
user to resolve them when detected. 
3 Evaluation 
3.1 Informal Assessment 
Subphrase Relations is a relatively high-
precision knowledge source compared to the 
others, producing many linked chains. Its 
performance can be improved by flagging and 
excluding proper names and idioms from its input 
(e..g, so that ?palm pilot? doesn?t show up as a 
kind-of ?pilot?). However, a chain of such relations 
can be interrupted by terms that aren?t lexically 
similar, but that are nevertheless in a kind-of 
relation. Some of these gaps are filled by 
transitivity relations involving other knowledge 
sources, especially Existing Ontologies, which is 
especially useful in filling gaps in some of the 
upper levels of the ontology. While Contextual 
Subsumption is good at  discovering associations 
between ?leaves? in the DAG and other concepts, 
the method cannot reliably infer the label of the 
relation. For example, in the IRS domain, we 
obtain ?divorce? as more general than ?decree of 
divorce? and ?separate maintenance?, but we don?t 
know the nature of the relations. Contextual 
Subsumption-inferred links are directed edges with 
label ?unknown?. 
Overall, the ontologies produced are noisy and 
require human correction, and the methods can 
produce many fragments that need to be linked by 
hand. While the system can detect cycles that need 
resolution by the human, these rarely arise
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: An IRS Ontology viewed in the Ontology Editor 
 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology50
 
Term Target  
DF 
Back-
ground 
DF 
LLR IG MI DF TF TF * 
IDF 
electric 80 61 99.9 99.9 81.3 99.9 99.9 27.8 
car 77 56 99.6 99.3 81.5 99.8 99.9 79.4 
battery 54 16 99.0 98.2 86.9 98.7 99.9 94.9 
emission 15 0 96.5 96.8 99.2 79.1 96.6 64.8 
year 58 505 67.9 67.6 25.0 99.2 99.7 65.7 
informal 10 29 66.2 66.3 0.2 48.6 99.7 99.2 
record 8 138 15.2 15.7 4.4 50.2 99.9 99.9 
osha 1 0 0.0 0.0 0.0 0.0 99.9 0.0 
Table 2: Comparing Topic 230 Term Percentile Rankings 
 
For a flavor of the kind of results we get, see 
Figure 2, which displays an ontology induced 
without any human intervention from IRS 
Publication 17. Here the DAG is displayed as a 
tree. The immediate children of  ?person?, a node 
high in the ontology, is shown in the left part of the 
window. Selecting ?child? brings up its kinds as 
well as some other children linked by ?unknown? 
label via Contextual Subsumption, e.g., ?full-time 
student?. A list of orphaned terms that aren?t 
related to any others are shown on the far right. 
The terms with checkboxes are those that occur in 
the corpus; the others are those that are found 
exclusively by Existing Ontology Relations. 
Checking a term allows it to be inspected in its 
occurrence context in the corpus. The editor comes 
with a variety of tools to help integrate ontology 
fragments. 
3.2 Human Evaluation 
3.2.1 Term Scoring  
To evaluate term scoring, we used a corpus of 
news articles about automobiles that consisted of 
85 documents relevant to the TREC Topic 230 
query: ?Is the automobile industry making an 
honest effort to develop and produce an electric-
powered automobile?? In Table 2, we provide 
some examples of how the LLR term scoring 
statistic performed with respect to five others on 
selected unigrams in the Topic 230 domain: term 
frequency, document frequency, term frequency 
times inverse document frequency (TF*IDF), 
pointwise mutual information (MI), and 
information gain (IG). Terms in bold are ones we 
judged important in the Topic 230 domain, the 
others are deemed unimportant. The numbers are 
percentile rankings. LLR and IG do equally well, 
outperforming the others. 
We carried out other comparisons for two other 
domains. In the income-tax domain, a hand-built 
term list from the IRS contained 82 terms which 
occurred in IRS Publication 17, of which the 
system discovered 77 (94% recall). In the ProMed 
domain, a pre-existing hand-built taxonomy 
produced by a bioterrorism analyst had 1048 terms 
which occurred in the ProMed message corpus, of 
which 607 were discovered by the system (58% 
recall). However, the hand-built taxonomy, which 
was built without consulting a corpus, wasn?t a 
full-fledged ontology, for example, there was no 
label for the parent-child relation.  
3.2.2 Term Relationships  
We also carried out an evaluation experiment to 
determine if the relations being discovered by the 
machine were in keeping with human judgments. 
We focused here on an evaluation of pairs of 
knowledge sources. Our experiment examined the 
case where the system discovered a kind-of 
relation. Here each subject was first asked to read 
four newspaper articles from the TREC topic-230 
sub-collection. The articles were then kept 
accessible to the subject in a browser window for 
the subject to consult if needed in answering 
subsequent questions. The subject was asked to 
judge, based on the documents read, whether term 
X was a kind of term Y, term Y was a kind of term 
X, or neither; e.g., ?Is acid a kind of pollutant, or is 
pollutant a kind of acid, or neither??. The subject 
had one of three mutually exclusive choices; the 
first two choices were presented in randomized 
order. 
The subjects were 16 native speakers of English 
unconnected with the project. Each subject was 
given ten questions to answer in each of the 
experiments. For each set of ten questions, five 
were chosen at random from pairs of terms related 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 51
by (immediate) kind-of relations. The remaining 
five questions were chosen at random from pairs of 
terms between which the system found no relation 
whatsoever. 
 
 Human 
System kind-
of(A, B) 
not kind-
of(A,B) 
kind-of(A, B) 56 18 
not kind-
of(A,B) 
6 
 
74 
 
Table 3: Is X a kind-of Y? 
 
We first discuss inter-subject agreement. Three 
subjects given the same relation to judge agreed 
75% of the time, leading to a Kappa score of 0.72, 
indicating a good level of agreement. This means 
that subjects were able to reliably make judgments 
as to whether A is a kind of B in some document. 
The results for the 16 subjects are shown in 
Table 3. When the system is compared to the 
human as ground truth, this gives a Precision of 
.90, a Recall of .75, and an F-measure of .82. This 
performance is also significantly better than 
random assignment: with chi-square=74.29, with p 
< 0.0019. The substantial effect sizes of the chi-
square indicates a very solid result. There were 62 
decisions involving Subphrase Relations (with 44 
True Positives and 18 False Negatives), and 10 
decisions involving WordNet (with 12 True 
Positives). This shows that there is solid agreement 
between the human subjects and the system on the 
kind-of relations.  However, these 154 decisions 
involved only four newspaper articles, so clearly 
more data would be helpful.  
3.3 Automatic Evaluation 
While evaluation by humans is valuable, it is 
expensive to carry out, and this expense must be 
incurred each time one wants to do an evaluation. 
Automatic comparison of a machine-generated 
ontology against reference ontologies constructed 
by humans, e.g., (Zhang et al 1996) (Sekine et al 
1999) (Daude et al 2001), is therefore desirable, 
provided suitable reference ontologies are 
available. In this evaluation, the human-generated 
taxonomy for ProMed described in Section 3.2.1 
was used as a reference ontology, with its 
unlabeled parent-child relation treated as a kind-of 
link. However, the human ?ontology? was created 
without looking at a corpus, and was developed for 
use with a different set of goals in mind. Although 
this involves comparing ?apples? and ?oranges?, a 
comparison is nevertheless illustrative, and can in 
addition be useful when comparing mutiple 
ontologies created under similar conditions. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3:  Automatically Induced Fragment 
from ProMed 
To set aside the problem of differences in 
terminology involved in the comparison, we 
decided to restrict our attention to the set of terms 
TH (of cardinality 3025) in the human ontology 
(H), and have our system induce relations between 
them using the ProMed corpus. Relations were 
induced automatically in the machine ontology (M) 
for just 761 of those terms, yielding a set TH1.  The 
structure of  TH1 is shown in a fragment in Figure 
3.  Here A is a kind-of B if it is printed under B 
without a label; A is a part-of B if it is printed 
under B with a ?p? label. 
We then automatically computed, for each pair 
of terms t1 and t2 in TH1 that were linked distance 1 
apart in M, the distance between those terms in H. 
Likewise, we also computed, for each pair of terms 
t1 and t2 in TH1 distance 1 apart in H, the distance 
between those terms in M. 
The results of this comparison are as follows. 
The number of relations where the two ontologies 
agree exactly is 63 (i.e., the terms are distance 1 
apart in both ontologies). Since, given a set of 
terms, there are many different ways to construct 
an ontology, this is encouraging.   
The number of relations that our system found 
which were ?missed?, i.e., more than distance 1 
away, in H is 1203. Given the previous experiment 
where the human subjects agreed with the system's 
relations, these 1203 relations are likely to contain 
many that the human probably missed. For 
example, the relations in the machine ontology 
between ?eye? and ?farsightedness?, and ?medicine? 
                                                     
9  The chi-square for Subphrase Relations is 61.68, 
and the chi-square for WordNet is 56.73, with p < 0.001 
in all cases. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology52
4 Related Work and ?chiropractic medicine? are missed by H. This 
highlights a problem with human-generated 
ontologies: substantial errors of omission.  The existing approaches to ontology induction include those that start from structured data, 
merging ontologies or database schemas (Doan et 
al. 2002). Other approaches use natural language 
data, sometimes just by analyzing the corpus 
(Sanderson and Croft 1999), (Caraballo 1999) or 
by learning to expand WordNet with clusters of 
terms from a corpus, e.g., (Girju et al 2003). 
Information extraction approaches that infer 
labeled relations either require substantial hand-
created linguistic or domain knowledge, e.g., 
(Craven and Kumlien 1999) (Hull and Gomez 
1993), or require human-annotated training data 
with relation information for each domain (Craven 
et al 1998).  
The number of relations in H that our system 
missed (relations that were more than distance 1 
away in the system ontology), is 3493. However, 
of these 3493 relations, 2955 involved at least 1 
term that was not included in M, leaving 538 
relations that we could calculate the distance for in 
M. These 538 relations in H include relations 
between ?acid indigestion medicine? and ?maalox?, 
and ?alternative medicine? and ?acupuncture? (a 
majority of the misses involved relations between a 
disease and the name of a specific drug for it, 
which aren?t part-of or kind-of relations).  
 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
1 2 3 4 5 6 7 8 9 
D 
Relation-
Prec.(H,M,D)
Relation-
Prec.(M,H,D)
 
Many, though not all, domain-independent 
approaches (Evans et al 1991) (Grefenstette 1997) 
have restricted themselves to discovering term-
associations, rather than labeled relations. A 
notable exception is (Sanderson and Croft 1995), 
which (unlike our approach) assumes the existence 
of a query that was used to originally retrieve the 
documents (so that terms can be extracted from the 
query and then expanded to generate additional 
terms for the ontology). Their approach also is 
restricted to one method to discover relations, 
while we use several.  
Our approach is complementary to approaches 
aimed at automatically enhancing existing 
resources for a particular domain, e.g. (Moldovan 
et al 2000). Finally, the prior methods, while they 
often carry out evaluation, lack standard criteria for 
ontology evaluation. Although ontology evaluation 
remains challenging, we have discussed several 
evaluation methods in this paper. 
Figure 4: Relation Precision 
These observations lead to a metric for 
comparing one ontology with another one serving 
as a reference ontology. Given two ontologies A 
and B, define Relation Precision (A, B, D) as the 
proportion of the distance 1 relations in A that are 
at most a distance D apart in B. This measure can 
be plotted for different values of D. In Figure 4, we 
show the Relation Precision(H, M, D), and 
Relation Precision(M, H, D), for our machine 
ontology M and human ontology H. Both curves 
show Relation Precision(H, M, D) growing faster 
than Relation Precision(M, H, D), with 70% of the 
area being below the former curve and 54% being 
below the latter curve. The graph shows that while 
22% of distance 1 relations in M are at most 3 
apart in H (but keep in mind the errors of omission 
in H), 40% of distance 1 relations in H are at most 
3 apart in M10. 
5 Conclusion 
The evidence combination described above is 
based on transitivity and union. Since the above 
evaluations, we have been experimenting with an 
ad hoc weighted evidence combination scheme, 
based on each knowledge source expressing a 
strength for a posited relation. In future, we will 
also investigate using an initial seed ontology to 
provide a better ?backbone? for induction, and then 
using a spreading activation method to activate 
nodes related by existing knowledge sources to 
seed nodes. Corpus statistics can be used to weight 
the links. For example, based on (Caraballo 1999), 
each parent of a leaf node could be viewed as a 
cluster label for its children, with the weight of a 
parent-child link being determined based on how 
strongly the child is associated with the cluster.  
                                                     
10 The mean distance in H between terms that are 
distance 1 apart in M is 5.17, with a standard deviation 
of 2.12. The mean distance in M between terms which 
are distance 1 apart in H is 3.85, with a standard 
deviation of 1.69. 
The ontology induction methods described here 
can allow for considerable savings in time in 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 53
constructing ontologies. The evaluations we have 
carried out are suggestive, but many issues remain 
open. There are many unanswered questions about 
human-created reference ontologies, including lack 
of inter-annotator agreement studies. Indeed, 
experience shows that without guidelines for 
ontology construction, humans are prone to come 
up with very different ontologies for a domain. 
Comparing a machine-induced ontology against an 
ideal human reference ontology, were one to be 
available, is also fraught with problems. Our 
experience with using an implementation of the 
(Daude et al 2001) constraint relaxation algorithm 
for ontology comparison suggests that much work 
is needed on distance metrics which are not over-
sensitive to small differences in structure.  
Our interest, therefore, is focused more towards 
an extrinsic evaluation. PRONTO, which is due to 
be released in 2004, offers the opportunity to 
measure costs of ontology induction and post-
editing on a large-scale problem of value to the 
biology community. We also plan to measure the 
effectiveness of PRONTO in query expansion for 
information access to MEDLINE and protein 
databases. Finally, we will investigate more 
sophisticated evidence combination methods, and 
compare against other automatic methods for 
ontology induction. 
The ontology induction tools are available for 
free distribution for research purposes. 
References  
Abney, S. 1996. Partial parsing Via Finite-State 
Cascades. Proceedings of the ESSLLI '96 Robust 
Parsing Workshop. 
Caraballo, S. A. 1999. Automatic Construction of a 
hypernym-labeled noun hierarchy from text. In 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'1999), 120-122.  
Cohen, P. R., Chaudhri, V., Pease, A. and Schrag, 
R. 1999. Does Prior Knowledge Facilitate the 
Development of Knowledge-based Systems? The 
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99).  
Craven, M. and Kumlien, J. 1999. Constructing 
biological knowledge bases by extracting 
information from text sources. Proc Int Conf 
Intell Syst Mol Biol., 77-86.  
Craven, M., DiPasquo, D., Freitag, D., McCallum, 
A., Mitchell, T., Nigam, K., and Slattery, S.. 
1998. Learning to Extract Symbolic Knowledge 
from the World Wide Web. Proceedings of 
AAAI-98, 509-516. 
Daude, J., Padro, L. and Rigau, G. 2001 A 
Complete WN1.5 to WN1.6 Mapping. NAACL-
2001 Workshop on WordNet and Other Lexical 
Resources: Applications, Extension, and 
Customization, 83-88.  
Doan, A.,  Madhavan, J. , Domings, P. and Halevy, 
A. 2002. Learning to Map between Ontologies 
on the Semantic Web. WWW?2002. 
Dunning, T. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence,? 
Computational Linguistics, 19(1):61-74, 1993. 
Girju, R.,  Badulescu, A., and Moldovan, D. 2003. 
Learning Semantic Constraints for the Automatic 
Discovery of Part-Whole Relations. Proceedings 
of HLT?2003, Edmonton. 
Grefenstette, G. 1997. Explorations in Automatic 
Thesaurus Discovery.  Kluwer International 
Series in Engineering and Computer Science, 
Vol 278. 
Hearst, M. 1992. Automatic Acquisition of 
Hyponyms from Large Text Corpora. 
Proceedings of the fourteenth International 
Conference on Computational Linguistics, 
Nantes, France, July 1992.  
Hull, R. and Gomez, F. 1993. Inferring Heuristic 
Classification Hierarchies from Natural 
Language Input. Telematics and Informatics, 
9(3/4), pp. 265-281. 
IRS (Internal Revenue Service). 2001. Tax Guide 
2001. Publication 17. http://www.irs.gov/pub/irs-
pdf/p17.pdf 
Lawrie, D., Croft, W. B., and Rosenberg, A. 2001. 
Finding topic words for hierarchical 
summarization. 24th ACM Intl. Conf. on 
Research and Development in Information 
Retrieval, 349-357, 2001. 
Miller, G. (1995). WordNet: A Lexical Database 
for English. Communications Of the Association 
For Computing Machinery (CACM) 38, 39-41.  
Sanderson, M. and Croft, B. 1995. Deriving 
concept hierarchies from text. Proceedings of the 
22nd Annual Internationaql ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, 160-170. 
Sekine, S., Sudo, K. and Ogino, T. 1999. Statistical 
Matching of Two Ontologies. Proceedings of 
ACL SIGLEX99 Workshop: Standardizing 
Lexical Resources. 
Zhang, K., Wang, J. T. L. and Shasha, D.  1996. 
On the Editing Distance between Undirected 
Acyclic Graphs and Related Problems. 
International Journal of Foundations of 
Computer Science 7, 43-58. 
 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology54
Producing Biographical Summaries: Combining Linguistic
Knowledge with Corpus Statistics1
Barry Schiffman
Columbia University
1214 Amsterdam Avenue
New York, NY 10027, USA
Bschiff@cs.columbia.edu
Inderjeet Mani2
The MITRE Corporation
11493 Sunset Hills Road
Reston, VA 20190, USA
imani@mitre.org
Kristian J. Concepcion
The MITRE Corporation
11493 Sunset Hills Road
Reston, VA 20190, USA
kjc9@mitre.org
                                                       
1
 This work has been funded by DARPA?s Translingual Information Detection, Extraction, and Summarization (TIDES)
research program, under contract number DAA-B07-99-C-C201 and ARPA Order H049.
2
 Also at the Department of Linguistics, Georgetown University, Washington, D. C. 20037.
Abstract
We describe a biographical multi-
document summarizer that summarizes
information about people described in
the news.  The summarizer uses corpus
statistics along with linguistic
knowledge to select and merge
descriptions of people from a document
collection, removing redundant
descriptions. The summarization
components have been extensively
evaluated for coherence, accuracy, and
non-redundancy of the descriptions
produced.
1 Introduction
The explosion of the World Wide Web has
brought with it a vast hoard of information, most
of it relatively unstructured. This has created a
demand for new ways of managing this often
unwieldy body of dynamically changing
information. The goal of automatic text
summarization is to take a partially-structured
source text, extract information content from it,
and present the most important content in a
condensed form in a manner sensitive to the
needs of the user and task (Mani and Maybury
1999). Summaries can be ?generic?, i.e., aimed
at a broad audience, or topic-focused, i.e.,
tailored to the requirements of a particular user
or group of users. Multi-Document
Summarization (MDS) is, by definition, the
extension of single-document summarization to
collections of related documents. MDS can
potentially help the user to see at a glance what a
collection is about, or to examine similarities
and differences in the information content in the
collection.
Specialized multi-document
summarization systems can be constructed for
various applications; here we discuss a
biographical summarizer. Biographies can, of
course, be long, as in book-length biographies,
or short, as in an author?s description on a book
jacket. The nature of descriptions in the
biography can vary, from physical
characteristics (e.g., for criminal suspects) to
scientific or other achievements (e.g., a
speaker?s biography). The crucial point here is
that facts about a person?s life are selected,
organized, and presented so as to meet the
compression and task requirements.
 While book-quality biographies are out
of reach of computers, many other kinds can be
synthesized by sifting through large quantities of
on-line information, a task that is tedious for
humans to carry out. We report here on the
development of a biographical MDS summarizer
that summarizes information about people
described in the news. Such a summarizer is of
interest, for example, to analysts who want to
automatically construct a dossier about a person
over time.
Rather than determining in advance
what sort of information should go into a
biography, our approach is more data-driven,
relying on discovering how people are actually
described in news reports in a collection. We use
corpus statistics from a background corpus along
with linguistic knowledge to select and merge
descriptions from a document collection,
removing redundant descriptions. The focus here
is on synthesizing succinct descriptions.  The
problem of assembling these descriptions into a
coherent narrative is not a focus of our paper;
the system currently uses canned text methods to
produce output text containing these
descriptions. Obviously, the merging of
descriptions should take temporal information
into account; this very challenging issue is also
not addressed here.
To give a clearer idea of the system?s output,
here are some examples of biographies produced
by our system (the descriptions themselves are
underlined, the rest is canned text). The
biographies contain descriptions of the salient
attributes and activities of people in the corpus,
along with lists of their associates. These short
summaries illustrate the extent of compression
provided. The first two summaries are of a
collection of 1300 wire service news documents
on the Clinton impeachment proceedings
(707,000 words in all, called the ?Clinton?
corpus). In this corpus, there are 607 sentences
mentioning Vernon Jordan by name, from which
the system extracted 82 descriptions expressed
as appositives (78) and relative clauses (4),
along with 65 descriptions consisting of
sentences whose deep subject is Jordan. The 4
relative clauses are duplicates of one another:
?who helped Lewinsky find a job?.  The 78
appositives fall into just 2 groups: ?friend? (or
equivalent descriptions, such as ?confidant?),
?adviser? (or equivalent such as ?lawyer?). The
sentential descriptions are filtered in part based
on the presence of verbs like ?testify, ?plead?, or
?greet? that are strongly associated with the
head noun of the appositive, namely ?friend?.
The target length can be varied to produce
longer summaries.
Vernon Jordan is a presidential friend and a
Clinton adviser. He is 63 years old. He helped
Ms. Lewinsky find a job. He testified  that Ms.
Monica Lewinsky said  that she had
conversations  with the president,  that she
talked  to the president. He has numerous
acquaintances, including Susan Collins, Betty
Currie, Pete Domenici, Bob Graham,  James
Jeffords and Linda Tripp.
1,300 docs, 707,000 words (Clinton corpus) 607
Jordan sentences, 78 extracted appositives, 2
groups: friend, adviser.
Henry Hyde is a Republican chairman of House
Judiciary Committee and a prosecutor in Senate
impeachment trial. He will lead the Judiciary
Committee's impeachment review. Hyde urged
his colleagues  to heed  their consciences ,  ?the
voice  that whispers  in our ear ,  ?duty,  duty,
duty.??
Clinton corpus, 503 Hyde sentences, 108
extracted appositives, 2 groups: chairman,
impeachment prosecutor.
Victor Polay  is the Tupac Amaru rebels' top
leader,  founder and the organization's
commander-and-chief. He was arrested  again
in  1992  and is serving  a life sentence. His
associates include  Alberto Fujimori, Tupac
Amaru Revolutionary, and Nestor Cerpa.
73 docs, 38,000 words, 24 Polay sentences, 10
extracted appositives, 3 groups: leader, founder
and commander-in-chief.   
2 Producing biographical descriptions
2.1 Preprocessing
Each document in the collection to be
summarized is processed by a sentence
tokenizer, the Alembic part-of-speech tagger
(Aberdeen et al 1995), the Nametag named
entity tagger  (Krupka 1995) restricted to people
names, and the CASS parser (Abney 1996).  The
tagged sentences are further analyzed by a
cascade of finite state machines leveraging
patterns with lexical and syntactic information,
to identify constructions such as pre- and post-
modifying appositive phrases, e.g., ?Presidential
candidate George Bush?, ?Bush, the presidential
candidate?, and relative clauses, e.g., ?Senator
..., who is running for re-election this Fall,?.
These appositive phrases and relative clauses
capture descriptive information which can
correspond variously to a person?s age,
occupation, or some role a person played in an
incident. In addition, we also extract sentential
descriptions in the form of sentences whose
(deep) subjects are person names.
2.2 Cross-document coreference
The classes of person names identified within
each document are then merged across
documents in the collection using a cross-
document coreference program from the
Automatic Content Extraction (ACE) research
program (ACE 2000), which compares names
across documents based on similarity of a
window of words surrounding each name, as
well as specific rules having to do with different
ways of abbreviating a person?s name (Mani and
MacMillan 1995). The end result of this process
is that for each distinct person, the set of
descriptions found for that person in the
collection are grouped together.
2.3 Appositives
2.3.1 Introduction
The appositive phrases usually provide
descriptions of attributes of a person. However,
the preprocessing component described in
Section 2.1 does produce errors in appositive
extraction, which are filtered out by syntactic
and semantic tests. The system also filters out
redundant descriptions, both duplicate
descriptions as well as similar ones. These
filtering methods are discussed next.
2.3.2 Pruning Erroneous  and Duplicate
Appositives
The appositive descriptions are first pruned to
record only one instance of an appositive phrase
which has multiple repetitions, and descriptions
whose head does not appear to refer to a person.
The latter test relies on a person typing program
which uses semantic information from WordNet
1.6 (Miller 1995) to test whether the head of the
description is a person. A given string is judged
as a person if a threshold percentage ?1  (set to
35% in our work) of senses of the string are
descended from the synset for Person in
WordNet. For example, this picks out ?counsel?
as a person, but ?accessory? as a non-person.
2.3.3 Merging Similar Appositives
The pruning of erroneous and duplicate
descriptions still leaves a large number of
redundant appositive descriptions across
documents. The system compares each pair of
appositive descriptions of a person, merging
them based on corpus frequencies of the
description head stem, syntactic information,
and semantic information based on the
relationship between the heads in WordNet. The
descriptions are merged if they have the same
head stem, or if both heads have a common
parent below Person in WordNet (in the latter
case the head which is more frequent in the
corpus is chosen as the merged head), or if one
head subsumes the other under Person in
WordNet (in which case the more general head
is chosen).
When the heads of descriptions are
merged, the most frequent modifying phrase that
appears in the corpus with the selected head is
used. When a person ends up with more than
one description, the modifiers are checked for
duplication, with distinct modifiers being
conjoined together, so that ?Wisconsin
lawmaker? and ?Wisconsin democrat? yields
?Wisconsin lawmaker and Democrat?.
Prepositional phrase variants of descriptions are
also merged here, so that ?chairman of the
Budget Committee? and ?Budget Committee
Chairman? are merged. Modifiers are dropped
but their original order is preserved for the sake
of fluency.
2.3.4 Appositive Description Weighting
The system then weights the appositives for
inclusion in a summary. A person?s appositives
are grouped into equivalence classes, with a
single head noun being chosen for each
equivalence class, with a weight for that class
based on the corpus frequency of the head noun.
The system then picks descriptions in decreasing
order of class weight until either the
compression rate is achieved or the head noun is
no longer in the top ?2 % most frequent
descriptions (?2 is set to 90% in our work). Note
that the summarizer refrains from choosing a
subsuming term from WordNet that is not
present in the descriptions, preferring to not risk
inventing new descriptions, instead confining
itself to cutting and pasting of actual words used
in the document.
2.4 Relative Clause Weighting
Once the relative clauses have been pruned for
duplicates, the system weights the appositive
clauses for inclusion in a summary. The
weighting is based on how often the relative
clause?s main verb is strongly associated with a
(deep) subject in a large corpus, compared to its
total number of appearances in the corpus. The
idea here is to weed out ?promiscuous? verbs
that are weakly associated with lots of subjects.
The corpus statistics are derived from the
Reuters portion of the North American News
Text Corpus (called ?Reuters? in this paper) --
nearly three years of wire service news reports
containing 105.5 million words.
Examples of verbs in the Reuters corpus
which show up as promiscuous include ?get?,
?like?, ?give?, ?intend?, ?add?, ?want?, ?be?,
?do?, ?hope?, ?think?, ?make?, ?dream?,
?have?, ?say?, ?see?, ?tell?, ?try?. In a test,
detailed below in Section 4.2, this feature fired
40 times in 184 trials.
To compute strong associations, we
proceed as follows. First, all subject-verb pairs
are extracted from the Reuters corpus with a
specially developed finite state grammar and the
CASS parser. The head nouns and main verbs
are reduced to their base forms by changing
plural endings and tense markers for the verbs.
Also included are ?gapped? subjects, such as the
subject of ?run? in ?the student promised to run
the experiment?; in this example, both pairs
?student-promise? and ?student-run? are
recorded. Passive constructions are also
recognized and the object of the by-PP
following the verb is taken as the deep subject.
Strength of association between subject i and
verb j is measured using mutual information
(Church and Hanks 1990):
)ln(),(
ji
ij
tftf
tfNjiMI
?
?
= .
Here tfij is the maximum frequency of
subject-verb pair ij in the Reuters corpus, tfi is
the frequency of subject head noun i in the
corpus, tfj is the frequency of verb j in the
corpus, and N is the number of terms in the
corpus. The associations are only scored for tf
counts greater than 4, and a threshold ?3  (set to
log score > -21 in our work) is used for a strong
association.
The relative clauses are thus filtered
initially (Filter 1) by excluding those whose
main verbs are highly promiscuous.  Next, they
are filtered (Filter 2) based on various syntactic
features, as well as the number of proper names
and pronouns. Finally, the relative clauses are
scored conventionally (Filter 3) by summing the
within-document relative term frequency of
content terms in the clause (i.e., relative to the
number of terms in the document), with an
adjustment for sentence length (achieved by
dividing by the total number of content terms in
the clause).
3 Sentential Descriptions
These descriptions are the relatively large set of
sentences which have a person name as a (deep)
subject. We filter them based on whether their
main verb is strongly associated with either of
the head nouns of the appositive descriptions
found for that person name (Filter 4). The
intuition here is that particular occupational
roles will be strongly associated with particular
verbs. For example, politicians vote and elect,
executives resign and appoint, police arrest and
shoot; so, a summary of information about a
policeman may include an arresting and
shooting event he was involved with. (The verb-
occupation association isn?t manifest in relative
clauses because the latter are too few in
number).
A portion of the results of doing this is
shown in Table 1.  The results for ?executive?
are somewhat loose, whereas for ?politician?
and ?police?, the associations seem tighter, with
the associated verbs meeting our intuitions.
All sentences which survive Filter 4 are
extracted and then scored, just as relative clauses
are, using Filter 1 and Filter 3. Filter 4 alone
provides a high degree of compression; for
example, it reduces a total of 16,000 words in
the combined sentences that include Vernon
Jordan' s name in the Clinton corpus to 578
words in 12 sentences; sentences up to the target
length can be selected from these based on
scores from Filter 1 and then Filter 3.
However, there are several difficulties with
these sentences. First, we are missing a lot of
them due to the fact that we do not as yet handle
pronominal subjects which are coreferential with
the proper name. Second, these sentences
contain lots of dangling anaphors, which will
need to be resolved. Third, there may be
redundancy between the sentential descriptions,
on one hand, and the appositive and relative
clause descriptions, on the other. Finally, the
entire sentence is extracted, including any
subordinate clauses, although we are working on
refinements involving sentence compaction. As
a result, we believe that more work is required
before the sentential descriptions can be fully
integrated into the biographies.
executive police politician
reprimand
16.36 shoot 17.37 clamor 16.94
conceal 17.46 raid 17.65 jockey 17.53
bank 18.27 arrest 17.96 wrangle 17.59
foresee 18.85 detain 18.04 woo 18.92
conspire 18.91 disperse 18.14 exploit 19.57
convene 19.69 interrogate18.36 brand 19.65
plead 19.83 swoop 18.44 behave 19.72
sue 19.85 evict 18.46 dare 19.73
answer 20.02 bundle 18.50 sway 19.77
commit 20.04 manhandle18.59 criticize 19.78
worry 20.04 search 18.60 flank 19.87
accompany
20.11
confiscate
18.63
proclaim
19.91
own 20.22 apprehend18.71 annul 19.91
witness 20.28 round 18.78 favor 19.92
testify 20.40 corner 18.80 denounce20.09
shift 20.42 pounce 18.81 condemn20.10
target 20.56 hustle 18.83 prefer 20.14
lie 20.58 nab 18.83 wonder 20.18
expand 20.65 storm 18.90 dispute 20.18
learn 20.73 tear 19.00 interfere 20.37
shut 20.80 overpower19.09 voice 20.38
Table 1. Verbs strongly associated with
particular classes of people in the Reuters
corpus (negative log scores).
4 Evaluation
4.1 Overview
Methods for evaluating text summarization can
be broadly classified into two categories
(Sparck-Jones and Galliers 1996). The first, an
extrinsic evaluation, tests the summarization
based on how it affects the completion of some
other task, such as comprehension, e.g., (Morris
et al 1992), or relevance assessment (Brandow
et al 1995) (Jing et al 1998) (Tombros and
Sanderson 1998) (Mani et al 1998). An intrinsic
evaluation, on the other hand, can involve
assessing the coherence of the summary
(Brandow et al 1995) (Saggion and Lapalme
2000).
Another intrinsic approach involves
assessing the informativeness of the summary,
based on to what extent key information from
the source is preserved in the system summary at
different levels of compression (Paice and Jones
1993), (Brandow et al 1995). Informativeness
can also be assessed in terms of how much
information in an ideal (or ?reference?) summary
is preserved in the system summary, where the
summaries being compared are at similar levels
of compression  (Edmundson 1969).
We have carried out a number of intrinsic
evaluations of the accuracy of components
involved in the summarization process, as well
as the succinctness, coherence and
informativeness of the descriptions. As this is a
MDS system, we also evaluate the non-
redundancy of the descriptions, since similar
information may be repeated across documents.
4.2 Person Typing Evaluation
The component evaluation tests how accurately
the tagger can identify whether a head noun in a
description is appropriate as a person description
The evaluation uses the WordNet 1.6 SEMCOR
semantic concordance, which has files from the
Brown corpus whose words have semantic tags
(created by WordNet' s creators) indicating
WordNet sense numbers. Evaluation on 6,000
sentences with almost 42,000 nouns compares
people tags generated by the program with
SEMCOR tags, and provided the following
results: right = 41,555, wrong = 1,298, missing
= 0, yielding Precision, Recall, and F-Measure
of 0.97.
4.3 Relative Clause Extraction Evaluation
This component evaluation tests the well-
formedness of the extracted relative clauses. For
this evaluation, we used the Clinton corpus. The
relative clause is judged correct if it has the right
extent, and the correct coreference index
indicating which person the relative clause
description pertains to. The judgments are based
on 36 instances of relative clauses from 22
documents. The results show 28 correct relative
clauses found, plus 4 spurious finds, yielding
Precision of 0.87, Recall of 0.78, and F-measure
of .82. Although the sample is small, the results
are very promising.
4.4 Appositive Merging Evaluation
This component evaluation tests the system?s
ability to accurately merge appositive
descriptions. The score is based on an automatic
comparison of the system?s merge of system-
generated appositive descriptions against a
human merge of them. We took all the names
that were identified in the Clinton corpus and
ran the system on each document in the corpus.
We took the raw descriptions that the system
produced before merging, and wrote a brief
description by hand for each person who had
two or more raw descriptions. The hand-written
descriptions were not done with any reference to
the automatically merged descriptions nor with
any reference to the underlying source material.
The hand-written descriptions were then
compared with the final output of the system
(i.e., the result after merging). The comparison
was automatic, measuring similarity among
vectors of content words (i.e., stop words such
as articles and prepositions were removed).
Here is an example to further clarify the
strict standard of the automatic evaluation
(words scored correct are underlined):
System: E. Lawrence Barcella is a Washington
lawyer, Washington white-collar defense lawyer,
former federal prosecutor
System Merge: Washington white-collar defense
lawyer
Human Merge: a Washington lawyer and former
federal prosecutor
Automatic Score: Correct=2; Extra-Words=2;
Missed-Words=3
Thus, although ?lawyer? and
?prosecutor? are synonymous in WordNet, the
automatic scorer doesn?t know that, and so
?prosecutor? is penalized as an extra word.
The evaluation was carried out over the
entire Clinton corpus, with descriptions
compared for 226 people who had more than
one description. 65 out of the 226 descriptions
were Correct (28%), with a further 32 cases
being semantically correct ?obviously similar?
substitutions which the automatic scorer missed
(giving an adjusted accuracy of 42%). As a
baseline, a merging program which performed
just a string match scored 21% accuracy. The
major problem areas were errors in coreference
(e.g., Clinton family members being put in the
same coreference class), lack of good
descriptions for famous people (news articles
tend not to introduce such people), and parsing
limitations (e.g., ?Senator Clinton? being parsed
erroneously as an NP in ?The Senator Clinton
disappointed??). Ultimately, of course,
domain-independent systems like ours are
limited semantically in merging by the lack of
world knowledge, e.g., knowing that Starr' s
chief lieutenant can be a prosecutor.
4.5 Description Coherence and
Informativeness Evaluation
To assess the coherence and informativeness of
the relative clause descriptions3, we asked  4
subjects who were unaware of our research to
judge descriptions generated by our system from
the Clinton corpus. For each relative clause
description, the subject was given the
description, a person name to whom that
description pertained, and a capsule description
consisting of merged appositives created by the
system. The subject was asked to assess (a) the
coherence of the relative clause description in
terms of its succinctness (was it a good length?)
and its comprehensibility (was it and
understandable by itself or in conjunction with
the capsule?), and (b) its informativeness in
terms of whether it was an accurate description
(does it conflict with the capsule or with what
you know?) and whether it was non-redundant
(is it distinct or does it repeat what is in the
capsule?).
 The subjects marked 87% of the
descriptions as accurate, 96% as non-redundant,
and 65% as coherent. A separate 3-subject inter-
                                                       
3
 Appositives are not assessed in this way as few errors of
coherence or informativeness were noticed in the
appositive extraction.
annotator agreement study, where all subjects
judged the same 46 decisions, showed that all
three subjects agreed on 82% of the accuracy
decisions, 85% of the non-redundancy decisions
and 82% of the coherence decisions.
5 Learning  to Produce Coherent
Descriptions
5.1 Overview
To learn rules for coherence for extracting
sentential descriptions, we used the examples
and judgments we obtained for coherence in the
evaluation of relative clause descriptions in
Section 4.5. Our focus was on features that
might relate to content and specificity: low verb
promiscuity scores, presence of proper names,
pronouns, definite and indefinite clauses. The
entire list is as follows:
badend:
boolean. is there an impossible
end, indicating a bad extraction (
... Mr.)?
bestverb:
continuous. use the verb
promiscuity threshhold ?3 to
find the score of the most non-
promiscuous verb in the clause
classes
(label):
boolean. accept the clause,
reject the clause
count
pronouns:
continuous. number of personal
pronouns
count
proper:
continuous. number of nouns
tagged as NP
hasobject: continuous. how many np'sfollow the verb?
haspeople: continuous. how many "name"
constituents are found?
has
possessive:
continuous. how many
possessive pronouns are there?
hasquote: boolean. is there a quotation?
hassubc: boolean. is there a subordinate
clause?
isdefinite: continuous. how many definiteNP's are there?
repeater: boolean. is the subject's name
repeated, or is there no subject?
timeref: boolean. is there a time
reference?
withquit: is there a ?quit? or ?resign?
verb?
withsay: boolean. is there a ?say? verb inthe clause?
5.2 Accuracy of Learnt Descriptions
Table 2 provides information on different
learning methods. The results are for a ten-fold
cross-validation on 165 training vectors and 19
test vectors, measured in terms of Predictive
Accuracy (percentage test vectors correctly
classified).
Tool Accuracy
Barry?s Rules .69
MC4 Decision Tree .69
C4.5Rules .67
Ripper .62
Naive Bayes .62
Majority Class (coherent) .60
Table 2.  Accuracy of Different Description
Learners on Clinton corpus
The best learning methods are comparable
with rules created by hand by one of the authors
(Barry?s rules). In the learners, the bestverb
feature is used heavily in tests for the negative
class, whereas in Barry?s Rules it occurs in tests
for the positive class.
6 Related Work
Our work on measuring subject-verb
associations has a different focus from the
previous work. (Lee and Pereira 1999), for
example, examined verb-object pairs. Their
focus was on a method that would improve
techniques for gathering statistics where there
are a multitude of sparse examples. We are
focusing on the use of the verbs for the specific
purpose of finding associations that we have
previously observed to be strong, with a view
towards selecting a clause or sentence, rather
than just to measure similarity. We also try to
strengthen the numbers by dealing with ?gapped?
constructions.
While there has been plenty of work on
extracting named entities and relations between
them, e.g., (MUC-7 1998), the main previous
body of work on biographical summarization is
that of (Radev and McKeown 1998). The
fundamental differences in our work are as
follows: (1) We extract not only appositive
phrases, but also clauses at large based on
corpus statistics; (2) We make heavy use of
coreference, whereas they don?t use coreference
at all; (3) We focus on generating succinct
descriptions by removing redundancy and
merging, whereas they categorize descriptions
using WordNet, without a focus on succinctness.
7 Conclusion
This research has described and evaluated
techniques for producing a novel kind of
summary called biographical summaries. The
techniques use syntactic analysis and semantic
type-checking (from WordNet), in combination
with a variety of corpus statistics. Future
directions could include improved sentential
descriptions as well as further intrinsic and
extrinsic evaluations of the summarizer as a
whole (i.e., including canned text).
References
 J. Aberdeen, J. Burger, D. Day, L. Hirschman,
P. Robinson, and M. Vilain. 1995. ?MITRE:
Description of the Alembic System system as used
for MUC-6?. In Proceedings of the Sixth Message
Understanding Conference (MUC-6), Columbia,
Maryland.
 S. Abney. 1996. ?Partial parsing Via Finite-State
Cascades?. Proceedings of the ESSLLI '96 Robust
Parsing Workshop.
Automatic Context Extraction Program.
http://www.nist.gov/speech/tests/ace/index.htm
R. Brandow, K. Mitze, and L. Rau. 1995. ?Automatic
condensation of electronic publications by
sentence selection.? Information Processing and
Management 31(5): 675-685. Reprinted in
Advances in Automatic Text Summarization, I.
Mani and M.T. Maybury (eds.), 293-303.
Cambridge, Massachusetts: MIT Press.
K. W. Church and P. Hanks. 1990. ?Word association
norms, mutual information, and lexicography?.
Computational Linguistics 16(1): 22-29.
H. P. Edmundson. 1969. ?New methods in automatic
abstracting?.  Journal of the Association for
Computing Machinery 16 (2): 264-285. Reprinted
in Advances in Automatic Text Summarization, I.
Mani and M.T. Maybury (eds.), 21-42.
Cambridge, Massachusetts: MIT Press.
G. Krupka. 1995. ?SRA: Description of the SRA
system as used for MUC-6?. In Proceedings of the
Sixth Message Understanding Conference (MUC-
6), Columbia, Maryland.
L. Lee and F. Pereira. 1999. ?Distributional
Similarity Models: Clustering vs. Nearest
Neighbors?. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, 33-40.
I. Mani and T. MacMillan. 1995. ?Identifying
Unknown Proper Names in Newswire Text?. In
Corpus Processing for Lexical Acquisition, B.
Boguraev and J. Pustejovsky (eds.), 41-73.
Cambridge, Massachusetts: MIT Press.
I. Mani and M. T. Maybury. (eds.). 1999. Advances
in Automatic Text Summarization. Cambridge,
Massachusetts: MIT Press.
G. Miller. 1995. ?WordNet: A Lexical Database for
English?. Communications of the Association For
Computing Machinery (CACM) 38(11): 39-41.
A. Morris, G. Kasper, and D. Adams. 1992. ?The
Effects and Limitations of Automatic Text
Condensing on Reading Comprehension
Performance?. Information Systems Research 3(1):
17-35. Reprinted in Advances in Automatic Text
Summarization, I. Mani and M.T. Maybury (eds.),
305-323. Cambridge, Massachusetts: MIT Press.
 MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference, DARPA.
C. D. Paice and P. A. Jones. 1993. ?The
Identification of Important Concepts in Highly
Structured Technical Papers.? In Proceedings of
the 16th International Conference on Research
and Development in Information Retrieval
(SIGIR'93), 69-78.
D. R. Radev and K. McKeown. 1998. ?Generating
Natural Language Summaries from Multiple On-
Line Sources?. Computational Linguistics 24(3):
469-500.
H. Saggion and G. Lapalme. 2000. ?Concept
Identification and Presentation in the Context of
Technical Text Summarization?. In Proceedings of
the Workshop on Automatic Summarization, 1-10.
K. Sparck-Jones and J. Galliers. 1996. Evaluating
Natural Language Processing Systems: An
Analysis and Review.  Lecture Notes in Artificial
Intelligence 1083. Berlin: Springer.
A. Tombros and M. Sanderson. 1998.?Advantages of
query biased summaries in information retrieval?.
In Proceedings of the 21st International
Conference on Research and Development in
Information Retrieval (SIGIR'98), 2-10.
Using Summarization for Automatic Briefing Generation 
Inderjeet Mani 
. Kristian Concepcion 
Linda Van Guilder 
The MITRE Corporation, W640 
11493 Sunset Hills Road 
Reston, VA 22090, USA 
{imani,kjc9,1cvg}@mitre.org 
Abst rac t  
We describe a system which automatically 
generates multimedia briefings from high- 
level outlines. The system uses 
summarization in content selection and 
creation, and in helping form a coherent 
narrative for the briefing. The approach does 
not require a domain knowledge base. 
1 In t roduct ion  
Document production is an important function in 
many organizations. In addition to instruction 
manuals, reports, courseware, system 
documentation, etc., briefings are a very 
common type of document product, often used 
in slide form as a visual accompaniment to a 
talk. Since so much time is spent by so many 
people in producing briefings, often under 
serious time constraints, any method to reduce 
the amount of time spent on briefing production 
could yield great gains in productivity. 
Briefings involve a high degree of condensation 
of information (e.g., no more than a few points, 
perhaps bul,leted, per slide), and they typically 
contain multimedia information. Many briefings 
have a stereotypical structure, dictated in part by 
the business rules of the organisation. For 
example, a commander may present a daily or 
weekly brief to her superiors, which is more in 
the nature of a routine update of activities ince 
the last briefing; or she may provide an action 
brief, which is triggered by a particular situation, 
and which consists of a situation update 
followed by arguments recommending a 
particular course of action. Further, the process 
of constructing a briefing may involve certain 
stereotypical activities, including culling 
information from particular sources, such as 
messages, news, web pages, previous briefings, 
etc. Thus, while part of the briefing content may 
be created anew by the briefing author 1, other 
parts of the briefing may be constructed from 
existing information sources. However, 
information in those sources need not 
necessarily be in the same form as needed by the 
briefing. 
All these characteristics of briefings make them 
attractive as an application of automatic 
summarization, which is aimed at producing a 
condensed, task-tailored representation f salient 
content in information sources. Often, the 
background information being used in a slide is 
quite considerable; the author needs to identify 
what's salient, presenting it in a succinct manner 
so as to fit on the slide, perhaps creating a 
graphic or other multimedia clip to do so. 
Automatic summarization; by definition, has a 
clear role to play here. A briefing usually 
involves a sequence of slides; as the summary 
becomes longer, it needs to form a coherent 
narrative, built around the prescribed structure. 
Finally, a briefing must strive, to the extent 
possible, to be persuasive and vivid, so that the 
point gets across. This in turn presents a further 
challenge for summarization: the ability to 
generate smoothly narrated, coherent 
summaries. 
I The noun "author" is used throughout the paper to 
designate a human author. 
_ 99  
It is therefore worthwhile investigating whether 
combining automatic summarization with 
intelligent multimedia presentation techniques 
can make the briefing generation amenable to 
full automation. In other words, the author 
should be able to use a computer program to 
generate an initial briefing, which she can then 
edit and revise as needed. The briefing can then 
be presented by the author if desired, or else 
directly by the computer (particularly useful if 
the briefing is being sent to someone lse). The 
starting point for this process would be a high- " 
level outline of the briefing on the part of the 
author. The outline would include references to
particular information sources that had to be, 
summarized in particular ways. If a program 
were able to take such outlines and generate 
briefings which didn't require extensive post- 
editing to massage into a state deemed 
acceptable for the task at hand, the program 
could be regarded as a worthwhile time saving 
tool. 
2 Approach 
Our work forms part of a larger DARPA-funded 
project aimed at improving analysis and 
decision-making in crisis situations by providing 
tools that allow analysts to collaborate to 
develop structured arguments in support of 
particular conclusions and to help predict likely 
future scenarios. These arguments, along with 
background evidence, are packaged together as 
briefing s to high-level decision-makers. In 
leveraging automatic methods along the lines 
suggested above to generate briefings, our 
approach needs to allow the analyst to take on as 
much of the briefing authoring as she wants to 
(e.g., it may take time for her to adapt o or trust 
the machine, or she may want the machine to 
present just part of the briefing). The analyst's 
organisation usually will instantiate one of 
several templates dictating the high-level 
structure of a briefing; for example, a briefing 
may always have to begin with an executive 
summary. The summarization methods also need 
to be relatively domain-independent, given that 
the subject matter of crises are somewhat 
unpredictable; an analyst in a crisis situation is 
likely to be inundated with large numbers of 
crisis-related news and intelligence r ports from 
many different sources. This means that we 
cannot require that a domain knowledge base be 
available to help the briefing generation process. 
Given these task requirements, we have adopted 
an approach that is flexible about 
accommodating different degrees of author 
involvement, that is relatively neutral about the 
rhetorical theory underlying the briefing 
structure (since a template may be provided by 
others), and that is domain-independent. I  our 
approach, the author creates the briefing outline, 
which is then fleshed out further by the system 
based on information i  the outline. The system 
fills out some content by invoking specified 
summarizers; it also makes decisions, when 
needed, about output media type; it introduces 
narrative lements to improve the coherence of 
the briefing; and finally, it assembles the final 
presentation, making decisions about spatial 
layout in the process. 
A briefing is represented asa tree. The structure 
of the tree represents he rhetorical structure of 
the briefing. Each node has a label, which offers 
a brief textual description of the node. Each leaf 
node has an associated goal, which, when 
realized, provides content for that node. There 
are two kinds of goals: content-level goals and 
narrative-level goals. Content-level goals are 
also of two kinds: retrieve goals, which retrieve 
existing media objects of a particular type (text, 
audio, image, audio, video) satisfying some 
description, and create goals, which create new 
media objects of these types using programs 
(called summarization filters). Narrative-level 
goals introduce descriptions of content at other 
nodes: they include captions and running text for 
media objects, and segues, which are rhetorical 
moves describing a transition to a node. 
Ordering relations reflecting temporal and 
spatial ayout are defined on nodes in the tree. 
Two coarse-grained relations, seq for 
precedence, and par for simultaneity, are used to 
specify a temporal ordering on the nodes in the 
tree. As an example, temporal constraints for a 
(tiny) tree of 9 nodes may be expressed as: 
<ordering> <seq> 
<par>7</par> 
<par>8</par> 
<par>3</par> 
<par>4 5</par> 
<par>6</par> 
100 
<par>l 9</par> 
<par>2</par> 
</seq> </ordering> 
The tree representation, along with the temporal 
constraints, can be rendered in text as XML; we 
refer to the XML representation as a script. 
@ 
Player i~ 
User 
Interface 
~ Tem~t~ 
r I Vail dator \[ 
Co 
Cr~ ~ 
C~ound ~t  
ixe utor I 
XMI1 resentati.on \[ 
~k~ Generator / ,  
' Brid"mg 
Generator 
Figure 1: System Architecture 
The overall architecture of our system is shown 
in Figure 1, The user creates the briefing outline 
in the form of a script, by using a GUI. The 
briefing generator takes the script as input. The 
Script Validator applies an XML parser to the 
script, to check for syntactic orrectness. It then 
builds a tree representation for the script, which 
represents the briefing outline, with temporal 
constraints attached to the leaves of the tree. 
Next, a Content Creator takes the input tree and 
expands it by introducing narrative-level goals 
including segues to content nodes, and rtmning 
text and captions describing media objects at 
content nodes. Running text and short captions 
are generated from meta-information associated 
with media objects, by using shallow text 
generation methods (canned text). The end result 
of content selection (which has an XML 
representation callod a ground script) is that the 
complete tree has been fully specified, with all 
the create and retrieve goals fully specified , 
with all the output media types decided. The 
Content Creator is thus responsible for both 
content selection and creation, in terms of tree 
structure and node content. 
Then, a Content Executor executes all the create 
and retrieve goals. This is a very simple step, 
resulting in the generation of all the media 
objects in the presentation, except for the audio 
files for speech to be synthesized. Thus, this step 
results in realization of the content at the leaves 
of the tree. 
Finally, the Presentation Generator takes the 
tree which is output from Content Execution, 
along with its temporal ordering constraints, and 
generates the spatial ayout of the presentation. 
If no spatial ayout constraints are specified (the 
default is to not specify these), the system 
allocates pace using a simple method based on 
the temporal layout for nodes which have spatial 
manifestations. Speech synthesis is also carried 
out here. Once the tree is augmented with spatial 
layout constraints, it is translated by the 
Presentation Generator into SMIL 2 
(Synchronized Multimedia Integration 
Language) (SMIL 99), a W3C-developod 
extension of HTML that can be played by 
standard multimedia players (such as Real 3 and 
Grins 4. This step thus presents the realized 
content, synthesizing it into a multimedia 
presentation laid out spatially and temporally. 
This particular architecture, driven by the above 
project requirements, does not use planning as 
an overall problem-solving strategy, as planning 
requires domain knowledge. It therefore differs 
from traditional intelligent multimedia 
presentation planners, e.g., (Wahlster etal. 93). 
Nevertheless, the system does make a number o f  
intelligent decisions in organizing and 
coordinating presentation decisions. These are 
discussed next, after which we turn to the main 
point of the paper, namely the leveraging of 
summarization in automatic briefing generation. 
2 h. ttp://www.w3.org/AudioVideo/ 
3 www.real.com 
4 www.oratrix.com 
_ J  
101 
3 Intelligent Multimedia Presentation 
Generation 
The author of a briefing may choose to flesh out 
as little of the tree as desired, with the caveat 
that the temporal ordering relations for non- 
narrative nodes need to be provided by her. 
When a media object is generated at a node by a 
create goal, the running text and captions are 
generated by the system. The motivation for this 
is obvious: when a summarization filter (which 
is a program under our control) is generating a
media object, we can often provide sufficient 
recta-information about hat object o generate a
short caption and some running text. By default, 
all segues and spatial layout relations are also 
specified by the system, so the author does not 
have to know about these unless she wants to. 
Finally, the decision as to when to produce 
audio, when not specified by the author, is left to 
the system. 
When summarization filters are used (for create 
goals), the media type of the output is specified 
as a parameter to the filter. This media type may 
be converted to some other type by the system, 
e.g., text to speech conversion using Festival 
(Taylor et al 98). By default, all narrative nodes 
attempt to realize their goals as a speech media 
type, using rules based on text length and 
tnmcatability to less than 250 bytes to decide 
when to use text-to-speech. The truncation 
algorithm is based on dropping syntactic 
constituents, using a method similar to (Mani et 
al. 99). Captions are always realized, in addition, 
as text (i.e., they have a text realization and a. 
possible audio realization). 
Spatial layout is decided in the Presentation 
Generator, after all the individual media objects 
are created along with their temporal constraints 
by the Content Executor. The layout algorithm 
walks through the temporal ordering in 
sequence, allocating a segment o each set of 
objects that is designated to occur 
simultaneously (grouped by par in the temporal 
constraints). Each segment can have up to 4 
frames, in each of which a media object is 
displayed (thus, no more than 4 media objects 
can be displayed at the same time). Since media 
objects declared to be simultaneous (using par) 
in the temporal constraints will go together in a 
separate segment, the temporal constraints 
determine what elements are grouped together in 
a segment. The layout within a segment handles 
two special cases. Captions are placed directly 
undemeath their associated media object. 
Running text, when realized as text, is placed 
beside the media object being described, so that 
they are paired together visually. Thus, 
coherence of a segment is influenced mainly by 
the temporal constraints (which have been 
fleshed out by the Content Creator to include 
narrative nodes), with further handling of special 
cases. Of course, an individual summarization 
filter may choose to coordinate component 
multimedia objects in particular ways in the 
course of generating a composite multimedia 
object. 
Details such as duration and onset of particular 
frames are specified in the translation to SMIL. 
Duration is determined by the number of frames 
present in a segment, unless there is an audio 
media object in the segment (this media object 
may have a spatial representation, e.g., as an 
audio icon, or it may not). If an audio media 
object occurs in a frame, the duration of all 
media objects in that frame is equal to the length 
of all the audio files in the segment. If there is 
no audio present in a segment, he duration is ot 
seconds (or has a default value of 5) times the 
number of frames created. 
4 Summarization Filters 
As mentioned above, create goals are satisfied 
by summarization filters, which create new 
media objects ummarizing information sources. 
These programs are called summarization filters 
because in the course of condensing information, 
they take input information and turn it into some 
more abstract and useful representation, filtering 
out unimportant information. Such filters 
provide a novel way of carrying out content 
selection and creation for automated 
presentation generation. 
Our approach relies on component-based 
software composition, i.e., assembly of software 
units that have contractually specified interfaces 
that can be independently deployed and reused. 
The idea of assembling complex language 
processing programs out of simpler ones is 
102 
hardly new; however, by employing current 
industry standards to specify the interaction 
between the components, we simultaneously 
increase the robustness of the system, ensure the 
reusability of individual components and create 
a more fully plug-and-play capability. Among 
the core technology standards that support his 
plug-and-play component assembly capability 
are (a) Java interfaces, used to specify functions 
that all summarization components must 
implement in order to be used in the system, (b) 
the JavaBeans standard, which allows the 
parameters and methods of individual 
components o be inspected by the system and 
revealed to the users (c) the XML markup 
standard, which we have adopted as an inter- 
component communication language. Using 
these technologies, legacy or third-party 
summarizers are incorporated into the system by 
"wrapping" them so as to meet the interface 
specification of the system. These technologies 
also make possible a graphical environment to 
assemble and configure complex summarization 
filters from individual summarization 
components. 
Among the most important wins over the 
traditional "piping" approach to filter assembly 
is the ability to impose build-time restrictions on 
the component assembly, disallowing "illegal" 
compositions, e.g. component X cannot provide 
input to component Y unless X's output type 
corresponds to Y's input type. Build-time 
restrictions uch as these play a clear role in 
increasing the overall robustness of the run-time 
summarization system. Another build-time win 
lies in the ability of JavaBeans to be serialized, 
i.e., written to disk in such a way as to preserve 
~he state of its parameters settings, ensuring that 
every component in the system can be 
configured and run at different times 
independently of whether the component 
provides aparameter file facility. 
Establishing the standard functions required of a 
summarization filter is challenging on several 
fronts. One class of functions required by the  
interface is necessary to handle the technicalities 
of exchanging information between otherwise 
discrete components. This set includes 
functions for discovering a component's input 
and output types, for handling messages, 
exceptions and events passed between 
components and for interpreting XML based on 
one or more system-wide document type 
definitions (DTDs). The other, more interesting 
set of functions gets to the core of 
summarization functionality. Selecting these 
functions involves identifying parameters likely 
to be broadly applicable across most or all 
summarizers and finding ways to group them 
and/or to generalize them. This is desirable in 
order to reduce the burden on the end user of 
understanding the subtle differences between the 
various settings in the summarizers available to 
her. 
An. example of the difficulty inherent in this 
endeavor is provided by the compression 
(summary length divided by source length) vs. 
reduction (l's complementof compression) vs. 
target length paradigm. Different summarizers 
will implement one or more of these. The 
wrapper maps from the high-level interface 
function, where the application/user can specify 
either compression ortarget length, but not both, 
to the individual summarizer's representation. 
Thus, a user doesn't need to know which 
representation(s) a particular summarizer uses 
for reduction/compression. 
A vanilla summarization Bean includes the 
following functionality, which every summarizer 
must be able to provide methods for: 
source: documents to be summarized 
(this can be a single document, or a 
collection) 
reduction-rate: either summary 
size/source size, or target length 
audience: user-focused or generic 
(user-focused requires the specification 
of a bag of terms, which can be of 
different types) 
output-type: specific data formats 
(specified by DTDs) 
The above are parameters which we expect all 
summarizers to support. More specialized 
summarizer beans can be constructed to reflect 
groupings of summarizers. Among other 
parameters are output-fluency, which specifies 
whether a textual summary is to be made up of 
passages (sentences, paras, blocks), named 
entities, lists of words, phrases, or topics, etc. 
Given that definitions of summarization i more 
103 
theoretical terms have not been entirely 
satisfactory (Mani 2000), it is worth noting that 
the above vanilla Bean provides an operational 
definition of what a summarizer is. 
text, and segues. The captions and running text, 
when not provided by the filters, are provided by 
the script input. In the case of retrieve goals, the 
objects may not have any meta-information, i  
which case a default caption and running-text is
generated. Clearly, a system's explanatory 
narrative will be enhanced by the availability of 
rich meta-information. 
The segues are provided by the system. For 
example, an item with a label "A biography of 
bin Laden" could result in a generated segue 
"Here is a biography of bin Laden". The 
Content Creator, when providing content for 
narrative nodes, uses a variety o f  different 
canned text patterns. For the above example, the 
pattern would be "Here is @6.label", where 6 is 
the number of a non-narrative node, with label 
being its label. 
Figure 2: Summarization Filter 
Composition 
In addition to its practical utility in the ability to 
assimilate, combine and reuse components in 
different combinations, and to do so within a 
GUI, this approach is interesting because it 
allows powerful summarization functions to be 
created by composing together simpler tools. 
(Note that this is different from automatically 
finding the best combination, which our system 
does not address). For example, Figure 2 
illustrates a complex filter created by using a 
GUI to compose together a named entity 
extractor, a date extractor, a component which 
discovers significant associations between the 
two and writes the result to a table, and a 
visualizer which plots the results as a graph. The 
resulting summarizer takes in a large collection 
of documents, and produces as a summary a 
graph (a jpeg) of salient named entity mentions 
over time. Each of its components can be easily 
reused within the filter composition system to 
build other summarizers. 
5 Narrative Summarization 
Peru Action Brief 
1 Preamble 
2 Situation Assessment 
2.1 Chronology of Events 
2.1.2 Late st document summary 
create C'summarize -generic 
-compression. 1 ~peru~p32") 
2.2 Biographies 
2.2.1 Biography of Victor Polay 
2.2.1.1 Picture of @2.2.2.percon 
retrieve("\]) Arawdata~,polay.jpg ") 
2.2.1.2 Biography of @~2.2.2.person 
create("summarize -bio -length 350 
-span multi -person 
@_~2.2.2.person - ut table 
/peru/* ") 
3 Coda 
"This briefing has aszessed aspects of the 
situation in Peru. Overall, the crisis 
appears to be worsening." 
Figure 3: Input Script 
As mentioned above, the system can construct a
narrative to accompany the briefing. Narrative 
nodes are generated to cover captions, running 
104 
Peru Action Brief 
1 Preamble 
audio -- "ln this briefin~ 1 will go over 
the @2.1abel. This ~?ill cover 
@2.1.1abel and @,2. 3.1.1aber" 
2 Situation Assessment 
2. l "An overvie~? of the ~2.2.label" 
(Meta-2.2) 
2.2 C-'hfonology of Events 
2.2.1 audio = "Here is the @2.2.2.laber" 
(1VIeta- 2.2.2) 
2.2.2 text = "Latest document summary" 
audio = text = 
create ("automatize -generic 
-compression .1/reru/p32") 
2.3 Biographies 
2.3.1 audio = 
"A profile of @2. 3.2.person" 
('NIeta-2.3.2) 
2.3.2 Biography of Victor~olay 
2.3.2.1 audio = text = 
"A file photo of 
@,2.3.2.person" 
(Meta-2.3.2.2) 
2.3.2.2 Picture of @,2.&2.person 
image = 
retrie ve("D Arawdata~polay.jpg") 
2.3.2.3 audio = text = 
"ProJile of @2. 3. 2.person" 
(Meta- 2.3.2.3) 
2.3.2.4 Biography of @2. 3.2.person 
audio = text = 
create(%-ummarize-bio length 350 
-span multi -person 
@_r2.Z 2.person -out tab& 
/rend* ") 
3 Coda 
audio = "This briefing has assessed 
a~79ect~r of the situation in Peru. Overall, 
the crisis appears to be ~orr"ening." 
<seq> 
</seq> 
<par> 1 </par> 
<par>2.2.1 2.2.2</par> 
<par>2.3.1 <lpar> 
<par>2.3.2.1 2.3.2.2 
2.3.2.3 2.3.2.4</par> 
<par~3</par> 
Figure 4: Ground Script 
All segue nodes are by default generated 
automatically by the system, based on node 
labels. We always introduce a segue node at the 
beginning of the presentation (called a preamble 
node), which provides a segue covering the 
"crown" of the tree, i.e., all nodes upto a 
particular depth d from the root (d=2) are 
marked with segue nodes. A segue node is also 
produced at the end (called a coda). (Both 
preamble and segue can of course be specified 
by the author if desired). 
For introducing intervening segue nodes, we use 
the following algorithm based on the distance 
between odes and the height in the tree, We 
traverse the non-narrative l aves of the tree in 
their temporal order, evaluating each pair of 
adjacent nodes A and B where A precedes B 
temporally. A segue is introduced between 
nodes A and B if either (a) the maximum of the 
2 distances from A and B to their least common 
ancestor isgreater than 3 nodes or (b) the sum of 
the 2 distances from A and B to the least 
common ancestor isgreater than 4 nodes. This is 
less intrusive than introducing segues at random 
or between every pair of successive nodes, and 
appears to perform better than introducing a
segue at each depth of the tree. 
6 An Example 
We currently have a working version of the 
system with a variety of different single and 
multi-document summarization filters. Figure 3 
shows an input script created by an author (the 
scripts in Figure 3 and 4 are schematic 
representations of the scripts, rather than the raw 
XML). The script includes two create goals, one 
with a single-document generic summarization 
filter, the other with a multi-document user- 
focused summarization filter. Figure 4 shows the 
ground script which was created automatically 
by the Content Creator component. Note the 
addition of media type specifications, the 
introduction of narrative nodes, and the 
extension of the temporal constraints. The final 
presentation generated is shown in Figure 5. 
Here we show screen dumps of the six SMIL 
segments produced, with the audio if any for 
each segment indicated in this paper next to an 
audio icon. 
105 
7 Status 
The summarization filters have incorporated 
several summarizers, including some that have 
been evaluated in the DARPA SUMMAC 
conference (Mani et al 99-1). These carry out 
both single-document and multi-document 
summarization, and include a preliminary 
biographical summarizer we have developed. 
The running text for the biography table in the 
second-last segment of Figure 5 is produced 
from meta-information i the table XML 
generated by the biographical summarizer. The 
production method for running text uses canned 
text which should work for any input table 
conforming to that DTD. 
The summarization filters are. being tested as 
part of a DARPA situated test with end-users. 
The briefing generator itself has been used 
internally to generate numerous briefings, and 
has been demonstrated aspart of the DARPA 
system. We also expect to carry out an 
evaluation to assess the extent to which the 
automation described here provides efficiency 
gains in briefing production. 
8 Related Work 
There is a fair amount of work on automatic 
authoring of multimedia presentations, e.g., 
(Wahlster et al 93), (Dalai et al 96), (Mittal et 
al. 95), (Andre and Rist 97) 5. These efforts 
differ from ours in two ways: first, unlike us, 
they are not open-domain; and, second, they 
don't use summarization components. While 
such efforts are extremely sophisticated 
compared to us in multimedia presentation 
planning and fine-grained coordination and 
synchronization capabilities, many of the 
components used in those efforts are clearly 
applicable to our work. For example, (Andre and 
Rist 96) include methods for leveraging lifelike 
characters in this process; these characters can 
be leveraged in our work as well, to help 
personify the computer narrator. In addition, our 
captions, which are very short, rely on canned 
text based on node labels in the initial script, or 
based on shallow meta-information generated by 
the summarization filter (in XML) along with 
the created media object. (Mittal e t  al. 95) 
describe avariety of strategies for generation of 
longer, more explanatory captions, some of 
which may be exploited in our work by 
deepening the level of recta-information, at least 
for summarization components developed by us. 
In our ability to leverage automatic 
summarization, our work should be clearly 
distinguished from work which attempts to 
format a summary (from an XML 
representation) into something akin to a 
Powerpoint briefing, e.g., (Nagao and Hasida 
98). Our work, by contrast, is focused on using 
summarization i  generating briefings from an 
abstract outline. 
9 Conclusion 
We have described methods for leveraging 
automatic summarization in the automatic 
generation of multimedia briefings. This work 
has taken an open-domain approach, in order to 
meet the requirements of the DARPA 
application we are involved with. We believe 
there is a stronger role that NL generation can 
play in the narrative aspects of our briefings, 
which currently rely for the most part on canned 
text. Our future work on description merging in 
biographical summaries, and on introducing 
referring expressions into the narrative nodes, 
would in effect ake advantage of more powerful 
generation methods, without sacrificing open- 
domain capabilities. This may require much 
richer meta-information specifications than the 
ones we currently use. 
Finally, we have begun the design of the Script 
Creator GUI (the only component in Figure l 
remaining to be built). This will allow the author 
to create scripts for the briefing generator 
(instead of editing templates by hand), by laying 
out icons for media objects in temporal order. A 
user will be able to select a "standard" briefing 
template from a menu, and then view it in a 
briefing/template structure ditor. The user can 
then provide content by adding annotations to 
any node in the briefing template. The user will 
have a choice of saving the edit version in 
template form, or in SMIL or possibly Microsoft 
Powerpoint format. 
106 
I 
I 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
Peru Act ion  Br ie f  ! ;! 
? Exeeadv? Smmmu'y  
o Hypothes is  
? S i tuat ion  Assessmml  :i 
o Ehromdo.e~ o f  \]~','?nls i i  
o B iograph les  :,~ 
? SWuctm-ed A~,mneats  :~ 
? .4 Jtentadve V iews  ' i  
? Der i s ion ,  ~:i 
.<e In this briefing I will go over the situation 
assessment. This will cover an overview of the 
chronology of  events and a profile of Victor 
Polay. 
"e Next, a biography of Victor Polay. 
::. Here is an overview of the chronology of 
events. 
I I III I II Illlll i 
I :  (3qN-  Peruv ian  cebe l~ re leet~e 2 bo , tages  - Dec.  IS~h ~i i  
3; JUOOUC ZOO hOS~flge~ ~.1~ d tn51cle the  h~ 0 ~' Japeme:~e ::~ 
J t~loan=edor Boc lh l=a kok i ,  vhece  Tupec  Jtz~l~u rebe l= were  ~! 
Victor Polay, also known as Comandante 
Rolando, is the Tupac Amaru founder, a
Peruvian guerrilla commander, a former ebel 
leader, and the Tupac Amaru rebels' top leader. 
He studied in both France and Spain. His wife is 
Rosa Polay and his mother is Otilia Campos de 
Polay. His associates include Alan Garcia. 
Here is the latest document summary. 
This briefing has assessed aspects of  the 
situation in Peru. Overall, the crisis appears to 
be worsening. 
Figure 5: Presentation 
107 
References 
Andre, E. and Rist, T. (1997) Towards a New 
Generation of Hypermedia Systems: Extending 
Automated Presentation Design for Hypermedia. 
L. Dybkjaer, ed., Proceedings of the Third Spoken 
Dialogue and Discourse Workshop, Topics in 
Natural Interactive Systems 1. The Maersk Me- 
Kinney Moiler Institute for Production 
Technology, Odense University, Denmark, pp. 10- 
27. 
Dalai, M., Feiner, S., McKeown, K., Pan, S., Zhou, 
M., Hollerer, T., Shaw, J., Feng, Y., and Fromer, J. 
(1996) Negotiation for Automated Generation of 
Temporal MultimediaPresentations. Proceedings 
of ACM Multimedia '96. 
Mani, 1., Gates, B., and Bloedorn, E. (1999) 
Improving Summaries by Revising Them. 
Proceedings of the 37 ~ Annual Meeting of the 
Association for Computational Lihguistics, College 
Park, MD, pp. 558-565. 
Mani, 1., Firmin, T., House, D., Klein, G., Sundheim, 
B., and Hirschman, L. (1999) The TIPSTER 
SUMMA C Text Summarization Evaluation. 
Proceedings of EACL'99, Bergen, Norway, pp. 77- 
85. 
Mani, 1. (2000)Automatic Text Summarization. John 
Benjamins Publishing Company. To appear. 
Mittal, V., Roth, S., Moore, J., Mattis, J., and 
Carenini, G. (1995) Generating Explanatory 
Captions for Information Graphics. Proceedings of 
the International Joint Conference on Artificial 
Intelligence (IJCAr95), pp. 1276-1283. 
Nagao, K. and K. Hasida, K. (1998) Automatic Text 
Summarization Based on the Global Document 
Annotation. Proceedings of COLING'98, Montreal, 
pp. 917-921. 
Power, R. and Scott, D. (1998) Multilingual" 
Authoring using Feedback Texts. Proceedings of 
COLING'98, Montreal, pp. 1053-1059. 
Taylor, P., Black, A., and Caley, R. (1998) The 
architecture of the Festival Speech Synthesis 
System. Proceedings of the Third ESCA Workshop 
on Speech Synthesis, Jenolan Caves, Australia, pp. 
147-151. 
Wahlster, W., Andre, E., Finkler, W., Profitlich, H.- 
J., and Rist, T. (1993) Plan-Based Integration of 
Natural Language and Graphics Generation. AI 
Journal, 63. 
108 
I 
l 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
