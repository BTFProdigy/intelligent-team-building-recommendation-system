Simple Syntactic and Morphological Processing Can Help English-Hindi
Statistical Machine Translation
Ananthakrishnan Ramanathan,
Pushpak Bhattacharyya
Department of Computer Science
and Engineering
Indian Institute of Technology
Powai, Mumbai-400076
India
{anand,pb}@cse.iitb.ac.in
Jayprasad Hegde, Ritesh M. Shah,
Sasikumar M
CDAC Mumbai (formerly NCST)
Gulmohar Cross Road No. 9
Juhu, Mumbai-400049
India
{jjhegde,ritesh,sasi}
@cdacmumbai.in
Abstract
In this paper, we report our work on incor-
porating syntactic and morphological infor-
mation for English to Hindi statistical ma-
chine translation. Two simple and compu-
tationally inexpensive ideas have proven to
be surprisingly effective: (i) reordering the
English source sentence as per Hindi syntax,
and (ii) using the suffixes of Hindi words.
The former is done by applying simple trans-
formation rules on the English parse tree.
The latter, by using a simple suffix separa-
tion program. With only a small amount of
bilingual training data and limited tools for
Hindi, we achieve reasonable performance
and substantial improvements over the base-
line phrase-based system. Our approach es-
chews the use of parsing or other sophisti-
cated linguistic tools for the target language
(Hindi) making it a useful framework for
statistical machine translation from English
to Indian languages in general, since such
tools are not widely available for Indian lan-
guages currently.
1 Introduction
Techniques for leveraging syntactic and morpholog-
ical information for statistical machine translation
(SMT) are receiving a fair amount of attention nowa-
days. For SMT from English to Indian languages,
these techniques are especially important for the fol-
lowing three reasons: (i) Indian languages differ
widely from English in terms of word-order; (ii) In-
dian languages are morphologically quite rich; and
(iii) large amounts of parallel corpora are not avail-
able for these languages, though smaller amounts of
text in specific domains (such as health, tourism, and
agriculture) are now becoming accessible. It might
therefore be expected that using syntactic and mor-
phological information for English to Indian lan-
guage SMT will prove highly beneficial in terms
of achieving reasonable performance out of limited
parallel corpora. However, the difficulty in this is
that crucial tools, such as parsers and morphological
analyzers, are not widely available for Indian lan-
guages yet.
In this paper, we present our work on incorporat-
ing syntactic and morphological information for En-
glish to Hindi SMT. Our approach, which eschews
the use of parsing and other tools for Hindi, is two-
pronged:
1. Incorporating syntactic information by com-
bining phrase-based models with a set of struc-
tural preprocessing rules on English
2. Incorporating morphological information by
using a simple suffix separation program for
Hindi, the likes of which can be created with
limited effort for other Indian languages as well
Significant improvements over the baseline
phrase-based SMT system are obtained using our
approach. Table 1 illustrates this with an example 1.
Since only limited linguistic effort and tools are
required for the target language, we believe that the
framework we propose is suitable for SMT from En-
glish to other Indian languages as well.
1This example is discussed further in section 4
513
input For a celestial trip of the scientific kind, visit the planetarium.
reference
	 ta	 	 e	   	 e , ta ae?
vaigyaanika tariike ke eka divya saira ke lie, taaraamandala aaem
scientific kind of a celestial trip for, planetarium visit (come)
baseline
	   	 	 pr	 , 	 ta

 ?
ke svargiiya yaatraa ke vaigyaanika prakaara, kaa taaraagruha hai
of celestial trip of scientific kind, of planetarium is
baseline+syn
	 pr	 	   	 e , ta

 ?
vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha hai
scientific kind of celestial trip for, planetarium is
baseline+syn+morph
	 pr	 	   	 e , ta

 ?
vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha dekhem
scientific kind of celestial trip for, planetarium visit (see)
Table 1: Effects of Syntactic and Morphological Processing (reference: human reference translation;
baseline: phrase-based system; syn: with syntactic information; morph: with morphological information)
The rest of this paper is organized as follows: Sec-
tion 2 outlines related work. Section 3 describes our
approach ? first, the phrase-based baseline system is
sketched briefly, leading up to the techniques used
for incorporating syntactic and morphological infor-
mation within this system. Experimental results are
discussed in section 4. Section 5 concludes the pa-
per with some directions for future work.
2 Related Work
Statistical translation models have evolved from the
word-based models originally proposed by Brown
et al (1990) to syntax-based and phrase-based tech-
niques.
The beginnings of phrase-based translation can
be seen in the alignment template model introduced
by Och et al (1999). A joint probability model
for phrase translation was proposed by Marcu and
Wong (2002). Koehn et al (2003) propose certain
heuristics to extract phrases that are consistent with
bidirectional word-alignments generated by the IBM
models (Brown et al, 1990). Phrases extracted us-
ing these heuristics are also shown to perform bet-
ter than syntactically motivated phrases, the joint
model, and IBM model 4 (Koehn et al, 2003).
Syntax-based models use parse-tree representa-
tions of the sentences in the training data to learn,
among other things, tree transformation probabili-
ties. These methods require a parser for the target
language and, in some cases, the source language
too. Yamada and Knight (2001) propose a model
that transforms target language parse trees to source
language strings by applying reordering, insertion,
and translation operations at each node of the tree.
Graehl and Knight (2004) and Melamed (2004), pro-
pose methods based on tree-to-tree mappings. Ima-
mura et al (2005) present a similar method that
achieves significant improvements over a phrase-
based baseline model for Japanese-English transla-
tion.
Recently, various preprocessing approaches have
been proposed for handling syntax within SMT.
These algorithms attempt to reconcile the word-
order differences between the source and target lan-
guage sentences by reordering the source language
data prior to the SMT training and decoding cy-
cles. Nie?en and Ney (2004) propose some restruc-
turing steps for German-English SMT. Popovic and
Ney (2006) report the use of simple local trans-
formation rules for Spanish-English and Serbian-
English translation. Collins et al (2006) propose
German clause restructuring to improve German-
English SMT.
The use of morphological information for SMT
has been reported in (Nie?en and Ney, 2004) and
(Popovic and Ney, 2006). The detailed experi-
ments by Nie?en and Ney (2004) show that the use
of morpho-syntactic information drastically reduces
the need for bilingual training data.
Recent work by Koehn and Hoang (2007) pro-
514
poses factored translation models that combine fea-
ture functions to handle syntactic, morphological,
and other linguistic information in a log-linear
model.
Our work uses a preprocessing approach for in-
corporating syntactic information within a phrase-
based SMT system. For incorporating morphology,
we use a simple suffix removal program for Hindi
and a morphological analyzer for English. These as-
pects are described in detail in the next section.
3 Syntactic & Morphological Information
for English-Hindi SMT
3.1 Phrase-Based SMT: the Baseline
Given a source sentence f , SMT chooses as its trans-
lation e?, which is the sentence with the highest prob-
ability:
e? = arg max
e
p(e|f)
According to Bayes? decision rule, this is written
as:
e? = arg max
e
p(e)p(f |e)
The phrase-based model that we use as our base-
line system (defined by Koehn et al (2003)) com-
putes the translation model p(f |e) by using a phrase
translation probability distribution. The decoding
process works by segmenting the input sentence f
into a sequence of I phrases f
I
1
. A uniform proba-
bility distribution over all possible segmentations is
assumed. Each phrase f
i
is translated into a target
language phrase e
i
with probability ?(f
i
|e
i
). Re-
ordering is penalized according to a simple exponen-
tial distortion model.
The phrase translation table is learnt in the fol-
lowing manner: The parallel corpus is word-aligned
bidirectionally, and using various heuristics (see
(Koehn et al, 2003) for details) phrase correspon-
dences are established. Given the set of collected
phrase pairs, the phrase translation probability is cal-
culated by relative frequency:
?(f |e) = count(f, e)?
f
count(f, e)
Lexical weighting, which measures how well
words within phrase pairs translate to each other,
validates the phrase translation, and addresses the
problem of data sparsity.
The language model p(e) used in our baseline sys-
tem is a trigram model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998).
The weights for the various components of the
model (phrase translation model, language model,
distortion model etc.) are set by minimum error rate
training (Och, 2003).
3.2 Syntactic Information
As mentioned in section 2, phrase-based models
have emerged as the most successful method for
SMT. These models, however, do not handle syntax
in a natural way. Reordering of phrases during trans-
lation is typically managed by distortion models,
which have proved not entirely satisfactory (Collins
et al, 2006), especially for language pairs that differ
a lot in terms of word-order. We use a preprocess-
ing approach to get over this problem, by reordering
the English sentences in the training and test corpora
before the SMT system kicks in. This reduces, and
often eliminates, the ?distortion load? on the phrase-
based system.
The reordering rules that we use for prepro-
cessing can be broadly described by the following
transformation rule going from English to Hindi
word order (Rao et al 2000):
SS
m
V V
m
OO
m
Cm ? C
?
m
S
?
m
S
?
O
?
m
O
?
V
?
m
V
?
where,
S: Subject
O: Object
V : Verb
C
m
: Clause modifier
X
?: Corresponding constituent in Hindi,
where X is S, O, or V
X
m
: modifier of X
Essentially, the SVO order of English is changed
to SOV order, and post-modifiers are converted to
pre-modifiers. Our preprocessing module effects
this by parsing the input English sentence 2 and ap-
2Dan Bikel?s parser was used for parsing
(http://www.cis.upenn.edu/d?bikel/license.html).
515
structural transformation
morph analysis (English) Giza++
alignment correction
phrase extraction
suffix separation 
(Hindi)
decoder
	



	



Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 82?87,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
IITB System for CoNLL 2013 Shared Task: A Hybrid Approach to
Grammatical Error Correction
Anoop Kunchukuttan Ritesh Shah Pushpak Bhattacharyya
Department of Computer Science and Engineering, IIT Bombay
{anoopk,ritesh,pb}@cse.iitb.ac.in
Abstract
We describe our grammar correction sys-
tem for the CoNLL-2013 shared task.
Our system corrects three of the five er-
ror types specified for the shared task -
noun-number, determiner and subject-verb
agreement errors. For noun-number and
determiner correction, we apply a classi-
fication approach using rich lexical and
syntactic features. For subject-verb agree-
ment correction, we propose a new rule-
based system which utilizes dependency
parse information and a set of conditional
rules to ensure agreement of the verb
group with its subject. Our system ob-
tained an F-score of 11.03 on the official
test set using the M2 evaluation method
(the official evaluation method).
1 Introduction
Grammatical Error Correction (GEC) is an inter-
esting and challenging problem and the existing
methods that attempt to solve this problem take
recourse to deep linguistic and statistical analy-
sis. In general, GEC may partly assist in solv-
ing natural language processing (NLP) tasks like
Machine Translation, Natural Language Genera-
tion etc. However, a more evident application of
GEC is in building automated grammar checkers
thereby benefiting non-native speakers of a lan-
guage. The CoNLL-2013 shared task (Ng et al,
2013) looks at improving the current approaches
for GEC and for inviting novel perspectives to-
wards solving the same. The shared task makes
the NUCLE corpus (Dahlmeier et al, 2013) avail-
able in the public domain and participants have
been asked to correct grammatical errors belong-
ing to the following categories: noun-number,
determiner, subject-verb agreement (SVA), verb
form and preposition. The key challenges are han-
dling interaction between different error groups
and handling potential mistakes made by off-the-
shelf NLP components run on erroneous text.
For the shared task, we have addressed the fol-
lowing problems: noun-number, determiner and
subject-verb agreement correction. For noun-
number and determiner correction, we use a clas-
sification based approach to predict corrections
- which is a widely used approach (Knight and
Chander, 1994; Rozovskaya and Roth, 2010). For
subject-verb agreement correction, we propose a
new rule-based approach which applies a set of
conditional rules to correct the verb group to en-
sure its agreement with its subject. Our system
obtained a score of 11.03 on the official test set
using the M2 method. Our SVA correction sys-
tem performs very well with a F-score of 28.45 on
the official test set.
Section 2 outlines our approach to solving the
grammar correction problem. Sections 3, 4 and
5 describe the details of the noun-number, deter-
miner and SVA correction components of our sys-
tem. Section 6 explains our experimental setup.
Section 7 discusses the results of the experiments
and Section 8 concludes the report.
2 Problem Formulation
In this work, we focus on correction of three
error categories related to nouns: noun-number,
determiner and subject-verb agreement. The
number of the noun, the choice of determiner and
verb?s agreement in number with the subject are
clearly inter-related. Therefore, a coordinated
approach is necessary to correct these errors. If
these problems are solved independently of each
other, wrong corrections may be generated. The
following are some examples:
Erroneous sentence
A good workmen does not blame his tools
Good corrections
A good workman does not blame his tools
Good workmen do not blame his tools
82
noun-number
subject-verb agreement determiner
Figure 1: Dependencies between the noun-
number, determiner and subject-verb agreement
errors
Bad corrections
A good workman do not blame his tools
Good workman does not blame his tools
The choice of noun-number is determined by
the discourse and meaning of the text. The choice
of determiner is partly determined by the noun-
number, whereas the verb?s agreement depends
completely on the number of its subject. Fig-
ure 1 shows the proposed dependencies between
the number of a noun, its determiner and num-
ber agreement with the verb for which the noun
is the subject. Assuming these dependencies, we
first correct the noun-number. The corrections to
the determiner and the verb?s agreement with the
subject are done taking into consideration the cor-
rected noun. The noun-number and determiner are
corrected using a classification based approach,
whereas the SVA errors are corrected using a rule-
based system; these are described in the following
sections.
3 Noun Number Correction
The major factors which determine the number
of the noun are: (i) the intended meaning of the
text, (ii) reference to the noun earlier in the dis-
course, and (iii) stylistic considerations. Gram-
matical knowledge is insufficient for determining
the noun-number, which requires a higher level of
natural language processing. For instance, con-
sider the following examples:
(1) I bought all the recommended books. These
books are costly.
(2) Books are the best friends of man.
In Example (1), the choice of plural noun in the
second sentence is determined by a reference to
the entity in the previous sentence. Example (2) is
a general statement about a class of entities, where
the noun is generally a plural. Such phenomena
make noun-number correction a difficult task. As
information at semantic and discourse levels is dif-
ficult to encode, we explored lexical and syntactic
Tokens, POS and chunk tags in
?2 word-window around the noun
Is the noun capitalized ?
Is the noun an acronym ?
Is the noun a named entity?
Is the noun a mass noun, pluralia tantum?
Does the noun group have an article/
demonstrative/quantifier?
What article/demonstrative/quantifier does
the noun phrase have ?
Are there words indicating plurality in
the context of the noun?
The first two words of the sentence
and their POS tags
The number of the verb for which this noun
is the subject
Grammatical Number of majority of nouns
in noun phrase conjunction
Table 1: Feature set for noun-number correction
information to obtain cues about the number of the
noun. The following is a summary of the cues we
have investigated:
Noun properties: Is the noun a mass noun, a plu-
ralia tantum, a named entity or an acronym?
Lexical context: The presence of a plurality indi-
cating word in the context of the noun (e.g. the
ancient scriptures such as the Vedas, Upanishads,
etc.)
Syntactic constraints:
? Nouns linked by a conjunction agree with
each other (e.g. The pens, pencils and books).
? Presence/value of the determiner in the noun
group. However, this is only a secondary cue,
since it is not possible to determine if it is the
determiner or the noun-number that is incor-
rect (e.g. A books).
? Agreement with the verb of which the noun is
the subject. This is also a secondary feature.
Given that we are dealing with erroneous text,
these cues could themselves be wrong. The prob-
lem of noun-number correction is one of mak-
ing a prediction based on multiple cues in the
face of such uncertainty. We model the prob-
lem as a binary classification problem, the task
being to predict if the observed noun-number
of every noun in the text needs correction (la-
bels: requires correction/no correction). Alterna-
83
tively, we could formulate the problem as a sin-
gular/plural number prediction problem, which
would not require annotated learner corpora text.
However, we prefer the former approach since we
can learn corrections from learner corpora text (as
opposed to native speaker text) and use knowledge
of the observed number for prediction. Use of ob-
served values has been shown to be beneficial for
grammar correction (Rozovskaya and Roth, 2010;
Dahlmeier and Ng, 2011).
If the model predicts requires correction, then
the observed number is toggled to obtain the cor-
rected noun-number. In order to bias the system
towards improved precision, we apply the correc-
tion only if classifier?s confidence score for the re-
quires correction prediction exceeds its score for
the no correction prediction by at least a threshold
value. This threshold value is determined empiri-
cally. The feature set designed for the classifier is
shown in Table 1.
4 Determiner Correction
Determiners in English consist of articles, demon-
stratives and quantifiers. The choice of deter-
miners, especially articles, depends on many fac-
tors including lexical, syntactic, semantic and dis-
course phenomena (Han et al, 2006). Therefore,
the correct usage of determiners is difficult to mas-
ter for second language learners, who may (i) in-
sert a determiner where it is not required, (ii) omit
a required determiner, or (iii) use the wrong de-
terminer. We pose the determiner correction prob-
lem as a classification problem, which is a well
explored method (Han et al, 2006; Dahlmeier and
Ng, 2011). Every noun group is a training in-
stance, with the determiner as the class label. Ab-
sence of a determiner is indicated by a special
class label NO DET. However, since the number
of determiners is large, a single multi-class classi-
fier will result in ambiguity. This ambiguity can
be reduced by utilizing of the fact that a partic-
ular observed determiner is replaced by one of a
small subset of all possible determiners (which we
call its confusion set). For instance, the confu-
sion set for a is {a, an, the, NO DET}. It is un-
likely that a is replaced by any other determiner
like this, that, etc. Rozovskaya and Roth (2010)
have used this method for training preposition cor-
rection systems, which we adopt for training a de-
terminer correction system. For each observed de-
terminer, we build a classifier whose prediction is
Description Path
1 Direct subject
verb
nounnsubj
verb
nounnsubjpass
2 Path through Wh-determiner
noun
wh-determiner
ref verbrcmodnsubj
3 Clausal subject
verb
nouncsubj
verb
nouncsubjpass
4 External subject
verb_1
nounnsubj
verb_2xsubjtoaux
5 Path through copula
verb
subj_complementcop
nounnsubj
6 Subject in a different clause
verb_1
verb_3 conjconjunctioncc nounnsubj verb_2conj
7 Multiple subjects
noun_1
noun_2 conjnoun_3conj conjunctioncc
verbnsubj
Table 2: Some rules from the singular-
ize verb group rule-set
limited to the confusion set of the observed deter-
miner. The confusion sets were obtained from the
training corpus. The feature set is almost the same
as the one for noun-number correction. The only
difference is that context window features (token,
POS and chunk tags) are taken around the deter-
miner instead of the noun.
5 Subject-Verb Agreement
The task in subject-verb agreement correction is to
correct the verb group components so that it agrees
with its subject. The correction could be made
either to the verb inflection (He run ? He runs)
or to the auxiliary verbs in the verb group (He
are running ? He is running). We assume that
noun-number and verb form errors (tense, aspect,
modality) do not exist or have already been cor-
rected. We built a rule-based system for perform-
ing SVA correction, whose major components are
(i) a system for detecting the subject of a verb, and
84
(ii) a set of conditional rules to correct the verb
group.
We use a POS tagger, constituency parser and
dependency parser for obtaining linguistic infor-
mation (noun-number, noun/verb groups, depen-
dency paths) required for SVA correction. Our as-
sumption is that these NLP tools are reasonably
robust and do a good analysis when presented with
erroneous text. We have used the Stanford suite of
tools for the shared task and found that it makes
few mistakes on the NUCLE corpus text.
The following is our proposed algorithm for
SVA correction:
1. Identify noun groups in a sentence and the in-
formation associated with each noun group:
(i) number of the head noun of the noun
group, (ii) associated noun groups, if the
noun group is part of a noun phrase conjunc-
tion, and (iii) head and modifier in each noun
group pair related by the if relation.
2. Identify the verb groups in a sentence.
3. For every verb group, identify its subject as
described in Section 5.1.
4. If the verb group does not agree in number
with its subject, correct each verb group by
applying the conditional rules described in
Section 5.2.
5.1 Identifying the subject of the verb
We utilize dependency relations (uncollapsed) ob-
tained from the Stanford dependency parser to
identify the subject of a verb. From analysis of de-
pendency graphs of sentences in the NUCLE cor-
pus, we identified different types of dependency
paths between a verb and its subject, which are
shown in Table 2. Given these possible depen-
dency path types, we identify the subject of a verb
using the following procedure:
? First, check if the subject can be reached us-
ing a direct dependency path (paths (1), (2),
(3) and (4))
? If a direct relation is not found, then look for
a subject via path (5)
? If the subject has not been found in the previ-
ous step, then look for a subject via path (6)
A verb can have multiple subjects, which can be
identified via dependency path (7).
Rule Condition Action
1 ?w ? vg, pos tag(w) = MD Do nothing
2 ?w ? vg, pos tag(w) = TO Do nothing
3 subject(vg) 6= I Replace are by is
4 subject(vg) = I Replace are by am
5 do, does /? vg ? subject(vg) 6= I Replace have by has
6 do, does /? vg ? subject(vg) = I Replace has by have
Table 3: Some rules from the singular-
ize verb group rule-set
w is a word, vg is a verb group, POS tags are from the Penn
tagset
5.2 Correcting the verb group
For correcting the verb group, we have two sets of
conditional rules (singularize verb group and plu-
ralize verb group). The singularize verb group
rule-set is applied if the subject is singular,
whereas the pluralize verb group rule-set is ap-
plied if the subject is plural or if there are multi-
ple subjects (path (7) in Table 2). For verbs which
have subjects related via dependency paths (3) and
(4) no correction is done.
The conditional rules utilize POS tags and lem-
mas in the verb group to check if the verb group
needs to be corrected and appropriate rules are ap-
plied for each condition. Some rules in the sin-
gularize verb group rule-set are shown in Table 3.
The rules for the pluralize verb group rule-set are
analogous.
6 Experimental Setup
Our training data came from the NUCLE corpus
provided for the shared task. The corpus was
split into three parts: training set (55151 sen-
tences), threshold tuning set (1000 sentences) and
development test set (1000 sentences). In addi-
tion, evaluation was done on the official test set
(1381 sentences). Maximum Entropy classifiers
were trained for noun-number and determiner cor-
rection systems. In the training set, the number
of instances with no corrections far exceeds the
number of instances with corrections. Therefore,
a balanced training set was created by including
all the instances with corrections and sampling
? instances with no corrections from the training
set. By trial and error, ? was determined to be
10000 for the noun-number and determiner cor-
rection systems. The confidence score threshold
which maximizes the F-score was calibrated on
the tuning set. We determined threshold = 0
85
Task
Development test set Official test set
P R F-1 P R F-1
Noun Number 31.43 40 35.2 28.47 9.84 14.66
Determiner 35.59 17.5 23.46 21.43 1.3 2.46
SVA 16.67 23.42 19.78 29.57 27.42 28.45
Integrated 29.59 17.24 21.79 28.18 4.99 11.03
Table 4: M2 scores for IIT Bombay correction system: component-wise and integrated
for the noun-number and the determiner correction
systems.
The following tools were used in the devel-
opment of the system for the shared task: (i)
NLTK (MaxEntClassifier, Wordnet lemmatizer),
(ii) Stanford tools - POS Tagger, Parser and NER
and Python interface to the Stanford NER, (iii)
Lingua::EN::Inflect module for noun and verb plu-
ralization, and (iv) Wiktionary list of mass nouns,
pluralia tantum.
7 Results and Discussion
Table 4 shows the results on the test set (de-
velopment and official) for each component of
the correction system and the integrated system.
The evaluation was done using the M2 method
(Dahlmeier and Ng, 2012). This involves comput-
ing F1 measure between a set of proposed system
edits and a set of human-annotated gold-standard
edits. However, evaluation is complicated by the
fact that there may be multiple edits which gen-
erate the same correction. The following example
illustrates this behaviour:
Source: I ate mango
Hypothesis: I ate a mango
The system edit is ? a, whereas the gold stan-
dard edit is mango?a mango. Though both the
edits result in the same corrected sentence, they do
not match. The M2 algorithm resolves this prob-
lem by providing an efficient method to detect the
sequence of phrase-level edits between a source
sentence and a system hypothesis that achieves the
highest overlap with the gold-standard annotation.
It is clear that the low recall of the noun-number
and determiner correction components have re-
sulted in a low overall score for the system. This
underscores the difficulty of the two problems.
The feature sets seem to have been unable to cap-
ture the patterns determining the noun-number and
determiner. Consider a few examples, where the
evidence for correction look strong:
1. products such as RFID tracking system have
become real
2. With the installing of the surveillances for
every corner of Singapore
A cursory inspection of the corpus indicates that
in the absence of a determiner (example (1)), the
noun tends to be plural. This pattern has not been
captured by the correction system. The coverage
of the Wiktionary mass noun and pluralia tantum
dictionaries is low, hence this feature has not had
the desired impact (example(2)).
The SVA correction component has a reason-
ably good precision and recall - performing best
amongst all the correction components. Since
most errors affecting agreement (noun-number,
verb form, etc.) were not corrected, the SVA
agreement component could not correct the agree-
ment errors. If these errors had been corrected, the
accuracy of the standalone SVA correction com-
ponent would have been higher than that indicated
by the official score. To verify this, we manually
analyzed the output from the SVA correction com-
ponent and found that 58% of the missed correc-
tions and 43% of the erroneous corrections would
not have occurred if some of the other related er-
rors had been fixed. If it is assumed that all these
errors are corrected, the effective accuracy of SVA
correction increases substantially as shown in Ta-
ble 5. A few errors in the gold standard for SVA
agreement were also considered for computing the
effective scores. The standalone SVA correction
module therefore has a good accuracy.
A major reason for SVA errors (?18%) is
wrong output from NLP modules like the POS tag-
ger, chunker and parser. The following are a few
examples:
? The verb group is incorrectly identified if
there is an adverb between the main and aux-
iliary verbs.
It [do not only restrict] their freedom in all
86
SVA Score
Development test set Official test set
P R F-1 P R F-1
Official 16.67 23.42 19.78 29.57 27.42 28.45
Effective 51.02 55.55 53.18 65.32 66.94 66.12
Table 5: M2 scores (original and modified) for SVA correction
aspects , but also causes leakage of personal
information .
? Two adjacent verb groups are not distin-
guished as separate chunks by the chunker
when the second verb group is non-finite in-
volving an infinitive.
The police arrested all of them before they
[starts to harm] the poor victim.
? The dependency parser makes errors in iden-
tifying the subject of a verb. The noun prob-
lems is not identified as the subject of is by
the dependency parser.
Although rising of life expectancies is an
challenge to the entire human nation , the
detailed problems each country that will en-
counter is different.
Some phenomena have not been handled by our
rules. Our system does not handle the case where
the subject is a gerund phrase. Consider the exam-
ple,
Collecting coupons from individuals are the first
step.
The verb-number should be singular when a
gerund phrase is the subject. In the absence of
rules to handle this case, coupons is identified as
the subject of are by the dependency parser and
consequently, no correction is done.
Our rules do not handle interrogative sentences
and interrogative pronouns. Hence the following
sentence is not corrected,
People do not know who are tracking them.
Table 6 provides an analysis of the error type
distribution for SVA errors on the official test set.
8 Conclusion
In this paper, we presented a hybrid grammati-
cal correction system which incorporates both ma-
chine learning and rule-based components. We
proposed a new rule-based method for subject-
verb agreement correction. As future work, we
plan to explore richer features for noun-number
and determiner errors.
Error types % distribution
Noun-number errors 58.02 %
Wrong tagging, chunking, parsing 18.52 %
Wrong gold annotations 7.40%
Rules not designed 6.1%
Others 9.88 %
Table 6: Causes for missed SVA corrections and
their distribution in the official test set
References
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies-Volume 1.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS Corpus of Learner English. In To
appear in Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in english article usage by
non-native speakers. Natural Language Engineer-
ing.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In AAAI.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In To appear in Proceedings of the Seventeenth Con-
ference on Computational Natural Language Learn-
ing.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
87
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 90?96,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The IIT Bombay Hindi?English Translation System at WMT 2014
Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra, Anoop Kunchukuttan,
Ritesh Shah, Pushpak Bhattacharyya
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay
{piyushdd,rajen,abhijitmishra,anoopk,ritesh,pb}@cse.iitb.ac.in
Abstract
In this paper, we describe our English-
Hindi and Hindi-English statistical sys-
tems submitted to the WMT14 shared task.
The core components of our translation
systems are phrase based (Hindi-English)
and factored (English-Hindi) SMT sys-
tems. We show that the use of num-
ber, case and Tree Adjoining Grammar
information as factors helps to improve
English-Hindi translation, primarily by
generating morphological inflections cor-
rectly. We show improvements to the
translation systems using pre-procesing
and post-processing components. To over-
come the structural divergence between
English and Hindi, we preorder the source
side sentence to conform to the target lan-
guage word order. Since parallel cor-
pus is limited, many words are not trans-
lated. We translate out-of-vocabulary
words and transliterate named entities in
a post-processing stage. We also investi-
gate ranking of translations from multiple
systems to select the best translation.
1 Introduction
India is a multilingual country with Hindi be-
ing the most widely spoken language. Hindi and
English act as link languages across the coun-
try and languages of official communication for
the Union Government. Thus, the importance of
English?Hindi translation is obvious. Over the
last decade, several rule based (Sinha, 1995) , in-
terlingua based (Dave et. al., 2001) and statistical
methods (Ramanathan et. al., 2008) have been ex-
plored for English-Hindi translation.
In the WMT 2014 shared task, we undertake
the challenge of improving translation between the
English and Hindi language pair using Statisti-
cal Machine Translation (SMT) techniques. The
WMT 2014 shared task has provided a standard-
ized test set to evaluate multiple approaches and
avails the largest publicly downloadable English-
Hindi parallel corpus. Using these resources,
we have developed a phrase-based and a factored
based system for Hindi-English and English-Hindi
translation respectively, with pre-processing and
post-processing components to handle structural
divergence and morphlogical richness of Hindi.
Section 2 describes the issues in Hindi?English
translation.
The rest of the paper is organized as follows.
Section 3 describes corpus preparation and exper-
imental setup. Section 4 and Section 5 describe
our English-Hindi and Hindi-English translation
systems respectively. Section 6 describes the post-
processing operations on the output from the core
translation system for handling OOV and named
entities, and for reranking outputs from multiple
systems. Section 7 mentions the details regarding
our systems submitted to WMT shared task. Sec-
tion 8 concludes the paper.
2 Problems in Hindi?English
Translation
Languages can be differentiated in terms of
structural divergences and morphological mani-
festations. English is structurally classified as
a Subject-Verb-Object (SVO) language with a
poor morphology whereas Hindi is a morpho-
logically rich, Subject-Object-Verb (SOV) lan-
guage. Largely, these divergences are responsi-
ble for the difficulties in translation using a phrase
based/factored model, which we summarize in this
section.
2.1 English-to-Hindi
The fundamental structural differences described
earlier result in large distance verb and modi-
fier movements across English-Hindi. Local re-
ordering models prove to be inadequate to over-
90
come the problem; hence, we transformed the
source side sentence using pre-ordering rules to
conform to the target word order. Availability of
robust parsers for English makes this approach for
English-Hindi translation effective.
As far as morphology is concerned, Hindi is
more richer in terms of case-markers, inflection-
rich surface forms including verb forms etc. Hindi
exhibits gender agreement and syncretism in in-
flections, which are not observed in English. We
attempt to enrich the source side English corpus
with linguistic factors in order to overcome the
morphological disparity.
2.2 Hindi-to-English
The lack of accurate linguistic parsers makes it dif-
ficult to overcome the structural divergence using
preordering rules. In order to preorder Hindi sen-
tences, we build rules using shallow parsing infor-
mation. The source side reordering helps to reduce
the decoder?s search complexity and learn better
phrase tables. Some of the other challenges in gen-
eration of English output are: (1) generation of ar-
ticles, which Hindi lacks, (2) heavy overloading of
English prepositions, making it difficult to predict
them.
3 Experimental Setup
We process the corpus through appropriate filters
for normalization and then create a train-test split.
3.1 English Corpus Normalization
To begin with, the English data was tokenized us-
ing the Stanford tokenizer (Klein and Manning,
2003) and then true-cased using truecase.perl pro-
vided in MOSES toolkit.
3.2 Hindi Corpus Normalization
For Hindi data, we first normalize the corpus us-
ing NLP Indic Library (Kunchukuttan et. al.,
2014)
1
. Normalization is followed by tokeniza-
tion, wherein we make use of the trivtokenizer.pl
2
provided with WMT14 shared task. In Table 1, we
highlight some of the post normalization statistics
for en-hi parallel corpora.
1
https://bitbucket.org/anoopk/indic_
nlp_library
2
http://ufallab.ms.mff.cuni.cz/~bojar/
hindencorp/
English Hindi
Token 2,898,810 3,092,555
Types 95,551 118,285
Total Characters 18,513,761 17,961,357
Total sentences 289,832 289,832
Sentences (word
count ? 10)
188,993 182,777
Sentences (word
count > 10)
100,839 107,055
Table 1: en-hi corpora statistics, post normalisa-
tion.
3.3 Data Split
Before splitting the data, we first randomize the
parallel corpus. We filter out English sentences
longer than 50 words along with their parallel
Hindi translations. After filtering, we select 5000
sentences which are 10 to 20 words long as the test
data, while remaining 284,832 sentences are used
for training.
4 English-to-Hindi (en-hi) translation
We use the MOSES toolkit (Koehn et. al., 2007a)
for carrying out various experiments. Starting with
Phrase Based Statistical Machine Translation (PB-
SMT)(Koehn et. al., 2003) as baseline system we
go ahead with pre-order PBSMT described in Sec-
tion 4.1. After pre-ordering, we train a Factor
Based SMT(Koehn, 2007b) model, where we add
factors on the pre-ordered source corpus. In Fac-
tor Based SMT we have two variations- (a) using
Supertag as factor described in Section 4.2 and (b)
using number, case as factors described in Section
4.3.
4.1 Pre-ordering source corpus
Research has shown that pre-ordering source lan-
guage to conform to target language word order
significantly improves translation quality (Collins
et. al, 2005). There are many variations of pre-
ordering systems primarily emerging from either
rule based or statistical methods. We use rule
based pre-ordering approach developed by (Pa-
tel et. al., 2013), which uses the Stanford parser
(Klein and Manning, 2003) for parsing English
sentences. This approach is an extension to an ear-
lier approach developed by (Ramanathan et. al.,
2008). The existing source reordering system re-
quires the input text to contain only surface form,
however, we extended it to support surface form
91
along with its factors like POS, lemma etc.. An
example of improvement in translation after pre-
ordering is shown below:
Example: trying to replace bad ideas with good
ideas .
Phr: replace b  r EvcAro\ ko aQC EvcAro\ k
sAT
(replace bure vichaaron ko acche vichaaron ke
saath)
Gloss: replace bad ideas good ideas with
Pre-order PBSMT: aQC EvcAro\ s b  r EvcAro\
ko bdln kF koEff kr rh h{\
(acche vichaaron se bure vichaaron ko badalane
ki koshish kara rahe hain)
Gloss: good ideas with bad ideas to replace trying
4.2 Supertag as Factor
The notion of Supertag was first proposed by
Joshi and Srinivas (1994). Supertags are elemen-
tary trees of Lexicalized Tree Adjoining Grammar
(LTAG) (Joshi and Schabes, 1991). They provide
syntactic as well as dependency information at the
word level by imposing complex constraints in a
local context. These elementary trees are com-
bined in some manner to form a parse tree, due
to which, supertagging is also known as ?An ap-
proach to almost parsing?(Bangalore and Joshi,
1999). A supertag can also be viewed as frag-
ments of parse trees associated with each lexi-
cal item. Figure 1 shows an example of su-
pertagged sentence ?The purchase price includes
taxes?described in (Hassan et. al., 2007). It clearly
shows the sub-categorization information avail-
able in the verb include, which takes subject NP
to its left and an object NP to its right.
Figure 1: LTAG supertag sequence obtained using
MICA Parser.
Use of supertags as factors has already been
studied by Hassan (2007) in context of Arabic-
English SMT. They use supertag language model
along with supertagged English corpus. Ours
is the first study in using supertag as factor
for English-to-Hindi translation on a pre-ordered
source corpus.
We use MICA Parser (Bangalore et. al., 2009)
for obtaining supertags. After supertagging we run
pre-ordering system preserving the supertags in it.
For translation, we create mapping from source-
word|supertag to target-word. An example of im-
provement in translation by using supertag as fac-
tor is shown below:
Example: trying to understand what your child is
saying to you
Phr: aApkA b?A aAps ?A kh rhA h{ yh
(aapkaa bacchaa aapse kya kaha rahaa hai yaha)
Gloss: your child you what saying is this
Supertag Fact: aApkA b?A aAps ?A kh rhA
h{ , us smJn kF koEff krnA
(aapkaa bacchaa aapse kya kaha rahaa hai, use
samajhane kii koshish karnaa)
Gloss: your child to you what saying is , that un-
derstand try
4.3 Number, Case as Factor
In this section, we discuss how to generate correct
noun inflections while translating from English to
Hindi. There has been previous work done in order
to solve the problem of data sparsity due to com-
plex verb morphology for English to Hindi trans-
lation (Gandhe, 2011). Noun inflections in Hindi
are affected by the number and case of the noun
only. Number can be singular or plural, whereas,
case can be direct or oblique. We use the factored
SMT model to incorporate this linguistic informa-
tion during training of the translation models. We
attach root-word, number and case as factors to
English nouns. On the other hand, to Hindi nouns
we attach root-word and suffix as factors. We de-
fine the translation and generation step as follows:
? Translation step (T0): Translates English
root|number|case to Hindi root|suffix
? Generation step (G0): Generates Hindi sur-
face word from Hindi root|suffix
An example of improvement in translation by
using number and case as factors is shown below:
Example: Two sets of statistics
Phr: do k aA kw
(do ke aankade)
Gloss: two of statistics
Num-Case Fact: aA kwo\ k do sV
(aankadon ke do set)
Gloss: statistics of two sets
92
4.3.1 Generating number and case factors
With the help of syntactic and morphological
tools, we extract the number and case of the En-
glish nouns as follows:
? Number factor: We use Stanford POS tag-
ger
3
to identify the English noun entities
(Toutanova, 2003). The POS tagger itself dif-
ferentiates between singular and plural nouns
by using different tags.
? Case factor: It is difficult to find the
direct/oblique case of the nouns as En-
glish nouns do not contain this information.
Hence, to get the case information, we need
to find out features of an English sentence
that correspond to direct/oblique case of the
parallel nouns in Hindi sentence. We use
object of preposition, subject, direct object,
tense as our features. These features are
extracted using semantic relations provided
by Stanford?s typed dependencies (Marneffe,
2008).
4.4 Results
Listed below are different statistical systems
trained using Moses:
? Phrase Based model (Phr)
? Phrase Based model with pre-ordered source
corpus (PhrReord)
? Factor Based Model with factors on pre-
ordered source corpus
? Supertag as factor (PhrReord+STag)
? Number, Case as factor (PhrReord+NC)
We evaluated translation systems with BLEU and
TER as shown in Table 2. Evaluation on the devel-
opment set shows that factor based models achieve
competitive scores as compared to the baseline
system, whereas, evaluation on the WMT14 test
set shows significant improvement in the perfor-
mance of factor based models.
5 Hindi-to-English (hi-en) translation
As English follows SVO word order and Hindi fol-
lows SOV word order, simple distortion penalty in
phrase-based models can not handle the reordering
well. For the shared task, we follow the approach
3
http://nlp.stanford.edu/software/tagger.shtml
Development WMT14
Model BLEU TER BLEU TER
Phr 27.62 0.63 8.0 0.84
PhrReord 28.64 0.62 8.6 0.86
PhrReord+STag 27.05 0.64 9.8 0.83
PhrReord+NC 27.50 0.64 10.1 0.83
Table 2: English-to-Hindi automatic evaluation on
development set and on WMT14 test set.
that pre-orders the source sentence to conform to
target word order.
A substantial volume of work has been done
in the field of source-side reordering for machine
translation. Most of the experiments are based on
applying reordering rules at the nodes of the parse
tree of the source sentence. These reordering rules
can be automatically learnt (Genzel, 2010). But,
many source languages do not have a good robust
parser. Hence, instead we can use shallow pars-
ing techniques to get chunks of words and then
reorder them. Reordering rules can be learned au-
tomatically from chunked data (Zhang, 2007).
Hindi does not have a functional constituency
or dependency parser available, as of now. But,
a shallow parser
4
is available for Hindi. Hence,
we follow a chunk-based pre-ordering approach,
wherein, we develop a set of rules to reorder
the chunks in a source sentence. The follow-
ing are the chunks tags generated by this shallow
parser: Noun chunks (NP), Verb chunks (VGF,
VGNF, VGNN), Adjectival chunks (JJP), Ad-
verb chunks (RBP), Negatives (NEGP), Conjuncts
(CCP), Chunk fragments (FRAGP), and miscella-
neous entities (BLK) (Bharati, 2006).
5.1 Development of rules
After chunking an input sentence, we apply hand-
crafted reordering rules on these chunks. Follow-
ing sections describe these rules. Note that we ap-
ply rules in the same order they are listed below.
5.1.1 Merging of chunks
After chunking, we merge the adjacent chunks, if
they follow same order in target language.
1. Merge {JJP VGF} chunks (Consider this
chunk as a single VGF chunk)
e.g., vEZta h{ (varnit hai), E-Tta h{ (sthit hai)
4
http://ltrc.iiit.ac.in/showfile.php?
filename=downloads/shallow_parser.php
93
2. Merge adjacent verb chunks (Consider this
chunk as a single verb chunk)
e.g., EgrtaA h{ (girataa hai), l  BAtaA h{ (lub-
haataa hai)
3. Merge NP and JJP chunks separated by com-
mas and CCP (Consider this chunk as a single
NP chunk)
e.g., bwA aOr ahm (badaa aur aham)
5.1.2 Preposition chunk reordering
Next we find sequence of contiguous chunks sep-
arated by prepositions (Can end in verb chunks).
We apply following reordering rules on these con-
tiguous chunks:
1. Reorder multi-word preposition locally by re-
versing the order of words in that chunk
e.g., k alAvA (ke alaawaa) ? alAvA k,
k sAmn (ke saamane)? sAmn k
2. Reorder contiguous preposition chunk by re-
versing the order of chunks (Consider this
chunk as a single noun chunk)
e.g., Eh\d Dm m\ taFT kA bwA mh(v (hinduu
dharma me tirtha ka badaa mahatva)? bwA
mh(v kA taFT m\ Eh\d Dm
5.1.3 Verb chunk reordering
We find contiguous verb chunks and apply follow-
ing reordering rules:
1. Reorder chunks locally by reversing the order
of the chunks
e.g., vEZta h{ (varnit hai)? h{ vEZta
2. Verb chunk placement: We place the new
verb chunk after first NP chunk. Same rule
applies for all verb chunks in a sentence, i.e.,
we place each verb chunk after first NP chunk
of the clause to which the verb belongs.
Note that, even though placing verb chunk af-
ter first NP chunk may be wrong reordering.
But we also use distortion window of 6 to 20
while using phrase-based model. Hence, fur-
ther reordering of verb chunks can be some-
what handled by phrase-based model itself.
Thus, using chunker and reordering rules, we
get a source-reordered Hindi sentence.
5.2 Results
We trained two different translation models:
? Phrase-based model without source reorder-
ing (Phr)
? Phrase-based model with chunk-based source
reordering (PhrReord)
Development WMT14
Model BLEU TER BLEU TER
Phr 27.53 0.59 13.5 0.87
PhrReord 25.06 0.62 13.7 0.90
Table 3: Hindi-to-English automatic evaluation on
development set and on WMT14 test set.
Table 3 shows evaluation scores for develop-
ment set and WMT14 test set. Even though we do
not see significant improvement in automatic eval-
uation of PhrReord, but this model contributes in
improving translation quality after ranking, as dis-
cussed in Section 5. In subjective evaluation we
found many translation to be better in PhrReord
model as shown in the following examples:
Example 1: sn 2004 s v kI bAr coVg}-ta
rh h{\ |
(sana 2004 se ve kaii baar chotagrasta rahe hain.)
Phr: since 2004 he is injured sometimes .
PhrReord: he was injured many times since 2004
.
Example 2: aobAmA kA rA?~ pEta pd k c  nAv
?cAr hta  bnAyA aAEDkAErk jAl-Tl
(obama ka rashtrapti pad ke chunaav prachaar
hetu banaayaa aadhikarik jaalsthal)
Phr: of Obama for election campaign
PhrReord: official website of Obama created for
President campaign
6 Post processing
All experimental results reported in this paper are
after post processing the translation output. In post
processing, we remove some Out-of-Vocabulary
(OOV) words as described in subsection 6.1, after
which we transliterate the remaining OOV words.
6.1 Removing OOV
We noticed, there are many words in the training
corpus which were not present in the phrase ta-
ble, but, were present in the lexical tranlsation ta-
ble. So we used the lexical table as a dictionary
to lookup bilingual translations. Table 4 gives the
statistics of number of OOV reduced.
94
Model Before After
Phrased Based 2313 1354
Phrase Based (pre-order) 2256 1334
Supertag as factor 4361 1611
Num-Case as factor 2628 1341
Table 4: Statistics showing number of OOV be-
fore and after post processing the English-to-Hindi
translation output of Development set.
6.2 Transliteration of Untranslated Words
OOV words which were not present in the lexi-
cal translation table were then transliterated using
a naive transliteration system. The transliteration
step was applied on Hindi-to-English translation
outputs only. After transliteration we noticed frac-
tional improvements in BLEU score varying from
0.1 to 0.5.
6.3 Ranking of Ensemble MT Output
We propose a ranking framework to select the best
translation output from an ensemble of multiple
MT systems. In order to exploit the strength of
each system, we augment the translation pipeline
with a ranking module as a post processing step.
For English-to-Hindi ranking we combine the
output of both factor based models, whereas,
for Hindi-to-English ranking we combine phrase
based and phrase based with pre-ordering outputs.
For most of the systems, the output translations
are adequate but not fluent enough. So, based on
their fluency scores, we decided to rank the candi-
date translations. Fluency is well quantified by LM
log probability score and Perplexity. For a given
translation , we compute these scores by querying
the 5-gram language model built using SRILM.
Table 5 shows more than 4% relative improvement
in BLEU score for en-hi as well as hi-en transla-
tion system after applying ranking module.
Model BLEU METEOR TER
Phr(en-hi) 27.62 0.41 0.63
After Ranking (en-hi) 28.82 0.42 0.63
Phr(hi-en) 27.53 0.27 0.59
After Ranking (hi-en) 28.69 0.27 0.59
Table 5: Comparision of ranking score with base-
line
7 Primary Systems in WMT14
For English-to-Hindi, we submitted the ranked
output of factored models trained on pre-ordered
source corpus. For Hindi-to-English, we submit-
ted the ranked output of phrase based and pre-
ordered phrase based models. Table 6 shows eval-
uation scores of these systems on WMT14 test set.
Lang. pair BLEU TER
en-hi 10.4 0.83
hi-en 14.5 0.89
Table 6: WMT14 evaluation for en-hi and hi-en.
8 Conclusion
We conclude that the difficulties in English-Hindi
MT can be tackled by the use of factor based SMT
and various pre-processing and post processing
techniques. Following are our primary contribu-
tions towards English-Hindi machine translation:
? Use of supertag factors for better translation
of structurally complex sentences
? Use of number-case factors for accurately
generating noun inflections in Hindi
? Use of shallow parsing for pre-ordering Hindi
source corpus
We also observed that simple ranking strategy ben-
efits in getting the best translation from an ensem-
ble of translation systems.
References
Avramidis, Eleftherios, and Philipp Koehn. 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. ACL.
Banerjee, Satanjeev, and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation with
improved correlation with human judgments. Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational linguistics.
Srinivas Bangalore, Pierre Boulllier, Alexis Nasr,
Owen Rambow, and Beno?
?
ot Sagot. 2009. MICA:
a probabilistic dependency parser based on tree in-
sertion grammars application note. Proceedings of
95
Human Language Technologies The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Associ-
ation for Computational Linguistics.
A. Bharati, R. Sangal, D. M. Sharma and L. Bai.
2006. AnnCorra: Annotating Corpora Guidelines
for POS and Chunk Annotation for Indian Lan-
guages. Technical Report (TR-LTRC-31), LTRC,
IIIT-Hyderabad.
Dave, Shachi and Parikh, Jignashu and Bhattacharyya,
Pushpak. 2001. Interlingua-based English?Hindi
Machine Translation and Language Divergence
Journal Machine Translation
Gandhe, Ankur, Rashmi Gangadharaiah, Karthik
Visweswariah, and Ananthakrishnan Ramanathan.
2011. Handling verb phrase morphology in highly
inflected Indian languages for Machine Translation.
IJCNLP.
Genzel, Dmitriy. 2010. Automatically learning
source-side reordering rules for large scale machine
translation Proceedings of the 23rd international
conference on computational linguistics. Associa-
tion for Computational Linguistics
Hany Hassan, Khalil Sima?an, and Andy Way 2007.
Supertagged phrase-based statistical machine trans-
lation. Proceedings of the Association for Compu-
tational Linguistics Association for Computational
Linguistics.
Aravind K. Joshi and Yves Schabes 1991. Tree-
adjoining grammars and lexicalized grammars.
Technical Report No. MS-CIS-91-22
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. Pro-
ceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?
?
Zej Bojar, Alexan-
dra Constantin and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. Proceedings of the Second Workshop on Hy-
brid Approaches to Translation. Association for
Computational Linguistics.
Philipp Koehn and Hieu Hoang 2007. Factored Trans-
lation Models Conference on Empirical Methods in
Natural Language Processing.
Anoop Kunchukuttan, Abhijit Mishra, Rajen Chatter-
jee,Ritesh Shah, and Pushpak Bhattacharyya. 2014.
Sata-Anuvadak: Tackling Multiway Translation of
Indian Languages. Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation Conference
De Marneffe, Marie-Catherine, and Christopher
D. Manning. 2008. Stanford typed de-
pendencies manual. URL http://nlp. stanford.
edu/software/dependencies manual. pdf (2008).
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. Proceed-
ings of the 40th annual meeting on association for
computational linguistics. Association for Compu-
tational Linguistics.
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale and
Sasikumar M. 2013. Reordering rules for English-
Hindi SMT. Proceedings of the Second Workshop
on Hybrid Approaches to Translation. Association
for Computational Linguistics.
Ananthakrishnan Ramanathan, Pushpak Bhat-
tacharyya, Jayprasad Hegde, Ritesh M. Shah,
and M. Sasikumar. 2008. Simple syntactic and
morphological processing can help English-Hindi
statistical machine translation. In International
Joint Conference on NLP.
Sinha, RMK and Sivaraman, K and Agrawal, A and
Jain, R and Srivastava, R and Jain, A. 1995.
ANGLABHARTI: a multilingual machine aided
translation project on translation from English to In-
dian languages IEEE International Conference on
Systems, Man and Cybernetics
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics.
Zhang, Yuqi, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for sta-
tistical machine translation Proceedings of the
NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation. Association
for Computational Linguistics
Collins, Michael, Philipp Koehn, and Ivona Ku
?
cerova
2005 Clause restructuring for statistical machine
translation. Proceedings of the 43rd annual meeting
on association for computational linguistics. Asso-
ciation for Computational Linguistics
96
