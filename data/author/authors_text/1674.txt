A complete integrated NLG system using AI and NLU tools
Laurence Danlos
Lattice
U. Paris 7, Case 7003
2, place Jussieu
75251 Paris Cedex 05
France
danlos@linguist.jussieu.fr
Adil El Ghali
Lattice
U. Paris 7, Case 7003
2, place Jussieu
75251 Paris Cedex 05
France
adil@linguist.jussieu.fr
Abstract
A standard architecture for an NLG system has
been defined in (Reiter and Dale, 2000). Their
work describes the modularization of an NLG
system and the tasks of each module. How-
ever, they do not indicate what kind of tools can
be used by each module. Nevertheless, we be-
lieve that certain tools widely used by the AI or
NLU community are appropriate for NLG tasks.
This paper presents a complete integrated NLG
system which uses a Description logic for the
content determination module, Segmented Dis-
course Representation Theory for the document
structuring module and a lexicalized formalism
for the tactical component. The NLG system,
which takes into account a user model, is illus-
trated with a generator which produces texts
explaining the steps taken by a proof assistant.
1 Introduction
The standard architecture of an NLG system
proposed in (Reiter and Dale, 2000) is repre-
sented schematically in Figure 1.1. The tool
used by a module and the data structure of
its output are not precisely defined. According
to Reiter and Dale, they vary from one author
to the other. However, we believe that certain
tools widely used by the AI or NLU community
are appropriate for NLG tasks. Therefore, we
reformulate in more specific terms Figure 1.1 as
Figure 1.2.
The paper describes the modules of Fig-
ure 1.2: Section 3 justifies the use of a descrip-
tion logic for the content determination task and
its ouput, a ?message?; Section 4 specifies the
use of sdrt for the document structuring task
Documentstructuring? la SDRT
Communicative goals
ContentDetermination
DocumentStructuring
description logicwith a
Determination Content
Communicative goals
LexicalizedMicro?planner
 SurfaceLexicalized
  Semantic
    Dependency tree
realizer 
Micro?planner
representation    Semantic
Doc
ume
nt P
lann
er
Figure 1.1
Standard architecture of an NLG system
Tac
tica
l com
pon
ent
SDRS
Figure 1.2
Architecture of an NLG system
Document plan
message logical form
with data structures
TextText
 Surfacerealizer 
and its output, a ?document plan?; Section 5
exposes briefly the use of a lexicalized formal-
ism in the tactical component. Each section is
illustrated with GePhoX, a generator that pro-
duces texts which explain the steps taken by
PhoX , a proof assistant (Raffalli and Roziere,
2002). GePhoX is presented in Section 2.
As this paper intends to present a complete
NLG system, there is no room for explaining
each module in detail. We refer the reader to (El
Ghali, 2001) for the content determination mod-
ule, to (Danlos et al, 2001) for the document
structuring module and to Danlos (1998; 2000)
for the lexicalized tactical component. These
goal ?p, d : N(d 6= N0 ? ?q, r : N(r < d ? p = q ? d + r))
1. intros.
2. elim ?4 H well founded.N.
3. intros.
4. elim ?1 d ?3 a lesseq.case1.N.
5. next.
6. intros ? ?.
7. next ?3.
8. instance ?1 N0.
9. instance ?2 a.
10. intro.
11. trivial.
12. local a? = a - d.
13. elim ?1 a? H3.
14. trivial.
15. elim lesseq.S rsub.N.
16. elim ?1 [case] H0.
17. trivial =H1 H5.
18. trivial.
19. lefts H5 ? ?.
20. intros ? ?.
21. next ?3.
22. instance ?4 r.
23. instance ?3 S q.
24. rewrite mul.lS.N ?r add.associative.N ?r H8.
25. intro.
26. trivial.
27. save euclide exists.
Table 1: Proof script for Euclidian division
modules have been built more or less indepen-
dently from each other, but with the same un-
derlying idea: adaptation of NLU/AI theories
and tools to NLG. They are now integrated in a
complete model, which is presented here and il-
lustrated with GePhoX, a generator whose im-
plementation is in progress.
2 GePhoX
PhoX is an extensible proof assistant based on
higher order logic, which has been developped to
help mathematicians building proofs and teach-
ing mathematics. Like other proof assistants,
PhoX works interactively. The user (a mathe-
matician) first gives the theorem to be proven
(a goal). PhoX returns a list of subgoals which
should be easier to prove than the initial goal.
The user enters a command to guide PhoX
in choosing or achieving a subgoal. The proof
is thus computed top-down from goals to ev-
idences. The user?s commands form a Proof
script. PhoX?s output is a list of successive
goals equivalent to a Proof tree.
Both the Proof script and PhoX?s output are
difficult to read (even for a mathematician), as
the reader can see for himself in Table 1 and
Table 2. Hence, the need of an NLG system in
order to obtain an easy-to-read version of the
proof.
GePhoX is given as input both the Proof
script and the Proof tree. It is one of the
Here is the goal:
goal 1/1
|- /\p,d:N (d != N0 ->
\/q,r:N (r < d & p = q * d + r))
End of goals.
%PhoX% intros.
1 goal created.
New goal is:
goal 1/1
H := N p
H0 := N d
H1 := d != N0
|- \/q,r:N (r < d & p = q * d + r)
End of goals.
. . .
Table 2: Proof tree for Euclidian division
main original proposals in our generator (simi-
lar generators, such as PROVERB (Huang and
Fiedler, 1997), take as input only the Proof
tree). It makes it possible for GePhoX to start
from an incomplete proof and produce texts
during the interactive session. These texts help
the mathematician user: before entering a new
command in the Proof script, he can read a text
reminding himself what he has been doing so
far. The Proof script is also useful for identify-
ing the reasoning strategies that have been used
(reasoning by contradiction or induction), while
it is very hard (if not impossible) to retrieve this
information from a Proof tree with its numerous
deduction steps.
Another originality of GePhoX is that it
takes into account the knowledge of the user
who can be either a mathematician using PhoX
or a person more or less novice in mathematics.
For the same proof, GePhoX can generate sev-
eral texts according to the user model.
3 Using a descrition logic (DL)
The knowledge representation system Kl-One
(Branchman et al, 1979) was the first DL. It
was created to formalize semantic networks and
frames (Minsky, 1974). It introduces the no-
tions of TBoxes and ABoxes respectively for
terminological and assertional knowledge. Kl-
One has been widely used in the NLG commu-
nity to formalize the domain model. On the
other hand, this is not the case for the more
recent DLs. Nevertheless, they present at least
two advantages compared to Kl-One : 1) for a
large variety of DLs, sound and complete algo-
rithms have been developped for main inference
problems such as subsumption, concepts satis-
fiability and consistency (Donini et al, 1996);
2) the relations between instances and classes
are well defined for all the constructors, and
their mathematical and computational proper-
ties have been studied in detail (Horrocks et al,
2000). So we believe that DLs are appropriate
for the content determination task as shown in
3.3. Let us first briefly present DLs.
3.1 A brief introduction to DL
The three fundamental notions of DLs are in-
dividuals (representing objects in the domain),
concepts (describing sets of individuals), and
roles (representing binary relations between
individuals or concepts). A DL is characterized
by a set of constructors that allow us to build
complex concepts/roles from atomic ones.
The set of constructors which seem useful for
GePhoX and their syntax are shown in Table
3; examples of concepts and roles with their
semantic are shown underneath Table 3.
Constructor (abbreviation) Syntax
atomic concept A
top >
bottom ?
conjonction C ? D
disjonction (U) C ? D
complement (C) qC
univ. quant. ?R.C
exist. quant. (E) ?R.C
numeral restrictions (N ) >n R.C
?n R.C
collection of individuals (O) {a1,. . .,an}
atomic role P
roles conjonction (R) Q ?R
inverse role R?1
role composition Q ? R
Table 3: Syntax of standard constructors
Examples of concepts with their semantic
Theorem, Variable, {H1}, ?CHOOSE.User
{ x / Theorem(x) } : Theorem concept
{ x / Variable(x) } : Variable concept
{ H1} : concept constructed by the O construc-
tor on individual H1
{ x / ? u : User, CHOOSE(u,x) }
Examples of roles with their semantic
IMPLIES, PROVES
{ x,y / IMPLIES(x,y) } : x implies y
{ x,y / PROVES(x,y) } : x proves y
The choice of constructors is domain depen-
dent. Constructors other than those used in
GePhoX (e.g. temporal extension) can be used
for other domains (e.g. domains with non triv-
ial temporal information), without altering the
mathematical and computational properties.
3.2 Domain and user models in DL
The Domain model is the set of concepts and
roles necessary to express the input of the gen-
erator. More formally, let TD be a TBox, such
that each input I can be described by means of
an ABox AD corresponding to TD. The knowl-
edge base ?D = (TD,AD) is called knowledge
base for the domain and noted dkb. The User
model is a knowledge base ?U = (TU ,AU ) such
that TU and AU are respectivly subsets of TD
and AD. ?U is noted ukb. Table 4 shows a
part of the dkb for GePhoX.
Goal MathObj
Subgoal Axiom
Hypothese Theorem
Rules well_founded
Intro lesseq.case1
Elim add.associative
Rewrite Operator
Trivial LogicalOper
Left Exist
ReasonningStrategy Forall
ByInduction LAnd
ByContradiction ArithOper
. . . Add
. . . Multi
Table 4: GePhoX Domain model
3.3 Content determination tasks
The content determination module performs
four tasks, as shown in Figure 2.
Translation: The input of the generator (as-
sertional information) is first translated into
concepts of the TBox. For that purpose, a
correspondence is established between the ele-
ments of the input and concepts and roles in
the dkb. The O constructor is used to keep
information about the individuals occurring in
the input. For example, command 2 in Table 1
with individual H is translated into the concept
C0
.
= ?EliminationWell founded.Hypothese
{H}, and commands 8 to 11 are translated into
C1
.
= ?ByInduction {p}.
Selection: The selection task consists of
choosing the relevant concepts among those
constructed in the translation phase with regard
to the ukb. For example, if C0 is an unknown
concept for the user, a concept C must be looked
up in the ukb such as C approximates1 C0.
TBoxConcepts
Concepts
Translation
Selection
Verification
Instanciation
Terminological Assertional
Logical Form
ABox
Input
Figure 2: Content Determination Tasks
Verification: At this point, the coherence of
all the concepts of the selection is verified. For
example, if the user tries to reason by induction
on a real number, GePhoX tells him that it is
not possible.
Instanciation: With the information about
individuals, which have been kept in the transla-
tion phase (with the use of the O constructor),
the instanciation task is straightforward. Ta-
ble 5 shows some instanciated concepts for the
Euclidian division.
As is well known, designing knowledge bases
(dkb and ukb) and translating the input of the
generator into concepts and roles of the DL is
a difficult task which has to be fulfilled for ev-
ery generator. However, with a DL, the selec-
tion, verification and instanciation tasks are do-
main independent: algorithms and their imple-
mentation are reusable. Moreover, when using
a DL for the content determination task, the
?message? is a first order logic formula (a stan-
dard representation shared by a large commu-
1Given two TBoxes T and T ? with T ? T ? and a
concept C ? T ? T ?, C? ? T ? approximates C if C
minimally subsumes C? or C? minimally subsumes C.
. ?p1 ? Entier
named(p1,p)
choose(user, p1)
. ?d1 ? EntierNonNul
named(d1,d)
choose(user, d1)
. ?f1 ? Formula
constant(f1,?q,r: N (r < d ? p = q.d + r))
. prove(user, f1)
induction(f1, p1)
. . .
Table 5: DL-Message for Euclidian division
nity) which takes into account the user knowl-
edge and whose coherence has been checked.
4 Using SDRT for document
structuring
In (Danlos et al, 2001) we advocate using sdrt
(Segmented Discourse Representation Theory
(Asher, 1993; Asher and Lascarides, 1998)) as
a discourse framework, since sdrt and drt
(Discourse Representation Theory, (Kamp and
Reyle, 1993)) are the most popular frameworks
for formal and computational semantics. Let us
briefly present sdrt.
4.1 A brief introduction to SDRT
sdrt, designed first for text understanding, was
introduced as an extension of drt in order to ac-
count for specific properties of discourse struc-
ture. sdrt can be viewed as a super-layer on
drt whose expressiveness is enhanced by the
use of discourse relations. Thus the drt struc-
tures (Discourse Representation Structures or
drs) are handled as basic discourse units in
sdrt.
drss are ?boxed? first order logic formulae.
Formally, a drs is a couple of sets ?U,Con?. U
(the universe) is the set of discourse referents.
Con contains the truth conditions representing
the meaning of the discourse.
A sdrs is a pair ?U,Con?, see Figure 3. U
is a set of labels of drs or sdrs which can
be viewed as ?speech act discourse referents?
(Asher and Lascarides, 1998). Con is a set of
conditions on labels of the form:
? pi : K, where pi is a label from U and K is
a (s)drs
? R(pii, pij), where pii and pij are labels and
R a discourse relation. Discourse relations
are inferred non-monotonically by means of
a defeasible glue logic exploiting lexical and
world knowledge.
SDRS
labels
  
 


	 Max
fall  
   


	 John
push    

	
Explanation        
discourse referents
DRS (basic discourse
constituents)
conditions(content/meaning)
discourse relation
Figure 3: sdrs for Max fell. John pushed him.
4.2 Building a SDRS
Starting from a ?message? encoded into a log-
ical form, the document structuring module
builds a sdrs. On a first step, the logical form
is translated into a drs. In the case of a purely
existential formula2, this amounts to putting all
the variables into the universe of the drs and
splitting the formula into elementary conjoined
conditions.
After this first step, the document structuring
task amounts to building a sdrs from a drs and
to go on recursively on each embedded (s)drs.
This process is schematized below.
universe
condition1
condition2
condition3
condition4
condition5
condition6
condition7
??
pi1 pi2 pi3
pi1 :
universe1
condition1
condition7
pi2 :
universe2
condition2
condition5
pi3 :
universe3
condition4
R1(pi1, pi2) ? condition3
R2(pi2, pi3) ? condition6
Let us first examine the principles governing
the splitting of the conditions. All the condi-
tions in the drs have to be expressed in the
sdrs. Two cases arise:
? either a condition in the drs appears as a
condition in one of the sub-drs; that is the
case for condition1 which appears in the
sub-drs labelled pi1;
2More complex formulas are not considered here.
? or it is expressed through a discourse re-
lation; that is the case for condition3 with
R1(pi1, pi2) ? condition3, which means that
R1(pi1, pi2) must have condition3 among its
consequences: no other element is in charge
of expressing condition3.
To establish discourse relations, the sdrt
conditions are reversed. As an illustration, in
sdrt for text understanding, there is the Ax-
iom (1) for Narration. This axiom states that if
Narration holds between two sdrss pi1 and pi2,
then the main event (me) of pi1 happens before
the main event of pi2.
(1) 2(Narration(pi1, pi2) ? me(pi1) < me(pi2))
For text generation, this axiom is reversed as
shown below (Roussarie, 2000, p. 154):
? If k1 and k2 are drss whose main eventu-
alities are not states,
? and if the main event of k1 occurs before
the main event of k2,
? then Narration(pi1, pi2) is valid when pi1 and
pi2 respectively label k1 and k2.
As another example, the condition
cause(e1, e2) can be expressed through Re-
sult(pi1, pi2) or Explanation(pi2, pi1) when pi1
and pi2 label the sub-drss that contain the
descriptions of e1 and e2 respectively.
Let us now examine how we determine the
universes of sub-drss, i.e. discourse refer-
ents, while observing two technical constraints,
namely:
? the arguments of any condition in a sub-
drs must appear in the universe of this
drs;
? the universes of all the sub-drss have to be
disjoint. This constraint is the counterpart
of the following constraint in understand-
ing: ?partial drss introduce new discourse
referents? (Asher, 1993, p. 71).
These two constraints are not independent.
Assuming that the first constraint is respected,
the second one can be respected with the fol-
lowing mechanism: if a variable x already ap-
pears in a preceding sub-drs labelled pix, then
a new variable y is created in the universe of
the current sub-drs labelled piy and the con-
dition y = x is added to the conditions of piy.
The discourse referent y will be generated as an
anaphora if pix is available to piy (Asher, 1993),
otherwise it will be generated as a definite or
demonstrative NP.
A document structuring module la sdrt
based on the principles we have just exposed
can be used for any generator (whose ?message?
is first order logic formula). The algorithm and
the rules establish discourse relations (obtained
by reversing the rules in NLU) are generic. See
below an example of sdrs in GePhoX, the
sdrs built from Table 5.
pi3pi4
pi3 :
pi1pi2
pi1 :
x u e1
user(u)
entier(x)
named(x,p)
choose(e1,u,x)
pi2 :
y v e2
entier-non-nul(y)
named(y, d)
choose(e2,v,y)
v = u
Parallel(pi1,pi2)
pi4 :
x1 f w e3
formula(f)
constant(f,?q,r:N . . . )
prove(e3,w,f)
induction(e3,x1)
w = u
x1 = x
Narration(pi3,pi4)
Table 6: sdrs for Euclidian division
5 Using a lexicalized grammar for
the tactical component
Lexicalized grammars are commonly used in
NLU and also in NLG (Stede, 1996). In Dan-
los (1998; 2000) we propose a lexicalized formal-
ism, called g-tag, for the tactical component of
an NLG system. It is modularized into a micro-
planner which produces a semantic dependency
tree and a surface realizer which produces the
text (see Figure 1.2).
The surface realizer is designed to use the syn-
tactic and lexical information of a lexicalized
tag grammar. The tag grammar is extended
to handle multi-sentential texts and not only
isolated sentences.
The microplanner is based on a lexicalized
conceptual-semantic interface. This interface is
made up of concepts; each concept is associated
with a lexical database. In our model, a con-
cept is either a term in the TBox or a discourse
relation. A lexical database for a given concept
records the lexemes lexicalizing it with their ar-
gument structure, and the mappings between
the conceptual and semantic arguments. The
process of generating a semantic dependency
tree from a sdrs ?U,Con? is recursive:
- An element pii in U is generated as a clause
if pii labels a drs and recursively as a text
(possibly a complex sentence) if pii labels a
sdrs.
- A condition R(pii, pij) in Con is generated as a
text ?Si. Cue Sj .? or as a complex sentence
?Si Cue Sj .?, where Si generates pii, Sj pij ,
and Cue is a cue phrase which is encoded
in the lexical database associated with R
(Cue may be empty).
- A condition pi : K in Con where K is a drs
?U,Con? is generated as a clause according
to the following constraints (which are the
counterparts of constraints in understand-
ing):
? A discourse referent in U is generated as an
NP or a tensed verb.
? Conditions guide lexical choices. Condi-
tions such as x = John correspond to
proper nouns. Equality conditions between
discourse referents (e.g. x = y) give rise
to (pronominal or nominal) anaphora. The
other conditions, e.g. prove(e1, x, y), are
lexicalized through the lexical data base as-
sociated with the concept (prove).
The surface realizer, based on a tag gram-
mar, is a set of lexical data bases. A data base
for a given lexical entry encodes the syntactic
structures realizing it with their syntactic argu-
ments. With such a tag grammar and a mor-
phological module, the text is computed in a de-
terministic way from the semantic dependency
tree.
6 Conclusion
Since NLG is a subfield of NLP, which is itself
a subfield of AI, it seems to be a good idea to
reuse tools developped by the NLP or AI com-
munity. We have shown in this paper how to
integrate DL, sdrt, and a lexicalized grammar
into an NLG system, while following the stan-
dard pipelined architecture3.
3Some authors (de Smedt et al, 1996) have made jus-
tified criticisms of the pipelined architecture. However,
we decided to keep it for the time being.
Theorem.
?p,d:IN (d 6= 0 ? ?q,r:IN (r < d ? p = q.d + r))
Proof. Let us choose p, d two natural numbers
with d 6= 0. By induction on p we prove
?q,r:IN (r < d ? p = q.d + r). Let take a a strictly
positive natural. We assume
?b:IN (b < a ? ?q,r:IN (r < d ? b = q.d + r))
and we must prove ?q,r:IN (r < d ? a = q.d + r).
We distinguish two cases: a < d and d ? a. In the
first case, we choose q = 0 and r = a. In the second
case, we take a? = a ? d. Using the induction hy-
pothesis on a?, we find two naturals q, r such that
r < d and a? = q.d + r. We take S q and r as quo-
tient and remaining for the division of a. We must
prove a = S q.d + r which is immediate.
Table 7: A Text of proof for Euclidian division
GePhoX illustrates the applicabilty of our
system. It is currently being implemented in
Java. The development of the document plan-
ner of GePhoX is work in progress. The goal
is to interface this module with CLEF (Meunier
and Reyes, 1999), an implementation of g-tag.
We intend to produce a text as shown in Ta-
ble 7.
References
N. Asher and A. Lascarides. 1998. The seman-
tics and pragmatics of presupposition. Jour-
nal of Semantics, 15(3):239?300.
N. Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer, Dordrecht.
R. Branchman, R. Bobrow, P. Cohen, J. Klovs-
tad, B. Webber, and W. Woods. 1979. Re-
search in natural language understanding.
Technical Report 4274, Bolt. Beranek and
Newman, Cambridge MA.
L. Danlos, B. Gaiffe, and L. Roussarie. 2001.
Document structring a` la sdrt. In ACL?2001
Toulouse Proceeding.
L. Danlos. 1998. G-TAG : un formalisme lexi-
calise? pour la ge?ne?ration de textes inspire? de
tag. Revue T.A.L., 39(2):7?33.
L. Danlos. 2000. G-TAG: A lexicalized formal-
ism for text generation inspired by Tree Ad-
joining Grammar. In A. Abeille? and O. Ram-
bow, editors, Tree Adjoining Grammars: for-
malisms, linguistics analysis and processing,
pages 343?370. CSLI Publications, Stanford.
K. de Smedt, H. Horacek, and M. Zock.
1996. Architectures for natural language
generation: Problems and perspectives. In
G. Adorni and M. Zock, editors, Trends in
NLG. Proceedings of the 4th European Work-
shop, EWNLG?93, Pisa. Springer-Verlag.
F. Donini, M. Lenzerini, D. Nardi, and
A. Schaerf. 1996. Reasoning in descrip-
tion logics. In G. Brewka, editor, Principles
of Knowledge Representation and Reasoning,
Studies in Logic, Language and Information.
CLSI Publications.
A. El Ghali. 2001. Une logique de description
pour le module quoi-dire-? DEA de linguis-
tique informatique, Universite? Paris 7.
I. Horrocks, U. Sattler, and S. Tobies. 2000.
Practical reasoning for very expressive de-
scription logics. Logic Journal of the IGPL,
8(3):239?264.
X. Huang and A. Fiedler. 1997. Proof verbal-
ization as an application of NLG. In IJCAI
(2), pages 965?972.
H. Kamp and U. Reyle. 1993. From Discourse
to Logic. Kluwer Academic Publishers, Dor-
drecht, The Netherlands.
F. Meunier and R. Reyes. 1999. La plate
forme de dveloppement de gnrateurs de textes
CLEF. In Actes du 2e Colloque Franco-
phone sur la Gnation Automatique de Textes,
GAT?99, Grenoble.
M. Minsky. 1974. A framework for representing
knowledge. MIT-AI Laboratory Memo 306.
C. Raffalli and P. Roziere, 2002. The PhoX
Proof checker documentation. LAMA, Uni-
versite? de Savoie / Universite? Paris 7.
E. Reiter and R. Dale. 2000. Building Natural
Language Generation Systems. Cambridge
University Press.
L. Roussarie. 2000. Un mode`le the?orique
d?infe?rences de structures se?mantiques et dis-
cursives dans le cadre de la ge?ne?ration au-
tomatique de textes. The`se de doctorat en lin-
guistique, Universite? Paris 7.
M. Stede. 1996. Lexical paraphrases in multi-
lingual sentences generation. Machine Trans-
lation, 11.
Automatic recognition of French expletive pronoun occurrences 
Laurence DANLOS 
Universit? Paris 7 
2 Place Jussieu 
F-75005 Paris 
danlos@linguist.jussieu.fr 
 
 
Abstract 
We present a tool, called ILIMP, which 
takes as input a raw text in French and 
produces as output the same text in 
which every occurrence of the pronoun 
il is tagged either with tag [ANA] for 
anaphoric or [IMP] for impersonal or 
expletive. This tool is therefore 
designed to distinguish between the 
anaphoric occurrences of il, for which 
an anaphora resolution system has to 
look for an antecedent, and the expletive 
occurrences of this pronoun, for which it 
does not make sense to look for an 
antecedent. The precision rate for ILIMP 
is 97,5%. The few errors are analyzed in 
detail. Other tasks using the method 
developed for ILIMP are described 
briefly, as well as the use of ILIMP in a 
modular syntactic analysis system. 
1 Introduction 
A lot of research is dedicated to anaphora 
resolution since it is a crucial issue, for example, 
for Information Retrieval or Text Summarization. 
Among anaphora, third person pronouns are quite 
frequent and therefore widely studied. Pronoun il 
in French, it in English, can be used either 
"impersonally" ("expletively") (il pleut, it rains) 
or anaphorically (il est violet, it is purple). 
Therefore, authors who have developed a 
pronoun resolution system acknowledge that the 
impersonal pronoun occurrences must be 
recognized first, before dealing with anaphoric 
pronouns. 
There exists a number of works on the 
English pronoun it, among them (Lapin, Leass, 
1994), (Kennedy, Bogurev, 1996) and (Evans 
2001). However, no work has been done on the 
French pronoun il1. This paper presents a tool, 
ILIMP, which is designed to mark any occurrence 
of il with either tag [IMP] or tag [ANA] (for 
impersonal or anaphoric use, respectively). This 
tool is rule based (as it is the case for Lapin and 
Leass' system); it works on raw texts (contrarily 
to Lapin and Leass' system which relies on a 
syntactic analysis). 
If ILIMP is imperative for an anaphora 
resolution system, it is also a tool which can be 
integrated into a processing chain within a 
modular approach to syntactic analysis. First, it 
should be noted that [IMP] and [ANA] can be 
viewed as an enhancement of the part-of-speech 
tag set generally used in taggers: the tag 
?pronoun? would be replaced by two tags, 
?anaphoric pronoun? versus ?impersonal 
(expletive) pronoun?. It is known that the richer 
the tag set is, the better would the syntactic 
analysis based on tagging be (Nasr, 2004). 
Moreover, it will be shown that tools derived 
from ILIMP can be used for other linguistic 
annotations. 
Section 2 presents the method, which is 
based, on linguistic grounds, on a French 
linguistic resource, the Lexicon-Grammar, and 
on computational grounds, on a tool, Unitex. 
Section 3 presents the realization of ILIMP, the 
                                                          
1 Some data on the discrepancies between il and it. While 
the English pronoun it can be found in subject and object 
positions, the French pronoun il can be found only in a 
subject position. When it is an anaphoric subject pronoun, it 
can have a clausal or nominal antecedent, while an 
anaphoric il can have only a nominal antecedent. The 
anaphoric subject pronoun il translates in English as he or it 
depending on the human nature of the nominal antecedent. 
73
difficulties which have been encountered and the 
choices made to solve them. Finally, Section 4 
presents an evaluation of ILIMP and discusses its 
positioning within a modular syntactic analysis.  
2 Method  
2.1 Lexicon-Grammar 
As for most linguistic phenomena, the 
impersonal use of il depends on both lexical and 
syntactic conditions. For example, the adjective 
violet (purple) can never be the lexical head of an 
impersonal clause ? see (1a); the adjective 
probable (likely) followed by a clausal 
complement anchors an impersonal clause ? see 
(1b); and the adjective difficile (difficult) when 
followed by an infinitival complement introduced 
by the preposition de (resp. ?) anchors an 
impersonal (resp. personal) clause ? see (1c) and 
(1d). 
(1)a Il est violet (It is purple) 
     b Il est probable que Fred viendra (It is 
likely that Fred will come) 
     c Il est difficile de r?soudre ce probl?me (It 
is difficult to solve this problem) 
    d Il est difficile ? r?soudre, ce probl?me (It 
is difficult to solve, this problem) 
Therefore, the French lexicon-grammar 
developed by Maurice Gross and his group 
(Gross 1994, Lecl?re 2003) is an appropriate 
linguistic resource for ILIMP since it describes, 
for each lexical head of a clause, its syntactic 
arguments and the possible alternations. From the 
lexicon-grammar, I have (manually) extracted all 
the items that can be the lexical head of an 
impersonal clause while recording their syntactic 
arguments. See below for a brief overview of the 
lexical heads that I have recorded.  
First, one has to distinguish verbal phrases for 
which the subject can only be the impersonal 
pronoun il, from those whose surface subject is 
impersonal il because their deep subject is 
extraposed in a post-verbal position.  
Among the former ones, I have compiled 45 
meteorological verbal phrases (Il neige (It 
snows), Il fait beau (It is a nice day)), 21 verbs 
from Table 17 of (Gross 1975) (Il faut que Fred 
vienne /du pain) and 38 frozen expressions (Il 
?tait une fois (once upon a time), quoi qu'il en 
soit (whatsoever)). 
Among the latter ones, one has to distinguish 
those with a clausal extraposed subject from 
those with a nominal extraposed subject. 
Among the former ones, I have compiled 682 
adjectives (Il est probable que Fred viendra (it is 
likely that Fred will come)), 88 expressions of the 
form Pr?p X (Danlos 1980) (Il est de r?gle de 
faire un cadeau (It is standard practice to make a 
present)), and around 250 verbs from (Gross 
1975) (Il est dit que Fred viendra (It is said that 
Fred will come)).  
Among the latter ones with a nominal 
extraposed subject, some are quite frequent verbs 
such as rester or manquer, while others are verbs 
in the passive form only used in a refined register 
(Il est venu trois personnes (Three persons 
came)). 
2.2 Unitex 
Unitex 2  is a tool which allows us to write 
linguistic patterns (regular expressions or 
automata) which are located in the input text, 
with a possible addition of information when an 
automaton is in fact a transducer. A raw text, 
when given as input to Unitex, is first pre-
processed: it is segmented into sentences, some 
compound expressions are recognized as such, 
and each token is tagged with all the parts of 
speech and inflexion features recorded in its 
entry (if any) in the French full-form 
morphologic dictionary DELAF (Courtois 2004). 
There is no disambiguation at all; in other words, 
the pre-processing in Unitex does not amount to 
a tagging. 
For ILIMP, the basic idea is to manually write 
patterns (transducers) such as the one presented 
in (2) in a simplified linear form. <?tre.V:3s> 
targets the third person singular inflected forms 
of the verb ?tre; <Adj1:ms> targets the masculine 
singular adjectives that belong to the class Adj1, 
which groups together  adjectives behaving as 
difficult; <V:K> targets any verb in the infinitive 
form. [IMP] is a tag which is added in the input 
text to the occurrences of il that appear in clauses 
which follow the pattern in (2). The occurrence 
of il in (1c) is thereby marked with tag [IMP]. 
(2) Il[IMP] <?tre.V:3s> <Adj1:ms> de 
<V:K> 
                                                          
2 Unitex is a GPL open source system, which is similar to 
Intex (Silberstein 1994). Documentation and download of 
Unitex can be found at the following url: http://ladl.univ-
mlv.fr. 
74
Tag [ANA] is the default value: it marks the 
occurrences of il that have not been tagged with 
[IMP]. The occurrence of il in (1d) is thereby 
marked with tag [ANA]. Nevertheless, the matter 
is slightly more complex, since there is a third 
tag, [AMB], which is explained in Section 3.2. 
The output of ILIMP is therefore the input text in 
which each occurrence of il is marked with one 
of [IMP], [ANA] and [AMB]. 
After this presentation of the theoretical 
principles underlying ILIMP, let us examine its 
realization. 
3 Realization 
3.1 Left context of the lexical head 
In (1c), the left context of the lexical head - the 
sequence of tokens on the left of difficile 
(difficult) - is reduced to Il est (it is). However, 
sentences such as (3a) or (3b), in which the left 
context of the lexical head is more complex, are 
frequently found in real texts. In (3a), the left 
context includes (from right to left) the adverb 
tr?s (very) which modifies the adjective, the verb 
para?tre (seem) in the infinitive form which is a 
?light verb? for adjectives, the pronoun lui (to 
him) and finally the modal verb peut (may) 
preceded by il (it). In (3b), it includes the light 
verb s'av?rer conjugated in a compound tense 
(s'est av?r?) and negated (ne s'est pas av?r?). 
(3)a Il peut lui para?tre tr?s difficile de 
r?soudre ce probl?me (It may seem very 
difficult to him to solve this problem) 
    b Il ne s'est pas av?r? difficile de r?soudre 
ce probl?me (It didn't turn out to be difficult 
to solve this problem) 
As a consequence, for each type of the lexical 
heads (adjectival, verbal) that anchors an 
impersonal clause, all the elements that may 
occur in the left-context have to be determined 
and integrated in patterns. This raises no real 
difficulty, though it is time consuming 3 . In 
contrast, we are faced with tough ambiguities 
when coming to the right context, as we are 
going to show it. 
In the rest of the paper, patterns are presented 
with simplified left-contexts - as in (2) - for the 
sake of readability. 
                                                          
3 ILIMP can be re-used in a tool which aims at identifying the 
lexical head of a clause. 
3.2 Right context of the lexical head 
Syntactic ambiguities 
There is a number of syntactic ambiguities in the 
right context since, as is well known, a sequence 
of parts of speech may receive several syntactic 
analyses. As an illustration, consider the pattern 
in (4a), in which the symbol ? matches any non-
empty sequence of tokens. This pattern 
corresponds to two syntactic analyses: (4b) in 
which il is impersonal and the infinitival phrase 
de <V:K> is subcategorized by difficile, and (4c) 
in which il is anaphoric and the infinitival phrase 
is part of an NP. These two analyses are 
illustrated in  (4d) and (4e) respectively - these 
sentences differ only in the adverb ici/juste. 
(4)a Il est difficile pour ? de <V:K> 
     b Il[IMP] est difficile pour [?]NP de 
<V:K> 
     c Il[ANA] est difficile pour [? de 
<V:K>]NP 
     d Il est difficile pour [les ?tudiants qui 
viennent ici]NP de r?soudre ce probl?me (It is 
difficult for the students who came here to 
solve this problem) 
     e Il est difficile pour [les ?tudiants qui 
viennent juste de r?soudre ce probl?me]NP (It 
is difficult for the students who have just 
solved this problem) 
To deal with syntactic ambiguities, one solution 
is to state explicitly that a pattern such as (4a) is 
ambiguous by means of the tag [AMB] which is 
to be interpreted as ?ILIMP cannot determine 
whether il is anaphoric or impersonal?. However 
this tag may be of no help for later processing, 
especially if it is used too often. Another solution 
is to rely upon heuristics based on frequencies. 
For example, sentences which follow the pattern 
in (4a) are more frequently analyzed as (4b) than 
as (4c). Therefore il in (4a) can be tagged as 
[IMP] despite some rare errors. I have adopted 
this latter solution. The heuristics I use are either 
based on my linguistic knowledge and intuition 
and/or on quantitative studies on corpora.  
Lexical ambiguities 
In about ten cases, a lexical item may anchor 
both impersonal and personal clauses with the 
same subcategorization frame, e.g. the adjective 
75
certain (certain) with a clausal complement as 
illustrated in sentence (5a). Since both readings 
of (5a) seem equally frequent, il in the pattern 
(5b) is tagged as [AMB]. 
(5)a Il est certain que Fred viendra (He/it is 
certain that Fred will come) 
     b Il[AMB] est certain que S4 
Other difficulties 
A last type of difficulties is found with 
impersonal clauses with an extraposed nominal 
subject. See the pair in (6a-b) in which the only 
difference is du/de, whereas (6a) is impersonal 
and (6b) personal. Along the same lines, see the 
pair in (6c-d) in which the only difference is 
valise/priorit?, whereas (6c) is impersonal and 
(6d) personal. 
(6)a Il manque du poivre (dans cette maison) 
(There is pepper missing (in this house)) 
     b Il manque de poivre (ce r?ti de porc) (It 
is lacking pepper (this roasting pork)) 
     c Il reste la valise du chef (dans la voiture) 
(There remains the boss' suitcase (in the car)) 
     d Il reste la priorit? du chef (le ch?mage) 
(It remains the boss' priority (unemployment)) 
I have tried to set up heuristics to deal with these 
subtle differences. However, I did not attempt 
(perilous) enterprises such as using the feature [? 
abstract] for nouns. 
In conclusion, ILIMP relies on a number of 
heuristics so as to avoid a too frequent use of 
[AMB]. These heuristics may lead to errors, 
which are going to be examined. 
4 Evaluation 
I have worked on the French newspaper Le 
Monde. More precisely, I have worked on a 
corpus of 3.782.613 tokens extracted from the 
corpus Le Monde'94. Unitex segments this 
corpus into 71.293 sentences. It contains 13.611 
occurrences of token il, and 20.549 occurrences 
of third person subject pronouns, i.e. il, elle, ils, 
elles (he, she, it, they). So il is the most frequent 
third person subject pronoun, with a rate of 66%. 
                                                          
4 S is the symbol for the pattern aiming at representing a 
sentence. This pattern is made up of a non-empty sequence 
of tokens which includes a finite verb. 
From this corpus, 8544 sentences which 
include at least one occurrence of il have been 
extracted, and they add up to around 10.000 
occurrences of il (a complex sentence with 
embedded clauses may include several 
occurrences of il). These sentences have been 
given as input to ILIMP and the results - the tags 
[IMP], [ANA] and [AMB]- have been manually 
evaluated. The evaluators were asked to follow 
only their intuition. 
The result of this evaluation is the following: 
the precision rate is 97,5\%. We are going to 
examine the 2,5\% errors, putting aside [AMB]. 
4.1 Errors from morphological ambiguities 
Errors coming from morphological ambiguities 
are (of course) counted as the other errors coming 
from the realization of ILIMP (which are 
examined in the next sections). Recall (Section 
2.2) that the pre-processing in Unitex does not 
include any disambiguation: it is not a tagger. To 
illustrate the consequences of this point, consider 
the pattern in (7a) in which <V6:W> targets 
verbs of Table 6 in the past participle, e.g. choisi 
(chosen), and S a sequence of tokens which 
includes a finite verb (see note 3). This pattern 
aims at targeting impersonal clauses such as (7b). 
Nevertheless, it also targets (7c), in which the 
pronoun il is thus wrongly tagged as [IMP]. This 
error comes from the fact that the dictionary 
DELAF rightly includes two entries for the word 
m?tres - finite form of the verb m?trer and plural 
form of the noun m?tre - and Unitex does not 
make any distinction between these two entries. 
Therefore, the sequence le b?ton pour soutenir 
une toiture de 170 m?tres is interpreted as 
including a finite verb, and hence follows pattern 
S. 
(7)a Il[IMP]  <avoir.V:3s> ?t? <V6:W> 
(ADV) que S 
    b Il a ?t? choisi que les s?ances se feraient 
le matin vers 9h (It has been chosen that 
sessions would take place around 9 am) 
     c Il a ?t? choisi plut?t que le b?ton pour 
soutenir une toiture de 170 m?tres (It has been 
chosen rather than concrete to support a 170 
meter roof) 
Any tagger should tag the word m?tres in (7c) as 
a noun. Taking as input not a raw text pre-
processed by Unitex but the output of a tagger 
76
would avoid the error on il in (7c). However 
ILIMP would be dependent of the errors of a 
tagger. What is best? More generally, assuming 
that a syntactic parser relies upon a modular 
approach in which a set of modules - tagger, 
named entity recognition module, ILIMP, 
chunker, etc. ? collaborates, the question of the 
order in which the modules should be chained 
arises. 
 Let us have this question open, and come 
back to the errors of ILIMP taking as input a raw 
text. 
4.2 il wrongly tagged as [IMP] instead of 
[ANA]: 0,3\% 
Very few errors: 33. This is surprising when 
considering the frequent appeal to ?brutal? 
heuristics. As an illustration, il in the pattern Il y 
a is systematically tagged as [IMP]. This 
heuristic gives two errors, as in (8a), but around 
1500 right tags, as in (8b). 
(8)a Il revient de Rimini. Il y a donn? la 
r?plique ? Madeleine. (He is back from 
Rimini. He gave there the cue to Madeleine.) 
    b Il y a beaucoup de trafic ? 8h (There is a 
lot of traffic at 8 am) 
4.3 il wrongly tagged as [ANA] instead of 
[IMP]: 2\% 
More errors. This type of errors comes from the 
fact that [ANA] is the default value. These errors 
are thus directly imputable to gaps in the patterns 
making up ILIMP. 
Among these gaps, there are first those 
coming from my laziness/tiredness/lack of time. 
For example, I have introduced quotation marks 
at some places in patterns but not everywhere. 
Hence, il is wrongly tagged as [ANA] in (9a) just 
because of the quotation marks. Similarly, I 
wrote some patterns for cases with subject 
inversion, but I did not take time to write all of 
them, hence the error in (9b).  
(9)a Il[ANA]  ?tait ?m?me souhaitable? que 
celui-ci soit issu ? (It was ?even desirable? 
that this one be from ?) 
     b Est-il [ANA] inconcevable ? (Is it 
inconceivable that ?) 
Secondly, there are lexical gaps. In particular, 
some adjectives which can be the head of 
impersonal clauses are missing: the list of 682 
adjectives I have compiled needs to be 
completed. 
Thirdly, there are syntactic gaps. In particular, 
I have considered any extraposed clausal subject 
as obligatory, whereas there exist cases where 
such a subject is not realized, for example, in 
phrases introduced by comme (as), (10). I have 
created a pattern to take into account such 
phrases but it does not handle all of them. 
(10)  comme il a ?t? annonc? (as it has been 
said 
Finally, gaps are found for impersonal clauses 
with a nominal extraposed subject. In particular, I 
have written no pattern for verbs in the passive 
form used in a refined register, see section 2.1. 
To conclude this section on the occurrences of 
il wrongly tagged as [ANA], I would like to add 
that though the first three types of errors can be 
avoided with a little effort, this is not the case for 
the last type. 
4.4 Other errors: 0,2\% 
Some errors come from the fact that the word il 
is not used as a subject pronoun but as part of a 
named entity in a foreign language, see (11)5.  
(10) Elle a publi? cette revue appel?e Il[ANA] 
Caff? (She published this magazine called Il 
Caff?) 
4.5 Evaluation on other corpora 
An evaluation of ILIMP has also been realized on 
French literary texts written in the XIXth 
century. It concerns 1858 occurrences of il. The 
precision rate falls compared to the journalistic 
genre: it goes from 97,5\% to 96,8\%. This fall 
comes, on the one hand, from impersonal 
expressions which are not used anymore, (11), on 
the other hand, from a high number of sentences 
with subject inversion, as in (9b) in Section 4.3. 
Recall that I have not handled subject inversion 
systematically.  
(11) Mais peut-?tre ?tait-il un peu matin pour 
organiser un  concert (But maybe was it a 
little bit morning to organize a concert) 
The percentage of impersonal il in literary texts 
increases compared to Le Monde corpus: it goes 
from 42\% to 49,8\%. In a more general way, I 
                                                          
5 This kind of error would be avoided if ILIMP took as input a 
text in which the named entities are recognized. 
77
expect important differences on the percentage of 
il with an impersonal use according to the genre 
of corpora 6 , though I don't expect significant 
differences on the precision rate of ILIMP 
(especially if the three first types of errors 
described in Section 4.2 are corrected). This is 
because the list of lexical heads for impersonal 
clauses is closed and stable. 
5 Conclusion and future work 
The method used in ILIMP to locate the 
occurrences of il in an impersonal use, which 
gives good results, can be used for other 
languages and for other tasks. For English, ILIMP 
can be straightforwardly adapted to disambiguate 
the impersonal versus anaphoric use of it as a 
subject pronoun. It has already been said (Section 
3.1) that a tool derived from ILIMP can be 
designed to identify the lexical head of a clause. 
Another tool can be designed to enhance a 
module in charge of the computation of syntactic 
functions, thanks to the notion of ?deep 
extraposed subject?, which is relevant for 
impersonal clauses. Finally, the method I have 
proposed to disambiguate an ambiguous and very 
frequent word as il can be used for other 
ambiguous frequent functional words such as the 
(French) word que (which can be a 
complementizer, a relative pronoun, or an adverb 
in discontinuous restrictive or comparative 
expressions) (Jacques 2005). 
The goal or ILIMP or related ?little? tools is 
obviously modest and restricted when compared 
to the goal of a robust parser which would give 
for any sentence THE correct and complete 
analysis, with a precision rate closed to 98\%. 
However, it has to be acknowledged that such an 
ideal parser does not exist, neither for French nor 
for English, despite many years of effort. So it 
could be a wise strategy to follow the saying 
which goes Many a little makes a mickle. If this 
strategy is followed, research effort is needed 
first to develop such ?little? tools, second to 
determine how to order them in an efficient 
sequential processing chain. 
                                                          
6  Le Monde contains a number of long papers which 
describe in detail the life and work of famous individuals. 
These papers, when they describe the life of a man, link up 
numerous occurrences of anaphoric il (he) referring to the 
man concerned. One may expect that the percentage of 
impersonal il increases in newspaper handling only news or 
economy. 
References 
COURTOIS B. (2004), Dictionnaires ?lectroniques 
DELAF, Syntax, Lexis and Lexicon-Grammar. 
Papers in honour of Maurice Gross , Lingvistic? 
Investigationes Supplementa 24, 
Amsterdam/Philadelphia: Benjamins, pp. 113?133. 
DANLOS L. (1980), Repr?sentation d'informations 
linguistiques: les constructions N ?tre Pr?p X. 
Th?se de troisi?me cycle, Paris: Universit? Paris 7.  
EVANS R. (2001), Applying Machine 
Learning toward an Automatic 
Classification of it, Literary and Linguistic 
Computing, Vol. 16-1 pp. 45-57. 
GROSS M. (1975), M?thode en syntaxe, Paris, 
Hermann. 
GROSS M. (1993), Les phrases fig?es en fran?ais, 
L'information grammaticale 59, Paris, pp. 36-41.  
GROSS M. (1994), Constructing Lexicon-Grammars, 
Computational Approaches to the Lexicon, Oxford, 
Oxford  University Press, pp. 213-263. 
JACQUES M.P. (2005), Que : la valse des ?tiquettes, 
Actes de TALN 05, Dourdan. 
KENNEDY C., BOGURAEV B. (1996), Anaphora for 
Eveyone; Pronominal Anaphora Resolution without 
a Parser, in COLING'96, Copenhagen. 
LAPIN S., LEASS H.J. (1994), An algorithm for 
pronominal anaphora resolution, Computational 
Linguistics, 20(4), pp. 535-561. 
LECL?RE . C. (2003), The lexicon-grammar of 
French verbs: a syntactic database, In Proceedings 
of the First International Conference on Linguistic 
Informatics, Kawaguchi Y. et ali (eds.), UBLI, 
Tokyo University of Foreign Studies. 
NASR A. (2004), Analyse syntaxique probabiliste pour 
grammaires de d?pendances extraites 
automatiquement, Habilitation ? diriger des 
recherches, Universit? Paris 7  
SILBERSTEIN M. (1994), INTEX: a corpus 
processing system, in COLING'94, Kyoto, Japon, 
vol. 1, pp. 579-583.  
 
78
Generat ing a Controlled Language 
Laurence Danlos Guy Lapalme 
Universitd Paris 7 Ddpartement  d' informatique t RO 
TALANA UFR Linguist ique Universitd de Montrdal 
Case 7003-2, Place Jussieu C.P. 6128, Succ Centre-Vil le 
75251 Paris;' France- " Montreal ;  QuEbec, CCana~dai::~3C 337 
danlos?linguist, jussieu, fr lapalme@iro, umontreal, ca 
Veron ika  Lux  * 
Xerox Research Centre Europe 
6, chemin de Maupertu is  
38240 Meylan, France 
Veron ika .  Lux@xrce. xerox ,  com 
Abst rac t  
This paper argues for looking at Controlled Lan- 
guages (CL) from a Natural Language Genera- 
tion (NLG) perspective. We show that CLs are 
used in a normative nvironment in which dif- 
ferent textual modules can be identified, each 
having its own set of rules constraining the text. 
These rules can be used as a basis for natural 
language generation. These ideas were tested in 
a proof of concept generator for the domain of 
aircraft maintenance manuals. 
1 What  is a Cont ro l led  Language? 
Controlled Languages (CLs) result from a grow- 
ing concern about technical documentation 
quality and translation, be it human or auto- 
matic. A CL consists of a glossary and of writ- 
ing rules for the linguistic aspect of the doc- 
umentation. These rules are given as recom- 
mendations or prohibitions for both the lexicon 
and the grammar. Currently, most CLs are 
varieties of "controlled English" which derive 
froth the Caterpillar Tractor Company Funda- 
veloped for CL users, the best known being con- 
formity checkers/controllers such as AlethCL or 
SECC (CLA, 1996). 
A writer expects that the checking tool should 
not only detect errors but also propose a CL 
conformable xpression. A. Nasr (Nasr, 1996), 
who worked on the problem of CL refornmla- 
tion, underlines the difficulties of this task. Re- 
formulation cannot make any hypotheses about 
the conformity of the input sentences, and 
therefore must deal with a wider variety of 
lexico-syntactical constructions than those al- 
lowed in a CL. Some instances of noncompliance 
are relatively easy to detect but much more dif- 
ficult to correct: for example, sentences that are 
longer than the prescribed number of words. 
So there is little hope that human writers will 
ever produce documentation complying strictly 
with a CL even with the help of a conformity 
checker. We argue that it may be more promis- 
ing to use NLG technology for generating doc- 
.umentation in. CL instead of analyzing it af- 
terwards, as it is the case with a conformity 
checker. Few researchers have looked at CLs 
mental English tha t vi~..elab0rated.in the S~: ...... from-:a~-~generation p int  of view.. (Nasr, 1996; 
ties (Scheursand Adriaens, 1992). However CLs Hartley and Paris, 1996); but we think that 
are presently being defined for German, Swedish 
and French. 
Technical writers find it difficult to comply 
with the writing rules of a CL which are often 
hard to justify (CLA. 1996). For them, a CL is 
seen as an additional constraint on an already 
complex task. This is why tools have been de- 
" Work done while at the Adrospatiale Research Center 
there are very compelling reasons for taking a 
generation perspective, in addition to the ad- 
vantages of NLG for CLs that will be presented 
in section 3: 
* As CLs can be viewed as linguistic specifi- 
cations for human beings, it seems natural 
"to, .consider them 'a:s specifica'tkms for the 
linguistic component of an NLG system. 
141 
e CL writing specifications come on top of 
other writing norms which deal with docu- 
ment structuring. For example, in the aero- 
nautical industry, CLs such Simplified En- 
glish (SE) (AEC, 1995) and Fran~ais Ra- 
tionalisd (FR) (GIFAS, 1996) extend the 
ATA 100 norms (Bur, 1995) which describe 
the divisionof the document into chapl:ers, 
sections, subsections, etc. reflecting a tree- 
structured functional organization of the 
airplane: a chapter corresponds to a sys- 
tem (e.g. main rotor), a section to a sub- 
system (e.g. gear box), a subsection to a 
sub-sub-system (e.g. set of gears), and so 
on. Over this thematic structure is added a 
communicative structure to fulfill two main 
goals: describe all systems of the airplane 
and prescribe all maintenance instructions 
for the airplane. The norms of the ATA 
can be viewed as specifications for the text 
structuring component of an NLG system. 
? The thematic and communicative structur- 
ing of the document must also conform 
to a systematic non-linear page number- 
ing system and strict formatting rules us- 
ing SGML tags. These constraints can be 
viewed as specifications for the layout com- 
ponent of an NLG system. 
So we claim that CLs should not be con- 
sidered outside the context of the production 
of complex structured ocuments, which natu- 
rally raises the question of the automatic gen- 
eration of this documentation given some for- 
real representation. This claim led V. Lux (Lux, 
1998) to redefine the notion of a CL. Her study 
has shown that only a few syntactic constraints 
(e.g. coordination constraints) are applicable to 
the whole document. Most constraints are only 
valid for sub-parts of the document, identified 
as "textual modules". Each textual module has 
a particular communicative goal  and a precise 
theme according to the ATA 100 norms. It can 
be divided into smaller modules: for example, 
the Task module is divided into simpler Sub- 
Task modules which are themselves composed 
of simpler Instructions modules. From a lin- 
guistic point of view, a textual module uses only 
a controlled sublanguage. V. Lux thus extended 
FR to a new CL .called.:RREM (.Fr.aa~gais Ra- 
tionalise'. Etendu Modulaire) comprising many 
CLs, each having its own syntactic rules for 
a specific textual module. She also performed 
a corpus study showing that the same textual 
modules could be identified for both French and 
English. It should thus be possible to remodu- 
larize SE similarly to what has been done to 
FR with FREM. In this paper, we therefore 
in t roduce  the: not ion of aii Extei ided Modular 
Controlled Language (EMCL) which first de- 
fines some general rules and then some more 
specific ones for each textual module. We now 
look at the problem of automatical ly generat- 
ing technical documentation complying both to 
structuration orms such as ATA 100 and to the 
rules of an EMCL. 
2 How to  generate  techn ica l  
documentat ion?  
We assume that a generation system can be di- 
vided into a What to say and How to say it 
components, even though this may be consid- 
ered as a gross simplification. 
2.1 What  to say component  
The main difficulty for NLG in a real environ- 
ment lies in knowledge modeling. For aircraft 
maintenance manuals, existing ontologies could 
probably be reused, but even then the model- 
ing efforts required are huge. Nevertheless, we 
assume that it is possible to design forms which 
are sequentially presented to the user to be 
filled, as in Drafter (Paris et al, 1995), through 
which the technical writer provides the infor- 
mation to convey in an appropriate fornlalism. 
These forms can be derived directly fi'om the 
tree-like structure of the document given in the 
ATA norms. The goal is that, once the writer 
has finished filling in these forms, the technical 
docunmntation is already properly structured in 
an abstract language instead of a natural one. 
In a general text generation setting, using forms 
to describe What is to be said might seem like 
a difficult task; but in the context of techni- 
cal writing, the informational content is almost 
already prescribed and forms are thus a sin> 
ple way of complying with the rules of a CL. 
Indeed in the now comlnon web enviromnents, 
forms are frequently used for eliciting informa- 
tion from users. This input can then be pro- 
cessed by the "tIow to say it and layout compo- 
nents. 
142 
The writers who find it very difficult to com- 
ply with the rules of a CL have no problem 
complying with the ATA 100 norms, thereby 
producing documents with the right thematic 
and communicative structuration. This can be 
seen as an illustration of observations made in - 
However, many writing rules in a CL place 
particular syntactic constraints on the use of, 
a given lexical item, e.g. in FR a rule forbids 
the use of emp~cher (prevent) when followed by 
an infinitive clause. To handle such numerous 
lexically dependent syntactic rules, a formal- 
psycholinguistics. 
describes a model of the speaker's activity in 
which choices in the What to say component 
are conscious, while choices in the How to say it 
component are automatic. This model helps un- 
derstand some of the difficulties that CL users 
face. A CL forces the writer to become con- 
scious of behavioral mechanisms that are usu- 
ally automatic; The writer is thus distracted 
from choices made earlier in her/his writing 
task. So s/he often ends up writing it in the  
way  it has  to be written but does not write ex- 
actly what  had to be written, thus defeating 
the whole purpose of a CL which was meant to 
produce a better expression of the information. 
This model also explains why a human writer 
has less difficulties following the ATA norms: 
this part of the job corresponds to conscious 
choices. In the NLG scenario, this is replaced 
by filling in some information in the forms that 
are presented. 
To sum up, the What to say component re- 
quires a modelization of the domain model and 
the design of a series of forms to be filled. A 
human writer using the NLG system has to fill 
forms but on the other hand, s/he does not have 
to learn a CL, since compliance with the CL 
norms is taken care by the How to say it com- 
ponent which we now describe. 
2.2 How to say it component  
In this section, it is assumed that if a CL is 
in fact an EMCL such as FREM, a specific How 
to say it component is designed for each textual 
module, but always retaining the same formal- 
ism. 
The lexicon used in the How to s~zyit corn- . . . .  
ponent should be exactly the one enforced by 
the CL. Similarly, the syntactic constructions 
and the discourse structures of this component 
should correspond to the set of allowed con- 
structions / structures in the CL. This can sim- 
plify some lexical, syntactic and even discourse 
choices to be made within the generation sys- 
tern and thus ensure that .the gener~ed text 
complies with the rules of the CL. 
Levelt (Levelt , 1989, p. 9): ism based on a lexicalized grammax:is needed. 
We chose Lexicalized Tree Adjoining Grammar 
(LTAG) for the following reasons: 
* A text generation formalism inspired from 
LTAG, called G-TAG, has been designed, 
implemented and used in several applica- 
tions (Danlos and Meunier, 1996; Meunier, 
1997; Danlos, 1998; Meunier and Danlos, 
1998; Danlos, 2000). G-TAG takes as in- 
put an event graph which can be provided 
by the user  by filling in some forms which 
ensure that all the necessary information 
for generation is provided. 
o G-TAG deals with textual phenomena such 
as sentence connectors by extending LTAG 
to handle discourse comprised of more than 
one sentence. One of the major innovations 
of FREM compared to FR (and of EMCL 
compared to CL) is to implement rules for 
connecting sentences (clauses). The way to 
connect sentences has largely been ignored 
in CLs, although this linguistic issue raises 
ambiguities which can lead to maintenance 
errors. For example, simple juxtaposition 
of sentences i allowed in FR but disallowed 
in FREM because it is highly dangerous. A 
technician reading Nettoyer X.  Verser Y 
sur X.  (Clean X. Pour Y on X.) could in- 
terpret this to mean either "Clean X with 
Y" or "Clean X with Z, and next pour 
Y on X". Only one of these operations is 
right, the other one may lead to a mainte- 
nance error. On the other hand, traditional 
syntactical ambiguities uch as a preposi- 
.... tional attaehment...will-.not, usually lead to 
maintenance rrors because the technician 
can usually solve them on the basis of some 
domain knowledge. 
o The lexicalized grammar in G-TAG is com- 
piled from the recta-grammar designed and 
implemented by M.H. Candito (Candito. 
1996). This makes it easy to follow the 
evolution ofru les  of an (EM)CL. For ex- 
ample, if the rule to write an Instruction 
143 
changes from "Put a verb in the infini- 
tive" to "Insert an imperative", then this 
must be changed everywhere in the lexi- 
calized grammar. Using the metagrammar 
we can achieve this quite easily because of 
the hierarchical organization of a LTAG: 
with only one rule, an imperative can be 
allowed and an-infinitive ~disallowed ( in a 
main clause) for every verb, whatever its 
argument structure and syntactic onstruc- 
tion. 
G-TAG thus seems a good candidate for pro- 
ducing technical documentation complying with 
the constraints of an (EM)CL. A technical doc- 
umentation generator prototype in the aeronau- 
tical domain is described in Section 4. It is writ- 
ten in Flaubert, an implementation f G-TAG 
(Danlos and Meunier, 1996). The How to say 
it component would have to be completed by 
adding a layout component complying with the 
norms of ATA 100. We should also provide re- 
vision tools to allow the writer to fine tune the 
final text. 
So, automatically generating technical docu- 
mentation seems technically possible provided 
the technical writer is willing to fill forms which 
in principle should be less demanding than 
learning the rules of an (EM)CL. This approach 
also has other advantages, described in the next 
section. 
3 Advantages  o f  automat ic  
generat ion  o f  techn ica l  
documentat ion  
3.1 Mult i l ingual i ty  
One of tile major assets of NLG is its capacity 
to simultaneously generate texts in several lan- 
guages, and to regenerate updates as often as 
necessary, using a single input representation, 
thus ensuring coherence among the generated 
texts. 
Until now, CLs-have .dealt-withr muttitingual- 
ity by means of the translation hypothesis. It 
is for this reason that FR was developed by 
adapting SE, in order to ease the translation 
from French to English. FR authors try to en- 
sure that everything that can also be written 
in FR can be translated into SE. From this 
point of view, the definition of a source CLt, 
depends on the. defini.tion:.of, a tin:get CL2. De- 
velopers of CL1 are more likely to select struc- 
tures which can be easily or even literally trans- 
lated into CL2. What then happens if CLt and 
CL2 are structurally different? This can lead 
to a situation where CL1 imposes a cumber- 
some writing style that contravene conventions 
shared by native speakers of Li, thereby con- 
tradicting CLs' aim of enhancing understand- 
ability. Rules 0f-aii (EM)CL should be elabo- 
rated without such multilingual considerations. 
Their definition should principally pay atten- 
tion to the characteristics of one language, try- 
ing to avoid typical ambiguities. Such criteria 
are difficult enough to deal within a single lan- 
guage without taking translation problems into 
account. 
Now if we consider multilingual generation i
(EM)CLs, we find that there are major benefits 
from the multilingualism odeling proposed by 
NLG. In particular, defining a common repre- 
sentation is possible since the structure of the 
documentation is language independent. Recall 
from section 1 that the thematic structure of the 
documentation in the aeronautical domain must 
reflect the functional decomposition of the air- 
plane and that the same textual modules can be 
identified in many languages. Thus nothing has 
to be changed in the What to say component 
(Section 2.1) going from one language to the 
other. Only the How to say it component (Sec- 
tion 2.2) need be adapted to the target (EM)CL 
which should be monolingually defined. 
3.2 NLCI as an aid for test ing and 
developing a CL 
An NLG system can provide concrete assistance 
for the testing and for tile development of a CL. 
An NLG system that integrates the CL con- 
straints can help discover contradictions in the 
CL definition. As an illustration, a major dif- 
ficulty in CL definition concerns the coherence 
between the lexicon and the writing rules, as il- 
lustrated by (Emorine, 1994) with the following 
example: 
o Emp~cher l'oxyg~ne de s'accumuler (Pre- 
vent the oxygen from accumulating) does 
not conform to a FR lexically depen- 
dent syntactic rule, according to which 
empdcher (prevent) should not be followed 
by an infinitive clause. 
....... ~ .~ Emp~cher I ~uccumulation ' d.'~ozyg~ne ? (Pre- 
vent oxygen accumulat ion) does not con- 
144 
agent object 
U0 O5 
Titled sub-task 
title :Sub-Task  
DISPOSER 
lst-inst 
I 
Precond-Inst 
lst-order pre-cond 
I I 
DISPOSER ENLEVER 
agent object agent object 
I I I I 
Uo 04 Uo Ingo 
2nd-inst 
I 
Simul-Inst 
lst-order 2nd-order 
I I 
EXTRAIRE DEPOSER 
agent object agent object 
I I I I 
U0 O2 u0 Oa 
3rd-inst 
I 
Inst 
I 
lst-order 
I 
DEPOSER 
agent object tool 
I I I 
u0 o5 To 
Figure 1: Event graph given as input to Flaubert. In the prototype, this information is entered in 
textual form. 
form to FR lexicon, according to which the 
verb s'accumuler (accumulate) should be 
used instead of the noun accumulation (ac- 
cumulation) 
Emp~cher que l'oxyg~ne ne s'accumule 
(Prevent that the oxygen accumulates) 
does not conform to the writing rule that 
forbids the use of the subjunctive mode. 
So we come to a dead end if we want to use the " 
verb empdcher (prevent). This problem can be 
detected automatically by the NLG system.and 
an appropriate fix be made in the grammar. 
NLG can be used for checking a CL, which 
is helpful even if the CL is intended for a hu- 
man writer because it may avoid the discovery 
of various cases of incoherence by the writer. If 
tile writers can justify their writing difficulties 
by pointing out inconsistencies in the CL defini- 
tion, they won't be motivated to use what they 
will tend to consider'as an~-abmird invention, by .... 
people who understand nothing about the .job. 
NLG can also help strengthen CLs' claim to 
lead to more homogeneous texts, which is equiv- 
alent to forbidding certain paraphrases. NLG 
precisely deals with paraphrase as, for some in- 
puts, a NLG system will produce several texts. 
In this way, NLG helps identify which para- 
phrases till remain possible in the CL. In prac- 
tice, when an NLG system proposes several 
texts for one input, it raises the question for 
the CL developer: Should a constraint be added 
to the CL definition in order to forbid some of 
these texts ? 
4 P roo f  o f  concept  generator  
The previous ections have argued for the inter- 
est of dealing with CL from all NLG perspec- 
tive which to our knowledge had never been ex- 
amined ill such details. To further pursue, V. 
Lux (Lux, 1998) has developed a proof of con- 
cept generator using Flaubert (Meunier, 1997; 
Meunier-and "Danlos., :1998)" ~o"gee ? ?howthese "
theoretical concerns could be applied in prac- 
145 
Sous-t~che 60-007 
3.1 DEpose du segment d'arr~t (5) 
- Apr~s avoir enlev~ le mastic PR, d~poser le segment d'arr~t (4). 
- Extraire le porte joint (2) et d~poser le joint (3). 
- D@oser le segment d'arr~t (5) g l'aide de l'outillage (Z). 
: ..-~ I~igure~2:~::-Text~gffaera~ed~:by.~-Elu~aber~t.,-from.the~:input of~Figure-1 . . . . . . . . . .  
tice. The generator can produce text for about 
ten subtasks in FREM. These tasks comprise 
from two to eleven instructions, illustrating ten 
different instruction types such as: simple in- 
struction with a goal, simple instruction with a 
condition, complex instruction with simultane- 
ous actions, etc. They involve the use of various 
syntactical constructions uch as infinitive or 
sentential subordinates, nominalisation, nega- 
tion, etc. 
Input to the prototype are event graphs such 
as the one given in Figure 1. The output is a 
well formed French text such as the one in Fig- 
ure 2 which was generated from Figure 1. In 
Lux's prototype, the event graphs were hand 
coded, but now Flaubert has been rewritten 
in CLEF (Meunier, 1999; Meunier and Reyes, 
1999), which has a better graphical input mech- 
anism that would have eased the input process. 
The output text is a sub-task including a ti- 
tle and instructions of different ypes (only the 
first three instructions are given in the Figures) 
to be performed by the same person (e.g. U0). 
FREM defines which connector to use for each 
instruction type (e.g. conjunction et for an in- 
struction with simultaneous actions). 
The generation of noun groups for the ob- 
jects (Oi), ingredients ( Ingi)  and tools (Ti) re- 
lies on a mapping table between these labels 
and their denominations; this was a temporary 
solution for problems outside the scope of the 
prototype. We should have relied on existing 
nomenclatures for tools'andi'ngredients; and on  
the fact that objects are systematically repre- 
sented in drawings associated with various sub- 
tasks e.g. O5, called segment d'arr~t, is labeled 
(5) on the drawing associated with the exam- 
ple above. In a graphical interface nvironment, 
authors would select these objects linked to a 
controlled terminology data base. 
This proof of,concept ,ge~erator .ser-~ed :, well ..... 
our purpose of testing our theoretical ideas but 
unfortunately it could not be evaluated in a re- 
alistic CL text production environment. Our 
sponsors were very interested in the results we 
have produced but changes in their organisation 
made it impossible to carry further investiga- 
tions. We intend to further pursue our research 
and use the new implementation of Flaubert to 
generate controlled language in an other area 
of application while keeping the concept of an 
extended modular CL. 
5 Conc lus ion  
This paper has argued that linguistic norms im- 
posed by CLs should not be considered in iso- 
lation. They are only a part of a set of more 
comprehensive norms on the document struc- 
ture and layout. This insight led us to define a 
notion of textual modules, each with its own 
linguistic norms, and to envisage the genera- 
tion of technical documentation using an ex- 
tended modular controlled language (EMCL). 
Norms for document structure such as ATA100, 
its linguistic characteristics and its layout re- 
quirements may be seen to respectively define 
the text structuring, the linguistic and the lay- 
out components of an NLG system. 
We have also shown that a generation point of 
view can help refine tile definition of an EMCL. 
The EMCL can be defined monolingually, mul- 
tilinguality being obtained through NLG. These 
ideas were tested within a proof of concept ext 
generator, in thedomain  of' aircraftmaintenance 
manuals. 
Acknowledgment  
We thank our former colleagues at Aerospa- 
tiale Research Center and Frdd~ric Meunier 
who implemented Flaubert. We also thank EI- 
-liottqkffacMowitch,-who suggestedmany improve- " " 
ments to the paper. 
146 
References 
AECMA Document PSC-85-16598, 1995. Sim- 
plified English Standard, a guide for the 
preparation of Aircraft Maintenance Doc- 
umentation in the International Aerospace 
Maintenance Language. 
? Bureau de Normatisation :de l~,A~ronaufique 
et de l'Espace (BNAE), Issy-les-Moulineaux, 
1995. Spdcification ATA no 100, traduc- 
tion frangaise. Specification for Manufactur- 
er's Technical Data - ATA Specification 10, 
October. 
M.-H. Candito. 1996. A principle-based hierar- 
chical representation f LTAGs. In Proceed- 
ings of the 16th International Conference on 
Computational Linguistics, pages 194-199, 
Copenhagen. 
CLAW. 1996. Proceedings of the First Interna- 
tional Workshop on Controlled Language Ap- 
plications (CLAW), Leuven. 
L. Danlos and F. Meunier. 1996. G-TAG, 
un formalisme pour la gEnEration de texte : 
presentation et applications industrielles. In 
Actes du colloque Informatique t Langue Na- 
turelle, Nantes. 
L. Danlos. 1998. G-TAG: un formalisme lex- 
icalis~ de gdn~ration de textes inspire de 
TAG. Traitement Automatique des Langues 
- T.A.L., 39(2):4-32. 
L. Danlos, 2000. Tag Grammars, chapter G- 
TAG: A Lexicalized Formalism for Text Gen- 
eration inspired by Tree Adjoining Grammar. 
CSLI. 
M. Emorine. 1994. Projet de recherche sur la 
mod61isation des entr6es verbales du fran~ais 
rationalis6. Technical report, Universit6 de 
Clermont II. 
GIFAS. 1996. Guide du r6dacteur - partie 2: 
Fran~ais rationalis6. Technical report, GI- 
FAS, Paris. 
A. Hartley and C. Paris, 1996. Le tezte 
procddural : langage, action et cognition, 
chapter Une analyse fonctionnelle de textes 
proc6duraux : apport de la g6n6ration au- 
tomatique ~ la d6finition des langues ratio- 
nalis6es, pages 211-222. Toulouse. 
W. Levelt. 1989. Speaking -.h'om intention 
to articulation. MIT Press. Cambridge Mas- 
sachuset ts. 
V. Lux. 1998. Elaboration d'unffangais ratio- 
nalisd dtendu pour un manuel de maintenance 
adronautique, test en gdndration automatique. 
Th~se de doctorat en linguistique, Universitd 
Paris 7. 
F. Meunier and L. Danlos. 1998. FLAUBERT: 
an user-friendly system for multilingual text 
generation. In Proceedings of the 9th Interna- 
tianal. Workshop. on Natural Language Gener- 
ation (INLG'98), pages 284-287, Niagara-on- 
the-Lake. 
F. Meunier and R. Reyes. 1999. Plate-forme de 
ddveloppement de gdn~rateurs multilingues. 
In Actes de la confdrence de Gdndration Au- 
tomatique de Texte CAT'99, pages 145-155, 
Grenoble, France. 
F. Meunier. 1997. Impldmentation de G-TAG, 
formalisme pour la gdndration inspirde des 
grammaires d'arbres adjoints. Th~se de doc- 
torat en informatique, Universitd Paris 7. 
F. Meunier. 1999. Mod~lisation des ressources 
linguistiques d'une application industrielle. 
In TALN'99, pages 243-252, Carg~se, Corse, 
12-17 juillet. 
A. Nasr. 1996. Un module de reformulation au- 
tomatique fondd sur la thdorie Sens-Texte - 
application aux Langues Controldes. Ph.D. 
thesis, Universit~ Paris 7. 
C. Paris, K. Vander Linden, M. Fischer, 
A. Hartley, L. Pemberton, R. Power, and 
D. Scott. 1995. A support tool for writ- 
ing multilingaul instructions. In Proceedings 
of the 14th International Joint Conference 
on Artificial Intelligence (IJCAI'95), pages 
1398-1404, MontrEal. 
J. Scheurs and G. Adriaens, 1992. Comput- 
ers and writing - state of the art, chapter 
From cogram to alcogram : toward a con- 
trolled english grammar checker, pages 206- 
221. Kluwer Academic Publishers, London. 
147 
Discourse dependency structures as constrained DAGs
Laurence Danlos
TALANA/ LATTICE
Universite? Paris 7
Laurence.Danlos@linguist.jussieu.fr
Abstract
I show that the semantic structure for dis-
courses, understood as a dependency represen-
tation, can be mathematically characterized as
DAGs, but these DAGs present heavy structural
constraints. The argumentation is based on a
simple case, i.e. discourses with three clauses
and two discourse connectives. I show that
only four types of DAGs are needed for these
discourses.
1 Introduction
Within a multi-level approach to discourse processing,
this paper focuses on the semantic level. This level re-
flects the discourse structure (how things are said, how
the discourse is rhetorically organized). This structure
plays an important role, e.g., it constrains both anaphora
resolution and the attachment of incoming propositions
in understanding. I assume that the informational content
level (what is said) is based on first order logic.
A nice tool for the semantic level is dependency
graphs. This is what is adopted in RST (rhetorical struc-
tures correspond roughly to dependency structures), but
it is not the case in SDRT1: discourse structures, called
SDRSs, are represented as boxes. Nevertheless, it is easy
to translate the conditions of an SDRS into a dependency
graph (Section 2.1).
Our goal in this paper is to determine to which mathe-
matical object dependency structures for discourses cor-
respond. In RST, it is a basic principle that this object is a
tree. In SDRT, the issue is not discussed. I will show that
this object is an ordered directed acyclic graph (DAG),
1SDRT stands for Segmented Discourse Representation The-
ory (Asher, 1993) (Asher and Lascarides, 2003). It is an ex-
tension of DRT, Discourse Representation Theory (Kamp and
Reyle, 1993). (S)DRS stands for (Segmented) Discourse Repre-
sentation Structure. RST stands for Rhetorical Structure Theory
(Mann and Thompson, 1987).
which may be not tree shaped. Some authors, e.g. (Bate-
man, 1999) and (Blackburn and Gardent, 1998), have al-
ready brought forward discourse structures which are not
tree shaped. However nobody says explicitly that dis-
course dependency structures are DAGs considering seri-
ously all the consequences of this claim2.
Our argumentation is based on one of the simplest
cases of discourses, namely discourses of type S1 Conna
S2 Connb S3 with two discourse connectives (Conna/b)
and three clauses (Si). A discourse connective Conn can
be either a subordinating or coordinating conjunction or
a discourse adverbial. It denotes a discourse relation R,
a predicate with two arguments. I will show (Section 3)
that they are topologically only four types of DAGs for
these discourses. This allows us to state that DAGs for
these discourses are not arbitrary: they satisfy structural
constraints (Section 5). I stipulate that this result can be
extrapolated to discourses in which sentences are sim-
ply juxtaposed without discourse connective. It can also
be foreseen that dependency structures for more complex
discourses (e.g. discourses with more than three clauses)
are also constrained DAGs.
This can be seen as an important result since many au-
thors in the discourse community hang on trees as dis-
course structures, even if it means to use artificial trees
as shown in Section 2.4. They reject DAGs because
they view them as completely unconstrained (except the
acyclicity constraint) and so as unusable in discourse pro-
cessing. This is truly not the case. Semantic dependency
structures for discourses are ordered DAGs but these DAGs
present heavy structural constraints, which can help us
to cut down the number of possibilities when processing
discourses (although this issue is not discussed here).
Before getting to the heart of the matter, let us give
some preliminaries.
2For example, (Blackburn and Gardent, 1998) exhibits an
example the structure of which is a ?re-entrant graph?, see (6c).
However, in (Duchier and Gardent, 2001), the semantic repre-
sentations of discourses are always tree shaped.
2 Preliminaries
2.1 Translation of an SDRS into a DAG
Formally, an SDRS is is a couple of sets ?U,Con?. U is
a set of labels of DRS or SDRS which may be viewed as
?speech act discourse referents?. Con is a set of condi-
tions on labels of the form:
? pi : K, where pi is a label from U and K is a (S)DRS
(labelling);
? R(pii, pij), where pii and pij are labels and R a dis-
course relation (structuring).
The set of conditions can be translated into a depen-
dency graph by applying the following rules.
? A condition R(pii, pij) is translated as a binary tree,
the root of which is R, the ordered leaves are pii and
pij . pii is the first argument of R (it corresponds gen-
erally to the ?nucleus? in RST), pij its second argu-
ment (it corresponds generally to the ?satellite? in
RST).
? A condition pi : K in which K is a SDRS leads to
a sub-graph obtained by translating recursively the
conditions in K, this sub-graph is labeled pi.
? A condition pi : K in whick K is a DRS is simply
translated as pi.
Figures 1 and 2 give examples of this translation mech-
anism.
2.2 Linear order
Subordinate conjunctions (noted as Conj) are the only
discourse connectives which allow us to invert the order
of the sentences: a subordinate clause can be postposed
(the linear order is then the ?canonical? one S1 Conj (,)
S2) or preposed (then the non canonical order is Conj S2,
S1). Following works in MTT3, a trace of the linear or-
der can be recorded in a semantic dependency represen-
tation, however it should not affect its structure. From
this principle, the position of subordinate clauses should
not affect semantic structures. That is to say that S1 Conj
S2 and Conj S2, S1 are both represented as R(pi1, pi2) in
which pii is the semantic representation of Si.
What happens for a sentence with two subordinate
clauses? Establishing the canonical order with only post-
posed subordinate clauses may generate ambiguities: for
example, a sentence X of the type Conja S1, S2 Conjb
S3, with a preposed subordinate clause and a postposed
one, corresponds, in the canonical order, either to Y1 =
S2 Conja S1 Conjb S3 or to Y2 = S2 Conjb S3 Conja S1.
3MTT stands for Meaning to Text Theory, a dependency for-
malism for sentences (Mel?cuk, 2001).
In (Danlos, 2003), I have shown, using LTAG as a syn-
tactic formalism, that X receives two syntactic analyses
which allow us to compute Y1 and Y2. From the principle
that the position of subordinate clauses does not affect se-
mantic structures (see above), X does not yield any other
semantics than Y1 and Y2, i.e. the semantics of X is in-
cluded in the semantics of Y1 and Y2.
As a consequence, our study on the semantics of sen-
tences with two subordinate clauses can be limited to the
study of such sentences in the canonical order. Since sub-
ordinate conjunctions are the only discourse connectives
which allow us to invert the order of the sentences, our
study on the semantics of discourses with three clauses
and two discourse connectives can be limited to dis-
courses which satisfy the linear order S1 Conna S2 Connb
S3.
2.3 Compositionality principle
Let Dn be a DAG with n leaves representing the depen-
dency structure of a discourse Dn. It will be shown that
the following principle is true: if Dp is a sub-graph of
Dn with p leaves, 1 < p < n, then the discourse Dp
corresponding to Dp can be inferred from the discourse
Dn. On the other hand, it will be shown that the converse
principle is not always true, i.e. if a sub-discourse Dp
can be inferred from Dn, it does not always mean that
the graph Dp is a sub-graph of Dn.
2.4 Interpretation of dependency relations in trees
Two different ways can be used to interpret dependency
relations in trees: the standard one used in mathematics
and computer science, and the ?nuclearity principle? put
forward in RST (Marcu, 1996). Let us illustrate them
with the tree in Figure 3. With the standard interpreta-
tion, the first argument (nucleus) of Rc is its left daughter
(the tree rooted at Ra), while with the nuclearity prin-
ciple, it is pi1 (the leaf which is the first argument (nu-
cleus) of Ra). Similarly, with the standard interpretation,
the second argument (satellite) of Ra is its right daughter
(the tree rooted at Rb), while with the nuclearity princi-
ple, it is pi2 (the leaf which is the first argument (nucleus)
of Rb). To put it in a nutshell, the arguments of a dis-
course relation can be intermediary nodes or leaves with
the standard interpretation, while they can only be leaves
with the nuclearity interpretation.
I will show (Section 4) that the standard interpretation
should be adopted. The point I want to make now is that
one could argue that the nuclearity interpretation should
be adopted instead, but one should not feel free to use
both interpretations for the same tree. This is however
what is done by some authors. For example, in (Webber
et al, 2003), the tree in Figure 4 is the discourse structure
associated with (1).
(1) a. Although John is very generous -
b. if you need some money,
c. you only have to ask him for it -
d. he?s very hard to find.
Let us show that some predicate-argument relations are
given by the nuclearity interpretation and other ones by
the standard interpretation in their tree. From (1), (2) can
be inferred. This is evidence that the arguments of the
discourse relation ?concession? in their tree are a and d.
These predicate-argument dependencies are given by the
nuclearity interpretation.
(2) a. Although John is very generous,
d. he?s very hard to find.
From (1), (3) can also be inferred. This is evidence that
the arguments of ?elaboration? in their tree are a and the
tree rooted at ?condition?. These dependencies are given
by the standard interpretation.
(3) a?. John is very generous -
b. if you need some money,
c. you only have to ask him for it.
Nevertheless, one should not feel free to use trees rely-
ing on a mixed interpretation (the standard and nuclearity
ones), except if the conditions governing the use of one
or the other interpretation are formally defined4 . In Sec-
tion 4, I will make an attempt to lay down rules on the
choice of one of these two interpretations according to
the ?coordinating or subordinating? type of discourse re-
lations. However, this enterprise leads to a failure: no
general rule can be laid down. Mixed interpretation for
trees should thus be discarded. As a consequence, one
has to admit that discourse structures are DAGs, for ex-
ample, the DAG in Figure 5 for (1). This DAG is conform
to our compositionality principle: it can be viewed as the
fusion of the dependency graphs for (2) and (3), while
the discourse in (1) can be viewed as the fusion of the
discourses in (2) and (3), with the factorization of John is
very generous which corresponds to the factorization of
?a? in the DAG.
3 DAGs for S1 Conna S2 Connb S3
It is standardly assumed that the arguments of a discourse
relation expressed through a discourse connective are
given by text units5 which are adjacent to the discourse
connective (Mann and Thompson, 1987), (Duchier and
Gardent, 2001). However, there exist counter-examples
to this adjacency principle, see (7) below. So I make
4I thank an anonymous reviewer for drawing my attention
on this point.
5A text unit (noted as U ) is either a clause or, recursively, a
non discontinuous sequence Ui Conn Uj .
a weaker assumption, that I call ?left1-right2 principle?
which states the following: the first (resp. second) ar-
gument of a discourse relation expressed through a dis-
course connective is given by a text unit which occurs
on the left (resp. right) of the discourse connective. This
principle makes sense only for discourses in the canonical
order. Recall (Section 2.2) that our study can be limited
to discourses which satisfy the canonical linear order S1
Conna S2 Connb S3.
A consequence of the left1-right2 principle in dis-
courses of the type S1 Conna S2 Connb S3, is that the
first argument of Ra is compulsorily pi1, the only text unit
which occurs on the left of Conna. On the other hand,
its second argument may vary depending on scope. More
specifically, it may a priori be:
? either the representation of the whole right hand side
of Conna, i.e. the semantic representation of the text
unit S2 Connb S3. I call this case ?wide scope? of
Conna or Ra. It leads to DAG (A) in Figure 66. The
dependency relations in (A), which is tree shaped,
must be interpreted in the standard way: the second
argument of Ra is its right daughter, i.e. the tree
rooted at Rb.
? or the representation of one of the two clauses on the
right of Conna. This case leads either to tree (A1) =
Ra(pi1, pi2) or to tree (A2) = Ra(pi1, pi3).
Similarly, the second argument of Rb is compulsorily
pi3, the only text unit on the right of Connb, but depend-
ing on the scope of Connb, its first argument may a pri-
ori be Ra(pi1, pi2), see (B) in Figure 6, or pi2 in (B1) =
Rb(pi2, pi3) or pi1 in (B2) = Rb(pi1, pi3).
We are now ready to study the combinatory coming
from the fusion of DAGs (Ai) and (Bj). The goal is to
distinguish the DAGs which correspond to coherent dis-
courses S1 Conna S2 Connb S3 from those which do not
(i.e. which cannot be linguistically realized).
A) Graph (A): This graph is linguistically realized in
(4a)7. The wide scope of Conna = because can be seen
in the dialogue in (4b-c) in which the answer is Because
S2 Connb S38. In conformity with our compositionality
principle, (A) includes the sub-graph Rb(pi2, pi3) and S2
Connb S3 can be inferred: if (4a) is true, then it is true
that Fred played tuba while Mary was taking a nap. The
reader will check that the adverbial Conna = therefore in
(4d) has also wide scope.
6In this figure, as well as in other subsequent figures, the
label for the sub-graph is omitted.
7To indicate that it is stressed when spoken, the word while
is written in capital letters in (4).
8When while is not stressed, the question in (4b) may be
given as answer only Because S2. The interpretation of (4a)
corresponds then to DAG (C) in Figure 6.
(4) a. Mary is in a bad mood because Fred played tuba
WHILE she was taking a nap.
b. - Why is Mary in a bad mood?
c. - Because Fred played tuba WHILE she was tak-
ing a nap.
d. Fred wanted to bother Mary. Therefore, he
played tuba WHILE she was taking a nap.
B) Graph (B): This graph is linguistically realized in
(5a). The wide scope of Connb = in order that/to can
be seen in the dialogue in (5b-c) in which the question
is Why S1 Connb S2? In conformity with our composi-
tionality principle, (B) includes the sub-graph Ra(pi1, pi2)
and S1 Connb S2 can be inferred from (5a). The adver-
bial Connb = therefore in (5d) has also wide scope.
(5) a. Fred played tuba WHILE Mary was taking a nap
in order to bother her.9
b. - Why did Fred play tuba WHILE Mary was tak-
ing a nap?
c. - In order to bother her.
d. Fred played tuba WHILE Mary was taking a nap.
Therefore, she is in a bad mood.
C) Graphs (A1) and (B1): The fusion of (A1) and
(B1) leads to DAG (C) in Figure 6. This DAG is not tree
shaped: pi2 has two parents. It is linguistically realized in
(6a), in which S2 is said to be ?factorized? since both S1
Conna S2 = Mary is in a bad mood because her son is ill
and S2 Connb S3 = Her son is ill. Specifically, he has an
attack of bronchitis can be inferred from (6a), which is in
conformity with our compositionality principle since (C)
includes both (A1) = Ra(pi1, pi2) and (B1) = Rb(pi2, pi3).
A similar situation is observed in (6b) and (6c).
(6) a. Mary is in a bad mood because her son is ill.
Specifically, he has an attack of bronchitis.
b. Fred played tuba. Next he prepared a pizza to
please Mary.
c. Fred was in a foul humor because he hadn?t
slept well that night because his electric blanket
hadn?t worked.10
D) Graphs (A1) and (B2): The fusion of (A1) and
(B2) leads to DAG (D) in Figure 6. This DAG is not tree
shaped: pi1 has two parents. It is linguistically realized
in (7a), in which S1 is said to be ?factorized? since both
S1 Conna S2 = Fred prepared a pizza to please Mary and
S1 Connb S3 = Fred prepared a pizza. Next he took a nap
can be inferred, in conformity with our compositionality
principle. A similar situation is observed in (7b) and (7c).
9When while is not stressed, the interpretation of (5a) may
correspond to DAG (D) in Figure 6.
10This discourse is a modified version (including discourse
connectives) of an example taken in (Blackburn and Gardent,
1998). These authors acknowledged that the structure of this
discourse is a ?re-entrant graph?.
(7) a. Fred prepared a pizza to please Mary. Next, he
took a nap.
b. Fred prepared a pizza, while it was raining, be-
fore taking a walk.
c. Fred is ill. More specifically, he has an attack of
bronchitis. Therefore, Mary is in a bad mood.
In discourses analyzed as (D), S3 is linked to S1 (which
is not adjacent) and not to S2 (which is adjacent). There-
fore, these discourses are counter-examples to the adja-
cency principle adopted in RST.
The DAG (D) exhibits crossing dependencies and it
does correspond to coherent discourses. (D) is thus a
counter-example to the stipulation made by (Webber et
al., 2003), namely ?discourse structure itself does not ad-
mit crossing structural dependencies?11.
E) Graphs (A2) and (B1): The fusion of (A2) and
(B1) leads to DAG (E) in Figure 7, in which pi3 has two
parents. I cannot find any discourse corresponding to (E),
i.e. with S3 factorized, although I wrote down all possible
examples I could think of. Laurence Delort, who works
on (French) corpus neither. I cannot prove that something
does not exist, I can just stipulate it. However there is
some evidence, coming from syntax, which supports my
stipulation when Conna and Connb are both subordinat-
ing conjunctions (Conj). Namely, no standard syntactic
analysis of sentences of the type S1 Conja S2 Conjb S3
can lead, in a compositional way, to an interpretation in
which S3 is factorized12. As I see no reason to make a
difference between subordinating conjunctions and other
discourse connectives at the semantic level (see note 11),
I extrapolate this result to other discourse connectives.
F) Graphs (A2) and (B2): The fusion of (A2) and
(B2) leads to DAG (F) in Figure 7. This graph cannot
represent a discourse S1 Conna S2 Connb S3 since it does
not include pi2.
So far, we have examined only cases where a discourse
relation has two arguments. It remains to examine what is
called ?multi satellite or nucleus cases? in RST, in which
a discourse relation is supposed to have more than two
arguments.
G) Graphs (A1), (A2) and (B2): The fusion of (A1),
(A2) and (B2) leads to DAG (G) in Figure 7. This DAG
could be said to be linguistically realized in (8a): since
11Among discourse connectives, (Webber et al, 2003) dis-
tinguish ?structural connectives? (e.g. subordinating conjunc-
tions) from discourse adverbials including then, also, otherwise,
. . . . They argue that discourse adverbials do admit crossing of
predicate-argument dependencies, while structural connectives
do not. I don?t make any distinction between discourse connec-
tives at the semantic level, but I emphasize that (7b) comprises
only structural connectives (subordinating conjunctions) and its
structure exhibits crossing structural dependencies.
12Recall that I feel entitled to make this claim because I have
studied in detail the syntactic analyses of sentences of the type
S1 Conja S2 Conjb S3 in (Danlos, 2003).
both S1 Conna S2 and S1 Conna S3 can be inferred from
(8a), one may be willing to lay down both Ra(pi1, pi2) and
Ra(pi1, pi3), i.e. to consider (8a) as a multi-satellite case
with Ra = Elaboration. Rb = Narration links pi2 and pi3.
The following question arises: is Rb in a dependency rela-
tion with Ra? It is hard to give an answer for (8a). How-
ever the answer seems positive for (8b), which could also
be analyzed as a multi-satellite case with Ra = Explana-
tion. Rb = Joint links pi2 and pi3. This leads to DAG (G?)
in Figure 7. However, consider (8c) which differs from
(8b) only by the use of or instead of and. Graphs (G)
or (G?) would not do justice to (8c): neither Ra(pi1, pi2)
nor Ra(pi1, pi3) can be laid down. (8c) can only be rep-
resented as DAG (A) with Ra = Explanation and Rb =
Disjunction.
(8) a. Guy experienced a lovely evening last night.
More specifically, he had a fantastic meal. Next
he won a dancing competition.13
b. Mary is in a bad mood because she had?nt slept
well and it is raining.
c. Mary is in a bad mood because she had?nt slept
well or it is raining.
It seems clear that (8b) and (8c) should be represented
at the semantic level as the very same graph. This graph
can only be (A), which is the only possibility for (8c). For
the sake of homogeneity and compatibility with SDRT,
(8a) should also be represented as (A)14. Recall more-
over that (4a) with wide scope of Conna is also repre-
sented as (A). All in all, (A) happens to be a semantic
structure which is shared by discourses whose informa-
tional content shows quite different relations between the
eventualities at stake. Is it a problem? I would say no, be-
cause, from (A), semantic to content rules, based on the
values of Ra and Rb, can make the difference: they can
compute the following (simplified) logical forms, which
show that the discourses in (8) and (4a) do not have the
same type of informational content as far as the relations
between eventualities are concerned, althoug they share
the same (dependency) semantic structure:
? for (8a) with Ra = Elaboration and Rb = Narration:
e1 ? e2 ? e3 ? precede(e2, e3)
? subevent(e1, e2) ? subevent(e1, e3)
? for (8b) with Ra = Explanation and Rb = Joint: e1 ?
e2 ? e3 ? cause(e1, and(e2, e3))
? e1 ? e2 ? e3 ? cause(e1, e2) ? cause(e1, e3)
13This discourse is a modified version (including discourse
connectives) of an example taken in (Asher and Lascarides,
2003).
14The (A) analysis is the translation of the SDRS proposed
by (Asher and Lascarides, 2003) for (8a), namely the SDRS in
Figure 1 with Ra = Elaboration and Rb = Narration. pi1 is con-
sidered as the ?topic? (common theme) for pi2 and pi3.
? for (8c) with Ra = Explanation and Rb = Disjunc-
tion: e1 ? e2 ? e3 ? cause(e1, or(e2, e3))
? e1 ? e2 ? e3 ? (cause(e1, e2) ? cause(e1, e3))
? for (4a) with Ra = Explanation and Rb = Circum-
stances: e1 ? e2 ? e3 ? overlap(e2, e3)
? cause(e1, overlap(e2, e3))
We have touched here a crucial question in discourse
processing (within a multi-level approach): to what ex-
tent should the semantic (dependency) level (how things
are said) echo the informational content level (what is
said)? I don?t pretend to give a general answer to this
fundamental question. However we have seen that the
same semantic dependency structure (or SDRS) can lead
to quite different informational contents according to the
values of the discourse relations at stake. What is called
multi-satellite case in RST, e.g. (8a) or (8b), leads to a
logical form in which the same eventuality variable, here
e1, occurs conjunctively multi-times as the argument of
the same predicate, e.g. preda(e1, e2) ? preda(e1, e3)
with preda = subevent in (8a) and preda = cause in
(8b). It is unnecessary to represent such a case at the se-
mantic level trough a predicate - a discourse relation -
with more than two arguments. The multi-satelitte anal-
ysis in RST comes from the following principle: if a sub-
discourse Dp can be inferred from a discouse Dn, with
1 < p < n, then the graph Dp must be a sub-graph of
Dn. This principle is simply wrong. On the other hand,
the converse implication is true.
H) Graphs (A1), (B1) and (B2): The fusion of (A1),
(B1) and (B2) leads to a DAG which could be said to be
linguistically realized in (9). This discourse allows us to
infer both S1 Connb S3 and S2 Connb S3. So it would be
classified as a multi-nucleus case in RST. However, by
the same argumentation as previously, it should be repre-
sented as (B).
(9) Fred washed the dishes and Guy cleaned up the bath-
room, while Mary was taking a nap.
I) Graphs (A1), (A2) and (B2): The fusion of these
graphs lead to DAG (I) in Figure 8. I cannot find any
example corresponding to this DAG.
J) Graphs (A2), (B1) and (B2): Along the same lines,
the fusion of these graphs lead to a DAG for which I can-
not find any instance.
No other fusion of graphs (Ai) and (Bj) leads to a DAG
which corresponds to a coherent discourse. So we have
arrived at the following result:
The dependency structure of a discourse S1
Conna S2 Connb S3 is one of the four DAGs
(A), (B), (C) and (D). (A) and (B), which are
tree shaped, cover wide scope cases (and multi
satellite or nucleus cases in RST). (C) and (D),
which are not tree shaped, cover multi parent
cases (factorization of a sentence). (D) exhibits
crossing dependencies.
Before commenting on this result, let us come back to
the interpretation of dependency relations in trees.
4 Interpretation of dependency relations in
trees (concluding episode)
First, let us underline the following point. Interpreting
tree shaped graphs (A) and (B) with the nuclearity princi-
ple amounts to interpreting (A) as (C), and (B) as (D)15.
But then, cases with wide scope are not taken into ac-
count, which is unacceptable. Therefore, the standard in-
terpretation of dependency relations in a tree is needed.
Next, the following question arises: is it possible to
state that the dependency relations in a tree should be
computed sometimes by the standard interpretation and
some other times by the nuclearity one? In the tree (B),
this question is instantiated in the following way: should
the first argument of Rb be given sometimes by the stan-
dard interpretation (it is then the tree rooted at Ra) and
some other times by the nuclearity principle (it is then
pi1, and (B) is equivalent to (D))16? An answer to this
question is sound only if it is possible to define formally
?sometimes?. The only sound answer consists in stat-
ing that there exist two types of discourse relations: the
dependency relations are computed with the standard in-
terpretation for the first type, and computed with the nu-
clearity interpretation for the second one. The only types
of discourse relations which have been put forward up
to now are the ?coordinating and subordinating? types
(Hobbs, 1979), (Asher and Lascarides, 2003), (Asher and
Vieu, 2003). Laurence Delort in (Delort, 2004) has ex-
amined, in the framework of SDRT, my DAGs (A)-(D) in
studying for each relation Ra or Rb if it could be of the
coordinating and/or subordinating type. Her results are
summarized in Table 1. This table shows that (B) is pos-
sible only when Ra is coordinating and (D) only when
Ra is subordinating (in both cases, Rb can be equally co-
ordinating or subordinating). Therefore, it is possible to
lay down the following rule: the dependency relations in
the tree (B) are computed with the standard interpretation
when Ra is coordinating, and with the nuclearity inter-
pretation when Ra is subordinating.
However, let us examine the situation for the tree (A).
From Table 1, the reader can check that no rule can be
laid down for the dependency relations in (A) when Rb is
coordinating: they can be computed with either the stan-
dard or the nuclearity interpretation. These two cases are
15With the nuclearity principle, the second argument of Ra in
(A) is pi2, and the first argument of Rb in (B) is pi1.
16For the other dependency relations in (B), both interpreta-
tions give the same result.
illustrated in (10) with Ra = Contrast and Rb = Narration:
(10a) should be analyzed with the standard interpretation
of (A) with wide scope of Conna, while (10b) should be
analyzed with the nuclearity interpretation of (A), i.e. as
(C) with S2 factorized.
(10) a. Fred has made no domestic chore this morning.
However, this afternoon, he wed up the dishes.
Next he ran the vacuum cleaner.
b. Fred has made no domestic chore this morn-
ing. However, this afternoon, he washed up the
dishes. Next he went to see a movie.
In conclusion, a mixed interpretation for trees must
be discarded: the coordinating or subordinating type of
discourse relations does not allow us to choose between
the standard and nuclearity interpretations. As a conse-
quence, since the standard interpretation is needed for
wide scope cases, the nuclearity principle should be dis-
carded.
5 Analysis of the result and conclusion
The result I arrived at does not take into account the dis-
course connectives / relations at stake. However, for a
given pair of connectives, it may happen that only some
of the DAGs among (A)-(D) are observed. For example,
if Conna is an adverbial and Connb a subordinate con-
junction, then (B) with wide scope of Rb should be ex-
cluded. On the top of part of speech considerations, the
lexical value of each connective may exclude some of
these DAGs. Finally, the distinction between coordinat-
ing and subordinating discourse relations must be taken
into account. Table 1 from (Delort, 2004) presented as in
Table 2 shows that a given DAG among (A)-(D) never cor-
responds to the 2?2 = 4 possibilities given by the combi-
natory Ra/b coordinating or subordinating discourse rela-
tion.
To put it in a nutshell, there is a maximum of four or-
dered DAGs representing the semantic structures of dis-
courses S1 Conna S2 Connb S3. I stipulate that this result
can be extrapolated to cases where sentences are simply
juxtaposed without discourse connective.
It can be considered that there is only a few DAGs cor-
responding to coherent discourses with three clauses17.
First, recall that the left1-right2 principle (Section 3) dis-
cards right away a number of DAGs, for example (K) in
Figure 8 (in (K), Ra is not the mother of pi1). Secondly,
among the DAGs which satisfy the left1-right2 principle,
some are not instantiated, e.g. (E), and also (F). A look
17In RST, there are only 2 trees (2 is the number of binary
trees with 3 leaves), namely trees (A) and (B), which are sup-
posed to be interpreted with the nuclearity principle (being so
interpreted as (B) and (D) respectively). We have seen that this
is too restrictive: wide scope cases are not taken into account.
on the topology of the ordered DAGs (A)-(D) allows us
to bring forward this other structural constraint: Ra must
?left-dominate? pi2. The definition of left-dominance in
a tree is the following (Danlos, 2003): a node X left-
dominates a node Y iff Y is a daughter of X (immediate
dominance) or there exists a daughter Z of X such that
Y belongs to the left-frontier of the tree rooted at Z. For
example, Ra left-dominates pi1, Rb and pi2 in (A), while
Rb left-dominates Ra, pi1 and pi3 in (B)18.
Let us here examine the consequences of this left-
dominance constraint in non formal terms. Ra must be
the mother of pi1 and must left-dominate pi2. This means
that Ra establishes some semantic link between S1 and
S219. This result may sound trivial on psycho-linguisitics
grounds: what would be a discourse in which the second
clause is not linked at all to the first one?20 It has the
following consequence: the semantic representation of a
discourse with four clauses and three discourse connec-
tives cannot be DAG (L) in Figure 8. In (L), Ra does not
left-dominate pi2, or informally, there is no link between
S1 and S2. (L) includes two crossing dependencies.
I have just half-opened the door towards an extension
of this study to discourses with more than three clauses.
I stipulate that the conclusion of this forthcoming study
will be the same. Namely, semantic dependency struc-
tures for discourses are ordered DAGs which satisfy heavy
structural constraints, which can help us to cut down the
number of possibilities when processing discourses.
Acknowledgements
I want to thank Laura Kallmeyer for her many valuable
comments.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge.
Nicholas Asher and Laure Vieu. 2003. Subordinating
and coordinating discourse relations. Lingua. forth-
coming.
18Left-dominance is a notion more restrictive than domi-
nance (e.g. Ra dominates pi1, Rb, pi2 and also pi3 in (A)) and
less restrictive than the nuclearity principle (e.g. by this princi-
ple, Ra dominates only the leaves pi1, and pi2 in (A)).
19This result can be seen as a weaker version of the adjacency
principle for the first two clauses of a discourse.
20The link between the first two clauses can be given by a
third sentence, as in (11) in which S3 establishes a joint link
between S1 and S2 through its plural subject. The DAG for (11)
is (B) with Ra = Joint and Rb = Comment.
(11) It is raining. Ted arrived late. These two facts irritated
Mary.
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer, Dordrecht.
John Bateman. 1999. The dynamics of ?surfacing?: an
initial exploration. In Proceedings of International
Workshop on Levels of Representations in Discourse
(LORID?99), pages 127?133, Edinburgh.
Patrick Blackburn and Claire Gardent. 1998. A specifi-
cation language for discourse semantics. In Proceed-
ings of LACL?98, pages 61?67, Grenoble.
Laurence Danlos. 2003. Repre?sentation se?mantique
sous-spe?cifie?e pour les conjonctions de subordination.
In Actes de TALN 2003, pages 44?54, Batz-sur-Mer,
France.
Laurence Delort. 2004. Relations subordonnantes et
coordonnantes pour la de?sambigu??sation du discours.
In Proceedings of International Workshop on SDRT,
TALN?04, Fe`s, Maroc.
Denis Duchier and Claire Gardent. 2001. Tree descrip-
tions, contraints and incrementality. In R. Muskens
H. Bunt and E. Thijsse, editors, Computing Meaning,
pages 205?227. Kluwer Academic Publishers, Dor-
drecht.
Jerry Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, (3):67?90.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic Publishers, Dordrecht.
William Mann and Sandra Thompson. 1987. Rhetorical
structure theory: Description and construction of texts
structures. In G. Kempen, editor, Natural Language
Generation, pages 85?95. Martinus Nijhoff Publisher,
Dordrecht.
Daniel Marcu. 1996. Building up rhetorical structure
trees. In The Proceedings of the 13th National Confer-
ence on Artificial Intelligence, pages 1069?1074, Port-
land.
Igor Mel?cuk. 2001. Communicative Organization
in Natural Language: The Semantic-Communicative
Structure of Sentences. John Benjamins Publishing
Company, Amsterdam.
B. Webber, A. Joshi, M. Stone, and A. Knott. 2003.
Anaphora and discourse structure. Computational Lin-
guistics, 44:1?45.
Ra Ra Rb Rb
coor. sub. coor. sub.
(A) + + + -
(B) + - + +
(C) + - + +
- + - +
(D) - + + +
Table 1
PPPPPPPRa
Rb
coordinating subordinating
coordinating (A), (B), (C) (B), (C)
subordinating (A), (D) (C), (D)
Table 2
pi1, pi0
pi1 :
e1
pi0 :
pi2, pi3
pi2 :
e2
pi3 :
e3
Rb(pi2, pi3)
Ra(pi1, pi0)
Ra
Rbpi1
pi2 pi3 pi0
Figure 1: Translation of an SDRS into a DAG
pi1, pi??
pi1 :
e1
pi?? :
pi2, pi5, pi?
pi2 :
e2
pi5 :
e5
pi? :
pi3, pi4
pi3 :
e3
pi4 :
e4
Narration(pi3, pi4)
Narration(pi2, pi5)
Elaboration(pi2, pi?)
Elaboration(pi1, pi??)
(12) 1 Max experienced a lovely evening last
night.
2 He had a fantastic meal.
3 He ate salmon.
4 He devoured lots of cheese.
5 He won a dancing competition.
Elaboration
Narration
Elab.
Narration
pi1
pi2 pi5
pi3 pi4 pi?
pi??
Figure 2: Translation of the SDRS for (12) into a DAG
- (12) and its SDRS are taken from (Asher and Lascarides, 2003) -
Ra
N
Rc
S
pi1
N
S
pi2
pi4
Rb
N
S
pi3
concession [although]
a
b
d
condition [if]
c
elaboration
concession [although]
d
elaboration
a
b
condition [if]
c
Figure 3: Binary tree Figure 4: Artificial tree for (1) Figure 5: DAG for (1)
Ra
Rb
pi1
pi2
pi3
Ra
Rb
pi1 pi2
pi3 Ra
Rb
pi1 pi2 pi3
Ra
Rb
pi1 pi2 pi3
(A) (B) (C) (D)
Figure 6: DAGs (A), (B), (C) and (D)
Ra
Rb
pi1 pi2 pi3
Ra
Rb
pi1 pi3
Ra
Rb
pi1 pi2 pi3
Ra
Rb
pi1 pi2 pi3
(E) (F) (G) (G?)
Figure 7: DAGs (E), (F), (G) and (G?)
Ra
Rb
pi1 pi2 pi3
Ra
Rb
pi1 pi2 pi3
Ra
Rb
pi1 pi2 pi3 pi4
(I) (K) (L)
Figure 8: DAGs (I), (K) and (L)
Document Structuring ? la SDRT
Laurence Danlos
LATTICE ? LORIA
U. Paris 7, Case 7003
2, pl. Jussieu
75251 Paris Cedex 05
France
danlos@linguist.jussieu.fr
Bertrand Gaiffe
LORIA
Campus scientifique
BP239
54506 Vand?uvre Cedex
France
gaiffe@loria.fr
Laurent Roussarie
LATTICE
U. Paris 7, Case 7003
2, pl. Jussieu
75251 Paris Cedex 05
France
roussari@linguist.jussieu.fr
Abstract
In this paper, the issue of document
structuring is addressed. To achieve this
task, we advocate that Segmented Dis-
course Representation Theory (SDRT)
is a most expressive discourse frame-
work. Then we sketch a discourse plan-
ning mechanism which aims at pro-
ducing as many paraphrastic document
structures as possible from a set of fac-
tual data encoded into a logical form.
1 Introduction
Using the terms of (Reiter and Dale, 2000), we
consider that the Document Planner architecture
is pipelined: first the content determination task
does its work, and then the document structuring
task takes the result and build a document plan.
Following the work of (Roussarie, 2000), we
adopt SDRT (Asher, 1993; Asher and Lascarides,
1998), which was designed first for text under-
standing, for the document structuring task1.
The input to the document structuring compo-
nent is a set of factual data encoded into a logical
form, as in (1).
(1)  	

 ?leave 	 ?fit-of-
tears

cause

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 130?134,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
French TimeBank: An ISO-TimeML Annotated Reference Corpus
Andre? Bittar
Alpage
Univ. Paris Diderot
andre.bittar@linguist.jussieu.fr
Pascal Amsili
LLF
Univ. Paris Diderot
amsili@linguist.jussieu.fr
Pascal Denis
Alpage
INRIA
pascal.denis@inria.fr
Laurence Danlos
Alpage
Univ. Paris Diderot
danlos@linguist.jussieu.fr
Abstract
This article presents the main points in the cre-
ation of the French TimeBank (Bittar, 2010),
a reference corpus annotated according to the
ISO-TimeML standard for temporal annota-
tion. A number of improvements were made
to the markup language to deal with linguistic
phenomena not yet covered by ISO-TimeML,
including cross-language modifications and
others specific to French. An automatic pre-
annotation system was used to speed up the
annotation process. A preliminary evaluation
of the methodology adopted for this project
yields positive results in terms of data quality
and annotation time.
1 Introduction
The processing of temporal information (events,
time expressions and relations between these enti-
ties) is essential for overall comprehension of nat-
ural language discourse. Determining the temporal
structure of a text can bring added value to numer-
ous NLP applications (information extraction, Q&A
systems, summarization...). Progress has been made
in recent years in the processing of temporal data,
notably through the ISO-TimeML standard (ISO,
2008) and the creation of the TimeBank 1.2 cor-
pus (Pustejovsky et al 2006) for English. Here we
present the French TimeBank (FTiB), a corpus for
French annotated in ISO-TimeML. We also present
the methodology adopted for the creation of this re-
source, which may be generalized to other annota-
tion tasks. We evaluate the effects of our methodol-
ogy on the quality of the corpus and the time taken
in the task.
2 ISO-TimeML
ISO-TimeML (ISO, 2008) is a surface-based lan-
guage for the marking of events (<EVENT> tag) and
temporal expressions (<TIMEX3>), as well as the
realization of the temporal (<TLINK>), aspectual
(<ALINK>) and modal subordination (<SLINK>)
relations that exist among these entities. The tags?
attributes capture semantic and grammatical features
such as event class, tense, aspect and modality, and
the type and normalized interpretative value of tem-
poral expressions. The <SIGNAL> tag is used to an-
notate relation markers, such as before and after. A
set of resources for English has been developed over
the years, including an annotated corpus, TimeBank
1.2 (TB1.2)1, which has become a reference for tem-
poral annotation in English.
3 Improving ISO-TimeML
We propose a number of improvements to ISO-
TimeML to deal with as yet untreated phenom-
ena. These include both cross-language annotation
guidelines, as well as guidelines specific to French.
All these guidelines are implemented in the FTiB.
Cross-language Improvements : ISO-TimeML
currently provides for the annotation of event
modality by capturing the lemma of a modal on
a subordinated event tag in the modality at-
tribute. Inspired by the fact that in French, modal-
ity is expressed by fully inflected verbs, we pro-
pose that those verbs be tagged as modal, and we
1Annotated according to the TimeML 1.2 specification, as
opposed to the more recent ISO-TimeML.
130
provide a set of normalized values for the modal-
ity attribute, within a manual annotation context,
that reflect the classic classes of linguistic modality
(Palmer, 1986): NECESSITY and POSSIBILITY
(epistemic), OBLIGATION and PERMISSION (de-
ontic). We also provide a way of capturing the dif-
ference between support verb constructions with
a neutral aspectual value (mener une attaque (carry
out an attack)) and those with an inchoative as-
pectual value (lancer une attaque (launch an at-
tack)). ISO-TimeML encodes the relation between
the verb and its nominal argument via a <TLINK>
of type IDENTITY. We encode aspectual variants
in the FTiB by using an <ALINK>. A signifi-
cant proportion (13/36) of the annotated <ALINK>
tags in the FTiB (36%) are used in this case. A
third improvement we propose is the introduction of
the event class EVENT CONTAINER2 to distinguish
predicates that take an event nominal as subject.
In TB1.2, these predicates were sometimes marked,
but not distinguished from the OCCURRENCE class.
The distinction is appropriate as these predicates
have events as arguments, unlike OCCURRENCEs.
The relative frequency of this class (19 occurrences)
compared to the standard PERCEPTION class (10)
also justifies its use. Although not yet dealt with
in ISO-TimeML, aspectual periphrases, such as
en train de + Vinf (akin to the English progres-
sive -ing), adding an aspectual value to an event,
are captured in the FTiB in the aspect attribute
for events. We also propose a new value for as-
pect, PROSPECTIVE, encoding the value of the
construction aller + Vinf (going to + Vinf ), as in
le soleil va exploser (the sun is going to explode).
Improvements for French : a correspondence had
to be made between the ISO-TimeML schema and
the grammatical tense system of French, in particu-
lar, to account for tenses such as the passe? compose?
(PAST tense value, as opposed to the present per-
fect used in English) and imparfait (IMPERFECT,
not present in English as a morphological tense).
French modal verbs behave differently to English
modal auxiliaries as they can be conjugated in all
tenses, fall within the scope of aspectual, negative
polarity and other modal operators. Unlike in TB1.2,
2After the terminology of (Vendler, 1967)
modal verbs (and adjectives), are marked <EVENT>
in FTiB and have the class MODAL. 72 events (3.4%)
are annotated with this class in the FTiB.
4 Methodology
Text sampling : the source texts for the FTiB were
selected from the Est Re?publicain corpus of journal-
istic texts.3 The journalistic genre was chosen for
its relatively high frequency of events and temporal
expressions. Texts were sampled from 7 different
sub-genres4, the distributions of which are shown in
Table 1. Certain sub-genres appear in higher pro-
portions than others, for two main reasons. Firstly,
to favor comparison with TB1.2 (which is made up
of news articles). Secondly, because the news gen-
res are relatively diverse in style compared to the
other sub-genres, which follow a certain format (e.g.
obituaries). We present some of the correlations be-
tween sub-genre and linguistic content in Section 5.
Sub-genre Doc # Doc % Token # Token %
Annmt. 22 20.2% 1 679 10.4%
Bio. 1 0.9% 186 1.1%
Intl. news 32 29.4% 5 171 31.9%
Loc. news 19 17.5% 4 370 27.0%
Natl. news 25 22.9% 3 347 20.7%
Obituary 2 1.8% 313 1.9%
Sport 8 7.3% 1 142 7.0%
Total 109 100% 16 208 100%
Table 1: Proportions of sub-genres in the FTiB.
Automatic pre-annotation : To speed up the an-
notation process, we carried out an automatic pre-
annotation of markables (events, temporal expres-
sions and some relation markers), followed by man-
ual correction. Relations were annotated entirely by
hand, as this task remains very difficult to automate.
Below we describe the two modules developed for
pre-annotation.
The TempEx Tagger marks temporal expressions
<TIMEX3> and sets the tag?s attributes, and anno-
tates certain <SIGNAL> tags. This module con-
sists of a set of Unitex (Paumier, 2008) transduc-
ers that are applied to raw text. We adapted and
3Available at http://www.cnrtl.fr.
4These are announcement, biography, international news,
local news, national news, obituary and sport.
131
EVENT 
correction
Adjudication Adjudication
Coherence 
check
TIMEX3 
correction
SIGNAL 
correction
Pre-
annotated 
text
Annotated 
Markables
Annotated 
Markables + 
LINKs
Gold 
Standard
LINK 
annotation
Figure 1: Schema of the annotation strategy.
enriched a pre-existing set of transducers for anno-
tating temporal expressions in French (Gross, 2002)
for our purposes. Marked expressions are classified
according to their ISO-TimeML type5 and the val-
ues of certain attributes are calculated. The value
attribute is only set during normalization, carried out
after the detection phase. A script calculates normal-
ized values for marked expressions, including index-
icals, such as lundi dernier (last Monday) or l?anne?e
prochaine (next year) (with the article?s publication
date as reference point). A comparative evaluation
with the DEDO system of (Parent et al 2008) shows
very similar performance (for exact match on tag
span and for the value attribute) over the same
evaluation corpus (Table 2).
System Prec. Rec. F-sc.
Match TempEx 84.2 81.8 83.0
DEDO 83.0 79.0 81.0
Value TempEx 55.0 44.9 49.4
DEDO 56.0 45.0 50.0
Table 2: Comparative evaluation of the TempEx Tagger
for exact match on tag span and value calculation.
The Event Tagger marks up events (<EVENT> tag)
and certain relation markers through the application
of a sequence of rules acting on the local chunk con-
text. The rules eliminate unlikely candidates or tag
appropriate ones, based on detailed lexical resources
and various contextual criteria. Input is a text pre-
processed with POS tags, morphological analysis
and chunking (carried out with the Macaon process-
5DATE (e.g. 15/01/2001, le 15 janvier 1010, jeudi, demain),
TIME (ex. 15h30, midi), DURATION (ex. trois jours, un an) ou
SET (ex. tous les jours, chaque mardi)
ing pipeline (Nasr et al 2010)). A reliable com-
parison with the DEDO system, to our knowledge
the only other system for this task in French, was
unfortunately not possible. Evaluations were made
on different, yet comparable, corpora, so results are
merely indicative. For event tagging, our system
scored a precision of 62.5 (62.5 for DEDO), recall
of 89.4 (77.7) and an F-score of 75.8 (69.3). There
is room for improvement, although the system still
yields significant gains in total annotation time and
quality. An experiment to evaluate the effects of the
pre-annotation showed a near halving of annotation
time compared to manual annotation, as well as a
significant reduction of human errors (Bittar, 2010).
Unfortunately, it was not possible to reliably com-
pare the performance of the Event Tagger with the
similar module by (Parent et al 2008) (DEDO), to
our knowledge the only other system developed for
this task for French. Evaluations of each system
were carried out on different, although similar, cor-
pora. Thus, results remain merely indicative. For the
task of event recognition, our system scored a preci-
sion of 62.5 (62.5 for DEDO), recall of 89.4 (77.7)
and an F-score of 75.8 (69.3).
Manual annotation and validation : after pre-
annotation of markables, texts were corrected by 3
human annotators (2 per text), using the Callisto6
and Tango7 tools, designed for this task. Figure 1
shows the process undergone by each document.
The final step of the process is a coherence check
of the temporal graph in each document, carried out
6http://callisto.mitre.org/
7http://timeml.org/site/tango/tool.html
132
via application of Allen?s algorithm (Allen, 1983)
and graph saturation (Tannier & Muller, 2008). Us-
ing the same method, we found 18 incoherent graphs
among the 183 files of the TB1.2 corpus for English.
At this stage, the corpus contained 8 incoherencies,
which were all eliminated by hand. Manually elim-
inating incoherencies is an arduous task, and per-
forming an online coherence check during annota-
tion of relations would be extremely useful in a man-
ual annotation tool. All files were validated against
a DTD, provided with the corpus.
5 French TimeBank
Our aim for the FTiB is to provide a corpus of
comparable size to TB1.2 (approx. 61 000 to-
kens). Version 1.0 of FTiB, presented here and
made available online8 in January 2011, represents
about 14 of the target tokens. Figure 2 shows that
proportions of annotated elements for French are
mostly very similar to those in TB1.2. This sug-
gests the annotation guidelines were applied in a
similar way in both corpora and that, for the journal-
istic genre, the distributions of the various marked
elements are similar in French and English. By far
the most common relation type in the French corpus
is the <TLINK>. Among these, 1 175 are marked
between two event arguments (EVENT-EVENT),
722 between an event and a temporal expression
(EVENT-TIMEX3), and 486 between two temporal
expressions (TIMEX3-TIMEX3).
Figure 2: Annotated content of the FTiB and TB1.2.
Inter-annotator agreement was measured over the
entire FTiB corpus and compared with reported
agreement for TB1.2.9. F-scores for agreement
8Via the INRIA GForge at https://gforge.inria.
fr/projects/fr-timebank/.
9Available at http://www.timeml.org/site/
timebank/documentation-1.2.html Note that fig-
'$7( 7,0( '85$7,21 6(7
*HQUH
7
,
0
(
;


7
\
S
H


Figure 3: Distribution of <TIMEX3> types by sub-genre.
&
O
D
V
V


,B67$7(
$63(&78$/
,B$&7,21
3(5&(371
67$7(
02'$/
&$86(
2&&855(1&(
5(3257,1*
(&217$,1(5
ELRLQWOORFDO
QDWORELWVSRUW
DQQ
Figure 4: Distribution of <EVENT> classes by sub-genre.
are significantly higher for the French corpus on
<EVENT> and <TIMEX3> tag spans than for
TB1.2, and very slightly lower for <SIGNAL>. Fig-
ures for tag attributes are higher for TB1.2, as a
much looser metric10 was used for agreement, so
comparison is not yet possible. The same measure
will need to be implemented to afford an accurate
comparison.
ures were only calculated for a small subset of the entire
corpus, unlike for the FTiB, for which all data was used.
10Agreement for TB1.2 was only calculated over tags with
matching spans and wrong attributes on non-matching spans
were not penalized. For the FTiB, all tags were considered and
all attributes for non-matching tag spans were penalized.
133
Corpus
<TIMEX3> <EVENT> <SIGNAL>
Span Attr Span Attr Span
FTiB .89 .86 .86 .85 .75
TB 1.2 .83 (.95) .78 (.95) .77
Table 3: Inter-annotator agreement (F-scores).
Sub-genre and linguistic content : a preliminary
study showed correlations between the various sub-
genres chosen for the corpus and the annotations
in the texts. For example, Figure 3 shows a high
proportion of TIMEs in announcement texts (46%
of the corpus total)11, while DURATIONs are in-
frequent (2%), but appear in higher proportions in
news (21?32%) and sports (13,5%). DATEs are by
far the most frequently marked (80%), with SETs
being the least. In Figure 4, the preponderance of
the OCCURRENCE class is obvious (62.1% of all
events). REPORTING is most frequent in local and
international news. Announcements stand out yet
again, with the highest number and highest propor-
tion of the class EVENT CONTAINER. These ini-
tial observations argue in favor of text sampling to
achieve a diversity of temporal information in a cor-
pus and suggest such features may prove useful in
text classification.
6 Conclusion
Our experiences show ISO-TimeML is a stable lan-
guage and, with some modification, is applicable
to French. The FTiB is a valuable resource that
will surely stimulate development and evaluation of
French temporal processing systems, providing es-
sential data for training machine learning systems.
An initial survey of the data suggests temporal in-
formation may be useful for text classification. Our
methodology is time-efficient and ensures data qual-
ity and usability (coherence). It could be adopted to
create temporally annotated corpora for other lan-
guages as well as being adapted and generalized to
other annotation tasks.
11This is particularly significant given the low proportion of
the total corpus tokens in this sub-genre.
References
ISO 2008. ISO DIS 24617-1: 2008 Language Resource
Management - Semantic Annotation Framework - Part 1:
Time and Events. International Organization for Stan-
dardization, Geneva, Switzerland.
Andre? Bittar 2010. Building a TimeBank for French:
a Reference Corpus Annotated According to the ISO-
TimeML Standard.. PhD thesis. Universite? Paris Diderot,
Paris, France.
Andre? Bittar 2009. Annotation of Temporal Informa-
tion in French Texts.. Computational Linguistics in the
Netherlands (CLIN 19).
Se?bastien Paumier 2008. Unitex 2.0 User Manual..
Universite? Paris Est Marne-la-Valle?e, Marne-la-Valle?e,
France.
Gabriel Parent, Michel Gagnon and Philippe Muller
2008. Annotation d?expressions temporelles et
d?e?ve?nements en franc?ais. Actes de TALN 2008.
Avignon, France.
Alexis Nasr, Fre?de?ric Be?chet and Jean-Franc?ois Rey
2010. MACAON : Une cha??ne linguistique pour le trait-
meent de graphes de mots. Actes de TALN 2010. Mon-
treal, Canada.
James F. Allen. 1983. Maintaining Knowledge About
Temporal Intervals. Communications of the ACM. 26:11
832-843.
Xavier Tannier and Philippe Muller 2008. Evalua-
tion Metrics for Automatic Temporal Annotation of Texts.
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08) Marrakech, Mo-
rocco.
Frank Robert Palmer 1986. Mood and Modality Cam-
bridge University Press Cambridge, UK.
James Pustejovsky, Marc Verhagen, Roser Saur??, Jes-
sica Littman, Robert Gaizauskas, Graham Katz, Inderjeet
Mani, Robert Knippen and Andrea Setzer 2006. Time-
Bank 1.2 Linguistic Data Consortium
Nabil Hathout, Fiammetta Namer and Georgette Dal
2002. An Experimental Constructional Database: The
MorTAL Project Many Morphologies 178?209 Paul
Boucher ed. Somerville, Mass., USA
Zeno Vendler 1967 Linguistics and Philosophy Cornell
University Press Ithaca, NY, USA
Maurice Gross 2002 Les de?terminants nume?raux, un
exemple : les dates horaires Langages 145 Larousse
Paris, France
134
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 1?9,
Gothenburg, Sweden, April 26, 2014.
c?2014 Association for Computational Linguistics
Because We Say So
Julie Hunter
Alpage
Universit?e Paris Diderot/INRIA
juliehunter@gmail.com
Laurence Danlos
Alpage
Universit?e Paris Diderot/INRIA
Laurence.Danlos@inria.fr
Abstract
In this paper, we show that contingency
connectives, which mark causal and con-
ditional relations (PDTB Group, 2008), re-
strict the possible interpretations of reports
in their scope in a way that many other
connectives, such as contrastive connec-
tives, do not. We argue that this result
has immediate implications for the seman-
tics of causal relations and for the anno-
tation of implicit connectives. In particu-
lar, it shows that the assumption, implicit
in some work on NLP, that the semantics
of explicit connectives can be translated to
implicit connectives is not anodyne.
1 Introduction
In addition to their standard intensional use, many
embedding verbs have a semantically parentheti-
cal use (Urmson, 1952; Simons, 2007), in which
the content of the embedded clause conveys the
main point of the report. Semantically parentheti-
cal uses can occur even when the report is not syn-
tactically parenthetical, as shown in (1) and (2). In
these examples, the embedded clause he is out of
town (labeled ???) conveys the main point because
its content offers an explanation of Fred?s absence.
(1) - [Why didn?t Fred come to my party?]
?
- Jane said [he is out of town.]
?
(2) [Fred didn?t come to my party.]
?
Jane said
[he is out of town.]
?
If the matrix clause does not contribute directly
to the explanation of Fred?s absence in (1) and
(2), it is arguable that only the content of the ?-
clauses contributes to the second argument of the
explanatory relations that hold in these examples.
In terms of Segmented Discourse Representation
Theory (SDRT) (Asher and Lascarides, 2003), for
example, the relation QUESTION-ANSWER-PAIR
in (1) should be taken to hold only between ? and
?; the content of the matrix clause should be like-
wise excluded from the second argument of EX-
PLANATION in (2) (Hunter et al., 2006). Similarly,
the Penn Discourse Treebank (PDTB) would relate
only ? and ? in (2) with implicit because (Dinesh
et al., 2005; Prasad et al., 2006).
Given this analysis of (1) and (2), however, it
is puzzling why the report in (3) cannot be under-
stood parenthetically. On the surface, (2) and (3)
differ only in that the two sentences in (2) have
been connected with the subordinating conjunc-
tion because in (3). Yet this seemingly harmless
change leads to a dramatic change in interpretive
possibilities.
(3) (#)
1
Fred didn?t come to my party because
Jane said he is out of town.
And as we?ll see in ?2, the contrast between (2)
and (3), heretofore unnoticed in the literature, can
be replicated for all contingency relations: all con-
tingency connectives exhibit a distaste for seman-
tically parenthetical reports.
The contrast between (2) and (3) is surprising
for a further reason, namely that many relations
and connectives that do not indicate causality do
appear to accept the embedded clauses of seman-
tically parenthetical reports as arguments.
(4) Lots of people are coming to my party. Jane
said (for example) that Fred is coming with
his whole family.
(5) Fred is coming to my party, although Jane
told me that Bill is not.
The report in (4) is understood parenthetically; it
is the content of the embedded clause, not the ma-
trix clause, that serves as a specific example of the
1
We use the symbol ?(#)? to mark examples containing re-
ports that cannot be interpreted parenthetically; ?(#)? does not
exclude the possibility of a non-parenthetical interpretation.
1
claim made in the first sentence. Unlike in (3),
this parenthetical reading is felicitous even when
for example is explicit. (5) shows that semanti-
cally parenthetical reports can occur in contrastive
relations, as the contrast intuitively holds between
Fred?s coming to the party and Bill?s not coming.
It also shows, given that although is a subordinat-
ing conjunction, that a parenthetical reading of (3)
is not blocked simply by the fact that because is a
subordinating conjunction.
The contrast between (2) and (3), as well as that
between (3) and (4)/(5), has direct implications for
the annotation of reports and the semantics of con-
tingency relations. In ?2, we argue for the follow-
ing generalization:
(C) if a contingency relation is marked by an ex-
plicit connective that has syntactic scope over
the matrix clause of a report, this report can-
not have a parenthetical interpretation.
With general support for (C) in place, ?3 returns
to the contrast, illustrated by (2) and (3), between
examples of EXPLANATION with implicit and ex-
plicit connectives. We argue that this contrast
raises problems for existing discourse theories and
annotation practices. ?4 discusses causal connec-
tives that have a temporal sense, e.g. after, which
appear to be counterexamples to (C). We show that
this problem is only superficial.
In what follows, we will use the term par-
enthetical to talk only about semantically paren-
thetical uses, unless otherwise stated. We will
also adopt the notation conventions of the PDTB
(PDTB Group, 2008). Each discourse connec-
tive has two arguments, Arg1 and Arg2. The text
whose interpretation is the basis for Arg1 appears
in italics, while the text that serves as the basis for
Arg2 appears in bold. If the connective is explicit,
it is underlined. An example is given in (6):
(6) Fred didn?t come to the party because he is
out of town.
Sections 2 and 3, like the current section, will
focus exclusively on data in English, though the
claims made about the data in these sections hold
for the French translations of the data as well. In
section 4, we will discuss a point on which the
data in English and French diverge in an interest-
ing way. In all cases, the examples that we use to
motivate our analysis are constructed for the sake
of simplicity. Nevertheless, our claims for English
are supported by data from the PDTB and The New
York Times, as we discuss in more detail in ?5.
2 Contingency relations
In the PDTB, the class of contingency relations
includes causal relations (EXPLANATION and RE-
SULT in SDRT) and their pragmatic counterparts
(EXPLANATION* and RESULT*), as well as se-
mantic and pragmatic conditional relations. To
this we add relations of purpose or GOAL, marked
by connectives such as so that and in order to. For
simplicity, we will adopt the vocabulary of SDRT
when talking about discourse relations, e.g. us-
ing EXPLANATION when the PDTB would talk of
?reason?, etc.
In section 2.1, we argue that EXPLANATION
and RESULT support (C). Section 2.2 introduces
an apparent counterexample to this claim but then
shows that this example can easily be explained
within the confines of (C). In section 2.3, we show
that EXPLANATION* and RESULT* pattern with
their semantic counterparts with regard to paren-
thetical reports, and section 2.4 rounds out the dis-
cussion of contingency connectives by showing
that CONDITION and GOAL support (C) as well.
2.1 Semantic explanations and results
EXPLANATION is lexically marked by the con-
junctions because, since, after, when, now that, as
and for; there are no adverbials that lexicalize this
relation. Since, like because, supports (C).
(7) a. Fred can?t come to my party since he?s
out of town.
b. (#) Fred can?t come to my party since Jane
said he?s out of town.
The remaining causal conjunctions follow suit, but
due to particularities that arise from their temporal
nature, we delay our discussion of them until ?4.
RESULT is lexicalized only by adverbial con-
nectives: therefore, hence, consequently, as a re-
sult, so, . . . . and these connectives appear to pat-
tern with markers of EXPLANATION with regard
to (C). In other words, if the matrix clause falls in
the syntactic scope of the adverbial, it falls in the
discourse scope of the adverbial as well.
Demonstrating that (C) holds for RESULT ad-
verbials requires care, because adverbials, unlike
conjunctions, can move around. Consider (8):
(8) Fred didn?t go to the party. (H,)
1
Jane said
(,H,)
2
that Luc (, H,)
3
did (, H)
4
.
2
However could be inserted in one of any of the
four locations marked with ?H? above to make the
example felicitous. Yet to test whether however
allows parenthetical readings of reports in its syn-
tactic scope, only position 2 matters. Even when
however is in position 1, syntactic scope over the
matrix clause is not ensured, as the placement of
the adverbial could be the result of extraction from
the embedded clause (Kroch and Joshi, 1987; Pol-
lard and Sag, 1994).
Once we restrict our attention to adverbials in
position 2, we can see more clearly that some al-
low parenthetical readings of reports in their syn-
tactic scope while others do not. A parenthetical
reading of the report in (8) is permitted with how-
ever in position 2. By contrast, the placement of
afterwards in the matrix clause of (9) blocks a par-
enthetical reading.
(9) Fred went to Dax for Christmas. Jane said
afterwards that he went to Pau.
To the extent that (9) is felicitous, the second sen-
tence cannot be rephrased as Jane said that he
went to Pau afterwards (although this would be a
possible rephrasing of the example if afterwards
were in position 1, 3 or 4). The more natural read-
ing is a non-parenthetical one according to which
the time at which Jane made her statement was af-
ter the time at which Fred went to Dax.
Thus we can distinguish two groups of adver-
bials: (i) adverbs that when they have syntactic
scope over the matrix clause of a report do not
allow parenthetical readings of that report, e.g. af-
terwards, and (ii) adverbs that, given the same syn-
tactic configuration, do allow a parenthetical read-
ing of the report, e.g. however. We can then extend
these groups to discourse connectives in general,
including conjunctions. In these terms, because
falls in group (i), because it conforms to (C), and
although, in group (ii).
With the foregoing discussion of adverbials in
mind, we return now to RESULT and the question
of whether RESULT adverbials fall in group (i) or
group (ii). Consider (10):
(10) a. Fred drank too much last night.
Therefore, he has a hangover today.
b. Fred drank too much last night, Jane
said/thinks, therefore, that he has a
hangover today.
A parenthetical reading of the report in (10b)
would be one in which the content of the matrix
clause does not contribute to the second argument
of RESULT. In the case of (2), we said that the act
of Jane?s saying that Fred is out of town in no way
explains Fred?s absence?only the content of what
she said matters. Yet a parallel analysis is not ob-
viously correct for (10b) (which is why we have
included the matrix clause of the report in Arg2
above). While if Jane is right, it is true that Fred?s
hangover is the result of his over zealous drinking,
it is also reasonable to say that Jane?s conclusions
are the result of Fred?s drinking too much: it was
his drinking that prompted her to say or think what
she does. We conclude that therefore falls in group
(i) and, more generally, that RESULT supports (C).
2.2 A clarification
Before moving on to pragmatic causal relations,
let?s take a closer look at examples of EXPLANA-
TION in which the source of an indirect speech re-
port in the scope of because is also the agent of
the eventuality described in Arg1. At first glance,
such cases might appear to be counterexamples to
(C), because the report in the syntactic scope of
because does not provide a literal explanation of
the eventuality described in Arg1.
(11) Jane didn?t hire Bill because she said he
didn?t give a good interview.
It is presumably not the case that Jane did not hire
Bill because she said he didn?t interview well, but
rather because she thought that he didn?t do well.
Yet in (11), the author is not even weakly com-
mitted to the claim that Bill?s interview perfor-
mance is responsible for his not being hired, so the
report cannot have a parenthetical interpretation
(thus we have placed the matrix clause in bold-
face above). And if the report is non-parenthetical,
then (11) is not problematic; because readily al-
lows non-parenthetical readings of reports in its
syntactic scope, as illustrated in (12a) and (12b).
(12) a. Jane didn?t hire Bill because she thought
he didn?t give a good interview.
b. Jane didn?t hire Bill because her secre-
tary said/thought that Bill didn?t give a
good interview.
The only feature that sets (11) off from the mun-
dane examples in (12) is the fact that Jane?s act
of saying what she did does not provide a literal
explanation for her hiring decision. We think that
the use of an indirect speech report is permitted de-
spite this fact only because Jane is both the agent
3
of Arg1 and the source of the report in Arg2. The
assumed close tie between an agent?s thoughts and
actions, together with the semantics of because,
allow us to conclude in (11) that Jane thought Bill
didn?t do well?the real explanation proffered for
her hiring decision.
Interestingly, despite the non-parenthetical
reading of the report in (11), this example can be
reformulated with a syntactic parenthetical:
(13) Jane didn?t hire Bill because, she said, he
didn?t give a good interview.
This is interesting because normally a syntactic
parenthetical construction would be taken to en-
tail a semantically parenthetical construction. Yet
we do not think that the speaker is required to ac-
cept the content of Jane?s report in (13) any more
than she is in (11). The use of the syntactic par-
enthetical appears rather to distance the speaker?s
point of view from Jane?s. But as we argued for
the phenomenon illustrated in (11), we think that
the non-parenthetical interpretation of the syntac-
tically parenthetical report in (13) is made possible
only by the fact that the agent of Arg1 is the source
of the report in Arg2 of EXPLANATION.
2.3 Pragmatic explanations and results
Pragmatic result, or RESULT* in SDRT, holds be-
tween two clauses ? and ? when ? provides justi-
fication for the author?s affirmation of ?. In other
words, RESULT*(Arg1, Arg2) if and only if RE-
SULT(Arg1, affirm(author, Arg2)). In examples
(14a-c), Arg1 does not provide an explanation of
the conclusion drawn in Arg2 (the accumulation
of newspapers did not cause the neighbors to be
out of town), but rather of why the speaker or Jane
formed the belief that the conclusion holds. (14b)
and (14c) are examples of RESULT because they
make this causal relation explicit with I think or
Jane said/thinks. (14a), an example of RESULT*,
leaves this connection implicit. (In order to visu-
ally signal the presence of a pragmatic relation in
the examples in this section, we mark the corre-
sponding connectives with a ?*?.)
(14) a. The newspapers are piling up on the
neighbors? stoop. Therefore
?
, they must
be out of town.
b. The newspapers are piling up on the
neighbors? stoop. I think, therefore, that
they must be out of town.
c. The newspapers are piling up on the
neighbors? stoop. Jane said/thinks,
therefore, that they must be out of town.
Reports in examples like (14b) and (14c) cannot
be read parenthetically, and the nature of RESULT*
prevents its second argument from ever being a
clause embedded by a parenthetically used verb.
EXPLANATION* reverses the order of explana-
tion from RESULT*, i.e. EXPLANATION*(Arg1,
Arg2) = EXPLANATION(affirm(author, Arg1),
Arg2). EXPLANATION* is marked by connec-
tives such as since, because, and for, which need
not be explicit, hence the parentheses in (15).
(15a) and (15c) are examples of EXPLANATION*,
while (15b) and (15d), which explicitly evoke the
speaker?s belief state for Arg1, are examples of
EXPLANATION.
2
(15) a. The neighbors must be out of town
(because
?
) newspapers are piling up on
their stoop.
b. I think that the neighbors must be out of
town because newspapers are piling up
on their stoop.
c. The neighbors must be out of town
(because
?
) Jane said that newspapers
are piling up on their stoop.
d. I think that the neighbors must be out of
town because Jane said that newspapers
are piling up on their stoop.
In both (15c) and (15d), the matrix clause Jane
said contributes to Arg2, i.e. the reports are not
parenthetical. These examples are not like (2) be-
cause the fact that the evidence comes from Jane
is crucial in the formation of the speaker?s belief
that the neighbors are out of town in (15c,d) in a
way that it is not crucial to Fred?s absence in (2).
In all three examples, there is a reasoning process
involved in which Jane figures, but the reasoning
process is not the main point of (2) in the way that
it is for (15c) and (15d).
In ?3 we will provide a further reason why (15c)
should not be considered parenthetical. This ar-
gument, together with those given in this section,
in turn supports our claim that connectives that
mark causal relations are members of group (i) of
discourse connectives, regardless of whether they
2
We assume that for Jane to sincerely say that P, Jane must
believe P; it might be more accurate to talk about Jane?s com-
mitments rather than her beliefs, but that detail is not impor-
tant here.
4
mark semantic or pragmatic relations. That is,
these connectives conform to (C).
2.4 Other contingency relations
A quick review of the remaining contingency rela-
tions shows that principle (C) is obeyed through-
out this class. GOAL can be lexically marked by
the subordinating conjunctions in order that and
so that; semantic conditional relations are gener-
ally marked by the conjunction if. In all cases,
principle (C) is respected because the reports in
examples like (16b) and (17b) cannot be under-
stood parenthetically.
(16) a. Fred made a pizza last night so that Mary
would be happy.
b. * Fred made a pizza last night so that Jane
said/thinks that Mary would be happy.
(17) a. Fred will play tennis if Mary doesn?t
show up.
b. (#) Fred will play tennis if Jane
said/thinks that Mary won?t show
up.
3 Commitment and veridicality
Now that we have shown that contingency rela-
tions support (C), we return to the contrast be-
tween (2) and (3) and discuss the problems that
this contrast raises for existing theories of dis-
course and annotation.
In (15c) note that while the verb say could be re-
placed by, for example, noticed or told me, it can-
not be replaced by believe or thinks.
(18) # The neighbors must be out of town because
Jane thinks that newspapers are piling up on
their stoop.
(18) can be repaired, however, by weakening the
modal in Arg1 from must to might:
(19) The neighbors might be out of town
(because) Jane thinks that newspapers are
piling up on their stoop.
This follows from the semantics of EXPLANA-
TION*, which holds when Arg2 is presented as the
reason for drawing the conclusion given in Arg1.
The speaker is not entitled to draw a stronger con-
clusion than her evidence allows. The use of thinks
in (18) implies that Jane is not fully committed
to the claim that newspapers are piling up on the
neighbor?s doorstep, so the speaker is only entitled
to affirm a possibility claim like that in Arg1 of
(19). Thus (18) is infelicitous for the same reason
that (20) is not an example of EXPLANATION*:
Jane?s saying what she did does not justify the con-
clusion that the neighbors are out of town (Danlos
and Rambow, 2011).
(20) The neighbors must be out of town. Jane said
that newspapers are piling up on their stoop,
but that?s not why I think they?re gone.
In contrast to (18), (2) is felicitous with thinks:
(21) Fred didn?t come to my party. Jane thinks
he?s out of town.
In (21), the author?s commitment to Fred?s ab-
sence is allowed to be higher than Jane?s com-
mitment to his being out of town. This is be-
cause Jane?s saying what she did is not presented
as the justification of the author?s belief that Fred
wasn?t at the party. The author has other reasons
for thinking and saying that Fred was not at his
party; now he?s exploring reasons for Fred?s ab-
sence. Thus the contrast between (18) and (21)
provides further support for our claim in ?2.3 that
the report in (15c) is not parenthetical; the seman-
tics of the report in (15c) affect the acceptability
of the example.
The foregoing discussion of parenthetical re-
ports has implications for the veridicality of dis-
course relations. In SDRT, which provides a the-
ory not only of discourse structure but also of the
interpretation of that structure, EXPLANATION and
RESULT, along with their pragmatic counterparts,
are veridical relations, where a relation R is veridi-
cal just in case if R(?, ?) is true at a world w,
then ? and ? are true at w as well. In the case
of causal relations, for it to be true that one even-
tuality caused another, it must be the case that both
eventualities truly occurred. In this paper, we have
limited our study of parenthetical reports to the
right argument (Arg2) of discourse relations. Ac-
cordingly, we will limit our discussion of veridi-
cality to right-veridicality.
From the data that we have so far, it is clear that
EXPLANATION* is right veridical: if Arg2 isn?t
true, it cannot justify Arg1. Even in the case of
(15c), while what Jane said can be false, it must
be true that Jane said what she said. Likewise,
the data that we have discussed for RESULT, RE-
SULT*, GOAL and conditional relations indicate
that these relations are also right-veridical.
5
The question is more complicated for EXPLA-
NATION. A speaker who asserts (2) or (21) and
offers Jane?s comment as an explanation is not
fully committed to Fred?s being out of town. This
is clear in (21), where the verb think indicates a
hedged commitment. Thus, if we analyze the re-
ports in (2) and (21) as parentheticals, then right
veridicality is not ensured for EXPLANATION, at
least when unmarked by an explicit connective.
When EXPLANATION is explicitly marked with
because, since, or for, right veridicality appears
to be ensured by the fact that these conjunctions
block parenthetical readings of reports in their
syntactic scope. Yet (3), repeated as (22a), is
greatly improved if we use a syntactic parentheti-
cal, which suggests that its infelicity has more to
do with syntax than with veridicality:
(22) a. (#) Fred didn?t come to my party because
Jane said he is out of town.
b. Fred didn?t come to my party because,
Jane said, he is out of town.
However, note that said in (22b) cannot be re-
placed with a weaker embedding verb like thinks:
(23) # Fred didn?t come to my party because, Jane
thinks, he is out of town.
This shows that even though a syntactic parenthet-
ical is used in (22b), the speaker must be fully
committed to the content of Arg2, i.e. right veridi-
cality is ensured for EXPLANATION when it is ex-
plicitly marked with because.
We have seen that EXPLANATION is right
veridical when explicitly marked, but that (2) does
not require the veridicality of the clause labeled
???. This difference forces us to make a choice.
We can maintain the claim that (2) is neverthe-
less an example of EXPLANATION; in this case,
we must adjust the semantics of EXPLANATION
accordingly and conclude that veridicality is a re-
quirement imposed by connectives, not relations.
Alternatively, we can maintain that EXPLANATION
is always (right) veridical; in this case, we must
give up the claim that (2) is an example of EX-
PLANATION.
We suspect that the second choice is better.
There is, after all, no connective that can be in-
serted between the sentences in (2) in such a way
that the meaning is preserved, which suggests that
a deep semantic difference is at play between (2)
and examples of EXPLANATION. Either way, how-
ever, existing theories of discourse structure will
need to be adjusted to account for our observations
on contingency relations and parenthetical reports.
For example, if (2) is not a genuine example of
EXPLANATION, SDRT needs to offer a viable al-
ternative relation. On the other hand, if (2) is a
genuine example of EXPLANATION, SDRT needs
to adjust the notion of veridicality in the semantics
of this relation and indeed, of any other suppos-
edly veridical discourse relations that allow their
Arg2 to be the embedded clause of a parenthetical
report.
Our observations also raise questions about the
semantic implications of the choice made in the
PDTB to insert an implicit connective in the ab-
sence of an explicit one. While this choice was a
practical one meant to facilitate the annotation task
for the PDTB, it has been taken to further levels in
other work on NLP, and we think this is danger-
ous from a semantic point of view. While NLP
systems designed to identify discourse relations in
the presence of explicit connectors have yielded
very positive results (f-scores over 90% for guess-
ing one of the four major PDTB sense classes, i.e.
Temporal, Contingency, Comparison and Expan-
sion (Pitler and Nenkova, 2009)), the task of iden-
tifying discourse relations that hold between spans
of text has proven very difficult in the absence
of explicit connectives. To handle the latter type
of case, systems have been designed that use the
deletion of explicit connectives, whose semantics
are known, to obtain examples with implicit con-
nectives that inherit the semantics of their explicit
counterparts in an effort to create new data that
can be exploited in the identification of implicit
relations (Marcu and Echihabi, 2002). In the other
direction, systems have been built to predict im-
plicit discourse connectives between two textual
units with the use of a language model (Zhou et
al., 2010).
In both kinds of systems, deleting an explicit
connective or adding an implicit connective is con-
sidered a harmless move, though this practice has
been questioned by (Sporleder and Lascarides,
2008). The data presented in this paper show that
the presence or absence of a discourse connective
may drastically change the data when reports of
saying or attitudes occur in the second argument
of a discourse relation ? positing an implicit be-
cause in (2) is not an anodyne move from a seman-
tic point of view.
6
4 Temporal relations
While afterwards falls in group (i) of discourse
connectives, because it does not allow parentheti-
cal readings of reports in its scope, as shown in (9),
other temporal markers appear to fall in group (ii).
Consider, for example, after and before in (24a)
and (24b), respectively.
(24) a. Fred arrived at the scene
?
after [police
say]
?
[the crime occurred.]
?
b. Fred had tried to improve his life
?
before
[police say]
?
[he robbed a bank.]
?
Both (24a) and (24b) have a reading according to
which the temporal relation indicated by the un-
derlined conjunction holds between the clauses ?
and ? rather than ? and ?, which suggests that the
reports are parenthetical. The fact that the relation
between ? and ? can be independent of the tem-
poral constraints of the connective is clearest in
(24a) in which the time of ? can actually be after
the time of ?.
The possibility that temporal connectives allow
parenthetical readings of reports in their scope is
potentially problematic for our arguments in ?2
because some temporal connectives, such as after,
now that, as and when, can have a causal sense in
addition to their temporal sense. And when they
do, parenthetical reports still appear to be possi-
ble, as shown in (25):
(25) Fred was arrested
?
after [police say]
?
[he
pulled a gun on an officer.]
?
In (25), we understand the arrest as a result of
Fred?s pulling a gun on an officer, so after has a
causal sense. Nevertheless, the time of ? can come
after the time of ?, thus suggesting a parenthetical
report.
Interestingly, the data on after and before in En-
glish are not supported cross-linguistically. Up
to example (24), all of the data that we have dis-
cussed are felicitous in French if and only if they
are felicitous in English,
3
but this is not so for (24)
and (25), whose French counterparts are syntacti-
cally ill-formed.
(26) a. * Fred est arriv?e sur les lieux apr`es que la
police dit/dise que le crime a eu lieu.
b. * Fred a essay?e d?am?eliorer sa vie avant
que la police dise qu?il a cambriol?e une
banque.
3
Some of the data presented in this paper are discussed
for French in (Danlos, 2013).
c. * Fred a ?et?e arr?et?e apr`es que la police
dit/dise qu?il a point?e un pistolet sur un
policier.
The parenthetical reading of the report in (25) is
greatly aided by the use of the present tense on
say, which excludes the possibility that the matrix
clause introduces an eventuality that held before
Fred was arrested. For whatever reason, the use of
the present and/or present subjunctive in similar
environments is not allowed in French, as shown
in (26). This difference could be taken two ways.
Perhaps after does violate (C) after all and the only
reason that parenthetical readings are blocked in
(26) is because French syntax does not allow this
reading to be brought out. On the other hand, it
could be that after does support (C), but that police
say in (25) is not functioning as a standard matrix
clause.
Evidence for the second option, which is con-
sistent with (C), comes from the fact that all of the
examples that we have found like (25) come from
newspapers and involve a matrix clause like po-
lice say (parents say, teachers say, ...) and can be
paraphrased using allegedly instead of police say:
(27) Fred was arrested after he allegedly pulled
a gun on an officer.
Parenthetical readings do not appear to be possible
for reports in which the matrix clause cannot be
paraphrased with allegedly, as shown in (28):
(28) (?) Fred revised his negative opinion of Paris
after Jane says/said he had a wonderful visit
there last summer.
If the result in (25) does not generalize to standard
reports like that in (28), it is unlikely that the inter-
pretation of the report in (25) should be explained
in terms of the causal nature of after; it is far more
likely to be due to an idiosyncracy of the matrix
clause police say.
In any case, a full discussion of examples like
(25) is not directly relevant to the discussion of
causality in this paper. For the temporal connec-
tives that can have a causal sense (after, now that,
when, as, and their French counterparts), it is the
case in both French and English that when they
have a causal + temporal sense, their interpretative
possibilities match those in which these connec-
tives have a purely temporal sense. This fact, com-
bined with the fact that these connectives rarely if
ever have a purely causal sense, tells us that their
7
temporal nature is more fundamental. So (25) is
not a direct challenge to the arguments that we
have made in this paper about causal relations and
parenthetical reports.
Let?s return to (C):
(C) if a contingency relation is marked by an ex-
plicit connective that has syntactic scope over
the matrix clause of a report, this report can-
not have a parenthetical interpretation.
We conclude that this generalization holds for all
contingency relations and markers with a purely
causal or otherwise contingent sense. We further-
more predict that if there are examples in which ei-
ther after, now that, when or as has a purely causal
interpretation, in none of these examples will we
find a parenthetical reading of a report in the con-
nective?s syntactic scope.
5 Conclusion
In this paper, we have examined the interaction be-
tween contingency connectives and the interpreta-
tion of reports that fall in their syntactic scope. We
have shown that contrary to certain other types of
connectives, such as contrastive connectives like
although and however, contingency connectives
restrict the interpretations of reports in their scope
so that these reports must be interpreted non-
parenthetically. That is, contingency connectives
support (C). We argued that this result has immedi-
ate implications for theories of discourse structure
and annotation. In particular, SDRT must either
adjust the semantics of EXPLANATION to include
examples like (2), which are not right-veridical, or
introduce a new relation to handle (2). And the
assumption that one can move between implicit
and explicit connectors?an assumption made for
practical reasons in the PDTB but taken to further
extremes in other work on NLP described in ?3?
is not semantically innocent.
Throughout this paper, we have used con-
structed examples to simplify the discussion.
However, data from the PDTB provide support for
our claims in the sense that it provides no coun-
terexamples to (C) with because or since. We
found only 6 results for a search of the PDTB with
the following criteria: explicit relation + (connec-
tor = because) + (Arg2 Source = Other). Our aim
was to find examples in which a report is in the
syntactic scope of because. Of the 6 examples
that we found, two involved continuations of di-
rect quotations and so did not have an explicit ma-
trix clause, while the 4 remaining examples were
of the sort discussed in ?2.2, where the agent of
Arg1 is the source of the report in Arg2. Nor did
we find any counterexamples with an equivalent
search for since (0 results for an equivalent search
with explicit since).
A separate search of the PDTB revealed no vio-
lations of (C) for examples in which now that, as,
and when have purely causal interpretations. That
is, for all examples in the PDTB in which now that,
as, and when are explicit and have a causal sense,
and in which ?Arg2 Source = Other? holds, these
connectors have a temporal sense as well. (There
are no examples in the PDTB in which after has
a purely causal sense). While a thorough study of
temporal connectives is needed to fully understand
the behavior of these conjunctions, as explained in
?4, these data provide strong prima facie support
for the claims made in ?4.
In future work we would like to extend our
study of contingency connectives, starting with
temporal connectives, to see how far (C) can be
generalized to other kinds of relations. We also
hope to back up our results for English and French
with more cross-linguistic research. In the mean-
time, data on contingency connectives in French
and English offer clear support for (C).
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press, Cam-
bridge.
Laurence Danlos and Owen Rambow. 2011. Discourse
Relations and Propositional Attitudes. In Proceed-
ings of the Constraints in Discourse Workshop (CID
2011), Agay, France.
Laurence Danlos. 2013. Connecteurs de dis-
cours adverbiaux: Probl`emes `a l?interface
syntaxe-s?emantique. Linguisticae Investigationes,
36(2):261?275.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, and Aravind Joshi. 2005. Attribution and
the (non-)alignment of syntactic and discourse argu-
ments of connectives. In Proceedings of ACL Work-
shop on Frontiers in Corpus Annotation, Ann Arbor,
MI, USA.
Julie Hunter, Nicholas Asher, Brian Reese, and Pascal
Denis. 2006. Evidentiality and intensionality: Two
uses of reportative constructions in discourse. In
Proceedings of the Constraints in Discourse Work-
shop (CID 2006), Maynoth, Ireland.
8
Anthony Kroch and Aravind Joshi. 1987. Analyzing
extraposition in a tree adjoining grammar. Syntax
and Semantics, 20:107?149.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. Proceedings of the ACL 2002 Conference,
pages 368?375.
PDTB Group. 2008. The Penn Discourse Treebank
2.0 annotation manual. Technical report, Institute
for Research in Cognitive Science, University of
Philadelphia.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. CSLI Publications, Stanford.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi, and Bonnie Webber. 2006. Attribution and
its annotation in the Penn Discourse Treebank. Re-
vue TAL, 47(2).
Mandy Simons. 2007. Observations on embedding
verbs, evidentiality, and presupposition. Lingua,
117(6):1034?1056.
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: A critical assessment. Natural Lan-
guage Engineering, 14(3):369?416.
James Opie Urmson. 1952. Parenthetical verbs. Lind,
61 (244):480?496.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recog-
nition. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING 2010, pages 1507?1514.
9
Proceedings of the 8th International Natural Language Generation Conference, pages 35?44,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
An ACG Analysis of the G-TAG Generation Process?
Laurence Danlos
Universite? Paris Diderot (Paris 7)
ALPAGE, INRIA Paris?Rocquencourt
Institut Universitaire de France, Paris, France
laurence.danlos@inria.fr
Aleksandre Maskharashvili and Sylvain Pogodalla
INRIA Villers-le`s-Nancy, France
Universite? de Lorraine,
CNRS, LORIA, UMR 7503
Vand?uvre-le`s-Nancy, France
aleksandre.maskharashvili@inria.fr
sylvain.pogodalla@inria.fr
Abstract
This paper presents an encoding of
Generation-TAG (G-TAG) within Abstract
Categorial Grammars (ACG). We show
how the key notions of G-TAG have a nat-
ural interpretation in ACG, allowing us to
use its reversibility property for text gen-
eration. It also offers solutions to several
limitations of G-TAG.
1 Motivations
G-TAG (Danlos, 1998; Danlos, 2000) is a formal-
ism based on the Tree Adjoining Grammar (TAG)
formalism (Joshi et al., 1975; Joshi and Schabes,
1997) dedicated to text generation. It focuses on
providing several notions to support useful data
structures, such as g-derivation trees or lexical
databases, to effectively relate a surface form (a
derived tree or a string) to a conceptual represen-
tation. An actual implementation in ADA was first
provided for French (Meunier, 1997), and it has re-
cently been implemented in the .NET framework
as the EasyText NLG system and is operational
at Kantar Media, a French subsidiary company of
TNS-Sofres (Danlos et al., 2011).
The G-TAG proposal can be seen as a result
of the observation of the mismatch between the
derivation tree notion of TAG and the expected se-
mantic dependencies (Schabes and Shieber, 1994)
from a generation perspective. Several approaches
that extend the derivation tree notion of TAG have
been proposed to overcome this difficulty. Other
approaches showed that the derivation trees still
could be used without additional modifications.
Such approaches rely on unification (Kallmeyer
and Romero, 2004; Kallmeyer and Romero, 2007)
or a functional approach to TAG (Pogodalla, 2004;
?This work has been supported by the French agency
Agence Nationale de la Recherche (ANR-12-CORD-0004).
Pogodalla, 2009)1 based on Abstract Categorial
Grammars (ACG) (de Groote, 2001). The latter
is intrinsically reversible: the grammars and the
algorithms are the same for parsing and for gener-
ation.
We propose then to study G-TAG under the
ACG perspective. We show that the key notion
of g-derivation tree naturally express itself in this
framework. The surface form construction from
a conceptual representation can then use the gen-
eral algorithms of ACG, the very same ones that
can be used in parsing to analyze mildly con-
text sensitive languages (TAG generated language,
LCFRS) (de Groote and Pogodalla, 2004), follow-
ing (Kanazawa, 2007)?s proposal here applied to
give an ACG account of G-TAG. We do not con-
sider here the G-TAG treatment of preferences be-
tween the different realizations of the same input.
Similarly, we do not consider the generation of
pronouns used in G-TAG and we will work on
integrating a theory of generation of referring ex-
pressions.
2 Sketching G-TAG
G-TAG deals with the How to say it? task of gen-
eration. The input is a conceptual representation.
A G-TAG grammar includes elementary trees, as
any TAG grammar. But it also makes g-derivation
trees primary objects, relating them to the elemen-
tary trees and considering them as pivot to the con-
ceptual representation level.
Conceptual Representation G-TAG concep-
tual representation makes use of notions as sec-
ond order relation, first order relation and thing.
Second order relations have two arguments which
are relations (either first or second order ones)
and typically correspond to discourse relations,
1Synchronous approaches (Nesson and Shieber, 2006) are
similar in many respects, as shown in (Storoshenk and Frank,
2012).
35
whereas first order relations have things as their
arguments. While (Danlos, 2000) uses reified for-
mulas of a logical conceptual representation lan-
guage as G-TAG inputs, it can also be represented
as a higher-order logical formula (Meunier, 1997)
or as a SDRT-like formula (Danlos et al., 2001).
We follow here this presentation. Equation (1) ex-
emplifies an input that could be realized as Jean
a passe? l?aspirateur pour e?tre re?compense? par
Marie. Puis il a fait une sieste (John has vacumed
in order to be rewarded by Mary. Then he took a
nap).
SUCCESSION(GOAL(VACUUMING(Jean),
REWARDING(Marie, Jean)),
NAPPING(Jean)) (1)
G-TAG Lexical Database A lexical entry of G-
TAG corresponds to a lemma. For each lexical en-
try (i.e. lemma) there is a set of TAG elementary
trees which corresponds to it. Among the TAG el-
ementary trees that correspond to a given lexical
entry, there is the canonical representative, and all
the other representatives are represented by adding
features to the canonical representative. For exam-
ple, if the lexical entry is to love, then the canon-
ical representative will be the active form of the
verb to love. Then the passive alternation is rep-
resented by adding a feature [+passive] to to love.
Moreover, all the lexical entries attached to a con-
cept (such as SUCCESSION) belong to a same lexi-
cal base. So for a concept, there can be a lexical
entry describing verbal realizations of the concept.
These realizations can correspond to the active or
to the passive forms, etc. There can also be a lex-
ical entry which corresponds to nominal realiza-
tions, etc.
G-Derivation Trees A TAG derivation tree can
be seen as a record of the substitutions and adjunc-
tion occurring during a TAG analysis. The same is
true for g-derivation tree. However, while TAG
derivation trees are considered as a by-product,
with inflected anchors, G-TAG derivation trees are
first class structures that are combined in order to
reflect the conceptual input. To abstract from the
surface form and from the derived tree they can
relate to, they don?t correspond to inflected forms
but bear features that are used in a post-processing
step. Complex g-derivation trees are built by going
through the dynamic selection process of a lexi-
cal item from the set of appropriate candidates for
a given concept. So contrary to TAG derivation
trees, they are not fully instantiated trees: their ar-
guments are represented by variables whose lexi-
calization are not carried out yet.
G-Derived Trees A g-derivation tree defines a
unique g-derived tree corresponding to it. This
correspondance is maintained all along the real-
ization process and a post-processing module out-
puts the surface representation (text) from the g-
derived tree. In addition to inflecting forms using
the feature values it can make some rewriting to
propose different versions of the initial text. In
this particular sense, g-derived tree corresponds
to possibly multiple text outputs generated by the
post-processing module.
3 The G-TAG Generation Process
Let us assume the input of Equation 1. The G-TAG
process starts by lexicalizing relations that have
the widest scope in the conceptual representation:
typically second order relations, then first order re-
lations, and things.2 Back to the example, we first
lexicalize the second order relation SUCCESSION.
Several items are associated with this relation:
apre`s (after), avant (before), ensuite (afterwards),
auparavant (beforehand), puis (then), etc. Each of
them has two arguments, however, some of them
produce texts comprising two or more sentences,
like ensuite(afterwards); some of them can pro-
duce either two sentence texts or one sentence text,
while others produce only one sentence. For in-
stance, Jean a passe? l?aspirateur. Ensuite, il a fait
une sieste (John has vacuumed. Afterwards, he
took a nap) is a two sentences text while John a
fait une sieste apre`s avoir passe? l?aspirateur (John
took a nap after having vacuumed) is a one sen-
tence text. For this reason, items describing the
arguments or the result of second order relations
have features expressing the following constraints:
(+T,+S) indicates it is a text (two ore more sen-
tences); (+S) indicates it is either a single sen-
tence or a text; (?T,+S) indicates it is a sentence
(not a text). Every second order relation has three
features: one for output, and two for inputs. 3
2Correctness of the process is ensured because the gram-
mars don?t contain auxiliary trees that would reverse the pred-
ication order. (Danlos, 2000) argues such cases don?t occur in
technical texts, the first target of G-TAG. We don?t elaborate
on this point since the ACG approach we propose remove this
constraint for free.
3In G-TAG, any discourse connective has exactly two ar-
guments. A discussion about this point is provided in (Dan-
36
Let us assume that the G-TAG g-derivation tree
ensuite(+T,+S) belonging to the lexical database
associated with the concept SUCCESSION is first
chosen, resulting in a text rather than a sentence
(illustrated by the leftmost g-derivation tree of Fig-
ure 1 . The process then tries to realize its two ar-
guments. The first one involves the GOAL relation
that can be realized either by pour (in order to) or
by pour que (so that), as exemplified by the right-
most g-derivation trees of Figure 1. Both have fea-
tures (?T,+S) for the inputs (i.e. arguments) and
return a tree labeled at the root by (?T,+S).
ensuite
(+T,+S)
(1st event)
(+S) (2nd event)(+S)
arg1 arg2
pour
(?T,+S)
(ACTION)
(?T,+S) (PURPOSE)(?T,+S)
arg1 arg2
pour que
(?T,+S)
(ACTION)
(?T,+S) (PURPOSE)(?T,+S)
arg1 arg2
Figure 1: G-derivation trees samples
Despite pour and pour que bearing the same
features, the syntactic trees corresponding to pour
and pour que are quite different. For pour que
S substitution nodes can be substituted by two
tensed sentences, while pour takes a finite sen-
tence and a ?sentence? in the infinitive form with-
out any nominal subject. Figure 2 shows the asso-
ciated elementary trees. Selecting one or the other
during the generation process restricts the possible
realizations for the arguments. This is enforced by
a feature associated to the elementary tree, namely
the (+reduc-subj) feature as shown in Fig. 3.
Again, we may assume that G-TAG selects pour,
S
S (arg1) PP
Prep
pour
S
C
que
S(arg2)
(mood:subj)
S
S (arg1) PP
Prep
pour
S(arg2)
(mood:inf)
Figure 2: Elementary trees of pour que (so that)
and pour (in order to)
which will enforce, because of the associated ele-
mentary trees, that the subject of the first and the
second arguments are the same. Afterwards, we
need to lexicalize these two arguments with a com-
mon subject Jean. From a semantic point of view,
the agent of VACUUMING has to be the beneficiary
of REWARDING (the rewardee). VACUUMING can
only be lexicalized as passer-l?aspirateur (run-the-
vacuum-cleaner), while there are several lexical-
los, 2000).
ization options for the REWARDING: re?compenser
(to reward), donner-re?compense (to give-reward),
and recevoir-re?compense (to receive-reward). Let
us notice that donner-re?compense does not meet
the constraint on a shared subject as it cannot
have the rewardee as its subject4. The remaining
options are: recevoir-re?compense, whose canon-
ical representation has the rewardee as subject;
and re?compense whose passive construction has
the rewardee as subject. s Assuming a choice of
re?compenser[+passive],5 the lexicalizations of the
arguments of the first order relations remain. As
Marie occurs only once and in subject position,
it can only be lexicalized as Marie. On the other
hand, Jean three times: one will be the implicit
subject of the subordinate, then as argument of
VACUUMING and NAPPING. Therefore it can be ei-
ther lexicalized in both of the cases as Jean, or
Jean and the pronoun il (he). In G-TAG, there
are some post-processing rules that take care of
the generation of referring expressions, but not in
a really principled way so we do not demonstrate
them here. We assume a lexicalization by Jean in
both cases. Figure 3 shows the g-derivation tree
associated with the input of Equation 1 and Fig. 4
show the unique resulting (non-flected) derived
tree. The post-processing modules then outputs:
Jean a passe? l?aspirateur pour e?tre re?compense?
par Marie. Ensuite, il a fait une sieste. (John
vacuumed in order to be rewarded by Mary. Af-
terwards, he took a nap.)
ensuite
pour
(+reduc-subj)
passer-l?aspirateur
Jean
recompenser
(+reduc-subj,+passive)
Marie 
faire-la-sieste
Jean
arg1
arg1
arg1
arg2
arg1 arg2
arg2
arg1
Figure 3: Fully instantiated g-derivation tree
4 ACG Definition
Abstract Categorial Grammars
(ACGs) (de Groote, 2001) are a type theo-
4It lacks passivation in French and there is no form equiv-
alent to: John was given a reward by Mary.
5Of course, all these branching points offer several real-
izations of the same entry. But for explanatory purposes, we
describe only one at each step.
37
S
S
S
NP
Jea
n
V
pa
sse
rl?
asp
ira
teu
r
PP
Prep
po
ur
S
NP

Va
e?tr
e
V
re?c
om
pe
nse
r
PP
Prep
pa
r
Ma
rie
. S
Adv
en
sui
te
S
NP
Jea
n
V
fai
tu
ne
sie
ste
Figure 4: Non-inflected derived tree
retical framework that is able to encode several
grammatical formalisms (de Groote and Pogo-
dalla, 2004). An ACG defines two languages:
the abstract one and the object one. The abstract
level describe the admissible parse structures and
a lexicon maps these structures to the ones we
observe at the object level (strings for surface
forms, logical formulas for semantic forms). In
all cases, the considered languages are sets of
?-terms that generalize string and tree languages.
Definition. A higher-order linear signature (also
called a vocabulary) is defined to be a triple ? =
?A,C, ??, where:
? A is a finite set of atomic types (also noted
A?),
? C is a finite set of constants (also noted C?),
? and ? is a mapping from C to TA the set of
types built on A: TA ::= A|TA ? TA (also
noted T?).
Given a higher-order linear signature ?, ?(?) is
the set of ?-terms built on ?, and for t ? ?(?)
and ? ? T? such that t has type ?, we note t :? ?
(the ? subscript is omitted when obvious from the
context).
Definition. An abstract categorial grammar is a
quadruple G = ??,?,L, s? where:
1. ? and ? are two higher-order linear signa-
tures, which are called the abstract vocabu-
lary and the object vocabulary, respectively;
2. L : ? ?? ? is a lexicon from the abstract
vocabulary to the object vocabulary. It is
a homomorphism that maps types and terms
built on ? to types and terms built on ? as
follows:
? if ? ? ? ? T? then L(? ? ?) =
L(?)? L(?)
? if x ? ?(?) (resp. ?x.t ? ?(?) and
t u ? ?(?)) then L(x) = x (resp.
L(?x.t) = ?x.L(t) and L(t u) =
L(t)L(u))
It is then enough to define L on the atomic
types and on the constants of ? to define it
on all types and terms, provided that for any
constant c : ? of ? we have L(c) : L(?).
We note t:=G u if L(t) = u and omit the G
subscript if obvious from the context.
3. s ? T? is a type of the abstract vocabulary,
which is called the distinguished type of the
grammar.
Table 1 provides an ACG example Gd-ed trees
where the abstract typed constants of ?der? en-
code the combinatorial properties of the associated
(through the lexicon Ld-ed trees) elementary trees.
Definition. The abstract language of an ACG G =
??,?,L, s? is A(G ) = {t ? ?(?) | t :? s}
The object language of the grammar O(G ) =
{t ? ?(?) | ?u ? A(G ). t = LG(u)}
For instance, the term Creward IS Iv CMary CJean :
S ? Gd-ed trees, and its image, the derived tree for
Marie re?compense Jean (Mary rewards John).
It is important to note that, from a purely math-
ematical point of view, there is no structural differ-
ence between the abstract and the object vocabu-
lary: both are higher-order signatures. This allows
for combining ACGs in different ways:
? by having a same abstract vocabulary shared
by several ACGs: this can be used to make
two object terms (for instance a string and
a logical formula) share the same underlying
structure. Gd-ed trees and GLog in Fig. 5 illustrate
such a composition.
? by making the abstract vocabulary of one
ACG the object vocabulary of another ACG,
allowing for the control of the admissible
structures of the former by the latter. Gyield
and Gd-ed trees in Fig. 5 illustrate such a com-
position.
?(?der?)
?(?trees)
Gd-ed trees
?(?string)
Gyield
?(?Log)
GLog
Figure 5: ACG architecture for TAG
38
Crucial to our analysis is that ACG parsing of
a term u amounts to finding an abstract term t
such that t:= u, no matter whether u represents
a string, a tree, or a logical formula. This can be
done in polynomial time for ACGs whose abstract
constant types are at most of order 2: second order
ACGs as (Kanazawa, 2007) shows.6 The result re-
lies on a reduction of the parsing problem to Data-
log querying where the term to be parsed is stored
in a database. Interestingly, this database can rep-
resent a set of terms (Kanazawa, 2011, Section
4.2) and the query reduces to checking whether at
least one of them can be retrieved. This allows the
query associated with a term representing a logical
formula to extend to all the terms that are equiva-
lent modulo the associativity and the commutativ-
ity of the conjunction.
5 ACG Encoding
5.1 TAG as ACG
Because ACG considers both the abstract lan-
guage and the object language, the encoding of
TAG into ACG makes (abstract) terms represent-
ing derivation trees primary. The encoding uses
two ACGs Gd-ed trees = ??der?,?trees,Ld-ed trees,S?
and Gyield = ??trees,?string,Lyield, ??.
We exemplify the encoding7 of a TAG analyz-
ing (2) in Fig. 6.8
(2) Marie
Mary
re?compense
rewards
ensuite
then
Jean
John
This sentence is usually analyzed in TAG with a
derivation tree where the adverb adjoins at the v
node.
The three higher-order signatures are:
?der?: Its atomic types include S, v, np, SA,
vA. . . where the X types stand for the cate-
gories X of the nodes where a substitution
can occur while the XA types stand for the
categories X of the nodes where an adjunc-
tion can occur. For each elementary tree
?lex. entry it contains a constant Clex. entry whose
type is based on the adjunction and substitu-
tion sites as Table 1 shows. It additionally
contains constants IX : XA that are meant
to provide a fake auxiliary tree on adjunction
6It actually extends this result to almost linear object
terms where variables with atomic type can be duplicated,
as it commonly happens at the semantic level.
7This corresponds to the systematic encoding of (Pogo-
dalla, 2009) of TAG and its semantics into ACG.
8We follow the grammar of (Abeille?, 2002).
sites where no adjunction actually takes place
in a TAG derivation.
?trees: Its unique atomic type is ? the type of
trees. Then, for any X of arity n belong-
ing to the ranked alphabet describing the ele-
mentary trees of the TAG, we have a constant
Xn :
n times? ?? ?
? ( ? ? ?( ? ( ?
?string: Its unique atomic type is ? the type of
strings. The constants are the terminal sym-
bols of the TAG (with type ?), the concatena-
tion + : ? ( ? ( ? and the empty string
? : ?.
Table 1 illustrates Ld-ed trees.9 Lyield is defined as
follows:
? Lyield(?) = ?;
? for n > 0, Lyield(Xn) = ?x1 ? ? ?xn.x1 +
? ? ?+ xn;
? for n = 0, X0 : ? represents a terminal sym-
bol and Lyield(X0) = X .
Then, the derivation tree, the derived tree, and the
yield of Fig. 6 are represented by:
?reward
?Jean?Marie?then
(a) Derivation tree
S
np
Jean
v
ensuitev
re?compense
np
Marie
(b) Derived tree
Figure 6: Marie re?compense ensuite Jean
?5 = Creward IS (Cvthen IS) CMarie CJean
Ld-ed trees(?5)
= S3 (np1 Marie)
(v2 (v1 re?compense) ensuite) (np1 Jean)
Lyield(Ld-ed trees(?5)) = Marie + re?compense
+ ensuite + Jean
5.2 G-TAG as ACG
In order to model G-TAG in ACG, first we need to
design the abstract signature ?g-der? in which we
can have entries for G-TAG. This entries will re-
flect the ideology that G-TAG is based on. For
instance, in G-TAG discourse level words like en-
suite can take as its arguments texts and sentences
and produces text. In order to model this, we
introduce types S and T. Then, we can define
DSSthen: S ( S ( T, which means that DSSthen has
takes two arguments of type S and returns a re-
sult of type T. As in G-TAG, ensuite can take two
9With Ld-ed trees(XA) = ? ( ? and for any other type
X , Ld-ed trees(XA) = ? .
39
Abstract constants of ?der? Their images by Ld-ed trees The corresponding TAG trees
CJean : np cJean : ?= np1 Jean ?Jean =
np
Jean
Cvthen : vA ( vA cvthen : (? ( ?) ( (? ( ?)= ?ovx.v (v2 xensuite) ?then = v
ensuitev?
Creward : SA ( vA ( np( np ( S creward
: (? ( ?) ( (? ( ?) ( ?( ? ( ?
= ?oavso.a (S3 s (v (v1 re?compense)) o)
?reward = S
npv
re?compense
np
IX : XA ?x.x : ? ( ?
Table 1: A TAG as an ACG: Ld-ed trees and Llog.sem lexicons
texts as arguments and return text as well, we need
to do have another entry for modeling this fact.
This makes us to introduce another constant DTTthen :
T ( T ( T. For the same kind of reason, we in-
troduce following constants: DSTthen: S ( T ( T,
DTSthen and T ( S ( T. Other relations, like au-
paravant is modeled in the same way as ensuite in
?g-der?.
Apart from ensuite and auparavant, there are
connectives as avant (before) and apre`s (after) that
need to be modeled differently from ensuite. In-
deed, while ensuite results in a text, placing side
by side a text and a sentence separated with a pe-
riod, avant and apre`s in French combine in a sin-
gle sentence a (full) clause and an infinitive clause
with an implicit subject: the one of the first clause.
It is clear that in order to type avant and apre`s in
the ?g-der? signature, one should use a type which
schematically looks as . . . ( S. On the other
hand, one needs to give the exact type to them.
Despite that in TAG and G-TAG avant and apre`s
take two sentential arguments (labelled by S), the
second argument bears a feature indicating it lacks
the subject and that the latter has to be shared with
the first sentence. For instance: Jean a fait une
sieste apre`s avoir passe? l?aspirateur (John took
a nap after having vacuumed), here the subject
of avoir passe? l?aspirateur (having vacuumed) is
Jean, which comes from the sentence Jean a fait
une sieste (John took a nap). So, Jean a fait une
sieste (John took a nap) can be seen as a sentence
whose subject is shared by another sentence as
well. In order to model this point, we use fol-
lowing type: Sws ( Sh ( np ( S. Indeed,
the Sws and the Sh types correspond to the type
of sentences missing a subject. Furthermore, we
need to model pour and pour que, which were in-
troduced in order to lexicalize the GOAL relation in
G-TAG. First, let us have a look at pour que. It can
take as its arguments two complete (from a syntax
point of view) sentences and results in a sentence
as in: Il travaille pour que vous puissiez manger.
So, Dpour que, which is an entry corresponding to
pour que, can be assigned a S ( S ( S type.
The syntactic difference between pour que and
pour was highlighted in Section 3: pour takes
as arguments a complete sentence and an infini-
tive form of a sentence missing a subject whose
subject comes from the first argument. Thus, in
this case, similarly to case of avant and apre`s,
pour has to be modeled as an entry that has type
Sws ( Sinf ( np ( S, where Sinf stands for
the type of an infinitive form of a clause missing a
subject. We also need to deal with encoding differ-
ent forms of a verb. For instance, re?compenser has
an active and a passive form. In G-TAG deriva-
tion, both of them can be encountered. In order
to model this fact, two different entries are intro-
duced: one for the passive form and one for the
active form, which is the canonical construction
for re?compenser. So, we need to have two distinct
entries Dpassiverecompense and Dactiverecompense, and both of them
have type SA ( vA ( np ( np ( S. More-
over, (Danlos, 2000) poses the problem that G-
TAG cannot handle a text where the adverb adjoin
at the v node rather than on the S node as in: Jean
a passe? l?aspirateur. Il a ensuite fait une sieste
(John vacuumed. He then took a nap.) According
to (Danlos, 2000) modelling such text production
requires a formalism more powerful than TAG. In
the ACG framework, this observations translates
into defining an entry Dvthen : S ( (vA ( S) (
T in ?g-der? which is third order and that is, as such,
beyond the TAC into ACG encoding (that only re-
quires second-order types).10 This also offers a
10Currently, there is no theoretical complexity result for
parsing such ACG fragments. However, in this particu-
40
general mechanism for providing constants encod-
ing adverbial connectives with two arguments as
in discourse grammars such as D-STAG (Danlos,
2011), but contrary to D-LTAG where one of the
arguments is anaphorically given from the preced-
ing discourse (Webber, 2004).
G-Derivation Trees to Derivation Trees We
translate terms of ?g-der?, which correspond to g-
derivation trees, into the TAG derivation tree lan-
guage defined on ?der? using the lexicon Lder-der
of Table 2. It is interesting to see how to inter-
Lder-der(S) = Lder-der(T) = Lder-der(Sws)
= Lder-der(Sinf) = Lder-der(Sh)
= S
Lder-der(SA) = SA
Lder-der(vA) = vA
Lder-der(np) = np
Lder-der(IS) = IS
Lder-der(Iv) = Iv
Table 2: The Lder-der lexicon
pret Dvthen: S ( (vA ( S) ( T into ?der?.
For this reason, we introduce in ?der? the follow-
ing constant: s2 : S ( S ( S that allows
for combining two sentences with a period. Now,
it is possible to translate Dvthen into ?der? as fol-
lows: Lder-der(Dvthen) = ?oS1 S2.s2 S1(S2Cvthen).
It means that Dvthen is interpreted as performing
both the operation of combining two sentences
with a period and the adjunction of ensuite on the
v node of the second sentence.
G-Derived Trees as Interpretation of G-
Derivation Trees As soon as g-derivation trees
as term built on ?g-der? are interpreted as term
built on ?der?, we can map them to derived trees.
Thus, by composing the two lexicons Lder-der and
Ld-ed trees we can get directly from G-TAG into de-
rived trees
5.3 From G-TAG to Montague Style
Semantics Using ACGs
(Pogodalla, 2009) defines a signature ?Log and a
lexicon LLog from ?der? to ?Log. The entries in
?Log have Montague like semantics. The lexicon
translates a derivation tree into a corresponding
formula. We will use the same kind of semantic
language for conceptual representations. In other
words, our language will produce the formulas
lar case, we could use a second-order?and polynomial?
encoding of multi-component TAG into ACG.
that are used in the conceptual representation of
G-TAG, while we will stick to the Montague style
translations from syntax to semantics.
So, we define a signature ?conrep of conceptual
representation that is similar to the one of (Pogo-
dalla, 2009). ?conrep defines two atomic types e
and t and constants such as: j, m . . . of type e, the
constant REWARD of type e ( e ( t, the con-
stant CLAIM of type e ( t ( t and the constant
SEEM of type t( t. Moreover, we have constants
SUCC, GOAL of type t( t( t.
We are able to translate ?g-der? into ?conrep
with the help of the lexicon Lder-con. The
lexicon Lder-con is extension of the lexicon
defined in (Pogodalla, 2009), because we
are adding to the domain (i.e. abstract lan-
guage) the constants that are not in the ?der?.
Lder-con(S) = Lder-con(T) = t
Lder-con(vA) = (e? t) ( (e? t)
Lder-con(SA) = t( t
Lder-con(np) = (e? t) ( t
Lder-con(Djean) = ?oP.P (j)
Lder-con(DSTthen) = Lder-con(DSSthen)
= Lder-con(DSTthen)
= Lder-con(DTSthen)
= Lder-con(DTTthen )
= ?s2s1.SUCC s2 s1
Lder-con(DSTbef. ) = Lder-con(DSSbef. )
= Lder-con(DSTbef. )
= Lder-con(DTSbef. )
= Lder-con(DTTbef. )
= ?o s1s2. SUCC s2 s1
Lder-con(Drewards) = ?os a O S.s(S(a(?ox.O(?oy.
(REWARD x y))))
Note that the interpretation of np is JnpK =
(e ? t) ( t, using a non-linear implication (but
almost linear). Typically, the sharing of the sub-
ject by the two clauses related by pour or avant de
induces non linearity.
The Sinf, Sh, and Sws types all are interpreted
as JnpK ( JSK = ((e ? t) ( t) ( t as they
denote clauses lacking a subject. Then we trans-late the constants Dpour, Dapre`s, and Davant in
the following way:
Lder-con(Dpour ) =
?os1.?os2.?oN.N(?x.(GOAL(s1(?P.P x))
(s2(?P.P x))))
Lder-con(Dapres) =
?os1.?os2.?oN.N(?x.(SUCC(s1(?P.P x))
(s2(?P.P x))))
41
Lder-con(Davant) =
?os1.?os2.?oN.N(?x.(SUCC(s2(?P.P x))
(s1(?P.P x))))
5.4 The G-TAG Process as a Morphism
Composition
We exemplify the whole process using the term
T0 = SUCC(VAC(jean),REWARD(marie, jean))
of type t.11 The terms representing the g-
derivation trees that generate this conceptual rep-
resentation are the antecedents of To by L ?1der-con:
L ?1der-con(T0) = {t1, . . . , t8} that all are of type
T. They are given in Figure 7. Each of these re-
trieved terms t1, . . . , t8 are then mapped to terms
representing TAG derivation trees, i.e. built on
?der? via the lexicon Lder-der. They can be can
in turn be interpreted as syntactic derived trees
via the lexicon Ld-ed trees, and the latter can be
interpreted as strings using the lexicon Lyield.
So from T0 we can have eight surface forms:
Lyield(Ld-ed trees(Lder-der(ti))), i ? [1, 8]. Let us
show this process on the example of t512. It il-
lustrates the generation of the example (3).13
(3) Jean
John
a passe? l?aspirateur.
vacuumed.
Marie
Mary
a re?compense?
rewarded
ensuite
afterwards
Jean.
John.
Lder-der(t5) = s2 (CvacISIvCjean)
(CrewardISCvthenCmarieCjean)
Ld-ed trees(Lder-der(t5) =
S3 (S2 (np1 Jean)(v1 a passe? l?aspirateur))
?
(S3(np1 Marie)(v2 (v1 a re?compense?) ensuite)(np1 Jean))
And the surface forms is given by composing the
interpretations:
Lyield(Ld-ed trees(Lder-der(t5)) =
Jean + a passe? + l?aspirateur + . +
Marie + a recompense? + ensuite + Jean
11The associated conceptual input is a simplified version of
the conceptual input of Equation 1 without the GOAL concept
and a replacement of the NAP one by the REWARDING one.
12t5 is such that Lder-der(t5) = ?5 and the term ?5 wasused as example at Section 5.1.
13For sake of simplicity we assume the adverb adjoins on
the whole auxiliary+verb phrase rather than only on the aux-
iliary as it would be in French.
t1 = DSSthen(DvacISIvDjean)(DrewardISIvDmarieDjean)
t2 = DSSthen(DvacISIvDjean)(DpassiverewardISIvDmarieDjean)
t3 = DSSbef.(DrewardISIvDmarieDjean)(DvacISIvDjean)
t4 = DSSbef.(DpassiverewardISIvDjeanDmarie)(DvacISIvDjean)
t5 = Dvthen(DvacISIvDjean)(?oa.Dreward IS a DmarieDjean)
t6 = Dvthen(DvacISIvDjean)(DpassiverewardISIvDjeanDmarie)
t7 = Dafter (DswsvacISIv)(Dreceive-rew.ISIvDjean)Dmarie
t8 = Dbef.(DswsvacISIv)(Dreceive-rew.ISIvDjean)Dmarie
Figure 7: Antecedents of T0 by Lder-con
6 Related Work
We can only quickly mention two related pieces
of work. On the one hand, (Gardent and Perez-
Beltrachini, 2010) also takes advantage of the
formal properties underlying the tree language
of derivation trees to propose a generation pro-
cess using TAG grammars. On the other hand,
(Nakatsu and White, 2010) also includes discourse
relations in the grammar with Discourse Combi-
natory Categorial Grammar and a type-theoretical
framework to provide a text (rather than sentence)
generation process.
7 Conclusion
This paper shows how G-TAG can be encoded as
ACG. It relies on the fact that both G-TAG and the
encoding of TAG within ACG make the deriva-
tion tree a primary notion. Then we can bene-
fit from the polynomial reversibility of the ACG
framework. It also offers a generalization of the
process to all kinds of adjunctions, including the
predicative ones. It also offers a new insight on
discourse grammars for the adverbial connective
encoding (Danlos, 2011). Note that contrary to an
important part of G-TAG that offers a way (based
on a semantic and a linguistic analysis) to rank the
different realizations of a conceptual representa-
tion, we do not deal here with such preferences.
As syntactic ambiguity treatment is not usually
part of the syntactic formalism, we prefer the ?re-
alization ambiguity? treatment not to be part of the
generation formalism. Finally, a crucial perspec-
tive is to integrate a theory of generation of re-
ferring expressions relying on type-theoretical ap-
proaches to dynamics semantics (de Groote, 2006;
de Groote and Lebedeva, 2010) that would ensure
a large compatibility with the ACG framework.
42
References
[Abeille?2002] Anne Abeille?. 2002. Une grammaire
e?lectronique du franc?ais. Sciences du langage.
CNRS E?ditions.
[Danlos et al.2001] Laurence Danlos, Bertrand Gaiffe,
and Laurent Roussarie. 2001. Document sructuring
a` la SDRT. In Helmut Horacek, Nicolas Nicolov,
and Leo Wanner, editors, Proceedings of the ACL
2001 Eighth European Workshop on Natural Lan-
guage Generation (EWNLG). http://aclweb.
org/anthology/W/W01/W01-0803.pdf.
[Danlos et al.2011] Laurence Danlos, Fre?de?ric Meu-
nier, and Vanessa Combet. 2011. EasyText: an
operational NLG system. In ENLG 2011, 13th
European Workshop on Natural Language Gener-
ation, September. http://hal.inria.fr/
inria-00614760/en/.
[Danlos1998] Laurence Danlos. 1998. G-TAG :
Un formalisme lexicalise? pour la ge?ne?ration de
textes inspire? de TAG. Traitement Automatique
des Langues, 39(2). http://hal.inria.fr/
inria-00098489.
[Danlos2000] Laurence Danlos. 2000. G-TAG: A lex-
icalized formalism for text generation inspired by
tree adjoining grammar. In Anne Abeille? and Owen
Rambow, editors, Tree Adjoining Grammars: For-
malisms, Linguistic Analysis, and Processing, pages
343?370. CSLI Publications.
[Danlos2011] Laurence Danlos. 2011. D-STAG:
a formalism for discourse analysis based on
SDRT and using synchronous TAG. In Philippe
de Groote, Markus Egg, and Laura Kallmeyer, ed-
itors, 14th conference on Formal Grammar - FG
2009, volume 5591 of LNCS/LNAI, pages 64?84.
Springer. http://dx.doi.org/10.1007/
978-3-642-20169-1_5.
[de Groote and Lebedeva2010] Philippe de Groote and
Ekaterina Lebedeva. 2010. Presupposition ac-
commodation as exception handling. In Proceed-
ings of the SIGDIAL 2010 Conference, pages 71?74,
Tokyo, Japan, September. Association for Computa-
tional Linguistics. http://www.aclweb.org/
anthology/W/W10/W10-4313.
[de Groote and Pogodalla2004] Philippe de Groote and
Sylvain Pogodalla. 2004. On the expressive power
of Abstract Categorial Grammars: Representing
context-free formalisms. Journal of Logic, Lan-
guage and Information, 13(4):421?438. http:
//hal.inria.fr/inria-00112956.
[de Groote2001] Philippe de Groote. 2001. Towards
Abstract Categorial Grammars. In Association
for Computational Linguistics, 39th Annual Meet-
ing and 10th Conference of the European Chap-
ter, Proceedings of the Conference, pages 148?
155. http://aclweb.org/anthology/P/
P01/P01-1033.pdf.
[de Groote2006] Philippe de Groote. 2006. To-
wards a montagovian account of dynam-
ics. In Masayuki Gibson and Jonathan
Howell, editors, Proceedings of Semantics
and Linguistic Theory (SALT) 16. http:
//elanguage.net/journals/index.
php/salt/article/view/16.1/1791.
[Gardent and Perez-Beltrachini2010] Claire Gardent
and Laura Perez-Beltrachini. 2010. RTG based sur-
face realisation for TAG. In Proceedings of the 23rd
International Conference on Computational Lin-
guistics (COLING 2010), pages 367?375, Beijing,
China, August. Coling 2010 Organizing Committee.
http://www.aclweb.org/anthology/
C10-1042.
[Joshi and Schabes1997] Aravind K. Joshi and Yves
Schabes. 1997. Tree-adjoining grammars. In
G. Rozenberg and A. Salomaa, editors, Handbook
of formal languages, volume 3, chapter 2. Springer.
[Joshi et al.1975] Aravind K. Joshi, Leon S. Levy, and
Masako Takahashi. 1975. Tree adjunct gram-
mars. Journal of Computer and System Sciences,
10(1):136?163.
[Kallmeyer and Romero2004] Laura Kallmeyer and
Maribel Romero. 2004. LTAG semantics with
semantic unification. In Proceedings of TAG+7,
pages 155?162.
[Kallmeyer and Romero2007] Laura Kallmeyer and
Maribel Romero. 2007. Scope and situation
binding for LTAG. Research on Language and
Computation, 6(1):3?52. http://dx.doi.
org/10.1007/s11168-008-9046-6.
[Kanazawa2007] Makoto Kanazawa. 2007. Pars-
ing and generation as datalog queries. In Pro-
ceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL),
pages 176?183. http://www.aclweb.org/
anthology/P/P07/P07-1023.
[Kanazawa2011] Makoto Kanazawa, 2011. Parsing
and generation as Datalog query evaluation. Un-
der review. http://research.nii.ac.jp/
?kanazawa/publications/pagadqe.pdf.
[Meunier1997] Fre?de?ric Meunier. 1997. Implantation
du formalisme de ge?ne?ration G-TAG. Ph.D. thesis,
Universite? Paris 7 ? Denis Diderot.
[Nakatsu and White2010] Crytal Nakatsu and Michael
White. 2010. Generating with discourse com-
binatory categorial grammar. Linguistic Is-
sues in Language Technology, 4(1). http:
//elanguage.net/journals/index.
php/lilt/article/view/1277/871.
[Nesson and Shieber2006] Rebecca Nesson and
Stuart M. Shieber. 2006. Simpler TAG seman-
tics through synchronization. In Proceedings
of the 11th Conference on Formal Grammar,
Malaga, Spain, 29?30 July. CSLI Publications.
43
http://cslipublications.stanford.
edu/FG/2006/nesson.pdf.
[Pogodalla2004] Sylvain Pogodalla. 2004. Comput-
ing Semantic Representation: Towards ACG Ab-
stract Terms as Derivation Trees. In Proceedings
of TAG+7, pages 64?71. http://hal.inria.
fr/inria-00107768.
[Pogodalla2009] Sylvain Pogodalla. 2009. Advances
in Abstract Categorial Grammars: Language The-
ory and Linguistic Modeling. ESSLLI 2009 Lec-
ture Notes, Part II. http://hal.inria.fr/
hal-00749297.
[Schabes and Shieber1994] Yves Schabes and Stu-
art M. Shieber. 1994. An alternative conception
of tree-adjoining derivation. Computational Lin-
guistics, 20(1):91?124. http://aclweb.org/
anthology/J/J94/J94-1004.pdf.
[Storoshenk and Frank2012] Dennis Ryan Storoshenk
and Robert Frank. 2012. Deriving syntax-semantics
mappings: node linking, type shifting and scope am-
biguity. In Proceedings of TAG+11, pages 10?18.
[Webber2004] Bonnie Webber. 2004. D-LTAG: Ex-
tending :exicalized TAG to discourse. Cognitive Sci-
ence, 28:751?779. http://dx.doi.org/0.
1207/s15516709cog2805_6.
44
