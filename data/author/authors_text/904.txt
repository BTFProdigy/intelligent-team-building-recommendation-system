231
232
233
234
Generating and Visualizing a Soccer Knowledge Base
Paul Buitelaar, Thomas Eigner, Greg Gul-
rajani, Alexander Schutz, Melanie Siegel,
Nicolas Weber
Language Technology Lab, DFKI GmbH
Saarbr?cken, Germany
{paulb,siegel}@dfki.de
Philipp Cimiano, G?nter Ladwig,
Matthias Mantel, Honggang Zhu
Institute AIFB, University of Karlsruhe
Karlsruhe, Germany
cimiano@aifb.uni-karlsruhe.de
Abstract
This demo abstract describes the SmartWeb
Ontology-based Annotation system (SOBA).
A key feature of SOBA is that all informa-
tion is extracted and stored with respect to
the SmartWeb Integrated Ontology
(SWIntO). In this way, other components of
the systems, which use the same ontology,
can access this information in a straightfor-
ward way. We will show how information
extracted by SOBA is visualized within its
original context, thus enhancing the browsing
experience of the end user.
1 Introduction
SmartWeb1 is a multi-modal dialog system,
which derives answers from unstructured re-
sources such as the Web, from automatically ac-
quired knowledge bases and from web services.
In this paper we describe the current status of
the SmartWeb Ontology-Based Annotation
(SOBA) system. SOBA automatically populates
a knowledge base by information extraction from
soccer match reports as available on the web.
The extracted information is defined with respect
to SWIntO, the underlying SmartWeb Integrated
Ontology (Oberle et al, in preparation) in order
to be smoothly integrated into the system.
The ability to extract information and describe
it ontologically is a basic requirement for more
complex processing tasks such as reasoning and
discourse analysis (for related work on ontology-
based information extraction see e.g. Maedche et
al., 2002; Lopez and Motta, 2004; M?ller et al,
2004; Nirenburg and Raskin, 2004).
1 http://www.smartweb-projekt.de/start_en.html
2 System Overview
The SOBA system consists of a web crawler,
linguistic annotation components and a compo-
nent for the transformation of linguistic annota-
tions into an ontology-based representation.
The web crawler acts as a monitor on relevant
web domains (i.e. the FIFA2 and UEFA3 web
sites), automatically downloads relevant
documents from them and sends them to a
linguistic annotation web service.
Linguistic annotation and information
extraction is based on the Heart-of-Gold (HoG)
architecture (Callmeier et al 2004), which
provides a uniform and flexible infrastructure for
building multilingual applications that use
semantics- and XML-based natural language
processing components.
The linguistically annotated documents are
further processed by the transformation
component, which generates a knowledge base
of soccer-related entities (players, teams, etc.)
and events (matches, goals, etc.) by mapping
annotated entities or events to ontology classes
and their properties.
Finally, an automatic hyperlinking component
is used for the visualization of extracted entities
and events. This component is based on the
VieWs system, which was developed
independently of SmartWeb (Buitelaar et al,
2005). In what follows we describe the different
components of the system in detail.
2.1 Web Crawler
The crawler enables the automatic creation of a
football corpus, which is kept up-to-date on a
daily basis. The crawler data is compiled from
texts, semi-structured data and copies of original
2 http://fifaworldcup.yahoo.com/
3 http://www.uefa.com/
123
HTML documents. For each football match, the
data source contains a sheet of semi-structured
data with tables of players, goals, referees, etc.
Textual data comprise of match reports as well as
news articles.
The crawler is able to extract data from two
different sources: FIFA and UEFA. Semi-
structured data, news articles and match reports
covering the WorldCup2006 are identified and
collected from the FIFA website. Match reports
and news articles are extracted from the UEFA
website. The extracted data are labeled by IDs
that match the filename. The IDs are derived
from the corresponding URL and are thus
unique.
The crawler is invoked continuously each day
with the same configuration, extracting only data
which is not yet contained in the corpus. In order
to distinguish between available new data and
data already present in the corpus, the URLs of
all available data from the website are matched
against the IDs of the already extracted data.
2.2 Linguistic Annotation and Information
Extraction
As mentioned before, linguistic annotation in the
system is based on the HoG architecture, which
provides a uniform and flexible infrastructure for
building multilingual applications that use
semantics- and XML-based natural language
processing components.
For the annotation of soccer game reports, we
extended the rule set of the SProUT (Drozdzyn-
ski et al 2004) named-entity recognition compo-
nent in HoG with gazetteers, part-of-speech and
morphological information. SProUT combines
finite-state techniques and unification-based al-
gorithms. Structures to be extracted are ordered
in a type hierarchy, which we extended with soc-
cer-specific rules and output types.
SProUT has basic grammars for the annotation
of persons, locations, numerals and date and time
expressions. On top of this, we implemented
rules for soccer-specific entities, such as actors in
soccer (trainer, player, referee ?), teams, games
and tournaments. Using these, we further imple-
mented rules for soccer-specific events, such as
player activities (shots, headers ?), game events
(goal, card ?) and game results. A soccer-
specific gazetteer contains soccer-specific enti-
ties and names and is supplemented to the gen-
eral named-entity gazetteer.
As an example, consider the linguistic annota-
tion for the following German sentence from one
of the soccer game reports:
Guido Buchwald wurde 1990 in Italien Welt-
meister (Guido Buchwald became world cham-
pion in 1990 in Italy)
<FS type="player_action">
<F name="GAME_EVENT">
<FS type="world champion"/>
<F name="ACTION_TIME">
<FS type="1990"/>
<F name="ACTION_LOCATION">
<FS type="Italy"/>
<F name="AGENT">
<FS type="player">
<F name="SURNAME">
<FS type="Buchwald"/>
<F name="GIVEN_NAME">
<FS type="Guido"/>
2.3 Knowledge Base Generation
The SmartWeb SportEventOntology (a subset of
SWIntO) contains about 400 direct classes onto
which named-entities and other, more complex
structures are mapped. The mapping is repre-
sented in a declarative fashion specifying how
the feature-based structures produced by SProUT
are mapped into structures which are compatible
with the underlying ontology. Further, the newly
extracted information is also interpreted in the
context of additional information about the
match in question.
This additional information is obtained by
wrapping the semi-structured data on relevant
soccer matches, which is also mapped to the on-
tology. The information obtained in this way
about the match in question can then be used as
contextual background with respect to which the
newly extracted information is interpreted.
The feature structure for player as displayed
above will be translated into the following F-
Logic (Kifer et al 1995) statements, which are
then automatically translated to RDF and fed to
the visualization component:
soba#player124:sportevent#FootballPlayer
[sportevent#impersonatedBy ->
soba#Guido_BUCHWALD].
soba#Guido_BUCHWALD:dolce#"natural-person"
[dolce#"HAS-DENOMINATION" ->
soba#Guido_BUCHWALD_Denomination].
soba#Guido_BUCHWALD_Denomination":dolce#"
natural-person-denomination"
[dolce#LASTNAME -> "Buchwald";
dolce#FIRSTNAME -> "Guido"].
124
2.4 Knowledge Base Visualization
The generated knowledge base is visualized by
way of automatically inserted hyperlink menus
for soccer-related named-entities such as players
and teams. The visualization component is based
on the VIeWs4 system. VIeWs allows the user to
simply browse a web site as usual, but is addi-
tionally supported by the automatic hyperlinking
system that adds additional information from a
(generated) knowledge base.
For some examples of this see the included
figures below, which show extracted information
for the Panama team (i.e. all of the football play-
ers in this team in Figure 1) and for the player
Roberto Brown (i.e. his team and events in which
he participated in Figure 2).
3 Implementation
All components are implemented in Java 1.5 and
are installed as web applications on a Tomcat
web server. SOAP web services are used for
communication between components so that the
system can be installed in a centralized as well as
decentralized manner. Data communication is
handled by XML-based exchange formats. Due
to a high degree of flexibility of components,
only a simple configuration over environment
variables is needed.
4 Conclusions and Future Work
We presented an ontology-based approach to
information extraction in the soccer domain that
aims at the automatic generation of a knowledge
base from match reports and the subsequent
visualization of the extracted information
through automatic hyperlinking. We argue that
such an approach is innovative and enhances the
user experience.
Future work includes the extraction of more
complex events, for which deep linguistic analy-
sis and/or semantic inference over the ontology
and knowledge base is required. For this purpose
we will use an HPSG-based parser that is avail-
able within the HoG architecture (Callmeier,
2000) and combine this with a semantic infer-
ence approach based on discourse analysis
(Cimiano et al, 2005).
4 http://views.dfki.de
Acknowledgements
This research has been supported by grants for
the projects SmartWeb (by the German Ministry
of Education and Research: 01 IMD01 A) and
VIeWs (by the Saarland Ministry of Economic
Affairs).
References
Paul Buitelaar, Thomas Eigner, Stefania Racioppa
Semantic Navigation with VIeWs In: Proc. of the
Workshop on User Aspects of the Semantic Web at
the European Semantic Web Conference, Herak-
lion, Greece, May 2005.
Callmeier, Ulrich (2000). PET ? A platform for ex-
perimentation with efficient HPSG processing
techniques. In: Natural Language Engineering, 6
(1) UK: Cambridge University Press pp. 99?108.
Callmeier, Ulrich, Eisele, Andreas, Sch?fer, Ulrich
and Melanie Siegel. 2004. The DeepThought Core
Architecture Framework In Proceedings of LREC
04, Lisbon, Portugal, pages 1205-1208.
Cimiano, Philipp, Saric, Jasmin and Uwe Reyle.
2005. Ontology-driven discourse analysis for in-
formation extraction, Data Knowledge Engineering
55(1).
Drozdzynski, Witold, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Sch?fer, and Feiyu Xu. 2004.
Shallow processing with unification and typed fea-
ture structures ? foundations and applications.
K?nstliche Intelligenz, 1:17-23.
Kifer, M., Lausen, G. and J.Wu. 1995. Logical Foun-
dations of Object-Oriented and Frame-Based Lan-
guages. Journal of the ACM 42, pp. 741-843.
Lopez, V. and E. Motta. 2004. Ontology-driven Ques-
tion Answering in AquaLog In Proceedings of 9th
International Conference on applications of natural
language to information systems.
Maedche, Alexander, G?nter Neumann and Steffen
Staab. 2002. Bootstrapping an Ontology-Based In-
formation Extraction System. In: Studies in Fuzzi-
ness and Soft Computing, editor J. Kacprzyk. Intel-
ligent Exploration of the Web, Springer.
M?ller HM, Kenny EE and PW Sternberg. 2004.
Textpresso: An ontology-based information re-
trieval and extraction system for biological litera-
ture. PLoS Biol 2: e309.
Nirenburg, Sergei and Viktor Raskin. 2004. Ontologi-
cal Semantics. MIT Press.
Oberle et al The SmartWeb Integrated Ontology
SWIntO, in preparation.
125
Figure 2: Generated hyperlink on ?Roberto Brown? with extracted information on his
team and events in which he participated
Figure 1: Generated hyperlink on ?Panama? with extracted information on this team
126
Reducing Lexical Semantic Complexity with Systematic Polysemous 
Classes and Underspecification 
Paul Buitelaar 
DFKI Language Technology Lab 
Stuhlsatzenhausweg 3, 
66123 Saarbrticken, Germany 
paulb@dfki.de 
Abstract 
This paper presents an algorithm for finding 
systematic polysemous classes in WordNet 
and similar semantic databases, based on a 
definition in (Apresjan 1973). The 
introduction of systematic polysemous 
classes can reduce the amount of lexical 
semantic processing, because the number of 
disambiguation decisions can be restricted 
more clearly to those cases that involve real 
ambiguity (homonymy). In many 
applications, for instance in document 
categorization, information retrieval, and 
information extraction, it may be sufficient 
to know if a given word belongs to a certain 
class (underspecified sense) rather than to 
know which of its (related) senses exactly to 
pick. The approach for finding systematic 
polysemous classes is based on that of 
(Buitelaar 1998a, Buitelaar 1998b), while 
addressing some previous hortcomings. 
Introduction 
This paper presents an algorithm for finding 
systematic polysemous classes in WordNet 
(Miller et al1990) and GermaNet (Hamp 
and Feldweg 1997) -- a semantic database 
for German similar to WordNet. The 
introduction of such classes can reduce the 
amount of lexical semantic processing, 
because the number of disambiguation 
decisions can be restricted more clearly to 
those cases that involve real ambiguity 
(homonymy). Different than with 
homonyms, systematically polysemous 
words need not always be disambiguated, 
because such words have several related 
senses that are shared in a systematic way by 
a group of similar words. In many 
applications then, for instance in document 
categorization and other areas of 
information retrieval, it may be sufficient o 
know if a given word belongs to this grou p 
rather than to know which of its (related) 
senses exactly to pick. In other words, it will 
suffice to assign a more coarse grained sense 
that leaves several related senses 
underspecified, but which can be further 
specified on demand 1. 
The approach for finding systematic 
polysemous classes is based on that of 
(Buitelaar 1998a, Buitelaar 1998b), but 
takes into account some shortcomings as 
pointed out in (Krymolowski and Roth 
1998) (Peters, Peters and Vossen 1998) 
(Tomuro 1998). Whereas the original 
approach identified a small set of top-level 
synsets for grouping together lexical items, 
i As pointed out in (Wilks 99), earlier work in AI on 
'Polaroid Words' (Hirst 87) and 'Word Experts' 
(Small 81) advocated a similar, incremental approach 
to sense representation and interpretation. In line with 
this, the CoreLex approach discussed here provides a 
large scale inventory of systematically polysemous 
lexical items with underspecified r presentations that 
can be incrementally refined. 
14 
the new approach compares lexical items 
according to all of their synsets on all 
hierarchy levels. In addition, the new 
approach is both more flexible and precise 
by using a clustering algorithm for 
comparing meaning distributions between 
lexical items. Whereas the original approach 
took into account only identical distributions 
(with additional human intervention to 
further group together sufficiently similar 
classes), the clustering approach allows for 
completely automatic omparisons, relative 
to certain thresholds, that identify partial 
overlaps in meaning distributions. 
1 Acquisition and Application of 
Systematic Polysemous Classes 
In lexical semantics, a distinction can be 
made between senses that are of a 
contrastive and those that are of a 
complementary nature (Weinreich 1964). 
Contrastive senses are unrelated to each 
other as with the two meanings of "bank". 
However, such clear-cut (contrastive) 
meaning distinctions are rather the exception 
than the rule. Often, words have a number of 
(complementary) senses that are somehow 
related to each other in systematic ways 
(Pustejovsky 1995). For instance, a word 
like "mouth" has several senses that are all 
somehow related (after Cruse 1986): 
John opened his mouth. 
This parasite attaches itself to their mouths. 
The mouth of the cave resembles a bottle. 
The mouth of the river starts here. 
2 CoreLex 
Related senses are, however, only 
systematic (or regular) if more than one 
example in a language can be found as 
formulated in (Apresjan 1973): 
Polysemy of the word A with the meanings ai
and aj is called regular if in the given 
language, there exists at least one other 
word B with the meanings bi bj, which are 
semantically distinguished from each other 
in exactly the same way as ai and aj and i f  ai 
and bi, aj and bj are nonsynonymous. 
With this definition, we can construct 
classes of systematically polysemous words 
as shown in the CoreLex approach 
(Buitelaar 1998a) (Buitelaar 1998b). This 
method takes WordNet sense assignments 
and compares their distribution by reducing 
them to a set of basic types. For instance, 
WordNet assigns to the noun "book" the 
following senses: 
1. publication 
2. product, production 
3. fact 
4. dramatic_composition, 
dramatic_work 
5. record 
6. section, subdivision 
7. journal 
At the top of the WordNet hierarchy these 
seven senses can be reduced to two basic 
types: the content that is being 
communicated and the medium of 
communication. We can arrive at 
systematically polysemous classes by 
investigating which other words share these 
same senses and are thus polysemous in the 
same way. For instance, the seven different 
senses that WordNet assigns to "book" can 
be reduced to two basic types: artifact and 
communication. We do this for each noun 
and then group them into classes according 
to their combination of basic types. Finally, 
by human introspection several classes were 
grouped together, because their members 
seemed sufficiently similar. 
Among the resulting classes are a number that 
are to be expected given the literature on 
systematic polysemy. For instance, the classes 
animal / food and plant / natural, product have 
been discussed widely. Other classes are less 
15 
expected, but seem quite intuitive. The class 
artifact / attribute / substance for instance 
includes a number of nouns ("chalk, charcoal, 
daub, fiber, fibre, tincture") that refer to an 
object hat is at the same time an artifact made 
of some substance and that is also an attribute. 
3 CoreLex-II 
Thereby following Apresjan's definition of 
systematic polysemy discussed above. 
3.2 The Algorithm 
The algorithm works 
example for nouns): 
as follows (for 
3.1 A More Flexible Approach 
The CoreLex database has been used and/or 
evaluated in a number of projects, leading to 
some criticisms of the approach in 
(Krymolowski and Roth 1998) (Peters, 
Peters and Vossen 1998) (Tomuro 1998) and 
in personal communication. Primarily it was 
argued that the choice of basic types is 
arbitrary and on too high a level. Systematic 
class discovery in the original approach is 
dependent on this set of basic types, which 
means that classes on lower levels are not 
captured at all. Further criticism arose on the 
arbitrariness (and inefficiency) of human 
intervention in grouping together esulting 
classes into more comprehensive ones based 
on the similarity of their members. 
In response to this, a new approach was 
formulated and implemented that addresses 
both these points. Comparison of sense 
distributions i now performed over synsets 
on all levels, not just over a small set on the 
top levels. In addition, similarity on sense 
distribution between words need no longer 
be complete (100%), as with the former 
approach. Instead, a threshold on similarity 
can be set that constraints a clustering 
algorithm for automatically grouping 
together words into systematic polysemous 
classes. (No human intervention to further 
group together resulting classes is required.) 
This approach took inspiration from the 
pioneering work by (Dolan 1994), but it is 
also fundamentally different, because 
instead of grouping similar senses together, 
the CoreLex approach groups together 
words according to all of their senses. 
1. foreach noun 
2. get al levell synsets (senses) 
3. if number of level1 synsets > 1 
then put noun in list 
4. foreaeh level1 synset 
5. get al higher level synsets (hypernyms) 
6. foreaeh nouna in list 
7. foreaeh noun2 in list 
8. compute similarity nounx and nounz 
9. if similarity > threshold 
then put nouns and nounz in matrix 
10. foreaeh nounl in matrix 
11. if noun1 not assigned to a cluster 
then construct a new cluster Ci and 
assign noun1 to it 
12. foreaeh noun2 similar to nounl 
13. if nounz not assigned to a cluster 
then assign nounz to new cluster Ci 
For every noun in the WordNet or 
GermaNet index, get al of its senses (which 
are in fact level1 synsets). If a noun has more 
than one sense put it in a separate list that 
will be used for further processing. Nouns 
with only one sense are not used in further 
processing because we are only interested in
systematic distributions of more than one 
sense over several nouns. In order to 
compare nouns not only on the sense level 
but rather over the whole of the WordNet 
hierarchy, also all higher level synsets 
(hypernyms) for each sense are stored. 
Then, for each noun we compare its "sense 
distribution" (the complete set of synsets 
derived in the previous steps) with each 
other noun. Similarity is computed using the 
Jaccard score, which compares objects 
16 
according to the attributes they share and 
their unique attributes. If the similarity is 
over a certain threshold, the noun pair is 
stored in a matrix which is consequently 
used in a final clustering step. 
Finally, the clustering itself is a simple, 
single link algorithm that groups together 
objects uniquely in discrete clusters. 
3.3 Quantitative and Qualitative 
Analysis 
Depending on the threshold on similarity, 
the algorithm generates a number of clusters 
of ambiguous words that share similar sense 
distributions, and which can be seen as 
systematic polysemous classes. In the 
following table an overview is given of 
results with different hresholds. The 
number of nouns in WordNet that were 
processed is 46.995, of which 10.772 have 
more than one sense. 
Threshold Number of Ambiguos 
Clusters Nouns in 
(Systematic Clusters 
Polysemous (Systematic 
Classe)s Polysemous 
Nouns) 
0,70 1.793 4.391 
0,75 1.341 3.336 
0,80 1.002 2.550 
0,90 649 1.449 
A qualitative analysis of the clusters shows 
that best results are obtained with a 
threshold of 0,75. Some of the resulting 
clusters with this threshold are: 
? ball/game 
baseball, basketball, 
handball, volleyball 
football, 
fish / food 
albacore, blowfish, bluefin, bluefish, 
bonito, bream, butterfish, crappie, 
croaker, dolphinfish, flatfish, 
flounder, grouper, halibut, lingcod, 
mackerel, mahimahi, mullet, 
muskellunge, pickerel, pompano, 
porgy, puffer, rockfish, sailfish, scup, 
striper, swordfish, tuna, tunny, 
weakfish 
? plant/nut 
almond, butternut, candlenut, cashew, 
chinquapin, chokecherry, cobnut, 
filbert, hazelnut, pistachio 
? plant / berry 
bilberry, blueberry, checkerberry, 
cowberry, cranberry, currant, feijoa, 
gooseberry, huckleberry, juneberry, 
lingonberry, serviceberry, spiceberry; 
teaberry, whortleberry 
? vessel \] measure 
bottle, bucket, cask, flask, jug, keg, 
pail, tub 
? cord / fabric 
chenille, lace, laniard, lanyard, 
ripcord, whipcord, worsted 
? taste_property/sensation 
acridity, aroma, odor, odour, 
pungency 
? communication / noise 
clamor, hiss, howl, roar, roaring, 
screaming, screech, screeching, shriek, 
sigh, splutter, sputter, whisper 
4 Application 
Systematic polysemous classes that are 
obtained in this way can be used as filters on 
sense disambiguation in a variety of 
applications in which a coarse grained sense 
assignment will suffice in many cases, but 
where an option of further specification 
exists. For instance, in information retrieval 
17 
it will not always be necessary to distinguish 
between the two interpretations of "baseball, 
,,2 basketball, football . . . . .  Users looking for 
information on a baseball-game may be 
interested also in baseball-balls. On the 
other hand, a user may be interested 
specifically in buying a new baseball-ball 
and does not wish to be flooded with 
irrelevant information on baseball-games. In 
this case, the underspecified ball / game 
sense needs to be further specified in the ball 
sense only. Similarly, it will not always be 
necessary to distinguish exactly between the 
vessel interpretation f "bottle, bucket, cask, 
..." and the measure interpretation, or 
between the communication i terpretation f
a "clamor, hiss, roar, ..." and the noise 
interpretation. 
Currently, a query expansion module based 
on the approach described here is under 
development as part of the prototype 
systems of two EU funded projects: 
MIETTA 3 (a cross-lingual search engine in 
the tourism domain - Buitelaar et al1998) 
and OLIVE 4 (a cross-lingual video retrieval 
system). 
Also in shallow processing applications like 
semantic pre-processing for document 
categorization it will be sufficient o use an 
underspecified sense instead of needless 
disambiguation between senses that are 
roughly equal in their relevance to a certain 
document category. Similarly, in shallow 
syntactic processing tasks, like statistical 
disambiguation f PP-attachment, the use of 
underspecified senses may be preferable as 
shown in experiments by (Krymolowski and 
Roth 1998). 
2 Compare (SchUtze 1997) for a similar, but purely 
statistical approach to underspecification i lexical 
semantic processing and its use in machine learning 
and information retrieval. 
3 http://www.mietta.net/mietta 
4 http:lltwentyone.tpd.tno.nllolivel 
In order to train systems to accurately 
perform syntactic analysis on the basis of 
semantic classes, semantically annotated 
corpora are needed. This is another area of 
application of the research described here. 
CoreLex clusters can be considered by 
annotators as alternatives to WordNet or 
GermaNet synsets if they are not able to 
choose between the senses given and instead 
prefer an underspecified sense. This 
approach is currently tested, in cooperation 
with the GermaNet group of the University 
of Ttibingen, in a preliminary project on 
semantic annotation of German newspaper 
text. 
Conclusion 
We presented a new algorithm for 
generating systematic polysemous classes 
from existing resources like WordNet and 
similar semantic databases. Results were 
discussed for classes of English nouns as 
generated from WordNet. With a threshold 
of 75% similarity between nouns, 1341 
classes could be found covering 3336 nouns. 
Not discussed were similar experiments for 
verbs and adjectives, both in English and 
German. The resulting classes can be used 
as filters on incremental sense 
disambiguation i  various applications in 
which coarse grained (underspecified) 
senses are preferred, but from which more 
fine grained senses can be derived on 
demand. 
References 
J. Apresjan (1973) Regular Polysemy. Linguistics, 
142. 
Paul Buitelaar (1998a) CoreLex: Systematic 
Polysemy and Underspecification. PhD Thesis, 
Brandeis University. 
Paul Buitelaar (1998b) CoreLex: An Ontology of 
Systematic Polysemous Classes. In: Formal 
Ontology in Information Systems. IOS Press, 
Amsterdam. 
18 
Paul Buitelaar, Klaus Netter and Feiyu Xu (1998) 
Integrating Different Strategies In Cross-Language 
Information Retrieval in the MIETTA Project. In: 
Proceedings of TWLT14, Enschede, the 
Netherlands, December. 
D. A. Cruse (1986) Lexical Semantics. Cambridge 
University Press. 
Bill Dolan (1994) Word Sense Ambiguation: 
Clustering Related Senses. In: Proceedings of 
COLING-94. Kyoto, Japan. 
Birgit Hamp and Helmut Feldweg (1997) GermaNet- 
a Lexical Semantic Net for German. In: 
Proceedings of the ACL Workshop on Automatic 
Information Extraction and Building of Lexieal 
Semantic Resources for NLP Applications. 
Madrid,. 
G. Hirst (1987) Semantic Interpretation and the 
Resolution of Ambiguity. Cambridge University 
Press. 
Yuval Krymolowski and Dan Roth (1998) 
Incorporating Knowledge in Natural Language 
Learning: A Case Study. In: Proceedings ACL-98 
Workshop on the Use of WordNet in NLP. 
G. A. Miller and R. Beckwith and Ch. Fellbaum and 
D. Gross and K. Miller (1990) Introduction to 
WordNet: An On-line Lexical Database. 
International Journal of Lexicography, 3,4. 
Wim Peters, Ivonne Peters and Piek Vossen (1998) 
Automatic Sense Clustering in EuroWordNet. In: 
Proceedings of LREC. Granada. 
James Pustejovsky (1995) The Generative Lexicon. 
MIT Press. 
Hinrich SchiRze (1997) Ambiguity Resolution in 
Language Learning. Volume 71 of CSLI 
Publications. Chicago University Press. 
S. Small (1981) Viewing Word Expert Parsing as 
Linguistic Theory. In: Proceedings of IJCAI. 
Noriko Tomuro (1998) Semi-Automatic Induction of 
Systematic Polysemy from WordNet. In: 
Proceedings ACL-98 Workshop on the Use of 
WordNet in NLP. 
Uriel Weinreich (1964) Webster's Third: A Critique 
of its Semantics. International Journal of American 
Linguistics, 405-409, 30. 
Yorick Wilks (1999) Is Word Sense Disambiguation 
just one more NLP task? Cs.CL/9902030. 
19 
Unsupervised Monolingual and Bilingual Word-Sense
Disambiguation of Medical Documents using UMLS
Dominic Widdows, Stanley Peters, Scott Cederberg, Chiu-Ki Chan
Stanford University, California
{dwiddows,peters,cederber,ckchan}@csli.stanford.edu
Diana Steffen
Consultants for Language Technology,
Saarbru?cken, Germany
steffen@clt-st.de
Paul Buitelaar
DFKI, Saarbru?cken, Germany
paulb@dfki.de
Abstract
This paper describes techniques for unsu-
pervised word sense disambiguation of En-
glish and German medical documents us-
ing UMLS. We present both monolingual
techniques which rely only on the structure
of UMLS, and bilingual techniques which
also rely on the availability of parallel cor-
pora. The best results are obtained using
relations between terms given by UMLS,
a method which achieves 74% precision,
66% coverage for English and 79% preci-
sion, 73% coverage for German on evalua-
tion corpora and over 83% coverage over the
whole corpus. The success of this technique
for German shows that a lexical resource
giving relations between concepts used to
index an English document collection can
be used for high quality disambiguation in
another language.
1 Introduction
This paper reports on experiments in monolingual
and multilingual word sense disambiguation (WSD)
in the medical domain using the Unified Medical
Language System (UMLS). The work described was
carried out as part of the MUCHMORE project 1 for
multilingual organisation and retrieval of medical in-
formation, for which WSD is particularly important.
The importance of WSD to multilingual applica-
tions stems from the simple fact that meanings repre-
sented by a single word in one language may be rep-
resented by multiple words in other languages. The
English word drug when referring to medically ther-
apeutic drugs would be translated as medikamente,
1http://muchmore.dfki.de
while it would be rendered as drogen when referring
to a recreationally taken narcotic substance of the
kind that many governments prohibit by law.
The ability to disambiguate is therefore essential
to the task of machine translation ? when translat-
ing from English to Spanish or from English to Ger-
man we would need to make the distinctions men-
tioned above and other similar ones. Even short of
the task of full translation, WSD is crucial to ap-
plications such as cross-lingual information retrieval
(CLIR), since search terms entered in the language
used for querying must be appropriately rendered in
the language used for retrieval. WSD has become a
well-established subfield of natural language process-
ing with its own evaluation standards and SENSE-
VAL competitions (Kilgarriff and Rosenzweig, 2000).
Methods for WSD can effectively be divided into
those that require manually annotated training data
(supervised methods) and those that do not (unsu-
pervised methods) (Ide and Ve?ronis, 1998). In gen-
eral, supervised methods are less scalable than unsu-
pervised methods because they rely on training data
which may be costly and unrealistic to produce, and
even then might be available for only a few ambigu-
ous terms. The goal of our work on disambiguation
in the MUCHMORE project is to enable the correct
semantic annotation of entire document collections
with all terms which are potentially relevant for or-
ganisation, retrieval and summarisation of informa-
tion. Therefore a decision was taken early on in the
project that we should focus on unsupervised meth-
ods, which have the potential to be scaled up enough
to meet our needs.
This paper is arranged as follows. In Section 2 we
describe the lexical resource (UMLS) and the cor-
pora we used for our experiments. We then describe
and evaluate three different methods for disambigua-
tion. The bilingual method (Section 3) takes ad-
vantage of our having a translated corpus, because
knowing the translation of an ambiguous word can
be enough to determine its sense. The collocational
method (Section 4) uses the occurence of a term in a
recognised fixed expression to determine its meaning.
UMLS relation based methods (Section 5) use rela-
tions between terms in UMLS to determine which
sense is being used in a particular instance. Other
techniques used in the MUCHMORE project in-
clude domain-specific sense selection (Buitelaar and
Sacaleanu, 2001), used to select senses appropri-
ate to the medical domain from a general lexical
resource, and instance-based learning, a machine-
learning technique that has been adapted for word-
sense disambiguation (Widdows et al, 2003).
2 Language resources used in these
experiments
2.1 Lexical Resource ? UMLS
The Unified Medical Language System (UMLS) is
a resource that contains linguistic, terminological
and semantic information in the medical domain.2
It is organised in three parts: Specialist Lexi-
con, MetaThesaurus and Semantic Network. The
MetaThesaurus contains concepts from more than
60 standardised medical thesauri, of which for our
purposes we only use the concepts from MeSH (the
Medical Subject Headings thesaurus). This decision
is based on the fact that MeSH is also available in
German. The semantic information that we use in
annotation is the so-called Concept Unique Identifier
(CUI), a code that represents a concept in the UMLS
MetaThesaurus. We consider the possible ?senses? of
a term to be the set of CUI?s which list this term
as a possible realisation. For example, UMLS con-
tains the term trauma as a possible realisation of the
following two concepts:
C0043251 Injuries and Wounds: Wounds
and Injuries: trauma: traumatic disorders:
Traumatic injury:
C0021501 Physical Trauma: Trauma
(Physical): trauma:
Each of these CUI?s is a possible sense of the term
trauma. The term trauma is therefore noted as am-
biguous, since it can be used to express more than
one UMLS concept. The purpose of disambiguation
is to find out which of these possible senses is ac-
tually being used in each particular context where
there term trauma is used.
2UMLS is freely available under license from
the United States National Library of Medicine,
http://www.nlm.nih.gov/research/umls/
CUI?s in UMLS are also interlinked to each other
by a number of relations. These include:
? ?Broader term? which is similar to the hyper-
nymy relation in WordNet (Fellbaum, 1998). In
general, x is a ?broader term? for y if every y is
also a (kind of) x.
? More generally, ?related terms? are listed, where
possible relationships include ?is like?, ?is clini-
cally associated with?.
? Cooccurring concepts, which are pairs of con-
cepts which are linked in some information
source. In particular, two concepts are regarded
as cooccurring if they have both been used to
manually index the same document in MED-
LINE. We will refer to such pairs of concepts
as coindexing concepts.
? Collocations and multiword expressions. For ex-
ample, the term liver transplant is included sep-
arately in UMLS, as well as both the terms liver
and transplant. This information can sometimes
be used for disambiguation.
2.2 The Springer Corpus of Medical
Abstracts
The experiments and implementations of WSD de-
scribed in this paper were all carried out on a par-
allel corpus of English-German medical scientific ab-
stracts obtained from the Springer Link web site.3
The corpus consists approximately of 1 million to-
kens for each language. Abstracts are from 41 medi-
cal journals, each of which constitutes a relatively ho-
mogeneous medical sub-domain (e.g. Neurology, Ra-
diology, etc.). The corpus was automatically marked
up with morphosyntactic and semantic information,
as described by S?pela Vintar et al (2002). In brief,
whenever a token is encountered in the corpus that is
listed as a term in UMLS, the document is annotated
with the CUI under which that term is listed. Ambi-
guity is introduced by this markup process because
the lexical resources often list a particular term as a
possible realisation of more than one concept or CUI,
as with the trauma example above, in which case
the document is annotated with all of these possible
CUI?s.
The number of tokens of UMLS terms included by
this annotation process is given in Table 1. The table
shows how many tokens were found by the annota-
tion process, listed according to how many possible
senses each of these tokens was assigned in UMLS (so
that the number of ambiguous tokens is the number
3http://link.springer.de/
Number of Senses 1 2 3 4
Before Disambiguation
English 223441 31940 3079 56
German 124369 7996 0 0
After Disambiguation
English 252668 5299 568 5
German 131302 1065 0 0
Table 1: The number of tokens of terms that have 1,
2, 3 and 4 possible senses in the Springer corpus
of tokens with more than one possible sense). The
greater number of concepts found in the English cor-
pus reflects the fact that UMLS has greater cover-
age for English than for German, and secondly that
there are many small terms in English which are ex-
pressed by single words which would be expressed
by larger compound terms in German (for exam-
ple knee + joint = kniegelenk). Table 1 also shows
how many tokens of UMLS concepts were in the an-
notated corpus after we applied the disambiguation
process described in Section 5, which proved to be
our most successful method. As can be seen, our
disambiguation methods resolved some 83% of the
ambiguities in the English corpus and 87% of the
ambiguities in the German corpus (we refer to this
proportion as the ?Coverage? of the method). How-
ever, this only measures the number of disambigua-
tion decisions that were made: in order to determine
how many of these decisions were correct, evaluation
corpora were needed.
2.3 Evaluation Corpora
An important aspect of word sense disambiguation is
the evaluation of different methods and parameters.
Unfortunately, there is a lack of test sets for evalu-
ation, specifically for languages other than English
and even more so for specific domains like medicine.
Given that our work focuses on German as well as
English text in the medical domain, we had to de-
velop our own evaluation corpora in order to test our
disambiguation methods.
Because in the MUCHMORE project we devel-
oped an extensive format for linguistic and semantic
annotation (S?pela Vintar et al, 2002) that includes
annotation with UMLS concepts, we could automat-
ically generate lists of all ambiguous UMLS types
(English and German) along with their token fre-
quencies in the corpus. Using these lists we selected a
set of 70 frequent types for English (token frequencies
at least 28, 41 types having token frequencies over
100). For German, we only selected 24 ambiguous
types (token frequencies at least 11, 7 types having
token frequencies over 100) because there are fewer
ambiguous terms in the German annotation (see Ta-
ble 1). We automatically selected instances to be
annotated using a random selection of occurrences if
the token frequency was higher than 100, and using
all occurrences if the token frequency was lower than
100. The level of ambiguity for these UMLS terms is
mostly limited to only 2 senses; only 7 English terms
have 3 senses.
Correct senses of the English tokens in context
were chosen by three medical experts, two native
speakers of German and one of English. The Ger-
man evaluation corpus was annotated by the two
German speakers. Interannotator agreement for in-
dividual terms ranged from very low to very high,
with an average of 65% for German and 51% for En-
glish (where all three annotators agreed). The rea-
sons for this low score are still under investigation.
In some cases, the UMLS definitions were insufficient
to give a clear distinction between concepts, espe-
cially when the concepts came from different origi-
nal thesauri. This allowed the decision of whether
a particular definition gave a meaningful ?sense? to
be more or less subjective. Approximately half of
the disagreements between annotators occured with
terms where interannotator agreement was less than
10%, which is evidence that a significant amount of
the disagreement between annotators was on the type
level rather than the token level. In other cases, it
is possible that there was insufficient contextual in-
formation provided for annotators to agree. If one of
the annotators was unable to choose any of the senses
and declared an instance to be ?unspecified?, this also
counted against interannotator agreement. What-
ever is responsible, our interannotator agreement fell
far short of the 88%-100% achieved in SENSEVAL
(Kilgarriff and Rosenzweig, 2000, ?7), and until this
problem is solved or better datasets are found, this
poor agreement casts doubt on the generality of the
results obtained in this paper.
A ?gold standard? was produced for the German
UMLS evaluation corpus and used to evaluate the
disambiguation of German UMLS concepts. The En-
glish experiments were evaluated on those tokens for
which the annotators agreed. More details and dis-
cussion of the annotation process is available in the
project report (Widdows et al, 2003).
In the rest of this paper we describe the techniques
that used these resources to build systems for word
sense disambiguation, and evaluate their level of suc-
cess.
3 Bilingual Disambiguation
The mapping between word-forms and senses differs
across languages, and for this reason the importance
of word-sense disambiguation has long been recog-
nised for machine translation. By the same token,
pairs of translated documents naturally contain in-
formation for disambiguation. For example, if in a
particular context the English word drugs is trans-
lated into French as drogues rather than medica-
ments, then the English word drug is being used
to mean narcotics rather than medicines. This ob-
servation has been used for some years on varying
scales. Brown et al (1991) pioneered the use of sta-
tistical WSD for translation, building a translation
model from one million sentences in English and
French. Using this model to help with translation
decisions (such as whether prendre should be trans-
lated as take or make), the number of acceptable
translations produced by their system increased by
8%. Gale et al (1992) use parallel translations to
obtain training and testing data for word-sense dis-
ambiguation. Ide (1999) investigates the information
made available by a translation of George Orwell?s
Nineteen Eighty-four into six languages, using this
to analyse the related senses of nine ambiguous En-
glish words into hierarchical clusters.
These applications have all been case studies of a
handful of particularly interesting words. The large
scale of the semantic annotation carried out by the
MUCHMORE project has made it possible to extend
the bilingual disambiguation technique to entire dic-
tionaries and corpora.
To disambiguate an instance of an ambiguous
term, we consulted the translation of the abstract
in which it appeared. We regarded the translated
abstract as disambiguating the ambiguous term if it
met the following two criteria:
? Only one of the CUI?s was assigned to any term
in the translated abstract.
? At least one of the terms to which this CUI
was assigned in the translated abstract was un-
ambiguous (i.e. was not also assigned another
CUI).
3.1 Results for Bilingual Disambiguation
We attempted both to disambiguate terms in the
German abstracts using the corresponding English
abstracts, and to disambiguate terms in the English
abstracts using the corresponding German ones. In
this collection of documents, we were able to disam-
biguate 1802 occurrences of 63 English terms and
1500 occurrences of 43 German terms. Comparing
this with the evaluation corpora gave the results in
Table 2.4
4In all of the results presented in this paper, Precision
is the proportion of decisions made which were correct
Precision Recall Coverage
English 81% 18% 22%
German 66% 22% 33%
Table 2: Results for bilingual disambiguation
As can be seen, the recall and coverage of this
method is not especially good but the precision (at
least for English) is very high. The German results
contain roughly the same proportion of correct deci-
sions as the English, but many more incorrect ones
as well.
Our disambiguation results break down into three
cases:
1. Terms ambiguous in one language that translate
as multiple unambiguous terms in the other lan-
guage; one of the meanings is medical and the
other is not.
2. Terms ambiguous in one language that trans-
late as multiple unambiguous terms in the other
language; both of the terms are medical.
3. Terms that are ambiguous between two mean-
ings that are difficult to distinguish.
One striking aspect of the results was that rel-
atively few terms were disambiguated to different
senses in different occurrences. This phenomenon
was particularly extreme in disambiguating the Ger-
man terms; of the 43 German terms disambiguated,
42 were assigned the same sense every time we were
able to disambiguate them. Only one term, Metas-
tase, was assigned difference senses; 88 times it was
assigned CUI C0027627 (?The spread of cancer from
one part of the body to another ...?, associated with
the English term Metastasis and 6 times it was as-
signed CUI C0036525 ?Used with neoplasms to in-
dicate the secondary location to which the neoplas-
tic process has metastasized?, corresponding to the
English terms metastastic and secondary). Metas-
tase therefore falls into category 2 from above, al-
though the distinction between the two meanings is
relatively subtle.
The first and third categories above account for
the vast majority of cases, in which only one mean-
ing is ever selected. It is easy to see why this would
according to the evaluation corpora, Recall is the pro-
portion of instances in the evaluation corpora for which
a correct decision was made, and Coverage is the propor-
tion of instances in the evaluation corpora for which any
decision was made. It follows that
Recall = Precision ? Coverage.
happen in the first category, and it is what we want
to happen. For instance, the German term Krebse
can refer either to crabs (Crustaceans) or to cancer-
ous growths; it is not surprising that only the latter
meaning turns up in the corpus under consideration
and that we can determine this from the unambigu-
ous English translation cancers.
In English somewhat more terms were disam-
biguated multiple ways: eight terms were assigned
two different senses across their occurrences. All
three types of ambiguity were apparent. For in-
stance, the second type (medical/medical ambiguity)
appeared for the term Aging, which can refer either
to aging people (Alte Menschen) or to the process of
aging itself (Altern); both meanings appeared in our
corpus.
In general, the bilingual method correctly find the
meanings of approximately one fifth of the ambigu-
ous terms, and makes only a few mistakes for English
but many more for German.
4 Collocational disambiguation
By a ?collocation? we mean a fixed expression formed
by a group of words occuring together, such as
blood vessel or New York. (For the purposes of
this paper we only consider contiguous multiword
expressions which are listed in UMLS.) There is a
strong and well-known tendency for words to ex-
press only one sense in a given collocation. This
property of words was first described and quantified
by Yarowsky (1993), and has become known gen-
erally as the ?One Sense Per Collocation? property.
Yarowsky (1995) used the one sense per collocation
property as an essential ingredient for an unsuper-
vised Word-Sense Disambiguation algorithm. For ex-
ample, the collocations plant life and manufacturing
plant are used as ?seed-examples? for the living thing
and building senses of plant, and these examples can
then be used as high-precision training data to per-
form more general high-recall disambiguation.
While Yarowsky?s algorithm is unsupervised (the
algorithm does not need a large collection of anno-
tated training examples), it still needs direct human
intervention to recognise which ambiguous terms are
amenable to this technique, and to choose appropri-
ate ?seed-collocations? for each sense. Thus the algo-
rithm still requires expert human judgments, which
leads to a bottleneck when trying to scale such meth-
ods to provide Word-Sense Disambiguation for a
whole document collection.
A possible method for widening this bottleneck is
to use existing lexical resources to provide seed collo-
cations. The texts of dictionary definitions have been
used as a traditional source of information for disam-
biguation (Lesk, 1986). The richly detailed structure
of UMLS provides a special opportunity to combine
both of these approaches, because many multiword
expressions and collocations are included in UMLS
as separate concepts.
For example, the term pressure has the following
three senses in UMLS, each of which is assigned to a
different semantic type (TUI):
Sense of pressure Semantic Type
Physical pressure Quantitative Concept
(C0033095)
Pressure - action Therapeutic or
(C0460139) Preventive Procedure
Baresthesia, sensation
of pressure (C0234222)
Organ or Tissue Func-
tion
Many other collocations and compounds which in-
clude the word pressure are also of these semantic
types, as summarised in the following table:
Quantitative
Concept
mean pressure, bar pressure,
population pressure
Therapeutic
Procedure
orthostatic pressure, acupres-
sure
Organ or Tissue
Function
arterial pressure, lung pres-
sure, intraocular pressure
This leads to the hypothesis that the term pres-
sure, when used in any of these collocations, is used
with the meaning corresponding to the same seman-
tic type. This allows deductions of the following
form:
Collocation bar pressure, mean pressure
Semantic type Quantitative Concept
Sense of pressure C0033095, physical pressure
Since nearly all English and German multiword
technical medical terms are head-final, it follows that
the a multiword term is usually of the same seman-
tic type as its head, the final word. (So for example,
lung cancer is a kind of cancer, not a kind of lung.)
For English, UMLS 2001 contains over 800,000 multi-
word expressions the last word in which is also a term
in UMLS. Over 350,000 of these expressions have a
last word which on its own, with no other context,
would be regarded as ambiguous (has more that one
CUI in UMLS). Over 50,000 of these multiword ex-
pressions are unambiguous, with a unique semantic
type which is shared by only one of the meanings of
the potentially ambiguous final word. The ambigu-
ity of the final word in such multiword expressions
is thus resolved, providing over 50,000 ?seed colloca-
tions? for use in semantically annotating documents
with disambiguated word senses.
4.1 Results for collocational disambiguation
Unfortunately, results for collocational disambigua-
tion (Table 3) were disappointing compared with the
promising number of seed collocations we expected
to find. Precision was high, but comparatively few
of the collocations suggested by UMLS were found
in the Springer corpus.
Precision Recall Coverage
English 79% 3% 4%
German 82% 1% 1.2%
Table 3: Results for collocational disambiguation
In retrospect, this may not be surprising given that
many of the ?collocations? in UMLS are rather col-
lections of words such as
C0374270 intracoronary percutaneous
placement s single stent transcatheter vessel
which would almost never occur in natural text.
Thus very few of the potential collocations we ex-
tracted from UMLS actually occurred in the Springer
corpus. This scarcity was especially pronounced for
German, because so many terms which are several
words in English are compounded into a single word
in German. For example, the term
C0035330 retinal vessel
does occur in the (English) Springer corpus and con-
tains the ambiguous word vessel, whose ambiguity is
successfully resolved using the collocational method.
However, in German this concept is represented by
the single word
C0035330 Retinagefaesse
and so this ambiguity never arises in the first place.
It should still be remarked that the few decisions
that were made by the collocational method were
very accurate, demonstrating that we can get some
high precision results using this method. It is pos-
sible that recall could be improved by relaxing the
conditions which a multiword expression in UMLS
must satisfy to be used as a seed-collocation.
5 Disambiguation using related
UMLS terms found in the same
context
While the collocational method turned out to give
disappointing recall, it showed that accurate infor-
mation could be extracted directly from the existing
UMLS and used for disambiguation, without extra
human intervention or supervision. What we needed
was advice on how to get more of this high-quality
information out of UMLS, which we still believed to
be a very rich source of information which we were
not yet exploiting fully. Fortunately, no less than 3
additional sources of information for disambiguation
using related terms from UMLS were suggested by a
medical expert.5 The suggestion was that we should
consider terms that were linked by conceptual rela-
tions (as given by the MRREL and MRCXT files
in the UMLS source) and which were noted as coin-
dexing concepts in the same MEDLINE abstract (as
given by the MRCOC file in the UMLS source). For
each separate sense of an ambiguous word, this would
give a set of related concepts, and if examples of any
of these related concepts were found in the corpus
near to one of the ambiguous words, it might indi-
cate that the correct sense of the ambiguous word
was the one related to this particular concept.
This method is effectively one of the many variants
of Lesk?s (1986) original dictionary-based method for
disambiguation, where the words appearing in the
definitions of different senses of ambiguous words are
used to indicate that those senses are being used if
they are observed near the ambiguous word. How-
ever, we gain over purely dictionary-based methods
because the words that occur in dictionary defini-
tions rarely correspond well with those that occur
in text. The information we collected from UMLS
did not suffer from this drawback: the pairs of coin-
dexing concepts from MRCOC were derived precisely
from human judgements that these two concepts
both occured in the same text in MEDLINE.
The disambiguation method proceeds as follows.
For each ambiguous word w, we find its possible
senses {sj(w)}. For each sense sj , find all CUI?s
in MRREL, MRCXT or MRCOC files that are re-
lated to this sense, and call this set {crel(sj)}. Then
for each occurrence of the ambiguous word w in the
corpus we examine the local context to see if a term
t occurs whose sense6 (CUI) is one of the concepts
in {crel(sj)}, and if so take this as positive evidence
that the sense sj is the appropriate one for this con-
text, by increasing the score of sj by 1. In this way,
each sense sj in context gets assigned a score which
measures the number of terms in this context which
are related to this sense. Finally, choose the sense
5Personal communication from Stuart Nelson (instru-
mental in the design of UMLS), at the MUCHMORE
workshop in Croatia, September 2002.
6This fails to take into account that the term t might
itself be ambiguous ? it is possible that results could be
improved still further by allowing for mutual disambigua-
tion of more than one term at once.
with the highest score.
One open question for this algorithm is what re-
gion of text to use as a context-window. We experi-
mented with using sentences, documents and whole
subdomains, where a ?subdomain? was considered to
be all of the abstracts appearing in one of the jour-
nals in the Springer corpus, such as Arthroskopie
or Der Chirurg. Thus our results (for each lan-
guage) vary according to which knowledge sources
were used (Conceptually Related Terms from MR-
REL and MRCXT or coindexing terms from MR-
COC, or a combination), and according to whether
the context-window for recording cooccurence was a
sentence, a document or a subdomain.
5.1 Results for disambiguation based on
related UMLS concepts
The results obtained using this method (Tables 5.1
and 5.1) were excellent, preserving (and in some
cases improving) the high precision of the bilingual
and collocational methods while greatly extending
coverage and recall. The results obtained by using
the coindexing terms for disambiguation were partic-
ularly impressive, which coincides with a long-held
view in the field that terms which are topically re-
lated to a target word can be much richer clues for
disambiguation that terms which are (say) hierarchi-
cally related. We are very fortunate to have such
a wealth of information about the cooccurence of
pairs of concepts through UMLS, which appears to
have provided the benefits of cooccurence data from
a manually annotated training sample without hav-
ing to perform the costly manual annotation.
In particular, for English (Table 5.1), results were
actually better using only coindexing terms rather
than combining this information with hierarchically
related terms: both precision and recall are best
when using only the MRCOC knowledge source. As
we had expected, recall and coverage increased but
precision decreased slightly when using larger con-
texts.
The German results (Table 5.1) were slightly dif-
ferent, and even more successful, with nearly 60% of
the evaluation corpus being correctly disambiguated,
nearly 80% of the decisions being correct. Here, there
was some small gain when combining the knowledge
sources, though the results using only coindexing
terms were almost as good. For the German experi-
ments, using larger contexts resulted in greater recall
and greater precision. This was unexpected ? one
hypothesis is that the sparser coverage of the German
UMLS contributed to less predictable results on the
sentence level.
These results are comparable with some of the bet-
ter SENSEVAL results (Kilgarriff and Rosenzweig,
2000) which used fully supervised methods, though
the comparison may not be accurate because we are
choosing between fewer senses than on avarage in
SENSEVAL, and because of the doubts over our in-
terannotator agreement.
Comparing these results with the number of words
disambiguated in the whole corpus (Table 1), it is
apparent that the average coverage of this method is
actually higher for the whole corpus (over 80%) than
for the words in the evaluation corpus. It is possible
that this reflects the fact the the evaluation corpus
was specifically chosen to include words with ?inter-
esting? ambiguities, which might include words which
are more difficult than average to disambiguate. It is
possible that over the whole corpus, the method ac-
tually works even better than on just the evaluation
corpus.
This technique is quite groundbreaking, because it
shows that a lexical resource derived almost entirely
from English data (MEDLINE indexing terms) could
successfully be used for automatic disambiguation in
a German corpus. (The alignment of documents and
their translations was not even considered for these
experiments so the results do not depend at all on
our having access to a parallel corpus.) This is be-
cause the UMLS relations are defined between con-
cepts rather than between words. Thus if we know
that there is a relationship between two concepts, we
can use that relationship for disambiguation, even if
the original evidence for this relationship was derived
from information in a different language from the
language of the document we are seeking to disam-
biguate. We are assigning the correct senses based
not upon how terms are related in language, but how
medical concepts are related to one another.
It follows that this technique for disambiguation
should be applicable to any language which UMLS
covers, and applicable at very little cost. This pro-
posal should stimulate further research, and not too
far behind, successful practical implementation.
6 Summary and Conclusion
We have described three implementations of unsu-
pervised word-sense disambiguation techniques for
medical documents. The bilingual method relies on
the availability of a translated parallel corpus: the
collocational and relational methods rely solely on
the structure of UMLS, and could therefore be ap-
plied to new collections of medical documents with-
out requiring any new resources. The method of
disambiguation using relations between terms given
by UMLS was by far the most successful method,
achieving 74% precision, 66% coverage for English
ENGLISH Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 50 14 28 60 9 15 78 32 41 74 32 43
Document 48 24 50 63 22 35 74 46 62 72 45 63
Subdomain 51 33 65 64 38 59 74 49 66 71 49 69
Table 4: Results for disambiguation based on UMLS relations (English)
GERMAN Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 64 24 38 75 11 15 76 29 38 77 31 40
Document 68 43 63 75 27 36 79 52 66 79 53 67
Subdomain 70 51 73 74 52 70 79 58 73 79 58 73
Table 5: Results for disambiguation based on UMLS relations (German)
and 79% precision, 73% coverage for German on the
evaluation corpora, and achieving over 80% coverage
overall. This result for German is particularly en-
couraging, because is shows that a lexical resource
giving relations between concepts in one language
can be used for high quality disambiguation in an-
other language.
Acknowledgments
This research was supported in part by the Re-
search Collaboration between the NTT Communi-
cation Science Laboratories, Nippon Telegraph and
Telephone Corporation and CSLI, Stanford Univer-
sity, and by EC/NSF grant IST-1999-11438 for the
MUCHMORE project.
We would like to thank the National Library of
Medicine for providing the UMLS, and in particular
Stuart Nelson for his advice and guidance.
References
P. Brown, S. de la Pietra, V. de la Pietra, and R Mer-
cer. 1991. Word sense disambiguation using sta-
tistical methods. In ACL 29, pages 264?270.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Rank-
ing and selecting synsets by domain relevance. In
Proceedings of WordNet and Other Lexical Re-
sources, NAACL 2001 Workshop, Pittsburgh, PA,
June.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge MA.
W. Gale, K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the Humanities, 26:415?
439.
Nancy Ide and Jean Ve?ronis. 1998. Introduction
to the special issue on word sense disambiguation:
The state of the art. Computational Linguistics,
24(1):1?40, March.
Nancy Ide. 1999. Parallel translations and
sense discriminators. In Proceedings of the ACL
SIGLEX workshop on Standardizing Lexical Re-
sources, pages 52?61.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for english senseval. Computers and
the Humanities, 34(1-2):15?48, April.
M. E. Lesk. 1986. Automated sense disambiguation
using machine-readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the SIGDOC conference. ACM.
S?pela Vintar, Paul Buitelaar, Ba?rbel Ripplinger,
Bogdan Sacaleanu, Diana Raileanu, and Detlef
Prescher. 2002. An efficient and flexible format
for linguistic and semantic annotation. In Third
International Language Resources and Evaluation
Conference, Las Palmas, Spain.
Dominic Widdows, Diana Steffen, Scott Ceder-
berg, Chiu-Ki Chan, Paul Buitelaar, and Bog-
dan Sacaleanu. 2003. Methods for word-sense
disambiguation. Technical report, MUCHMORE
project report.
David Yarowsky. 1993. One sense per collocation.
In ARPA Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pages
189?196.
Towards Metadata Interoperability 
Peter Wittenburg 
MPI for Psycholinguistics 
Wundtlaan 1 
6525 XD Nijmegen, Netherlands 
Peter.Wittenburg@mpi.nl 
Daan Broeder 
MPI for Psycholinguistics 
Wundtlaan 1 
6525 XD Nijmegen, Netherlands 
Daan.Broeder@mpi.nl 
Paul Buitelaar 
DFKI 
Stuhlsatzenhausweg 3 
D-66123 Saarbr?cken 
paulb@dfki.de 
 
Abstract 
Within two European projects metadata 
interoperability is one of the central top-
ics. While the INTERA project has as 
one of its goals to achieve an interopera-
bility between two widely used metadata 
sets for the domain of language re-
sources, the ECHO project created an in-
tegrated metadata domain of in total nine 
data providers from five different disci-
plines from the humanities. In both pro-
jects ad hoc techniques are used to 
achieve results. In the INTERA project, 
however, machine readable and ISO 
compliant concept definitions are created 
as a first step towards the Semantic Web. 
In the ECHO project a complex ontology 
was realized purely relying on XML. It is 
argued that concept definitions should be 
registered in open Data Category Reposi-
tories and that relations between them 
should be described as RDF assertions. 
Yet we are missing standards that would 
allow us to overcome the ad hoc solu-
tions. 
1 Introduction 
Metadata is a key source of information towards 
realization of the Semantic Web that could be 
exploited in many different ways. Several pro-
jects are starting to focus on exploiting rich 
metadata in and between projects and disciplines. 
For instance, the ECHO (European Cultural 
Heritage Online)1 project brings together meta-
data for resources from the History of Arts, His-
tory of Science, Linguistics, Ethnology and 
Philosophy. One aspect of the work in ECHO is 
to create a cross-disciplinary domain for resource 
discovery. In the INTERA (Integrated European 
Language Resource Area)2 project one of the 
                                                           
1 ECHO: http://www.mpi.nl/echo 
2 INTERA: http://www.elda.fr/rubrique22.html 
tasks is to establish a foundation for a more 
flexible definition and use of metadata for lan-
guage resources. 
 
We can distinguish two types of metadata. The 
first one concerns its use as ?data about data?. 
This definition of metadata includes for example 
text that describes images, sounds, videos and 
other texts. Such metadata can exist in different 
forms like complex annotations of media re-
cordings as discussed for example by Bird (2001) 
and Brugman (2001). A second type of metadata 
consists of keywords describing objects that form 
the catalogues of the increasingly large digital 
collections, e.g., of linguistic data. This type of 
metadata was introduced by initiatives such as 
Dublin Core3 for general type web-resources, 
OLAC4 for general type linguistic resources and 
IMDI5 for more elaborate linguistic resource de-
scriptions that are useful not only for discovery 
but also for management purposes.  
 
Although the first type of metadata is very im-
portant for the above mentioned use in content 
descriptions, in this paper we will focus on as-
pects that are related to the second, keyword type 
of metadata. It is obvious that this type of meta-
data  
? contains amongst others important informa-
tion about a resource that cannot be retrieved 
from its content; 
? are especially relevant for the discovery and 
management of multimedia resources since 
speech and image recognition are still far 
away from being applicable in most cases; 
? includes a reduced set of descriptive ele-
ments and requires classification such that 
content information in many cases is richer; 
? offers a limited set of semantically well-
defined data categories (ISO 12620) that can 
be related with other concepts. 
 
                                                           
3 Dublin Core: http://dublincore.org 
4 OLAC: http://www.language-archives.org 
5 IMDI: http://www.mpi.nl/IMDI 
In this paper we will describe the problems that 
we encountered in the INTERA and the ECHO 
projects to come to interoperable metadata do-
mains, the structural and semantic solutions that 
were chosen to solve the tasks and the solutions 
we are aiming at in the long run. In this context 
we will also refer to the intentions within ISO 
TC37/SC46. 
2 Current tasks 
The INTERA task 
One focus of the work in the INTERA project is 
on the integration of metadata elements that are 
used in describing language resources for open 
data category repositories. Two metadata sets are 
being used currently for the discovery and man-
agement of language resources. The OLAC set is 
used for discovery purposes and aims to be used 
for all kinds of language resources. The set was 
derived from the Dublin Core set, i.e., on pur-
pose it only includes a limited set of elements.  
 
The IMDI set was designed bottom-up and is 
used for discovery and management purposes. It 
is a rich and structured set especially derived for 
annotated resources and lexica. The distributed 
IMDI domain was extended in the INTERA and 
ECHO projects to more than 27 participating 
European institutions sees itself as an OLAC data 
provider, i.e., the OLAC harvester can read all 
IMDI records that are offered via the Open Ar-
chives Initiative metadata harvesting protocol7 
(OAI MHP). A wrapper is used to map the IMDI 
elements to the OLAC elements, i.e., the map-
                                                           
6 ISO TC37/SC4: http://www.tc37sc4.org 
7 OAI MHP: http:// www.ukoln.ac.uk/cd-
focus/presentations/ cldprac/sld020.htm 
ping relations are hardwired into a server-based 
program.  
 
Recently, a new version of the IMDI metadata 
set (version 3.0.3) was provided. In parallel, also 
the new version of the OLAC metadata set (Au-
gust 2003) was worked out. Both metadata sets 
are described by human readable definition docu-
ments available in the web. New mapping rules 
have to be constructed which for short-term 
needs will again be hard-wired into a server-
based program.  
 
But this is not seen as being sufficient to serve 
future needs. New ways have to be developed for 
making the mapping more transparent and to pre-
pare the metadata domain for Semantic Web ap-
plications. Therefore, as a first step, the IMDI 
metadata concepts are entered into the open data 
category registry that is currently emerging 
within ISO TC37/SC4.  
The ECHO task 
In the ECHO project one of the tasks is to create 
a metadata domain that covers five disciplines 
and several institutions within each discipline. In 
total we were confronted with nine different 
metadata sets.  
 
The table below gives an overview of the meta-
data types that we were confronted with. One of 
the sets is DC compliant, two produce descrip-
tions that are close to DC, two provide true OAI 
compliance including the delivery of DC records. 
Most of the data is extracted from relational da-
tabases, encoding other types of data as well. In 
many cases the elements used were not well de-
fined, possibly leading to differences in usage by 
the metadata creators. 
 
Domain ? Sub-domain size Type MD 
Formal 
State 
Harvesting 
Type Comment 
HoA - Fotothek very large MIDAS Iconclass 
non 
validated XML export from a database 
HoA - Lineamenta small close to DC non val XML export from a database 
HoA ? Maps of Rome small self-defined non val XML export from a database 
HoS ? Berlin Collection large close to DC validated XML export from a database 
HoS ? IMSS pot large DC non val XML export from a database 
E ? Ethnology Museum 
Leiden RMV very large 
OMV 
OMV Thesaurus validated OAI export from a database 
E ? NECEP database small self defined validated XML export from a database 
L ? IMDI Domain large IMDI set validated XML/OAI true XML domain 
P ? Collection of Texts small self defined non val XML XML texts 
History of arts (HoA), History of Science (HoS), Linguistics (L), Ethnology (E), Phylosophy (P) 
Also the way in which the content of resources is 
described differs substantially. In Fotothek the 
IconClass thesaurus is used to categorize the con-
tent of photos and images. In the RMV catalogue 
the OVM thesaurus is used which is similar to 
the AAT thesaurus. Some use the subject field 
from the DC element set with all its weaknesses, 
others have an unconstrained keyword field and 
the elaborate IMDI set has a couple of elements 
that describe the content such as ?task?, ?genre?, 
?subgenre?, ?language? and ?modalities?.  
 
A variety of description options is used for the 
indication of geographic regions. In the RMV 
case a geographic thesaurus is used. Others use 
descriptors such as ?country? and ?region?. In 
some instances language names have to be used 
to indicate a geographical overlap. 
 
When creating an interoperable metadata domain 
one has to cope with problems at each layer: 
character encoding, data harvesting, syntactical 
aspects and semantic integration. Only the last 
point is of relevance in the context of this paper.  
 
To enable semantic integration an ontology was 
built that covers  
 
? nine metadata repositories; 
? a file where all metadata concepts rele-
vant for the integrated domain ECHO 
domain are listed including their descrip-
tion in a number of major languages (the 
setup is similar to the one used within 
ISO TC37/SC4); 
? a file that includes all mappings between 
these concepts where each individual set 
presents a view that is mapped to all oth-
ers; 
? two geographic thesauri containing dif-
ferent types of geographic information 
with cross-links between them; 
? two category thesauri describing the con-
tent of the resources; 
? two mapping files containing one-
directional cross-links between the two 
thesauri; 
? a file that contains all content type of de-
scriptions that occur in the metadata re-
cords and which do not use one of the 
big thesauri with mappings to these two. 
 
As we are currently using the existing files sim-
ply as exchange formats they have been repre-
sented in XML (rather than RDF for instance). 
To implement fast search, specially optimized 
internal representations are chosen and combined 
with fast indexes. The representations are such 
that all occurring references are expanded in 
preparation time and not during execution time. 
A special engine was programmed that can oper-
ate on these extended representations. 
 
To illustrate this we use an example with geo-
graphic thesaurus information. A search for 
?Country=Italy? should result in hits for all ob-
jects that have to do with ?Italy? either as the 
creation site or as the site where the scene takes 
place. The metadata records are now extended 
such that for all locations that are within ?Italy? 
the nodes appearing higher up in the thesaurus 
hierarchy are added. This assures that a record 
containing for example ?Rome? will also be in-
dicated as a hit when ?Italy? was entered in the 
query. 
 
Exploiting all repositories during run-time by 
intelligent crawlers would require fast parallel 
algorithms. Only parallelism would yield the 
execution speed needed to satisfy the users.  
Relation types 
We have discovered different types of relations 
between the concepts used in the INTERA and 
ECHO projects.  
 
In the INTERA project we can indicate internal 
relations within the structured IMDI metadata 
set, i.e., structure conveys semantic relations. An 
example can be given by the many attributes of a 
participant. A certain participant has a ?name? as 
an identifier and various attributes such as ?age?, 
?role? and ?education?. Between the IMDI and 
OLAC concepts there are three types of relations: 
(1) For some concepts one can speak of equality 
and it was agreed that the controlled vocabularies 
will be unified where possible. (2) There are also 
hierarchical relations such as ?subClass? and 
?superClass? between some of the concepts. (3) 
There is a type of relation where we can speak 
about a semantic overlap that we cannot specify 
in more detail. Finally, there are concepts such as 
?age? or ?education? of a participant that do not 
map at all.  
 
For the mappings in ECHO we have identified 
four useful types of relations: (1) ?isEqualTo? 
defines semantic equivalence, (2) ?isSubclassOf? 
defines a hyponymy relation, (3) ?isSuperclas-
sOf? defines the inverse and (4) ?mapsTo? is 
used to express a semantic overlap. In most 
cases, the ?mapsTo? relation type was used ? a 
one-directional relation indicating semantic over-
lap that should be exploitable. It is not clear yet 
in how far it makes sense to define the fuzzy 
?mapsTo? relation in terms of the standard types 
provided by RDF(S)8 and/or OWL9. All concepts 
that do not map to others or that are too special 
(for example ?size of an image?) were excluded 
in the ontology definition process. 
Examples from ECHO 
Using the described ECHO interoperability 
framework a number of experiments were carried 
out for evaluation purposes. A few examples will 
be discussed here. 
 
Example 1 
Simple Search ?dogon? 
 1 match was found: NECEP: 1 
Complex Search ?dogon? 
 View NECEP - society name: 1 in NECEP 
 View IMSS - Ianguage: 1 in NECEP 
 View DC - language: 1 in NECEP 
 View Language - language: 1 in NECEP 
Complex Search ?mali? 
 View Language - country: 1 in NECEP 
 
This example demonstrates the effect of the 
mapping between the metadata sets and of the 
geographical thesaurus. The language element is 
mapped to the society name element in NECEP 
although this is semantically not correct. Enter-
ing ?mali? in the country specification yields a 
hit since ?mali? is seen as a superclass to 
?dogon?. Here a relation type such as 
?has_language? would be semantically more ap-
propriate.  
 
Example 2 
Simple Search ?inuit? 
 2 matches are found: Language: 1, NECEP: 1 
Complex Search ?inuit? 
 View Language - *: 0 in Language (could not be  
found in the Language domain) 
 View Language ? language: 1 in NECEP 
Complex Search ?greenland? 
 View Language ? language: 1 in NECEP 
 
The results are similar compared to example 1. It 
indicates that the element including ?inuit? in the 
language domain is not an element that is used 
for mapping. It was used as avalue of an optional 
                                                           
8 RDF: http://www.w3.org/RDF 
9 OWL: http://www.w3.org/2001/sw/WebOnt 
element by one specific researcher. This example 
shows that simple search covering all metadata 
elements can lead to improved results. 
 
Example 3 
Simple Search ?agriculture? 
 75 matches are found: Language: 73, Fotothek: 2 
Complex Search ?agriculture? 
 View Fotothek - iconography: 2 in Fotothek  
 View RMV ? content: 2 in Fotothek 
 View IMDI ? content: 2 in Fotothek 
 
These results are misleading and demonstrate the 
weakness of simple search. The 73 hits for lan-
guage result from matching with the recording 
place (?southern agriculture kindergarten?) and 
the affiliation of an actor (?ministry of agricul-
ture?). These results obviously do not refer to 
documents the user was serching for. In the case 
of Fotothek the hits make sense since it is about 
?harvesting?. The mapping in complex leads to 
the expected results, the misleading hits from the 
language domain are not found.  
 
Example 4 
Simple Search ?clothing?  
 22 matches: Language: 8, RMV: 8, Fotothek: 6 
Complex Search ?clothing? 
 View RMV ? content: 8 in RMV, 6 in Fotothek 
 View Fotothek ? iconography: 8 in RMV, 6 in  
Fotothek 
 View Language ? content: 8 in RMV, 6 in  
Fotothek 
 
Again the rich annotations that are used in vari-
ous free-text fields in the language domain lead 
to wrong hits. They are about chats at the bakery 
shop and the clothes people are wearing ? so it?s 
not about clothing as an object which may be 
intended by the person specifying the search. The 
results for complex search from different do-
mains shows the correctness of the mappings.  
 
Example 5 
Simple Search ?horses? 
 7 matches: Fotothek: 2, Language: 2, IMSS: 3 
Complex Search ?horses? 
 View Fotothek ? object title: 3 in IMSS 
 View Fotothek ? iconography: 2 in Fotothek 
 View Lineamenta ? title: 3 in IMSS 
 View Lineamenta ? keywords: 2 in Fotothek 
 View IMSS ? title: 3 in IMSS 
 View IMSS ?subject: 2 in Fotothek 
 View Language ? title: 3 in IMSS 
 View Language ? content: 2 in Fotothek 
 
This example clearly indicates the strength of 
simple search and the weakness of complex 
search. The pattern used by complex search can 
be compared with a narrow path in the complex 
semantic space. If selecting the title element the 
hits of IMSS are found, if the content element is 
chosen the Fotothek hits are found. Both, how-
ever, are leading to useful hits where ?horses? 
are central concepts in the resources. The reason 
for the indicated results are partly caused by very 
sparsely encoded metadata. In the case of IMSS 
the term ?horses? is only mentioned in the title, 
the content element is yet not used. In the lan-
guage case thesaurus information is used to infer 
from the string found in the title element (?spa-
tial layout task, farm scenarios?) to ?horses?.  
Summary 
Only the first three relations (equality, hypo-
nomy, hyperonomy) can be used in a strictly 
logical way. The fourth relation type is of a fuzzy 
nature but occurs most frequently. To prevent a 
semantic cycle during searching, the specially 
tailored inference engine is restricted to one in-
ference step over this fuzzy relation and exploits 
all relations only in one direction10. It is evident 
that the existing ontology does not describe a 
complete logical system. 
 
In case of the INTERA project we will continue 
to rely on a wrapper that will map IMDI to 
OLAC records to allow OAI style of harvesting. 
In the ECHO project we created optimized in-
dexes such that searching can be executed fast, 
i.e., the knowledge components in XML are sim-
ply used as interchange formats allowing for the 
easy identification of all structural components 
and for their validation. 
3 Foundation for Metadata-
Interoperability 
In the previous sections we described the current 
state of the practical work in two projects to 
achieve semantic interoperability. The way cho-
sen has a number of disadvantages in the long-
run: 
 
? In the ECHO project there are no con-
cept definitions that adhere to open and 
emerging standards such as ISO 11179 
and ISO 12620, and which are available 
in validated machine-readable registries. 
                                                           
10 It should be noted, however, that advanced infer-
ence systems can handle semantic cycles of this na-
ture. 
? The current definitions do not contain 
hierarchical relations, which could be 
part of the concept definitions if agreed 
upon by the community.  
? A contribution from other experts, for 
example to improve the definitions and 
to add other language specific aspects, is 
largely excluded. 
? The representation of the semantic rela-
tions between concepts is partly encapsu-
lated in a program preventing any 
flexibility. In the ECHO case they are 
structurally described with the help of 
XML tags, however, it would be much 
better to provide them in a way that in-
ference engines relying on RDF(S) and 
OWL could operate on them.  
 
From the practical work we learned that often the 
semantic scope of the metadata elements is not 
specified as precisely as seems possible and also 
necessary. This will allow for a spectrum of us-
age that will have effects not only on human in-
terpretation, but especially on the way of 
mapping relations to chose. It is obvious from 
this experience that users will not always agree 
on the interpretation of the definitions and on the 
types of mappings applied. At this moment we 
cannot make final statements in how far hierar-
chical relations will be effected by this that 
would constitute an implicit thesaurus as is ex-
pected within ISO TC37/SC4. 
Open Data Category Repositories 
Based on the experience so far it can be recom-
mended to include into open repositories only 
concepts that have been used for a while and 
therefore have shown their semantic stability 
within a certain community. For the area of lan-
guage resources ISO TC37/SC4 is on the way to 
create such a repository, which is compliant with 
widely recognized standards such as ISO 11179 
and ISO 12620. Therefore, it makes sense to reg-
ister all elements used within IMDI and OLAC 
as data categories in this repository.  
 
This will open up several new possibilities for 
projects and initiatives: (1) IMDI and OLAC can 
create schemas that define their sets by referring 
to machine-readable definitions. For instance, an 
equality relationship can be directly indicated by 
referring to the same data category registry 
(DCR) entry. Search engines could make use of 
this information. (2) It is our experience that pro-
jects often like to tailor their own metadata sets 
due to their specific needs. In this case an open 
registry would simply allow to create a new 
schema and to re-use existing definitions as 
much as possible11. By referring to DCR entries 
again a direct form of interoperability is 
achieved.  
 
We assume that we will have widely recognized 
DCRs as currently defined within ISO 
TC37/SC4. They should contain the concepts 
that are based on a wide agreement within com-
munities. However, due to the slow acceptance 
processes within standardization bodies and the 
different needs that result for example from dif-
ferent languages there could be a need for re-
searchers to set up their own temporary DCRs. 
We therefore foresee a large number of data 
category repositories.  
 
 
For the ECHO project the usage of an open DCR 
is not yet an option. To be of use for the commu-
nity there has to be a wide acceptance. The do-
main of ?cultural heritage? addressed within 
ECHO covers too many different disciplines and 
the concepts are semantically mostly too differ-
ent. Disciplines such as history of arts, history of 
science and ethnology have to start their disci-
pline oriented discussion process to define useful 
concepts and to start building widely recognized 
registries. What seems necessary is to start creat-
ing files with concept definitions that can be eas-
ily integrated later into open registries and that 
are compliant to emerging standards.  
Open Relation repositories 
Concept definitions in DCRs are one important 
aspect in defining metadata ontologies. Another 
aspect are repositories that store relations be-
tween these concepts. From our experience in the 
two projects mentioned, it seems required to 
separate these two types of information in order 
to achieve a high degree of independence and 
flexibility. However, other experiences as that of 
the GOLD initiative (Farrar, 2005) indicate that 
opinions on this vary largely. 
 
Theoretically, it is possible to include all infor-
mation that defines a concept into the DCR. The 
concept ?country? that is used within IMDI is 
                                                           
11 IMDI already provides a step towards this kind of 
flexibility by allowing projects to define profiles or 
individuals to define new key-value pairs. 
typically a sub-part of a ?continent?. However, 
the proper definition of the concept ?country? in 
the context of language resources is not depend-
ent on the availability of this hierarchical rela-
tion. But this again may be completely different 
for abstract linguistic concepts such as ?transitive 
verb? where we know that the class relation 
?transitive-verb isSubClassOf verb? is part of the 
definition.  
 
In general, we argue that whenever it is not 
strictly necessary for the proper definition of a 
concept, relation aspects should be kept outside 
of DCRs as much as possible, since they often 
form a constraint with only little agreement.  
 
For the represention of relations in a machine-
readable format, RDF(S) seems to be the most 
suitable choice. In RDF, all relations are repre-
sented as tertiary assertions as indicated in Figure 
1. Actually, each of these RDF assertions defines 
a relation between two resources, since the value 
can be an arbitrary web-resource as well. 
 
 
 
 
Figure 1 shows a basic RDF assertion specifying that 
a (web) resource identified by a URI has properties 
that may have values.  
 
Obviously, this simple mechanism allows us to 
create complex repositories of semantic relations. 
Since all objects of such an assertion can be web-
resources we can for example point to concepts 
defined in a DCR and relate them with each 
other.  
 
From the two mentioned projects we can give 
two typical examples. From the INTERA project 
we notice that according to our interpretation the 
concept ?IMDI:Participant.Role=Collector? is a 
sub-class of ?OLAC:Creator? (Figure 2).  
 
 
 
 
Figure 2 shows a typical relation that can be found in 
the INTERA project. 
 
 
 
 
Figure 3 shows a typical relation that can be found in 
the ECHO project. 
 
resource value 
property 
I:Participant O:Creator 
isSubClassOf 
I:Genre F:Iconography 
mapsTo 
In the ECHO project we can identify a semantic 
overlap between ?IMDI:Genre? and ?Foto-
thek:Iconography? (Figure 3).  
 
We can imagine that RDF will be used by some 
projects, initiatives and institutions to establish 
widely recognized and used repositories with 
mapping relations.  
 
We also assume that many persons, projects and 
institutions will create their own mappings to 
tune their operations like searching according to 
their specific needs, i.e., a large variety of ?prac-
tical ontologies? will emerge. These practical 
ontologies may re-use most of the semantics 
found in a repository, or they overwrite a certain 
number of relations or they introduce new rela-
tions that are not yet defined elsewhere. 
 
In contrast to the ISO data category repository 
that is based on the experiences of the work 
about ISO 11179 and ISO 12620, there is no 
work yet of how to represent relations for the 
domain of language resources. For INTERA this 
creates the need of using ad hoc solutions. ISO 
TC37/SC4 should urgently take up this issue. 
4 Registries and Engines 
Given the discussion above, we can expect the 
Semantic Web era to produce a large number of 
data category definitions stored in different 
DCRs and mapping relations between these 
stored in other repositories. Amongst these com-
ponents there will be some that deserve a larger 
interest by the language resource community, 
since they are maintained by recognized experts, 
but there will also be many others created within 
projects and institutions or even by individuals to 
satisfy only ad-hoc purposes. Therefore, we need 
an infrastructure for registering these compo-
nents for making them visible and searchable. 
 
Current inference engines such as provided by 
Jena12 assume that there is one database of mean-
ingful RDF triples. This would allow us to inte-
grate all our mapping relations from the INTERA 
or ECHO ontologies (such as ?Country isSub-
ClassOf Continent? and ?Place isSubClassOf 
Country?), that is currently part of an XML-
based thesaurus. To arrive at an RDF-based da-
tabase instead, we would need to harvest meta-
data from the XML-based thesaurus, i.e., we 
                                                           
12 Jena: http://jena.sourceforge.net 
would first have to write a wrapper that converts 
XML structure information into RDF assertions. 
 
Further, we would like to harvest RDF triples 
from different sites, since we need to integrate 
already existing knowledge. Two problems can 
be foreseen here: (1) How do we know where to 
find useful RDF triple instances? We need 
mechanisms to register the existence of sites with 
that type of information and to semi-formally 
describe the content. (2) When we harvest triples 
from such a site we may include knowledge ? 
metadata ontologies defined in RDF(S) - that is 
conflicting with what is already available. How 
can we deal with this and how can we be selec-
tive? 
 
Currently, there are no answers to these ques-
tions. But they have to be addressed soon. Also 
here ISO TC37/SC4 could play an important 
role, since it is about infrastructure aspects that 
have to be worked out for the language resource 
community. 
5 XML vs RDF 
We explained why XML was chosen in repre-
senting the knowledge involved in the projects 
mentioned. Mainly short-term arguments guided 
us to take this decision. This may not be the cor-
rect decision in the long-term. Nevertheless, also 
ISO TC37/SC4 has chosen to represent data 
category definitions as XML structures including 
hierarchical references needed to properly define 
a concept. 
 
The underlying data models of XML and RDF 
are very different. XML is based on a tree model, 
i.e., it has a strong bias towards hierarchies. All 
expressive power is gained from structural rela-
tions, which to a certain extent allow for the rep-
resentation of semantic relations. 
 
In contrast to this, RDF is based on a loose col-
lection of relations. It is therefore very simple to 
combine relations from different RDF reposito-
ries into larger collections. Although implicit 
hierarchies will be difficult to recover. 
 
Semantically, RDF Schema offers the user the 
option to define the value range of any user-
defined relation (property) used in an RDF file 
with user-defined classes, while XML only offers 
basic data types. OWL has even more expressive 
power. A good overview is given by Gil and 
Ratnaker, 2001.  
 
Summarizing, we would like to emphasize the 
following two points that need to be taken into 
account by any follow-up projects of INTERA 
and ECHO. Such a project should:  
 
? represent all concept definitions of a resource 
metadata set in an ISO DCR compliant way 
and turn them over to RDF-based reposito-
ries that may emerge within the disciplines in 
the coming years; 
? represent relations as much as possible in 
external RDF(S)-based metadata ontologies 
using all needed expressional power of 
RDF(S) and OWL so that users can easily 
add their own relations or reformulate exist-
ing ones. 
6 Conclusion 
The work on metadata interoperability in the two 
projects mentioned clearly indicate that this type 
of work is in its beginning phase. Ad hoc meth-
ods are used to achieve high speed and to guaran-
tee efficient exchange of knowledge components, 
but they form obstacles on the way towards a 
flexible and open Semantic Web type of infra-
structures. The examples indicate that the chosen 
mapping strategies lead to the expected results in 
many cases. They also indicate some of the prob-
lems that are associated with using specific ele-
ments for searching. Amongst others these are 
caused by sparsely filled in metadata descrip-
tions, unawareness about the underlying element 
semantics, insufficient mappings between meta-
data elements and thesaurus concepts. 
 
The usage of ISO 11179 and ISO 12620 compli-
ant open Data Category Registries for machine 
readable definitions of metadata concepts within 
INTERA is a first step in the right direction. 
However, other disciplines than linguistics lack 
such a widely agreed registry type. For building 
up and combining repositories of RDF-based 
relations between registered concepts there is yet 
no infrastructure. Even in the linguistics domain 
yet there is no suggestion for standards. ISO 
TC37/SC4 should take up this issue, since Data 
Category repositories with concept definitions 
and relation repositories are mutually dependent 
on each other to form exploitable knowledge 
bases. Due to the many contributions from pro-
jects, institutions and even individuals that will 
disagree with proposed definitions and relations 
we will need an efficient infrastructure for dis-
covering and combining useful knowledge com-
ponents. 
References 
S. Bird and M. Liberman. 2001. A formal framework 
for linguistic annotation. 
http://www.ldc.upenn.edu/Papers/CIS9901_1999/r
evised_13Aug99.pdf 
H. Brugman and P. Wittenburg. 2001. The application 
of annotation models for the construction of data-
bases and tools. 
http://www.ldc.upenn.edu/annotation/database/pap
ers/Brugman_Wittenburg/20.2.brugman.pdf 
S. Farrar and D.T. Langendoen. 2003. Markup and the 
GOLD Ontology. 
http://saussure.linguistlist.org/cfdocs/emeld/worksh
op/2003/paper-terry.html 
Y. Gil and V. Ratnaker. A Comparison of (Semantic) 
Markup Languages. In Proceedings of AAAI 2001. 
http://trellis.semanticweb.org.expect/web/semantic
web.comparison.html 
 
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 114?115,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Statistical Term Profiling for Query Pattern Mining
Paul Buitelaar Pinar Oezden Wennerberg, Sonja ZillnerDFKI GmbH Siemens AGLanguage Technology Lab Knowledge Management CT IC 1Saarbr?cken, Germany Munich, Germanypaulb@dfki.de pinar.wennerberg.ext@siemens.com, sonja.zillner@siemens.com
1 Introduction
Through advanced technologies in clinical careand research, especially the rapid progress in imag-ing technologies, more and more medical imaging data and patient text data is generated by hospitals, pharmaceutical companies, and medical research. For enabling advanced access to clinical imaging and text data, it is relevant to know what kind of knowledge the clinician wants to know or the que-ries that clinicians are interested in. Through inten-sive interviews and discussions with radiologists and clinicians, we have learned that medical imag-ing data is analyzed - and hence queried ? from three different perspectives, i.e. the anatomic per-spective addressing the involved body parts, theradiology-specific spatial perspective describing the relationships of located anatomical regions to other anatomical parts, and the disease perspectivedistinguishing between normal and abnormal im-aging features. Our aim is to establish query pat-terns reflecting those three perspectives that would typically be used by clinicians and radiologists to find patient-specific sets of relevant images. The context of our work is in the Theseus-MEDICO1 project on cross-modal image and in-formation retrieval in the medical domain. The focus of the work reported here is on setting up Wikipedia-based corpora of human anatomy and radiology and on obtaining a statistical profile of concepts from three semantic knowledge resourceswith these corpora: the Foundational Model of Anatomy (FMA), the radiology lexicon RadLex, and a subset of the international classification of disease codes ICD-9 CM. Using this information,we intend to extract relations that are likely to oc-cur between statistically relevant terms and the concepts they express. The final goal of our work is to derive potential query patterns from the extracted set of relations that can be used in the MEDICO semantic-based 
1 http://theseus-programm.de/scenarios/en/medico
image retrieval application. For example when re-staging head and neck lymphoma, clinicians and radiologists look for information and images that report on essential radiological patterns as ?an enlargement in the dimension of the lymph node in the neck?. Therefore, within our approach, we aim at establishing hypotheses about possible user que-ries, i.e. the query patterns that reflect the three perspectives discussed above. Accordingly, an ex-ample query pattern might look like this:
[ANATOMICALSTRUCTURE] located_in [ANATOMICALSTRUCTURE]AND[[RADIOLOGY]IMAGE]Modality] is_about [ANATOMICALSTRUCTURE]AND[[RADIOLOGYIMAGE]Modality] shows_symptom [DISEASE SYMPTOM]
Once an initial set of similar patterns has been es-tablished in this way, they will be evaluated byclinicians for their validity and relevance.
2 Corpora
A central aspect of the query pattern mining task is the statistical analysis of the FMA and RadLexterms in relevant text collections. In this way rele-vance scores can be assigned to terms that allow to investigate the most likely expressed (and hence queried) relations between them. For this purpose we need access to a representative corpus of texts that at the same time reflects the joint view of anatomy, spatial aspects of radiology and disease that we are targeting. Patient records would be our first choice, but due to strict anonymization re-quirements these are difficult to obtain. We there-fore constructed a corpus based on the WikipediaCategories Anatomy and Radiology. We then ran all text sections of each corpus through a part-of-speech tagger (Brants, 2000) to extract all nouns in the corpus and to compute a relevance score (chi-square) for each by comparing anatomy and radi-ology frequencies with those in the British Na-
114
tional Corpus. A next step will be to parse and an-notate sentences with predicate-structure informa-tion, which may then be used for relation extraction along the lines of (Schutz and Buitelaar, 2005).
3 FMA Terms
The statistically most relevant FMA terms wereidentified on the basis of chi-square scores com-puted for nouns in each corpus. Single word terms in the FMA and occurring in the corpus correspond directly to the noun that the term is build up of(e.g. the noun ?ear? corresponds to the FMA term ear). In this case, the statistical relevance of the term is the chi-square score of the corresponding noun. In the case of multi-word terms occurring in the corpus, the statistical relevance is computed on the basis of the chi-square score for each constitut-ing noun and/or adjective in the term, summed and normalized over the length of the term. Thus, the relevance value for lymph node is the summation of chi-square scores for ?lymph? and ?node? di-vided by 2. In order to take frequency in account, we further multiplied the summed relevance value by the frequency of the term. This assures that only frequently occurring terms are judged as relevant.
FMA Term Freq. Score POS
lateral 464 338724,00 JJ
anterior 452 314721,00 JJ
artery 237 281961,00 NN
anterior spinal artery 2 219894,33 JJ JJ NN
lateral thoracic artery 2 217815,33 JJ JJ NNTable 1: top FMA terms in anatomy corpus
FMA Term Freq. Score POS
artery 65 6724,00 NN
coronary artery 17 5284,00 JJ NN
small bowel 11 4651,79 JJ NN
renal artery 3 4286,50 JJ NN
pulmonary artery 1 3974,50 JJ NNTable 2: top FMA terms in radiology corpus
4 RADLEX Terms 
Analogously, RadLex was used to identify the most relevant radiology terms. The most relevant RadLex terms are shown below. As with the FMA, the most relevant RadLex terms in the anatomy corpus are centered on ?artery?. In contrast, in the radiology corpus the RadLex relevance scores in-deed point to a radiology profile: 
RadLex Term Freq. Score POS
lateral 464 338724,00 JJ
anterior 452 314721,00 JJ
artery 237 281961,00 NN
anterior spinal artery 2 219894,33 JJ JJ NN
lateral thoracic artery 2 217815,33 JJ JJ NNTable 3: top RadLex terms in anatomy corpus
RadLex Term Freq. Score POS
x-ray 253 81901,64 NN
imaging modality 6 58682,00 NN NN
volume imaging 1 57855,09 NN NN
molecular imaging 4 57850,00 JJ NN
mr imaging 9 57850,00 JJ NNTable 4: topRadLex terms in radiology cor pus
5 Conclusions and Future Work
Using ICD-9 lymphoma terminology, we will derive a Pubmed-based corpus on lymphoma to analyse the context of the statistically top most relevant terms from the FMA and RadLex termi-nologies. In this way we will be able to identify relationships and eventually query patterns across the three dimensions of anatomy, radiology and lymphoma research. 
Acknowledgments
This research has been supported in part by the THESEUS- MEDICO Project, which is funded by the German Federal Ministry of Economics and Technology under the grant number 01MQ07016. 
References 
Brants T. (2000). TnT - A Statistical Part-of-SpeechTagger. In: Proc. of the 6th ANLP Conference, Seat-tle, WALanglotz, CP. (2006). RadLex: A New Method for In-dexing Online Educational Materials In: Radiograph-ics 26, pp.1595-1597.Rosse C. and J.L.V. Mejino Jr. (2003). A reference on-tology for biomedical informatics: The FoundationalModel of Anatomy. Journal of Biomedical Informat-ics, 36(6), pp. 478?500.Schutz A., and P Buitelaar. (2005). RelExt: A Tool for-Relation Extraction in Ontology Extension In: Proc.the 4th International Semantic Web Conference, Galway, Ireland.
115
Proceedings of the 8th International Conference on Computational Semantics, pages 351?354,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Identifying the Epistemic Value of Discourse
Segments in Biology Texts
Anita de Waard
(1)
Paul Buitelaar
(2)
Thomas Eigner
(3)
(1) Elsevier & Universiteit Utrecht, the Netherlands (anita@cs.uu.nl)
(2) DERI - NLP Unit, Galway, Ireland (paulb@deri.org)
(3) DFKI, Saarbrcken, Germany (teigner@dfki.de)
1 Introduction
To manage the flood of information that threatens to engulf (life-)scientists,
an abundance of computer-aided tools are being developed. These tools aim
to provide access to the knowledge conveyed within a collection of research
papers, without actually having to read the papers. Many of these tools
focus on text mining, by looking for specific named-entities that have scien-
tific meaning, and relationships between these. An overview of the current
state of the art is given in Rebholz-Schuhmann et al (2005) and Couto
et al (2003). Typically, these tools identify a list of sentences containing
relationships between two specific named-entities that can be found using
rules or a thesaurus of synonyms. These sentences represent an overview of
the interactions that are known with a specific entity, thus precluding the
need for an exhaustive literature study. For example, the following are a
few sentences that have been found using a typical text mining tool for the
relationship ?p53 activates *?:
1. The p53 tumor suppressor protein exerts most of its anti-tumorigenic
activity by transcriptionally activating several pro-apoptotic genes.
2. We found that p53 ... activates[,] the promoter of the myosin VI gene.
However, in order to be able to use these statements and to draw conclu-
sions about its subject (?Which entities does p53 activate??) we still need
to read the article that they appeared in, identify the experimental context
and the epistemic (?truth?) value of each statement. For instance, 1. does
not seem to represent an experimental finding that is arrived at in the paper
351
that the sentence is taken from; instead, it seems to be a citation. So, to be
able to evaluate its epistemic value (?How true is this??) we need to read
the paper that contained the sentence and paper(s) where the statement
was first experimentally motivated. In the case of 2., a clear statement is
given on what the authors of the paper have found. But ?How did they
find it??, ?What experimental setup and control experiments were used??,
?What were their assumptions?? Biologists will need to check these and
other issues before accepting 2. as a fact. Our research therefore concerns
the classification of sentences in biology texts by ?epistemic segment type?,
with the purpose of enabling a better way to summarize, mine and compare
statements within biology texts. The current paper describes a first venture
into doing this in a computational way.
2 Epistemic Segment Types for Biology Texts
As motivated elsewhere we have identified seven epistemic segment types
(De Waard, 2007):
? Fact: statement presumed to be accepted as true by the commu-
nity, e.g. Cellular transformation requires the expression of oncogenic
RASV12.
? Hypothesis: possible explanation for a set of phenomena, e.g. This
suggests possible roles for APC in G1 and G0 phases of the cell cycle.
? Implication: interpretation of results, e.g. These results indicate that
our procedure is sensitive enough to detect mild growth differences.
? Method: ways in which the experiment was performed, e.g. We
inserted 500 bp fragments ... in a modified pMSCV-Blasticidin vector.
? Problem: discrepancies or unknown aspects of the known fact corpus,
e.g. The small number of miRNAs with a known function stresses
the need for a systematic screening approach to identify more miRNA
functions.
? Goal: implicit hypothesis and problem, e.g. identify miRNAs that
can interfere with this process and ... contribute to the development of
tumor cells
? Result: a summary of the results of measurements, e.g. we observed
an approximately 4-fold increase in miR-311 signal
352
For example, Fact segments are taken from another source of knowledge
(explicitly referred to or presumed to be known) and therefore not experi-
mentally ascertained in the article, whereas Result segments are obtained
by measurements discussed in the paper itself. For the sentences in the pre-
vious section, we therefore see that 1. is a Fact and 2. is a typical Result
segment. To classify the segments (manually first), we have used several
linguistic clues, as well as an understanding of the context of a segment.
Important linguistic clues are the verb tense of the segment and specific
markers used to identify a segment transition, e.g. the transition between a
Result and an Implication segment is usually indicated by a phrase such as
?These results suggest that?. The segment types and selected specific mark-
ers that we used in our research here are as follows (using regular expressions
to shorten notation):
? Hypothesis: results indicate, suggest, suggesting that
? Implication: data?results demonstrate?suggest?indicate, data?results
show
? Method: by cloning?using, using additionally, we activated?constructed
? Goal: to examine?identify?investigate?mimic?shed light?start to
? Result: as expected?predicted, resulting in, shows that, this confirms
3 Automatic Identification of Epistemic Segment
Types
To investigate if we could use this set of markers for the automatic iden-
tification of segment type, we applied them to an independently developed
data set of 1721 biomedical abstracts on ?mantle cell lymphoma? that we
downloaded from PubMed. We randomly selected 100 sentences, in which
a marker was identified, and to which one out of five segment types (Hy-
pothesis, Implication, Method, Goal, Result) was assigned by a sim-
ple automatic procedure, i.e. we matched the markers to a part-of-speech
enriched version of the PubMed corpus. One or more segment types were
assigned in case of a match. The resulting assignments were then evaluated
by the first author of this paper. Results were encouraging as only 30 out of
100 assignments were incorrect. Most of these (12) were between Hypoth-
esis, Implication, which is not surprising as their markers are overlapping
353
and therefore ambiguous. Others that were somewhat frequent were: Hy-
pothesis instead of Fact (3), Result instead of Fact (3), Result instead
of Method (2), Goal instead of Method (2), Goal instead of Problem
(2). Of these however, Fact and Problem were not covered by our set of
segment specific markers and could therefore not be recognized.
4 Conclusions and Future Work
As a first conclusion, results are encouraging enough to merit further re-
search. We have identified several follow-up steps that can help improve our
results. First, we plan to segment the sentences into smaller discourse units.
For instance, sentences such as the following are quite clearly divided into
two parts: a Goal and a Method:
? Goal: To examine miRNA expression from the miR-Vec system,
? Method: a miR-24 minigene-containing virus was transduced into
human cells.
Such sentences are quite common, as are sentences containing Method,
Result and Result, Implication segments; this clearly indicates that this
move order is logical and occurs often. Secondly, there is a clear correlation
between segment type and verb tense. Method, Result are overwhelm-
ingly stated in the past tense, whereas Fact, Implication are given in the
present tense. Using verb tense as a marker could further improve classi-
fication scores. Lastly, we are interested in applying our epistemic values
to augment and improve bioinformatics tools, and investigating the value of
these categories with users. We are actively pursuing collaborations in this
area.
References
Rebholz-Schuhmann, D., H.Kirsch & F. Couto (2005) Facts from text - is text
mining ready to deliver? PLoS Biology 3(2).
Couto, F., M.J. Silva & P. Coutinho (2003) Improving information extraction
through biological correlation. In Proc. European Workshop on Data Mining
and Text Mining, Dubrovnik.
Waard, A. de (2007) The pragmatic research article. In Proc. 2nd International
Conference on the Pragmatic Web, Tilburg.
354
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 40?46,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Ontology Label Translation
Mihael Arcan and Paul Buitelaar
Unit for Natural Language Processing,
Digital Enterprise Research Institute (DERI)
National University of Ireland Galway (NUIG)
Galway, Ireland
{mihael.arcan , paul.buitelaar}@deri.org
Abstract
Our research investigates the translation of on-
tology labels, which has applications in mul-
tilingual knowledge access. Ontologies are
often defined only in one language, mostly
English. To enable knowledge access across
languages, such monolingual ontologies need
to be translated into other languages. The
primary challenge in ontology label trans-
lation is the lack of context, which makes
this task rather different than document trans-
lation. The core objective therefore, is to
provide statistical machine translation (SMT)
systems with additional context information.
In our approach, we first extend standard SMT
by enhancing a translation model with context
information that keeps track of surrounding
words for each translation. We compute a se-
mantic similarity between the phrase pair con-
text vector from the parallel corpus and a vec-
tor of noun phrases that occur in surrounding
ontology labels. We applied our approach to
the translation of a financial ontology, translat-
ing from English to German, using Europarl as
parallel corpus. This experiment showed that
our approach can provide a slight improve-
ment over standard SMT for this task, with-
out exploiting any additional domain-specific
resources.
1 Introduction
The biggest barrier for EU-wide cross-lingual busi-
ness intelligence is the large number of various lan-
guages used by banks or investment firms for their
financial reports. In contrast to that, most of the
ontologies used for knowledge access are available
in English, e.g. the financial ontology FINREP1
(FINancial REPorting) or COREP2 (COmmon sol-
vency ratio REPorting). To make the targeted trans-
parency of financial information possible, these on-
tologies have to be translated first into another lan-
guage; see also (Declerck et al, 2010). The chal-
lenge here lies in translating domain-specific on-
tology vocabulary, e.g. Equity-equivalent partner
loans, Subordinated capital or Write-downs of long-
term financial assets and securities.
Since domain-specific parallel corpora for SMT
are hardly available, we used a large general parallel
corpus, whereby a translation model built by such
a resource will tend to translate a segment into the
most common word sense. This can be seen for in-
stance when we translate the financial ontology label
Equity-equivalent partner loans from the German
GAAP ontology (cf. Section 3.1). Using a baseline
SMT system we get the translation Gerechtigkeit-
gleichwertige Partner Darlehen. Although this la-
bel provides contextual information, equity is trans-
lated into its general meaning, i.e. Gerechtigkeit in
the meaning of justice, righteousness or fairness, al-
though Eigenkapital would be the preferred transla-
tion in the financial domain.
To achieve accurate disambiguation we developed
a method using context vectors. We extract semantic
information from the ontology, i.e. the vocabulary
and relations between labels and compare it with the
contextual information extracted from a parallel cor-
pus.
The remainder of the paper is organized as fol-
1http://eba.europa.eu/Supervisory-Reporting/FINER.aspx
2http://eba.europa.eu/Supervisory-Reporting/COREP.aspx
40
lows. Section 2 gives an overview of the related
work on including semantic information into SMT.
Section 3 describes the ontology and the parallel
corpus used in our experiment. Then we describe
the approach of enhancing the standard SMT model
with ontological knowledge for improving the trans-
lation of labels in Section 4. In Section 5 the results
of exploiting the ontological knowledge described in
the previous section are illustrated. Finally we con-
clude our findings and give an outlook for further
research.
2 Related Work
Word sense disambiguation (WSD) systems gener-
ally perform on the word level, for an input word
they generate the most probable meaning. On the
other hand, state of the art translation systems op-
erate on sequences of words. This discrepancy be-
tween unigrams versus n-grams was first described
in (Carpuat and Wu, 2005). Likewise, (Apidianaki
et al, 2012) use a WSD classifier to generate a prob-
ability distribution of phrase pairs and to build a lo-
cal language model. They show that the classifier
does not only improve the translation of ambiguous
words, but also the translation of neighbour words.
We investigate this discrepancy as part of our re-
search in enriching the ontology label translation
with ontological information. Similar to their work
we incorporate the idea of enriching the translation
model with neighbour words information, whereby
we extend the window to 5-grams.
(Mauser et al, 2009) generate a lexicon that pre-
dicts the bag of output words from the bag of input
words. In their research, no alignment between input
and output words is used, words are chosen based
on the input context. The word predictions of the in-
put sentences are considered as an additional feature
that is used in the decoding process. This feature de-
fines a new probability score that favours the trans-
lation hypothesis containing words, which were pre-
dicted by the lexicon model. Similarly, (Patry and
Langlais, 2011) train a model by translating a bag-
of-words. In contrast to their work, our approach
uses bag-of-word information to enrich the missing
contextual information that arises from translating
ontology labels in isolation.
(McCrae et al, 2011) exploit in their research
the ontology structure for translation of ontologies
and taxonomies. They compare the structure of
the monolingual ontology to the structure of already
translated multilingual ontologies, where the source
and target labels are used for the disambiguation
process of phrase pairs. We incorporated the idea of
using the ontology structure, but avoided the draw-
back of exploiting existing domain-specific multilin-
gual ontologies.
3 Data sets
For our experiment we used a general parallel cor-
pus to generate the mandatory SMT phrase table
and language model. Further, the corpus was used
to generate feature vectors on the basis of the con-
textual information provided by surrounding words.
Finally we calculate the semantic similarity between
the extracted information from the parallel corpus
and the ontology vocabulary.
3.1 Financial ontology
For our experiment we used the financial ontol-
ogy German GAAP (Generally Accepted Account-
ing Practice),3 which holds 2794 concepts with la-
bels in German and English.
Balance sheet
. . . Total equity and liabilities
Equity
Equity-equivalent partner loans Revenue reserves
Legal reserve
Legal reserve, of which transferred from prior year net retained profits
. . .
. . .
Figure 1: The financial label Equity-equivalent partner
loans and its neighbours in the German GAAP ontology
As seen in Figure 1 the financial labels do not cor-
respond to phrases from a linguistic point of view.
They are used in financial or accounting reports as
unique financial expressions or identifiers to organ-
ise and retrieve the reported information automati-
cally. Therefore it is important to translate these fi-
nancial labels with exact meaning preservation.
3http://www.xbrl.de/
41
3.2 Europarl
As a baseline approach we used the Europarl par-
allel corpus,4 which holds proceedings of the Euro-
pean Parliament in 21 European languages. We used
the English-German parallel corpus with around 1.9
million aligned sentences and 40 million English
and 43 million German tokens (Koehn, 2005).
Although previous research showed that a trans-
lation model built by using a general parallel cor-
pus cannot be used for domain-specific vocabulary
translation (Wu et al, 2008), we decided to train a
baseline translation model on this general corpus to
illustrate any improvement steps gained by enrich-
ing the standard approach with the semantic infor-
mation of the ontology vocabulary and structure.
4 Experiment
Since ontology labels (or label segments) translated
by the Moses toolkit (Section 4.1) do not have much
contextual information, we addressed this lack of
information and generated fromthe Europarl corpus
a new resource with contextual information of sur-
rounding words as feature vectors (Section 4.2). A
similar approach was done with the ontology struc-
ture and vocabulary (Section 4.3).
4.1 Moses toolkit
To translate the English financial labels into Ger-
man, we used the statistical translation toolkit Moses
(Koehn et al, 2007), where the word alignments
were built with the GIZA++ toolkit (Och and Ney,
2003). The SRILM toolkit (Stolcke, 2002) was used
to build the 5-gram language model.
4.2 Building the contextual-semantic resource
from the parallel corpus Europarl
To enhance the baseline approach with additional se-
mantic information, we built a new resource of con-
textual information from Europarl.
From the original phrase table, which was gen-
erated from the Europarl corpus, we used the sub-
phrase table, which was generated to translate the
German GAAP financial ontology in the baseline
approach. Although this sub-phrase table holds only
segments necessary to translate the financial labels,
it still contains 2,394,513 phrase pairs. Due to the
4http://www.statmt.org/europarl/, version 7
scalability issue, we reduced the number of phrase
pairs by filtering the sub-phrase table based on the
following criteria:
a) the direct phrase translation probability ?(e|f)
has to be larger than 0.0001
b) a phrase pair should not start or end with a
functional word, i.e. prepositions, conjunctions,
modal verbs, pronouns
c) a phrase pair should not start with punctuation
After applying these criteria to the sub-phrase ta-
ble, the new filtered phrase table holds 53,283 enti-
ties, where phrase pairs, e.g. tax rate ||| Steuersatz
or tax liabilities ||| Steuerschulden were preserved.
In the next step, the phrase pairs stored in the fil-
tered phrase table were used to find sentences in Eu-
roparl, where these phrase pairs appear. The goal
was to extract the surrounding words as the con-
textual information of these phrase pairs. If a seg-
ment from the filtered phrase table appeared in the
sentence we extracted the lemmatised contextual in-
formation of the phrase pair, whereby we consid-
ered 10 tokens to the left and 10 to the right of
the analysed phrase pair. To address the problem
of different inflected forms (financial asset vs. fi-
nancial assets) of the same lexical entity (financial
asset) we lemmatised the English part of the Eu-
roparl corpus with TreeTagger(Schmid, 1995). Sim-
ilar to the phrase table filtering approach, an n-gram
should not start with a functional word or punctua-
tion. The extracted surrounding words were stored
together with its phrase pairs, i.e. for the phrase
pairs Equity-Gerechtigkeit and Equity-Eigenkapital
different contextual vectors were generated.
Example 1.a) illustrates a sentence, which holds
the source segment Equity from the filtered phrase
table. Example 1.b) represents its translation into
German. This example illustrates the context in
which Equity is translated into the German expres-
sion Gerechtigkeit. The segment Equity is also
present in the second sentence, (example 2.a)), in
contrast to the first one, equity is translated into
Eigenkapital, (2.b)), since the sentence reports fi-
nancial information.
1. a) ... which could guarantee a high standard of ef-
ficiency, safety and equity for employees and
users alike, right away.
42
b) ... , der heute ein hohes Niveau an Leistung,
Qualita?t, Sicherheit und Gerechtigkeit fu?r die
Bediensteten und die Nutzer garantieren ko?nnte.
2. a) ... or organisations from making any finance,
such as loans or equity, available to named
Burmese state-owned enterprises.
b) ... bzw. Organisationen zu verbieten, bir-
manischen staatlichen Unternehmen jegliche Fi-
nanzmittel wie Darlehen oder Eigenkapital zur
Verfu?gung zu stellen.
Applying this methodology on all 1.9 million sen-
tences in Europarl, we generated a resource with
feature vectors for all phrase pairs of the filtered
phrase table. Table 1 illustrates the contextual differ-
ences between the vectors for Equity-Gerechtigkeit
and Equity-Eigenkapital phrase pairs.
4.3 Contextual-semantic resource generation
for the financial ontology German GAAP
To compare the contextual information extracted
from Europarl a similar approach was applied to the
vocabulary in the German GAAP ontology.
First, to avoid unnecessary segments, e.g. provi-
sions for or losses from executory, we parsed the fi-
nancial ontology with the Stanford parser (Klein and
Manning, 2003) and extracted meaningful segments
from the ontology labels. This step was done pri-
marily to avoid comparing all possible n-gram seg-
ments with the filtered segments extracted from the
Europarl corpus (cf. Subsection 4.2). With the syn-
tactical information given by the Stanford parser we
extracted a set of noun segments for the ontology la-
bels, which we defined by the rules shown in Table
2.
# Syntactic Patterns
1 (NN(S) w+)
2 (NP (NN(S) w+)+))
3 (NP (JJ w+)+ (NN(S) w+)+))
4 (NP (NN(S) w+)+ (CC w+) (NN(S) w+)+)
5 (NP (NN(S) w+)+ (PP (IN/.. w+) (NP (NN(S) w+)+))
Table 2: Syntactic patterns for extracting noun segments
from the parsed financial ontology labels
Applying these patterns to the ontology label Pro-
visions for expected losses from executory contracts
extracts the following noun segments: provisions,
losses and contracts (pattern 1), expected losses and
executory contracts (pattern 3), provisions for ex-
pected losses and expected losses from executory
contracts (pattern 5).
In the next step, for all 2794 labels from the finan-
cial ontology, a unique contextual vector was gen-
erated as follows: for the label Equity-equivalent
partner loans (cf. Figure 1), the vector holds the
extracted (lemmatised) noun segments of the direct
parent, Equity, and all its siblings in the ontology,
e.g. Revenue reserves . . . (Table 3).
targeted label: Equity-equivalent partner loans
contextual information: capital (6), reserve (3), loss
(3), balance sheet (2) . . . currency translation (1),
negative consolidation difference (1), profit (1)
Table 3: Contextual information for the financial label
Equity-equivalent partner loans
4.4 Calculating the Semantic Similarity
Using the resources described in the previous sec-
tions in a final step we apply the Cosine, Jaccard and
Dice similarity measures on these feature vectors.
For the first evaluation step we translated all finan-
cial labels with the general translation model. Ta-
ble 4 illustrates the translation of the financial ex-
pression equity as part of the label Equity-equivalent
partner loans.5
With the n-best (n=50) translations for each fi-
nancial label we calculated the semantic similarity
between the contextual information of the phrase
pairs (equity-Eigenkapital) extracted from the par-
allel corpus (cf. Table 1) with the semantic informa-
tion of the financial label Equity extracted from the
ontology (cf. Table 3).
After calculating a semantic similarity, we reorder
the translations based on this additional information,
which can be seen in Table 5.
5ger. Gerechtigkeit-gleichwertige Partner Darlehen
Source label Target label p(e|f)
equity Gerechtigkeit -10.6227
equity Gleichheit -11.5476
equity Eigenkapital -12.7612
equity Gleichbehandlung -13.0936
equity Fairness -13.6301
Table 4: Top five translations and its translation probabil-
ities generated by the Europarl translation model
43
Source label Target label Context (frequency)
equity Gerechtigkeit social (19), efficiency (18), efficiency and equity (14), justice (13), social eq-
uity (11), education (9), principle (8), transparency (7), training (7), great (7)
equity Eigenkapital capital (19), equity capital (15), venture (3), venture capital (3), rule (2), capital
and risk (2), equity capital and risk (2), bank (2), risk (2), debt (1)
Table 1: Contextual information for Equity with its target labels Gerechtigkeit and Eigenkapital extracted from the
Europarl corpus
Source label Target label Jaccard
equity Eigenkapital 0.0780169232
equity Equity 0.0358268041
equity Kapitalbeteiligung 0.0341965597
equity Gleichheit 0.0273327211
equity Gerechtigkeit 0.0266209669
Table 5: Top five re-ranked translations after calculating
the Jaccard similarity
5 Evaluation
Our evaluation was conducted on the translations
generated by the baseline approach, using only Eu-
roparl, and the ontology-enhanced translations of fi-
nancial labels.
We undertook an automatic evaluation using the
BLEU (Papineni et al, 2002), NIST (Dodding-
ton, 2002), TER (Snover et al, 2006), and Me-
teor6 (Denkowski and Lavie, 2011) algorithms.
5.1 Baseline Evaluation of general corpus
At the beginning of our experiment, we trans-
lated the financial labels with the Moses Toolkit,
where the translation model was generated from the
English-German Europarl aligned corpus. The re-
sults are shown in Table 7 as baseline.
5.2 Baseline Evaluation of filtered general
corpus
A second evaluation on translations was done on
a filtered Europarl corpus, depending if a sentence
holds the vocabulary of the ontology to be translated.
We generated five training sets, based on n-grams of
the ontology vocabulary (from unigram to 5-gram)
appearing in the sentence. From the set of aligned
sentences we generated new translation models and
translated again the financial ontology labels with
them. Table 6 illustrates the results of filtering the
6Meteor configuration: -l de, exact, stem, paraphrase
Europarl parallel corpus into smaller (n-gram) train-
ing sets, whereby no training set outperforms signif-
icantly the baseline approach.
model sentences BLEU-4 Meteor OOV
baseline 1920209 4.22 0.1138 37
unigram 1591520 4.25 0.1144 37
bigram 322607 4.22 0.1077 46
3-gram 76775 1.99 0.0932 92
4-gram 4380 2.45 0.0825 296
5-gram 259 0.69 0.0460 743
Table 6: Evaluation results for the filtered Europarl base-
line translation model (OOV - out of vocabulary)
5.3 Evaluation of the knowledge enhanced
general translation model
The final part of our research concentrated on trans-
lations where the general translation model was en-
hanced with ontological knowledge. Table 7 illus-
trates the results using the different similarity mea-
sures, i.e. Dice, Jaccard, Cosine similarity coeffi-
cient.
For the Cosine coefficient we performed two ap-
proaches. For the first step we used only binary val-
ues (bv) from the vector, where in the second ap-
proach we used the frequencies of the contextual in-
formation as real values (rv). The results show that
the Cosine measure using frequencies (rv) performs
best for the METEOR metric. On the other hand the
binary Cosine measure (bv) performs better than the
other metrics in BLEU-2 and NIST metrics.
The Jaccard and Dice similarity coefficient per-
form very similar. They both outperform the general
translation model in BLEU, NIST and TER metrics,
whereby the Jaccard coefficient performs slightly
better than the Dice coefficient. On the other hand
both measures perform worse on the METEOR met-
ric regarding the general model. Overall we observe
that the Jaccard coefficient outperforms the baseline
44
Bleu-2 Bleu-4 NIST Meteor TER
baseline 13.05 4.22 1.789 0.113 1.113
Dice 13.16 4.43 1.800 0.111 1.075
Jaccard 13.17 4.44 1.802 0.111 1.074
Cosine (rv) 12.91 4.20 1.783 0.117 1.108
Cosine (bv) 13.27 4.34 1.825 0.116 1.077
Table 7: Evaluation results for Europarl baseline transla-
tion model and the different similarity measures
approach by 0.22 BLEU points.
5.4 Comparison of translations provided by
the general model and Jaccard similarity
Table 7 illustrates the different approaches that were
performed in our research. As the automatic metrics
give just a slight intuition about the improvements of
the different approaches, we compared the transla-
tions of the general translation model manually with
the translations on which Jaccard similarity coeffi-
cient was performed.
As discussed, Equity can be translated into Ger-
man as Gerechtigkeit when translating it in a gen-
eral domain or into Eigenkapital when translat-
ing it in the financial domain. In the financial
ontology, the segment Equity appears 126 times.
The general translation model translates it wrongly
as Gerechtigkeit, whereby the Jaccard coefficient,
with the help of contextual information, favours
the preferred translation Eigenkapital. Further-
more Equit can be also part of a larger financial
label, e.g. Equity-equivalent partner loans, but
the general translation model still translates it into
Gerechtigkeit. This can be explained by the seg-
mentation during the decoding process, i.e. the SMT
system tokenises this label into separate tokens and
translates each token separately from each other. On
the contrary, the Jaccard similarity coefficient cor-
rects the unigram segment to Eigenkapital.
As part of the label Uncalled unpaid contributions
to subscribed capital (deducted from equity on the
face of the balance sheet), equity is again translated
by the general translation model as Gerechtigkeit. In
this case the Jaccard coefficient cannot correct the
translation, which is caused by the general model
itself, since in all n-best (n=50) translations equity is
translated as Gerechtigkeit. In this case the Jaccard
coefficient reordering does not have any affect.
The manual analysis further showed that the am-
biguous ontology label Securities, e.g. in Write-
downs of long-term financial assets and securities
was also often translated as Sicherheiten7 in the
meaning of certainties or safeties, but was corrected
by the Jaccard coefficient into Wertpapiere, which is
the correct translation in the financial domain.
Finally, the analysis showed that the segment Bal-
ance in Central bank balances was often trans-
lated by the baseline model into Gleichgewichte,8
i.e. Zentralbank Gleichgewichte, whereas the Jac-
card coefficient favoured the preferred translation
Guthaben, i.e. Zentralbank Bankguthaben.
Conclusion and Future Work
Our approach to re-using existing resources showed
slight improvements in the translation quality of the
financial vocabulary. Although the contextual infor-
mation favoured correct translations in the financial
domain, we see a need for more research on the con-
textual information stored in the parallel corpus and
also in the ontology. Also more work has to be done
on analysis of the overlap of the contextual informa-
tion and the ontology vocabulary, e.g. which con-
textual words should have more weight for the simi-
larity measure. Furthermore, dealing with the ontol-
ogy structure, the relations between the labels, i.e.
part-of and parent-child relations, have to be consid-
ered. Once these questions are answered, the next
step will be to compare the classical cosine mea-
sure against more sophisticated similarity measures,
i.e. Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). Instead of measuring simi-
larity between the vectors directly using cosine, we
will investigate the application of ESA to calculate
the similarities between short texts by taking their
linguistic variations into account (Aggarwal et al,
2012).
Acknowledgments
This work is supported in part by the European
Union under Grant No. 248458 for the Monnet
project and Grant No. 296277 for the EuroSenti-
ment project as well as by the Science Foundation
Ireland under Grant No. SFI/08/CE/I1380 (Lion-2).
7ger. Abschreibungen der langfristigen finanziellen
Vermo?genswerte und Sicherheiten
8en. equilibrium, equation, balance
45
References
Aggarwal, N., Asooja, K., and Buitelaar, P. (2012).
DERI&UPM: Pushing corpus based relatedness to
similarity: Shared task system description. In
SemEval-2012.
Apidianaki, M., Wisniewski, G., Sokolov, A., Max, A.,
and Yvon, F. (2012). Wsd for n-best reranking and
local language modeling in smt. In Proceedings of the
Sixth Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 1?9, Jeju, Republic of
Korea. Association for Computational Linguistics.
Carpuat, M. and Wu, D. (2005). Word sense disambigua-
tion vs. statistical machine translation. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ?05, pages 387?394,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Declerck, T., Krieger, H.-U., Thomas, S. M., Buitelaar,
P., O?Riain, S., Wunner, T., Maguet, G., McCrae, J.,
Spohr, D., and Montiel-Ponsoda, E. (2010). Ontology-
based multilingual access to financial reports for shar-
ing business knowledge across europe. In Internal
Financial Control Assessment Applying Multilingual
Ontology Framework.
Denkowski, M. and Lavie, A. (2011). Meteor 1.3: Au-
tomatic Metric for Reliable Optimization and Evalua-
tion of Machine Translation Systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 85?91, Edinburgh, Scotland. Association
for Computational Linguistics.
Doddington, G. (2002). Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145.
Gabrilovich, E. and Markovitch, S. (2007). Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In Proceedings of The Twentieth In-
ternational Joint Conference for Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
Klein, D. and Manning, C. D. (2003). Accurate unlex-
icalized parsing. In Proceeding of the 41st annual
meeting of the association for computational linguis-
tics, pages 423?430.
Koehn, P. (2005). Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages
79?86. AAMT.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran,
C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and
Herbst, E. (2007). Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL, ACL ?07, pages 177?
180.
Mauser, A., Hasan, S., and Ney, H. (2009). Extend-
ing statistical machine translation with discriminative
and trigger-based lexicon models. In Conference on
Empirical Methods in Natural Language Processing,
pages 210?217, Singapore.
McCrae, J., Espinoza, M., Montiel-Ponsoda, E., Aguado-
de Cea, G., and Cimiano, P. (2011). Combining sta-
tistical and semantic approaches to the translation of
ontologies and taxonomies. In Fifth workshop on Syn-
tax, Structure and Semantics in Statistical Translation
(SSST-5).
Och, F. J. and Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Computa-
tional Linguistics, 29.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: A Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, ACL ?02, pages 311?318, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Patry, A. and Langlais, P. (2011). Going beyond word
cooccurrences in global lexical selection for statisti-
cal machine translation using a multilayer perceptron.
In 5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP?11), pages 658?666, Chi-
ang Mai, Thailand.
Schmid, H. (1995). Improvements in part-of-speech tag-
ging with an application to german. In In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In In Proceedings of
Association for Machine Translation in the Americas,
pages 223?231.
Stolcke, A. (2002). Srilm-an extensible language model-
ing toolkit. In Proceedings International Conference
on Spoken Language Processing, pages 257?286.
Wu, H., Wang, H., and Zong, C. (2008). Domain adap-
tation for statistical machine translation with domain
dictionary and monolingual corpora. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08, pages
993?1000.
46
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 146?149,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
DERIUNLP: A Context Based Approach to Automatic Keyphrase
Extraction
Georgeta Bordea
Unit for Natural Language Processing
Digital Enterprise Research Institute
National University of Ireland, Galway
georgeta.bordea@deri.org
Paul Buitelaar
Unit for Natural Language Processing
Digital Enterprise Research Institute
National University of Ireland, Galway
paul.buitelaar@deri.org
Abstract
The DERI UNLP team participated in the
SemEval 2010 Task #5 with an unsuper-
vised system that automatically extracts
keyphrases from scientific articles. Our
approach does not only consider a general
description of a term to select keyphrase
candidates but also context information in
the form of ?skill types?. Even though
our system analyses only a limited set of
candidates, it is still able to outperform
baseline unsupervised and supervised ap-
proaches.
1 Introduction
Keyphrases provide users overwhelmed by the
richness of information currently available with
useful insight into document content but at the
same time they are a valuable input for a variety of
NLP applications such as summarization, cluster-
ing and searching. The SemEval 2010 competition
included a task targeting the Automatic Keyphrase
Extraction from Scientific Articles (Kim et al,
2010). Given a set of scientific articles partic-
ipants are required to assign to each document
keyphrases extracted from text.
We participated in this task with an unsuper-
vised approach for keyphrase extraction that does
not only consider a general description of a term
to select candidates but also takes into consider-
ation context information. The larger context of
our work is the extraction of expertise topics for
Expertise Mining (Bordea, 2010).
Expertise Mining is the task of automatically
extracting expertise topics and expertise profiles
from a collection of documents. Even though the
Expertise Mining task and the Keyphrase Extrac-
tion task are essentially different, it is important
to assess the keyphraseness of extracted expertise
topics, i.e., their ability to represent the content
of a document. Here we will report only relevant
findings for the Keyphrase Extraction task, focus-
ing on the overlapping aspects of the two afore-
mentioned tasks.
After giving an overview of related work in sec-
tion 2 we introduce skill types and present our can-
didate selection method in section 3. Section 4 de-
scribes the features used for ranking and filtering
the candidate keyphrases and Section 5 presents
our results before we conclude in Section 6.
2 Related Work
The current methods for keyphrase extraction can
be categorized in supervised and unsupervised ap-
proaches. Typically any keyphrase extraction sys-
tem works in two stages. In the first stage a gen-
eral set of candidates is selected by extracting the
tokens of a text. In the second stage unsupervised
approaches combine a set of features in a rank to
select the most important keyphrases and super-
vised approaches use a training corpus to learn a
keyphrase extraction model.
Mihalcea and Tarau (2004) propose an unsuper-
vised approach that considers single tokens as ver-
tices of a graph and co-occurrence relations be-
tween tokens as edges. Candidates are ranked us-
ing PageRank and adjacent keywords are merged
into keyphrases in a post-processing step. The
frequency of noun phrase heads is exploited by
Barker and Cornacchia (2000), using noun phrases
as candidates and ranking them based on term fre-
quency and term length.
Kea is a supervised system that uses all n-grams
of a certain length, a Naive Bayes classifier and
tf-idf and position features (Frank et al, 1999).
Turney (2000) introduces Extractor, a supervised
system that selects stems and stemmed n-grams
as candidates and tunes its parameters (mainly re-
lated to frequency, position, length) with a ge-
netic algorithm. Hulth (2004) experiments with
three types of candidate terms (i.e., n-grams, noun
phrase chunks and part-of-speech tagged words
146
that match a set of patterns) and constructs classi-
fiers by rule induction using features such as term
frequency, collection frequency, relative position
and PoS tags.
The candidate selection method is the main dif-
ference between our approach and previous work.
We did not use only a general description of a term
to select candidates, but we also took into consid-
eration context information.
3 The Skill Types Candidate Selection
Method
Skill types are important domain words that are
general enough to be used in different subfields
and that reflect theoretical or practical expertise.
Consider for instance the following extracts from
scientific articles:
...analysis of historical trends...
...duplicate photo detection algorithm ...
...approach for data assimilation...
...methodology for reservoir characterization...
In all four examples the expertise topic (e.g.,
?historical trends?, ?duplicate photo detection al-
gorithm?, ?data assimilation?, ?reservoir charac-
terization?) is introduced by a skill type (e.g.,
?analysis?, ?algorithm?, ?approach?, ?methodol-
ogy?). Some of these skill types are valid for
any scientific area (e.g. ?approach?, ?method?,
?analysis?, ?solution?) but we can also identify
domain specific skill types, e.g., for computer
science ?implementation?, ?algorithm?, ?develop-
ment?, ?framework?, for physics ?proof?, ?prin-
ciples?, ?explanation? and for chemistry ?law?,
?composition?, ?mechanism?, ?reaction?, ?struc-
ture?.
Our system is based on the GATE natural lan-
guage processing framework (Cunningham et al,
2002) and it uses the ANNIE IE system included
in the standard GATE distribution for text tok-
enization, sentence splitting and part-of-speech
tagging. The GATE processing pipeline is de-
picted in Figure 1, where the light grey boxes em-
body components available as part of the GATE
framework whereas the dark grey boxes represent
components implemented as part of our system.
We manually extract a set of 81 single word skill
types for the Computer Science field by analysing
word frequencies for topics from the ACM classi-
fication system
1
. The skill types that appear most
1
ACM classification system: http://www.acm.
org/about/class/
Figure 1: GATE Processing Pipeline
frequently in keyphrases given in the training set
are ?system?, ?model? and ?information?. The
Skill Types Gazetteer adds annotations for skill
types and then the JAPE Transducer uses regular
expressions to annotate candidates.
We rely on a syntactic description of a term to
discover candidate keyphrases that appear in the
right context of a skill type or that include a skill
type. The syntactic pattern for a term is defined
by a sequence of part-of-speech tags, mainly a
noun phrase. We consider that a noun phrase is a
head noun accompanied by a set of modifiers (i.e
nouns, adjectives) that includes proper nouns, car-
dinal numbers (e.g., ?P2P systems?) and gerunds
(e.g., ?ontology mapping?, ?data mining?). Terms
that contain the preposition ?of? (e.g., ?quality of
service?) or the conjunction ?and? (e.g., ?search
and rescue?) were also allowed.
4 Ranking and Filtering
For the ranking stage we use several features al-
ready proposed in the literature such as length of
a keyphrase, tf-idf and position. We also take into
consideration the collection frequency in the con-
text of a skill type.
Ranking. Longer candidates in terms of
number of words are ranked higher, because they
are more descriptive. Keyphrases that appear
more frequently with a skill type in the collection
of documents are also ranked higher. Therefore
we define the rank for a topic as:
147
Method 5P 5R 5F 10P 10R 10F 15P 15R 15F
TF-IDF 22 7.5 11.19 17.7 12.07 14.35 14.93 15.28 15.1
NB 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7
ME 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7
DERIUNLP 27.4 9.35 13.94 23 15.69 18.65 22 22.51 22.25
DUB 15.83 5.13 7.75 13.40 8.68 10.54 13.33 12.96 13.14
Table 1: Baseline and DERIUNLP Performance aver Combined Keywords
System 5P 5R 5F 10P 10R 10F 15P 15R 15F
Best 39.0 13.3 19.8 32.0 21.8 26.0 27.2 27.8 27.5
Average 29.6 10.1 15 26.1 17.8 21.2 21.9 22.4 22.2
Worst 9.4 3.2 4.8 5.9 4.0 4.8 5.3 5.4 5.3
DERIUNLP 27.4 9.4 13.9 23.0 15.7 18.7 22.0 22.5 22.3
Table 2: Performance over Combined Keywords
R
i,j
= Tn
i
? Fn
i
? tfidf
i,j
Where R
i
is the rank for the candidate i and the
document j, Tn
i
is the normalized number of to-
kens (number of tokens divided by the maximum
number of tokens for a keyphrase), Fn
i
is the nor-
malized collection frequency of the candidate in
the context of a skill type (collection frequency di-
vided by the maximum collection frequency), and
tfidf
i
is the TF-IDF for candidate i and topic j
(computed based on extracted topics not based on
all words).
Filtering. Several approaches (Paukkeri et al,
2008; Tomokiyo and Hurst, 2003) use a reference
corpus for keyphrase extraction. We decided to
use the documents available on the Web as a ref-
erence corpus, therefore we use an external web
search engine to filter out the candidates that are
too general from the final result set. If a candi-
date has more than 10
9
hits on the web it is too
general to be included in the final result set. A lot
of noise is introduced by general combination of
words that could appear in any document. We re-
move candidates longer than eight words and we
ignore keyphrases that have one letter words or
that include non-alphanumerical characters.
Acronyms. Acronyms usually replace long
or frequently referenced terms. Results are im-
proved by analysing acronyms (Krulwich and
Burkey, 1996) because most of the times the ex-
panded acronym is reported as a keyphrase, not the
acronym and because our rank is sensitive to the
number of words in a keyphrase. We consider the
length of an acronym to be the same as the length
of its expansion and we report only the expansion
as a keyphrase.
Position. The candidates that appear in the title
or the introduction of a document are more likely
to be relevant for the document. We divide each
document in 10 sections relative to document size
and we increase the ranks for keyphrases first men-
tioned in one of these sections (200% increase for
the first section, 100% increase for the second sec-
tion and 25% for the third section). Candidates
with a first appearance in the last section of a doc-
ument are penalised by 25%.
5 Evaluation
The SemEval task organizers provided two sets
of scientific articles, a set of 144 documents for
training and a set of 100 documents for test-
ing. No information was provided about the sci-
entific domain of the articles but at least some
of them are from Computer Science. The av-
erage length of the articles is between 6 and
8 pages including tables and pictures. Three
sets of answers were provided: author-assigned
keyphrases, reader-assigned keyphrases and com-
bined keyphrases (combination of the first two
sets). The participants were asked to assign a num-
ber of exactly 15 keyphrases per document.
All reader-assigned keyphrases are extracted
from the papers, whereas some of the author-
assigned keyphrases do not occur explicitly in the
text. Two alternations of keyphrase are accepted:
A of B / B A and A?s B. In case that the seman-
tics changes due to the alternation, the alternation
is not included in the answer set. The traditional
evaluation metric was followed, matching the ex-
tracted keyphrases with the keyphrases in the an-
swer sets and calculating precision, recall and F-
score. In both tables the column labels start with a
number which stands for the top 5, 10 or 15 candi-
dates. The characters P, R, F mean micro-averaged
precision, recall and F-scores. For baselines, 1, 2,
3 grams were used as candidates and TF-IDF as
features.
In Table 1 the keyphrases extracted by our sys-
tem are compared with keyphrases extracted by
148
an unsupervised method that ranks the candidates
based on TF-IDF scores and two supervised meth-
ods using Naive Bayes (NB) and maximum en-
tropy(ME) in WEKA
2
. Our performance is well
above the baseline in all cases.
To show the contribution of skill types we in-
cluded the results for a baseline version of our
system (DUB) that does not rank the candidates
using the normalized collection frequency in the
context of a skill type Fn
i
but the overall collec-
tion frequency (i.e., the number of occurrences of
a keyphrase in the corpus). The significantly in-
creased results compared to our baseline version
show the effectiveness of skill types for keyphrase
candidate ranking.
Table 2 presents our results in comparison with
results of other participants. Even though our sys-
tem considers in the first stage a significantly lim-
ited set of candidates the results are very close to
the average results of other participants. Our sys-
tem performed 8th best out of 19 participants for
top 15 keyphrases, 10th best for top 10 keyphrases
and 13th best for top 5 keyphrases, which indicates
that our approach could be improved by using a
more sophisticated ranking method.
6 Conclusions
In this paper we have reported the performance
of an unsupervised approach for keyphrase extrac-
tion that does not only consider a general descrip-
tion of a term to select keyphrase candidates but
also takes into consideration context information.
The method proposed here uses term extraction
techniques (the syntactic description of a term),
classical keyword extraction techniques(TF-IDF,
length, position) and contextual evidence (skill
types).
We argued that so called ?skill types? (e.g.,
?methods?, ?approach?, ?analysis?) are a useful
instrument for selecting keyphrases from a doc-
ument. Another novel aspect of this approach is
using the collection of documents available on the
Web (i.e., number of hits for a keyphrase) instead
of a reference corpus. It would be interesting to
evaluate the individual contributions of skill types
for Keyphrase Extraction by adding them as a fea-
ture in a classical system like KEA.
Future work will include an algorithm for auto-
matic extraction of skill types for a domain and an
analysis of the performance of each skill type.
2
WEKA:http://www.cs.waikato.ac.nz/ml/
weka/
7 Aknowledgements
This work is supported by Science Foundation Ire-
land under Grant No. SFI/08/CE/I1380 (Lion-2).
References
Ken Barker and Nadia Cornacchia. 2000. Using Noun
Phrase Heads to Extract Document Keyphrases. In
Canadian Conference on AI, pages 40?52. Springer.
Georgeta Bordea. 2010. Concept Extraction Applied
to the Task of Expert Finding. In Extended Semantic
Web Conference 2010, PhD Symposium. Springer.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graph-
ical Development Environment for Robust NLP
Tools and Applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Compu-
tational Linguistics.
Eibe Frank, Gordon W Paynter, Ian H Witten, Carl
Gutwin, and Craig G Nevill-Manning. 1999.
Domain-Specific Keyphrase Extraction. In Pro-
ceedings of the 16th International Joint Conference
on Aritfiicial Intelligence, pages 668?673.
Anette Hulth. 2004. Enhancing Linguistically Ori-
ented Automatic Keyword Extraction. In Proceed-
ings of HLT/NAACL: Short Papers, pages 17?20.
Su Nam Kim, Alyona Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the ACL 2010 Workshop on
Evaluation Exercises on Semantic Evaluation (Se-
mEval 2010).
Bruce Krulwich and Chad Burkey. 1996. Learn-
ing user information interests through extraction of
semantically significant phrases. In Proc. AAAI
Spring Symp. Machine Learning in Information Ac-
cess, Menlo Park, Calif. Amer. Assoc. for Artificial
Intelligence.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 404?411.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Polla Matti,
and Timo Honkela. 2008. A Language-Independent
Approach to Keyphrase Extraction and Evaluation.
In Coling 2008 Posters, number August, pages 83?
86.
Takashi Tomokiyo and Matthew Hurst. 2003. A Lan-
guage Model Approach to Keyphrase Extraction. In
Proceedings of the ACL 2003 work- shop on Multi-
word expressions, pages 33?40.
Peter D Turney. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval, 2:303?
336.
149
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 643?647,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DERI&UPM: Pushing Corpus Based Relatedness to Similarity: Shared
Task System Description
Nitish Aggarwal? Kartik Asooja? Paul Buitelaar?
?Unit for Natural Language Processing
Digital Enterprise Research Institute
National University of Ireland, Galway, Ireland
firstname.lastname@deri.org
?Ontology Engineering Group
Universidad Politecnica de Madrid
Madrid, Spain
asooja@gmail.com
Abstract
In this paper, we describe our system submit-
ted for the semantic textual similarity (STS)
task at SemEval 2012. We implemented two
approaches to calculate the degree of simi-
larity between two sentences. First approach
combines corpus-based semantic relatedness
measure over the whole sentence with the
knowledge-based semantic similarity scores
obtained for the words falling under the same
syntactic roles in both the sentences. We fed
all these scores as features to machine learn-
ing models to obtain a single score giving the
degree of similarity of the sentences. Lin-
ear Regression and Bagging models were used
for this purpose. We used Explicit Semantic
Analysis (ESA) as the corpus-based seman-
tic relatedness measure. For the knowledge-
based semantic similarity between words, a
modified WordNet based Lin measure was
used. Second approach uses a bipartite based
method over the WordNet based Lin measure,
without any modification. This paper shows
a significant improvement in calculating the
semantic similarity between sentences by the
fusion of the knowledge-based similarity mea-
sure and the corpus-based relatedness measure
against corpus based measure taken alone.
1 Introduction
Similarity between sentences is a central concept
of text analysis, however previous studies about
semantic similarities have mainly focused either
on single word similarity or complete document
similarity. Sentence similarity can be defined by the
degree of semantic equivalence of two given sen-
tences, where sentences are typically 10-20 words
long. The role of sentence semantic similarity mea-
sures in text-related research is increasing due to
potential number of applications such as document
summarization, question answering, information
extraction & retrieval and machine translation.
One plausible limitation of existing methods for
sentence similarity is their adaptation from long text
(e.g. documents) similarity methods, where word
co-occurrence plays a significant role. However,
sentences are too short, thats why taking syntac-
tic role of each word with its narrow semantic
meaning into account, can be highly relevant to
reflect the semantic equivalence of two sentences.
These narrow semantics can be reflected from any
existing large lexicons [(Wu and Palmer, 1994)
and (Lin, 1998)]; nevertheless, these lexicons can
not provide the semantics of words which are out
of lexicon (e.g. guy) or multiword expressions.
These semantics can be represented by a large
distributed semantic space such as Wikipedia and
similarity can be reflected by relatedness of these
extracted semantics. However, relatedness covers
broader space than similarity, which forced us to
tune the Wikipedia based relatedness with lexical
structure (e.g. WordNet) based similarities driven
by linguistic syntactic structure, in reflecting more
sophisticated similarity of two given sentences.
In this work, we present a sentence similarity using
ESA and syntactic similarities. The rest of this
paper is organized as follows. Section 2 explores the
related work. Section 3 describes our approaches
643
in detail. Section 4 explains our three different
submitted runs for STS task. Section 5 shows the
results and finally we conclude in section 6.
2 Related Work
In recent years, there have been a variety of efforts
in improving semantic similarity measures, however
most of these approaches address this problem from
the viewpoint of large document similarity based on
word co-occurrence using string pattern or corpus
statistics. Corpus based approaches such as Latent
Semantic Analysis (LSA) [(Landauer et. al, 1998)
and (Foltz et. al, 1998)] and ESA (Gabrilovich and
Markovitch, 2007) use corpus statistics information
about all words and reflect their semantics in
distributional high semantic space. However, these
approaches perform quite well for long texts as they
use word co-occurrence and relying on the principle
that words which are used in the same contexts
tend to have related meanings. In case of short text
similarities, syntactic role of each word with its
meaning plays an important role.
There are several linguistic measures [( Achananu-
parp et. al, 2008) and (Islam and Inkpen, 2008)],
which can account for pseudo-syntactic information
by analyzing their word order using n-gram. To do
this, Islam and Inkpen defined a syntactic measure,
which considers the word order between two
strings by computing the maximal ordered word
overlapping. (Oliva et. al, 2011) present a similarity
measure for sentences and short text that takes
syntactic information, such as morphology and
parsing tree, into account and calculate similarities
between words with same syntactic role, by using
WordNet.
Our work takes inspiration from existing approaches
that exploit a combination of Wikipedia based re-
latedness with lexical structure based similarities
driven by linguistic syntactic structure.
3 Methodology
We implemented two approaches for the STS
task [(Agirre et. al, 2012)]. First approach is a
fusion of corpus-based semantic relatedness and
knowledge-based semantic similarity measures.
The core of this combination is the corpus-based
measure because the combination includes the
corpus-based semantic relatedness score over the
whole sentences and the knowledge-based semantic
similarity scores for the words falling under the
same syntactic roles in both the sentences. Machine
learning models are trained by taking all these
scores as different features. For the submission,
we used Linear regression and Bagging models.
Also, the equation obtained after training the linear
regression model shows more weightage to the score
obtained by the corpus-based relatedness measure
as this is the only score (feature), which reflects the
semantic relatedness/similarity score over the full
sentences, out of all the considered features for the
model. We used ESA as the corpus based semantic
relatedness measure and modified WordNet-based
Lin measure as the knowledge-based similarity.
The WordNet-based Lin relatedness measure was
modified to reflect better the similarity between
the words. For the knowledge-based similarity,
currently we considered only the words lying in the
three major syntactic role categories i.e. subjects,
actions and the objects. We see the first approach
as the corpus-based measure ESA tuned with the
knowledge-based measure. Thus, it is referred as
TunedESA later in the paper.
Our second approach is based on the bipartite
method over the WordNet based semantic relat-
edness measures. WordNet-based Lin measure
(without any modification) was used for calcu-
lating the relatedness scores for all the possible
corresponding pair of words appearing in both the
sentences. Then, the similarity/relatedness score
for the sentences is calculated by perceiving the
problem as the computation of a maximum total
matching weight of a bipartite graph having the
words as nodes and the relatedness scores as the
weight of the edges between the nodes. To solve
this, we used Hungarian method. Later, we refer
this method as WordNet-Bipartite.
3.1 TunedESA
In this approach, the ESA based relatedness score
for the full sentences is combined with the modified
WordNet-based Lin similarity scores calculated for
the words falling under the corresponding syntactic
role category in both the sentences.
644
ALL Rank-ALL ALLnrm RankNrm Mean RankMean
Baseline 0.3110 87 0.6732 85 0.4356 70
Run1 0.5777 52 0.8158 20 0.5466 52
Run2 0.5833 51 0.8183 17 0.5683 42
Run3 0.4911 67 0.7696 57 0.5377 53
Table 1: Overall Rank and Pearson Correlation of all runs
MSRpar MSRvid SMTeuro OnWN SMTnews
Baseline 0.4334 0.2996 0.4542 0.5864 0.3908
ESA? 0.2778 0.8178 0.3914 0.6541 0.4366
Run1 0.3675 0.8427 0.3534 0.6030 0.4430
Run2 0.3720 0.8330 0.4238 0.6513 0.4489
Run3 0.5320 0.6874 0.4514 0.5827 0.2818
Table 2: Pearson Correlation of all runs with all five STS test datasets
TunedESA could be summarized as these four
basic steps:
? Calculate the ESA relatedness score between
the sentences.
? Find the words corresponding to the linguistic
syntactical categories like subject, action and
object of both the sentences.
? Calculate the semantic similarity between the
words falling in the corresponding subjects, ac-
tions and objects in both the sentences using
modified WordNet-based measure Lin.
? Combine these four scores for ESA, Subject,
Action and Object to get the final similarity
score on the basis of an already learned ma-
chine learning model with the training data.
ESA is a promising technique to find the relatedness
between documents. The texts which need to be
compared are represented as high dimensional vec-
tors containing the TF-IDF weight between the term
and the Wikipedia article. The semantic relatedness
measure is calculated by taking the cosine measure
between these vectors. In this implementation of
ESA 1, the score was calculated by considering the
1ESA? considering full sentence at a time to make the vector
i.e. different from standard ESA
full sentence at a time for making the Wikipedia
article vector while in the standard ESA, vectors
are made for each word of the text followed by the
addition of all these vectors to represent the final
vector for the text/sentence. It was done just to
reduce the time complexity.
To calculate the lexical similarity between the
words, we implemented WordNet-based semantic
relatedness measure Lin. This score was modified to
reflect a better similarity between the words. In the
current system, basic linguistic syntactic categories
i.e. subjects, actions and objects were used. For
instance, below is a sentences pair from the training
MSRvid dataset with the gold standard score and
the syntactic roles.
Sentence 1: A man is playing a guitar.
Subject: Man, Action: play, Object: guitar
Sentence 2: A man is playing a flute.
Subject: Man, Action: play, Object: flute
Gold Standard Score (0-5): 2.2
As the modification, the scores given by Lin
measure were used only for the cases where sub-
sumption relation or hypernymy/hyponymy exists
645
between the words. This modification was done
only for the words falling under the category of
subjects and objects.
3.2 WordNet Bipartite
WordNet-based semantic relatedness measure was
used for the second approach.
Following steps are performed :
? Each sentence is tokenized to obtain the words.
? Semantic relatedness between every possible
pair of words in both the sentences is calculated
using WordNet-based measure e.g. Lin.
? Using the scores obtained in the second step,
the semantic similarity/relatedness between the
sentences is calculated by transforming the
problem as that of computing the maximum to-
tal matching weight of a bipartite graph, which
can be done by using Hungarian method.
4 System Description
We submitted three runs in the semantic textual
similarity task. The first two runs are based on the
first approach i.e. TunedESA and they differ only in
the machine learning algorithm used for obtaining
the final similarity score based on all the considered
scores/features.
ESA was implemented on the current Wikipedia
dump. WordNet based relatedness measure Lin
was modified to give a better semantic similarity
degree. Stanford Core-NLP library was used for
obtaining the words with their syntactic roles.
All the required scores/feature i.e. ESA based
relatedness for the complete sentences and mod-
ified WordNet-based Lin similarity scores were
calculated for the corresponding words lying in
the same syntactic categories. Bagging and Linear
Regression models were built using the training data
for the first and second runs respectively. Based on
the category of the test dataset, model was trained
on the corresponding training dataset.
For the surprise test datasets, we trained our
model with the training dataset of the MSRvid data
based on the fact that we obtained good results with
this category. Then the built models were used for
calculating the similarity scores for the test data.
For the third run, WordNet Bipartite method
was used to calculate the similarity scores. It didn?t
require any training.
5 Results and Discussion
All above described runs are evaluated on STS
test dataset. Table 1 shows the overall results2 of
our three runs against the baseline system which
follows the bag of words approach. Table 2 shows
the Pearson correlation on different test datasets for
all the three runs. It provides a comparison between
corpus based relatedness measure ESA and our
system TunedESA (Run 1 & Run 2).
The results show significant improvement against
ESA. Although, it can be seen that the baseline
results are even better than of the ESA in the cases
of MSRpar and SMTeuro. It may be because this
implementation of ESA is not the standard one.
6 Conclusion
We presented a method to calculate the degree of
sentence similarity based on tuning the corpus based
relatedness measure with the knowledge-based sim-
ilarity measure over the syntactic roles. The results
show a definite improvement by the fusion. As
future work, we plan to improve the syntactic role
handling and considering more syntactical cate-
gories. Also, experimentation3 with standard ESA
and other semantic similarity/relatedness measures
needs to be performed.
Acknowledgments
This work is supported in part by the European
Union under Grant No. 248458 for the Monnet
project as well as by the Science Foundation Ireland
under Grant No. SFI/08/CE/I1380 (Lion-2).
2results can also be found at http://www.cs.york.
ac.uk/semeval-2012/task6/index.php?id=
results-update with the name nitish aggarwal
3We plan to provide the further results and information at
http://www.itssimilar.com/
646
References
Achananuparp Palakorn and Xiaohua Hu and Xiajiong
Shen 2008 The Evaluation of Sentence Similarity
Measures, In: DaWaK. pp. 305-316
Agirre Eneko , Cer Daniel, Diab Mona and Gonzalez-
Agirre Aitor 2012 SemEval-2012 Task 6: A Pilot on
Semantic Textual Similarity. In: Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Foltz P. W., Kintsch W. and Landauer T. K. 1998. In:
journal of the Discourse Processes. pp. 285-307, The
measurement of textual Coherence with Latent Se-
mantic Analysis,
Gabrilovich Evgeniy and Markovitch Shaul 2007 Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis, In: Proceedings of The
Twentieth International Joint Conference for Artificial
Intelligence. pp. 1606?1611,
Islam, Aminul and Inkpen, Diana 2008 Semantic
text similarity using corpus-based word similarity and
string similarity, In: journal of ACM Trans. Knowl.
Discov. Data. pp. 10:1?10:25
Landauer Thomas K. ,Foltz Peter W. and Laham Darrell
1998. An Introduction to Latent Semantic Analysis,
In: Journal of the Discourse Processes. pp. 259-284,
Lin Dekang 1998 Proceeding of the 15th International
Conference on Machine Learning. pp. 296?304 An
information-theoretic definition of similarity
Oliva, Jesu?s and Serrano, Jose? Ignacio and del Castillo,
Mar??a Dolores and Iglesias, A?ngel April, 2011
SyMSS: A syntax-based measure for short-text seman-
tic similarity In: journal of Data Knowledge Engineer-
ing. pp. 390?405
Wu, Zhibiao and Palmer, Martha 1994 Verbs seman-
tics and lexical selection, In: Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics,
647
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 51?56,
Dublin, Ireland, August 23-24 2014.
Exploring ESA to Improve Word Relatedness
Nitish Aggarwal Kartik Asooja Paul Buitelaar
Insight Centre for Data Analytics
National University of Ireland
Galway, Ireland
firstname.lastname@deri.org
Abstract
Explicit Semantic Analysis (ESA) is an ap-
proach to calculate the semantic relatedness
between two words or natural language texts
with the help of concepts grounded in human
cognition. ESA usage has received much at-
tention in the field of natural language pro-
cessing, information retrieval and text analy-
sis, however, performance of the approach de-
pends on several parameters that are included
in the model, and also on the text data type
used for evaluation. In this paper, we investi-
gate the behavior of using different number of
Wikipedia articles in building ESA model, for
calculating the semantic relatedness for differ-
ent types of text pairs: word-word, phrase-
phrase and document-document. With our
findings, we further propose an approach to
improve the ESA semantic relatedness scores
for words by enriching the words with their
explicit context such as synonyms, glosses and
Wikipedia definitions.
1 Introduction
Explicit Semantic Analysis (ESA) is a distributional
semantic model (Harris, 1954) that computes the
relatedness scores between natural language texts
by using high dimensional vectors. ESA builds
the high dimensional vectors by using the explicit
concepts defined in human cognition. Gabrilovich
and Markovitch (2007) introduced the ESA model
in which Wikipedia and Open Directory Project
1
was used to obtain the explicit concepts. ESA con-
siders every Wikipedia article as a unique explicit
1
http://www.dmoz.org
topic. It also assumes that the articles are topically
orthogonal. However, recent work (Gottron et
al., 2011) has shown that by using the documents
from Reuters corpus instead of Wikipedia articles
can also achieve comparable results. ESA model
includes various parameters (Sorg and Cimiano,
2010) that play important roles on its performance.
Therefore, the model requires further investigation
in order to better tune the parameters.
ESA model has been adapted very quickly in
different fields related to text analysis, due to the
simplicity of its implementation and the availability
of Wikipedia corpus. Gabrilovich and Markovitch
(2007) evaluated the ESA against word relatedness
dataset WN353 (Finkelstein et al., 2001) and doc-
ument relatedness dataset Lee50 (Lee et al., 2005)
by using all the articles from Wikipedia snapshot of
11 Nov, 2005. However, the results reported using
different implementations (Polajnar et al., 2013)
(Hassan and Mihalcea, 2011) of ESA on same
datasets (WN353 and Lee50) vary a lot, due the
specificity of ESA implementation. For instance,
Hassan and Mihalcea (2011) found a significant
difference between the scores obtained from their
own implementation and the scores reported in the
original article (Gabrilovich and Markovitch, 2007).
In this paper, first, we investigate the behavior
of ESA model in calculating the semantic related-
ness for different types of text pairs: word-word,
phrase-phrase and document-document by using
different number of Wikipedia articles for building
the model. Second, we propose an approach
51
for context enrichment of words to improve the
performance of ESA on word relatedness task.
2 Background
The ESA model can be described as a method of
obtaining the relatedness score between two texts by
quantifying the distance between two high dimen-
sional vectors. Every explicit concept represents a
dimension of the ESA vector, and the associativity
weight of a given word with the explicit concept
can be taken as magnitude of the corresponding
dimension. For instance, there is a word t, ESA
builds a vector v, where v =
?
N
i=0
a
i
? c
i
and c
i
is
i
th
concept from the explicit concept space, and a
i
is the associativity weight of word t with the concept
c
i
. Here, N represents the total number of concepts.
In our implementation, we build ESA model by
using Wikipedia articles as explicit concepts, and
take the TFIDF weights as associativity strength.
Similarly, ESA builds the vector for natural lan-
guage text by considering it as a bag of words. Let
T = {t
1
, t
2
, t
3
...t
n
}, where T is a natural language
text that has n words. ESA generates the vector
V, where V =
?
t
k
T
v
k
and v =
?
N
i=0
a
i
? c
i
. v
k
represents the ESA vector of a individual words as
explained above. The relatedness score between two
natural language texts is calculated by computing
cosine product of their corresponding ESA vectors.
In recent years, some extensions (Polajnar et
al., 2013) (Hassan and Mihalcea, 2011) (Scholl et
al., 2010) have been proposed to improve the ESA
performance, however, they have not discussed the
consistency in the performance of ESA. Polajnar
et al. (2013) used only 10,000 Wikipedia articles
as the concept space, and got significantly different
results on the previously evaluated datasets. Hassan
and Mihalcea (2011) have not discussed the ESA
implementation in detail but obtained significantly
different scores. Although, these proposed exten-
sions got different baseline ESA scores but they
improve the relatedness scores with their additions.
Polajnar et al. (2013) used the concept-concept
correlation to improve the ESA model. Hassan and
Mihalcea (2011) proposed a model similar to ESA,
which builds the high dimensional vector of salient
concepts rather than explicit concepts. Gortton et
al. (2011) investigated the ESA performance for
document relatedness and showed that ESA scores
are not tightly dependent on the explicit concept
spaces.
Minimum unique Total number of
words (K) articles (N)
100 438379
300 110900
500 46035
700 23608
900 13718
1100 8322
1300 5241
1500 3329
1700 2126
1900 1368
Table 1: The total number of retrieved articles for differ-
ent values of K
3 Investigation of ESA model
Although Gortton et al. (2011) has shown that ESA
performance on document pairs does not get af-
fected by using different number of Wikipedia ar-
ticles, we further examine it for word-word and
phrase-phrase pairs. We use three different datasets
WN353, SemEvalOnWN (Agirre et al., 2012) and
Lee50. WN353 contains 353 word pairs, SemEval-
OnWN consists of 750 short phrase/sentence pairs,
and Lee50 is a collection of 50 document pairs.
All these datasets contain relatedness scores given
by human annotators. We evaluate ESA model
on these three datasets against different number of
Wikipedia articles. In order to select different num-
ber of Wikipedia articles, we sort them according to
the total number of unique words appearing in each
article. We select N articles, where N is total num-
ber of articles which have at least K unique words.
Table 1 shows the total number of retrieved articles
for different values of K. We build 20 different ESA
models with the different values of N retrieved by
varying K from 100 to 2000 with an interval of 100.
Figure 1 illustrates Spearman?s rank correlation of
all the three types of text pairs on Y-axis while X-
axis shows the different values of N which are taken
to build the model. It shows that ESA model gener-
ates very consistent results for phrase pairs similar
to the one reported in (Aggarwal et al., 2012), how-
52
Figure 1: ESA performance on different types of text
pairs by varying the total number of articles
ever, the correlation scores decreases monotonously
in the case of word pairs as the number of articles
goes down. In the case of document pairs, ESA pro-
duces similar results until the value of N is chosen
according to K = 1000, but after that, it decreases
quickly because the number of articles becomes too
low for making a good enough ESA model. All this
indicates that word-word relatedness scores have a
strong impact of changing the N in comparison of
document-document or phrase-phrase text pairs. An
explanation to this is that the size of the ESA vec-
tor for a word solely depends upon the popularity
of the given word, however, in the case of text, the
vector size depends on the popularity summation of
all the words appearing in the given text. It suggests
that the word relatedness problem can be reduced to
short text relatedness by adding some related con-
text with the given word. Therefore, to improve
the ESA performance for word relatedness, we pro-
pose an approach for context enrichment of words.
We perform context enrichment by concatenating re-
lated context with the given word and use this con-
text to build the ESA vector, which transforms the
word relatedness problem to phrase relatedness.
4 Context Enrichment
Context enrichment is performed by concatenating
the context defining text to the given word before
building the ESA vector. Therefore, instead of build-
ing the ESA vector of a word, the vector is built for
the short text that is obtained after concatenating the
related context. This is similar to classical query ex-
pansion task (Aggarwal and Buitelaar, 2012; Pan-
tel and Fuxman, 2011), where related concepts are
concatenated with a query to improve the informa-
tion retrieval performance. We propose three differ-
ent methods to obtain related context: 1) WordNet-
based Context Enrichment 2) Wikipedia-based Con-
text Enrichment, and 3) WikiDefinition-based Con-
text Enrichment.
4.1 WordNet-based Context Enrichment
WordNet-based context enrichment uses the Word-
Net synonyms to obtain the context, and concate-
nates them into the given word to build the ESA vec-
tor. However, WordNet may contain more than one
synset for a word, where each synset represents a
different semantic sense. Therefore, we obtain more
than one contexts for a given word, by concatenat-
ing the different synsets. Further, we calculate ESA
score of every context of a given word against all the
contexts of the other word which is being compared,
and consider the highest score as the final related-
ness score. For instance, there is a given word pair
?train and car?, car has 8 different synsets that build
8 different contexts, and train has 6 different synsets
that build 6 different contexts. We calculate the ESA
score of these 8 contexts of car to the 6 contexts of
train, and finally select the highest obtained score
from all of the 24 calculated scores.
4.2 Wikipedia-based Context Enrichment
In this method, the context is defined by the word
usage in Wikipedia articles. We retrieve top 5
Wikipedia articles by querying the articles? content,
and concatenate the short abstracts of the retrieved
articles to the given word to build the ESA vector.
Short abstract is the first two sentences of Wikipedia
article and has a maximum limit of 500 characters.
In order to retrieve the top 5 articles from Wikipedia
for a given word, we build an index of all Wikipedia
articles and use TF-IDF scores. We further explain
53
our implementation in Section 5.1.
4.3 WikiDefinition-based Context Enrichment
This method uses the definition of a given word from
Wikipedia. To obtain a definition from Wikipedia,
we first try to find a Wikipedia article on the given
word by matching the Wikipedia title. As definition,
we take the short abstract of the Wikipedia article.
For instance, for a given word ?train?, we take the
Wikipedia article with the title ?Train?
2
. If there is
no such Wikipedia article, then we use the previous
method ?Wikipedia-based Context Enrichment? to
get the context defining text for the given word. In
contrary to the previous method for defining context,
here we first try to get a more precise context as it
comes from the Wikipedia article on that word only.
After obtaining the definition, we concatenate it to
the given word to build the ESA vector. At the time
of experimentation, we were able to find 339 words
appearing as Wikipedia articles out of 437 unique
words in the WN353 dataset.
Figure 2: Effect of different types of context enrichments
on WN353 gold standard
2
http://en.wikipedia.org/wiki/Train
5 Experiment
5.1 ESA implementation
In this section, we describe the implementation of
ESA and the parameters used to build the model.
We build an index over all Wikipedia articles from
the pre-processed Wikipedia dump from November
11, 2005 (Gabrilovich, 2006). We use Lucene
3
to
build the index and retrieve the articles using TF-
IDF scores. As described in section 3, we build 20
different indices with different values of total num-
ber of articles (N).
5.2 Results and Discussion
To evaluate the effect of the aforementioned
approaches for context enrichment, we compare
the results obtained by them against the results
generated by ESA model as a baseline. We cal-
culated the scores on WN353 word pairs dataset
by using ESA, WordNet-based Context Enrich-
ment (ESA CEWN), Wikipedia-based Context
Enrichment (ESA CEWiki) and WikiDefition-based
Context Enrichment (ESA CEWikiDef). Further,
we examine the performance of context enrichment
approaches by reducing the total number of articles
taken to build the model. Figure 2 shows that the
proposed methods of context enrichment signifi-
cantly improve over the ESA scores for different
values of N.
Table 2 reports the results obtained by using
different context enrichments and ESA model.
It shows Spearman?s rank correlation on four
different values of N. All the proposed con-
text enrichment methods improve over the ESA
baseline scores. Context enrichments based on
Wikipedia outperforms the other methods, and
ESA CEWikiDef significantly improves over the
ESA baseline. Moreover, given a very less number
of Wikipedia articles used for building the model,
ESA CEWikiDef obtains a correlation score which
is considerably higher than the one obtained by
ESA baseline. ESA CEWN and ESA CEWiki can
include some unrelated context as they do not care
about the semantic sense of the given word, for
instance, for a given word ?car?, ESA CEWiki
3
https://lucene.apache.org/
54
K Total articles (N) ESA ESA CEWN ESA CEWiki ESA CEWikiDef
100 438,379 0.711 0.692 0.724 0.741
200 221,572 0.721 0.707 0.726 0.743
500 46,035 0.673 0.670 0.679 0.698
1000 10,647 0.563 0.593 0.598 0.614
Table 2: Spearman rank correlation scores on WN353 gold standard
includes the context about the word ?car? at surface
level rather than at the semantic level. However,
ESA CEWikiDef only includes the definition if it
does not refer to more than one semantic sense,
therefore, ESA CEWikiDef outperforms all other
types of context enrichment.
We achieved best results in all the cases by tak-
ing all the articles which has a minimum of 200
unique words (K=200). This indicates that further
increasing the value of K considerably decreases
the value of N, consequently, it harms the overall
distributional knowledge of the language, which is
the core of ESA model. However, decreasing the
value of K introduces very small Wikipedia articles
or stubs, which do not provide enough content on a
subject.
6 Conclusion
In this paper, we investigated the ESA performance
for three different types of text pairs: word-word,
phrase-phrase and document-document. We showed
that ESA scores varies significantly for word re-
latedness measure with the change in the number
(N) and length (?K which is the number of unique
words) of the Wikipedia articles used for building
the model. Further, we proposed context enrichment
approaches for improving word relatedness compu-
tation by ESA. To this end, we presented three dif-
ferent approaches: 1) WordNet-based, 2) Wikipedia-
based, and 3) WikiDefinition-based, and we real-
ized that concatenating the context defining text im-
proves the ESA performance for word relatedness
task.
Acknowledgments
This work has been funded in part by a research
grant from Science Foundation Ireland (SFI) under
Grant Number SFI/12/RC/2289 (INSIGHT) and by
the EU FP7 program in the context of the project
LIDER (610782).
References
Nitish Aggarwal and Paul Buitelaar. 2012. Query expan-
sion using wikipedia and dbpedia. In CLEF.
Nitish Aggarwal, Kartik Asooja, and Paul Buitelaar.
2012. DERI&UPM: Pushing corpus based relatedness
to similarity: Shared task system description. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceedings of
the Main Conference and the Shared Task, and Volume
2: Proceedings of the Sixth International Workshop on
Semantic Evaluation, SemEval ?12, pages 643?647,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385?393. Association for Computational
Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of the 10th international
conference on World Wide Web, pages 406?414. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich. 2006. Feature generation for
textual information retrieval using world knowledge.
Ph.D. thesis, Technion - Israel Institute of Technology,
Haifa, Israel, December.
Thomas Gottron, Maik Anderka, and Benno Stein. 2011.
Insights into explicit semantic analysis. In Proceed-
ings of the 20th ACM international conference on In-
formation and knowledge management, pages 1961?
1964. ACM.
Zellig Harris. 1954. Distributional structure. In Word 10
(23), pages 146?162.
55
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Michael David Lee, BM Pincombe, and Matthew Brian
Welsh. 2005. An empirical evaluation of models of
text document similarity. Cognitive Science.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures:
Associating web queries with structured entities. In
ACL, pages 83?92.
Tamara Polajnar, Nitish Aggarwal, Kartik Asooja, and
Paul Buitelaar. 2013. Improving esa with docu-
ment similarity. In Advances in Information Retrieval,
pages 582?593. Springer.
Philipp Scholl, Doreen B?ohnstedt, Renato Dom??nguez
Garc??a, Christoph Rensing, and Ralf Steinmetz. 2010.
Extended explicit semantic analysis for calculating
semantic relatedness of web resources. In Sustain-
ing TEL: From Innovation to Learning and Practice,
pages 324?339. Springer.
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natural
Language Processing and Information Systems, pages
36?48. Springer.
56
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 346?350,
Dublin, Ireland, August 23-24, 2014.
INSIGHT Galway: Syntactic and Lexical Features for Aspect Based
Sentiment Analysis
Sapna Negi
Insight Centre for Data Analytics
National University of Ireland
Galway
{sapna.negi, paul.buitelaar}@insight-centre.org
Paul Buitelaar
Insight Centre for Data Analytics
National University of Ireland
Galway
Abstract
This work analyses various syntactic and
lexical features for sentence level aspect
based sentiment analysis. The task fo-
cuses on detection of a writer?s sentiment
towards an aspect which is explicitly men-
tioned in a sentence. The target sentiment
polarities are positive, negative, conflict
and neutral. We use a supervised learning
approach, evaluate various features and
report accuracies which are much higher
than the provided baselines. Best features
include unigrams, clauses, dependency re-
lations and SentiWordNet polarity scores.
1 Introduction
The term aspect refers to the features or aspects
of a product, service or topic being discussed in a
text. The task of detection of sentiment towards
these aspects involves two major processing steps,
identifying the aspects in the text and identifying
the sentiments towards these aspects. Our work
describes a submitted system in the Aspect Based
Sentiment Analysis task of SemEval 2014 (Pontiki
et al., 2014). The task was further divided into 4
subtasks; our work corresponds to the subtask 2,
called Aspect Term Polarity Detection. We pre-
dict the polarity of sentiments expressed towards
the aspect terms which are already annotated in a
sentence. The target polarity types are positive,
negative, neutral and conflict.
We employ a statistical classifier and experiment
with various syntactic and lexical features. Se-
lected features for the submitted system include
words which hold certain dependency relations
with the aspect terms, clause in which the aspect
This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
term appears, unigrams, and sum of lexicon based
sentiment polarities of the words in the clause.
2 Related Work
Pang et al. (2002) proved that unigrams and
bigrams, adjectives and part of speech tags are
important features for a machine learning based
sentiment classifier. Later, verbs and adjectives
were also identified as important features (Ches-
ley, 2006). Meena and Prabhakar (2007) per-
formed sentence level sentiment analysis using
rules based on clauses of a sentence. However,
in our case we cannot simply consider the adjec-
tives and verbs as features, since they might re-
late to different aspects. For example, in the sen-
tence ?The pizza is the best if you like thin crusted
pizza.?, sentiment towards ?pizza? is positive be-
cause of the adjective ?best?; however for the term
?thin crusted pizza?, ?like? would be the sentiment
verb. Therefore, only those adjectives and verbs
which relate to the target aspect, can be consid-
ered as the indicator of their polarity. Wilson et
al. (2009) showed that the words which share cer-
tain dependency relations with aspect terms, tend
to indicate the sentiments expressed towards those
terms.
Saif et al. (2012) showed the co-relation between
topic and sentiment polarity in tweets, and as-
serted that majority of people tend to express sim-
ilar sentiments towards same topics, especially in
the case of positive sentiments. The baseline ap-
proach for this task (Pontiki et al., 2014) also as-
sociates polarity with aspect terms. Therefore, we
also consider aspect term as a potential feature.
Our approach for this task is based on our obser-
vation of the data, with a provenance of the above
mentioned findings.
3 Approach
We employ a statistical classifier which trains on
the provided training datasets.
346
Datasets: Training datasets comprise of 3000 sen-
tences from laptop and restaurant reviews. Train-
ing sentences were tagged with the target aspect
term and the corresponding polarity, where more
than one aspect term can be tagged in a sentence.
3.1 Feature Sets
We divide the candidate features into four feature
sets.
1. Non-contextual: These features comprise of
training vocabulary. They do not target as-
pect based sentiments, but the overall senti-
ment of the sentence. There might be cases
where the aspect based sentiment is same as
the overall sentiment of the sentence. The
feature set comprises of three feature types,
unigrams, bigrams, adjectives and verbs of
the sentence.
2. Lexicon Non-Contextual: These features
are the Sentiwordnet v3.0 polarity scores
(Andrea Baccianella and Sebastiani, 2010)
of the words obtained from the best non-
contextual feature type. This feature set
would include two numerical features, posi-
tive polarity score and negative polarity score
of the best non-contextual feature types. Best
non-contextual feature type is decided by
comparing the classification accuracies of in-
dividual feature types, with cross validation
on the training data (Table 1). We evaluated
two algorithms to obtain the positive and neg-
ative polarities of words using SentiWordNet.
Later, we would provide details of these algo-
rithms.
3. Contextual: These features target aspect
based sentiments. Feature types comprise of
the clause in which an aspect term appears,
the adjective and verbs of this clause, aspect
term itself, and the words which hold certain
dependency relations with aspect term. We
only considered the Stanford parser depen-
dencies ?nn?, ?amod?, and ?nsubj?. The de-
pendency relations were chosen on the basis
of best classification accuracy in a cross vali-
dation trial, where the only features were the
words holding different dependency relations
with the aspect term. However, we only list
the accuracy from the best performing depen-
dency relations in the Tables 1, 3. By the fea-
ture type clause, we mean the unigrams con-
tained in a clause.
4. Lexicon Contextual: Similar to Lexicon
Non-Contextual features, these are the nu-
meric values obtained from SentiWordNet
polarity scores of the best performing contex-
tual feature type.
Polarity Calculation using SentiWordNet:
WordNet (Fellbaum, 1998) is a lexical database
for the English language. It assigns each listed
word the senses to which it may belong, where
each unique sense is represented by a synset id.
SentiWordNet is built on the top of WordNet,
where a pair of positive and negative polarity
score is assigned to each sense of a word. Senti-
Wordnet entry for each word comprises of all the
possible parts of speech in which the word could
appear, all the senses corresponding to each part
of speech, and a pair of polarity scores associated
with each sense
1
. The magnitude of positive and
negative polarity scores for each sense ranges
from 0 to 1.
In order to automatically obtain the polarity
scores corresponding to the desired sense of a
word, word sense disambiguation is required
to be performed. We did not perform sense
disambiguation, and picked the polarity scores
simply on the basis of word and part of speech
matching. This gives more than one candidate
senses, and thus more than one pair of polarity
scores for each word. We evaluated the following
2 methods to assign single values of sentiment
polarity scores to each word.
1. Default: The SentiWordnet website
2
pro-
vides a basic algorithm to assign sentiword-
net based polarities to a word. SentiWordnet
assigns a rank to each sense of a word, where
most commonly appearing sense is ranked
as 1. The default algorithm first calculates
an overall polarity (Positive score - Negative
score), for each sense of a word. It then cal-
culates a weighted sum of the overall polarity
scores of all the senses of a word, where the
weights are the ranks of senses. This sum is
considered as a single value polarity score of
a word, which can be a positive or negative
number.
1
http://sentiwordnet.isti.cnr.it/search.php?q=good
2
http://sentiwordnet.isti.cnr.it
347
2. Our algorithm: We do not obtain an overall
polarity score for each word, but we obtain a
pair of aggregated negative and positive score
for each word. Aggregate positive score is
obtained by taking the average of the positive
scores of each sense of the word, and same
goes for the aggregate negative score.
One reason for keeping the positive and negative
scores separate in our algorithm is that the task
also involves sentiment classes conflict and neu-
tral. Using only the overall polarity score results
in a loss of information in the case of very low neg-
ativity and positivity (neutral sentiments), or high
but comparable negativity and positivity (conflict-
ing sentiments). Also, our algorithm produced
better results when used with an SVM classifier,
with features as unigrams and their polarity scores.
3.2 Classifier Model
Our system is built on the state of the art LibSVM
classifier (EL-Manzalawy and Honavar, 2005).
We used Weka 3.7.10 toolkit (Hall et al., 2009) for
our experiments. The parameters
3
of the SVM
classifier are tuned to the values which give best
results with unigrams. Table 2 provides the tuned
parameters, rest of the parameters are set at default
values.
Pre-processing: We perform stemming using
Weka?s implementation of Snowball stemmer,
convert strings to lower case and filter out stop-
words. We use a customised list of stopwords,
based on our observations of the data. The cus-
tomised list is created using the stopword list of
Weka, with certain words removed. For example,
negators like ?not?, ?didn?t? etc. are important for
negative sentiments, for example ?I can barely use
any usb devices because they will not stay prop-
erly connected?. Words like ?but?, ?however? are
prominent in conflicting sentiments, for example
?No backlit keyboard, but not an issue for me?. Ta-
bles 1, 3 show the difference in results on using fil-
tered stopword list, compared against no stopword
removal, and original stopword list.
G R C E Z
0.10 1.0 2 1.0 normalise
Table 2: Parameter Settings for SVM Classifier.
3
http://weka.sourceforge.net/doc.stable/weka/classifiers/
functions/LibSVM.html
3.3 Feature Evaluation
We evaluated our features using 8-fold cross val-
idation on the training data. We evaluated each
feature by using it as the only feature for the clas-
sifier (Tables 1, 3). We performed experiments
on different combinations of features, but we only
present the best performing combination of fea-
tures in the last row of the tables. The baseline
approach (Pontiki et al., 2014) provided by the or-
ganisers, produced an accuracy of 47% for laptop
and 57% for restaurant, by splitting the training
data.
Metrics include, F score for each class, and overall
classification accuracy. F score ranges from 0-1,
and overall accuracy range from 0-100.
4 Submission and Results
Submission involved the prediction of sentiment
polarity towards the already tagged aspect terms
in two test datasets. There were 800 sentences
in each test dataset. The laptop test dataset was
obtained by dividing the original laptop data into
training and test. However, restaurant test dataset
and training dataset come from different sources.
We trained our classifier using the provided train-
ing dataset and the highlighted features (last row)
in the Tables 1, 3. In order to evaluate the submis-
sion, gold standard datasets corresponding to each
test dataset were later released, and submission?s
accuracy was compared against it.
Results: The system performance was evaluated
and ranked on the basis of overall accuracy of sen-
timent prediction. We were ranked as 20/32 for
the laptop domain, and 16/34 for the restaurant
domain. The task organisers reported that 8 polar-
ity predictions for laptop data, and 34 for restau-
rant data were missing from our submission. We
later debugged our system, and obtained the actual
accuracy which our system is capable of produc-
ing with the given test data. The results are sum-
marised in Table 4.
5 Observations and Analysis
We hypothesize that aspect terms should serve as
features when training data and test data come
from same source, which means that they relate
to the same brand, product, service etc. This is
because aspect terms change with data, for exam-
ple names of dishes would change with different
restaurants even if the domain is same. In our
case, the laptop test data was obtained from the
348
Feature Set Features Positive Negative Conflict Neutral Accuracy
non-contextual
unigrams,bigrams 0.827 0.590 0.210 0.422 70.699
unigrams 0.830 0.584 0.154 0.413 70.962
adjectives,verbs 0.704 0.412 0.000 0.257 63.465
adjectives 0.623 0.430 0.000 0.000 56.410
non-contextual + lexicon unigrams, unigram polarity scores 0.833 0.596 0.154 0.414 71.300
contextual
clause 0.823 0.571 0.117 0.0.456 71.170
adjective, verbs within clause 0.784 0.472 0.000 0.257 66. 465
aspects 0.734 0.154 0.000 0.264 59.442
dependencies 0.751 0.235 0.000 0.061 61.257
contextual + lexicon clause, clause polarity score 0.735 0.000 0.000 0.000 58.101
combined
unigrams, clause, dependencies,
clause polarity score, filtered
stopword list
0.837 0.610 0.162 0.418 71.960
used original stopword 0.825 0.587 0.078 0.371 70.830
no stopwords used 0.830 0.610 0.151 0.435 72.000
Table 1: Feature Analysis for Restaurant Reviews.
Feature Set Features Positive Negative Conflict Neutral Accuracy
Non-Contextual
unigrams,bigrams 0.827 0.590 0.210 0.422 70.699
unigrams 0.781 0.747 0.110 0.484 71.202
adjectives,verbs 0.569 0.620 0.000 0.164 54.516
adjectives 0.521 0.613 0.000 0.090 51.230
non-contextual + lexicon unigrams, unigram polarity scores 0.783 0.754 0.179 0.529 71.850
Contextual
Clause 0.823 0.571 0.117 0.0.456 71.170
adjective, verbs within clause 0.569 0.620 0.000 0.164 54.510
aspects 0.602 0.259 0.000 0.050 45.240
dependencies 0.590 0.078 0.000 0.000 42.480
contextual + lexicon clause, clause polarity score 0.750 0.705 0.000 0.407 67.230
combined
unigrams, clause, dependencies,
clause polarity score, filtered
stopword list
0.786 0.752 0.100 0.498 71.600
weka stopword list 0.780 0.744 0.113 0.442 70.590
no stopwords 0.782 0.758 0.154 0.530 72.170
Table 3: Feature Analysis for Laptop Reviews.
same dataset which was used to prepare training
data, while restaurant was from a different source.
We observed that, although aspect terms produced
better results with cross validation, it did not hap-
pen in the case of test data. The restaurant test data
produced better accuracy without aspect term fea-
tures, while laptop test data produced better accu-
racy with aspect term features. We submitted our
systems without using aspect terms as features. If
aspect terms were used as features, the laptop test
data would have been classified with an accuracy
of 60.8 %. Another interesting observation is, uni-
grams produce better results on their own, as com-
pared to adjectives and verbs. Dependency and
clauses also seem to be very important features,
since they produce an accuracy of above 60% on
their own. We also observed that some stopwords
are important features for this task, and complete
removal of stopwords lowers the classification ac-
curacy.
Domain Baseline Best
System
Submitted
System
Debugged
System
laptop 51.07 70.48 57.03 59.15
restaurant 64.28 80.95 70.70 71.44
Table 4: Results on Gold Standard Data.
6 Conclusion
We presented an analysis and evaluation of syn-
tactic and lexical features for performing sentence
level aspect based sentiment analysis. Our fea-
tures depend on part of speech tagging and depen-
dency parsing, and therefore the accuracy might
vary with different parsers. Although our system
did not produce the highest accuracy for the task, it
is capable of achieving accuracies much above the
baselines. Therefore, the proposed features can be
worth testing on different datasets and can be used
in combination with other features.
Acknowledgement
This work has been funded by the Euro-
pean project EUROSENTIMENT under grant no.
296277, and the Science Foundation Ireland under
Grant Number SFI/12/RC/2289 (Insight Center).
References
Stefano Esuli Andrea Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
349
(LREC?10). European Language Resources Associ-
ation (ELRA).
Paula Chesley. 2006. Using verbs and adjectives to
automatically classify blog sentiment. In In Pro-
ceedings of AAAI-CAAW-06, the Spring Symposia
on Computational Approaches, pages 27?29.
Yasser EL-Manzalawy and Vasant Honavar, 2005.
WLSVM: Integrating LibSVM into Weka Environ-
ment.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software, an update.
SIGKDD Explorations, 11:10?18.
Arun Meena and Prabhakar T.V. 2007. Sentence level
sentiment analysis in the presence of conjuncts us-
ing linguistic analysis. In ECIR, volume 4425 of
Lecture Notes in Computer Science. Springer.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval 2014, Dublin, Ireland.
Hassan Saif, Yulan He, and Harith Alani. 2012. Se-
mantic sentiment analysis of twitter. In Interna-
tional Semantic Web Conference (1), volume 7649
of Lecture Notes in Computer Science, pages 508?
524. Springer.
Wilson Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, pages 399?433.
350
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86?94,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Domain-specific and Collaborative Resources for Term Translation
Mihael Arcan, Paul Buitelaar
Unit for Natural Language Processing
Digital Enterprise Research Institute
Galway, Ireland
firstname.lastname@deri.org
Christian Federmann
Language Technology Lab
German Research Center for AI
Saarbru?cken, Germany
cfedermann@dfki.de
Abstract
In this article we investigate the translation
of terms from English into German and vice
versa in the isolation of an ontology vocab-
ulary. For this study we built new domain-
specific resources from the translation search
engine Linguee and from the online encyclo-
pedia Wikipedia. We learned that a domain-
specific resource produces better results than
a bigger, but more general one. The first find-
ing of our research is that the vocabulary and
the structure of the parallel corpus are impor-
tant. By integrating the multilingual knowl-
edge base Wikipedia, we further improved the
translation wrt. the domain-specific resources,
whereby some translation evaluation metrics
outperformed the results of Google Translate.
This finding leads us to the conclusion that
a hybrid translation system, a combination of
bilingual terminological resources and statis-
tical machine translation can help to improve
translation of domain-specific terms.
1 Introduction
Our research on translation of ontology vocabularies
is motivated by the challenge of translating domain-
specific terms with restricted or no additional textual
context that in other cases can be used for transla-
tion improvement. For our experiment we started
by translating financial terms with baseline systems
trained on the EuroParl (Koehn, 2005) corpus and
the JRC-Acquis (Steinberger et al, 2006) corpus.
Although both resources contain a large amount of
parallel data, the translations were not satisfying. To
improve the translations of the financial ontology
vocabulary we built a new parallel resource, which
was generated using Linguee1, an online translation
query service. With this data, we could train a small
system, which produced better translations than the
baseline model using only general resources.
Since the manual development of terminological
resources is a time intensive and expensive task, we
used Wikipedia as a background knowledge base
and examined articles, tagged with domain-specific
categories. With this extracted domain-specific data
we built a specialised English-German lexicon to
store translations of domain-specific terms. These
terms were then used in a pre-processing method in
the decoding approach. This approach incorporates
the work by Aggarwal et al (2011), which suggests
a sub-term analysis. We split the financial terms
into n-grams and search for financial sub-terms in
Wikipedia.
The remainder of the paper is organised like this.
In Section 2 we describe related work while in Sec-
tion 3 the ontology data, the training data that we
used in training the language model, and the trans-
lation decoder are discussed. Section 4 presents the
new resources which were used for improving the
term translation. In Section 5 we discuss the results
of exploiting the different resources. We conclude
with a summary and give an outlook on future work
in Section 6.
2 Related Work
Kerremans (2010) presents the issue of terminologi-
cal variation in the context of specialised translation
on a parallel corpus of biodiversity texts. He shows
that a term often cannot be aligned to any term in
1See www.linguee.com
86
the target language. As a result, he proposes that
specialised translation dictionaries should store dif-
ferent translation possibilities or term variants.
Weller et al (2011) describe methods for termi-
nology extraction and bilingual term alignment from
comparable corpora. In their compound translation
task, they are using a dictionary to avoid out-of-
domain translation.
Zesch et al (2008) address issues in accessing
the largest collaborative resources: Wikipedia and
Wiktionary. They describe several modules and
APIs for converting a Wikipedia XML Dump into a
more suitable format. Instead of parsing the large
Wikipedia XML Dump, they suggest to store the
Dump into a database, which significantly increases
the performance in retrieval time of queries.
Wikipedia has not only a dense link structure be-
tween articles, it has also inter-language links be-
tween articles in different languages, which was the
main reason to use this invaluable collaborative re-
source. Erdmann et al (2008) regarded the titles of
Wikipedia articles as terminology. They assumed
that two articles connected by an Interlanguage link
are likely to have the same content and thus an
equivalent title.
Vivaldi and Rodriguez (2010) proposed a method-
ology for term extraction in the biomedical domain
with the help of Wikipedia. As a starting point, they
manually select a set of seed words for a domain,
which is used to find corresponding nodes in this re-
source. For cleaning their collected data, they use
thresholds to avoid storing undesirable categories.
Mu?ller and Gurevych (2008) use Wikipedia and
Wiktionary as knowledge bases to integrate seman-
tic knowledge into Information retrieval. Their
models, text semantic relatedness (for Wikipedia)
and word semantic relatedness (for Wiktionary),
are compared to a statistical model implemented in
Lucene. In their approach to Bilingual Retrieval,
they use the cross-language links in Wikipedia,
which improved the retrieval performance in their
experiment, especially when the machine translation
system generated incorrect translations.
3 Experiments
Our experiment started with an analysis of the terms
in the ontology to be translated, which was stored
in RDF2 data model. These terms were used to
automatically extract any corresponding Wikipedia
Categories, which helped us to define more exactly
the domain(s) of the ontology to be translated. The
collected Categories were further used to build a
domain-specific lexicon to be used for improving
term translation. At the same time a new parallel
corpus was built, which was also generated with the
help of the ontology terms. This new data was then
used to pre-process the input data for the decoder
and to build a specialised training model which
yielded to a translation improvement.
In this section, several types of data will be
presented and furthermore the translation decoder,
which has to access this data to build the training
models. Section 3.1 gives an overview of the data
that was used in translation. In Sections 3.2 and
3.3 we describe the data that is used to train the
translation and language model. We used differ-
ent parallel corpora, JRC-Acquis, EuroParl and a
domain-specific corpus built from Linguee. In Sec-
tion 3.4, we discuss a domain-specific lexicon, ex-
tracted from Wikipedia. In the last Section 3.5 we
describe the phrase-based machine translation de-
coder Moses that we used for our experiments.
3.1 xEBR Dataset
For the translation dataset a financial ontology de-
veloped by the XBRL European Business Registers3
(xEBR) Working Group was used. This financial
ontology is a framework for describing financial ac-
counting and profile information of business entities
across Europe, see also Declerck et al (2010). The
ontology holds 263 concepts and is partially trans-
lated into German, Dutch, Spanish, French and Ital-
ian. The terms in each language are aligned via
the SKOS4 Exact Match mechanism to the xEBR
core taxonomy. In this partially translated taxon-
omy, we identified 63 English financial terms and
their German equivalents, which were used as refer-
ence translations in evaluating the different experi-
ment steps.
The xEBR financial terms are not really terms
from a linguistic point of view, but they are used
in financial or accounting reports as unique finan-
2RDF: Resource Description Framework
3XBRL: eXtensible Business Reporting Language
4SKOS: Simple Knowledge Organization System
87
Length Count Examples
11 1 Taxes Remuneration And Social Security
Payable After More Than One Year
10 2 Amounts Owed To Credit Institutions After
More Than One Year, Variation In Stocks Of
Finished Goods And Work In Progress
. . .
2 57 Net Turnover, Liquid Assets, . . .
1 10 Assets, Capital, Equity, . . .
Table 1: Examples of xEBR terms
cial expressions or tags to organize and retrieve au-
tomatically reported information. Therefore it is im-
portant to translate these financial terms exactly.
Table 1 illustrates the structure of xEBR terms.
It is obvious that they are not comparable to gen-
eral language, but instead are more like headlines in
newspapers, which are often short, very informative
and written in a telegraphic style. xEBR terms are
often only noun phrases without determiners. The
length of the financial terms varies, e.g. the longest
financial term considered for translation has a length
of 11 tokens, while others may consist of 1 or 2.
3.2 General Resources: EuroParl and
JRC-Acquis
As a baseline, the largest available parallel corpora
were used: EuroParl and the JRC-Acquis parallel
corpus. The EuroParl parallel corpus holds the pro-
ceedings of the European Parliament in 11 European
languages. The JRC-Acquis corpus is available in
almost all EU official languages (except Irish) and is
a collection of legislative texts written between 1950
and today.
Although research work proved, that a training
model built by using a general resource cannot be
used to translate domain-specific terms (Wu et al,
2008), we decided to train a baseline model on these
resources to illustrate any improvement steps from a
general resource to specialised domain resources.
3.3 Domain Resource: Linguee
Linguee is a combination of a dictionary and a
search engine, which indexes around 100 Million
bilingual texts on words and expressions. Linguee
search results show example sentences that depict
how the searched expression has been translated in
context.
In contrast to translation engines like Google
Translate and Bing Translator, which give you the
most probable translation of a source text, every en-
try in the Linguee database has been translated by
humans. The bilingual dataset was gathered from
the web, particularly from multilingual websites
of companies, organisations or universities. Other
sources include EU documents and patent specifica-
tions.
The language pairs available for query-
ing are English?German, English?Spanish,
English?French and English?Portuguese.
Since Linguee includes EU documents, they also
use parallel sentences from EuroParl and JRC-
Acquis. We investigated the proportion of sentences
returned by Linguee which are contained in Eu-
roParl or JRC-Acquis. The outcome is that the num-
ber of sentences is very low, where 131 sentences
(0.54%) are gathered from JRC-Acquis corpus and
466 (1.92%) from EuroParl.
3.4 Collaborative Resource: Wikipedia
Wikipedia is a multilingual, freely available ency-
clopedia that was built by a collaborative effort of
voluntary contributors. All combined Wikipedias
hold approximately 20 million articles or more than
8 billion words in more than 280 languages. With
these facts it is the largest collection of freely avail-
able knowledge5.
With the heavily interlinked information base,
Wikipedia forms a rich lexical and semantic re-
source. Besides a large amount of articles, it
also holds a hierarchy of Categories that Wikipedia
Articles are tagged with. It includes knowledge
about named entities, domain-specific terms and
word senses. Furthermore, the redirect system of
Wikipedia articles can be used as a dictionary for
synonyms, spelling variations and abbreviations.
3.5 Translation System: Moses
For generating translations from English into Ger-
man and vice versa, the statistical translation toolkit
Moses (Koehn et al, 2007) was used to build the
training model and for decoding. For this approach,
a phrase-based approach was taken instead of a tree
based model. Further, we aimed at improving the
translations only on the surface level, and therefore
no part-of-speech information was taken into ac-
count. Word and phrase alignments were built with
5
http://en.wikipedia.org/wiki/Wikipedia:Size_comparison
88
the GIZA++ toolkit (Och and Ney, 2003), whereby
the 5-gram language model was built by SRILM
(Stolcke, 2002).
4 Domain-specific Resource Generation
In this section, two different types of data and the
approach of building them will be presented. Sec-
tion 4.1 gives an overview of generating a paral-
lel resource from Linguee, which was used in gen-
erating a new domain-specific training model. In
Section 4.2 a detailed description is given how we
extracted terms from Wikipedia for generating a
domain-specific lexicon.
4.1 Domain-specific parallel corpus generation
To build a new training model that is specialised on
our xEBR ontology, we used the Linguee search en-
gine. This resource can be queried on single words
and on word expressions with or without quotation
marks. We stored the HTML output of the Linguee
queries on our financial terms and parsed these files
to extract plain parallel text. From this, we built a fi-
nancial parallel corpus with 13,289 translation pairs,
including single words, multi-word expressions and
sentences. The English part of the parallel resource
contained 410,649 tokens, the German part 347,246.
4.2 Domain-specific lexicon generation
To improve translation based on the domain-specific
parallel corpus, we built a cross-lingual terminolog-
ical lexicon extracted from Wikipedia. From the
Wikipedia Articles we used different information
units, i.e. the Title of a Wikipedia Article, the Cat-
egory (or Categories) of the Title and the internal
Interwiki
Interlanguage links of the Title. The concept of
Interwiki links can be used to make links to other
Wikipedia Articles in the same language or to an-
other Wikipedia language i.e. Interlanguage links.
In our first approach, we used Wikipedia to de-
termine the domain (or several domains) of the on-
tology. This approach (a) is to understand as the
identification of the domain through the vocabulary
of the ontology. For this approach, the financial
terms, which were extracted from the ontology, were
used to query the Wikipedia knowledge base6. The
6For the Wikipedia Query we used the Wikipedia XML
Collected Wikipedia Categories
Frequency Name
8 Generally Accepted Accounting Principles
4 Debt
4 Accounting terminology
. . .
1 Political science terms
1 Physical punishments
Table 2: Collected Wikipedia Categories based on the ex-
tracted financial terms
Wikipedia Article was considered for further exami-
nation, if its Title is equivalent to our financial terms.
In this first step, 7 terms of our ontology were iden-
tified in the Wikipedia knowledge base. With this
step, we collected the Categories of these Titles,
which was the main goal of this approach. In a sec-
ond round, we split all financial terms into all pos-
sible n-grams and repeated the query again to find
additional Categories based on the split n-grams. Ta-
ble 2 shows the collected Categories of the first ap-
proach and how often they appeared in respect to the
extracted financial terms.
After storing all Categories, only such Categories
were considered, which frequency had a value more
than the calculated arithmetic mean of all frequen-
cies (> 3.15). For the calculation of the arithmetic
mean only Categories were considered, which had
a frequency more than 1, since 2,262 of 3,615 col-
lected Categories (62.6%) had a frequency equals 1.
With this threshold we avoided extraction of a vo-
cabulary that is not related to the ontology. Without
this threshold, out-of-domain Categories would be
stored, which would extend the lexicon with vocab-
ulary that would not benefit the ontology translation,
e.g. Physical punishments, which was access by the
financial term Stocks.
In the next step, we further extended the list of
Categories collected previously by use of full and
split terms. This was done by storing new Categories
based on the Wikipedia Interwiki links of each Arti-
cle which was tagged with a Category from Table 2.
For example, we collected all Categories wherewith
the Article Balance sheet7 is tagged and the Cate-
gories of the 106 Interwiki links of the Article Bal-
ance sheet. The frequencies of these Categories
were summed up for all Interwiki links. Finally a
dump; enwiki-20120104-pages-articles.xml
7Financial statements, Accounting terminology
89
Final Category List
Frequency Name
95 Economics terminology
62 Generally Accepted Accounting Principles
61 Macroeconomics
55 Accounting terminology
47 Finance
44 Economic theories
. . .
Table 3: Most frequent Categories based on the xEBR
terms and their Interwiki links
new Category was added to the final Category list, if
the new Category frequency exceeds the arithmetic
mean threshold (> 18.40).
The final Category list contained 33 financial
Wikipedia Categories (Table 3), which was in the
next step used for financial term extraction.
With the final list of Categories, we started an
investigation of all Wikipedia articles tagged with
these financial Categories. Each Wikipedia Title
was considered as a useful domain-specific term
and was stored in our lexicon if a German title in
the Wikipedia knowledge base also existed. As
an example, we examined the Category Account-
ing terminology and stored the English Wikipedia
Title Balance sheet with the German equivalent
Wikipedia Title Bilanz.
At the end of the lexicon generation we examined
5228 Wikipedia Articles, which were tagged with
one or more financial Categories. From this set of
Articles we were able to generate a terminological
lexicon with 3228 English-German entities.
5 Evaluation
Tables 4 to 5 illustrate the final results for our exper-
iments on translating xEBR ontology terms, using
the NIST (Doddington, 2002), BLEU (Papineni et
al., 2002), and Meteor (Lavie and Agarwal, 2005)
algorithms. To further study any translation im-
provements of our experiment, we also used Google
Translate8 in translating 63 financial xEBR terms
(cf. Section 3.1) from English into German and from
German into English.
5.1 Interpretation of Evaluation Metrics
In our experiments translation models built from
a general resource performed worst. These re-
8Translations were generated on February 2012.
Scoring Metric
Source # correct BLEU NIST Meteor
Google Translate 18 0.264 4.382 0.369
JRC-Acquis 12 0.167 3.598 0.323
EuroParl 4 0.113 2.630 0.326
Linguee 25 0.347 4.567 0.408
Lexical substitution 4 0.006 0.223 0.233
Linguee+Wiki 25 0.324 4.744 0.432
Table 4: Evaluation scores for German term translations
Scoring Metric
Source # correct BLEU NIST Meteor
Google Translate 21 0.452 4.830 0.641
JRC-Acquis 9 0.127 2.458 0.480
EuroParl 5 0.021 1.307 0.412
Linguee 15 0.364 3.938 0.631
Lexical substitution 4 0.006 0.243 0.260
Linguee+Wiki 22 0.348 3.993 0.644
Table 5: Evaluation scores for English term translations
sults show that building resources from general lan-
guage does not improve the translation of terms.
The Linguee financial corpus, which is built from
13,289 sentences and holds 304K English and Ger-
man 250K words, however demonstrates the ben-
efit of domain-specific resources. Its size is less
than two percent of that of the JRC-Acquis cor-
pus (1,131,922 sentences, 21M English words, 19M
German words), but evaluation scores are more than
double than those for JRC-Acquis. This is clear evi-
dence that such a resource benefits the translation of
terms in a specific domain.
The models produced by the Linguee search en-
gine are generating better translations than those
produced by general resources. This approach out-
performs Google Translate translations from Ger-
man into English for all used evaluation metrics.
The table further shows results for our approach
in using extracted Wikipedia terms as an example-
based approach. For this we used the terms extracted
from Wikipedia and exchanged English terms with
German translations and vice versa. The evaluation
metrics are very low in this case; only for Correct
Translation we generate four positive findings.
Finally, the table gives results for our approach
in using a combination of domain-specific paral-
lel financial corpus with the lexicon extracted from
Wikipedia. The domain-specific lexicon contains
3228 English-German translations, which were ex-
tracted from 18 different financial Categories. This
90
combination of highly specialised resources gives
the best results in our experiment. Translating fi-
nancial terms into German, we get more Correct
Translations as well as the Meteor metric shows
better results compared to Google Translate. For
translations into English, all used evaluation metrics
show better results than those of Google Translate.
As a final observation, we learned that translations
made by domain-specific resources are on the same
quality level, either if we translate from English
into German or vice versa. In comparison, we see
that Google Translate has a larger discrepancy when
translating into German or English respectively. Our
research showed that translations from English into
German built by specialised resources were slightly
better, which goes along with Google Translate that
also produces better translations into German.
5.2 Manual Evaluation of Translation Quality
In addition to the automatic evaluation with BLEU,
NIST, and Meteor scores, we have also undertaken
a manual evaluation campaign to assess the transla-
tion quality of the different systems. In this section,
we will a) describe the annotation setup and task
presented to the human annotators, b) report on the
translation quality achieved by the different systems,
and c) present inter-annotator agreement scores that
allow to judge the reliability of the human rankings.
5.2.1 Annotation Setup
In order to manually assess the translation quality
of the different systems under investigation, we de-
signed a simple classification scheme consisting of
three distinct classes:
1. Acceptable (A): terms classified as acceptable
are either fully identical to the reference term
or semantically equivalent;
2. Can easily be fixed (C): terms in this class
require some minor correction (such as fixing
of typos, removal of punctuation, etc.) but are
nearly acceptable. The general semantics of
the reference term are correctly conveyed to
the reader.
3. None of both (N): the translation of the term
does not match the intended semantics or it is
plain wrong. Items in this class are considered
severe errors which cannot easily be fixed and
hence should be avoided wherever possible.
Classes
System A C N
Linguee+Wiki 58% 27% 15%
Google Translate 55% 31% 14%
Linguee 51% 37% 12%
JRC-Acquis 32% 28% 40%
EuroParl 5% 25% 70%
Table 6: Results from the manual evaluation into German
Classes
System A C N
Linguee+Wiki 56% 32% 12%
Linguee 56% 31% 13%
Google Translate 39% 40% 21%
JRC-Acquis 39% 31% 30%
EuroParl 15% 30% 55%
Table 7: Results from the manual evaluation into English
5.2.2 Annotation Data
We setup ten evaluation tasks, five for transla-
tions into English, five for translations into German.
Each of these sets was comprised of 63 term transla-
tions and the corresponding reference. Every set was
given to at least three human annotators who then
classified the observed translation output according
to the classification scheme described above. The
human annotators included both domain experts and
lay users without knowledge of the terms domain.
In total, we collected 2,520 classification items
from six annotators. Tables 6, 7 show the results
from the manual evaluation for term translations into
German and English, respectively. We report the
distribution of classes per evaluation task which are
displayed in best-to-worst order.
In order to better be able to interpret these rank-
ings, we computed the inter-annotator agreement be-
tween human annotators. We report scores gener-
ated with the following agreement metrics:
? S (Bennet et al, 1954);
? pi (averaged across annotators) (Scott, 1955);
? ? (Fleiss and others, 1971);
? ? (Krippendorff, 1980).
Tables 8, 9 present the aforementioned metrics
scores for German and English term translations.
Overall, we achieve an average ? score of 0.463,
which can be interpreted as moderate agreement fol-
lowing (Landis and Koch, 1977). Notably, we also
reach substantial agreement for one of the anno-
tation tasks with a ? score of 0.657. Given the
91
Agreement Metric
System S pi ? ?
Linguee+Wiki 0.599 0.528 0.533 0.530
Google Translate 0.698 0.655 0.657 0.657
Linguee 0.484 0.416 0.437 0.419
JRC-Acquis 0.412 0.406 0.413 0.408
EuroParl 0.515 0.270 0.269 0.273
Table 8: Annotator agreement scores for German
Agreement Metric
System S pi ? ?
Linguee+Wiki 0.532 0.452 0.457 0.454
Linguee 0.599 0.537 0.540 0.539
Google Translate 0.480 0.460 0.465 0.463
JRC-Acquis 0.363 0.359 0.366 0.360
EuroParl 0.552 0.493 0.499 0.495
Table 9: Annotator agreement scores for English
observed inter-annotator agreement, we expect the
reported ranking results to be meaningful. Our
Linguee+Wiki system performs best for both trans-
lation directions while out-of-domain systems such
as JRC-Acquis and EuroParl perform badly.
5.3 Manual error analysis
Table 10 provides a manual analysis of the provided
translations from Google Translate and the com-
bined Linguee and Wikipedia Lexicon approach.
Example Ex. 1 shows the results for [Other intan-
gible] fixed assets. Since both translating systems
translate it the same, namely Vermo?genswerte, they
could be considered as term variants.
A similar example is [Receivables and other] as-
sets in Ex. 4. Google Translate translates the
segment asset into Vermo?gensgegensta?nde, whereby
the domain-specific approach translates it into
Vermo?genswerte. These examples prove the re-
search by Kerremans (2010) that one term does not
necessarily have only one translation on the target
side. As term variants can further be considered
Aufwendungen and Kosten, which were translated
from Costs [of old age pensions] (Ex. 5).
In contrast, the German term in [sonstige be-
triebliche] Aufwendungen (Ex. 8) is according to the
xEBR translated into [Other operating] expenses,
which was translated correctly by both systems.
A deeper terminological analysis has to be done
in the translation of the English term [Cost of] old
age pensions (Ex. 5). In general it can be translated
into Altersversorgung (provided by Google Trans-
late and xEBR) or Altersrente (generated by the
domain-specific model). Doing a compound anal-
ysis, the translation of [Alters]versorgung is supply
or maintenance. On the other side, the translation of
[Alters]rente is pension, which has a stronger con-
nection to the financial term in this domain.
Ex. 6 shows an improvement of domain spe-
cific translation model in comparison to a general
resource. Both general resources translated Securi-
ties as Sicherheiten, which is correct but not in the fi-
nancial domain. The domain-specific trained model
translates the ambiguous term correctly, namely
Wertpapiere. Google Translate generates the same
term as on the source site, Securities. Further, the
term Equity (Ex. 7) is translated by Google Translate
as Gerechtigkeit, the domain-specific model trans-
lates it as Eigenkapital, which is the correct trans-
lation. Finally, Ex. 2 and Ex. 3 open the issue of
accurateness of the references for translation evalu-
ation. The translations of these terms are correct if
we consider the source language. On the other hand,
if we compare them with the proposed references,
they are not the same. In Ex. 2 they are truncated
or extended in Ex. 3, which opens up problems in
translation evaluation.
5.4 Discussion
Our approach shows the differences between im-
proving translations with different resources. It was
shown to be necessary to use additional language
resources, i.e. specialised parallel corpora and if
available, specialised lexica with appropriate trans-
lations. Nevertheless, to move further in this direc-
tion, translation of specific terms, more research is
required in several areas that we identified in our ex-
periment. One is the quality of the translation model.
Because the translation model can only translate
terms that are in the training model, it is necessary
to use a domain-specific resource. Although we got
better results with a smaller resource (if we translate
into English), comparing those results with Google
Translate, we learned that more effort has to be done
in the direction of extending the size and quality of
domain-specific resources.
Apart from that, with the aid of Wikipedia, which
can be easily adapted for other language pairs, we
further improved the translations into English to a
92
Term Translations
# Source Reference Google Domain-specific
1 Other intangible sonstige immaterielle Sonstige immaterielle Sonstige immaterielle
fixed assets Vermo?gensgegensta?nde Vermo?genswerte Vermo?genswerte
2 Long-term Finanzanlagen Langfristige finanzielle Langfristige finanzielle
financial assets Vermo?genswerte Vermo?genswerte
3 Financial result Finanz- und Finanzergebnis Finanzergebnis
Beteiligungsergebnis
4 Receivables and Forderungen und sonstige Forderungen und sonstige Forderungen und sonstige
other assets Vermo?gensgegensta?nde Vermo?gensgegensta?nde Vermo?genswerte
5 Cost of old age Aufwendungen fu?r Aufwendungen fu?r Kosten der Altersrenten
pensions Altersversorgung Altersversorgung
6 Securities Wertpapiere Securities Wertpapiere
7 Equity Eigenkapital Gerechtigkeit Eigenkapital
8 sonstige betriebliche Other operating expenses other operating expenses other operating expenses
Aufwendungen (TC)
Table 10: Translations provided by Google Translate and by the domain-specific resource
point where we outperform translations provided
by Google Translate. Nevertheless, our experiment
showed that the translations into German were bet-
ter in regard of Google translate only for the Meteor
evaluation system, for BLEU and NIST we did not
achieve significant improvements. Also here more
work has to be done in domain adaptation in a more
sophisticated way to avoid building out-of-domain
vocabulary.
6 Conclusion
The approach of building new resources showed a
large impact on the translation quality. Therefore,
generating specialised resources for different do-
mains will be the focus of our future work. On
the one hand, building appropriate training models
is important, but our experiment also highlighted
the importance of additional collaborative resources,
like Wikipedia, Wiktionary, and DBpedia. Besides
extracting Wikipedia Articles with their multilin-
gual equivalents, as shown in Section 4.2, Wikipedia
holds much more information in the articles itself.
Therefore exploiting non-parallel resources, shown
by Fis?er et al (2011), would clearly help the trans-
lation system to improve performance. Future work
needs to better include the redirect system, which
would allow a better understanding of synonymy
and spelling variety of terms.
Focusing on translating ontologies, we will try
to better exploit the structure of the ontology itself.
Therefore, more work has to be done in the combi-
nation of linguistic and semantic information (struc-
ture of an ontology) as demonstrated by Aggarwal et
al. (2011), which showed first experiments in com-
bining semantic, terminological and linguistic infor-
mation. They suggest that a deeper semantic analy-
sis of terms, i.e. understanding the relations between
terms and analysing sub-terms needs to be consid-
ered. Another source of useful information may be
found in using existing translations for improving
the translation of other related terms in the ontology.
Acknowledgments
This work has been funded under the Seventh
Framework Programme for Research and Techno-
logical Development of the European Commission
through the T4ME contract (grant agreement no.:
249119) and in part by the European Union under
Grant No. 248458 for the Monnet project as well as
by the Science Foundation Ireland under Grant No.
SFI/08/CE/I1380 (Lion-2). The authors would like
to thank Susan-Marie Thomas, Tobias Wunner, Ni-
tish Aggarwal and Derek De Brandt for their help
with the manual evaluation. We are grateful to the
anonymous reviewers for their valuable feedback.
References
Nitish Aggarwal, Tobias Wunner, Mihael Arcan, Paul
Buitelaar, and Sea?n O?Riain. 2011. A similarity mea-
sure based on semantic, terminological and linguistic
93
information. In The Sixth International Workshop on
Ontology Matching collocated with the 10th Interna-
tional Semantic Web Conference (ISWC?11).
E. M. Bennet, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited response question-
ing. Public Opinion Quarterly, 18:303?308.
Thierry Declerck, Hans-Ulrich Krieger, Susan M.
Thomas, Paul Buitelaar, Sean O?Riain, Tobias Wun-
ner, Gilles Maguet, John McCrae, Dennis Spohr, and
Elena Montiel-Ponsoda. 2010. Ontology-based mul-
tilingual access to financial reports for sharing busi-
ness knowledge across europe. In Internal Financial
Control Assessment Applying Multilingual Ontology
Framework.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2008. An approach for extracting bilingual terminol-
ogy from wikipedia. Lecture Notes in Computer Sci-
ence, (4947):380?392. Springer.
Darja Fis?er, S?pela Vintar, Nikola Ljubes?ic?, and Senja Pol-
lak. 2011. Building and using comparable corpora for
domain-specific bilingual lexicon extraction. In Pro-
ceedings of the 4th Workshop on Building and Using
Comparable Corpora: Comparable Corpora and the
Web, BUCC ?11, pages 19?26.
J.L. Fleiss et al 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Koen Kerremans. 2010. A comparative study of termino-
logical variation in specialised translation. In Recon-
ceptualizing LSP Online proceedings of the XVII Eu-
ropean LSP Symposium 2009, pages 1?14.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL, ACL
?07, pages 177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86. AAMT.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Methodology. Sage Publications, Inc.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
Alon Lavie and Abhaya Agarwal. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of the
EMNLP 2011 Workshop on Statistical Machine Trans-
lation, pages 65?72.
Christof Mu?ller and Iryna Gurevych. 2008. Using
wikipedia and wiktionary in domain-specific informa-
tion retrieval. In Working Notes for the CLEF 2008
Workshop.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318.
W. A. Scott. 1955. Reliability of Content Analysis: The
Case of Nominal Scale Coding. Public Opinion Quar-
terly, 19:321?325.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dniel
Varga. 2006. The jrc-acquis: A multilingual aligned
parallel corpus with 20+ languages. In Proceedings
of the 5th International Conference on Language Re-
sources and Evaluation (LREC?2006).
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286.
Jorge Vivaldi and Horacio Rodriguez. 2010. Using
wikipedia for term extraction in the biomedical do-
main: first experiences. Procesamiento del Lenguaje
Natural, 45:251?254.
Marion Weller, Anita Gojun, Ulrich Heid, Be?atrice
Daille, and Rima Harastani. 2011. Simple methods
for dealing with term variation and term alignment.
In Proceedings of the 9th International Conference on
Terminology and Artificial Intelligence, pages 87?93.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
wikipedia and wiktionary. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC?08).
94
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 43?47,
Dublin, Ireland, August 23 2014.
Using Distributional Semantics to Trace Influence and Imitation in
Romantic Orientalist Poetry
Nitish Aggarwal
Insight Centre for Data Analytics
National University of Ireland
Galway, Ireland
nitish.aggarwal@deri.org
Justin Tonra
School of Humanities
National University of Ireland
Galway, Ireland
justin.tonra@nuigalway.ie
Paul Buitelaar
Insight Centre for Data Analytics
National University of Ireland
Galway, Ireland
paul.buitelaar@deri.org
Abstract
In this paper, we investigate whether textual analysis can yield evidence of shared vocabulary or
formal textual characteristics in the works of 19th century poets Lord Byron and Thomas Moore
in the genre of Romantic Orientalism. In particular, we identify and trace Byron?s influence on
Moore?s writings to query whether Moore imitated Byron, as many reviewers of the time sug-
gested. We use a Distributional Semantic Model (DSM) to analyze if there is a shared vocabulary
of Romantic Orientalism, or if it is possible to characterize a literary genre in terms of vocabu-
lary, rather than in terms of the particular plots, characters and themes. We discuss the results
that DSM models are able to provide for an abstract overview of the influence of Lord Byron?s
work on Thomas Moore.
1 Introduction
Literary criticism has often marshalled the serendipitous discovery in the service of constructing an ar-
gument or a critical judgment. Such serendipity can take material or cognitive form, and provide the
raw materials for analysis and conjecture. In literary criticism, arguments are often based upon evidence
gleaned from close reading of a text in support of a hypothesis, but quantitative methods have shown how
literary texts can yield evidence that is not immediately discernible to the human eye for a similar inter-
pretive purposes. In literary studies, computers have assisted in the collection of such data with varying
degrees of complexity and sophistication for about half a century. How can we use the information from
such computing processes for creating new knowledge, or, in literary-critical terms, for articulating the
meaning in a text? To what degree can literary criticism and computing enrich one another? Is algorith-
mic criticism derived from algorithmic manipulation of text (Ramsay, 2011) possible?
Inspired by general questions such as these, this paper discusses a particular project that uses Distribu-
tional Semantics to trace influence and imitation between two particular poets writing in the genre of
Romantic Orientalism. Our intuition is that if text analysis can yield evidence of shared vocabulary to
trace influence between poets, we can build a network of different authors with their degree of influences.
This can help a reader in finding a similar literature and in discovering implicit information.
In the period from 1813 to 1817, friends and fellow-poets Lord Byron and Thomas Moore wrote a series
of long poems which are now seen as representative of Romantic Orientalism (a subset of Romantic
literature recognisable by its Oriental and Middle-Eastern themes and settings). Throughout this period,
an unusual pattern of coincidence is evident in the writings of the two poets, with correspondence be-
tween the poets describing similar plots, settings, and characters names in their respective works. The
publication of Byron?s quartet of Oriental tales in 1813 and 1814 (The Giaour, The Bride of Abydos, The
Corsair, Lara) anticipated much of the substance of Moore?s work, and delayed the publication of his
own suite of four Oriental poems, Lalla Rookh, until 1817. On the publication of the latter, many re-
viewers accused Moore of imitating Byron?s work, correctly fulfilling Moore?s own prediction of 1813,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
43
that he would be seen as ?an humble follower?a Byronian? (Moore, 1964). Subsequent critics have
generally acknowledged Byron as a direct influence on Moore, but the basis of these acknowledgements
is usually subjective critical interpretation of plot, character, and poetic form in the published texts (see,
in particular, (Vail, 2001), (Sultana, 1989), (Gregory, 2008)). More general accounts of Byron?s and
Moore?s literary association can be found in (Hamilton, 1948), (Jordan, 1948), (Tessier, 2014).
The purpose of this project is to investigate further the possible causes for the unusual pattern of co-
incidence in the writings of these two poets during this time. A Computational Linguistics approach to
Byron?s and Moore?s Orientalism was identified as a potentially productive way of studying coincidence,
influence, and imitation between their writings and how they related to the genre of Romantic Oriental-
ism. Fresh empirical insight into this topic is desirable because of the difficulty of thinking about and
articulating these issues in a way that is not speculative or nebulous. Such methodologies have not been
applied to these texts, and offer the possibility of yielding fresh perspectives on questions about the texts,
and the genre of Romantic Orientalism: is it defined by a limited vocabulary which inevitably leads to
similarities and coincidences between its practitioners? Does writing within a specific genre impose top-
ical or semantic constraint upon the author?
The motivations for the project emerge from a conviction that Computational Linguistics techniques may
reveal evidence of shared vocabulary or formal textual characteristics in the works of Byron and Moore
during the period 1813-17. The questions that the project seeks to answer include: can we identify and
trace Byron?s influence on Moore?s writings? Did Moore imitate Byron, as many reviewers of the time
suggested? Is there a shared vocabulary of Romantic Orientalism? Is it possible to characterise a literary
genre in these terms, rather than in terms of plot, character, theme, etc.? The basis for such enquiries
must go beyond a subjective comparison of the poems: the method that has characterised literary-critical
approaches to these texts to date.
2 Distributional semantics
How related are love and emotion? Reasoning about semantic relatedness of natural language text is
not a very difficult task for a human because of sufficient background knowledge and other related in-
formation to understand the semantics of natural language text. However, for computers, it is still an
open issue to provide significant background knowledge to understand the complex structure of natural
language. One plausible way to provide such background knowledge is taking the usage of given text in
large contextual space into account.
Semantic relatedness of two given terms (text fragments, phrases or words) can be obtained by calculat-
ing the correlation between two high dimensional vectors of a Distributional Semantic Model (DSM),
which is based on the assumption that semantic meaning of a text can be inferred from its usage in context
(Harris, 1954), i.e. its distribution in text. DSM builds this semantic representation through a statistical
analysis over the large contextual information in which a term occurs (see for details (Landauer, 1998),
(Blei, 2003)). One recent popular model to calculate this semantic relatedness by using the distributional
semantics is Explicit Semantic Analysis (ESA) proposed by (Gabrilovich and Markovitch, 2007), which
attempts to represent the semantics of the given term by a high dimensional vector in explicit concept
space such as Wikipedia concepts. Every explicit concept represents a dimension of the ESA vector, and
associativity weight of a given term with the explicit concept reflects the vector dimension weight. For
instance, for a given term t, ESA builds a vector v, where v =
?
N
i=0
a
i
? c
i
and c
i
is i
th
concept from the
explicit concept space, and a
i
is the associativity weight of term t with the concept c
i
. Here, N represents
the total number of concepts. The semantic relatedness score is calculated by taking cosine between the
corresponding high dimensional vectors.
3 Approach
Section 1 described our aim to investigate whether textual analysis techniques can yield evidence about
Byron?s influence on Moore?s writings by analyzing the four long poems (published in 1813-14) by
Lord Byron and a collection of four long poems (published in 1817) by Thomas Moore. To analyze this
influence, we calculate semantic relatedness scores between Byron?s poems and Moore?s poems. We split
44
these poems in line-groups
1
and obtain 227 line-groups from Byron?s poems and 246 line-groups from
Moore?s poems. We calculate ESA scores of every line-group of Byron?s poems with every line-group
of Moore?s poems. All the line-group pairs can be sorted according to their relatedness scores, which
can provide highly related line-group pairs. After getting these highly related pairs, we can manually
analyze them, and if manual analysis confirms the high relatedness of the pairs provided by ESA, then it
may indicate some degree of influence or imitation between the poets. Also, these results will conclude
that text analysis techniques can reduce the human effort in analyzing the influence between work by
different authors.
4 Evaluation
4.1 Experiment
We built two ESA models; one by using Wikipedia and the other by using a corpus of poetry primarily
from the eighteenth and nineteenth centuries
2
. In the first model, we take every Wikipedia article as a
dimension of the ESA vector, and TF-IDF weight of a given text with article content is considered as the
associativity strength with the corresponding dimension. We use modified ESA (Aggarwal, 2012) which
builds the ESA vector by taking all words of a given text together rather than taking them individually.
Wikipedia may not have a good coverage of the vocabulary of poems in Romantic Orientalism, which
led us to try another ESA model that utilizes a Poetic Corpus. This corpus consists of 892 long poems
and some of the poems contain more than 7K lines. Therefore, we split each poem with their line-groups
and obtain 22K different line-groups. Similar to Wikipedia-based ESA, we take every line-group as a
dimension of the ESA vector, and TF-IDF weight of the given text with line-group is considered as as-
sociativity strength with the corresponding dimension.
We use both ESA models: Wikipedia-based ESA and Poetic Corpus-based ESA to calculate the semantic
relatedness scores of every line-group of Byron?s poems with every line-group of Moore?s poems. Both
the results obtained by these two models are analyzed manually to check if Poetic Corpus-based ESA
outperforms Wikipedia-based ESA as Poetic Corpus has better vocabulary coverage for Romantic Ori-
entalism poems. We described in section 3 that we obtain 227 line-groups from Byron?s poems and 246
line-groups from Moore?s poems that means 56K line-group pairs. Manual analysis of 56K line-group
pairs will take a very long time, therefore, we analyze only a small subset of the 56K pairs. To select the
sample, we categorize the line-group pairs in three different categories: Highly-Related, May-be-Related
and Not-Related. In the ranked list of line-group pairs, the top 1K are considered Highly-Related, the
pairs ranked between 25K to 26K are considered May-be-Related, and the bottom 1K are taken as Not-
Related. We randomly selected 5 line-group pairs from each category and manually analyzed the results
obtained from ESA. Hence, we analyzed 15 pairs obtained according to Wikipedia-based ESA and 15
pairs according to PoeticCorpus-based ESA.
4.2 Results and Discussion
Manual (close-reading) analysis of 15 line-group pairs from the Wikipedia-based ESA took place first.
At first glance, the pairs identified as Highly-Related were indeed quite closely related, particularly in
terms of their narrative content. While some individual line-groups appeared in more than one pair-
ing identified by the model, the pair exhibited a frequently occurring narrative scenario where a female
character addressed her male lover before the departure or death of one of the parties. The model also
succeeded in identifying this scenario in poems by both Byron and Moore. The scenes are unsurprisingly
united by the presence of strong emotional language and imagery on the theme of love. However, the
recognition of a leavetaking (whether in death or departure) in the scenes is also noteworthy, as is the fact
that the identified line-groups are comprised of direct quotations from characters (as opposed to poetic
narrative).
1
Line-groups in poetry are similar to paragraphs in prose. On the printed page, a line of white space separates one line group
from the next. Like paragraphs, they vary in length, and are often semantically, syntactically, or thematically self-contained.
2
The poems in this corpus come from Women Writers Project (1560-1845), Eighteenth-Century Collections Online (1701-
1800), and poetic corpora shared by Ted Underwood (1701-1899)
45
Subjected to manual analysis, pairs of line-groups in the May-be-Related and Not-Related categories
exhibit varying degrees of relatedness. Most are lacking the immediate recognition of narrative similar-
ity evident in the Highly-Related pairs, with some pairs containing vastly different narrative scenarios.
Many of the consistencies from the Highly-Related category are also absent: some pairs vary greatly in
length, and some contain a mix of narrative and quotation. One example from the manually-analysed
examples proved to be a potential anomaly: a pair determined by the model to be Not-Related (i.e. in the
bottom 1K of pairs in terms of relatedness) might easily be considered related in that both line-groups
are florid poetic descriptions of a pastoral landscape.
The results of the Poetic Corpus-based ESA model were similar, if a little more refined. Interestingly,
the line-group pairs in the Highly-Related category were largely similar to those resulting from the
Wikipedia-based ESA. They were comprised of direct quotation (rather than narrative), and featured
a character speaking to their lover in strong emotional language. In some cases (though not consistently)
greater linguistic similarities between the pairings were more evident than in the results of the Wikipedia-
based ESA. This was an anticipated consequence of using the Poetic Corpus-based ESA, where the model
would be more likely to recognise the more unconventional features of nineteenth-century poetic diction
than the Wikipedia-based ESA.
From a literary-critical perspective, however, identification of the Highly-Related pairs by a computer is
no great advance on the capabilities of human scholarship. A traditional scholar can just as easily recog-
nise the similarities in the scenes identified by both the Wikipedia- and Poetic Corpus-based ESAs in the
course of reading the eight poems by Byron and Moore. Their narrative similarity is the most prominent
characteristic that contributes to their relatedness. This identification can be made by the lone scholar
because the dataset is relatively small in this project, and the time needed to read and analyse it is not
prohibitive. The potential value of this kind of automated semantic-relatedness identification is increased
when it is applied in a more exploratory fashion to larger datasets, and to poetic corpora whose scales
are beyond the reasonable comprehension of the individual scholar. In this scenario, a potential appli-
cation of the process would involve identifying and mapping the patterns and networks of relatedness in
large-scale poetic corpora. For the present purposes of this project?studying imitation and influence in
the texts of Byron and Moore?semantic relatedness measurements have been of limited value on their
own, but have offered promise in other areas. The first aspect of their success has been in identifying
sentiment analysis as a potential next step in drilling down into the texts to further reveal the essence of
their similarity. The second is revealing a wider application of semantic relatedness in examining broader
patterns of similarity within the history of poetry.
5 Conclusions and Future Work
We developed a method to identify influence and imitation in Romantic Orientalism poetry. We built two
Explicit Semantic Analysis (ESA) models by using Wikipedia and a Poetic Corpus. The results from
the analysis conducted with the Poetic Corpus-based ESA were a slight improvement on those resulting
from the Wikipedia-based ESA. This was as anticipated, and results might be improved even further with
a refined Poetic Corpus comprised of works from a more concentrated time period, which are more likely
to share linguistic similarities with the Byron and Moore poems.
The performance of ESA model depends on several parameters (Aggarwal, 2014) that are included
in the model, therefore, future work will include an investigation of ESA model in literature research.
Also, we are planning to use an improved version of the ESA (Polajnar, 2013) model which reduce
the orthogonality problem in the model. The value of ESA to the particular task of tracing imitation
and influence in the Romantic Orientalist poetry of Byron and Moore has been limited thus far, but it
has provided evidence of linguistic similarities in the expression of emotion. The next step for further
investigation of imitation and influence between the two poets will involve the use of sentiment analysis.
ESA was successful in identifying line groups that were closely related in terms of their narrative content
and in their use of similarly emotional language. For such a small dataset, this does not represent a
significant improvement on close reading, as similar results could have been obtained in this manner
quite quickly. But the automated identification of semantic relatedness demonstrated in this project has
46
potentially valuable applications for exploring broader literary corpora. For instance, a semantic mapping
of transnational and transhistorical poetic relatedness is a possible future venue for our research.
Acknowledgments
This work has been funded in part by a research grant from Science Foundation Ireland (SFI) under Grant
Number SFI/12/RC/2289 (INSIGHT) and by the EU FP7 program in the context of the project LIDER
(610782).
References
Harris, Zellig, Distributional structure, 1954. Word 10 (23): 146-162.
Gabrilovich, Evgeniy and Markovitch, Shaul, Computing semantic relatedness using Wikipedia-based explicit
semantic analysis, 2007. Proceedings of the 20th international joint conference on Artifical intelligence Hy-
derabad, India 1606?1611
Landauer, Thomas K and Foltz, Peter W and Laham, Darrell, An introduction to latent semantic analysis, Dis-
course processes, 25, 2-3, 259?284, 1998, Taylor & Francis
Blei, David M and Ng, Andrew Y and Jordan, Michael I Latent dirichlet allocation, the Journal of machine
Learning research, 3, 993?1022, 2003, JMLR. org
Aggarwal, Nitish and Asooja, Kartik and Buitelaar, Paul DERI&UPM: Pushing corpus based relatedness to simi-
larity: Shared task system description., Proceedings of the First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics, 2012.
Ramsay, Stephen Reading Machines: Toward an Algorithmic Criticism, Urbana: University of Illinois Press,
2011. Print.
Moore, Thomas The Letters of Thomas Moore, Ed. Wilfred S. Dowden. 2 vols. Oxford: Clarendon Press, 1964.
Print.
Gregory, Allan, ?Thomas Moore?s Orientalism.? Byron and Orientalism, Ed. Peter Cochran. Newcastle upon
Tyne: Cambridge Scholars, 2008. 173-82. Print.
Hamilton, Ian, ?Byron and the Best of Friends.? Keepers of the Flame Literary Estates and the Rise of Biography.
London: Hutchinson, 1992. Print.
Jordan, Hoover H., ?Byron and Moore.? Modern Language Quarterly, 9.4 (1948): 429-39. Print.
Sultana, Fehmida, ?Romantic Orientalism and Islam: Southey, Shelley, Moore, and Byron.?, Unpublished disser-
tation. Tufts University, 1989. Print.
Polajnar, Tamara and Aggarwal, Nitish and Asooja, Kartik and Buitelaar, Paul, Improving ESA with document
similarity, Advances in Information Retrieval, 582?593, 2013, Springer
Aggarwal, Nitish and Asooja, Kartik and Buitelaar, Paul Exploring ESA to Improve Word Relatedness, Third Joint
Conference on Lexical and Computational Semantics (*SEM), 2014
Tessier, Therese, ?Byron and Thomas Moore: A Great Literary Friendship.?, The Byron Journal 20 (1992): 4658.
MetaPress. Web. 22 Jan. 2014.
Vail, Jeffery W., The Literary Relationship of Lord Byron & Thomas Moore, Baltimore: Johns Hopkins University
Press, 2001. Print.
47
Proceedings of the 4th International Workshop on Computational Terminology, pages 22?31,
Dublin, Ireland, August 23 2014.
Identification of Bilingual Terms from Monolingual Documents for
Statistical Machine Translation
Mihael Arcan1 Claudio Giuliano2 Marco Turchi2 Paul Buitelaar1
1 Unit for Natural Language Processing, Insight @ NUI Galway, Ireland
{mihael.arcan , paul.buitelaar}@insight-centre.org
2 FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
{giuliano, turchi}@fbk.eu
Abstract
The automatic translation of domain-specific documents is often a hard task for generic Sta-
tistical Machine Translation (SMT) systems, which are not able to correctly translate the large
number of terms encountered in the text. In this paper, we address the problems of automatic
identification of bilingual terminology using Wikipedia as a lexical resource, and its integration
into an SMT system. The correct translation equivalent of the disambiguated term identified in
the monolingual text is obtained by taking advantage of the multilingual versions of Wikipedia.
This approach is compared to the bilingual terminology provided by the Terminology as a Ser-
vice (TaaS) platform. The small amount of high quality domain-specific terms is passed to the
SMT system using the XML markup and the Fill-Up model methods, which produced a relative
translation improvement up to 13% BLEU score points
1 Introduction
Translation tasks often need to deal with domain-specific terms in technical documents, which require
specific lexical knowledge of the domain. Nowadays, SMT systems are suitable to translate very frequent
expressions but fail in translating domain-specific terms. This mostly depends on a lack of domain-
specific parallel data from which the SMT systems can learn. Translation tools such as Google Translate
or open source phrase-based SMT systems, trained on generic data, are the most common solutions and
they are often used to translate manuals or very specific texts, resulting in unsatisfactory translations.
This problem is particular relevant for professional translators that work with documents coming from
different domains and are supported by generic SMT systems. A valuable solution to help them in han-
dling domain-specific terms is represented by online terminology resources, e.g. IATE - Inter-Active
Terminology for Europe,1 which are continuously updated and can be easily queried. However, the man-
ual use of these services can be very time demanding. For this reason, the identification and embedding
of domain-specific terms in an SMT system is a crucial step towards increasing translator productivity
and translation quality in highly specific domains.
In this paper, we propose an approach to automatically detect monolingual domain-specific terms from
a source language document and identify their equivalents using Wikipedia cross-lingual links. For this
purpose we extend The Wiki Machine API,2 a tool for linking terms in text to Wikipedia pages, adding
two more components able to first identify domain-specific terms, and to find their translations in a target
language. The identified bilingual terms are then compared with those obtained by TaaS (Skadins? et al.,
2013). The embedding of the domain-specific terms into an SMT system is performed by use of the
XML markup approach, which uses the terms as preferred translation candidates at run time, and the
Fill-Up model (Bisazza et al., 2011), which emphasizes phrase pairs extracted from the bilingual terms.
Our results show that the performance of our technique and TaaS are comparable in the identification
of monolingual and bilingual domain-specific terms. From the machine translation point of view, our
experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative
improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1 http://iate.europa.eu/ 2 https://bitbucket.org/fbk/thewikimachine/
Terminology questions in texts authored by patients
Noemie Elhadad
Department of Biomedical Informatics
Columbia University, USA
noemie@dbmi.columbia.edu
This work is c er a Creative Com ons Attribution 4.0 Internatio l License. Page numb rs and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
22
2 Methodology
Given a source document, it is processed by our pipeline that: (i) with the help of The Wiki Machine, it
identifies, disambiguates and links all terms in the document to the Wikipedia pages; (ii) the terms and
their links are used to identify the domain of the document and filter out the terms that are not domain-
specific; (iii) the translation of such terms is obtained following the Wikipedia cross-lingual links; (iv)
the bilingual domain-specific terms are embedded into the SMT system using different strategies. In the
rest of this section, each step is described in detail.
2.1 Bilingual Term Identification
Term Detection and Linking The Wiki Machine is a tool for linking terms in text to Wikipedia pages
and enriching them with information extracted from Wikipedia and Linked Open Data (LOD) resources
such as DBPedia or Freebase. The Wiki Machine has been preferred among other approaches because it
achieves the best performance in term disambiguation and linking (Mendes et al., 2011), and facilitates
the extraction of structured information from Wikipedia.
The annotation process consists of a three-step pipeline based on statistical and machine learning
methods that exclusively uses Wikipedia to train the models. No linguistic processing, such as stemming,
morphology analysis, POS tagging, or parsing, is performed. This choice facilitates the portability of the
system as the only requirement is the existence of a Wikipedia version with a sufficient coverage for the
specific language and domain. The first step identifies and ranks the terms by relevance using a simple
statistical approach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated
and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages.
The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must
be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense,
a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses
an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano
et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means
of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The
third step enriches the linked terms using information extracted from Wikipedia and LOD resources.
The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e.,
orthographical and morphological variants, synonyms, and related terms), images, topic, type, cross
language links, etc. For example, in the text ?click right mouse key to pop up menu and Gnome panel?,
The Wiki Machine identifies the terms mouse, key, pop up menu and Gnome panel. For the ambiguous
term mouse, the linking algorithm returns the Wikipedia page ?Mouse (computing)?, and the other terms
used to link that page in Wikipedia with their frequency, i.e., computer mouse, mice, and Mouse.
In the context of the experiments reported here, we were specifically interested in the identification of
domain-specific bilingual terminology to be embedded into the SMT system. For this reason, we extend
The Wiki Machine adding the functionality of filtering out terms that do not belong to the document
domain, and of automatically retrieving term translations.
Domain Detection To identify specific terms, we assign a domain to each linked term in a text, after
that we obtain the most frequent domain and filter out the terms that are out of scope. In the example
above, the term mouse is accepted because it belongs to the domain computer science, as the majority of
terms (mouse, pop up menu and Gnome panel), while the term key in the domain music is rejected.
The large number of languages and domains to cover prevents us from using standard text classification
techniques to categorize the document. For this reason, we implemented an approach based on the
mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia
categories are created and assigned by different human editors, and are therefore less rigorous, coherent
and consistent than usual ontologies. In addition, the Wikipedia?s category hierarchy forms a cyclic graph
(Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a
hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural
Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity,
allows us reducing the number of domains to few tens instead of some hundred thousands (800,000
23
categories in the English Wikipedia) and does not require any language-specific training data. Wikipedia
categories that contain more pages (?1,000) have been manually mapped to WordNet domains. The
domain for a term is obtained as follows. First, for each term, we extract its set of categories, C, from
the Wikipedia page linked to it. Second, by means of a recursive procedure, all possible outgoing paths
(usually in a large number) from each category in C are followed in the graph of Wikipedia categories.
When one of the mapped categories to a WordNet domain is found, the approach stops and associates the
relative WordNet domain to the term. In this way, more and more domains are assigned to a single term.
Third, to isolate the most relevant one, these domains are ranked according the number of times they have
been found following all the paths. The most frequent domain is assigned to the terms. Although this
process needs the human intervention for the manual mapping, it is done once and it is less demanding
than annotating large amounts of training documents for text classification, because it does not require
the reading of the document for topic identification.
Bilingual Term Extraction The last phase consists in finding the translation of the domain terminol-
ogy. We exploit the Wikipedia cross-language links, which, however, provide an alignment at page level
not at term level. To deal with this issue we introduced the following procedure. If the term is equal to
the source page title (ignoring case) we return the target page; otherwise, we return the most frequent al-
ternative form of the term in the target language. From the previous example, the system is able to return
the Italian page Mouse and all terms used in the Italian Wikipedia to express this concept of Mouse in
computer science. Using this information, the term mouse is paired with its translation into Italian.
2.2 Integration of Bilingual Terms into SMT
A straightforward approach for adding bilingual terms to the SMT system consists of concatenating the
training data and the terms. Although it has been shown to perform better than more complex techniques
(Bouamor et al., 2012), it is still affected by major disadvantages that limits its use in real applications.
In particular, when small amounts of bilingual terms are concatenated with a large training dataset, terms
with ambiguous translations are penalised, because the most frequent and general translations often
receive the highest probability, which drives the SMT system to ignore specific translations.
In this paper, we focus on two techniques that give more priority to specific translations than generic
ones: the Fill-Up model and the XML markup approach. The Fill-Up model has been developed to
address a common scenario where a large generic background model exists, and only a small quantity
of in-domain data can be used to build an in-domain model. Its goal is to leverage the large coverage
of the background model, while preserving the domain-specific knowledge coming from the in-domain
data. Given the generic and the in-domain phrase tables, they are merged. For those phrase pairs that
appear in both tables, only one instance is reported in the Fill-Up model with the largest probabilities
according to the tables. To keep track of a phrase pair?s provenance, a binary feature that penalises if
the phrase pair comes from the background table is added. The same strategy is used for reordering
tables. In our experiments, we use the bilingual terms identified from the source data as in-domain
data. Word alignments are computed on the concatenation of the data. Phrase extraction and scoring
are carried out separately on each corpus. The XML markup approach makes it possible to directly pass
external knowledge to the decoder, specifying translations for particular spans of the source sentence. In
our scenario, the source term is used to identify a span in the source sentence, while the target term is
directly passed to the decoder. With the setting exclusive, the decoder uses only the specified translations
ignoring other possible translations in the translation model.
3 Experimental Setting
In our experiments, we used different English-Italian and Italian-English test sets from two domains: (i)
a small subset of the GNOME project data3 (4,3K tokens) and KDE4 Data4 (9,5K) for the IT domain
and (ii) a subset of the EMEA corpus (11K) for the medical domain.
In order to assess the quality of the monolingual and bilingual terms, we create a terminological gold
standard. Two annotators with a linguistic background and English and Italian proficiency were asked
3 https://l10n.gnome.org/ 4 http://i18n.kde.org/
24
to mark all domain-specific terms in a set of 66 English and Italian documents of the GNOME corpus,
and a set of 100 paragraphs (4,3K tokens) from the KDE4 corpus.5 Domain-specificity was defined as
all (multi-)words that are typically used in the IT domain and that may have different Italian translations
in other domains. The average Cohen?s Kappa of GNOME and KDE anno computed at token level was
0.66 for English and 0.53 for Italian. Following Landis and Koch (1977), this corresponds to a substantial
and moderate agreement between the annotators.
Finally the gold standard dataset was generated by the intersection of the annotations of the two an-
notators. In detail, for the GNOME dataset the annotators marked 93 single-word and 134 multi-word
expressions (MWEs), resulting 227 terms in overall. For the KDE anno dataset, 321 monolingual terms
for the GNOME dataset were annotated, whereby 192 of them were multi-word expressions. This results
in 190 unique bilingual terms for the GNOME corpus and 355 for the KDE anno dataset.
We compare the monolingual and bilingual terms identified by our approach to the terms obtained
by the online service TaaS,6 which is a cloud-based platform for terminology services based on the
state-of-the-art terminology extraction and bilingual terminology alignment methods. TaaS provides
several options in term identification, of which we selected TWSC, Tilde wrapper system for CollTerm,
(Pinnis et al., 2012). TWSC is based on linguistic analysis, i.e. part of speech tagging and morpho-
syntactic patterns, enriched with statistical features. TaaS allows for lookup in several manually and
automatically built monolingual and bilingual terminological resources and for our experiment we use
EuroTermBank (ETB), Taus Data and Web Data. Accessing several resources, TaaS may provide several
translations for a unique source term, but not an indicator of their translation quality. To avoid assigning
the same probability to all the translations of the same source term, we prioritise a translation by the
resource it was provided. In our case, we favour first the translation provided by ETB. If no translation
is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting
the term extraction approach, TaaS requires manual specification of the source and target languages, the
domain, and the source document. Since we focused on the IT and medical domains we set the options
to ?Information and communication technology? and ?Medicine and pharmacy?, respectively.
For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where
the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit
(Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage,
we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl
(Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of
?37M tokens and a development set of ?10K tokens.
In our experiments, an instance of Moses trained on the generic parallel dataset was used in three
different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup
approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the
Fill-Up method as background translation model.
4 Evaluation
In this Section, we report the performance of the different term identification tools and term embedding
methods for the two domains: IT and the medical domain. For evaluating the extracted monolingual
and bilingual terms, we calculate precision, recall and f-measure using the manually labelled KDE anno
and GNOME datasets. In addition, we perform a manual inspection of a subset of the bilingual identi-
fied terms. The BLEU metric (Papineni et al., 2002) was used to automatically evaluate the quality of
the translations. The metric calculates the overlap of n-grams between the SMT system output and a
reference translation, provided by a professional translator.
4.1 Monolingual Term Identification
In Table 1, the column ?Ident.? represents the number of identified terms for each tool, whereby we
observed TaaS always extracts more terms than The Wiki Machine. While extracting Italian terms,
TaaS extracts twice as more terms as The Wiki Machine, which can be explained by the overall lower
5 In the rest of the paper, we refer to the annotated part of KDE4 as KDE anno
6 https://demo.taas-project.eu/
25
English Italian
KDE anno Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 431 144 287 0.442 0.594 0.507 518 147 371 0.326 0.511 0.398
The Wiki Machine 327 247 80 0.400 0.406 0.403 207 184 23 0.429 0.268 0.330
GNOME Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 311 119 192 0.260 0.355 0.301 359 110 249 0.272 0.415 0.329
The Wiki Machine 275 199 76 0.303 0.364 0.330 196 167 29 0.331 0.275 0.301
Table 1: Evaluation of monolingual term identification for the KDE anno and GNOME dataset.
amount of Italian pages in Wikipedia compared to the English version. Focusing on the amount of
identified single-word and multi-word expressions, it is interesting to notice that TaaS, independently of
the language, extracts around twice as more MWEs than single words. Differently, The Wiki Machine
identifies mostly single-word terms, whereby they represent around three-fourth of all identified terms
for English and around 12% for Italian.
For the KDE anno dataset, TaaS in most cases (except in precision for the Italian KDE anno dataset)
outperforms The Wiki Machine approach in all metrics. Especially we observed a higher recall produced
by the TaaS approach, which can be deduced from the higher number of extracted MWEs compared to
The Wiki Machine approach. On the English GNOME dataset, The Wiki Machine performs comparable
results to TaaS, with a slightly higher recall and F1. On the Italian side, The Wiki Machine identifies less
MWEs than TaaS, which results in a low recall and F1.
In summary, we observe that TaaS performs best on the KDE anno dataset, whereas The Wiki Machine
and TaaS perform comparable results on the GNOME dataset. Analysing the overall results, we notice
that precision, recall and F1 are generally better in English than in Italian. This is due to the fact that
Italian tends to use more words to express the same concept compared to English.
4.2 Bilingual Term Identification
Table 2 reports the performance of The Wiki Machine and TaaS in the identification of bilingual terms
evaluated against the manually produced list of terms. In both language pairs and datasets, TaaS and The
Wiki Machine mostly identify similar amounts of bilingual terms (column ?Ident.?) and match with the
gold standard (column ?Mat.?). Only for KDE anno, It?En, TaaS identifies almost 50% more bilingual
terms than The Wiki Machine.
It is worth noticing that, although TaaS is accessing high quality manually-produced termbases, e.g.
ETB in our results, there is no evidence that it works significantly better than The Wiki Machine access-
ing Wikipedia. In fact, in terms of F1, The Wiki Machine performs best on the GNOME annotated test
set, while it is outperformed by TaaS on KDE anno. In both cases, differences in performance are mini-
mal. According to the precision measure, The Wiki Machine seems to be able to produce more accurate
bilingual terms.
The automatic evaluation shows difficulties (low F1 scores) for The Wiki Machine and TaaS in iden-
tifying bilingual terms that perfectly match the gold standard. To better understand the quality of term
translations, we asked one of the annotators involved in the creation of the gold standard to perform a
manual evaluation of a subset of fifty bilingual terms randomly selected from each list. We used the
four error categories proposed in (Aker et al., 2013): 1) The terms are exact translations of each other
in the domain; 2) Inclusion: Not an exact translation, but an exact translation of one term is entirely
contained within the term in the other language; 3) Overlap: Not category 1 or 2, but the terms share at
least one translated word; 4) Unrelated: No word in either term is a translation of a word in the other.
The percentages of bilingual terms assigned to each class are shown in Table 3.
In terms of comparison between the two tools, the manual evaluation confirms that there is no evidence
that a tool produces better term translations than the other in all the test sets. In fact, except for KDE anno
En?It where TaaS outperforms The Wiki Machine, the percentage of bilingual terms assigned to class
1 for both the tools is almost similar. In terms of absolute scores, the manual evaluation shows that
the quality of the identified bilingual terms is relatively high (merging the terms assigned to classes 1
26
GNOME En?It Ident. Mat. Precision Recall F1
TaaS 145 20 0.138 0.105 0.119
The Wiki Machine 156 25 0.160 0.130 0.144
GNOME It?En Ident. Mat. Precision Recall F1
TaaS 139 21 0.151 0.110 0.127
The Wiki Machine 140 23 0.164 0.121 0.139
KDE anno En?It Ident. Mat. Precision Recall F1
TaaS 249 65 0.261 0.183 0.215
The Wiki Machine 229 49 0.202 0.138 0.164
KDE anno It?En Ident. Mat. Precision Recall F1
TaaS 228 58 0.254 0.163 0.199
The Wiki Machine 155 48 0.292 0.135 0.185
Table 2: Automatic evaluation of bilingual terms ex-
tracted from GNOME and KDE anno.
GNOME En?It 1 2 3 4
TaaS 0.66 0.08 0.00 0.26
The Wiki Machine 0.70 0.08 0.06 0.16
GNOME It?En 1 2 3 4
TaaS 0.78 0.08 0.02 0.12
The Wiki Machine 0.68 0.12 0.04 0.16
KDE anno En?It 1 2 3 4
TaaS 0.90 0.00 0.06 0.04
The Wiki Machine 0.70 0.10 0.06 0.14
KDE anno It?En 1 2 3 4
TaaS 0.70 0.10 0.10 0.10
The Wiki Machine 0.64 0.22 0.08 0.06
Table 3: Manual evaluation of bilingual terms
based on four error categories (1-4).
and 2, we reach a score, in most of the cases, larger than 80%). This is in contrast with the automatic
evaluation, which reports limited performances (F1 ? 0.2) for both methods. The main reason is that
the automatic evaluation requires a perfect match between the identified and the gold standard bilingual
terms to measure an improvement in F1, while the manual evaluation can reward bilingual terms that do
not perfectly match any gold standard terms but are correct translations of each other. An example is
the multi-word bilingual term ?settings of the network connection? impostazioni della connessione di
rete? that is present in the gold standard as a single multi-word term, while it is identified by The Wiki
Machine as two distinct bilingual terms, i.e. ?network connection? connessione di rete? and ?settings
? impostazioni?. From the translation point of view, both the distinct terms are correct and they are
assigned to class 1 during the manual evaluation, but they are ignored by the automatic evaluation.
The analysis of terms assigned to error class four shows that both methods are affected by similar
problems. The main source of error is the correct detection of the source term domain, which results in
a translated term that does not belong to the correct domain. For instance, in the bilingual term ?stringhe
? shoe and boot laces?, the term ?stringhe? (?strings? in the IT domain) is translated into ?laces?. Simi-
larly, the English term ?launchers? (?lanciatori? in Italian in the IT domain) is translated into ?lanciarazzi
multiplo? (?multiple rocket launchers? in English), which is clearly not an IT term. Furthermore, The
Wiki Machine seems to have more problems in identifying the right morphological variation, e.g. ?in-
dirizzi ip? ip address?, where ?indirizzi? is a plural noun and needs to be translated into ?addresses?.
This is expected because page titles in Wikipedia are not always inflected. An interesting example high-
lighted by the annotator in the TaaS translations is: ?percorso di ricerca? ? ?how do i access refresh
grid texture??, where the Italian term (?search path? in English) is translated with a completely wrong
translation. In the next Section we evaluate whether the automatic identified bilingual terms can improve
the performance of an SMT system and if it is robust to the aforementioned errors.
4.3 Embedding Terminology into SMT
Our further experiments focused on the automatic evaluation of the translation quality of the EMEA,
GNOME and KDE test sets (Table 4). The obtained bilingual terminology from TaaS and The Wiki Ma-
chine was embedded through the Fill-Up and XML markup approaches. The approximate randomization
approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances
are statistically significant with a p-value < 0.05. The parameters of the baseline method and the Fill-Up
models were optimized on the development set.
Injecting the obtained TaaS bilingual terms improves the BLEU score in several cases. XML markup
outperforms the general baseline approach in three (out of eight) datasets, whereby three of them are
statistically significant (GNOME En?It, KDE anno En?It). Embedding the same bilingual terminol-
ogy into the Fill-Up model helped to outperform the baseline approach for all test sets, whereby only the
result for EMEA En?It is not statistically significant.
27
GNOME KDE anno EMEA KDE4
En?It It?En En?It It?En En?It It?En En?It It?En
general baseline 15.39 21.62 15.58 22.64 25.88 25.75 19.22 23.54
XML Mark-up (TaaS) 15.87 22.45* 17.62* 23.88* 25.84 25.74 18.97 24.27*
Fill-Up Model (TaaS) 16.22* 22.73* 17.61* 23.45* 25.95 26.02* 19.69* 24.56*
XML Mark-up (The Wiki Machine) 15.49 20.57 17.19* 23.44* 25.59 24.97 17.74 22.16
Fill-Up Model (The Wiki Machine) 15.82 21.70 16.48* 23.28* 26.35* 26.44* 19.61* 24.14*
Table 4: Automatic BLEU Evaluation on GNOME, KDE and EMEA datasets with different term em-
bedding strategies (bold results = best performance ; * statistically significant compared to baseline).
Finally, we investigate the impact of embedding the identified terms provided by The Wiki Machine.
When we suggest translation candidates with the XML markup, it only slightly outperforms the baseline
approach for GNOME En?It, but statistically significant improves the translations for the KDE anno
test set for both language directions. Similarly to previous observations, the Fill-Up model improves
further the translations, i.e. the translations are statistically significant better than the baseline for both
language pairs of both KDE test sets as well as for EMEA.
To better understand our translation results, we manually inspected the EMEA En?It sentences, which
have the best translation performance. For each of the source sentence and the translation method,
we analyse the translated sentences and the bilingual terms that match at least one word in the source
sentence. Both translation strategies tried to encapsulate the bilingual terms, but there is clear evidence
that the Fill-Up model better embeds the target terms in the context of the translation. For instance in
the following example, the target sentence produced by the XML markup (XML trg) does not contain
the article ?la?, uses a wrong conjunction (?di? instead of ?per?) and wrongly orders the adjective with
the noun (?adulti pazienti? instead of ?pazienti adulti?). All these issues are correctly addressed by the
Fill-Up model (Fill-Up trg) which produces a smoother translation.
source sentence: adult patients receive therapy for tumours
reference sentence: pazienti adulti ricevono la terapia per i tumori
bilingual terms: therapy? terapia, patients? pazienti, adult? adulti
XML trg: adulti pazienti ricevono terapia di tumori
Fill-Up trg: pazienti adulti ricevono la terapia per i tumori
Analysing the number of suggested bilingual terms per sentence, we notice that The Wiki Machine
tends to propose more terms than TaaS (on average, The Wiki Machine 3.1, TaaS 2.5 per sentence).
Of these terms, TaaS provides on average more translations for each unique source term than The Wiki
Machine (on average, TaaS 1.51, The Wiki Machine 1).
In addition to evaluating the performance of TaaS and The Wiki Machine separately, for the EMEA
dataset we concatenate the terminological lists provided by the tools and supply it to the XML markup
and the Fill-Up approach. Embedding the combined terminology with the XML markup produces a
BLEU score of 25.59 for En?It and 24.92 for It?En. This performance is similar to the scores obtained
using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole
terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En?It and 27.02 for It?En,
which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of
the two term identification methods and suggests a novel research direction.
5 Related Work
The main focus of our research is on bilingual term identification and the embedding of this knowledge
into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that
an SMT system built by using a large general resource cannot be used to translate domain-specific terms,
we have to provide the system domain-specific lexical knowledge.
Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term
identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual
dictionary entries from Wikipedia to support the machine translation system. Based on exact string
28
matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual
dictionary. Besides the interwiki link system, Erdmann et al. (2009) enhances their bilingual dictionary
by using redirection page titles and anchor text within Wikipedia. To filter out incorrect term translation
pairs, the authors use the backward link information to prove if a redirect page title or an anchor text
represents a synonymous expression. Niehues and Waibel (2011) analyse different methods to integrate
the extracted Wikipedia titles into their system, whereby they explore methods to disambiguate between
different translations by using the text in the articles. In addition, the authors use morphological forms
of terms to enhance the extracted bilingual dictionary. The results show that the number of out-of-
vocabulary words could be reduced by 50% on computer science lectures, which improved the translation
quality by more than 1 BLEU point. Arcan et al. (2013) restrict term identification to the observed
domain by using the frequency information of Wikipedia categories. Different from these approaches
we focus on domain-specific dictionary generation, ignoring identified terms which do not belong to the
domain to be observed. Furthermore, we take advantage of the Wikipedia category graph representation
and its linking to WordNet domain, which allowed us to identify the domain we were interested in.
Furthermore, research has been done on the integration of domain-specific parallel data into SMT,
either by retraining small domain-specific and large general resources as one concatenated parallel data
(Koehn and Schroeder, 2007), adding new phrase pairs directly into the phrase table (Langlais, 2002;
Ren et al., 2009; Haddow and Koehn, 2012) or assigning adequate weights to the in- and out-of-domain
translation models (Foster and Kuhn (2007); La?ubli et al. (2013)). Bouamor et al. (2012) address the
problem of finding the best approach to integrate new obtained knowledge in an SMT system, and show
that they should be used as additional parallel sentences to train the translation model. In our approach,
we use the XML markup and the Fill-Up approach, which handles the in-domain parallel data equally
to the out-domain data. Furthermore, Okita and Way (2010) investigate the effect of integrating bilin-
gual terminology in the training step of an SMT system, and analyse in particular the performance and
sensitivity of the word aligner. As opposed to their approach, we do not have prior knowledge about the
bilingual terminology, since we extract it from the document to be translated.
6 Conclusion
In this paper we presented an approach to identify bilingual domain-specific terms starting from a mono-
lingual text and to integrate these into an SMT system. With the help of terminological and lexical
resources, we are able to discover a small amount (?200) of high-quality domain-specific terms and
enhanced the performance of an SMT system trained on large amounts (1.8M) of parallel sentences.
Monolingual and bilingual term evaluation showed no evidence that one of the tested tools (The Wiki
Machine or TaaS) produces better terms than the other in all the test sets. Depending on the manual map-
ping between the Wikipedia categories and WordNet domains and the existence of a Wikipedia version,
our approach is language and domain independent, does not need training data and is able to overcome
the sparseness and coherence problems of the Wikipedia categories. Evaluation of the two systems on
different language directions and domains shows significant improvements over the baseline in terms
of two BLEU scores (up to 13%) and confirms the applicability of such techniques in a real scenario.
It is interesting to notice that the Fill-Up technique regularly outperforms the XML markup approach,
taking advantage of all terms and not only the overlapping terms in the text to be translated. Our contri-
bution shows a different context of using Fill-Up and extends the usability of it in terms of embedding
terminological knowledge into SMT. In future work, we plan to focus on exploiting morphological term
variations taking advantage of the alternative terms (i.e., orthographical and morphological variants,
synonyms, and related terms) provided by The Wiki Machine. This will make it possible to increase the
coverage adding new terms and the accuracy of the proposed method for bilingual term identification.
Acknowledgments
This publication has emanated from research supported in part by a research grant from Science Founda-
tion Ireland (SFI) under Grant Number SFI/12/RC/2289 and by the European Union supported projects
EuroSentiment (Grant No. 296277), LIDER (Grant No. 610782) and MateCat (ICT-2011.4.2-287688).
29
References
Ahmet Aker, Monica Paramita, and Robert Gaizauskas. 2013. Extracting bilingual terminologies from comparable
corpora. In Proceedings of ACL, Sofia, Bulgaria.
Mihael Arcan, Susan Marie Thomas, Derek De Brandt, and Paul Buitelaar. 2013. Translating the FINREP taxon-
omy using a domain-specific corpus. In Machine Translation Summit XIV, pages 199?206.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the wordnet domains
hierarchy: semantics, coverage and balancing. In Proceedings of the Workshop on Multilingual Linguistic
Ressources, pages 101?108. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based
SMT Adaptation. In Proceedings of IWSLT.
Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2012. Identifying bilingual multi-word expres-
sions for statistical machine translation. In Proceedings of the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instability . In Proceedings of the Association for Computational
Lingustics.
Maike Erdmann, Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio. 2009. Improving the extraction of bilin-
gual terminology from wikipedia. ACM Trans. Multimedia Comput. Commun. Appl., 5(4):31:1?31:17, Novem-
ber.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages 1618?1621. ISCA.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo Strapparava. 2009. Kernel methods for minimally
supervised wsd. Computational Linguistics, 35(4):513?528.
Barry Haddow and Philipp Koehn. 2012. Analysing the Effect of Out-of-Domain Data on SMT Systems. In
Proceedings of the Seventh Workshop on Statistical Machine Translation, Montre?al, Canada. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In
Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual
Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages 79?86. AAMT.
J. Richard Landis and Gary G. Koch. 1977. Measurement of Observer Agreement for Categorical Data. In
Biometrics, volume 33, pages 159?174.
Philippe Langlais. 2002. Improving a general-purpose statistical translation engine by terminological lexicons. In
Proceedings of the 2nd International Workshop on Computational Terminology (COMPUTERM) ?2002, Taipei,
Taiwan, pages 1?7.
Samuel La?ubli, Mark Fishel, Martin Volk, and Manuela Weibel. 2013. Combining statistical machine translation
and translation memories with domain adaptation. In Stephan Oepen, Kristin Hagen, and Janne Bondi Johan-
nesse, editors, Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013),
May 22?24, 2013, Oslo University, Norway, Linko?ping Electronic Conference Proceedings, pages 331?341,
Oslo, May. Linko?pings universitet Electronic Press.
30
Pablo N Mendes, Max Jakob, Andre?s Garc??a-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shedding light
on the web of documents. In Proceedings of the 7th International Conference on Semantic Systems, pages 1?8.
ACM.
Rada Mihalcea. 2007. Using Wikipedia for Automatic Word Sense Disambiguation. In Proceedings of NAACL-
HLT, pages 196?203.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia to Translate Domain-specific Terms in SMT. In nterna-
tional Workshop on Spoken Language Translation, San Francisco, CA, USA.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29.
Tsuyoshi Okita and Andy Way. 2010. Statistical Machine Translation with Terminology. In Proceedings of the
First Symposium on Patent Information Processing (SPIP), Tokyo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 311?318.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, Inguna Skadin?a, Marko Tadic?, and Tatiana Gornostay. 2012.
Term extraction, tagging, and mapping tools for under-resourced languages. In Proceedings of the Terminology
and Knowledge Engineering (TKE2012) Conference.
Zhixiang Ren, Yajuan Lu?, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation
using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions:
Identification, Interpretation, Disambiguation and Applications, MWE ?09, pages 47?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Raivis Skadins?, Marcis Pinnis, Tatiana Gornostay, and Andrejs Vasiljevs. 2013. Application of online terminology
services in statistical machine translation. In Proceedings of the XIV Machine Translation Summit, Nice, France.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel Varga.
2006. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th
International Conference on Language Resources and Evaluation (LREC?2006).
Jo?rg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mariani, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Francis M. Tyers and Jacques A. Pieanaar. 2008. Extracting bilingual word pairs from wikipedia. In Collabo-
ration: interoperability between people in the creation of language resources for less-resourced languages (A
SALTMIL workshop).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08, pages 993?1000.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In
Proceedings of the TextGraphs-2 Workshop (NAACL-HLT), pages 1?8, Rochester, April. Association for Com-
putational Linguistics.
31
