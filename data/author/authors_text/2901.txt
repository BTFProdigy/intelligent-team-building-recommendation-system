Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Topic Modelling for Multi-Party Spoken Discourse
Matthew Purver
CSLI
Stanford University
Stanford, CA 94305, USA
mpurver@stanford.edu
Konrad P. Ko?rding
Dept. of Brain & Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
kording@mit.edu
Thomas L. Griffiths
Dept. of Cognitive & Linguistic Sciences
Brown University
Providence, RI 02912, USA
tom griffiths@brown.edu
Joshua B. Tenenbaum
Dept. of Brain & Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
jbt@mit.edu
Abstract
We present a method for unsupervised
topic modelling which adapts methods
used in document classification (Blei et
al., 2003; Griffiths and Steyvers, 2004) to
unsegmented multi-party discourse tran-
scripts. We show how Bayesian infer-
ence in this generative model can be
used to simultaneously address the prob-
lems of topic segmentation and topic
identification: automatically segmenting
multi-party meetings into topically co-
herent segments with performance which
compares well with previous unsuper-
vised segmentation-only methods (Galley
et al, 2003) while simultaneously extract-
ing topics which rate highly when assessed
for coherence by human judges. We also
show that this method appears robust in
the face of off-topic dialogue and speech
recognition errors.
1 Introduction
Topic segmentation ? division of a text or dis-
course into topically coherent segments ? and
topic identification ? classification of those seg-
ments by subject matter ? are joint problems. Both
are necessary steps in automatic indexing, retrieval
and summarization from large datasets, whether
spoken or written. Both have received significant
attention in the past (see Section 2), but most ap-
proaches have been targeted at either text or mono-
logue, and most address only one of the two issues
(usually for the very good reason that the dataset
itself provides the other, for example by the ex-
plicit separation of individual documents or news
stories in a collection). Spoken multi-party meet-
ings pose a difficult problem: firstly, neither the
segmentation nor the discussed topics can be taken
as given; secondly, the discourse is by nature less
tidily structured and less restricted in domain; and
thirdly, speech recognition results have unavoid-
ably high levels of error due to the noisy multi-
speaker environment.
In this paper we present a method for unsuper-
vised topic modelling which allows us to approach
both problems simultaneously, inferring a set of
topics while providing a segmentation into topi-
cally coherent segments. We show that this model
can address these problems over multi-party dis-
course transcripts, providing good segmentation
performance on a corpus of meetings (compara-
ble to the best previous unsupervised method that
we are aware of (Galley et al, 2003)), while also
inferring a set of topics rated as semantically co-
herent by human judges. We then show that its
segmentation performance appears relatively ro-
bust to speech recognition errors, giving us con-
fidence that it can be successfully applied in a real
speech-processing system.
The plan of the paper is as follows. Section 2
below briefly discusses previous approaches to the
identification and segmentation problems. Sec-
tion 3 then describes the model we use here. Sec-
tion 4 then details our experiments and results, and
conclusions are drawn in Section 5.
2 Background and Related Work
In this paper we are interested in spoken discourse,
and in particular multi-party human-human meet-
ings. Our overall aim is to produce information
which can be used to summarize, browse and/or
retrieve the information contained in meetings.
User studies (Lisowska et al, 2004; Banerjee et
al., 2005) have shown that topic information is im-
portant here: people are likely to want to know
17
which topics were discussed in a particular meet-
ing, as well as have access to the discussion on
particular topics in which they are interested. Of
course, this requires both identification of the top-
ics discussed, and segmentation into the periods of
topically related discussion.
Work on automatic topic segmentation of text
and monologue has been prolific, with a variety of
approaches used. (Hearst, 1994) uses a measure of
lexical cohesion between adjoining paragraphs in
text; (Reynar, 1999) and (Beeferman et al, 1999)
combine a variety of features such as statistical
language modelling, cue phrases, discourse infor-
mation and the presence of pronouns or named
entities to segment broadcast news; (Maskey and
Hirschberg, 2003) use entirely non-lexical fea-
tures. Recent advances have used generative mod-
els, allowing lexical models of the topics them-
selves to be built while segmenting (Imai et al,
1997; Barzilay and Lee, 2004), and we take a sim-
ilar approach here, although with some important
differences detailed below.
Turning to multi-party discourse and meetings,
however, most previous work on automatic seg-
mentation (Reiter and Rigoll, 2004; Dielmann
and Renals, 2004; Banerjee and Rudnicky, 2004),
treats segments as representing meeting phases or
events which characterize the type or style of dis-
course taking place (presentation, briefing, discus-
sion etc.), rather than the topic or subject matter.
While we expect some correlation between these
two types of segmentation, they are clearly differ-
ent problems. However, one comparable study is
described in (Galley et al, 2003). Here, a lex-
ical cohesion approach was used to develop an
essentially unsupervised segmentation tool (LC-
Seg) which was applied to both text and meet-
ing transcripts, giving performance better than that
achieved by applying text/monologue-based tech-
niques (see Section 4 below), and we take this
as our benchmark for the segmentation problem.
Note that they improved their accuracy by com-
bining the unsupervised output with discourse fea-
tures in a supervised classifier ? while we do not
attempt a similar comparison here, we expect a
similar technique would yield similar segmenta-
tion improvements.
In contrast, we take a generative approach,
modelling the text as being generated by a se-
quence of mixtures of underlying topics. The ap-
proach is unsupervised, allowing both segmenta-
tion and topic extraction from unlabelled data.
3 Learning topics and segments
We specify our model to address the problem of
topic segmentation: attempting to break the dis-
course into discrete segments in which a particu-
lar set of topics are discussed. Assume we have a
corpus of U utterances, ordered in sequence. The
uth utterance consists of Nu words, chosen from
a vocabulary of size W . The set of words asso-
ciated with the uth utterance are denoted wu, and
indexed as wu,i. The entire corpus is represented
by w.
Following previous work on probabilistic topic
models (Hofmann, 1999; Blei et al, 2003; Grif-
fiths and Steyvers, 2004), we model each utterance
as being generated from a particular distribution
over topics, where each topic is a probability dis-
tribution over words. The utterances are ordered
sequentially, and we assume aMarkov structure on
the distribution over topics: with high probability,
the distribution for utterance u is the same as for
utterance u?1; otherwise, we sample a new distri-
bution over topics. This pattern of dependency is
produced by associating a binary switching vari-
able with each utterance, indicating whether its
topic is the same as that of the previous utterance.
The joint states of all the switching variables de-
fine segments that should be semantically coher-
ent, because their words are generated by the same
topic vector. We will first describe this generative
model in more detail, and then discuss inference
in this model.
3.1 A hierarchical Bayesian model
We are interested in where changes occur in the
set of topics discussed in these utterances. To this
end, let cu indicate whether a change in the distri-
bution over topics occurs at the uth utterance and
let P (cu = 1) = pi (where pi thus defines the ex-
pected number of segments). The distribution over
topics associated with the uth utterance will be de-
noted ?(u), and is a multinomial distribution over
T topics, with the probability of topic t being ?(u)t .
If cu = 0, then ?(u) = ?(u?1). Otherwise, ?(u)
is drawn from a symmetric Dirichlet distribution
with parameter ?. The distribution is thus:
P (?(u)|cu, ?
(u?1)) =
(
?(?(u), ?(u?1)) cu = 0
?(T?)
?(?)T
QT
t=1(?
(u)
t )
??1 cu = 1
18
Figure 1: Graphical models indicating the dependencies among variables in (a) the topic segmentation
model and (b) the hidden Markov model used as a comparison.
where ?(?, ?) is the Dirac delta function, and ?(?)
is the generalized factorial function. This dis-
tribution is not well-defined when u = 1, so
we set c1 = 1 and draw ?(1) from a symmetric
Dirichlet(?) distribution accordingly.
As in (Hofmann, 1999; Blei et al, 2003; Grif-
fiths and Steyvers, 2004), each topic Tj is a multi-
nomial distribution ?(j) over words, and the prob-
ability of the word w under that topic is ?(j)w . The
uth utterance is generated by sampling a topic as-
signment zu,i for each word i in that utterance with
P (zu,i = t|?(u)) = ?
(u)
t , and then sampling a
word wu,i from ?(j), with P (wu,i = w|zu,i =
j, ?(j)) = ?(j)w . If we assume that pi is generated
from a symmetric Beta(?) distribution, and each
?(j) is generated from a symmetric Dirichlet(?)
distribution, we obtain a joint distribution over all
of these variables with the dependency structure
shown in Figure 1A.
3.2 Inference
Assessing the posterior probability distribution
over topic changes c given a corpus w can be sim-
plified by integrating out the parameters ?, ?, and
pi. According to Bayes rule we have:
P (z, c|w) =
P (w|z)P (z|c)P (c)
P
z,c P (w|z)P (z|c)P (c)
(1)
Evaluating P (c) requires integrating over pi.
Specifically, we have:
P (c) =
R 1
0 P (c|pi)P (pi) dpi
= ?(2?)?(?)2
?(n1+?)?(n0+?)
?(N+2?)
(2)
where n1 is the number of utterances for which
cu = 1, and n0 is the number of utterances for
which cu = 0. Computing P (w|z) proceeds along
similar lines:
P (w|z) =
R
?TW
P (w|z, ?)P (?) d?
=
?
?(W?)
?(?)W
?T QT
t=1
QW
w=1 ?(n
(t)
w +?)
?(n(t)? +W?)
(3)
where ?TW is the T -dimensional cross-product of
the multinomial simplex on W points, n(t)w is the
number of times word w is assigned to topic t in
z, and n(t)? is the total number of words assigned
to topic t in z. To evaluate P (z|c) we have:
P (z|c) =
Z
?UT
P (z|?)P (?|c) d? (4)
The fact that the cu variables effectively divide
the sequence of utterances into segments that use
the same distribution over topics simplifies solving
the integral and we obtain:
P (z|c) =
?
?(T?)
?(?)T
?n1 Y
u?U1
QT
t=1 ?(n
(Su)
t + ?)
?(n(Su)? + T?)
. (5)
19
P (cu|c?u, z,w) ?
8
>
><
>
>:
QT
t=1 ?(n
(S0u)
t +?)
?(n
(S0u)
? +T?)
n0+?
N+2? cu = 0
?(T?)
?(?)T
QT
t=1 ?(n
(S1u?1)
t +?)
?(n
(S1u?1)
? +T?)
QT
t=1 ?(n
(S1u)
t +?)
?(n
(S1u)
? +T?)
n1+?
N+2? cu = 1
(7)
where U1 = {u|cu = 1}, U0 = {u|cu = 0}, Su
denotes the set of utterances that share the same
topic distribution (i.e. belong to the same segment)
as u, and n(Su)t is the number of times topic t ap-
pears in the segment Su (i.e. in the values of zu?
corresponding for u? ? Su).
Equations 2, 3, and 5 allow us to evaluate the
numerator of the expression in Equation 1. How-
ever, computing the denominator is intractable.
Consequently, we sample from the posterior dis-
tribution P (z, c|w) using Markov chain Monte
Carlo (MCMC) (Gilks et al, 1996). We use Gibbs
sampling, drawing the topic assignment for each
word, zu,i, conditioned on all other topic assign-
ments, z?(u,i), all topic change indicators, c, and
all words, w; and then drawing the topic change
indicator for each utterance, cu, conditioned on all
other topic change indicators, c?u, all topic as-
signments z, and all words w.
The conditional probabilities we need can be
derived directly from Equations 2, 3, and 5. The
conditional probability of zu,i indicates the prob-
ability that wu,i should be assigned to a particu-
lar topic, given other assignments, the current seg-
mentation, and the words in the utterances. Can-
celling constant terms, we obtain:
P (zu,i|z?(u,i), c,w) =
n(t)wu,i + ?
n(t)? + W?
n(Su)zu,i + ?
n(Su)? + T?
. (6)
where all counts (i.e. the n terms) exclude zu,i.
The conditional probability of cu indicates the
probability that a new segment should start at u.
In sampling cu from this distribution, we are split-
ting or merging segments. Similarly we obtain the
expression in (7), where S1u is Su for the segmen-
tation when cu = 1, S0u is Su for the segmentation
when cu = 0, and all counts (e.g. n1) exclude cu.
For this paper, we fixed ?, ? and ? at 0.01.
Our algorithm is related to (Barzilay and Lee,
2004)?s approach to text segmentation, which uses
a hiddenMarkov model (HMM) to model segmen-
tation and topic inference for text using a bigram
representation in restricted domains. Due to the
adaptive combination of different topics our algo-
rithm can be expected to generalize well to larger
domains. It also relates to earlier work by (Blei
and Moreno, 2001) that uses a topic representation
but also does not allow adaptively combining dif-
ferent topics. However, while HMM approaches
allow a segmentation of the data by topic, they
do not allow adaptively combining different topics
into segments: while a new segment can be mod-
elled as being identical to a topic that has already
been observed, it can not be modelled as a com-
bination of the previously observed topics.1 Note
that while (Imai et al, 1997)?s HMM approach al-
lows topic mixtures, it requires supervision with
hand-labelled topics.
In our experiments we therefore compared our
results with those obtained by a similar but simpler
10 state HMM, using a similar Gibbs sampling al-
gorithm. The key difference between the twomod-
els is shown in Figure 1. In the HMM, all variation
in the content of utterances is modelled at a single
level, with each segment having a distribution over
words corresponding to a single state. The hierar-
chical structure of our topic segmentation model
allows variation in content to be expressed at two
levels, with each segment being produced from a
linear combination of the distributions associated
with each topic. Consequently, our model can of-
ten capture the content of a sequence of words by
postulating a single segment with a novel distribu-
tion over topics, while the HMM has to frequently
switch between states.
4 Experiments
4.1 Experiment 0: Simulated data
To analyze the properties of this algorithm we first
applied it to a simulated dataset: a sequence of
10,000 words chosen from a vocabulary of 25.
Each segment of 100 successive words had a con-
1Say that a particular corpus leads us to infer topics corre-
sponding to ?speech recognition? and ?discourse understand-
ing?. A single discussion concerning speech recognition for
discourse understanding could be modelled by our algorithm
as a single segment with a suitable weighted mixture of the
two topics; a HMM approach would tend to split it into mul-
tiple segments (or require a specific topic for this segment).
20
Figure 2: Simulated data: A) inferred topics; B)
segmentation probabilities; C) HMM version.
stant topic distribution (with distributions for dif-
ferent segments drawn from a Dirichlet distribu-
tion with ? = 0.1), and each subsequence of 10
words was taken to be one utterance. The topic-
word assignments were chosen such that when the
vocabulary is aligned in a 5?5 grid the topics were
binary bars. The inference algorithm was then run
for 200,000 iterations, with samples collected after
every 1,000 iterations to minimize autocorrelation.
Figure 2 shows the inferred topic-word distribu-
tions and segment boundaries, which correspond
well with those used to generate the data.
4.2 Experiment 1: The ICSI corpus
We applied the algorithm to the ICSI meeting
corpus transcripts (Janin et al, 2003), consist-
ing of manual transcriptions of 75 meetings. For
evaluation, we use (Galley et al, 2003)?s set of
human-annotated segmentations, which covers a
sub-portion of 25 meetings and takes a relatively
coarse-grained approach to topic with an average
of 5-6 topic segments per meeting. Note that
these segmentations were not used in training the
model: topic inference and segmentation was un-
supervised, with the human annotations used only
to provide some knowledge of the overall segmen-
tation density and to evaluate performance.
The transcripts from all 75 meetings were lin-
earized by utterance start time and merged into a
single dataset that contained 607,263 word tokens.
We sampled for 200,000 iterations of MCMC, tak-
ing samples every 1,000 iterations, and then aver-
aged the sampled cu variables over the last 100
samples to derive an estimate for the posterior
probability of a segmentation boundary at each ut-
terance start. This probability was then thresh-
olded to derive a final segmentation which was
compared to the manual annotations. More pre-
cisely, we apply a small amount of smoothing
(Gaussian kernel convolution) and take the mid-
points of any areas above a set threshold to be the
segment boundaries. Varying this threshold allows
us to segment the discourse in a more or less fine-
grained way (and we anticipate that this could be
user-settable in a meeting browsing application).
If the correct number of segments is known for
a meeting, this can be used directly to determine
the optimum threshold, increasing performance; if
not, we must set it at a level which corresponds to
the desired general level of granularity. For each
set of annotations, we therefore performed two
sets of segmentations: one in which the threshold
was set for each meeting to give the known gold-
standard number of segments, and one in which
the threshold was set on a separate development
set to give the overall corpus-wide average number
of segments, and held constant for all test meet-
ings.2 This also allows us to compare our results
with those of (Galley et al, 2003), who apply a
similar threshold to their lexical cohesion func-
tion and give corresponding results produced with
known/unknown numbers of segments.
Segmentation We assessed segmentation per-
formance using the Pk and WindowDiff (WD) er-
ror measures proposed by (Beeferman et al, 1999)
and (Pevzner and Hearst, 2002) respectively; both
intuitively provide a measure of the probability
that two points drawn from the meeting will be
incorrectly separated by a hypothesized segment
boundary ? thus, lower Pk and WD figures indi-
cate better agreement with the human-annotated
results.3 For the numbers of segments we are deal-
ing with, a baseline of segmenting the discourse
into equal-length segments gives both Pk and WD
about 50%. In order to investigate the effect of the
number of underlying topics T , we tested mod-
els using 2, 5, 10 and 20 topics. We then com-
pared performance with (Galley et al, 2003)?s LC-
Seg tool, and with a 10-state HMM model as de-
scribed above. Results are shown in Table 1, aver-
aged over the 25 test meetings.
Results show that our model significantly out-
performs the HMM equivalent ? because the
HMM cannot combine different topics, it places
a lot of segmentation boundaries, resulting in in-
ferior performance. Using stemming and a bigram
2The development set was formed from the other meet-
ings in the same ICSI subject areas as the annotated test meet-
ings.
3WD takes into account the likely number of incorrectly
separating hypothesized boundaries; Pk only a binary cor-
rect/incorrect classification.
21
Figure 3: Results from the ICSI corpus: A) the words most indicative for each topic; B) Probability of a
segment boundary, compared with human segmentation, for an arbitrary subset of the data; C) Receiver-
operator characteristic (ROC) curves for predicting human segmentation, and conditional probabilities
of placing a boundary at an offset from a human boundary; D) subjective topic coherence ratings.
Number of topics T
Model 2 5 10 20 HMM LCSeg
Pk .284 .297 .329 .290 .375 .319
known unknown
Model Pk WD Pk WD
T = 10 .289 .329 .329 .353
LCSeg .264 .294 .319 .359
Table 1: Results on the ICSI meeting corpus.
representation, however, might improve its perfor-
mance (Barzilay and Lee, 2004), although simi-
lar benefits might equally apply to our model. It
also performs comparably to (Galley et al, 2003)?s
unsupervised performance (exceeding it for some
settings of T ). It does not perform as well as their
hybrid supervised system, which combined LC-
Seg with supervised learning over discourse fea-
tures (Pk = .23); but we expect that a similar ap-
proach would be possible here, combining our seg-
mentation probabilities with other discourse-based
features in a supervised way for improved per-
formance. Interestingly, segmentation quality, at
least at this relatively coarse-grained level, seems
hardly affected by the overall number of topics T .
Figure 3B shows an example for one meeting of
how the inferred topic segmentation probabilities
at each utterance compare with the gold-standard
segment boundaries. Figure 3C illustrates the per-
formance difference between our model and the
HMM equivalent at an example segment bound-
ary: for this example, the HMM model gives al-
most no discrimination.
Identification Figure 3A shows the most indica-
tive words for a subset of the topics inferred at the
last iteration. Encouragingly, most topics seem
intuitively to reflect the subjects we know were
discussed in the ICSI meetings ? the majority of
them (67 meetings) are taken from the weekly
meetings of 3 distinct research groups, where dis-
cussions centered around speech recognition tech-
niques (topics 2, 5), meeting recording, annotation
and hardware setup (topics 6, 3, 1, 8), robust lan-
guage processing (topic 7). Others reflect general
classes of words which are independent of subject
matter (topic 4).
To compare the quality of these inferred topics
we performed an experiment in which 7 human
observers rated (on a scale of 1 to 9) the seman-
tic coherence of 50 lists of 10 words each. Of
these lists, 40 contained the most indicative words
for each of the 10 topics from different models:
the topic segmentation model; a topic model that
had the same number of segments but with fixed
evenly spread segmentation boundaries; an equiv-
22
alent with randomly placed segmentation bound-
aries; and the HMM. The other 10 lists contained
random samples of 10 words from the other 40
lists. Results are shown in Figure 3D, with the
topic segmentation model producing the most co-
herent topics and the HMM model and random
words scoring less well. Interestingly, using an
even distribution of boundaries but allowing the
topic model to infer topics performs similarly well
with even segmentation, but badly with random
segmentation ? topic quality is thus not very sus-
ceptible to the precise segmentation of the text,
but does require some reasonable approximation
(on ICSI data, an even segmentation gives a Pk of
about 50%, while random segmentations can do
much worse). However, note that the full topic
segmentation model is able to identify meaningful
segmentation boundaries at the same time as infer-
ring topics.
4.3 Experiment 2: Dialogue robustness
Meetings often include off-topic dialogue, in par-
ticular at the beginning and end, where infor-
mal chat and meta-dialogue are common. Gal-
ley et al (2003) annotated these sections explic-
itly, together with the ICSI ?digit-task? sections
(participants read sequences of digits to provide
data for speech recognition experiments), and re-
moved them from their data, as did we in Ex-
periment 1 above. While this seems reasonable
for the purposes of investigating ideal algorithm
performance, in real situations we will be faced
with such off-topic dialogue, and would obviously
prefer segmentation performance not to be badly
affected (and ideally, enabling segmentation of
the off-topic sections from the meeting proper).
One might suspect that an unsupervised genera-
tive model such as ours might not be robust in the
presence of numerous off-topic words, as spuri-
ous topics might be inferred and used in the mix-
ture model throughout. In order to investigate this,
we therefore also tested on the full dataset with-
out removing these sections (806,026 word tokens
in total), and added the section boundaries as fur-
ther desired gold-standard segmentation bound-
aries. Table 2 shows the results: performance is
not significantly affected, and again is very simi-
lar for both our model and LCSeg.
4.4 Experiment 3: Speech recognition
The experiments so far have all used manual word
transcriptions. Of course, in real meeting pro-
known unknown
Experiment Model Pk WD Pk WD
2 T = 10 .296 .342 .325 .366
(off-topic data) LCSeg .307 .338 .322 .386
3 T = 10 .266 .306 .291 .331
(ASR data) LCSeg .289 .339 .378 .472
Table 2: Results for Experiments 2 & 3: robust-
ness to off-topic and ASR data.
cessing systems, we will have to deal with speech
recognition (ASR) errors. We therefore also tested
on 1-best ASR output provided by ICSI, and re-
sults are shown in Table 2. The ?off-topic? and
?digits? sections were removed in this test, so re-
sults are comparable with Experiment 1. Segmen-
tation accuracy seems extremely robust; interest-
ingly, LCSeg?s results are less robust (the drop in
performance is higher), especially when the num-
ber of segments in a meeting is unknown.
It is surprising to notice that the segmentation
accuracy in this experiment was actually slightly
higher than achieved in Experiment 1 (especially
given that ASR word error rates were generally
above 20%). This may simply be a smoothing ef-
fect: differences in vocabulary and its distribution
can effectively change the prior towards sparsity
instantiated in the Dirichlet distributions.
5 Summary and Future Work
We have presented an unsupervised generative
model which allows topic segmentation and iden-
tification from unlabelled data. Performance on
the ICSI corpus of multi-party meetings is compa-
rable with the previous unsupervised segmentation
results, and the extracted topics are rated well by
human judges. Segmentation accuracy is robust
in the face of noise, both in the form of off-topic
discussion and speech recognition hypotheses.
Future Work Spoken discourse exhibits several
features not derived from the words themselves
but which seem intuitively useful for segmenta-
tion, e.g. speaker changes, speaker identities and
roles, silences, overlaps, prosody and so on. As
shown by (Galley et al, 2003), some of these fea-
tures can be combined with lexical information to
improve segmentation performance (although in a
supervised manner), and (Maskey and Hirschberg,
2003) show some success in broadcast news seg-
mentation using only these kinds of non-lexical
features. We are currently investigating the addi-
tion of non-lexical features as observed outputs in
23
our unsupervised generative model.
We are also investigating improvements into the
lexical model as presented here, firstly via simple
techniques such as word stemming and replace-
ment of named entities by generic class tokens
(Barzilay and Lee, 2004); but also via the use of
multiple ASR hypotheses by incorporating word
confusion networks into our model. We expect
that this will allow improved segmentation and
identification performance with ASR data.
Acknowledgements
This work was supported by the CALO project
(DARPA grant NBCH-D-03-0010). We thank
Elizabeth Shriberg and Andreas Stolcke for pro-
viding automatic speech recognition data for the
ICSI corpus and for their helpful advice; John
Niekrasz and Alex Gruenstein for help with the
NOMOS corpus annotation tool; and Michel Gal-
ley for discussion of his approach and results.
References
Satanjeev Banerjee and Alex Rudnicky. 2004. Using
simple speech-based features to detect the state of a
meeting and the roles of the meeting participants. In
Proceedings of the 8th International Conference on
Spoken Language Processing.
Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
David Blei and Pedro Moreno. 2001. Topic segmenta-
tion with an aspect hidden Markov model. In Pro-
ceedings of the 24th Annual International Confer-
ence on Research and Development in Information
Retrieval, pages 343?348.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Alfred Dielmann and Steve Renals. 2004. Dynamic
Bayesian Networks for meeting structuring. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 562?569.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter, edi-
tors. 1996. Markov Chain Monte Carlo in Practice.
Chapman and Hall, Suffolk.
Thomas Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Science, 101:5228?5235.
Marti A. Hearst. 1994. Multi-paragraph segmenta-
tion of expository text. In Proc. 32nd Meeting of
the Association for Computational Linguistics, Los
Cruces, NM, June.
Thomas Hofmann. 1999. Probablistic latent semantic
indexing. In Proceedings of the 22nd Annual SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 50?57.
Toru Imai, Richard Schwartz, Francis Kubala, and
Long Nguyen. 1997. Improved topic discrimination
of broadcast news using a model of multiple simul-
taneous topics. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 727?730.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The ICSI Meeting Cor-
pus. In Proceedings of the IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), pages 364?367.
Agnes Lisowska, Andrei Popescu-Belis, and Susan
Armstrong. 2004. User query analysis for the spec-
ification and evaluation of a dialogue processing and
retrieval system. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation.
Sameer R. Maskey and Julia Hirschberg. 2003. Au-
tomatic summarization of broadcast news using
structural features. In Eurospeech 2003, Geneva,
Switzerland.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):19?
36.
Stehpan Reiter and Gerhard Rigoll. 2004. Segmenta-
tion and classification of meeting events using mul-
tiple classifier fusion and dynamic programming. In
Proceedings of the International Conference on Pat-
tern Recognition.
Jeffrey Reynar. 1999. Statistical models for topic seg-
mentation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 357?364.
24
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 673?680,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Contextual Dependencies in Unsupervised Word Segmentation?
Sharon Goldwater and Thomas L. Griffiths and Mark Johnson
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912
{Sharon Goldwater,Tom Griffiths,Mark Johnson}@brown.edu
Abstract
Developing better methods for segment-
ing continuous text into words is impor-
tant for improving the processing of Asian
languages, and may shed light on how hu-
mans learn to segment speech. We pro-
pose two new Bayesian word segmenta-
tion methods that assume unigram and bi-
gram models of word dependencies re-
spectively. The bigram model greatly out-
performs the unigram model (and previous
probabilistic models), demonstrating the
importance of such dependencies for word
segmentation. We also show that previous
probabilistic models rely crucially on sub-
optimal search procedures.
1 Introduction
Word segmentation, i.e., discovering word bound-
aries in continuous text or speech, is of interest for
both practical and theoretical reasons. It is the first
step of processing orthographies without explicit
word boundaries, such as Chinese. It is also one
of the key problems that human language learners
must solve as they are learning language.
Many previous methods for unsupervised word
segmentation are based on the observation that
transitions between units (characters, phonemes,
or syllables) within words are generally more pre-
dictable than transitions across word boundaries.
Statistics that have been proposed for measuring
these differences include ?successor frequency?
(Harris, 1954), ?transitional probabilities? (Saf-
fran et al, 1996), mutual information (Sun et al,
?This work was partially supported by the following
grants: NIH 1R01-MH60922, NIH RO1-DC000314, NSF
IGERT-DGE-9870676, and the DARPA CALO project.
1998), ?accessor variety? (Feng et al, 2004), and
boundary entropy (Cohen and Adams, 2001).
While methods based on local statistics are
quite successful, here we focus on approaches
based on explicit probabilistic models. Formulat-
ing an explicit probabilistic model permits us to
cleanly separate assumptions about the input and
properties of likely segmentations from details of
algorithms used to find such solutions. Specifi-
cally, this paper demonstrates the importance of
contextual dependencies for word segmentation
by comparing two probabilistic models that dif-
fer only in that the first assumes that the proba-
bility of a word is independent of its local context,
while the second incorporates bigram dependen-
cies between adjacent words. The algorithms we
use to search for likely segmentations do differ,
but so long as the segmentations they produce are
close to optimal we can be confident that any dif-
ferences in the segmentations reflect differences in
the probabilistic models, i.e., in the kinds of de-
pendencies between words.
We are not the first to propose explicit prob-
abilistic models of word segmentation. Two
successful word segmentation systems based on
explicit probabilistic models are those of Brent
(1999) and Venkataraman (2001). Brent?s Model-
Based Dynamic Programming (MBDP) system as-
sumes a unigram word distribution. Venkatara-
man uses standard unigram, bigram, and trigram
language models in three versions of his system,
which we refer to as n-gram Segmentation (NGS).
Despite their rather different generative structure,
the MBDP and NGS segmentation accuracies are
very similar. Moreover, the segmentation accuracy
of the NGS unigram, bigram, and trigram mod-
els hardly differ, suggesting that contextual depen-
dencies are irrelevant to word segmentation. How-
673
ever, the segmentations produced by both these
methods depend crucially on properties of the
search procedures they employ. We show this by
exhibiting for each model a segmentation that is
less accurate but more probable under that model.
In this paper, we present an alternative frame-
work for word segmentation based on the Dirich-
let process, a distribution used in nonparametric
Bayesian statistics. This framework allows us to
develop extensible models that are amenable to
standard inference procedures. We present two
such models incorporating unigram and bigram
word dependencies, respectively. We use Gibbs
sampling to sample from the posterior distribution
of possible segmentations under these models.
The plan of the paper is as follows. In the next
section, we describe MBDP and NGS in detail. In
Section 3 we present the unigram version of our
own model, the Gibbs sampling procedure we use
for inference, and experimental results. Section 4
extends that model to incorporate bigram depen-
dencies, and Section 5 concludes the paper.
2 NGS and MBDP
The NGS and MBDP systems are similar in some
ways: both are designed to find utterance bound-
aries in a corpus of phonemically transcribed ut-
terances, with known utterance boundaries. Both
also use approximate online search procedures,
choosing and fixing a segmentation for each utter-
ance before moving onto the next. In this section,
we focus on the very different probabilistic mod-
els underlying the two systems. We show that the
optimal solution under the NGS model is the un-
segmented corpus, and suggest that this problem
stems from the fact that the model assumes a uni-
form prior over hypotheses. We then present the
MBDP model, which uses a non-uniform prior but
is difficult to extend beyond the unigram case.
2.1 NGS
NGS assumes that each utterance is generated in-
dependently via a standard n-gram model. For
simplicity, we will discuss the unigram version of
the model here, although our argument is equally
applicable to the bigram and trigram versions. The
unigram model generates an utterance u according
to the grammar in Figure 1, so
P (u) = p$(1? p$)n?1
n
?
j=1
P (wj) (1)
1? p$ U?W U
p$ U?W
P (w) W?w ?w ? ??
Figure 1: The unigram NGS grammar.
where u consists of the words w1 . . . wn and p$ is
the probability of the utterance boundary marker
$. This model can be used to find the highest prob-
ability segmentation hypothesis h given the data d
by using Bayes? rule:
P (h|d) ? P (d|h)P (h)
NGS assumes a uniform prior P (h) over hypothe-
ses, so its goal is to find the solution that maxi-
mizes the likelihood P (d|h).
Using this model, NGS?s approximate search
technique delivers competitive results. However,
the true maximum likelihood solution is not com-
petitive, since it contains no utterance-internal
word boundaries. To see why not, consider the
solution in which p$ = 1 and each utterance is a
single ?word?, with probability equal to the empir-
ical probability of that utterance. Any other so-
lution will match the empirical distribution of the
data less well. In particular, a solution with ad-
ditional word boundaries must have 1 ? p$ > 0,
which means it wastes probability mass modeling
unseen data (which can now be generated by con-
catenating observed utterances together).
Intuitively, the NGS model considers the unseg-
mented solution to be optimal because it ranks all
hypotheses equally probable a priori. We know,
however, that hypotheses that memorize the input
data are unlikely to generalize to unseen data, and
are therefore poor solutions. To prevent memo-
rization, we could restrict our hypothesis space to
models with fewer parameters than the number of
utterances in the data. A more general and mathe-
matically satisfactory solution is to assume a non-
uniform prior, assigning higher probability to hy-
potheses with fewer parameters. This is in fact the
route taken by Brent in his MBDP model, as we
shall see in the following section.
2.2 MBDP
MBDP assumes a corpus of utterances is gener-
ated as a single probabilistic event with four steps:
1. Generate L, the number of lexical types.
2. Generate a phonemic representation for each
type (except the utterance boundary type, $).
674
3. Generate a token frequency for each type.
4. Generate an ordering for the set of tokens.
In a final deterministic step, the ordered tokens
are concatenated to create an unsegmented cor-
pus. This means that certain segmented corpora
will produce the observed data with probability 1,
and all others will produce it with probability 0.
The posterior probability of a segmentation given
the data is thus proportional to its prior probability
under the generative model, and the best segmen-
tation is that with the highest prior probability.
There are two important points to note about
the MBDP model. First, the distribution over L
assigns higher probability to models with fewer
lexical items. We have argued that this is neces-
sary to avoid memorization, and indeed the unseg-
mented corpus is not the optimal solution under
this model, as we will show in Section 3. Second,
the factorization into four separate steps makes
it theoretically possible to modify each step in-
dependently in order to investigate the effects of
the various modeling assumptions. However, the
mathematical statement of the model and the ap-
proximations necessary for the search procedure
make it unclear how to modify the model in any
interesting way. In particular, the fourth step uses
a uniform distribution, which creates a unigram
constraint that cannot easily be changed. Since our
research aims to investigate the effects of different
modeling assumptions on lexical acquisition, we
develop in the following sections a far more flex-
ible model that also incorporates a preference for
sparse solutions.
3 Unigram Model
3.1 The Dirichlet Process Model
Our goal is a model of language that prefers
sparse solutions, allows independent modification
of components, and is amenable to standard search
procedures. We achieve this goal by basing our
model on the Dirichlet process (DP), a distribution
used in nonparametric Bayesian statistics. Our un-
igram model of word frequencies is defined as
wi|G ? G
G|?0, P0 ? DP(?0, P0)
where the concentration parameter ?0 and the
base distribution P0 are parameters of the model.
Each word wi in the corpus is drawn from a
distribution G, which consists of a set of pos-
sible words (the lexicon) and probabilities asso-
ciated with those words. G is generated from
a DP(?0, P0) distribution, with the items in the
lexicon being sampled from P0 and their proba-
bilities being determined by ?0, which acts like
the parameter of an infinite-dimensional symmet-
ric Dirichlet distribution. We provide some intu-
ition for the roles of ?0 and P0 below.
Although the DP model makes the distribution
G explicit, we never deal with G directly. We
take a Bayesian approach and integrate over all
possible values of G. The conditional probabil-
ity of choosing to generate a word from a particu-
lar lexical entry is then given by a simple stochas-
tic process known as the Chinese restaurant pro-
cess (CRP) (Aldous, 1985). Imagine a restaurant
with an infinite number of tables, each with infinite
seating capacity. Customers enter the restaurant
and seat themselves. Let zi be the table chosen by
the ith customer. Then
P (zi|z?i) =
?
?
?
n(z?i)k
i?1+?0 0 ? k < K(z?i)
?0
i?1+?0 k = K(z?i)
(2)
where z?i = z1 . . . zi?1, n(z?i)k is the number of
customers already sitting at table k, and K(z?i) is
the total number of occupied tables. In our model,
the tables correspond to (possibly repeated) lexical
entries, having labels generated from the distribu-
tion P0. The seating arrangement thus specifies
a distribution over word tokens, with each cus-
tomer representing one token. This model is an
instance of the two-stage modeling framework de-
scribed by Goldwater et al (2006), with P0 as the
generator and the CRP as the adaptor.
Our model can be viewed intuitively as a cache
model: each word in the corpus is either retrieved
from a cache or generated anew. Summing over
all the tables labeled with the same word yields
the probability distribution for the ith word given
previously observed words w?i:
P (wi|w?i) =
n(w?i)wi
i? 1 + ?0
+ ?0P0(wi)i? 1 + ?0
(3)
where n(w?i)w is the number of instances of w ob-
served in w?i. The first term is the probability
of generating w from the cache (i.e., sitting at an
occupied table), and the second term is the proba-
bility of generating it anew (sitting at an unoccu-
pied table). The actual table assignments z?i only
become important later, in the bigram model.
675
There are several important points to note about
this model. First, the probability of generating a
particular word from the cache increases as more
instances of that word are observed. This rich-
get-richer process creates a power-law distribution
on word frequencies (Goldwater et al, 2006), the
same sort of distribution found empirically in nat-
ural language. Second, the parameter ?0 can be
used to control how sparse the solutions found by
the model are. This parameter determines the total
probability of generating any novel word, a proba-
bility that decreases as more data is observed, but
never disappears. Finally, the parameter P0 can
be used to encode expectations about the nature
of the lexicon, since it defines a probability distri-
bution across different novel words. The fact that
this distribution is defined separately from the dis-
tribution on word frequencies gives the model ad-
ditional flexibility, since either distribution can be
modified independently of the other.
Since the goal of this paper is to investigate the
role of context in word segmentation, we chose
the simplest possible model for P0, i.e. a unigram
phoneme distribution:
P0(w) = p#(1? p#)n?1
n
?
i=1
P (mi) (4)
where word w consists of the phonemes
m1 . . . mn, and p# is the probability of the
word boundary #. For simplicity we used
a uniform distribution over phonemes, and
experimented with different fixed values of p#.1
A final detail of our model is the distribution
on utterance lengths, which is geometric. That is,
we assume a grammar similar to the one shown in
Figure 1, with the addition of a symmetric Beta( ?2 )
prior over the probability of the U productions,2
and the substitution of the DP for the standard
multinomial distribution over the W productions.
3.2 Gibbs Sampling
Having defined our generative model, we are left
with the problem of inference: we must determine
the posterior distribution of hypotheses given our
input corpus. To do so, we use Gibbs sampling,
a standard Markov chain Monte Carlo method
(Gilks et al, 1996). Gibbs sampling is an itera-
tive procedure in which variables are repeatedly
1Note, however, that our model could be extended to learn
both p# and the distribution over phonemes.
2The Beta distribution is a Dirichlet distribution over two
outcomes.
W
U
w1 = w2.w3
UW
U
W
w3
w2
h1: h2:
Figure 2: The two hypotheses considered by the
unigram sampler. Dashed lines indicate possible
additional structure. All rules except those in bold
are part of h?.
sampled from their conditional posterior distribu-
tion given the current values of all other variables
in the model. The sampler defines a Markov chain
whose stationary distribution is P (h|d), so after
convergence samples are from this distribution.
Our Gibbs sampler considers a single possible
boundary point at a time, so each sample is from
a set of two hypotheses, h1 and h2. These hy-
potheses contain all the same boundaries except
at the one position under consideration, where h2
has a boundary and h1 does not. The structures are
shown in Figure 2. In order to sample a hypothe-
sis, we need only calculate the relative probabili-
ties of h1 and h2. Since h1 and h2 are the same ex-
cept for a few rules, this is straightforward. Let h?
be all of the structure shared by the two hypothe-
ses, including n? words, and let d be the observed
data. Then
P (h1|h?, d) = P (w1|h?, d)
= n
(h?)
w1 + ?0P0(w1)
n? + ?0
(5)
where the second line follows from Equation 3
and the properties of the CRP (in particular, that it
is exchangeable, with the probability of a seating
configuration not depending on the order in which
customers arrive (Aldous, 1985)). Also,
P (h2|h?, d)
= P (r, w2, w3|h?, d)
= P (r|h?, d)P (w2|h?, d)P (w3|w2, h?, d)
= nr +
?
2
n? + 1 + ? ?
n(h
?)
w2 + ?0P0(w2)
n? + ?0
?n
(h?)
w3 + I(w2 = w3) + ?0P0(w3)
n? + 1 + ?0
(6)
where nr is the number of branching rules r =
U ? W U in h?, and I(.) is an indicator func-
tion taking on the value 1 when its argument is
676
true, and 0 otherwise. The nr term is derived by
integrating over all possible values of p$, and not-
ing that the total number of U productions in h?
is n? + 1.
Using these equations we can simply proceed
through the data, sampling each potential bound-
ary point in turn. Once the Gibbs sampler con-
verges, these samples will be drawn from the pos-
terior distribution P (h|d).
3.3 Experiments
In our experiments, we used the same corpus
that NGS and MBDP were tested on. The cor-
pus, supplied to us by Brent, consists of 9790
transcribed utterances (33399 words) of child-
directed speech from the Bernstein-Ratner cor-
pus (Bernstein-Ratner, 1987) in the CHILDES
database (MacWhinney and Snow, 1985). The ut-
terances have been converted to a phonemic rep-
resentation using a phonemic dictionary, so that
each occurrence of a word has the same phonemic
transcription. Utterance boundaries are given in
the input to the system; other word boundaries are
not.
Because our Gibbs sampler is slow to converge,
we used annealing to speed inference. We began
with a temperature of ? = 10 and decreased ? in
10 increments to a final value of 1. A temperature
of ? corresponds to raising the probabilities of h1
and h2 to the power of 1? prior to sampling.
We ran our Gibbs sampler for 20,000 iterations
through the corpus (with ? = 1 for the final 2000)
and evaluated our results on a single sample at
that point. We calculated precision (P), recall (R),
and F-score (F) on the word tokens in the corpus,
where both boundaries of a word must be correct
to count the word as correct. The induced lexicon
was also scored for accuracy using these metrics
(LP, LR, LF).
Recall that our DP model has three parameters:
?, p#, and ?0. Given the large number of known
utterance boundaries, we expect the value of ? to
have little effect on our results, so we simply fixed
? = 2 for all experiments. Figure 3 shows the ef-
fects of varying of p# and ?0.3 Lower values of
p# cause longer words, which tends to improve re-
call (and thus F-score) in the lexicon, but decrease
token accuracy. Higher values of ?0 allow more
novel words, which also improves lexicon recall,
3It is worth noting that all these parameters could be in-
ferred. We leave this for future work.
0.1 0.3 0.5 0.7 0.9
50
55
60
(a) Varying P(#)
 
 
1 2 5 10 20 50 100 200 500
50
55
60
(b) Varying ?0
 
 
LF
F
LF
F
Figure 3: Word (F) and lexicon (LF) F-score (a)
as a function of p#, with ?0 = 20 and (b) as a
function of ?0, with p# = .5.
but begins to degrade precision after a point. Due
to the negative correlation between token accuracy
and lexicon accuracy, there is no single best value
for either p# or ?0; further discussion refers to the
solution for p# = .5, ?0 = 20 (though others are
qualitatively similar).
In Table 1(a), we compare the results of our sys-
tem to those of MBDP and NGS.4 Although our
system has higher lexicon accuracy than the oth-
ers, its token accuracy is much worse. This result
occurs because our system often mis-analyzes fre-
quently occurring words. In particular, many of
these words occur in common collocations such
as what?s that and do you, which the system inter-
prets as a single words. It turns out that a full 31%
of the proposed lexicon and nearly 30% of tokens
consist of these kinds of errors.
Upon reflection, it is not surprising that a uni-
gram language model would segment words in this
way. Collocations violate the unigram assumption
in the model, since they exhibit strong word-to-
word dependencies. The only way the model can
capture these dependencies is by assuming that
these collocations are in fact words themselves.
Why don?t the MBDP and NGS unigram mod-
els exhibit these problems? We have already
shown that NGS?s results are due to its search pro-
cedure rather than its model. The same turns out
to be true for MBDP. Table 2 shows the probabili-
4We used the implementations of MBDP and NGS avail-
able at http://www.speech.sri.com/people/anand/ to obtain re-
sults for those systems.
677
(a) P R F LP LR LF
NGS 67.7 70.2 68.9 52.9 51.3 52.0
MBDP 67.0 69.4 68.2 53.6 51.3 52.4
DP 61.9 47.6 53.8 57.0 57.5 57.2
(b) P R F LP LR LF
NGS 76.6 85.8 81.0 60.0 52.4 55.9
MBDP 77.0 86.1 81.3 60.8 53.0 56.6
DP 94.2 97.1 95.6 86.5 62.2 72.4
Table 1: Accuracy of the various systems, with
best scores in bold. The unigram version of NGS
is shown. DP results are with p# = .5 and ?0 =
20. (a) Results on the true corpus. (b) Results on
the permuted corpus.
Seg: True None MBDP NGS DP
NGS 204.5 90.9 210.7 210.8 183.0
MBDP 208.2 321.7 217.0 218.0 189.8
DP 222.4 393.6 231.2 231.6 200.6
Table 2: Negative log probabilities (x 1000) un-
der each model of the true solution, the solution
with no utterance-internal boundaries, and the so-
lutions found by each algorithm. Best solutions
under each model are bold.
ties under each model of various segmentations of
the corpus. From these figures, we can see that
the MBDP model assigns higher probability to the
solution found by our Gibbs sampler than to the
solution found by Brent?s own incremental search
algorithm. In other words, Brent?s model does pre-
fer the lower-accuracy collocation solution, but his
search algorithm instead finds a higher-accuracy
but lower-probability solution.
We performed two experiments suggesting that
our own inference procedure does not suffer from
similar problems. First, we initialized our Gibbs
sampler in three different ways: with no utterance-
internal boundaries, with a boundary after every
character, and with random boundaries. Our re-
sults were virtually the same regardless of initial-
ization. Second, we created an artificial corpus by
randomly permuting the words in the true corpus,
leaving the utterance lengths the same. The ar-
tificial corpus adheres to the unigram assumption
of our model, so if our inference procedure works
correctly, we should be able to correctly identify
the words in the permuted corpus. This is exactly
what we found, as shown in Table 1(b). While all
three models perform better on the artificial cor-
pus, the improvements of the DP model are by far
the most striking.
4 Bigram Model
4.1 The Hierarchical Dirichlet Process Model
The results of our unigram experiments suggested
that word segmentation could be improved by
taking into account dependencies between words.
To test this hypothesis, we extended our model
to incorporate bigram dependencies using a hi-
erarchical Dirichlet process (HDP) (Teh et al,
2005). Our approach is similar to previous n-gram
models using hierarchical Pitman-Yor processes
(Goldwater et al, 2006; Teh, 2006). The HDP is
appropriate for situations in which there are multi-
ple distributions over similar sets of outcomes, and
the distributions are believed to be similar. In our
case, we define a bigram model by assuming each
word has a different distribution over the words
that follow it, but all these distributions are linked.
The definition of our bigram language model as an
HDP is
wi|wi?1 = w,Hw ? Hw ?w
Hw|?1, G ? DP(?1, G) ?w
G|?0, P0 ? DP(?0, P0)
That is, P (wi|wi?1 = w) is distributed accord-
ing to Hw, a DP specific to word w. Hw is linked
to the DPs for all other words by the fact that they
share a common base distribution G, which is gen-
erated from another DP.5
As in the unigram model, we never deal with
Hw or G directly. By integrating over them, we get
a distribution over bigram frequencies that can be
understood in terms of the CRP. Now, each word
type w is associated with its own restaurant, which
represents the distribution over words that follow
w. Different restaurants are not completely inde-
pendent, however: the labels on the tables in the
restaurants are all chosen from a common base
distribution, which is another CRP.
To understand the HDP model in terms of a
grammar, we consider $ as a special word type,
so that wi ranges over ?? ? {$}. After observing
w?i, the HDP grammar is as shown in Figure 4,
5This HDP formulation is an oversimplification, since it
does not account for utterance boundaries properly. The
grammar formulation (see below) does.
678
P2(wi|w?i, z?i) Uwi?1?Wwi Uwi ?wi ? ??,
wi?1 ? ???{$}
P2($|w?i, z?i) Uwi?1?$ ?wi?1 ? ??
1 Wwi ?wi ?wi ? ??
Figure 4: The HDP grammar after observing w?i.
with
P2(wi|h?i) =
n(wi?1,wi) + ?1P1(wi|h?i)
nwi?1 + ?1
(7)
P1(wi|h?i) =
?
?
?
t??+ ?2
t+? ?
twi+?0P0(wi)
t??+?0 wi ? ?
?
t$+ ?2
t+? wi = $
where h?i = (w?i, z?i); t$, t?? , and twi are the
total number of tables (across all words) labeled
with $, non-$, and wi, respectively; t = t$ + t??
is the total number of tables; and n(wi?1,wi) is the
number of occurrences of the bigram (wi?1, wi).
We have suppressed the superscript (w?i) nota-
tion in all cases. The base distribution shared by
all bigrams is given by P1, which can be viewed as
a unigram backoff where the unigram probabilities
are learned from the bigram table labels.
We can perform inference on this HDP bigram
model using a Gibbs sampler similar to our uni-
gram sampler. Details appear in the Appendix.
4.2 Experiments
We used the same basic setup for our experiments
with the HDP model as we used for the DP model.
We experimented with different values of ?0 and
?1, keeping p# = .5 throughout. Some results
of these experiments are plotted in Figure 5. With
appropriate parameter settings, both lexicon and
token accuracy are higher than in the unigram
model (dramatically so, for tokens), and there is
no longer a negative correlation between the two.
Only a few collocations remain in the lexicon, and
most lexicon errors are on low-frequency words.
The best values of ?0 are much larger than in the
unigram model, presumably because all unique
word types must be generated via P0, but in the
bigram model there is an additional level of dis-
counting (the unigram process) before reaching
P0. Smaller values of ?0 lead to fewer word types
with fewer characters on average.
Table 3 compares the optimal results of the
HDP model to the only previous model incorpo-
rating bigram dependencies, NGS. Due to search,
the performance of the bigram NGS model is not
much different from that of the unigram model. In
100 200 500 1000 2000
40
60
80
(a) Varying ?0
 
 
F
LF
5 10 20 50 100 200 500
40
60
80
(b) Varying ?1
 
 
F
LF
Figure 5: Word (F) and lexicon (LF) F-score (a)
as a function of ?0, with ?1 = 10 and (b) as a
function of ?1, with ?0 = 1000.
P R F LP LR LF
NGS 68.1 68.6 68.3 54.5 57.0 55.7
HDP 79.4 74.0 76.6 67.9 58.9 63.1
Table 3: Bigram system accuracy, with best scores
in bold. HDP results are with p# = .5, ?0 =
1000, and ?1 = 10.
contrast, our HDP model performs far better than
our DP model, leading to the highest published ac-
curacy for this corpus on both tokens and lexical
items. Overall, these results strongly support our
hypothesis that modeling bigram dependencies is
important for accurate word segmentation.
5 Conclusion
In this paper, we have introduced a new model-
based approach to word segmentation that draws
on techniques from Bayesian statistics, and we
have developed models incorporating unigram and
bigram dependencies. The use of the Dirichlet
process as the basis of our approach yields sparse
solutions and allows us the flexibility to modify
individual components of the models. We have
presented a method of inference using Gibbs sam-
pling, which is guaranteed to converge to the pos-
terior distribution over possible segmentations of
a corpus.
Our approach to word segmentation allows us to
investigate questions that could not be addressed
satisfactorily in earlier work. We have shown that
the search algorithms used with previous models
of word segmentation do not achieve their ob-
679
P (h1|h?, d) =
n(wl,w1) + ?1P1(w1|h?, d)
nwl + ?1
?
n(w1,wr) + I(wl =w1 =wr) + ?1P1(wr|h?, d)
nw1 + 1 + ?1
P (h2|h?, d) =
n(wl,w2) + ?1P1(w2|h?, d)
nwl + ?1
?
n(w2,w3) + I(wl =w2 =w3) + ?1P1(w3|h?, d)
nw2 + 1 + ?1
?
n(w3,wr) + I(wl =w3, w2 =wr) + I(w2 =w3 =wr) + ?1P1(wr|h?, d)
nw3 + 1 + I(w2 =w4) + ?1
Figure 6: Gibbs sampling equations for the bigram model. All counts are with respect to h?.
jectives, which has led to misleading results. In
particular, previous work suggested that the use
of word-to-word dependencies has little effect on
word segmentation. Our experiments indicate in-
stead that bigram dependencies can be crucial for
avoiding under-segmentation of frequent colloca-
tions. Incorporating these dependencies into our
model greatly improved segmentation accuracy,
and led to better performance than previous ap-
proaches on all measures.
References
D. Aldous. 1985. Exchangeability and related topics. In
?Ecole d?e?te? de probabilite?s de Saint-Flour, XIII?1983,
pages 1?198. Springer, Berlin.
C. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametric problems. The An-
nals of Statistics, 2:1152?1174.
N. Bernstein-Ratner. 1987. The phonology of parent-child
speech. In K. Nelson and A. van Kleeck, editors, Chil-
dren?s Language, volume 6. Erlbaum, Hillsdale, NJ.
M. Brent. 1999. An efficient, probabilistically sound al-
gorithm for segmentation and word discovery. Machine
Learning, 34:71?105.
P. Cohen and N. Adams. 2001. An algorithm for segment-
ing categorical timeseries into meaningful episodes. In
Proceedings of the Fourth Symposium on Intelligent Data
Analysis.
H. Feng, K. Chen, X. Deng, and W. Zheng. 2004. Acces-
sor variety criteria for chinese word extraction. Computa-
tional Lingustics, 30(1).
W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors.
1996. Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Interpo-
lating between types and tokens by estimating power-law
generators. In Advances in Neural Information Process-
ing Systems 18, Cambridge, MA. MIT Press.
Z. Harris. 1954. Distributional structure. Word, 10:146?162.
B. MacWhinney and C. Snow. 1985. The child language data
exchange system. Journal of Child Language, 12:271?
296.
J. Saffran, E. Newport, and R. Aslin. 1996. Word segmenta-
tion: The role of distributional cues. Journal of Memory
and Language, 35:606?621.
M. Sun, D. Shen, and B. Tsou. 1998. Chinese word seg-
mentation without using lexicon and hand-crafted training
data. In Proceedings of COLING-ACL.
Y. Teh, M. Jordan, M. Beal, and D. Blei. 2005. Hierarchical
Dirichlet processes. In Advances in Neural Information
Processing Systems 17. MIT Press, Cambridge, MA.
Y. Teh. 2006. A Bayesian interpretation of interpolated
kneser-ney. Technical Report TRA2/06, National Univer-
sity of Singapore, School of Computing.
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):351?372.
Appendix
To sample from the posterior distribution over seg-
mentations in the bigram model, we define h1 and
h2 as we did in the unigram sampler so that for the
corpus substring s, h1 has a single word (s = w1)
where h2 has two (s = w2.w3). Let wl and wr be
the words (or $) preceding and following s. Then
the posterior probabilities of h1 and h2 are given
in Figure 6. P1(.) can be calculated exactly using
the equation in Section 4.1, but this requires ex-
plicitly tracking and sampling the assignment of
words to tables. For easier and more efficient im-
plementation, we use an approximation, replacing
each table count twi by its expected value E[twi ].
In a DP(?,P ), the expected number of CRP tables
for an item occurring n times is ? log n+?? (Anto-
niak, 1974), so
E[twi ] = ?1
?
j
log
n(wj ,wi) + ?1
?1
This approximation requires only the bigram
counts, which we must track anyway.
680
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 887?896, Prague, June 2007. c?2007 Association for Computational Linguistics
A Probabilistic Approach to Diachronic Phonology
Alexandre Bouchard-Co?te?? Percy Liang? Thomas L. Griffiths? Dan Klein?
?Computer Science Division ?Department of Psychology
University of California at Berkeley
Berkeley, CA 94720
Abstract
We present a probabilistic model of di-
achronic phonology in which individual
word forms undergo stochastic edits along
the branches of a phylogenetic tree. Our ap-
proach allows us to achieve three goals with
a single unified model: (1) reconstruction
of both ancient and modern word forms, (2)
discovery of general phonological changes,
and (3) selection among different phyloge-
nies. We learn our model using a Monte
Carlo EM algorithm and present quantitative
results validating the model.
1 Introduction
Modeling how languages change phonologically
over time (diachronic phonology) is a central topic
in historical linguistics (Campbell, 1998). The ques-
tions involved range from reconstruction of ancient
word forms, to the elucidation of phonological drift
processes, to the determination of phylogenetic re-
lationships between languages. However, this prob-
lem has received relatively little attention from the
computational community. What work there is has
focused on the reconstruction of phylogenies on the
basis of a Boolean matrix indicating the properties
of words in different languages (Gray and Atkinson,
2003; Evans et al, 2004; Ringe et al, 2002; Nakhleh
et al, 2005).
In this paper, we present a novel framework, along
with a concrete model and experiments, for the prob-
abilistic modeling of diachronic phonology. We fo-
cus on the case where the words are etymological
cognates across languages, e.g. French faire and
Spanish hacer from Latin facere (to do). Given
this information as input, we learn a model acting
at the level of individual phoneme sequences, which
can be used for reconstruction and prediction, Our
model is fully generative, and can be used to reason
about a variety of types of information. For exam-
ple, we can observe a word in one or more modern
languages, say French and Spanish, and query the
corresponding word form in another language, say
Italian. This kind of lexicon-filling has applications
in machine translation. Alternatively, we can also
reconstruct ancestral word forms or inspect the rules
learned along each branch of a phylogeny to identify
salient patterns. Finally, the model can be used as a
building block in a system for inferring the topology
of phylogenetic trees. We discuss all of these cases
further in Section 4.
The contributions of this paper are threefold.
First, the approach to modeling language change at
the phoneme sequence level is new, as is the spe-
cific model we present. Second, we compiled a new
corpus1 and developed a methodology for quantita-
tively evaluating such approaches. Finally, we de-
scribe an efficient inference algorithm for our model
and empirically study its performance.
1.1 Previous work
While our word-level model of phonological change
is new, there have been several computational inves-
tigations into diachronic linguistics which are rele-
vant to the present work.
The task of reconstructing phylogenetic trees
1nlp.cs.berkeley.edu/pages/historical.html
887
for languages has been studied by several authors.
These approaches descend from glottochronology
(Swadesh, 1955), which views a language as a col-
lection of shared cognates but ignores the structure
of those cognates. This information is obtained from
manually curated cognate lists such as the data of
Dyen et al (1997).
As an example of a cognate set encoding, consider
the meaning ?eat?. There would be one column for
the cognate set which appears in French as manger
and Italian as mangiare since both descend from the
Latin mandere (to chew). There would be another
column for the cognate set which appears in both
Spanish and Portuguese as comer, descending from
the Latin comedere (to consume). If this were the
only data, algorithms based on this data would tend
to conclude that French and Italian were closely re-
lated and that Spanish and Portuguese were equally
related. However, the cognate set representation has
several disadvantages: it does not capture the fact
that the cognate is closer between Spanish and Por-
tuguese than between French and Spanish, nor do
the resulting models let us conclude anything about
the regular processes which caused these languages
to diverge. Also, the existing cognate data has been
curated at a relatively high cost. In our work, we
track each word using an automatically obtained
cognate list. While our cognates may be noisier,
we compensate by modeling phonological changes
rather than boolean mutations in cognate sets.
There has been other computational work in this
broad domain. Venkataraman et al (1997) describe
an information theoretic measure of the distance be-
tween two dialects of Chinese. Like our approach,
they use a probabilistic edit model as a formaliza-
tion of the phonological process. However, they do
not consider the question of reconstruction or infer-
ence in multi-node phylogenies, nor do they present
a learning algorithm for such models.
Finally, for the specific application of cog-
nate prediction in machine translation, essentially
transliteration, there have been several approaches,
including Kondrak (2002). However, the phenom-
ena of interest, and therefore the models, are ex-
tremely different. Kondrak (2002) presents a model
for learning ?sound laws,? general phonological
changes governing two completely observed aligned
cognate lists. His model can be viewed as a special
la
es it
la
vl
ib
es pt
it
la
it pt
es
la
it es
pt
Topology 1 Topology 2 *Topology 3 *Topology 4
Figure 1: Tree topologies used in our experiments. *Topology
3 and *Topology 4 are incorrect evolutionary tree used for our
experiments on the selection of phylogenies (Section 4.4).
case of ours using a simple two-node topology.
There is also a rich literature (Huelsenbeck et al,
2001) on the related problems of evolutionary biol-
ogy. A good reference on the subject is Felsenstein
(2003). In particular, Yang and Rannala (1997), Mau
and Newton (1997) and Li et al (2000) each inde-
pendently presented a Bayesian model for comput-
ing posteriors over evolutionary trees. A key dif-
ference with our model is that independence across
evolutionary sites is assumed in their work, while
the evolution of the phonemes in our model depends
on the environment in which the change occurs.
2 A model of phonological change
Assume we have a fixed set of word types (cog-
nate sets) in our vocabulary V and a set of languages
L. Each word type i has a word form wil in each lan-
guage l ? L, which is represented as a sequence of
phonemes and might or might not be observed. The
languages are arranged according to some tree topol-
ogy T (see Figure 1 for examples). One might con-
sider models that simultaneously induce the topol-
ogy and cognate set assignments, but let us fix both
for now. We discuss one way to relax this assump-
tion and present experimental results in Section 4.4.
Our generative model (Figure 3) specifies a dis-
tribution over the word forms {wil} for each word
type i ? V and each language l ? L. The genera-
tive process starts at the root language and generates
all the word forms in each language in a top-down
manner. One appealing aspect about our model is
that, at a high-level, it reflects the actual phonolog-
ical process that languages undergo. However, im-
portant phenomena like lexical drift, borrowing, and
other non-phonological changes are not modeled.
888
Our generative model can be summarized as fol-
lows:
For each word i ? V :
?wiROOT ? LanguageModel
For each branch (k ? l) ? T :
??k?l ? Dirichlet(?) [choose edit params.]
?For each word i ? V :
??wil ? Edit(wik, ?k?l) [sample word form]
In the remainder of this section, we describe each
of the steps in the model.
2.1 Language model
For the distributionw ? LanguageModel, we used a
simple bigram phoneme model. The phonemes were
partitioned into natural classes (see Section 4 for de-
tails). A root word form consisting of n phonemes
x1 ? ? ?xn is generated with probability
plm(x1)
n?
j=2
plm(xj | NaturalClass(xj?1)),
where plm is the distribution of the language model.
2.2 Edit model
The stochastic edit model y ? Edit(x, ?) describes
how a single old word form x = x1 ? ? ?xn changes
along one branch of the phylogeny with parameters
? to produce a new word form y. This process is
parameterized by rule probabilities ?k?l, which are
specific to branch (k ? l).
The generative process is as follows: for each
phoneme xi in the old word form, walking from
left to right, choose a rule to apply. There are
three types of rules: (1) deletion of the phoneme,
(2) substitution with another phoneme (possibly the
same one), or (3) insertion of another phoneme, ei-
ther before or after the existing one. The prob-
ability of applying a rule depends on a context
(NaturalClass(xi?1),NaturalClass(xi+1)). Figure 2
illustrates the edits on an example. The context-
dependence allows us to represent phenomena such
as the fact that s is likely to be deleted only in word-
final contexts.
The edit model we have presented approximately
encodes a limited form of classic rewrite-driven seg-
mental phonology (Chomsky and Halle, 1968). One
# C V C V C #
# f o k u s #
# f w O k o #
# C V V C V #
f ? f / # Vo ? w O / C Ck ? k / V Vu ? o / C Cs ? / V #
Edits applied Rules used
Figure 2: An example of edits that were used to transform
the Latin word FOCUS (/fokus/) into the Italian word fuoco
(/fwOko/) (fire) along with the context-specific rules that were
applied.
could imagine basing our model on more modern
phonological theory, but the computational proper-
ties of the edit model are compelling, and it is ade-
quate for many kinds of phonological change.
In addition to simple edits, we can model some
classical changes that appear to be too complex to be
captured by a single left-to-right edit model of this
kind. For instance, bleeding and feeding arrange-
ments occur when one phonological change intro-
duces a new context, which triggers another phono-
logical change, but the two cannot occur simultane-
ously. For example, vowel raising e ? i / c might
be needed before palatalization t ? c / i. Instead
of capturing such an interaction directly, we can
break up a branch into two segments joined at an in-
termediate language node, conflating the concept of
historically intermediate languages with the concept
of intermediate stages in the application of sequen-
tial rules.
However, many complex processes are not well-
represented by our basic model. One problem-
atic case is chained shifts such as Grimm?s law in
Proto-Germanic or the Great Vowel Shift in English.
To model such dependent rules, we would need
to use a more complex prior distributions over the
edit parameters. Another difficult case is prosodic
changes, such as unstressed vowel neutralizations,
which would require a representation of supraseg-
mental features. While our basic model does not
account for these phenomena, extensions within the
generative framework could capture such richness.
3 Learning and inference
We use a Monte Carlo EM algorithm to fit the pa-
rameters of our model. The algorithm iterates be-
tween a stochastic E-step, which computes recon-
889
...
wiA
wiB
wiC wiD
... ...word type i = 1 . . . |V |
eiA?B?A?B
eiB?C?B?C eiB?D ?B?D
Figure 3: The graphical model representation of our model: ?
are the parameters specifying the stochastic edits e, which gov-
ern how the words w evolve. The plate notation indicates the
replication of the nodes corresponding to the evolving words.
structions based on the current edit parameters, and
an M-step, which updates the edit parameters based
on the reconstructions.
3.1 Monte Carlo E-step: sampling the edits
The E-step needs to produce expected counts of how
many times each edit (such as o ? O) was used in
each context. An exact E-step would require sum-
ming over all possible edits involving all languages
in the phylogeny (all unobserved {e}, {w} variables
in Figure 3). Unfortunately, unlike in the case of
HMMs and PCFGs, our model permits no tractable
dynamic program to compute these counts exactly.
Therefore, we resort to a Monte Carlo E-step,
where many samples of the edit variables are col-
lected, and counts are computed based on these sam-
ples. Samples are drawn using Gibbs sampling (Ge-
man and Geman, 1984): for each word form of a
particular language wil, we fix all other variables in
the model and sample wil along with its correspond-
ing edits.
In the E-step, we fix the parameters, which ren-
ders the word types conditionally independent, just
as in an HMM. Therefore, we can process each word
type in turn without approximation.
First consider the simple 4-language topology in
Figure 3. Suppose that the words in languages A,
C and D are fixed, and we wish to infer the word
at language B along with the three corresponding
sets of edits (remember the edits fully determine the
words). There are an exponential number of possi-
ble words/edits, but it turns out that we can exploit
theMarkov structure in the edit model to consider all
such words/edits using dynamic programming, in a
way broadly similar to the forward-backward algo-
rithm for HMMs.
Figure 4 shows the lattice for the dynamic pro-
gram. Each path connecting the two shaded end-
point states represents a particular word form for
language B and a corresponding set of edits. Each
node in the lattice is a state of the dynamic pro-
gram, which is a 5-tuple (iA, iC , iD, c1, c2), where
iA, iC and iD are the cursor positions (represented
by dots in Figure 4) in each of the word forms of
A,C and D, respectively; c1 is the natural class of
the phoneme in the word form for B that was last
generated; and c2 corresponds to the phoneme that
will be generated next.
Each state transition involves applying a rule
to A?s current phoneme (which produces 0?2
phonemes in B) and applying rules to B?s new 0?2
phonemes. There are three types of rules (deletion,
substitution, insertion), resulting in 30+32+34 = 91
types of state transitions. For illustration, Figure 4
shows the simpler case where B only has one child
C. Given these rules, the new state is computed by
advancing the appropriate cursors and updating the
natural classes c1 and c2. The weight of each tran-
sition w(s ? t) is a product of the language model
probability and the rule probabilities that were cho-
sen.
For each state s, the dynamic program computes
W (s), the sum of the weights of all paths leaving s,
W (s) =
?
s?t
w(s ? t)W (t).
To sample a path, we start at the leftmost state,
choose the transition with probability proportional
to its contribution in the sum for computing W (s),
and repeat until we reach the rightmost state.
We applied a few approximations to speed up the
sampling of words, which reduced the running time
by several orders of magnitude. For example, we
pruned rules with low probability and restricted the
890
An example of a dynamic programming lattice
...
...
... ... ... ... ... ... ...
...
patr ? ia
# C V C C# p a t r ? V #a #
patr ? ja
x [T1] p1ed(i ? /C V) x
x [T3] plm(j | C) p1ed(i ? j/C V) p2ed(j ? j/C V) x
x [T11] plm(j | C) plm(i | C) p1ed(i ? j i/C V) p2ed(j ? j/C V) p2ed(i ? /C V) x
. . .
patri ? a
# C V C C# p a t r ? V #a #
patr ? ja
patri ? a
# C V C C C# p a t r j ? V #a #
patrj ? a
patri ? a
# C V C C C V# p a t r j i ? V #a #
patrj ? a
. . .
Types of state transitions (x: ancient phoneme, y: intermediate, z: modern)
x
y
x
y
z
x
y
z
x
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z z
y
z
x
y
z z
y
z
x
y
z z
y
z z[T1] [T2] [T3] [T4] [T5] [T6] [T7] [T8] [T9] [T10] [T11] [T12] [T13]
Figure 4: The dynamic program involved in sampling an intermediate word form given one ancient and one modern word form.
One lattice node is expanded to show the dynamic program state (represented by the part not grayed out) and three of the many
possible transitions leaving the state. Each transition is labeled with the weight of the transition, which is the product of the relevant
model probabilities. At the bottom, the 13 types of state transitions are shown.
state space of the dynamic program by limiting the
deviation in cursor positions.
3.2 M-step: updating the parameters
The M-step is standard once we have computed
the expected counts of edits in the E-step. For
each branch (k ? l) ? T in the phylogeny,
we compute the maximum likelihood estimate
of the edit parameters {?k?l(x ? ? / c1 c2)}.
For example, the parameter corresponding to
x = /e/, ? = /e s/, c1 = ALVEOLAR, c2 = # is
the probability of inserting a final /s/ after an /e/
which is itself preceded by an alveolar phoneme.
The probability of each rule is estimated as follows:
?k?l(x ? ? / c1 c2) =
#(x ? ? / c1 c2) + ?(x ? ? / c1 c2)? 1?
?? #(x ? ?? / c1 c2) + ?(x ? ?? / c1 c2)? 1
,
where ? is the concentration hyperparameter of the
Dirichlet prior. The value ? ? 1 can be interpreted
as the number of pseudocounts for a rule.
4 Experiments
In this section we show the results of our experi-
ments with our model. The experimental conditions
are summarized in Table 1, with additional informa-
Experiment Topology Heldout
Latin reconstruction (4.2) 1 la:293
Italian reconstruction (4.2) 1 it:117
Sound changes (4.3) 2 None
Phylogeny selection (4.4) 2, 3, 4 None
Table 1: Conditions under which each of the experiments pre-
sented in this section were performed. The topology indices
correspond to those displayed in Figure 1. Note that by condi-
tional independence, the topology used for Spanish reconstruc-
tion reduces to a chain. The heldout column indicates howmany
words, if any, were heldout for edit distance evaluation, and
from which language.
tion on the specifics of the experiments presented in
Section 4.5. We start with a description of the corpus
we created for these experiments.
4.1 Corpus
In order to train and evaluate our system, we
compiled a corpus of Romance cognate words.
The raw data was taken from three sources: the
wiktionary.org website, a Bible parallel cor-
pus (Resnik et al, 1999) and the Europarl corpus
(Koehn, 2002). From an XML dump of the Wik-
tionary data, we extracted multilingual translations,
which provide a list of word tuples in a large num-
ber of languages, including a few ancient languages.
891
The Europarl and the biblical data were processed
and aligned in the standard way, using combined
GIZA++ alignments (Och and Ney, 2003).
We performed our experiments with four lan-
guages from the Romance family (Latin, Italian,
Spanish, and Portuguese). For each of these lan-
guages, we used a simple in-house rule-based sys-
tem to convert the words into their IPA represen-
tations.2 After augmenting our alignments with
the transitive closure3 of the Europarl, Bible and
Wiktionary data, we filtered out non-cognate words
by thresholding the ratio of edit distance to word
length.4 The preprocessing is constraining in that we
require that all the elements of a tuple to be cognates,
which leaves out a significant portion of the data be-
hind (see the row Full entries in Table 2). However,
our approach relies on this assumption, as there is no
explicit model of non-cognate words. An interest-
ing direction for future work is the joint modeling of
phonology with the determination of the cognates,
but our simpler setting lets us focus on the proper-
ties of the edit model. Moreover, the restriction to
full entries has the side advantage that the Latin bot-
tleneck prevents the introduction of too many neol-
ogisms, which are numerous in the Europarl data, to
the final corpus.
Since we used automatic tools for preparing our
corpus rather than careful linguistic analysis, our
cognate list is much noiser in terms of the pres-
ence of borrowed words and phonemeic transcrip-
tion errors compared to the ones used by previous
approaches (Swadesh, 1955; Dyen et al, 1997). The
benefit of our mechanical preprocessing is that more
cognate data can easily be made available, allowing
us to effectively train richer models. We show in the
rest of this section that our phonological model can
indeed overcome this noise and recover meaningful
patterns from the data.
2The tool and the rules we used are available at
nlp.cs.berkeley.edu/pages/historical.html.
3For example, we would infer from an la-es bible align-
ment confessionem-confesio?n (confession) and an es-it Eu-
roparl alignment confesio?n-confessione that the Latin word con-
fessionem and the Italian word confessione are related.
4To be more precise we keep a tuple (w1, w2, . . . , wp) iff
d(wi,wj)
l?(wi,wj)
? 0.7 for all i, j ? {1, 2, . . . , p}, where l? is the mean
length
|wi|+|wj |
2 and d is the Levenshtein distance.
Name Languages Tuples Word forms
Raw sources of data used to create the corpus
Wiktionary es,pt,la,it 5840 11724
Bible la,es 2391 4782
Europarl es,pt 36905 73773
it,es 39506 78982
Main stages of preprocessing of the corpus
Closure es,pt,la,it 40944 106090
Cognates es,pt,la,it 27996 69637
Full entries es,pt,la,it 586 2344
Table 2: Statistics of the dataset we compiled for the evaluation
of our model. We show the languages represented, the number
of tuples and the number of word forms found in each of the
source of data and pre-processing steps involved in the creation
of the dataset we used to test our model. By full entry, we mean
the number of tuples that are jointly considered cognate by our
preprocessing system and that have a word form known for each
of the languages of interest. These last row forms the dataset
used for our experiments.
Language Baseline Model Improvement
Latin 2.84 2.34 9%
Spanish 3.59 3.21 11%
Table 3: Results of the edit distance experiment. The language
column corresponds to the language held-out for evaluation. We
show the mean edit distance across the evaluation examples.
4.2 Reconstruction of word forms
We ran the system using Topology 1 in Figure 1 to
demonstrate the the system can propose reasonable
reconstructions of Latin word forms on the basis of
modern observations. Half of the Latin words at the
root of the tree were held out, and the (uniform cost)
Levenshtein edit distance from the predicted recon-
struction to the truth was computed. Our baseline is
to pick randomly, for each heldout node in the tree,
an observed neighboring word (i.e. copy one of the
modern forms). We stopped EM after 15 iterations,
and reported the result on a Viterbi derivation using
the parameters obtained. Our model outperformed
this baseline by a 9% relative reduction in average
edit distance. Similarly, reconstruction of modern
forms was also demonstrated, with an improvement
of 11% (see Table 3).
To give a qualitative feel for the operation of the
system (good and bad), consider the example in Fig-
ure 5, taken from this experiment. The Latin dentis
/dEntis/ (teeth) is nearly correctly reconstructed as
/dEntes/, reconciling the appearance of the /j/ in the
892
/dEntis/
/djEntes/ /dEnti/
i ? E
E? j E s ?
Figure 5: An example of a Latin reconstruction given the Span-
ish and Italian word forms.
Spanish and the disappearance of the final /s/ in the
Italian. Note that the /is/ vs. /es/ ending is difficult
to predict in this context (indeed, it was one of the
early distinctions to be eroded in vulgar Latin).
While the uniform-cost edit distance misses im-
portant aspects of phonology (all phoneme substitu-
tions are not equal, for instance), it is parameter-free
and still seems to correlate to a large extent with lin-
guistic quality of reconstruction. It is also superior
to held-out log-likelihood, which fails to penalize er-
rors in the modeling assumptions, and to measuring
the percentage of perfect reconstructions, which ig-
nores the degree of correctness of each reconstructed
word.
4.3 Inference of phonological changes
Another use of our model is to automatically recover
the phonological drift processes between known or
partially known languages. To facilitate evaluation,
we continued in the well-studied Romance evolu-
tionary tree. Again, the root is Latin, but we now add
an additional modern language, Portuguese, and two
additional hidden nodes. One of the nodes charac-
terizes the least common ancestor of modern Span-
ish and Portuguese; the other, the least common an-
cestor of all three modern languages. In Figure 1,
Topology 2, these two nodes are labelled vl (Vulgar
Latin) and ib (Proto-Ibero Romance) respectively.
Since we are omitting many other branches, these
names should not be understood as referring to ac-
tual historical proto-languages, but, at best, to col-
lapsed points representing several centuries of evo-
lution. Nonetheless, the major reconstructed rules
still correspond to well known phenomena and the
learned model generally places them on reasonable
branches.
Figure 6 shows the top four general rules for
each of the evolutionary branches in this experiment,
ranked by the number of times they were used in the
derivations during the last iteration of EM. The la,
es, pt, and it forms are fully observed while the
vl and ib forms are automatically reconstructed.
Figure 6 also shows a specific example of the evolu-
tion of the Latin VERBUM (word/verb), along with
the specific edits employed by the model.
While quantitative evaluation such as measuring
edit distance is helpful for comparing results, it is
also illuminating to consider the plausibility of the
learned parameters in a historical light, which we
do here briefly. In particular, we consider rules on
the branch between la and vl, for which we have
historical evidence. For example, documents such
as the Appendix Probi (Baehrens, 1922) provide in-
dications of orthographic confusions which resulted
from the growing gap between Classical Latin and
Vulgar Latin phonology around the 3rd and 4th cen-
turies AD. The Appendix lists common misspellings
of Latin words, from which phonological changes
can be inferred.
On the la to vl branch, rules for word-final dele-
tion of classical case markers dominate the list (rules
ranks 1 and 3 for deletion of final /s/, ranks 2 and
4 for deletion of final /m/). It is indeed likely that
these were generally eliminated in Vulgar Latin. For
the deletion of the /m/, the Appendix Probi contains
pairs such as PASSIM NON PASSI and OLIM NON
OLI. For the deletion of final /s/, this was observed
in early inscriptions, e.g. CORNELIO for CORNE-
LIOS (Allen, 1989). The frequent leveling of the
distinction between /o/ and /u/ (rules ranked 5 and 6)
can be also be found in the Appendix Probi: COLU-
BER NON COLOBER. Note that in the specific ex-
ample shown, the model lowers the orignal /u/ and
then re-raises it in the pt branch due to a latter pro-
cess along that branch.
Similarily, major canonical rules were discovered
in other branches as well, for example, /v/ to /b/
fortition in Spanish, /s/ to /z/ voicing in Italian,
palatalization along several branches, and so on. Of
course, the recovered words and rules are not per-
fect. For example, reconstructed Ibero /tRinta/ to
Spanish /tReinta/ (thirty) is generated in an odd fash-
ion using rules /e/ to /i/ and /n/ to /in/. Moreover,
even when otherwise reasonable systematic sound
changes are captured, the crudeness of our fixed-
granularity contexts can prevent the true context
893
r ? R / many environmentse ? / #i ? / #t ? d / UNROUNDED UNROUNDED
u ? o / many environmentsv ? b / initial or intervocalict ? t e / ALVEOLAR #z ? s / ROUNDED UNROUNDED
/werbum/ (la)
/verbo/ (vl)
/veRbo/ (ib)
/beRbo/ (es) /veRbu/ (pt)
/vErbo/ (it)
s ? / #m ? /u ? o / many environmentsw ? v / # UNROUNDED
u ? o / ALVEOLAR #e ? E / many environmentsi ? / many environmentsi ? e / ALVEOLAR #
a ? 5 / ALVEOLAR #n ? m / UNROUNDED ALVEOLARo ? u / ALVEOLAR #e ? 1 / BILABIAL ALVEOLAR
m ?u ? ow ? v
r ? R
v ? b o ? u
e ? E
Figure 6: The tree shows the system?s hypothesised derivation of a selected Latin word form, VERBUM (word/verb) into the modern
Spanish, Italian and Portuguese pronunciations. The Latin root and modern leaves were observed while the hidden nodes as well as
all the derivations were obtained using the parameters computed by our model after 15 iterations of EM. Nontrivial rules (i.e. rules
that are not identities) used at each stage are shown along the corresponding edge. The boxes display the top four nontrivial rules
corresponding to each of these evolutionary branches, ordered by the number of time they were applied during the last E round of
sampling. Note that since our natural classes are of fixed granularity, some rules must be redundantly discovered, which tends to
flood the top of the rule lists with duplicates of the top few rules. We summarized such redundancies in the above tables.
from being captured, resulting in either rules apply-
ing with low probability in overly coarse environ-
ments or rules being learned redundantly in overly
fine environments.
4.4 Selection of phylogenies
In this experiment, we show that our model can be
used to select between various topologies of phylo-
genies. We first presented to the algorithm the uni-
versally accepted evolutionary tree corresponding to
the evolution of Latin into Spanish, Portuguese and
Italian (Topology 2 in Figure 1). We estimated the
log-likelihood L? of the data under this topology.
Next, we estimated the log-likelihood L? under two
defective topologies (*Topology 3 and *Topology
4). We recorded the log-likelihood ratio L? ? L?
after the last iteration of EM. Note that the two like-
lihoods are comparable since the complexity of the
two models is the same.5
We obtained a ratio of L? ? L? = ?4458 ?
(?4766) = 307 for Topology 2 versus *Topology
3, and ?4877? (?5125) = 248 for Topology 2 ver-
sus *Topology 4 (the experimental setup is described
in Table 1). As one would hope, this log-likelihood
ratio is positive in both cases, indicating that the sys-
tem prefers the true topology over the wrong ones.
While it may seem, at the first glance, that this re-
sult is limited in scope, knowing the relative arrange-
5If a word was not reachable in one of the topology, it was
ignored in both models for the computation of the likelihoods.
ment of all groups of four nodes is actually sufficient
for constructing a full-fledged phylogenetic tree. In-
deed, quartet-based methods, which have been very
popular in the computational biology community,
are precisely based on this fact (Erdos et al, 1996).
There is a rich literature on this subject and approxi-
mate algorithms exist which are robust to misclassi-
fication of a subset of quartets (Wu et al, 2007).
4.5 More experimental details
This section summarizes the values of the parame-
ters we used in these experiments, their interpreta-
tion, and the effect of setting them to other values.
The Dirichlet prior on the parameters can be in-
terpreted as adding pseudocounts to the correspond-
ing edits. It is an important way of infusing par-
simony into the model by setting the prior of the
self-substitution parameters much higher than that
of the other parameters. We used 6.0 as the prior on
the self-substitution parameters, and for all environ-
ments, 1.1 was divided uniformly across the other
edits. As long as the prior on self-substitution is
kept within this rough order of magnitude, varying
them has a limited effect on our results. We also ini-
tialized the parameters with values that encourage
self-substitutions. Again, the results were robust to
perturbation of initialization as long as the value for
self-substitution dominates the other parameters.
The experiments used two natural classes for
vowels (rounded and unrounded), and six natural
894
classes for consonants, based on the place of ar-
ticulation (alveolar, bilabial, labiodental, palatal,
postalveolar, and velar). We conducted experi-
ments to evaluate the effect of using different natural
classes and found that finer ones can help if enough
data is used for training. We defer the meticulous
study of the optimal granularity to future work, as it
would be a more interesting experiment under a log-
linear model. In such a model, contexts of different
granularities can coexist, whereas such coexistence
is not recognized by the current model, giving rise
to many duplicate rules.
We estimated the bigram phoneme model on the
words in the root languages that were not heldout.
Just as in machine translation, the language model
was found to contribute significantly to reconstruc-
tion performance. We tried to increase the weight of
the language model by exponentiating it to a power,
as is often done in NLP applications, but we did
not find that it had any significant impact on per-
formance.
In the reconstruction experiments, when the data
was not reachable by the model, the word used in
the initialization was used as the prediction, and
the evolution of these words were ignored when re-
estimating the parameters. Words were initialized
by picking at random, for each unobserved node, an
observed node?s corresponding word.
5 Conclusion
We have presented a novel probabilistic model of
diachronic phonology and an associated inference
procedure. Our experiments indicate that our model
is able to both produce accurate reconstructions as
measured by edit distance and identify linguisti-
cally plausible rules that account for the phonologi-
cal changes. We believe that the probabilistic frame-
work we have introduced for diachronic phonology
is promising, and scaling it up to richer phylogenetic
may indeed reveal something insightful about lan-
guage change.
6 Acknowledgement
We would like to thank Bonnie Chantarotwong for
her help with the IPA converter and our reviewers
for their comments. This work was supported by
a FQRNT fellowship to the first author, a NDSEG
fellowship to the second author, NSF grant number
BCS-0631518 to the third author, and a Microsoft
Research New Faculty Fellowship to the fourth au-
thor.
References
W. Sidney Allen. 1989. Vox Latina: The Pronunciation
of Classical Latin. Cambridge University Press.
W.A. Baehrens. 1922. Sprachlicher Kommentar zur
vulga?rlateinischen Appendix Probi. Halle (Saale) M.
Niemeyer.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
N. Chomsky and M. Halle. 1968. The Sound Pattern of
English. Harper & Row.
I. Dyen, J.B. Kruskal, and P. Black.
1997. FILE IE-DATA1. Available at
http://www.ntu.edu.au/education/langs/ielex/IE-
DATA1.
P. L. Erdos, M. A. Steel, L. A. Szekely, and T. J. Warnow.
1996. Local quartet splits of a binary tree infer all
quartet splits via one dyadic inference rule. Technical
report, DIMACS.
S. N. Evans, D. Ringe, and T. Warnow. 2004. Inference
of divergence times as a statistical inverse problem. In
P. Forster and C. Renfrew, editors, Phylogenetic Meth-
ods and the Prehistory of Languages. McDonald Insti-
tute Monographs.
Joseph Felsenstein. 2003. Inferring Phylogenies. Sin-
auer Associates.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
R. D. Gray and Q. Atkinson. 2003. Language-tree di-
vergence times support the Anatolian theory of Indo-
European origins. Nature.
John P. Huelsenbeck, Fredrik Ronquist, Rasmus Nielsen,
and Jonathan P. Bollback. 2001. Bayesian inference
of phylogeny and its impact on evolutionary biology.
Science.
P. Koehn. 2002. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
895
S. Li, D. K. Pearl, and H. Doss. 2000. Phylogenetic tree
construction using Markov chain Monte Carlo. Jour-
nal of the American Statistical Association.
Bob Mau and M.A. Newton. 1997. Phylogenetic in-
ference for binary data on dendrograms using markov
chain monte carlo. Journal of Computational and
Graphical Statistics.
L. Nakhleh, D. Ringe, and T. Warnow. 2005. Perfect
phylogenetic networks: A new methodology for re-
constructing the evolutionary history of natural lan-
guages. Language, 81:382?420.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29:19?51.
P. Resnik, Mari Broman Olsen, and Mona Diab. 1999.
The bible as a parallel corpus: Annotating the ?book of
2000 tongues?. Computers and the Humanities, 33(1-
2):129?153.
D. Ringe, T. Warnow, and A. Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100:59?129.
M. Swadesh. 1955. Towards greater accuracy in lex-
icostatistic dating. Journal of American Linguistics,
21:121?137.
A. Venkataraman, J. Newman, and J.D. Patrick. 1997.
A complexity measure for diachronic chinese phonol-
ogy. In J. Coleman, editor, Computational Phonology.
Association for Computational Linguistics.
G. Wu, J. A. You, and G. Lin. 2007. Quartet-based
phylogeny reconstruction with answer set program-
ming. IEEE/ACM Transactions on computational bi-
ology, 4:139?152.
Ziheng Yang and Bruce Rannala. 1997. Bayesian phy-
logenetic inference using dna sequences: A markov
chain monte carlo method. Molecular Biology and
Evolution 14.
896
Proceedings of NAACL HLT 2007, pages 139?146,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Bayesian Inference for PCFGs via Markov chain Monte Carlo
Mark Johnson
Cognitive and Linguistic Sciences
Brown University
Mark Johnson@brown.edu
Thomas L. Griffiths
Department of Psychology
University of California, Berkeley
Tom Griffiths@berkeley.edu
Sharon Goldwater
Department of Linguistics
Stanford University
sgwater@stanford.edu
Abstract
This paper presents two Markov chain
Monte Carlo (MCMC) algorithms for
Bayesian inference of probabilistic con-
text free grammars (PCFGs) from ter-
minal strings, providing an alternative
to maximum-likelihood estimation using
the Inside-Outside algorithm. We illus-
trate these methods by estimating a sparse
grammar describing the morphology of
the Bantu language Sesotho, demonstrat-
ing that with suitable priors Bayesian
techniques can infer linguistic structure
in situations where maximum likelihood
methods such as the Inside-Outside algo-
rithm only produce a trivial grammar.
1 Introduction
The standard methods for inferring the parameters of
probabilistic models in computational linguistics are
based on the principle of maximum-likelihood esti-
mation; for example, the parameters of Probabilistic
Context-Free Grammars (PCFGs) are typically es-
timated from strings of terminals using the Inside-
Outside (IO) algorithm, an instance of the Ex-
pectation Maximization (EM) procedure (Lari and
Young, 1990). However, much recent work in ma-
chine learning and statistics has turned away from
maximum-likelihood in favor of Bayesian methods,
and there is increasing interest in Bayesian methods
in computational linguistics as well (Finkel et al,
2006). This paper presents two Markov chain Monte
Carlo (MCMC) algorithms for inferring PCFGs and
their parses from strings alone. These can be viewed
as Bayesian alternatives to the IO algorithm.
The goal of Bayesian inference is to compute a
distribution over plausible parameter values. This
?posterior? distribution is obtained by combining the
likelihood with a ?prior? distribution P(?) over pa-
rameter values ?. In the case of PCFG inference ? is
the vector of rule probabilities, and the prior might
assert a preference for a sparse grammar (see be-
low). The posterior probability of each value of ?
is given by Bayes? rule:
P(?|D) ? P(D|?)P(?). (1)
In principle Equation 1 defines the posterior prob-
ability of any value of ?, but computing this may
not be tractable analytically or numerically. For this
reason a variety of methods have been developed to
support approximate Bayesian inference. One of the
most popular methods is Markov chain Monte Carlo
(MCMC), in which a Markov chain is used to sam-
ple from the posterior distribution.
This paper presents two new MCMC algorithms
for inferring the posterior distribution over parses
and rule probabilities given a corpus of strings. The
first algorithm is a component-wise Gibbs sampler
which is very similar in spirit to the EM algo-
rithm, drawing parse trees conditioned on the cur-
rent parameter values and then sampling the param-
eters conditioned on the current set of parse trees.
The second algorithm is a component-wise Hastings
sampler that ?collapses? the probabilistic model, in-
tegrating over the rule probabilities of the PCFG,
with the goal of speeding convergence. Both algo-
139
rithms use an efficient dynamic programming tech-
nique to sample parse trees.
Given their usefulness in other disciplines, we
believe that Bayesian methods like these are likely
to be of general utility in computational linguis-
tics as well. As a simple illustrative example, we
use these methods to infer morphological parses for
verbs from Sesotho, a southern Bantu language with
agglutinating morphology. Our results illustrate that
Bayesian inference using a prior that favors sparsity
can produce linguistically reasonable analyses in sit-
uations in which EM does not.
The rest of this paper is structured as follows.
The next section introduces the background for our
paper, summarizing the key ideas behind PCFGs,
Bayesian inference, and MCMC. Section 3 intro-
duces our first MCMC algorithm, a Gibbs sampler
for PCFGs. Section 4 describes an algorithm for
sampling trees from the distribution over trees de-
fined by a PCFG. Section 5 shows how to integrate
out the rule weight parameters ? in a PCFG, allow-
ing us to sample directly from the posterior distribu-
tion over parses for a corpus of strings. Finally, Sec-
tion 6 illustrates these methods in learning Sesotho
morphology.
2 Background
2.1 Probabilistic context-free grammars
Let G = (T,N, S,R) be a Context-Free Grammar
in Chomsky normal form with no useless produc-
tions, where T is a finite set of terminal symbols, N
is a finite set of nonterminal symbols (disjoint from
T ), S ? N is a distinguished nonterminal called the
start symbol, and R is a finite set of productions of
the form A ? BC or A ? w, where A,B,C ? N
and w ? T . In what follows we use ? as a variable
ranging over (N ?N) ? T .
A Probabilistic Context-Free Grammar (G, ?) is
a pair consisting of a context-free grammar G and
a real-valued vector ? of length |R| indexed by pro-
ductions, where ?A?? is the production probability
associated with the production A ? ? ? R. We
require that ?A?? ? 0 and that for all nonterminals
A ? N , ?A???R ?A?? = 1.
A PCFG (G, ?) defines a probability distribution
over trees t as follows:
PG(t|?) =
?
r?R
?fr(t)r
where t is generated by G and fr(t) is the number
of times the production r = A ? ? ? R is used
in the derivation of t. If G does not generate t let
PG(t|?) = 0. The yield y(t) of a parse tree t is
the sequence of terminals labeling its leaves. The
probability of a string w ? T+ of terminals is the
sum of the probability of all trees with yield w, i.e.:
PG(w|?) =
?
t:y(t)=w
PG(t|?).
2.2 Bayesian inference for PCFGs
Given a corpus of strings w = (w1, . . . , wn), where
each wi is a string of terminals generated by a known
CFG G, we would like to be able to infer the pro-
duction probabilities ? that best describe that corpus.
Taking w to be our data, we can apply Bayes? rule
(Equation 1) to obtain:
P(?|w) ? PG(w|?)P(?), where
PG(w|?) =
n
?
i=1
PG(wi|?).
Using t to denote a sequence of parse trees for w,
we can compute the joint posterior distribution over
t and ?, and then marginalize over t, with P(?|w) =
?
t P(t, ?|w). The joint posterior distribution on t
and ? is given by:
P(t, ?|w) ? P(w|t)P(t|?)P(?)
=
( n
?
i=1
P(wi|ti)P(ti|?)
)
P(?)
with P(wi|ti) = 1 if y(ti) = wi, and 0 otherwise.
2.3 Dirichlet priors
The first step towards computing the posterior dis-
tribution is to define a prior on ?. We take P(?) to
be a product of Dirichlet distributions, with one dis-
tribution for each non-terminal A ? N . The prior
is parameterized by a positive real valued vector ?
indexed by productions R, so each production prob-
ability ?A?? has a corresponding Dirichlet param-
eter ?A??. Let RA be the set of productions in R
140
with left-hand side A, and let ?A and ?A refer to
the component subvectors of ? and ? respectively
indexed by productions in RA. The Dirichlet prior
PD(?|?) is:
PD(?|?) =
?
A?N
PD(?A|?A), where
PD(?A|?A) =
1
C(?A)
?
r?RA
??r?1r and
C(?A) =
?
r?RA ?(?r)
?(?r?RA ?r)
(2)
where ? is the generalized factorial function and
C(?) is a normalization constant that does not de-
pend on ?A.
Dirichlet priors are useful because they are con-
jugate to the distribution over trees defined by a
PCFG. This means that the posterior distribution
on ? given a set of parse trees, P(?|t, ?), is also a
Dirichlet distribution. Applying Bayes? rule,
PG(?|t, ?) ? PG(t|?) PD(?|?)
?
(
?
r?R
?fr(t)r
)(
?
r?R
??r?1r
)
=
?
r?R
?fr(t)+?r?1r
which is a Dirichlet distribution with parameters
f(t) + ?, where f(t) is the vector of production
counts in t indexed by r ? R. We can thus write:
PG(?|t, ?) = PD(?|f(t) + ?)
which makes it clear that the production counts com-
bine directly with the parameters of the prior.
2.4 Markov chain Monte Carlo
Having defined a prior on ?, the posterior distribu-
tion over t and ? is fully determined by a corpus
w. Unfortunately, computing the posterior probabil-
ity of even a single choice of t and ? is intractable,
as evaluating the normalizing constant for this dis-
tribution requires summing over all possible parses
for the entire corpus and all sets of production prob-
abilities. Nonetheless, it is possible to define al-
gorithms that sample from this distribution using
Markov chain Monte Carlo (MCMC).
MCMC algorithms construct a Markov chain
whose states s ? S are the objects we wish to sam-
ple. The state space S is typically astronomically
large ? in our case, the state space includes all pos-
sible parses of the entire training corpus w ? and
the transition probabilities P(s?|s) are specified via a
scheme guaranteed to converge to the desired distri-
bution ?(s) (in our case, the posterior distribution).
We ?run? the Markov chain (i.e., starting in initial
state s0, sample a state s1 from P(s?|s0), then sam-
ple state s2 from P(s?|s1), and so on), with the prob-
ability that the Markov chain is in a particular state,
P(si), converging to ?(si) as i ??.
After the chain has run long enough for it to ap-
proach its stationary distribution, the expectation
E?[f ] of any function f(s) of the state s will be
approximated by the average of that function over
the set of sample states produced by the algorithm.
For example, in our case, given samples (ti, ?i) for
i = 1, . . . , ? produced by an MCMC algorithm, we
can estimate ? as
E?[?] ?
1
?
?
?
i=1
?i
The remainder of this paper presents two MCMC
algorithms for PCFGs. Both algorithms proceed by
setting the initial state of the Markov chain to a guess
for (t, ?) and then sampling successive states using
a particular transition matrix. The key difference be-
twen the two algorithms is the form of the transition
matrix they assume.
3 A Gibbs sampler for P(t, ?|w, ?)
The Gibbs sampler (Geman and Geman, 1984) is
one of the simplest MCMC methods, in which tran-
sitions between states of the Markov chain result
from sampling each component of the state condi-
tioned on the current value of all other variables. In
our case, this means alternating between sampling
from two distributions:
P(t|?,w, ?) =
n
?
i=1
P(ti|wi, ?), and
P(?|t,w, ?) = PD(?|f(t) + ?)
=
?
A?N
PD(?A|fA(t) + ?A).
Thus every two steps we generate a new sample of
t and ?. This alternation between parsing and up-
dating ? is reminiscent of the EM algorithm, with
141
tit1 tn
w1 wi wn
?Aj. . .?A1 . . . ?A|N|
?A1 . . .. . . ?Aj ?A|N|
. . .
. . .. . .
. . .
Figure 1: A Bayes net representation of dependen-
cies among the variables in a PCFG.
the Expectation step replaced by sampling t and the
Maximization step replaced by sampling ?.
The dependencies among variables in a PCFG are
depicted graphically in Figure 1, which makes clear
that the Gibbs sampler is highly parallelizable (just
like the EM algorithm). Specifically, the parses ti
are independent given ? and so can be sampled in
parallel from the following distribution as described
in the next section.
PG(ti|wi, ?) =
PG(ti|?)
PG(wi|?)
We make use of the fact that the posterior is a
product of independent Dirichlet distributions in or-
der to sample ? from PD(?|t, ?). The production
probabilities ?A for each nonterminal A ? N are
sampled from a Dirchlet distibution with parameters
??A = fA(t) + ?A. There are several methods for
sampling ? = (?1, . . . , ?m) from a Dirichlet distri-
bution with parameters ? = (?1, . . . , ?m), with the
simplest being sampling xj from a Gamma(?j) dis-
tribution for j = 1, . . . ,m and then setting ?j =
xj/
?m
k=1 xk (Gentle, 2003).
4 Efficiently sampling from P(t|w, ?)
This section completes the description of the Gibbs
sampler for (t, ?) by describing a dynamic program-
ming algorithm for sampling trees from the set of
parses for a string generated by a PCFG. This al-
gorithm appears fairly widely known: it was de-
scribed by Goodman (1998) and Finkel et al(2006)
and used by Ding et al(2005), and is very simi-
lar to other dynamic programming algorithms for
CFGs, so we only summarize it here. The algo-
rithm consists of two steps. The first step con-
structs a standard ?inside? table or chart, as used in
the Inside-Outside algorithm for PCFGs (Lari and
Young, 1990). The second step involves a recursion
from larger to smaller strings, sampling from the
productions that expand each string and construct-
ing the corresponding tree in a top-down fashion.
In this section we take w to be a string of terminal
symbols w = (w1, . . . , wn) where each wi ? T ,
and define wi,k = (wi+1, . . . , wk) (i.e., the sub-
string from wi+1 up to wk). Further, let GA =
(T,N,A,R), i.e., a CFG just like G except that the
start symbol has been replaced with A, so, PGA(t|?)
is the probability of a tree t whose root node is la-
beled A and PGA(w|?) is the sum of the probabili-
ties of all trees whose root nodes are labeled A with
yield w.
The Inside algorithm takes as input a PCFG
(G, ?) and a string w = w0,n and constructs a ta-
ble with entries pA,i,k for each A ? N and 0 ?
i < k ? n, where pA,i,k = PGA(wi,k|?), i.e., the
probability of A rewriting to wi,k. The table entries
are recursively defined below, and computed by enu-
merating all feasible i, k and A in any order such that
all smaller values of k?i are enumerated before any
larger values.
pA,k?1,k = ?A?wk
pA,i,k =
?
A?B C?R
?
i<j<k
?A?B C pB,i,j pC,j,k
for all A,B,C ? N and 0 ? i < j < k ? n. At the
end of the Inside algorithm, PG(w|?) = pS,0,n.
The second step of the sampling algorithm uses
the function SAMPLE, which returns a sample from
PG(t|w, ?) given the PCFG (G, ?) and the inside
table pA,i,k. SAMPLE takes as arguments a non-
terminal A ? N and a pair of string positions
0 ? i < k ? n and returns a tree drawn from
PGA(t|wi,k, ?). It functions in a top-down fashion,
selecting the production A ? BC to expand the A,
and then recursively calling itself to expand B and
C respectively.
function SAMPLE(A, i, k) :
if k ? i = 1 then return TREE(A,wk)
(j,B,C) = MULTI(A, i, k)
return TREE(A, SAMPLE(B, i, j), SAMPLE(C, j, k))
In this pseudo-code, TREE is a function that con-
structs unary or binary tree nodes respectively, and
142
MULTI is a function that produces samples from
a multinomial distribution over the possible ?split?
positions j and nonterminal children B and C ,
where:
P(j,B,C) = ?A?BC PGB (wi,j|?) PGC (wj,k|?)PGA(wi,k|?)
5 A Hastings sampler for P(t|w, ?)
The Gibbs sampler described in Section 3 has
the disadvantage that each sample of ? re-
quires reparsing the training corpus w. In
this section, we describe a component-wise
Hastings algorithm for sampling directly from
P(t|w, ?), marginalizing over the produc-
tion probabilities ?. Transitions between
states are produced by sampling parses ti from
P(ti|wi, t?i, ?) for each string wi in turn, where
t?i = (t1, . . . , ti?1, ti+1, . . . , tn) is the current set
of parses for w?i = (w1, . . . , wi?1, wi+1, . . . , wn).
Marginalizing over ? effectively means that the
production probabilities are updated after each
sentence is parsed, so it is reasonable to expect
that this algorithm will converge faster than the
Gibbs sampler described earlier. While the sampler
does not explicitly provide samples of ?, the results
outlined in Sections 2.3 and 3 can be used to sample
the posterior distribution over ? for each sample of
t if required.
Let PD(?|?) be a Dirichlet product prior, and let
? be the probability simplex for ?. Then by inte-
grating over the posterior Dirichlet distributions we
have:
P(t|?) =
?
?
PG(t|?)PD(?|?)d?
=
?
A?N
C(?A + fA(t))
C(?A)
(3)
where C was defined in Equation 2. Because we
are marginalizing over ?, the trees ti become depen-
dent upon one another. Intuitively, this is because
wi may provide information about ? that influences
how some other string wj should be parsed.
We can use Equation 3 to compute the conditional
probability P(ti|t?i, ?) as follows:
P(ti|t?i, ?) =
P(t|?)
P(t?i|?)
=
?
A?N
C(?A + fA(t))
C(?A + fA(t?i))
Now, if we could sample from
P(ti|wi, t?i, ?) =
P(wi|ti)P(ti|t?i, ?)
P(wi|t?i, ?)
we could construct a Gibbs sampler whose states
were the parse trees t. Unfortunately, we don?t even
know if there is an efficient algorithm for calculat-
ing P(wi|t?i, ?), let alne an efficient sampling al-
gorithm for this distribution.
Fortunately, this difficulty is not fatal. A Hast-
ings sampler for a probability distribution ?(s) is
an MCMC algorithm that makes use of a proposal
distribution Q(s?|s) from which it draws samples,
and uses an acceptance/rejection scheme to define a
transition kernel with the desired distribution ?(s).
Specifically, given the current state s, a sample s? 6=
s drawn from Q(s?|s) is accepted as the next state
with probability
A(s, s?) = min
{
1, ?(s
?)Q(s|s?)
?(s)Q(s?|s)
}
and with probability 1 ?A(s, s?) the proposal is re-
jected and the next state is the current state s.
We use a component-wise proposal distribution,
generating new proposed values for ti, where i is
chosen at random. Our proposal distribution is the
posterior distribution over parse trees generated by
the PCFG with grammar G and production proba-
bilities ??, where ?? is chosen based on the current
t?i as described below. Each step of our Hastings
sampler is as follows. First, we compute ?? from
t?i as described below. Then we sample t?i from
P(ti|wi, ??) using the algorithm described in Sec-
tion 4. Finally, we accept the proposal t?i given the
old parse ti for wi with probability:
A(ti, t?i) = min
{
1, P(t
?
i|wi, t?i, ?)P(ti|wi, ??)
P(ti|wi, t?i, ?)P(t?i|wi, ??)
}
= min
{
1, P(t
?
i|t?i, ?)P(ti|wi, ??)
P(ti|t?i, ?)P(t?i|wi, ??)
}
The key advantage of the Hastings sampler over the
Gibbs sampler here is that because the acceptance
probability is a ratio of probabilities, the difficult to
143
compute P(wi|t?i, ?) is a common factor of both
the numerator and denominator, and hence is not re-
quired. The P (wi|ti) term also disappears, being 1
for both the numerator and the denominator since
our proposal distribution can only generate trees for
which wi is the yield.
All that remains is to specify the production prob-
abilities ?? of the proposal distribution P(t?i|wi, ??).
While the acceptance rule used in the Hastings
algorithm ensures that it produces samples from
P(ti|wi, t?i, ?) with any proposal grammar ?? in
which all productions have nonzero probability, the
algorithm is more efficient (i.e., fewer proposals are
rejected) if the proposal distribution is close to the
distribution to be sampled.
Given the observations above about the corre-
spondence between terms in P(ti|t?i, ?) and the
relative frequency of the corresponding productions
in t?i, we set ?? to the expected value E[?|t?i, ?] of
? given t?i and ? as follows:
??r =
fr(t?i) + ?r
?
r??RA fr?(t?i) + ?r?
6 Inferring sparse grammars
As stated in the introduction, the primary contribu-
tion of this paper is introducing MCMC methods
for Bayesian inference to computational linguistics.
Bayesian inference using MCMC is a technique of
generic utility, much like Expectation-Maximization
and other general inference techniques, and we be-
lieve that it belongs in every computational linguist?s
toolbox alongside these other techniques.
Inferring a PCFG to describe the syntac-
tic structure of a natural language is an obvi-
ous application of grammar inference techniques,
and it is well-known that PCFG inference us-
ing maximum-likelihood techniques such as the
Inside-Outside (IO) algorithm, a dynamic program-
ming Expectation-Maximization (EM) algorithm for
PCFGs, performs extremely poorly on such tasks.
We have applied the Bayesian MCMC methods de-
scribed here to such problems and obtain results
very similar to those produced using IO. We be-
lieve that the primary reason why both IO and the
Bayesian methods perform so poorly on this task
is that simple PCFGs are not accurate models of
English syntactic structure. We know that PCFGs
? = (0.1, 1.0)
? = (0.5, 1.0)
? = (1.0, 1.0)
Binomial parameter ?1
P(?1|?)
10.80.60.40.20
5
4
3
2
1
0
Figure 2: A Dirichlet prior ? on a binomial parame-
ter ?1. As ?1 ? 0, P(?1|?) is increasingly concen-
trated around 0.
that represent only major phrasal categories ignore
a wide variety of lexical and syntactic dependen-
cies in natural language. State-of-the-art systems
for unsupervised syntactic structure induction sys-
tem uses models that are very different to these kinds
of PCFGs (Klein and Manning, 2004; Smith and
Eisner, 2006).1
Our goal in this section is modest: we aim merely
to provide an illustrative example of Bayesian infer-
ence using MCMC. As Figure 2 shows, when the
Dirichlet prior parameter ?r approaches 0 the prior
probability PD(?r|?) becomes increasingly concen-
trated around 0. This ability to bias the sampler
toward sparse grammars (i.e., grammars in which
many productions have probabilities close to 0) is
useful when we attempt to identify relevant produc-
tions from a much larger set of possible productions
via parameter estimation.
The Bantu language Sesotho is a richly agglutina-
tive language, in which verbs consist of a sequence
of morphemes, including optional Subject Markers
(SM), Tense (T), Object Markers (OM), Mood (M)
and derivational affixes as well as the obligatory
Verb stem (V), as shown in the following example:
re
SM
-a
T
-di
OM
-bon
V
-a
M
?We see them?
1It is easy to demonstrate that the poor quality of the PCFG
models is the cause of these problems rather than search or other
algorithmic issues. If one initializes either the IO or Bayesian
estimation procedures with treebank parses and then runs the
procedure using the yields alone, the accuracy of the parses uni-
formly decreases while the (posterior) likelihood uniformly in-
creases with each iteration, demonstrating that improving the
(posterior) likelihood of such models does not improve parse
accuracy.
144
We used an implementation of the Hastings sampler
described in Section 5 to infer morphological parses
t for a corpus w of 2,283 unsegmented Sesotho
verb types extracted from the Sesotho corpus avail-
able from CHILDES (MacWhinney and Snow, 1985;
Demuth, 1992). We chose this corpus because the
words have been morphologically segmented manu-
ally, making it possible for us to evaluate the mor-
phological parses produced by our system. We con-
structed a CFG G containing the following produc-
tions
Word ? V
Word ? V M
Word ? SM V M
Word ? SM T V M
Word ? SM T OM V M
together with productions expanding the pretermi-
nals SM,T,OM,V and M to each of the 16,350 dis-
tinct substrings occuring anywhere in the corpus,
producting a grammar with 81,755 productions in
all. In effect, G encodes the basic morphologi-
cal structure of the Sesotho verb (ignoring factors
such as derivation morphology and irregular forms),
but provides no information about the phonological
identity of the morphemes.
Note that G actually generates a finite language.
However, G parameterizes the probability distribu-
tion over the strings it generates in a manner that
would be difficult to succintly characterize except
in terms of the productions given above. Moreover,
with approximately 20 times more productions than
training strings, each string is highly ambiguous and
estimation is highly underconstrained, so it provides
an excellent test-bed for sparse priors.
We estimated the morphological parses t in two
ways. First, we ran the IO algorithm initialized
with a uniform initial estimate ?0 for ? to produce
an estimate of the MLE ??, and then computed the
Viterbi parses t? of the training corpus w with respect
to the PCFG (G, ??). Second, we ran the Hastings
sampler initialized with trees sampled from (G, ?0)
with several different values for the parameters of
the prior. We experimented with a number of tech-
niques for speeding convergence of both the IO and
Hastings algorithms, and two of these were particu-
larly effective on this problem. Annealing, i.e., us-
ing P(t|w)1/? in place of P(t|w) where ? is a ?tem-
perature? parameter starting around 5 and slowly ad-
justed toward 1, sped the convergence of both algo-
rithms. We ran both algorithms for several thousand
iterations over the corpus, and both seemed to con-
verge fairly quickly once ? was set to 1. ?Jittering?
the initial estimate of ? used in the IO algorithm also
sped its convergence.
The IO algorithm converges to a solution where
?Word? V = 1, and every string w ? w is analysed
as a single morpheme V. (In fact, in this grammar
P(wi|?) is the empirical probability of wi, and it is
easy to prove that this ? is the MLE).
The samples t produced by the Hastings algo-
rithm depend on the parameters of the Dirichlet
prior. We set ?r to a single value ? for all pro-
ductions r. We found that for ? > 10?2 the sam-
ples produced by the Hastings algorithm were the
same trivial analyses as those produced by the IO
algorithm, but as ? was reduced below this t be-
gan to exhibit nontrivial structure. We evaluated
the quality of the segmentations in the morpholog-
ical analyses t in terms of unlabeled precision, re-
call, f-score and exact match (the fraction of words
correctly segmented into morphemes; we ignored
morpheme labels because the manual morphological
analyses contain many morpheme labels that we did
not include in G). Figure 3 contains a plot of how
these quantities vary with ?; obtaining an f-score of
0.75 and an exact word match accuracy of 0.54 at
? = 10?5 (the corresponding values for the MLE ??
are both 0). Note that we obtained good results as ?
was varied over several orders of magnitude, so the
actual value of ? is not critical. Thus in this appli-
cation the ability to prefer sparse grammars enables
us to find linguistically meaningful analyses. This
ability to find linguistically meaningful structure is
relatively rare in our experience with unsupervised
PCFG induction.
We also experimented with a version of IO modi-
fied to perform Bayesian MAP estimation, where the
Maximization step of the IO procedure is replaced
with Bayesian inference using a Dirichlet prior, i.e.,
where the rule probabilities ?(k) at iteration k are es-
timated using:
?(k)r ? max(0,E[fr|w, ?(k?1)] + ?? 1).
Clearly such an approach is very closely related to
the Bayesian procedures presented in this article,
145
Exact
Recall
Precision
F-score
Dirichlet prior parameter ?r
1 0.01 1e-04 1e-06 1e-08 1e-10
1
0.75
0.5
0.25
0
Figure 3: Accuracy of morphological segmentations
of Sesotho verbs proposed by the Hastings algo-
rithms as a function of Dirichlet prior parameter
?. F-score, precision and recall are unlabeled mor-
pheme scores, while Exact is the fraction of words
correctly segmented.
and in some circumstances this may be a useful
estimator. However, in our experiments with the
Sesotho data above we found that for the small val-
ues of ? necessary to obtain a sparse solution,the
expected rule count E[fr] for many rules r was less
than 1??. Thus on the next iteration ?r = 0, result-
ing in there being no parse whatsoever for many of
the strings in the training data. Variational Bayesian
techniques offer a systematic way of dealing with
these problems, but we leave this for further work.
7 Conclusion
This paper has described basic algorithms for per-
forming Bayesian inference over PCFGs given ter-
minal strings. We presented two Markov chain
Monte Carlo algorithms (a Gibbs and a Hastings
sampling algorithm) for sampling from the posterior
distribution over parse trees given a corpus of their
yields and a Dirichlet product prior over the produc-
tion probabilities. As a component of these algo-
rithms we described an efficient dynamic program-
ming algorithm for sampling trees from a PCFG
which is useful in its own right. We used these
sampling algorithms to infer morphological analy-
ses of Sesotho verbs given their strings (a task on
which the standard Maximum Likelihood estimator
returns a trivial and linguistically uninteresting so-
lution), achieving 0.75 unlabeled morpheme f-score
and 0.54 exact word match accuracy. Thus this
is one of the few cases we are aware of in which
a PCFG estimation procedure returns linguistically
meaningful structure. We attribute this to the ability
of the Bayesian prior to prefer sparse grammars.
We expect that these algorithms will be of inter-
est to the computational linguistics community both
because a Bayesian approach to PCFG estimation is
more flexible than the Maximum Likelihood meth-
ods that currently dominate the field (c.f., the use
of a prior as a bias towards sparse solutions), and
because these techniques provide essential building
blocks for more complex models.
References
Katherine Demuth. 1992. Acquisition of Sesotho. In Dan
Slobin, editor, The Cross-Linguistic Study of Language Ac-
quisition, volume 3, pages 557?638. Lawrence Erlbaum As-
sociates, Hillsdale, N.J.
Ye Ding, Chi Yu Chan, and Charles E. Lawrence. 2005. RNA
secondary structure prediction by centroids in a Boltzmann
weighted ensemble. RNA, 11:1157?1166.
Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate Bayesian inference for linguistic annotation
pipelines. In Proceedings of the 2006 Conference on Empir-
ical Methods in Natural Language Processing, pages 618?
626, Sydney, Australia. Association for Computational Lin-
guistics.
Stuart Geman and Donald Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of images.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 6:721?741.
James E. Gentle. 2003. Random number generation and Monte
Carlo methods. Springer, New York, 2nd edition.
Joshua Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Harvard University. available from
http://research.microsoft.com/?joshuago/.
Dan Klein and Chris Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and con-
stituency. In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages 478?485.
K. Lari and S.J. Young. 1990. The estimation of Stochastic
Context-Free Grammars using the Inside-Outside algorithm.
Computer Speech and Language, 4(35-56).
Brian MacWhinney and Catherine Snow. 1985. The child lan-
guage data exchange system. Journal of Child Language,
12:271?296.
Noah A. Smith and Jason Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 569?576, Sydney,
Australia. Association for Computational Linguistics.
146
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 65?73,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improved Reconstruction of Protolanguage Word Forms
Alexandre Bouchard-Co?te?? Thomas L. Griffiths? Dan Klein?
?Computer Science Division ?Department of Psychology
University of California at Berkeley
Berkeley, CA 94720
Abstract
We present an unsupervised approach to re-
constructing ancient word forms. The present
work addresses three limitations of previous
work. First, previous work focused on faith-
fulness features, which model changes be-
tween successive languages. We add marked-
ness features, which model well-formedness
within each language. Second, we introduce
universal features, which support generaliza-
tions across languages. Finally, we increase
the number of languages to which these meth-
ods can be applied by an order of magni-
tude by using improved inference methods.
Experiments on the reconstruction of Proto-
Oceanic, Proto-Malayo-Javanic, and Classical
Latin show substantial reductions in error rate,
giving the best results to date.
1 Introduction
A central problem in diachronic linguistics is the re-
construction of ancient languages from their modern
descendants (Campbell, 1998). Here, we consider
the problem of reconstructing phonological forms,
given a known linguistic phylogeny and known cog-
nate groups. For example, Figure 1 (a) shows a col-
lection of word forms in several Oceanic languages,
all meaning to cry. The ancestral form in this case
has been presumed to be /taNis/ in Blust (1993). We
are interested in models which take as input many
such word tuples, each representing a cognate group,
along with a language tree, and induce word forms
for hidden ancestral languages.
The traditional approach to this problem has been
the comparative method, in which reconstructions
are done manually using assumptions about the rel-
ative probability of different kinds of sound change
(Hock, 1986). There has been work attempting to
automate part (Durham and Rogers, 1969; Eastlack,
1977; Lowe and Mazaudon, 1994; Covington, 1998;
Kondrak, 2002) or all of the process (Oakes, 2000;
Bouchard-Co?te? et al, 2008). However, previous au-
tomated methods have been unable to leverage three
important ideas a linguist would employ. We ad-
dress these omissions here, resulting in a more pow-
erful method for automatically reconstructing an-
cient protolanguages.
First, linguists triangulate reconstructions from
many languages, while past work has been lim-
ited to small numbers of languages. For example,
Oakes (2000) used four languages to reconstruct
Proto-Malayo-Javanic (PMJ) and Bouchard-Co?te? et
al. (2008) used two languages to reconstruct Clas-
sical Latin (La). We revisit these small datasets
and show that our method significantly outperforms
these previous systems. However, we also show that
our method can be applied to a much larger data
set (Greenhill et al, 2008), reconstructing Proto-
Oceanic (POc) from 64 modern languages. In ad-
dition, performance improves with more languages,
which was not the case for previous methods.
Second, linguists exploit knowledge of phonolog-
ical universals. For example, small changes in vowel
height or consonant place are more likely than large
changes, and much more likely than change to ar-
bitrarily different phonemes. In a statistical system,
one could imagine either manually encoding or auto-
matically inferring such preferences. We show that
both strategies are effective.
Finally, linguists consider not only how languages
change, but also how they are internally consistent.
Past models described how sounds do (or, more of-
ten, do not) change between nodes in the tree. To
borrow broad terminology from the Optimality The-
ory literature (Prince and Smolensky, 1993), such
models incorporated faithfulness features, captur-
ing the ways in which successive forms remained
similar to one another. However, each language
has certain regular phonotactic patterns which con-
65
strain these changes. We encode such patterns us-
ing markedness features, characterizing the internal
phonotactic structure of each language. Faithfulness
and markedness play roles analogous to the channel
and language models of a noisy-channel system. We
show that markedness features improve reconstruc-
tion, and can be used efficiently.
2 Related work
Our focus in this section is on describing the prop-
erties of the two previous systems for reconstruct-
ing ancient word forms to which we compare our
method. Citations for other related work, such as
similar approaches to using faithfulness and marked-
ness features, appear in the body of the paper.
In Oakes (2000), the word forms in a given pro-
tolanguage are reconstructed using a Viterbi multi-
alignment between a small number of its descendant
languages. The alignment is computed using hand-
set parameters. Deterministic rules characterizing
changes between pairs of observed languages are ex-
tracted from the alignment when their frequency is
higher than a threshold, and a proto-phoneme inven-
tory is built using linguistically motivated rules and
parsimony. A reconstruction of each observed word
is first proposed independently for each language. If
at least two reconstructions agree, a majority vote
is taken, otherwise no reconstruction is proposed.
This approach has several limitations. First, it is not
tractable for larger trees, since the time complexity
of their multi-alignment algorithm grows exponen-
tially in the number of languages. Second, deter-
ministic rules, while elegant in theory, are not robust
to noise: even in experiments with only four daugh-
ter languages, a large fraction of the words could not
be reconstructed.
In Bouchard-Co?te? et al (2008), a stochastic model
of sound change is used and reconstructions are in-
ferred by performing probabilistic inference over an
evolutionary tree expressing the relationships be-
tween languages. The model does not support gener-
alizations across languages, and has no way to cap-
ture phonotactic regularities within languages. As a
consequence, the resulting method does not scale to
large phylogenies. The work we present here ad-
dresses both of these issues, with a richer model
and faster inference allowing improved reconstruc-
tion and increased scale.
3 Model
We start this section by introducing some notation.
Let ? be a tree of languages, such as the examples
in Figure 3 (c-e). In such a tree, the modern lan-
guages, whose word forms will be observed, are the
leaves of ? . All internal nodes, particularly the root,
are languages whose word forms are not observed.
Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings ?? in the
International Phonological Alphabet (IPA).1
We assume that word forms evolve along the
branches of the tree ? . However, it is not the case
that each cognate set exists in each modern lan-
guage. Formally, we assume there to be a known
list of C cognate sets. For each c ? {1, . . . , C}
let L(c) denote the subset of modern languages that
have a word form in the c-th cognate set. For each
set c ? {1, . . . , C} and each language ` ? L(c), we
denote the modern word form by wc`. For cognate
set c, only the minimal subtree ?(c) containing L(c)
and the root is relevant to the reconstruction infer-
ence problem for that set.
From a high-level perspective, the generative pro-
cess is quite simple. Let c be the index of the cur-
rent cognate set, with topology ?(c). First, a word
is generated for the root of ?(c) using an (initially
unknown) root language model (distribution over
strings). The other nodes of the tree are drawn incre-
mentally as follows: for each edge ` ? `? in ?(c) use
a branch-specific distribution over changes in strings
to generate the word at node `?.
In the remainder of this section, we clarify the ex-
act form of the conditional distributions over string
changes, the distribution over strings at the root, and
the parameterization of this process.
3.1 Markedness and Faithfulness
In Optimality Theory (OT) (Prince and Smolensky,
1993), two types of constraints influence the selec-
tion of a realized output given an input form: faith-
fulness andmarkedness constraints. Faithfulness en-
1The choice of a phonemic representation is motivated by
the fact that most of the data available comes in this form. Dia-
critics are available in a smaller number of languages and may
vary across dialects, so we discarded them in this work.
66
t a
a
?
n g
i
i
s
#
#
#
#
a
?
n
g
/angi/
/a?i/
/ta?i/
/angi/
/a?i/
/ta?i/
?
S
?
I
x
1
x
2
x
3
x
7
y
1
y
2
y
3
y
7
x
4
y
4
y
5
y
6
x
5
x
6
?
n
g
1[Insert]
1[Subst]
1[(n g)@Kw]
1[??g@Kw]
1[??g]
1[(n)@Kw]
1[(g)@Kw]
Language Word form
Proto Oceanic /taNis/
Lau /aNi/
Kwara?ae /angi/
Taiof /taNis/
Table 1: A cognate set from the Austronesian dataset. All
word forms mean to cry.
constrain these changes. We encode such patterns
using markedness features, characterizing the inter-
nal phonotactic structure of each language. Faith-
fulness and marked ess play roles analogous to the
channel and language models of a noisy-channel
system. We show that markedness features greatly
improve reconstruction quality, and we show how to
work with them efficiently.
2 Related Work
Our focus in this section is on describing the prop-
erties of the two previous systems for reconstruct-
ing ancient word forms to which we compare our
method. Citations for other related work, such as
similar approaches to using faithfulness and marked-
ness features, appear in the body of the paper.
In Oakes (2000), the word forms in a given proto-
language are reconstructed using a Viterbi multi-
alignment between a small number of its descendant
languages. The alignment is computed using hand-
set parameters. Deterministic rules characterizing
changes between pairs of observed languages are ex-
tracted from the alignment when their frequency is
higher than a threshold, and a proto-phoneme inven-
tory is built using linguistically motivated rules and
parsimony. A reconstruction of each observed word
is first proposed independently for each language. If
at least two reconstructions agree, a majority vote
is taken, otherwise no reconstruction is proposed.
This approach has several limitations. First, it is
not tractable for larger trees since the complexity of
the multi-alignment algorithm grows exponentially
in the number of languages. Second, determinis-
tic rules, while elegant in theory, are not robust to
noise: even in experiments with only four daughter
languages, a large fraction of the words could not be
reconstructed.
In Bouchard-Co?te? et al (2008), a stochastic model
of sound change is used and reconstructions are in-
ferred by performing probabilistic inference over an
evolutionary tree expressing the relationships be-
tween languages. Use of approximate inference and
stochastic rules addresses some of the limitations of
(Oakes, 2000), but the resulting method is computa-
tionally demanding and consequently does not scale
to large phylogenies. The high computational cost
of probabilistic inference also limits the features that
can be included in the model (omitting global fea-
tures supporting generalizations across languages,
and markedness features within languages). The
work we present here addresses both of these issues,
with faster inference and a richer model allowing in-
creased scale and improved reconstruction.
3 Model
We start this section by introducing some notation.
Let ? be a tree of languages, such as the examples in
Figure 4 (c-e). In such a tree, the modern languages,
whose word forms will be observed, are the leaves
"1 . . . "m. All internal nodes, particularly the root,
are languages " whose word forms are not observed.
Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings ?? in the
International Phonological Alphabet (IPA).1
As a first approximation, we assume that word
forms evolve along the branches of the tree ? . How-
ever, it is not the case that each cognate set exists
in each modern langugage. Formally, we assume
there to be a known list of C cognate sets. For each
c ? {1, . . . , C} let L(c) denote the subset of mod-
ern languages that have a word form in the c-th cog-
nate set. For each set c ? {1, . . . , C} and each lan-
guage " ? L(c), we denote the modern word form
by wc!. For cognate set c, only the minimal subtree
?(c) containing L(c) and the root is relevant to the
reconstruction inference problem for that set.
From a high-level perspective, the generative pro-
cess is quite simple. Let c be the index of the cur-
rent cognate set, with topology ?(c). First, a word
is generated for the root of ?(c) using an (initially
unknown) root language model (distribution over
strings). The other nodes of the tree are drawn in-
crementally as follows: for each edge " ? "? in ?(c)
1The choice of a phonemic representation is motivated by
the fact that most of the data available comes in this form. Dia-
critics are available in a smaller number of languages and may
vary across dialects, so we discarted them in this work.
(a) (b)
(f)
(c)
(d)(e)
..
?
Figure 1: (a) A cognate set from the Austronesian dataset.
All word orms mean to cry. (b-d) The mutation model
used in this paper. (b) The mutation of POc /taNis/ to
Kw. /angi/. (c) Graphical model depicting the dependen-
cie among variables in one step of the mutation Markov
chain. (d) Active features for one step in this process.
(e-f) Comparison of two inference procedures on trees:
Single sequence resampling (e) draws one sequence at a
time, conditio ed on its parent and children, while ances-
try resampling (f) draws an aligned slice from all words
simultaneously. In large trees, the latter is ore efficien
than the former.
courages similarity between the input and output
while markedness favors well-formed output.
Viewed from this perspective, previous comput -
tional approaches to reconstruction are based almost
xclusively n faithf lnes , ex r ssed thr ug a mu-
tation model. Only the words in the language at the
root of the tree, if any, are explicitly encouraged to
be w ll-formed. In ontrast, we incorporate con-
straints on markedness for each language with both
general and branc -specific constraints on faithful-
ness. This is done using a lexicalized stochastic
string transducer (Varadarajan et al, 2008).
We now make precise the conditional distribu-
tions over pairs of evolving strings, referring to Fig-
ure 1 (b-d). Consider a language `? evolving to `
for cognate set c. Assume we have a word form
x = wcl? . The generative process for producing
y = wcl works as follows. First, we consider
x to be composed of characters x1x2 . . . xn, with
the first and last being a special boundary symbol
x1 = # ? ? which is never deleted, mutated, or
created. The process generates y = y1y2 . . . yn in
n chunks yi ? ??, i ? {1, . . . , n}, one for each xi.
The yi?s may be a single character, multiple charac-
ters, or even empty. In the example shown, all three
of these cases occur.
T generat yi, we define a mutation Markov
chain that incrementally adds zero or more charac-
ters to an initially empty yi. First, we decide whether
the current phoneme in the top word t = xi will be
deleted, in which case yi =  as in the example of
/s/ being deleted. If t is not deleted, we chose a sin-
gle substitution character in the bottom word. This
is the case both when /a/ is unchanged and when /N/
substitutes to /n/. We writeS = ??{?} for this set
of outcomes, where ? is the special outcome indi-
cating deletion. Importantly, the probabilities of this
multinomial can depend on both the previous char-
acter gen rated so far (i.e. the rightmost character
p of yi?1) and the current character in the previous
generation string (t). As we will see shortly, this al-
lows modelling markedness and faithfulness at every
branch, jointly. This multinomial decision acts as
the initial distribution of the mutation Markov chain.
We consider insertions only if a deletion was not
selected in the first step. Here, we draw from a
multinomial overS , where this time the special out-
come ? corresponds to stopping insertions, and the
other elements ofS correspond to symbols that are
appende to yi. In this case, the conditioning envi-
ronment is t = xi and the current rightmost symbol
p in yi. Insertions continue until ? is selected. In
the example, w follow the substitution of /N/ to /n/
with an insertion of /g/, followed by a decision to
stop that yi. We will use ?S,t,p,` and ?I,t,p,` to denote
the probabilities ver the substitution and insertion
decisions in the current branch `? ? `.
A similar process generates the word at the root
` of a tree, treating this word as a single string
y1 generated from a dummy ancestor t = x1. In
this case, only the insertion probabilities matter, and
we separately parameterize these probabilities with
?R,t,p,`. There is no actual dependence on t at the
root, but this formulation allows us to unify the pa-
rameterization, with each ??,t,p,` ? R|?|+1 where
? ? {R,S, I}.
3.2 Parameterization
Instead of directly estimating the transition proba-
bilities of the mutation Markov chain (as the param-
eters of a collection of multinomial distributions) we
67
express them as the output of a log-linear model. We
used the following feature templates:
OPERATION identifies whether an operation in the
mutation Markov chain is an insertion, a deletion,
a substitution, a self-substitution (i.e. of the form
x ? y, x = y), or the end of an insertion event.
Examples in Figure 1 (d): 1[Subst] and 1[Insert].
MARKEDNESS consists of language-specific n-
gram indicator functions for all symbols in ?. Only
unigram and bigram features are used for computa-
tional reasons, but we show in Section 5 that this
already captures important constraints. Examples in
Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw
stands for Kwara?ae, a language of the Solomon
Islands), the unigram indicators 1[(n)@Kw] and
1[(g)@Kw].
FAITHFULNESS consists of indicators for muta-
tion events of the form 1[x ? y], where x ? ?,
y ? S . Examples: 1[N ? n], 1[N ? n@Kw].
Feature templates similar to these can be found
for instance in Dreyer et al (2008) and Chen (2003),
in the context of string-to-string transduction. Note
also the connection with stochastic OT (Goldwater
and Johnson, 2003; Wilson, 2006), where a log-
linear model mediates markedness and faithfulness
of the production of an output form from an under-
lying input form.
3.3 Parameter sharing
Data sparsity is a significant challenge in protolan-
guage reconstruction. While the experiments we
present here use an order of magnitude more lan-
guages than previous computational approaches, the
increase in observed data also brings with it addi-
tional unknowns in the form of intermediate pro-
tolanguages. Since there is one set of parameters
for each language, adding more data is not sufficient
for increasing the quality of the reconstruction: we
show in Section 5.2 that adding extra languages can
actually hurt reconstruction using previous methods.
It is therefore important to share parameters across
different branches in the tree in order to benefit from
having observations from more languages.
As an example of useful parameter sharing, con-
sider the faithfulness features 1[/p/ ? /b/] and
1[/p/ ? /r/], which are indicator functions for the
appearance of two substitutions for /p/. We would
like the model to learn that the former event (a sim-
ple voicing change) should be preferred over the lat-
ter. In Bouchard-Co?te? et al (2008), this has to be
learned for each branch in the tree. The difficulty is
that not all branches will have enough information
to learn this preference, meaning that we need to de-
fine the model in such a way that it can generalize
across languages.
We used the following technique to address this
problem: we augment the sufficient statistics of
Bouchard-Co?te? et al (2008) to include the current
language (or language at the bottom of the current
branch) and use a single, global weight vector in-
stead of a set of branch-specific weights. Gener-
alization across branches is then achieved by using
features that ignore `, while branch-specific features
depend on `.
For instance, in Figure 1 (d), 1[N ? n] is
an example of a universal (global) feature shared
across all branches while 1[N ? n@Kw] is branch-
specific. Similarly, all of the features in OPERA-
TION, MARKEDNESS and FAITHFULNESS have uni-
versal and branch-specific versions.
3.4 Objective function
Concretely, the transition probabilities of the muta-
tion and root generation are given by:
??,t,p,`(?) = exp{??, f(?, t, p, `, ?)?}Z(?, t, p, `, ?) ? ?(?, t, ?),
where ? ? S , f : {S, I,R}?????L?S ? Rk
is the sufficient statistics or feature function, ??, ??
denotes inner product and ? ? Rk is a weight vector.
Here, k is the dimensionality of the feature space of
the log-linear model. In the terminology of exponen-
tial families, Z and ? are the normalization function
and reference measure respectively:
Z(?, t, p, `, ?) = ?
???S
exp{??, f(?, t, p, `, ??)?}
?(?, t, ?) =
?
???
???
0 if ? = S, t = #, ? 6= #
0 if ? = R, ? = ?
0 if ? 6= R, ? = #
1 o.w.
Here, ? is used to handle boundary conditions.
We will also need the following notation: let
P?(?),P?(?|?) denote the root and branch probabil-ity models described in Section 3.1 (with transition
probabilities given by the above log-linear model),
I(c), the set of internal (non-leaf) nodes in ?(c),
pa(`), the parent of language `, r(c), the root of ?(c)
68
and W (c) = (??)|I(c)|. We can summarize our ob-
jective function as follows:
CX
c=1
log
X
~w?W (c)
P?(wc,r(c))
Y
`?I(c)
P?(wc,`|wc,pa(`)) ? ||?||
2
2
2?2
The second term is a standard L2 regularization
penalty (we used ?2 = 1).
4 Learning algorithm
Learning is done using a Monte Carlo variant of the
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977). The M step is convex and com-
puted using L-BFGS (Liu et al, 1989); but the E
step is intractable (Lunter et al, 2003), so we used
a Markov chain Monte Carlo (MCMC) approxima-
tion (Tierney, 1994). At E step t = 1, 2, . . . , we
simulated the chain for O(t) iterations; this regime
is necessary for convergence (Jank, 2005).
In the E step, the inference problem is to com-
pute an expectation under the posterior over strings
in a protolanguage given observed word forms at the
leaves of the tree. The typical approach in biology
or historical linguistics (Holmes and Bruno, 2001;
Bouchard-Co?te? et al, 2008) is to use Gibbs sam-
pling, where the entire string at a single node in the
tree is sampled, conditioned on its parent and chil-
dren. This sampling domain is shown in Figure 1 (e),
where the middle word is completely resampled but
adjacent words are fixed. We will call this method
Single Sequence Resampling (SSR). While concep-
tually simple, this approach suffers from problems
in large trees (Holmes and Bruno, 2001). Con-
sequently, we use a different MCMC procedure,
called Ancestry Resampling (AR) that alleviates
the mixing problems (Figure 1 (f)). This method
was originally introduced for biological applications
(Bouchard-Co?te? et al, 2009), but commonalities be-
tween the biological and linguistic cases make it
possible to use it in our model.
Concretely, the problem with SSR arises when the
tree under consideration is large or unbalanced. In
this case, it can take a long time for information
from the observed languages to propagate to the root
of the tree. Indeed, samples at the root will ini-
tially be independent of the observations. AR ad-
dresses this problem by resampling one thin vertical
slice of all sequences at a time, called an ancestry.
For the precise definition, see Bouchard-Co?te? et al
(2009). Slices condition on observed data, avoiding
the problems mentioned above, and can propagate
information rapidly across the tree.
5 Experiments
We performed a comprehensive set of experiments
to test the new method for reconstruction outlined
above. In Section 5.1, we analyze in isolation the
effects of varying the set of features, the number of
observed languages, the topology, and the number
of iterations of EM. In Section 5.2 we compare per-
formance to an oracle and to three other systems.
Evaluation of all methods was done by computing
the Levenshtein distance (Levenshtein, 1966) be-
tween the reconstruction produced by each method
and the reconstruction produced by linguists. We
averaged this distance across reconstructed words to
report a single number for each method. We show
in Table 2 the average word length in each corpus;
note that the Latin average is much larger, giving
an explanation to the higher errors in the Romance
dataset. The statistical significance of all perfor-
mance differences are assessed using a paired t-test
with significance level of 0.05.
5.1 Evaluating system performance
We used the Austronesian Basic Vocabulary
Database (Greenhill et al, 2008) as the basis for
a series of experiments used to evaluate the per-
formance of our system and the factors relevant to
its success. The database includes partial cognacy
judgments and IPA transcriptions, as well as a few
reconstructed protolanguages. A reconstruction of
Proto-Oceanic (POc) originally developed by Blust
(1993) using the comparative method was the basis
for evaluation.
We used the cognate information provided in
the database, automatically constructing a global
tree2 and set of subtrees from the cognate set in-
dicator matrix M(`, c) = 1[` ? L(c)], c ?
{1, . . . , C}, ` ? L. For constructing the global tree,
we used the implementation of neighbor joining in
the Phylip package (Felsenstein, 1989). We used
a distance based on cognates overlap, dc(`1, `2) =?C
c=1 M(`1, c)M(`2, c). We bootstrapped 1000
2The dataset included a tree, but it was out of date as of
November 2008 (Greenhill et al, 2008).
69
NggelaBugotuTapeAvavaNeveeiNamanNeseSantaAnaNahavaqNatiKwaraaeSol
LauKwameraToloMarshalles
PuloAnnaChuukeseAK
SaipanCaro
Puluwatese
WoleaianPuloAnnan
Carolinian
WoleaiChuukeseNaunaPaameseSou
AnutaVaeakauTau
TakuuTokelauTonganSamoanIfiraMeleM
TikopiaTuvaluNiueFutunaEast
UveaEastRennellese
EmaeKapingamar
SikaianaNukuoroLuangiuaHawaiianMarquesan
TahitianthRurutuanMaoriTuamotuMangareva
Rarotongan
PenrhynRapanuiEas
PukapukaMwotlapMotaFijianBauNamakirNgunaArakiSouth
SaaRagaPeteraraMa
ItEsPtSndJvMadMal POc LaPMJ
Figure 3: Phylogenetic trees for three language families.Clockwise, from the top left: Romance, Austronesian andProto-Malayo-Javanic.
formance of our system and the factors relevant toits success. The database contained, as of Novem-ber 2008, 124,468 lexical items from 587 languagesmostly from the Austronesian language family. Thedatabase includes partial cognacy judgments andIPA transcriptions, as well as a few reconstructedproto-languages. A reconstruction of Proto Oceanic(POc) originally developed by (Blust, 1993) usingthe comparative method was the basis for evaluation.We used the cognate information provided in thedatabase, automatically constructing a global tree2and set of subtrees from the cognate set indicatormatrix M(!, c) = 1[! ? L(c)], c ? {1, . . . , C}, ! ?
L. For constructing the global tree, we used theimplementation of neighbor joining in the Phylippackage (Felsenstein, 1989). The distance ma-trix used the Hamming distance of cognate indi-cators, dc(!1, !2) = ?Cc=1 M(!1, c)M(!2, c). Webootstrapped 1000 samples and formed an accurate(90%) consensus tree. The tree obtained is not bi-nary, but the AR inference algorithm scales linearlyin the branching factor of the tree (in contrast, SSRscales exponentially (Lunter et al, 2003)).The first claim we verified experimentally is thathaving more observed languages aids reconstructionof proto-languages. To test this hypothesis we addedobserved modern languages in increasing order ofdistance dc to the target reconstruction of POc sothat the languages that are most useful for POc re-construction are added first. This prevents the ef-fects of adding a close language after several distant
2The dataset included a tree, but as of November 2008, itwas generated automatically and ?has [not] been updated in awhile.?
0 10 20 30 40 50 60 701.4
1.6
1.8
2
2.2
2.4
2.6
Number of modern languages
Error
Figure 4: Mean distance to the target reconstruction ofproto Oceanic as a function of the number of modern lan-guages used by the inference procedure.
ones being confused with an improvement producedby increasing the number of languages.The results are reported in Figure 4. They con-firm that large-scale inference is desirable for auto-matic proto-language reconstruction: going from 2-to-4, 4-to-8, 8-to-16, 16-to-32 languages all signifi-cantly helped reconstruction. There was still an av-erage edit distance improvement of 0.05 from 32 to64 languages, altough this was not statistically sig-nificant.We then conducted a number of experiments in-tended to assess the robustness of the system, and toidentify the contribution made by different factors itincorporates. First, we ran the system with 20 dif-ferent random seeds and assessed the stability of thesolution found. In each cases, learning was stableand helded performances. See Figure 5.Next, we found that all of the following ablationssignificantly hurts reconstruction: using a flat treein which all languages are equidistant from the re-constructed root and from each other instead of theconsensus tree, dropping the markedness features,disabling sharing across branches and dropping thefaithfulness features. The results of these experi-ments are shown in Table 2.For comparison, we also included in the sametable the performance of a semi-supervised systemtrained by K-fold validation. The system was ran
K time, with disjoint 1 ? K?1 of the POc. wordsgiven to the system (as observations in the graph-
Condition Edit dist.Unsupervised full system 1.87-FAITHFULNESS 2.02-MARKEDNESS 2.18-Sharing 1.99-Topology 2.06Semi-supervised system 1.75
Table 2: Effects of ablation of various aspects of ourunsupervised system on mean edit distance to protoOceanic. -Sharing corresponds to the subset of the fea-tures in OPERATION, FAITHFULNESS and MARKEDNESSthat condition on the current language, -Topolo y corre-sponds to using a flat topology where the only edges inthe tree connect modern languages to proto Oc anic. Thesemi-supervised system i described in text. All dif-ferences (compared to the unsupervised full system) arestatistically significant.
ical model) fo each run. It is semi-supervised inthe sense that gold reconstruction for many internalnodes are not avail bl (such as th common ances-tor of Kw. nd Lau in Fi re 6).3
Figure 6 shows the results of a concrete run over32 languages, zooming in to a pair of the Solomoniclanguages and the cognate set from Table 1. In theexample shown, the reconstruction is as good as theoracle, though off by one character (the final /s/ isnot resent in any of the 32 inputs and thereforeis not reconstructed). The diagrams show, for boththe global and the local features, the expectationsof each substitution superimposed on an IPA soundch rt, as well as a list of the top changes. Darkerlines indicate higher counts. T is run did not usen tural class constraints, but it can be seen that lin-guistically plausibl substitutions are learned. Theglobal features prefer a range of voic ng changes,manner changes, adjace t vowel motion, and so on,including mu ations like /s/ to /h/ which are commonbut poorly repre ented in a naive attribute-based nat-ural class scheme. On the other hand, the features l -cal to the lang ag Kwara?a (Kw.) pick out the sub-set of these change which are active in that branch,such as /s/?/t/ fortition.
3We also tried a fully supervised system where a flat topol-ogy is used so that all of these latent internal nodes are avoided;but it did not perform as well.
0 2 4 6 8 10 12 14 16 18 201.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4
3.6
EM Iteration
Error
Figure 5: Mean distance to the target reconstruction ofPOc as a function of the EM iteration.
5.2 Comparisons against other methods
The first two competing methods, PRAGUE and
BCLKG, are described in Oakes (2000) andBouchard-Co?te? et al (2008) respectively and sum-marized them in Section 1. Neither approach scaleswell to large datasets. In the first case, the bottleneckis the complexity of computing multi-alignmentswithout guide trees and the vanishing probabilitythat independent reconstructions agree. In the sec-ond case, the problem comes from slow mixing ofthe inference algorithm and the unregularized pro-liferation of parameters. For this reason, we built athird baseline that scales well in large datasets.This third baseline, CENTROID, computes thecentroid of the observed word forms in Leven-shtein distance. Let L(x, y) denote the Lev-enshtein distance between word forms x and
y. Ideally, we would like the baseline toreturn argminx????y?O L(x, y), where O ={y1, . . . , y|O|} is the set of observed word forms.Note that the optimum is not changed if we restrictthe minimization to be taken on x ? ?(O)? suchthat m ? |x| ? M where m = mini |yi|,M =maxi |yi| and?(O) is the set of characters occurringin O. Even with this restriction, this optimizationis intractable. As an approximation, we consideredonly strings built by at most k contiguous substringstaken from the word forms in O. If k = 1, then itis equivalent to taking the min over x ? O. At theother end of the spectrum, if k = M , it is exact.This scheme is exponential in k, but since words arerelatively short, we found that k = 2 often finds the
E
r
r
o
r
N. of m d rn lang. EM iteration
100 20300 60
1.4
1.8
2.2
2.6
1.8
2.4
3
3.6
Figure 2: Left: Mean distance to the target reconstruction
of POc as a function of the number of modern languages
used by the inference procedure. Right: Mean distance
and confidenc intervals as a function of th EM it ration,
averag d over 20 random seeds an ran on 4 languages.
samples nd forme a accurate (90%) consen us
tre . The tree obtained is o binary, but the AR
infer ce algorithm scales lin arly in the branching
factor of the tree (in contrast, SSR scale exp nen-
tially (Lunter et al, 2003)).
T e first laim we ver fied experimentally is that
having more observed languages aids reconstruction
of protolanguages. To t t this hypothesis we added
observed mod rn l nguage in increasing order of
distance dc to the target reconstruction of POc so
that the languages that are most useful for POc re-
construction are added first. This prevents the ef-
fects of adding a close language after several distant
ones being confused with an improvement produced
by increasing the number of languages.
The results are reported in Figure 2 (a). They con-
firm that large-scale inference is desirable for au-
tomatic protolanguage reconstruction: reconstruc-
tion improved statistically significantly with each in-
crease except from 32 to 64 languages, where the
average edit distance improvement was 0.05.
We then conducted a number of experiments in-
tended to assess the robustness of the system, and to
identify the contribution made by different factors it
incorporates. First, we ran the system with 20 dif-
ferent random seeds to assess the stability of the so-
lutions found. In each case, learning was stable and
accuracy improved during training. See Figure 2 (b).
Next, we found that all of the following ablations
significantly hurt reconstruction: using a flat tree (in
which all languages are equidistant from the recon-
structed root and from each other) instead of the con-
sensus tree, dropping the markedness features, drop-
Condition Edit dist.
Unsupervised full system 1.87
-FAITHFULNESS 2.02
-MARKEDNESS 2.18
-Sharing 1.99
-Topology 2.06
Semi-supervised system 1.75
Table 1: Effects of ablation of various aspects of our
unsupervised system on mean edit distance to POc.
-Sharing corresponds to the restriction to the subset of the
features in OPERATION, FAITHFULNESS and MARKED-
NESS that are branch-specific, -Topology corresponds to
using a flat topology where the only edges in the tree con-
nect modern languages to POc. The semi-supervised sys-
tem is described in the text. All differences (compared to
the unsupervised full system) are statistically significant.
ping the faithfulness features, and disabling sharing
across branches. The results of these experiments
are shown in Table 1.
For comparison, we also included in the same
table the performance of a semi-supervised system
trained by K-fold validation. The system was ran
K = 5 times, with 1?K?1 of the POc words given
to the system as observations in the graphical model
for each run. It is semi-supervised in the sense that
gold reconstruction for many internal nodes are not
available in the dataset (for example the common an-
cestor of Kwara?ae (Kw.) and Lau in Figure 3 (b)),
so they are still not filled.3
Figure 3 (b) shows the results of a concrete run
over 32 languages, zooming in to a pair of the
Solomonic languages and the cognate set from Fig-
ure 1 (a). In the example shown, the reconstruc-
tion is as good as the ORACLE (described in Sec-
tion 5.2), though off by one character (the final /s/
is not present in any of the 32 inputs and therefore
is not reconstructed). In (a), diagrams show, for
both the global and the local (Kwara?ae) features,
the expectations of each substitution superimposed
on an IPA sound chart, as well as a list of the top
changes. Darker lines indicate higher counts. This
run did not use natural class constraints, but it can
3We also tried a fully supervised system where a flat topol-
ogy is used so that all of these latent internal nodes are avoided;
but it did not perform as well?this is consistent with the
-Topology experiment of Table 1.
70
be seen that linguistically plausible substitutions are
learned. The global features prefer a range of voic-
ing changes, manner changes, adjacent vowel mo-
tion, and so on, including mutations like /s/ to /h/
which are common but poorly represented in a naive
attribute-based natural class scheme. On the other
hand, the features local to the language Kwara?ae
pick out the subset of these changes which are ac-
tive in that branch, such as /s/?/t/ fortition.
5.2 Comparisons against other methods
The first two competing methods, PRAGUE and
BCLKG, are described in Oakes (2000) and
Bouchard-Co?te? et al (2008) respectively and sum-
marized in Section 1. Neither approach scales well
to large datasets. In the first case, the bottleneck is
the complexity of computing multi-alignments with-
out guide trees and the vanishing probability that in-
dependent reconstructions agree. In the second case,
the problem comes from the unregularized prolifera-
tion of parameters and slow mixing of the inference
algorithm. For this reason, we built a third baseline
that scales well in large datasets.
This third baseline, CENTROID, computes the
centroid of the observed word forms in Leven-
shtein distance. Let L(x, y) denote the Lev-
enshtein distance between word forms x and
y. Ideally, we would like the baseline to
return argminx???
?
y?O L(x, y), where O =
{y1, . . . , y|O|} is the set of observed word forms.
Note that the optimum is not changed if we restrict
the minimization to be taken on x ? ?(O)? such
that m ? |x| ? M where m = mini |yi|,M =
maxi |yi| and ?(O) is the set of characters occurring
in O. Even with this restriction, this optimization
is intractable. As an approximation, we considered
only strings built by at most k contiguous substrings
taken from the word forms in O. If k = 1, then it
is equivalent to taking the min over x ? O. At the
other end of the spectrum, if k = M , it is exact.
This scheme is exponential in k, but since words are
relatively short, we found that k = 2 often finds the
same solution as higher values of k. The difference
was in all the cases not statistically significant, so we
report the approximation k = 2 in what follows.
We also compared against an oracle, denoted OR-
ACLE, which returns argminy?OL(y, x?), where x?
is the target reconstruction. We will denote it by OR-
Comparison CENTROID PRAGUE BCLKG
Protolanguage POc PMJ La
Heldout (prop.) 243 (1.0) 79 (1.0) 293 (0.5)
Modern languages 70 4 2
Cognate sets 1321 179 583
Observed words 10783 470 1463
Mean word length 4.5 5.0 7.4
Table 2: Experimental setup: number of held-out proto-
word from (absolute and relative), of modern languages,
cognate sets and total observed words. The split for
BCLKG is the same as in Bouchard-Co?te? et al (2008).
ACLE. This is superior to picking a single closest
language to be used for all word forms, but it is pos-
sible for systems to perform better than the oracle
since it has to return one of the observed word forms.
We performed the comparison against Oakes
(2000) and Bouchard-Co?te? et al (2008) on the same
dataset and experimental conditions as those used in
the respective papers (see Table 2). Note that the
setup of Bouchard-Co?te? et al (2008) provides super-
vision (half of the Latin word forms are provided);
all of the other comparisons are performed in a com-
pletely unsupervised manner.
The PMJ dataset was compiled by Nothofer
(1975), who also reconstructed the corresponding
protolanguage. Since PRAGUE is not guaranteed to
return a reconstruction for each cognate set, only 55
word forms could be directly compared to our sys-
tem. We restricted comparison to this subset of the
data. This favors PRAGUE since the system only pro-
poses a reconstruction when it is certain. Still, our
system outperformed PRAGUE, with an average dis-
tance of 1.60 compared to 2.02 for PRAGUE. The
difference is marginally significant, p = 0.06, partly
due to the small number of word forms involved.
We also exceeded the performance of BCLKG on
the Romance dataset. Our system?s reconstruction
had an edit distance of 3.02 to the truth against 3.10
for BCLKG. However, this difference was not signifi-
cant (p = 0.15). We think this is because of the high
level of noise in the data (the Romance dataset is the
only dataset we consider that was automatically con-
structed rather than curated by linguists). A second
factor contributing to this small difference may be
that the the experimental setup of BCLKG used very
few languages, while the performance of our system
improves markedly with more languages.
71
Nggela
Bugotu
TapeAvava
Neveei
Naman
NeseSantaAna
Nahavaq
NatiKwaraaeSol
LauKwamera
ToloMarshalles
PuloAnna
ChuukeseAK
SaipanCaro
Puluwatese
Woleaian
PuloAnnan
Carolinian
Woleai
Chuukese
Nauna
PaameseSou
AnutaVaeakauTau
Takuu
Tokelau
Tongan
Samoan
IfiraMeleM
Tikopia
Tuvalu
NiueFutunaEast
UveaEast
Rennellese
EmaeKapingamar
Sikaiana
Nukuoro
Luangiua
Hawaiian
Marquesan
Tahitianth
Rurutuan
Maori
Tuamotu
Mangareva
Rarotongan
Penrhyn
RapanuiEas
Pukapuka
Mwotlap
MotaFijianBau
Namakir
Nguna
ArakiSouth
SaaRagaPeteraraMa
ItEsPtSndJvMadMal POc LaPMJ
/a?i/ (Lau)/angi/ (Kw.)
/a?i/
/ta?i/ (POc)
....
Universal
a ?? el ?? rs ?? hk ?? gr ?? l
Kwa
N ?? ng ?? ks ?? te ?? io ?? a
1
Universal
a ?? el ?? rs ?? hk ?? gr ?? l
Kwa
N ?? ng ?? ks ?? te io a
1
? ??
??
?
?
?
? ? ??
?
?
?
???
?
? ?
? ??
?
?
??
?
?
?
?
f
gdb c
n
?
m
j
k
hv
t
s
r
qp ?
z
?
? x
? ??
??
?
?
?
? ? ??
?
?
?
???
?
? ?
? ??
?
?
??
?
?
?
?
f
gdb c
n
?
m
j
k
hv
t
s
r
qp ?
z
?
? x
(a) (b) (c) (d) (e)
Figure 3: (a) A visualization of two learned faithfulness parameters: on the top, from the universal features, on
the bottom, for one particular branch. Each pair of phonemes have a link with grayscale value proportional to the
expectation of a transition between them. The five strongest links are also included at the right. (b) A sample taken
from our POc experiments (see text). (c-e) Phylogenetic trees for three language families: Proto-Malayo-Javanic,
Austronesian and Romance.
We conducted another experiment to verify this
by running both systems in larger trees. Because the
Romance dataset had only three modern languages
transcribed in IPA, we used the Austronesian dataset
to perform the test. The results were all significant in
this setup: while our method went from an edit dis-
tance of 2.01 to 1.79 in the 4-to-8 languages exper-
iment described in Section 5.1, BCLKG went from
3.30 to 3.38. This suggests that more languages can
actually hurt systems that do not support parameter
sharing.
Since we have shown evidence that PRAGUE and
BCLKG do not scale well to large datasets, we
also compared against ORACLE and CENTROID in a
large-scale setting. Specifically, we compare to the
experimental setup on 64 modern languages used to
reconstruct POc described before. Encouragingly,
while the system?s average distance (1.49) does not
attain that of the ORACLE (1.13), we significantly
outperform the CENTROID baseline (1.79).
5.3 Incorporating prior linguistic knowledge
The model also supports the addition of prior lin-
guistic knowledge. This takes the form of feature
templates with more internal structure. We per-
formed experiments with an additional feature tem-
plate:
STRUCT-FAITHFULNESS is a structured version of
FAITHFULNESS, replacing x and y with their natu-
ral classes N?(x) and N?(y) where ? indexes types
of classes, ranging over {manner, place, phonation,
isOral, isCentral, height, backness, roundedness}.
This feature set is reminiscent of the featurized rep-
resentation of Kondrak (2000).
We compared the performance of the system with
and without STRUCT-FAITHFULNESS to check if the
algorithm can recover the structure of natural classes
in an unsupervised fashion. We found that with
2 or 4 observed languages, FAITHFULNESS under-
performed STRUCT-FAITHFULNESS, but for larger
trees, the difference was not significant. FAITH-
FULNESS even slightly outperformed its structured
cousin with 16 observed languages.
6 Conclusion
By enriching our model to include important fea-
tures like markedness, and by scaling up to much
larger data sets than were previously possible, we
obtained substantial improvements in reconstruc-
tion quality, giving the best results on past data
sets. While many more complex phenomena are
still unmodeled, from reduplication to borrowing to
chained sound shifts, the current approach signifi-
cantly increases the power, accuracy, and efficiency
of automatic reconstruction.
Acknowledgments
We would like to thank Anna Rafferty and our re-
viewers for their comments. This work was sup-
ported by a NSERC fellowship to the first author and
NSF grant number BCS-0631518 to the second au-
thor.
72
References
R. Blust. 1993. Central and central-Eastern Malayo-
Polynesian. Oceanic Linguistics, 32:241?293.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change. In
Advances in Neural Information Processing Systems
20.
A. Bouchard-Co?te?, M. I. Jordan, and D. Klein. 2009.
Efficient inference in phylogenetic InDel trees. In Ad-
vances in Neural Information Processing Systems 21.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
M. A. Covington. 1998. Alignment of multiple lan-
guages for historical comparison. In Proceedings of
ACL 1998.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
M. Dreyer, J. R. Smith, and J. Eisner. 2008. Latent-
variable modeling of string transductions with finite-
state methods. In Proceedings of EMNLP 2008.
S. P. Durham and D. E. Rogers. 1969. An application
of computer programming to the reconstruction of a
proto-language. In Proceedings of the 1969 confer-
ence on Computational linguistics.
C. L. Eastlack. 1977. Iberochange: A program to
simulate systematic sound change in Ibero-Romance.
Computers and the Humanities.
J. Felsenstein. 1989. PHYLIP - PHYLogeny Inference
Package (Version 3.2). Cladistics, 5:164?166.
S. Goldwater and M. Johnson. 2003. Learning OT
constraint rankings using a maximum entropy model.
Proceedings of the Workshop on Variation within Op-
timality Theory.
S. J. Greenhill, R. Blust, and R. D. Gray. 2008. The
Austronesian basic vocabulary database: From bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271?283.
H. H. Hock. 1986. Principles of Historical Linguistics.
Walter de Gruyter.
I. Holmes and W. J. Bruno. 2001. Evolutionary HMM:
a Bayesian approach to multiple alignment. Bioinfor-
matics, 17:803?820.
W. Jank. 2005. Stochastic variants of EM: Monte Carlo,
quasi-Monte Carlo and more. In Proceedings of the
American Statistical Association.
G. Kondrak. 2000. A new algorithm for the alignment of
phonetic sequences. In Proceedings of NAACL 2000.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
V. I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions and reversals. Soviet Physics
Doklady, 10, February.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
J. B. Lowe and M. Mazaudon. 1994. The reconstruction
engine: a computer implementation of the comparative
method. Comput. Linguist., 20(3):381?417.
G. A. Lunter, I. Miklo?s, Y. S. Song, and J. Hein. 2003.
An efficient algorithm for statistical multiple align-
ment on arbitrary phylogenetic trees. Journal of Com-
putational Biology, 10:869?889.
B. Nothofer. 1975. The reconstruction of Proto-Malayo-
Javanic. M. Nijhoff.
M. P. Oakes. 2000. Computer estimation of vocabu-
lary in a protolanguage from word lists in four daugh-
ter languages. Journal of Quantitative Linguistics,
7(3):233?244.
A. Prince and P. Smolensky. 1993. Optimality theory:
Constraint interaction in generative grammar. Techni-
cal Report 2, Rutgers University Center for Cognitive
Science.
L. Tierney. 1994. Markov chains for exploring posterior
distributions. The Annals of Statistics, 22(4):1701?
1728.
A. Varadarajan, R. K. Bradley, and I. H. Holmes. 2008.
Tools for simulating evolution of aligned genomic re-
gions with integrated parameter estimation. Genome
Biology, 9:R147.
C. Wilson. 2006. Learning phonology with substantive
bias: An experimental and computational study of ve-
lar palatalization. Cognitive Science, 30.5:945?982.
73
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744?751,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging?
Sharon Goldwater
Department of Linguistics
Stanford University
sgwater@stanford.edu
Thomas L. Griffiths
Department of Psychology
UC Berkeley
tom griffiths@berkeley.edu
Abstract
Unsupervised learning of linguistic structure
is a difficult problem. A common approach
is to define a generative model and max-
imize the probability of the hidden struc-
ture given the observed data. Typically,
this is done using maximum-likelihood es-
timation (MLE) of the model parameters.
We show using part-of-speech tagging that
a fully Bayesian approach can greatly im-
prove performance. Rather than estimating
a single set of parameters, the Bayesian ap-
proach integrates over all possible parame-
ter values. This difference ensures that the
learned structure will have high probability
over a range of possible parameters, and per-
mits the use of priors favoring the sparse
distributions that are typical of natural lan-
guage. Our model has the structure of a
standard trigram HMM, yet its accuracy is
closer to that of a state-of-the-art discrimi-
native model (Smith and Eisner, 2005), up
to 14 percentage points better than MLE. We
find improvements both when training from
data alone, and using a tagging dictionary.
1 Introduction
Unsupervised learning of linguistic structure is a dif-
ficult problem. Recently, several new model-based
approaches have improved performance on a vari-
ety of tasks (Klein and Manning, 2002; Smith and
?This work was supported by grants NSF 0631518 and
ONR MURI N000140510388. We would also like to thank
Noah Smith for providing us with his data sets.
Eisner, 2005). Nearly all of these approaches have
one aspect in common: the goal of learning is to
identify the set of model parameters that maximizes
some objective function. Values for the hidden vari-
ables in the model are then chosen based on the
learned parameterization. Here, we propose a dif-
ferent approach based on Bayesian statistical prin-
ciples: rather than searching for an optimal set of
parameter values, we seek to directly maximize the
probability of the hidden variables given the ob-
served data, integrating over all possible parame-
ter values. Using part-of-speech (POS) tagging as
an example application, we show that the Bayesian
approach provides large performance improvements
over maximum-likelihood estimation (MLE) for the
same model structure. Two factors can explain the
improvement. First, integrating over parameter val-
ues leads to greater robustness in the choice of tag
sequence, since it must have high probability over
a range of parameters. Second, integration permits
the use of priors favoring sparse distributions, which
are typical of natural language. These kinds of pri-
ors can lead to degenerate solutions if the parameters
are estimated directly.
Before describing our approach in more detail,
we briefly review previous work on unsupervised
POS tagging. Perhaps the most well-known is that
of Merialdo (1994), who used MLE to train a tri-
gram hidden Markov model (HMM). More recent
work has shown that improvements can be made
by modifying the basic HMM structure (Banko and
Moore, 2004), using better smoothing techniques or
added constraints (Wang and Schuurmans, 2005), or
using a discriminative model rather than an HMM
744
(Smith and Eisner, 2005). Non-model-based ap-
proaches have also been proposed (Brill (1995); see
also discussion in Banko and Moore (2004)). All of
this work is really POS disambiguation: learning is
strongly constrained by a dictionary listing the al-
lowable tags for each word in the text. Smith and
Eisner (2005) also present results using a diluted
dictionary, where infrequent words may have any
tag. Haghighi and Klein (2006) use a small list of
labeled prototypes and no dictionary.
A different tradition treats the identification of
syntactic classes as a knowledge-free clustering
problem. Distributional clustering and dimen-
sionality reduction techniques are typically applied
when linguistically meaningful classes are desired
(Schu?tze, 1995; Clark, 2000; Finch et al, 1995);
probabilistic models have been used to find classes
that can improve smoothing and reduce perplexity
(Brown et al, 1992; Saul and Pereira, 1997). Unfor-
tunately, due to a lack of standard and informative
evaluation techniques, it is difficult to compare the
effectiveness of different clustering methods.
In this paper, we hope to unify the problems of
POS disambiguation and syntactic clustering by pre-
senting results for conditions ranging from a full tag
dictionary to no dictionary at all. We introduce the
use of a new information-theoretic criterion, varia-
tion of information (Meila?, 2002), which can be used
to compare a gold standard clustering to the clus-
tering induced from a tagger?s output, regardless of
the cluster labels. We also evaluate using tag ac-
curacy when possible. Our system outperforms an
HMM trained with MLE on both metrics in all cir-
cumstances tested, often by a wide margin. Its ac-
curacy in some cases is close to that of Smith and
Eisner?s (2005) discriminative model. Our results
show that the Bayesian approach is particularly use-
ful when learning is less constrained, either because
less evidence is available (corpus size is small) or
because the dictionary contains less information.
In the following section, we discuss the motiva-
tion for a Bayesian approach and present our model
and search procedure. Section 3 gives results illus-
trating how the parameters of the prior affect re-
sults, and Section 4 describes how to infer a good
choice of parameters from unlabeled data. Section 5
presents results for a range of corpus sizes and dic-
tionary information, and Section 6 concludes.
2 A Bayesian HMM
2.1 Motivation
In model-based approaches to unsupervised lan-
guage learning, the problem is formulated in terms
of identifying latent structure from data. We de-
fine a model with parameters ?, some observed vari-
ables w (the linguistic input), and some latent vari-
ables t (the hidden structure). The goal is to as-
sign appropriate values to the latent variables. Stan-
dard approaches do so by selecting values for the
model parameters, and then choosing the most prob-
able variable assignment based on those parame-
ters. For example, maximum-likelihood estimation
(MLE) seeks parameters ?? such that
?? = argmax
?
P (w|?), (1)
where P (w|?) = ?t P (w, t|?). Sometimes, a
non-uniform prior distribution over ? is introduced,
in which case ?? is the maximum a posteriori (MAP)
solution for ?:
?? = argmax
?
P (w|?)P (?). (2)
The values of the latent variables are then taken to
be those that maximize P (t|w, ??).
In contrast, the Bayesian approach we advocate in
this paper seeks to identify a distribution over latent
variables directly, without ever fixing particular val-
ues for the model parameters. The distribution over
latent variables given the observed data is obtained
by integrating over all possible values of ?:
P (t|w) =
?
P (t|w, ?)P (?|w)d?. (3)
This distribution can be used in various ways, in-
cluding choosing the MAP assignment to the latent
variables, or estimating expected values for them.
To see why integrating over possible parameter
values can be useful when inducing latent structure,
consider the following example. We are given a
coin, which may be biased (t = 1) or fair (t = 0),
each with probability .5. Let ? be the probability of
heads. If the coin is biased, we assume a uniform
distribution over ?, otherwise ? = .5. We observe
w, the outcomes of 10 coin flips, and we wish to de-
termine whether the coin is biased (i.e. the value of
745
t). Assume that we have a uniform prior on ?, with
p(?) = 1 for all ? ? [0, 1]. First, we apply the stan-
dard methodology of finding the MAP estimate for
? and then selecting the value of t that maximizes
P (t|w, ??). In this case, an elementary calculation
shows that the MAP estimate is ?? = nH/10, where
nH is the number of heads in w (likewise, nT is
the number of tails). Consequently, P (t|w, ??) favors
t = 1 for any sequence that does not contain exactly
five heads, and assigns equal probability to t = 1
and t = 0 for any sequence that does contain exactly
five heads ? a counterintuitive result. In contrast,
using some standard results in Bayesian analysis we
can show that applying Equation 3 yields
P (t = 1|w) = 1/
(
1 + 11!nH !nT !210
)
(4)
which is significantly less than .5 when nH = 5, and
only favors t = 1 for sequences where nH ? 8 or
nH ? 2. This intuitively sensible prediction results
from the fact that the Bayesian approach is sensitive
to the robustness of a choice of t to the value of ?,
as illustrated in Figure 1. Even though a sequence
with nH = 6 yields a MAP estimate of ?? = 0.6
(Figure 1 (a)), P (t = 1|w, ?) is only greater than
0.5 for a small range of ? around ?? (Figure 1 (b)),
meaning that the choice of t = 1 is not very robust to
variation in ?. In contrast, a sequence with nH = 8
favors t = 1 for a wide range of ? around ??. By
integrating over ?, Equation 3 takes into account the
consequences of possible variation in ?.
Another advantage of integrating over ? is that
it permits the use of linguistically appropriate pri-
ors. In many linguistic models, including HMMs,
the distributions over variables are multinomial. For
a multinomial with parameters ? = (?1, . . . , ?K), a
natural choice of prior is the K-dimensional Dirich-
let distribution, which is conjugate to the multino-
mial.1 For simplicity, we initially assume that all
K parameters (also known as hyperparameters) of
the Dirichlet distribution are equal to ?, i.e. the
Dirichlet is symmetric. The value of ? determines
which parameters ? will have high probability: when
? = 1, all parameter values are equally likely; when
? > 1, multinomials that are closer to uniform are
1A prior is conjugate to a distribution if the posterior has the
same form as the prior.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
 
P(
 ? 
| w
 
)
 
 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.5
1
?
 
P(
 
t 
=
 
1 
| w
,
 
? 
)
 
 
 w = HHTHTTHHTH
 w = HHTHHHTHHH
 w = HHTHTTHHTH
 w = HHTHHHTHHH
(a)
(b)
Figure 1: The Bayesian approach to estimating the
value of a latent variable, t, from observed data, w,
chooses a value of t robust to uncertainty in ?. (a)
Posterior distribution on ? given w. (b) Probability
that t = 1 given w and ? as a function of ?.
preferred; and when ? < 1, high probability is as-
signed to sparse multinomials, where one or more
parameters are at or near 0.
Typically, linguistic structures are characterized
by sparse distributions (e.g., POS tags are followed
with high probability by only a few other tags, and
have highly skewed output distributions). Conse-
quently, it makes sense to use a Dirichlet prior with
? < 1. However, as noted by Johnson et al (2007),
this choice of ? leads to difficulties with MAP esti-
mation. For a sequence of draws x = (x1, . . . , xn)
from a multinomial distribution ? with observed
counts n1, . . . , nK , a symmetric Dirichlet(?) prior
over ? yields the MAP estimate ?k = nk+??1n+K(??1) .
When ? ? 1, standard MLE techniques such as
EM can be used to find the MAP estimate simply
by adding ?pseudocounts? of size ? ? 1 to each of
the expected counts nk at each iteration. However,
when ? < 1, the values of ? that set one or more
of the ?k equal to 0 can have infinitely high poste-
rior probability, meaning that MAP estimation can
yield degenerate solutions. If, instead of estimating
?, we integrate over all possible values, we no longer
encounter such difficulties. Instead, the probability
that outcome xi takes value k given previous out-
comes x?i = (x1, . . . , xi?1) is
P (k|x?i, ?) =
?
P (k|?)P (?|x?i, ?) d?
= nk + ?i? 1 + K? (5)
746
where nk is the number of times k occurred in x?i.
See MacKay and Peto (1995) for a derivation.
2.2 Model Definition
Our model has the structure of a standard trigram
HMM, with the addition of symmetric Dirichlet pri-
ors over the transition and output distributions:
ti|ti?1 = t, ti?2 = t?, ? (t,t
?) ? Mult(? (t,t?))
wi|ti = t, ?(t) ? Mult(?(t))
? (t,t?)|? ? Dirichlet(?)
?(t)|? ? Dirichlet(?)
where ti and wi are the ith tag and word. We assume
that sentence boundaries are marked with a distin-
guished tag. For a model with T possible tags, each
of the transition distributions ? (t,t?) has T compo-
nents, and each of the output distributions ?(t) has
Wt components, where Wt is the number of word
types that are permissible outputs for tag t. We will
use ? and ? to refer to the entire transition and out-
put parameter sets. This model assumes that the
prior over state transitions is the same for all his-
tories, and the prior over output distributions is the
same for all states. We relax the latter assumption in
Section 4.
Under this model, Equation 5 gives us
P (ti|t?i, ?) =
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
(6)
P (wi|ti, t?i,w?i, ?) =
n(ti,wi) + ?
n(ti) + Wti?
(7)
where n(ti?2,ti?1,ti) and n(ti,wi) are the number of
occurrences of the trigram (ti?2, ti?1, ti) and the
tag-word pair (ti, wi) in the i ? 1 previously gener-
ated tags and words. Note that, by integrating out
the parameters ? and ?, we induce dependencies
between the variables in the model. The probabil-
ity of generating a particular trigram tag sequence
(likewise, output) depends on the number of times
that sequence (output) has been generated previ-
ously. Importantly, trigrams (and outputs) remain
exchangeable: the probability of a set of trigrams
(outputs) is the same regardless of the order in which
it was generated. The property of exchangeability is
crucial to the inference algorithm we describe next.
2.3 Inference
To perform inference in our model, we use Gibbs
sampling (Geman and Geman, 1984), a stochastic
procedure that produces samples from the posterior
distribution P (t|w, ?, ?) ? P (w|t, ?)P (t|?). We
initialize the tags at random, then iteratively resam-
ple each tag according to its conditional distribution
given the current values of all other tags. Exchange-
ability allows us to treat the current counts of the
other tag trigrams and outputs as ?previous? obser-
vations. The only complication is that resampling
a tag changes the identity of three trigrams at once,
and we must account for this in computing its condi-
tional distribution. The sampling distribution for ti
is given in Figure 2.
In Bayesian statistical inference, multiple samples
from the posterior are often used in order to obtain
statistics such as the expected values of model vari-
ables. For POS tagging, estimates based on multi-
ple samples might be useful if we were interested in,
for example, the probability that two words have the
same tag. However, computing such probabilities
across all pairs of words does not necessarily lead to
a consistent clustering, and the result would be diffi-
cult to evaluate. Using a single sample makes stan-
dard evaluation methods possible, but yields sub-
optimal results because the value for each tag is sam-
pled from a distribution, and some tags will be as-
signed low-probability values. Our solution is to
treat the Gibbs sampler as a stochastic search pro-
cedure with the goal of identifying the MAP tag se-
quence. This can be done using tempering (anneal-
ing), where a temperature of ? is equivalent to rais-
ing the probabilities in the sampling distribution to
the power of 1? . As ? approaches 0, even a single
sample will provide a good MAP estimate.
3 Fixed Hyperparameter Experiments
3.1 Method
Our initial experiments follow in the tradition begun
by Merialdo (1994), using a tag dictionary to con-
strain the possible parts of speech allowed for each
word. (This also fixes Wt, the number of possible
words for tag t.) The dictionary was constructed by
listing, for each word, all tags found for that word in
the entire WSJ treebank. For the experiments in this
section, we used a 24,000-word subset of the tree-
747
P (ti|t?i,w, ?, ?) ?
n(ti,wi) + ?
nti + Wti?
?
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
?
n(ti?1,ti,ti+1) + I(ti?2 = ti?1 = ti = ti+1) + ?
n(ti?1,ti) + I(ti?2 = ti?1 = ti) + T?
?
n(ti,ti+1,ti+2) + I(ti?2 = ti = ti+2, ti?1 = ti+1) + I(ti?1 = ti = ti+1 = ti+2) + ?
n(ti,ti+1) + I(ti?2 = ti, ti?1 = ti+1) + I(ti?1 = ti = ti+1) + T?
Figure 2: Conditional distribution for ti. Here, t?i refers to the current values of all tags except for ti, I(.)
is a function that takes on the value 1 when its argument is true and 0 otherwise, and all counts nx are with
respect to the tag trigrams and tag-word pairs in (t?i,w?i).
bank as our unlabeled training corpus. 54.5% of the
tokens in this corpus have at least two possible tags,
with the average number of tags per token being 2.3.
We varied the values of the hyperparameters ? and
? and evaluated overall tagging accuracy. For com-
parison with our Bayesian HMM (BHMM) in this
and following sections, we also present results from
the Viterbi decoding of an HMM trained using MLE
by running EM to convergence (MLHMM). Where
direct comparison is possible, we list the scores re-
ported by Smith and Eisner (2005) for their condi-
tional random field model trained using contrastive
estimation (CRF/CE).2
For all experiments, we ran our Gibbs sampling
algorithm for 20,000 iterations over the entire data
set. The algorithm was initialized with a random tag
assignment and a temperature of 2, and the temper-
ature was gradually decreased to .08. Since our in-
ference procedure is stochastic, our reported results
are an average over 5 independent runs.
Results from our model for a range of hyperpa-
rameters are presented in Table 1. With the best
choice of hyperparameters (? = .003, ? = 1), we
achieve average tagging accuracy of 86.8%. This
far surpasses the MLHMM performance of 74.5%,
and is closer to the 90.1% accuracy of CRF/CE on
the same data set using oracle parameter selection.
The effects of ?, which determines the probabil-
2Results of CRF/CE depend on the set of features used and
the contrast neighborhood. In all cases, we list the best score
reported for any contrast neighborhood using trigram (but no
spelling) features. To ensure proper comparison, all corpora
used in our experiments consist of the same randomized sets of
sentences used by Smith and Eisner. Note that training on sets
of contiguous sentences from the beginning of the treebank con-
sistently improves our results, often by 1-2 percentage points or
more. MLHMM scores show less difference between random-
ized and contiguous corpora.
Value Value of ?
of ? .001 .003 .01 .03 .1 .3 1.0
.001 85.0 85.7 86.1 86.0 86.2 86.5 86.6
.003 85.5 85.5 85.8 86.6 86.7 86.7 86.8
.01 85.3 85.5 85.6 85.9 86.4 86.4 86.2
.03 85.9 85.8 86.1 86.2 86.6 86.8 86.4
.1 85.2 85.0 85.2 85.1 84.9 85.5 84.9
.3 84.4 84.4 84.6 84.4 84.5 85.7 85.3
1.0 83.1 83.0 83.2 83.3 83.5 83.7 83.9
Table 1: Percentage of words tagged correctly by
BHMM as a function of the hyperparameters ? and
?. Results are averaged over 5 runs on the 24k cor-
pus with full tag dictionary. Standard deviations in
most cases are less than .5.
ity of the transition distributions, are stronger than
the effects of ?, which determines the probability
of the output distributions. The optimal value of
.003 for ? reflects the fact that the true transition
probability matrix for this corpus is indeed sparse.
As ? grows larger, the model prefers more uniform
transition probabilities, which causes it to perform
worse. Although the true output distributions tend to
be sparse as well, the level of sparseness depends on
the tag (consider function words vs. content words
in particular). Therefore, a value of ? that accu-
rately reflects the most probable output distributions
for some tags may be a poor choice for other tags.
This leads to the smaller effect of ?, and suggests
that performance might be improved by selecting a
different ? for each tag, as we do in the next section.
A final point worth noting is that even when
? = ? = 1 (i.e., the Dirichlet priors exert no influ-
ence) the BHMM still performs much better than the
MLHMM. This result underscores the importance
of integrating over model parameters: the BHMM
identifies a sequence of tags that have high proba-
748
bility over a range of parameter values, rather than
choosing tags based on the single best set of para-
meters. The improved results of the BHMM demon-
strate that selecting a sequence that is robust to vari-
ations in the parameters leads to better performance.
4 Hyperparameter Inference
In our initial experiments, we experimented with dif-
ferent fixed values of the hyperparameters and re-
ported results based on their optimal values. How-
ever, choosing hyperparameters in this way is time-
consuming at best and impossible at worst, if there
is no gold standard available. Luckily, the Bayesian
approach allows us to automatically select values
for the hyperparameters by treating them as addi-
tional variables in the model. We augment the model
with priors over the hyperparameters (here, we as-
sume an improper uniform prior), and use a sin-
gle Metropolis-Hastings update (Gilks et al, 1996)
to resample the value of each hyperparameter after
each iteration of the Gibbs sampler. Informally, to
update the value of hyperparameter ?, we sample a
proposed new value ?? from a normal distribution
with ? = ? and ? = .1?. The probability of ac-
cepting the new value depends on the ratio between
P (t|w, ?) and P (t|w, ??) and a term correcting for
the asymmetric proposal distribution.
Performing inference on the hyperparameters al-
lows us to relax the assumption that every tag has
the same prior on its output distribution. In the ex-
periments reported in the following section, we used
two different versions of our model. The first ver-
sion (BHMM1) uses a single value of ? for all word
classes (as above); the second version (BHMM2)
uses a separate ?j for each tag class j.
5 Inferred Hyperparameter Experiments
5.1 Varying corpus size
In this set of experiments, we used the full tag dictio-
nary (as above), but performed inference on the hy-
perparameters. Following Smith and Eisner (2005),
we trained on four different corpora, consisting of
the first 12k, 24k, 48k, and 96k words of the WSJ
corpus. For all corpora, the percentage of ambigu-
ous tokens is 54%-55% and the average number of
tags per token is 2.3. Table 2 shows results for
the various models and a random baseline (averaged
Corpus size
Accuracy 12k 24k 48k 96k
random 64.8 64.6 64.6 64.6
MLHMM 71.3 74.5 76.7 78.3
CRF/CE 86.2 88.6 88.4 89.4
BHMM1 85.8 85.2 83.6 85.0
BHMM2 85.8 84.4 85.7 85.8
? < .7 .2 .6 .2
Table 2: Percentage of words tagged correctly
by the various models on different sized corpora.
BHMM1 and BHMM2 use hyperparameter infer-
ence; CRF/CE uses parameter selection based on an
unlabeled development set. Standard deviations (?)
for the BHMM results fell below those shown for
each corpus size.
over 5 random tag assignments). Hyperparameter
inference leads to slightly lower scores than are ob-
tained by oracle hyperparameter selection, but both
versions of BHMM are still far superior to MLHMM
for all corpus sizes. Not surprisingly, the advantages
of BHMM are most pronounced on the smallest cor-
pus: the effects of parameter integration and sensible
priors are stronger when less evidence is available
from the input. In the limit as corpus size goes to in-
finity, the BHMM and MLHMM will make identical
predictions.
5.2 Varying dictionary knowledge
In unsupervised learning, it is not always reasonable
to assume that a large tag dictionary is available. To
determine the effects of reduced or absent dictionary
information, we ran a set of experiments inspired
by those of Smith and Eisner (2005). First, we col-
lapsed the set of 45 treebank tags onto a smaller set
of 17 (the same set used by Smith and Eisner). We
created a full tag dictionary for this set of tags from
the entire treebank, and also created several reduced
dictionaries. Each reduced dictionary contains the
tag information only for words that appear at least
d times in the training corpus (the 24k corpus, for
these experiments). All other words are fully am-
biguous between all 17 classes. We ran tests with
d = 1, 2, 3, 5, 10, and ? (i.e., knowledge-free syn-
tactic clustering).
With standard accuracy measures, it is difficult to
749
Value of d
Accuracy 1 2 3 5 10 ?
random 69.6 56.7 51.0 45.2 38.6
MLHMM 83.2 70.6 65.5 59.0 50.9
CRF/CE 90.4 77.0 71.7
BHMM1 86.0 76.4 71.0 64.3 58.0
BHMM2 87.3 79.6 65.0 59.2 49.7
? < .2 .8 .6 .3 1.4
VI
random 2.65 3.96 4.38 4.75 5.13 7.29
MLHMM 1.13 2.51 3.00 3.41 3.89 6.50
BHMM1 1.09 2.44 2.82 3.19 3.47 4.30
BHMM2 1.04 1.78 2.31 2.49 2.97 4.04
? < .02 .03 .04 .03 .07 .17
Corpus stats
% ambig. 49.0 61.3 66.3 70.9 75.8 100
tags/token 1.9 4.4 5.5 6.8 8.3 17
Table 3: Percentage of words tagged correctly and
variation of information between clusterings in-
duced by the assigned and gold standard tags as the
amount of information in the dictionary is varied.
Standard deviations (?) for the BHMM results fell
below those shown in each column. The percentage
of ambiguous tokens and average number of tags per
token for each value of d is also shown.
evaluate the quality of a syntactic clustering when
no dictionary is used, since cluster names are inter-
changeable. We therefore introduce another evalua-
tion measure for these experiments, a distance met-
ric on clusterings known as variation of information
(Meila?, 2002). The variation of information (VI) be-
tween two clusterings C (the gold standard) and C ?
(the found clustering) of a set of data points is a sum
of the amount of information lost in moving from C
to C ?, and the amount that must be gained. It is de-
fined in terms of entropy H and mutual information
I: V I(C,C ?) = H(C)+H(C ?)? 2I(C,C ?). Even
when accuracy can be measured, VI may be more in-
formative: two different tag assignments may have
the same accuracy but different VI with respect to
the gold standard if the errors in one assignment are
less consistent than those in the other.
Table 3 gives the results for this set of experi-
ments. One or both versions of BHMM outperform
MLHMM in terms of tag accuracy for all values of
d, although the differences are not as great as in ear-
lier experiments. The differences in VI are more
striking, particularly as the amount of dictionary in-
formation is reduced. When ambiguity is greater,
both versions of BHMM show less confusion with
respect to the true tags than does MLHMM, and
BHMM2 performs the best in all circumstances. The
confusion matrices in Figure 3 provide a more intu-
itive picture of the very different sorts of clusterings
produced by MLHMM and BHMM2 when no tag
dictionary is available. Similar differences hold to a
lesser degree when a partial dictionary is provided.
With MLHMM, different tokens of the same word
type are usually assigned to the same cluster, but
types are assigned to clusters more or less at ran-
dom, and all clusters have approximately the same
number of types (542 on average, with a standard
deviation of 174). The clusters found by BHMM2
tend to be more coherent and more variable in size:
in the 5 runs of BHMM2, the average number of
types per cluster ranged from 436 to 465 (i.e., to-
kens of the same word are spread over fewer clus-
ters than in MLHMM), with a standard deviation
between 460 and 674. Determiners, prepositions,
the possessive marker, and various kinds of punc-
tuation are mostly clustered coherently. Nouns are
spread over a few clusters, partly due to a distinction
found between common and proper nouns. Like-
wise, modal verbs and the copula are mostly sep-
arated from other verbs. Errors are often sensible:
adjectives and nouns are frequently confused, as are
verbs and adverbs.
The kinds of results produced by BHMM1 and
BHMM2 are more similar to each other than to
the results of MLHMM, but the differences are still
informative. Recall that BHMM1 learns a single
value for ? that is used for all output distribu-
tions, while BHMM2 learns separate hyperparame-
ters for each cluster. This leads to different treat-
ments of difficult-to-classify low-frequency items.
In BHMM1, these items tend to be spread evenly
among all clusters, so that all clusters have simi-
larly sparse output distributions. In BHMM2, the
system creates one or two clusters consisting en-
tirely of very infrequent items, where the priors on
these clusters strongly prefer uniform outputs, and
all other clusters prefer extremely sparse outputs
(and are more coherent than in BHMM1). This
explains the difference in VI between the two sys-
tems, as well as the higher accuracy of BHMM1
for d ? 3: the single ? discourages placing low-
frequency items in their own cluster, so they are
more likely to be clustered with items that have sim-
750
1 2 3 4 5 6 7 8 9 1011121314151617
N
INPUNC
ADJ
V
DET
PREP
ENDPUNC
VBG
CONJ
VBN
ADV
TO
WH
PRT
POS
 LPUNC
RPUNC
 (a) BHMM2
Found Tags
Tr
ue
 T
ag
s
1 2 3 4 5 6 7 8 9 1011121314151617
N
INPUNC
ADJ
V
DET
PREP
ENDPUNC
VBG
CONJ
VBN
ADV
TO
WH
PRT
POS
 LPUNC
RPUNC
 (b) MLHMM
Found Tags
Tr
ue
 T
ag
s
Figure 3: Confusion matrices for the dictionary-free clusterings found by (a) BHMM2 and (b) MLHMM.
ilar transition probabilities. The problem of junk
clusters in BHMM2 might be alleviated by using a
non-uniform prior over the hyperparameters to en-
courage some degree of sparsity in all clusters.
6 Conclusion
In this paper, we have demonstrated that, for a stan-
dard trigram HMM, taking a Bayesian approach
to POS tagging dramatically improves performance
over maximum-likelihood estimation. Integrating
over possible parameter values leads to more robust
solutions and allows the use of priors favoring sparse
distributions. The Bayesian approach is particularly
helpful when learning is less constrained, either be-
cause less data is available or because dictionary
information is limited or absent. For knowledge-
free clustering, our approach can also be extended
through the use of infinite models so that the num-
ber of clusters need not be specified in advance. We
hope that our success with POS tagging will inspire
further research into Bayesian methods for other nat-
ural language learning tasks.
References
M. Banko and R. Moore. 2004. A study of unsupervised part-
of-speech tagging. In Proceedings of COLING ?04.
E. Brill. 1995. Unsupervised learning of disambiguation rules
for part of speech tagging. In Proceedings of the 3rd Work-
shop on Very Large Corpora, pages 1?13.
P. Brown, V. Della Pietra, V. de Souza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural language.
Computational Linguistics, 18:467?479.
A. Clark. 2000. Inducing syntactic categories by context dis-
tribution clustering. In Proceedings of the Conference on
Natural Language Learning (CONLL).
S. Finch, N. Chater, and M. Redington. 1995. Acquiring syn-
tactic information from distributional statistics. In J. In Levy,
D. Bairaktaris, J. Bullinaria, and P. Cairns, editors, Connec-
tionist Models of Memory and Language. UCL Press, Lon-
don.
S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs
distributions and the Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
6:721?741.
W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors.
1996. Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In Proceedings of HLT-NAACL.
M. Johnson, T. Griffiths, and S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo.
D. Klein and C. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proceed-
ings of the ACL.
D. MacKay and L. Bauman Peto. 1995. A hierarchical Dirich-
let language model. Natural Language Engineering, 1:289?
307.
M. Meila?. 2002. Comparing clusterings. Technical Report 418,
University of Washington Statistics Department.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?172.
L. Saul and F. Pereira. 1997. Aggregate and mixed-order
markov models for statistical language processing. In Pro-
ceedings of the Second Conference on Empirical Methods in
Natural Language Processing (EMNLP).
H. Schu?tze. 1995. Distributional part-of-speech tagging. In
Proceedings of the European Chapter of the Association for
Computational Linguistics (EACL).
N. Smith and J. Eisner. 2005. Contrastive estimation: Training
log-linear models on unlabeled data. In Proceedings of ACL.
I. Wang and D. Schuurmans. 2005. Improved estimation
for unsupervised part-of-speech tagging. In Proceedings
of the IEEE International Conference on Natural Language
Processing and Knowledge Engineering (IEEE NLP-KE).
751
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 49?57,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Exploring the relationship between learnability and linguistic universals
Anna N. Rafferty (rafferty@cs.berkeley.edu)
Computer Science Division, University of California, Berkeley, CA 94720 USA
Thomas L. Griffiths (tom griffiths@berkeley.edu)
Department of Psychology, University of California, Berkeley, CA 94720 USA
Marc Ettlinger (marc@northwestern.edu)
Department of Communication Sciences and Disorders
Northwestern University, Evanston, IL 60208 USA
Abstract
Greater learnability has been offered as an ex-
planation as to why certain properties appear
in human languages more frequently than oth-
ers. Languages with greater learnability are
more likely to be accurately transmitted from
one generation of learners to the next. We ex-
plore whether such a learnability bias is suffi-
cient to result in a property becoming preva-
lent across languages by formalizing language
transmission using a linear model. We then
examine the outcome of repeated transmission
of languages using a mathematical analysis, a
computer simulation, and an experiment with
human participants, and show several ways in
which greater learnability may not result in a
property becoming prevalent. Both the ways
in which transmission failures occur and the
relative number of languages with and with-
out a property can affect whether the rela-
tionship between learnability and prevalence
holds. Our results show that simply finding
a learnability bias is not sufficient to explain
why a particular property is a linguistic univer-
sal, or even frequent among human languages.
1 Introduction
A comparison of languages around the world reveals
that certain properties are far more frequent than
others, which are taken to reflect linguistic univer-
sals (Greenberg, 1963; Comrie, 1981; Croft, 2002).
Understanding the origins of linguistic universals is
an important project for linguistics, and understand-
ing how they relate to human cognitive processes
is an important project for cognitive science. One
prominent explanation for the existence of these pat-
terns is the presence of cognitive biases that make
certain properties of language more easily learned
than others (Slobin, 1973; Wilson, 2003; Finley &
Badecker, 2007; Wilson, 2006). Under this hypothe-
sis, certain properties are common across languages
because they are more easily learned than others (a
learnability bias) and are therefore more likely to be
maintained when a language is passed from one gen-
eration to the next. These universals generally reflect
tendencies, rather than properties that are present in
each and every language (Croft, 2002).
Recent work in psycholinguistics has provided
support for a relationship between learnability bi-
ases and the properties that are prevalent in human
languages. A number of studies have shown that cer-
tain common phonological patterns, such as vowel
harmony, voicing agreement and final devoicing are,
indeed, more learnable than other unattested patterns
(Finley & Badecker, 2007; Moreton, 2008; Becker,
Ketrez, & Nevins, 2011). Based on these findings,
it is tempting to argue that learnability biases alone
might account for the prevalence of these proper-
ties in human languages. However, this argument
assumes that more accurate learning of a language
with a certain property is sufficient for that property
to become widespread across languages and does
not account for why a property might be prevalent
but not universal across languages.
In this paper, we examine the assumption that
greater learnability is sufficient for a property to be-
come prevalent. We formalize language transmis-
sion using a simple linear model, and then show two
basic scenarios in which greater learnability for a
particular language does not result in that language
becoming prevalent. We first perform a mathemat-
ical analysis to show that one way this can occur
is for errors in transmission to favor particular lan-
49
guages over others. We next use a simulation to
show another scenario in which greater learnabil-
ity can fail to result in a dominant pattern: when
the number of alternative languages is large. We
conduct two experiments with human participants to
illustrate the occurrence of this second scenario in
the case of a particular property of human language,
vowel harmony.
2 Linking Learnability and Transmission
Languages change over time due to transmission
from generation to generation (e.g., Labov, 2001).
Our goal is to understand how long-term trends of
language change are related to cognitive, perceptual,
and production biases observed in a single instance
of transmission. We begin by formalizing transmis-
sion using a general mathematical model in order
to uncover what long term trends emerge given that
certain languages are more likely to be accurately
transmitted than others.
We use a linear model of cultural transmission,
in which it is assumed that each person learns a lan-
guage from utterances produced by one person in the
previous generation. This linear model of transmis-
sion has many specific instantiations in the literature
on language evolution, such as the iterated learn-
ing model (Kirby, 2001; Griffiths & Kalish, 2007)
or the replicator dynamics (Schuster & Sigmund,
1983; Komarova & Nowak, 2003). To specify this
model, we first define the set of possible languages,
denoted H. Each element h ? H is one possible lan-
guage. Transmission occurs when a new member of
the population receives linguistic data (a set of utter-
ances) from another member of the population and
learns a language h ? H. We assume transmission
occurs only from one person to another person, and
that each person learns only one language. For ex-
ample, someone who knows language j might speak
to another member of the population, and based on
hearing those utterances, the learner might also learn
the language j. Alternatively, the learner might learn
another language: The learner might not have heard
enough language to fully specify j as the language
or might have misheard something, and thus simply
infers another language i that is consistent with the
data she or he heard. More generally, we assume
that for all i, j ? H, qi j is the probability that some-
Q =
?
?
0.90 0.05 0.07
0.04 0.76 0.08
0.06 0.19 0.85
?
?
(a)
(b) (c)
? =
?
?
0.39
0.20
0.41
?
?
This process may continue indefinitely, with the tth learner re-
ceiving the output of the (t ?1)th learner. The iterated learn-
ing models we analyze make the simplifying assumptions
that language evolution occurs in only one direction (previ-
ous generations do not change their hypotheses based on the
data produced by future generations) and that each learner re-
ceives input from only one previous learner. We first charac-
terize how learning occurs, independent of specific represen-
tation, and then give a more detailed d scription of the form
of these hypotheses and data.
Our models assume that learners represent (or act as if they
represent) the degree to which const ints predispose them to
certain hypotheses about language through a probability dis-
tribution over hypotheses, and that they combine these pre-
dispositions with information from the dat using Bayesian
inference. Starting with a prior distribution over hypotheses
p(h) for all hypotheses h in a hypothesis space H, the pos-
terior distribution over hypotheses given data d is giv n by
Bayes? rule,
p(h|d) =
p(d|h)p(h)
?h??H p(d|h?)p(h?)
(1)
where the likelihood p(d|h) indicates the probability of see-
ing d under hypothesis h. The learners thus shape the lan-
guage they are learning through their own bias s in the form
of the prior probabilities: the prior p(h) incorporates the hu-
man learning constraints. These probabilities might, for ex-
ample, tend to favor lword forms wi h alternating c so ant-
vowel phonemes. We assume that learners? expectations
about the distribution of the data given the hypothesis are
consistent with the actual distribution (i.e. that the probabil-
ity of the previous learner generating data d from hypothesis
h matches the likelihood function p(d|h)). Finally, we as-
sume that learners choose a hypothesis by sampling from the
posterior distribution (although we consider other ways of se-
lecting hypotheses in the Discussion section).1
The analyses we present in this paper are based on the ob-
servation that iterated learning defines a Markov chain. A
Markov chain is a sequence of random variables Xt such that
each Xt is independent of all preceding variables when condi-
tioned on the immediately preceding variable, Xt?1. Thus,
p(xt |x1, . . . ,xt?1) = p(xt |xt?1). There are several ways of
reducing iterated learning to a Markov chain (Griffiths &
Kalish, 2007). We will focus on the Markov chain on hy-
potheses, where transitions from one state to another occur
each generation: the tth learner assumes the data were gen-
erated by ht , where these data are dependent only on the
hypothesis ht?1 chosen by the previous learner. The transi-
tion probabilities for this Markov chain are obtained by sum-
ming over the data from the previous time step di?1, with
p(ht |ht?1) = ?di?1 p(ht |di?1)p(di?1|ht?1) (see Figure 1).
Identifying iterated learning as a Markov chain allows us to
draw on mathematical results concerning the convergence of
1Note that these various probabilities form our model of the
learners. Learners need not actually hold them explicitly, nor per-
form the exact computations, provided that they act as if they do.
data ...
(a)
(c)
...(b)
...
data
hypothesis hypothesis
data
?d p(h|d)p(d|h)
d0 h2d1h1
p(h|d) p(d|h) p(h|d) p(d|h)
d2
h2h1
?d p(h|d)p(d|h)
Figure 1: Language evolution by iterated learning. (a) Each
learner sees data, forms a hypothesis, and generates the data
provided to the next learner. (b) The underlying stochastic
process, with dt and ht being the data generated by the tth
learner and the hypothesis selected by that learner respec-
tively. (c) We consider the Markov chain over hypotheses
formed by summing over the data variables. All learners
share the same prior p(h), and each learner assumes the input
data were created using the same p(d|h).
Markov chains. In particular, Markov chains can converge to
a stationary distribution, meaning that after some number of
generations t, the marginal probability that a variable Xt takes
value xt becomes fixed and independent of the value of the
first variable in the chain (Norris, 1997). Intuitively, the sta-
tionary distribution is a distribution over states in which the
probability of each state is not affected by further iterations
of the Markov chain; in our case, the probability that a learner
learns a specific grammar at time t is equal to the probability
of any future learner learning that grammar. The stationary
distribution is thus an equilibrium state that iterated learn-
ing will eventually reach, regardless of the hypothesis of the
first ancestral learner, provided simple technical conditions
are satisfied (see Griffiths & Kalish, 2007, for details).
Previous work has shown that the stationary distribution
of the Markov chain defined by Bayesian learners sampling
from the posterior is the learners? prior distribution over hy-
potheses, p(h) (Griffiths & Kalish, 2007). These results illus-
trate how constraints on learning can influence the languages
that people come to speak, indicating that it is possible for
iterated learning to converge to an equilibrium that is deter-
mined by these constraints and independent of the language
spoken by the first learner in the chain.
However, characterizing the stationary distribution of iter-
ated learning still leaves open the question of whether enough
generations of learning have occurred for convergence to this
distribution to have taken place in human languages. To un-
derstand the degree to which linguistic universals reflect con-
straints on learning rather than descent from a common ances-
tor, it is necessary to establish bounds on convergence time.
Previous work has identified factors influencing the rate of
convergence in very simple settings (e.g., Griffiths & Kalish,
2007). Our contribution is to provide analytic upper bounds
on the convergence time of iterated learning with relatively
complex representations of the structure of a language that
are consistent with linguistic theories.
Q
language language
t
Figure 1: (a) A general model of the cultural transmis-
sion of languages. A language is passed from one learner
to another, and the matrix Q encodes the probability a
learner will learn a particular language i from someone
who knows language j. (b) An example transition matrix
Q with three states. (c) The solution to the eigenvector
equation Qpi = pi for this transition matrix. pi gives the
equilibrium probability that a learner will learn a particu-
lar language when languages are transmitted via a process
that has transition matrix Q.
one will learn language i from someone who knows
language j. These can be encoded in a transition
matrix Q where the (i, j)th entry of the matrix cor-
responds to qi j (see Figure 1).
Using this framework, we can formally define
learnability biases and determine whether learn-
ability bias for some property necessarily implies
that this property will be present in the majority
of languages. As mentioned previously, we define
learnability bias to mean that one type of language
is more likely to be transmitted accurately to the next
generation than another; this is similar to the notion
of ?cognitive bias? discussed in Wilson (2003) and
is what is tested in experiments. Formally, a learn-
ability bias for some language i over some other
language j means that qii > q j j. For example, one
might expose one group of learners to language i and
another group to language j. If more le rners in the
first group accurately learned the language they were
exposed to, this would indicate a learnability bias for
language i over language j.
We can extend the idea of a learnability bias to
a property of a language, rather than a specific lan-
guage, by applying a similar definition to sets of lan-
guages. Imagine there are two sets of languages, H1
and H2. These sets might be defined by classifying
50
all languages with a particular property in H1 and
all languages without the property in H2. One way
of defining a learnability bias that favors a particular
property is for each language with that property to be
more likely to be transmitted successfully than each
language without that property. That is, for all pos-
sible pairs i ? H1 and j ? H2, qii > q j j. This would
indicate a general learnability bias for languages in
H1 over languages in H2.
Using this definition of a learnability bias, we can
determine whether such a bias is sufficient to estab-
lish that the property will be present in the majority
of languages. That is, if H1 denotes the languages
with the property of interest, we want to determine
whether a learnability bias for languages in H1 im-
plies that after many generations, the majority of the
languages in the population will be in H1 and not
in H2. We can determine the consequences of many
instances of language transmission in this model by
appealing to existing results on the equilibrium of
this linear dynamical system. As mentioned above,
this linear transmission model is related to two kinds
of models that have been used to study language
evolution: If we assume that learners are organized
in a chain, this linear model is called iterated learn-
ing (Kirby, 2001); alternatively, if we assume that
there are an infinite number of learners in the pop-
ulation, the model is called the replicator dynam-
ics (Schuster & Sigmund, 1983). In either case, the
probability that a learner will learn language h, as-
suming the population has reached equilibrium, is
given by the solution to the eigenvector equation
Qpi = pi, normalized such that ?ni=1pii = 1 (for de-
tails, see Griffiths & Kalish, 2007). For languages in
H1 to occur the majority of the time, it thus must be
the case that ?h?H1 pih > ?h?H2 pih.
We can now identify one context in which a learn-
ability bias is not sufficient to ensure that a property
will appear in the majority of languages. Consider
the example transition matrix Q shown in Figure 1
(b). Let H1 = {s1} and H2 = {s2,s3}, where each
state si represents a distinct language. We have that
q11 > qii for all i ? H2: each state in H2 has a lower
self transition probability than state s1, the only state
in H1. Thus, we have a learnability bias for state
s1 over all states in H2. However, the eigenvector
pi shown in Figure 1 (c) indicates that the equilib-
rium of this system, which will be reached after lan-
guages are transmitted from person to person many
times, favors state s3 over the other states. Overall,
?h?H1 pih = 0.39 while ?h?H2 pih = 0.61: most of thelearners will learn a language in H2.1
Intuitively, this result comes from the fact that
transmission failures tend to favor languages in H2.
A learner who learns from someone who speaks a
language i in H2 will rarely learn the language in
H1, although she may learn a different language than
i in H2. This pattern of transmission failures over-
whelms the learnability bias that the language in H1
has over the languages in H2. Note that this pattern
holds even given that q1i > qi1 for all i?H2, another
common criterion for a learnability bias.
This result implies that if the linear transmission
model is an accurate model for understanding hu-
man language evolution, then it is not sufficient to
compare how accurately languages are maintained
over a single generation in order to predict what
trends will emerge after many generations. Instead,
one must also look at what happens when languages
are not maintained accurately. The ways in which
mutations occur may be as important as the relative
fidelities of transmission in determining long term
trends. When one only looks for a learnability bias,
the rate of different mutations is not accounted for,
leaving open the possibility that predictions about
long term trends will be incorrect.
3 Simulating Language Transmission
In the previous section, we used a simple linear
transmission model to identify one context in which
a learnability bias is not sufficient for languages with
a certain property to become prevalent. We now ex-
plore a second context in which a learnability bias
is not sufficient to guarantee that languages with a
particular property become prevalent, using a sim-
ulation of language transmission. We use an iter-
ated learning model in which our representation of
language is inspired by the principles and parame-
ters approach (Chomsky & Lasnik, 1993). Rafferty,
Griffiths, and Klein (2009) present a model similar
to the one we consider here and show that compa-
1While one might try to resolve this issue by collapsing all
languages in H2 into a single state in the Markov chain, such
a transformation is possible only in cases where qi j = qik for
all languages j,k ? H2 and i /? H2 (Burke & Rosenblatt, 1958;
Kemeny & Snell, 1960).
51
0 200 400 600 800 10000
512
1024
Lang
uage
Samples From Transition Matrix
0 256 512 768 10240
3500
7000
Freq
uenc
y
Language Frequency
Other Target0
5000
10000
Freq
uenc
y
Relative Frequency of Target Language
0 200 400 600 800 10000
512
1024
Sample Iteration
Lang
uage
0 256 512 768 1024
3500
0
7000
Language
Freq
uenc
y
Other Target0
5000
10000
Freq
uenc
y
? = 0.6
? = 0.1
Figure 2: Model results for the frequency of the target language based on adjusting the bias towards that hypothesis.
The rows in the above figure correspond to two possible values of ?; larger ? results in a higher prior probability on
the target language. The leftmost column shows 1,000 samples from the transition matrix, with black x marks corre-
sponding to occurrences of the target language. The middle column corresponds to the frequency of each language in
the full 10,000 samples; the rightmost bar in each figure corresponds to the target language. The rightmost column
shows the frequency of the target language versus all other languages for the same 10,000 samples.
rable results hold using other representations of lan-
guage, such as those based on optimality theory.
In order to define the transition matrix Q, we need
to specify the process by which learners select a lan-
guage. We assume that learners are Bayesian, mean-
ing that they infer a language h based on the data d
that they receive according to Bayes? rule. The pos-
terior probability assigned to h after observing d is
p(h|d) ? p(d|h)p(h), where p(d|h) (the likelihood)
indicates the probability of d being generated from
h, and p(h) (the prior) indicates the extent to which
the learner was biased towards h before observing d.
If we assume learners select hypotheses with proba-
bility equal to their posterior probability, we obtain
a transition matrix Q with entries
qi j = p(h(t+1) = i|h(t) = j)
=?
d
p(h(t+1) = i|d)p(d|h(t) = j)
where h(t) and h(t+1) are the languages of learners at
iterations t and t +1 respectively.
To represent languages, we use binary vectors of
length N. Each place corresponds to the setting for
a particular parameter. We consider one particular
setting of the parameters to be the target language
and include a learnability bias for this language in
the model; we then look at whether this language
is more prevalent than other languages after many
transmissions. In the iterated learning model that we
use, learners are organized into a chain, with each
learner learning from data generated by the previous
learner (Kirby, 2001). The previous learner gener-
ates k pieces of data that match her or his language.
These pieces of data each specify the correct param-
eter setting for one of the properties represented by
the binary vector. The other N?k properties are left
unspecified in the data given to the next learner.
In order to define the transition probability be-
tween languages, we need to define the two terms
in Bayes rule: the prior p(h) and the likelihood
p(d|h). Intuitively, the prior probability distribution
over languages corresponds to how much evidence
is required for the learner to learn each hypothesis.
If one hypothesis has a very high prior probability,
only a small amount of evidence will be required to
convince the learner that that hypothesis is the cor-
rect one. By controlling the prior probability of the
target language versus the other languages, we can
manipulate the learnability bias for the target lan-
guage. We thus set the prior probability of the target
language to ? and then divide the remaining proba-
bility mass of 1?? uniformly across all of the lan-
guages (including the target language). The param-
eter ? thus controls the strength of the learnability
bias for the target language, but this language is al-
ways favored for any ? greater than 0.
The likelihood p(d|h) reflects the probability that
a given hypothesis h would produce data d. We as-
sume d is a string of length N that contains 0s, 1s,
and ?s. A ??? in the ith position means that no in-
formation was given about the ith property. We also
assume there is a probability ? that the chosen lan-
guage will not match the data at each position; that
is, with probability ?, the language chosen by the
52
learner will have a 1 in the ith spot if the data had a
0 in that spot. This gives:
p(d|h) = ?Ni=1,di 6=? ?I(h`di)(1? ?)I(h0di)
where h ` di means that h has the same setting of the
ith property as di.
Given these specifications for the prior and the
likelihood, we can calculate the 2N ? 2N transition
matrix and sample from this matrix to simulate a
sequence of learners each learning a language from
the utterances produced by the previous learner. We
let N = 10 and k = 5. As shown in Griffiths and
Kalish (2007), in this model ? iterated learning with
Bayesian learners ? the equilibrium pi is simply the
prior distribution p(h). The distribution over lan-
guages is thus unaffected by the error parameter ?;
this parameter only affects the time to reach equilib-
rium (Rafferty et al, 2009). We present results using
? = 0.25. Figure 2 shows how relative frequency of
the target language is affected by changing the pa-
rameter ?, using ? = 0.6 and ? = 0.1. Frequencies
are based on taking 11,000 samples from the ma-
trix and discarding the first 1,000 to ensure that the
population had reached equilibrium.
The middle column of Figure 2 shows that the
frequency with which learners chose the target lan-
guage was greater than that of the other languages
for both values of ?. This is consistent with the
target language having a higher prior probability
than other languages. However, depending on the
strength of the bias, this language may still not be
chosen the majority of the time, as shown in the
rightmost column of Figure 2. When ? is large,
its probability overwhelms that of its competitors.
However, if ? is relatively small, the combined fre-
quencies of all other languages exceed that of the
target language. Thus, despite being favored by a
learnability bias, the target language is not chosen by
the majority of learners. Like the previous example,
this simulation demonstrates that learnability biases
may not always lead to accurate prediction of long
term trends. More specifically, it highlights that one
must consider the size of the comparison set: If there
are many alternate possible languages, learners may
tend to learn one of these languages even if some
particular language with a learnability bias is more
frequent than any other given individual language.
4 Language Transmission in the Lab
While we have shown two scenarios in which a sim-
ple linear transmission model does not predict that
learnability biases will necessarily lead to linguistic
universals, human learners are not necessarily con-
sistent with this model and could follow a differ-
ent pattern. Thus, we conducted two experiments
to determine if the same dissociation between in-
dividual bias and long-term change can be shown
when teaching human learners an artificial gram-
mar. In Experiment 1, we establish a learnability
bias for a linguistic pattern that is common in the
world?s languages over an arbitrary pattern. In Ex-
periment 2, we explore what happens when a lan-
guage with the common pattern is transmitted mul-
tiple times among learners in the lab. Each learner
learns a language and then produces data from this
language to teach the next learner. By examining the
languages that emerge after several transmissions,
we will show that the learnability bias in Experi-
ment 1 does not translate to the pattern becoming
widespread across the learned languages in Experi-
ment 2. This pattern is an instance of the scenario
in which the many alternative languages overwhelm
the language with the learnability bias.
In our experiments, we use the property of vowel
harmony. Relatively common across the world?s
languages (van der Hulst & van de Weijer, 1995),
vowel harmony is a linguistic pattern wherein the
vowels in words in a language must share some
phonological feature. For example, in Turkish, the
plural suffix is -lar in bash-lar ?heads?, but -ler in
bebek-ler ?babies? so as to adhere to the requirement
that words are front-back harmonic. In the former,
both vowels are back vowels and in the latter, both
vowels are front vowels. Harmony is well-suited for
use in this case because English speakers have no fa-
miliarity with vowel harmony from their native lan-
guage input and because previous work has shown
that typologically attested vowel harmony patterns
are generally more easily learned (Moreton, 2008;
Finley & Badecker, 2009).
5 Experiment 1: Establishing a Bias
5.1 Methods
Participants. There were 40 participants who
received either monetary compensation or course
53
credit for their participation. All were native speak-
ers of English.
Stimuli. A native speaker of English was recorded
saying 160 CVCVC words. Each word began with
one of 80 CVC stems, twenty each with the vow-
els /i/, /e/, /u/ and /o/ and random consonants.
Each stem was recorded with both variants, or al-
lomorphs, of a suffix, [it] and [ut]. Thus, half the
words were front-harmonic (e.g., pel-it, bis-it) and
half were front-disharmonic (e.g., pel-ut, bis-ut).
Procedure. The procedure followed a modified arti-
ficial grammar paradigm. Participants were assigned
to one of two conditions: the harmonic condition
or the height-front dependency condition, which is
unattested. In both conditions, participants were ex-
posed in training to 40 words from the language
they were learning. In the harmonic condition, 40
harmonic words were selected. In the height-front
dependency condition, words were selected such
that mid-vowel stems received the front vowel suffix
(e.g., pel-it, bod-it) and high-vowel stems received
the back-vowel suffix (e.g., bis-ut, tug-ut). This rule
was chosen arbitrarily from the space of possible
languages to test the hypothesis that vowel harmony
would have a learnability bias over other patterns.
Participants were familiarized with the words in
the same way regardless of condition. They were
given alternating blocks of passive listening and
blocks in which for each trial, two words were
played and they were required to choose which word
they had previously heard. In the forced choice tri-
als, the choice was between a word that had been
played in the passive listening section and a word
with the same prefix and the alternate allomorph. A
total of five blocks of 40 trials each were included in
training: three passive listening blocks with a forced
choice block in between each.
Following the training trials, participants com-
pleted one block of 80 test trials. On each test
trial, participants were asked to choose which of
two words they thought was from the language they
had learned in the training trials. In each trial, the
two words both had the same stem and differed in
the suffix. 40 of the test trials included words from
training, and 40 were generalization trials involving
novel words.
Height?Frontness Harmony0
0.5
1
Proporti
on of Ge
neraliza
tions
Proportion of Generalizations Following Training Set Rule   
Height?Frontness Harmony0
0.5
1
Test Ac
curacy
Test Accuracy by Training Set Rule
Figure 3: Results for harmonic versus height-frontness
rule conditions. By condition, there are significant dif-
ferences in the proportion of generalizations following
the rule (0.70 for harmony rule versus 0.57 for height-
frontness rule, t(38) = 2.05, p < 0.05; left) and in test
accuracy (0.80 for harmony rule versus 0.68 for height-
frontness rule, t(38) = 2.23, p< 0.05; right).
5.2 Results
As shown in Figure 3, we found a learnability bias
for the harmonic language. Learners had signifi-
cantly greater accuracy in test when they learned the
vowel harmonic language than when they learned
the height-front dependency language (80% correct
for learners of the harmony rule versus 68% cor-
rect for the height-frontness rule, t(38) = 2.23, p <
0.05). Additionally, 70% of generalizations made
by learners in the harmony rule condition followed
the harmonic rule while only 57% of generaliza-
tions made by learners in the height-front depen-
dency condition followed the height-frontness rule
(t(38) = 2.05, p < 0.05).2 The result of these two
phenomena was that the final languages produced by
the learners in the harmony condition had a greater
prevalence of harmonic words than the final lan-
guages of learners in the height-frontness depen-
dency had of adhering words.
These results establish that the probability of tran-
sitioning from a harmonic language to another lan-
guage with a high proportion of harmonic words
is higher than the probability of transitioning from
a height-front dependency language to another lan-
guage with a high proportion of adhering words. In
2For the second experiment, participants who had low ac-
curacy (< 62.5% of previously heard words chosen in test as
?from the language?) were excluded. Performing this exclusion
in this experiment preserves the same results: Mean accuracy
of 87% for the harmonic condition versus 73% for the height-
front dependency condition (t(28) = 2.74, p< 0.025), and 77%
mean proportion of generalizations following the rule for the
harmonic condition versus 58% for the height-front dependency
condition (t(28) = 2.43, p< 0.025). This exclusion criterion re-
sulted in removing five participants from each condition.
54
terms of the transition matrix, this corresponds to
q`harm,`harm > q`h-f,`h-f , where `harm is the set of lan-
guages with a high proportion of harmonic words
and `h-f is the set of languages with a high propor-
tion of words that follow the height-frontness rule.
In other words, the harmonic language is easier to
learn than the height-front dependency language.
6 Experiment 2: Language Transmission
6.1 Methods
Participants. There were a total of 104 partici-
pants who received either monetary compensation or
course credit for their participation. All were native
speakers of English.
Stimuli. The same stimuli were used as in Experi-
ment 1.
Procedure. The procedure for this experiment was
similar to Experiment 1, but the way that words were
chosen for training differed. For the first subject in
each chain, a total of 40 prefixes were selected at
random, and based on the starting condition of the
chain, the allophone for each prefix was selected.
For example, for the 50% harmonic starting con-
dition, 40 prefixes were chosen and of those pre-
fixes, half were chosen to have the appropriate al-
lophone to make the word harmonic and half were
chosen to have the allophone to make the word non-
harmonic. For subsequent subjects in each chain,
40 words were chosen at random from those words
which the previous subject had said was in the lan-
guage. In order to exclude subjects who had not ac-
tually learned the language in training, subjects were
not included in the chain if their accuracy in test on
previously seen words was below 62.5%; this is the
lowest level of accuracy that is significantly differ-
ent (binomial test, p < 0.05) from chance guessing.
Chains were started at 100%, 75%, 50%, 25%, and
0% harmonic. One chain with 10 subjects was run
for each starting point except for 100%. Four chains
of 10 subjects each were run at this starting point as
this is the point of most interest: given a learnabil-
ity bias, does the percentage of harmonic words in a
language remain consistently large?
6.2 Results
While Experiment 1 showed a learnability bias for
the harmonic language over an arbitrarily chosen
language, the iterated learning chains in Experiment
2 did not favor the harmonic language. As shown
in Figure 4, all chains tended toward languages with
approximately 50% harmonic words, and after sev-
eral generations, the chains that began with 100%
harmonic words did not differ significantly from the
other chains. There is also no difference in accuracy
on the harmonic items over time, as shown in Figure
5. This is empirical evidence that the pattern shown
in simulation can also occur with human learners:
One language is more accurately transmitted than
others, but due to the large number of other possi-
ble languages, this language does not predominate
after many transmissions.
7 General Discussion
In this paper, we formalized language transmission
using a linear model in order to examine whether
a learnability bias for some property of language is
sufficient for that property to become prevalent in
human languages. We showed two ways in which
a learnability bias for a property can exist but not
cause that property to become prevalent. First, using
a mathematical analysis, we showed that this can oc-
cur when transmission failures favor languages other
than those that have greater learnability. This illus-
trates the importance of considering the entire trans-
mission matrix, not just the probabilities of accurate
transmissions that are considered when establishing
a learnability bias.
Second, we showed that it is possible for the sheer
number of other possible languages to overwhelm
greater learnability for a particular language. We
then illustrated that this second scenario might lead
to incorrect predictions in an experimental context.
In artificial language experiments, greater learnabil-
ity is often established by comparing the accuracy
of transmission for a language with the property of
interest to an arbitrary language. However, in our
experiment, we established such a learnability bias
for vowel harmony, but this did not result in vowel
harmony being maintained after many instances of
transmission. This result seems to be due to the fact
that numerous languages other than harmonic lan-
guages were possible, so learners tended to learn one
of these many other languages.
One limitation of our analysis is the use of the
55
1 2 3 4 5 6 7 8 9 100.2
0.4
0.6
0.8
1.0
Prop
ortio
n Ha
rmon
ic Proportion of Harmonic Words Chosen in Test
 
 100%75%50%25%0%
1 2 3 4 5 6 7 8 9 100.2
0.4
0.6
0.8
1.0 Proportion of Generalizations in Test that were Harmonic
Iteration in ChainP
ropo
rtion
 Har
mon
ic
 
 
100%75%50%25%0%
Figure 4: Iterated learning chain results. Dotted lines show the two-tailed 95% confidence interval for chance re-
sponding; confidence intervals differ between the two graphs because there are 40 opportunities to generalize versus
80 opportunities to choose harmonic words.
1 2 3 4 5 6 7 8 9 100.2
0.40.6
0.81.0
Accu
racy
Accuracy for Harmonic Words
 
 
1 2 3 4 5 6 7 8 9 100.2
0.40.6
0.81.0
Iteration in Chain
Accu
racy
Accuracy for Non?Harmonic Words
 
 100%75%50%25%0%
100%75%50%25%0%
Figure 5: Accuracy on harmonic versus non-harmonic words by iteration. Overall, there is no difference in accuracy.
simple linear transmission model, in which each
learner learns from one member of the previous gen-
eration. It is easy to imagine variants on this model
that make more realistic assumptions about cultural
transmission of languages. However, we suspect
that these more complex models would not alter the
conclusions that we have drawn here. For exam-
ple, learning from multiple members of the previous
generation tends to dilute the effects of learnability
on the languages produced by a population (Smith,
2009; Burkett & Griffiths, 2010).
Overall, the result of a more complicated relation-
ship between learnability biases and linguistic uni-
versals is congruent with the evidence that all lan-
guages do not exhibit all properties for which learn-
ability biases have been found. Indeed, in histori-
cal linguistics, the general principle is one of lan-
guage divergence, rather than convergence on some
universal language (e.g., Greenberg, 1971). Given
this relationship, one must rethink using experimen-
tal evidence for particular learnability biases to ex-
plain linguistic tendencies. Instead, one must either
estimate all of the values in the transmission matrix,
or actually simulate the process of multiple trans-
missions in the lab to establish whether a particu-
lar property with a learnability bias is actually main-
tained over many generations. While this process is
dependent on assuming a particular model of how
transmission occurs in populations, such as the lin-
ear iterated learning paradigm we used in our exper-
iments, it provides a way of understanding what mu-
tations are likely to occur and of exploring the long
term trends that result from particular learnability bi-
ases. As we showed for vowel harmony, long term
trends may not match what one predicted based on a
learnability bias. Given such a result, one must look
to factors other than the learnability bias to explain
why a property is common across languages.
Acknowledgements. This work was supported by an
NSF Graduate Research Fellowship to ANR, grant num-
ber BCS-0704034 from the NSF to TLG, and grant num-
ber T32 NS047987 from the NIH to ME.
56
References
Becker, M., Ketrez, N., & Nevins, A. (2011). The surfeit
of the stimulus: Analytic biases filter lexical statistics
in turkish laryngeal alternations. Language, 87(1), 84-
125.
Burke, C. J., & Rosenblatt, M. (1958). A Markovian
function of a Markov chain. The Annals of Mathemat-
ical Statistics, 29(4), 1112?1122.
Burkett, D., & Griffiths, T. (2010). Iterated learning
of multiple languages from multiple teachers. In The
Evolution of Language: Proceedings of the 8th Inter-
national Conference (EVOLANG8).
Chomsky, N., & Lasnik, H. (1993). The theory of prin-
ciples and parameters. In J. Jacobs, A. von Stechow,
W. Sternefeld, & T. Vannemann (Eds.), Syntax: An in-
ternational handbook of contemporary research (pp.
506?569). Berlin: Walter de Gruyter.
Comrie, B. (1981). Language universals and linguistic
typology. Chicago: University of Chicago Press.
Croft, W. (2002). Typology and universals. Cambridge
University Press.
Finley, S., & Badecker, W. (2007). Towards a substan-
tively biased theory of learning. Berkeley Linguistics
Society, 33.
Finley, S., & Badecker, W. (2009). Artificial language
learning and feature-based generalization. Journal of
Memory and Language, 61, 423?437.
Greenberg, J. (Ed.). (1963). Universals of language.
Cambridge, MA: MIT Press.
Greenberg, J. (1971). Language, culture, and communi-
cation. Stanford: Stanford University Press.
Griffiths, T. L., & Kalish, M. L. (2007). A Bayesian view
of language evolution by iterated learning. Cognitive
Science, 31, 441-480.
Kemeny, J., & Snell, J. (1960). Finite markov chains.
Princeton, NJ: van Nostrand.
Kirby, S. (2001). Spontaneous evolution of linguistic
structure: An iterated learning model of the emergence
of regularity and irregularity. IEEE Journal of Evolu-
tionary Computation, 5, 102-110.
Komarova, N. L., & Nowak, M. A. (2003). Language
dynamics in finite populations. Journal of Theoretical
Biology, 221, 445-457.
Labov, W. (2001). Principles of linguistic change. Vol-
ume II: Social Factors. Blackwell.
Moreton, E. (2008). Analytic bias and phonological ty-
pology. Phonology, 25(1), 83?127.
Rafferty, A. N., Griffiths, T. L., & Klein, D. (2009).
Convergence bounds for language evolution by iterated
learning. Proceedings of the Thirty-First Annual Con-
ference of the Cognitive Science Society.
Schuster, P., & Sigmund, K. (1983). Replicator dynam-
ics. Journal of Theoretical Biology, 100(3), 533 - 538.
Slobin, D. (1973). Cognitive prerequisites for the acqui-
sition of grammar. In C. Ferguson & D. Slobin (Eds.),
Studies of child language development (pp. 173?208).
Smith, K. (2009). Iterated learning in populations of
Bayesian agents. In Proceedings of the 31st Annual
Conference of the Cognitive Science Society.
van der Hulst, H., & van de Weijer, J. (1995). Vowel har-
mony. In J. Goldsmith (Ed.), The Handbook of Phono-
logical Theory (pp. 495?534). Blackwell.
Wilson, C. (2003). Experimental investigation of phono-
logical naturalness. Proceedings of the 22nd West
Coast Conference on Formal Linguistics.
Wilson, C. (2006). Learning phonology with substan-
tive bias: An experimental and computational study of
velar palatalization. Cognitive Science, 30, 945?982.
57
