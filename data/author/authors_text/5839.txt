129
130
131
132
133
134
135
136
Quantitative and Qualitative Evaluation of the OntoLearn Ontology Learning
System
Roberto NAVIGLI, Paola VELARDI
Dipartimento di Informatica
Universit? ?La Sapienza?
via Salaria 113
Roma, Italy, 00198
{velardi,navigli}@di.uniroma1.it
Alessandro CUCCHIARELLI, Francesca NERI
DIIGA
Universit? Politecnica delle Marche
via Brecce Bianche 12
Ancona, Italy, 60131
{cucchiarelli, neri}@diiga.univpm.it
Abstract
Ontology evaluation is a critical task, even more so
when the ontology is the output of an automatic
system, rather than the result of a conceptualisation
effort produced by a team of domain specialists
and knowledge engineers. This paper provides an
evaluation of the OntoLearn ontology learning
system. The proposed evaluation strategy is
twofold: first, we provide a detailed quantitative
analysis of the ontology learning algorithms, in
order to compute the accuracy of OntoLearn under
different learning circumstances. Second, we
automatically generate natural language
descriptions of formal concept specifications, in
order to facilitate per-concept qualitative analysis
by domain specialists.
1 Evaluating ontologies
Automatic methods for ontology learning and
population have been proposed in recent literature
(e.g. ECAI-2002 and KCAP-2003 workshops1) but
a co-related issue then becomes the evaluation of
such automatically generated ontologies, not only
with the goal of comparing the different
approaches (Hovy, 2001) and ontology-based tools
(Angele and Sure, 2002), but also to verify whether
an automatic process may actually compete with
the typically human process of converging on an
agreed conceptualization of a given domain.
Ontology construction, apart from the technical
aspects of a knowledge representation task (i.e.
choice of representation languages, consistency
and correctness with respect to axioms, etc.), is a
consensus building process, one that implies long
and often harsh discussions among the specialists
of a given domain. Can an automatic method
simulate this process? Can we provide domain
specialists with a means to measure the adequacy
of a specific set of concepts as a model of a given
                                                       
1ECAI-2002 http://www-sop.inria.fr/acacia/WORKSHOPS/
ECAI2002-OLT/accepted-papers.html
KCAP-2003 http://km.aifb.uni-karlsruhe.de/ws/semannot
2003/papers.html
domain?, Specialists are often unable to evaluate
the formal content of a computational ontology
(e.g. the denotational theory, the formal notation,
the knowledge representation system capabilities
like property inheritance, consistency, etc.).
Evaluation of the formal content is rather tackled
by computational scientists, or by automatic
verification systems. The role of the specialists is
instead to compare their intuition of a domain with
the description of this domain, as provided by the
ontology concepts. To facilitate one such
qualitative per-concept evaluation, we devised a
method for automatic generation of textual
explanations (glosses) of automatically learned
concepts. Glosses provide a description, in natural
language, of the formal specifications assigned to
the learned concepts. An expert can easily compare
his intuition with these natural language
descriptions.
The objective of the gloss-based evaluation is, as
previously remarked, to obtain a judgement, by
domain specialists, concerning the adequacy of an
automatically derived domain conceptualisation.
On the computational side, an ontology learning
tool is based on a battery of software programs
aimed at extracting and formalising domain
knowledge, usually starting from unstructured
data. Therefore, it is equally important to produce a
detailed evaluation of these programs, on a
quantitative ground, in order to gain insight on the
internal and external contingencies that may affect
the result of an ontology learning process.
In what follows, we firstly provide a quantitative
evaluation of the OntoLearn ontology learning
system, under different learning circumstances.
Secondly, we describe the gloss-based per-concept
evaluation method. Both evaluation strategies are
experimented in two application domains: Tourism
and Economy.
The subsequent section provides a sketchy
description of the OntoLearn algorithms. Details
are found in (Navigli and Velardi, 2004) and
(Navigli, Velardi and Gangemi, 2003). Sections 3
and 4 are dedicated to the quantitative and
qualitative analyses of OntoLearn.
2 Summary of the OntoLearn system
OntoLearn is an ontology population method
based on text mining and machine learning
techniques. OntoLearn starts with an existing
generic ontology (we use WordNet, though other
choices are possible) and a set of documents in a
given domain, and produces a domain extended
and trimmed version of the initial ontology. The
ontology generated by OntoLearn is anchored to
texts, it can be therefore classified as a linguistic
ontology (G?mez-P?rez et al 2004).
OntoLearn has been applied to different domains
(tourism, computer networks, economy) and in
several European projects2.
Concept learning is achieved in the following
three phases:
1) Terminology Extraction: A list of domain
multi-word expressions (MWE hereafter) is
extracted from a set of documents that are
judged representative of a given domain.
MWEs are extracted using natural language
processing and statistical techniques.
Contrastive corpora and glossaries in different
domains are used to prune terminology that is
not domain-specific. Domain MWEs are
selected also on the basis of an entropy-based
measure that simulates specialist consensus on
concepts choice: in words, the probability
distribution of a ?good? domain MWE must be
uniform across the individual documents of the
domain corpus.
2) Semantic interpretation of MWEs: Semantic
interpretation is based on a principle,
compositional interpretation, and on a novel
algorithm, called structural semantic
interconnections (SSI) .  Composi t ional
interpretation signifies that the meaning of a
multi-word expression (MWE) can be derived
compositionally from its components3, e.g. the
meaning of business plan is derived first, by
associating the appropriate concept identifier,
with reference to the initial top ontology, to the
component terms (i.e. sense #2 of business and
sense #1 of plan in WordNet), and then, by
identifying the semantic relations holding
among the involved concepts (e.g.
                                                       
2
 E.g. : Harmonize IST-2000-29329  and the INTEROP network of
excellence, started on december 2003.
3
 In the literature, multi word expressions are classified as
compositional, idiosyncratically compositional and non-
compositional.  In mid-technical domains, compositional MWEs
cover about 60-70% of MWE (we cannot support with data our
statitics for sake of space)
plan#1 topic? ? ? ? business# 2 ).
3) Extending and trimming the initial
o n t o l o g y : Once the terms have been
semantically interpreted, they are organized in
sub-trees, and appended under the appropriate
node  o f  t he  i n i t i a l  on to logy,
e.g. business _ plan# 1 kind _ of? ? ? ? ? ? plan# 1 .
Furthermore, certain upper and lower nodes of
the initial ontology are pruned to create a
domain-view  of the ontology. The final
ontology is output in OWL language.
SSI lies in the area of syntactic pattern matching
algorithms (Bunke and Sanfeliu, 1990). It is a word
sense disambiguation algorithm used to determine
the correct sense (with reference to the initial
ontology) for each component of a complex MWE.
The algorithm is based on building a graph
representation for alternative senses of each MWE
component4, and then selecting the appropriate
senses on the basis of detected s e m a n t i c
interconnection patterns between graph pairs. The
SSI algorithm seeks for semantic interconnections
among the words of a context T. Contexts Ti are
generated from groups of partially overlapping
complex MWEs (extracted during phase 1 of the
OntoLearn procedure) sharing the same syntactic
head . For example, given the list of complex
MWEs securities portfolio, investment portfolio,
real-estate portfolio, junk-bond portfolio,
diversified portfolio, stock portfolio, bond
portfolio, loan portfolio, the following list of term
components is created:
T=[security, investment, real-estate, estate, bond,
junk-bond, diversified, stock, portfolio, loan ]
Relevant pattern types  are described by a
context free grammar G. An example of rule in G
is the following (S1 S2 and S are concepts, i.e.
synsets in WordNet):
Rule Name:gloss+hyperonymy/meronymy (S1,S2):
Def: :SynsetsG?? S1
gloss
? ? ? ?  S
 and there is a
hyperonymy/meronymy path between S and S2
For instance, in railways company, the gloss of
railway#1 contains the word organization, and
there is an hyperonymy path of length 2 between
company#1 and organization#1. That is:
railway#1 gloss? ? ? ?  o r g a n i z a t i o n # 1 ,  a n d :
company#1
  
kind _of
? ? ? ? ? ? institution#1 
  
kind _of
? ? ? ? ? ? 
organization#1. This pattern (an instance of the
gloss+hypeonymyr/meronymy rule) cumulates
                                                       
4
 We remark again that a detailed description of the SSI algorithm
is in (Navigli & Velardi, 2004) and (Navigli, Velardi and Gangemi,
2003). Graphs are generated on the basis of lexico-semantic
information in WordNet and in a variety of on-line resources, see the
mentioned papers for details.
evidence for senses #1 of both ra i lway and
company.
In SSI, the correct sense St for a term t?T is
selected depending upon the number and weight of
patterns matching with rules in G. The weights of
patterns are automatically learned using a
perceptron5 model. The weight function is given
by:
)
_
1
()()1(
j
jjj patternlength
patternweight ?? +=
where ? j  is the weight of rule j in G, and the
second addend is a smoothing parameter inversely
proportional to the length of the matching pattern
(e.g. 2 in the previous example, since 2 is the
minimal length of the rule, and the actual length of
the pattern is 3). The perceptron has been trained
on the SemCor6 semantically annotated corpus.
In order to complete the semantic interpretation
process, OntoLearn then attempts to determine the
semantic relations that hold between the
components of a complex concept. In order to do
this, it was first necessary to select an inventory of
semantic relations. We examined several
proposals, like EuroWordnet (Vossen, 1999),
DOLCE (Masolo et al, 2002), FrameNet
(Ruppenhofer Fillmore & Baker, 2002) and others.
 As also remarked in (Hovy, 2001), no
systematic methods are available in literature to
compare the different sets of relations. Since our
objective was to define an automatic method for
semantic relation extraction, our final choice was
to use a reduced set of FrameNet relations, which
seemed general enough to cover our application
domains (tourism, computer networks, economy).
The choice of FrameNet is motivated by the
availability of a sufficiently large set of annotated
examples of conceptual relations7, that we used to
train an available machine learning algorithm,
TiMBL (Daelemans et al, 2002). The relations
used are: Material, Purpose, Use, Topic, Product,
Constituent Parts, Attribute8. Examples for each
relation are the following:
net # 1 attribute? ? ? ? ? ? loss# 3
takeover# 2 topic? ? ? ? proposal# 1
sand# 1 material? ? ? ? ? ? beach# 1
merger# 1 purpose? ? ? ? ? ? agreement# 1
                                                       
5
 http://www.cs.waikato.ac.nz/ml/weka/
6
 http://www.cs.unt.edu/~rada/downloads.html#semcor
7
 The choice of FrameNet was motivated more by availability than
appropriateness.
8
 The relation Attribute is not in FrameNet, however it was a
useful relation for terminological strings of the adjective_noun type.
meeting# 1 use? ? ? ? ro om# 1
bond# 2 const _ part? ? ? ? ? ? ? market# 1
c om puter# 1 product? ? ? ? ?  com pany# 1
We represented training instances as pairs of
concepts annotated with the appropriate conceptual
relation, e.g.:
[(computer#1,maker#3),Product]
Each concept is in turn represented by a feature-
vec to r  where attributes are the concept?s
hyperonyms in WordNet, e.g.:
(computer#1,maker#3):
((computer#1,machine#1,device#1
,instrumentality#3),(maker#3,business
#1,enterprise#2,organization#1))
3 Quantitative Evaluation of OntoLearn
This section provides a quantitative evaluation of
OntoLearn s main algorithms. We believe that a
quantitative evaluation is particularly important in
complex learning systems, where errors can be
produced at almost any stage. Even though some
of these errors (e.g. subtle sense distinctions) may
not have a percievable effect on the final ontology,
as shown by the results of the qualitative
evaluation in Section 4.2, it is nevertheless
important to gain insight on the actual system
capabilities, as well as on the pararmeters and
external circumstances that may positively or
negatively influence the final performance.
3.1 Evaluating the MWE extraction
algorithm
The terminology extraction algorithm has been
evaluated in the context of the European project
Harmonise on Tourism interoperability. We first
collected a corpus of about 1 million words of
tourism documents, mainly descriptions of travel
and tourism sites. From this corpus, a syntactic
parser extracted an initial list of 14,383 candidate
complex MWEs from which the statistical filters
selected a list of 3,840 domain-relevant complex
MWEs, that were submitted to the domain
specialists. The Harmonise ontology partners were
not skilled to evaluate the OntoLearn semantic
interpretation of MWEs, therefore we let them
evaluate only the domain appropriateness of the
terms. The gloss generation method described in
Section 4 was subsequently concieved to overcome
this limitation.
We obtained a precision ranging from 72.9% to
about 80% and a recall of 52.74%. The precision
shift is due to the well-known fact that experts may
have different intuitions about the relevance of a
concept. The recall estimate was produced by
manually inspecting 6,000 of the initial 14,383
candidate MWEs, asking the experts to mark all
the MWEs judged as ?good? domain MWEs, and
comparing the obtained list with the list of terms
automatically filtered by OntoLearn.
We ran similar experiments on an Economy
corpus and a Computer Network corpus, but in this
case the evaluation was performed by the authors.
Overall, the performance of the MWE extraction
task appears to be influenced by the dimension and
the focus of the starting corpus (e.g. ?generic
tourism? vs. ?hotel accomodation descriptions?).
Small and unfocused corpora do not favor the
efficacy of statistical analysis. However, the
availability of sufficiently large and focused
corpora seems a realistic requirement for most
applications.
3.2 Evaluating the ontology learning
algorithms
The distinctive task performed by OntoLearn is
semantic disambiguation. The performance of the
SSI algorithm critically depends upon two factors:
the first is the ability to detect semantic
interrelations among concepts associated to the
words of complex MWEs, the second is the
dimension of the context T available to start the
disambiguation process.
As for the first factor, there are two possible
ways of enhancing reliable identification of
semantic interconnections: one is to tune at best the
weight of individual rules in G (e.g. formula (1) in
Section 2), the second is to enrich the semantic
information associated to alternative word senses.
The latter is an on-going research activity.
As far as the context T is concerned, the
intuition is that, with a larger T , there are higher
chances of detecting semantic patterns among the
?correct? senses of the terms in T. However, the
dimension of contexts Ti is an external
contingency, it depends upon the available corpus.
Accordingly, we evaluated the SSI algorithm
using as parameters the dimension of T, T , and
the weights associated to rules in G. We ran several
experiments over the full terminology extracted
from the Economy and Tourism corpora, but
performances are computed only on, respectively,
453 and 638 manually disambiguated terms. This
means that in a context Ti including, e.g. k terms,
we evaluate OntoLearn?s sense choices only for
the fragment of j ? k terms, for which the ?true?
sense has been manually assigned.
Table 1 shows the performance of SSI (precision
and recall) when using only patterns whose weight,
computed with formula (1) is over a threshold ? .
The ?Core? column in Table 1 shows the
performance of SSI when accepting only these core
patterns, while the third column refers to all
matching patterns. With ? = 0,7  a subset of 7-9
rules9 in G (over a total of 20) are used by the
algorithm. Interestingly enough, these rules have a
high probability of being hired, as shown by the
relatively low difference in recall. The Baseline
tower in Table 1 is computed selecting always the
first sense (senses in WordNet are ordered by
probability in everyday language).
Table 2 shows that performance of SSI is indeed
affected by the dimension of T. Large T , as
expected, improves the performance, however,
overly large contexts (>80 terms) may favor the
detection of non-relevant patterns.
In general, both experiments show that the
Economy corpus performs better than the Tourism,
since the latter is less technical (the baseline is
quite high), rather unfocused, and contexts Ti are
much less populated.
Table 1. Performances as a function of pattern?s
weight
Table 2. Performances as a function of |T|
We remark that SSI performs better than
standard WSD (word sense disambiguation) tasks
but this is also motivated by the fact that context
words in T are more interrelated than co-occurring
words in generic sentences. The SSI algorithm, by
                                                       
9
 in formula (1), ? , that depends upon the rule, has a much
higher influence than ? , that depends upon the matching pattern)
86.40%
57.17% 53.76%
59.72%
80.21%76.48%
81.77%
85.48%
71.30% 67.33%
0%
20%
40%
60%
80%
100%
Baseline Core All Rules Baseline Core All Rules
Prec.
Recall Finance Tourism
78.57% 81.82% 82.84% 80.29% 80%
58.51% 63.28%
73.16%
55.46%
73.20%
0%
20%
40%
60%
80%
100%
|T| < 35 35 <= |T| < 65 |T| >= 65 |T| < 35 |T| >= 35
Prec. Recall
Finance Tourism
its very nature, is favored by focused and large
contexts. In any case, it is worth mentioning that
SSI received the second best score in the latest
SenSeval-310, gloss disambiguation exercise,
placed about 1% below the first and about 11%
before the third participant.
3.3 Evaluating the semantic annotation
algorithm
To test the semantic relation annotation task, we
used a learning set (including selected annotated
examples from FrameNet (FN), Tourism (Tour),
and Economy (Econ)), and a test set with a
distribution of examples shown in Table 3.
Table 3. Distribution of examples in the learning
and test set for the semantic annotation task
Learning Set Test Set
Sem_Rel FN Tour Econ Tot FN Tour Econ Tot
MATERIAL 8 3 0 11 5 2 0 7
USE 9 32 2 43 6 20 1 27
TOPIC 52 79 100 231 29 43 50 122
C_PART 3 7 12 22 2 4 6 12
PURPOSE 26 64 22 112 14 34 11 59
PRODUCT 3 1 6 10 1 1 4 6
Total 101 186 142 429 57 104 72 233
Notice that the relation Attribute is generated
whenever the term associated to one of the
concepts is an adjective. Therefore, this semantic
relation is not included in the evaluation
experiment, since it would artificially increase
performances. We then tested the learner on test
sets for individual domains11, leading to the
results shown in Table 4 a and b.
Table 4 Performance of the semantic annotation
task on a) Tourism b) Economy
d<=10% d<=30% d<=100%
Precision MACRO 0,958 0,875 0,847
Recall MACRO 0,283 0,636 0,793
F1 MACRO 0,437 0,737 0,819
Precision micro 0,900 0,857 0,798
Recall micro 0,087 0,635 0,798
F1 micro 0,158 0,721 0,798
d<=10% d<=30% d<=100%
Precision MACRO 1,000 0,804 0,651
Recall MACRO 0,015 0,403 0,455
F1 MACRO 0,030 0,537 0,536
Precision micro 1,000 0,758 0,750
Recall micro 0,042 0,653 0,750
F1 micro 0,080 0,701 0,750
The performance measures are those adopted in
TREC competitions12. The parameter d  in the
above Tables is a confidence factor defined in the
TiMBL algorithm. This parameter can be used to
                                                       
10
 SensEval?3   http://www.senseval.org/senseval3
11
 This of course penalised the results (the performance over a test
set composed by examples of all the three domains is much higher),
but provides a more realistic test bed of the generality of the approach.
12
 http://trec.nist.gov/
increase system?s robustness in the following way:
whenever the confidence associated by TiMBL to
the classification of a new instance is lower than a
given threshold, we output a ?generic? conceptual
relation, named Relatedness. We experimentally
fixed the threshold for d  around 30% (central
column of Table 4).
Table 4 demonstrates rather good performances,
however the main problem with semantic relation
annotation is the unavailability of an agreed set of
conceptual relations, and a sufficiently large and
balanced training set. Consequently, we need to
update the set of used relations whenever we
analyse a new domain, and re-run the training
phase enriching the training corpus with manually
tagged examples from the new domain (as for in
Table 2).
4  Qualitative evaluation: Evaluating the
generated ontology on a per-concept basis
The lesson learned during the Harmonise EC
project was that the domain specialists, tourism
operators in our case, can hardly evaluate the
formal aspects of a computational ontology. When
presented with the domain extended and trimmed
version of WordNet (OntoLearn?s phase 3 in
Section 2), they were only able to express a generic
judgment on each node of the hierarchy, based on
the concept label. These judgments were used to
evaluate the terminology extraction task, but the
experiment suggested that, indeed, it was necessary
to provide a better description for the learned
concepts.
4.1 Gloss generation grammar
To help human evaluation on a per-concept
basis, we decided to enhance OntoLearn with a
gloss generation algorithm. The idea is to generate
glosses in a way that closely reflects the key
aspects of the concept learning process, i.e.
semantic disambiguation and annotation with a
conceptual relation.
The gloss generation algorithm is based on the
definition of a grammar with distinct generation
rules for each type of semantic relation.
Let Sih
sem _ rel
? ? ? ? ? ? Sj
k
 be the complex concept
associated to a complex term whw k (e.g. jazz
festival, or long-term debt), and let:
<H>= the syntactic head of whwk (e.g. festival,
debt)
<M> = the syntactic modifier of whwk (e.g. jazz,
long-term)
<GNC>= be the gloss of the new complex concept
Shk
<HYP>= the selected sense of <H>(e.g.
respectively, festival#1 and debt#1).
<MSGHYP>= the main sentence13 of the
WordNet gloss of <HYP>
<MSGM>= the main sentence of the WordNet
gloss of the selected sense for <M>
Here we provide two examples of rules for
generating GNCs:
If sem_rel=Topic, <GNC>:: = a kind of <HYP>,
<MSGHYP>, relating to the <M>, <MSGM>.
e,g.: GNC(jazz festival): a kind of festival,  a
day or period of time set aside for feasting and
celebration,  relating to the jazz, a style of dance
music popular in the 1920.
If sem_rel=Attribute, <GNC>:= a kind of <HYP>,
<MSGHYP>, <MSGM>.
e.g.:GNC(long term debt)= a kind of debt, the
state of owing something (especially money),
relating to or extending over a relatively long time.
4.2 Per-concept evaluation experiment
To verify the utility of gloss generation, the
automatically generated glosses were submitted for
evaluation to two human experts, a tourism
specialist from ECCA14, and an economist from
the University of Ancona. The specialists were not
aware of the method used to generate glosses; they
have been presented with a list of concept-gloss
pairs and asked to fill in an evaluation form (see
Appendix) as follows: vote 1 means
?unsatisfactory definition?, vote 2 means ?the
definition is helpful?, vote 3 means ?the definition
is fully acceptable?. Whenever he was not fully
happy with a definition (vote 2 or 1), the specialist
was asked to provide a brief explanation. For
comparison, Appendix 2 shows also glossary
definitions extracted from the web for the same
MWEs, that were not shown to the specialists.
Table 5 provides a summary of the
evaluation..
Table 5. Evaluation of glosses by domain
specialists.
vote =1 vote=2 vote=3 uncertai
n
average
Tourism
total
(97)
33
(34.7)
14
(14.4)
45
(46.3)
5 (5.1) 2,13
Ecomo
my total
(134)
52
(38.8)
16
(11.9)
66
(49.2)
- 2.10
The following conclusions can be drawn from
this experiment:
1 .  Overall, the two domain specialists fully
accepted the system?s choices in 45-49% of the
cases, and were reasonably satisfied in 12-14%
                                                       
13
 The main sentence is the gloss pruned of subordinates,
examples, etc.
14
 ECCA ? eTourism Competence Center Austria.
of the cases. The average vote is above 2 in
both cases.
2. As expected, if a MWE is compositional, the
generated definition is more often accepted or
fully accepted (e.g. examples 25_E and 14_T
in Appendix 2). When a compositional
interpretation is not accepted (vote=1), this is
motivated either by an OntoLearn
interpretation error (wrong sense or wrong
conceptual relations) or by the unavailability
of a correct sense in WordNet, despite the fact
that the sense is not idiosyncratic. OntoLearn
errors for compositional MWEs are 7 (5%) in
Economy and 12 (13%) in Tourism.  Examples
of OntoLearn errors and core ontology
?misses? are the definitions 14_T (wrong sense
of form) and 19_E (no good sense for bilateral
in WordNet), respectively.
3.  Sometimes the specialists found it acceptable
also an idiosyncratic or non compositional
definition. This happens in 16 cases for the
Tourism domain (16%) and in 19 cases for the
Economy domain (13%). Examples are the
MWEs 45_E and 76_E, both idiosyncratically
decomposable, in Appendix 2.
One of the specialists is particularly involved in
ontology building projects, therefore we report his
valuable comment: ?some of the descriptions
would not be appropriate to take them over in a
tourism ontology just as they are. But most of them
are quite helpful as basis for building the ontology.
The most important problem from my point of view
is the too detailed descriptions of the components
itself instead of the meaning of the overall term in
this context. Best example is the term ?bed tax?.
Nobody would expect a definition of a bed or a
tax.? In other terms, he found disturbing the fact
that a definition extensively reports the definitions
of its components. On the other side, our objective
is not only to produce concept definitions, but also
to organize concepts in hierarchies. Showing the
definitions of individual components is a ?natural?
mean to verify that the correct senses have been
selected (e.g. the correct senses of bed and tax).
This is clearly the case, since, for example in
definition 14_T (booking form), the specialist was
immediately able to diagnose a sense
disambiguation error for form , though he was
unaware of the OntoLearn methodology.
5 Concluding remarks
This paper presented an in-depth evaluation of
the Ontolearn ontology learning system. The three
basic algorithms (terminology extraction, sense
disambiguation and annotation with semantic
relation) have been individually evaluated in two
domains, under different parametrizations, to
obtain a realistic and comprehensible picture of
system?s capabilities. The critical algorithm, SSI,
has very good performances that are favored by the
fact that word sense disambiguation is applied to
group of words (domain MWEs) that are strongly
semantically related, unlike for generic WSD tasks
(e.g. Senseval). The performance of the SSI
algorithm can be further improved through an
extension of the grammar G, which is an on-going
research activity.
6 Acknowledgements
Our thanks go to Dr. Wolfram H?pken, from
ECCA ? eTourism Competence Center Austria
(wolfram@hoepken.org ) and Dr. Renato
Iacobucci, from the University of Ancona, who
gave up their precious time to evaluate our glosses.
This work has been in part supported by the
INTEROP Network of Excellence IST-2003-
508011
References
J. Angele and Y. Sure (2002) ?Whitepaper:
Evaluation of Ontology-based Tools?, Workshop
on evaluation of ontology-based tools
(EON2002), at the 13th Int. EKAW 2002,
Sigueza (Spain), September 2002.
H. Bunke and A. Sanfeliu (editors) (1990).
Syntactic and Structural pattern Recognition:
Theory and Applications, World Scientific,
Series in Computer Science vol. 7, 1990.
Daelemans,W. Zavrel, J. Van den Sloot, K. & Van
den Bosch, A. (2002). TiMBL: Tilburg Memory
Based Learner. Version 4.3 Reference Guide.
Tilburg University.
G?mez-P?rez, A., Fern?ndez-Lopez M. and
Corcho O. (2004). Ontological Engineering,
Springer Verlag, London, 2004.
Hovy, E. (2001). Comparing Sets of Semantic
relations in Ontologies. In R. Geen, C.A. Bean
and S. Myaeng Semantic of relations. Kluwer.
Masolo, C., Borgo, S., Gangemi, A., Guarino, N.
Oltramari, A. & Schneider, L. (2002).
Sweetening Ontologies with DOLCE.
Proceedings of the 13th International Conference
on Knowledge Engineering and Knowledge
Management. Ontologies and the Semantic Web.
Navigli, R. & Velardi, P. (2004). Learning Domain
Ontologies from Document Warehouses and
Dedicated Web Sites. Computational Linguistics,
MIT press, (50)2.
Navigli, R., Velardi, P. Gangemi, A. (2003).
Corpus Driven Ontology Learning: a Method
and its Application to Automated Terminology
Translation. IEEE Intelligent Systems (18)1.22-
31.
Ruppenhofer, J., Fillmore, C.J. & Baker, C.F.
(2002). Collocational Information in the
FrameNet Database. In Braasch, A. and Povlsen,
C. (eds.), Proceedings of the Tenth Euralex
International Congress. Copenhagen, Denmark.
Vol. I: 359--369, 2002.
Vossen, P. (1999). EuroWordNet: General
D o c u m e n t .  V e r s i o n  3  F i n a l .
http://www.hum.uva.nl/~ewn
APPENDIX: Excerpt of the per-concept evaluation form
Concept #: 25_E Term: business_plan Synt: N-N Rel<w1,w2>: Topic
Gloss: a kind of plan, a series of steps to be carried out or goals to be accomplished, relating to the business, the activity
of providing goods and services involving financial and commercial and industrial aspects.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: a written report that states what a company (or a part of a company) aims to do increase sales,
develop new products, etc. within a certain period, and how it will obtain the necessary finances and resources.
Concept #: 2_T Term: affiliated_hotel Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of hotel, a building where travellers can pay for lodging and meals and other services, being joined in close
association.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: a hotel that is a member of a chain, franchise, or referral system. Membership provides special
advantages, particularly a national reservation system.
Concept #: 14_T Term: booking_form Synt: N-N Rel<w1,w2>: Purpose
Gloss: a kind of form, alternative names for the body of a human being, for booking, the act of reserving (a place or
passage) or engaging the services of (a person or group).
Specialist vote: 1
Comment by Specialist: definition of form  wrong in this context
Diagnose: OntoLearn disambiguation error for form
Glossary definition: a document which purchasers of tours must complete to give the operator full particulars about who
is buying the tour.
Concept #: 19_E Term: bilateral_aid Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, having
identical parts on each side of an axis.
Specialist vote: 1
Comment by Specialist: Fully wrong definition.
Diagnose: WordNet gloss of bilateral  is not adequate to domain (no better definition is available in WordNet).
Glossary definition:  assistance given by one country to another.
Concept #: 45_E Term: cyclical_uneployment Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of unemployment, the state of being unemployed or not having a job, recurring in cycles.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: workers are without a job because of a lack of aggregate demand due to a down turn in economic
activity.
Concept #: 76_E Term: foreign_aid Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, of
concern to or concerning the affairs of other nations .
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary defonition: the international transfer of public and private funds in the form of loans or grants from donor
countries to recipient countries.
107
108
109
110
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 594?602,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Using Cycles and Quasi-Cycles to Disambiguate Dictionary Glosses
Roberto Navigli
Dipartimento di Informatica
Sapienza - Universita` di Roma
Via Salaria, 113 - 00198 Roma Italy
navigli@di.uniroma1.it
Abstract
We present a novel graph-based algo-
rithm for the automated disambiguation
of glosses in lexical knowledge resources.
A dictionary graph is built starting from
senses (vertices) and explicit or implicit
relations in the dictionary (edges). The
approach is based on the identification of
edge sequences which constitute cycles in
the dictionary graph (possibly with one
edge reversed) and relate a source to a
target word sense. Experiments are per-
formed on the disambiguation of ambigu-
ous words in the glosses of WordNet and
two machine-readable dictionaries.
1 Introduction
In the last two decades, we have witnessed an
increasing availability of wide-coverage lexical
knowledge resources in electronic format, most
notably thesauri (such as Roget?s Thesaurus (Ro-
get, 1911), the Macquarie Thesaurus (Bernard,
1986), etc.), machine-readable dictionaries (e.g.,
the Longman Dictionary of Contemporary En-
glish (Proctor, 1978)), computational lexicons
(e.g. WordNet (Fellbaum, 1998)), etc.
The information contained in such resources
comprises (depending on their kind) sense inven-
tories, paradigmatic relations (e.g. flesh3n is a kind
of plant tissue1n),
1 text definitions (e.g. flesh3n is
defined as ?a soft moist part of a fruit?), usage ex-
amples, and so on.
Unfortunately, not all the semantics are made
explicit within lexical resources. Even Word-
Net, the most widespread computational lexicon
of English, provides explanatory information in
the form of textual glosses, i.e. strings of text
1We denote as wip the ith sense in a reference dictionary
of a word w with part of speech p.
which explain the meaning of concepts in terms
of possibly ambiguous words.
Moreover, while computational lexicons like
WordNet contain semantically explicit informa-
tion such as, among others, hypernymy and
meronymy relations, most thesauri, glossaries, and
machine-readable dictionaries are often just elec-
tronic transcriptions of their paper counterparts.
As a result, for each entry (e.g. a word sense or
thesaurus entry) they mostly provide implicit in-
formation in the form of free text.
The production of semantically richer lexical
resources can help alleviate the knowledge ac-
quisition bottleneck and potentially enable ad-
vanced Natural Language Processing applications
(Cuadros and Rigau, 2006). However, in order to
reduce the high cost of manual annotation (Ed-
monds, 2000), and to avoid the repetition of this
effort for each knowledge resource, this task must
be supported by wide-coverage automated tech-
niques which do not rely on the specific resource
at hand.
In this paper, we aim to make explicit
large quantities of semantic information implic-
itly contained in the glosses of existing wide-
coverage lexical knowledge resources (specifi-
cally, machine-readable dictionaries and computa-
tional lexicons). To this end, we present a method
for Gloss Word Sense Disambiguation (WSD),
called the Cycles and Quasi-Cycles (CQC) algo-
rithm. The algorithm is based on a novel notion
of cycles in the dictionary graph (possibly with
one edge reversed) which support a disambigua-
tion choice. First, a dictionary graph is built from
the input lexical knowledge resource. Next, the
method explicitly disambiguates the information
associated with sense entries (i.e. gloss words)
by associating senses for which the richest sets of
paths can be found in the dictionary graph.
In Section 2, we provide basic definitions,
present the gloss disambiguation algorithm, and il-
594
lustrate the approach with an example. In Section
3, we present a set of experiments performed on
a variety of lexical knowledge resources, namely
WordNet and two machine-readable dictionaries.
Results are discussed in Section 4, and related
work is presented in Section 5. We give our con-
clusions in Section 6.
2 Approach
2.1 Definitions
Given a dictionary D, we define a dictionary
graph as a directed graph G = (V,E) whose ver-
tices V are the word senses in the sense inventory
of D and whose set of unlabeled edges E is ob-
tained as follows:
i) Initially, E := ?;
ii) For each sense s ? V , and for each lexico-
semantic relation in D connecting sense s to
s? ? V , we perform: E := E ? {(s, s?)};
iii) For each sense s ? V , let gloss(s) be the set
of content words in its part-of-speech tagged
gloss. Then for each content word w? in
gloss(s) and for each sense s? of w?, we
add the corresponding edge to the dictionary
graph, i.e.: E := E ? {(s, s?)}.
For instance, consider WordNet as our input
dictionary D. As a result of step (ii), given the se-
mantic relation ?sport1n is a hypernym of racing
1
n?,
the edge (racing1n, sport
1
n) is added toE (similarly,
an inverse edge is added due to the hyponymy rela-
tion holding between sport1n and racing
1
n). During
step (iii), the gloss of racing1n ?the sport of engag-
ing in contests of speed? is part-of-speech tagged,
obtaining the following set of content words:
{ sportn, engagev, contestn, speedn }. The fol-
lowing edges are then added to E: { (racing1n,
sport1n), (racing
1
n, sport
2
n), . . . , (racing
1
n, sport
6
n),
. . . , (racing1n, speed
1
n), . . . , (racing
1
n, speed
5
n) }.
The above steps are performed for all the senses in
V .
We now recall the definition of graph cycle. A
cycle in a graphG is a sequence of edges ofG that
forms a path v1 ? v2 ? ? ? ? ? vn (vi ? V ) such
that the first vertex of the path corresponds to the
last, i.e. v1 = vn (Cormen et al, 1990, p. 88).
For example, the cycle in Figure 1(a) is given by
the path racing1n ? contest
1
n ? race
3
n ? run
3
n ?
racing1n in the WordNet dictionary graph. In fact
racing1n
contest1n
race3n
run3n
(a)
racing1n
contest1n
compete1v
race2v
(b)
Figure 1: An example of cycle (a) and quasi-cycle
(b) in WordNet.
contestn occurs in the gloss of racing1n, race
3
n is a
hyponym of contest1n, and so on.
We further provide the definition of quasi-cycle
as a sequence of edges in which the reversal of
the orientation of a single edge creates a cycle
(Bohman and Thoma, 2000). For instance, the
quasi-cycle in Figure 1(b) is given by the path rac-
ing1n ? contest
1
n ? compete
1
v ? race
2
v ? rac-
ing1n. In fact, the reversal of the edge (racing
1
n,
race2v) creates a cycle.
Finally, we call a path a (quasi-)cycle if it is ei-
ther a cycle or a quasi-cycle. Further, we say that
a path is (quasi-)cyclic if it forms a (quasi-)cycle
in the graph.
2.2 The CQC Algorithm
Given a dictionary graph G = (V,E) built as de-
scribed in the previous section, our objective is
to disambiguate dictionary glosses with the sup-
port of (quasi-)cycles. (Quasi-)cyclic paths are in-
tuitively better than unconstrained paths as each
sense choice s is reinforced by the very fact of s
being reachable from itself through a sequence of
other senses.
Let a(s) be the set of ambiguous words to be
disambiguated in the part-of-speech tagged gloss
of sense s. Given a word w? ? a(s), our aim is
to disambiguate w? according to the sense inven-
tory of D, i.e. to assign it the right sense chosen
from its set of senses Senses(w?). To this end, we
propose the use of a graph-based algorithm which
searches the dictionary graph and collects the fol-
lowing kinds of (quasi-)cyclic paths:
i) s? s? ? s1 ? ? ? ? ? sn?2 ? s (cycle)
ii) s? s? ? s1 ? ? ? ? ? sn?2 ? s
(quasi-cycle)
595
CQC-Algorithm(s, w?)
1 for each sense s? ? Senses(w?)
2 CQC(s?) := DFS(s?, s)
3 All CQC :=
?
s??Senses(w?)CQC(s
?)
4 for each sense s? ? Senses(w?)
5 score(s?) := 0
6 for each path c ? CQC(s?)
7 l := length(c)
8 v := ?(l) ? 1NumCQC(All CQC,l)
9 score(s?) := score(s?) + v
10 return argmax
s??Senses(w?)
score(s?)
Table 1: The Cycles and Quasi-Cycles (CQC) al-
gorithm in pseudocode.
where s is our source sense, s? is a candidate sense
of w? ? gloss(s), si is a sense in V , and n is
the length of the path (given by the number of its
edges). We note that both kinds of paths start and
end with the same vertex s, and that we restrict
quasi-cycles to those whose inverted edge departs
from s. To avoid any redundancy, we require that
no vertex is repeated in the path aside from the
start/end vertex (i.e. s 6= s? 6= si 6= sj for any
i, j ? {1, . . . , n? 2}).
The Cycles and Quasi-Cycles (CQC) algorithm,
reported in pseudo-code in Table 1, takes as input a
source sense s and a target wordw? (in our setting2
w? ? a(s)). It consists of two main phases.
During steps 1-3, cycles and quasi-cycles are
sought for each sense of w?. This step is per-
formed with a depth-first search (DFS, cf. (Cor-
men et al, 1990, pp. 477?479)) up to a depth
?. To this end, we first define next(s) = {s?? :
(s, s??) ? E}, that is the set of senses which can
be directly reached from sense s. The DFS starts
from a sense s? ? Senses(w?), and recursively ex-
plores the senses in next(s?) until sense s or a
sense in next(s) is encountered, obtaining a cy-
cle or a quasi-cycle, respectively. For each sense
s? of w? the DFS returns the full set CQC(s?)
of (quasi-)cyclic paths collected. Note that the
DFS recursively keeps track of previously visited
senses, so as to discard (quasi-)cycles including
the same sense twice. Finally, in step 3, All CQC
is set to store the cycles and quasi-cycles for all
the senses of w?.
2Note that potentially w? can be any word of interest. The
very same algorithm can be applied to determine semantic
similarity or to disambiguate collocations.
The second phase (steps 4-10) computes a score
for each sense s? of w? based on the paths col-
lected for s? during the first phase. Let c be such
a path, and let l be its length, i.e. the number of
edges in the path. Then the contribution of c to the
score of s? is given by a function of its length ?(l),
which associates with l a number between 0 and 1.
This contribution is normalized by a factor given
byNumCQC(All CQC, l), which calculates the
overall number of paths of length l. In this work,
we will employ the function ?(l) = 1/el, which
weighs a path with the inverse of the exponential
of its length (so as to exponentially decrease the
contribution of longer paths)3. Steps 4-9 are re-
peated for each candidate sense ofw?. Finally, step
10 returns the highest-scoring sense of w?.
As a result of the systematic application of
the CQC algorithm to the dictionary graph G =
(V,E) associated with a dictionary D, a graph
G? = (V, E?) is output, where V is again the sense
inventory of D, and E? ? E, such that each edge
(s, s?) ? E? either represents an unambiguous re-
lation in E (i.e. it was either a lexico-semantic re-
lation in D or a relation between s and a monose-
mous word occurring in its gloss) or is the result
of an execution of the CQC algorithm with input s
and w? ? a(s).
2.3 An Example
Consider the following example: WordNet defines
the third sense of fleshn as ?a soft moist part of a
fruit?. As a result of part-of-speech tagging, we
obtain:
gloss(flesh3n) = {softa ,moista , partn , fruitn}
Let us assume we aim to disambiguate the noun
fruit. Our call to the CQC algorithm in Table 1 is
then CQC-Algorithm(flesh3n, fruitn).
As a result of the first two steps of the algorithm,
a set of cycles and quasi-cycles for each sense of
fruitn is collected, based on a DFS starting from
the respective senses of our target word (we as-
sume ? = 5). In Figure 2, we show some of the
(quasi-)cycles collected for senses #1 and #3 of
fruitn, respectively defined as ?the ripened repro-
ductive body of a seed plant? and ?an amount of
a product? (we neglect sense #2 as the length and
number of its paths is not dissimilar from that of
sense #3).
3Other weight functions, such as ?(l) = 1 (which weighs
each path independent of its length) proved to perform worse.
596
flesh3n
fruit1n berry11n
pulpy1a
parenchyma1n
plant tissue1n
lychee1n
custard apple1n
mango2n
moist1a
flora2n
edible fruit1n
skin2n
hygrophyte1n
(a)
flesh3n
fruit3n
newspaper4n
mag1n
production4n
(b)
Figure 2: Some cycles and quasi-cycles connect-
ing flesh3n to fruit
1
n (a), and fruit
3
n (b).
During the second phase of the algorithm, and
for each sense of fruitn, the contribution of each
(quasi-)cycle is calculated (steps 6-9 of the algo-
rithm). For example, for sense fruit1n in Figure
2(a), 5 (quasi-)cycles of length 4 and 2 of length 5
were returned by DFS(fruit1n, flesh
3
n). As a result,
the following score is calculated:4
score(fruit1n) =
5
e4 ?
1
NumCQC(all chains,4)
+ 2e5 ?
1
NumCQC(all chains,5)
= 5e4?7 +
2
e5?2
= 0.013 + 0.006 = 0.019
whereas for fruit3n (see Figure 2(b)) we get:
score(fruit3n) =
2
e4 ?
1
NumCQC(all chains,4)
= 2e4?7 = 0.005
where NumCQC(All CQC, l) is the total num-
ber of cycles and quasi-cycles of length l over all
the senses of fruitn (according to Figure 2, this
amounts to 7 paths for l = 4 and 2 paths for l = 5).
Finally, the sense with the highest score (i.e.
fruit1n) is returned.
3 Experiments
To test and compare the performance of our al-
gorithm, we performed a set of experiments on a
4Note that, for the sake of simplicity, we are calculating
our scores based on the paths shown in Figure 2. However,
we tried to respect the proportion of paths collected by the
algorithm for the two senses.
variety of resources. First, we summarize the re-
sources (Section 3.1) and algorithms (Section 3.2)
that we adopted. In Section 3.3 we report our ex-
perimental results.
3.1 Resources
The following resources were used in our experi-
ments:
? WordNet (Fellbaum, 1998), the most
widespread computational lexicon of En-
glish. It encodes concepts as synsets, and
provides textual glosses and lexico-semantic
relations between synsets. Its latest version
(3.0) contains around 155,000 lemmas, and
over 200,000 word senses;
? Macquarie Concise Dictionary (Yallop,
2006), a machine-readable dictionary of
(Australian) English, which includes around
50,000 lemmas and almost 120,000 word
senses, for which it provides textual glosses
and examples;
? Ragazzini/Biagi Concise (Ragazzini and Bi-
agi, 2006), a bilingual English-Italian dic-
tionary, containing over 90,000 lemmas and
150,000 word senses. The dictionary pro-
vides Italian translations for each English
word sense, and vice versa.
We used TreeTagger (Schmid, 1997) to part-of-
speech tag the glosses in the three resources.
3.2 Algorithms
Hereafter we briefly summarize the algorithms
that we applied in our experiments:
? CQC: we applied the CQC algorithm as de-
scribed in Section 2.2;
? Cycles, which applies the CQC algorithm but
searches for cycles only (i.e. quasi-cycles are
not collected);
? An adaptation of the Lesk algorithm (Lesk,
1986), which, given a source sense s of word
w and a word w? occurring in the gloss of s,
determines the right sense of w? as that which
maximizes the (normalized) overlap between
each sense s? of w? and s:
argmax
s??Senses(w?)
|next?(s) ? next?(s?)|
max{|next?(s)|, |next?(s?)|}
597
where we define next?(s) = words(s) ?
next(s), and words(s) is the set of lexical-
izations of sense s (e.g. the synonyms in the
synset s). When WordNet is our reference re-
source, we employ an extension of the Lesk
algorithm, namely Extended Gloss Overlap
(Banerjee and Pedersen, 2003), which ex-
tends the sense definition with words from
the definitions of related senses (such as hy-
pernyms, hyponyms, etc.). We use the same
set of relations available in the authors? im-
plementation of the algorithm.
We also compared the performance of the above
algorithms with two standard baselines, namely
the First Sense Baseline (abbreviated as FS BL)
and the Random Baseline (Random BL).
3.3 Results
Our experiments concerned the disambiguation of
the gloss words in three datasets, one for each re-
source, namely WordNet, Macquarie Concise, and
Ragazzini/Biagi. In all datasets, given a sense s,
our set a(s) is given by the set of part-of-speech-
tagged ambiguous content words in the gloss of
sense s from our reference dictionary.
WordNet. When using WordNet as a reference
resource, given a sense s whose gloss we aim to
disambiguate, the dictionary graph includes not
only edges connecting s to senses of gloss words
(step (iii) of the graph construction procedure, cf.
Section 2.1), but also those obtained from any of
the WordNet lexico-semantics relations (step (ii)).
For WordNet gloss disambiguation, we em-
ployed the dataset used in the Senseval-3 Gloss
WSD task (Litkowski, 2004), which contains
15,179 content words from 9,257 glosses5. We
compared the performance of CQC, Cycles, Lesk,
and the two baselines. To get full coverage and
high performance, we learned a threshold for each
system below which they recur to the FS heuris-
tic. The threshold and maximum path length were
tuned on a small in-house manually-annotated
dataset of 100 glosses. The results are shown in
Table 2. We also included in the table the perfor-
mance of the best-ranking system in the Senseval-
5Recently, Princeton University released a richer corpus
of disambiguated glosses, namely the ?Princeton WordNet
Gloss Corpus? (http://wordnet.princeton.edu).
However, in order to allow for a comparison with the state
of the art (see below), we decided to adopt the Senseval-3
dataset.
Algorithm Prec./Recall
CQC 64.25
Cycles 63.74
Lesk 51.75
TALP 68.60/68.30
FS BL 55.44
Random BL 26.29
Table 2: Gloss WSD performance on WordNet.
3 Gloss WSD task, namely the TALP system
(Castillo et al, 2004).
CQC outperforms all other proposed ap-
proaches, obtaining a 64.25% precision and recall.
We note that Cycles also gets high performance,
compared to Lesk and the baselines. Also, com-
pared to CQC, the difference is not statistically
significant. However, we observe that, if we do
not recur to the first sense as a backoff strategy, we
get a much lower recall for Cycles (P = 65.39, R =
26.70 for CQC, P = 72.03, R = 16.39 for Cycles).
CQC performs about 4 points below the TALP
system. As also discussed later, we believe this re-
sult is relevant, given that our approach does not
rely on additional knowledge resources, as TALP
does (though both algorithms recur to the FS back-
off strategy).
Finally, we observe that the FS baseline has
lower performance than in typical all-words dis-
ambiguation settings (usually above 60% accu-
racy). We believe that this is due to the absence
of monosemous words from the test set, and to
the possibly different distribution of senses in the
dataset.
Macquarie Concise. Automatically disam-
biguating glosses in a computational lexicon
such as WordNet is certainly useful. However,
disambiguating a machine-readable dictionary
is an even more ambitious task. In fact, while
computational lexicons typically encode some ex-
plicit semantic relations which can be used as an
aid to the disambiguation task, machine-readable
dictionaries only rarely provide sense-tagged
information (often in the form of references to
other word senses). As a result, in this latter
setting the dictionary graph typically contains
only edges obtained from the gloss words of sense
s (step (iii), Section 2.1).
To experiment with machine-readable dictio-
naries, we employed the Macquarie Concise Dic-
598
Algorithm Prec./Recall
CQC 77.13
Cycles 67.63
Lesk 30.16
FS BL 51.48
Random BL 23.28
Table 3: Gloss WSD performance on Macquarie
Concise.
tionary (Yallop, 2006). A dataset was prepared
by randomly selecting 1,000 word senses from
the dictionary and annotating the content words in
their glosses according to the dictionary sense in-
ventory. Overall, 2,678 words were sense tagged.
The results are shown in Table 3. CQC obtains
an accuracy of 77.13% (in case of ties, a random
choice is made, thus leading to the same precision
and recall), Cycles achieves an accuracy of almost
10% less than CQC (the difference is statistically
significant; p < 0.01). The FS baseline, here, is
based on the first sense listed in the Macquarie
sense inventory, which ? in contrast to WordNet
? does not depend on the occurrence frequency of
senses in a semantically-annotated corpus. How-
ever, we note that the FS baseline is not very dif-
ferent from that of the WordNet experiment.
We observe that the Lesk performance is very
low on this dataset (around 7 points above the Ran-
dom BL), due to the impossibility of using the
Extended Gloss Overlap approach (semantic rela-
tions are not available in the Macquarie Concise)
and to the low number of matches between source
and target entries.
Ragazzini/Biagi. Finally, we performed an ex-
periment on the Ragazzini/Biagi English-Italian
machine-readable dictionary. In this experiment,
disambiguating a word w? in the gloss of a sense
s from one section (e.g. Italian-English) equals to
selecting a word sense s? of w? listed in the other
section of the dictionary (e.g. English-Italian). For
example, given the English entry race1n, translated
as ?corsan, garan?, our objective is to assign the
right Italian sense from the Italian-English section
to corsan and garan.
To apply the CQC algorithm, a simple adapta-
tion is needed, so as to allow (quasi-)cycles to con-
nect word senses from the two distinct sections.
The algorithm must seek cyclic and quasi-cyclic
paths, respectively of the kind:
Algorithm Prec./Recall
CQC 89.34
Cycles 85.40
Lesk 63.89
FS BL 73.15
Random BL 51.69
Table 4: Gloss WSD performance on Ragazz-
ini/Biagi.
i) s? s? ? s1 ? ? ? ? ? sn?2 ? s
ii) s? s? ? s1 ? ? ? ? ? sn?2 ? s
where n is the path length, s and s? are senses re-
spectively from the source (e.g. Italian/English)
and the target (e.g. English/Italian) section of the
dictionary, si is a sense from the target section for
i ? k and from the source section for i > k,
for some k such that 0 ? k ? n ? 2. In other
words, the DFS can jump at any time from the tar-
get section to the source section. After the jump,
the depth search continues in the source section, in
the hope to reach s. For example, the following is
a cycle with k = 1:
race1n? corsa
2
n? gara
2
n? race
1
n
where the edge between corsa2n and gara
2
n is due
to the occurrence of garan in the gloss of corsa2n
as a domain label for that sense.
To perform this experiment, we randomly se-
lected 250 entries from each section (500 over-
all), including a total number of 1,069 translations
that we manually sense tagged. In Table 4 we re-
port the results of CQC, Cycles and Lesk on this
task. Overall, the figures are higher than in previ-
ous experiments, thanks to a lower average degree
of polysemy of the resource, which also impacts
positively on the FS baseline. However, given a
random baseline of 51.69%, the performance of
CQC, over 89% precision and recall, is signif-
icantly higher. Cycles obtains around 4 points
less than CQC (the difference is statistically sig-
nificant; p < 0.01). The performance of Lesk
(63.89%) is also much higher than in our previ-
ous experiments, thanks to the higher chance of
finding a 1:1 correspondence between the two sec-
tions. However, we observed that this does not al-
ways hold, as also supported by the better results
of CQC.
599
4 Discussion
The experiments presented in the previous section
are inherently heterogeneous, due to the different
nature of the resources adopted (a computational
lexicon, a monolingual and a bilingual machine-
readable dictionary). Our aim was to show the
flexibility of our approach in tagging gloss words
with senses from the same dictionary.
We show the average polysemy of the three
datasets in Table 5. Notice that none of the
datasets included monosemous items, so our ex-
periments cannot be compared to typical all-words
disambiguation tasks, where monosemous words
are part of the test set.
Given that words in the Macquarie dataset have
a higher average polysemy than in the Word-
Net dataset, one might wonder why disambiguat-
ing glosses from a computational lexicon such as
WordNet is more difficult than performing a sim-
ilar task on a machine-readable dictionary such
as the Macquarie Concise Dictionary, which does
not provide any explicit semantic hint. We be-
lieve there are at least two reasons for this out-
come: the first specifically concerns the Senseval-
3 Gloss WSD dataset, which does not reflect the
distribution of genus-differentiae terms in dictio-
nary glosses: less than 10% of the items were hy-
pernyms, thus making the task harder. As for the
second reason, we believe that the Macquarie Con-
cise provides more clear-cut definitions, thus mak-
ing sense assignments relatively easier.
An analytical comparison of the results of Cy-
cles and CQC show that, especially for machine-
readable dictionaries, employing both cycles and
quasi-cycles is highly beneficial, as additional sup-
port is provided by the latter patterns. Our results
on WordNet prove to be more difficult to analyze,
because of the need of employing the first sense
heuristic to get full coverage. Also, the maximum
path length used for WordNet was different (? = 3
according to our tuning, compared to ? = 4 for
Macquarie and Ragazzini/Biagi). However, quasi-
cycles are shown to provide over 10% improve-
ment in terms of recall (at the price of a decrease
in precision of 6.6 points).
Further, we note that the performance of the
CQC algorithm dramatically improves as the max-
imum score (i.e. the score which leads to a sense
assignment) increases. As a result, users can tune
the disambiguation performance based on their
specific needs (coverage, precision, etc.). For in-
WN Mac R/B
Polysemy 6.68 7.97 3.16
Table 5: Average polysemy of the three datasets.
stance, WordNet Gloss WSD can perform up to
85.7% precision and 10.1% recall if we require the
score to be? 0.2 and do not use the FS baseline as
a backoff strategy. Similarly, we can reach up to
93.8% prec., 20.0% recall for Macquarie Concise
(score? 0.12) and even 95.2% prec., 70.6% recall
(score ? 0.1) for Ragazzini/Biagi.
5 Related Work
Word Sense Disambiguation is a large research
field (see (Navigli, 2009) for an up-to-date
overview). However, in this paper we focused on
a specific kind of WSD, namely the disambigua-
tion of dictionary definitions. Seminal works on
the topic date back to the late 1970s, with the de-
velopment of models for the identification of tax-
onomies from lexical resources (Litkowski, 1978;
Amsler, 1980). Subsequent works focused on the
identification of genus terms (Chodorow et al,
1985) and, more in general, on the extraction of
explicit information from machine-readable dic-
tionaries (see, e.g., (Nakamura and Nagao, 1988;
Ide and Ve?ronis, 1993)). Kozima and Furugori
(1993) provide an approach to the construction
of ambiguous semantic networks from glosses in
the Longman Dictionary of Contemporary English
(LDOCE). In this direction, it is worth citing the
work of Vanderwende (1996) and Richardson et
al. (1998), who describe the construction of Mind-
Net, a lexical knowledge base obtained from the
automated extraction of lexico-semantic informa-
tion from two machine-readable dictionaries. As a
result, weighted relation paths are produced to in-
fer the semantic similarity between pairs of words.
Several heuristics have been presented for the
disambiguation of the genus of a dictionary defini-
tion (Wilks et al, 1996; Rigau et al, 1997). More
recently, a set of heuristic techniques has been pro-
posed to semantically annotate WordNet glosses,
leading to the release of the eXtended WordNet
(Harabagiu et al, 1999; Moldovan and Novischi,
2004). Among the methods, the cross reference
heuristic is the closest technique to our notion of
cycles and quasi-cycles. Given a pair of words w
and w?, this heuristic is based on the occurrence of
600
w in the gloss of a sense s? of w? and, vice versa,
ofw? in the gloss of a sense s ofw. In other words,
a graph cycle s? s? ? s of length 2 is sought.
Based on the eXtended WordNet, a gloss dis-
ambiguation task was organized at Senseval-3
(Litkowski, 2004). Interestingly, the best perform-
ing systems, namely the TALP system (Castillo et
al., 2004), and SSI (Navigli and Velardi, 2005),
are knowledge-based and rely on rich knowledge
resources: respectively, the Multilingual Central
Repository (Atserias et al, 2004), and a propri-
etary lexical knowledge base.
In contrast, the approach presented in this paper
performs the disambiguation of ambiguous words
by exploiting only the reference dictionary itself.
Furthermore, as we showed in Section 3.3, our
method does not rely on WordNet, and can be ap-
plied to any lexical knowledge resource, including
bilingual dictionaries.
Finally, methods in the literature more focused
on a specific disambiguation task include statisti-
cal methods for the attachment of hyponyms un-
der the most likely hypernym in the WordNet tax-
onomy (Snow et al, 2006), structural approaches
based on semantic clusters and distance met-
rics (Pennacchiotti and Pantel, 2006), supervised
machine learning methods for the disambiguation
of meronymy relations (Girju et al, 2003), etc.
6 Conclusions
In this paper we presented a novel approach to dis-
ambiguate the glosses of computational lexicons
and machine-readable dictionaries, with the aim of
alleviating the knowledge acquisition bottleneck.
The method is based on the identification of cy-
cles and quasi-cycles, i.e. circular edge sequences
(possibly with one edge reversed) relating a source
to a target word sense.
The strength of the approach lies in its weakly
supervised nature: (quasi-)cycles rely exclusively
on the structure of the input lexical resources. No
additional resource (such as labeled corpora or ex-
ternal knowledge bases) is required, assuming we
do not resort to the FS baseline. As a result, the
approach can be applied to obtain a semantic net-
work from the disambiguation of virtually any lex-
ical resource available in machine-readable format
for which a sense inventory is provided.
The utility of gloss disambiguation is even
greater in bilingual dictionaries, as idiosyncrasies
such as missing or redundant translations can be
discovered, thus helping lexicographers improve
the resources6. An adaptation similar to that de-
scribed for disambiguating the Ragazzini/Biagi
can be employed for mapping pairs of lexical
resources (e.g. FrameNet (Baker et al, 1998)
to WordNet), thus contributing to the beneficial
knowledge integration process. Following this di-
rection, we are planning to further experiment on
the mapping of FrameNet, VerbNet (Kipper et al,
2000), and other lexical resources.
The graphs output by the CQC algo-
rithm for our datasets are available from
http://lcl.uniroma1.it/cqc. We
are scheduling the release of a software pack-
age which includes our implementation of the
CQC algorithm and allows its application to any
resource for which a standard interface can be
written.
Finally, starting from the work of Budanitsky
and Hirst (2006), we plan to experiment with the
CQC algorithm when employed as a semantic sim-
ilarity measure, and compare it with the most suc-
cessful existing approaches. Although in this pa-
per we focused on the disambiguation of dictio-
nary glosses, the same approach can be applied for
disambiguating collocations according to a dictio-
nary of choice, thus providing a way to further en-
rich lexical resources with external knowledge.
Acknowledgments
The author is grateful to Ken Litkowski and the
anonymous reviewers for their useful comments.
He also wishes to thank Zanichelli and Macquarie
for kindly making their dictionaries available for
research purposes.
References
Robert A. Amsler. 1980. The structure of the
Merriam-Webster pocket dictionary, Ph.D. Thesis.
University of Texas, Austin, TX, USA.
Jordi Atserias, Lu??s Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The meaning multilingual central
repository. In Proceedings of GWC 2004, pages 23?
30, Brno, Czech Republic.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of COLING-ACL 1998, pages 86?90, Montreal,
Canada.
6This is indeed an ongoing line of research in collabora-
tion with the Zanichelli dictionary publisher.
601
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of IJCAI 2003, pages 805?810, Aca-
pulco, Mexico.
John Bernard, editor. 1986. Macquarie Thesaurus.
Macquarie, Sydney, Australia.
Tom Bohman and Lubos Thoma. 2000. A note on
sparse random graphs and cover graphs. The Elec-
tronic Journal of Combinatorics, 7:1?9.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of semantic dis-
tance. Computational Linguistics, 32(1):13?47.
Mauro Castillo, Francis Real, Jordi Asterias, and Ger-
man Rigau. 2004. The talp systems for dis-
ambiguating wordnet glosses. In Proceedings of
ACL 2004 SENSEVAL-3 Workshop, pages 93?96,
Barcelona, Spain.
Martin Chodorow, Roy Byrd, and George Heidorn.
1985. Extracting semantic hierarchies from a large
on-line dictionary. In Proceedings of ACL 1985,
pages 299?304, Chicago, IL, USA.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to algorithms.
MIT Press, Cambridge, MA.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proceedings of EMNLP 2006, pages 534?541, Syd-
ney, Australia.
Philip Edmonds. 2000. Designing a task for
SENSEVAL-2. Technical note.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of NAACL 2003, pages 1?8, Edmonton, Canada.
Sanda Harabagiu, George Miller, and Dan Moldovan.
1999. Wordnet 2 - a morphologically and se-
mantically enhanced resource. In Proceedings of
SIGLEX-99, pages 1?8, Maryland, USA.
Nancy Ide and Jean Ve?ronis. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings
of Workshop on Knowledge Bases and Knowledge
Structures, pages 257?266, Tokyo, Japan.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
Proceedings of AAAI 2000, pages 691?696, Austin,
TX, USA.
Hideki Kozima and Teiji Furugori. 1993. Similarity
between words computed by spreading activation on
an english dictionary. In Proceedings of ACL 1993,
pages 232?239, Utrecht, The Netherlands.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th SIGDOC, pages 24?26, New York, NY.
Kenneth C. Litkowski. 1978. Models of the semantic
structure of dictionaries. American Journal of Com-
putational Linguistics, (81):25?74.
Kenneth C. Litkowski. 2004. Senseval-3 task:
Word-sense disambiguation of wordnet glosses. In
Proceedings of ACL 2004 SENSEVAL-3 Workshop,
pages 13?16, Barcelona, Spain.
Dan Moldovan and Adrian Novischi. 2004. Word
sense disambiguation of wordnet glosses. Computer
Speech & Language, 18:301?317.
Jun-Ichi Nakamura and Makoto Nagao. 1988. Extrac-
tion of semantic information from an ordinary en-
glish dictionary and its evaluation. In Proceedings
of COLING 1988, pages 459?464, Budapest, Hun-
gary.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions of Pattern Analysis and Machine Intelligence
(TPAMI), 27(7):1075?1088.
Roberto Navigli. 2009. Word sense disambiguation: a
survey. ACM Computing Surveys, 41(2):1?69.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proceedings of
COLING-ACL 2006, pages 793?800, Sydney, Aus-
tralia.
Paul Proctor, editor. 1978. Longman Dictionary of
Contemporary English. Longman Group, UK.
Giuseppe Ragazzini and Adele Biagi, editors. 2006. Il
Ragazzini-Biagi, 4th Edition. Zanichelli, Italy.
Stephen D. Richardson, William B. Dolan, and Lucy
Vanderwende. 1998. Mindnet: acquiring and struc-
turing semantic information from text. In Proceed-
ings of COLING 1998, pages 1098?1102, Montreal,
Quebec, Canada.
German Rigau, Jordi Atserias, and Eneko Agirre.
1997. Combining unsupervised lexical knowledge
methods for word sense disambiguation. In Pro-
ceedings of ACL/EACL 1997, pages 48?55, Madrid,
Spain.
Peter M. Roget. 1911. Roget?s International The-
saurus (1st edition). Cromwell, New York, USA.
Helmut Schmid. 1997. Probabilistic part-of-speech
tagging using decision trees. In Daniel Jones and
Harold Somers, editors, New Methods in Language
Processing, Studies in Computational Linguistics,
pages 154?164. UCL Press, London, UK.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL 2006,
pages 801?808, Sydney, Australia.
Lucy Vanderwende. 1996. The analysis of noun se-
quences using semantic information extracted from
on-line dictionaries, Ph.D. Thesis. Georgetown
University, Washington, USA.
Yorick Wilks, Brian Slator, and Louise Guthrie, editors.
1996. Electric words: Dictionaries, computers and
meanings. MIT Press, Cambridge, MA.
Colin Yallop, editor. 2006. The Macquarie Concise
Dictionary 4th Edition. Macquarie Library Pty Ltd,
Sydney, Australia.
602
c? 2004 Association for Computational Linguistics
Learning Domain Ontologies from
Document Warehouses and Dedicated
Web Sites
Roberto Navigli? Paola Velardi
Universita` di Roma ?La Sapienza? Universita` di Roma ?La Sapienza?
We present a method and a tool, OntoLearn, aimed at the extraction of domain ontologies from
Web sites, and more generally from documents shared among the members of virtual organiza-
tions. OntoLearn first extracts a domain terminology from available documents. Then, complex
domain terms are semantically interpreted and arranged in a hierarchical fashion. Finally, a
general-purpose ontology, WordNet, is trimmed and enriched with the detected domain concepts.
The major novel aspect of this approach is semantic interpretation, that is, the association of a
complex concept with a complex term. This involves finding the appropriate WordNet concept
for each word of a terminological string and the appropriate conceptual relations that hold among
the concept components. Semantic interpretation is based on a new word sense disambiguation
algorithm, called structural semantic interconnections.
1. Introduction
The importance of domain ontologies is widely recognized, particularly in relation to
the expected advent of the Semantic Web (Berners-Lee 1999). The goal of a domain on-
tology is to reduce (or eliminate) the conceptual and terminological confusion among
the members of a virtual community of users (for example, tourist operators, commer-
cial enterprises, medical practitioners) who need to share electronic documents and
information of various kinds. This is achieved by identifying and properly defining a
set of relevant concepts that characterize a given application domain. An ontology is
therefore a shared understanding of some domain of interest (Uschold and Gruninger
1996). The construction of a shared understanding, that is, a unifying conceptual frame-
work, fosters
? communication and cooperation among people
? better enterprise organization
? interoperability among systems
? system engineering benefits (reusability, reliability, and specification)
Creating ontologies is, however, a difficult and time-consuming process that involves
specialists from several fields. Philosophical ontologists and artificial intelligence lo-
gicians are usually involved in the task of defining the basic kinds and structures
of concepts (objects, properties, relations, and axioms) that are applicable in every
? Dipartimento di Informatica, Universita` di Roma ?La Sapienza,? Via Salaria, 113 - 00198 Roma, Italia.
E-mail: {navigli, velardi}@di.uniroma1.it.
152
Computational Linguistics Volume 30, Number 2
 
 
     Core Ontology  
Foundational  Ontology  
Specific Domain Ontology
Figure 1
The three levels of generality of a domain ontology.
possible domain. The issue of identifying these very few ?basic? principles, now often
referred to as foundational ontologies (FOs) (or top, or upper ontologies; see Figure 1)
(Gangemi et al 2002), meets the practical need of a model that has as much generality
as possible, to ensure reusability across different domains (Smith and Welty 2001).
Domain modelers and knowledge engineers are involved in the task of identify-
ing the key domain conceptualizations and describing them according to the organi-
zational backbones established by the foundational ontology. The result of this effort
is referred to as the core ontology (CO), which usually includes a few hundred ap-
plication domain concepts. While many ontology projects eventually succeed in the
task of defining a core ontology,1 populating the third level, which we call the specific
domain ontology (SDO), is the actual barrier that very few projects have been able to
overcome (e.g., WordNet [Fellbaum 1995], Cyc [Lenat 1993], and EDR [Yokoi 1993]),
but they pay a price for this inability in terms of inconsistencies and limitations.2
It turns out that, although domain ontologies are recognized as crucial resources
for the Semantic Web, in practice they are not available and when available, they are
rarely used outside specific research environments.
So which features are most needed to build usable ontologies?
? Coverage: The domain concepts must be there; the SDO must be
sufficiently (for the application purposes) populated. Tools are needed to
extensively support the task of identifying the relevant concepts and the
relations among them.
? Consensus: Decision making is a difficult activity for one person, and it
gets even harder when a group of people must reach consensus on a
given issue and, in addition, the group is geographically dispersed.
When a group of enterprises decide to cooperate in a given domain, they
have first to agree on many basic issues; that is, they must reach a
consensus of the business domain. Such a common view must be
reflected by the domain ontology.
? Accessibility: The ontology must be easily accessible: tools are needed to
easily integrate the ontology within an application that may clearly show
1 Several ontologies are already available on the Internet, including a few hundred more or less
extensively defined concepts.
2 For example, it has been claimed by several researchers (e.g., Oltramari et al, 2002) that in WordNet
there is no clear separation between concept-synsets, instance-synsets, relation-synsets, and
meta-property-synsets.
153
Navigli and Velardi Learning Domain Ontologies
its decisive contribution, e.g., improving the ability to share and
exchange information through the web.
In cooperation with another research institution,3 we defined a general architecture
and a battery of systems to foster the creation of such ?usable? ontologies. Consensus
is achieved in both an implicit and an explicit way: implicit, since candidate concepts
are selected from among the terms that are frequently and consistently employed in
the documents produced by the virtual community of users; explicit, through the use
of Web-based groupware aimed at consensual construction and maintenance of an
ontology. Within this framework, the proposed tools are OntoLearn, for the automatic
extraction of domain concepts from thematic Web sites; ConSys, for the validation
of the extracted concepts; and SymOntoX, the ontology management system. This
ontology-learning architecture has been implemented and is being tested in the con-
text of several European projects,4 aimed at improving interoperability for networked
enterprises.
In Section 2, we provide an overview of the complete ontology-engineering archi-
tecture. In the remaining sections, we describe in more detail OntoLearn, a system that
uses text mining techniques and existing linguistic resources, such as WordNet and
SemCor, to learn, from available document warehouses and dedicated Web sites, do-
main concepts and taxonomic relations among them. OntoLearn automatically builds
a specific domain ontology that can be used to create a specialized view of an exist-
ing general-purpose ontology, like WordNet, or to populate the lower levels of a core
ontology, if available.
2. The Ontology Engineering Architecture
Figure 2 reports the proposed ontology-engineering method, that is, the sequence of
steps and the intermediate outputs that are produced in building a domain ontol-
ogy. As shown in the figure, ontology engineering is an iterative process involving
concept learning (OntoLearn), machine-supported concept validation (ConSys), and
management (SymOntoX).
The engineering process starts with OntoLearn exploring available documents
and related Web sites to learn domain concepts and detect taxonomic relations among
them, producing as output a domain concept forest. Initially, we base concept learning
on external, generic knowledge sources (we use WordNet and SemCor). In subsequent
cycles, the domain ontology receives progressively more use as it becomes adequately
populated.
Ontology validation is undertaken with ConSys (Missikoff and Wang 2001), a
Web-based groupware package that performs consensus building by means of thor-
ough validation by the representatives of the communities active in the application
domain. Throughout the cycle, OntoLearn operates in connection with the ontology
management system, SymOntoX (Formica and Missikoff 2003). Ontology engineers use
this management system to define and update concepts and their mutual connections,
thus allowing the construction of a semantic net. Further, SymOntoX?s environment
can attach the automatically learned domain concept trees to the appropriate nodes of
the core ontology, thereby enriching concepts with additional information. SymOntoX
3 The LEKS-CNR laboratory in Rome.
4 The Fetish EC project, ITS-13015 (http://fetish.singladura.com/index.php) and the Harmonise EC
project, IST-2000-29329 (http://dbs.cordis.lu), both in the tourism domain, and the INTEROP Network
of Excellence on interoperability IST-2003-508011.
154
Computational Linguistics Volume 30, Number 2
 
Domain 
Concept 
Forest 
OntoLearn ConSys  Domain 
Ontology 
Application domain  
Communities 
Domain 
Web sites 
Generic 
Knowledge  
Sources 
Self-learning 
SymOntoX 
Validation 
Self-learni
S ntoX
Figure 2
The ontology-engineering chain.
also performs consistency checks. The self-learning cycle in Figure 2 consists, then, of
two steps: first, domain users and experts use ConSys to validate the automatically
learned ontology and forward their suggestions to the knowledge engineers, who im-
plement them as updates to SymOntoX. Then, the updated domain ontology is used
by OntoLearn to learn new concepts from new documents.
The focus of this article is the description of the OntoLearn system. Details on
other modules of the ontology-engineering architecture can be found in the referenced
papers.
3. Architecture of the OntoLearn System
Figure 3 shows the architecture of the OntoLearn system. There are three main phases:
First, a domain terminology is extracted from available texts in the application domain
(specialized Web sites and warehouses, or documents exchanged among members of
a virtual community), and filtered using natural language processing and statistical
techniques. Second, terms are semantically interpreted (in a sense that we clarify in
Section 3.2) and ordered according to taxonomic relations, generating a domain con-
cept forest (DCF). Third, the DCF is used to update the existing ontology (WordNet
or any available domain ontology).
In a ?stand-alone? mode, OntoLearn automatically creates a specialized view of
WordNet, pruning certain generic concepts and adding new domain concepts. When
used within the engineering chain shown in Figure 2, ontology integration and up-
dating is performed by the ontology engineers, who update an existing core ontology
using SymOntoX.
In this article we describe the stand-alone procedure.
3.1 Phase 1: Terminology Extraction
Terminology is the set of words or word strings that convey a single (possibly complex)
meaning within a given community. In a sense, terminology is the surface appearance,
in texts, of the domain knowledge of a community. Because of their low ambiguity
and high specificity, these words are also particularly useful for conceptualizing a knowledge
domain or for supporting the creation of a domain ontology. Candidate terminological
expressions are usually captured with more or less shallow techniques, ranging from
stochastic methods (Church and Hanks 1989; Yamamoto and Church 2001) to more
sophisticated syntactic approaches (Jacquemin 1997).
155
Navigli and Velardi Learning Domain Ontologies
WordNet
domain
 corpus
contrastive
  corpora
terminology extraction
candidate
extraction
terminology
   filtering
semantic interpretation
     semantic
disambiguation
   identification of
taxonomic relations
   identification of
conceptual relations
Inductive
  learner
  Natural
 Language
Processor
ontology integration
     and updating
3
2
1
  Lexical
Resources
Domain Concept Forest
Figure 3
The architecture of OntoLearn.
Obviously, richer syntactic information positively influences the quality of the
result to be input to the statistical filtering. In our experiments we used the linguis-
tic processor ARIOSTO (Basili, Pazienza, and Velardi 1996) and the syntactic parser
CHAOS (Basili, Pazienza, and Zanzotto 1998). We parsed the available documents in
the application domain in order to extract a list Tc of syntactically plausible termino-
logical noun phrases (NPs), for example, compounds (credit card), adjective-NPs (local
tourist information office), and prepositional-NPs (board of directors). In English, the first
two constructs are the most frequent.
OntoLearn uses a novel method for filtering ?true? terminology, described in detail
in (Velardi, Missikoff, and Basili 2001). The method is based on two measures, called
Domain Relevance (DR) and Domain Consensus (DC), that we introduce hereafter.
156
Computational Linguistics Volume 30, Number 2
High frequency in a corpus is a property observable for terminological as well as
nonterminological expressions (e.g., last week or real time). We measure the specificity of
a terminological candidate with respect to the target domain via comparative analysis
across different domains. To this end a specific DR score has been defined. A quantita-
tive definition of the DR can be given according to the amount of information captured
within the target corpus with respect to a larger collection of corpora. More precisely,
given a set of n domains {D1, . . . , Dn} and related corpora, the domain relevance of a
term t in class Dk is computed as
DRt,k =
P(t|Dk)
max
1?j?n
P(t|Dj)
(1)
where the conditional probabilities (P(t|Dk)) are estimated as
E(P(t|Dk)) =
ft,k
?
t??Dk
ft?,k
where ft,k is the frequency of term t in the domain Dk (i.e., in its related corpus).
Terms are concepts whose meaning is agreed upon by large user communities in a
given domain. A more selective analysis should take into account not only the overall
occurrence of a term in the target corpus but also its appearance in single documents.
Domain terms (e.g., travel agent) are referred to frequently throughout the documents
of a domain, while there are certain specific terms with a high frequency within single
documents but completely absent in others (e.g., petrol station, foreign income). Dis-
tributed usage expresses a form of consensus tied to the consolidated semantics of a
term (within the target domain) as well as to its centrality in communicating domain
knowledge.
A second relevance indicator, DC, is then assigned to candidate terms. DC mea-
sures the distributed use of a term in a domain Dk. The distribution of a term t in
documents d ? Dk can be taken as a stochastic variable estimated throughout all
d ? Dk. The entropy of this distribution expresses the degree of consensus of t in Dk.
More precisely, the domain consensus is expressed as follows:
DCt,k =
?
d?Dk
(
Pt(d) log
1
Pt(d)
)
(2)
where
E(Pt(dj)) =
ft,j
?
dj?Dk
ft,j
Nonterminological (or nondomain) candidate terms are filtered using a combination
of measures (1) and (2).
For each candidate term the following term weight is computed:
TWt,k = ?DRt,k + ?DCnormt,k
where DCnormt,k is a normalized entropy and ?,? ? (0, 1). We experimented with several
thresholds for ? and ?, with consistent results in two domains (Velardi, Missikoff, and
Basili 2001). Usually, a value close to 0.9 is to be chosen for ?. The threshold for
157
Navigli and Velardi Learning Domain Ontologies
Table 1
The first 10 terms from a tourism (left) and
finance (right) domain.
tourism finance
travel information vice president
shopping street net income
airline ticket executive officer
booking form composite trading
bus service stock market
car rental interest rate
airport transfer million share
contact detail holding company
continental breakfast third-quarter net
tourist information office chief executive
service
ferry service boat service
car ferry service
bus service transport
service
public transport
service
coach
service
taxi service
express servicetrain service
car service customer
service
Figure 4
A lexicalized tree in a tourism domain.
? depends upon the number N of documents in the training set of Dk. When N is
sufficiently large, ?good? values are between 0.35 and 0.25. Table 1 shows some of the
accepted terms in two domains, ordered by TW.
3.2 Phase 2: Semantic Interpretation
The set of terms accepted by the filtering method described in the previous section are
first arranged in subtrees, according to simple string inclusion.5 Figure 4 is an example
of what we call a lexicalized tree T . In absence of semantic interpretation, it is not
possible to fully capture conceptual relationships between concepts (for example, the
taxonomic relation between bus service and public transport service in Figure 4).
Semantic interpretation is the process of determining the right concept (sense) for
each component of a complex term (this is known as sense disambiguation) and then
identifying the semantic relations holding among the concept components, in order to
build a complex concept. For example, given the complex term bus service, we would
like to associate a complex concept with this term as in Figure 5, where bus#1 and
service#1 are unique concept names taken from a preexisting concept inventory (e.g.,
WordNet, though other general-purpose ontologies could be used), and INSTR is a
semantic relation indicating that there is a service, which is a type of work (service#1),
operated through (instrument) a bus, which is a type of public transport (bus#1).
5 Inclusion is on the right side in the case of compound terms (the most common syntactic construct for
terminology in English).
158
Computational Linguistics Volume 30, Number 2
bus#1 service#1
INSTR
Figure 5
A complex term represented as a complex concept.
This kind of semantic interpretation is indeed possible if the meaning of a new
complex concept can be interpreted compositionally from its components. Clearly, this
is not always possible. Furthermore, some of the component concepts may be absent
in the initial ontology. In this case, other strategies can be adopted, as sketched in
Section 6.
To perform semantic disambiguation, we use available lexical resources, like Word-
Net and annotated corpora, and a novel word sense disambiguation (WSD) algorithm
called structural semantic interconnection. A state-of-art inductive learner is used to
learn rules for tagging concept pairs with the appropriate semantic relation.
In the following, we first describe the semantic disambiguation algorithm (Sec-
tions 3.2.1 to 3.2.4). We then describe the semantic relation extractor (Section 3.2.5).
3.2.1 The Structural Semantic Interconnection Algorithm. OntoLearn is a tool for
extending and trimming a general-purpose ontology. In its current implementation,
it uses a concept inventory taken from WordNet. WordNet associates one or more
synsets (e.g., unique concept names) to over 120,000 words but includes very few
domain terms: for example, bus and service are individually included, but not bus
service as a unique term.
The primary strategy used by OntoLearn to attach a new concept under the ap-
propriate hyperonym of an existing ontology is compositional interpretation. Let
t = wn ? . . . ? w2 ? w1 be a valid multiword term belonging to a lexicalized tree T .
Let w1 be the syntactic head of t (e.g., the rightmost word in a compound, or the
leftmost in a prepositional NP). The process of compositional interpretation associates
the appropriate WordNet synset Sk with each word wk in t. The sense of t is hence
compositionally defined as
S(t) = [Sk|Sk ? Synsets(wk), wk ? t]
where Synsets(wk) is the set of senses provided by WordNet for word wk, for instance:
S (?transport company??) = [{transportation#4, shipping#1, transport#3},
{company#1}]
corresponding to sense 1 of company (an institution created to conduct business) and
sense 3 of transport (the commercial enterprise of transporting goods and materials).
Compositional interpretation is a form of word sense disambiguation. In this sec-
tion, we define a new approach to sense disambiguation called structural semantic
interconnections (SSI).
The SSI algorithm is a kind of structural pattern recognition. Structural pattern
recognition (Bunke and Sanfeliu 1990) has proven to be effective when the objects to
be classified contain an inherent, identifiable organization, such as image data and
time-series data. For these objects, a representation based on a ?flat? vector of fea-
tures causes a loss of information that has a negative impact on classification per-
159
Navigli and Velardi Learning Domain Ontologies
coachvehicle transport passenger
PATIENTPURPOSEKIND-OF
(vehicle, passenger, transport) (a)
(b)
Figure 6
Two representations of the same concept: (a) as a feature vector and (b) as a semantic graph.
formances. The classification task in a structural pattern recognition system is imple-
mented through the use of grammars that embody precise criteria to discriminate
among different classes. The drawback of this approach is that grammars are by their
very nature application and domain specific. However, automatic learning techniques
may be adopted to learn from available examples.
Word senses clearly fall under the category of objects that are better described
through a set of structured features. Compare for example the following two feature-
vector (a) and graph-based (b) representations of the WordNet definition of coach#5 (a
vehicle carrying many passengers, used for public transport) in Figure 6. The graph
representation shows the semantic interrelationships among the words in the defini-
tion, in contrast with the flat feature vector representation.
Provided that a graph representation for alternative word senses in a context is
available, disambiguation can be seen as the task of detecting certain ?meaningful? intercon-
necting patterns among such graphs. We use a context-free grammar to specify the type
of patterns that are the best indicators of a semantic interrelationship and to select the
appropriate sense configurations accordingly.
In what follows, we first describe the method to obtain a graph representation
of word senses from WordNet and other available resources. Then, we illustrate the
disambiguation algorithm.
Creating a graph representation for word senses. A graph representation of word senses is
automatically built using a variety of knowledge source:
1. WordNet. In WordNet, in addition to synsets, the following information
is provided:
(a) a textual sense definition (gloss);
(b) hyperonymy links (i.e., kind-of relations: for example, bus#1
is a kind of public transport#1);
(c) meronymy relations (i.e., part-of relations: for example, bus#1
has part roof#2 and window#2);
(d) other syntactic-semantic relations, as detailed later, not
systematically provided throughout the lexical knowledge
base.
2. Domain labels6 extracted by a semiautomatic methodology described in
Magnini and Cavaglia (2000) for assigning domain information (e.g.,
tourism, zoology, sport) to WordNet synsets.
3. Annotated corpora providing examples of word sense usages in contexts:
6 Domain labels have been kindly made available by the IRST to our institution for research purposes.
160
Computational Linguistics Volume 30, Number 2
(a) SemCor7 is a corpus in which each word in a sentence is
assigned a sense selected from the WordNet sense inventory for
that word. Examples of a SemCor document are the following:
Color#1 was delayed#1 until 1935, the widescreen#1 until the early#1
fifties#1.
Movement#7 itself was#7 the chief#1 and often#1 the only#1
attraction#4 of the primitive#1 movies#1 of the nineties#1.
(b) LDC/DSO8 is a corpus in which each document is a collection of
sentences having a certain word in common. The corpus
provides a sense tag for each occurrence of the word within the
document. Examples from the document focused on the noun
house are the following:
Ten years ago, he had come to the house#2 to be interviewed.
Halfway across the house#1, he could have smelled her morning
perfume.
(c) In WordNet, besides glosses, examples are sometimes provided
for certain synsets. From these examples, as for the LDC and
SemCor corpora, co-occurrence information can be extracted.
Some examples are the following:
Overnight accommodations#4 are available.
Is there intelligent#1 life in the universe?
An intelligent#1 question.
The use of other semantic knowledge repositories (e.g., FrameNet9 and Verbnet10)
is currently being explored, the main problem being the need of harmonizing these
resources with the WordNet sense and relations inventory.
The information available in WordNet and in the other resources described in the
previous section is used to automatically generate a labeled directed graph (digraph)
representation of word senses. We call this a semantic graph.
Figure 7 shows an example of the semantic graphs generated for senses 1 (coach)
and 2 (conductor) of bus; in the figure, nodes represent concepts (WordNet synsets)
and edges are semantic relations. In each graph in the figure, we include only nodes
with a maximum distance of three from the central node, as suggested by the dashed
oval. This distance has been experimentally established.
The following semantic relations are used: hyperonymy (car is a kind of vehicle,
denoted with kind?of?? ), hyponymy (its inverse, has?kind?? ), meronymy (room has-part wall,
has?part?? ), holonymy (its inverse, part?of?? ), pertainymy (dental pertains-to tooth, pert??), at-
tribute (dry value-of wetness, att?), similarity (beautiful similar-to pretty, sim?), gloss (gloss??),
7 http://www.cs.unt.edu/?rada/downloads.html#semcor
8 http://www.ldc.upenn.edu/
9 http://www.icsi.berkeley.edu/?framenet/
10 http://www.cis.upenn.edu/verbnet/
161
Navigli and Velardi Learning Domain Ontologies
bus#1
public
transport#1
transport#1
school bus#1 window#2
instrumentation#1
roof#2 vehicle#1
passenger#1
traveler#1
express#2
gloss
window frame#1
protection#2
framework#3
pane#1
covering#2
kin
d-o
f kind-of
has-
part
has-part
kind
-of
has-kind
kin
d-o
f kind-of
kin
d-o
fglo
ss
kind
-of
has-p
art has-part
kind-of
plate glass#1
person#1
kind-of
has-
kind
gloss
bus#2
conductor#4
device#1electrical#2
instrumentality#3
computer#1
connection#2
wiring#1
machine#1
calculator#2
has-kind
union#4
kind
-of
kind-of
kind-of
gloss
electrical device#1
part-of
gloss
electricity#1
gloss
circuit#1
kind-
of
inter
connection#1
has-kindkind-
of
gloss
has-kind
glo
ss
kind-of
state#4
kind-
of
kin
d-
of
pert
connected#6
(a)
(b)
Figure 7
Graph representations for (a) sense 1 and (b) sense 2 of bus.
topic (
topic??), and domain ( dl?). All these relations are explicitly encoded in WordNet, ex-
cept for the last three. Topic, gloss, and domain are extracted from annotated corpora,
sense definitions, and domain labels, respectively. Topic expresses a co-occurrence rela-
tion between concepts in texts, extracted from annotated corpora and usage examples.
Gloss relates a concept to another concept occurring in its natural language defini-
tion. Finally, domain relates two concepts sharing the same domain label. In parsing
glosses, we use a stop list to eliminate the most frequent words.
The SSI algorithm. The SSI algorithm is a knowledge-based iterative approach to
word sense disambiguation. The classification problem can be stated as follows:
? t is a term
? T (the context of t) is a list of co-occurring terms, including t.
? I is a structural representation of T (the semantic context).
162
Computational Linguistics Volume 30, Number 2
? St1, St2, . . . , Stn are structural specifications of the possible senses for t
(semantic graphs).
? G is a grammar describing structural relations (semantic
interconnections) among the objects to be analyzed.
? Determine how well the structure of I matches that of each of
St1, S
t
2, . . . , S
t
n, using G.
? Select the best matching.
Structural representations are graphs, as previously detailed. The SSI algorithm con-
sists of an initialization step and an iterative step.
In a generic iteration of the algorithm, the input is a list of co-occurring terms
T = [t1, . . . , tn] and a list of associated senses I = [St1 , . . . , Stn ], that is, the semantic
interpretation of T, where Sti 11 is either the chosen sense for ti (i.e., the result of a
previous disambiguation step) or the empty set (i.e., the term is not yet disambiguated).
A set of pending terms is also maintained, P = {ti|Sti = ?}. I is referred to as the semantic
context of T and is used, at each step, to disambiguate new terms in P.
The algorithm works in an iterative way, so that at each stage either at least
one term is removed from P (i.e., at least one pending term is disambiguated) or
the procedure stops because no more terms can be disambiguated. The output is the
updated list I of senses associated with the input terms T.
Initially, the list I includes the senses of monosemous terms in T. If no monosemous
terms are found, the algorithm uses an initialization policy described later.
During a generic iteration, the algorithm selects those terms t in P showing an
interconnection between at least one sense S of t and one or more senses in I. The
likelihood that a sense S will be the correct interpretation of t, given the semantic
context I, is estimated by the function fI : Synsets ? T ? , where Synsets is the set of
all the concepts in WordNet, and defined as follows:
fI(S, t) =
{
?({?(S, S?)|S?? I}) if S ? Senses(t) ? Synsets
0 otherwise
where Senses(t) is the subset of synsets in WordNet associated with the term t, and
?(S, S?) = ??({w(e1, e2, . . . , en)|S
e1? S1
e2? . . . en?1? Sn?1
en? S?}), that is, a function (??) of
the weights (w) of each path connecting S with S?, where S and S? are represented
by semantic graphs. A semantic path between two senses S and S?, S e1? S1
e2? . . . en?1?
Sn?1
en? S?, is represented by a sequence of edge labels e1, e2, . . . , en. A proper choice
for both ? and ?? may be the sum function (or the average sum function).
A context-free grammar G = (E, N, SG, PG) encodes all the meaningful semantic
patterns. The terminal symbols (E) are edge labels, while the nonterminal symbols (N)
encode (sub)paths between concepts; SG is the start symbol of G, and PG the set of its
productions.
We associate a weight with each production A ? ? ? PG, where A ? N and
? ? (N ? E)?, that is, ? is a sequence of terminal and nonterminal symbols. If the
sequence of edge labels e1, e2, . . . , en belongs to L(G), the language generated by the
grammar, and G is not ambiguous, then w(e1, e2, . . . , en) is given by the sum of the
11 Note that with Sti we refer interchangeably to the semantic graph associated with a sense or to the
sense label (i.e., the synset).
163
Navigli and Velardi Learning Domain Ontologies
weights of the productions applied in the derivation SG ?? e1, e2, . . . , en. (The grammar
G is described in the next subsection.)
Finally, the algorithm selects St =argmax
S?Synsets
fI(S, t) as the most likely interpretation of
t and updates the list I with the chosen concept. A threshold can be applied to fI(S, t)
to improve the robustness of the system?s choices.
At the end of a generic iteration, a number of terms are disambiguated, and each
of them is removed from the set of pending terms P. The algorithm stops with output
I when no sense S? can be found for the remaining terms in P such that fI(S?, t?) > 0,
that is, P cannot be further reduced. In each iteration, interconnections can be found
only between the sense of a pending term t and the senses disambiguated during the
previous iteration.
If no monosemous words are found, we explore two alternatives: either we provide
manually the synset of the root term h (e.g., service#1 in Figure 4: work done by one
person or group that benefits another), or we fork the execution of the algorithm into
as many processes as the number of senses of the root term h. Let n be such a number.
For each process i (i = 1, . . . , n), the input is given by Ii = [?, ?, . . . , Shi , . . . , ?], where
Shi is the ith sense of h in Senses(h). Each execution outputs a (partial or complete)
semantic context Ii. Finally, the most likely context Im is obtained by choosing
m = arg max
1?i?n
?
St?Ii
fIi(S
t, t)
Figure 8 provides pseudocode for the SSI algorithm.
3.2.2 The Grammar. The grammar G has the purpose of describing meaningful inter-
connecting patterns among semantic graphs representing concepts in the ontology. We
define a pattern as a sequence of consecutive semantic relations e1?e2?. . .?en where ei ? E,
the set of terminal symbols, that is, the vocabulary of conceptual relations. Two rela-
tions ei ? ei+1 are consecutive if the edges labeled with ei and ei+1 are incoming and/or
outgoing from the same concept node, for example,
ei? S ei+1? , ei? S ei+1? , ei? S ei+1? , ei? S ei+1? .
A meaningful pattern between two senses S and S? is a sequence e1 ? e2 ? . . . ? en that
belongs to L(G).
In its current version, the grammar G has been defined manually, inspecting the
intersecting patterns automatically extracted from pairs of manually disambiguated
word senses co-occurring in different domains. Some of the rules in G are inspired
by previous work in the eXtended WordNet12 project. The terminal symbols ei are
the conceptual relations extracted from WordNet and other on-line lexical-semantic
resources, as described in Section 3.2.1.
G is defined as a quadruple (E, N, SG, PG), where E = { ekind-of, ehas-kind, epart-of,
ehas-part, egloss, eis-in-gloss, etopic, . . .}, N = { SG, Ss, Sg, S1, S2, S3, S4, S5, S6, E1, E2, . . .}, and
PG includes about 50 productions. An excerpt from the grammar is shown in Table 2.
As stated in the previous section, the weight w(e1, e2, . . . , en) of a semantic path
e1, e2, . . . , en is given by the sum of the weights of the productions applied in the
derivation SG ?? e1, e2, . . . , en. These weights have been experimentally established on
standard word sense disambiguation data, such as the SemCor corpus, and have been
normalized so that the weight of a semantic path always ranges between 0 and 1.
The main rules in G are as follows (S1 and S2 are two synsets in I):
12 http://xwn.hlt.utdallas.edu/papers.html.
164
Computational Linguistics Volume 30, Number 2
SSI(T : list of terms, I : initial list of interpretation synsets)
{
for each t ? T
if (t is monosemous) I[t] = the only sense of t
P := {t ? T : I[t] = ?}
{ while there are more terms to disambiguate }
do
{
P? := P
for each t ? P? { for each pending term }
{
bestSense := ?
maxValue := 0
{ for each possible interpretation of t }
for each sense S of t in WordNet
{
f [S] := 0
for each synset S? ? I
{
? := 0
for each semantic path e1e2 . . . en between S and S?
? := ? + w(e1e2 . . . en)
f [S] := f [S] + ?
}
if (f [S] > maxValue)
{
maxValue := f [S]
bestSense := S
}
}
if (maxValue > 0)
{
I[t] := bestSense
P := P \ {t}
}
}
} while(P = P?)
return I
}
Figure 8
The SSI algorithm in pseudocode.
165
Navigli and Velardi Learning Domain Ontologies
Table 2
Excerpt from the context-free grammar for the recognition of semantic
interconnections.
SG ? Ss|Sg (all the rules)
Ss ? S1|S2|S3 (simple rules)
S1 ? E1S1|E1 (hyperonymy/meronymy)
E1 ? ekind?of|epart?of
S2 ? E2S2|E2 (hyponymy/holonymy)
E2 ? ehas?kind|ehas?part
S3 ? ekind?ofS3ehas?kind|ekind?ofehas?kind (parallelism)
Sg ? eglossSs|S4|S5|S6 (gloss rules)
S4 ? egloss (gloss rule)
S5 ? etopic (topic rule)
S6 ? eglosseis?in?gloss (gloss + gloss?1 rule)
1. color, if S1 is in the same adjectival cluster as chromatic#3 and S2 is a
hyponym of a concept that can assume a color like physical object#1 and
food#1 (e.g., S1 ? yellow#1 and S2 ? wall#1)
2. domain, if the gloss of S1 contains one or more domain labels and S2 is a
hyponym of those labels (for example, white#3 is defined as ?(of wine)
almost colorless,? therefore it is the best candidate for wine#1 in order to
disambiguate the term white wine)
3. synonymy, if
(a) S1 ? S2 or (b) ?N ? Synsets : S1
pert?? N ? S2
(for example, in the term open air, both the words belong to synset
{ open#8, air#2, . . . , outdoors#1 })
4. hyperonymy/meronymy path, if there is a sequence of
hyperonymy/meronymy relations (for example, mountain#1
has-part??
mountain peak#1 kind-of?? top#3 provides the right sense for each word of
mountain top)
5. hyponymy/holonymy path, if there is a sequence of
hyponymy/holonymy relations (for example, in sand beach, sand#1
part-of??
beach#1);
6. parallelism, if S1 and S2 have a common ancestor (for example, in
enterprise company, organization#1 is a common ancestor of both
enterprise#2 and company#1)
7. gloss, if S1
gloss?? S2 (for example, in web site, the gloss of web#5 contains
the word site; in waiter service, the gloss of restaurant attendant#1,
hyperonym of waiter#1, contains the word service)
8. topic, if S1
topic?? S2 (for example, in the term archeological site, in which
both words are tagged with sense 1 in a SemCor file; notice that
WordNet provides no mutual information about them; also consider
picturesque village: WordNet provides the example ?a picturesque village?
for sense 1 of picturesque)
166
Computational Linguistics Volume 30, Number 2
9. gloss+hyperonymy/meronymy path, if ?G ? Synsets : S1
gloss?? G and there
is a hyperonymy/meronymy path between G and S2 (for example, in
railway company, the gloss of railway#1 contains the word organization and
company#1 kind-of?? institution#1 kind-of?? organization#1)
10. gloss+parallelism, if ?G ? Synsets : S1
gloss?? G and there is a parallelism
path between G and S2 (for example, in transport company, the gloss of
transport#3 contains the word enterprise and organization#1 is a common
ancestor of both enterprise#2 and company#1)
11. gloss+gloss, if ?G ? Synsets : S1
gloss?? G gloss?? S2 (for example, in mountain
range, mountain#1 and range#5 both contain the word hill so that the
right senses can be chosen)
12. hyperonymy/meronymy+gloss path, if ?G ? Synsets : G gloss?? S2 and there
is a hyperonymy/meronymy path between S1 and G
13. parallelism+gloss, if ?G ? Synsets : G gloss?? S2 and there is a parallelism
path between S1 and G.
3.2.3 A Complete Example. We now provide a complete example of the SSI algorithm
applied to the task of disambiguating a lexicalized tree T . With reference to Figure 4,
the list T is initialized with all the component words in T , that is, [service, train, ferry,
car, boat, car-ferry, bus, coach, transport, public transport, taxi, express, customer].
Step 1. In T there are four monosemous words, taxi, car-ferry, public transport, and
customer; therefore, we have
I = [taxi#1, car ferry#1, public transport#1, customer#1 ]
P = {service, train, ferry, car, boat, bus, coach, transport, express}.
Step 2. During the second iteration, the following rules are matched:13
{taxi} kind-of?? {car, auto}(hyper)
{taxi} kind-of?? {car, auto} kind-of?? {motor vehicle,automotive vehicle}
kind-of?? {vehicle} gloss?? {bus, autobus, coach}(hyper + gloss)
{taxi} kind-of?? {car, auto} kind-of?? {motor vehicle,automotive vehicle} kind-of?? {vehicle}
gloss?? {ferry, ferryboat}(hyper + gloss)
{bus, autobus, coach} kind-of?? {public transport}(hyper)
{car ferry} kind-of?? {ferry, ferryboat}(hyper)
13 More than one rule may contribute to the disambiguation of a term. We list here only some of the
detected patterns.
167
Navigli and Velardi Learning Domain Ontologies
{customer, client} topic?? {service}(topic)
{service} gloss?? {person, someone} has-kind?? {consumer}
has-kind?? {customer, client}(gloss + hypo)
{train, railroad train} kind-of?? {public transport}(hyper)
{express, expressbus} kind-of?? {bus, autobus, coach} kind-of?? {public transport}(hyper)
{conveyance, transport} has-kind?? {public transport}(hypo)
obtaining:
I = [taxi#1, car ferry#1, public transport#1, customer#1, car#1, ferry#1, bus#1,
coach#5, train#1, express#2, transport#1, service#1] 14
P = {boat}.
Step 3.
{boat} has-kind?? {ferry, ferryboat}(hypo)
I = [taxi#1, car ferry#1, public transport#1, customer#1, car#1, ferry#1, bus#1,
coach#5, train#1, express#2, boat#1, transport#1, service#1 ]
P = ?.
Then the algorithm stops since the list P is empty.
3.2.4 Creating Domain Trees. During the execution of the SSI algorithm, (possibly)
all the terms in a lexicalized tree T are disambiguated. Subsequently, we proceed as
follows:
a. Concept clustering: Certain concepts can be clustered in a unique
concept on the basis of pertainymy, similarity, and synonymy (e.g.,
manor house and manorial house, expert guide and skilled guide, bus service
and coach service, respectively); notice again that we detect semantic
relations between concepts, not words. For example, bus#1 and coach#5
are synonyms, but this relation does not hold for other senses of these
two words.
b. Hierarchical structuring: Taxonomic information in WordNet is used to
replace syntactic relations with kind-of relations (e.g., ferry service kind-of??
boat service), on the basis of hyperonymy, rather than string inclusion as
in T .
14 Notice that bus#1 and coach#5 belong to the same synset, therefore they are disambiguated by the same
rule.
168
Computational Linguistics Volume 30, Number 2
service
transport service
car service public transport service car service#2 boat service
coach service, bus service train servicebus service#2 taxi service
coach service#2
express service#2express service
coach service#3 ferry service
car-ferry service
customer service
Figure 9
Domain concept tree.
Each lexicalized tree T is finally transformed into a domain concept tree ?. Fig-
ure 9 shows the concept tree obtained from the lexicalized tree of Figure 4. For the
sake of legibility, in Figure 9 concepts are labeled with the associated terms (rather
than with synsets), and numbers are shown only when more than one semantic in-
terpretation holds for a term. In fact, it is possible to find more than one matching
hyperonymy relation. For example, an express can be a bus or a train, and both inter-
pretations are valid, because they are obtained from relations between terms within
the domain.
3.2.5 Adding Conceptual Relations. The second phase of semantic interpretation in-
volves finding the appropriate semantic relations holding among concept components.
In order to extract semantic relations, we need to do the following:
? Select an inventory of domain-appropriate semantic relations.
? Learn a formal model to select the relations that hold between pairs of
concepts, given ontological information on these concepts.
? Apply the model to semantically relate the components of a complex
concept.
First, we selected an inventory of semantic relations types. To this end, we con-
sulted John Sowa?s (1984) formalization on conceptual relations, as well as other
studies conducted within the CoreLex,15 FrameNet, and EuroWordNet (Vossen 1998)
projects. In the literature, no systematic definitions are provided for semantic relations;
therefore we selected only the more intuitive and widely used ones.
To begin, we selected a kernel inventory including the following 10 relations,
which we found pertinent (at least) to the tourism and finance16 domains: place (e.g.,
room PLACE?? service, which reads ?the service has place in a room? or ?the room is
the place of service?), time (afternoon TIME?? tea), matter (ceramics MATTER?? tile), topic (art
TOPIC?? gallery), manner (bus MANNER?? service), beneficiary (customer BENEF?? service), purpose
(booking PURPOSE?? service), object (wine OBJ?? production), attribute (historical ATTR?? town),
15 http://www.cs.brandeis.edu/?paulb/CoreLex/corelex.html
16 Financial terms are extracted from the Wall Street Journal.
169
Navigli and Velardi Learning Domain Ontologies
characteristics (first-class CHRC?? hotel). This set can be easily adapted or extended to
other domains.
In order to associate the appropriate relation(s) that hold among the components
of a domain concept, we decided to use inductive machine learning. In inductive
learning, one has first to manually tag with the appropriate semantic relations a subset
of domain concepts (this is called the learning set) and then let an inductive learner
build a tagging model. Among the many available inductive learning programs, we
experimented both with Quinlan?s C4.5 and with TiMBL (Daelemans et al 1999).
An inductive learning system requires selecting a set of features to represent in-
stances in the learning domain. Instances in our case are concept-relation-concept
triples (e.g., wine
OBJ?? production), where the type of relation is given only in the learning
set.
We explored several alternatives for feature selection. We obtained the best result
when representing each concept component by the complete list of its hyperonyms
(up to the topmost), as follows:
feature ? vector[[list of hyperonyms]?modifier[list of hyperonyms]head]
For example, the feature vector for tourism operator, where tourism is the modifier and
operator is the head, is built as the sequence of hyperonyms of tourism#1: [tourism#1,
commercial enterprise#2, commerce#1, transaction#1, group-action#1, act#1, human-
action#1], followed by the sequence of hyperonyms for operator#2 [operator#2, capi-
talist#2, causal agent#1, entity#1, life form#1, person#1, individual#1].
Features are converted into a binary representation to obtain vectors of equal
length. We ran several experiments, using a tagged set of 405 complex concepts, a
varying fragment of which were used for learning, the remainder for testing (we used
two-fold cross-validation). Overall, the best experiment provided a 6% error rate over
405 examples and produced around 20 classification rules.
The following are examples of extracted rules (from C4.5), along with their confi-
dence factor (in parentheses) and examples:
If in modifier [knowledge domain#1, knowledge base#1 ]
= 1 then relation THEME(63%)
Examples : arts festival, science center
If in modifier [building material#1 ] = 1 then relation MATTER(50%)
Examples : stave church, cobblestone street
If in modifier [conveyance#3, transport#1 ] = 1 and in head[act#1,human act#1 ]
= 1 then relation MANNER(92.2%)
Examples : bus service, coach tour
Selection and extraction of conceptual relations is one of the active research areas in
the OntoLearn project. Current research is directed toward the exploitation of on-line
resources (e.g., the tagged set of conceptual relations in FrameNet) and the automatic
170
Computational Linguistics Volume 30, Number 2
generation of glosses for complex concepts (e.g., for travel service we have travel#1
PURPOSE?? service#1: ?a kind of service, work done by one person or group that benefits
another, for travel, the act of going from one place to another?). Automatic generation
of glosses (see Navigli et al [2004] for preliminary results) relies on the compositional
interpretation criterion, as well as the semantic information provided by conceptual
relations.
3.3 Phase 3: Ontology Integration
The domain concept forest generated by OntoLearn is used to trim and update Word-
Net, creating a domain ontology. WordNet is pruned and trimmed as follows:
? After the domain concept trees are attached to the appropriate nodes in
WordNet in either a manual or an automatic manner, all branches not
containing a domain node can be removed from the WordNet hierarchy.
? An intermediate node in WordNet is pruned whenever the following
conditions all hold:
1. It has no ?brother? nodes.
2. It has only one direct hyponym.
3. It is not the root of a domain concept tree.
4. It is not at a distance greater than two from a WordNet unique
beginner (this is to preserve a ?minimal? top ontology).
Figure 10 shows an example of pruning the nodes located over the domain concept
tree rooted at wine#1. The appendix shows an example of a domain-adapted branch
of WordNet in the tourism domain.
4. Evaluation
The evaluation of ontologies is recognized to be an open problem.17 Though the num-
ber of contributions in the area of ontology learning and construction has considerably
increased in the past few years, especially in relation to the forthcoming Semantic Web,
experimental data on the utility of ontologies are not available, other than those in Far-
quhar et al (1998), in which an analysis of user distribution and requests is presented
for the Ontology Server system. A better performance indicator would have been the
number of users that access the Ontology Server on a regular basis, but the authors
mention that regular users account for only a small percentage of the total. Efforts
have recently being made on the side of ontology evaluation tools and methods, but
available results are on the methodological rather than on the experimental side. The
ontology community is still in the process of assessing an evaluation framework.
We believe that, in absence of a commonly agreed-upon schema for analyzing the
properties of an ontology, the best way to proceed is evaluating an ontology within
some existing application. Our current work is precisely in this direction: The results of
a terminology translation experiment appear in Navigli, Velardi, and Gangemi (2003),
while preliminary results on a query expansion task are presented in Navigli and
Velardi (2003).
17 OntoWeb D.1.3 Tools (2001), ?Whitepaper: Ontology Evaluation Tools,? available at
http://www.aifb.unikarlsruhe.de/WBS/ysu/publications/eon2002 whitepaper.pdf
171
Navigli and Velardi Learning Domain Ontologies
drug of abuse#1
object#1
substance#1
fluid#1
liquid#1
 
 
artifact#1
drug#1
wine#1
food#1
beverage#1
alchool#1
entity#1(a)
object#1
substance#1
fluid#1
liquid#1
 
 
artifact#1
drug#1
drug of abuse#1
wine#1
food#1
beverage#1
alchool#1
entity#1(b)
object#1
substance#1
fluid#1
liquid#1
  
  
artifact#1
drug#1
wine#1
food#1
entity#1(d)
object#1
substance#1
fluid#1
liquid#1
 
 
artifact#1
drug#1
drug of abuse#1
wine#1
food#1
beverage#1
entity#1(c)
Figure 10
Pruning steps over the domain concept tree for wine1.
In this evaluation section we proceed as follows: First, we provide an account of
the feedback that we obtained from tourism experts participating in the Harmonise
EC project on interoperability in the tourism domain. Then, we evaluate in detail the
SSI algorithm, which is the ?heart? of the OntoLearn methodology.
4.1 OntoLearn as a Support for Ontology Engineers
During the first year of the Harmonise project, a core ontology of about three hundred
concepts was developed using ConSys and SymOntoX. In parallel, we collected a
corpus of about one million words from tourism documents, mainly descriptions of
travels and tourism sites. From this corpus, OntoLearn extracted an initial list of 14,383
172
Computational Linguistics Volume 30, Number 2
candidate terms (the first phase of terminology extraction in Section 3.1), from which
the system derived a domain concept forest of 3,840 concepts, which were submitted
to the domain experts for ontology updating and integration.
The Harmonise ontology partners lacked the requisite expertise to evaluate the
WordNet synset associations generated by OntoLearn for each complex term, therefore
we asked them to evaluate only the domain appropriateness of the terms, arranged in
a hierarchical fashion (as in Figure 9). We obtained a precision ranging from 72.9% to
about 80% and a recall of 52.74%.18 The precision shift is due to the well-known fact
that experts may have different intuitions about the relevance of a concept for a given
domain. The recall estimate was produced by manually inspecting 6,000 of the initial
14,383 candidate terms, asking the experts to mark all the terms judged as ?good?
domain terms, and comparing the obtained list with the list of terms automatically
filtered by OntoLearn (the phase of terminology filtering described in Section 3.1).
As a result of the feedback obtained from the tourism experts, we decided that
experts? interpretation difficulties could indeed be alleviated by associating a textual
definition with each new concept proposed by OntoLearn. This new research (auto-
matic generation of glosses) was mentioned in Section 3.2.5. We still need to produce
an in-field evaluation of the improved readability of the ontology enriched with textual
definitions.
In any case, OntoLearn favored a considerable speed up in ontology development,
since shortly after we provided the results of our OntoLearn tool, the Harmonise
ontology reached about three thousand concepts. Clearly, the definition of an initial
set of basic domain concepts is sufficiently crucial, to justify long-lasting and even
heated discussions. But once an agreement is reached, filling the lower levels of the
ontology can still take a long time, simply because it is a tedious and time-consuming
task. Therefore we think that OntoLearn revealed itself indeed to be a useful tool
within Harmonise.
4.2 Evaluation of the SSI Word Sense Disambiguation Algorithm
As we will argue in Section 5, one of the novel aspects of OntoLearn with respect
to current ontology-learning literature is semantic interpretation of extracted terms.
The SSI algorithm described in section 3.2 was subjected to several evaluation exper-
iments by the authors of this article. The output of these experiments was used to
tune certain heuristics adopted by the algorithm, for example, the dimension of the
semantic graph (i.e., the maximum distance of a concept S? from the central concept
S) and the weights associated with grammar rules. To obtain a domain-independent
tuning, tuning experiments were performed applying the SSI algorithm on standard
word sense disambiguation data,19 such as SemCor and Senseval all-words.20
However, OntoLearn?s main task is terminology disambiguation, rather than plain
word sense disambiguation. In complex terms, words are likely to be more tightly se-
mantically related than in a sentence; therefore the SSI algorithm seems more
appropriate.21 To test the SSI algorithm, we selected 650 complex terms from the set of
3,840 concepts mentioned in Section 4.1, and we manually assigned the appropriate
18 In a paper specifically dedicated to terminology extraction and evaluation (Velardi, Missikoff, and
Basili 2001) we performed an evaluation also on an economics domain, with similar results.
19 In standard WSD tasks, the list T in input to the SSI algorithm is the set of all words in a sentence
fragment to be disambiguated.
20 http://www.itri.brighton.ac.uk/events/senseval/ARCHIVE/resources.html#test
21 For better performance on a standard WSD task, it would be essential to improve lexical knowledge of
verbs (e.g. by integrating VerbNet and FrameNet, as previously mentioned), as well as to enhance the
grammar.
173
Navigli and Velardi Learning Domain Ontologies
81
.1
5%
79
.7
8%
83
.1
2%
84
.5
6%
79
.0
5%
75
.1
8%
73
.2
2%
77
.8
3%
79
.7
6%
72
.2
4%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
a
ll 
ru
le
s
e
xc
l. 
7,
 9
,
10
, 1
2,
 1
3
e
xc
l. 
9
e
xc
l. 
11
,
12
, 1
3
e
xc
l. 
7,
 8
,
9,
 1
0,
 1
2,
13
including monosemous terms
excluding monosemous terms
Figure 11
Different runs of the semantic disambiguation algorithm when certain rules in the grammar G
are removed.
WordNet synset to each word composing the term. We used two annotators to ensure
some degree of objectivity in the test set. In this task we experienced difficulties al-
ready pointed out by other annotators, namely, that certain synsets are very similar, to
the point that choosing one or the other?even with reference to our specific tourism
domain?seemed a mere guess. Though we can?t say that our 650 tagged terms are
a ?gold standard,? evaluating OntoLearn against this test set still produced interest-
ing outcomes and a good intuition of system performance. Furthermore, as shown by
the example of Section 3.2.3, OntoLearn produces a motivation for its choices, that
is, the detected semantic patterns. Though it was not feasible to analyze in detail all
the output of the system, we found more than one example in which the choices
of OntoLearn were more consistent22 and more convincing than those produced by
the annotators, to the point that OntoLearn could also be used to support human
annotators in disambiguation tasks.
First, we evaluated the effectiveness of the rules in G (Section 3.2.2) in regard to the
disambiguation algorithm. Since certain rules are clearly related (for example, rules 4
and 5, rules 9 and 11), we computed the precision of the disambiguation when adding
or removing groups of rules. The results are shown in Figure 11. The shaded bars in
the figure show the results obtained when those terms containing unambiguous words
are removed from the set of complex terms.
We found that the grammar rules involving the gloss and hyperonym relations
contribute more than others to the precision of the algorithm. Certain rules (not listed
in 3.2.2 since they were eventually removed) were found to produce a negative effect.
All the rules described in 3.2.2 were found to give more or less a comparable positive
contribution to the final performance.
22 Consistent at least with respect to the lexical knowledge encoded in WordNet.
174
Computational Linguistics Volume 30, Number 2
84
.5
6%
82
.2
5%
75
.7
4%
73
.8
0%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
manually dis. head fully automatic
precision recall
Figure 12
Precision and recall for the terminology disambiguation task: manual disambiguation of the
head and fully automatic disambiguation.
The precision computed in Figure 11 refers to the case in which the head node of
each term tree is sense-tagged manually. In Figure 12 the light and dark bars represent
precision and recall, respectively, of the algorithm when the head (i.e., the root) of
a term tree is manually assigned and when the disambiguation is fully automatic.
The limited drop in performance (2%) of the fully automated task with respect to
manual head disambiguation shows that, indeed, the assumption of a strong semantic
interrelationship between the head and the other terms of the term tree is indeed
justified.
Finally, we computed a baseline, comparing the performance of the algorithm with
that obtained by a method that always chooses the first synset for each word in a com-
plex term. (We remind readers that in WordNet, the first sense is the most probable.)
The results are shown in Figure 13, where it is seen, as expected, that the increment
in performance with respect to the baseline is higher (around 5%) when only polyse-
mous terms are considered. A 5% difference (3% with respect to the fully automatic
disambiguation) is not striking, however, the tourism domain is not very technical,
and often the first sense is the correct one. We plan in the future to run experiments
with more technical domains, for example, economics or software products.
5. Related Work
Comprehensive ontology construction and learning has been an active research field
in the past few years. Several workshops23 have been dedicated to ontology learning
and related issues. The majority of papers in this area propose methods for extending
an existing ontology with unknown words (e.g., Agirre et al 2000 and Alfonseca and
Manandhar 2002). Alfonseca and Manandhar present an algorithm to enrich WordNet
with unknown concepts on the basis of hyponymy patterns. For example, the pattern
hypernism(N2, N1) :?appositive(N2, N1) captures a hyponymy relation between Shake-
speare and poet in the appositive NP ?Shakespeare, the poet.? This approach heavily
23 ECAI-2000 First Workshop on Ontology Learning (http://ol2000.aifb.uni-karlsruhe.de/) and
IJCAI-2001 Second Workshop on Ontology Learning (http://ol2001.aifb.uni-karlsruhe.de/).
175
Navigli and Velardi Learning Domain Ontologies
84
.5
6%
80
.6
1%
79
.7
6%
73
.9
6%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
best result baseline
including monosemous terms
excluding monosemous terms
Figure 13
Comparison with a baseline.
depends upon the ability of discovering such patterns, however, it appears a useful
complementary strategy with respect to OntoLearn. OntoLearn, in fact, is unable to
analyze totally unknown terms (though ongoing research is in progress to remedy this
limitation). Berland and Charniak (1999) propose a method for extracting whole-part
relations from corpora and enrich an ontology with this information. Few papers pro-
pose methods of extensively enriching an ontology with domain terms. For example,
Vossen (2001) uses statistical methods and string inclusion to create lexicalized trees,
as we do (see Figure 4). However, no semantic disambiguation of terms is performed.
Very often, in fact, ontology-learning papers regard domain terms as concepts. A statis-
tical classifier for automatic identification of semantic roles between co-occuring terms
is presented in Gildea and Jurafsky (2002). In order to tag texts with the appropriate
semantic role, Gildea and Jurafsky use a training set of fifty thousand sentences man-
ually annotated within the FrameNet semantic labeling project. Finally, in Maedche
and Staab (2000, 2001), an architecture is presented to help ontology engineers in the
difficult task of creating an ontology. The main contribution of this work is in the
area of ontology engineering, although machine-learning methods are also proposed
to automatically enrich the ontology with semantic relations.
6. Conclusions and Ongoing Developments
We believe that the OntoLearn system is innovative in several respects:
1. in presenting an overall ontology development system.
2. in stressing the importance of appropriate terminology extraction to the
ontology-building enterprise.
3. in avoiding a common confusion between domain terms and domain
concepts, since it performs a semantic interpretation of terms. This is indeed
the strongest aspect of our method.
176
Computational Linguistics Volume 30, Number 2
4. in presenting a new structural approach to sense classification (SSI). This
method is general and has been applied to other sense disambiguation
tasks, such as sense-based query expansion (Navigli and Velardi 2003)
and gloss disambiguation (Gangemi, Navigli, and Velardi 2003).
Ontology learning is a complex enterprise, and much is left to be done. We list
here some of the drawbacks and gaps of our method, along with hints for ongoing
and future developments. OntoLearn is in fact a fully active area of research within
our group.
1. The SSI method presupposes that each term component has at least one
synset in WordNet. In our ongoing research, we try to cope with this
limitation, parsing textual definitions in glossaries (e.g., in a computer
network application) whenever a term cannot be interpreted
compositionally in WordNet. Terms in glossaries are first arranged in
trees according to detected taxonomic relations, then the head terms of
each tree are attached to the appropriate node of WordNet, if an
appropriate node indeed exists. Rule-based and algebraic methods are
jointly used to construct term trees and to compute measures of the
similarity between the textual definitions in glossaries and those in
WordNet.
2. OntoLearn detects taxonomic relations between complex concepts and
other types of semantic relations among the components of a complex
concept. However, an ontology is more than a taxonomy. The result of
concept disambiguation in OntoLearn is more than an ordered list of
synsets, since we obtain semantic nets and intersecting patterns among
them (Section 3.2.2). This information is not currently exploited to
generate richer concept definitions. A preliminary attempt to generate
formal concept definitions from informal ones is described in Gangemi,
Navigli, and Velardi (2003). Furthermore, an automatic gloss generation
algorithm has been defined (Navigli et al 2004).
3. A large-scale evaluation is still to be done. As we have already pointed
out, evaluation of ontologies is recognized as an open problem, and few
results are available, mostly on the procedural (?how to?) side. We partly
evaluated OntoLearn in an automatic translation task (Navigli, Velardi,
and Gangemi 2003), and the SSI algorithm in generic WSD tasks as
mentioned in item 4 of the previous list. In addition, it would be
interesting to run OntoLearn on different domains, in order to study the
effect of higher or lower levels of ambiguity and technicality on the
output domain ontology.
Appendix: A Fragment of Trimmed WordNet for the Tourism Domain
{ activity%1 }
{ work%1 }
{ project:00508925%n }
{ tourism project:00193473%n }
{ ambitious project:00711113%a }
{ service:00379388%n }
177
Navigli and Velardi Learning Domain Ontologies
{ travel service:00191846%n }
{ air service#2:00202658%n }
{ air service#4:00194802%n }
{ transport service:00716041%n }
{ ferry service#2:00717167%n }
{ express service#3:00716943%n }
{ exchange service:02413424%n }
{ guide service:04840928%n }
{ restaurant service:03233732%n }
{ rail service:03207559%n }
{ customer service:07197309%n }
{ guest service:07304921%n }
{ regular service#2:07525988%n }
{ outstanding customer service:02232741%a }
{ tourism service:00193473%n }
{ waiter service:07671545%n }
{ regular service:02255650%a,scheduled service:02255439%a }
{ personalized service:01703424%a,personal service:01702632%a }
{ secretarial service:02601509%a }
{ religious service:02721678%a }
{ church service:00666912%n }
{ various service:00462055%a }
{ helpful service:02376874%a }
{ quality service:03714294%n }
{ air service#3:03716758%n }
{ room service:03250788%n }
{ maid service:07387889%n }
{ laundry service:02911395%n }
{ car service#5:02364995%n }
{ hour room service:10938063%n }
{ transport service#2:02495376%n }
{ car service:02383458%n }
{ bus service#2:02356871%n }
{ taxi service:02361877%n }
{ coach service#2:02459686%n }
{ public transport service:03184373%n }
{ bus service:02356526%n,coach service:02356526%n }
{ express service#2:02653414%n }
{ local bus service:01056664%a }
{ train service:03528724%n }
{ express service:02653278%n }
{ car service#2:02384604%n }
{ coach service#3:03092927%n }
{ boat service:02304226%n }
{ ferry service:02671945%n }
{ car-ferry service:02388365%n }
{ air service:05270417%n }
{ support service:05272723%n }
178
Computational Linguistics Volume 30, Number 2
References
Agirre, Eneko, Olatz Ansa, Eduard, Hovy,
and David Mart??nez. 2000. Enriching very
large ontologies using the WWW. In ECAI
Ontology Learning Workshop 2000, available
at http://ol2000.aifb.uni-karlsruhe.de/
Alfonseca, Enrique and Suresh Manandhar.
2002. Improving an ontology refinement
Method with hyponymy patterns. In
Language Resources and Evaluation
(LREC-2002), LasPalmas, Spain, May.
Basili, Roberto, Maria Teresa Pazienza, and
Paola Velardi. 1996. An empirical
symbolic approach to natural language
processing, Artificial Intelligence, 85(1?2):
59?99.
Basili, Roberto, Maria Teresa Pazienza, and
Fabio Massimo Zanzotto. 1998. A robust
parser for information extraction. In
Proceedings of the European Conference on
Artificial Intelligence (ECAI ?98), Brighton,
U.K., August.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large corpora.
In Proceedings of the the 37th Annual Meeting
of the Association for Computational
Linguistics (ACL-99), College Park, MD.
Berners-Lee, Tim. 1999. Weaving the Web.
Harper, San Francisco.
Bunke, Horst and Alberto Sanfeliu, editors.
1990. Syntactic and Structural Pattern
Recognition: Theory and Applications. World
Scientific.
Church, Kenneth Ward and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In ACL-89,
Vancouver, British Columbia, Canada.
Daelemans, Walter, Jakub Zavrel, Ko van
der Sloot, and Antal van den Bosch. 1999.
TiMBL: Tilburg Memory Based Learner,
version 2.0, reference manual. Technical
Report ILK-9901, ILK, Tilburg University,
Tilburg, the Netherlands.
Farquhar, Adam, Richard Fikes, Wanda
Pratt, and James Rice. 1998.
?Collaborative Ontology Construction for
Information Integration.? http://www-
ksl-svc.stanford.edu:5915/doc/project-
papers.html.
Fellbaum, Christiane, editor. 1995. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Formica, Anna, and Michele Missikoff. 2003.
Ontology Validation in OPAL. In 2003
International Conference on Web Services
(ICWS? 03), Las Vegas, NV. Springer.
Gangemi, Aldo, Nicola Guarino, Claudio
Masolo, Alessandro Oltramari, and Luc
Schneider. 2001. Sweetening ontologies
with DOLCE. In Proceedings of EKAW02,
Siguenza, Spain. Springer, pages 166?181
Gangemi, Aldo, Roberto Navigli, and Paola
Velardi. 2003. Axiomatising WordNet: A
hybrid methodology. In Workshop on
Human Language Technology for the Semantic
Web and Web Services, Held in Conjunction
with Second International Semantic Web
Conference, Sanibel Island, FL.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3): 245?288.
Jacquemin, Christian. 1997. Guessing
morphology from terms and corpora. In
Proceedings of the 20th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR?
97), Philadelphia, PA, pages 156?167.
Lenat, Douglas. 1993. CYC: A large scale
investment in knowledge infrastructure.
Communications of the ACM, 3(11).
Maedche, Alexander and Steffen Staab.
2000. Semi-automatic engineering of
ontologies from text. In Proceedings of the
12th International Conference on Software
Engineering and Knowledge Engineering
(SEKE?2000), Chicago, IL.
Maedche, Alexander and Steffen Staab 2001.
Ontology learning for the semantic web.
IEEE Intelligent Systems, 16(2): 72?79.
Magnini, Bernardo and Gabriella Cavaglia.
2000. Integrating subject field codes into
WordNet. In Proceedings of the second
International Conference on Language
Resources and Evaluation (LREC2000),
Atenes.
Missikoff, Michele and X.F. Wang. 2001.
Consys?A group decision-making
support system for collaborative ontology
building. In Proceedings of Group Decision &
Negotiation 2001 Conference, La Rochelle,
France.
Navigli, Roberto, and Paola Velardi. 2003.
An analysis of ontology-based query
expansion strategies. Workshop on Adaptive
Text Extraction and Mining, held in
conjunction with ECML 2003, Cavtat
Dubrovnik, Croatia, September 22.
Navigli, Roberto, Paola Velardi, Alessandro
Cucchiarelli, and Francesca Neri. 2004.
Extending and enriching WordNet with
OntoLearn. In Second Global WordNet
Conference, Brno, Czech Republic, January
20?23. Springer-Verlag.
Navigli, Roberto, Paola Velardi, and Aldo
Gangemi. 2003. Corpus driven ontology
learning: A method and its application to
automated terminology translation, IEEE
Intelligent Systems, 18(1): 11?27.
179
Navigli and Velardi Learning Domain Ontologies
Oltramari, Alessandro, Aldo Gangemi,
Nicola Guarino, and Claudio Masolo.
2002. Restructuring WordNet?s top-level:
The OntoClean approach. In Proceedings of
the International Conference on Language
Resources and Evaluation (LREC2002), Las
Palmas, Spain.
Smith, Barry and Christopher A. Welty.
2001. Ontology: Towards a new synthesis.
Formal Ontology in Information Systems,
ACM Press.
Sowa, John F. 1984. Conceptual Structures:
Information Processing in Mind and Machine.
Addison-Wesley, Reading, MA.
Uschold, Mike and Michael Gruninger.
1996. Ontologies: Principles, methods and
applications. Knowledge Engineering
Review, 11(2).
Velardi, Paola, Michele Missikoff, and
Roberto Basili. 2001. Identification of
relevant terms to support the construction
of domain ontologies. In ACL-EACL
Workshop on Human Language Technologies,
Toulouse, France, July.
Vossen, Piek, editor. 1998. EuroWordNet: A
Multilingual Database with Lexical Semantic
Networks. Kluwer Academic, Dordrecht,
Netherlands.
Vossen, Piek. 2001. Extending, trimming and
fusing WordNet for technical documents.
In NAACL 2001 Workshop on WordNet and
Other Lexical Resources, Pittsburgh, July.
Yamamoto, Mikio and Kenneth W. Church.
2001. Using suffix arrays to compute term
frequency and document frequency for all
substrings in a corpus. Computational
Linguistics, 27(1): 1?30.
Yokoi, Toshio. 1993. The EDR electronic
dictionary. Communications of the ACM,
38(11): 42?44.
Consistent Validation of Manual and
Automatic Sense Annotations with
the Aid of Semantic Graphs
Roberto Navigli?
Universita` di Roma ?La Sapienza?
The task of annotating texts with senses from a computational lexicon is widely recognized to be
complex and often subjective. Although strategies like interannotator agreement and voting can
be applied to deal with the divergences between sense taggers, the consistency of sense choices
with respect to the reference dictionary is not always guaranteed.
In this article, we introduce Valido, a visual tool for the validation of manual and auto-
matic sense annotations. The tool employs semantic interconnection patterns to smooth possible
divergences and support consistent decision making.
1. Introduction
Sense tagging is the task of assigning senses chosen from a computational lexicon to
words in context. This is a task where both machines and humans find it difficult to
reach an agreement. The problem depends on a variety of factors, ranging from the
inherent subjectivity of the task to the granularity of sense discretization, coverage of
the reference dictionary, etc.
The problem of validation is even amplified when sense tags are collected through
acquisition interfaces like the Open Mind Word Expert (Chklovski and Mihalcea 2002),
due to the unknown source of the contributions of possibly unskilled volunteers.
Strategies like voting for automatic sense annotations and the use of interannotator
agreement with adjudication for human sense assignments only partially solve the issue
of disagreement. Especially when there is no clear preference towards a certain word
sense, the final choice made by a judge can be subjective, if not arbitrary. This is a case
where analyzing the intrinsic structure of the reference lexicon is essential for producing
a consistent decision. A lexicographer is indeed expected to review a number of related
dictionary entries in order to adjudicate a sense coherently. This work can be tedious,
time-consuming, and often incomplete due to the complex structure of the resource. As
a result, inconsistent choices can be made.
In this article, we present Valido, a tool for supporting the validation of both manual
and automatic sense annotations through the use of semantic graphs, particularly of
semantic interconnection patterns (Navigli and Velardi 2005).
? Dipartimento di Informatica, Universita` di Roma ?La Sapienza,? Via Salaria, 113 - 00198 Roma, Italia.
E-mail: navigli@di.uniroma1.it.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 2
2. Semantic Networks and Semantic Interconnection Patterns
Semantic networks are a graphical notation developed to represent knowledge expli-
citly as a set of conceptual entities and their interrelationships. The availability of wide-
coverage computational lexicons like WordNet (Fellbaum 1998), as well as semantically
annotated corpora like SemCor (Miller et al 1993), has certainly contributed to the
exploration and exploitation of semantic graphs for several tasks like the analysis of
lexical text cohesion (Morris and Hirst 1991), word sense disambiguation (Agirre and
Rigau 1996; Mihalcea and Moldovan 2001), and ontology learning (Navigli and Velardi
2004), etc.
Recently, a knowledge-based algorithm for word sense disambiguation called
structural semantic interconnections (SSI, http://lcl.di.uniroma1.it/ssi) (Navigli and
Velardi 2004, 2005), has been shown to provide interesting insights into the choice of
word senses by providing structural justifications in terms of semantic graphs. Given
a word context and a lexical knowledge base (LKB), obtained by integrating WordNet
with annotated corpora and collocation resources (Navigli 2005), SSI selects a semantic
graph including those word senses having a higher degree of interconnection, according
to a measure of connectivity.
A semantic interconnection pattern is a relevant sequence of edges selected ac-
cording to a context-free grammar, i.e., a path connecting a pair of word senses
(dark nodes in Figure 1), possibly including a number of intermediate concepts (light
nodes in Figure 1). For example, if the context of words to be disambiguated is [cross-v,
street-n, intersection-n], the senses chosen by SSI with respect to WordNet are [cross-v#1,
street#2, intersection#2],1 supported, among others, by the pattern intersection#2
??
part?of road#1 ??kind?of thoroughfare#1 ??kind?of street#2. Semantic interconnection patterns
are inspired by several works on semantic relatedness and similarity (Rada et al 1989;
Hirst and St-Onge 1998; Mihalcea and Moldovan 2001).
An excerpt of the manually written context-free grammar encoding semantic inter-
connection patterns for the WordNet lexicon is reported in Table 1. For further details
the reader can refer to Velardi 2005.
3. Supporting Validation with Semantic Interconnection Patterns
The validation task can be defined as follows: Let w be a word in a sentence ?,
previously annotated by a set of annotators A = {a1, a2, ..., an} each providing a sense
for w, and let S = {s1, s2, ..., sm} ? Senses(w) be the set of senses chosen for w by the
annotators in A, where Senses(w) is the set of senses of w in the reference inven-
tory (e.g., WordNet). A validator is asked to validate, that is, to adjudicate a sense
s ? Senses(w) for a word w over the others. Notice that s is a word sense for w in the
sense inventory, but is not necessarily in S, although it is likely to be. Also note that
the annotators in A can be either human or automatic, depending upon the purpose of
the exercise.
Based on SSI, we developed a visual tool, Valido (http://lcl.di.uniroma1.it/valido),
to support the validator in the difficult task of assessing the quality and suitability of
sense annotations. The tool takes as input a corpus of documents whose sentences are
1 We indicate a word sense with the convention w-p#i, where w is a word, p its part of speech (n for
nouns, a for adjectives, v for verbs, r for adverbs) and i its sense number in the reference inventory.
For readability, in the following we omit the noun part of speech.
274
Navigli Consistent Validation of Manual and Automatic Sense Annotations
Figure 1
Structural interconnection patterns for the sentence We crossed the street near the intersection when
sense #2 of street is chosen, as suggested by the validation policy (?).
tagged by one or more annotators with word senses from the WordNet inventory. The
user can then browse the sentences and adjudicate a choice over the others in case of
disagreement among the annotators. To the end of facilitating the user in the validation
task, the tool highlights each word in a sentence with different colors, namely, green for
words having a full agreement, red for words where no agreement can be found, and
orange for those words to which a validation policy can be applied.
Table 1
An excerpt of the context-free grammar for the recognition of semantic interconnections.
S ? S?S1|S?S2|S?S3 (start rule)
S? ? eNOMINALIZATION|ePERTAINYMY| (part-of-speech jump)
S1 ? eKIND?OFS1|ePART?OFS1|eKIND?OF|ePART?OF (hyperonymy/meronymy)
S2 ? eKIND?OFS2|eRELATEDNESSS2|eKIND?OF|eRELATEDNESS (hypernymy/relatedness)
S3 ? eSIMILARITYS3|eANTONYMYS3|eSIMILARITY|eANTONYMY (adjectives)
275
Computational Linguistics Volume 32, Number 2
A validation policy is a strategy for suggesting a default sense choice to the val-
idator in case of disagreement. Initially, the validator can choose one of four vali-
dation policies to be applied to those words with disagreement on which sense to
assign:
(?) majority voting: If there exists a sense s ? S such that
|{a ? A | a annotated w with s}|
|A| ?
1
2 ,
s is proposed as the preferred sense for w.
(?) majority voting + SSI: The same as the previous policy, with the addition
that if there exists no sense chosen by a majority of annotators, SSI is
applied to w, and the sense chosen by the algorithm, if any, is proposed
to the validator.
(?) SSI: The SSI algorithm is applied to w, and the chosen sense, if any, is
proposed to the validator.
(?) no validation: w is left untagged.
Notice that for policies (?) and (?) Valido applies the SSI algorithm to w in the
context of its sentence ? by taking into account for disambiguation only the senses in S
(i.e., the set of senses chosen by the annotators). In general, given a set of words with
disagreement W ? ?, SSI is applied to W using as a fixed context the agreed senses
chosen for the words in ? \ W.
Also note that the suggestion of a sense choice, marked in orange based on the
validation policy, is just a proposal and can be freely modified by the validator, as
explained hereafter.
Before starting the interface, the validator can also choose whether to add a vir-
tual annotator aSSI to the set of annotators A. This virtual annotator tags each word
w ? ? with the sense chosen by the application of the SSI algorithm to ?. As a re-
sult, the selected validation policy will be applied to the new set of annotators A? =
A ? {aSSI}. This is useful especially when |A| = 1 (e.g., in the automatic application of
a single word sense disambiguation system), that is, when validation policies are of
no use.
Figure 1 illustrates the interface of the tool: In the top pane the sentence at hand
is shown, marked with colors as explained above. The main pane shows the semantic
interconnections between senses for which either there is a full agreement or the chosen
validation policy can be applied. When the user clicks on a word w, the left pane reports
the sense inventory for w, including information about the hypernym, definition, and
usage for each sense of w. The validator can then click on a sense and see how the
semantic graph shown in the main pane changes after the selection, possibly resulting
in a different number and strength of semantic interconnection patterns supporting that
sense choice.
In the following subsections, we describe the application of the Valido tool to the
validation of manual and automatic annotations, and we discuss cases of uncertain
applicability of the tool.
276
Navigli Consistent Validation of Manual and Automatic Sense Annotations
3.1 Validating Manual Annotations
In the following, we illustrate the tool by presenting two examples of validation of a
manual annotation (the validation policy ? was selected).
Figure 1 shows the senses chosen by the validators for the following sentence:
(a) We crossed the street near the intersection.
Sense #2 of intersection and sense #1 of cross are marked in green in the top pane,
meaning that the annotators fully agreed on those choices. On the other hand, sense
#2 of street is marked in orange, due to a disagreement between the annotators, one
preferring sense #1. Such an inconsistency is reported on the left pane, showing the
dictionary definitions of the two senses. The validator can then visualize in the same or
in a new window the semantic graphs concerning conflicting sense choices, comparing
the interconnection patterns available for sense #1 and #2 of street.
After evaluating the respective semantic interconnections, the validator can either
confirm the human annotator?s choice, accept the SSI interpretation, or assess the se-
mantic interconnection patterns resulting from different sense choices (reported in the
left pane of Figure 1).
It is worth mentioning that all the occurrences of the phrase cross the street in the
SemCor corpus are tagged with the first sense of street [defined as a thoroughfare (usually
including sidewalks) that is lined with buildings], but it is clear, from the definition of the
second sense (the part of a thoroughfare between the sidewalks; the part of the thoroughfare on
which vehicles travel; ?be careful crossing the street?), that a pedestrian crosses that part of
the thoroughfare between the sidewalks. Though questionable, this is a subtlety made
explicit in the dictionary and reinforced by the usage example of sense #2 above. The
tool reflects this fact, showing that both senses are connected with other word senses in
context, the first sense having a smaller degree of overall connectivity.2
As a second example, consider the WordNet definition of motorcycle:
(b) Motorcycle: a motor vehicle with two wheels and a strong frame
In the Gloss Word Sense Disambiguation task at Senseval-3 (Litkowski 2004), the
human annotators assigned the first sense to the word frame (a structure supporting or
containing something), unintentionally neglecting that the dictionary encodes a specific
sense of frame concerning the structure of objects (e.g., vehicles, buildings, etc.). In fact,
a chassis#3 is a kind of frame#6 (the internal supporting structure that gives an artifact its
shape), and is also part of a motor vehicle#1. While regular polysemy holds between sense
#1 and #6, there is no justification for the former choice, as it does not refer to vehicles
at all (as reflected by the lack of semantic interconnection patterns concerning frame#1).
The tool applies the validation policy and suggests sense #6 to the validator.
From these two real-world cases, it is evident that Valido can point at inconsistent,
although acceptable, choices made by human annotators due, among others, to the fine
granularity of the sense inventory and to regular polysemy. In Section 4 we present an
experiment showing that this claim still holds on a larger scale.
2 In the case of a large, connected graph, a pruned version is shown, and a link is available for viewing a
more complete, extended version of the graph.
277
Computational Linguistics Volume 32, Number 2
Apart from tagging mistakes, most of the cases of disagreement between manual
annotators is due to the fine granularity of the lexicon inventory. We recognize that
subtle distinctions, like those encoded in WordNet, are rarely useful in any NLP appli-
cation, but, as a matter of fact, WordNet is at the moment the de facto standard within
the research community, as no other computational lexicon of that size and complexity
is freely available.
3.2 Validating Automatic Annotations
While the task of manual annotation is mostly restricted to lexicographers, automatic
annotations of texts (especially Web pages) are gaining a huge popularity in the Seman-
tic Web vision (Berners-Lee 1999). In order to perform automatic tagging, one or more
word sense disambiguation systems are applied, resulting in a semantically enhanced
resource. Unfortunately, even when dealing with restricted sense inventories or selected
domains, automated systems can make mistakes in the sense assignment, also due to
the difficulty in training a supervised program with a sufficient number of annotated
instances and again the fine granularity of the dictionary inventory.
The recognition of intuitive and convincing interconnection patterns reinforces a
consistent choice of senses throughout the discourse, a desirable condition for guaran-
teeing semantic coherence. For example, semantic interconnections can help deal with
partially justifiable, but incorrect, interpretations for words in context. Consider for
instance the sentence from the Senseval-3 English all-words competition:
(c) The driver stopped swearing at them, turned on his heel and went back to
his truck.
A partial interpretation of driver and heel can be provided in the golf domain (a
heel#6 is part of a driver#5). This can be a reasonable choice for a word sense disam-
biguator, but the overall semantic graph exposes a poor structural quality. A different
choice of senses pointed out by Valido (driver as an operator of a vehicle and heel as
the back part of the foot) provides a more interconnected structure (among others,
driver#1 ??related?to motor vehicle#1 ??kind?of truck#1, turn ? v#1 ??related?to heel#2, etc.).
3.3 Weaknesses of the Approach
It can happen that semantic interconnection patterns proposed by the validation tool
convey weak suggestions due to the lack of structure in the lexical knowledge base
used to extract patterns like those in Table 1. In that case, the validator is expected to
reject the possible suggestion and make a more reasonable choice. As a result, if no
interesting proposal is provided to the validator, it is less likely that the final choice will
be inconsistent with the lexicon structure. Typical examples are:
(d) A payment was made last week.
(e) I spent three days in that hospital.
WordNet encodes two senses of payment: the sum of money paid (sense #1) and the
act of paying money (sense #2). Such regular polysemy makes it hard to converge on a
sense choice for payment in sentence (d). This difficulty is also manifested in the anno-
tations of similar expressions involving make and payment within SemCor. Furthermore,
278
Navigli Consistent Validation of Manual and Automatic Sense Annotations
Table 2
Precision and recall of the Valido tool in the appropriateness of its suggestions for 360 words.
Part of speech Precision Recall F1 measure
Nouns 73.83% (79/107) 65.83% (79/120) 69.60%
Adjectives 89.29% (25/28) 20.83% (25/120) 33.78%
Verbs 82.14% (69/84) 57.50% (69/120) 67.65%
Total 79.00% (173/219) 48.05% (173/360) 59.76%
apart from the distinction between the act of doing the action and the amount of money
paid, there are not many structural suggestions that allow us to distinguish between
the two senses. Semantic interconnection patterns cannot help the validator here, but
any choice will not violate the structural consistency of the lexicon. As for sentence (e),
WordNet encodes two senses for hospital: the building where patients receive treatment
(sense #1) and the medical institution (sense #2). This case is diametrically opposite
in that here WordNet encodes much information about both senses, but such ?noisy?
knowledge does not help discriminate. As a result, a number of semantic interconnec-
tion patterns are presented to the validator, indicating the relevance of both senses for
tagging, but no evidence in favor of the choice of sense #1 (which is most appropriate
in the sentence).
4. Evaluation
We performed an evaluation of the tool on SemCor (Miller et al 1993), a selection of
documents from the Brown Corpus where each content word is annotated with concepts
(specifically, synsets) from the WordNet inventory.
The objective of our evaluation is to show that Valido constitutes good support
for a validator in detecting bad or inconsistent annotations. A total of 360 sentences
of average length (9 or 10 content words) were uniformly selected from the set of
documents in the SemCor corpus. The average ambiguity of an arbitrary word in the
data set was 5.77, while the average ambiguity of the most ambiguous word in a
sentence was 8.70.
For each sentence ? = w1w2 . . .wn annotated in SemCor with the senses
sw1 sw2 . . . swn (swi ? Senses(wi), i ? {1, 2, . . . , n}), we identified the most ambiguous word
wi ? ?, and randomly chose a different sense swi for that word, that is, swi ? Senses(wi) \
{swi}. The experiment simulates in vitro a situation in which, for each sentence, the
annotators agree on which sense to assign to all the words but one, where one annotator
provides an appropriate sense and the other selects a different sense. The random factor
guarantees an approximation to the uniform distribution in the test set of all the possible
degrees of disagreement between sense annotators (ranging from regular polysemy to
homonymy).
We applied Valido with validation policy (?) to the annotated sentences and evalu-
ated the performance of the tool in suggesting the appropriate choice for the words with
disagreement. We assessed precision (the number of correct suggestions over the overall
number of suggestions from the Valido tool), recall (the number of correct suggestions
over the total number of words to be validated), and the F1 measure
( 2pr
p+r
)
.
The results are reported in Table 2 for nouns, adjectives, and verbs (we neglected
adverbs, as very few interconnections can be found for them). The experiment shows
279
Computational Linguistics Volume 32, Number 2
that evidences of inconsistency are provided by the tool with good precision (and a
good F1 measure, especially for nouns and verbs, beating the random baseline of 50%).
Notice that this test differs from the typical evaluation of word sense disambiguation
tasks, like the Senseval exercises (http://www.senseval.org), in that we are assessing
highly polysemous (possibly, very fine grained) words. Comparing the results with a
smart baseline, like the most frequent sense heuristic, is not feasible in this experiment,
as the frequency of WordNet senses was calculated on the same data set (i.e., SemCor).
Notice anyway that beating a baseline is not necessarily our objective if we are not able
to provide justifications (like semantic graphs) of which the human validator can take
advantage in order to take the final decision.
The low recall resulting for parts of speech other than nouns (mainly, adjectives)
is due to a lack of connectivity in the lexical knowledge base, especially when dealing
with connections across different parts of speech. This is a problem already discussed in
Navigli and Velardi (2005) and partially taken into account in Navigli (2005). Valido can
indeed be used as a tool to collect new, consistent collocations that could grow the LKB
from which the semantic interconnection patterns are extracted, possibly in an iterative
process. We plan to investigate this topic in the near future.
5. Conclusions
In this article we discussed a tool, Valido, for supporting validators in the difficult task
of assessing the quality of both manual and automatic sense assignments. The validator
can analyze the correctness of a sense choice in terms of its structural semantic inter-
connections (SSI) with respect to the other word senses chosen in context. The use of
semantic interconnection patterns to support validation allows one to smooth possible
divergences between the annotators and to corroborate choices consistent with the LKB.
Furthermore, the method is independent of the adopted lexicon (i.e., WordNet), in that
patterns can be derived from any sufficiently rich ontological resource. Moreover, the
approach allows the validator to discover mistakes in the lexicon: For instance, the
semantic graphs analyzed in a number of experiments helped us find out that a Swiss
canton#1 is not a Chinese city (canton#1) but a division of a country (canton#2), that a male
horse should be a kind of horse, that carelessness is not a kind of attentiveness, but rather
the contrary, and so on. These inconsistencies of WordNet 2.0 were promptly reported
to the resource maintainers, and most of them have been corrected in the latest version
of the lexicon.
Finally, we would like to point out that, in the future, the tool could also be used
during the annotation phase by taggers looking for suggestions based on the structure
of the LKB, with the result of improving the coherence and awareness of the decisions
to be taken.
References
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings
of COLING 1996, pages 16?22,
Copenhagen, Denmark.
Berners-Lee, Tim. 1999. Weaving the Web.
Harper, San Francisco, CA.
Chklovski, Tim and Rada Mihalcea. 2002.
Building a sense tagged corpus with
Open Mind Word Expert. In Proceedings
of ACL 2002 Workshop on Word Sense
Disambiguation: Recent Successes and Future
Directions, pages 116?122, Philadelphia, PA.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Hirst, Graeme and David St-Onge. 1998.
Lexical chains as representations of
context for the detection and correction
280
Navigli Consistent Validation of Manual and Automatic Sense Annotations
of malapropisms. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database.
The MIT Press, Cambridge, MA,
pages 305?332.
Litkowski, Kenneth C. 2004. SENSEVAL-3
task: Word-sense disambiguation of
WordNet glosses. In Proceedings of ACL
2004 SENSEVAL-3 Workshop, pages 13?16,
Barcelona, Spain.
Mihalcea, Rada and Dan Moldovan. 2001.
eXtended WordNet: Progress report. In
Proceedings of NAACL Workshop on WordNet
and Other Lexical Resources, pages 95?100,
Pittsburgh, PA.
Miller, George, Claudia Leacock, Tengi
Randee, and Ross Bunker. 1993. A
semantic concordance. In Proceedings 3rd
DARPA Workshop on Human Language
Technology, Plainsboro, NJ.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 17(1):21?48.
Navigli, Roberto. 2005. Semi-automatic
extension of large-scale linguistic
knowledge bases. In Proceedings of
18th FLAIRS International Conference,
pages 548?553, Clearwater Beach, FL,
May 16?18, 2005.
Navigli, Roberto and Paola Velardi. 2004.
Learning domain ontologies from
document warehouses and dedicated
websites, Computational Linguistics,
30(2):151?179.
Navigli, Roberto and Paola Velardi.
2005. Structural semantic interconnections:
a knowledge-based approach to
word sense disambiguation. IEEE
Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 27(7):
1075?1086.
Rada, Roy, Hafedh Mili, Ellen Bickell, and
Maria Blettner. 1989. Development and
application of a metric on semantic nets.
IEEE Transactions on Systems, Man and
Cybernetics, 19(1):17?30.
281

Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 97?104,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ensemble Methods for Unsupervised WSD
Samuel Brody
School of Informatics
University of Edinburgh
s.brody@sms.ed.ac.uk
Roberto Navigli
Dipartimento di Informatica
Universita di Roma ?La Sapienza?
navigli@di.uniroma1.it
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
Combination methods are an effective way
of improving system performance. This
paper examines the benefits of system
combination for unsupervised WSD. We
investigate several voting- and arbiter-
based combination strategies over a di-
verse pool of unsupervised WSD systems.
Our combination methods rely on predom-
inant senses which are derived automati-
cally from raw text. Experiments using the
SemCor and Senseval-3 data sets demon-
strate that our ensembles yield signifi-
cantly better results when compared with
state-of-the-art.
1 Introduction
Word sense disambiguation (WSD), the task of
identifying the intended meanings (senses) of
words in context, holds promise for many NLP
applications requiring broad-coverage language
understanding. Examples include summarization,
question answering, and text simplification. Re-
cent studies have also shown that WSD can ben-
efit machine translation (Vickrey et al, 2005) and
information retrieval (Stokoe, 2005).
Given the potential of WSD for many NLP
tasks, much work has focused on the computa-
tional treatment of sense ambiguity, primarily us-
ing data-driven methods. Most accurate WSD sys-
tems to date are supervised and rely on the avail-
ability of training data, i.e., corpus occurrences of
ambiguous words marked up with labels indicat-
ing the appropriate sense given the context (see
Mihalcea and Edmonds 2004 and the references
therein). A classifier automatically learns disam-
biguation cues from these hand-labeled examples.
Although supervised methods typically achieve
better performance than unsupervised alternatives,
their applicability is limited to those words for
which sense labeled data exists, and their accu-
racy is strongly correlated with the amount of la-
beled data available (Yarowsky and Florian, 2002).
Furthermore, obtaining manually labeled corpora
with word senses is costly and the task must be
repeated for new domains, languages, or sense in-
ventories. Ng (1997) estimates that a high accu-
racy domain independent system for WSD would
probably need a corpus of about 3.2 million sense
tagged words. At a throughput of one word per
minute (Edmonds, 2000), this would require about
27 person-years of human annotation effort.
This paper focuses on unsupervised methods
which we argue are useful for broad coverage
sense disambiguation. Unsupervised WSD algo-
rithms fall into two general classes: those that per-
form token-based WSD by exploiting the simi-
larity or relatedness between an ambiguous word
and its context (e.g., Lesk 1986); and those that
perform type-based WSD, simply by assigning
all instances of an ambiguous word its most fre-
quent (i.e., predominant) sense (e.g., McCarthy
et al 2004; Galley and McKeown 2003). The pre-
dominant senses are automatically acquired from
raw text without recourse to manually annotated
data. The motivation for assigning all instances
of a word to its most prevalent sense stems from
the observation that current supervised approaches
rarely outperform the simple heuristic of choos-
ing the most common sense in the training data,
despite taking local context into account (Hoste
et al, 2002). Furthermore, the approach allows
sense inventories to be tailored to specific do-
mains.
The work presented here evaluates and com-
pares the performance of well-established unsu-
pervised WSD algorithms. We show that these
algorithms yield sufficiently diverse outputs, thus
motivating the use of combination methods for im-
proving WSD performance. While combination
approaches have been studied previously for su-
pervised WSD (Florian et al, 2002), their use
in an unsupervised setting is, to our knowledge,
novel. We examine several existing and novel
combination methods and demonstrate that our
combined systems consistently outperform the
97
state-of-the-art (e.g., McCarthy et al 2004). Im-
portantly, our WSD algorithms and combination
methods do not make use of training material in
any way, nor do they use the first sense informa-
tion available in WordNet.
In the following section, we briefly describe the
unsupervised WSD algorithms considered in this
paper. Then, we present a detailed comparison of
their performance on SemCor (Miller et al, 1993).
Next, we introduce our system combination meth-
ods and report on our evaluation experiments. We
conclude the paper by discussing our results.
2 The Disambiguation Algorithms
In this section we briefly describe the unsuper-
vised WSD algorithms used in our experiments.
We selected methods that vary along the follow-
ing dimensions: (a) the type of WSD performed
(i.e., token-based vs. type-based), (b) the represen-
tation and size of the context surrounding an am-
biguous word (i.e., graph-based vs. word-based,
document vs. sentence), and (c) the number and
type of semantic relations considered for disam-
biguation. We base most of our discussion below
on the WordNet sense inventory; however, the ap-
proaches are not limited to this particular lexicon
but could be adapted for other resources with tra-
ditional dictionary-like sense definitions and alter-
native structure.
Extended Gloss Overlap Gloss Overlap was
originally introduced by Lesk (1986) for perform-
ing token-based WSD. The method assigns a sense
to a target word by comparing the dictionary defi-
nitions of each of its senses with those of the words
in the surrounding context. The sense whose defi-
nition has the highest overlap (i.e., words in com-
mon) with the context words is assumed to be the
correct one. Banerjee and Pedersen (2003) aug-
ment the dictionary definition (gloss) of each sense
with the glosses of related words and senses. The
extended glosses increase the information avail-
able in estimating the overlap between ambiguous
words and their surrounding context.
The range of relationships used to extend the
glosses is a parameter, and can be chosen from
any combination of WordNet relations. For every
sense sk of the target word we estimate:
SenseScore(sk) = ?
Rel?Relations
Overlap(context,Rel(sk))
where context is a simple (space separated) con-
catenation of all words wi for ?n ? i ? n, i 6= 0 ina context window of length ?n around the target
word w0. The overlap scoring mechanism is also
parametrized and can be adjusted to take the into
account gloss length or to ignore function words.
Distributional and WordNet Similarity
McCarthy et al (2004) propose a method for
automatically ranking the senses of ambiguous
words from raw text. Key in their approach is the
observation that distributionally similar neighbors
often provide cues about a word?s senses. As-
suming that a set of neighbors is available, sense
ranking is equivalent to quantifying the degree
of similarity among the neighbors and the sense
descriptions of the polysemous word.
Let N(w) = {n1,n2, . . . ,nk} be the k most (dis-tributionally) similar words to an ambiguous tar-
get word w and senses(w) = {s1,s2, . . .sn} the setof senses for w. For each sense si and for eachneighbor n j, the algorithm selects the neighbor?ssense which has the highest WordNet similarity
score (wnss) with regard to si. The ranking scoreof sense si is then increased as a function of theWordNet similarity score and the distributional
similarity score (dss) between the target word and
the neighbor:
RankScore(si) = ?
n j?Nw
dss(w,n j)
wnss(si,n j)
?
s?i?senses(w)
wnss(s?i,n j)
where wnss(si,n j) = max
nsx?senses(n j)
wnss(si,nsx).
The predominant sense is simply the sense with
the highest ranking score (RankScore) and can be
consequently used to perform type-based disam-
biguation. The method presented above has four
parameters: (a) the semantic space model repre-
senting the distributional properties of the target
words (it is acquired from a large corpus repre-
sentative of the domain at hand and can be aug-
mented with syntactic relations such as subject or
object), (b) the measure of distributional similarity
for discovering neighbors (c) the number of neigh-
bors that the ranking score takes into account, and
(d) the measure of sense similarity.
Lexical Chains Lexical cohesion is often rep-
resented via lexical chains, i.e., sequences of re-
lated words spanning a topical text unit (Mor-
ris and Hirst, 1991). Algorithms for computing
lexical chains often perform WSD before infer-
ring which words are semantically related. Here
we describe one such disambiguation algorithm,
proposed by Galley and McKeown (2003), while
omitting the details of creating the lexical chains
themselves.
Galley and McKeown?s (2003) method consists
of two stages. First, a graph is built represent-
ing all possible interpretations of the target words
98
in question. The text is processed sequentially,
comparing each word against all words previously
read. If a relation exists between the senses of the
current word and any possible sense of a previous
word, a connection is formed between the appro-
priate words and senses. The strength of the con-
nection is a function of the type of relationship and
of the distance between the words in the text (in
terms of words, sentences and paragraphs). Words
are represented as nodes in the graph and seman-
tic relations as weighted edges. Again, the set of
relations being considered is a parameter that can
be tuned experimentally.
In the disambiguation stage, all occurrences of a
given word are collected together. For each sense
of a target word, the strength of all connections
involving that sense are summed, giving that sense
a unified score. The sense with the highest unified
score is chosen as the correct sense for the target
word. In subsequent stages the actual connections
comprising the winning unified score are used as a
basis for computing the lexical chains.
The algorithm is based on the ?one sense per
discourse? hypothesis and uses information from
every occurrence of the ambiguous target word in
order to decide its appropriate sense. It is there-
fore a type-based algorithm, since it tries to de-
termine the sense of the word in the entire doc-
ument/discourse at once, and not separately for
each instance.
Structural Semantic Interconnections In-
spired by lexical chains, Navigli and Velardi
(2005) developed Structural Semantic Intercon-
nections (SSI), a WSD algorithm which makes use
of an extensive lexical knowledge base. The latter
is primarily based on WordNet and its standard re-
lation set (i.e., hypernymy, meronymy, antonymy,
similarity, nominalization, pertainymy) but is also
enriched with collocation information represent-
ing semantic relatedness between sense pairs. Col-
locations are gathered from existing resources
(such as the Oxford Collocations, the Longman
Language Activator, and collocation web sites).
Each collocation is mapped to the WordNet sense
inventory in a semi-automatic manner (Navigli,
2005) and transformed into a relatedness edge.
Given a local word context C = {w1, ...,wn},SSI builds a graph G = (V,E) such that V =
n
S
i=1
senses(wi) and (s,s?) ? E if there is at least
one interconnection j between s (a sense of the
word) and s? (a sense of its context) in the lexical
knowledge base. The set of valid interconnections
is determined by a manually-created context-free
Method WSD Context Relations
LexChains types document first-order
Overlap tokens sentence first-order
Similarity types corpus higher-order
SSI tokens sentence higher-order
Table 1: Properties of the WSD algorithms
grammar consisting of a small number of rules.
Valid interconnections are computed in advance
on the lexical database, not at runtime.
Disambiguation is performed in an iterative
fashion. At each step, for each sense s of a word
in C (the set of senses of words yet to be disam-
biguated), SSI determines the degree of connectiv-
ity between s and the other senses in C :
SSIScore(s) =
?
s??C\{s} ?j?Interconn(s,s?)
1
length( j)
?
s??C\{s}
|Interconn(s,s?)|
where Interconn(s,s?) is the set of interconnec-
tions between senses s and s?. The contribution of a
single interconnection is given by the reciprocal of
its length, calculated as the number of edges con-
necting its ends. The overall degree of connectiv-
ity is then normalized by the number of contribut-
ing interconnections. The highest ranking sense s
of word wi is chosen and the senses of wi are re-moved from the context C . The procedure termi-
nates when either C is the empty set or there is no
sense such that its SSIScore exceeds a fixed thresh-
old.
Summary The properties of the different
WSD algorithms just described are summarized
in Table 1. The methods vary in the amount of
data they employ for disambiguation. SSI and Ex-
tended Gloss Overlap (Overlap) rely on sentence-
level information for disambiguation whereas Mc-
Carthy et al (2004) (Similarity) and Galley and
McKeown (2003) (LexChains) utilize the entire
document or corpus. This enables the accumula-
tion of large amounts of data regarding the am-
biguous word, but does not allow separate consid-
eration of each individual occurrence of that word.
LexChains and Overlap take into account a re-
stricted set of semantic relations (paths of length
one) between any two words in the whole docu-
ment, whereas SSI and Similarity use a wider set
of relations.
99
3 Experiment 1: Comparison of
Unsupervised Algorithms for WSD
3.1 Method
We evaluated the disambiguation algorithms out-
lined above on two tasks: predominant sense ac-
quisition and token-based WSD. As previously
explained, Overlap and SSI were not designed for
acquiring predominant senses (see Table 1), but
a token-based WSD algorithm can be trivially
modified to acquire predominant senses by dis-
ambiguating every occurrence of the target word
in context and selecting the sense which was cho-
sen most frequently. Type-based WSD algorithms
simply tag all occurrences of a target word with its
predominant sense, disregarding the surrounding
context.
Our first set of experiments was conducted on
the SemCor corpus, on the same 2,595 polyse-
mous nouns (53,674 tokens) used as a test set by
McCarthy et al (2004). These nouns were attested
in SemCor with a frequency > 2 and occurred in
the British National Corpus (BNC) more than 10
times. We used the WordNet 1.7.1 sense inventory.
The following notation describes our evaluation
measures: W is the set of all noun types in the
SemCor corpus (|W | = 2,595), and W f is the setof noun types with a dominant sense. senses(w)
is the set of senses for noun type w, while fs(w)and fm(w) refer to w?s first sense according to theSemCor gold standard and our algorithms, respec-
tively. Finally, T (w) is the set of tokens of w and
senses(t) denotes the sense assigned to token t ac-cording to SemCor.
We first measure how well our algorithms can
identify the predominant sense, if one exists:
Accps =
|{w ?W f | fs(w) = fm(w)}|
|Wf |
A baseline for this task can be easily defined for
each word type by selecting a sense at random
from its sense inventory and assuming that this is
the predominant sense:
Baselinesr =
1
|Wf | ?w ?W f
1
|senses(w)|
We evaluate the algorithms? disambiguation per-
formance by measuring the ratio of tokens for
which our models choose the right sense:
Accwsd =
?
w?W
|{t ? T (w)| fm(w) = senses(t)}|
?
w?W
|T (w)|
In the predominant sense detection task, in case of
ties in SemCor, any one of the predominant senses
was considered correct. Also, all algorithms were
designed to randomly choose from among the top
scoring options in case of a tie in the calculated
scores. This introduces a small amount of ran-
domness (less than 0.5%) in the accuracy calcu-
lation, and was done to avoid the pitfall of default-
ing to the first sense listed in WordNet, which is
usually the actual predominant sense (the order of
senses in WordNet is based primarily on the Sem-
Cor sense distribution).
3.2 Parameter Settings
We did not specifically tune the parameters of our
WSD algorithms on the SemCor corpus, as our
goal was to use hand labeled data solely for testing
purposes. We selected parameters that have been
considered ?optimal? in the literature, although
admittedly some performance gains could be ex-
pected had parameter optimization taken place.
For Overlap, we used the semantic relations
proposed by Banerjee and Pedersen (2003),
namely hypernyms, hyponyms, meronyms,
holonyms, and troponym synsets. We also
adopted their overlap scoring mechanism which
treats each gloss as a bag of words and assigns an
n word overlap the score of n2. Function words
were not considered in the overlap computation.
For LexChains, we used the relations reported
in Galley and McKeown (2003). These are all
first-order WordNet relations, with the addition of
the siblings ? two words are considered siblings
if they are both hyponyms of the same hypernym.
The relations have different weights, depending
on their type and the distance between the words
in the text. These weights were imported from
Galley and McKeown into our implementation
without modification.
Because the SemCor corpus is relatively small
(less than 700,00 words), it is not ideal for con-
structing a neighbor thesaurus appropriate for Mc-
Carthy et al?s (2004) method. The latter requires
each word to participate in a large number of co-
occurring contexts in order to obtain reliable dis-
tributional information. To overcome this prob-
lem, we followed McCarthy et al and extracted
the neighbor thesaurus from the entire BNC. We
also recreated their semantic space, using a RASP-
parsed (Briscoe and Carroll, 2002) version of the
BNC and their set of dependencies (i.e., Verb-
Object, Verb-Subject, Noun-Noun and Adjective-
Noun relations). Similarly to McCarthy et al, we
used Lin?s (1998) measure of distributional simi-
larity, and considered only the 50 highest ranked
100
Method Accps Accwsd/dir Accwsd/ps
Baseline 34.5 ? 23.0
LexChains 48.3??$ ? 40.7?#?$
Overlap 49.4??$ 36.5$ 42.5??$
Similarity 54.9? ? 46.5?$
SSI 53.7? 42.7 47.9?
UpperBnd 100 ? 68.4
Table 2: Results of individual disambiguation al-
gorithms on SemCor nouns2 (?: sig. diff. from
Baseline, ?: sig. diff. from Similarity, $: sig diff.
from SSI, #: sig. diff. from Overlap, p < 0.01)
neighbors for a given target word. Sense similar-
ity was computed using the Lesk?s (Banerjee and
Pedersen, 2003) similarity measure1.
3.3 Results
The performance of the individual algorithms is
shown in Table 2. We also include the baseline
discussed in Section 3 and the upper bound of
defaulting to the first (i.e., most frequent) sense
provided by the manually annotated SemCor. We
report predominant sense accuracy (Accps), andWSD accuracy when using the automatically ac-
quired predominant sense (Accwsd/ps). For token-based algorithms, we also report their WSD per-
formance in context, i.e., without use of the pre-
dominant sense (Accwsd/dir).As expected, the accuracy scores in the WSD
task are lower than the respective scores in the
predominant sense task, since detecting the pre-
dominant sense correctly only insures the correct
tagging of the instances of the word with that
first sense. All methods perform significantly bet-
ter than the baseline in the predominant sense de-
tection task (using a ?2-test, as indicated in Ta-
ble 2). LexChains and Overlap perform signif-
icantly worse than Similarity and SSI, whereas
LexChains is not significantly different from Over-
lap. Likewise, the difference in performance be-
tween SSI and Similarity is not significant. With
respect to WSD, all the differences in performance
are statistically significant.
1This measure is identical to the Extended gloss Overlapfrom Section 2, but instead of searching for overlap betweenan extended gloss and a word?s context, the comparison isdone between two extended glosses of two synsets.2The LexChains results presented here are not directlycomparable to those reported by Galley and McKeown(2003), since they tested on a subset of SemCor, and includedmonosemous nouns. They also used the first sense in Sem-Cor in case of ties. The results for the Similarity method areslightly better than those reported by McCarthy et al (2004)due to minor improvements in implementation.
Overlap LexChains Similarity
LexChains 28.05
Similarity 35.87 33.10
SSI 30.48 31.67 37.14
Table 3: Algorithms? pairwise agreement in de-
tecting the predominant sense (as % of all words)
Interestingly, using the predominant sense de-
tected by the Gloss Overlap and the SSI algo-
rithm to tag all instances is preferable to tagging
each instance individually (compare Accwsd/dirand Accwsd/ps for Overlap and SSI in Table 2).This means that a large part of the instances which
were not tagged individually with the predominant
sense were actually that sense.
A close examination of the performance of the
individual methods in the predominant-sense de-
tection task shows that while the accuracy of all
the methods is within a range of 7%, the actual
words for which each algorithm gives the cor-
rect predominant sense are very different. Table 3
shows the degree of overlap in assigning the ap-
propriate predominant sense among the four meth-
ods. As can be seen, the largest amount of over-
lap is between Similarity and SSI, and this cor-
responds approximately to 23 of the words theycorrectly label. This means that each of these two
methods gets more than 350 words right which the
other labels incorrectly.
If we had an ?oracle? which would tell us
which method to choose for each word, we would
achieve approximately 82.4% in the predominant
sense task, giving us 58% in the WSD task. We
see that there is a large amount of complementa-
tion between the algorithms, where the successes
of one make up for the failures of the others. This
suggests that the errors of the individual methods
are sufficiently uncorrelated, and that some advan-
tage can be gained by combining their predictions.
4 Combination Methods
An important finding in machine learning is that
a set of classifiers whose individual decisions are
combined in some way (an ensemble) can be more
accurate than any of its component classifiers, pro-
vided that the individual components are relatively
accurate and diverse (Dietterich, 1997). This sim-
ple idea has been applied to a variety of classi-
fication problems ranging from optical character
recognition to medical diagnosis, part-of-speech
tagging (see Dietterich 1997 and van Halteren
et al 2001 for overviews), and notably supervised
101
WSD (Florian et al, 2002).
Since our effort is focused exclusively on un-
supervised methods, we cannot use most ma-
chine learning approaches for creating an en-
semble (e.g., stacking, confidence-based combina-
tion), as they require a labeled training set. We
therefore examined several basic ensemble com-
bination approaches that do not require parameter
estimation from training data.
We define Score(Mi,s j) as the (normalized)score which a method Mi gives to word sense s j.The predominant sense calculated by method Mifor word w is then determined by:
PS(Mi,w) = argmax
s j?senses(w)
Score(Mi,s j)
All ensemble methods receive a set {Mi}ki=1 of in-dividual methods to combine, so we denote each
ensemble method by MethodName({Mi}ki=1).
Direct Voting Each ensemble component has
one vote for the predominant sense, and the sense
with the most votes is chosen. The scoring func-
tion for the voting ensemble is defined as:
Score(Voting({Mi}ki=1),s)) =
k
?
i=1
eq[s,PS(Mi,w)]
where eq[s,PS(Mi,w)] =
{ 1 if s = PS(Mi,w)0 otherwise
Probability Mixture Each method provides
a probability distribution over the senses. These
probabilities (normalized scores) are summed, and
the sense with the highest score is chosen:
Score(ProbMix({Mi}ki=1),s)) =
k
?
i=1
Score(Mi,s)
Rank-Based Combination Each method
provides a ranking of the senses for a given target
word. For each sense, its placements according to
each of the methods are summed and the sense
with the lowest total placement (closest to first
place) wins.
Score(Ranking({Mi}ki=1),s)) =
k
?
i=1
(?1)?Placei(s)
where Placei(s) is the number of distinct scoresthat are larger or equal to Score(Mi,s).
Arbiter-based Combination One WSD
method can act as an arbiter for adjudicating dis-
agreements among component systems. It makes
sense for the adjudicator to have reasonable
performance on its own. We therefore selected
Method Accps Accwsd/ps
Similarity 54.9 46.5
SSI 53.5 47.9
Voting 57.3?$ 49.8?$
PrMixture 57.2?$ 50.4?$?
Rank-based 58.1?$ 50.3?$?
Arbiter-based 56.3?$ 48.7?$?
UpperBnd 100 68.4
Table 4: Ensemble Combination Results (?: sig.
diff. from Similarity, $: sig. diff. from SSI, ?: sig.
diff. from Voting, p < 0.01)
SSI as the arbiter since it had the best accuracy on
the WSD task (see Table 2). For each disagreed
word w, and for each sense s of w assigned by
any of the systems in the ensemble {Mi}ki=1, wecalculate the following score:
Score(Arbiter({Mi}ki=1),s) = SSIScore?(s)
where SSIScore?(s) is a modified version of the
score introduced in Section 2 which exploits as a
context for s the set of agreed senses and the re-
maining words of each sentence. We exclude from
the context used by SSI the senses of w which were
not chosen by any of the systems in the ensem-
ble . This effectively reduces the number of senses
considered by the arbiter and can positively influ-
ence the algorithm?s performance, since it elimi-
nates noise coming from senses which are likely
to be wrong.
5 Experiment 2: Ensembles for
Unsupervised WSD
5.1 Method and Parameter Settings
We assess the performance of the different en-
semble systems on the same set of SemCor nouns
on which the individual methods were tested. For
the best ensemble, we also report results on dis-
ambiguating all nouns in the Senseval-3 data set.
We focus exclusively on nouns to allow com-
parisons with the results obtained from SemCor.
We used the same parameters as in Experiment 1
for constructing the ensembles. As discussed ear-
lier, token-based methods can disambiguate target
words either in context or using the predominant
sense. SSI was employed in the predominant sense
setting in our arbiter experiment.
5.2 Results
Our results are summarized in Table 4. As can be
seen, all ensemble methods perform significantly
102
Ensemble Accps Accwsd/ps
Rank-based 58.1 50.3
Overlap 57.6 (?0.5) 49.7 (?0.6)
LexChains 57.2 (?0.7) 50.2 (?0.1)
Similarity 56.3 (?1.8) 49.4 (?0.9)
SSI 56.3 (?1.8) 48.2 (?2.1)
Table 5: Decrease in accuracy as a result of re-
moval of each method from the rank-based ensem-
ble.
better than the best individual methods, i.e., Simi-
larity and SSI. On the WSD task, the voting, prob-
ability mixture, and rank-based ensembles signif-
icantly outperform the arbiter-based one. The per-
formances of the probability mixture, and rank-
based combinations do not differ significantly but
both ensembles are significantly better than vot-
ing. One of the factors contributing to the arbiter?s
worse performance (compared to the other ensem-
bles) is the fact that in many cases (almost 30%),
none of the senses suggested by the disagreeing
methods is correct. In these cases, there is no way
for the arbiter to select the correct sense. We also
examined the relative contribution of each compo-
nent to overall performance. Table 5 displays the
drop in performance by eliminating any particular
component from the rank-based ensemble (indi-
cated by ?). The system that contributes the most
to the ensemble is SSI. Interestingly, Overlap and
Similarity yield similar improvements in WSD ac-
curacy (0.6 and 0.9, respectively) when added to
the ensemble.
Figure 1 shows the WSD accuracy of the best
single methods and the ensembles as a function of
the noun frequency in SemCor. We can see that
there is at least one ensemble outperforming any
single method in every frequency band and that
the rank-based ensemble consistently outperforms
Similarity and SSI in all bands. Although Similar-
ity has an advantage over SSI for low and medium
frequency words, it delivers worse performance
for high frequency words. This is possibly due to
the quality of neighbors obtained for very frequent
words, which are not semantically distinct enough
to reliably discriminate between different senses.
Table 6 lists the performance of the rank-based
ensemble on the Senseval-3 (noun) corpus. We
also report results for the best individual method,
namely SSI, and compare our results with the best
unsupervised system that participated in Senseval-
3. The latter was developed by Strapparava et al
(2004) and performs domain driven disambigua-
tion (IRST-DDD). Specifically, the approach com-
1-4 5-9 10-19 20-99 100+Noun frequency bands
40
42
44
46
48
50
52
54
WS
D A
ccu
rac
y (%
)
SimilaritySSIArbiter
VotingProbMixRanking
Figure 1: WSD accuracy as a function of noun fre-
quency in SemCor
Method Precision Recall Fscore
Baseline 36.8 36.8 36.8
SSI 62.5 62.5 62.5
IRST-DDD 63.3 62.2 61.2
Rank-based 63.9 63.9 63.9
UpperBnd 68.7 68.7 68.7
Table 6: Results of individual disambiguation al-
gorithms and rank-based ensemble on Senseval-3
nouns
pares the domain of the context surrounding the
target word with the domains of its senses and uses
a version of WordNet augmented with domain la-
bels (e.g., economy, geography). Our baseline se-
lects the first sense randomly and uses it to disam-
biguate all instances of a target word. Our upper
bound defaults to the first sense from SemCor. We
report precision, recall and Fscore. In cases where
precision and recall figures coincide, the algorithm
has 100% coverage.
As can be seen the rank-based, ensemble out-
performs both SSI and the IRST-DDD system.
This is an encouraging result, suggesting that there
may be advantages in developing diverse classes
of unsupervised WSD algorithms for system com-
bination. The results in Table 6 are higher than
those reported for SemCor (see Table 4). This is
expected since the Senseval-3 data set contains
monosemous nouns as well. Taking solely polyse-
mous nouns into account, SSI?s Fscore is 53.39%
and the ranked-based ensemble?s 55.0%. We fur-
ther note that not all of the components in our en-
semble are optimal. Predominant senses for Lesk
and LexChains were estimated from the Senseval-
3 data, however a larger corpus would probably
yield more reliable estimates.
103
6 Conclusions and Discussion
In this paper we have presented an evaluation
study of four well-known approaches to unsuper-
vised WSD. Our comparison involved type- and
token-based disambiguation algorithms relying on
different kinds of WordNet relations and different
amounts of corpus data. Our experiments revealed
two important findings. First, type-based disam-
biguation yields results superior to a token-based
approach. Using predominant senses is preferable
to disambiguating instances individually, even for
token-based algorithms. Second, the outputs of
the different approaches examined here are suffi-
ciently diverse to motivate combination methods
for unsupervised WSD. We defined several ensem-
bles on the predominant sense outputs of individ-
ual methods and showed that combination systems
outperformed their best components both on the
SemCor and Senseval-3 data sets.
The work described here could be usefully em-
ployed in two tasks: (a) to create preliminary an-
notations, thus supporting the ?annotate automati-
cally, correct manually? methodology used to pro-
vide high volume annotation in the Penn Treebank
project; and (b) in combination with supervised
WSD methods that take context into account; for
instance, such methods could default to an unsu-
pervised system for unseen words or words with
uninformative contexts.
In the future we plan to integrate more com-
ponents into our ensembles. These include not
only domain driven disambiguation algorithms
(Strapparava et al, 2004) but also graph theoretic
ones (Mihalcea, 2005) as well as algorithms that
quantify the degree of association between senses
and their co-occurring contexts (Mohammad and
Hirst, 2006). Increasing the number of compo-
nents would allow us to employ more sophisti-
cated combination methods such as unsupervised
rank aggregation algorithms (Tan and Jin, 2004).
Acknowledgements
We are grateful to Diana McCarthy for her help with thiswork and to Michel Galley for making his code availableto us. Thanks to John Carroll and Rob Koeling for in-sightful comments and suggestions. The authors acknowl-edge the support of EPSRC (Brody and Lapata; grantEP/C538447/1) and the European Union (Navigli; InteropNoE (508011)).
References
Banerjee, Satanjeev and Ted Pedersen. 2003. Extended glossoverlaps as a measure of semantic relatedness. In Proceed-
ings of the 18th IJCAI. Acapulco, pages 805?810.
Briscoe, Ted and John Carroll. 2002. Robust accurate statis-tical annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Dietterich, T. G. 1997. Machine learning research: Four cur-rent directions. AI Magazine 18(4):97?136.
Edmonds, Philip. 2000. Designing a task for SENSEVAL-2.Technical note.
Florian, Radu, Silviu Cucerzan, Charles Schafer, and DavidYarowsky. 2002. Combining classifiers for word sense dis-ambiguation. Natural Language Engineering 1(1):1?14.
Galley, Michel and Kathleen McKeown. 2003. Improvingword sense disambiguation in lexical chaining. In Pro-
ceedings of the 18th IJCAI. Acapulco, pages 1486?1488.Hoste, Ve?ronique, Iris Hendrickx, Walter Daelemans, andAntal van den Bosch. 2002. Parameter optimization formachine-learning of word sense disambiguation. Lan-
guage Engineering 8(4):311?325.
Lesk, Michael. 1986. Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pine conefrom an ice cream cone. In Proceedings of the 5th SIG-
DOC. New York, NY, pages 24?26.
Lin, Dekang. 1998. An information-theoretic definition ofsimilarity. In Proceedings of the 15th ICML. Madison,WI, pages 296?304.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-roll. 2004. Finding predominant senses in untagged text.In Proceedings of the 42th ACL. Barcelona, Spain, pages280?287.
Mihalcea, Rada. 2005. Unsupervised large-vocabulary wordsense disambiguation with graph-based algorithms for se-quence data labeling. In Proceedings of the HLT/EMNLP.Vancouver, BC, pages 411?418.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of the SENSEVAL-3. Barcelona, Spain.
Miller, George A., Claudia Leacock, Randee Tengi, andRoss T. Bunker. 1993. A semantic concordance. In Pro-
ceedings of the ARPA HLT Workshop. Morgan Kaufman,pages 303?308.
Mohammad, Saif and Graeme Hirst. 2006. Determining wordsense dominance using a thesaurus. In Proceedings of the
EACL. Trento, Italy, pages 121?128.
Morris, Jane and Graeme Hirst. 1991. Lexical cohesion com-puted by thesaural relations as an indicator of the structureof text. Computational Linguistics 1(17):21?43.
Navigli, Roberto. 2005. Semi-automatic extension of large-scale linguistic knowledge bases. In Proceedings of the
18th FLAIRS. Florida.
Navigli, Roberto and Paola Velardi. 2005. Structural seman-tic interconnections: a knowledge-based approach to wordsense disambiguation. PAMI 27(7):1075?1088.
Ng, Tou Hwee. 1997. Getting serious about word sense dis-ambiguation. In Proceedings of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why, What,
and How?. Washington, DC, pages 1?7.
Stokoe, Christopher. 2005. Differentiating homonymy andpolysemy in information retrieval. In Proceedings of the
HLT/EMNLP. Vancouver, BC, pages 403?410.
Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano.2004. Word-sense disambiguation for machine transla-tion. In Proceedings of the SENSEVAL-3. Barcelona,Spain, pages 229?234.
Tan, Pang-Ning and Rong Jin. 2004. Ordering patterns bycombining opinions from multiple sources. In Proceed-
ings of the 10th KDD. Seattle, WA, pages 22?25.
van Halteren, Hans, Jakub Zavrel, and Walter Daelemans.2001. Improving accuracy in word class tagging throughcombination of machine learning systems. Computational
Linguistics 27(2):199?230.
Vickrey, David, Luke Biewald, Marc Teyssier, and DaphneKoller. 2005. Word-sense disambiguation for machinetranslation. In Proceedings of the HLT/EMNLP. Vancou-ver, BC, pages 771?778.
Yarowsky, David and Radu Florian. 2002. Evaluating sensedisambiguation across diverse parameter spaces. Natural
Language Engineering 9(4):293?310.
104
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 105?112,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Meaningful Clustering of Senses
Helps Boost Word Sense Disambiguation Performance
Roberto Navigli
Dipartimento di Informatica
Universita` di Roma ?La Sapienza?
Roma, Italy
navigli@di.uniroma1.it
Abstract
Fine-grained sense distinctions are one of
the major obstacles to successful Word
Sense Disambiguation. In this paper,
we present a method for reducing the
granularity of the WordNet sense inven-
tory based on the mapping to a manually
crafted dictionary encoding sense hierar-
chies, namely the Oxford Dictionary of
English. We assess the quality of the map-
ping and the induced clustering, and eval-
uate the performance of coarse WSD sys-
tems in the Senseval-3 English all-words
task.
1 Introduction
Word Sense Disambiguation (WSD) is undoubt-
edly one of the hardest tasks in the field of Nat-
ural Language Processing. Even though some re-
cent studies report benefits in the use of WSD in
specific applications (e.g. Vickrey et al (2005)
and Stokoe (2005)), the present performance of
the best ranking WSD systems does not provide a
sufficient degree of accuracy to enable real-world,
language-aware applications.
Most of the disambiguation approaches adopt
the WordNet dictionary (Fellbaum, 1998) as a
sense inventory, thanks to its free availability, wide
coverage, and existence of a number of standard
test sets based on it. Unfortunately, WordNet is a
fine-grained resource, encoding sense distinctions
that are often difficult to recognize even for human
annotators (Edmonds and Kilgariff, 1998).
Recent estimations of the inter-annotator agree-
ment when using the WordNet inventory report
figures of 72.5% agreement in the preparation of
the English all-words test set at Senseval-3 (Sny-
der and Palmer, 2004) and 67.3% on the Open
Mind Word Expert annotation exercise (Chklovski
and Mihalcea, 2002). These numbers lead us to
believe that a credible upper bound for unrestricted
fine-grained WSD is around 70%, a figure that
state-of-the-art automatic systems find it difficult
to outperform. Furthermore, even if a system were
able to exceed such an upper bound, it would be
unclear how to interpret such a result.
It seems therefore that the major obstacle to ef-
fective WSD is the fine granularity of the Word-
Net sense inventory, rather than the performance
of the best disambiguation systems. Interestingly,
Ng et al (1999) show that, when a coarse-grained
sense inventory is adopted, the increase in inter-
annotator agreement is much higher than the re-
duction of the polysemy degree.
Following these observations, the main ques-
tion that we tackle in this paper is: can we pro-
duce and evaluate coarse-grained sense distinc-
tions and show that they help boost disambigua-
tion on standard test sets? We believe that this is
a crucial research topic in the field of WSD, that
could potentially benefit several application areas.
The contribution of this paper is two-fold. First,
we provide a wide-coverage method for clustering
WordNet senses via a mapping to a coarse-grained
sense inventory, namely the Oxford Dictionary of
English (Soanes and Stevenson, 2003) (Section 2).
We show that this method is well-founded and ac-
curate with respect to manually-made clusterings
(Section 3). Second, we evaluate the performance
of WSD systems when using coarse-grained sense
inventories (Section 4). We conclude the paper
with an account of related work (Section 5), and
some final remarks (Section 6).
105
106
107
108
109
110
111
112
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 13?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
13
14
15
16
Structural Semantic Interconnection: a knowledge-based approach to Word 
Sense Disambiguation 
 
Roberto NAVIGLI 
Dipartimento di Informatica, 
Universit? di Roma ?La Sapienza? 
Via Salaria, 113 - 00198 Roma, Italy 
navigli@di.uniroma1.it 
 
Paola VELARDI 
Dipartimento di Informatica, 
Universit? di Roma ?La Sapienza? 
Via Salaria, 113 - 00198 Roma, Italy 
velardi@di.uniroma1.it 
 
Abstract 
In this paper we describe the SSI algorithm, a 
structural pattern matching algorithm for 
WSD. The algorithm has been applied to the 
gloss disambiguation task of Senseval-3. 
1 Introduction 
Our approach to WSD lies in the structural 
pattern recognition framework. Structural or 
syntactic pattern recognition (Bunke and Sanfeliu, 
1990) has proven to be effective when the objects 
to be classified contain an inherent, identifiable 
organization, such as image data and time-series 
data. For these objects, a representation based on a 
?flat? vector of features causes a loss of 
information that negatively impacts on 
classification performances. Word senses clearly 
fall under the category of objects that are better 
described through a set of structured features.  
The classification task in a structural pattern 
recognition system is implemented through the 
use of grammars that embody precise criteria to 
discriminate among different classes. Learning a 
structure for the objects to be classified is often a 
major problem in many application areas of 
structural pattern recognition. In the field of 
computational linguistics, however, several efforts 
have been made in the past years to produce large 
lexical knowledge bases and annotated resources, 
offering an ideal starting point for constructing 
structured representations of word senses. 
2 Building structural representations of 
word senses 
We build a structural representation of word 
senses using a variety of knowledge sources, i.e. 
WordNet, Domain Labels (Magnini and Cavaglia, 
2000), annotated corpora like SemCor and LDC-
DSO1. We use this information to automatically 
 
1 LDC http://www.ldc.upenn.edu/ 
generate labeled directed graphs (digraphs)
representations of word senses. We call these 
semantic graphs, since they represent alternative 
conceptualizations for a lexical item. 
Figure 1 shows an example of the semantic 
graph generated for senses #1 of market, where 
nodes represent concepts (WordNet synsets), and 
edges are semantic relations. In each graph, we 
include only nodes with a maximum distance of 3 
from the central node, as suggested by the dashed 
oval in Figure 1. This distance has been 
experimentally established.  
market#1
goods#1
trading#1
gloss
gloss
merchandise
 #1
k ind-of
monopoly#1
kind
-of
export#1
has
-kin
dactivity#1has-kind
consumer
 goods#1
grocery#2
kind-of
kind-of
load#3
kind
-of
commercial
enterprise#2
has-part
commerce#1 kind -of
transportation#5
has-p
art
business
activity#1
glo
ss
service#1
gloss to p
ic
industry#2
kind-of
h as
-pa
rt
gloss
kind-of
food#1
clothing#1
glo
ss
glos
s
enterprise#1
kind-of
production#1
artifact#1
k i n d -o f
express#1
kind-of
consumption#1
gloss
Figure 1. Graph representations for sense #1 of market.
All the used semantic relations are explicitly 
encoded in WordNet, except for three relations 
named topic, gloss and domain, extracted 
respectively from annotated corpora, sense 
definitions and domain labels.  
3 Summary description of the SSI algorithm  
The SSI algorithm consists of an initialization step 
and an iterative step.  
In a generic iteration of the algorithm the input 
is a list of co-occurring terms T = [ t1, ?, tn ] and 
a list of associated senses I = ],...,[ 1 ntt SS , i.e. the 
semantic interpretation of T, where itS 2 is either 
the chosen sense for ti (i.e., the result of a previous 
 
2 Note that with itS we refer interchangeably to the semantic 
graph associated with a sense or to the sense name.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
disambiguation step) or the empty set (i.e., the 
term is not yet disambiguated).  
A set of pending terms is also maintained, P =
}|{ =iti St . I is named the semantic context of T
and is used, at each step, to disambiguate new 
terms in P. 
The algorithm works in an iterative way, so that 
at each stage either at least one term is removed 
from P (i.e., at least a pending term is 
disambiguated) or the procedure stops because no 
more terms can be disambiguated. The output is 
the updated list I of senses associated with the 
input terms T.
Initially, the list I includes the senses of 
monosemous terms in T. If no monosemous terms 
are found, the algorithm makes an initial guess 
based on the most probable sense of the less 
ambiguous term. The initialisation policy is 
adjusted depending upon the specific WSD task 
considered. Section 5 describes the policy adopted 
for the task of gloss disambiguation in WordNet. 
During a generic iteration, the algorithm selects 
those terms t in P showing an interconnection 
between at least one sense S of t and one or more 
senses in I. The likelihood for a sense S of being 
the correct interpretation of t, given the semantic 
context I, is estimated by the function 
CxTfI : , where C is the set of all the 
concepts in the ontology O, defined as follows: 


 	


=
otherwise
SynsetstSensesSifISSS
tSf I 0
)(})'|)',(({
),(

where Senses(t) is the subset of concepts C in O
associated with the term t, and 
})'...|)...(({')',( 1121
121 SSSSeeewSS nn
e
n
eee
n = 
 ,
i.e. a function (?) of the weights (w) of each path 
connecting S with S?, where S and S? are 
represented by semantic graphs. A semantic path 
between two senses S and S?, '... 11
121 SSSS nn
e
n
eee  
 ,
is represented by a sequence of edge labels 
neee  ...21 . A proper choice for both  and ? may 
be the sum function (or the average sum function). 
A context-free grammar G = (E, N, SG, PG)
encodes all the meaningful semantic patterns. The 
terminal symbols (E) are edge labels, while the 
non-terminal symbols (N) encode (sub)paths 
between concepts; SG is the start symbol of G and 
PG the set of its productions. 
We associate a weight with each production 
A in PG, where NA
 and *)( EN 
 , i.e. 
 is a sequence of terminal and non-terminal 
symbols. If the sequence of edge labels neee  ...21
belongs to L(G), the language generated by the 
grammar, and provided that G is not ambiguous, 
then )...( 21 neeew  is given by the sum of the 
weights of the productions applied in the 
derivation nG eeeS + ...21 . The grammar G is 
described in the next section. 
Finally, the algorithm selects ),(maxarg tSfI
CS

as 
the most likely interpretation of t and updates the 
list I with the chosen concept. A threshold can be 
applied to ),( tSf to improve the robustness of 
system?s choices. 
At the end of a generic iteration, a number of 
terms is disambiguated and each of them is 
removed from the set of pending terms P. The 
algorithm stops with output I when no sense S can 
be found for the remaining terms in P such that 
0),( >tSfI , that is, P cannot be further reduced. 
In each iteration, interconnections can only be 
found between the sense of a pending term t and 
the senses disambiguated during the previous 
iteration.  
A special case of input for the SSI algorithm is 
given by ]..., ,,[ =I , that is when no initial 
semantic context is available (there are no 
monosemous words in T). In this case, an 
initialization policy selects a term t 
 T and the 
execution is forked into as many processes as the 
number of senses of t.
4 The grammar 
The grammar G has the purpose of describing 
meaningful interconnecting patterns among 
semantic graphs representing conceptualisations 
in O. We define a pattern as a sequence of 
consecutive semantic relations neee  ...21 where 
Eei 
 , the set of terminal symbols, i.e. the 
vocabulary of conceptual relations in O. Two 
relations 1+ii ee are consecutive if the edges 
labelled with ie and 1+ie are incoming and/or 
outgoing from the same concept node, that is 
1)( + ii ee S , 1)( + ii ee S , 1)( + ii ee S , 1)( + ii ee S . A meaningful 
pattern between two senses S and S? is a sequence 
neee  ...21 that belongs to L(G). 
In its current version, the grammar G has been 
defined manually, inspecting the intersecting 
patterns automatically extracted from pairs of 
manually disambiguated word senses co-occurring 
in different domains. Some of the rules in G are 
inspired by previous work on the eXtended 
WordNet project described in (Milhalcea and 
Moldovan, 2001). The terminal symbols ei are the 
conceptual relations extracted from WordNet and 
other on-line lexical-semantic resources, as 
described in Section 2. 
G is defined as a quadruple (E, N, SG, PG), 
where E = { ekind-of, ehas-kind, epart-of, ehas-part, egloss, eis-
in-gloss, etopic, ? }, N = { SG, Ss, Sg, S1, S2, S3, S4, S5,
S6, E1, E2, ? }, and PG includes about 50 
productions.  
As stated in previous section, the weight 
)...( 21 neeew  of a semantic path neee  ...21 is given 
by the sum of the weights of the productions 
applied in the derivation nG eeeS + ...21 . These 
weights have been learned using a perceptron 
model, trained with standard word sense 
disambiguation data, such as the SemCor corpus. 
Examples of the rules in G are provided in the 
subsequent Section 5. 
5 Application of the SSI algorithm to the 
disambiguation of WordNet glosses 
For the gloss disambiguation task, the SSI 
algorithm is initialized as follows: In step 1, the 
list I includes the synset S whose gloss we wish to 
disambiguate, and the list P includes all the terms 
in the gloss and in the gloss of the hyperonym of 
S. Words in the hyperonym?s gloss are useful to 
augment the context available for disambiguation.  
In the following, we present a sample execution of 
the SSI algorithm for the gloss disambiguation 
task applied to sense #1 of retrospective: ?an
exhibition of a representative selection of an 
artist?s life work?. For this task the algorithm uses 
a context enriched with the definition of the synset 
hyperonym, i.e. art exhibition#1: ?an exhibition of 
art objects (paintings or statues)?.  
Initially we have: 
I = { retrospective#1 }3
P = { work, object, exhibition, life, statue, artist, 
selection, representative, painting, art }
At first, I is enriched with the senses of 
monosemous words in the definition of 
retrospective#1 and its hyperonym: 
I = { retrospective#1, statue#1, artist#1 }
P = { work, object, exhibition, life, selection, 
representative, painting, art }
since statue and artist are monosemous terms in 
WordNet. During the first iteration, the algorithm 
finds three matching paths4:
retrospective#1 2  ofkind exhibition#2, statue#1 
3  ofkind  art#1 and statue#1 
3 For convenience here we denote I as a set rather 
than a list. 
4 With S R  i S? we denote a path of i consecutive 
edges labeled with the relation R interconnecting S
with S?.
6  ofkind object#1 
This leads to: 
I = { retrospective#1, statue#1, artist#1, 
exhibition#2, object#1, art#1 }
P = { work, life, selection, representative, painting 
}
During the second iteration, a 
hyponymy/holonymy path (rule S2) is found:  
art#1 2  kindhas painting#1 (painting is a kind 
of art)which leads to: 
I = { retrospective#1, statue#1, artist#1, 
exhibition#2, object#1, art#1, painting#1 }
P = { work, life, selection, representative }
The third iteration finds a co-occurrence (topic 
rule) path between artist#1 and sense 12 of life 
(biography, life history): 
artist#1 topic  life#12 
then, we get: 
I = { retrospective#1, statue#1, artist#1, 
exhibition#2, object#1, art#1, painting#1, life#12 
}
P = { work, selection, representative }
The algorithm stops because no additional 
matches are found. The chosen senses concerning 
terms contained in the hyperonym?s gloss were of 
help during disambiguation, but are now 
discarded. Thus we have: 
GlossSynsets(retrospective#1) = { artist#1, 
exhibition#2, life#12, work#2 }
6 Evaluation  
The SSI algorithm is currently tailored for noun 
disambiguation. Additional semantic knowledge 
and ad-hoc rules would be needed to detect 
semantic patterns centered on concepts associated 
to verbs. Current research is directed towards 
integrating in semantic graphs information from 
FrameNet and VerbNet, but the main problem is 
harmonizing these knowledge bases with 
WordNet?s senses and relations inventory.  A 
second problem of SSI, when applied to 
unrestricted WSD tasks, is that it is designed to 
disambiguate with high precision, possibly low 
recall. In many interesting applications of WSD, 
especially in information retrieval, improved 
document access may be obtained even when only 
few words in a query are disambiguated, but the 
disambiguation precision needs to be well over 
the 70% threshold. Supporting experiments are 
described in (Navigli and Velardi, 2003). 
The results obtained by our system in Senseval-
3 reflect these limitations (see Figure 2).  
The main run, named OntoLearn, uses a 
threshold to select only those senses with a weight 
over a given threshold. OntoLearnEx uses a non-
greedy version of the SSI algorithm. Again, a 
threshold is used to accepts or reject sense 
choices. Finally, OntoLearnB uses the ?first 
sense? heuristics to select a sense, every since a 
sense choice is below the threshold (or no patterns 
are found for a given word).  
82.60% 75.30%
37.50%
68.50%
68.40%
32.30%39.10%
49.70%
99.90%
0%
20%
40%
60%
80%
100%
OntoLearn OntoLearnB OntoLearnEx
Precision Recall Attempted
Figure 2. Results of three runs submitted to Senseval-3. 
Table 1 shows the precision and recall of 
OntoLearn main run by syntactic category. It 
shows that, as expected, the SSI algorithm is 
currently tuned for noun disambiguation. 
 
Nouns Verbs Adj. 
Precision 86.0% 69.4% 78.6% 
Recall 44.7% 13.5% 26.2% 
Attempted 52.0% 19.5% 33.3% 
Table 1. Precision and Recall by syntactic category. 
The official Senseval-3 evaluation has been 
performed against a set of so called ?golden 
glosses? produced by Dan Moldovan and its 
group5. This test set however had several 
problems, that we partly detected and submitted to 
the organisers. 
Besides some technical errors in the data set 
(presence of WordNet 1.7 and 2.0 senses, missing 
glosses, etc.) there are sense-tagging 
inconsistencies that are very evident. 
For example, one of our highest performing 
sense tagging rules in SSI is the direct 
hyperonymy path.  This rule reads as follows: ?if 
the word wj appears in the gloss of a synset Si, and 
if one of the synsets of wj, Sj, is the direct 
hyperonym of Si, then, select Sj as the correct 
sense for wj?. 
An example is custom#4 defined as ?habitual 
patronage?. We have that: 
{custom-n#4} kind _ of  {trade,patronage-n#5} 
 
5 http://xwn.hlt.utdallas.edu/wsd.html 
therefore we select sense #5 of patronage, while 
Moldovan?s ?golden? sense is #1. 
We do not intend to dispute whether the 
?questionable? sense assignment is the one 
provided in the golden gloss or rather the 
hyperonym selected by the WordNet 
lexicographers. In any case, the detected patterns 
show a clear inconsistency in the data.  
These patterns (313) have been submitted to the 
organisers, who then decided to remove them 
from the data set.   
7 Conclusion 
The interesting feature of the SSI algorithm, 
unlike many co-occurrence based and statistical 
approaches to WSD, is a justification (i.e. a set of 
semantic patterns) to support a sense choice. 
Furthermore, each sense choice has a weight 
representing the confidence of the system in its 
output. Therefore SSI can be tuned for high 
precision (possibly low recall), an asset that we 
consider more realistic for practical WSD 
applications. 
Currently, the system is tuned for noun 
disambiguation, since we build structural 
representations of word senses using lexical 
knowledge bases that are considerably richer for 
nouns. Extending semantic graphs associated to 
verbs and adding appropriate interconnection 
rules implies harmonizing WordNet and available 
lexical resources for verbs, e.g. FrameNet and 
VerbNet. This extension is in progress. 
References  
H. Bunke and A. Sanfeliu (editors) (1990) 
Syntactic and Structural pattern Recognition: 
Theory and Applications World Scientific, Series 
in Computer Science vol. 7, 1990. 
A. Gangemi, R. Navigli and P. Velardi (2003) 
?The OntoWordNet Project: extension and 
axiomatization of conceptual relations in 
WordNet?, 2nd Int. Conf. ODBASE, ed. Springer 
Verlag, 3-7 November 2003, Catania, Italy. 
B. Magnini and G. Cavaglia (2000) 
?Integrating Subject Field Codes into WordNet?, 
Proceedings of  LREC2000, Atenas 2000. 
Milhalcea R., Moldovan D. I. (2001) 
?eXtended WordNet: progress report?. NAACL 
2001 Workshop on WordNet and other lexical 
resources, Pittsburg, June 2001. 
Navigli R. and Velardi P. (2003) ?An Analysis 
of Ontology-based Query Expansion Strategies?, 
Workshop on Adaptive Text Extraction and 
Mining September 22nd, 2003 Cavtat-Dubrovnik 
(Croatia), held in conjunction with ECML 2003. 
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 1?9,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Enriching a formal ontology with a thesaurus: an application in the cultural heritage domain 
 Roberto Navigli, Paola Velardi Dipartimento di Informatica,  Universit? ?La Sapienza?, Italy  navigli,velardi@di.uniroma1.it   Abstract This paper describes a pattern-based method to automatically enrich a core ontology with the definitions of a domain glossary. We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology. To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others. 1 Introduction Large-scale, automatic semantic annotation of web documents based on well established domain ontologies would allow various Semantic Web applications to emerge and gain acceptance. Wide coverage ontologies are indeed available for general-purpose domains (e.g. WordNet, CYC, SUMO1), however semantic annotation in unconstrained areas seems still out of reach for state of art systems. Domain-specific ontologies are preferable since they limit the domain and make the applications feasible. Furthermore, real-world applications (e.g tourism, cultural heritage, e-commerce) are dominated by the requirements of the related web communities, who began to believe in the benefits deriving from the application of Semantic Web techniques.  These communities are interested in extracting from texts specific types of information, rather than general-purpose relations. Accordingly, they produced remarkable efforts to conceptualize their competence domain through the definition of a core ontology2.                                                 1 WordNet: http://wordnet.princeton.edu, CYC: http://www.opencyc.org, SUMO: http://www.ontologyportal.org 2 a core ontology is a very basic ontology consisting only of the minimal concepts relations and axioms 
Relevant examples are in the area of enterprise modeling (Fox et al 1997) (Uschold et al 1998) and cultural heritage (Doerr, 2003). Core ontologies are indeed a necessary starting point to model in a principled way the basic concepts, relations and axioms of a given domain. But in order for an ontology to be really usable in applications, it is necessary to enrich the core structure with the thousands of concepts and instances that ?make? the domain.  In this paper we present a methodology to automatically annotate a glossary G with the semantic relations of an existing core ontology O. Glosses are then converted into formal concepts, used to enrich O. The annotation of glossary definitions is performed using regular expressions, a widely adopted text mining approach. However, while in the literature regular expressions seek mostly for patterns at the lexical and part of speech level, we defined more complex expressions enriched with syntactic and semantic constraints.  A word sense disambiguation algorithm, SSI (Velardi and Navigli, 2005), is used to automatically replace the high level semantic constraints specified in the core ontology with fine?grained sense restrictions, using the sense inventory of a general purpose lexicalized ontology, WordNet.  We experimented our methodology in the cultural heritage domain, since for this domain several well-established resources are available, like the CIDOC-CRM core ontology, the AAT art and architecture thesaurus, and others. The paper is organized as follows: in Section 2 we briefly present the CIDOC and the other resources used in this work. In Section 3 we describe in detail the ontology enrichment algorithm. Finally, in Section 4 we provide a performance evaluation on a subset of CIDOC                                                                     required to understand the other concepts in the domain. 
1
properties and a sub-tree of the AAT thesaurus. Related literature is examined in Section 5. 2 Semantic and lexical resources in the cultural heritage domain  In this section we briefly describe the resources that have been used in this work. 2.1 The CIDOC CRM The core ontology O is the CIDOC CRM (Doerr, 2003), a formal core ontology whose purpose is to facilitate the integration and exchange of cultural heritage information between heterogeneous sources. It is currently being elaborated to become an ISO standard. In the current version (4.0) the CIDOC includes 84 taxonomically structured concepts (called entities) and a flat set of 141 semantic relations, called properties. Properties are defined in terms of domain (the class for which a property is formally defined) and range (the class that comprises all potential values of a property), e.g.:  P46 is composed of (forms part of) Domain:  E19 Physical Object Range:  E42 Object Identifier  The CIDOC is an ?informal? resource. To make it usable by a computer program, we replaced specifications written in natural language with formal ones. For each property R, we created a tuple R(Cd,Cr) where Cd and Cr are the domain and range entities specified in the CIDOC reference manual. 2.2 The AAT thesaurus The domain glossary G is the Art and Architecture Thesaurus (AAT) a controlled vocabulary for use by indexers, catalogers, and other professionals concerned with information management in the fields of art and architecture. In its current version3 it includes more than 133,000 terms, descriptions, bibliographic citations, and other information relating to fine art, architecture, decorative arts, archival materials, and material culture.  An example is the following:  maest? Note: Refers to a work of a specific iconographic type, depicting the Virgin Mary and Christ Child enthroned in                                                 3 http://www.getty.edu/research/conducting_research/ vocabularies/aat/ 
the center with saints and angels in adoration to each side. The type developed in Italy in the 13th century and was based on earlier Greek types. Works of this type are typically two-dimensional, including painted panels (often altarpieces), manuscript illuminations, and low-relief carvings. Hierarchical Position:  Objects Facet  .... Visual and Verbal Communication  ........ Visual Works  ............ <visual works>  ................ <visual works by subject type>  .................... maest?  We manually mapped the top CIDOC entities to AAT concepts, as shown in Table 1.  AAT topmost CIDOC entities Top concept of AAT  CRM Entity (E1), Persistent Item (E77) Styles and Periods Period (E4) Events Event (E5) Activities Facet Activity (E7) Processes/Techniques Beginning of Existence (E63) Objects Facet Physical Stuff (E18), Physical Object (E19) Artifacts Physical Man-Made Stuff (E24) Materials Facet Material  (E57) Agents Facet Actor (E39) Time Time-Span (E52) Place Place (E53) Table 1: mapping between AAT and CIDOC. 2.3 Additional resources A general purpose lexicalised ontology, WordNet, is used to bridge the high level concepts defined in the core ontology with the words in a fragment of text. As better clarified later, WordNet  is used to verify that certain words in a string of text f satisfy the range constraints R(Cd,Cr) in the CIDOC. In order to do so, we manually linked the WordNet topmost concepts to the CIDOC entities. For example, the concept E19 (Physical Object) is mapped to the WordNet synset ?object, physical object?. Furthermore, we created a gazetteer I of named entities extracting names from the Dmoz4, a large human-edited directory of the web, the Union List of Artist Names (ULAN) and the Getty Thesaurus of Geographic Names (GTG) provided by the Getty institute, along with the AAT. Named entities often occur in AAT definitions, therefore, NE recognition is relevant for our task. 
                                                4 http://dmoz.org/about.html 
2
3 Enriching the CIDOC CRM with the AAT thesaurus In this Section we describe in detail the method for automatic semantic annotation and ontology enrichment in the cultural heritage domain.  We start with an example of the task to be performed: given a gloss G of a term t in the glossary G, the first objective is to annotate certain gloss fragments with CIDOC relations. For example, the following gloss fragment for ?vedute? is annotated with a CIDOC relation, as follows: [..]The first vedute probably were <carried-out-by>painted by northern European artists</carried-out-by> [...] Then, for each annotated fragment, we extract a semantic relation instance R(Ct,Cw), where R is a relation in O, Ct and Cw are respectively the domain and range of R. The concept Ct corresponds to its lexical realization t, while Cw is the concept associated to the ?head? word w in the annotated segment of the gloss.  In the previous example, the relation instance is: R carried_out_by(vedute,European_artist) The annotation process allows to automatically enrich O with an existing glossary in the same domain of O, since each pair of term and gloss (t,G) in the glossary G is transformed into a formal definition, compliant with O. Furthermore, the very same method used to annotate definitions can be used to annotate free text with the relations of the enriched ontology O?.  We now describe the method in detail. Let G be a glossary, t a term in G and G the corresponding natural language definition (gloss).  The main steps of the algorithm are the following: 1. Part-of-Speech analysis. Each input gloss is processed with a part-of-speech tagger, TreeTagger5. As a result, for each gloss G = w1 w2 ? wn, a string of part-of-speech tags p1 p2 ? pn is produced, where pi 
? 
?P is the part-of-speech tag chosen by TreeTagger for word wi, and P = { N, A, V, J, R, C, P, S, W } is a simplified set of syntactic categories (respectively, nouns, articles, verbs, adjectives, adverbs, conjunctions, prepositions,                                                 5 TreeTagger is available at: http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger.  
symbols, wh-words). Terminological strings (european artist) are detected using our Term Extractor tool, already described in (Navigli and Velardi, 2004).  2. Named Entity recognition. We augmented TreeTagger with the ability to capture named entities of locations, organizations, persons, numbers and time expressions. In order to do so, we use regular expressions (Friedl, 1997) in a rather standard way, therefore we omit details. When a named entity string wi wi+1 ? wi+j is recognized, it is transformed into a single term and a specific part of speech denoting the kind of entity is assigned to it (L for cities (e.g. Venice), countries and continents, T for time and historical periods (e.g. Middle Ages), O for organizations and persons (e.g. Leonardo Da Vinci), B for numbers). 3. Annotation of sentence segments with CIDOC properties. Once the text has been parsed, we use manually defined regular expressions to capture relevant fragments. The regular expressions are used to annotate gloss segments with properties grounded on the CIDOC-CRM relation model. Given a gloss G and a property6 R, we define a relation checker cR taking in input G and producing in output a set FR of fragments of G annotated with the property R: <R>f</R>. The selection of a fragment f to be included in the set FR is based on three different kinds of constraints:  ? a part-of-speech constraint p(f, pos-string) matches the part-of-speech (pos) string associated with the fragment f against a regular expression (pos-string), specifying the required syntactic structure. ? a lexical constraint l(f, k, lexical-constraint) matches the lemma of the word in k-th position of f against a regular expression (lexical-constraint), constraining the lexical conformation of words occurring within the fragment f. ? semantic constraints on domain and range sD(f, semantic-domain) and s(f, k, semantic-range) are valid, respectively, if the term t and the word in the k-th position of f match the semantic constraints on domain and range imposed by the CIDOC, i.e. if there exists at least one sense of t Ct and one sense of w Cw such that:                                                 6 In what follows, we adopt the CIDOC terminology for relations and concepts, i.e. properties and entities. 
3
Rkind-of*(Cd, Ct) and Rkind-of*(Cr, Cw)7  More formally, the annotation process is defined as follows: A relation checker cR for a property R is a logical expression composed with constraint predicates and logical connectives, using the following production rules:  cR ? sD(f, semantic-domain) 
? 
? cR? cR? ? ?cR?| (cR? ? cR?) | (cR? 
? 
? cR?) cR? ? p(f, pos-string) | l(f, k, lexical-constraint)  | s(f, k, semantic-range)  where f is a variable representing a sentence fragment. Notice that a relation checker must always specify a semantic constraint sD on the domain of the relation R being checked on fragment f. Optionally, it must also satisfy a semantic constraint s on the k-th element of f, the range of R. For example, the following excerpt of the checker for the is-composed-of relation (P46):  (1) cis-composed-of(f) = sD(f, physical object#1)  
? 
? p(f, ?(V)1(P)2R?A?[CRJVN]*(N)3?)  
? 
? l(f, 1,  ?^(consisting|composed|comprised|constructed)$?)  
? 
? l(f, 2, ?of?) 
? 
? s(f, 3, physical_object#1)  reads as follows: ?the fragment f is valid if it consists of a verb in the set { consisting, composed, comprised, constructed }, followed by a preposition ?of?, a possibly empty number of adverbs, adjectives, verbs and nouns, and terminated by a noun interpretable as a physical object in the WordNet concept inventory?. The first predicate, sD, requires that also the term t whose gloss contains f (i.e., its domain) be interpretable as a physical object. Notice that some letter in the regular expression specified for the part-of-speech constraint is enclosed in parentheses. This allows it to identify the relative positions of words to be matched against lexical and semantic constraints, as shown graphically in Figure 1. 
(V)
1
(P)
2
R?A?[CRJVN]*(N)
3
(composed)
1
 (of)
2
 two or more (negatives)
3
part-of-speech string
gloss fragmentFigure 1. Correspondence between parenthesized part-of-speech tags and words in a gloss fragment.  Checker (1) recognizes, among others, the following fragments (the words whose part-of-                                                7 Rkind-of* denotes zero, one, or more applications of Rkind-of. 
speech tags are enclosed in parentheses are indicated in bold):  (consisting)1 (of)2 semi-precious (stones)3 (matching part-of-speech string: (V)1(P)2 J(N)3) (composed)1 (of)2 (knots)3 (matching part-of-speech string: (V) 1(P)2(N)3) As a second example, an excerpt of the checker for the consists-of (P45) relation is the following:  (2) cconsists-of(f) = sD(f, physical object#1)  
? 
?p(f, ?(V)1(P)2A?[JN,VC]*(N)3?) 
? 
? l(f, 1, ?^(make|do|produce|decorated)$?)  
? 
? l(f, 2, ?^(of|by|with)$?)  
? 
? ?s(f, 3, color#1)
? 
? ?s(f, 3, activity#1)  
? 
? (s(f, 3, material#1) 
? 
? s(f, 3, solid#1)  
? 
? s(f, 3, liquid#1))  recognizing, among others, the following phrases: ? (made)1 (with)2 the red earth pigment (sinopia)3 (matching part-of-speech string: (V)1(P)2AJNN(N)3) ? (decorated)1 (with)2 red, black, and white (paint)3 (matching part-of-speech string: (V)1(P)2JJCJ(N)3) Notice that in both checkers (1) and (2) semantic constraints are specified in terms of WordNet sense numbers (material#1, solid#1 and liquid#1), and can also be negative (?color#1 and ?activity#1). The motivation is that CIDOC constraints are coarse-grained due to the small number of available core concepts: for example, the property P45 consists of simply requires that the range belongs to the class Material (E57). Using these coarse grained constraints would produce false positives in the annotation task, as discussed later. Using WordNet for semantic constraints has two advantages: first, it is possible to write more fine-grained (and hence more reliable) constraints, second, regular expressions can be re-used, at least in part, for other domains and ontologies. In fact, several CIDOC properties are rather general-purpose.  Notice that, as remarked in section 2.3, replacing coarse CIDOC sense restrictions with WordNet fine-grained restrictions is possible since we mapped the 84 CIDOC entities onto WordNet topmost concepts. 4. Formalisation of glosses. The annotations generated in the previous step are the basis for extracting property instances to enrich the CIDOC CRM with a conceptualization of the AAT terms. In general, for each gloss G defining a concept Ct, 
4
and for each fragment f ? FR of G annotated with the property R: <R>f</R>, it is possible to extract one or more property instances in the form of a triple R(Ct, Cw), where Cw is the concept associated with a term or multi-word expression w occurring in f (i.e. its language realization) and Ct is the concept associated to the defined term t in AAT. For example, from the definition of tatting (a kind of lace) the algorithm automatically annotates the phrase composed of knots, suggesting that this phrase specifies the range of the is-composed-of property for the term tatting: Ris-composed-of(Ctatting, Cknot) In this property instance, Ctatting is the domain of the property (a term in the AAT glossary) and Cknot is the range (a specific term in the definition G of tatting).  Selecting the concept associated to the domain is rather straightforward: glossary terms are in general not ambiguous, and, if they are, we simply use a numbering policy to identify the appropriate concept. In the example at hand, Ctatting=tatting#1 (the first and only sense in AAT). Therefore, if Ct matches the domain restrictions in the regular expression for R, then the domain of the relation is considered to be Ct. Selecting the range of a relation is instead more complicated. The first problem is to select the correct words in a fragment f. Only certain words of an annotated gloss fragment can be exploited to extract the range of a property instance. For example, in the phrase ?depiction of fruit, flowers, and other objects? (from the definition of still life), only fruit, flowers, objects represent the range of the property instances of kind depicts (P62). When writing relation checkers, as described in the previous paragraph of this Section, we can add markers of ontological relevance by specifying a predicate r(f, k) for each relevant position k in a fragment f. The purpose of these markers is precisely to identify words in f whose corresponding concepts are in the range of a property. For instance, the checker (1) cis-composed-of from the previous paragraph is augmented with the conjunction: 
? 
? r(f, 3). We added the predicate r(f, 3) because the third parenthesis in the part-of-speech string refers to an ontologically relevant element (i.e. the candidate range of the is-composed-of property).  The second problem is that words that are candidate ranges can be ambiguous, and they 
often are, especially if they do not belong to the domain glossary G. Considering the previous example of the property depicts, the word fruit is not a term of the AAT glossary, and it has 3 senses in WordNet (the fruit of a plant, the consequence of some action, an amount of product). The property depicts, as defined in the CIDOC, simply requires that the range be of type Entity (E1). Therefore, all the three senses of fruit in WordNet satisfy this constraint. Whenever the range constraints in a relation checker do not allow a full disambiguation, we apply the SSI algorithm (Navigli and Velardi, 2005), a semantic disambiguation algorithm based on structural pattern recognition, available on-line8. The algorithm is applied to the words belonging to the segment fragment f and is based on the detection of relevant semantic interconnection patterns between the appropriate senses. These patterns are extracted from a lexical knowledge base that merges WordNet with other resources, like word collocations, on-line dictionaries, etc. For example, in the fragment ?depictions of fruit, flowers, and other objects? the following properties are created for the concept still_ life#1: 
 Rdepicts(still_ life#1, fruit#1) Rdepicts (still_ life#1, flower#2) Rdepicts (still_ life#1, object#1)  Some of the semantic patterns supporting this sense selection are shown in Figure 2. A further possibility is that the range of a relation R is a concept instance. We create concept instances if the word w extracted from the fragment f is a named entity. For example, the definition of Venetian lace is annotated as ?Refers to needle lace created <current-or-former-location> in Venice</current-or-former-location> [?]?. As a result, the following triple is produced: Rhas-current-or-former-location(Venetian_lace#1, Venice:city#1) where Venetian_ lace#1 is the concept label generated for the term Venetian lace in the AAT and Venice is an instance of the concept city#1 (city, metropolis, urban center) in WordNet.                                                  8 SSI is an on-line knowledge-based WSD algorithm accessible from http://lcl.di.uniroma1.it/ssi. The on-line version also outputs the detected semantic connections (as those in Figure 2). 
5
fruit#1
flower#2
object#1
depiction#1
bunch#1
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
flower
head#1
related-to
h
a
s
-
p
a
r
t
cyme#1
r
e
l
a
t
e
d
-
t
o
inflorescence
#2
k
i
n
d
-
o
f
r
e
l
a
t
e
d
-
t
o
still life#1
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
description#1
k
i
n
d
-
o
f
statement#1
k
i
n
d
-
o
f
thing#5
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
appearance#1
portrayal#1
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
k
i
n
d
-
o
f
forest#2
land#3
k
i
n
d
-
o
f
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
plant#1
r
e
l
a
t
e
d
-
t
o
organism#1
living thing#1
k
i
n
d
-
o
f
kind-of
k
i
n
d
-
o
f
 Figure 2. Semantic Interconnections selected by the SSI algorithm when given the word list: ?depiction, fruit, flower, object?. 4 Evaluation Since the CIDOC-CRM model formalizes a large number of fine-grained properties (precisely, 141), we selected a subset of properties for our experiments (reported in Table 2). We wrote a relation checker for each property in the Table. By applying the checkers in cascade to a gloss G, a set of annotations is produced. The following is an example of an annotated gloss for the term ?vedute?:  Refers to detailed, largely factual topographical views, especially <has-time-span>18th-century</has-time-span> Italian paintings, drawings, or prints of cities. The first vedute probably were <carried-out-by>painted by northern European artists</carried-out-by> who worked <has former-or-current-location>in Italy</has former-or-current-location><has-time-span>in the 16th century</has-time-span>. The term refers more generally to any painting, drawing or print <depicts>representing a landscape or town view</depicts> that is largely topographical in conception.  Figure 3 shows a more comprehensive graph representation of the outcome for the concepts vedute#1 and maest?#1 (see the gloss in Section 2.2). To evaluate the methodology described in Section 3 we considered 814 glosses from the Visual Works sub-tree of the AAT thesaurus, containing a total of 27,925 words. The authors wrote the relation checkers by tuning them on a subset of 122 glosses, and tested their generality on the remaining 692. The test set was manually tagged with the subset of the CIDOC-CRM properties shown in Table 2 by two annotators with adjudication (requiring a careful comparison of the two sets of annotations). We performed two experiments: in the first, we evaluated the gloss annotation task, in the 
second the property instance extraction task, i.e. the ability to identify the appropriate domain and range of a property instance. In the case of the gloss annotation task, for evaluating each piece of information we adopted the measures of ?labeled? precision and recall. These measures are commonly used to evaluate parse trees obtained by a parser (Charniak, 1997) and allow the rewarding of good partial results. Given a property R, labeled precision is the number of words annotated correctly with R over the number of words annotated automatically with R, while labeled recall is the number of words annotated correctly with R over the total number of words manually annotated with R. Table 3 shows the results obtained by applying the checkers to tag the test set (containing a total number of 1,328 distinct annotations and 5,965 annotated words). Note that here we are evaluating the ability of the system to assign the correct tag to every word in a gloss fragment f, according to the appropriate relation checker. We choose to evaluate the tag assigned to single words rather than to a whole phrase, because each misalignment would count as a mistake even if the most part of a phrase was tagged correctly by the automatic annotator. The second experiment consisted in the evaluation of the property instances extracted. Starting from 1,328 manually annotated fragments of 692 glosses, the checkers extracted an overall number of 1,101 property instances. We randomly selected a subset of 160 glosses for evaluation, from which we manually extracted 344 property instances. Two aspects of the property instance extraction task had to be assessed: ? the extraction of the appropriate range words in a gloss, for a given property instance ? the precision and recall in the extraction of the appropriate concepts for both domain and range of the property instance.  An overall number of 233 property instances were automatically collected by the checkers, out of which 203 were correct with respect to the first assessment (87.12% precision (203/233), 59.01% recall (203/344)). In the second evaluation, for each property instance R(Ct, Cw) we assessed the semantic correctness of both the concepts Ct and Cw. The appropriateness of the concept Ct chosen 
6
for the domain must be evaluated, since, even if a term t satisfies the semantic constraints of the domain for a property R, still it can be the case that a fragment f in G does not refer to t, like in the following example: 
pastels (visual works) -- Works of art, typically on a paper or vellum support, to which designs are applied using crayons made of ground pigment held together with a binder, typically oil or water and gum. 
 Code Name Domain Range Example P26 moved to Move Place P26(installation of public sculpture, public place) P27 moved from Move Place P27(removal of cornice pictures, wall) P53 has former/current location Physical Stuff Place P53(fancy pictures, London) P55 has current location Physical Object Place P55(macrame, Genoa) P46 is composed of (is part of) Physical Stuff Physical Stuff P46(lace, knot) P62 depicts Physical Man-Made Stuff Entity P62(still life, fruit) P4 has time span Temporal Entity Time Span P4(pattern drawings, Renaissance) P14 carried out by (performed) Activity Actor P14(blotted line drawings, Andy Warhol) P92 brought into existence by Persistent Item Beginning of Existence P92(aquatints, aquatint process) P45 consists of (incorporated in) Physical Stuff Material P45(sculpture, stone) Table 2: A subset of the relations from the CIDOC-CRM model.  
maest?
Virgin Mary
Christ child
Italy
13
th
 century
painted panel
carving
altarpiece
illuminations
d
e
p
i
c
t
s
d
e
p
i
c
t
s
i
s
-
c
o
m
p
o
s
e
d
-
o
f
i
s
-
c
o
m
p
o
s
e
d
-
o
f
i
s
-
c
o
m
p
o
s
e
d
-
o
f
is-c
om
po
se
d-o
f
h
a
s
-
c
u
r
r
e
n
t
o
r
-
f
o
r
m
e
r
-
l
o
c
a
t
i
o
n
h
a
s
t
i
m
e
-
s
p
a
n
vedute
landscape
town view
Italy
18
th
 century
artist
d
e
p
i
c
t
s
d
e
p
i
c
t
s
c
a
r
r
i
e
d
-
o
u
t
b
y
h
a
s
-
c
u
r
r
e
n
t
o
r
-
f
o
r
m
e
r
-
l
o
c
a
t
i
o
n
h
a
s
t
i
m
e
-
s
p
a
n
16
th
 century
h
a
s
t
i
m
e
-
s
p
a
n
work
h
a
s
-
t
y
p
e
topographical
views
h
a
s
-
t
y
p
e  Figure 3. Extracted conceptualisation (in graphical form) of the terms maest?#1 and vedute#1 (sense numbers are omitted for clarity). In this example, ground pigment refers to crayons (not to pastels). The evaluation of the semantic correctness of the domain and range of the property instances extracted led to the final figures of 81.11% (189/233) precision and 54.94% (189/344) recall, due to 9 errors in the choice of Ct as a domain for an instance R(Ct, Cw) and 5 errors in the semantic disambiguation of range words w not appearing in AAT, but encoded in WordNet (as described in the last part of Section 3). A final experiment was performed to evaluate the generality of the approach presented in this paper. As already remarked, the same procedure used for annotating the glosses of a thesaurus can be used to annotate web documents. Our objective in this third experiment was to: ? Evaluate the ability of the system to annotate fragments of web documents with CIDOC relations ? Evaluate the domain dependency of the relation checkers, by letting the system annotate documents not in the cultural heritage domain.  
We then selected 5 documents at random from an historical archive and an artist?s biographies archive9 including about 6,000 words in total, about 5,000 of which in the historical domain. We then ran the automatic annotation procedure on these documents and we evaluated the result, using the same criteria as in Table 3.  Property Precision Recall  P26 ? moved to 84.95% (79/93) 64.23% (79/123)  P27 ? moved from 81.25% (39/48) 78.00% (39/50)  P53 - has former or  current location 78.09% (916/1173) 67.80% (916/1351)  P55 ? has current  location 100.00% (8/8) 100.00% (8/8)  P46 ?composed of 87.49% (944/1079) 70.76% (944/1334)  P62 ? depicts 94.15%  (370/393) 65.26% (370/567)  P4 ? has time span 91.93% (547/595) 76.40% (547/716)  P14 - carried out by 91.71% (343/374) 71.91% (343/477)  P92 ? brought into  existence 89.54% (471/526) 62.72% (471/751)  P45 ? consists of 74.67% (398/533) 57.60% (398/691) Avg. performance 85.34% (4115/4822) 67.81% (4115/6068) Table 3: Precision and Recall of the gloss annotation task. Table 4 presents the results of the experiment. Only 5 out of 10 properties had at least one                                                 9 http://historicaltextarchive.com and http://www.artnet.com/library 
7
instance in the analysed documents. It is remarkable that, especially for the less domain-dependent properties, the precision and recall of the algorithm is still high, thus showing the generality of the method. Notice that the historical documents influenced the result much more than the artist biographies, because of their dimension. In Table 4 the recall of P14 (carried out by) is omitted. This is motivated by the fact that this property, in a generic domain, corresponds to the agent relation (?an active animate entity that voluntarily initiates an action?10), while in the cultural heritage domain it has a more narrow interpretation (an example of this relation in the CIDOC handbook is: ?the painting of the Sistine Chapel (E7) was carried out by Michelangelo Buonarroti (E21) in the role of master craftsman (E55)?). However, the domain and range restrictions for P14 correspond to an agent relation, therefore, in a generic domain, one should annotate as ?carried out by? almost any verb phrase with the subject (including pronouns and anaphoric references) in the class Human.  Property Precision Recall P53 ? has former or current location 79.84% (198/248) 77.95% (198/254) P46 ? composed of 83.58% (112/134) 96.55% (112/116) P4 ? has time span 78.32% (112/143) 50.68% (112/221) P14 ? carried out by 60.61% (40/66) - - P45 ? consists of 85.71% (6/7) 37.50% (6/16) Avg. performance 78.26% (468/598) 77.10% (468/607) Table 4: Precision and Recall of a web document annotation task.  5 Related work This paper presented a method to automatically annotate the glosses of a thesaurus, the AAT, with the properties (conceptual relations) of a core ontology, the CIDOC-CRM. Several methods for ontology population and semantic annotation described in literature (e.g. (Thelen and Riloff, 2002; Califf and Mooney, 2004; Cimiano et al 2005; Valarakos et al 2004)) use regular expressions to identify named entities, i.e. concept instances. Other methods extract hypernym11 relations using syntactic and lexical                                                 10 http://www.jfsowa.com/ontology/thematic.htm 11 In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically extracted hypernym is the same as in AAT, and in 26% of the cases, either the extracted hypernym is more general than the one defined in AAT, or the contrary, 
patterns (Snow et al 2005; Morin and Jaquemin 2004) or supervised clustering techniques (Kashyap et al 2003).  In our work, we automatically learn formal concepts, not simply instances or taxonomies (e.g. the graphs of Figure 3) compliant with the semantics of a well-established core ontology, the CIDOC. The method is unsupervised, in the sense that it does not need manual annotation of a significant fragment of text. However, it relies on a set of manually written regular expressions, based on lexical, part-of-speech, and semantic constraints. The structure of regular expressions is rather more complex than in similar works using regular expressions, especially for the use of automatically verified semantic constraints. This complexity is indeed necessary to identify non-trivial relations in an unconstrained text and without training. The issue is however how much this method generalizes to other domains:  ? A first problem is the availability of lexical and semantic resources used by the algorithm. The most critical requirement of the method is the availability of sound domain core ontologies, which hopefully will be produced by other web communities stimulated by the recent success of CIDOC CRM. On the other side, in absence of an agreed conceptual reference model, no large scale annotation is possible at all. As for the other resources used by our algorithm, glossaries, thesaura and gazetteers are widely available in ?mature? domains. If not, we developed a methodology, described in (Navigli and Velardi, 2005b), to automatically create a glossary in novel domains (e.g. enterprise interoperability), extracting definition sentences from domain-relevant documents and authoritative web sites. ? The second problem is about the generality of regular expressions. Clearly, the relation checkers that we defined are tuned on the CIDOC properties. This however is consistent with our target: in specific domains users are interested to identify specific relations, not general purpose. Certain relevant application domains ?like cultural heritage, e-commerce, or tourism- are those that dictate specifications for real-world applications of NLP techniques. However, several CIDOC properties are rather general (especially locative and                                                                        wrt the AAT hierarchy. 
8
temporal relations) therefore some relation checkers easily apply to other domains, as demonstrated by the experiment on automatic annotation of historical archives in Table 4. Furthermore, the method used to verify semantic constraints is fully general, since it is based on WordNet and a general-purpose, untrained semantic disambiguation algorithm, SSI.   ? Finally, the authors believe with some degree of convincement that automatic pattern-learning methods often require non-trivial human effort just like manual methods (because of the need of annotated data, careful parameter setting, etc.), and furthermore they are unable to combine in a non-trivial way different types of features (e.g. lexical, syntactic, semantic). To make an example, a recent work on learning hypernymy patterns (Morin and Jacquemin, 2004) provides the full list of learned patterns. The complexity of these patterns is certainly lower than the regular expression structures used in this work, and many of them are rather intuitive.   In the literature the tasks on which automatic methods have been tested are rather constrained, and do not convincingly demonstrate the superiority of automatic with respect to manually defined patterns. For example, in Senseval-3 (automated labeling of semantic roles12), participating systems are requested to identify semantic roles in a sentence fragment for which the ?frame semantics? is given, therefore the possible semantic relations to be identified are quite limited.  However, we believe that our method can be automated to some degree (for example, machine learning methods can be used to bootstrap the syntactic patterns, and to learn semantic constraints), a research line we are currently exploring. References  M. E. Califf and R.J. Mooney, ?Bottom-up relational learning of pattern matching rules for information extraction? Machine Learning research, 4 (2)177-210, 2004. E. Charniak, ?Statistical Techniques for Natural Language Parsing?, AI Magazine 18(4), 33-44, 1997. P. Cimiano, G. Ladwig and S. Staab, ?Gimme the context: context-driven automatic semantic                                                 12 http://www.clres.com/SensSemRoles.html 
annotation with C-PANKOW? In: Proceedings of the 14th International WWW Conference, WWW 2005, Chiba, Japan, May, 2005. ACM Press. M. Doerr, ?The CIDOC Conceptual Reference Module: An Ontological Approach to Semantic Interoperability of Metadata?. AI Magazine, Volume 24, Number 3, Fall 2003. M. S. Fox, M. Barbuceanu, M. Gruninger, and J. Lin, "An Organisation Ontology for Enterprise Modeling", In Simulating Organizations: Computational Models of Institutions and Groups, M. Prietula, K. Carley & L. Gasser (Eds), Menlo Park CA: AAAI/MIT Press, pp. 131-152. 1997 J.E. F. Friedl ?Mastering Regular Expressions? O?Reilly eds., ISBN: 1-56592-257-3, First edition January 1997. V. Kashyap, C. Ramakrishnan, T. Rindflesch. "Toward (Semi)-Automatic Generation of Bio-medical Ontologies", Proceedings of American Medical Informatics Association, 2003 G. A. Miller, ``WordNet: a lexical database for English.'' In: Communications of the ACM 38 (11), November 1995, pp. 39 - 41.  E. Morin and C. Jacquemin ?Automatic acquisition and expansion of hypernym links? Computer and the Humanities, 38: 363-396, 2004 R. Navigli, P. Velardi. Learning Domain Ontologies from Document Warehouses and Dedicated Websites, Computational Linguistics (30-2), MIT Press, June, 2004. R. Navigli and P. Velardi, ?Structural Semantic Interconnections: a knowledge-based approach to word sense disambiguation?, Special Issue-Syntactic and Structural Pattern Recognition, IEEE TPAMI, Volume: 27, Issue: 7, 2005. R. Navigli, P. Velardi. Automatic Acquisition of a Thesaurus of Interoperability Terms, Proc. of 16th IFAC World Congress, Praha, Czech Republic, July 4-8th, 2005b. R. Snow, D. Jurafsky, A. Y. Ng, "Learning syntactic patters for automatic hypernym discovery", NIPS 17, 2005. M. Thelen, E. Riloff, "A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts", Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2002. M. Uschold, M. King, S. Moralee and Y. Zorgios, ?The Enterprise Ontology?, The Knowledge Engineering Review , Vol. 13, Special Issue on Putting Ontologies to Use (eds. Uschold. M. and Tate. A.), 1998 Valarakos, G. Paliouras, V. Karkaletsis, G. Vouros, ?Enhancing Ontological Knowledge through Ontology Population and Enrichment? in Proceedings of the 14th EKAW conf., LNAI, Vol. 3257, pp. 144-156, Springer Verlag, 2004. 
9
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 30?35,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 07: Coarse-Grained English All-Words Task
Roberto Navigli
Universita` di Roma ?La Sapienza?
Dipartimento di Informatica
Via Salaria, 00198 - Roma Italy
navigli@di.uniroma1.it
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus MD 20872
ken@clres.com
Orin Hargraves
Lexicographer
orinhargraves
@googlemail.com
Abstract
This paper presents the coarse-grained En-
glish all-words task at SemEval-2007. We
describe our experience in producing a
coarse version of the WordNet sense inven-
tory and preparing the sense-tagged corpus
for the task. We present the results of par-
ticipating systems and discuss future direc-
tions.
1 Introduction
It is commonly thought that one of the major obsta-
cles to high-performance Word Sense Disambigua-
tion (WSD) is the fine granularity of sense inven-
tories. State-of-the-art systems attained a disam-
biguation accuracy around 65% in the Senseval-3
all-words task (Snyder and Palmer, 2004), where
WordNet (Fellbaum, 1998) was adopted as a ref-
erence sense inventory. Unfortunately, WordNet is
a fine-grained resource, encoding sense distinctions
that are difficult to recognize even for human an-
notators (Edmonds and Kilgarriff, 2002). Making
WSD an enabling technique for end-to-end applica-
tions clearly depends on the ability to deal with rea-
sonable sense distinctions.
The aim of this task was to explicitly tackle the
granularity issue and study the performance of WSD
systems on an all-words basis when a coarser set
of senses is provided for the target words. Given
the need of the NLP community to work on freely
available resources, the solution of adopting a dif-
ferent computational lexicon is not viable. On the
other hand, the production of a coarse-grained sense
inventory is not a simple task. The main issue
is certainly the subjectivity of sense clusters. To
overcome this problem, different strategies can be
adopted. For instance, in the OntoNotes project
(Hovy et al, 2006) senses are grouped until a 90%
inter-annotator agreement is achieved. In contrast,
as we describe in this paper, our approach is based
on a mapping to a previously existing inventory
which encodes sense distinctions at different levels
of granularity, thus allowing to induce a sense clus-
tering for the mapped senses.
We would like to mention that another SemEval-
2007 task dealt with the issue of sense granularity
for WSD, namely Task 17 (subtask #1): Coarse-
grained English Lexical Sample WSD. In this paper,
we report our experience in organizing Task 07.
2 Task Setup
The task required participating systems to annotate
open-class words (i.e. nouns, verbs, adjectives, and
adverbs) in a test corpus with the most appropriate
sense from a coarse-grained version of the WordNet
sense inventory.
2.1 Test Corpus
The test data set consisted of 5,377 words of run-
ning text from five different articles: the first three
(in common with Task 17) were obtained from the
WSJ corpus, the fourth was the Wikipedia entry for
computer programming1, the fifth was an excerpt of
Amy Steedman?s Knights of the Art, biographies of
Italian painters2. We decided to add the last two
1http://en.wikipedia.org/wiki/Computer programming
2http://www.gutenberg.org/etext/529
30
article domain words annotated
d001 JOURNALISM 951 368
d002 BOOK REVIEW 987 379
d003 TRAVEL 1311 500
d004 COMPUTER SCIENCE 1326 677
d005 BIOGRAPHY 802 345
total 5377 2269
Table 1: Statistics about the five articles in the test
data set.
texts to the initial dataset as we wanted the corpus to
have a size comparable to that of previous editions
of all-words tasks.
In Table 1 we report the domain, number of run-
ning words, and number of annotated words for the
five articles. We observe that articles d003 and d004
are the largest in the corpus (they constitute 51.87%
of it).
2.2 Creation of a Coarse-Grained Sense
Inventory
To tackle the granularity issue, we produced a
coarser-grained version of the WordNet sense inven-
tory3 based on the procedure described by Navigli
(2006). The method consists of automatically map-
ping WordNet senses to top level, numbered entries
in the Oxford Dictionary of English (ODE, (Soanes
and Stevenson, 2003)). The semantic mapping be-
tween WordNet and ODE entries was obtained in
two steps: first, we disambiguated with the SSI algo-
rithm (Navigli and Velardi, 2005) the definitions of
the two dictionaries, together with additional infor-
mation (hypernyms and domain labels); second, for
each WordNet sense, we determined the best match-
ing ODE coarse entry. As a result, WordNet senses
mapped to the same ODE entry were assigned to the
same sense cluster. WordNet senses with no match
were associated with a singleton sense.
In contrast to the automatic method above, the
sense mappings for all the words in our test cor-
pus were manually produced by the third author, an
expert lexicographer, with the aid of a mapping in-
terface. Not all the words in the corpus could be
mapped directly for several reasons: lacking entries
in ODE (e.g. adjectives underlying and shivering),
3We adopted WordNet 2.1, available from:
http://wordnet.princeton.edu
different spellings (e.g. after-effect vs. aftereffect,
halfhearted vs. half-hearted, etc.), derivatives (e.g.
procedural, gambler, etc.). In most of the cases, we
asked the lexicographer to map senses of the orig-
inal word to senses of lexically-related words (e.g.
WordNet senses of procedural were mapped to ODE
senses of procedure, etc.). When this mapping was
not straightforward, we just adopted the WordNet
sense inventory for that word.
We released the entire sense groupings (those in-
duced from the manual mapping for words in the
test set plus those automatically derived on the other
words) and made them available to the participants.
2.3 Sense Annotation
All open-class words (i.e. nouns, verbs, adjectives,
and adverbs) with an existing sense in the WordNet
inventory were manually annotated by the third au-
thor. Multi-word expressions were explicitly iden-
tified in the test set and annotated as such (this was
made to allow a fair comparison among systems in-
dependent of their ability to identify multi-word ex-
pressions).
We excluded auxiliary verbs, uncovered phrasal
and idiomatic verbs, exclamatory uses, etc. The
annotator was allowed to tag words with multiple
coarse senses, but was asked to make a single sense
assignment whenever possible.
The lexicographer annotated an overall number
of 2,316 content words. 47 (2%) of them were ex-
cluded because no WordNet sense was deemed ap-
propriate. The remaining 2,269 content words thus
constituted the test data set. Only 8 of them were as-
signed more than one sense: specifically, two coarse
senses were assigned to a single word instance4 and
two distinct fine-grained senses were assigned to 7
word instances. This was a clear hint that the sense
clusters were not ambiguous for the vast majority of
words.
In Table 2 we report information about the pol-
ysemy of the word instances in the test set. Over-
all, 29.88% (678/2269) of the word instances were
monosemous (according to our coarse sense inven-
tory). The average polysemy of the test set with the
coarse-grained sense inventory was 3.06 compared
to an average polysemy with the WordNet inventory
4d005.s004.t015
31
polysemy N V A R all
monosemous 358 86 141 93 678
polysemous 750 505 221 115 1591
total 1108 591 362 208 2269
Table 2: Statistics about the test set polysemy (N =
nouns, V = verbs, A = adjectives, R = adverbs).
of 6.18.
2.4 Inter-Annotator Agreement
Recent estimations of the inter-annotator agreement
when using the WordNet inventory report figures of
72.5% agreement in the preparation of the English
all-words test set at Senseval-3 (Snyder and Palmer,
2004) and 67.3% on the Open Mind Word Expert an-
notation exercise (Chklovski and Mihalcea, 2002).
As the inter-annotator agreement is often consid-
ered an upper bound for WSD systems, it was de-
sirable to have a much higher number for our task,
given its coarse-grained nature. To this end, beside
the expert lexicographer, a second author indepen-
dently performed part of the manual sense mapping
(590 word senses) described in Section 2.2. The
pairwise agreement was 86.44%.
We repeated the same agreement evaluation on
the sense annotation task of the test corpus. A sec-
ond author independently annotated part of the test
set (710 word instances). The pairwise agreement
between the two authors was 93.80%. This figure,
compared to those in the literature for fine-grained
human annotations, gives us a clear indication that
the agreement of human annotators strictly depends
on the granularity of the adopted sense inventory.
3 Baselines
We calculated two baselines for the test corpus: a
random baseline, in which senses are chosen at
random, and the most frequent baseline (MFS), in
which we assign the first WordNet sense to each
word in the dataset.
Formally, the accuracy of the random baseline
was calculated as follows:
BLRand = 1|T |
|T |?
i=1
1
|CoarseSenses(wi)|
where T is our test corpus, wi is the i-th word
instance in T , and CoarseSenses(wi) is the set of
coarse senses for wi according to the sense cluster-
ing we produced as described in Section 2.2.
The accuracy of the MFS baseline was calculated
as:
BLMFS = 1|T |
|T |?
i=1
?(wi, 1)
where ?(wi, k) equals 1 when the k-th sense of
word wi belongs to the cluster(s) manually associ-
ated by the lexicographer to word wi (0 otherwise).
Notice that our calculation of the MFS is based on
the frequencies in the SemCor corpus (Miller et al,
1993), as we exploit WordNet sense rankings.
4 Results
12 teams submitted 14 systems overall (plus two
systems from a 13th withdrawn team that we will
not report). According to the SemEval policy for
task organizers, we remark that the system labelled
as UOR-SSI was submitted by the first author (the
system is based on the Structural Semantic Inter-
connections algorithm (Navigli and Velardi, 2005)
with a lexical knowledge base composed by Word-
Net and approximately 70,000 relatedness edges).
Even though we did not specifically enrich the al-
gorithm?s knowledge base on the task at hand, we
list the system separately from the overall ranking.
The results are shown in Table 3. We calcu-
lated a MFS baseline of 78.89% and a random base-
line of 52.43%. In Table 4 we report the F1 mea-
sures for all systems where we used the MFS as a
backoff strategy when no sense assignment was at-
tempted (this possibly reranked 6 systems - marked
in bold in the table - which did not assign a sense
to all word instances in the test set). Compared
to previous results on fine-grained evaluation exer-
cises (Edmonds and Kilgarriff, 2002; Snyder and
Palmer, 2004), the systems? results are much higher.
On the other hand, the difference in performance
between the MFS baseline and state-of-the-art sys-
tems (around 5%) on coarse-grained disambiguation
is comparable to that of the Senseval-3 all-words ex-
ercise. However, given the novelty of the task we
believe that systems can achieve even better perfor-
32
System A P R F1
NUS-PT 100.0 82.50 82.50 82.50
NUS-ML 100.0 81.58 81.58 81.58
LCC-WSD 100.0 81.45 81.45 81.45
GPLSI 100.0 79.55 79.55 79.55
BLMFS 100.0 78.89 78.89 78.89
UPV-WSD 100.0 78.63 78.63 78.63
TKB-UO 100.0 70.21 70.21 70.21
PU-BCD 90.1 69.72 62.80 66.08
RACAI-SYNWSD 100.0 65.71 65.71 65.71
SUSSX-FR 72.8 71.73 52.23 60.44
USYD 95.3 58.79 56.02 57.37
UOFL 92.7 52.59 48.74 50.60
SUSSX-C-WD 72.8 54.54 39.71 45.96
SUSSX-CR 72.8 54.30 39.53 45.75
UOR-SSI? 100.0 83.21 83.21 83.21
Table 3: System scores sorted by F1 measure (A =
attempted, P = precision, R = recall, F1 = F1 mea-
sure, ?: system from one of the task organizers).
mance by heavily exploiting the coarse nature of the
sense inventory.
In Table 5 we report the results for each of the
five articles. The interesting aspect of the table is
that documents from some domains seem to have
predominant senses different from those in Sem-
Cor. Specifically, the MFS baseline performs more
poorly on documents d004 and d005, from the
COMPUTER SCIENCE and BIOGRAPHY domains
respectively. We believe this is due to the fact that
these documents have specific predominant senses,
which correspond less often to the most frequent
sense in SemCor than for the other three documents.
It is also interesting to observe that different systems
perform differently on the five documents (we high-
light in bold the best performing systems on each
article).
Finally, we calculated the systems? performance
by part of speech. The results are shown in Table
6. Again, we note that different systems show dif-
ferent performance depending on the part-of-speech
tag. Another interesting aspect is that the perfor-
mance of the MFS baseline is very close to state-of-
the-art systems for adjectives and adverbs, whereas
it is more than 3 points below for verbs, and around
5 for nouns.
System F1
NUS-PT 82.50
NUS-ML 81.58
LCC-WSD 81.45
GPLSI 79.55
BLMFS 78.89
UPV-WSD 78.63
SUSSX-FR 77.04
TKB-UO 70.21
PU-BCD 69.72
RACAI-SYNWSD 65.71
SUSSX-C-WD 64.52
SUSSX-CR 64.35
USYD 58.79
UOFL 54.61
UOR-SSI? 83.21
Table 4: System scores sorted by F1 measure with
MFS adopted as a backoff strategy when no sense
assignment is attempted (?: system from one of the
task organizers). Systems affected are marked in
bold.
System N V A R
NUS-PT 82.31 78.51 85.64 89.42
NUS-ML 81.41 78.17 82.60 90.38
LCC-WSD 80.69 78.17 85.36 87.98
GPLSI 80.05 74.45 82.32 86.54
BLMFS 77.44 75.30 84.25 87.50
UPV-WSD 79.33 72.76 84.53 81.25
TKB-UO 70.76 62.61 78.73 74.04
PU-BCD 71.41 59.69 66.57 55.67
RACAI-SYNWSD 64.02 62.10 71.55 75.00
SUSSX-FR 68.09 51.02 57.38 49.38
USYD 56.06 60.43 58.00 54.31
UOFL 57.65 48.82 25.87 60.80
SUSSX-C-WD 52.18 35.64 42.95 46.30
SUSSX-CR 51.87 35.44 42.95 46.30
UOR-SSI? 84.12 78.34 85.36 88.46
Table 6: System scores by part-of-speech tag (N
= nouns, V = verbs, A = adjectives, R = adverbs)
sorted by overall F1 measure (best scores are marked
in bold, ?: system from one of the task organizers).
33
d001 d002 d003 d004 d005
System P R P R P R P R P R
NUS-PT 88.32 88.32 88.13 88.13 83.40 83.40 76.07 76.07 81.45 81.45
NUS-ML 86.14 86.14 88.39 88.39 81.40 81.40 76.66 76.66 79.13 79.13
LCC-WSD 87.50 87.50 87.60 87.60 81.40 81.40 75.48 75.48 80.00 80.00
GPLSI 83.42 83.42 86.54 86.54 80.40 80.40 73.71 73.71 77.97 77.97
BLMFS 85.60 85.60 84.70 84.70 77.80 77.80 75.19 75.19 74.20 74.20
UPV-WSD 84.24 84.24 80.74 80.74 76.00 76.00 77.11 77.11 77.10 77.10
TKB-UO 78.80 78.80 72.56 72.56 69.40 69.40 70.75 70.75 58.55 58.55
PU-BCD 77.16 67.94 75.52 67.55 64.96 58.20 68.86 61.74 64.42 60.87
RACAI-SYNWSD 71.47 71.47 72.82 72.82 66.80 66.80 60.86 60.86 59.71 59.71
SUSSX-FR 79.10 57.61 73.72 53.30 74.86 52.40 67.97 48.89 65.20 51.59
USYD 62.53 61.69 59.78 57.26 60.97 57.80 60.57 56.28 47.15 45.51
UOFL 61.41 59.24 55.93 52.24 48.00 45.60 53.42 47.27 44.38 41.16
SUSSX-C-WD 66.42 48.37 61.31 44.33 55.14 38.60 50.72 36.48 42.13 33.33
SUSSX-CR 66.05 48.10 60.58 43.80 59.14 41.40 48.67 35.01 40.29 31.88
UOR-SSI? 86.14 86.14 85.49 85.49 79.60 79.60 86.85 86.85 75.65 75.65
Table 5: System scores by article (best scores are marked in bold, ?: system from one of the task organizers).
5 Systems Description
In order to allow for a critical and comparative in-
spection of the system results, we asked the partici-
pants to answer some questions about their systems.
These included information about whether:
1. the system used semantically-annotated and
unannotated resources;
2. the system used the MFS as a backoff strategy;
3. the system used the coarse senses provided by
the organizers;
4. the system was trained on some corpus.
We believe that this gives interesting information
to provide a deeper understanding of the results. We
summarize the participants? answers to the question-
naires in Table 7. We report about the use of seman-
tic resources as well as semantically annotated cor-
pora (SC = SemCor, DSO = Defence Science Organ-
isation Corpus, SE = Senseval corpora, OMWE =
Open Mind Word Expert, XWN = eXtended Word-
Net, WN = WordNet glosses and/or relations, WND
= WordNet Domains), as well as information about
the use of unannotated corpora (UC), training (TR),
MFS (based on the SemCor sense frequencies), and
the coarse senses provided by the organizers (CS).
As expected, several systems used lexico-semantic
information from the WordNet semantic network
and/or were trained on the SemCor semantically-
annotated corpus.
Finally, we point out that all the systems perform-
ing better than the MFS baseline adopted it as a
backoff strategy when they were not able to output a
sense assignment.
6 Conclusions and Future Directions
It is commonly agreed that Word Sense Disambigua-
tion needs emerge and show its usefulness in end-
to-end applications: after decades of research in the
field it is still unclear whether WSD can provide
a relevant contribution to real-world applications,
such as Information Retrieval, Question Answering,
etc. In previous Senseval evaluation exercises, state-
of-the-art systems achieved performance far below
70% and even the agreement between human anno-
tators was discouraging. As a result of the discus-
sion at the Senseval-3 workshop in 2004, one of the
aims of SemEval-2007 was to tackle the problems
at the roots of WSD. In this task, we dealt with the
granularity issue which is a major obstacle to both
system and human annotators. In the hope of over-
coming the current performance upper bounds, we
34
System SC DSO SE OMWE XWN WN WND OTHER UC TR MFS CS
GPLSI
? ? ? ? ? ? ? ? ? ? ? ?
LCC-WSD
? ? ? ? ? ? ? ? ? ? ? ?
NUS-ML
? ? ? ? ? ? ? ? ? ? ? ?
NUS-PT
? ? ? ? ? ? ? Parallel corpus ? ? ? ?
PU-BCD
? ? ? ? ? ? ? ? ? ? ? ?
RACAI-SYNWSD ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-C-WD ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-CR ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-FR ? ? ? ? ? ? ? ? ? ? ? ?
TKB-UO ? ? ? ? ? ? ? ? ? ? ? ?
UOFL ? ? ? ? ? ? ? ? ? ? ? ?
UOR-SSI? ? ? ? ? ? ? ? SSI LKB ? ? ? ?
UPV-WSD ? ? ? ? ? ? ? ? ? ? ? ?
USYD
? ? ? ? ? ? ? ? ? ? ? ?
Table 7: Information about participating systems (SC = SemCor, DSO = Defence Science Organisation
Corpus, SE = Senseval corpora, OMWE = Open Mind Word Expert, XWN = eXtended WordNet, WN =
WordNet glosses and/or relations, WND = WordNet Domains, UC = use of unannotated corpora, TR = use
of training, MFS = most frequent sense backoff strategy, CS = use of coarse senses from the organizers, ?:
system from one of the task organizers).
proposed the adoption of a coarse-grained sense in-
ventory. We found the results of participating sys-
tems interesting and stimulating. However, some
questions arise. First, it is unclear whether, given
the novelty of the task, systems really achieved the
state of the art or can still improve their performance
based on a heavier exploitation of coarse- and fine-
grained information from the adopted sense inven-
tory. We observe that, on a technical domain such
as computer science, most supervised systems per-
formed worse due to the nature of their training set.
Second, we still need to show that coarse senses can
be useful in real applications. Third, a full coarse
sense inventory is not yet available: this is a major
obstacle to large-scale in vivo evaluations. We be-
lieve that these aspects deserve further investigation
in the years to come.
Acknowledgments
This work was partially funded by the Interop NoE (508011),
6th European Union FP. We would like to thank Martha Palmer
for providing us the first three texts of the test corpus.
References
Tim Chklovski and Rada Mihalcea. 2002. Building a sense
tagged corpus with open mind word expert. In Proc. of ACL
2002 Workshop on WSD: Recent Successes and Future Di-
rections. Philadelphia, PA.
Philip Edmonds and Adam Kilgarriff. 2002. Introduction to the
special issue on evaluating word sense disambiguation sys-
tems. Journal of Natural Language Engineering, 8(4):279?
291.
Christiane Fellbaum, editor. 1998. WordNet: an Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Comp. Volume, pages 57?
60, New York City, USA.
George A. Miller, Claudia Leacock, Randee Tengi, and Ross T.
Bunker. 1993. A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technology, pages
303?308, Princeton, NJ, USA.
Roberto Navigli and Paola Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach to word
sense disambiguation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 27(7):1063?1074.
Roberto Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In Proc. of
the 44th Annual Meeting of the Association for Computa-
tional Linguistics joint with the 21st International Confer-
ence on Computational Linguistics (COLING-ACL 2006),
pages 105?112. Sydney, Australia.
Benjamin Snyder and Martha Palmer. 2004. The english all-
words task. In Proc. of ACL 2004 SENSEVAL-3 Workshop,
pages 41?43. Barcelona, Spain.
Catherine Soanes and Angus Stevenson, editors. 2003. Oxford
Dictionary of English. Oxford University Press.
35
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48?53,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 10: English Lexical Substitution Task
Diana McCarthy
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
dianam@sussex.ac.uk
Roberto Navigli
University of Rome ?La Sapienza?
Via Salaria, 113
00198 Roma, Italy
navigli@di.uniroma1.it
Abstract
In this paper we describe the English Lexical
Substitution task for SemEval. In the task,
annotators and systems find an alternative
substitute word or phrase for a target word in
context. The task involves both finding the
synonyms and disambiguating the context.
Participating systems are free to use any lex-
ical resource. There is a subtask which re-
quires identifying cases where the word is
functioning as part of a multiword in the sen-
tence and detecting what that multiword is.
1 Introduction
Word sense disambiguation (WSD) has been de-
scribed as a task in need of an application. Whilst
researchers believe that it will ultimately prove use-
ful for applications which need some degree of se-
mantic interpretation, the jury is still out on this
point. One problem is that WSD systems have been
tested on fine-grained inventories, rendering the task
harder than it need be for many applications (Ide
and Wilks, 2006). Another significant problem is
that there is no clear choice of inventory for any
given task (other than the use of a parallel corpus
for a specific language pair for a machine translation
application).
The lexical substitution task follows on from
some previous ideas (McCarthy, 2002) to exam-
ine the capabilities of WSD systems built by re-
searchers on a task which has potential for NLP
applications. Finding alternative words that can
occur in given contexts would potentially be use-
ful to many applications such as question answer-
ing, summarisation, paraphrase acquisition (Dagan
et al, 2006), text simplification and lexical acquisi-
tion (McCarthy, 2002). Crucially this task does not
specify the inventory for use beforehand to avoid
bias to one predefined inventory and makes it eas-
ier for those using automatically acquired resources
to enter the arena. Indeed, since the systems in
SemEval did not know the candidate substitutes for
a word before hand, the lexical resource is evaluated
as much as the context based disambiguation com-
ponent.
2 Task set up
The task involves a lexical sample of nouns, verbs,
adjectives and adverbs. Both annotators and sys-
tems select one or more substitutes for the target
word in the context of a sentence. The data was
selected from the English Internet Corpus of En-
glish produced by Sharoff (2006) from the Inter-
net (http://corpus.leeds.ac.uk/internet.html). This is
a balanced corpus similar in flavour to the BNC,
though with less bias to British English, obtained
by sampling data from the web. Annotators are not
provided with the PoS (noun, verb, adjective or ad-
verb) but the systems are. Annotators can provide
up to three substitutes but all should be equally as
good. They are instructed that they can provide a
phrase if they can?t think of a good single word sub-
stitute. They can also use a slightly more general
word if that is close in meaning. There is a ?NAME?
response if the target is part of a proper name and
?NIL? response if annotators cannot think of a good
substitute. The subjects are also asked to identify
48
if they feel the target word is an integral part of
a phrase, and what that phrase was. This option
was envisaged for evaluation of multiword detec-
tion. Annotators did sometimes use it for paraphras-
ing a phrase with another phrase. However, for an
item to be considered a constituent of a multiword,
a majority of at least 2 annotators had to identify the
same multiword.1
The annotators were 5 native English speakers
from the UK. They each annotated the entire dataset.
All annotations were semi-automatically lemma-
tised (substitutes and identified multiwords) unless
the lemmatised version would change the meaning
of the substitute or if it was not obvious what the
canonical version of the multiword should be.
2.1 Data Selection
The data set comprises 2010 sentences, 201 target
words each with 10 sentences. We released 300 for
the trial data and kept the remaining 1710 for the
test release. 298 of the trial, and 1696 of the test
release remained after filtering items with less than
2 non NIL and non NAME responses and a few with
erroneous PoS tags. The words included were se-
lected either manually (70 words) from examination
of a variety of lexical resources and corpora or au-
tomatically (131) using information in these lexical
resources. Words were selected from those having a
number of different meanings, each with at least one
synonym. Since typically the distribution of mean-
ings of a word is strongly skewed (Kilgarriff, 2004),
for the test set we randomly selected 20 words in
each PoS for which we manually selected the sen-
tences 2 (we refer to these words as MAN) whilst for
the remaining words (RAND) the sentences were se-
lected randomly.
2.2 Inter Annotator Agreement
Since we have sets of substitutes for each item and
annotator, pairwise agreement was calculated be-
tween each pair of sets (p1, p2 ? P ) from each pos-
sible pairing (P ) as
?
p1,p2?P
p1?p2
p1?p2
|P |
1Full instructions given to the annotators are posted at
http://www.informatics.susx.ac.uk/research/nlp/mccarthy/files/
instructions.pdf.
2There were only 19 verbs due to an error in automatic se-
lection of one of the verbs picked for manual selection of sen-
tences.
Pairwise inter-annotator agreement was 27.75%.
73.93% had modes, and pairwise agreement with the
mode was 50.67%. Agreement is increased if we re-
move one annotator who typically gave 2 or 3 sub-
stitutes for each item, which increased coverage but
reduced agreement. Without this annotator, inter-
annotator agreement was 31.13% and 64.7% with
mode.
Multiword detection pairwise agreement was
92.30% and agreement on the identification of the
exact form of the actual multiword was 44.13%.
3 Scoring
We have 3 separate subtasks 1) best 2) oot and 3)
mw which we describe below. 3 In the equations
and results tables that follow we use P for precision,
R for recall, and Mode P and Mode R where we
calculate precision and recall against the substitute
chosen by the majority of annotators, provided that
there is a majority.
Let H be the set of annotators, T be the set of test
items with 2 or more responses (non NIL or NAME)
and hi be the set of responses for an item i ? T for
annotator h ? H .
For each i ? T we calculate the mode (mi) i.e.
the most frequent response provided that there is a
response more frequent than the others. The set of
items where there is such a mode is referred to as
TM . Let A (and AM ) be the set of items from T
(or TM ) where the system provides at least one sub-
stitute. Let ai : i ? A (or ai : i ? AM ) be the set
of guesses from the system for item i. For each i
we calculate the multiset union (Hi) for all hi for all
h ? H and for each unique type (res) in Hi will
have an associated frequency (freqres) for the num-
ber of times it appears in Hi.
For example: Given an item (id 9999) for happy;a
supposing the annotators had supplied answers as
follows:
annotator responses
1 glad merry
2 glad
3 cheerful glad
4 merry
5 jovial
3The scoring measures are as described in the doc-
ument at http://nlp.cs.swarthmore.edu/semeval/tasks/task10/
task10documentation.pdf released with our trial data.
49
then Hi would be glad glad glad merry merry
cheerful jovial. The res with associated frequencies
would be glad 3 merry 2 cheerful 1 and jovial 1.
best measures This requires the best file produced
by the system which gives as many guesses as the
system believes are fitting, but where the credit
for each correct guess is divided by the number of
guesses. The first guess in the list is taken as the
best guess (bg).
P =
?
ai:i?A
?
res?ai
freqres
|ai|
|Hi|
|A| (1)
R =
?
ai:i?T
?
res?ai
freqres
|ai|
|Hi|
|T | (2)
Mode P =
?
bgi?AM 1 if bg = mi
|AM | (3)
Mode R =
?
bgi?TM 1 if bg = mi
|TM | (4)
A system is permitted to provide more than one
response, just as the annotators were. They can
do this if they are not sure which response is bet-
ter, however systems will maximise the score if they
guess the most frequent response from the annota-
tors. For P and R the credit is divided by the num-
ber of guesses that a system makes to prevent a sys-
tem simply hedging its bets by providing many re-
sponses. The credit is also divided by the number of
responses from annotators. This gives higher scores
to items with less variation. We want to emphasise
test items with better agreement.
Using the example for happy;a id 9999 above, if
the system?s responses for this item was glad; cheer-
ful the credit for a9999 in the numerator of P and R
would be
3+1
2
7 = .286
For Mode P and Mode R we use the system?s
first guess and compare this to the mode of the anno-
tators responses on items where there was a response
more frequent than the others.
oot measures This allows a system to make up to
10 guesses. The credit for each correct guess is not
divided by the number of guesses. This allows for
the fact that there is a lot of variation for the task and
we only have 5 annotators. With 10 guesses there is
a better chance that the systems find the responses
of these 5 annotators. There is no ordering of the
guesses and the Mode scores give credit where the
mode was found in one of the system?s 10 guesses.
P =
?
ai:i?A
?
res?ai
freqres
|Hi|
|A| (5)
R =
?
ai:i?T
?
res?ai
freqres
|Hi|
|T | (6)
Mode P =
?
ai:i?AM 1 if any guess ? ai = mi
|AM |
(7)
Mode R =
?
ai:i?TM 1 if any guess ? ai = mi
|TM |
(8)
mw measures For this measure, a system must
identify items where the target is part of a multiword
and what the multiword is. The annotators do not all
have linguistics background, they are simply asked
if the target is an integral part of a phrase, and if so
what the phrase is. Sometimes this option is used
by the subjects for paraphrasing a phrase of the sen-
tence, but typically it is used when there is a mul-
tiword. For scoring, a multiword item is one with
a majority vote for the same multiword with more
than 1 annotator identifying the multiword.
Let MW be the subset of T for which there
is such a multiword response from a majority of
at least 2 annotators. Let mwi ? MW be the
multiword identified by majority vote for item i.
Let MWsys be the subset of T for which there is a
multiword response from the system and mwsysi
be a multiword specified by the system for item i.
detection P =
?
mwsysi?MWsys 1 if mwi exists at i
|MWsys| (9)
detection R =
?
mwsysi?MW 1 if mwi exists at i
|MW | (10)
identification P =
?
mwsysi?MWsys 1 if mwsysi = mwi
|MWsys| (11)
50
identification R =
?
mwsysi?MW 1 if mwsysi = mwi
|MW | (12)
3.1 Baselines
We produced baselines using WordNet 2.1 (Miller et
al., 1993a) and a number of distributional similarity
measures. For the WordNet best baseline we found
the best ranked synonym using the criteria 1 to 4
below in order. For WordNet oot we found up to 10
synonyms using criteria 1 to 4 in order until 10 were
found:
1. Synonyms from the first synset of the target
word, and ranked with frequency data obtained
from the BNC (Leech, 1992).
2. synonyms from the hypernyms (verbs and
nouns) or closely related classes (adjectives) of
that first synset, ranked with the frequency data.
3. Synonyms from all synsets of the target word,
and ranked using the BNC frequency data.
4. synonyms from the hypernyms (verbs and
nouns) or closely related classes (adjectives) of
all synsets of the target, ranked with the BNC
frequency data.
We also produced best and oot baselines using
the distributional similarity measures l1, jaccard, co-
sine, lin (Lin, 1998) and ?SD (Lee, 1999) 4. We took
the word with the largest similarity (or smallest dis-
tance for ?SD and l1) for best and the top 10 for oot.
For mw detection and identification we used
WordNet to detect if a multiword in WordNet which
includes the target word occurs within a window of
2 words before and 2 words after the target word.
4 Systems
9 teams registered and 8 participated, and two of
these teams (SWAG and IRST) each entered two sys-
tems, we distinguish the first and second systems
with a 1 and 2 suffix respectively.
The systems all used 1 or more predefined inven-
tories. Most used web queries (HIT, MELB, UNT)
or web data (Brants and Franz, 2006) (IRST2, KU,
4We used 0.99 as the parameter for ? for this measure.
SWAG1, SWAG2, USYD, UNT) to obtain counts for
disambiguation, with some using algorithms to de-
rive domain (IRST1) or co-occurrence (TOR) infor-
mation from the BNC. Most systems did not use
sense tagged data for disambiguation though MELB
did use SemCor (Miller et al, 1993b) for filtering in-
frequent synonyms and UNT used a semi-supervised
word sense disambiguation combined with a host of
other techniques, including machine translation en-
gines.
5 Results
In tables 1 to 3 we have ordered systems accord-
ing to R on the best task, and in tables 4 to 6 ac-
cording to R on oot. We show all scores as per-
centages i.e. we multiply the scores in section 3
by 100. In tables 3 and 6 we show results using
the subset of items which were i) NOT identified as
multiwords (NMWT) ii) scored only on non multi-
word substitutes from both annotators and systems
(i.e. no spaces) (NMWS). Unfortunately we do not
have space to show the analysis for the MAN and
RAND subsets here. Please refer to the task website
for these results. 5 We retain the same ordering for
the further analysis tables when we look at subsets
of the data. Although there are further differences
in the systems which would warrant reranking on an
individual analysis, since we combined the subanal-
yses in one table we keep the order as for 1 and 4
respectively for ease of comparison.
There is some variation in rank order of the sys-
tems depending on which measures are used. 6 KU
is highest ranking on R for best. UNT is best at find-
ing the mode, particularly on oot, though it is the
most complicated system exploiting a great many
knowledge sources and components. IRST2 does
well at finding the mode in best. The IRST2 best
R score is lower because it supplied many answers
for each item however it achieves the best R score
on the oot task. The baselines are outperformed by
most systems. The WordNet baseline outperforms
those derived from distributional methods. The dis-
tributional methods, especially lin, show promising
results given that these methods are automatic and
5The task website is at http://www.informatics.sussex.ac.uk/
research/nlp/mccarthy/task10index.html.
6There is not a big difference between P and R because
systems typically supplied answers for most items.
51
Systems P R Mode P Mode R
KU 12.90 12.90 20.65 20.65
UNT 12.77 12.77 20.73 20.73
MELB 12.68 12.68 20.41 20.41
HIT 11.35 11.35 18.86 18.86
USYD 11.23 10.88 18.22 17.64
IRST1 8.06 8.06 13.09 13.09
IRST2 6.95 6.94 20.33 20.33
TOR 2.98 2.98 4.72 4.72
Table 1: best results
Systems P R Mode P Mode R
WordNet 9.95 9.95 15.28 15.28
lin 8.84 8.53 14.69 14.23
l1 8.11 7.82 13.35 12.93
lee 6.99 6.74 11.34 10.98
jaccard 6.84 6.60 11.17 10.81
cos 5.07 4.89 7.64 7.40
Table 2: best baseline results
don?t require hand-crafted inventories. As yet we
haven?t combined the baselines with disambiguation
methods.
Only HIT attempted the mw task. It outperforms
all baselines from WordNet.
5.1 Post Hoc Analysis
Choosing a lexical substitute for a given word is
not clear cut and there is inherently variation in the
task. Since it is quite likely that there will be syn-
onyms that the five annotators do not think of we
conducted a post hoc analysis to see if the synonyms
selected by the original annotators were better, on
the whole, than those in the systems responses. We
randomly selected 100 sentences from the subset of
items which had more than 2 single word substitutes,
no NAME responses, and where the target word was
NMWT NMWS
Systems P R P R
KU 13.39 13.39 14.33 13.98
UNT 13.46 13.46 13.79 13.79
MELB 13.35 13.35 14.19 13.82
HIT 11.97 11.97 12.55 12.38
USYD 11.68 11.34 12.48 12.10
IRST1 8.44 8.44 8.98 8.92
IRST2 7.25 7.24 7.67 7.66
TOR 3.22 3.22 3.32 3.32
Table 3: Further analysis for best
Systems P R Mode P Mode R
IRST2 69.03 68.90 58.54 58.54
UNT 49.19 49.19 66.26 66.26
KU 46.15 46.15 61.30 61.30
IRST1 41.23 41.20 55.28 55.28
USYD 36.07 34.96 43.66 42.28
SWAG2 37.80 34.66 50.18 46.02
HIT 33.88 33.88 46.91 46.91
SWAG1 35.53 32.83 47.41 43.82
TOR 11.19 11.19 14.63 14.63
Table 4: oot results
Systems P R Mode P Mode R
WordNet 29.70 29.35 40.57 40.57
lin 27.70 26.72 40.47 39.19
l1 24.09 23.23 36.10 34.96
lee 20.09 19.38 29.81 28.86
jaccard 18.23 17.58 26.87 26.02
cos 14.07 13.58 20.82 20.16
Table 5: oot baseline results
NMWT NMWS
Systems P R P R
IRST2 72.04 71.90 76.19 76.06
UNT 51.13 51.13 54.01 54.01
KU 48.43 48.43 49.72 49.72
IRST1 43.11 43.08 45.13 45.11
USYD 37.26 36.17 40.13 38.89
SWAG2 39.95 36.51 40.97 37.75
HIT 35.60 35.60 36.63 36.63
SWAG1 37.49 34.64 38.36 35.67
TOR 11.77 11.77 12.22 12.22
Table 6: Further analysis for oot
HIT WordNet BL
P R P R
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
Table 7: MW results
52
good reasonable bad
sys 9.07 19.08 71.85
origA 37.36 41.01 21.63
Table 8: post hoc results
not one of those identified as a multiword (i.e. a ma-
jority vote by 2 or more annotators for the same mul-
tiword as described in section 2). We then mixed the
substitutes from the human annotators with those of
the systems. Three fresh annotators7 were given the
test sentence and asked to categorise the randomly
ordered substitutes as good, reasonable or bad. We
take the majority verdict for each substitute, but if
there is one reasonable and one good verdict, then
we categorise the substitute as reasonable. The per-
centage of substitutes for systems (sys) and original
annotators (origA) categorised as good, reasonable
and bad by the post hoc annotators are shown in ta-
ble 8. We see the substitutes from the humans have
a higher proportion of good or reasonable responses
by the post hoc annotators compared to the substi-
tutes from the systems.
6 Conclusions and Future Directions
We think this task is an interesting one in which to
evaluate automatic approaches of capturing lexical
meaning. There is an inherent variation in the task
because several substitutes may be possible for a
given context. This makes the task hard and scoring
is less straightforward than a task which has fixed
choices. On the other hand, we believe the task taps
into human understanding of word meaning and we
hope that computers that perform well on this task
will have potential in NLP applications. Since a
pre-defined inventory is not used, the task allows us
to compare lexical resources as well as disambigua-
tion techniques without a bias to any predefined in-
ventory. It is possible for those interested in disam-
biguation to focus on this, rather than the choice of
substitutes, by using the union of responses from the
annotators in future experiments.
7 Acknowledgements
We acknowledge support from the Royal Society UK for fund-
ing the annotation for the project, and for a Dorothy Hodgkin
7Again, these were native English speakers from the UK.
Fellowship to the first author. We also acknowledge support
to the second author from INTEROP NoE (508011, 6th EU
FP). We thank the annotators for their hard work. We thank
Serge Sharoff for the use of his Internet corpus, Julie Weeds for
the software we used for producing the distributional similarity
baselines and Suzanne Stevenson for suggesting the oot task .
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
corpus version 1.1. Technical Report.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? In Proceedings of Text, Speech,
Dialogue, Brno, Czech Republic.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 25?32.
Geoffrey Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
Diana McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 109?115, Philadelphia, USA.
George Miller, Richard Beckwith, Christine Fellbaum,
David Gross, and Katherine Miller, 1993a. Intro-
duction to WordNet: an On-Line Lexical Database.
ftp://clarity.princeton.edu/pub/WordNet/5papers.ps.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993b. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kaufman.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
53
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 5?7, Dublin, Ireland, August 23-29 2014.
Multilingual Word Sense Disambiguation and Entity Linking
Roberto Navigli and Andrea Moro
Dipartimento di Informatica, Sapienza Universit`a di Roma
Viale Regina Elena 295, 00161 Roma, Italy
{moro, navigli}@di.uniroma1.it
Tutorial Motivation and Description
Nowadays the textual information available online is provided in an increasingly wide range of lan-
guages. This language explosion clearly forces researchers to focus on the challenging problem of being
able to analyze and understand text written in any language. At the core of this problem lies the lexical
ambiguity of language, an issue which is addressed by two key tasks in computational lexical semantics:
multilingual Word Sense Disambiguation (WSD) and Entity Linking (EL).
WSD (Navigli, 2009) is a historical task in Computational Linguistics aimed at explicitly assigning
meanings to word occurrences within text, while EL (Erbs et al., 2011; Rao et al., 2013; Cornolti et al.,
2013) is a recent task focused on finding mentions of entities within a text and linking them to the most
suitable entry in a knowledge base, if one exists. The two main differences between WSD and EL are
in the kind of inventory used, i.e. dictionary vs. encyclopedia, and the assumption that the mention is
complete or potentially incomplete, respectively. Notwithstanding these differences, the tasks are pretty
similar in nature, in that they both involve the disambiguation of textual fragments in a given language
according to a reference inventory. Nevertheless, the research community has tackled the two tasks
separately, often duplicating efforts and solutions. Moreover, the vast majority of the state-of-the-art
approaches only marginally take into account languages different from English.
In this tutorial, we present the two tasks of multilingual WSD and EL, by surveying the challenges
involved and the most effective solutions, both in isolation by illustrating the state of the art in the two
fields, and when the tasks are addressed in a unified, multilingual way.
In particular, this tutorial covers three key aspects of the multilingual WSD and EL tasks:
1. Multilingual inventories of word senses and named entities;
2. State-of-the-art methods to perform disambiguation and linking;
3. Evaluation of the systems: gold standard datasets and performance measures.
The tutorial is aimed at stressing the key challenges of the tasks of WSD and EL when moving from a
monolingual to a multilingual setup. The tutorial includes examples and discussions intended to illustrate
and clarify the major challenges of the tasks and which solutions are most appropriate to which problem.
Organization of the tutorial
The half-day tutorial contains sessions on the following topics:
1. Introduction (30 mins) This first session will provide the necessary background, definitions and
examples for the two considered tasks: multilingual WSD and EL.
2. The multilingual inventory for word senses and named entities (45 mins) In this session we will
overview the definitions of the inventories used in state-of-the-art approaches both for WSD and
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
5
EL. We will then discuss the key aspects of partial matching for EL and, finally, we will describe
multilingual inventories of word senses and named entities, among which Open Multilingual Word-
Net (Bond and Foster, 2013), Wikipedia
1
, DBpedia (Auer et al., 2007), BabelNet (Navigli and
Ponzetto, 2012).
3. State of the art in WSD and EL (75 mins) This session will introduce the key challenges to the tasks
of WSD and EL and the well-known approaches, such as IMS (Zhong and Ng, 2010) and UKB
(Agirre et al., 2013) for WSD, and Babelfy (Moro et al., 2014), AIDA (Hoffart et al., 2011; Hoffart
et al., 2012), Tagme (Ferragina and Scaiella, 2010; Ferragina and Scaiella, 2012), Illinois Wikifier
(Cheng and Roth, 2013) and DBpedia Spotlight (Mendes et al., 2011; Daiber et al., 2013), that can
partially address them. Challenges include: the lack of training data for non-English languages,
the granularity of the sense inventory, the high level of ambiguity in EL, the most frequent sense
baseline challenge, etc.
4. Evaluation measures and gold standard datasets (30 mins) We will conclude the tutorial by describ-
ing the standard performance measures for these tasks and well-known datasets for multilingual
WSD and EL together with a discussion of the results.
Speakers
Roberto Navigli is an associate professor in the Department of Computer Science at the Sapienza Uni-
versity of Rome. He is the recipient of an ERC Starting Grant in computer science and informatics on
multilingual word sense disambiguation (2011-2016) and a co-PI of a Google Focused Research Award
on Natural Language Understanding. His research interests lie in the field of Word Sense Disambiguation
and Induction, multilingual knowledge acquisition and applications of lexical semantics.
Andrea Moro is a PhD student in the Department of Computer Science at the Sapienza University of
Rome. His research interests focus on Natural Language Understanding with an emphasis on Unsuper-
vised Relation Extraction, Word Sense Disambiguation and Entity Linking.
Acknowledgments
The authors gratefully acknowledge the support of the ERC Starting Grant
MultiJEDI No. 259234.
References
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2013. Random Walks for Knowledge-Based Word Sense
Disambiguation. Computational Linguistics.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DB-
pedia: A Nucleus for a Web of Open Data. In Proc. of ISWC/ASWC, pages 722?735.
Francis Bond and Ryan Foster. 2013. Linking and extending an open multilingual wordnet. In Proc. of ACL,
pages 1352?1362.
Xiao Cheng and Dan Roth. 2013. Relational Inference for Wikification. In Proc. of EMNLP.
Marco Cornolti, Paolo Ferragina, and Massimiliano Ciaramita. 2013. A framework for benchmarking entity-
annotation systems. In Proc. of WWW, pages 249?260.
Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N. Mendes. 2013. Improving Efficiency and Accuracy in
Multilingual Entity Extraction. In Proc. of I-Semantics.
Nicolai Erbs, Torsten Zesch, and Iryna Gurevych. 2011. Link Discovery: A Comprehensive Analysis. In Proc. of
ICSC, pages 83?86.
1
http://wikipedia.org
6
Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by wikipedia
entities). In Proc. of CIKM, pages 1625?1628.
Paolo Ferragina and Ugo Scaiella. 2012. Fast and accurate annotation of short texts with wikipedia pages. Soft-
ware, IEEE, 29(1):70?75, Jan.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust Disambiguation of Named Entities in Text. In Proc.
of EMNLP, pages 782?792.
Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard Weikum. 2012. KORE:
keyphrase overlap relatedness for entity disambiguation. In Proc. of CIKM, pages 545?554.
Pablo N. Mendes, Max Jakob, Andr?es Garc??a-Silva, and Christian Bizer. 2011. DBpedia spotlight: shedding light
on the web of documents. In Proc. of I-Semantics, pages 1?8.
Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking meets Word Sense Disambigua-
tion: A Unified Approach. Transactions of the Association for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217?250.
Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Comput. Surv., 41(2):1?69.
Delip Rao, Paul McNamee, and Mark Dredze. 2013. Entity linking: Finding extracted entities in a knowledge
base. In Multi-source, Multilingual Information Extraction and Summarization, pages 93?115. Springer.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for
Free Text. In Proc. of the ACL 2010 System Demonstrations, pages 78?83, Uppsala, Sweden, July. Association
for Computational Linguistics.
7
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 116?126,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Inducing Word Senses to Improve Web Search Result Clustering
Roberto Navigli and Giuseppe Crisafulli
Dipartimento di Informatica
Sapienza Universita` di Roma
navigli@di.uniroma1.it, crisafulli.giu@gmail.com
Abstract
In this paper, we present a novel approach to
Web search result clustering based on the au-
tomatic discovery of word senses from raw
text, a task referred to as Word Sense Induc-
tion (WSI). We first acquire the senses (i.e.,
meanings) of a query by means of a graph-
based clustering algorithm that exploits cycles
(triangles and squares) in the co-occurrence
graph of the query. Then we cluster the search
results based on their semantic similarity to
the induced word senses. Our experiments,
conducted on datasets of ambiguous queries,
show that our approach improves search result
clustering in terms of both clustering quality
and degree of diversification.
1 Introduction
Over recent years increasingly huge amounts of
text have been made available on the Web. Popular
search engines such as Yahoo! and Google usually
do a good job at retrieving a small number of rel-
evant results from such an enormous collection of
Web pages (i.e. retrieving with high precision, low
recall). However, current search engines are still fac-
ing the lexical ambiguity issue (Furnas et al, 1987)
? i.e. the linguistic property owing to which any
particular word may convey different meanings. In
a recent study (Sanderson, 2008) ? conducted us-
ing WordNet (Miller et al, 1990) and Wikipedia as
sources of ambiguous words ? it was reported that
around 3% of Web queries and 23% of the most
frequent queries are ambiguous. Examples include:
?buy B-52? (a cocktail? a bomber? a DJ worksta-
tion? tickets for a band?), ?Alexander Smith quotes?
(the novelist? the poet?), ?beagle search? (dogs? the
Linux search tool? the landing spacecraft?).
Ambiguity is often the consequence of the low
number of query words entered on average by Web
users (Kamvar and Baluja, 2006). While average
query length is increasing ? it is now estimated at
around 3 words per query1 ? many search engines
such as Google have already started to tackle the
query ambiguity issue by reranking and diversify-
ing their results, so as to prevent Web pages that are
similar to each other from ranking too high on the
list.
In the past few years, Web clustering engines
(Carpineto et al, 2009) have been proposed as a
solution to the lexical ambiguity issue in Web In-
formation Retrieval. These systems group search re-
sults, by providing a cluster for each specific aspect
(i.e., meaning) of the input query. Users can then se-
lect the cluster(s) and the pages therein that best an-
swer their information needs. However, many Web
clustering engines group search results on the ba-
sis of their lexical similarity. For instance, consider
the following snippets returned for the beagle search
query:
1. Beagle is a search tool that ransacks your...
2. ...the beagle disappearing in search of game...
3. Beagle indexes your files and searches...
While snippets 1 and 3 both concern the Linux
search tool, they do not have any content word in
1http://www.hitwise.com/us/press-center/
press-releases/google-searches-apr-09
116
common except our query words. As a result, they
will most likely be assigned to two different clusters.
In this paper we present a novel approach to Web
search result clustering which is based on the auto-
matic discovery of word senses from raw text ? a
task referred to as Word Sense Induction (WSI). At
the core of our approach is a graph-based algorithm
that exploits cycles in the co-occurrence graph of
the input query to detect the query?s meanings. Our
experiments on two datasets of ambiguous queries
show that our WSI approach boosts search result
clustering in terms of both clustering quality and de-
gree of diversification.
2 Related Work
Web directories. A first, historical solution to
query ambiguity is that of Web directories, that
is taxonomies providing categories to which Web
pages are manually assigned (e.g., the Open Direc-
tory Project ? http://dmoz.org). Given a query,
search results are organized by category. This ap-
proach has three main weaknesses: first, it is static,
thus it needs manual updates to cover new pages;
second, it covers only a small portion of the Web;
third, it classifies Web pages based on coarse cate-
gories. This latter feature of Web directories makes
it difficult to distinguish between instances of the
same kind (e.g., pages about artists with the same
surname classified as Arts:Music:Bands and
Artists). While methods for the automatic clas-
sification of Web documents have been proposed
(e.g., (Liu et al, 2005b; Xue et al, 2008)) and some
problems have been effectively tackled (Bennett and
Nguyen, 2009), these approaches are usually super-
vised and still suffer from relying on a predefined
taxonomy of categories.
Semantic Information Retrieval (SIR). A dif-
ferent direction consists of associating explicit se-
mantics (i.e., word senses or concepts) with queries
and documents, that is, performing Word Sense Dis-
ambiguation (WSD, see Navigli (2009)). SIR is per-
formed by indexing and/or searching concepts rather
than terms, thus potentially coping with two linguis-
tic phenomena: expressing a single meaning with
different words (synonymy) and using the same word
to express various different meanings (polysemy).
Over the years, different methods for SIR have been
proposed (Krovetz and Croft, 1992; Voorhees, 1993;
Mandala et al, 1998; Gonzalo et al, 1999; Kim et
al., 2004; Liu et al, 2005a, inter alia). However, con-
trasting results have been reported on the benefits of
these techniques: it has been shown that WSD has
to be very accurate to benefit Information Retrieval
(Sanderson, 1994) ? a result that was later debated
(Gonzalo et al, 1999; Stokoe et al, 2003). Also,
it has been reported that WSD has to be very pre-
cise on minority senses and uncommon terms, rather
than on frequent words (Krovetz and Croft, 1992;
Sanderson, 2000).
SIR relies on the existence of a reference dictio-
nary to perform WSD (typically, WordNet) and thus
suffers from its static nature and its inherent paucity
of most proper nouns. This latter problem is partic-
ularly important for Web searches, as users tend to
retrieve more information about named entities (e.g.,
singers, artists, cities) than concepts (e.g., abstract
information about singers or artists).
Search Result Clustering. A more popular ap-
proach to query ambiguity is that of search result
clustering. Typically, given a query, the system starts
from a flat list of text snippets returned from one or
more commonly-available search engines and clus-
ters them on the basis of some notion of textual simi-
larity. At the root of the clustering approach lies van
Rijsbergen?s (1979) cluster hypothesis: ?closely as-
sociated documents tend to be relevant to the same
requests?, whereas documents concerning different
meanings of the input query are expected to belong
to different clusters.
Approaches to search result clustering can be
classified as data-centric or description-centric
(Carpineto et al, 2009). The former focus more on
the problem of data clustering than on presenting the
results to the user. A pioneering example is Scat-
ter/Gather (Cutting et al, 1992), which divides the
dataset into a small number of clusters and, after the
selection of a group, performs clustering again and
proceeds iteratively. Developments of this approach
have been proposed which improve on cluster qual-
ity and retrieval performance (Ke et al, 2009). Other
data-centric approaches use agglomerative hierar-
chical clustering (e.g., LASSI (Yoelle Maarek and
Pelleg, 2000)), rough sets (Ngo and Nguyen, 2005)
or exploit link information (Zhang et al, 2008).
Description-centric approaches are, instead, more
117
focused on the description to produce for each
cluster of search results. Among the most popular
and successful approaches are those based on suf-
fix trees (Zamir et al, 1997; Zamir and Etzioni,
1998), including later developments (Crabtree et al,
2005; Bernardini et al, 2009). Other methods in
the literature are based on formal concept analy-
sis (Carpineto and Romano, 2004), singular value
decomposition (Osinski and Weiss, 2005), spectral
clustering (Cheng et al, 2005), spectral geometry
(Liu et al, 2008), link analysis (Gelgi et al, 2007),
and graph connectivity measures (Di Giacomo et al,
2007). Search result clustering has also been viewed
as a supervised salient phrase ranking task (Zeng et
al., 2004).
Diversification. Another recent research topic
dealing with the query ambiguity issue is diversifi-
cation, which aims to rerank top search results based
on criteria that maximize their diversity. One of the
first examples of diversification algorithms is based
on the use of similarity functions to measure the
diversity among documents and between document
and query (Carbonell and Goldstein, 1998). Other
techniques use conditional probabilities to deter-
mine which document is most different from higher-
ranking ones (Chen and Karger, 2006) or use affinity
ranking (Zhang et al, 2005), based on topic variance
and coverage. More recently, an algorithm called Es-
sential Pages (Swaminathan et al, 2009) has been
proposed to reduce information redundancy and re-
turn Web pages that maximize coverage with respect
to the input query.
Word Sense Induction (WSI). In contrast to the
above approaches, we perform WSI to dynamically
acquire an inventory of senses of the input query.
Instead of performing clustering on the basis of the
surface similarity of Web snippets, we use our in-
duced word senses to group snippets. Very little
work on this topic exists: vector-based WSI was suc-
cessfully shown to improve bag-of-words ad-hoc In-
formation Retrieval (Schu?tze and Pedersen, 1995)
and preliminary studies (Udani et al, 2005; Chen
et al, 2008) have provided interesting insights into
the use of WSI for Web search result clustering.
A more recent attempt at automatically identify-
ing query meanings is based on the use of hidden
topics (Nguyen et al, 2009). However, in this ap-
proach topics ? estimated from a universal dataset ?
are query-independent and thus their number needs
to be established beforehand. In contrast, we aim
to cluster snippets based on a dynamic and finer-
grained notion of sense.
3 Approach
Web search result clustering is usually performed in
three main steps:
1. Given a query q, a search engine (e.g., Yahoo!) is
used to retrieve a list of results R = (r1, . . . , rn);
2. A clustering C = (C0, C1, . . . , Cm) of the results
in R is obtained by means of a clustering algo-
rithm;
3. The clusters in C are optionally labeled with an
appropriate algorithm (e.g., see Zamir and Etzioni
(1998) and Carmel et al (2009)) for visualization
purposes.
Our key idea is to improve step 2 by means of a
Word Sense Induction algorithm: given a query q,
we first dynamically induce, from a text corpus, the
set of word senses of q (Section 3.1); next, we clus-
ter the Web results on the basis of the word senses
previously induced (Section 3.2).
3.1 Word Sense Induction
Word Sense Induction algorithms are unsupervised
techniques aimed at automatically identifying the
set of senses denoted by a word. These methods in-
duce word senses from text by clustering word oc-
currences based on the idea that a given word ?
used in a specific sense ? tends to co-occur with the
same neighbouring words (Harris, 1954). Several
approaches to WSI have been proposed in the litera-
ture (see Navigli (2009) for a survey), ranging from
clustering based on context vectors (e.g., Schu?tze
(1998)) to word clustering (e.g., Lin (1998)) and
co-occurrence graphs (e.g., Widdows and Dorow
(2002)).
Successful approaches such as HyperLex
(Ve?ronis, 2004) ? a graph algorithm based on the
identification of hubs in co-occurrence graphs ?
have to cope with a high number of parameters to
be tuned (Agirre et al, 2006). To deal with this
issue we propose two variants of a simple, yet
effective, graph-based algorithm for WSI, that we
118
describe hereafter. The algorithm consists of two
steps: graph construction and identification of word
senses.
3.1.1 Graph construction
Given a target query q, we build a co-occurrence
graph Gq = (V,E) such that V is a set of context
words related to q and E is the set of undirected
edges, each denoting a co-occurrence between pairs
of words in V . To determine the set of co-occurring
words V , we use the Google Web1T corpus (Brants
and Franz, 2006), a large collection of n-grams (n =
1, . . . , 5) ? i.e., windows of n consecutive tokens ?
occurring in one terabyte of Web documents. First,
for each content word w we collect the total num-
ber c(w) of its occurrences and the number of times
c(w,w?) that w and w? occur together in any 5-gram
(we include inflected forms in the count); second,
we use the Dice coefficient to determine the strength
of co-occurrence between w and w?:
Dice(w,w?) =
2c(w,w?)
c(w) + c(w?)
. (1)
The rationale behind Dice is that dividing by the
sum of total counts of the two words drastically de-
creases the ranking of words that tend to co-occur
frequently with many other words (e.g., new, old,
nice, etc.).
The graph Gq = (V,E) is built as follows:
? Our initial vertex set V (0) contains all the con-
tent words from the snippet results of query q
(excluding stopwords); then, we add to V (0) the
highest-ranking words co-occurring with q in the
Web1T corpus, i.e., those words w for which
Dice(q, w) ? ? (the threshold ? is established ex-
perimentally, see Section 4.1). We set V := V (0)
and E := ?.
? For each word w ? V (0), we select the high-
est ranking words co-occurring with w in Web1T,
that is those words w? for which Dice(w,w?) ?
?. We add each of these words to V (note that
some w? might already be in V (0)) and the
corresponding edge {w,w?} to E with weight
Dice(w,w?). Finally, we remove disconnected
vertices.
3.1.2 Identification of word senses
The main idea behind our approach is that edges
in the co-occurrence graph participating in cycles
are likely to connect vertices (i.e., words) belonging
to the same meaning component. Specifically, we fo-
cus on cycles of length 3 and 4, called respectively
triangles and squares in graph theory.
For each edge e, we calculate the ratio of triangles
in which e participates:
Tri(e) =
# triangles e participates in
# triangles e could participate in
(2)
where the numerator is the number of cycles of
length 3 in which e = {w,w?} participates, and the
denominator is the total number of neighbours of w
and w?. Similarly, we define a measure Sqr(e) of
the ratio of squares (i.e., cycles of length 4) an edge
e participates in to the number of possible squares e
could potentially participate in:
Sqr(e) =
# squares e participates in
# squares e could participate in
(3)
where the numerator is the number of squares con-
taining e and the denominator is the number of pos-
sible distinct pairs of neighbours of w and w?. If no
triangle (or square) exists for e, the value of the cor-
responding function is set to 0.
In order to disconnect the graph and determine
the meaning components, we remove all the edges
whose Tri (or Sqr) value is below a threshold ?. The
resulting connected components represent the word
senses induced for the query q. Notice that the num-
ber of senses is dynamically chosen based on the co-
occurrence graph and the algorithm?s thresholds.
Our triangular measure is the edge counterpart
of the clustering coefficient (or curvature) for ver-
tices, previously used to perform WSI (Widdows
and Dorow, 2002). However, it is our hunch that
measuring the ratio of squares an edge participates
in provides a stronger clue of how important that
edge is within a meaning component. In Section 4,
we will corroborate this idea with our experiments.
3.1.3 An example
As an example, let q = beagle. Two steps are per-
formed:
119
1. Graph construction. We build the co-occurrence
graph Gbeagle = (V,E), an excerpt of which is
shown in Figure 1(a).
2. Identification of word senses. We calculate the
Sqr values of each edge in the graph. The edges
e whose Sqr(e) < ? are removed (we assume
? = 0.25). For instance, Sqr({ dog, breed }) = 12 ,
as the edge participates in the square dog ? breed
? puppy ? canine ? dog, but it could also have
participated in the potential square dog ? breed
? puppy ? search ? dog. In fact, the other neigh-
bours of dog are canine, puppy and search, and
the other neighbour of breed is puppy, thus the
square can only be closed by connecting puppy
to either canine or search. In our example, the
only edges whose Sqr is below ? are: { dog,
puppy }, { dog, search } and { linux, mission }
(they participate in no square). We remove these
edges and select the resulting connected compo-
nents as the senses of the query beagle (shown in
Figure 1(b)). Note that, if we selected triangles
as our pruning measure, we should also remove
the following edges { search, index }, { index,
linux }, { linux, system } and { system, search }.
In fact, these edges do not participate in any tri-
angle (while they do participate in a square). As a
result, we would miss the computer science sense
of the query.
3.2 Clustering of Web results
Given our query q, we submit it to a search engine,
which returns a list of relevant search results R =
(r1, . . . , rn). We process each result ri by consid-
ering the corresponding text snippet and transform-
ing it to a bag of words bi (we apply tokenization,
stopwords and target word removal, and lemmatiza-
tion2). For instance, given the snippet:
?the beagle is a breed of medium-sized dog?,
we produce the following bag of words:
{ breed, medium, size, dog }.
As a result of the above processing, we obtain a
list of bags of words B = (b1, . . . , bn). Now, our
aim is to cluster our Web results R, i.e., the corre-
sponding bags of words B. To this end, rather than
2We use the WordNet lemmatizer.
dog
puppy
canine
index
linux
mission
system
search
breed
mars
lander
spacecraft
(a)
dog
puppy
canine
index
linux
mission
system
search
breed
mars
lander
spacecraft
(b)
Figure 1: The beagle example: (a) graph construction,
?weak? edges (according to Sqr) drawn in bold, (b) the
word senses induced after edge removal.
considering the interrelationships between them (as
is done in traditional search result clustering), we
intersect each bag of words bi ? B with the sense
clusters {S1, . . . , Sm} acquired as a result of our
Word Sense Induction algorithm (cf. Section 3.1).
The sense cluster with the largest intersection with
bi is selected as the most likely meaning of ri. For-
mally:
Sense(ri) =
8
><
>:
argmax
j=1,...,m
|bi ? Sj | ifmax
j
|bi ? Sj | > 0
0 else
(4)
where 0 denotes that no sense is assigned to result ri,
as the intersection is empty for all senses Sj . Oth-
erwise the function returns the index of the sense
having the largest overlap with bi ? the bag of words
associated with the search result ri. As a result of
sense assignment for each ri ? R, we obtain a clus-
tering C = (C0, C1, . . . , Cm) such that:
Cj = {ri ? R : Sense(ri) = j}, (5)
that is, Cj contains the search results classified with
the j-th sense of query q (C0 includes unassigned
results). Finally, we sort the clusters in our clus-
tering C based on their ?quality?. For each cluster
Cj ? C \ {C0}, we determine its similarity with
120
the corresponding meaning Sj by calculating the fol-
lowing formula:
avgsim(Cj , Sj) =
?
ri?Cj
sim(ri, Sj)
|Cj |
. (6)
The formula determines the average similarity be-
tween the search results in cluster Cj and the corre-
sponding sense cluster Sj . The similarity between a
search result ri and Sj is determined as the normal-
ized overlap between its bag of words bi and Sj :
sim(ri, Sj) = sim(bi, Sj) =
|bi ? Sj |
|bi|
. (7)
Finally, we rank the elements ri within each clus-
ter Cj by their similarity sim(ri, Sj). We note that
the ranking and optimality of clusters can be im-
proved with more sophisticated techniques (Crab-
tree et al, 2005; Kurland, 2008; Kurland and
Domshlak, 2008; Lee et al, 2008, inter alia). How-
ever, this is outside the scope of this paper.
4 Experiments
4.1 Experimental Setup
Test Sets. We conducted our experiments on two
datasets:
? AMBIENT (AMBIguous ENTries), a recently
released dataset which contains 44 ambiguous
queries3. The sense inventory for the mean-
ings (i.e., subtopics)4 of queries is given by
Wikipedia disambiguation pages. For instance,
given the beagle query, its disambiguation page
in Wikipedia provides the meanings of dog, Mars
lander, computer search service, beer brand, etc.
The top 100 Web results of each query returned
by the Yahoo! search engine were tagged with
the most appropriate query senses according to
Wikipedia (amounting to 4400 sense-annotated
search results). To our knowledge, this is cur-
rently the largest dataset of ambiguous queries
available on-line. Other datasets, such as those
from the TREC competitions, are not focused on
distiguishing the subtopics of a query.
3http://credo.fub.it/ambient
4In the following, we use the terms subtopic and word sense
interchangeably.
dataset queries
queries by length avg.
1 2 3 4 polys.
AMBIENT 44 35 6 3 0 17.9
MORESQUE 114 0 47 36 31 6.7
Table 1: Statistics on the datasets of ambiguous queries.
? MORESQUE (MORE Sense-tagged QUEry re-
sults), a new dataset of 114 ambiguous queries
which we developed as a complement to AMBI-
ENT following the guidelines provided by its au-
thors. In fact, our aim was to study the behaviour
of Web search algorithms on queries of differ-
ent lengths, ranging from 1 to 4 words. How-
ever, the AMBIENT dataset is composed mostly
of single-word queries. MORESQUE provides
dozens of queries of length 2, 3 and 4, together
with the 100 top results from Yahoo! for each
query annotated as in the AMBIENT dataset
(overall, we tagged 11,400 snippets). We decided
to carry on using Yahoo! mainly for homogeneity
reasons.
We report the statistics on the composition of the
two datasets in Table 1. Given that the snippets could
possibly be annotated with more than one Wikipedia
subtopic, we also determined the average number
of subtopics per snippet. This amounted to 1.01 for
AMBIENT and 1.04 for MORESQUE for snippets
with at least one subtopic annotation (51% and 53%
of the respective datasets). We can thus conclude
that multiple subtopic annotations are infrequent.
Parameters. Our graph-based algorithms have
two parameters: the Dice threshold ? for graph
construction (Section 3.1.1) and the threshold ?
for edge removal (Section 3.1.2). The best pa-
rameters, used throughout our experiments, were
(? = 0.00033, ? = 0.45) with triangles and (? =
0.00033, ? = 0.33) with squares. The parameter
values were obtained as a result of tuning on a small
in-house development dataset. The dataset was built
by automatically identifying monosemous words
and creating pseudowords following the scheme
proposed by Schu?tze (1998).
Systems. We compared Triangles and Squares
against the best systems reported by Bernardini et
al. (2009, cf. Section 2):
121
? Lingo (Osinski and Weiss, 2005): a Web clus-
tering engine implemented in the Carrot2 open-
source framework5 that clusters the most frequent
phrases extracted using suffix arrays.
? Suffix Tree Clustering (STC) (Zamir and Et-
zioni, 1998): the original Web search clustering
approach based on suffix trees.
? KeySRC (Bernardini et al, 2009): a state-of-the-
art Web clustering engine built on top of STC with
part-of-speech pruning and dynamic selection of
the cut-off level of the clustering dendrogram.
? Essential Pages (EP) (Swaminathan et al, 2009):
a recent diversification algorithm that selects fun-
damental pages which maximize the amount of
information covered for a given query.
? Yahoo!: the original search results returned by
the Yahoo! search engine.
The first three of the above are Web search result
clustering approaches, whereas the last two produce
lists of possibly diversified results (cf. Section 2).
4.2 Experiment 1: Clustering Quality
Measure. While assessing the quality of cluster-
ing is a notably hard problem, given a gold standard
G we can calculate the Rand index (RI) of a cluster-
ing C, a common quality measure in the literature,
determined as follows (Rand, 1971; Manning et al,
2008):
RI(C) =
?
(w,w?)?W?W,w 6=w? ?(w,w
?)
|{(w,w?) ? W ?W : w 6= w?}|
(8)
where W is the union set of all the words in C and
?(w,w?) = 1 if any two words w and w? are in the
same cluster both in C and in the gold standard G or
they are in two different clusters in both C and G,
otherwise ?(w,w?) = 0. In other words, we calcu-
late the percentage of word pairs that are in the same
configuration in both C and G. For the gold standard
G we use the clustering induced by the sense annota-
tions provided in our datasets for each snippet (i.e.,
each cluster contains the snippets manually associ-
ated with a particular Wikipedia subtopic). Similarly
to what was done in Section 3.2, untagged results are
grouped together in a special cluster of G.
5http://project.carrot2.org
System AMBIENT MORESQUE All
Squares 72.59 65.41 67.28
Triangles 66.13 64.47 64.93
Lingo 62.75 52.68 55.49
STC 61.48 51.52 54.29
KeySRC 66.49 55.82 58.78
Table 2: Results by Rand index (percentages).
Results. The results of all systems on the AM-
BIENT and MORESQUE datasets according to
the average Rand index are shown in Table 26.
In accordance with previous results in the litera-
ture, KeySRC performed generally better than the
other search result clustering systems, especially
on smaller queries. Our Word Sense Induction sys-
tems, Squares and Triangles, outperformed all other
systems by a large margin, thus showing a higher
clustering quality (with the exception of KeySRC
performing better than Triangles on AMBIENT).
Interestingly, all clustering systems perform more
poorly on longer queries (i.e., on the MORESQUE
dataset), however our WSI systems, and especially
Triangles, are more robust across query lengths.
Compared to Triangles, the Squares algorithm per-
forms better, confirming our hunch that Squares is a
more solid graph pattern.
4.3 Experiment 2: Diversification
Measure. Search result clustering can also be used
to diversify the top results returned by a search en-
gine. Thus, for each query q, one natural way of
measuring a system?s performance is to calculate the
subtopic recall-at-K (Zhai et al, 2003) given by the
number of different subtopics retrieved for q in the
top K results returned:
S-recall@K =
|
?K
i=1 subtopics(ri)|
M
(9)
where subtopics(ri) is the set of subtopics manually
assigned to the search result ri and M is the number
of subtopics for query q (note that in our experiments
M is the number of subtopics occurring in the 100
results retrieved for q, so S-recall@100 = 1). How-
ever, this measure is only suitable for systems re-
turning ranked lists (such as Yahoo! and EP). Given
6For reference systems we used the implementations of
Bernardini et al (2009) and Osinski and Weiss (2005).
122
System K=3 K=5 K=10 K=15 K=20
Squares 51.9 63.4 75.8 83.3 87.4
Triangles 50.8 62.4 75.2 82.7 86.6
Yahoo! 49.2 60.0 72.9 78.5 82.7
EP 40.6 53.2 68.6 77.2 83.3
KeySRC 44.3 55.8 72.0 79.1 83.2
Table 3: S-recall@K on all queries (percentages).
a clustering C = (C0, C1, . . . , Cm), we flatten it to a
list as follows: we add to the initially empty list the
first element of each cluster Cj (j = 1, . . . ,m); then
we iterate the process by selecting the second ele-
ment of each cluster Cj such that |Cj | ? 2, and so
on. The remaining elements returned by the search
engine, but not included in any cluster of C \ {C0},
are appended to the bottom of the list in their orig-
inal order. Note that the elements are selected from
each cluster according to their internal ranking (e.g.,
for our algorithms we use Formula 7 introduced in
Section 3.2).
Results. For the sake of clarity and to save space,
we selected the best systems from our previous ex-
periment, namely Squares, Triangles and KeySRC,
and compared their output with the original snippet
list returned by Yahoo! and the output of the EP di-
versification algorithm (cf. Section 4.1).
The S-recall@K (with K = 3, 5, 10, 15, 20) cal-
culated on AMBIENT+MORESQUE is reported
in Table 3. Squares and Triangles show the high-
est degree of diversification, with a subtopic recall
greater than all other systems, and with Squares con-
sistently performing better than Triangles. It is inter-
esting to observe that KeySRC performs worse than
Yahoo! with low values of K and generally better
with higher values of K.
Given that the two datasets complement each
other in terms of query lengths (with AMBIENT
having queries of length ? 2 and MORESQUE
with many queries of length ? 3), we studied the S-
recall@K trend for the two datasets. The results are
shown in Figures 2 and 3. While KeySRC does not
show large differences in the presence of short and
long ambiguous queries, our graph-based algorithms
do. For instance, as soon as K = 3 the Squares al-
gorithm obtains S-recall values of 37% and 57.5%
on AMBIENT and MORESQUE, respectively. The
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 10  20  30  40  50  60  70  80  90  100
S-r
eca
ll-a
t-K
number of results K
SquaresTrianglesYahoo!EPKeysrc
Figure 2: Results by S-recall@K on AMBIENT.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 10  20  30  40  50  60  70  80  90  100
S-r
eca
ll-a
t-K
number of results K
SquaresTrianglesYahoo!EPKeysrc
Figure 3: S-recall@K on MORESQUE.
difference decreases as K increases, but is still sig-
nificant when K = 10. We hypothesize that, because
they are less ambiguous, longer queries are easier
to diversify with the aid of WSI. However, we note
that, even with low values of K, Squares and Tri-
angles obtain higher S-recall than the other systems
(with KeySRC competing on AMBIENT whenK ?
15). Finally, we observe that ? with low values of K
? the Squares algorithm performs significantly better
than Triangles on shorter queries, and only slightly
better on longer ones.
5 Discussion
Results. Our results show that our graph-based al-
gorithms are able to consistently produce clusters of
better quality than all other systems tested in our
experiments. The results on S-recall@K show that
our approach can also be used effectively as a diver-
sification technique, performing better than a very
123
recent proposal such as Essential Pages. The lat-
ter outperforms Yahoo! and KeySRC when K ?
30 on AMBIENT, whereas on MORESQUE it per-
forms generally worse until higher values of K are
reached. If we analyze the entire dataset of 158
queries by length, EP works best after examining at
least 20 results on 1- and 2-word ambiguous queries,
whereas on longer queries a larger number of docu-
ments (? 30) needs to be analyzed before surpassing
Yahoo! performance.
The above considerations might not seem intu-
itive at first glance, as the average polysemy of
longer queries is lower (17.9 on AMBIENT vs. 6.7
on MORESQUE according to our gold standard).
However, we note that while the kind of ambigu-
ity of 1-word queries is generally coarser (e.g., bea-
gle as dog vs. lander vs. search tool), with longer
queries we often encounter much finer sense distinc-
tions (e.g., Across the Universe as song by The Bea-
tles vs. a 2007 film based on the song vs. a Star Trek
novel vs. a rock album by Trip Shakespeare, etc.).
Word Sense Induction is able to deal better with this
latter kind of ambiguity as discriminative words be-
come part of the meanings acquired.
Performance issues. Inducing word senses from
the query graph comes at a higher computational
cost than other non-semantic clustering techniques.
Indeed, the most time-consuming phase of our ap-
proach is the construction of the query graph, which
requires intensive querying of our database of co-
occurrences calculated from the Web1T corpus.
While graphs can be precomputed or cached, previ-
ously unseen queries will still require the construc-
tion of new graphs. Instead, triangles and squares, as
well as the resulting connected components, can be
calculated on the fly.
6 Conclusions
In this paper we have presented a novel approach
to Web search result clustering. Our key idea is to
induce senses for the target query automatically by
means of a graph-based algorithm focused on the
notion of cycles. The results of a Web search engine
are then mapped to the query senses and clustered
accordingly.
The paper provides three novel contributions.
First, we show that WSI boosts the quality of search
result clustering and improves the diversification of
the snippets returned as a flat list. We provide a clear
indication on the usefulness of a loose notion of
sense to cope with ambiguous queries. This is in
contrast to research on Semantic Information Re-
trieval, which has obtained contradictory and often
inconclusive results. The main advantage of WSI
lies in its dynamic production of word senses that
cover both concepts (e.g., beagle as a breed of dog)
and instances (e.g., beagle as a specific instance of
a space lander). In contrast, static dictionaries such
as WordNet ? typically used in Word Sense Dis-
ambiguation ? by their very nature encode mainly
concepts. Second, we propose two simple, yet ef-
fective, graph algorithms to induce the senses of
our queries. The best performing approach is based
on squares (cycles of length 4), a novel graph pat-
tern in WSI. Third, we contribute a new dataset of
114 ambiguous queries and 11,400 sense-annotated
snippets which complements an existing dataset of
ambiguous queries7. Given the lack of ambiguous
query datasets available (Sanderson, 2008), we hope
our new dataset will be useful in future compara-
tive experiments. Finally, we note that our approach
needed very little tuning. Moreover, its requirement
of a Web corpus of n-grams is not a stringent one, as
such corpora are available for several languages and
can be produced for any language of interest.
As regards future work, we intend to combine
our clustering algorithm with a cluster labeling al-
gorithm. We also aim to implement a number of
Word Sense Induction algorithms and compare them
in the same evaluation framework with more Web
search and Web clustering engines. Finally, it should
be possible to use precisely the same approach pre-
sented in this paper for document clustering, by
grouping the contexts in which the target query oc-
curs ? and we will also experiment on this in the
future.
Acknowledgments
We thank Google for providing the Web1T corpus
for research purposes. We also thank Massimiliano
D?Amico for producing the output of KeySRC and
EP, and Stanislaw Osinski and Dawid Weiss for their
7The MORESQUE dataset is available at the following
URL: http://lcl.uniroma1.it/moresque
124
help with Lingo and STC. Additional thanks go to
Jim McManus, Senja Pollak and the anonymous re-
viewers for their useful comments.
References
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Evaluating and optimizing
the parameters of an unsupervised graph-based WSD
algorithm. In Proc. of TextGraphs ?06, pages 89?96,
New York, USA.
Paul N. Bennett and Nam Nguyen. 2009. Refined ex-
perts: improving classification in large taxonomies. In
Proc. of SIGIR ?09, pages 11?18, Boston, MA, USA.
Andrea Bernardini, Claudio Carpineto, and Massimil-
iano D?Amico. 2009. Full-subtopic retrieval with
keyphrase-based search results clustering. In Proc. of
WI ?09, pages 206?213, Milan, Italy.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram,
ver. 1, ldc2006t13. In LDC, PA, USA.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of SIGIR
?98, pages 335?336, Melbourne, Australia.
David Carmel, Haggai Roitman, and Naama Zwerdling.
2009. Enhancing cluster labeling using Wikipedia. In
Proc. of SIGIR ?09, pages 139?146, MA, USA.
Claudio Carpineto and Giovanni Romano. 2004. Ex-
ploiting the potential of concept lattices for informa-
tion retrieval with CREDO. Journal of Universal
Computer Science, 10(8):985?1013.
Claudio Carpineto, Stanislaw Osin?ski, Giovanni Ro-
mano, and Dawid Weiss. 2009. A survey of web clus-
tering engines. ACM Computing Surveys, 41(3):1?38.
Harr Chen and David R. Karger. 2006. Less is more:
probabilistic models for retrieving fewer relevant doc-
uments. In Proc. of SIGIR ?06, pages 429?436, Seat-
tle, WA, USA.
Jiyang Chen, Osmar R. Za??ane, and Randy Goebel. 2008.
An unsupervised approach to cluster web search re-
sults based on word sense communities. In Proc. of
WI-IAT 2008, pages 725?729, Sydney, Australia.
David Cheng, Santosh Vempala, Ravi Kannan, and Grant
Wang. 2005. A divide-and-merge methodology for
clustering. In Proc. of PODS ?05, pages 196?205,
New York, NY, USA.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selec-
tion. In Proc. of WI ?05, pages 172?178, Compie`gne,
France.
Douglass R. Cutting, David R. Karger, Jan O. Pedersen,
and John W. Tukey. 1992. Scatter/gather: A cluster-
based approach to browsing large document collec-
tions. In Proc. of SIGIR ?92, pages 318?329, Copen-
hagen, Denmark.
Emilio Di Giacomo, Walter Didimo, Luca Grilli, and
Giuseppe Liotta. 2007. Graph visualization tech-
niques for web clustering engines. IEEE Transactions
on Visualization and Computer Graphics, 13(2):294?
304.
George W. Furnas, Thomas K. Landauer, Louis Gomez,
and Susan Dumais. 1987. The vocabulary problem
in human-system communication. Communications of
the ACM, 30(11):964?971.
Fatih Gelgi, Hasan Davulcu, and Srinivas Vadrevu. 2007.
Term ranking for clustering web search results. In
Proc. of WebDB ?07, Beijing, China.
Julio Gonzalo, Anselmo Penas, and Felisa Verdejo. 1999.
Lexical ambiguity and Information Retrieval revisited.
In Proc. of EMNLP/VLC 1999, pages 195?202, Col-
lege Park, MD, USA.
Zellig Harris. 1954. Distributional structure. Word,
10:146?162.
Maryam Kamvar and Shumeet Baluja. 2006. A large
scale study of wireless search behavior: Google mo-
bile search. In Proc. of CHI ?06, pages 701?709, New
York, NY, USA.
Weimao Ke, Cassidy R. Sugimoto, and Javed Mostafa.
2009. Dynamicity vs. effectiveness: studying online
clustering for scatter/gather. In Proc. of SIGIR ?09,
pages 19?26, MA, USA.
Sang-Bum Kim, Hee-Cheol Seo, and Hae-Chang Rim.
2004. Information Retrieval using word senses: root
sense tagging approach. In Proc. of SIGIR ?04, pages
258?265, Sheffield, UK.
Robert Krovetz and William B. Croft. 1992. Lexical am-
biguity and Information Retrieval. ACM Transactions
on Information Systems, 10(2):115?141.
Oren Kurland and Carmel Domshlak. 2008. A rank-
aggregation approach to searching for optimal query-
specific clusters. In Proc. of SIGIR ?08, pages 547?
554, Singapore.
Oren Kurland. 2008. The opposite of smoothing: a lan-
guage model approach to ranking query-specific doc-
ument clusters. In Proc. of SIGIR ?08, pages 171?178,
Singapore.
Kyung Soon Lee, W. Bruce Croft, and James Allan.
2008. A cluster-based resampling method for pseudo-
relevance feedback. In Proc. of SIGIR ?08, pages 235?
242, Singapore.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of the 17th COLING, pages
768?774, Montreal, Canada.
125
Shuang Liu, Clement Yu, and Weiyi Meng. 2005a. Word
Sense Disambiguation in queries. In Proc. of CIKM
?05, pages 525?532, Bremen, Germany.
Tie-Yan Liu, Yiming Yang, Hao Wan, Hua-Jun Zeng,
Zheng Chen, and Wei-Ying Ma. 2005b. Support vec-
tor machines classification with a very large-scale tax-
onomy. SIGKDD Explor. Newsl., 7(1):36?43.
Ying Liu, Wenyuan Li, Yongjing Lin, and Liping Jing.
2008. Spectral geometry for simultaneously clustering
and ranking query search results. In Proc. of SIGIR
?08, pages 539?546, Singapore.
Rila Mandala, Takenobu Tokunaga, and Hozumi Tanaka.
1998. The use of WordNet in Information Retrieval.
In Proc. of the COLING-ACL workshop on Usage of
Wordnet in Natural Language Processing, pages 31?
37, Montreal, Canada.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
George A. Miller, Richard T. Beckwith, Christiane D.
Fellbaum, Derek Gross, and Katherine Miller. 1990.
WordNet: an online lexical database. International
Journal of Lexicography, 3(4):235?244.
Roberto Navigli. 2009. Word Sense Disambiguation: a
survey. ACM Computing Surveys, 41(2):1?69.
Chi Lang Ngo and Hung Son Nguyen. 2005. A method
of web search result clustering based on rough sets. In
Proc. of WI ?05, pages 673?679, Compie`gne, France.
Cam-Tu Nguyen, Xuan-Hieu Phan, Susumu Horiguchi,
Thu-Trang Nguyen, and Quang-Thuy Ha. 2009.
Web search clustering and labeling with hidden top-
ics. ACM Transactions on Asian Language Informa-
tion Processing, 8(3):1?40.
Stanislaw Osinski and Dawid Weiss. 2005. A concept-
driven algorithm for clustering search results. IEEE
Intelligent Systems, 20(3):48?54.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
Mark Sanderson. 1994. Word Sense Disambiguation
and Information Retrieval. In Proc. of SIGIR ?94,
pages 142?151, Dublin, Ireland.
Mark Sanderson. 2000. Retrieving with good sense. In-
formation Retrieval, 2(1):49?69.
Mark Sanderson. 2008. Ambiguous queries: test collec-
tions need more sense. In Proc. of SIGIR ?08, pages
499?506, Singapore.
Hinrich Schu?tze and Jan Pedersen. 1995. Information
Retrieval based on word senses. In Proceedings of
SDAIR?95, pages 161?175, Las Vegas, Nevada, USA.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?124.
Christopher Stokoe, Michael J. Oakes, and John I. Tait.
2003. Word Sense Disambiguation in Information Re-
trieval revisited. In Proc. of SIGIR ?03, pages 159?
166, Canada.
Ashwin Swaminathan, Cherian V. Mathew, and Darko
Kirovski. 2009. Essential pages. In Proc. of WI ?09,
pages 173?182, Milan, Italy.
Goldee Udani, Shachi Dave, Anthony Davis, and Tim
Sibley. 2005. Noun sense induction using web search
results. In Proc. of SIGIR ?05, pages 657?658, Sal-
vador, Brazil.
Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Butterworths, second edition.
Jean Ve?ronis. 2004. HyperLex: lexical cartography
for Information Retrieval. Computer Speech and Lan-
guage, 18(3):223?252.
Ellen M. Voorhees. 1993. Using WordNet to disam-
biguate word senses for text retrieval. In Proc. of SI-
GIR ?93, pages 171?180, Pittsburgh, PA, USA.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Proc.
of the 19th COLING, pages 1?7, Taipei, Taiwan.
Gui-Rong Xue, Dikan Xing, Qiang Yang, and Yong Yu.
2008. Deep classification in large-scale text hierar-
chies. In Proc. of SIGIR ?08, pages 619?626, Singa-
pore.
Israel Ben-Shaul Yoelle Maarek, Ron Fagin and Dan Pel-
leg. 2000. Ephemeral document clustering for web
applications. IBM Research Report RJ 10186.
Oren Zamir and Oren Etzioni. 1998. Web document
clustering: a feasibility demonstration. In Proc. of SI-
GIR ?98, pages 46?54, Melbourne, Australia.
Oren Zamir, Oren Etzioni, Omid Madani, and Richard M.
Karp. 1997. Fast and intuitive clustering of web docu-
ments. In Proc. of KDD ?97, pages 287?290, Newport
Beach, California.
Hua-Jun Zeng, Qi-Cai He, Zheng Chen, Wei-Ying Ma,
and Jinwen Ma. 2004. Learning to cluster web
search results. In Proc. of SIGIR ?04, pages 210?217,
Sheffield, UK.
ChengXiang Zhai, William W. Cohen, and John Lafferty.
2003. Beyond independent relevance: Methods and
evaluation metrics for subtopic retrieval. In Proc. of
SIGIR ?03, pages 10?17, Toronto, Canada.
Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Weiguo
Fan, Zheng Chen, and Wei-Ying Ma. 2005. Improv-
ing web search results using affinity graph. In Proc. of
SIGIR ?05, pages 504?511, Salvador, Brazil.
Xiaodan Zhang, Xiaohua Hu, and Xiaohua Zhou. 2008.
A comparative evaluation of different link types on en-
hancing document clustering. In Proc. of SIGIR ?08,
pages 555?562, Singapore.
126
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1399?1410, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation
Roberto Navigli and Simone Paolo Ponzetto
Dipartimento di Informatica
Sapienza Universita` di Roma
{navigli,ponzetto}@di.uniroma1.it
Abstract
We present a multilingual joint approach
to Word Sense Disambiguation (WSD). Our
method exploits BabelNet, a very large mul-
tilingual knowledge base, to perform graph-
based WSD across different languages, and
brings together empirical evidence from these
languages using ensemble methods. The re-
sults show that, thanks to complementing
wide-coverage multilingual lexical knowledge
with robust graph-based algorithms and com-
bination methods, we are able to achieve the
state of the art in both monolingual and multi-
lingual WSD settings.
1 Introduction
Nowadays the textual information needed by a user
accessing websites for content such as news re-
ports, commentaries and encyclopedic knowledge
is provided in an increasingly wide range of lan-
guages. For example, even though English is still
the majority language of the Web, the Chinese and
Spanish languages are moving fast to capture their
?juicy share?, and more languages are about to join
them in the near future. This language explosion
clearly forces researchers to focus on the challeng-
ing problem of being able to analyze and under-
stand text written in any language. However, it also
opens up novel perspectives for multilingual Natural
Language Processing (NLP) such as, for instance,
the development of approaches aimed at ?joining
forces? and taking advantage of the lexico-semantic
knowledge provided in the different languages to
improve text understanding. These two aspects are
strongly intertwined: on the one hand, enabling
language-independent text understanding would al-
low for the harvesting of more knowledge in arbi-
trary languages, while, on the other hand, bringing
together the lexical and semantic information avail-
able in different languages would improve the qual-
ity of text understanding in arbitrary languages.
However, these two goals have hitherto never
been achieved, as is attested to by the fact that re-
search in a core language understanding task such as
Word Sense Disambiguation (Navigli, 2009, WSD)
has always been focused mostly on English. His-
torically, English became established as the lan-
guage used and understood by the scientific com-
munity and, consequently, most resources were de-
veloped for it, including large-scale computational
lexicons like WordNet (Fellbaum, 1998) and sense-
tagged corpora like SemCor (Miller et al 1993).
As a result WSD in other languages was hindered
by a lack of resources, which in turn led to poor re-
sults or low involvement on the part of the research
community (Magnini et al 2004; Ma`rquez et al
2004; Orhan et al 2007; Okumura et al 2010).
Nonetheless, already in the 1990s it had been re-
marked that WSD could be improved by means of
multilingual information: a recurring idea proposed
by several researchers was that plausible transla-
tions of a word in context would restrict its pos-
sible senses to a manageable subset of meanings
(Dagan et al 1991; Gale et al 1992; Resnik and
Yarowsky, 1999). While the lack of resources at that
time hampered the development of effective multi-
lingual approaches to WSD, recently this idea has
been revamped with the organization of SemEval
tasks dealing with cross-lingual WSD (Lefever and
Hoste, 2010) and cross-lingual lexical substitution
(Mihalcea et al 2010). At the same time, new re-
1399
search on the topic has been done, including the use
of statistical translations of sentences into many lan-
guages as features for supervised models (Banea and
Mihalcea, 2011; Lefever et al 2011), and the pro-
jection of monolingual knowledge onto another lan-
guage (Khapra et al 2011).
Yet the above two goals, i.e., disambiguating in
an arbitrary language and using lexical and seman-
tic knowledge from many languages in a joint way
to improve the WSD task, have not hitherto been
attained. In this paper, we address both objectives
and propose a graph-based approach to multilingual
joint Word Sense Disambiguation. Our proposal
brings together the lexical knowledge from differ-
ent languages by exploiting empirical evidence for
disambiguation from each of them, and then com-
bining this information in a synergistic way: each
language provides a piece of sense evidence for the
meaning of a target word in context, and subsequent
integration of these various pieces enables them to
(soft) constrain each other. The results show that
this way we are able to improve over previous, high-
performing graph-based methods in both a monolin-
gual and multilingual setting, thus showing for the
first time the beneficial effects of exploiting multi-
lingual knowledge in a joint fashion.
2 Related Work
Parallel corpora have been used in the literature
for the automatic creation of a sense-tagged dataset
for supervised WSD in different languages (Gale
et al 1992; Chan and Ng, 2005; Zhong and Ng,
2009). Other approaches include the use of a coher-
ence index for identifying the tendency to lexicalize
senses differently across languages (Ide, 2000) and
the clustering of source words which translate into
the same target word, then used to perform WSD
using a similarity measure (Diab, 2003). A histori-
cal approach (Brown et al 1991) uses bilingual cor-
pora to perform unsupervised word alignment and
determine the most appropriate translation for a tar-
get word from a set of contextual features.
All the above approaches to multilingual or cross-
lingual WSD rely on bilingual corpora, including
those which exploit existing multilingual WordNet-
like resources (Ide et al 2002), or use automatically
induced multilingual co-occurrence graphs (Silberer
and Ponzetto, 2010). However, this requirement is
often very hard to satisfy, especially if we need wide
coverage. To overcome this limitation, in this work
we make use of BabelNet (Navigli and Ponzetto,
2010), a very large multilingual lexical knowledge
base. This resource ? complementary in nature
to other recent efforts presented by de Melo and
Weikum (2010), Nastase et al(2010) and Meyer and
Gurevych (2012), inter alia ? provides a truly multi-
lingual semantic network by combining Wikipedia?s
multilinguality with the output of a state-of-the-art
machine translation system to achieve high cover-
age for all languages. The key insight here is that
Word Sense Disambiguation and Machine Transla-
tion (MT) are highly intertwined tasks, as previously
shown by Carpuat and Wu (2007) and Chan et al
(2007), who successfully used sense information to
boost state-of-the-art statistical MT. In this work we
focus instead on the benefits of using multilingual
information for WSD by exploiting the structure of
a multilingual semantic network.
3 Multilingual Joint WSD
We present our methodology for multilingual WSD:
we first introduce BabelNet, the resource used in our
work (Section 3.1) and then present our algorithm
for multilingual joint WSD (Section 3.2), including
its main components, namely graph-based WSD, en-
semble methods and translation weighting (sections
3.3, 3.4 and 3.5).
3.1 BabelNet
BabelNet (Navigli and Ponzetto, 2010) follows the
structure of a traditional lexical knowledge base and,
accordingly, consists of a labeled directed graph
whose nodes represent concepts and named entities,
and whose edges express semantic relations between
them. Concepts and relations are harvested from
the largest available semantic lexicon of English,
i.e., WordNet, and a wide-coverage collaboratively-
edited encyclopedia, i.e., Wikipedia1, thus making
BabelNet a multilingual ?encyclopedic dictionary?
which combines lexicographic information with en-
cyclopedic knowledge on the basis of an unsuper-
vised mapping framework. In addition to a core
1http://www.wikipedia.org. In the following, we
refer to Wikipedia pages and senses using SMALL CAPS.
1400
semantic network, BabelNet provides a multilin-
gual lexical dimension. Each of its nodes, called
Babel synsets, contains a set of lexicalizations of
the concept for different languages, e.g., { bankENn ,
BankDEn , bancaITn , . . . , bancoESn }2. Multilin-
gual lexicalizations for all concepts are collected
from Wikipedia?s inter-language links (e.g., the En-
glish Wikipedia page BANK links to the Italian
BANCA), as well as by acquiring missing trans-
lations by means of a statistical machine transla-
tion system applied to sense-tagged data from Sem-
Cor and Wikipedia itself ? for instance, most oc-
currences of bank1n in SemCor3 are translated into
German and Italian as Ufer and riva, respectively.
As a result of combining human-edited translations
from Wikipedia and automatically generated ones
from sense-labeled data, BabelNet is able to achieve
wide coverage for all its languages (Catalan, En-
glish, French, German, Italian and Spanish): accord-
ingly, we chose it to perform graph-based WSD in
a multilingual setting since it is specifically focused
on lexical knowledge. In addition, BabelNet is avail-
able for any language required to perform standard
SemEval cross-lingual disambiguation tasks (e.g.,
Spanish, in order to perform cross-lingual lexical
substitution). Since previous work in knowledge-
based WSD shows the benefits of using rich lexical
resources (Navigli and Lapata, 2010; Ponzetto and
Navigli, 2010), BabelNet is a suitable choice for per-
forming graph-based multilingual WSD.
3.2 Exploiting multilingual information in a
knowledge-based WSD framework
We present a multilingual approach to WSD
which exploits three main factors:
i) the fact that translations of a target word pro-
vide complementary information on the range
of its candidate senses in context;
ii) the wide-coverage, multilingual lexical knowl-
edge stored in BabelNet;
iii) the support for disambiguation from different
languages in a synergistic, unified way.
2BabelNet senses are referred to with wlp, namely the sense
of a word w in a language l with part of speech p.
3We denote WordNet senses with wip, namely the i-th sense
of a word w with part of speech p.
Algorithm 1 Multilingual joint WSD
Input: a word sequence ? = (w1, . . . , wn)
a target word w ? ?
BabelNet BN
an ensemble method M
Output: a distribution of scores for the senses of w
( indicates a comment)
1: S ? SynsetsBN (w)
2: T ? {w}
3: for each s ? S
4: T ? T ? getTranslations(s)
5: ctx? ? ? {w}
6:  LScore := {lScorei,j}i=1,...,|T |, j=1,...,|S|
7: for each ti ? T
8: ?? ? {ti} ? ctx
9:  Gi := (Vi, Ei)
10: Gi ? createGraph(??, BN)
11: for each sj ? S ? Vi
12: lScorei,j ? score(Gi, sj)
13:  Score := (score1, . . . , score|S|)
14: Score?M(LScore)
15: return Score
We call this approach multilingual joint WSD,
since disambiguation is performed by exploiting dif-
ferent languages together at the same time. To this
end, we first perform graph-based WSD using the
target word in context as input, and then combine
sense evidence from its translations using an ensem-
ble method. The key idea of our joint approach is
that sense evidence from different translations pro-
vides complementary views for the senses of a tar-
get word in context. Therefore, combining such ev-
idence should produce more accurate sense predic-
tions. We view WSD as a sense ranking problem.
Given a word sequence ? = (w1, . . . , wn), we dis-
ambiguate a target word w ? ? by scoring each of
its senses and selecting the highest-ranking one:
s? = argmax
s ? SynsetsBN (w)
score(s) , (1)
where SynsetsBN (w) is the set of Babel synsets con-
taining the different senses for w.4 We score these
4Babel synsets unambiguously identify different senses
of the target word, e.g., { bankENn , BankDEn , bancoESn . . . ,
bancaITn } corresponds to the ?financial institute? sense of
bankENn (i.e., bank2n in WordNet).
1401
synsets using Algorithm 1, which we illustrate in
the following by means of the example sentence
?bank bonuses are paid in stock?, where we focus on
bankENn as the target word and { bonusENn , payENv ,
stockENn } as its context. The following steps are
performed:
Initialization. We start by gathering the data re-
quired for disambiguation (lines 1?5). First, we
collect in line 1 the set S of Babel synsets corre-
sponding to the different senses of the target word w
? namely, the synsets containing the ?financial in-
stitution?, ?money container?, ?building? senses of
bankENn , among others. Next, we obtain the multi-
lingual lexicalizations of the target word: to this end,
we first include in T the word w itself (line 2), and
then iterate through each synset s ? S to collect the
translations of each of its senses in the languages of
interest (lines 3?4). For instance, given the English
word bankENn , we collect its sense-specific German,
Italian and Spanish translations and obtain a set of
multilingual terms T = { bankENn , . . . , BankDEn ,
Sparbu?chseDEn , Bankgeba?udeDEn , . . . , bancaITn ,
salvadanaioITn , . . . , bancoESn , huchaESn }. Finally,
we create a disambiguation context ctx by taking the
word sequence ? and removing w from it (line 5, as
a result, e.g., ctx = { bonusENn , payENv , stockENn }).
Collecting sense distributions. In the next phase
(lines 6?12), we collect a scoring distribution over
the different synsets S of w for each term ti ? T .
Each distribution quantifies the empirical support for
the different senses of the target word, obtained us-
ing ti and the context ctx: we store this informa-
tion in a |T | ? |S| matrix LScore, where each cell
lScorei,j quantifies the support for synset sj ? S,
computed using the term in ti ? T . We calculate the
scores as follows:
- We select at each step an element ti from T (line
7), for instance bancoESn .
- Next, we create a multilingual context ?? by com-
bining ti with the words in ctx (line 8, e.g., we set
?? = { bancoESn , bonusENn , payENv , stockENn }.
- We use ?? to build a graph Gi = (Vi, Ei) by
computing the paths in BabelNet which connect
the synsets of ti with those of the other words
in ?? (line 10, see Section 3.3 for details on the
createGraph function). Note that by selecting at
each step a different element from T we create a
new graph where different sets of Babel synsets
get activated by the context words in ctx. In our
example, Figures 1(a)?(c) show the graphs ob-
tained by setting at different steps ti to bankENn ,
bancoESn and BankDEn , respectively (we show ex-
cerpts by using only stockENn as context word for
ease of readability).
- Finally, we compute the support from term ti for
each synset sj ? S of the target word by applying
a graph connectivity measure to Gi and store the
result in lScorei,j (lines 11?12). For instance, us-
ing degree as graph measure, we can compute the
following scores from the graph in Figure 1(b):
bank2n bank8n bank9n
bancoESn 2 0 1
By repeating the process for each term in T (lines 7?
12) we compute all values in the matrixLScore. For
instance, given T = {bankENn , bancoESn , BankDEn },
we create the set of graphs in Figures 1(a)?(c), and
compute from each of them the following scores
(again, using degree as scoring measure):
LScore =
bank2n bank8n bank9n
bankENn
?
?
2 2 1
?
?bancoESn 2 0 1
BankDEn 2 0 0
Combining sense distributions. In the last step
(line 14) we aggregate the scores associated with
each term of T using an ensemble method M (see
Section 3.4 for details). For instance, M could sim-
ply consist of summing the scores associated with
each sense over all distributions and thus return a
score of 6, 2, and 2 for bank2n, bank8n and bank9n,
respectively. As a result of the execution of Al-
gorithm 1, the combined scoring distribution is re-
turned (line 15). This sense distribution in turn can
be used to select the best sense using Equation 1.
The main hunch behind our approach is that using
information from different languages improves dis-
ambiguation performance, as in the example of Fig-
ure 1 where more accurate disambiguation is per-
formed by combining scores computed from trans-
lations in different languages, as opposed to using
1402
(a) Disambiguation graph using the target word bankENn .
bank2nBankDEbancoESbancaIT
bank8nSparbu?chseDEhuchaESsalvadanaioIT
bank9nBankgeba?udeDEbancoES bancaIT
stock1nAktienDEaccionesESazioniIT
stock4nAktienzertifikatDEaccionesES azioniIT
stock17nViehDEganadoESbestiameIT
commercial
bank
investment
banking
stock
broker trader
piggy
bank pig
building abattoir
(b) Disambiguation graph using bancoESn as translation.
bank2nBankDEbancoESbancaIT
bench1nBankDEbancoESpanchinaIT
bank8nSparbu?chseDEhuchaESsalvadanaioIT
bank9nBankgeba?udeDEbancoES bancaIT
stock1nAktienDEaccionesESazioniIT
stock4nAktienzertifikatDEaccionesES azioniIT
stock17nViehDEganadoESbestiameIT
commercial
bank
investment
banking
stock
broker trader
piggy
bank pig
building abattoir
(c) Disambiguation graph using BankDEn as translation.
bank2nBankDEbancoESbancaIT
bench1n BankDEbancoES panchinaIT
bed4n BankDEestratoES lettoIT
bank8nSparbu?chseDEhuchaESsalvadanaioIT
bank9nBankgeba?udeDEbancoES bancaIT
stock1nAktienDEaccionesESazioniIT
stock4nAktienzertifikatDEaccionesES azioniIT
stock17nViehDEganadoESbestiameIT
commercial
bank
investment
banking
stock
broker trader
piggy
bank pig
building abattoir
(d) List of corresponding WordNet senses and their glosses
bank2n financial institution that accepts deposits and channels
the money into lending activities
bank8n a container (usually with a slot in the top) for keeping
money at home
bank9n a building in which the business of banking transacted
stock1n the capital raised by a corporation through the issue
of shares entitling holders to an ownership interest
stock4n a certificate documenting the shareholder?s
ownership in the corporation
stock17n any animals kept for use or profit
Figure 1: Multilingual graph construction for the input sentence ?bank bonuses are paid in stock?. We show excerpts
using only stockENn as context word for ease of readability.
monolingual sense evidence only. Figure 1(a) shows
the graph created to disambiguate the English target
word bankENn in our example sentence. In the graph,
some of the possible senses of this word are acti-
vated, including the correct one (bank2n) but also re-
lated, yet incorrect ones such as bank8n and bank9n.
Figure 1(b) and 1(c) show instead the graphs ob-
tained from replacing the target word with its Span-
ish and German translations, respectively. In these
graphs, different subsets of the senses of bankENn
are activated, together with others pertaining to the
translations only (e.g., the meaning of bancoESn cor-
responding to the English bench1n). However, the
sense that is consistently activated across all graphs
is the correct one ? i.e., bankENn as financial insti-
tution ? which is in fact the sense selected by our
multilingual approach by means of combining the
scoring distributions from all these graphs.
3.3 Graph-based WSD
We use graph-based algorithms to exploit multilin-
gual knowledge from BabelNet for WSD. These are
a natural choice for our approach, since BabelNet is
a semantic network, and such algorithms have been
shown to achieve high performance across domains
(Agirre et al 2009; Navigli et al 2011), as well
as to compete with supervised methods on a vari-
ety of lexical disambiguation tasks (Ponzetto and
Navigli, 2010). To this end, we use the method
of Navigli and Lapata (2010) and construct a di-
rected graphG = (V,E) for an input word sequence
? = (w1, . . . , wn)5 using the lexical and semantic
relations found in BabelNet. The result of this pro-
cedure is a subgraph of BabelNet containing (1) the
senses of the words in context, (2) all edges and in-
termediate senses found in BabelNet alg all paths
that connect them. Given G, a target word w ? ?
and its set of senses in BabelNet S ? V , we com-
pute a score distribution (score1, . . . , score|S|) over
S, where scorej refers to the confidence score for
the j-th sense of w, e.g. bank2n, based on some con-
nectivity measure applied to G. In this paper, we
specifically focus on two such measures.
5In our experiments we always take ? to be a single sen-
tence, thus disambiguating on a sentence-by-sentence basis.
1403
Degree Centrality (Degree): The first measure
ranks the senses of a given word in the graph based
on the number of their incident edges, namely:
scorej = |{{sj , v} ? E : v ? V }| .
This standard connectivity measure weights a sense
as more appropriate if it has a higher degree. We
chose context-based Degree since, albeit simple, it
had previously been shown to yield a highly com-
petitive performance on various WSD tasks (Navigli
and Lapata, 2010; Ponzetto and Navigli, 2010).
Inverse path length sum (PLength): We then de-
veloped a graph connectivity measure which scores
each sense by summing over the inverse length of all
paths which connect it to other senses in the graph:
scorej =
?
p? paths(sj)
1
elength(p)?1
,
where paths(sj) is the set of simple paths con-
necting sj to the senses of other context words,
length(p) is the number of edges in the path p and
each path is scored with the exponential inverse de-
cay of the path length. This measure overcomes the
locality of Degree by aggregating over all paths be-
tween a sense of the target word and those of the
context words, thus being able to capture the rich-
ness of the BabelNet subgraph and the semantic den-
sity of the underlying knowledge base.
3.4 Ensemble methods for multilingual WSD
At the core of our algorithm lies the combination of
the scores generated using the different translations
of the target word w. For this purpose, we apply so-
called ensemble methods, which have been shown
to improve the performance of both supervised (Flo-
rian et al 2002) and unsupervised WSD systems
(Brody et al 2006). Given |T | lexicalizations and
|S| senses for w, the input to the combination com-
ponent consists of a |T |? |S|matrix LScore, where
each cell lScorei,j quantifies the empirical support
for sense sj from a term ti ? T (see Section 3.2 for
an example). The ensemble method computes from
this translation-sense matrix a combined scoring, ex-
pressing the joint confidence across terms in differ-
ent languages over the set of senses S. In this work,
we use the ?Probability Mixture? (PMixture) method
proposed by Brody et al(2006), which they show
to be the best performing for WSD. This method
takes the scores associated with each term, normal-
izes and combines them by summing across distri-
butions. Formally, it computes the score for the j-th
sense of w as follows:
scorej =
|T |?
i=1
p(si,j), p(si,j) =
lScorei,j
?|S|
s=1 lScorei,s
.
For instance, using the (normalized) sense distribu-
tions from our example, the ensemble distribution
will be the following:
bank2n bank8n bank9n
bankENn 0.40 0.40 0.20
bancoESn 0.67 0.00 0.33
BankDEn 1.00 0.00 0.00
PMixture 2.07 0.40 0.53
3.5 Weighting multilingual sense distribution
Computing a sense distribution for each translation
using the same graph connectivity measure assumes
that all translations are equal. However, a leitmotif
of multilingual WSD research is that translations re-
strict the set of candidate senses of the target word
in the source language. In our example of Figure
1, for instance, BankDEn provides structural support
only for the financial sense of English bank, since
this is the only sense it covers. Within our frame-
work this can potentially lead to skewed sense dis-
tributions when only some senses of the target word
have a translation. In such cases, in fact, scores tend
to be concentrated mostly on the senses covered by
the translations, with the result that sense evidence
for uncovered English senses is disregarded. In or-
der to cope with this issue, we weight the elements
of each sense distribution lScorei for the i-th trans-
lation ti ? T by a factor of 1+log2 cov(ti, w), where
cov(ti, w) is the number of Babel synsets where ti
co-occurs with the target word w ? i.e., the number
of senses of w that it covers (we use the log func-
tion to dampen the effect of high coverage values).
This is to say, in order to level off the effects of un-
balanced sense coverage we assume that, all things
being equal, the more senses a translation covers, the
stronger the disambiguation evidence it provides in
context for specific senses. As a result, the contri-
butions of each translation are weighted differently
1404
and we are thus able to dampen the effects of a
highly skewed distribution like, for instance, that of
BankDEn :
bank2n bank8n bank9n
bankENn 1.72 1.72 0.86
bancoESn 1.34 0.00 0.66
BankDEn 1.00 0.00 0.00
Weighted PMixture 4.04 1.70 1.52
4 Experiments
We evaluate our approach in two different settings,
namely a monolingual all-words WSD task in Sec-
tion 4.1, as well as two different cross-lingual dis-
ambiguation gold standards in Section 4.2.
4.1 Monolingual WSD
Experimental setting. We first evaluate the per-
formance of multilingual joint WSD on a standard
monolingual dataset, namely the SemEval-2010 do-
main WSD task 17 (Agirre et al 2010), since it
provides the latest dataset for fine-grained WSD in
English. We opt for an English all-words task for
two main reasons: first, it is a well-established and
widely-participated task in the WSD community ?
thus ensuring a comparison of our method with a
wide range of state-of-the-art approaches, includ-
ing other graph-based techniques (e.g., Personalized
PageRank), as well as weakly-supervised and super-
vised approaches (see Agirre et al(2010) for de-
tails on the participating systems); second, we want
to assess whether a multilingual approach benefits
lexical disambiguation in all settings, namely even
in a standard monolingual one. We use in our ex-
periments the dataset?s nouns-only subset (1032 in-
stances), since BabelNet currently contains multi-
lingual lexicalizations for nouns only (and thus no
multilingual strategy can be applied to other parts
of speech). We perform graph-based WSD with
BabelNet in two different configurations, namely a
monolingual and multilingual setting. The multi-
lingual system performs WSD by means of the full
joint multilingual approach described in Algorithm
1. The monolingual approach, instead, simply uses
the English input sentence for disambiguation ? that
is, we skip lines 3?4 of Algorithm 1. Knowledge-
based systems typically suffer from a low recall ?
i.e., they cannot provide an answer if no information
Algorithm P R F1
Monolingual Degree 50.6 45.2 47.7
graph PLength 51.0 47.3 49.1
Multilingual Degree? 53.9 48.6 51.1
ensemble PLength? 54.3 50.2 52.2
SemCor MFS 51.9 51.2 51.5
Random 25.3 25.3 25.3
Table 1: Performance on SemEval-2010 all-words do-
main WSD (nouns only subset). Best results for each
measure are bolded. ? indicates statistically significant
differences with respect to the monolingual setting.
can be found with senses of the context words. To
overcome this issue, in both settings we use a type-
based fallback strategy which assigns to the target
word the sense which has been most frequently as-
signed by the system to other instances of the word
in the dataset.
Results and discussion. We report our results in
terms of precision (P), recall (R) and F1 measure in
Table 1, where we compare the monolingual vari-
ant (rows 1?2 of the table) with our multilingual
approach (rows 3?4). Following standard practice,
(1) we benchmark our method against two baselines,
namely a random sense assignment and the most fre-
quent sense (MFS) from SemCor; (2) we test for sta-
tistical significance by computing a 95% confidence
interval on the recall score (i.e., the main evaluation
measure for the WSD task) using bootstrap resam-
pling (Noreen, 1989).
The results show that our multilingual approach
improves over the monolingual one by a substan-
tial (i.e., statistically significant) margin. Combining
multilingual information from different languages
yields a higher precision (+3.3 for both graph algo-
rithms) and recall (+3.4 and +2.9 for Degree and
PLength, respectively). Manual inspection of the
output reveals that these increases in precision are
due to translations in different languages constrain-
ing each other ? e.g., an implausible English sense is
?ruled out? from the sense distributions of the other
languages (cf. the example in Figure 1). The in-
creases in recall, instead, indicate that using trans-
lations triggers responses in those cases where no
sense of the English target word can be connected
to the senses of the context words ? i.e., some trans-
1405
Algorithm P R F1
Monolingual Degree 52.0 51.3 51.6
graph PLength 55.0 54.2 54.6
Multilingual Degree? 61.6 59.5 60.5
ensemble PLength? 62.5 60.4 61.4
CFILT 61.4 59.4 60.4
IIITH 56.4 55.3 55.8
Table 2: Performance on SemEval-2010 all-words do-
main WSD (nouns only subset) using the most frequent
sense assigned by the system as back-off strategy when
no sense assignment is attempted.
lations activate senses in the knowledge base which
are closer to the senses of the context words. The
result is an overall increase in F1 measure of 3.4
and 3.1 points for Degree and PLength, respectively,
which makes it possible for us to beat the MFS
baseline (notably a difficult competitor for WSD
systems). Among the different graph algorithms,
PLength consistently outperforms Degree: however,
the differences are not statistically significant.
In order to better understand the impact of our ap-
proach we follow previous work (e.g., Navigli and
Lapata (2010)) and explore a weakly-supervised set-
ting where the system attempts no sense assignment
if the highest score among those assigned to the
senses of a target word is below a certain threshold.
If this is the case, in order to provide an answer for
all items, we output the most frequent sense assigned
by the system to other instances of the target word,
and fall back to SemCor?s MFS if no assignment has
been attempted. We estimate the optimal value for
the threshold by maximizing F1 on a development
set obtained by combining the Senseval-2 (Palmer et
al., 2001) and Senseval-3 (Snyder and Palmer, 2004)
English all-words datasets. The results for this set-
ting are shown in Table 2, where we also compare
with the top-performing systems from the SemEval
competition, namely CFILT (Kulkarni et al 2010)
and IIITH (Reddy et al 2010).
By complementing our multilingual method with
the MFS heuristic we achieve a performance compa-
rable with the state of the art on this task. Again, the
multilingual ensemble approach consistently outper-
forms the monolingual one and enables us to achieve
the best overall results for this dataset: without mul-
tilingual information, in fact, we achieve only aver-
age performance above the MFS level, whereas by
effectively combining sense evidence from multilin-
gual translations we are able to boost the F1 measure
by a 6-8 point margin, and thus outperform the top-
ranking SemEval systems. While differences with
CFILT are not statistically significant, we still take
this to be good news, since our system is general
purpose in nature and, accordingly, does not use any
domain information such as manually-labeled exam-
ples for the most frequent domain words (CFILT) or
a domain-specific sense ranking (IIITH).
4.2 Cross-lingual lexical disambiguation
Using a multilingual lexical resource makes it possi-
ble to perform WSD in any of its languages. Ac-
cordingly, we complement our evaluation on En-
glish texts with a second set of experiments where
we quantify the impact of our approach on a lex-
ical disambiguation task in a multilingual setting.
To this end, we use the SemEval-2010 cross-lingual
lexical substitution (Mihalcea et al 2010, CL-LS,
henceforth) and WSD (Lefever et al 2011, CL-
WSD) tasks and evaluate our methodology on per-
forming disambiguation across different languages.
Both cross-lingual WSD tasks cast disambiguation
as a word translation problem: given an English pol-
ysemous noun in context as input, the system dis-
ambiguates it by providing a translation into another
language (translations are deemed correct if they
preserve the meaning of the source word in the target
language). Their main difference, instead, lies in the
range of translations which are assumed to be valid:
that is, while CL-LS assumes no predefined sense in-
ventory (i.e., any translation can be potentially cor-
rect), CL-WSD makes use of a sense inventory built
on the basis of the Europarl corpus (Koehn, 2005).
Our approach to lexical disambiguation involves
two steps: first, given a target word in context, we
disambiguate it as usual to the highest-ranked Ba-
bel synset; next, given the translations in the se-
lected synset, we return the most suitable lexical-
ization in the language of interest. Since the se-
lected synset can contain multiple translations in a
target language for the input English word, we ex-
plore using an unsupervised strategy to select the
most reliable translation from multiple candidates.
To this end, we return for each test instance only the
1406
Algorithm P/R/F1
Baseline 23.80
Monolingual Degree 30.52
graph PLength 30.64
Multilingual Degree 32.21
ensemble PLength 32.47
UBA-T 32.17
Table 3: Performance on SemEval-2010 lexical substitu-
tion (best results are bolded).
most frequent translation found in the Babel synset.
Given that the two tasks make different assumptions
on the sense inventory (no fixed inventory for CL-
LS vs. Europarl-based for CL-WSD), the frequency
of a translation is calculated as either the number
of Babel synsets in which it occurs (CL-LS), or its
frequency of alignment with the target word, as ob-
tained by applying GIZA++ (Och and Ney, 2003) to
Europarl (CL-WSD). To provide an answer for all
instances, we return this most frequent translation
even when no sense assignment is attempted ? i.e.,
no sense of the target word is connected to any other
sense of the context words ? or a tie occurs.
Results and discussion. We report our results for
CL-LS and CL-WSD in Tables 3 and 4. We evalu-
ate using the nouns-only subset of the CL-LS dataset
and the full CL-WSD dataset, consisting of 300 and
1,000 instances of nouns in context, respectively.
The evaluation scheme is based on the SemEval-
2007 English lexical substitution task (McCarthy
and Navigli, 2009), and consists of an adaptation of
the metrics of precision and recall for the translation
setting. For each task, we compare our monolingual
and multilingual approaches against the best per-
forming SemEval systems for these tasks, namely
UBA-T (Basile and Semeraro, 2010) and UVT-v
(van Gompel, 2010) for CL-LS and CL-WSD, re-
spectively, as well as a recent supervised proposal
that exploits automatically generated multilingual
features from parallel text and translated contexts
(Lefever et al 2011, Parasense). For each task
we also report its official baseline, namely the first
translation from an online-dictionary6 for CL-LS,
and the most frequent word alignment obtained by
6www.spanishdict.com
applying GIZA++ to the Europarl data for CL-WSD.
Our cross-lingual results confirm all trends of the
English monolingual evaluation, namely that: a) our
joint multilingual approach substantially improves
over the simple monolingual graph-based approach;
b) it enables us to achieve state-of-the-art perfor-
mance for these tasks. In the case of both CL-
LS and CL-WSD, using a rich multilingual knowl-
edge base like BabelNet makes it possible to achieve
a respectable performance already with the simple
monolingual approach, thus indicating the viability
of a knowledge-rich approach to sense-driven word
translation. The use of multilingual ensembles al-
ways improves the monolingual setting for all lan-
guages, and allows us to achieve the best overall re-
sults for both CL-LS and CL-WSD. Similarly to the
case of monolingual WSD, manual inspection of the
output reveals that translations help us rule out in-
correct senses and let the disambiguation algorithm
focus on the more coherent set of senses for the in-
put context in a way similar to the one highlighted
by the example in Figure 1. As a result of this we
are able to improve the performance of both mono-
lingual Degree and PLength, and compete with the
state of the art on all disambiguation tasks.
5 Conclusions
In this paper we presented a multilingual joint ap-
proach to WSD. Key to our methodology is the ef-
fective use of a wide-coverage multilingual knowl-
edge base, BabelNet, which we exploit to perform
graph-based WSD across languages and combine
complementary sense evidence from translations in
different languages using an ensemble method. This
is the first proposal to exploit structured multilingual
information within a joint, knowledge-rich frame-
work for WSD. The APIs to perform multilingual
WSD using BabelNet are freely available for re-
search purposes (Navigli and Ponzetto, 2012b).
Thanks to multilingual joint WSD we achieve
state-of-the-art performance on three different gold
standards. The good news about these results is that
not only can further advances be achieved by using
multilingual lexical knowledge, but, more impor-
tantly, that combining multilingual sense evidence
from different languages at the same time yields
consistent improvements over a monolingual ap-
1407
French German Italian Spanish
P/R/F1 P/R/F1 P/R/F1 P/R/F1
Baseline 21.25 13.16 15.18 19.74
UvT-v N/A N/A N/A 23.39
Parasense 24.54 16.88 18.03 22.80
Monolingual Degree 22.94 17.15 18.03 22.48
graph PLength 23.42 17.72 18.19 22.76
Multilingual Degree 24.02 18.07 18.93 23.51
ensemble PLength 24.61 18.26 19.05 23.65
Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
proach in both monolingual and cross-lingual lexical
disambiguation tasks ? that is, ?joining forces pays
off?. Effectively leveraging multilingual knowledge
for WSD helps overcome the shortcomings of the
underlying resource (noise, coverage, etc.), thus in-
dicating that further performance boosts can come
in the future from even better multilingual lexical
resources. Moreover, our methodology is general-
purpose and can be adapted to tasks other than
WSD: in fact, we have already taken the first steps
in this direction by showing the beneficial effects of
a joint multilingual approach to computing semantic
relatedness (Navigli and Ponzetto, 2012a). In ad-
dition, we plan in the very near future to general-
ize our multilingual joint approach and apply it to
high-end tasks such as multilingual textual entail-
ment (Mehdad et al 2011) and sentiment analysis
(Lu et al 2011) ? so as to provide a general frame-
work for knowledge-rich multilingual NLP.
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
BabelNet and its API are available for download at
http://lcl.uniroma1.it/babelnet.
References
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proceedings of the 21st International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), pages 1501?
1506.
Eneko Agirre, Oier Lo?pez de Lacalle, Christiane Fell-
baum, Shu-Kai Hsieh, Maurizio Tesconi, Monica
Monachini, Piek Vossen, and Roxanne Segers. 2010.
Semeval-2010 task 17: All-words Word Sense Disam-
biguation on a specific domain. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 75?80.
Carmen Banea and Rada Mihalcea. 2011. Word Sense
Disambiguation with multilingual features. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics (IWCS 2011), pages 25?34.
Pierpaolo Basile and Giovanni Semeraro. 2010. UBA:
Using automatic translation and Wikipedia for cross-
lingual lexical substitution. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 242?247.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised WSD.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL-06), pages 97?104.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-sense dis-
ambiguation using statistical methods. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics (ACL-91), pages 264?270.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Language Learning
(EMNLP-CoNLL-07), pages 61?72.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up
Word Sense Disambiguation via parallel texts. In Pro-
ceedings of the 20th National Conference on Artificial
Intelligence (AAAI-05), pages 1037?1042.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation improves Statistical Ma-
1408
chine Translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 33?40.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics (ACL-91), pages 130?137.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing multilingual taxonomies from Wikipedia. In
Proceedings of the 19th ACM Conference on Informa-
tion and Knowledge Management (CIKM-10), pages
1099?1108.
Mona Diab. 2003. Word Sense Disambiguation within a
Multilingual Framework. Ph.D. thesis, University of
Maryland, College Park, Maryland.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Radu Florian, Silviu Cucerzan, Charles Schafer, and
David Yarowsky. 2002. Combining classifiers for
Word Sense Disambiguation. Natural Language En-
gineering, 8(4):1?14.
William A. Gale, Kenneth Church, and David Yarowsky.
1992. Using bilingual materials to develop Word
Sense Disambiguation methods. In Proceedings of the
Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation, pages
101?112.
Nancy Ide, Tomaz Erjavec, and Dan Tufis?. 2002. Sense
discrimination with parallel corpora. In Proceedings
of the ACL-02 Workshop on WSD: Recent Successes
and Future Directions, pages 54?60.
Nancy Ide. 2000. Cross-lingual sense determination:
Can it work? Computers and the Humanities, 34:223?
234.
Mitesh M. Khapra, Salil Joshi, Arindam Chatterjee, and
Pushpak Bhattacharyya. 2011. Together we can:
Bilingual bootstrapping for WSD. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics, (ACL-11), pages 561?569.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of Ma-
chine Translation Summit X.
Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. CFILT: Resource
conscious approaches for all-words domain specific
WSD. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), pages
421?426.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
task 3: Cross-lingual Word Sense Disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluations (SemEval-2010), pages 15?20.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
Word Sense Disambiguation. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics, (ACL-11), pages 317?322.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin
K. Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics, (ACL-11), pages 320?330.
Bernardo Magnini, Danilo Giampiccolo, and Alessan-
dro Vallin. 2004. The Italian lexical sample task at
Senseval-3. In Proceedings of the 3rd International
Workshop on the Evaluation of Systems for the Seman-
tic Analysis of Text (SENSEVAL-3), pages 17?20.
Lluis Ma`rquez, Mariona Taule?, Antonia Mart??, Nu?ria
Artigas, Mar Garc??a, Francis Real, and Dani Ferre?s.
2004. Senseval-3: The Spanish lexical sample task.
In Proceedings of the 3rd International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text (SENSEVAL-3), pages 21?24.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, (ACL-11), pages 1336?1345.
Christian M. Meyer and Iryna Gurevych. 2012.
Ontowiktionary ? Constructing an ontology from
the collaborative online dictionary Wiktionary. In
Maria Teresa Pazienza and Armando Stellato, edi-
tors, Semi-Automatic Ontology Development: Pro-
cesses and Resources. IGI Global, Hershey, Penn.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), pages
9?14.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In Pro-
ceedings of the 3rd DARPA Workshop on Human Lan-
guage Technology, pages 303?308.
Vivi Nastase, Michael Strube, Benjamin Bo?rschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation, (LREC ?10).
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Anaylsis and Machine Intelligence, 32(4):678?
692.
1409
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belNet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
(ACL-10), pages 216?225.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belRelate! A joint multilingual approach to computing
semantic relatedness. In Proceedings of the 26th Con-
ference on Artificial Intelligence (AAAI-12).
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
Multilingual WSD with just a few lines of code: The
BabelNet API. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, (ACL-12). System Demonstrations.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier Lopez
de Lacalle, and Eneko Agirre. 2011. Two birds with
one stone: Learning semantic models for Text Cate-
gorization and Word Sense Disambiguation. In Pro-
ceedings of the 20th ACM Conference on Informa-
tion and Knowledge Management (CIKM-11), pages
2317?2320.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Eric W. Noreen, editor. 1989. Computer-intensive meth-
ods for testing hypotheses: an introduction. New
York, N.Y.: John Wiley.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and
Hikaru Yokono. 2010. SemEval-2010 task: Japanese
WSD. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), pages
69?74.
Zeynep Orhan, Emine C?elik, and Demirgu?c? Neslihan.
2007. SemEval-2007 task 12: Turkish lexical sample
task. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007), pages
59?63.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-
ren Delfs, and Hoa Trang Dang. 2001. English tasks:
All-words and verb lexical sample. In Proceedings of
the 2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2), pages
21?24.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised system. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, (ACL-10), pages 1522?1531.
Siva Reddy, Abhilash Inumella, Diana McCarthy, and
Mark Stevenson. 2010. IIITH: Domain specific
Word Sense Disambiguation. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 387?391.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evaluation
methods for Word Sense Disambiguation. Journal of
Natural Language Engineering, 5(2):113?133.
Carina Silberer and Simone Paolo Ponzetto. 2010. UHD:
Cross-lingual Word Sense Disambiguation using mul-
tilingual co-occurrence graphs. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 134?137.
Benjamin Snyder and Martha Palmer. 2004. The English
all-words task. In Proceedings of the 3rd International
Workshop on the Evaluation of Systems for the Seman-
tic Analysis of Text (SENSEVAL-3), pages 41?43.
Maarten van Gompel. 2010. UvT-WSD1: A cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluations (SemEval-2010), pages 238?241.
Zhi Zhong and Hwee Tou Ng. 2009. Word Sense Dis-
ambiguation for all words without hard labor. In Pro-
ceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI-09), pages 1616?1622.
1410
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1411?1422, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A New Minimally-Supervised Framework
for Domain Word Sense Disambiguation
Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{faralli,navigli}@di.uniroma1.it
Abstract
We present a new minimally-supervised
framework for performing domain-driven
Word Sense Disambiguation (WSD). Glos-
saries for several domains are iteratively ac-
quired from the Web by means of a boot-
strapping technique. The acquired glosses are
then used as the sense inventory for fully-
unsupervised domain WSD. Our experiments,
on new and gold-standard datasets, show that
our wide-coverage framework enables high-
performance results on dozens of domains at
a coarse and fine-grained level.
1 Introduction
Domain information pervades most of the text we
read every day. If we just think of the Web, the vast
majority of its textual content is domain oriented.
A case in point is Wikipedia, which provides ency-
clopedic coverage for a huge number of knowledge
domains (Medelyan et al 2009), but most blogs,
Web sites and newspapers also provide a great deal
of information focused on specific areas of knowl-
edge. When it comes to automatic text understand-
ing, then, it is crucial to take into account the domain
specificity of a piece of text, so as to perform a fo-
cused and as-precise-as-possible analysis which, in
its turn, can enable domain-aware applications such
as question answering and information extraction.
Domain knowledge also has the potential to improve
open-text applications such as summarization (Cey-
lan et al 2010) and machine translation (Foster et
al., 2010).
Research in Word Sense Disambiguation (Nav-
igli, 2009, WSD), the task aimed at the automatic
labeling of text with word senses, has been ori-
ented towards domain text understanding for sev-
eral years now. Many approaches have been devised,
including the identification of domain-specific pre-
dominant senses (McCarthy et al 2007; Lapata and
Keller, 2007), the development of domain resources
(Magnini and Cavaglia`, 2000; Magnini et al 2002),
their application to WSD (Gliozzo et al 2004), and
the effective use of link analysis algorithms such as
Personalized PageRank (Agirre et al 2009; Nav-
igli et al 2011). More recently, semi-supervised ap-
proaches to domain WSD have been proposed which
aim at decreasing the amount of supervision needed
to carry out the task (Khapra et al 2010).
High-performance domain WSD, however, has
been hampered by the widespread use of a general-
purpose sense inventory, i.e., WordNet (Miller et
al., 1990; Fellbaum, 1998). Unfortunately WordNet
does not contain many specialized terms, making
it difficult to use it in work on arbitrary special-
ized domains. While Wikipedia has recently been
considered a valid alternative (Mihalcea, 2007), it
is mainly focused on covering named entities and,
strictly speaking, does not contain a formal wide-
coverage sense inventory (not even in disambigua-
tion pages, which are often incomplete, especially
in the lexicographic sense).
In this paper we provide three main contributions:
? We tackle the above issues by introducing
a new framework based on the minimally-
supervised acquisition of specialized glossaries
for dozens of domains.
1411
? In turn, we use the acquired domain glossaries
as a sense inventory for domain WSD. As a re-
sult, we redefine the domain WSD task as one
of picking out the most appropriate gloss (fine-
grained setting) or domain (coarse-grained set-
ting) from a multi-domain glossary.
? We show that our framework represents a con-
siderable departure from the common usage
of a general-purpose sense inventory such as
WordNet, in that, thanks to the wide cov-
erage of domain meanings, it enables high-
performance unsupervised WSD on many do-
mains in the range of 69-80% F1.
Furthermore, our approach can be customized to
any set of domains of interest, and new senses, i.e.,
glosses, can be added at any time (either manually or
automatically) to the multi-domain sense inventory.
2 Related Work
Domain WSD has been the focus of much interest
in the last few years. An important research direc-
tion identifies distributionally similar neighbors in
raw text as cues for determining the predominant
sense of a target word by means of a semantic simi-
larity measure (McCarthy et al 2004; Koeling et al
2005; McCarthy et al 2007). Other distributional
methods include the use of a word-category cooccur-
rence matrix, where categories are coarse senses ob-
tained from an existing thesaurus (Mohammad and
Hirst, 2006), and synonym-based word occurrence
counts (Lapata and Keller, 2007). Domain-informed
methods have also been proposed which make use of
domain labels as cues for disambiguation purposes
(Gliozzo et al 2004).
Domain-driven approaches have been shown to
obtain the best performance among the unsupervised
alternatives (Strapparava et al 2004), especially
when domain kernels are coupled with a syntag-
matic one (Gliozzo et al 2005). However, their per-
formance is typically lower than supervised systems.
On the other hand, supervised systems fall short
of carrying out high-performance WSD within do-
mains, the main reason being the need for retraining
on each new specific knowledge domain. Nonethe-
less, the knowledge acquisition bottleneck can be
relieved by means of domain adaptation (Chan and
Ng, 2006; Chan and Ng, 2007; Agirre and de La-
calle, 2009) or by effectively injecting a general-
purpose corpus into a smaller domain-specific train-
ing set (Khapra et al 2010).
However, as mentioned above, most work on
domain WSD uses WordNet as a sense inven-
tory. But even if WordNet senses have been en-
riched with topically-distinctive words and concepts
(Agirre and de Lacalle, 2004; Cuadros and Rigau,
2008), manually-developed domain labels (Magnini
et al 2002), and disambiguated semantic relations
(Navigli, 2005), the main obstacle of being stuck
with an open-ended fine-grained sense inventory re-
mains. Recent results on the SPORTS and FINANCE
gold standard dataset (Koeling et al 2005) show
that domain WSD can achieve accuracy in the 50-
60% ballpark when a state-of-the-art algorithm such
as Personalized PageRank is paired with a distribu-
tional approach (Agirre et al 2009) or with seman-
tic model vectors acquired for many domains (Nav-
igli et al 2011).
In this paper, we take domain WSD to the next
level by proposing a new framework based on
the minimally-supervised acquisition of large do-
main sense inventories thanks to which high per-
formance can be attained on virtually any domain
using unsupervised algorithms. Glossary acquisi-
tion approaches in the literature are mostly fo-
cused on pattern-based definition extraction (Fujii
and Ishikawa, 2000; Hovy et al 2003; Fahmi and
Bouma, 2006, among others) and lattice-based su-
pervised models (Navigli and Velardi, 2010) start-
ing from an initial terminology, while we jointly
bootstrap the lexicon and the definitions for sev-
eral domains with minimal supervision and without
the requirement of domain-specific corpora. To do
so, we adapt bootstrapping techniques (Brin, 1998;
Agichtein and Gravano, 2000; Pasca et al 2006) to
the novel task of domain glossary acquisition from
the Web.
Approaches to domain sense modeling have al-
ready been proposed which go beyond the WordNet
sense inventory (Duan and Yates, 2010). Distinc-
tive collocations are extracted from corpora and used
as features to bootstrap a supervised WSD system.
Experiments in the biomedical domain show good
performance, however only in-domain ambiguity is
addressed. In contrast, our approach tackles cross-
1412
Figure 1: The bootstrapping process for glossary acquisition.
domain ambiguity, by working with virtually any set
of domains and minimizing the requirements by har-
vesting domain terms and definitions from the Web,
bootstrapped using a small number of seeds.
The existing approach closest to ours is that of
Huang and Riloff (2010), who devised a bootstrap-
ping approach to induce semantic class taggers from
domain text. The semantic classes are associated
with arbitrary NPs and must be established before-
hand. Our objective, instead, is to perform domain
disambiguation at the word level. To do this, we re-
define the domain WSD problem as one of selecting
the most suitable gloss from those available in our
full-fledged multi-domain glossary.
3 A Minimally-Supervised Framework for
Domain WSD
In this section we present our new framework for
performing domain WSD. The framework consists
of two phases: glossary bootstrapping (Section 3.1)
and domain WSD (Section 3.2).
3.1 Phase 1: Bootstrapping Domain Glossaries
The objective of the first phase is to acquire a multi-
domain glossary from the Web with minimal super-
vision. We initially select a set D of domains of
interest. For each individual domain d ? D we start
with an empty set of HTML patterns Pd (i.e., Pd :=
?), used for gloss harvesting. During this phase we
iteratively populate the pattern set by means of six
steps, described in the next six subsections and de-
picted in Figure 1. The final output of this phase will
be a glossary Gd consisting of domain terms and
their automatically-harvested glosses.
3.1.1 Step 1: Initial seed selection
First, given the domain d, we manually
pick out K hypernymy relation seeds Sd =
{(t1, h1), . . . , (tK , hK)}, where the pair (ti, hi)
contains a domain term ti and its generalization hi
(e.g., (firewall, security system)). The only con-
straint we impose is that the selected relations must
be distinctive for the domain d of interest. The cho-
sen hypernymy relations have to be as topical and
representative as possible for the given domain (e.g.,
(compiler, computer program) is an appropriate pair
for computer science, while (byte, unit of measure-
ment) is not, as it might cause the extraction of sev-
eral glossaries of various units and measures). Note
that this is the only human intervention in the entire
glossary acquisition process.
We now set the iteration counter k to 1 and start
the first iteration of the process (steps 2-5). After
each iteration k, we keep track of the set of glosses
Gkd, acquired during iteration k.
3.1.2 Step 2: Seed queries
For each seed pair (ti, hi), we submit the follow-
ing three queries to a Web search engine: ?ti? ?hi?
glossary1, ?ti? ?hi? definition, ?ti? ?hi?
dictionary and collect the 64 top-ranking results
for each query2. Each resulting page is a candidate
glossary for the domain d identified by our relation
seeds Sd.
3.1.3 Step 3: Pattern and glossary extraction
We initialize the glossary for iteration k as fol-
lows: Gkd := ?. Next, from each resulting page,
we harvest all the text snippets s starting with
ti and ending with hi (e.g., firewall</b> -- a
<i>security system), i.e., s = ti . . . hi. For each
such text snippet s, we perform five substeps:
a) extraction of the term/gloss separator: we
1In what follows, we use the typewriter font for key-
words and term/gloss separators.
2We use the Google AJAX API, which returns 64 results.
1413
Term Gloss Hypernym # seeds Gloss score
dynamic
packet filter
A firewall facility that monitors the state of connections and uses this
information to determine which network packets to allow through the firewall
firewall 2 0.75
peripheral Hardware that extends the capabilities of the computer, such as a printer,
modem, or scanner.
hardware 1 0.83
die An integrated circuit chip cut from a finished wafer. integrated circuit 1 0.75
constructor a method used to help create a new object and initialise its data method 0 1.00
schema In database terminology, a schema is the organization of the tables, the fields in
each table, and the relationships between fields and tables.
database 0 0.78
Table 1: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms are underlined).
start from ti and move right until we extract
the longest sequence pM of HTML tags and
non-alphanumeric characters, which we call the
term/gloss separator, between ti and the glossary
definition (e.g., ?</b> --? between ?firewall?
and ?a? in the above example);
b) gloss extraction: we expand the snippet s to
the right of hi in search of the entire gloss of
ti, i.e., until we reach a non-formatting tag el-
ement (e.g., <span>, <p>, <div>), while ig-
noring formatting elements such as <b>, <i>
and <a> which are typically included within a
definition sentence. As a result, we obtain the
sequence ti pM glosss(ti) pR, where glosss(ti)
is our gloss for seed term ti in snippet s (which
includes hi by construction) and pR is the non-
formatting HTML tag element to the right of
the extracted gloss. For example, we extend the
above definition for firewall to: ?a <i>security
system</i> for protecting against illegal entry
to a local area network.?.
c) pattern instance extraction: we extract the fol-
lowing pattern instance:
pL ti pM glosss(ti) pR,
where pL and pR are, respectively, the left bound-
ary of ti and the right boundary of glosss(ti), and
pM is the term/gloss separator extracted at step
3(a). The two boundaries pL and pR are obtained
by extracting the longest sequence of HTML
tags and non-alphanumeric characters obtained
when moving to the left of ti and the right of
glosss(ti), respectively3. For the above exam-
ple, we extract the following pattern instance:
3The minimum and maximum length of both pL and pR are
set to 4 and 50 characters, respectively, as a result of a tuning
phase described in Section 4.1.
pL = ?<p><b>?, ti = ?firewall?, pM = ?</b>
--?, glosss(ti) = ?a <i>security system</i>
for protecting against illegal entry to a local area
network.?, pR =?</p>?.
d) pattern extraction: we generalize the above pat-
tern instance to the following pattern:
pL ? pM ? pR,
i.e., we replace ti and glosss(ti) with *. In the
above example, we obtain the following pattern:
<p><b> ? </b> -- ? </p>.
Finally, we add the generalized pattern to the set
of patterns Pd, i.e., we set Pd := Pd ? {pL ?
pM ? pR}. We also add the first sentence of
the retrieved definition glosss(ti) to our glossary
Gkd, i.e., G
k
d := G
k
d ? {(ti, first(glosss(ti)))},
where first(g) returns the first sentence of gloss
g.
e) pattern matching: we look for additional pairs
of terms/glosses in the Web page containing the
snippet s by matching the page against the gen-
eralized pattern pL ? pM ? pR. We then add
to Gkd the new (term, gloss) pairs matching the
generalized pattern.
As a result of this step, we obtain a glossary Gkd
for the terms discovered at iteration k.
3.1.4 Step 4: Gloss ranking and filtering
Importantly not all the extracted definitions per-
tain to the domain of interest. In order to rank by
domain pertinence the glosses obtained at iteration
k, we define the terminology T k?11 of the terms
accumulated up until iteration k ? 1 as follows:
T k?11 :=
?k?1
i=1 T
i, where T i := {t : ?(t, g) ? Gid}.
1414
Gloss Domain
Measures undertaken to return a degraded ecosystem?s functions and values, including its hydrology, plant and. . . BIOLOGY
The renewing or repairing of a natural system so that its functions and qualities are comparable to its original. . . GEOGRAPHY
The reign of Charles II in England. ROYALTY
A goal of criminal sentencing that attempts to make the victim ?whole again.? LAW
The process and work of improving the degraded quality of the sound or image in terms of video and audio preservation. MEDIA
A process used by radio astronomers to eliminate the smoothing effect observed in radio maps that is caused by. . . PHYSICS
Table 2: Examples of glosses harvested for the term restoration.
For the base step k = 1, we define T 01 := T
1, i.e.,
we use the first-iteration terminology itself. To rank
the glosses, we first transform each acquired gloss
g to its bag-of-words representation Bag(g), which
contains all the single- and multi-word expressions
in g. We then score each gloss g by the ratio of do-
main terms found in its bag of words:
score(g) =
|Bag(g) ? T k?11 |
|Bag(g)|
. (1)
In Table 1 we show some glosses in the computer
science domain (second column, domain terms are
underlined) together with their score (last column).
Next, we use a threshold ? (tuned on a held-out do-
main, described in Section 4.1) to remove from Gkd
those glosses g whose score(g) < ?.
3.1.5 Step 5: Seed selection for next iteration
We now aim at selecting the new set of hyper-
nymy relation seeds to be used to start the next it-
eration. We perform three substeps:
a) Hypernym extraction: for each newly-acquired
term/gloss pair (t, g) ? Gkd, we automatically ex-
tract a candidate hypernym h from the textual
gloss g. To do this we use a simple unsupervised
heuristic which just selects the first term in the
gloss. More sophisticated, supervised approaches
could have been used for hypernym extraction
from glosses (Navigli and Velardi, 2010). How-
ever, note that, for the purposes of our glossary
extraction task, it is not crucial to extract ac-
curate hypernyms, but rather to harvest terms h
which are very likely to occur in the glosses of t.
We show an example of hypernym extraction for
some terms in Table 1 (we report the term in col-
umn 1, the gloss in column 2 and the hypernyms
extracted by our hypernym extraction technique
in column 3).
b) (Term, Hypernym)-ranking: we sort all the
glosses in Gkd by the number of seed terms found
in each gloss. In the case of ties (i.e., glosses with
the same number of seed terms), we further sort
the glosses by the score shown in Formula 1. We
show the number of seed terms and the scores
for some glosses in Table 1 (columns 4 and 5,
respectively), where seed terms are in bold and
domain terms (i.e., in T k?11 ) are underlined.
c) New seed selection: as new seeds we select the
(term, hypernym) pairs corresponding to the K
top-ranking glosses.
If k equals the maximum number of iterations, we
stop. Else, we increment the iteration counter (i.e.,
k := k + 1) and jump to step (2) of our glossary
bootstrapping algorithm after replacing Sd with the
new set of seeds.
The output of the glossary bootstrapping phase is
a domain glossary Gd :=
?
i=1,...,maxG
i
d, where
max is the total number of iterations.
3.1.6 Step 6: Increasing Coverage
Given the nature of Web domain glossaries one
can rarely find terms and definitions for general
terms (e.g., jurisprudence for the LAW domain). In
order to cover this gap, we apply domain filtering
(see Section 3.1.4) to all the glosses contained in a
general-purpose dictionary (we use WordNet). We
then add the surviving term/gloss pairs to Gd.
3.2 Phase 2: Domain WSD
Now that we have acquired a glossary for each do-
main in our set D, we can create a multi-domain
glossary G := {((t, g), d) : d ? D, (t, g) ? Gd}.
Our glossary G is thus a set of term/gloss pairs
for many domains. Note that one pair might indi-
vidually belong to more than one domain, as glos-
sary bootstrapping is performed separately for each
domain. In Table 2 we show an example of the
1415
glosses acquired for the term restoration. We ob-
serve that 5 out of 6 senses are not available in Word-
Net (namely: the BIOLOGY, GEOGRAPHY, LAW, ME-
DIA and PHYSICS senses). Many of them are domain-
specific meanings for the general concept of ?the
act of restoring?, with the BIOLOGY and GEOGRA-
PHY senses being very similar. However, this is a
perfectly acceptable phenomenon as any of the two
senses, i.e., glosses, would be equally valid when
disambiguating a domain text dealing with ecosys-
tem restoration.
3.2.1 Gloss-driven WSD
We redefine the task of domain WSD as one of
selecting the most suitable gloss, if one exists, for
an input term t. For instance, consider the sentence:
?He performed the restoration of heavily corrupted
images?. An appropriate option for this occurrence
would be the MEDIA sense of restoration in Table 2.
Our gloss-driven WSD paradigm has the desir-
able property of automatically providing two levels
of sense granularity: a domain, coarse-grained level,
similar in spirit to Word Domain Disambiguation
(Sanfilippo et al 2006), in which the sense inven-
tory of a term t is just the set of domains for which t
is covered (e.g., BIOLOGY, GEOGRAPHY, ROYALTY, LAW,
MEDIA, PHYSICS in the example of Table 2), and a
fine-grained level, which requires the selection of
the gloss which best describes the sense denoted
by the given word occurrence. A second desirable
property of our gloss-driven WSD paradigm is that
it relies on a flexible framework, which allows for
the bootstrapping of new domain glossaries or the
expansion of existing ones. However, while these
two properties ? i.e., double level of granularity dis-
tinctions and flexibility ? are naturally inherent in
the gloss-driven paradigm, the same cannot be said
for mainstream open-text WSD in which general-
purpose static dictionaries are typically used.
In order to evaluate our framework for domain
WSD, we propose two fully unsupervised algo-
rithms for gloss-driven domain WSD. Ideally, high
performance could be obtained using state-of-the-art
supervised WSD systems. However, in order to train
such systems, a wide-coverage sense-labeled corpus
should be available for each domain, a heavy task
which we leave to future work. Instead, our objec-
tive is to show that high-performance domain WSD
can be enabled with little effort by our framework.
3.2.2 Algorithm 1: WSD with Personalized
PageRank
Domain Glossaries as Graphs For each domain
d ? D, we create an undirected graph Nd =
(Vd, Ed) as follows: Vd is the set of concepts identi-
fied by term/gloss pairs in the domain glossary Gd,
i.e., Vd := Gd; Ed is the set of edges between pairs
of concepts, where an edge {(t, g), (t?, g?)} exists if
and only if t? is such that t? 6= t and t? occurs in the
bag of words of the gloss g of t. In other words, t is
connected to all the domain senses of words used in
its definition g.
Graph-based WSD Given an input text, for each
domain d ? D, we produce its bag of domain con-
tent words Cd = {w1, w2, . . . , wn} by perform-
ing tokenization, lemmatization and compounding
based on the lexicon of domain d. Then, given a
target word t, we use Cd \ {t} as the context to dis-
ambiguate t within the domain d. In order to carry
out domain WSD, i.e., to pick out the most suit-
able sense of t across domains, we apply a state-of-
the-art graph-based algorithm, namely Personalized
PageRank (Haveliwala, 2002, PPR), to each domain
graph Nd. PPR is a variant of the popular PageRank
algorithm (Brin and Page, 1998) in which the damp-
ing probability mass is concentrated on a selected
number of graph nodes, instead of being uniformly
distributed across all nodes. Specifically, following
Agirre and Soroa (2009) we concentrate the proba-
bility mass on the nodes (t?, g?) ? Vd for which the
term t? is a context word, i.e., t? ? Cd. Next, for each
domain d ? D, we run PPR for a given number of
iterations and obtain as output a probability distribu-
tion PPVd over the graph nodes. Finally, we select
the most suitable gloss of t as follows:
SensePPR(t) = argmax
g:?d?D,(t,g)?Vd
PPVd(t, g) (2)
where PPVd(t, g) is the PPR probability for the
term/gloss pair (t, g) and SensePPR(t) contains the
best interpretation of t across all the domains D.
3.2.3 Algorithm 2: PPR Boosted with Domain
Distribution Information
The words in a given text do not typically deal
with a single domain. Instead, they touch different
1416
ART BIOLOGY BUSINESS CHEMISTRY COMPUTING EDUCATION ENGINEERING ENVIRONMENT FOOD & DRINK GEOGRAPHY
GEOLOGY HEALTH HISTORY LANGUAGE LAW LITERATURE MATHS MEDIA METEOROLOGY MUSIC
PHILOSOPHY PHYSICS POLITICS PSYCHOLOGY RELIGION ROYALTY SPORTS TOURISM VIDEOGAMES WARFARE
Table 3: List of the 30 domains used in our experiments.
COMPUTING FOOD ENVIRONMENT BUSINESS
chip circuit timbale dish sewage waste eurobond bond
destructor method brioche bread acid rain rain asset play stock
compiler program macaroni pasta ecosystem system income stock security
html language pizza dish air monitoring sampling financial intermediary institution
firewall security system ice cream dessert global warming temperature derivative financial product
remote lan access process pasteurized milk milk fermentation decomposition arbitrage pricing theory economic theory
relational database tabular database salted butter butter attainment area area banker?s draft bill of exchange
admin console user interface prosecco wine fugitive dust matter working capital cash
Table 4: Hypernymy relation seeds used to bootstrap glossary acquisition in four of the 30 domains.
areas of knowledge which are intertwined with each
other within the discourse. For example, a text deal-
ing with VIDEOGAMES will often concern domains
such as BUSINESS, COMPUTING, SPORTS, etc. Given an
input text, we can capture its relevance for each do-
main by calculating the following domain score:
?d =
|Cd|
?
d??D |Cd? |
(3)
where, as above, Cd is the set of content words from
the input text which are covered by domain d. We
thus propose a second algorithm which synergisti-
cally combines the spreading effect of PPR with the
domain distribution information. The best sense for
a given term t is calculated as follows:
SenseDomPPR(t) = argmax
g:?d?D,(t,g)?Vd
?dPPVd(t, g)
(4)
that is, we select as the most suitable gloss for t the
one which maximizes the product of its domain rel-
evance score by its domain PPVd value. Note that
the same gloss can occur in multiple domains and
that it might obtain different scores depending on the
domain. Again, since the approach is gloss-driven,
we do not see this as a problem, but rather as a natu-
ral characteristic of our framework.
4 Experimental Setup
4.1 Domains
We selected 30 domains starting from the Wikipedia
featured articles4. We show the domain labels in Ta-
4http://en.wikipedia.org/wiki/Wikipedia:Featured articles
Table 5: Statistics on the multi-domain acquired glossary.
From the Web From WordNet From both Total
Terms 74,295 83,904 18,313 176,512
Glosses 153,920 68,731 596 223,247
ble 3 (some labels have been conveniently short-
ened, e.g., PHYSICS should read PHYSICS & ASTRON-
OMY). We manually identified 8 hypernym/hyponym
seeds for each domain, totalizing 240 seeds. We
used two criteria for selecting a seed: i) it covers a
separate segment of the domain, and ii) it has to be
specialized enough to avoid ambiguity. We show the
seeds used in four of our domains in Table 4. We
bootstrapped our glossary acquisition technique (cf.
Section 3.1) on each domain and performed 5 itera-
tions. For increasing the coverage of domain terms
we used WordNet glosses (see Section 3.1.6). As a
result, we obtained 30 domain glossaries. We also
kept aside a 31st domain, namely FASHION, which
we employed for tuning the minimum and maximum
length of both pL and pR in Section 3.1.3 and the
threshold ? used to filter out non-domain glosses in
Section 3.1.4.
In Table 5 we show the statistics for the ac-
quired multi-domain glossary by distinguishing
Web-derived and WordNet terms and glosses.
4.2 Sense Inventory
Our sense inventory is given by the 30-domain
glossary obtained as a result of our glossary boot-
strapping phase. Overall we collected 176,512 and
223,247 distinct terms and glosses, respectively,
with an important contribution from both the Web
1417
and WordNet (see Table 5). The average num-
ber of glosses per term in our inventory is 1.9 (3.6
glosses on polysemous terms). However, note that
a monosemous word in our domain sense inventory
does not necessarily make disambiguation easier,
as i) we might have missed other domain-specific
senses, ii) an uncovered, non-domain sense might fit
a word occurrence (in this case, the domain WSD
algorithms might be (wrongly) biased towards re-
turning the only possible choice if a non-zero dis-
ambiguation score is calculated for it).
In order to determine the suitability of our multi-
domain sense inventory, we compared it with the
latest version of WordNet Domains (Magnini et al
2002, WND 3.2), a well-known resource which
provides domain labels for almost 65,000 nomi-
nal WordNet synsets (we removed all the synsets
tagged with the FACTOTUM label, which indicates no
domain specificity). Since WND uses about 160
finer-grained domain labels, we manually mapped
them to our 30 labels when possible (e.g. SOCCER
and SWIMMING were mapped to SPORTS), totalizing
62,100 domain-labeled synsets.
We calculated the coverage of our sense inventory
against WND at the synset and the sense level, for
each non-FACTOTUM synset. Given a WordNet synset
S, let d =
?
s?S ds be the union of the domains ds
provided for each synonym s ? S by our sense in-
ventory (ds = ? if not present), and let d? be the do-
main labels assigned to S by WND. A synset is cov-
ered if d and d? intersect. At the sense level, instead,
we consider a synonym s ? S to be covered if ds and
d? intersect. Our synset and sense coverage is 65.9%
(40,969/62,100) and 63.7% (71,950/112,875), re-
spectively. We also calculated an extra-coverage of
203.2% (229,384/112,875), that is the fraction of do-
main senses which are not available in WND, but
we are able to provide in our sense inventory (see
e.g. the example in Table 2) over the total number of
senses in WND. While coverage and extra-coverage
provide a good indicator of the completeness of our
sense inventory, we need to calculate its precision to
determine its correctness. To do so, we randomly
sampled 500 domain glosses of terms for which no
WordNet sense was tagged with the same domain in
WND. A manual validation of this sample resulted
in an 87.0% (435/500) estimate of the precision of
our sense inventory.
4.3 Datasets
A dataset for 30 domains We used the Giga-
word corpus (Graff and Cieri, 2003) to extract a 6-
paragraph text snippet for each of the 30 domains.
As a result, we obtained a domain dataset made up
of 180 paragraphs to which we applied tokeniza-
tion, lemmatization and compounding, totaling 1432
domain content words overall (47.7 content words
per domain on average). The average polysemy of
the words in the dataset was of 9.7 glosses and 4.4
domains per word. Each content word was manu-
ally tagged with the most suitable glosses from our
multi-domain glossary (3.9 glosses, i.e., senses per
word were assigned on average). The annotation
task was performed by two annotators with adjudi-
cation.
Sports and Finance We also experimented with
the gold standard produced by Koeling et al(2005).
The dataset covers two domains: SPORTS and FI-
NANCE. The dataset comprises 41 ambiguous words
(with an average polysemy of 6.7 senses), many
of which express different meanings in the two do-
mains. In each domain, and for each word, around
100 sentences were sense-annotated with WordNet.
Environment Finally, we also carried out an ex-
periment on the ENVIRONMENT dataset from the
Semeval-2010 domain WSD task (Agirre et al
2010). The dataset includes 1,398 content words (of
which 1,032 content nouns) tagged with WordNet
senses.
4.4 Systems
We applied the two algorithms proposed in Section
3.2, namely vanilla PPR and domain-boosted PPR.
For both versions of PPR we employed UKB, a
readily-available implementation of PPR for WSD5,
successfully experimented by Agirre and Soroa
(2009) and Agirre et al(2009).
4.5 Baselines
Random baseline We compared our algorithms
with the random baseline, which associates a ran-
dom gloss among those available for each word oc-
currence according to a uniform distribution.
5http://ixa2.si.ehu.es/ukb/
1418
Predominant domain We also compared our al-
gorithms with a predominant sense baseline which
assigns to each word occurrence the domain label
with the highest domain score ?d among those avail-
able for the word (cf. Formula 3). Note that this is
a strong baseline, because it aims at identifying the
domain covered by the majority of terms in the input
text, however it can disambiguate only at a coarse-
grained level, i.e., at the domain level.
5 Experimental Results
30 domains We ran our WSD systems and the
baselines on our 30-domain dataset, on a sentence-
by-sentence basis. We calculated results at the two
levels of granularity enabled by our WSD frame-
work: a coarse-grained setting where systems out-
put the most appropriate domain label for each word
item to be disambiguated; a fine-grained setting
where systems are required to output the most suit-
able gloss for the input word. The results are shown
in Table 6. Domain PPR outperforms Vanilla PPR
by some points in precision, recall and F1 in both the
coarse-grained and the fine-grained setting, achiev-
ing an F1 around 80% and 69%, respectively (dif-
ferences in recall performance are statistically sig-
nificant using a ?2 test). The predominant domain
baseline, available only in the coarse-grained set-
ting, lags behind Domain PPR by more than 3 points
in precision and 2 in recall. While these differences
are not statistically significant, the variance across
domains is much higher, thus suggesting lower reli-
ability of the method.
These results were obtained in a fully unsuper-
vised setting in which no structured knowledge was
provided, unlike previous applications of PPR to
WSD (Agirre et al 2009; Agirre and Soroa, 2009)
which relied on the underlying WordNet graph, a
manually created resource. Furthermore, our graph
contains ?noisy? semantic relations, as we connect
each gloss to all the senses of its gloss words (cf.
Section 3.2.2). Finally, we note that the results
shown in Table 6 could never have been obtained
with WordNet. In fact, drawing on our domain map-
ping, we calculated that the correct domain sense is
not in WordNet for about 68% of the words in the
dataset. Instead, the results in Table 6 show that our
framework enables high-performance unsupervised
Coarse-grained Fine-grained
P R F1 P R F1
Vanilla PPR 76.7 74.3? 75.5 66.1 64.1? 65.1
Domain PPR 81.2 78.7? 79.9 69.7 67.6? 68.6
Predom. domain 77.9 76.8 77.3 - - -
Random baseline 42.5 42.5 42.5 44.1 44.1 44.1
Table 6: Performance results on the 30-domain dataset
(? differences between the two systems are statistically
significant using a ?2 test, p < 0.05).
WSD thanks to the wide coverage of domain mean-
ings.
As regards the random baseline, this performs
42.5% and 44.1% in the two settings. Despite the
higher polysemy of glosses (9.7 glosses vs. 4.4 do-
mains per word in the dataset), the performance is
higher in the fine-grained setting because often there
is more than one gloss covering the same meaning of
a domain word.
Sports, Finance and Environment For the
SPORTS, FINANCE and ENVIRONMENT datasets (cf. Sec-
tion 4.3) we did not have gloss-based sense annota-
tions, so we could not perform a fine-grained evalu-
ation. Therefore, we first studied the different sys-
tems at a coarse level on the basis of the domain dis-
tribution of the senses returned for the word items
in the dataset. We show the 3 most frequent domain
labels for each system and each dataset in Figure 2.
The figure seems to confirm our results showing Do-
main PPR as being more robust than its Vanilla ver-
sion. Next, to get a more accurate evaluation, we
randomly sampled 200 sentences from each dataset
and manually validated the coarse-grained senses,
i.e., domain assignments, output by each system on
this set of sentences. We remark that several words
in the datasets did not pertain to the domain of inter-
est. For instance, will and share do not have any
sports sense in WordNet, while the same applies
to half and chip for the business domain. Table 7
shows the results of our validation, where a domain
output by a system was considered correct if a suit-
able gloss existed for that domain in our inventory.
The results show that our framework enables
coarse-grained recall in the 70-80% ballpark even
on difficult gold standard datasets for which fine-
grained recall with WordNet struggles to surpass the
50-60% range. For instance, the best performance
1419
Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom.
FINANCE SPORTS ENVIRONMENT
Figure 2: Frequency of the most common domain labels returned by our 3 systems on standard domain datasets.
FINANCE SPORTS ENVIRONMENT
P R F1 P R F1 P R F1
Vanilla PPR 57.8 56.5 57.1 65.5 63.2 64.3 81.5 77.9 79.7
Domain PPR 77.8 76.1 76.9 72.1 71.3 71.7 83.1 79.4 81.2
Predom. domain 80.0 78.3 79.1 72.6 70.1 71.3 72.7 70.6 71.6
Table 7: Coarse-grained performance results on gold-standard domain datasets.
on the ENVIRONMENT dataset was around 60% re-
call (Kulkarni et al 2010) using a semi-supervised
WSD system, trained on the domain. Similarly, both
the FINANCE and SPORTS datasets are notoriously dif-
ficult gold standards on which state-of-the-art recall
using WordNet is lower than 60% (Navigli et al
2011).
Interestingly, the predominant domain baseline
shows a bias towards BUSINESS, thus performing best
on the FINANCE dataset. This is because of the large
number of terms covered in our domain glossary,
and consequently the high overlap with cue words
in context. On the other two domains, we observe
performance in line with our 30-domain experiment.
6 Conclusion
We have here presented a new framework for do-
main Word Sense Disambiguation. We depart from
the use of general-purpose sense inventories like
WordNet and propose a bootstrapping approach to
the acquisition of sense inventories for virtually any
domain. While we selected 30 domains for this
study, nothing would prevent us from using a smaller
or larger set of these domains, or a set of completely
different domains.
Our work provides three main contributions:
i) we propose a new, flexible approach to glossary
bootstrapping which harvests hundreds of thou-
sands of term/gloss pairs; the resulting multi-
domain glossary is shown to have wide cov-
erage across domains and to include a large
amount of terms not available in WordNet;
ii) we propose a novel framework for fully-
unsupervised domain WSD which uses the
multi-domain glossary as our sense inventory;
iii) we show that high performance can be achieved
by means of simple, unsupervised WSD algo-
rithms (around 80% and 69% in a coarse- and
fine-grained setting, respectively).
Note that our aim here has not been to determine
which system performs best, but rather to show that
a reliable, full-fledged framework for domain WSD
can be set up with minimal supervision. Addition-
ally, our framework can be applied to any language
of interest, provided enough glossaries are available
online, by simply translating the keywords used for
our queries.
The multi-domain glossary (and sense inven-
tory) together with the seeds used for bootstrapping
are available from http://lcl.uniroma1.
it/dwsd.
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
1420
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digi-
tal Libraries (DL 2000), pages 85?94, San Antonio,
Texas, United States.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC 2004, pages 1123?1126, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for WSD. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, EACL
2009, pages 42?50, Athens, Greece.
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL 2009, pages 33?41, Athens, Greece.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI), pages 1501?1506,
Pasadena, California.
Eneko Agirre, Oier Lo?pez de Lacalle, Christiane Fell-
baum, Shu-Kai Hsieh, Maurizio Tesconi, Monica
Monachini, Piek Vossen, and Roxanne Segers. 2010.
Semeval-2010 task 17: All-words word sense disam-
biguation on a specific domain. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 75?80, Uppsala, Sweden.
Sergey Brin and Michael Page. 1998. Anatomy of a
large-scale hypertextual web search engine. In Pro-
ceedings of the 7th Conference on World Wide Web,
WWW 2007, pages 107?117, Brisbane, Australia.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proceedings of the In-
ternational Workshop on The World Wide Web and
Databases (WebDB 1998), pages 172?183, London,
UK.
Hakan Ceylan, Rada Mihalcea, Umut O?zertem, Elena
Lloret, and Manuel Palomar. 2010. Quantifying the
limits and success of extractive summarization sys-
tems across domains. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 903?911, Los Angeles, California.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL 2006, pages 89?96, Sydney, Aus-
tralia.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, ACL 2007,
pages 49?56, Prague, Czech Republic.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the Web.
In Proceedings of the 22nd International Conference
on Computational Linguistics, COLING 2008, pages
161?168, Manchester, U.K.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceedings
of Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, NAACL 2010, pages 627?635,
Los Angeles, California, USA.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications,
pages 64?71, Trento, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2010), pages 451?
459, Cambridge, Massachusetts.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the world wide web as an encyclopedia: extracting
term descriptions from semi-structured texts. In Pro-
ceedings of the 38th Annual Meeting on Association
for Computational Linguistics, ACL 2000, pages 488?
495, Hong Kong.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2004.
Unsupervised and supervised exploitation of semantic
domains in lexical disambiguation. Computer Speech
and Language, 18(3):275?299.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL 2005, pages
403?410, Ann Arbor, Michigan.
David Graff and Christopher Cieri. 2003. English Giga-
word, LDC2003T05. In Linguistic Data Consortium,
Philadelphia.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of 11th International Conference on
World Wide Web, WWW 2002, pages 517?526, Hon-
olulu, Hawaii.
1421
Eduard Hovy, Andrew Philpot, Judith Klavans, Ulrich
Germann, and Peter T. Davis. 2003. Extending meta-
data definitions by automatically extracting and orga-
nizing glossary definitions. In Proceedings of the 2003
Annual National Conference on Digital Government
Research, pages 1?6, Boston, MA.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 275?285, Uppsala, Sweden.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. All words domain
adapted WSD: Finding a middle ground between su-
pervision and unsupervision. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1532?1541, Sweden.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 419?426, Vancouver, B.C., Canada.
Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. CFILT: Resource con-
scious approaches for all-words domain specific WSD.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (Semeval-2010), pages 421?426,
Stroudsburg, PA, USA.
Mirella Lapata and Frank Keller. 2007. An information
retrieval approach to sense ranking. In Proceedings of
Human Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL 2007, pages
348?355, Rochester, USA.
Bernardo Magnini and Gabriela Cavaglia`. 2000. In-
tegrating subject field codes into WordNet. In Pro-
ceedings of the 2nd Conference on Language Re-
sources and Evaluation, LREC 2000, pages 1413?
1418, Athens, Greece.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Language
Engineering, 8:359?373.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2004, pages 280?287, Barcelona, Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716?
754.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proceedings of Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL, pages 196?
203, Rochester, N.Y.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235?244.
Saif Mohammad and Graeme Hirst. 2006. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2006, pages 121?128, Trento, Italy.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 1318?1327, Uppsala, Sweden.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier de La-
calle, and Eneko Agirre. 2011. Two birds with one
stone: Learning semantic models for text categoriza-
tion and Word Sense Disambiguation. In Proceed-
ings of the 20th ACM Conference on Information and
Knowledge Management, CIKM 2011, pages 2317?
2320, Glasgow, UK.
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research Sym-
posium Conference (FLAIRS), 15?17 May 2005, pages
548?553, Clearwater Beach, Florida.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and searching
the world wide web of facts - step one: the one-million
fact extraction challenge. In Proceedings of the 21st
National Conference on Artificial intelligence (AAAI
2006), pages 1400?1405, Boston, MA.
Antonio Sanfilippo, Stephen Tratz, and Michelle Gre-
gory. 2006. Word domain disambiguation via word
sense disambiguation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL 2006,
pages 141?144, New York, USA.
Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano.
2004. Pattern abstraction and term similarity for
Word Sense Disambiguation: IRST at Senseval-3.
In Proceedings of the 3rd International Workshop on
the Evaluation of Systems for the Semantic Analy-
sis of Text (SENSEVAL-3), pages 229?234, Barcelona,
Spain.
1422
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 170?181,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Growing Multi-Domain Glossaries from a Few Seeds
using Probabilistic Topic Models
Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{faralli,navigli}@di.uniroma1.it
Abstract
In this paper we present a minimally-
supervised approach to the multi-domain ac-
quisition of wide-coverage glossaries. We start
from a small number of hypernymy rela-
tion seeds and bootstrap glossaries from the
Web for dozens of domains using Probabilis-
tic Topic Models. Our experiments show that
we are able to extract high-precision glos-
saries comprising thousands of terms and def-
initions.
1 Introduction
Dictionaries, thesauri and glossaries are useful
sources of information for students, scholars and ev-
eryday readers, who use them to look up words of
which they either do not know, or have forgotten,
the meaning. With the advent of the Web an increas-
ing number of dictionaries and technical glossaries
has been made available online, thereby speeding
up the definition search process. However, finding
definitions is not always immediate, especially if the
target term pertains to a specialized domain. Indeed,
not even well-known services such as Google Define
are able to provide definitions for scientific or tech-
nical terms such as taxonomy learning or distant su-
pervision in AI or figure-four leglock and suspended
surfboard in wrestling.
Domain-specific knowledge of a definitional na-
ture is not only useful for humans, it is also use-
ful for machines (Hovy et al, 2013). Examples
include Natural Language Processing tasks such as
Question Answering (Cui et al, 2007), Word Sense
Disambiguation (Duan and Yates, 2010; Faralli and
Navigli, 2012) and ontology learning (Velardi et al,
2013). Unfortunately, most of the Web dictionar-
ies and glossaries available online comprise just a
few hundred definitions, and they therefore provide
only a partial view of a domain. This is also the
case with manually compiled glossaries created by
means of collaborative efforts, such as Wikipedia.1
The coverage issue is addressed by online aggrega-
tion services such as Google Define, which bring to-
gether definitions from several online dictionaries.
However, these services do not classify textual def-
initions by domain: they just present the collected
definitions for all the possible meanings of a given
term.
In order to automatically obtain large domain
glossaries, in recent years computational approaches
have been developed which extract textual defi-
nitions from corpora (Navigli and Velardi, 2010;
Reiplinger et al, 2012) or the Web (Velardi et al,
2008; Fujii and Ishikawa, 2000). The methods in-
volving corpora start from a given set of terms (pos-
sibly automatically extracted from a domain cor-
pus) and then harvest textual definitions for these
terms from the input corpus using a supervised sys-
tem. Web-based methods, instead, extract text snip-
pets from Web pages which match pre-defined lex-
ical patterns, such as ?X is a Y?, along the lines
of Hearst (1992). These approaches typically per-
form with high precision and low recall, because
they fall short of detecting the high variability of the
syntactic structure of textual definitions. To address
the low-recall issue, recurring cue terms occurring
1See http://en.wikipedia.org/wiki/Portal:
Contents/Glossaries
170
within dictionary and encyclopedic resources can be
automatically extracted and incorporated into lexical
patterns (Saggion, 2004). However, this approach is
term-specific and does not scale to arbitrary termi-
nologies and domains.
The goal of the new approach outlined in this pa-
per is to enable the automatic harvesting of large-
scale, full-fledged domain glossaries for dozens of
domains, an outcome which should be very use-
ful for both human activities and automatic tasks.
We present ProToDoG (Probabilistic Topics for
multi-Domain Glossaries), a framework for growing
multi-domain glossaries which has three main nov-
elties:
i) minimal human supervision: a small set of
hypernymy relation seeds for each domain is
used to bootstrap the multi-domain acquisition
process;
ii) jointness: our approach harvests terms and
glosses at the same time;
iii) probabilistic topic models are leveraged for
a simultaneous, high-precision multi-domain
classification of the extracted definitions, with
substantial performance improvements over
our previous work on glossary bootstrapping,
i.e., GlossBoot (De Benedictis et al, 2013).
ProToDog is able to harvest definitions from the
Web and thus drop the requirement of large corpora
for each domain. Moreover, apart from the need to
select a few seeds, it avoids the use of training data
or manually defined sets of lexical patterns. It is thus
applicable to virtually any language of interest.
2 ProToDoG
Given a set of domains D = {d1, ..., dn}, for each
domain d ? D ProToDoG harvests a domain glos-
sary Gd containing pairs of the kind (t, g) where t
is a domain term and g is its textual definition, i.e.,
gloss. We show the pseudocode of ProToDoG in Al-
gorithm 1.
Step 1. Initial seed selection: Algorithm 1 takes
as input a set of domains D and, for each domain
d ? D, a small set of hypernymy relation seeds
Sd = {(t1, h1), . . . , (t|Sd|, h|Sd|)}, where the seed
Algorithm 1 ProToDoG
Input: the set of domains D,
a set Sd of hypernymy seeds for each domain
d ? D
Output: a multi-domain glossary G
1: k ? 1
2: repeat
3: for each domain d ? D do
4: Gkd ? ?
5: for each seed (tj , hj) ? Sd do
6: pages? webSearch(tj , hj , ?glossary?)
7: Gkd ? G
k
d ? extractGlossary(pages)
8: end for
9: end for
10: create a topic model using glossaries from previ-
ous iterations
11: infer topic assignments for iteration-k glosses
12: filter out non-domain glosses for each domain
13: for each d ? D do
14: Sd ? seedSelectionForNextIteration(Gkd)
15: end for
16: k ? k + 1
17: until k > max
18: for each domain d ? D do
19: recover filtered glosses into Gmax+1d
20: Gd ?
?
j=1,...,max+1G
j
d
21: end for
22: return G = {(Gd, d) : d ? D}
pair (tj , hj) contains a term tj and its generalization
hj (e.g., (linux, operating system)). This is the only
human input to the entire glossary acquisition pro-
cess. The selection of the input seeds plays a key
role in the bootstrapping process, in that the pattern
and gloss extraction process will be driven by them.
The chosen hypernymy relations thus have to be as
topical and representative as possible for the domain
of interest (e.g., (compiler, computer program) is an
appropriate pair for computer science, while (byte,
unit of measurement) is not, as it might cause the
extraction of out-of-domain glossaries of units and
measures).
The algorithm first sets the iteration counter k to
1 (line 1) and starts the first iteration of the glos-
sary bootstrapping process (lines 2-17), each involv-
ing steps 2-4, described below. After each iteration
k, for each domain d we keep track of the set of
glosses Gkd acquired during that iteration. After the
last iteration, we perform step (5) of gloss recovery
(lines 18-21).
171
Step 2. Web search and glossary extraction (lines
3-9): For each domain d, we first initialize the do-
main glossary for iteration k: Gkd := ? (line 4).
Then, for each seed pair (tj , hj) ? Sd, we submit
the following query to a Web search engine: ?tj?
?hj? glossary and collect the top-ranking results
for each query (line 6).2 Each resulting page is a
candidate glossary for the domain d.
We then call the extractGlossary function (line
7) which extracts terms and glosses from the re-
trieved pages as follows. From each candidate page,
we harvest all the text snippets s starting with tj and
ending with hj (e.g., ?linux</b> ? an<i>operating
system?), i.e., s = tj . . . hj . For each such text snip-
pet s, we extract the following pattern instance:
pL tj pM glosss(tj) pR,
where:
? pM is the longest sequence of HTML tags and
non-alphanumeric characters between tj and
the glossary definition (e.g., ?</b> ?? between
?linux? and ?an? in the above example);
? glosss(tj) is the gloss of tj obtained by mov-
ing to the right of pM until we reach a non-
formatting tag element (e.g., <span>, <p>,
<div>), while ignoring formatting elements
such as <b>, <i> and <a> which are typi-
cally included within a definition sentence;
? pL and pR are the longest sequences of HTML
tags on the left of tj and the right of glosss(tj),
respectively.
For instance, given the HTML snippet
?. . .<p><b>linux</b> ? an <i>operating
system</i> developed by Linus Torvalds</p>. . . ?
we extract the following pattern instance: pL =
?<p><b>?, tj = ?linux?, pM = ?</b> ??,
glosss(tj) = ?an <i>operating system</i>
developed by Linus Torvalds?, pR =?</p>?.
Then we generalize the above pattern instance by
replacing tj and glosss(tj) with *, obtaining:
pL ? pM ? pR,
For the above example, we obtain the following
pattern:
2We use the Google Ajax API, which returns the 64 top-
ranking search results.
<p><b> * </b> ? * </p>.
We add the first sentence of the retrieved gloss
glosss(tj) to our glossary Gkd, i.e., G
k
d := G
k
d ?
{(tj , first(glosss(tj)))}, where first(g) returns
the first sentence of gloss g. Finally, we look for ad-
ditional pairs of terms/glosses in the Web page con-
taining the snippet s by matching the page against
the generalized pattern pL ? pM ? pR, and adding
them to Gkd.
As a result of step (2), for each domain d ? D
we obtain a glossary Gkd for the terms discovered at
iteration k.
Step 3. Topic modeling and gloss filtering (lines
10-12): Unfortunately, not all (term, gloss) pairs
in a glossaryGkd will pertain to the domain d. For in-
stance, we might end up retrieving interdisciplinary
or even unrelated glossaries. In order to address this
fuzziness, we model domains with a Probabilistic
Topic Model (PTM) (Blei et al, 2003; Steyvers and
Griffiths, 2007). PTMs model a given text document
as a mixture of topics. In our case topics are do-
mains and we, first, create a topic model from the
domain glossaries acquired before the current iter-
ation k, then, second, use the topic model to esti-
mate the domain assignment of each new pair (term,
gloss) in our glossaries Gkd, i.e., obtained at iteration
k, third, filter out non-domain glosses.
Creating the topic model (line 10): For a given
iteration k and domain d, we first define the ter-
minology accumulated up until iteration k ? 1 for
that domain as the set T 1,k?1d :=
?k?1
j=1 T
j
d , where
T jd is the set of terms acquired at iteration j, i.e.,
T jd := {t : ?(t, g) ? G
j
d}.
3 Then we define:
? W :=
?
d?D T
1,k?1
d as the entire terminology
acquired up until iteration k?1 for all domains,
i.e., the full set of terms independently of their
domain;
? M :=
?
d?D
?k?1
j=1 G
j
d as the multi-domain
glossary acquired up until iteration k ? 1, i.e.,
the full set of pairs (term, gloss) independently
of their domain;4
3For the first iteration, i.e., when k = 1, we define T 1,0d :=
{t : ?(t, g) ? G1d}, i.e., we use the terminology resulting from
step (2) of the first iteration.
4For k = 1, M :=
?
d?D G
1
d.
172
? Two count matrices, i.e., the word-domain ma-
trix CWD and the gloss-domain matrix CMD,
such that: CWDw,d counts the number of times
w ?W is assigned to domain d ? D, i.e., it oc-
curs in the glosses of domain d; CMD(t,g),d counts
the number of words in g assigned to domain d.
At this point, as shown by Steyvers and Grif-
fiths (2007), we can estimate the probability ?(d)w for
word w, and the probability ?(t,g)d for a term/gloss
pair (t, g), of belonging to domain d:
?(d)w =
CWDw,d + ?
?|W |
w?=1 C
WD
w?,d + |W |?
; (1)
?(t,g)d =
CMD(t,g),d + ?
?|D|
d?=1 C
MD
(t,g),d? + |D|?
(2)
where ? and ? are smoothing factors.5 The two
above probabilities represent the core of our topic
model of the domain knowledge acquired up until
iteration k ? 1.
Probabilistic modeling of iteration-k glosses (line
11): We now utilize the above topic model to es-
timate the probabilities in Formulas 1 and 2 for the
newly acquired glosses at iteration k. To this end we
define M ? :=
?
d?DG
k
d as the union of the (term,
gloss) pairs at iteration k and W ? :=
?
d?D T
k
d
?
W
as the union of terms acquired at iteration k, but also
occurring in W (i.e., the entire terminology up un-
til iteration k ? 1). Then we apply Gibbs sampling
(Blei et al, 2003; Phan et al, 2008) to estimate the
probability of each pair (t, g) ? M ? of pertaining to
a domain d by computing:
?
?(t,g)
d =
RM
?D
(t,g),d + ?
?|D|
d?=1R
M ?D
(t,g),d? + |D|?
(3)
where the gloss-domain matrixRM
?D is initially de-
fined by counting random domain assignments for
each word w? in the bag of words of each (term,
gloss) pair ? M ?. Next, the domain assignment
counts in RM
?D are iteratively updated using Gibbs
sampling.6
5As experienced by Steyvers and Griffiths (2007), the values
of ? = 50/|D| and ? = 0.01 work well with many different
text collections.
6For the PTM part of ProToDoG we used the JGibbLDA
Filtering out non-domain glosses (line 12): Now,
for each domain d ? D, for each pair (t, g) ? Gkd we
have a probability ?
?(t,g)
d of belonging to d. We mark
(t, g) as a non-domain item if ?
?(t,g)
d < ?, where ? is
a confidence threshold, or if ?
?(t,g)
d is not maximum
among all domains in D. Non-domain pairs are re-
moved from Gkd and stored into a set Ad for possible
recovery after the last iteration (see step (5)).
Step 4. Seed selection for next iteration (lines
13-15): For each domain d ? D, we now select
the new set of hypernymy relation seeds to be used
to start the next iteration. First, for each newly-
acquired term/gloss pair (t, g) ? Gkd, we automat-
ically extract a candidate hypernym h from the tex-
tual gloss g. To do this we use a simple heuristic
which just selects the first content term in the gloss.7
Then we sort all the glosses in Gkd by the number of
seed terms found in each gloss. In the case of ties
(i.e., glosses with the same number of seed terms),
we further sort the glosses by ?
?(t,g)
d . Finally we se-
lect the (term, hypernym) pairs corresponding to the
|Sd| top-ranking glosses as the new set of seeds for
the next iteration.
Next, we increment k (line 16 of Algorithm 1)
and if the maximum number of iterations is reached
we jump to step (5). Otherwise, we go back to step
(2) of our glossary bootstrapping algorithm with the
new set of seeds Sd.
Step 5. Gloss recovery (line 19): After all iter-
ations, the entire multi-domain terminology W (cf.
step (3)) may contain several new terms which were
not present when a given gloss g was filtered out.
So, thanks to the last-iteration topic model, the gloss
g might come back into play because its words are
now important cues for a domain. To reassess the
domain pertinence of (term, gloss) pairs in Ad for
each d, we just reapply the entire step (3) by setting
Gmax+1d := Ad for each d ? D. As a result, we
library, a Java Implementation of Latent Dirichlet Allocation
(LDA) using Gibbs Sampling for Parameter Estimation and In-
ference, available at: http://jgibblda.sourceforge.
net/
7While more complex strategies could be devised, e.g.,
lattice-based hypernym extraction (Navigli and Velardi, 2010),
we found that this heuristic works well because, even when it is
not a hypernym, the first term acts as a cue word for the defined
term.
173
obtain an updated glossary Gmax+1d which contains
all the recovered glosses.
Final output: For each domain d ? D the final
output of ProToDoG is a domain glossary Gd :=?
j=1,...,max+1G
j
d. Finally the algorithm aggregates
all glossaries Gd into a multi-domain glossary G
(line 22).
3 Experimental Setup
3.1 Domains
For our experiments we selected 30 different do-
mains ranging from Arts to Warfare, mostly follow-
ing the domain classification of Wikipedia featured
articles (full list at http://lcl.uniroma1.
it/protodog). The set includes several techni-
cal domains, such as Chemistry, Geology, Meteorol-
ogy, Mathematics, some of which are highly inter-
disciplinary. For instance, the Environment domain
covers terms from fields such as Chemistry, Biology,
Law, Politics, etc.
3.2 Gold Standard
Since our evaluations required considerable human
effort, in what follows we calculated all perfor-
mances on a random set of 10 domains, shown in the
top row of Table 1. For each of these 10 domains we
selected well-reputed glossaries on the Web as gold
standards, including the Reuters glossary of finance,
the Utah computing glossary and many others (full
list at the above URL). We show the size of our 10
gold-standard datasets in Table 1.
3.3 Evaluation measures
We evaluated the quality of both terms and glosses,
as jointly extracted by ProToDoG.
3.3.1 Terms
For each domain we calculated coverage, extra-
coverage and precision of the acquired terms T .
Coverage is the ratio of extracted terms in T also
contained in the gold standard T? over the size of T? .
Extra-coverage is calculated as the ratio of the ad-
ditional extracted terms in T \ T? over the number of
gold standard terms T? . Finally, precision is the ra-
tio of extracted terms in T deemed to be within the
domain. To calculate precision we randomly sam-
pled 5% of the retrieved terms and asked two human
annotators to manually tag their domain pertinence
(with adjudication in case of disagreement; ? = .62,
indicating substantial agreement). Note that by ran-
domly sampling on the entire set T we calculate the
precision of both terms in T ? T? , i.e., in the gold
standard, and terms in T \ T? , i.e., not in the gold
standard, but which are not necessarily outside the
domain.
3.3.2 Glosses
We calculated the precision of the extracted
glosses as the ratio of glosses which were both well-
formed textual definitions and specific to the tar-
get domain. Precision was determined on a random
sample of 5% of the acquired glosses for each do-
main. The annotation was made by two annotators,
with ? = .675, indicating substantial agreement.
The annotators were provided with specific guide-
lines available on the ProToDoG Web site (see URL
above).
3.4 Comparison
We compared ProToDog against:
? BoW: a bag-of-words variant in which step
(3) is replaced by a simple bag-of-words scor-
ing approach which assigns a score to each
term/gloss pair (t, g) ? Gkd as follows:
score(g) =
|Bag(g) ? T 1,k?1d |
|Bag(g)|
. (4)
where Bag(g) contains all content words in
g. At iteration k, we filter out those glosses
whose score(g) < ?, where ? is a thresh-
old tuned in the same manner as ? (see Sec-
tion 3.5). This approach essentially implements
GlossBoot, our previous work on domain glos-
sary bootstrapping (De Benedictis et al, 2013).
? Wikipedia: since Wikipedia is the largest
collaborative resource, covering hundreds
of fields of knowledge, we devised a simple
heuristic for producing multi-domain glos-
saries from Wikipedia, so as to compare their
performance against our gold standards. For
each target domain we manually selected one
174
45%
50%
55%
60%
65%
70%
75%
80%
85%
 2  4  6  8  10  12  14  16  18  20
iteration
BotanyFashion
Figure 1: Harmonic mean of precision and coverage for
Botany and Fashion (tuning domains) over 20 iterations
(|Sd|=5, ?=0.03).
or more Wikipedia categories representing
the domain (for instance, Category:Arts
for Arts, Category:Business for Fi-
nance, etc.). Then, for each domain d,
we picked out all the Wikipedia pages
tagged either with the categories selected
for d or their direct subcategories (e.g.,
Category:Creative works) or sub-
subcategories (e.g., Category:Genres).
From each page we extracted a (page title,
gloss) pair, where the gloss was obtained by
extracting the first sentence of the Wikipedia
page, as done, e.g., in BabelNet (Navigli and
Ponzetto, 2012). Since subcategories might
have more parents and might thus belong to
multiple domains, we discarded pages assigned
to more than 2 domains.
3.5 Parameter tuning
In order to choose the optimal values of the parame-
ters of ProToDoG (number |Sd| of seeds per domain,
number max of iterations, and filtering threshold ?)
and BoW (? threshold) we selected two extra do-
mains, i.e., Botany and Fashion, not used in our
tests, together with the corresponding gold standard
Web glossaries.
As regards the number of seeds, we defined an
initial pool of 10 seeds for each of the two tun-
ing domains and studied the average performance
of 5 random sets of x seeds (from the initial pool),
when x = 1, 3, 5, 7, 9. As regards the number of
iterations, we explored all values between 1 and
20. Finally, for the filtering thresholds ? and ?
for ProToDoG PTM and its BoW variant, we tried
values of ? ? {0, 0.03, 0.06, . . . , 0.6} and ? ?
{0, 0.05, 0.1, . . . , 1.0}, respectively.
Given the high number of possible parameter
value configurations, we first explored the entire
search space automatically by calculating the cov-
erage of ProToDoG PTM (and BoW) with each con-
figuration against our tuning gold standards. Then
we identified as optimal candidates those ?fron-
tier? configurations for which, when moving from
a lower-coverage configuration, coverage reached a
maximum. We then calculated the precision of each
optimal candidate configuration by manually vali-
dating a 3% random sample of the resulting glos-
saries for the two tuning domains. The optimal con-
figuration for ProToDoG was |Sd| = 5, max = 5,
? = 0.03, while for BoW was ? = 0.1.
In Figure 1 we show the performance trend over
iterations for our two tuning domains when |Sd| = 5
and ? = 0.03. Performance is calculated as the
harmonic mean of precision and coverage of the ac-
quired glossary after each iteration, from 1 to 20. We
can see that after 5 iterations performance decreases
for Botany (a highly interdisciplinary domain) due
to lower precision, while it remains stable for Fash-
ion due to the lack of newly-acquired glosses.
3.6 Seed Selection
For each domain d we manually selected five seed
hypernymy relations as the seed sets Sd input to Al-
gorithm 1 (see Section 3.5). The seeds were selected
by the authors on the basis of just two conditions: i)
the seeds should cover different aspects of the do-
main and, indeed, should identify the domain im-
plicitly; ii) at least 10,000 results should be returned
by the search engine when querying it with the seeds
plus the glossary keyword (see line 6 of Algo-
rithm 1). The seed selection was not fine-tuned (i.e.,
it was not adjusted to improve performance), so it
might well be that better seeds would provide better
results (see (Kozareva and Hovy, 2010a)). However,
such a study is beyond the scope of this paper.
175
A
rt
B
us
in
es
s
C
he
m
is
tr
y
C
om
pu
ti
ng
E
nv
ir
on
m
en
t
Fo
od
L
aw
M
us
ic
P
hy
si
cs
S
po
rt
Gold t/g 394 1777 164 421 713 946 180 218 315 146
PTM t 4253 7370 2493 3412 3009 1526 1836 1647 3847 1696
g 7386 9795 3841 4186 3552 2175 4141 2729 5197 2938
BoW t 4012 7639 1174 3127 3644 1827 1773 1166 4471 1990
g 5923 8999 1414 3662 4334 2601 4024 1249 6956 3425
Wiki t,g 107.1k 48.4k 8137 32.0k 23.6k 5698 13.5k 84.1k 33.8k 267.5k
Table 1: Size of the gold standard and the automatically-acquired glossaries for 10 of the 30 selected domains (t:
number of terms, g: number of glosses).
4 Results and Discussion
4.1 Terms
The size of the extracted terminologies for the 10 do-
mains after five iterations is reported in Table 1 (the
output for all 30 domains is available at the above
URL, cf. Section 3.1). ProToDoG PTM and its BoW
variant extract thousands of terms and glosses for
each domain, whereas the number of glosses ob-
tained from Wikipedia (cf. Section 3.4) varies de-
pending upon the domain, from thousands to hun-
dreds of thousands. Note that there is no overlap
between the glossaries extracted by ProToDoG and
the set of Wikipedia articles, since the latter are not
organized as glossaries.
In Table 2 we show the percentage results in
terms of precision (P), coverage (C), and extra-
coverage (X, see Section 3.3 for definitions) for
ProToDoG PTM and its BoW variant and for the
Wikipedia glossary. With the exception of the
Food domain, ProToDoG achieves the best pre-
cision. The Wikipedia glossary has fluctuating
precision values, ranging between 25% and 90%,
due to the heterogeneous nature of subcategories.
ProToDog achieves the best coverage of gold stan-
dard terms on 6 of the 10 domains, with the BoW
variant obtaining slightly higher coverage on 3 do-
mains and +10% on the Food domain. The cov-
erage of Wikipedia glossaries, instead, with the
sole exception of Sport, is much lower, despite the
use of (sub)subcategories (cf. Section 3.4). Both
ProToDoG PTM and BoW achieve very high extra-
coverage percentages, meaning that they are able to
go substantially beyond our domain gold standards,
but it is the Wikipedia glossary which achieves the
highest extra-coverage values. To get a better in-
sight into the quality of extra-coverage we calcu-
lated the percentage of named entities (i.e., encyclo-
pedic) among the terms extracted by each of the dif-
ferent approaches. Comparing results across the (E)
columns of Table 2 it can be seen that high percent-
ages of the terms extracted by Wikipedia are named
entities, which is in marked contrast to the 0%-1%
extracted by ProToDog. This is as should be ex-
pected for an encyclopedia, whose coverage focuses
on people, places, brands, etc. rather than concepts.
To summarize, ProToDoG PTM outperforms both
BoW and Wikipedia in terms of precision, while
at the same time achieving both competitive cov-
erage and extra-coverage. The Wikipedia glossary
suffers from fluctuating precision values across do-
mains and overly encyclopedic coverage of terms.
4.2 Glosses
We show the results of gloss evaluation in Table 2
(last two columns) for ProToDoG PTM and BoW
(we do not report the precision values for Wikipedia,
as they are slightly lower than those obtained for
terms). Precision ranges between 89% and 99%
for ProToDoG PTM and between 82% and 97%
for BoW. We observe that these results are strongly
correlated with the precision of the extracted terms
(cf. Table 2), because the retrieved glosses of do-
main terms are usually in-domain too, and follow a
definitional style since they come from glossaries.
Note, however, that the gloss precision could also be
176
terms glosses
PTM BoW Wiki PTM BoW
P C X E P C X E P C X E P P
Art 92 26 1053 1 86 25 992 0 81 19 23.4k 67 93 87
Business 95 41 374 0 90 43 387 0 37 15 2692 31 96 91
Chemistry 99 77 1410 0 95 73 643 0 49 18 12.9k 3 98 96
Computing 95 43 767 0 93 40 702 0 81 30 7506 36 96 94
Environment 91 29 393 0 84 28 482 0 25 9 3302 12 89 82
Food 91 21 1404 0 97 31 1621 0 81 9 3997 25 92 95
Law 98 89 931 0 95 87 897 0 35 34 7406 16 99 97
Music 94 98 660 0 93 84 453 0 90 50 37.1k 84 96 95
Physics 97 43 1178 0 91 46 1373 0 68 25 10.6k 10 95 89
Sport 98 22 1139 1 96 23 1339 1 87 44 178.2k 83 97 96
Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
A
rt
B
us
in
es
s
C
he
m
is
tr
y
C
om
pu
tin
g
E
nv
ir
on
m
en
t
Fo
od
L
aw
M
us
ic
Ph
ys
ic
s
Sp
or
t
Google Define 76 80 93 86 88 91 96 96 98 84
ProToDoG 27 41 81 40 37 19 85 98 47 27
Table 3: Number of domain glosses (from a random sam-
ple of 100 gold standard terms per domain) retrieved us-
ing Google Define and ProToDoG.
higher than term precision, thanks to many pertinent
glosses being extracted for the same term (cf. Table
1).
In Table 4 we show an excerpt of the multi-
domain glossary extracted by ProToDoG for the Art,
Business and Sport domains.
5 Comparative Evaluation
5.1 Comparison with Google Define
We performed a comparison with Google Define,8
a state-of-the-art definition search service. This
service inputs a term query and outputs a list of
glosses. First, we randomly sampled 100 terms
from our gold standard for each domain. Next, for
each domain, we manually calculated the fraction
of terms for which at least one in-domain defini-
tion was provided by Google Define and ProToDoG.
8Accessible from Google search with the define: key-
word.
Table 3 shows the coverage results. In this exper-
iment, Google Define outperforms our system on
9 of the 10 analyzed domains. However, we note
that when searching for domain-specific knowledge
only, Google Define: i) needs to know the domain
term to be defined in advance, while ProToDoG
jointly acquires domain terms and glosses starting
from just a few seeds; ii) does not discriminate be-
tween glosses pertaining to the target domain and
glosses pertaining to other fields or senses, whereas
ProToDog extracts terms and glosses specific to each
domain of interest.
5.2 Comparison with TaxoLearn
We also compared ProToDoG with the output of
a state-of-the-art taxonomy learning framework,
called TaxoLearn (Navigli et al, 2011). We did
this because i) TaxoLearn extracts terms and glosses
from domain corpora in order to create a domain tax-
onomy; ii) it is one of the few systems which extracts
both terms and glosses from specialized corpora; iii)
the extracted glossaries are available online.9 There-
fore we compared the performance of ProToDoG on
two domains for which glossaries were extracted by
TaxoLearn, i.e. AI and Finance. The glossaries were
harvested from large collections of scholarly arti-
cles. For ProToDoG we selected 10 seeds to cover
all the fields of AI, while for the financial domain
we selected the same 5 seeds used in the Business
9http://ontolearn.org and http://lcl.
uniroma1.it/taxolearn
177
Art
rock art includes pictographs (designs painted on stone surfaces) and petroglyphs (designs
pecked or incised on stone surfaces).
impressionism Late 19th-century French school dedicated to defining transitory visual impressions
painted directly from nature, with light and color of primary importance.
point Regarding paper, a unit of thickness equating 1/1000 inch.
Business
hyperinflation Extremely rapid or out of control inflation.
interbank rate The rate of interest charged by a bank on a loan to another bank.
points Amount of discount on a mortgage loan stated as a percentage; one point equals one
percent of the face amount of the loan; a discount of one point raises the net yield on
the loan by one-eighth of one percent.
Sport
gross score The actual number of strokes taken by a player for hole or round before the player?s
handicap is deducted.
obstructing preventing the opponent from going around a player by standing in the path of move-
ment.
points a team statistic indicating its degree of success, calculated as follows: 2 points for a
win (3 in the 1994 World Cup), 1 point for a tie, 0 points for a loss.
Table 4: An excerpt of the resulting multi-domain glossary obtained with ProToDoG.
domain of our experiments above (cf. Section 3).
We show the number of extracted terms and
glosses for ProToDoG and TaxoLearn in Table 5.
We also show the precision values calculated on a
random sample of 5% of terms and glosses. As can
be clearly seen, on both domains ProToDoG extracts
a number of terms and glosses which is an order
of magnitude greater than those obtained by Tax-
oLearn, while at the same time obtaining consider-
ably higher precision.
6 Related Work
Current approaches to automatic glossary acquisi-
tion suffer from two main issues: i) the poor avail-
ability of large domain-specific corpora from which
terms and glosses are extracted at different times;
ii) the focus on individual domains. ProToDog ad-
dresses both issues by providing a joint multi-
domain approach to term and glossary extraction.
Among the approaches which extract unre-
stricted textual definitions from open text, Fujii and
Ishikawa (2000) determine the definitional nature of
text fragments by using an n-gram model, whereas
Klavans and Muresan (2001) apply pattern match-
ing techniques at the lexical level guided by cue
phrases such as ?is called? and ?is defined as?.
More recently, a domain-independent supervised ap-
proach, named Word-Class Lattices (WCLs), was
presented which learns lattice-based definition clas-
sifiers applied to candidate sentences containing the
input terms (Navigli and Velardi, 2010). To avoid
the burden of manually creating a training dataset,
definitional patterns can be extracted automatically.
Faralli and Navigli (2013) utilized Wikipedia as
a huge source of definitions and simple, yet ef-
fective heuristics to automatically annotate them.
Reiplinger et al (2012) experimented with two dif-
ferent approaches for the acquisition of lexical-
syntactic patterns. The first approach bootstraps pat-
terns from a domain corpus and then manually re-
fines the acquired patterns. The second approach, in-
stead, automatically acquires definitional sentences
by using a more sophisticated syntactic and seman-
tic processing. The results show high precision
in both cases. However, all the above approaches
need large domain corpora, the poor availability of
which hampers the creation of wide-coverage glos-
saries for several domains. To avoid the need to
use a large corpus, domain terminologies can be ob-
tained by using Doubly-Anchored Patterns (DAPs)
178
AI Finance
# terms P # glosses P # terms P # glosses P
ProToDoG 4983 83% 5326 84% 7370 95% 9795 96%
TaxoLearn 427 77% 834 79% 2348 86% 1064 88%
Table 5: Number and precision of terms and glosses extracted by ProToDoG and TaxoLearn in the Artificial Intelli-
gence (AI) and Finance domains.
which, given a (term, hypernym) pair, extract from
the Web sentences matching manually-defined pat-
terns like ?<hypernym> such as <term>, and *?
(Kozareva and Hovy, 2010b). This term extrac-
tion process is further extended by harvesting new
hypernyms using the corresponding inverse pat-
terns (called DAP?1) like ?* such as <term1>, and
<term2>?. Similarly to ProToDoG, this approach
drops the requirement of a domain corpus and starts
from a small number of (term, hypernym) seeds.
However, while DAPs have proven useful in the in-
duction of domain taxonomies (Kozareva and Hovy,
2010b), they cannot be applied to the glossary learn-
ing task because the extracted sentences are not for-
mal definitions. In contrast, ProToDoG performs
the novel task of multi-domain glossary acquisition
from the Web by bootstrapping the extraction pro-
cess with a few (term, hypernym) seeds. Bootstrap-
ping techniques (Brin, 1998; Agichtein and Gra-
vano, 2000; Pas?ca et al, 2006) have been success-
fully applied to several tasks, including learning se-
mantic relations (Pantel and Pennacchiotti, 2006),
extracting surface text patterns for open-domain
question answering (Ravichandran and Hovy, 2002),
semantic tagging (Huang and Riloff, 2010) and un-
supervised Word Sense Disambiguation (Yarowsky,
1995). ProToDoG synergistically integrates boot-
strapping with probabilistic topic models so as to
keep the glossary acquisition process within the tar-
get domains as much as possible.
7 Conclusions
In this paper we have presented ProToDoG, a new,
minimally-supervised approach to multi-domain
glossary acquisition. Starting from a small set of hy-
pernymy seeds which identify each domain of inter-
est, we apply a bootstrapping approach which itera-
tively obtains generalized patterns from Web glos-
saries and then applies them to the extraction of
term/gloss pairs. To our knowledge, ProToDoG is
the first approach to large-scale probabilistic glos-
sary learning which jointly acquires thousands of
terms and glosses for dozens of domains with mini-
mal supervision.
At the core of ProToDoG lies our glossary boot-
strapping approach, thanks to which we can drop
the requirements of existing techniques such as the
ready availability of domain corpora, which often do
not contain enough definitions (cf. Table 5), and the
manual definition of lexical patterns, which typically
extract sentence snippets instead of formal glosses.
ProToDoG will be made available to the re-
search community. Beyond the immediate usabil-
ity of the output glossaries (we show an excerpt
in Table 4), we also wish to show the benefit
of ProToDoG in gloss-driven approaches to taxon-
omy learning (Navigli et al, 2011; Velardi et al,
2013) and Word Sense Disambiguation (Duan and
Yates, 2010; Faralli and Navigli, 2012). The 30-
domain glossaries and gold standards created for
our experiments are available from http://lcl.
uniroma1.it/protodog.
We remark that the terminologies covered with
ProToDoG are not only precise, but are also one
order of magnitude greater than those covered in
individual online glossaries. As future work, we
plan to study the ability of ProToDoG to acquire
domain glossaries at different levels of specificity
(i.e., domains vs. subdomains). Finally, we will
adapt ProToDoG to other languages, by translating
the glossary keyword used in step (2), along the
lines of (De Benedictis et al, 2013).
Acknowledgments
The authors gratefully acknowledge
the support of the ?MultiJEDI? ERC
Starting Grant No. 259234.
179
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the 5th ACM conference on Digital
Libraries, pages 85?94, San Antonio, Texas, USA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Sergey Brin. 1998. Extracting patterns and relations
from the World Wide Web. In Proceedings of the
International Workshop on The World Wide Web and
Databases, pages 172?183, London, UK.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft
pattern matching models for definitional question an-
swering. ACM Transactions on Information Systems,
25(2):8.
Flavio De Benedictis, Stefano Faralli, and Roberto Nav-
igli. 2013. GlossBoot: Bootstrapping Multilingual
Domain Glossaries from the Web. In Proceedings of
the 51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 528?538, Sofia, Bulgaria.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceedings
of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 627?
635, Los Angeles, CA, USA.
Stefano Faralli and Roberto Navigli. 2012. A New
Minimally-supervised Framework for Domain Word
Sense Disambiguation. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 1411?1422, Jeju, Korea.
Stefano Faralli and Roberto Navigli. 2013. A Java
Framework for Multilingual Definition and Hypernym
Extraction. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics, Sys-
tem Demonstrations, pages 103?108, Sofia, Bulgaria.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: extracting
term descriptions from semi-structured texts. In Pro-
ceedings of the 38th Annual Meeting on Association
for Computational Linguistics, pages 488?495, Hong
Kong.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 539?545, Nantes, France.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-structured
content and Artificial Intelligence: The story so far.
Artificial Intelligence, 194:2?27.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
275?285, Uppsala, Sweden.
Judith Klavans and Smaranda Muresan. 2001. Evalu-
ation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of the American
Medical Informatics Association (AMIA) Symposium,
pages 324?328, Washington, D.C., USA.
Zornitsa Kozareva and Eduard H. Hovy. 2010a. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Proceedings of Human Language Tech-
nologies: The 11th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 618?626, Los Angeles, Cali-
fornia, USA.
Zornitsa Kozareva and Eduard H. Hovy. 2010b. A semi-
supervised method to learn and construct taxonomies
using the web. In Proceedings of Empirical Methods
in Natural Language Processing, pages 1110?1118,
Cambridge, MA, USA.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318?1327, Uppsala, Sweden.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In Proceedings of the
22th International Joint Conference on Artificial Intel-
ligence, pages 1872?1877, Barcelona, Spain.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the Web: Fact extraction in the fast lane. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 809?
816, Sydney, Australia.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING-ACL), Syd-
ney, Australia, pages 113?120, Sydney, Australia.
Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu
Horiguchi. 2008. Learning to classify short and sparse
text & web with hidden topics from large-scale data
collections. In Proceedings of the 17th international
180
conference on World Wide Web, WWW ?08, pages 91?
100, New York, NY, USA.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering sys-
tem. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pages 41?
47, Philadelphia, PA, USA.
Melanie Reiplinger, Ulrich Scha?fer, and Magdalena Wol-
ska. 2012. Extracting glossary sentences from schol-
arly articles: A comparative evaluation of pattern
bootstrapping and deep analysis. In Proceedings of
the ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries, pages 55?65, Jeju Island, Korea.
Horacio Saggion. 2004. Identifying definitions in text
collections for question answering. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, pages 1927?1930, Lisbon,
Portugal.
Mark Steyvers and Tom Griffiths, 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.
2008. Mining the web to create specialized glossaries.
IEEE Intelligent Systems, 23(5):18?25.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algorithm
for taxonomy induction. Computational Linguistics,
39(3):665?707.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA, USA.
181
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 414?423,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Knowledge-based Representation
for Cross-Language Document Retrieval and Categorization
Marc Franco-Salvador
1,2
, Paolo Rosso
2
and Roberto Navigli
1
1
Department of Computer Science
Sapienza Universit`a di Roma, Italy
{francosalvador,navigli}@di.uniroma1.it
2
Natural Language Engineering Lab - PRHLT Research Center
Universitat Polit`ecnica de Val`encia, Spain
{mfranco,prosso}@dsic.upv.es
Abstract
Current approaches to cross-language doc-
ument retrieval and categorization are
based on discriminative methods which
represent documents in a low-dimensional
vector space. In this paper we pro-
pose a shift from the supervised to the
knowledge-based paradigm and provide a
document similarity measure which draws
on BabelNet, a large multilingual knowl-
edge resource. Our experiments show
state-of-the-art results in cross-lingual
document retrieval and categorization.
1 Introduction
The huge amount of text that is available on-
line is becoming ever increasingly multilingual,
providing an additional wealth of useful informa-
tion. Most of this information, however, is not eas-
ily accessible to the majority of users because of
language barriers which hamper the cross-lingual
search and retrieval of knowledge.
Today?s search engines would benefit greatly
from effective techniques for the cross-lingual re-
trieval of valuable information that can satisfy
a user?s needs by not only providing (Landauer
and Littman, 1994) and translating (Munteanu and
Marcu, 2005) relevant results into different lan-
guages, but also by reranking the results in a lan-
guage of interest on the basis of the importance of
search results in other languages.
Vector-based models are typically used in the
literature for representing documents both in
monolingual and cross-lingual settings (Manning
et al., 2008). However, because of the large size
of the vocabulary, having each term as a compo-
nent of the vector makes the document represen-
tation very sparse. To address this issue several
approaches to dimensionality reduction have been
proposed, such as Principal Component Analysis
(Jolliffe, 1986), Latent Semantic Indexing (Hull,
1994), Latent Dirichlet Allocation (LDA) (Blei et
al., 2003) and variants thereof, which project these
vectors into a lower-dimensional vector space. In
order to enable multilinguality, the vectors of com-
parable documents written in different languages
are concatenated, making up the document ma-
trix which is then reduced using linear projection
(Platt et al., 2010; Yih et al., 2011). However, to
do so, comparable documents are needed as train-
ing. Additionally, the lower dimensional represen-
tations are not of easy interpretation.
The availability of wide-coverage lexical
knowledge resources extracted automatically
from Wikipedia, such as DBPedia (Bizer et al.,
2009), YAGO (Hoffart et al., 2013) and BabelNet
(Navigli and Ponzetto, 2012a), has considerably
boosted research in several areas, especially where
multilinguality is a concern (Hovy et al., 2013).
Among these latter are cross-language plagiarism
detection (Potthast et al., 2011; Franco-Salvador
et al., 2013), multilingual semantic relatedness
(Navigli and Ponzetto, 2012b; Nastase and
Strube, 2013) and semantic alignment (Navigli
and Ponzetto, 2012a; Matuschek and Gurevych,
2013). One main advantage of knowledge-based
methods is that they provide a human-readable,
semantically interconnected, representation of
the textual item at hand (be it a sentence or a
document).
Following this trend, in this paper we provide
a knowledge-based representation of documents
which goes beyond the lexical surface of text,
while at the same time avoiding the need for train-
ing in a cross-language setting. To achieve this
we leverage a multilingual semantic network, i.e.,
BabelNet, to obtain language-independent repre-
sentations, which contain concepts together with
semantic relations between them, and also include
semantic knowledge which is just implied by the
input text. The integration of our multilingual
graph model with a vector representation enables
us to obtain state-of-the-art results in comparable
414
document retrieval and cross-language text cate-
gorization.
2 Related Work
The mainstream representation of documents
for monolingual and cross-lingual document re-
trieval is vector-based. A document vector, whose
components quantify the relevance of each term in
the document, is usually highly dimensional, be-
cause of the variety of terms used in a document
collection. As a consequence, the resulting docu-
ment matrices are very sparse. To address the data
sparsity issue, several approaches to the reduc-
tion of dimensionality of document vectors have
been proposed in the literature. A popular class of
methods is based on linear projection, which pro-
vides a low-dimensional mapping from a high di-
mensional vector space. A historical approach to
linear projection is Principal Component Analysis
(PCA) (Jolliffe, 1986), which performs a singular
value decomposition (SVD) on a document matrix
D of size n?m, where each row in D is the term
vector representation of a document. PCA uses
an orthogonal transformation to convert a set of
observations of possibly correlated variables into
a set of values of linearly uncorrelated variables
called principal components, which make up the
low-dimensional vector. Latent Semantic Analy-
sis (LSA) (Deerwester et al., 1990) is very simi-
lar to PCA but performs the SVD using the cor-
relation matrix instead of the covariance matrix,
which implies a lower computational cost. LSA
preserves the amount of variance in an eigenvector
~v by maximizing its Rayleigh ratio:
~v
T
C~v
~v
T
~v
, where
C = D
T
D is the correlation matrix of D.
A generalization of PCA, called Oriented Prin-
cipal Component Analysis (OPCA) (Diamantaras
and Kung, 1996), is based on a noise covari-
ance matrix to project the similar components of
D closely. Other projection models such as La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
are based on the extraction of generative models
from documents. Another approach, named Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007), represents each document by
its similarities to a document collection. Using a
low domain specificity document collection such
as Wikipedia, the model has proven to obtain com-
petitive results.
Not only have these methods proven to be suc-
cessful in a monolingual scenario (Deerwester
et al., 1990; Hull, 1994), but they have also
been adapted to perform well in tasks at a cross-
language level (Potthast et al., 2008; Platt et al.,
2010; Yih et al., 2011). Cross-language Latent Se-
mantic Indexing (CL-LSI) (Dumais et al., 1997)
was the first linear projection approach used in
cross-lingual tasks. CL-LSI provides a cross-
lingual representation for documents by reducing
the dimensionality of a matrix D whose rows are
obtained by concatenating comparable documents
from different languages. Similarly, PCA and
OPCA can be adapted to a multilingual setting.
LDA was also adapted to perform in a multilingual
scenario with models such as Polylingual Topic
Models (Mimno et al., 2009), Joint Probabilistic
LSA and Coupled Probabilistic LSA (Platt et al.,
2010), which, however, are constrained to using
word counts, instead of better weighting strate-
gies, such as log(tf)-idf, known to perform bet-
ter with large vocabularies (Salton and McGill,
1986). Another variant, named Canonical Cor-
relation Analysis (CCA) (Thompson, 2005), uses
a cross-covariance matrix of the low-dimensional
vectors to find the projections. Cross-language
Explicit Semantic Analysis (CL-ESA) (Potthast et
al., 2008; Cimiano et al., 2009; Potthast et al.,
2011), instead, adapts ESA to be used at cross-
language level by exploiting the comparable doc-
uments across languages from Wikipedia. CL-
ESA represents each document written in a lan-
guage L by its similarities with a document collec-
tion in the same language L. Using a multilingual
document collection with comparable documents
across languages, the resulting vectors from dif-
ferent languages can be compared directly.
An alternative unsupervised approach, Cross-
language Character n-Grams (CL-CNG) (Mc-
namee and Mayfield, 2004), does not draw upon
linear projections and represents documents as
vectors of character n-grams. It has proven to ob-
tain good results in cross-language document re-
trieval (Potthast et al., 2011) between languages
with lexical and syntactic similarities.
Recently, a novel supervised linear projec-
tion model based on Siamese Neural Networks
(S2Net) (Yih et al., 2011) achieved state-of-the-
art performance in comparable document retrieval.
S2Net performs a linear combination of the terms
of a document vector
~
d to obtain a reduced vector
~r, which is the output layer of a neural network.
Each element in ~r has a weight which is a linear
combination of the original weights of
~
d, and cap-
tures relationships between the original terms.
However, linear projection approaches need a
high number of training documents to achieve
state-of-the-art performance (Platt et al., 2010;
Yih et al., 2011). Moreover, although they are
good at identifying a few principal components,
415
the representations produced are opaque, in that
they cannot explicitly model the semantic content
of documents with a human-interpretable repre-
sentation, thereby making the data analysis diffi-
cult. In this paper, instead, we propose a language-
independent knowledge graph representation for
documents which is obtained from a large multi-
lingual semantic network, without using any train-
ing information. Our knowledge graph represen-
tation explicitly models the semantics of the docu-
ment in terms of the concepts and relations evoked
by its co-occurring terms.
3 A Knowledge-based Document
Representation
We propose a knowledge-based document rep-
resentation aimed at expanding the terms in a doc-
ument?s bag of words by means of a knowledge
graph which provides concepts and semantic rela-
tions between them. Key to our approach is the
use of a graph representation which does not de-
pend on any given language, but, indeed, is multi-
lingual. To build knowledge graphs of this kind we
utilize BabelNet, a multilingual semantic network
that we present in Section 3.1. Then, in Section
3.2, we describe the five steps needed to obtain our
graph-based multilingual representation of docu-
ments. Finally, we introduce our knowledge graph
similarity measure in Section 3.3.
3.1 BabelNet
BabelNet (Navigli and Ponzetto, 2012a) is a
multilingual semantic network whose concepts
and relations are obtained from the largest avail-
able semantic lexicon of English, WordNet (Fell-
baum, 1998), and the largest wide-coverage
collaboratively-edited encyclopedia, Wikipedia,
by means of an automatic mapping algorithm. Ba-
belNet is therefore a multilingual ?encyclopedic
dictionary? that combines lexicographic informa-
tion with wide-coverage encyclopedic knowledge.
Concepts in BabelNet are represented similarly to
WordNet, i.e., by grouping sets of synonyms in
the different languages into multilingual synsets.
Multilingual synsets contain lexicalizations from
WordNet synsets, the corresponding Wikipedia
pages and additional translations output by a sta-
tistical machine translation system. The relations
between synsets are collected from WordNet and
from Wikipedia?s hyperlinks between pages.
We note that, in principle, we could use any
multilingual network providing a similar kind of
information, e.g., EuroWordNet (Vossen, 2004).
However, in our work we chose BabelNet be-
cause of its larger size, its coverage of both lex-
icographic and encyclopedic knowledge, and its
free availability.
1
In our work we used BabelNet
1.0, which encodes knowledge for six languages,
namely: Catalan, English, French, German, Italian
and Spanish.
3.2 From Document to Knowledge Graph
We now introduce our five-step method for repre-
senting a given document d from a collection D of
documents written in language L as a language-
independent knowledge graph.
Building a Basic Vector Representation Ini-
tially we transform a document d into a traditional
vector representation. To do this, we score each
term t
i
? d with a weight w
i
. This weight is usu-
ally a function of term and document frequency.
Following the literature, one method that works
well is the log tf-idf weighting (Salton et al., 1983;
Salton and McGill, 1986):
w
i
= log
2
(f
i
+ 1)log
2
(n/n
i
). (1)
where f
i
is the number of times term i occurs in
document d, n is the total number of documents in
the collection and n
i
is the number of documents
that contain t
i
. We then create a weighted term
vector ~v = (w
1
, ..., w
n
), where w
i
is the weight
corresponding to term t
i
. We exclude stopwords
from the vector.
Selecting the Relevant Document Terms We
then create the set T of base forms, i.e., lemmas
2
,
of the terms in the document d. In order to keep
only the most relevant terms, we sort the terms T
according to their weight in vector ~v and retain a
maximum number of K terms, obtaining a set of
terms T
K
.
3
The value of K is calculated as a func-
tion of the vector size, as follows:
K = (log
2
(1 + |~v|))
2
, (2)
The rationale is that K must be high enough to
ensure a good conceptual representation but not
too high, so as to avoid as much noise as possi-
ble in the set T
K
.
Populating the Graph with Initial Concepts
Next, we create an initially-empty knowledge
graph G = (V,E), i.e., such that V = E = ?.
We populate the vertex set V with the set S
K
of
all the synsets in BabelNet which contain any term
in T
K
in the document language L, that is:
1
http://babelnet.org
2
Following the setup of (Platt et al., 2010), our initial data
is represented using term vectors. For this reason we lemma-
tize in this step.
3
Since the vector ~v provides weights for all the word
forms, and not only lemmas, occurring in d, we take the best
weight among those word forms of the considered lemma.
416
Figure 1: (a) initial graph from T
K
= {?European?, ?apple?, ?tree?, ?Malus?, ?species?, ?America?}; (b)
knowledge graph obtained by retrieving all paths from BabelNet. Gray nodes are the original concepts.
S
K
=
?
t?T
K
Synsets
L
(t), (3)
where Synsets
L
(t) is the set of synsets in Ba-
belNet which contain a term t in the language
of interest L. For example, in Figure 1(a) we
show the initial graph obtained from the set T
K
=
{?European?, ?apple?, ?tree?, ?Malus?, ?species?,
?America?}. Note, however, that each retrieved
synset is multilingual, i.e., it contains lexicaliza-
tions for the same concept in other languages too.
Therefore, the nodes of our knowledge graph pro-
vide a language-independent representation of the
document?s content.
Creating the Knowledge Graph Similarly to
Navigli and Lapata (2010), we create the knowl-
edge graph by searching BabelNet for paths con-
necting pairs of synsets in V . Formally, for each
pair v, v
?
? V such that v and v
?
do not share any
lexicalization
4
in T
K
, for each path in BabelNet
v ? v
1
? . . . ? v
n
? v
?
, we set: V := V ?
{v
1
, . . . , v
n
} and E := E?{(v, v
1
), . . . , (v
n
, v
?
)},
that is, we add all the path vertices and edges to
G. After prototyping, the path length is limited
to maximum length 3, so as to avoid an excessive
semantic drift.
As a result of populating the graph with inter-
mediate edges and vertices, we obtain a knowl-
edge graph which models the semantic context of
document d. We point out that our knowledge
graph might have different isolated components.
We view each component as a different interpreta-
tion of document d. To select the main interpre-
tation, we keep only the largest component, i.e.,
the one with the highest number of vertices, which
we consider as the most likely semantic represen-
tation of the document content.
Figure 1(b) shows the knowledge graph ob-
tained for our example term set. Note that our
approach retains, and therefore weights, only the
subgraph focused on the ?apple fruit? meaning.
4
This prevents different senses of the same term from be-
ing connected via a path in the resulting knowledge graph.
Knowledge Graph Weighting The final step
consists of weighting all the concepts and se-
mantic relations of the knowledge graph G. For
weighting relations we use the original weights
from BabelNet, which provide the degree of re-
latedness between the synset end points of each
edge (Navigli and Ponzetto, 2012a). As for con-
cepts, we weight them on the basis of the origi-
nal weights of the terms in the vector ~v. In or-
der to score each concept in our knowledge graph
G, we applied the topic-sensitive PageRank al-
gorithm (Haveliwala et al., 2003) to G. While
the well-known PageRank algorithm (Page et al.,
1998) calculates the global importance of vertices
in a graph, topic-sensitive PageRank is a variant
in which the importance of vertices is biased us-
ing a set of representative ?topics?. Formally, the
topic-sensitive PageRank vector ~p is calculated by
means of an iterative process until convergence as
follows: ~p = cM~p+(1?c)~u, where c is the damp-
ing factor (conventionally set to 0.85), 1? c repre-
sents the probability of a surfer randomly jumping
to any node in the graph, M is the transition proba-
bility matrix of graph G, with M
ji
= degree(i)
?1
if an edge from i to j exists, 0 otherwise, ~u is
the random-jumping transition probability vector,
where each u
i
represents the probability of jump-
ing randomly to the node i, and ~p is the resulting
PageRank vector which scores the nodes of G. In
contrast to vanilla PageRank, the ?topic-sensitive?
variant gives more probability mass to some nodes
in G and less to others. In our case we perturbate
~u by concentrating the probability mass to the ver-
tices in S
K
, which are the synsets corresponding
to the document terms T
K
(cf. Formula 3).
3.3 Similarity between Knowledge Graphs
We can now determine the similarity between two
documents d, d
?
? D in terms of the similarity of
their knowledge graph representations G and G
?
.
Following the literature (Montes y G?omez et
al., 2001) we calculate the similarity between the
vertex sets in the two graphs using Dice?s coeffi-
cient (Jackson et al., 1989):
417
Figure 2: Knowledge graph examples from two comparable documents in different languages.
S
c
(G,G
?
) =
2 ?
?
c?V (G)?V (G
?
)
w(c)
?
c?V (G)
w(c) +
?
c?V (G
?
)
w(c)
, (4)
where w(c) is the weight of a concept c (see Sec-
tion 3.2). Likewise, we calculate the similarity be-
tween the two edge sets as:
S
r
(G,G
?
) =
2 ?
?
r?E(G)?E(G
?
)
w(r)
?
r?E(G)
w(r) +
?
r?E(G
?
)
w(r)
, (5)
where w(r) is the weight of a semantic relation
edge r.
We combine the two above measures of concep-
tual (S
c
) and relational (S
r
) similarity to obtain an
integrated measure S
g
(G,G
?
) between knowledge
graphs:
S
g
(G,G
?
) =
S
c
(G,G
?
) + S
r
(G,G
?
)
2
. (6)
Notably, since we are working with a language-
independent representation of documents, this
similarity measure can be applied to the knowl-
edge graphs built from documents written in any
language. In Figure 2 we show two knowledge
graphs for comparable documents written in dif-
ferent languages (for clarity, labels are in English
in both graphs). As expected, the graphs share sev-
eral key concepts and relations.
4 A Multilingual Vector Representation
4.1 From Document to Multilingual Vector
Since our knowledge graphs will only cover the
most central concepts of a document, we comple-
ment this core representation with a more tradi-
tional vector-based representation. However, as
we are interested in the cross-language compari-
son of documents, we translate our monolingual
vector ~v
L
of a document d written in language L
into its corresponding vector ~v
L
?
in language L
?
Algorithm 1 Dictionary-based term-vector translation.
Input: a weighted document vector ~v
L
= (w
1
, . . . , w
n
), a
source language L and a target language L
?
Output: a translated vector ~v
L
?
1: ~v
L
?
? (0, . . . , 0) of length n
2: for i = 1 to n
3: if w
i
= 0 continue
4: // let t
i
be the term corresponding to w
i
in ~v
L
5: S
L
? Synsets
L
(t
i
)
6: for each synset s ? S
L
7: T ? getTranslations(s, L
?
)
8: if T 6= ? then
9: for each tr ? T
10: w
new
= w
i
? confidence(tr, t
i
)
11: // let index(tr) be the index of tr in ~v
L
12: if ? index(tr) then
13: v
L
?
(index(tr)) = w
new
14: return ~v
L
?
using BabelNet as our multilingual dictionary. We
detail the document-vector translation process in
Algorithm 1.
The translated vector ~v
L
?
is obtained as follows:
for each term t
i
with non-zero weight in v
L
we
obtain all the possible meanings of t
i
in BabelNet
(see line 5) and, for each of these, we retrieve all
the translations (line 7), i.e., lexicalizations of the
concept, in language L
?
available in the synset. We
set a non-zero value in the translation vector ~v
L
?
,
5
in correspondence with each such translation tr,
proportional to the weight of t
i
in the original vec-
tor and the confidence of the translation (line 10),
as provided by the BabelNet semantic network.
6
In order to increase the amount of information
available in the vector and counterbalance possible
wrong translations, we avoid translating all vec-
tors to one language. Instead, in the present work
we create a multilingual vector representation of a
5
To make the translation possible, while at the same time
keeping the same number of dimensions in our vector repre-
sentation, we use a shared vocabulary which covers both lan-
guages. See Section 6 for details on the experimental setup.
6
Non-English lexicalizations in BabelNet have confi-
dence 1 if originating from Wikipedia inter-language links
and ? 1 if obtained by means of statistical machine transla-
tion (Navigli and Ponzetto, 2012a).
418
document d written in language L by concatenat-
ing the corresponding vector ~v
L
with the translated
vector ~v
L
?
of d for language L
?
. As a result, we
obtain a multilingual vector ~v
LL
?
, which contains
lexicalizations in both languages.
4.2 Similarity between Multilingual Vectors
Following common practice for document similar-
ity in the literature (Manning et al., 2008), we use
the cosine similarity as the similarity measure be-
tween multilingual vectors:
S
v
(~v
LL
?
, ~v
?
LL
?) =
~v
LL
?
? ~v
?
LL
?
||~v
LL
?
|| ||~v
?
LL
?
||
. (7)
5 Knowledge-based Document Similarity
Given a source document d and a target docu-
ment d
?
, we calculate the similarities between the
respective knowledge-graph and multilingual vec-
tor representations, and combine them to obtain a
knowledge-based similarity as follows:
KBSim(d, d
?
) = c(G)S
g
(G,G
?
) + (1? c(G))S
v
(~v
LL
?
, ~v
?
LL
?
),
(8)
where c(G) is an interpolation factor calculated as
the edge density of knowledge graph G:
c(G) =
|E(G)|
|V (G)|(|V (G)| ? 1)
. (9)
Note that, using the factor c(G) to interpolate
the two similarities in Eq. 8, we determine the rel-
evance for the knowledge graphs and the multi-
lingual vectors in a dynamic way. Indeed, c(G)
makes the contribution of graph similarity depend
on the richness of the knowledge graph.
6 Evaluation
In this section we compare our knowledge-
based document similarity measure, KBSim,
against state-of-the-art models on two different
tasks: comparable document retrieval and cross-
lingual text categorization.
6.1 Comparable Document Retrieval
In our first experiment we determine the effective-
ness of our knowledge-based approach in a com-
parable document retrieval task. Given a docu-
ment d written in language L and a collection D
L
?
of documents written in another language L
?
, the
task of comparable document retrieval consists of
finding the document in D
L
?
which is most simi-
lar to d, under the assumption that there exists one
document d
?
? D
L
?
which is comparable with d.
6.1.1 Corpus and Task Setting
Dataset We followed the experimental setting
described in (Platt et al., 2010; Yih et al., 2011)
and evaluated KBSim on the Wikipedia dataset
made available by the authors of those papers.
The dataset is composed of Wikipedia compara-
ble encyclopedic entries in English and Spanish.
For each document in English there exists a ?real?
pair in Spanish which was defined as a compara-
ble entry by the Wikipedia user community. The
dataset of each language was split into three parts:
43,380 training, 8,675 development and 8,675 test
documents. The documents were tokenized, with-
out stemming, and represented as vectors using a
log(tf)-idf weighting (Salton and Buckley, 1988).
The vocabulary of the corpus was restricted to
20,000 terms, which were the most frequent terms
in the two languages after removing the top 50
terms.
Methodology To evaluate the models we com-
pared each English document against the Spanish
dataset and vice versa. Following the original set-
ting, the results are given as the average perfor-
mance between these two experiments. For eval-
uation we employed the averaged top-1 accuracy
and Mean Reciprocal Rank (MMR) at finding the
real comparable document in the other language.
We compared KBSim against the state-of-the-art
supervised models S2Net, OPCA, CCA, and CL-
LSI (cf. Section 2). In contrast to these models,
KBSim does not need a training step, so we ap-
plied it directly to the testing partition.
In addition we also included the results of
CL-ESA
7
, CL-C3G
8
and two simple vector-based
models which translate all documents into English
on a word-by-word basis and compared them us-
ing cosine similarity: the first model (CosSim
E
)
uses a statistical dictionary trained with Europarl
using Wavelet-Domain Hidden Markov Models
(He, 2007), a model similar to IBM Model 4;
the second model (CosSim
BN
) instead uses Algo-
rithm 1 to translate the vectors with BabelNet.
6.1.2 Results
As we can see from Table 1,
9
the CosSim
BN
model, which uses BabelNet to translate the docu-
ment vectors, achieves better results than CCA and
CL-LSI. We hypothesize that this is due to these
linear projection models losing information during
the projection. CosSim
E
yields results similar to
CosSim
BN
, showing that BabelNet is a good al-
ternative statistical dictionary. In contrast to CCA
7
Document collections with sizes higher than 10
5
provide
high performance (Potthast et al., 2008). Here we used 15k
documents from the training set to index the test documents.
8
CL-C3G is CL-CNG using character 3-grams, which has
proven to be the best length (Mcnamee and Mayfield, 2004).
9
In this work, statistically significant results according to
a ?
2
test are highlighted in bold.
419
Model Dimension Accuracy MMR
S2Net 2000 0.7447 0.7973
KBSim N/A 0.7342 0.7750
OPCA 2000 0.7255 0.7734
CosSim
E
N/A 0.7033 0.7467
CosSim
BN
N/A 0.7029 0.7550
CCA 1500 0.6894 0.7378
CL-LSI 5000 0.5302 0.6130
CL-ESA 15000 0.2660 0.3305
CL-C3G N/A 0.2511 0.3025
Table 1: Test results for comparable document re-
trieval in Wikipedia. S2Net, OPCA, CosSim
E
,
CCA and CL-LSI are from (Yih et al., 2011).
and CL-LSI, OPCA performs better thanks to its
improved projection method using a noise covari-
ance matrix, which enables it to obtain the main
components in a low-dimensional space.
CL-C3G and CL-ESA obtain the lowest results.
Considering that English and Spanish do not have
many lexical similarities, the low performance of
CL-C3G is justified because these languages do
not share many character n-grams. The reason be-
hind the low results of CL-ESA can be explained
by the low number of intersecting concepts be-
tween Spanish and English in Wikipedia, as con-
firmed by Potthast et al. (2008). Despite both us-
ing Wikipedia in some way, KBSim obtains much
higher performance than CL-ESA thanks to the
use of our multilingual knowledge graph repre-
sentation of documents, which makes it possible
to expand and semantically relate its original con-
cepts. As a result, in contrast to CL-ESA, KB-
Sim can integrate conceptual and relational simi-
larity functions which provide more accurate per-
formance. Interestingly, KBSim also outperforms
OPCA which, in contrast to our system, is super-
vised, and in terms of accuracy is only 1 point be-
low S2Net, the supervised state-of-the-art model
using neural networks.
6.2 Cross-language Text Categorization
The second task in which we tested the differ-
ent models was cross-language text categorization.
The task is defined as follows: given a document
d
L
in a language L and a corpus D
?
L
?
with docu-
ments in a different language L
?
, and C possible
categories, a system has to classify d
L
into one of
the categories C using the labeled collection D
?
L
?
.
6.2.1 Corpus and Task Setting
Dataset To perform this task we used the Mul-
tilingual Reuters Collection (Amini et al., 2009),
which is composed of five datasets of news from
five different languages (English, French, German,
Spanish and Italian) and classified into six possi-
Model Dim. EN News ES News
Accuracy Accuracy
KBSim N/A 0.8189 0.6997
Full MT 50 0.8483 0.6484
CosSim
BN
N/A 0.8023 0.6737
OPCA 100 0.8412 0.5954
CCA 150 0.8388 0.5323
CL-LSI 5000 0.8401 0.5105
CosSim
E
N/A 0.8046 0.4481
Table 2: Test results for cross-language text cat-
egorization. Full MT, OPCA, CCA, CL-LSI and
CosSim
E
are from (Platt et al., 2010).
ble categories. In addition, each dataset of news
is translated into the other four languages using
the Portage translation system (Sadat et al., 2005).
As a result, we have five different multilingual
datasets, each containing source news documents
in one language and four sets of translated doc-
uments in the other languages. Each of the lan-
guages has an independent vocabulary. Document
vectors in the collection are created using TFIDF-
based weighting.
Methodology To evaluate our approach we used
the English and Spanish news datasets. From
the English news dataset we randomly selected
13,131 news as training and 1,875 as test docu-
ments. From the Spanish news dataset we selected
all 12,342 news as test documents. To classify
both test sets we used the English news training
set. We performed the experiment at cross-lingual
level using Spanish and English languages avail-
able for both Spanish and English news datasets,
therefore we classified each test set selecting the
documents in English and using the Spanish doc-
uments in the training dataset, and vice versa. We
followed Platt et al. (2010) and averaged the val-
ues obtained from the two comparisons for each
test set to obtain the final result. To categorize
the documents we applied k-NN to the ranked
list of documents according to the similarity mea-
sure employed for each model. We evaluated each
model by estimating its accuracy in the classifica-
tion of the English and Spanish test sets.
We compared our approach against the state-
of-the-art supervised models in this task: OPCA,
CCA and CL-LSI (Platt et al., 2010). In addi-
tion, we include the results of the CosSim
BN
and
CosSim
E
models that we introduced in Section
6.1.1, as well as the results of a full statistical ma-
chine translation system trained with Europarl and
post-processed by LSA (Full MT), as reported by
Platt et al. (2010).
420
6.2.2 Results
Table 2 shows the cross-language text categoriza-
tion accuracy. CosSim
E
obtained the lowest re-
sults. This is because there is a significant number
of untranslated terms in the translation process that
the statistical dictionary cannot cover. This is not
the case in the CosSim
BN
model which achieves
higher results using BabelNet as a statistical dic-
tionary, especially on the Spanish news corpus.
On the other hand, however, the linear projec-
tion methods as well as Full MT obtained the high-
est results on the English corpus. The differences
between the linear projection methods are evident
when looking at the Spanish corpus results; OPCA
performed best with a considerable improvement,
which indicates again that it is one of the most ef-
fective linear projection methods. Finally, our ap-
proach, KBSim, obtained competitive results on
the English corpus, performing best among the un-
supervised systems, and the highest results on the
Spanish news, surpassing all alternatives.
Since KBSim does not need any training for
document comparison, and because it based,
moreover, on a multilingual lexical resource, we
performed an additional experiment to demon-
strate its ability to carry out the same text cate-
gorization task in many languages. To do this, we
used the Multilingual Reuters Collection to cre-
ate a 3,000 document test dataset and 9,000 train-
ing dataset
10
for five languages: English, German,
Spanish, French and Italian. Then we calculated
the classification accuracy on each test set using
each training set. Results are shown in Table 3.
The best results for each language were ob-
tained when working at the monolingual level,
which suggests that KBSim might be a good
untrained alternative in monolingual tasks, too.
In general, cross-language comparisons produced
similar results, demonstrating the general applica-
bility of KBSim to arbitrary language pairs in mul-
tilingual text categorization. However, we note
that German, Italian and Spanish training parti-
tions produced low results compared to the oth-
ers. After analyzing the length of the documents
in the different datasets we discovered that they
have different average lengths in words: 79 (EN),
76 (FR), 75 (DE), 60 (ES) and 55 (IT). German,
Spanish and especially Italian documents have the
lowest average length, which makes it more diffi-
cult to build a representative knowledge graph of
the content of each document when it is perform-
ing at cross-language level.
10
Note that training is needed for the k-NN classifier, but
not for document comparison.
Testing Training datasets
datasets DE EN ES FR IT
DE 0.8053 0.6872 0.5373 0.6417 0.5920
EN 0.5827 0.8463 0.5540 0.6530 0.5820
ES 0.5883 0.6153 0.8707 0.6237 0.7010
FR 0.6867 0.7103 0.6667 0.8227 0.6887
IT 0.5973 0.5487 0.6263 0.5973 0.8317
Table 3: KBSim accuracy in a multilingual setup.
7 Conclusions
In this paper we introduced a knowledge-based
approach to represent and compare documents
written in different languages. The two main
contributions of this work are: i) a new graph-
based model for the language-independent rep-
resentation of documents based on the Babel-
Net multilingual semantic network; ii) KBSim, a
knowledge-based cross-language similarity mea-
sure between documents, which integrates our
multilingual graph-based model with a traditional
vector representation.
In two different cross-lingual tasks, i.e., compa-
rable document retrieval and cross-language text
categorization, KBSim has proven to perform on
a par or better than the supervised state-of-the-art
models which make use of linear projections to
obtain the main components of the term vectors.
We remark that, in contrast to the best systems in
the literature, KBSim does not need any parameter
tuning phase nor does it use any training informa-
tion. Moreover, when scaling to many languages,
supervised systems need to be trained on each pair,
which can be very costly.
The gist of our approach is in the knowl-
edge graph representation of documents, which re-
lates the original terms using expanded concepts
and relations from BabelNet. The knowledge
graphs also have the nice feature of being human-
interpretable, a feature that we want to exploit in
future work. We will also explore the integration
of linear projection models, such as OPCA and
S2Net, into our multilingual vector-based similar-
ity measure. Also, to ensure a level playing field,
following the competing models, in this work we
did not use multi-word expressions as vector com-
ponents. We will study their impact on KBSim in
future work.
Acknowledgments
The authors gratefully acknowledge the support
of the ERC Starting Grant MultiJEDI No. 259234,
EC WIQ-EI IRSES (Grant No. 269180) and
MICINN DIANA-Applications (TIN2012-38603-
C02-01). Thanks go to Yih et al. for their support
and Jim McManus for his comments.
421
References
Massih-Reza Amini, Nicolas Usunier, and Cyril
Goutte. 2009. Learning from multiple partially ob-
served views - an application to multilingual text
categorization. In Advances in Neural Information
Processing Systems 22 (NIPS 2009), pages 28?36.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. J. Web Sem.,
7(3):154?165.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information
retrieval. In Proceedings of the International Joint
Conference on Artificial Intelligence (IJCAI), vol-
ume 9, pages 1513?1518.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391?407.
Konstantinos I. Diamantaras and Sun Y. Kung. 1996.
Principal component neural networks. Wiley New
York.
Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic cross-language retrieval using latent seman-
tic indexing. In Proc. of AAAI Spring Symposium
on Cross-language Text and Speech Retrieval, pages
18?24.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. Bradford Books.
Marc Franco-Salvador, Parth Gupta, and Paolo Rosso.
2013. Cross-language plagiarism detection using
a multilingual semantic network. In Proc. of the
35th European Conference on Information Retrieval
(ECIR?13), volume LNCS(7814), pages 710?713.
Springer-Verlag.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proc. of the 20th
International Joint Conference on Artifical Intelli-
gence (IJCAI), pages 1606?1611.
Taher Haveliwala, Sepandar Kamvar, and Glen Jeh.
2003. An analytical comparison of approaches to
personalizing pagerank. Technical Report 2003-35,
Stanford InfoLab, June.
Xiaodong He. 2007. Using word dependent transition
models in hmm based word alignment for statistical
machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
80?87. Association for Computational Linguistics.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194:28?61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
David Hull. 1994. Improving text retrieval for the
routing problem using latent semantic indexing. In
Proceedings of the 17th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR), pages 282?291.
Springer.
Donald A. Jackson, Keith M. Somers, and Harold H.
Harvey. 1989. Similarity coefficients: measures of
co-occurrence and association or simply measures of
occurrence? American Naturalist, pages 436?453.
Ian T. Jolliffe. 1986. Principal component analysis,
volume 487. Springer-Verlag New York.
Thomas K. Landauer and Michael L. Littman. 1994.
Computerized cross-language document retrieval
using latent semantic indexing, April 5. US Patent
5,301,109.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-WSA: A graph-based approach to word
sense alignment. Transactions of the Association for
Computational Linguistics (TACL), 1:151?164.
Paul Mcnamee and James Mayfield. 2004. Charac-
ter n-gram tokenization for european language text
retrieval. Information Retrieval, 7(1-2):73?97.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2-Volume 2, pages
880?889. Association for Computational Linguis-
tics.
Manuel Montes y G?omez, Alexander F. Gelbukh, Au-
relio L?opez-L?opez, and Ricardo A. Baeza-Yates.
2001. Flexible comparison of conceptual graphs.
In Proc. of the 12th International Conference on
Database and Expert Systems Applications (DEXA),
pages 102?111.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
422
Vivi Nastase and Michael Strube. 2013. Transform-
ing wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62?85.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsuper-
vised word sense disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2012a.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
BabelRelate! a joint multilingual approach to com-
puting semantic relatedness. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence (AAAI-12), pages 108?114, Toronto, Canada.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The PageRank Citation
Ranking: Bringing Order to the Web. Technical re-
port, Stanford Digital Library Technologies Project.
John C. Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 251?261.
Martin Potthast, Benno Stein, and Maik Anderka.
2008. A wikipedia-based multilingual retrieval
model. In Advances in Information Retrieval, pages
522?530. Springer.
Martin Potthast, Alberto Barr?on-Cede?no, Benno Stein,
and Paolo Rosso. 2011. Cross-language plagia-
rism detection. Language Resources and Evalua-
tion, 45(1):45?62.
Fatiha Sadat, Howard Johnson, Akakpo Agbago,
George Foster, Joel Martin, and Aaron Tikuisis.
2005. Portage: A phrase-based machine translation
system. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, Ann Arbor, USA.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513?
523.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Gerard Salton, Edward A. Fox, and Harry Wu. 1983.
Extended boolean information retrieval. Communi-
cations of the ACM, 26(11):1022?1036.
Bruce Thompson. 2005. Canonical correlation analy-
sis. Encyclopedia of statistics in behavioral science.
Piek Vossen. 2004. EuroWordNet: A multilin-
gual database of autonomous and language-specific
wordnets connected via an inter-lingual index. In-
ternational Journal of Lexicography, 17(2):161?
173.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning, pages 247?256.
423
OntoLearn Reloaded: A Graph-Based
Algorithm for Taxonomy Induction
Paola Velardi?
Sapienza University of Rome
Stefano Faralli?
Sapienza University of Rome
Roberto Navigli?
Sapienza University of Rome
In 2004 we published in this journal an article describing OntoLearn, one of the first systems
to automatically induce a taxonomy from documents and Web sites. Since then, OntoLearn has
continued to be an active area of research in our group and has become a reference work within
the community. In this paper we describe our next-generation taxonomy learning methodol-
ogy, which we name OntoLearn Reloaded. Unlike many taxonomy learning approaches in the
literature, our novel algorithm learns both concepts and relations entirely from scratch via the
automated extraction of terms, definitions, and hypernyms. This results in a very dense, cyclic
and potentially disconnected hypernym graph. The algorithm then induces a taxonomy from
this graph via optimal branching and a novel weighting policy. Our experiments show that we
obtain high-quality results, both when building brand-new taxonomies and when reconstructing
sub-hierarchies of existing taxonomies.
1. Introduction
Ontologies have proven useful for different applications, such as heterogeneous data
integration, information search and retrieval, question answering, and, in general, for
fostering interoperability between systems. Ontologies can be classified into three main
types (Sowa 2000), namely: i) formal ontologies, that is, conceptualizations whose cat-
egories are distinguished by axioms and formal definitions, stated in logic to support
complex inferences and computations; ii) prototype-based ontologies, which are based
on typical instances or prototypes rather than axioms and definitions in logic; iii) lexical-
ized (or terminological) ontologies, which are specified by subtype-supertype relations
and describe concepts by labels or synonyms rather than by prototypical instances.
Here we focus on lexicalized ontologies because, in order to enable natural
language applications such as semantically enhanced information retrieval and ques-
tion answering, we need a clear connection between our formal representation of the
? Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy.
E-mail: {velardi,faralli,navigli}@di.uniroma1.it.
Submission received: 17 December 2011; revised submission received: 28 July 2012; accepted for publication:
10 October 2012.
doi:10.1162/COLI a 00146
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
domain and the language used to express domain meanings within text. And, in turn,
this connection can be established by producing full-fledged lexicalized ontologies for
the domain of interest. Manually constructing ontologies is a very demanding task,
however, requiring a large amount of time and effort, even when principled solutions
are used (De Nicola, Missikoff, and Navigli 2009). A quite recent challenge, referred
to as ontology learning, consists of automatically or semi-automatically creating a
lexicalized ontology using textual data from corpora or the Web (Gomez-Perez and
Manzano-Mancho 2003; Biemann 2005; Maedche and Staab 2009; Petasis et al 2011). As
a result of ontology learning, the heavy requirements of manual ontology construction
can be drastically reduced.
In this paper we deal with the problem of learning a taxonomy (i.e., the backbone
of an ontology) entirely from scratch. Very few systems in the literature address this
task. OntoLearn (Navigli and Velardi 2004) was one of the earliest contributions in this
area. In OntoLearn taxonomy learning was accomplished in four steps: terminology
extraction, derivation of term sub-trees via string inclusion, disambiguation of domain
terms using a novel Word Sense Disambiguation algorithm, and combining the sub-
trees into a taxonomy. The use of a static, general-purpose repository of semantic
knowledge, namely, WordNet (Miller et al 1990; Fellbaum 1998), prevented the system
from learning taxonomies in technical domains, however.
In this paper we present OntoLearn Reloaded, a graph-based algorithm for learning
a taxonomy from the ground up. OntoLearn Reloaded preserves the initial step of
our 2004 pioneering work (Navigli and Velardi 2004), that is, automated terminology
extraction from a domain corpus, but it drops the requirement for WordNet (thereby
avoiding dependence on the English language). It also drops the term compositionality
assumption that previously led to us having to use a Word Sense Disambiguation
algorithm?namely, SSI (Navigli and Velardi 2005)?to structure the taxonomy. Instead,
we now exploit textual definitions, extracted from a corpus and the Web in an iterative
fashion, to automatically create a highly dense, cyclic, potentially disconnected hyper-
nym graph. An optimal branching algorithm is then used to induce a full-fledged tree-
like taxonomy. Further graph-based processing augments the taxonomy with additional
hypernyms, thus producing a Directed Acyclic Graph (DAG).
Our system provides a considerable advancement over the state of the art in
taxonomy learning:
 First, excepting for the manual selection of just a few upper nodes, this
is the first algorithm that has been experimentally shown to build from
scratch a new taxonomy (i.e., both concepts and hypernym relations)
for arbitrary domains, including very technical ones for which
gold-standard taxonomies do not exist.
 Second, we tackle the problem with no simplifying assumptions: We cope
with issues such as term ambiguity, complexity of hypernymy patterns,
and multiple hypernyms.
 Third, we propose a novel algorithm to extract an optimal branching
from the resulting hypernym graph, which?after some recovery
steps?becomes our final taxonomy. Taxonomy induction is the
main theoretical contribution of the paper.
 Fourth, the evaluation is not limited, as it is in most papers, to the number
of retrieved hypernymy relations that are found in a reference taxonomy.
666
Velardi, Faralli, and Navigli OntoLearn Reloaded
Instead, we also analyze the extracted taxonomy in its entirety;
furthermore, we acquire two ?brand new? taxonomies in the
domains of ARTIFICIAL INTELLIGENCE and FINANCE.
 Finally, our taxonomy-building workflow is fully implemented and
the software components are either freely available from our Web
site,1 or reproducible.
In this paper we extend our recent work on the topic (Navigli, Velardi, and Faralli
2011) as follows: i) we describe in full detail the taxonomy induction algorithm; ii) we
enhance our methodology with a final step aimed at creating a DAG, rather than a strict
tree-like taxonomical structure; iii) we perform a large-scale multi-faceted evaluation
of the taxonomy learning algorithm on six domains; and iv) we contribute a novel
methodology for evaluating an automatically learned taxonomy against a reference
gold standard.
In Section 2 we illustrate the related work. We then describe our taxonomy-
induction algorithm in Section 3. In Section 4 we present our experiments, and discuss
the results. Evaluation is both qualitative (on new ARTIFICIAL INTELLIGENCE and
FINANCE taxonomies), and quantitative (on WordNet and MeSH sub-hierarchies). Sec-
tion 5 is dedicated to concluding remarks.
2. Related Work
Two main approaches are used to learn an ontology from text: rule-based and distri-
butional approaches. Rule-based approaches use predefined rules or heuristic patterns
to extract terms and relations. These approaches are typically based on lexico-syntactic
patterns, first introduced by Hearst (1992). Instances of relations are harvested from text
by applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y).
Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999;
Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques
(Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case,
a number of term pairs in the wanted relation are manually picked and the relation is
sought within text corpora or the Web. Other rule-based approaches learn a taxonomy
by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci,
and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computa-
tional lexicons such as WordNet (Ponzetto and Navigli 2009).
Distributional approaches, instead, model ontology learning as a clustering or
classification task, and draw primarily on the notions of distributional similarity (Pado
and Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poon
and Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012). Such
approaches are based on the assumption that paradigmatically-related concepts2 appear
in similar contexts and their main advantage is that they are able to discover relations
that do not explicitly appear in the text. They are typically less accurate, however, and
the selection of feature types, notion of context, and similarity metrics vary considerably
depending on the specific approach used.
1 http://lcl.uniroma1.it/ontolearn reloaded and http://ontolearn.org.
2 Because we are concerned with lexical taxonomies, in this paper we use the words concepts and terms
interchangeably.
667
Computational Linguistics Volume 39, Number 3
Recently, Yang and Callan (2009) presented a semi-supervised taxonomy induc-
tion framework that integrates contextual, co-occurrence, and syntactic dependencies,
lexico-syntactic patterns, and other features to learn an ontology metric, calculated
in terms of the semantic distance for each pair of terms in a taxonomy. Terms are
incrementally clustered on the basis of their ontology metric scores. In their work, the
authors assume that the set of ontological concepts C is known, therefore taxonomy
learning is limited to finding relations between given pairs in C. In the experiments,
they only use the word senses within a particular WordNet sub-hierarchy so as to avoid
any lexical ambiguity. Their best experiment obtains a 0.85 precision rate and 0.32 recall
rate in replicating is-a links on 12 focused WordNet sub-hierarchies, such as PEOPLE,
BUILDING, PLACE, MILK, MEAL, and so on.
Snow, Jurafsky, and Ng (2006) propose the incremental construction of taxonomies
using a probabilistic model. In their work they combine evidence from multiple
supervised classifiers trained on very large training data sets of hyponymy and cousin
relations. Given the body of evidence obtained from all the relevant word pairs in
a lexico-syntactic relation, the taxonomy learning task is defined probabilistically as
the problem of finding the taxonomy that maximizes the probability of having that
evidence (a supervised logistic regression model is used for this). Rather than learning
a new taxonomy from scratch, however, this approach aims at attaching new concepts
under the appropriate nodes of an existing taxonomy (i.e., WordNet). The approach is
evaluated by manually assessing the quality of the single hypernymy edges connecting
leaf concepts to existing ones in WordNet, with no evaluation of a full-fledged struc-
tured taxonomy and no restriction to a specific domain. A related, weakly supervised
approach aimed at categorizing named entities, and attaching them to WordNet leaves,
was proposed by Pasca (2004). Other approaches use formal concept analysis (Cimiano,
Hotho, and Staab 2005), probabilistic and information-theoretic measures to learn tax-
onomies from a folksonomy (Tang et al 2009), and Markov logic networks and syntactic
parsing applied to domain text (Poon and Domingos 2010).
The work closest to ours is that presented by Kozareva and Hovy (2010). From an
initial given set of root concepts and basic level terms, the authors first use Hearst-like
lexico-syntactic patterns iteratively to harvest new terms from the Web. As a result a
set of hyponym?hypernym relations is obtained. Next, in order to induce taxonomic
relations between intermediate concepts, the Web is searched again with surface pat-
terns. Finally, nodes from the resulting graph are removed if the out-degree is below
a threshold, and edges are pruned by removing cycles and selecting the longest path
in the case of multiple paths between concept pairs. Kozareva and Hovy?s method has
some limitations, which we discuss later in this paper. Here we note that, in evalu-
ating their methodology, the authors discard any retrieved nodes not belonging to a
WordNet sub-hierarchy (they experiment on PLANTS, VEHICLES, and ANIMALS), thus
it all comes down to Yang and Callan?s (2009) experiment of finding relations between a
pre-assigned set of nodes.
In practice, none of the algorithms described in the literature was actually applied
to the task of creating a new taxonomy for an arbitrary domain of interest truly from
scratch. Instead, what is typically measured is the ability of a system to reproduce as
far as possible the relations of an already existing taxonomy (a common test is WordNet
or the Open Directory Project3), when given the set of domain concepts. Evaluating
against a gold standard is, indeed, a reasonable validation methodology. The claim to be
3 http://www.dmoz.org/.
668
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 1
The OntoLearn Reloaded taxonomy learning workflow.
?automatically building? a taxonomy needs also to be demonstrated on new domains
for which no a priori knowledge is available, however. In an unknown domain, tax-
onomy induction requires the solution of several further problems, such as identifying
domain-appropriate concepts, extracting appropriate hypernym relations, and detect-
ing lexical ambiguity, whereas some of these problems can be ignored when evaluating
against a gold standard (we will return to this issue in detail in Section 4). In fact,
the predecessor of OntoLearn Reloaded, that is, OntoLearn (Navigli and Velardi 2004),
suffers from a similar problem, in that it relies on the WordNet taxonomy to establish
paradigmatic connections between concepts.
3. The Taxonomy Learning Workflow
OntoLearn Reloaded starts from an initially empty directed graph and a corpus for the
domain of interest (e.g., an archive of artificial intelligence papers). We also assume
that a small set of upper terms (entity, abstraction, etc.), which we take as the end
points of our algorithm, has been manually defined (e.g., from a general purpose taxon-
omy like WordNet) or is available for the domain.4 Our taxonomy-learning workflow,
summarized in Figure 1, consists of five steps:
1. Initial Terminology Extraction (Section 3.1): The first step applies a term
extraction algorithm to the input domain corpus in order to produce an
initial domain terminology as output.
2. Definition & Hypernym Extraction (Section 3.2): Candidate definition
sentences are then sought for the extracted domain terminology. For each
term t, a domain-independent classifier is used to select well-formed
definitions from the candidate sentences and extract the corresponding
hypernyms of t.
4 Although very few domain taxonomies are available, upper (core) concepts have been defined in several
domains, such as MEDICINE, ART, ECONOMY, and so forth.
669
Computational Linguistics Volume 39, Number 3
3. Domain Filtering (Section 3.3): A domain filtering technique is applied
to filter out those definitions that do not pertain to the domain of interest.
The resulting domain definitions are used to populate the directed graph
with hypernymy relations connecting t to the extracted hypernym h.
Steps (2) and (3) are then iterated on the newly acquired hypernyms,
until a termination condition occurs.
4. Graph Pruning (Section 3.4): As a result of the iterative phase we obtain
a dense hypernym graph that potentially contains cycles and multiple
hypernyms for most nodes. In this step we combine a novel weighting
strategy with the Chu-Liu/Edmonds algorithm (Chu and Liu 1965;
Edmonds 1967) to produce an optimal branching (i.e., a tree-like
taxonomy) of the initial noisy graph.
5. Edge Recovery (Section 3.5): Finally, we optionally apply a recovery
strategy to reattach some of the hypernym edges deleted during the
previous step, so as to produce a full-fledged taxonomy in the form
of a DAG.
We now describe in full detail the five steps of OntoLearn Reloaded.5
3.1 Initial Terminology Extraction
Domain terms are the building blocks of a taxonomy. Even though in many cases an
initial domain terminology is available, new terms emerge continuously, especially
in novel or scientific domains. Therefore, in this work we aim at fully automatizing
the taxonomy induction process. Thus, we start from a text corpus for the domain
of interest and extract domain terms from the corpus by means of a terminology
extraction algorithm. For this we use our term extraction tool, TermExtractor,6 that
implements measures of domain consensus and relevance to harvest the most relevant
terms for the domain from the input corpus.7 As a result, an initial domain terminol-
ogy T(0) is produced that includes both single- and multi-word expressions (such as,
respectively, graph and flow network). We add one node to our initially empty graph
Gnoisy = (Vnoisy, Enoisy) for each term in T(0)?that is, we set Vnoisy := T(0) and Enoisy := ?.
In Table 1 we show an excerpt of our ARTIFICIAL INTELLIGENCE and FINANCE
terminologies (cf. Section 4 for more details). Note that our initial set of domain terms
(and, consequently, nodes) will be enriched with the new hypernyms acquired during
the subsequent iterative phase, described in the next section.
3.2 Definition and Hypernym Extraction
The aim of our taxonomy induction algorithm is to learn a hypernym graph by means of
several iterations, starting from T(0) and stopping at very general terms U, that we take
as the end point of our algorithm. The upper terms are chosen from WordNet topmost
5 A video of the first four steps of OntoLearn Reloaded is available at
http://www.youtube.com/watch?v=-k3cOEoI Dk.
6 http://lcl.uniroma1.it/termextractor.
7 TermExtractor has already been described in Sclano and Velardi (2007) and in Navigli and Velardi (2004);
therefore the interested reader is referred to these papers for additional details.
670
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 1
An excerpt of the terminology extracted for the ARTIFICIAL INTELLIGENCE and FINANCE
domains.
ARTIFICIAL INTELLIGENCE
acyclic graph parallel corpus flow network
adjacency matrix parse tree pattern matching
artificial intelligence partitioned semantic network pagerank
tree data structure pathfinder taxonomic hierarchy
FINANCE
investor shareholder open economy
bid-ask spread profit maximization speculation
long term debt shadow price risk management
optimal financing policy ratings profit margin
synsets. In other words, U contains all the terms in the selected topmost synsets. In
Table 2 we show representative synonyms of the upper-level synsets that we used for
the ARTIFICIAL INTELLIGENCE and FINANCE domains. Seeing that we use high-level
concepts, the set U can be considered domain-independent. Other choices are of course
possible, especially if an upper ontology for a given domain is already available.
For each term t ? T(i) (initially, i = 0), we first check whether t is an upper term (i.e.,
t ? U). If it is, we just skip it (because we do not aim at extending the taxonomy beyond
an upper term). Otherwise, definition sentences are sought for t in the domain corpus
and in a portion of the Web. To do so we use Word-Class Lattices (WCLs) (Navigli and
Velardi 2010, introduced hereafter), which is a domain-independent machine-learned
classifier that identifies definition sentences for the given term t, together with the
corresponding hypernym (i.e., lexical generalization) in each sentence.
For each term in our set T(i), we then automatically extract definition candidates
from the domain corpus, Web documents, and Web glossaries, by harvesting all the
sentences that contain t. To obtain on-line glossaries we use a Web glossary extraction
system (Velardi, Navigli, and D?Amadio 2008). Definitions can also be obtained via a
lightweight bootstrapping process (De Benedictis, Faralli, Navigli 2013).
Finally, we apply WCLs and collect all those sentences that are classified as defini-
tional. We show some terms with their definitions in Table 3 (first and second column,
respectively). The extracted hypernym is shown in italics.
Table 2
The set of upper concepts used in OntoLearn Reloaded for AI and FINANCE (only representative
synonyms from the corresponding WordNet synsets are shown).
ability#n#1 abstraction#n#6 act#n#2 code#n#2
communication#n#2 concept#n#1 data#n#1 device#n#1
discipline#n#1 entity#n#1 event#n#1 expression#n#6
research#n#1 instrumentality#n#1 knowledge#n#1 knowledge domain#n#1
language#n#1 methodology#n#2 model#n#1 organization#n#1
person#n#1 phenomenon#n#1 process#n#1 property#n#2
quality#n#1 quantity#n#1 relation#n#1 representation#n#2
science#n#1 system#n#2 technique#n#1 theory#n#1
671
Computational Linguistics Volume 39, Number 3
Table 3
Some definitions for the ARTIFICIAL INTELLIGENCE domain (defined term in bold, extracted
hypernym in italics).
Term Definition Weight Domain?
adjacency matrix an adjacency matrix is a zero-one matrix 1.00 
flow network in graph theory, a flow network is a directed graph 0.57 
flow network global cash flow network is an online company that
specializes in education and training courses in
teaching the entrepreneurship
0.14 ?
Table 4
Example definitions (defined terms are marked in bold face, their hypernyms in italics).
[In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF.
[In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of . . . ]REST.
[In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]REST.
[Myrtales]DF [are an order of]VF [ flowering plants]GF [placed as a basal group . . . ]REST.
3.2.1 Word-Class Lattices. We now describe our WCL algorithm for the classification of
definitional sentences and hypernym extraction. Our model is based on a formal notion
of textual definition. Specifically, we assume a definition contains the following fields
(Storrer and Wellinghoff 2006):
 The DEFINIENDUM field (DF): this part of the definition includes the
definiendum (that is, the word being defined) and its modifiers
(e.g., ?In computer science, a pixel?);
 The DEFINITOR field (VF): which includes the verb phrase used to
introduce the definition (e.g., ?is?);
 The DEFINIENS field (GF): which includes the genus phrase (usually
including the hypernym, e.g., ?a dot?);
 The REST field (RF): which includes additional clauses that further
specify the differentia of the definiendum with respect to its genus
(e.g., ?that is part of a computer image?).
To train our definition extraction algorithm, a data set of textual definitions was
manually annotated with these fields, as shown in Table 4.8 Furthermore, the single-
or multi-word expression denoting the hypernym was also tagged. In Table 4, for each
sentence the definiendum and its hypernym are marked in bold and italics, respectively.
Unlike other work in the literature dealing with definition extraction (Hovy et al 2003;
Fahmi and Bouma 2006; Westerhout 2009; Zhang and Jiang 2009), we covered not only
a variety of definition styles in our training set, in addition to the classic X is a Y pattern,
but also a variety of domains. Therefore, our WCL algorithm requires no re-training
when changing the application domain, as experimentally demonstrated by Navigli and
Velardi (2010). Table 5 shows some non-trivial patterns for the VF field.
8 Available on-line at: http://lcl.uniroma1.it/wcl.
672
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 5
Some nontrivial patterns for the VF field.
is a term used to describe is a specialized form of
is the genus of was coined to describe
is a term that refers to a kind of is a special class of
can denote is the extension of the concept of
is commonly used to refer to is defined both as
Starting from the training set, the WCL algorithm learns generalized definitional
models as detailed hereafter.
Generalized sentences. First, training and test sentences are part-of-speech tagged with the
TreeTagger system, a part-of-speech tagger available for many languages (Schmid 1995).
The first step in obtaining a definitional pattern is word generalization. Depending on
its frequency we define a word class as either a word itself or its part of speech. Formally,
let T be the set of training sentences. We first determine the set F of words in T whose
frequency is above a threshold ? (e.g., the, a, an, of ). In our training sentences, we replace
the defined term with the token ?TARGET? (note that ?TARGET? ? F).
Given a new sentence s = t1, t2, . . . , tn, where ti is the i-th token of s, we generalize
its words ti to word classes t?i as follows:
t?i =
{
ti if ti ? F
POS(ti) otherwise
that is, a word ti is left unchanged if it occurs frequently in the training corpus (i.e.,
ti ? F); otherwise it is replaced with its part of speech (POS(ti)). As a result we obtain a
generalized sentence s?. For instance, given the first sentence in Table 4, we obtain the
corresponding generalized sentence: ?In NNS, a ?TARGET? is a JJ NN,? where NN and
JJ indicate the noun and adjective classes, respectively. Generalized sentences are dou-
bly beneficial: First, they help reduce the annotation burden, in that many differently
lexicalized sentences can be caught by a single generalized sentence; second, thanks
to their reduction of the definition variability, they allow for a higher-recall definition
model.
Star patterns. Let T again be the set of training sentences. In this step we associate a
star pattern ?(s) with each sentence s ? T . To do so, let s ? T be a sentence such that
s = t1, t2, . . . , tn, where ti is its i-th token. Given the set F of most frequent words in T ,
the star pattern ?(s) associated with s is obtained by replacing with * all the tokens ti 
? F,
that is, all the tokens that are non-frequent words. For instance, given the sentence ?In
arts, a chiaroscuro is a monochrome picture,? the corresponding star pattern is ?In *, a
?TARGET? is a *,? where ?TARGET? is the defined term.
Sentence clustering. We then cluster the sentences in our training set T on the basis of
their star pattern. Formally, let ? = (?1, . . . ,?m) be the set of star patterns associated
with the sentences in T . We create a clustering C = (C1, . . . , Cm) such that Ci = {s ? T :
?(s) = ?i}, that is, Ci contains all the sentences whose star pattern is ?i.
As an example, assume ?3 = ?In *, a ?TARGET? is a *.? The first three sentences
reported in Table 4 are all grouped into cluster C3. We note that each cluster Ci contains
673
Computational Linguistics Volume 39, Number 3
sentences whose degree of variability is generally much lower than for any pair of
sentences in T belonging to two different clusters.
Word-class lattice construction. The final step consists of the construction of a WCL for
each sentence cluster, using the corresponding generalized sentences. Given such a
cluster Ci ? C, we apply a greedy algorithm that iteratively constructs the WCL.
Let Ci = {s1, s2, . . . , s|Ci|} and consider its first sentence s1 = t1, t2, . . . , tn. Initially, we
create a directed graph G = (V, E) such that V = {t1, . . . , tn} and E = {(t1, t2), (t2, t3), . . . ,
(tn?1, tn)}. Next, for each j = 2, . . . , |Ci|, we determine the alignment between sentence sj
and each sentence sk ? Ci such that k < j according to the following dynamic program-
ming formulation (Cormen, Leiserson, and Rivest 1990, pages 314?319):
Ma,b = max {Ma?1,b?1 + Sa,b, Ma,b?1, Ma?1,b}, (1)
where a ? {0, . . . , |sk|} and b ? {0, . . . , |sj|}, Sa,b is a score of the matching between the
a-th token of sk and the b-th token of sj, and M0,0, M0,b and Ma,0 are initially set to 0 for
all values of a and b.
The matching score Sa,b is calculated on the generalized sentences s?k and s
?
j as
follows:
Sa,b =
{
1 if t?k,a = t
?
j,b
0 otherwise
where t?k,a and t
?
j,b are the a-th and b-th tokens of s
?
k and s
?
j , respectively. In other words, the
matching score equals 1 if the a-th and the b-th tokens of the two generalized sentences
have the same word class.
Finally, the alignment score between sk and sj is given by M|sk|,|sj|, which calculates
the minimal number of misalignments between the two token sequences. We repeat this
calculation for each sentence sk (k = 1, . . . , j ? 1) and choose the one that maximizes its
alignment score with sj. We then use the best alignment to add sj to the graph G: We add
to the set of nodes V the tokens of s?j for which there is no alignment to s
?
k and we add to
E the edges (t?1, t
?
2), . . . , (t
?
|sj|?1, t
?
|sj|).
Example. Consider the first three definitions in Table 4. Their star pattern is ?In *,
a ?TARGET? is a *.? The corresponding WCL is built as follows: The first part-
of-speech tagged sentence, ?In/IN arts/NN , a/DT ?TARGET?/NN is/VBZ a/DT
monochrome/JJ picture/NN,? is considered. The corresponding generalized sentence is
?In NN1 , a ?TARGET? is a JJ NN2.? The initially empty graph is thus populated with one
node for each word class and one edge for each pair of consecutive tokens, as shown in
Figure 2a. Note that we use a rectangle to denote the hypernym token NN2 . We also add
to the graph a start node and an end node ?, and connect them to the corresponding
initial and final sentence tokens. Next, the second sentence, ?In mathematics, a graph
is a data structure that consists of...,? is aligned to the first sentence. The alignment
is perfect, apart from the NN3 node corresponding to ?data.? The node is added to
the graph together with the edges ?a?? NN3 and NN3 ? NN2 (Figure 2b, node and
edges in bold). Finally, the third sentence in Table 4, ?In computer science, a pixel is a
dot that is part of a computer image,? is generalized as ?In NN4 NN1 , a ?TARGET?
is a NN2.? Thus, a new node NN4 is added, corresponding to ?computer? and new
674
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 2
The Word-Class Lattice construction steps on the first three sentences in Table 4. We show in
bold the nodes and edges added to the lattice graph as a result of each sentence alignment step.
The support of each word class is reported beside the corresponding node.
edges are added that connect node ?In? to NN4 and NN4 to NN1. Figure 2c shows the
resulting lattice.
Variants of the WCL model. So far we have assumed that our WCL model learns lattices
from the training sentences in their entirety (we call this model WCL-1). We also consid-
ered a second model that, given a star pattern, learns three separate WCLs, one for each
of the three main fields of the definition, namely: definiendum (DF), definitor (VF), and
definiens (GF). We refer to this latter model as WCL-3. Note that our model does not
take into account the REST field, so this fragment of the training sentences is discarded.
The reason for introducing the WCL-3 model is that, whereas definitional patterns are
highly variable, DF, VF, and GF individually exhibit a lower variability, thus WCL-3
improves the generalization power.
Once the learning process is over, a set of WCLs is produced. Given a test sentence
s, the classification phase for the WCL-1 model consists of determining whether there
exists a lattice that matches s. In the case of WCL-3, we consider any combination of
definiendum, definitor, and definiens lattices. Given that different combinations might
match, for each combination of three WCLs we calculate a confidence score as follows:
score(s, lDF, lVF, lGF ) = coverage ? log2(support + 1) (2)
where s is the candidate sentence, lDF, lVF, and lGF are three lattices (one for
each definition field), coverage is the fraction of sentence tokens covered by the
675
Computational Linguistics Volume 39, Number 3
third lattice, and support is the total number of sentences in the corresponding star
pattern.
WCL-3 selects, if any, the combination of the three WCLs that best fits the sentence
in terms of coverage and support from the training set. In fact, choosing the most
appropriate combination of lattices impacts the performance of hypernym extraction.
Given its higher performance (Navigli and Velardi 2010), in OntoLearn Reloaded we
use WCL-3 for definition classification and hypernym extraction.
3.3 Domain Filtering and Creation of the Hypernym Graph
The WCLs described in the previous section are used to identify definitional sentences
and harvest hypernyms for the terms obtained as a result of the terminology extraction
phase. In this section we describe how to filter out non-domain definitions and create a
dense hypernym graph for the domain of interest.
Given a term t, the common case is that several definitions are found for it (e.g.,
the flow network example provided at the beginning of this section). Many of these
will not pertain to the domain of interest, however, especially if they are obtained
from the Web or if they define ambiguous terms. For instance, in the COMPUTER
SCIENCE domain, the cash flow definition of flow network shown in Table 3 was not
pertinent. To discard these non-domain sentences, we weight each definition candidate
d(t) according to the domain terms that are contained therein using the following
formula:
DomainWeight(d(t)) =
|Bd(t) ? D|
|Bd(t)|
(3)
where Bd(t) is the bag of content words in the definition candidate d(t) and D is given
by the union of the initial terminology T(0) and the set of single words of the terms in
T(0) that can be found as nouns in WordNet. For example, given T(0) = { greedy algo-
rithm, information retrieval, minimum spanning tree }, our domain terminology D = T(0) ?
{ algorithm, information, retrieval, tree }. According to Equation (3), the domain weight
of a definition is normalized by the total number of content words in the definition, so
as to penalize longer definitions. Domain filtering is performed by keeping only those
definitions d(t) whose DomainWeight(d(t)) ? ?, where ? is an empirically tuned thresh-
old.9 In Table 3 (third column), we show some values calculated for the corresponding
definitions (the fourth column reports a check mark if the domain weight is above
the threshold, an ? otherwise). Domain filtering performs some implicit form of Word
Sense Disambiguation (Navigli 2009), as it aims at discarding senses of hypernyms
which do not pertain to the domain.
Let Ht be the set of hypernyms extracted with WCLs from the definitions of term t
which survived this filtering phase. For each t ? T(i), we add Ht to our graph Gnoisy =
(Vnoisy, Enoisy), that is, we set Vnoisy := Vnoisy ? Ht. For each t, we also add a directed
edge (h, t)10 for each hypernym h ? Ht, that is, we set Enoisy := Enoisy ? {(h, t)}. As a result
9 Empirically set to 0.38, as a result of tuning on several data sets of manually annotated definitions in
different domains.
10 In what follows, (h, t) or h ? t reads ?t is-a h.?
676
Velardi, Faralli, and Navigli OntoLearn Reloaded
of this step, the graph contains our domain terms and their hypernyms obtained from
domain-filtered definitions. We now set:
T(i+1) :=
?
t?T(i)
Ht \
i
?
j=1
T(j) (4)
that is, the new set of terms T(i+1) is given by the hypernyms of the current set of terms
T(i) excluding those terms that were already processed during previous iterations of
the algorithm. Next, we move to iteration i + 1 and repeat the last two steps, namely,
we perform definition/hypernym extraction and domain filtering on T(i+1). As a result
of subsequent iterations, the initially empty graph is increasingly populated with new
nodes (i.e., domain terms) and edges (i.e., hypernymy relations).
After a given number of iterations K, we obtain a dense hypernym graph Gnoisy
that potentially contains more than one connected component. Finally, we connect all
the upper term nodes in Gnoisy to a single top node . As a result of this connecting
step, only one connected component of the noisy hypernym graph?which we call
the backbone component?will contain an upper taxonomy consisting of upper
terms in U.
The resulting graph Gnoisy potentially contains cycles and multiple hypernyms for
the vast majority of nodes. In order to eliminate noise and obtain a full-fledged taxon-
omy, we perform a step of graph pruning, as described in the next section.
3.4 Graph Pruning
At the end of the iterative hypernym harvesting phase, described in Sections 3.2 and 3.3,
the result is a highly dense, potentially disconnected, hypernymy graph (see Section 4
for statistics concerning the experiments that we performed). Wrong nodes and edges
might stem from errors in any of the definition/hypernym extraction and domain filter-
ing steps. Furthermore, for each node, multiple ?good? hypernyms can be harvested.
Rather than using heuristic rules, we devised a novel graph pruning algorithm, based
on the Chu-Liu/Edmonds optimal branching algorithm (Chu and Liu 1965; Edmonds
1967), that exploits the topological graph properties to produce a full-fledged taxonomy.
The algorithm consists of four phases (i.e., graph trimming, edge weighting, optimal
branching, and pruning recovery) that we describe hereafter with the help of the noisy
graph in Figure 3a, whose grey nodes belong to the initial terminology T(0) and whose
bold node is the only upper term.
3.4.1 Graph Trimming. We first perform two trimming steps. First, we disconnect ?false?
roots, i.e., nodes which are not in the set of upper terms and with no incoming edges
(e.g., image in Figure 3a). Second, we disconnect ?false? leaves, namely, leaf nodes which
are not in the initial terminology and with no outgoing edges (e.g., output in Figure 3a).
We show the disconnected components in Figure 3b.
3.4.2 Edge Weighting. Next, we weight the edges in our noisy graph Gnoisy. A policy based
only on graph connectivity (e.g., in-degree or betweenness, see Newman [2010] for a
complete survey) is not sufficient for taxonomy learning.11 Consider again the graph in
11 As also remarked by Kozareva and Hovy (2010), who experimented with in-degree.
677
Computational Linguistics Volume 39, Number 3
Figure 3
A noisy graph excerpt (a), its trimmed version (b), and the final taxonomy resulting from
pruning (c).
Figure 3: In choosing the best hypernym for the term token sequence, a connectivity-based
measure might select collection rather than list, because the former reaches more nodes.
In taxonomy learning, however, longer hypernymy paths should be preferred (e.g., data
structure ? collection ? list ? token sequence is better than data structure ? collection ?
token sequence).
We thus developed a novel weighting policy aimed at finding the best trade-off
between path length and the connectivity of traversed nodes. It consists of three steps:
i) Weight each node v by the number of nodes belonging to the initial
terminology that can be reached from v (potentially including v itself).12
Let w(v) denote the weight of v (e.g., in Figure 3b, node collection reaches
list and token sequence, thus w(collection) = 2, whereas w(graph) = 3).
All weights are shown in the corresponding nodes in Figure 3b.
ii) For each node v, consider all the paths from an upper root r to v.
Let ?(r, v) be the set of such paths. Each path p ? ?(r, v) is weighted
by the cumulative weight of the nodes in the path, namely:
?(p) =
?
v??p
w(v?) (5)
iii) Assign the following weight to each incoming edge (h, v) of v (i.e., h is one
of the direct hypernyms of v):
w(h, v) = max
r?U
max
p??(r,h)
?(p) (6)
This formula assigns to edge (h, v) the value ?(p) of the highest-weighting
path p from h to any upper root ? U. For example, in Figure 3b, w(list) = 2,
w(collection) = 2, w(data structure) = 5. Therefore, the set of paths ?(data
structure, list) = { data structure ? list, data structure ? collection ? list },
whose weights are 7 (w(data structure) + w(list)) and 9 (w(data structure) +
w(collection) + w(list)), respectively. Hence, according to Formula 6, w(list,
token sequence) = 9. We show all edge weights in Figure 3b.
12 Nodes in a cycle are visited only once.
678
Velardi, Faralli, and Navigli OntoLearn Reloaded
3.4.3 Optimal Branching. Next, our goal is to move from a noisy graph to a tree-like
taxonomy on the basis of our edge weighting strategy. A maximum spanning tree
algorithm cannot be applied, however, because our graph is directed. Instead, we need
to find an optimal branching, that is, a rooted tree with an orientation such that every
node but the root has in-degree 1, and whose overall weight is maximum. To this end,
we first apply a pre-processing step: For each (weakly) connected component in the
noisy graph, we consider a number of cases, aimed at identifying a single ?reasonable?
root node to enable the optimal branching to be calculated. Let R be the set of candidate
roots, that is, nodes with no incoming edges. We perform the following steps:
i) If |R| = 1 then we select the only candidate as root.
ii) Else if |R| > 1, if an upper term is in R, we select it as root, else we choose
the root r ? R with the highest weight w according to the weighting
strategy described in Section 3.4.2. We also disconnect all the unselected
roots, that is, those in R \ {r}.
iii) Else (i.e., if |R| = 0), we proceed as for step (ii), but we search candidates
within the entire connected component and select the highest weighting
node. In contrast to step (ii), we remove all the edges incoming to the
selected node.
This procedure guarantees not only the selection but also the existence of a single
root node for each component, from which the optimal branching algorithm can start.
We then apply the Chu-Liu/Edmonds algorithm (Chu and Liu 1965; Edmonds 1967) to
each component Gi = (Vi, Ei) of our directed weighted graph Gnoisy in order to find an
optimal branching. The algorithm consists of two phases: a contraction phase and an
expansion phase. The contraction phase is as follows:
1. For each node which is not a root, we select the entering edge with the
highest weight. Let S be the set of such |Vi| ? 1 edges;
2. If no cycles are formed in S, go to the expansion phase. Otherwise,
continue;
3. Given a cycle in S, contract the nodes in the cycle into a pseudo-node k,
and modify the weight of each edge entering any node v in the cycle from
some node h outside the cycle, according to the following equation:
w(h, k) = w(h, v) + (w(x(v), v) ? minv(w(x(v), v))) (7)
where x(v) is the predecessor of v in the cycle and w(x(v), v) is the weight
of the edge in the cycle which enters v;
4. Select the edge entering the cycle which has the highest modified weight
and replace the edge which enters the same real node in S by the new
selected edge;
5. Go to step 2 with the contracted graph.
The expansion phase is applied if pseudo-nodes have been created during step 3.
Otherwise, this phase is skipped and Ti = (Vi, S) is the optimal branching of component
679
Computational Linguistics Volume 39, Number 3
Gi (i.e., the i-th component of Gnoisy). During the expansion phase, pseudo-nodes are
replaced with the original cycles. To break the cycle, we select the real node v into which
the edge selected in step 4 enters, and remove the edge entering v belonging to the
cycle. Finally, the weights on the edges are restored. For example, consider the cycle
in Figure 4a. Nodes pagerank, map, and rank are contracted into a pseudo-node, and
the edges entering the cycle from outside are re-weighted according to Equation (7).
According to the modified weights (Figure 4b), the selected edge, that is, (table, map),
is the one with weight w = 13. During the expansion phase, the edge (pagerank, map) is
eliminated, thus breaking the cycle (Figure 4c).
The tree-like taxonomy resulting from the application of the Chu-Liu/Edmonds
algorithm to our example in Figure 3b is shown in Figure 3c.
3.4.4 Pruning Recovery. The weighted directed graph Gnoisy input to the Chu-Liu/
Edmonds algorithm might contain many (weakly) connected components. In this case,
an optimal branching is found for each component, resulting in a forest of taxonomy
trees. Although some of these components are actually noisy, others provide an impor-
tant contribution to the final tree-like taxonomy. The objective of this phase is to recover
from excessive pruning, and re-attach some of the components that were disconnected
during the optimal branching step. Recall from Section 3.3 that, by construction, we
have only one backbone component, that is, a component which includes an upper tax-
onomy. Our aim is thus to re-attach meaningful components to the backbone taxonomy.
To this end, we apply Algorithm 1. The algorithm iteratively merges non-backbone trees
to the backbone taxonomy tree T0 in three main steps:
 Semantic reconnection step (lines 7?9 in Algorithm 1): In this step we
reuse a previously removed ?noisy? edge, if one is available, to reattach a
non-backbone component to the backbone. Given a root node rTi of a
non-backbone tree Ti (i > 0), if an edge (v, rTi ) existed in the noisy graph
Gnoisy (i.e., the one obtained before the optimal branching phase), with
v ? T0, then we connect the entire tree Ti to T0 by means of this edge.
Figure 4
A graph excerpt containing a cycle (a); Edmonds? contraction phase: a pseudo-node enclosing
the cycle with updated weights on incoming edges (b); and Edmonds? expansion phase: the
cycle is broken and weights are restored (c).
680
Velardi, Faralli, and Navigli OntoLearn Reloaded
Algorithm 1 PruningRecovery(G, Gnoisy)
Require: G is a forest
1: repeat
2: Let F := {T0, T1, . . . , T|F|} be the forest of trees in G = (V, E)
3: Let T0 ? F be the backbone taxonomy
4: E? ? E
5: for all T in F \ {T0} do
6: rT ? rootOf (T)
7: if ?v ? T0 s.t. (v, rT ) ? Gnoisy then
8: E ? E ? {(v, rT )}
9: break
10: else
11: if out-degree(rT ) = 0 then
12: if ?v ? T0 s.t. v is the longest right substring of rT then
13: E := E ? {(v, rT )}
14: break
15: else
16: E ? E \ {(rT, v) : v ? V}
17: break
18: until E = E?
 Reconnection step by lexical inclusion (lines 11?14): Otherwise, if Ti is a
singleton (the out-degree of rTi is 0) and there exists a node v ? T0 such
that v is the longest right substring of rTi by lexical inclusion,
13 we connect
Ti to the backbone tree T0 by means of the edge (v, rTi ).
 Decomposition step (lines 15?17): Otherwise, if the component Ti is not a
singleton (i.e., if the out-degree of the root node rTi is > 0) we disconnect
rTi from Ti. At first glance, it might seem counterintuitive to remove edges
during pruning recovery. Reconnecting by lexical inclusion within a
domain has already been shown to perform well in the literature (Vossen
2001; Navigli and Velardi 2004), but we want to prevent any cascading
errors on the descendants of the root node, and at the same time free up
other pre-existing ?noisy? edges incident to the descendants.
These three steps are iterated on the newly created components, until no change
is made to the graph (line 18). As a result of our pruning recovery phase we return the
enriched backbone taxonomy. We show in Figure 5 an example of pruning recovery that
starts from a forest of three components (including the backbone taxonomy tree on top,
Figure 5a). The application of the algorithm leads to the disconnection of a tree root,
that is, ordered structure (Figure 5a, lines 15?17 of Algorithm 1), the linking of the trees
rooted at token list and binary search tree to nodes in the backbone taxonomy (Figures 5b
and 5d, lines 7?9), and the linking of balanced binary tree to binary tree thanks to lexical
inclusion (Figure 5c, lines 11?14 of the algorithm).
13 Similarly to our original OntoLearn approach (Navigli and Velardi 2004), we define a node?s string
v = wnwn?1 . . .w2w1 to be lexically included in that of a node v? = w?mw
?
m?1 . . .w
?
2w
?
1 if m > n and
wj = w?j for each j ? {1, . . . , n}.
681
Computational Linguistics Volume 39, Number 3
Figure 5
An example starting with three components, including the backbone taxonomy tree on the
top and two other trees on the bottom (a). As a result of pruning recovery, we disconnect ordered
structure (a); we connect token sequence to token list by means of a ?noisy? edge (b); we connect
binary tree to balanced binary tree by lexical inclusion (c); and finally binary tree to binary search
tree by means of another ?noisy? edge (d).
3.5 Edge Recovery
The goal of the last phase was to recover from the excessive pruning of the optimal
branching phase. Another issue of optimal branching is that we obtain a tree-like tax-
onomic structure, namely, one in which each node has only one hypernym. This is not
fully appropriate in taxonomy learning, because systematic ambiguity and polysemy
often require a concept to be paradigmatically related to more than one hypernym. In
fact, a more appropriate structure for a conceptual hierarchy is a DAG, as in WordNet.
For example, two equally valid hypernyms for backpropagation are gradient descent search
682
Velardi, Faralli, and Navigli OntoLearn Reloaded
procedure and training algorithm, so two hypernym edges should correctly be incident to
the backpropagation node.
We start from our backbone taxonomy T0 obtained after the pruning recovery
phase described in Section 3.4.4. In order to obtain a DAG-like taxonomy we apply
the following step: for each ?noisy? edge (v, v?) ? Enoisy such that v, v? are nodes in T0
but the edge (v, v?) does not belong to the tree, we add (v, v?) to T0 if:
i) it does not create a cycle in T0;
ii) the absolute difference between the length of the shortest path from v to
the root rT0 and that of the shortest path from v
? to rT0 is within an interval
[m, M]. The aim of this constraint is to maintain a balance between the
height of a concept in the tree-like taxonomy and that of the hypernym
considered for addition. In other words, we want to avoid the connection
of an overly abstract concept with an overly specific one.
In Section 4, we experiment with three versions of our OntoLearn Reloaded algo-
rithm, namely: one version that does not perform edge recovery (i.e., which learns a
tree-like taxonomy [TREE], and two versions that apply edge recovery (i.e., which learn
a DAG) with different intervals for constraint (ii) above (DAG[1, 3] and DAG[0, 99]; note
that the latter version virtually removes constraint (ii)). Examples of recovered edges
will be presented and discussed in the evaluation section.
3.6 Complexity
We now perform a complexity analysis of the main steps of OntoLearn Reloaded. Given
the large number of steps and variables involved we provide a separate discussion of
the main costs for each individual step, and we omit details about commonly used data
structures for access and storage, unless otherwise specified. Let Gnoisy = (Vnoisy, Enoisy)
be our noisy graph, and let n = |Vnoisy| and m = |Enoisy|.
1. Terminology extraction: Assuming a part-of-speech tagged corpus as
input, the cost of extracting candidate terms by scanning the corpus with a
maximum-size window is in the order of the word size of the input
corpus. Thus, the application of statistical measures to our set of candidate
terms has a computational cost that is on the order of the square of the
number of term candidates (i.e., the cost of calculating statistics for each
pair of terms).
2. Definition and hypernym extraction: In the second step, we first retrieve
candidate definitions from the input corpus, which costs on the order of
the corpus size.14 Each application of a WCL classifier to an input
candidate sentence s containing a term t costs on the order of the word
length of the sentence, and we have a constant number of such classifiers.
So the cost of this step is given by the sum of the lengths of the candidate
sentences in the corpus, which is lower than the word size of the corpus.
14 Note that this corpus consists of both free text and Web glossaries (cf. Section 3.2).
683
Computational Linguistics Volume 39, Number 3
3. Domain filtering and creation of the graph: The cost of domain filtering
for a single definition is in the order of its word length, so the running time
of domain filtering is in the order of the sum of the word size of the
acquired definitions. As for the hypernym graph creation, using an
adjacency-list representation of the graph Gnoisy, the dynamic addition of a
newly acquired hypernymy edge costs O(n), an operation which has to be
repeated for each (hypernymy, term) pair.
4. Graph pruning, consisting of the following steps:
 Graph trimming: This step requires O(n) time in order to identify
false leaves and false roots by iterating over the entire set of nodes.
 Edge weighting: i) We perform a DFS (O(n + m)) to weight all the
nodes in the graph; ii) we collect all paths from upper roots to any
given node, totalizing O(n!) paths in the worst case (i.e., in a
complete graph). In real domains, however, the computational cost
of this step will be much lower. In fact, over our six domains, the
average number of paths per node ranges from 4.3 (n = 2107,
ANIMALS) to 3175.1 (n = 2616, FINANCE domain): In the latter,
worst case, in practice, the number of paths is in the order of n, thus
the cost of this step, performed for each node, can be estimated by
O(n2) running time; iii) assigning maximum weights to edges costs
O(m) if in the previous step we keep track of the maximum value
of paths ending in each node h (see Equation (6)).
 Optimal branching: Identifying the connected components of our
graph costs O(n + m) time, identifying root candidates and
selecting one root per component costs O(n), and finally applying
the Chu-Liu/Edmonds algorithm costs O(m ? log2n) for sparse
graphs, O(n2) for dense ones, using Tarjan?s implementation
(Tarjan 1977).
5. Pruning recovery: In the worst case, m iterations of Algorithm 1 will be
performed, each costing O(n) time, thus having a total worst-case cost of
O(mn).
6. Edge recovery: For each pair of nodes in T0 we perform i) the
identification of cycles (O(n + m)) and ii) the calculation of the shortest
paths to the root (O(n + m)). By precomputing the shortest path for each
node, the cost of this step is O(n(n + m)) time.
Therefore, in practice, the computational complexity of OntoLearn Reloaded is
polynomial in the main variables of the problem, namely, the number of words in the
corpus and nodes in the noisy graph.
4. Evaluation
Ontology evaluation is a hard task that is difficult even for humans, mainly because
there is no unique way of modeling the domain of interest. Indeed several different
taxonomies might model a particular domain of interest equally well. Despite this
difficulty, various evaluation methods have been proposed in the literature for assessing
684
Velardi, Faralli, and Navigli OntoLearn Reloaded
the quality of a taxonomy. These include Brank, Mladenic, and Grobelnik (2006) and
Maedche, Pekar, and Staab (2002):
a) automatic evaluation against a gold standard;
b) manual evaluation performed by domain experts;
c) structural evaluation of the taxonomy;
d) application-driven evaluation, in which a taxonomy is assessed on the
basis of the improvement its use generates within an application.
Other quality indicators have been analyzed in the literature, such as accuracy,
completeness, consistency (Vo?lker et al 2008), and more theoretical features (Guarino
and Welty 2002) like essentiality, rigidity, and unity. Methods (a) and (b) are by far the
most popular ones. In this section, we will discuss in some detail the pros and cons of
these two approaches.
Gold standard evaluation. The most popular approach for the evaluation of lexicalized
taxonomies (adopted, e.g., in Snow, Jurafsky, and Ng 2006; Yang and Callan 2009;
and Kozareva and Hovy 2010) is to attempt to reconstruct an existing gold standard
(Maedche, Pekar, and Staab 2002), such as WordNet or the Open Directory Project.
This method is applicable when the set of taxonomy concepts are given, and the
evaluation task is restricted to measuring the ability to reproduce hypernymy links
between concept pairs. The evaluation is far more complex when learning a specialized
taxonomy entirely from scratch, that is, when both terms and relations are unknown.
In reference taxonomies, even in the same domain, the granularity and cotopy15 of an
abstract concept might vary according to the scope of the taxonomy and the expertise
of the team who created it (Maedche, Pekar, and Staab 2002). For example, both the
terms chiaroscuro and collage are classified under picture, image, icon in WordNet, but in
the Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspective
and shading techniques whereas collage is classified under image-making processes and
techniques. As long as common-sense, non-specialist knowledge is considered, it is still
feasible for an automated system to replicate an existing classification, because the
Web will provide abundant evidence for it. For example, Kozareva and Hovy (2010,
K&H hereafter) are very successful at reproducing the WordNet sub-taxonomy for
ANIMALS, because dozens of definitional patterns are found on the Web that classify,
for example, lion as a carnivorous feline mammal, or carnivorous, or feline. As we show
later in this section, however, and as also suggested by the previous AA&T example,
finding hypernymy patterns in more specialized domains is far more complex. Even in
simpler domains, however, it is not clear how to evaluate the concepts and relations not
found in the reference taxonomy. Concerning this issue, Zornitsa Kozareva comments
that: ?When we gave sets of terms to annotators and asked them to produce a taxonomy,
people struggled with the domain terminology and produced quite messy organization.
Therefore, we decided to go with WordNet and use it as a gold truth? (personal
communication). Accordingly, K&H do not provide an evaluation of the nodes and
relations other than those for which the ground truth is known. This is further clarified
in a personal communication: ?Currently we do not have a full list of all is-a outside
15 The cotopy of a concept is the set of its hypernyms and hyponyms.
16 http://www.getty.edu/vow/AATHierarchy.
685
Computational Linguistics Volume 39, Number 3
WordNet. [...] In the experiments, we work only with the terms present in WordNet
[...] The evaluation is based only on the WordNet relations. However, the harvesting
algorithm extracts much more. Currently, we do not know how to evaluate the Web
taxonomization.?
To conclude, gold standard evaluation has some evident drawbacks:
 When both concepts and relations are unknown, it is almost impossible to
replicate a reference taxonomy accurately.
 In principle, concepts not in the reference taxonomy can be either wrong
or correct; therefore the evaluation is in any case incomplete.
Another issue in gold standard evaluation is the definition of an adequate evalu-
ation metric. The most common measure used in the literature to compare a learned
with a gold-standard taxonomy is the overlapping factor (Maedche, Pekar, and Staab
2002). Given the set of is-a relations in the two taxonomies, the overlapping factor
simply computes the ratio between the intersection and union of these sets. Therefore
the overlapping factor gives a useful global measure of the similarity between the
two taxonomies. It provides no structural comparison, however: Errors or differences
in grouping concepts in progressively more general classes are not evidenced by this
measure.
Comparison against a gold standard has been analyzed in a more systematic way
by Zavitsanos, Paliouras, and Vouros (2011) and Brank, Mladenic, and Grobelnik (2006).
They propose two different strategies for escaping the ?naming? problem that we have
outlined. Zavitsanos, Paliouras, and Vouros (2011) propose transforming the ontology
concepts and their properties into distributions over the term space of the source data
from which the ontology has been learned. These distributions are used to compute
pairwise concept similarity between gold standard and learned ontologies.
Brank, Mladenic, and Grobelnik (2006) exploit the analogy between ontology learn-
ing and unsupervised clustering, and propose OntoRand, a modified version of the
Rand Index (Rand 1971) for computing the similarity between ontologies. Morey and
Agresti (1984) and Carpineto and Romano (2012), however, demonstrated a high de-
pendency of the Rand Index (and consequently of OntoRand itself) upon the number of
clusters, and Fowlkes and Mallows (1983) show that the Rand Index has the undesirable
property of converging to 1 as the number of clusters increases, even in the unrealistic
case of independent clusterings. These undesired outcomes have also been experienced
by Brank, Mladenic, and Grobelnik (2006, page 5), who note that ?the similarity of an
ontology to the original one is still as high as 0.74 even if only the top three levels of
the ontology have been kept.? Another problem with the OntoRand formula, as also
remarked in Zavitsanos, Paliouras, and Vouros (2011), is the requirement of comparing
ontologies with the same set of instances.
Manual evaluation. Comparison against a gold standard is important because it repre-
sents a sort of objective evaluation of an automated taxonomy learning method. As
we have already remarked, however, learning an existing taxonomy is not particularly
interesting in itself. Taxonomies are mostly needed in novel, often highly technical do-
mains for which there are no gold standards. For a system to claim to be able to acquire
a taxonomy from the ground up, manual evaluation seems indispensable. Nevertheless,
none of the taxonomy learning systems surveyed in Section 2 performs such evaluation.
Furthermore, manual evaluation should not be limited to an assessment of the acquired
686
Velardi, Faralli, and Navigli OntoLearn Reloaded
hypernymy relations ?in isolation,? but must also provide a structural assessment
aimed at identifying common phenomena and the overall quality of the taxonomic
structure. Unfortunately, as already pointed out, manual evaluation is a hard task.
Deciding whether or not a concept belongs to a given domain is more or less feasible
for a domain expert, but assessing the quality of a hypernymy link is far more complex.
On the other hand, asking a team of experts to blindly reconstruct a hierarchy, given a
set of terms, may result in the ?messy organization? reported by Zornitsa Kozareva. In
contrast to previous approaches to taxonomy induction, OntoLearn Reloaded provides
a natural solution to this problem, because is-a links in the taxonomy are supported by
one or more definition sentences from which the hypernymy relation was extracted. As
shown later in this section, definitions proved to be a very helpful feature in supporting
manual analysis, both for hypernym evaluation and structural assessment.
The rest of this section is organized as follows. We first describe the experimen-
tal set-up (Section 4.1): OntoLearn Reloaded is applied to the task of acquiring six
taxonomies, four of which attempt to replicate already existing gold standard sub-
hierarchies in WordNet17 and in the MeSH medical ontology,18 and the other two are
new taxonomies acquired from scratch. Next, we present a large-scale multi-faceted
evaluation of OntoLearn Reloaded focused on three of the previously described eval-
uation methods, namely: comparison against a gold standard, manual evaluation, and
structural evaluation. In Section 4.2 we introduce a novel measure for comparing an
induced taxonomy against a gold standard one. Finally, Section 4.3 is dedicated to a
manual evaluation of the six taxonomies.
4.1 Experimental Set-up
We now provide details on the set-up of our experiments.
4.1.1 Domains. We applied OntoLearn Reloaded to the task of acquiring six taxonomies:
ANIMALS, VEHICLES, PLANTS, VIRUSES, ARTIFICIAL INTELLIGENCE, and FINANCE.
The first four taxonomies were used for comparison against three WordNet sub-
hierarchies and the viruses sub-hierarchy of MeSH. The ANIMALS, VEHICLES, and
PLANTS domains were selected to allow for comparison with K&H, who experimented
on the same domains. The ARTIFICIAL INTELLIGENCE and FINANCE domains are ex-
amples of taxonomies truly built from the ground up, for which we provide a thorough
manual evaluation. These domains were selected because they are large, interdisci-
plinary, and continuously evolving fields, thus representing complex and specialized
use cases.
4.1.2 Definition Harvesting. For each domain, definitions were sought in Wikipedia and
in Web glossaries automatically obtained by means of a Web glossary extraction system
(Velardi, Navigli, and D?Amadio 2008). For the ARTIFICIAL INTELLIGENCE domain we
also used a collection consisting of the entire IJCAI proceedings from 1969 to 2011 and
the ACL archive from 1979 to 2010. In what follows we refer to this collection as the ?AI
corpus.? For FINANCE we used a combined corpus from the freely available collection
of Journal of Financial Economics from 1995 to 2012 and from Review Of Finance from 1997
to 2012 for a total of 1,575 papers.
17 http://wordnet.princeton.edu.
18 http://www.nlm.nih.gov/mesh/.
687
Computational Linguistics Volume 39, Number 3
4.1.3 Terminology. For the ANIMALS, VEHICLES, PLANTS, and VIRUSES domains, the
initial terminology was a fragment of the nodes of the reference taxonomies,19 sim-
ilarly to, and to provide a fair comparison with, K&H. For the AI domain instead,
the initial terminology was selected using our TermExtractor tool20 on the AI corpus.
TermExtractor extracted over 5,000 terms from the AI corpus, ranked according to a
combination of relevance indicators related to the (direct) document frequency, domain
pertinence, lexical cohesion, and other indicators (Sclano and Velardi 2007). We manu-
ally selected 2,218 terms from the initial set, with the aim of eliminating compounds
like order of magnitude, empirical study, international journal, that are frequent but not
domain relevant. For similar reasons a manual selection of terms was also applied to the
terminology automatically extracted for the FINANCE domain, obtaining 2,348 terms21
from those extracted by TermExtractor. An excerpt of extracted terms was provided in
Table 1.
4.1.4 Upper Terms. Concerning the selection of upper terms U (cf. Section 3.2), again
similarly to K&H, we used just one concept for each of the four domains focused
upon: ANIMALS, VEHICLES, PLANTS, and VIRUSES. For the AI and FINANCE domains,
which are more general and complex, we selected from WordNet a core taxonomy of
32 upper concepts U (resulting in 52 terms) that we used as a stopping criterion for
our iterative definition/hypernym extraction and filtering procedure (cf. Section 3.2).
The complete list of upper concepts was given in Table 2. WordNet upper concepts are
general enough to fit most domains, and in fact we used the same set U for AI and
FINANCE. Nothing, however, would have prevented us from using a domain-specific
core ontology, such as the CRM-CIDOC core ontology for the domain of ART AND
ARCHITECTURE.22
4.1.5 Algorithm Versions and Structural Statistics. For each of the six domains we ran the
three versions of our algorithm: without pruning recovery (TREE), with [1, 3] recovery
(DAG[1, 3]), and with [0, 99] recovery (DAG[0, 99]), for a total of 18 experiments. We
remind the reader that the purpose of the recovery process was to reattach some of the
edges deleted during the optimal branching step (cf. Section 3.5).
Figure 6 shows an excerpt of the AI tree-like taxonomy under the node data structure.
Notice that, even though the taxonomy looks good overall, there are still a few errors,
such as ?neuron is a neural network? and overspecializations like ?network is a digraph.?
Figure 7 shows a sub-hierarchy of the FINANCE tree-like taxonomy under the concept
value.
In Table 6 we give the structural details of the 18 taxonomies extracted for our six
domains. In the table, edge and node compression refers to the number of surviving
nodes and edges after the application of optimal branching and recovery steps to the
noisy hypernymy graph. To clarify the table, consider the case of VIRUSES, DAG[1, 3]:
we started with 281 initial terms, obtaining a noisy graph with 1,174 nodes and 1,859
edges. These were reduced to 297 nodes (i.e., 1,174?877) and 339 edges (i.e., 1,859?1,520)
after pruning and recovery. Out of the 297 surviving nodes, 222 belonged to the initial
19 For ANIMALS, VEHICLES, and PLANTS we used precisely the same seeds as K&H.
20 http://lcl.uniroma1.it/termextractor.
21 These dimensions are quite reasonable for large technical domains: as an example, The Economist?s
glossary of economic terms includes on the order of 500 terms (http://www.economist.com/
economics-a-to-z/).
22 http://cidoc.mediahost.org/standard crm(en)(E1).xml.
688
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 6
An excerpt of the ARTIFICIAL INTELLIGENCE taxonomy.
terminology; therefore the coverage over the initial terms is 0.79 (222/281). This means
that, for some of the initial terms, either no definitions were found, or the definition
was rejected in some of the processing steps. The table also shows, as expected, that the
term coverage is much higher for ?common-sense? domains like ANIMALS, VEHICLES,
and PLANTS, is still over 0.75 for VIRUSES and AI, and is a bit lower for FINANCE
(0.65). The maximum and average depth of the taxonomies appears to be quite variable,
with VIRUSES and FINANCE at the two extremes. Finally, Table 6 reports in the last
column the number of glosses (i.e., domain definitional sentences) obtained in each
run. We would like to point out that providing textual glosses for the retrieved domain
hypernyms is a novel feature that has been lacking in all previous approaches to
ontology learning, and which can also provide key support to much-needed manual
validation and enrichment of existing semantic networks (Navigli and Ponzetto 2012).
4.2 Evaluation Against a Gold Standard
In this section we propose a novel, general measure for the evaluation of a learned
taxonomy against a gold standard. We borrow the Brank, Mladenic, and Grobelnik
689
Computational Linguistics Volume 39, Number 3
Figure 7
An excerpt of the FINANCE taxonomy.
(2006) idea of exploiting the analogy with unsupervised clustering but, rather than
representing the two taxonomies as flat clusterings, we propose a measure that takes
into account the hierarchical structure of the two analyzed taxonomies. Under this
perspective, a taxonomy can be transformed into a hierarchical clustering by replacing
each label of a non-leaf node (e.g., perspective and shading techniques) with the transitive
closure of its hyponyms (e.g., cangiatismo, chiaroscuro, foreshortening, hatching).
4.2.1 Evaluation Model. Techniques for comparing clustering results have been surveyed
in Wagner and Wagner (2007), although the only method for comparing hierarchical
clusters, to the best of our knowledge, is that proposed by Fowlkes and Mallows (1983).
Suppose that we have two hierarchical clusterings H1 and H2, with an identical set of n
objects. Let k be the maximum depth of both H1 and H2, and Hij a cut of the hierarchy,
where i ? {0, . . . , k} is the cut level and j ? {1, 2} selects the clustering of interest. Then,
for each cut i, the two hierarchies can be seen as two flat clusterings Ci1 and C
i
2 of the n
concepts. When i = 0 the cut is a single cluster incorporating all the objects, and when
i = k we obtain n singleton clusters. Now let:
 n11 be the number of object pairs that are in the same cluster in both Ci1
and Ci2;
 n00 be the number of object pairs that are in different clusters in both Ci1
and Ci2;
 n10 be the number of object pairs that are in the same cluster in Ci1 but
not in Ci2;
690
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 6
Structural evaluation of three versions of our taxonomy-learning algorithm on six different
domains.
Experiment Term Coverage Depth |V| |E| V Compress. E Compress. Glosses
A
I
TREE 75.51% 12 max 2,387 2,386 43.00% 67.31% 1,249(1,675/2,218) 6.00 avg (1,801/4,188) (4,915/7,301)
DAG [1,3] 75.51% 19 max 2,387 3,554 43.00% 51.32% 2,081(1,675/2,218) 8.27 avg (1,801/4,188) (3,747/7,301)
DAG [0,99] 75.51% 20 max 2,387 3,994 43.00% 45.29% 2,439(1,675/2,218) 8.74 avg (1,801/4,188) (3,307/7,301)
FI
N
A
N
C
E
TREE 65.20% 14 max 2,038 2,037 22.09% 47.99% 1,064(1,533/2,348) 6.83 avg (578/2,616) (1,880/3,917)
DAG [1,3] 65.20% 38 max 2,038 2,524 22.09% 35.56% 1,523(1,533/2,348) 18.82 avg (578/2,616) (1,393/3,917)
DAG [0,99] 65.20% 65 max 2,038 2,690 22.09% 31.32% 1,677(1,533/2,348) 33.54 avg (578/2,616) (1,227/3,917)
V
IR
U
SE
S
TREE 79.00% 5 max 297 296 74.70% 84.07% 172(222/281) 2.13 avg (877/1,174) (1,563/1,859)
DAG [1,3] 79.00% 5 max 297 339 74.70% 81.76% 212(222/281) 2.20 avg (877/1,174) (1,520/1,859)
DAG [0,99] 79.00% 5 max 297 360 74.70% 80.63% 233(222/281) 2.32 avg (877/1,174) (1,563/1,859)
A
N
IM
A
L
S
TREE 93.56% 10 max 900 899 57.28% 66.96% 724(640/684) 4.35 avg (1,207/2,107) (1,822/2,721)
DAG [1,3] 93.56% 16 max 900 1,049 57.28% 61.44% 872(640/684) 5.21 avg (1,207/2,107) (1,672/2,721)
DAG [0,99] 93.56% 16 max 900 1,116 57.28% 58.98% 939(640/684) 5.39 avg (1,207/2,107) (1,605/2,721)
P
L
A
N
T
S
TREE 96.57% 19 max 710 709 72.69% 84.53% 638(535/554) 5.85 avg (1,890/2,600) (3,877/4,586)
DAG [1,3] 96.57% 19 max 710 922 72.69% 79.89% 851(535/554) 6.65 avg (1,890/2,600) (3,664/4,586)
DAG [0,99] 96.57% 19 max 710 1,242 72.69% 72.91% 1,171(535/554) 6.54 avg (1,890/2,600) (3,344/4,586)
V
E
H
IC
L
E
S
TREE 95.72% 8 max 169 168 71.50% 80.48% 150(112/117) 3.44 avg (424/593) (693/861)
DAG [1,3] 95.72% 8 max 169 200 71.50% 76.77% 182(112/117) 3.94 avg (424/593) (661/861)
DAG [0,99] 95.72% 10 max 169 231 71.50% 73.17% 213(112/117) 4.48 avg (424/593) (630/861)
 n01 be the number of object pairs that are in the same cluster in Ci2 but not
in Ci1;
The generalized Fowlkes and Mallows (F&M) measure of cluster similarity for the
cut i (i ? {0, . . . , k}), as reformulated by Wagner and Wagner (2007), is defined as:
Bi1,2 =
ni11
?
(ni11 + n
i
10) ? (ni11 + ni01)
. (8)
Note that the formula can be interpreted as the geometric mean of precision and
recall of an automated method in clustering the same concept pairs as in a gold-standard
691
Computational Linguistics Volume 39, Number 3
clustering. This formula has a few undesirable properties: first, the value of Bi1,2 gets
close to its maximum 1.0 as we approach the root of the hierarchy (i = 0); second, the
two hierarchies need to have the same maximum depth k; third, the hierarchies need to
have the same number of initial objects and a crisp classification.
In order to apply the F&M measure to the task of comparing a learned and a gold-
standard taxonomy, we need to mitigate these problems. Equation (8) copes with the
third problem without modifications. In fact, if the sets of objects in H1 and H2 are
different, the integers n10 and n01 can be considered as also including objects that belong
to one hierarchy and not to the other. In this case, the value of B01,2 will provide a measure
of the overlapping objects in the learned taxonomy and the gold standard one. In order
to take into account multiple (rather than crisp) classifications, again, there is no need
to change the formula, which is still meaningful if an object is allowed to belong to
more than one cluster. As before, mismatches between H1 and H2 would result in higher
values of n10 and n01 and lower Bi1,2.
A more serious problem with Equation (8) is that the lower the value of i, the higher
the value of the formula, whereas, ideally, we would like to reward similar clusterings
when the clustering task is more difficult and fine-grained, that is, for cuts that are close
to the leaf nodes. To assign a reward to ?early? similarity values, we weight the values
of Bi1,2 with a coefficient
i+1
k . We can then compute a cumulative measure of similarity
with the following formula:
B1,2 =
?k?1
i=0
i+1
k B
i
1,2
?k?1
i=0
i+1
k
=
?k?1
i=0
i+1
k B
i
1,2
k+1
2
. (9)
Finally, to solve the problem of different depths of the two hierarchies, we define a
policy that penalizes a learned taxonomy that is less structured than the gold standard
one, and rewards?or at least does not penalize?the opposite case.
As an example, consider Figure 8, which shows two taxonomies H1 and H2, with
non-identical sets of objects {a, b, c, d, e, f} and {a, b, c, d, e, g}. In the figure each edge is
labeled by its distance from the root node (the value i in the F&M formula). Notice that
H1 and H2 have multiple classifications (i.e., multiple hypernyms in our case) for the
object e, thus modeling the common problem of lexical ambiguity and polysemy. Let
us suppose that H1 is the learned taxonomy, and H2 the gold standard one. We start
comparing the clusterings at cut 0 and stop at cut kr ? 1, where kr is the depth of the
Figure 8
Two hierarchical clusters of n non-identical objects.
692
Velardi, Faralli, and Navigli OntoLearn Reloaded
gold standard taxonomy. This means that if the learned taxonomy is less structured
we replicate the cut kl ? 1 for kr ? kl times (where kl is the maximum depth of the
learned taxonomy), whereas if it is more structured we stop at cut kr ? 1. In contrast to
previous evaluation models, our aim is to reward (instead of penalize) more structured
taxonomies provided they still match the gold standard one.
Table 7 shows the cuts from 0 to 3 of H1 and H2 and the values of Bi1,2. For i = 2 the
B value is 0, if H2 is the learned taxonomy, and is not defined, if H2 is the gold standard.
Therefore, when computing the cumulative Equation (9), we obtain the desired effect of
penalizing less the structured learned taxonomies. Note that, when the two hierarchies
have different depths, the value k ? 1 in Equation (9) is replaced by kr ? 1.
Finally, we briefly compare our evaluation approach with the OntoRand index,
introduced by Brank, Mladenic, and Grobelnik (2006). The Rand Index measures the
similarity between two clusterings Cl and Cr by the formula:
R(Cl, Cr) =
2(n11 + n00)
n(n ? 1) (10)
where n11, n00, and n have the same meaning described earlier. In Brank, Mladenic,
and Grobelnik (2006), a clustering is obtained from an ontology by associating each
ontology instance to its concept. The set of clusters is hence represented by the set of
leaf concepts in the hierarchy, namely, according to our notation, the clustering Ck?1i . In
order to take into account the hierarchical structure, they define the OntoRand formula.
This measure, rather than summing up to 1 or 0, depending on whether or not two
given instances i and j belong to the same cluster in the compared ontologies, returns a
real number in [0, 1] depending upon the distance between i and j in terms of common
ancestors. In other terms, if i and j do not belong to the same concept but have a very
close common ancestor, the OntoRand measure returns a value still close to 1.
Our measure has several advantages over the OntoRand index:
i) It allows for a comparison at different levels of depth of the hierarchy,
and the cumulative similarity measure penalizes the contribution of the
highest cuts of the hierarchy.
ii) It does not require that the two hierarchies have the same depth, nor that
they have the same number of leaf nodes.
iii) The measure can be extended to lattices (e.g., it is not required that each
object belongs precisely to one cluster).
Table 7
Application of the evaluation method to the hierarchies of Figure 8. The values of Bi1,2 are shown
both when H1 and H2 are the learned taxonomy (penultimate and last column, respectively).
i C1 C2 n11 n10 n01 H1 H2
Bi1,2
0 {a,b,c,d,e,f} {a,b,c,d,e,g} 10 5 5 0.67 0.67
1 {a,b,c,d,e},{e,f} {a,b,c,d,e},{e},{g} 10 1 0 0.95 0.95
2 {a,b},{c,d},{e},{f} {a},{b},{c},{d},{e},{g} 0 2 0 n.a. 0
3 {a},{b},{c},{d},{e},{f} {a},{b},{c},{d},{e},{g} 0 0 0 n.a. n.a.
693
Computational Linguistics Volume 39, Number 3
iv) It is not dependent, as the Rand Index is, on the number n00, the value of
which has the undesirable effect of producing an ever higher similarity as
the number of singleton clusters grows (Morey and Agresti 1984).
4.2.2 Results. This section presents the results of the F&M evaluation model for gold
standard evaluation, therefore we focus on four domains and do not consider AI and
FINANCE. The three WordNet sub-hierarchies are also compared with the taxonomies
automatically created by Kozareva and Hovy (2010) in the same domains, kindly made
available by the authors. It is once more to be noted that Kozareva and Hovy, during hy-
pernym extraction, reject all the nodes not belonging to WordNet, whereas we assume
no a-priori knowledge of the domain, apart from adopting the same set of seed terms
used by K&H.
Figure 9 shows, for each domain (ANIMALS, PLANTS, VEHICLES, and VIRUSES), and
for each cut level of the hierarchy, a plot of Bi1,2 values multiplied by the penalty factor.
As far as the comparison with K&H is concerned, we notice that, though K&H obtain
better performance in general, OntoLearn has higher coverage over the domain, as is
shown by the highest values for i = 0, and has a higher depth of the derived hierarchy,
especially with DAG[0, 99]. Another recurrent phenomenon is that K&H curves grace-
fully degrade from the root to the leaf nodes, possibly with a peak in the intermediate
levels, whereas OntoLearn has a hollow in the mid-high region (see the region 4?6 for
ANIMALS and 1?2 for the other three hierarchies) and often a relative peak in the lowest
Figure 9
Gold standard evaluation of our three versions of OntoLearn Reloaded against WordNet
(ANIMALS, PLANTS, and VEHICLES) and MeSH (VIRUSES). A comparison with K&H is also
shown for the first three domains.
694
Velardi, Faralli, and Navigli OntoLearn Reloaded
levels. In the manual evaluation section we explain this phenomenon, which also occurs
in the ARTIFICIAL INTELLIGENCE taxonomy.
The generally decreasing values of Bi1,2 in Figure 9 show that, as expected, mim-
icking the clustering criteria of a taxonomy created by a team of experts proves very
difficult at the lowest levels, while performance grows at the highest levels. At the
lowest taxonomy levels there are two opposing phenomena: overgeneralization and
overspecialization. For example, macaque has monkey as a direct hypernym in WordNet,
and we find short-tailed monkey as a direct hypernym of macaque. An opposite case is
ganoid, which is a taleostan in WordNet and simply a fish in our taxonomy. The first
case does not reward the learned taxonomy (though, unlike for the overlapping factor
[Maedche, Pekar, and Staab 2002], it does not cause a penalty), whereas the second is
quite penalizing. More of these examples will be provided in Section 4.3.
Finally, in Table 8 we show the cumulative B1,2 values for the four domains, ac-
cording to Equation (9). Here, except for the VEHICLES domain, the unconstrained
DAG[0, 99] performs best.
4.3 Manual Evaluation
This section is dedicated to the manual assessment of the learned ontologies. The
section is divided in three parts: Section 4.3.1 is concerned with the human validation of
hypernymy relations, Section 4.3.2 examines the global learned taxonomic structure in
the search for common phenomena across the six domains, and finally Section 4.3.3 in-
vestigates the possibility of enhancing our hypernymy harvesting method with K&H?s
Hearst-like patterns, applying their method to the AI domain and manually evaluating
the extracted hypernyms.
4.3.1 Hypernym Evaluation. To reduce subjectivity in taxonomy evaluation, we asked
three annotators, only one of whom was a co-author, to validate, for each of the three
experiments of each of the six domains, a random sample of hypernymy relations. For
each relation the definition(s) supporting the relation were also provided. This was
especially helpful for domains like VIRUSES, but also PLANTS and ANIMALS, in which
the annotators were not expert. The size of each random sample was 300 for the (larger)
AI and FINANCE domains and 100 for the others.
Each evaluator was asked to tag incorrect relations, regardless of whether the error
was due to the selection of non-domain definitions (e.g., for VEHICLES: ?a driver is a
golf club with a near vertical face that is used for hitting long shots from the tee?), to
a poor definition (e.g., for AI: ?a principle is a fundamental essence, particularly one
producing a given quality?) or to a wrong selection of the hypernym. As an example of
the latter, in the PLANTS domain, we extracted the hypernym species from the sentence:
?geranium is a genus of 422 species of flowering annual, biennial, and perennial plants
Table 8
Values of B1,2 for the domains of VIRUSES, ANIMALS, PLANTS, and VEHICLES.
Experiment VIRUSES ANIMALS PLANTS VEHICLES
TREE 0.093 0.064 0.059 0.065
DAG [1,3] 0.101 0.062 0.072 0.069
DAG [0,99] 0.115 0.097 0.080 0.103
K&H n.a. 0.067 0.068 0.158
695
Computational Linguistics Volume 39, Number 3
Table 9
Precision of hypernym edges on six domains (calculated on a majority basis) and inter-annotator
agreement on the corresponding sample of relations.
Experiment # of Sample Precision ?
AI
TREE 300 80.3% [241/300] 0.45
DAG [1,3] 300 73.6% [221/300] 0.42
DAG [0,99] 300 73.0% [219/300] 0.41
FINANCE
TREE 300 93.6% [281/300] 0.40
DAG [1,3] 300 93.0% [279/300] 0.43
DAG [0,99] 300 92.6% [278/300] 0.41
VIRUSES
TREE 100 99.0% [99/100] 0.49
DAG [1,3] 100 99.0% [99/100] 0.39
DAG [0,99] 100 99.0% [99/100] 0.32
ANIMALS
TREE 100 92.0% [92/100] 0.53
DAG [1,3] 100 92.0% [92/100] 0.36
DAG [0,99] 100 90.0% [90/100] 0.56
PLANTS
TREE 100 89.0% [89/100] 0.49
DAG [1,3] 100 85.0% [85/100] 0.53
DAG [0,99] 100 97.0% [97/100] 0.26
VEHICLES
TREE 100 92.0% [92/100] 0.64
DAG [1,3] 100 92.0% [92/100] 0.49
DAG [0,99] 100 91.0% [91/100] 0.44
? Interpretation
< 0 Poor agreement
0.01?0.20 Slight agreement
0.21?0.40 Fair agreement
0.41?0.60 Moderate agreement
0.61?0.80 Substantial agreement
0.81?1.00 Almost perfect agreement
that are commonly known as the cranesbills? since, in the WCL verb set, we have ?is
a * species of? and ?is a * genus of?, but not the concatenation of these two patterns.
Annotators could mark with ? a hyponym?hypernym pair for which they felt uncertain.
Though it would have been useful to distinguish between the different types of error,
we found that regarding many error types there was, anyway, very low inter-annotator
agreement. Indeed the annotation task would appear to be intrinsically complex and
controversial. In any case, an assessment of the definition and hypernym extraction
tasks in isolation was already provided by Navigli and Velardi (2010).
Table 9 summarizes the results. Precision of each classification was computed on a
majority basis, and we used Fleiss? kappa statistics (Fleiss 1971) to measure the inter-
annotator agreement. In general, the precision is rather good, though it is lower for the
AI domain, probably due to its high ?vitality? (many new terms continuously arise, and
for some of them it is difficult to find good quality definitions). In general, precision is
higher in focused domains (VIRUSES, ANIMALS, PLANTS, and VEHICLES) than in wide-
range domains (AI and FINANCE). The former domains, however, have just one quite
696
Velardi, Faralli, and Navigli OntoLearn Reloaded
?narrow? upper concept (virus for VIRUSES, etc.), whereas AI and FINANCE have several
upper concepts (e.g., person or abstraction), and furthermore they are less focused. This
means that there is an inherently higher ambiguity and this may be seen as justifying
the lower performance. In Table 9 we also note that TREE structures achieve in general a
higher precision, except for PLANTS, whereas the DAG has the advantage of improving
recall (see also Section 4.2.2).
Note that high precision here does not contradict the results shown in Section 4.2.2:
In this case, each single relation is evaluated in isolation, therefore overgenerality or
overspecificity do not imply a penalty, provided the relation is judged to be correct.
Furthermore, global consistency is not considered here: for example, distance metric
learning ? parametric technique, and eventually ends up in technique, whereas belief
network learning ? machine learning algorithm ends up in algorithm and then in procedure.
In isolation, these hypernymy patterns are acceptable, but within a taxonomic structure
one would like to see a category node grouping all terms denoting machine learning
algorithms. This behavior should be favored by the node weighting strategy described
in Section 3.4, aimed at attracting nodes with multiple hypernyms towards the most
populated category nodes. As in the previous example, however, there are category
nodes that are almost equally ?attractive? (e.g., algorithm and technique), and, further-
more, the taxonomy induction algorithm can only select among the set of hypernyms
extracted during the harvesting phase. Consequently, when no definition suggests that
distance metric learning is a hyponym of machine learning algorithm, or of any other
concept connected to machine learning algorithm, there is no way of grouping distance
metric learning and belief network learning in the desired way. This task must be postponed
to manual post-editing.
Concerning the kappa statistics, we note that the values range from moderate to
substantial in most cases. These numbers are apparently low, but the task of evaluating
hypernymy relations is quite a complex one. Similar kappa values were obtained in
Yang and Callan (2008) in a human-guided ontology learning task.
4.3.2 Structural Assessment. In addition to the manual evaluation summarized in
Table 9, a structural assessment was performed to identify the main sources of error.
To this end, one of the authors analyzed the full AI and FINANCE taxonomies and a
sample of the other four domains in search of recurring errors. In general, our optimal
branching algorithm and weighting schema avoids many of the problems highlighted in
well-known studies on taxonomy acquisition from dictionaries (Ide and Ve?ronis 1993),
like circularity, over-generalization, and so forth. There are new problems to be faced,
however.
The main sources of error are the following:
 Ambiguity of terms, especially at the intermediate levels
 Low quality of definitions
 Hypernyms described by a clause rather than by a single- or multi-word
expression
 Lack of an appropriate WCL to analyze the definition
 Difficulty of extracting the correct hypernym string from phrases with
identical syntactic structure
We now provide examples for each of these cases.
697
Computational Linguistics Volume 39, Number 3
Figure 10
Error distribution of the TREE version of our algorithm on the ARTIFICIAL INTELLIGENCE
domain.
Ambiguity. Concerning ambiguity of terms, consider Figures 10 and 11, which show the
distribution of errors at the different levels of the learned AI and FINANCE taxonomies
for the TREE experiment. The figures provide strong evidence that most errors are
located in the intermediate levels of the taxonomy. As we move from leaf nodes to
the upper ontology, the extracted terms become progressively more general and con-
sequently more ambiguous. For these terms the domain heuristics may turn out to be
inadequate, especially if the definition is a short sentence.
But why are these errors frequent at the intermediate levels and not at the highest
levels? To understand this, consider the following example from the AI domain: For
the term classifier the wrong hypernym is selected from the sentence ?classifier is a
person who creates classifications.? In many cases, wrong hypernyms do not accumulate
sufficient weight and create ?dead-end? hypernymy chains, which are pruned during
the optimal branching step. But, unfortunately, a domain appropriate definition is
Figure 11
Error distribution of the TREE version of our algorithm on the FINANCE domain.
698
Velardi, Faralli, and Navigli OntoLearn Reloaded
found for person: ?person is the more general category of an individual,? due to the
presence of the domain word category. On the other hand, this new sentence produces
an attachment that, in a sense, recovers the error, because category is a ?good? domain
concept that eventually ends up in subsequent iterations to the upper node abstraction.
Therefore, what happens is that the upper taxonomy nodes, with the help of the domain
heuristic, mitigate the ?semantic drift? caused by out-of-domain ambiguity, recovering
the ambiguity errors of the intermediate levels. This phenomenon is consistently found
in all domains, as shown by the hollow that we noticed in the graphs of Section 4.2.2.
An example in the ANIMALS domain is represented by the hypernymy sequence
fawn ? color ? race ? breed ? domestic animal, where the wrong hypernym color was
originated by the sentence ?fawn is a light yellowish brown color that is usually used in
reference to a dog?s coat color.? Only in VIRUSES is the phenomenon mitigated by the
highly specific and very focused nature of the domain.
In addition to out-of-domain ambiguity, we have two other phenomena: in-domain
ambiguity and polysemy. In-domain ambiguity is rare, but not absent (Agirre et al
2010; Faralli and Navigli 2012). Consider the example of Figure 12a, from the VEHICLES
domain: tractor has two definitions corresponding to two meanings, which are both
correct. The airplane meaning is ?tractor is an airplane where the propeller is located in
front of the fuselage,? whereas the truck meaning is ?tractor is a truck for pulling a semi-
trailer or trailer.? Here the three hyponyms of tractor (see the figure) all belong to the
truck sense. We leave to future developments the task of splitting in-domain ambiguous
nodes in the appropriate way.
Another case is systematic polysemy, which is shown in Figure 13. The graph in
the figure, from the AI domain, captures the fact that a semantic network, as well as
its hyponyms, are both a methodology and a representation. Another example is shown
in Figure 12b for the PLANTS domain, where systematic polysemy can be observed
for terms like olive, orange, and breadfruit, which are classified as evergreen tree and
fruit. Polysemy, however, does not cause errors, as it does for in-domain ambiguity,
because hyponyms of polysemous concepts inherit the polysemy: In the two graphs
of Figures 13 and 12b, both partitioned semantic network and tangerine preserve the
polysemy of their ancestors. Note that in-domain ambiguity and polysemy are only
captured by the DAG structure; therefore this can be seen as a further advantage (in
addition to higher recall) of the DAG model over and against the more precise TREE
structure.
Figure 12
An example of in-domain ambiguity (a) and an example of systematic polysemy (b). Dashed
edges were added to the graph as a result of the edge recovery phase (see Section 3.5).
699
Computational Linguistics Volume 39, Number 3
Figure 13
An example of systematic polysemy. Dashed edges were added to the graph as a result of the
edge recovery phase (see Section 3.5).
Low quality of definitions. Often textual definitions, especially if extracted from the
Web, do not have a high quality. Examples are: ?artificial intelligence is the next big
development in computing? or ?aspectual classification is also a necessary prerequi-
site for interpreting certain adverbial adjuncts.? These sentences are definitions on a
syntactic ground, but not on a semantic ground. As will be shown in Section 4.3.3,
this problem is much less pervasive than for Hearst-like lexico-syntactic patterns,
although, neither domain heuristics nor the graph pruning could completely eliminate
the problem. We can also include overgeneralization in this category of problems: Our
algorithm prefers specific hypernyms to general hypernyms, but for certain terms no
specific definitions are found. The elective way to solve this problem would be to assign
a quality confidence score to the definition source (document or Web page), for example,
by performing an accurate and stable classification of its genre (Petrenz and Webber
2011).
Hypernym is a clause. There are cases in which, although very descriptive and good
quality definitions are found, it is not possible to summarize the hypernym with a
term or multi-word expression. For example ?anaphora resolution is the process of
determining whether two expressions in natural language refer to the same real world
entity.? OntoLearn extracts process of determining which ends up in procedure, process.
This is not completely wrong, however, and in some case is even fully acceptable, as
for ?summarizing is a process of condensing or expressing in short something you
have read, watched or heard?: here, process of condensing is an acceptable hypernym.
An example for FINANCE is: ?market-to-book ratio is book value of assets minus book
value of equity plus market value of equity,? where we extracted book value, rather than
the complete formula. Another example is: ?roa is defined as a ratio of operating income
to book value of assets,? from which we extracted ratio, which is, instead, acceptable.
Lack of an appropriate definitional pattern. Though we acquired hundreds of different
definitional patterns, there are still definitions that are not correctly parsed. We already
700
Velardi, Faralli, and Navigli OntoLearn Reloaded
mentioned the geranium example in the PLANTS domain. An example in the AI domain
is ?execution monitoring is the robot?s process of observing the world for discrepancies
between the actual world and its internal representation of it,? where the extracted
hypernym is robot because we have no WCL with a Saxon genitive.
Wrong hypernym string. This is the case in which the hypernym is a superstring or
substring of the correct one, like: ?latent semantic analysis is a machine learning proce-
dure.? Here, the correct hypernym is machine learning procedure, but OntoLearn extracts
machine because learning is POS tagged as a verb. In general, it is not possible to evaluate
the extent of the hypernym phrase except case-by-case. The lattice learner acquired a
variety of hypernymy patterns, but the precision of certain patterns might be quite low.
For example, the hypernymy pattern ?* of *? is acceptable for ?In grammar, a lexical
category is a linguistic category of words? or ?page rank is a measure of site popularity?
but not for ?page rank is only a factor of the amount of incoming and outgoing links
to your site? nor for ?pattern recognition is an artificial intelligence area of considerable
importance.? The same applies to the hypernymy pattern ADJ NN: important algorithm is
wrong, although greedy algorithm is correct.
4.3.3 Evaluation of Lexico-Syntactic Patterns. As previously remarked, Kozareva and Hovy
(2010) do not actually apply their algorithm to the task of creating a new taxonomy, but
rather they try to reproduce three WordNet taxonomies, under the assumption that the
taxonomy nodes are known (cf. Section 4). Therefore, there is no evidence of the preci-
sion of their method on new domains, where the category nodes are unknown. On the
other hand, if Hearst?s patterns, which are at the basis of K&H?s hypernymy harvesting
algorithm, could show adequate precision, we would use them in combination with
our definitional patterns. This section investigates the matter.
As briefly summarized in Section 2, K&H create a hypernym graph in three steps.
Given a few root concepts (e.g., animal) and basic level concepts or instances (e.g.,
lion), they:
1) harvest new basic and intermediate concepts from the Web in an iterative
fashion, using doubly anchored patterns (DAP) like ??root? such as ?seed?
and ?? and inverse DAP (i.e., DAP?1) like ?? such as ?term1? and ?term2??.
The procedure is iterated until no new terms can be found;
2) rank the nodes extracted with DAP by out-degree and those extracted
with inverse DAP by in-degree, so as to prune out less promising terms;
3) induce the final taxonomic structure by positioning the intermediate nodes
between basic level and root terms using a concept positioning procedure
based on a variety of Hearst-like surface patterns. Finally, they eliminate
cycles, as well as nodes with no predecessor or no successor, and they
select the longest path in the case of multiple paths between node pairs.
In this section we apply their method23 to the domain of AI in order to manually
analyze the quality of the extracted relations. To replicate the first two steps of K&H
algorithm we fed the algorithm with a growing set of seed terms randomly selected
from our validated terminology, together with their hierarchically related root terms
23 We followed the exact procedure described in Figure 2 of Kozareva & Hovy (2010).
701
Computational Linguistics Volume 39, Number 3
Table 10
K&H performance on the AI domain.
number of root/seed pairs 1 10 100 1,000
# new concepts 131 163 227 247
# extracted is-a relations 114 146 217 237
correct and in-domain 21.05% 24.65% 18.89% 18.56%
(24/114) (36/146) (41/217) (44/237)
in the upper taxonomy (e.g., unsupervised learning is a method or maximum entropy is a
measure). We then performed the DAP and DAP?1 steps iteratively until no more terms
could be retrieved, and we manually evaluated the quality of the harvested concepts
and taxonomic relations using the same thresholding formula described in K&H.24 We
give the results in Table 10.
As we said earlier, our purpose here is mainly to evaluate the quality of Hearst
patterns in more technical domains, and the efficacy of DAP and DAP?1 steps in
retrieving domain concepts and relations. Therefore, replicating step (3) above is not
useful in this case since, rather than adding new nodes, step (3) is aimed, as in our
optimal branching and pruning recovery steps, at reorganizing and trimming the final
graph.
Table 10 should be compared with the first three rows (AI) of Table 9: It shows that
in the absence of a priori knowledge on the domain concepts the quantity and quality
of the is-a links extracted by the K&H algorithm is much lower than those extracted by
OntoLearn Reloaded. First, the number of new nodes found by the K&H algorithm is
quite low: For the same domain of ARTIFICIAL INTELLIGENCE, our method, as shown in
Table 9, is able to extract from scratch 2,387 ? 52 = 2,335 nodes,25 in comparison with the
247 new nodes of Table 10, obtained with 1,000 seeds. Second, many nodes extracted by
the K&H algorithm, like fan speed, guidelines, chemical engineering, and so on, are out-of-
domain and many hypernym relations are incorrect irrespective of their direction, like
computer program is a slow and data mining is a contemporary computing problem. Third, the
vast majority of the retrieved hypernyms are overgeneral, like discipline, method, area,
problem, technique, topic, and so forth, resulting in an almost flat hypernymy structure. A
high in-degree threshold and a very high number of seeds do not mitigate the problem,
demonstrating that Hearst-like patterns are not very good at harvesting many valid
hypernym relations in specialized domains.26
Following this evaluation, we can outline several advantages of our method over
K&H?s work (and, as a consequence, over Hearst?s patterns):
i) We obtain higher precision and recall when no a priori knowledge is
available on the taxonomy concepts, because hypernyms are extracted
from expert knowledge on the Web (i.e., technical definitions rather than
patterns reflecting everyday language).
24 The technique is based on the in-degree and out-degree of the graph nodes.
25 Remember that the 52 domain-independent upper terms are manually defined (cf. Section 4.1.4).
26 This result is in line with previous findings in a larger, domain-balanced experiment (Navigli and Velardi
2010) in which we have shown that WCLs outperform Hearst patterns and other methods in the task of
hypernym extraction.
702
Velardi, Faralli, and Navigli OntoLearn Reloaded
ii) We cope better with sense ambiguity via the domain filtering step.27
iii) We use a principled algorithmic approach to graph pruning and cycle
removal.28
iv) Thanks to the support provided by textual definitions, we are able to
cope with the problem of manually evaluating the retrieved concepts
and relations, even in the absence of a reference taxonomy.
4.3.4 Summary of Findings. We here summarize the main findings of our manifold
evaluation experiments:
i) With regard to the two versions of our graph pruning algorithm, we found
that TREE structures are more precise, whereas DAGs have a higher recall.
ii) Errors are mostly concentrated in the mid-level of the hierarchy, where
concepts are more ambiguous and the ?attractive? power of top nodes is
less influential. This was highlighted by our quantitative (F&M) model
and justified by means of manual analysis.
iii) The quality and number of definitions is critical for high performance.
Less-focused domains in which new terms continuously emerge are the
most complex ones, because it is more difficult to retrieve high-quality
definitions for them.
iv) Definitions, on the other hand, are a much more precise and high-coverage
source of knowledge for hypernym extraction than (Hearst-like) patterns
or contexts, because they explicitly represent expert knowledge on a
given domain. Furthermore, they are a very useful support for manual
validation and structural analysis.
5. Conclusions
In this paper we presented OntoLearn Reloaded, a graph-based algorithm for learning
a taxonomy from scratch using highly dense, potentially disconnected, hypernymy
graphs. The algorithm performs the task of eliminating noise from the initial graph
remarkably well on arbitrary, possibly specialized, domains, using a weighting scheme
that draws both on the topological properties of the graph and on some general prin-
ciples of taxonomic structures. OntoLearn Reloaded provides a considerable advance-
ment over the state of the art in taxonomy learning. First, it is the first algorithm that
experimentally demonstrates its ability to build a new taxonomy from the ground up,
without any a priori assumption on the domain except for a corpus and a set of (possibly
general) upper terms. The majority of existing systems start from a set of concepts
and induce hypernymy links between these concepts. Instead, we automatically learn
both concepts and relations via term extraction and iterative definition and hypernym
27 In the authors? words (Kozareva and Hovy 2010, page 1,115): ?we found that the learned terms in the
middle ranking do not refer to the meaning of vehicle as a transportation device, but to the meaning of
vehicle as media (i.e., seminar, newspapers), communication and marketing.?
28 Again in the authors? words (Kozareva and Hovy 2010, page 1,115): ?we found that in-degree is not
sufficient by itself. For example, highly frequent but irrelevant hypernyms such as meats and others are
ranked at the top of the list, while low frequent but relevant ones such as protochordates, hooved-mammals,
homeotherms are discarded.?
703
Computational Linguistics Volume 39, Number 3
extraction. Second, we cope with issues such as term ambiguity, complexity, and
multiplicity of hypernymy patterns. Third, we contribute a multi-faceted evaluation,
which includes a comparison against gold standards, plus a structural and a manual
evaluation. Taxonomy induction was applied to the task of creating new ARTIFICIAL
INTELLIGENCE and FINANCE taxonomies and four taxonomies for gold-standard
comparison against WordNet and MeSH.29
Our experimental analysis shows that OntoLearn Reloaded greatly simplifies the
task of acquiring a taxonomy from scratch: Using a taxonomy validation tool,30 a team
of experts can correct the errors and create a much more acceptable taxonomy in a
matter of hours, rather than man-months, also thanks to the automatic acquisition of
textual definitions for our concepts. As with any automated and unsupervised learning
tool, however, OntoLearn does make errors, as we discussed in Section 4. The accuracy
of the resulting taxonomy is clearly related to the number and quality of discovered
definitional patterns, which is in turn related to the maturity and generality of a domain.
Even with good definitions, problems might arise due to in- and out-domain ambiguity,
the latter being probably the major source of errors, together with complex definitional
structures. Although we believe that there is still room for improvement to OntoLearn
Reloaded, certain errors would appear unavoidable, especially for less focused and
relatively dynamic domains like ARTIFICIAL INTELLIGENCE and FINANCE, in which
new terms arise continuously and have very few, or no definitions on the Web.
Future work includes the addition of non-taxonomical relations along the lines of
ReVerb (Etzioni et al 2011) and WiSeNet (Moro and Navigli 2012), and a more sophis-
ticated rank-based method for scoring textual definitions. Finally, we plan to tackle
the issue of automatically discriminating between in-domain ambiguity and systematic
polysemy (as discussed in Section 4.3.2).
Acknowledgments
Stefano Faralli and Roberto Navigli
gratefully acknowledge the support of the
ERC Starting Grant MultiJEDI No. 259234.
The authors wish to thank Jim McManus for
his valuable comments on the paper, and
Zornitsa Kozareva and Eduard Hovy for
making their data available.
References
Agirre, Eneko, Oier Lo?pez de Lacalle,
Christiane Fellbaum, Shu-Kai Hsieh,
Maurizio Tesconi, Monica Monachini,
Piek Vossen, and Roxanne Segers.
2010. SemEval-2010 Task 17: All-words
Word Sense Disambiguation on a
specific domain. In Proceedings of the
5th International Workshop on Semantic
Evaluation (SemEval-2010), pages 75?80,
Uppsala.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large
corpora. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 57?64, College
Park, MD.
Biemann, Chris. 2005. Ontology learning
from text?A survey of methods.
LDV-Forum, 20(2):75?93.
Brank, Janez, Dunja Mladenic, and
Marko Grobelnik. 2006. Gold standard
based ontology evaluation using instance
assignment. In Proceedings of 4th Workshop
Evaluating Ontologies for the Web (EON),
Edinburgh.
Carpineto, Claudio and Giovanni Romano.
2012. Consensus Clustering Based on
a New Probabilistic Rand Index with
Application to Subtopic Retrieval.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 34(12):2315?2326.
Chu, Yoeng-Jin and Tseng-Hong Liu.
1965. On the shortest arborescence
of a directed graph. Science Sinica,
14:1396?1400.
Cimiano, Philipp, Andreas Hotho, and
Steffen Staab. 2005. Learning concept
29 Data sets are available at: http://lcl.uniroma1.it/ontolearn reloaded.
30 For example, http://lcl.uniroma1.it/tav/.
704
Velardi, Faralli, and Navigli OntoLearn Reloaded
hierarchies from text corpora using
formal concept analysis. Journal of
Artificial Intelligence Research,
24(1):305?339.
Cohen, Trevor and Dominic Widdows.
2009. Empirical distributional semantics:
Methods and biomedical applications.
Journal of Biomedical Informatics,
42(2):390?405.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction
to Algorithms. MIT Electrical Engineering
and Computer Science. MIT Press,
Cambridge, MA.
De Benedictis, Flavio, Stefano Faralli, and
Roberto Navigli. 2013. GlossBoot:
Bootstrapping Multilingual Domain
Glossaries from the Web. In Proceedings
of the 51st Annual Meeting of the
Association for Computational Linguistics
(ACL), Sofia.
De Nicola, Antonio, Michele Missikoff,
and Roberto Navigli. 2009. A software
engineering approach to ontology
building. Information Systems,
34(2):258?275.
Edmonds, Jack. 1967. Optimum branchings.
Journal of Research of the National Bureau of
Standards, 71B:233?240.
Etzioni, Oren, Anthony Fader, Janara
Christensen, Stephen Soderland, and
Mausam. 2011. Open information
extraction: The second generation. In
Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI),
pages 3?10, Barcelona.
Fahmi, Ismail and Gosse Bouma. 2006.
Learning to identify definitions using
syntactic features. In Proceedings of
the EACL 2006 workshop on Learning
Structured Information in Natural
Language Applications, pages 64?71,
Trento.
Faralli, Stefano and Roberto Navigli.
2012. A new minimally supervised
framework for domain Word Sense
Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL),
pages 1,411?1,422, Jeju.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fleiss, Joseph L. 1971. Measuring
nominal scale agreement among
many raters. Psychological Bulletin,
76(5):378?382.
Fountain, Trevor and Mirella Lapata. 2012.
Taxonomy induction using hierarchical
random graphs. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics: Human Language
Technologies (HLT-NAACL), pages 466?476,
Montre?al.
Fowlkes, Edward B. and Colin L. Mallows.
1983. A method for comparing two
hierarchical clusterings. Journal of the
American Statistical Association,
78(383):553?569.
Girju, Roxana, Adriana Badulescu, and
Dan Moldovan. 2006. Automatic discovery
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Gomez-Perez, Asuncio?n and David
Manzano-Mancho. 2003. A survey of
ontology learning methods and
techniques. OntoWeb Delieverable 1.5.
Universidad Polite?cnica de Madrid.
Guarino, Nicola and Chris Welty. 2002.
Evaluating ontological decisions with
OntoClean. Communications of the ACM,
45(2):61?65.
Hearst, Marti A. 1992. Automatic acquisition
of hyponyms from large text corpora.
In Proceedings of the 14th International
Conference on Computational Linguistics
(COLING), pages 539?545, Nantes.
Hovy, Eduard, Andrew Philpot,
Judith Klavans, Ulrich Germann, and
Peter T. Davis. 2003. Extending metadata
definitions by automatically extracting
and organizing glossary definitions. In
Proceedings of the 2003 Annual National
Conference on Digital Government Research,
pages 1?6, Boston, MA.
Ide, Nancy and Jean Ve?ronis. 1993.
Extracting knowledge bases from
machine-readable dictionaries: Have
we wasted our time? In Proceedings
of the Workshop on Knowledge Bases and
Knowledge Structures, pages 257?266,
Tokyo.
Kozareva, Zornitsa and Eduard Hovy. 2010.
A semi-supervised method to learn and
construct taxonomies using the Web.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 1,110?1,118,
Cambridge, MA.
Kozareva, Zornitsa, Ellen Riloff, and
Eduard Hovy. 2008. Semantic class
learning from the Web with hyponym
pattern linkage graphs. In Proceedings
of the 46th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 1,048?1,056, Columbus, OH.
705
Computational Linguistics Volume 39, Number 3
Maedche, Alexander, Viktor Pekar, and
Steffen Staab. 2002. Ontology learning
part one?on discovering taxonomic
relations from the Web. In N. Zhong,
J. Liu, and Y. Y. Yao, editors, Web
Intelligence. Springer Verlag, Berlin,
pages 301?322.
Maedche, Alexander and Steffen Staab.
2009. Ontology learning. In Steffen Staab
and Rudi Studer, editors, Handbook on
Ontologies. Springer, Berlin, pages 245?268.
Miller, George A., R. T. Beckwith,
Christiane D. Fellbaum, D. Gross, and
K. Miller. 1990. WordNet: An online
lexical database. International Journal of
Lexicography, 3(4):235?244.
Morey, Leslie C. and Alan Agresti. 1984. The
measurement of classification agreement:
An adjustment to the Rand statistic for
chance agreement. Educational and
Psychological Measurement, 44:33?37.
Moro, Andrea and Roberto Navigli. 2012.
WiSeNet: Building a Wikipedia-based
semantic network with ontologized
relations. In Proceedings of the 21st
ACM Conference on Information and
Knowledge Management (CIKM 2012),
pages 1,672?1,676, Maui, HI.
Navigli, Roberto. 2009. Word Sense
Disambiguation: A survey. ACM
Computing Surveys, 41(2):1?69.
Navigli, Roberto, and Simone Paolo
Ponzetto. 2012. BabelNet: The automatic
construction, evaluation and application of
a wide-coverage multilingual semantic
network. Artificial Intelligence 193,
pp. 217?250.
Navigli, Roberto and Paola Velardi. 2004.
Learning domain ontologies from
document warehouses and dedicated
websites. Computational Linguistics,
30(2):151?179.
Navigli, Roberto and Paola Velardi. 2005.
Structural semantic interconnections:
A knowledge-based approach to Word
Sense Disambiguation. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
27(7):1075?1088.
Navigli, Roberto and Paola Velardi. 2010.
Learning Word-Class Lattices for
definition and hypernym extraction.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 1,318?1,327,
Uppsala.
Navigli, Roberto, Paola Velardi, and Stefano
Faralli. 2011. A graph-based algorithm for
inducing lexical taxonomies from scratch.
In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI),
pages 1,872?1,877, Barcelona.
Newman, Mark E. J. 2010. Networks: An
Introduction. Oxford University Press.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pantel, Patrick and Marco Pennacchiotti.
2006. Espresso: Leveraging generic
patterns for automatically harvesting
semantic relations. In Proceedings of
44th Annual Meeting of the Association for
Computational Linguistics joint with 21st
Conference on Computational Linguistics
(COLING-ACL), pages 113?120, Sydney.
Pasca, Marius. 2004. Acquisition of
categorized named entities for web search.
In Proceedings of the 13th ACM International
Conference on Information and Knowledge
Management (CIKM), pages 137?145,
Washington, DC.
Petasis, Georgios, Vangelis Karkaletsis,
Georgios Paliouras, Anastasia Krithara,
and Elias Zavitsanos. 2011. Ontology
population and enrichment: State of the
art. In Georgios Paliouras, Constantine
Spyropoulos, and George Tsatsaronis,
editors, Knowledge-Driven Multimedia
Information Extraction and Ontology
Evolution, volume 6050 of Lecture Notes
in Computer Science. Springer, Berlin /
Heidelberg, pages 134?166.
Petrenz, Philipp and Bonnie L. Webber.
2011. Stable classification of text genres.
Computational Linguistics, 37(2):385?393.
Ponzetto, Simone Paolo and Roberto Navigli.
2009. Large-scale taxonomy mapping for
restructuring and integrating Wikipedia.
In Proceedings of the 21st International Joint
Conference on Artificial Intelligence (IJCAI),
pages 2,083?2,088, Pasadena, CA.
Ponzetto, Simone Paolo and Michael Strube.
2011. Taxonomy induction based on a
collaboratively built knowledge repository.
Artificial Intelligence, 175:1737?1756.
Poon, Hoifung and Pedro Domingos. 2010.
Unsupervised ontology induction from
text. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 296?305, Uppsala.
Rand, William M. 1971. Objective criteria for
the evaluation of clustering methods.
Journal of the American Statistical
Association, 66(336):846?850.
Schmid, Helmut. 1995. Improvements in
part-of-speech tagging with an application
to German. In Proceedings of the ACL
SIGDAT-Workshop, pages 47?50, Dublin.
706
Velardi, Faralli, and Navigli OntoLearn Reloaded
Sclano, Francesco and Paola Velardi. 2007.
TermExtractor: A Web application to
learn the shared terminology of emergent
Web communities. In Proceedings of the 3th
International Conference on Interoperability
for Enterprise Software and Applications
(I-ESA), pages 287?290, Funchal.
Snow, Rion, Dan Jurafsky, and Andrew Ng.
2006. Semantic taxonomy induction from
heterogeneous evidence. In Proceedings of
44th Annual Meeting of the Association for
Computational Linguistics joint with 21st
Conference on Computational Linguistics
(COLING-ACL), pages 801?808, Sydney.
Sowa, John F. 2000. Knowledge Representation:
Logical, Philosophical, and Computational
Foundations. Brooks Cole Publishing Co.,
Pacific Grove, CA.
Storrer, Angelika and Sandra Wellinghoff.
2006. Automated detection and annotation
of term definitions in German text corpora.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC), pages 2,373?2,376,
Genova.
Suchanek, Fabian M., Gjergji Kasneci,
and Gerhard Weikum. 2008. YAGO:
A large ontology from Wikipedia and
WordNet. Journal of Web Semantics,
6(3):203?217.
Tang, Jie, Ho Fung Leung, Qiong Luo,
Dewei Chen, and Jibin Gong. 2009.
Towards ontology learning from
folksonomies. In Proceedings of the
21st International Joint Conference on
Artificial Intelligence (IJCAI),
pages 2,089?2,094, Pasadena, CA.
Tarjan, Robert Endre. 1977. Finding optimum
branchings. Networks, 7(1):25?35.
Velardi, Paola, Roberto Navigli, and Pierluigi
D?Amadio. 2008. Mining the Web to create
specialized glossaries. IEEE Intelligent
Systems, 23(5):18?25.
Vo?lker, Johanna, Denny Vrandec?ic?,
York Sure, and Andreas Hotho. 2008.
AEON?An approach to the automatic
evaluation of ontologies. Journal of
Applied Ontology, 3(1-2):41?62.
Vossen, Piek. 2001. Extending, trimming
and fusing WordNet for technical
documents. In Proceedings of the North
American Chapter of the Association
for Computational Linguistics Workshop
on WordNet and Other Lexical
Resources: Applications, Extensions
and Customizations (NAACL),
pages 125?131, Pittsburgh, PA.
Wagner, Silke and Dorothea Wagner. 2007.
Comparing clusterings: An overview.
Technical Report 2006-04, Faculty of
Informatics, Universita?t Karlsruhe (TH).
Westerhout, Eline. 2009. Definition extraction
using linguistic and structural features.
In Proceedings of the RANLP Workshop
on Definition Extraction, pages 61?67,
Borovets.
Yang, Hui and Jamie Callan. 2008.
Human-guided ontology learning.
In Proceedings of Human-Computer
Interaction and Information Retrieval
(HCIR), pages 26?29, Redmond, WA.
Yang, Hui and Jamie Callan. 2009. A
metric-based framework for automatic
taxonomy induction. In Proceedings of
the 47th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 271?279, Suntec.
Zavitsanos, Elias, Georgios Paliouras,
and George A. Vouros. 2011. Gold
standard evaluation of ontology learning
methods through ontology transformation
and alignment. IEEE Transactions on
Knowledge and Data Engineering,
23(11):1635?1648.
Zhang, Chunxia and Peng Jiang. 2009.
Automatic extraction of definitions.
In Proceedings of 2nd IEEE International
Conference on Computer Science and
Information Technology (ICCSIT),
pages 364?368, Beijing.
707

Clustering and Diversifying Web Search
Results with Graph-Based Word
Sense Induction
Antonio Di Marco?
Sapienza University of Rome
Roberto Navigli?
Sapienza University of Rome
Web search result clustering aims to facilitate information search on the Web. Rather than the
results of a query being presented as a flat list, they are grouped on the basis of their similarity
and subsequently shown to the user as a list of clusters. Each cluster is intended to represent
a different meaning of the input query, thus taking into account the lexical ambiguity (i.e.,
polysemy) issue. Existing Web clustering methods typically rely on some shallow notion of
textual similarity between search result snippets, however. As a result, text snippets with no
word in common tend to be clustered separately even if they share the same meaning, whereas
snippets with words in common may be grouped together even if they refer to different meanings
of the input query.
In this article we present a novel approach to Web search result clustering based on the
automatic discovery of word senses from raw text, a task referred to as Word Sense Induction.
Key to our approach is to first acquire the various senses (i.e., meanings) of an ambiguous
query and then cluster the search results based on their semantic similarity to the word senses
induced. Our experiments, conducted on data sets of ambiguous queries, show that our approach
outperforms both Web clustering and search engines.
1. Introduction
The Web is by far the largest information archive available worldwide. This vast pool of
text contains information of the most wildly disparate kinds, and is potentially capable
of satisfying virtually any conceivable user need. Unfortunately, however, in this setting
retrieving the precise item of information that is relevant to a given user search can be
like looking for a needle in a haystack. State-of-the-art search engines such as Google
and Yahoo! generally do a good job at retrieving a small number of relevant results from
such an enormous collection of data (i.e., retrieving with high precision, low recall).
Such systems today, however, still find themselves up against the lexical ambiguity issue
? Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy.
E-mail: {dimarco,navigli}@di.uniroma1.it.
Submission received: 25 April 2012; revised submission received: 26 July 2012; accepted for publication:
12 September 2012.
doi:10.1162/COLI a 00148
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
(Furnas et al 1987; Navigli 2009), that is, the linguistic property due to which a single
word may convey different meanings.
Recently, the degree of ambiguity of Web queries has been studied using WordNet
(Miller et al 1990; Fellbaum 1998) and Wikipedia1 as sources of ambiguous words.2
It has been estimated that around 4% of Web queries and 16% of the most frequent
queries are ambiguous (Sanderson 2008), as also confirmed in later studies (Clough et
al. 2009; Song et al 2009). An example of an ambiguous query is Butterfly effect, which
could refer to either chaos theory, a film, a band, an album, a novel, or a collection of
poetry. Similarly, black spider could refer to either an arachnid, a car, or a frying pan, and
so forth.
Lexical ambiguity is often the consequence of the low number of query words that
Web users, on average, tend to type (Kamvar and Baluja 2006). This issue could be
solved by expanding the initial query with unequivocal cue words. Interestingly, the
average query length is continually growing. The average number of words per query is
now estimated around three words per query,3 a number that is still too low to eradicate
polysemy.
The fact that there may be different informative needs for the same user query has
been tackled by diversifying search results, an approach whereby a list of heterogene-
nous results is presented, and Web pages that are similar to ones already near the top
are prevented from ranking too highly in the list (Agrawal et al 2009; Swaminathan,
Mathew, and Kirovski 2009). Today even commercial search engines are starting to
rerank and diversify their results. Unfortunately, recent work suggests that diversity
does not yet play a primary role in ranking algorithms (Santamar??a, Gonzalo, and
Artiles 2010), but it undoubtedly has the potential to do so (Chapelle, Chang, and Liu
2011).
Another mainstream approach to the lexical ambiguity issue is that of Web cluster-
ing engines (Carpineto et al 2009), such as Carrot4 and Yippy.5 These systems group
search results by providing a cluster for each specific meaning of the input query. Users
can then select the cluster(s) and the pages therein that best answer their information
needs. These approaches, however, do not perform any semantic analysis of search
results, clustering them solely on the basis of their lexical similarity.
For instance, given the query snow leopard, Google search returns, among others,
the snippets reported in Table 1.6 In the third column of the table we provide the correct
meanings associated with each snippet (i.e., either the operating system or the animal
sense). Although snippets 2, 4, and 5 all refer to the same meaning, they have no content
word in common apart from our query words. As a result, a traditional Web clustering
engine would most likely assign these snippets to different clusters. Moreover, snippet
6 shares words with snippets referring to both query meanings (i.e., snippets 1, 2, and
3 in Table 1), thus making it even harder for Web clustering engines to group search
1 http://www.wikipedia.org.
2 Note that we focus here on the ambiguity of queries in terms of their polysemy, rather than on the
identification of aspects or subsenses of a given meaning of a query, as was done in recent work on topic
identification (Wang, Chakrabarti, and Punera 2009; Xue and Yin 2011; Wu, Madhavan, and Halevy
2011). We discuss this point further in Section 2.6.
3 See Hitwise on 2008?2009 Google data: http://www.hitwise.com/us/press-center/press-releases/
google-searches-apr-09.
4 http://search.carrot2.org/stable/search.
5 http://search.yippy.com.
6 Results retrieved in May 2011.
710
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Table 1
Some of the top-ranking snippets returned for snow leopard by Google search.
# Snippet Meaning
1 To advance Mac OS X Leopard, Apple engineers... SOFTWARE
2 The snow leopard (Uncia uncia or Panthera uncia) is a moderately
large cat native to the mountain ranges
ANIMAL
3 Mac OS X Snow Leopard (version 10.6) is the seventh and current major... SOFTWARE
4 Get the facts on snow leopards. Endangered Species Act (ESA): the
snow leopard is listed as endangered...
ANIMAL
5 Snow leopards are exceptional athletes capable of making huge leaps over
ravines.
ANIMAL
6 Snow Leopard. Even the name seems to underpromise ? it?s the
first ?big cat? OS X codename to reference
SOFTWARE
results effectively. Finally, none of the top-ranking snippets refers to The Snow Leopard,
a popular 1978 book by Peter Matthiessen.
In this article, we present a novel approach to Web search result clustering that
explicitly addresses the language ambiguity issue. Key to our approach is the use of
Word Sense Induction (WSI), that is, techniques aimed at automatically discovering the
different meanings of a given term (i.e., query). Each sense of the query is represented
as a cluster of words co-occurring in raw text with the query. Each search result snippet
returned by a Web search engine is then mapped to the most appropriate meaning
(i.e., cluster) and the resulting clustering of snippets is returned.
This article provides four main contributions:
 We present a general evaluation framework for Web search result
clustering, which we also exploit to perform a large-scale end-to-end
experimental comparison of several graph-based WSI algorithms. In fact,
the output of WSI (i.e., the automatically discovered senses) is evaluated
in terms of both the quality of the corresponding search result clusters
and the resulting ability to diversify search results. This is in contrast with
most literature in the field of Word Sense Induction, where experiments
are mainly performed in vitro (i.e., not in the context of an everyday
application; Matsuo et al 2006; Manandhar et al 2010).
 In order to test whether our results were strongly dependent on
the evaluation measures we implemented in the framework, we
complemented our extrinsic experimental evaluation with a qualitative
analysis of the automatically induced senses. This study was performed
via a manual evaluation carried out by several human annotators.
 We present novel versions of previously proposed WSI graph-based
algorithms, namely, SquaT++ and Balanced Maximum Spanning Tree
(B-MST) (the former is an enhancement of the original SquaT algorithm
[Navigli and Crisafulli 2010], and the latter is a variant of MST [Di Marco
and Navigli 2011] that produces more balanced clusters).
 We show how, thanks to our framework, WSI can be successfully
integrated into real-world applications, such as Web search result
711
Computational Linguistics Volume 39, Number 3
Table 2
The top five categories returned by the Open Directory Project for the query snow leopard.
ODP Category # pages
Science: Biology: Flora and Fauna: . . . Felidae: Uncia 6
Kids and Teens: School Time: Science: . . . Leopards: Snow Leopards 4
Science: Environment: Biodiversity: Conservation: Mammals: Felines 3
Kids and Teens: School Time: Science: . . . Animals: Endangered Species 1
Computers: Emulators: Apple: Macintosh: SheepShaver 1
clustering, so as to outperform non-semantic state-of-the-art Web
clustering systems. To the best of our knowledge, with the exception
of some very preliminary results (Ve?ronis 2004; Basile, Caputo,
and Semeraro 2009), this is the first time that unsupervised text
understanding techniques have been shown to considerably boost
an Information Retrieval task in a solid evaluation framework.
This article extends previous conference work (Navigli and Crisafulli 2010;
Di Marco and Navigli 2011) by performing a novel, in-depth study of the interactions
between different corpora and several different WSI algorithms, including novel ones,
within the same framework, and, additionally, by providing a comparison with a state-
of-the-art search result clustering engine.
The article is structured as follows: in Section 2 we present related work, in
Section 3 we illustrate our approach, end-to-end experiments are reported in Section 4,
and in vitro experiments are discussed in Section 5. We present a time performance
analysis in Section 6, and conclude the paper in Section 7.
2. Related Work
Our work is aimed at addressing the difficulties arising within the different approaches
to the issue of lexical ambiguity in Web Information Retrieval. Given the large body
of work in this field, in this section we summarize the main research directions on
the topic.
2.1 Web Directories
In Web 1.0?mainly based on static Web pages?the solution to clustering search results
was that of manually organizing and categorizing Web sites. The resulting repositories
are called Web directories and list Web sites by category and possible subcategories.
These categories are sometimes organized as taxonomies (like in the Open Directory
Project, ODP7).
Although Web directories are not search engines, information can be searched
therein. So, given a query, the returned search results are organized by category. For
instance, given the query snow leopard the ODP returns the categories shown in Table 2
7 http://www.dmoz.org.
712
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
(the number of matching Web pages is reported in the second column). As can be seen
from this example, the Web directory approach has evident limits:
1. It is static, thus it needs manual updates to cover new pages and new
meanings (e.g., the book sense of snow leopard is not considered in ODP).
2. It covers only a small portion of the Web (e.g., we only have one Web page
categorized with the computing sense of snow leopard, cf. the last row of
Table 2).
3. It classifies Web pages using coarse categories. This latter feature of
Web directories makes it difficult to distinguish between instances of the
same kind (e.g., pages about artists with the same surname classified as
Arts:Music:Bands and Artists).
Although methods for the automatic classification of Web documents have been
proposed (Liu et al 2005; Xue et al 2008, inter alia) and some problems have been
tackled effectively (Bennett and Nguyen 2009), these approaches are usually supervised
and still suffer from reliance on a predefined taxonomy of categories. Finally, it has been
reported that directory-based systems are among the most ineffective solutions to Web
information retrieval (Bruza, McArthur, and Dennis 2000).
2.2 Semantic Information Retrieval
A second approach to query ambiguity consists of associating explicit semantics (i.e.,
word senses or concepts) with queries and documents, that is, performing Semantic
Information Retrieval (SIR). SIR is performed by indexing and searching concepts
rather than terms, that is, by means of Word Sense Disambiguation (WSD; Navigli 2009),
thus potentially coping with two linguistic phenomena: expressing a single meaning
with different words (synonymy) and using the same word to express various different
meanings (polysemy). The main idea is that assigning concepts to words can potentially
overcome these two issues, enabling a shift from the lexical to the semantic level to be
achieved.
Over the years, various methods for SIR have been proposed (Krovetz and Croft
1992; Voorhees 1993; Mandala, Tokunaga, and Tanaka 1998; Gonzalo, Penas, and Verdejo
1999; Kim, Seo, and Rim 2004; Liu, Yu, and Meng 2005, inter alia). Contrasting results
have been reported on the benefits of these techniques, however: It has been shown
that WSD has to be very accurate to benefit Information Retrieval (Sanderson 1994)?a
result that was later debated (Gonzalo, Penas, and Verdejo 1999; Stokoe, Oakes, and Tait
2003). Also, it has been reported that WSD has to be very precise on minority senses and
uncommon terms, rather than on frequent words (Krovetz and Croft 1992; Sanderson
2000).
The main drawback of SIR is that it relies on the existence of a reference dictionary
to perform WSD (typically, WordNet) and thus suffers this dictionary?s static nature and
its inherent paucity of most proper nouns. This latter problem is particularly important
for Web searches, as users tend to retrieve more information about named entities
(e.g., singers, artists, cities) than concepts (such as abstract information about singers or
artists). Although lexical knowledge resources that integrate lexicographic senses with
named entites on a large scale have recently been created (Navigli and Ponzetto 2012),
713
Computational Linguistics Volume 39, Number 3
it is still to be shown that their use for SIR is beneficial. Moreover, these resources do
not yet tackle the dynamic evolution of language.
In contrast, our WSI approach to search result clustering automatically discovers
both lexicographic and encyclopedic senses of a query (including new ones), thus taking
into account all of the mentioned issues.
2.3 Search Result Clustering
A more popular approach to query ambiguity is that of search result clustering. Typ-
ically, given a query, the system starts from a flat list of text snippets returned from
one or more commonly available search engines and clusters them on the basis of some
notion of textual similarity. At the root of the clustering approach lies van Rijsbergen?s
cluster hypothesis (van Rijsbergen 1979, page 45): ?closely associated documents tend
to be relevant to the same requests,? whereas results concerning different meanings of
the input query are expected to belong to different clusters.
Approaches to search result clustering can be classified as data-centric or
description-centric (Carpineto et al 2009). The former focus more on the problem of
data clustering than on presenting the results to the user. A pioneering example is
Scatter/Gather (Cutting et al 1992), which divides the data set into a small number
of clusters and, after the selection of a group, performs clustering again and proceeds
iteratively. Developments of this approach have been proposed that improve on cluster
quality and retrieval performance (Ke, Sugimoto, and Mostafa 2009). Other data-centric
approaches use agglomerative hierarchical clustering (e.g., LASSI [Maarek et al 2000]),
rough sets (Ngo and Nguyen 2005), or exploit link information (Zhang, Hu, and Zhou
2008).
Description-centric approaches are, instead, more focused on the description to
produce for each cluster of search results. Among the most popular and successful
approaches are those based on suffix trees. Suffix trees are rooted directed trees that
contain all the suffixes of a string s. The label of each edge is a non-empty substring of
s and each vertex v is labeled with the concatenation of the edge labels on the path
from the root to v. If we view the search result snippets to be clustered as a set of
strings (i.e., their bag of words), each vertex of the corresponding suffix tree can be
considered as a set of documents that share a phrase (i.e., the label of the vertex itself)
and therefore the vertices represent a set of base clusters B = (b1, b2, . . . , bn). The original
Suffix Tree Clustering (STC; Zamir et al 1997; Zamir and Etzioni 1998) algorithm obtains
the final clustering by merging the clusters in B with a high overlap in the documents
they contain. A scoring function is defined, based on both the number of documents in
the base cluster and the length of the common phrase, with the aim of returning only
the top k clusters.
Later developments improved the performance of the STC algorithm using
document?document similarity scores in order to overcome the low scalability of the
original approach (Branson and Greenberg 2002). Crabtree, Gao, and Andreae (2005)
identified an issue in the original scoring function whereby unreasonably high scores
tend to be assigned to clusters obtained as a result of the merging of very similar
base clusters. To solve this problem, they proposed the Extended Suffix Tree Clustering
algorithm (ESTC) with a novel scoring function and a new procedure for selecting the
top k clusters to be returned.
More recent approaches based on suffix trees extract relevant keyphrases from
generalized suffix trees (i.e., trees which contain suffixes of a set of strings
714
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
S = {s1, s2, . . . , s|S|}) in order to choose meaningful labels for the output clusters
(Bernardini, Carpineto, and D?Amico 2009; Carpineto, D?Amico, and Bernardini 2011).
Other approaches to description-centric search result clustering in the literature
are based on formal concept analysis (Carpineto and Romano 2004), singular value
decomposition (Osinski and Weiss 2005), spectral clustering (Cheng et al 2005), spectral
geometry (Liu et al 2008), link analysis (Gelgi, Davulcu, and Vadrevu 2007), and graph
connectivity measures (Di Giacomo et al 2007). Search result clustering has also been
viewed as a supervised salient phrase ranking task (Zeng et al 2004).
Whereas search result clustering has heretofore been performed without the explicit
use of lexical semantics, in our work we show how to exploit search result clustering
as the common evaluation framework of both semantic and non-semantic clustering
engines.
2.4 Diversification
Rather than clustering the top search results by their similarity, one can aim at reranking
them on the basis of criteria that maximize their diversity, so as to present top results
which are as different from each other as possible. This technique, called diversification
of search results, is a recent research topic that, yet again, deals with the query ambiguity
issue. To some extent, today?s search engines, such as Google and Yahoo!, apply some
diversification technique to their top-ranking results.
One of the first examples of diversification algorithms was based on the use of
similarity functions to measure the diversity between documents and between docu-
ment and query (Carbonell and Goldstein 1998). Other diversification techniques use
conditional probabilities to determine which document is most different from higher-
ranking ones (Chen and Karger 2006), or use affinity ranking (Zhang et al 2005), based
on topic variance and coverage.
An algorithm called Essential Pages (Swaminathan, Mathew, and Kirovski 2009)
has been proposed that aims to reduce information redundancy and returns Web pages
that maximize coverage with respect to the input query. In this approach the Web
search results for a query q are transformed into bags of words containing the terms
occurring in the corresponding Web page. Frequency information from raw corpora is
then used to find relevant words for q, that is, words which are generally infrequent,
but occur often in the results retrieved for q. The coverage score of a search result r is
then calculated as a function of the number of terms relevant for q and contained in
r. Another interesting approach reformulates the problem explicitly in terms of how
to minimize the risk of dissatisfaction for the average user (Agrawal et al 2009). A
greedy algorithm is proposed that balances between relevance and diversity of the
search results. The algorithm is evaluated using generalizations of classical Information
Retrieval metrics that are based on statistical considerations and take into account the
intentions of the user.
More recently, vector space model representations have been explored to improve
diversity in search results (Santamar??a, Gonzalo, and Artiles 2010). Web page results
are represented as vectors and compared against vector representations of encyclopedic
entries available from Wikipedia using cosine similarity. Search results are diversified
accordingly.
Finally, in the last few years the specific structure of the Web has been exploited
to perform diversification, as proposed by Ma, Lyu, and King (2010), who make use of
Markov random walks on query-URL bipartite graphs, and Chandar and Carterette
715
Computational Linguistics Volume 39, Number 3
(2010), who cluster search results by exploiting the links in Web pages in order to
identify the subtopics of the returned documents.
2.5 Word Sense Induction
A fifth solution to the query ambiguity issue is Word Sense Induction (WSI), namely, the
automatic discovery of word (i.e., query) senses from raw text (see Navigli [2009, 2012]
for a survey). WSI allows us to go beyond the surface similarity of Web snippets (which
hampers the performance of Web search result clustering) by dynamically acquiring an
inventory of senses of the input query. The core idea is to then use these query senses to
cluster the Web snippets returned by a traditional search engine.
Very little work on this topic exists: Vector-based WSI was successfully shown to
improve bag-of-words ad hoc Information Retrieval (Schu?tze and Pedersen 1995) and
preliminary studies (Udani et al 2005; Chen, Za??ane, and Goebel 2008) have provided
interesting insights into the use of WSI for Web search result clustering. A more recent
attempt at automatically identifying query meanings is based on the use of hidden
topics (Nguyen et al 2009). In this approach, however, topics (estimated from a uni-
versal data set) are query-independent and thus their number needs to be established
beforehand. In contrast, we aim to cluster snippets on the basis of a dynamic and finer-
grained notion of sense.
An exploratory study on ten query words has shown that the majority of relevant
uses of query words can be identified using graph-based WSI (Ve?ronis 2004). In the
present work we take this preliminary finding to the next level, by studying the impact
of several graph-based WSI algorithms on a large scale and by integrating them into a
Web search result clustering framework. As a result, we are able not only to perform
an end-to-end evaluation of WSI approaches, but also to compare them with traditional
search result clustering techniques, which instead lack explicit semantics for the query
meanings.
2.6 Aspect Identification
Over recent years a line of research has been developed in the field of Information
Retrieval that makes use of query logs and clickthrough information to identify and
model the aspects of a given query in terms of the user intents for that query. Aspects
can be identified by exploiting those queries in the past that enabled the user to retrieve
documents that are close to the current input query (Wang and Zhai 2007). A different
approach aims, instead, at extracting related queries from query logs as candidate
aspects and discarding duplicate and redundant aspects using search results. Wikipedia
InfoBoxes are used to cluster candidate aspects into classes (Wu, Madhavan, and Halevy
2011). Latent aspects of queries can also be extracted from query reformulations within
historical search session logs (Wang, Chakrabarti, and Punera 2009). More recently, a
topic modeling approach based on query logs and click data has been proposed that
aims at discovering generic aspects pervading manually fixed categories of named
entities (Xue and Yin 2011). The implicit user-specific aspect of a query can be obtained
from short query log sessions of other users using a Markov logic learning model. This
results in the documents that best model the user?s intentions when entering a query
(Mihalkova and Mooney 2009). Finally, a semi-supervised approach has recently been
applied to create class labels that are later assigned to latent clusters of queries using a
Hierarchical Dirichlet Process (Reisinger and Pasca 2011).
716
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
This line of research has some points of contact with WSI, but also important
differences:
 Most important, aspect identification aims at discriminating between
very fine-grained facets of a given query, such as those of rental, pricing,
and accidents of a car, in contrast to WSI whose goal is that of inducing
different meanings of the given query, such as car as a motor vehicle,
railroad car, song, novel, or even primitive in the LISP programming
language. In this respect, the two tasks are complementary, because
once WSI has discovered the different senses of a query, then one can
apply aspect identification to detect subsenses of each meaning.
 Much work based on query logs and click data requires reliable statistics,
which are not always available in all languages. WSI relies instead on
raw text corpora, which can easily be obtained for any language. This
difference also holds for custom search engines not working on the Web,
which might not have enough statistics from their users, but could instead
resort to raw (domain) corpora.
 Privacy and availability issues are often mentioned in connection with
query logs and clickthrough data, therefore making research on this topic
hard to replicate and evaluate objectively, especially in comparison with
other systems.
The framework presented in this article focuses on the ambiguity of queries at the
meaning level, leaving the further application of aspect identification techniques to
future work, in the hope that the previously mentioned issues of privacy and availability
will somehow be mitigated.
3. Semantically Enhanced Search Result Clustering
Web search result clustering is usually performed in three main steps:
1. Given a query q, a search engine is used to retrieve a list of results
R = (r1, . . . , rn).
2. A clustering C = (C1, . . . , Cm) of the results in R is obtained by means
of a clustering algorithm.
3. The clusters in C are optionally labeled with an appropriate algorithm
(e.g., Zamir and Etzioni 1998; Carmel, Roitman, and Zwerdling 2009)
for visualization purposes.
First, we preprocess the set R of search results returned by the search engine
(Section 3.1). Next, to inject semantics into search result clustering, we propose
improving Step 2 by means of a WSI algorithm: Given a query q, we first dynamically
induce, from a text corpus, the set of word senses of q (Section 3.2); next, we cluster the
Web results on the basis of the word senses previously induced (Section 3.3). We show
our framework in Figure 1.
3.1 Preprocessing of Web Search Results
As a result of submitting our query q to a search engine, we obtain a list of relevant
search results R = (r1, . . . , rn). In order to make this list usable by a clustering algorithm,
717
Computational Linguistics Volume 39, Number 3
Figure 1
The workflow of semantically enhanced Web search result clustering.
each result ri is processed by means of four steps aimed at transforming it into a bag of
words bi:
1. We obtain the snippet si corresponding to the result ri.
2. We apply tokenization to si, thus splitting the string into tokens and
setting them to lowercase.
3. We augment the current token set with multi-word expressions obtained
by compounding subsequent word tokens up to ? words (a parameter
whose tuning is described later in Section 4.1.4). The terms in the resulting
token set are lemmatized using WordNet as reference lexicon. We remove
tokens that are not in the WordNet lexicon (e.g., the, esa).
4. We remove the stopwords (e.g., get, on, be, as) and the target query words
(e.g., snow, leopard, snow leopard) from the token set.
An example of the application of the four steps to a snippet returned for the query snow
leopard is shown in Table 3. As a result of this process, we obtain a list of bags of words
B = (b1, . . . , bn), where bi is the bag of words of the search result ri.
3.2 Graph-Based Word Sense Induction
The next step is to dynamically discover the senses of the input query q and provide a
representation for them that will later be used for semantically clustering the snippets
preprocessed in the previous step. WSI algorithms are unsupervised techniques aimed
at automatically identifying the set of senses denoted by a word. These methods induce
word senses from text by clustering word occurrences on the basis of the idea that a
Table 3
Processing steps for one of the search results of the query snow leopard.
step output
initial snippet ?Get the facts on snow leopards. Endangered Species Act (ESA): the
snow leopard is listed as endangered?
tokenization { get, the, facts, on, snow, leopards, endangered, species, act, esa,
leopard, is, listed, as }
compounding and
lemmatization
{ get, fact, on, snow, leopard, snow leopard, endangered, species,
endangered species, act, be, listed, as }
stopword and query
words removal
{ fact, endangered, species, endangered species, act, listed }
718
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
given word?used in a specific sense?tends to co-occur with the same neighboring
words (Harris 1954). Several approaches to WSI have been proposed in the literature
(see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors
(e.g., Schu?tze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks
(Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011),
and co-occurrence graphs (e.g., Widdows and Dorow 2002).
In our work, we chose to focus on approaches based on co-occurrence graphs for
two reasons:
i) They have been shown to achieve state-of-the-art performance in standard
evaluation tasks (Agirre et al 2006b; Agirre and Soroa 2007; Korkontzelos
and Manandhar 2010).
ii) Other approaches are either based on syntactic dependency statistics (Lin
1998; Van de Cruys and Apidianaki 2011), which are hard to obtain on a
large scale for many domains and languages, or based on large matrix
computation methods such as context-group discrimination (Schu?tze
1998), non-negative matrix factorization (Van de Cruys and Apidianaki
2011) and Clustering by Committee (Lin and Pantel 2002). Instead, in our
approach we aim to exploit the relational structure of word co-occurrences
with lower requirements (i.e., using just a stopword list, a lemmatizer,
and a compounder, cf. Section 3.1), assuming that the semantics of a word
are represented by means of a co-occurrence graph whose vertices are
co-occurrences and whose edges are co-occurrence relations.
We therefore integrated the following algorithms into our framework:
 Curvature clustering (Dorow et al 2005), an algorithm based on the
participation ratio of words in graph triangles, that is, complete graphs
with three vertices.
 Squares, Triangles, and Diamonds (SquaT++), an algorithm that
integrates two graph patterns previously exploited in the literature
(Navigli and Crisafulli 2010), namely, squares and triangles, with a novel
pattern called diamond.
 Balanced Maximum Spanning Tree Clustering (B-MST), an extension of
a WSI algorithm based on the calculation of a Maximum Spanning Tree
(Di Marco and Navigli 2011) that aims at balancing the number of
co-occurrences in each sense cluster.
 HyperLex (Ve?ronis 2004), an algorithm based on the identification of hubs
(representing basic meanings) in co-occurrence graphs.
 Chinese Whispers (Biemann 2006), a randomized algorithm that
partitions the graph vertices by iteratively transferring the mainstream
message (i.e., word sense) to neighboring vertices.
All of these graph algorithms for WSI consist of a common step, namely, co-
occurrence graph construction (described in Section 3.2.1) and a second step, namely,
the discovery of word senses, whose implementation depends on the specific algorithm
adopted. We discuss the second phase of each algorithm separately (Section 3.2.2).
719
Computational Linguistics Volume 39, Number 3
Table 4
Example co-occurrences of word w = lion.
word w? c(w?) c(w, w?) Dice(w, w?)
animal 213,414 5,109 0.2534
videogame 201,342 4,945 0.2042
mac 194,056 4,940 0.1568
africa 189,011 4,521 0.1961
feline 167,487 4,548 0.1472
cat 161,980 4,493 0.1214
savannah 159,693 3,535 0.1091
predator 145,239 3,643 0.1065
apple 140,670 3,261 0.1043
tiger 134,702 2,147 0.1024
technology 129,483 2,017 0.0097
software 113,045 1,846 0.0084
iPod 112,100 1,803 0.0070
simulation 93,899 1,367 0.0031
3.2.1 Step 1: Graph Construction. Given a target query q, we build a co-occurrence graph
Gq = (V, E) such that V is the set of words8 co-occurring with q, and E is the set of
undirected edges, each denoting a co-occurrence between pairs of words in V. We
harvest the statistics for co-occurring words V from a text corpus (we used two different
corpora, see Section 4.1.2), which was previously tokenized and lemmatized.
First, for each word w we calculate the total number c(w) of its occurrences and
the number of times c(w, w?) that w occurs together with some word w? in the same
context (to this end, we use the lemmas corresponding to inflected forms in the text).
For instance, in Table 4, assuming w = lion, we show the absolute count c(w?) of some
words (second column) together with the joint co-occurrence count c(w, w?) of words w?
occurring with w = lion in the same context (third column). Note that the co-occurrences
w? may refer to different senses of word w?for example, africa and savannah refer to
the animal sense of lion, whereas technology and software to the operating system sense.
Moreover, w? may be ambiguous itself in the context of w (e.g., tiger as either an animal
or an operating system).
Second, we calculate the Dice coefficient to determine the strength of co-occurrence
between any two words w and w?:9
Dice(w, w?) =
2c(w, w?)
c(w) + c(w?)
. (1)
Table 4 reports the Dice coefficients in the fourth column for the example words.
The rationale behind the use of the Dice coefficient, as opposed to, for example, a
simple co-occurrence count such as c(w, w?), is that dividing by the average of the total
8 Because our application (i.e., Web search result clustering) typically deals with nominal senses,
and to avoid overly large graphs, we restrict our vocabulary to nouns only.
9 We note that the Dice coefficient can have a probabilistic interpretation in terms of the conditional
probabilities P(w|w? ) and P(w?|w) or, alternatively, the joint probability P(w, w? ) and the marginal
probabilities P(w) and P(w? ) (Smadja, McKeown, and Hatzivassiloglou 1996).
720
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
counts of the two words drastically decreases the ranking of words that tend to co-occur
frequently with many other words (home, page, etc.).
Finally, we use the occurrence and co-occurrence counts just collected to construct
the co-occurrence graph Gq = (V, E) for the input query q. The pseudocode of our
graph construction procedure is shown in Algorithm 1 and consists of the following
steps:
a. Initialization with snippet words (lines 1?2): Initially we set V to
contain all the content words from the bags of words obtained from
the snippet results of query q, that is, V :=
?n
j=1 bj, where bj is the bag
of words corresponding to the search result rj ? R as obtained after the
preprocessing step (see Section 3.1). We also set E := ?, that is, the
edge set is initially empty.
b. Adding first-order co-occurrences (lines 3?5): We augment V with the
highest-ranking words co-occurring with query q in the selected text
corpus, that is, those words w for which the following equations are
satisfied:
?
?
?
c(q, w)
c(q)
? ?
Dice(q, w) ? ??
(2)
where ? and ?? are experimentally tuned thresholds (cf. Section 4.1.4).
c. Adding second-order co-occurrences (lines 6?11): Optionally, we create an
auxiliary copy V(0) of V. For each word w ? V(0) we augment V with those
words w? which are strongly related to w in the text corpus. In other words
we add w? to V if both Equations (2) are satisfied for the pair of words w
and w?.
Algorithm 1 The graph construction algorithm.
Input: query q, the bag of words (b1, ? ? ? , bn) for q
Output: a graph Gq = (V, E)
1: V :=
?n
j=1 bj
2: E := ?
3: for each word w in the corpus
4: if c(q, w)/c(q) ? ? and Dice(q, w) ? ?? then
5: V := V ? {w}
6: if second order = true then
7: V(0) := V
8: for each word w ? V(0)
9: for each word w? in the corpus
10: if c(w, w?)/c(w) ? ? and Dice(w, w?) ? ?? then
11: V := V ? {w?}
12: for each (w, w?) ? V ? V s. t. w = w?
13: if Dice(w, w?) ? ? then
14: E := E ? {{w, w?}}
15: remove all disconnected vertices from V
16: return Gq = (V, E)
721
Computational Linguistics Volume 39, Number 3
d. Creating the co-occurrence graph (lines 12?15): For each pair of words
(w, w?) ? V ? V, we add the corresponding edge {w, w?} to E with weight
Dice(w, w?) if the following condition is satisfied:
Dice(w, w?) ? ? (3)
where ? is a confidence threshold for the co-occurrence relation. Note
that we use a threshold ?? to select which vertices to add to the graph Gq
(Step [b]) and we use a potentially different threshold ? for the selection of
which edges to add to Gq. Finally, we remove from V all the disconnected
vertices (i.e., those with degree 0).
As a result of this algorithm a co-occurrence graph Gq for the query q is pro-
duced. Consider again the target word lion and let us assume that the words in
Table 4 are the only co-occurrences of lion. In Figure 2 we show the execution of the
four steps of our graph construction algorithm for the input query lion, assuming
(a) videogame mac
feline
animal
(b) videogame
software
mac
tiger
apple
animal
feline
savannah
africa
predator
(c) videogame
simulation
software
technology
java
mac
tiger
iPod
apple
animal
feline
cat
savannah
africa
predator malawi
(d) videogame
simulation
software
technology
mac
tiger
iPod
apple
animal
feline
cat
savannah
africa
predator
0.015
0.
00
9
0.
04
0.
00
5
0.003
0.0015
0.002
0.00
1
0.027
0.05
0.
00
7
0.0012
0.045
0.06
0.0
38
0.0
17
0.
05
9
0.00740.0034
0.0023
0.001
3
0.
04
2
Figure 2
Graph construction for the lion example: (a) initializing V with the snippets words (lines 1?2);
(b) adding first-order co-occurrences to V (lines 3?5); (c) adding second-order co-occurrences
to V (lines 6?11); (d) adding the edges corresponding to strong relations and removing
disconnected vertices (lines 12?15).
722
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
? = 0.38, ?? = ? = 0.003, and c(lion) = 350, 727. First, we initialize the graph with the
words in the snippets returned for lion (Figure 2a), next we add the words co-
occurring with the query (Figure 2b), then second-order co-occurrences, that is, words
co-occurring with those just added to the graph (Figure 2c), and finally we add those
edges between word pairs whose Dice value is above a threshold (Figure 2d).
3.2.2 Step 2: Sense Discovery. All the graph-based WSI algorithms that we implemented
in our framework are designed to discover the senses of an input term, which in
our specific application is the query q. This process of meaning discovery is carried
out through the use of the relational and structural information contained in the co-
occurrence graph we have just created. In fact, a co-occurrence graph Gq = (V, E) for
a query q contains: (i) vertices w ? V corresponding to words highly related to q, and
(ii) edges e ? E representing co-occurrence relations between vertices (i.e., words) in V.
The key idea behind graph-based WSI is to obtain a partition S = (S1, . . . , Sm) of Gq
such that each component Si = (Vi, Ei) contains structurally (i.e., semantically) related
vertices (i.e., words). In other words, each vertex set Vi is intended to contain only words
related to a specific sense of q. As a result S is the sense inventory for the query q and
each Si is a sense cluster.
We now introduce each graph-based WSI algorithm in detail.
Curvature. The curvature algorithm aims at quantifying how strongly the neighbors of
a vertex are related to each other. To measure this degree of correlation, the curvature
coefficient for a vertex w is calculated as follows:
curv(w) =
# triangles w participates in
# triangles w could participate in
(4)
where a triangle is a cycle of length 3. The numerator of Equation (4) is trivially calcu-
lated as the number of links between neighbors of w, and the denominator is calculated
by counting all the possible pairs of neighbors. According to Equation (4), the curvature
coefficient can assume values between 0 and 1. A vertex whose neighbors are highly
connected (i.e., with a high value of curvature) is assumed to be part of a component
that represents a specific meaning of the target query. Conversely, a vertex with low
curvature acts as a connection between different meanings.
The curvature algorithm is designed to identify the meaning components by means
of the removal of all vertices whose curvature is below a certain threshold ?. For ex-
ample, we can attribute two different meanings to the word Napoleon, namely, a French
emperor and an American city. By looking at the graph in Figure 3 we can easily find
Napoleon
France
revolution
Ohio
America
Figure 3
Example of curvature for Napoleon.
723
Computational Linguistics Volume 39, Number 3
that Napoleon participates in two triangles (represented by continuous lines) and it po-
tentially could also participate in four additional triangles (i.e., those including dashed
lines). It follows that curv(Napoleon) = 26 = 0.33. The deletion of the vertex Napoleon
results in two components (respectively, containing the vertices { France, revolution }
and { Ohio, America }) representing the two mentioned meanings.
SquaT++. The curvature clustering algorithm is based on the hunch that local connec-
tivity is correlated with meaning consistency. We take this idea to the next level by
proposing a more elaborate local connectivity approach that exploits three different
graph patterns, namely: triangles (i.e., cycles of length 3, like in curvature clustering),
squares (i.e., cycles of length 4) and diamonds (i.e., graphs with 4 vertices and 5 edges,
forming a square with a diagonal), hence the name SquaT++ (Squares, Triangles, and
?more?). We determine the strength of the three patterns for a vertex w in the co-
occurrence graph as follows:
Tri(w) =
# triangles w participates in
# triangles w could participate in
(5)
Sqr(w) =
# squares w participates in
# squares w could participate in
(6)
Dia(w) =
# diamonds w participates in
# diamonds w could participate in
(7)
where w is a vertex. Then we linearly combine the three measures as follows:
SquaT++(w) = ? ? Tri(w) + ? ? Sqr(w) + ? ? Dia(w) (8)
where ?+ ?+ ? = 1. Similarly to the curvature algorithm, the sense clusters are ob-
tained by removing all those vertices whose SquaT++ value is below a threshold ?. In
Figure 4(a) we show in bold the vertices selected for removal, and in Figure 4(b) the
sense clusters obtained after removal, namely: { videogame, simulation, software }, { iPod,
apple, mac }, and { cat, animal, predator, africa, savannah }.
SquaT++ is a generalization of the curvature algorithm in that: (i) it uses the triangle
pattern to calculate curvature, and (ii) it disconnects the graph using the same algorithm
as curvature. SquaT++ is a novel algorithm, however, that extends the previously
proposed SquaT (Navigli and Crisafulli 2010), based on triangles and squares, by in-
troducing a new pattern, namely, the diamond, whose clustering coefficient is linearly
combined with the other two. Moreover, in our experiments we tested two versions of
SquaT++: the traditional one in which the coefficient is calculated on vertices (like in
Equation (8)), and a variant calculated on edges. Our hunch here is that removing low-
ranking edges rather than vertices might produce more informative clusters, because
no word is removed from the original graph. In what follows, we refer to the vertex
version as SquaT++V and to the variant on edges as SquaT++E, and we refer to the
general algorithm as SquaT++.
B-MST. A more global approach to the identification of sense components is the
Balanced Maximum Spanning Tree (B-MST), which is based on the computation of
724
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
(a) videogame
simulation
software
technology
mac
tiger
iPod
apple
feline
cat
animal savannah
africa
predator
(b) videogame
simulation
software
mac
iPod
apple
animal
cat
savannah
africa
predator
(c) videogame
simulation
software
technology
mac
tiger
iPod
apple
animal
feline
cat
savannah
africa
predator
(d) videogame
simulation
software
technology
mac
tiger
iPod
apple
animal
feline
cat
savannah
africa
predator
(e)
lion
videogame
simulation
software
technology
mac
tiger
iPod
apple
animal
feline
cat
savannah
africa
predator
(f) videogame
simulation
software
technology
mac
tiger
iPod
apple
animal
feline
cat
savannah
africa
predator
(g) videogame
simulation
software
technology
mac
tiger
iPod
apple
animal
feline
cat
savannah
africa
predator
0.015
0.
00
9
0.
04
0.
00
5
0.003
0.0015
0.002
0.00
1
0.027
0.05
0.
00
7
0.0012
0.045
0.06
0.0
38
0.0
17
0.
05
9
0.00740.0034
0.0023
0.001
3
0.
04
2
?
?
?
0.015
0.
00
9
0.
04
0.
00
5
0.003
0.0015
0.002
0.00
1
0.027
0.05
0.
00
7
0.0012
0.045
0.06
0.0
38
0.0
17
0.
05
9
0.00740.0034
0.0023
0.001
3
0.
04
2
Figure 4
The lion example: (a) SquaT++ selection and (b) removal of edges below the threshold; (c) B-MST
spanning tree calculation and (d) edge removal; (e) HyperLex hub selection and (f) identification
of word senses; (g) Chinese Whispers cluster creation.
725
Computational Linguistics Volume 39, Number 3
the Maximum Spanning Tree (MST) of the co-occurrence graph. Cluster meanings are
identified by iteratively removing the edges which represent structurally weak rela-
tions, i.e., those with lower weight in the MST. The procedure is as follows:
 Eliminate from Gq all vertices whose degree is 1.
 Calculate the maximum spanning tree TGq of the graph Gq (e.g., the bold
edges in Figure 4(c) represent the maximum spanning tree for our initial
graph).
 The original MST algorithm for WSI, proposed by Di Marco and Navigli
(2011), iteratively eliminates the minimum-weight edge e ? TGq whose
degree ? 2, until N connected components (i.e., word clusters) are
obtained or there are no more edges to eliminate. The problem with this
approach is that it can generate unbalanced clusters (i.e., a few very large
clusters and several small clusters); for this reason we developed the
B-MST variant which calculates an appropriate cluster mean cardinality10
and removes an edge e ? TGq if its elimination does not lead to connected
components with cardinality less than half of the calculated mean value.
This additional constraint prevents the creation of very small clusters,
while at the same time avoiding artificial equal-size clusters.
Following our lion example, and assuming that the value of the only parameter of
B-MST (i.e., the maximum number N of meanings to be identified) is set to 3, we obtain
the clusters in Figure 4(d).
HyperLex. Another option for sense discovery is that of HyperLex, which identifies the
most interconnected vertices in the graph Gq, called hubs. Each hub acts as the ?root?
of a specific component of Gq and, correspondingly, a meaning of the target query q.
First, a list L of the vertices w? of the graph Gq is created and sorted by their absolute
count c(w?) in decreasing order. Each vertex w? ? L is then selected as hub if it satisfies
the following conditions:
?
?
?
?
?
?
?
?
?
degree(w?)
maxw???V degree(w??)
? ?
?
{w?,w??}?E Dice(w
?, w??)
degree(w?)
? ??
(9)
that is, the normalized degree of vertex w? and the average weight of the edges in-
cident on w? must be, respectively, above the thresholds ? and ??. Once it has been
selected, the hub and all its neighbors are removed from L so as to avoid neighboring
vertices from also being selected as hubs. The hub selection process stops when the
next vertex in the sorted list does not satisfy either of the Equations (9) or if the list L
is empty.
As an example, consider the co-occurrence graph in Figure 2(d). A list of the vertices
in the graph is created, sorted by c(w?), as shown in Table 4. For the purpose of our
10 We calculate the mean cardinality of a cluster by dividing the total number of vertices in the graph by the
maximum number N of clusters that we want to obtain.
726
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
example, let us assume ? = 0.5 and ?? = 0.015. The first hub to be selected is animal.
All its neighbors (tiger, feline, cat, predator, africa, and savannah in our example) are also
removed from the list. The next hub is videogame (its neighbors simulation, software, and
technology are also removed from the list). The last hub is mac; after the removal of its
neighbor from the list (apple) the last vertex to be examined is iPod, which cannot be
selected as hub because it does not satisfy the second condition of Equation (9). The
selected hubs are shown as rectangles in Figure 4(e).
Once the hub selection process is complete, the target query q is added to the set
of vertices V of graph Gq and each hub is connected to q with an infinite-weight edge
(see vertex lion and its edges added to the graph in Figure 4(e)). Then, a maximum
spanning tree Tq of the graph is calculated starting from vertex q (see the bold edges
in Figure 4(e)). As a result, Tq will include all the infinite-weight edges from q to its
direct descendants, namely, the hubs. Vertex q is then removed from the graph so that
each subtree rooted at a hub in Tq represents a word sense for the target query q (see
Figure 4(f)). In our example, three clusters are produced: { videogame, simulation, software,
technology }, { mac, apple, iPod }, and { animal, feline, tiger, cat, predator, africa, savannah }.
Note that, in our example, HyperLex and SquaT++ found the same meanings for the
query word lion (namely, the animal, the operating system, and the videogame), but
produced different clusters (e.g., HyperLex assigns the word tiger to the animal cluster
whereas SquaT++ removes it from the graph). Finally, notice that in HyperLex the
number of senses is dynamically chosen on the basis of the co-occurrences of q and
the algorithm?s thresholds.
An alternative approach to hub selection as performed in HyperLex consists of
using the PageRank algorithm to sort the vertices of the co-occurrence graph and choose
the best ranking ones as hubs of the target word (Agirre et al 2006b). Given that the
performance of this variant is comparable to that of HyperLex, in this work we focus on
the original version of the induction algorithm.
Chinese Whispers. All the previously presented algorithms work in a top?down fashion,
that is, they iteratively remove edges or vertices from an initial co-occurrence graph
until a number of partitions are obtained. The last algorithm we consider, called Chinese
Whispers, works, instead, bottom?up. The pseudocode, shown in Algorithm 2, consists
of the following two steps:
1. First, the algorithm assigns a distinct class i to each vertex vi and creates a
clustering C containing the singleton clusters Ci (lines 1?4 of the
algorithm).
2. Second, a series of iterations is performed aimed at merging the clusters
(lines 5?11). Specifically, at each iteration the algorithm analyzes each
vertex v in random order and assigns it to the majority class among those
associated with its neighbors. In other words, it assigns each vertex v to the
class c that maximizes the sum of the weights of the edges {u, v} incident
on v such that c is the class of u, according to the following formula:
class(v) := argmax
c
?
{u,v}?E(Gq )
s.t. class(u)=c
Dice(u, v) (10)
727
Computational Linguistics Volume 39, Number 3
Algorithm 2 The Chinese Whispers algorithm.
Input: a graph Gq = (V, E) to be clustered
Output: a clustering C of the vertices in V
1: for each vi ? V
2: class(vi) := i
3: Ci := {vi}
4: C := {Ci : i = 1, . . . , |V|}
5: repeat
6: C? := C
7: for each v ? V, randomized order
8: class(v) := argmax
c
?
{u,v}?E(Gq )
s.t. class(u)=c
Dice(u, v)
9: for each i do Ci := {v ? V : class(v) = i}
10: C := {Ci : Ci = ?}
11: until C = C?
12: return C
As soon as an iteration produces no change in the clustering (line 11), the algorithm
stops and outputs the final clustering (line 12). In contrast to the previous algorithm,
Chinese Whispers is parameter-free. Figure 4(g) shows an output example for this
algorithm on the lion co-occurrence graph.
3.3 Clustering of Web Search Results
We are now ready to semantically cluster our Web search results R, which we previously
transformed into bags of words B (cf. Section 3.1). To this end we use the automatically
discovered senses for our input query q (cf. Section 3.2). We adopt different measures,
each of which calculates the similarity between a bag of words bi ? B and the sense
clusters {S1, . . . , Sm} acquired as a result of Word Sense Induction.
Given a result bi, the sense cluster closer to bi will be selected as the most likely
meaning of ri. Formally:
Sense(ri) =
?
?
?
?
?
argmax
j=1,...,m
sim(bi, Sj) if max
j=1,...,m
sim(bi, Sj) > 0
0 else
(11)
where sim(bi, Sj) is a generic similarity value between bi and Sj (0 denotes that no sense
is assigned to result ri). As a result of sense assignment for each ri ? R, we obtain a
clustering C = (C1, . . . , Cm) such that:
Cj = {ri ? R : Sense(ri) = j} (12)
that is, Cj contains the search results classified with the j-th sense of query q.
We now present three different similarity measures between snippet bags of words
and sense clusters (cf. Equation (11)), which we implemented in our framework.
728
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Word Overlap. It calculates the size of the intersection between the two word sets:
simWO(bi, Sj) =
|bi ? Vj|
|bi|
(13)
where Sj = (Vj, Ej) as defined in Section 3.2.2.
Degree Overlap. It calculates the sum of the degrees in the co-occurrence graph compo-
nent of Sj of the snippet?s words in bi:
simDO(bi, Sj) =
?
w?bi?Vj degree(w, Sj)
|bi| ? |Ej|
(14)
where degree(w, Sj) is the number of edges incident on w in the Sj component of the
co-occurrence graph.
Token Overlap. The third measure is similar in spirit to Word Overlap, but takes into
account each token occurrence in the snippet bag of words bi:
simTO(bi, Sj) =
?
w?bi?Vj c(w, ri)
?
w?bi c(w, ri)
(15)
where c(w, ri) is the number of occurrences of the word w in the result ri.
3.4 Cluster Sorting
As a natural consequence of the different similarity values between snippet results and
a given cluster, first, not all the snippets will have the same degree of relevance for the
cluster, and second, the produced clusters will show a different ?quality? depending on
the relevance of the search results therein. We thus sort the clusters in our clustering
C using a similarity-based notion of cluster ?quality.? For each cluster Cj ? C, we de-
termine its similarity with the corresponding meaning Sj by calculating the following
formula:
avgsim(Cj, Sj) =
?
ri?Cj sim(bi, Sj)
|Cj|
(16)
The formula determines the average similarity between the bags of words bi of
the search results ri in cluster Cj and the corresponding sense cluster Sj. The similarity
function sim is the same as that stated in Equation (11) and defined in Section 3.3.
Finally, we rank the elements ri within each cluster Cj by their similarity sim(bi, Sj).
We note that the ranking and optimality of clusters can be improved with more sophis-
ticated techniques (e.g., Crabtree, Gao, and Andreae 2005; Kurland 2008; Kurland and
Domshlak 2008; Lee, Croft, and Allan 2008). This is beyond the scope of this article,
however.
729
Computational Linguistics Volume 39, Number 3
4. In Vivo Experiments: Web Search Result Clustering
We now present two extrinsic experiments aimed at determining the impact of WSI
when integrated into Web search result clustering. We first describe our experimental
set-up (Section 4.1). Next, we present a first experiment focused on the quality of the
output search result clusters (Section 4.2) and a second experiment on the degree of
diversification of semantically enhanced versus non-semantic search result clustering
algorithms (Section 4.3).
4.1 Experimental Set-up
4.1.1 Lexicon. In all our experiments our lexicon was given by the entire WordNet
vocabulary (Miller et al 1990; Fellbaum 1998) augmented with the set of queries in our
test data sets.
4.1.2 Corpora. To calculate the co-occurrence strength between words we need a large
corpus to extract co-occurrence counts and calculate the Dice values (cf. Equation (1)). To
this end we performed separate experiments on two different corpora and constructed
the corresponding co-occurrence databases:
 Google Web1T (Brants and Franz 2006): This corpus is a large
collection of n-grams (n = 1, . . . , 5)?namely, windows of n consecutive
tokens?occurring in one terabyte of Web documents as collected by
Google. We consider all the co-occurrences for lemmas which appear in
the same n-gram (we applied the WordNet lemmatizer to obtain the
canonical form of any word sequence).
 ukWaC (Ferraresi et al 2008): This corpus was constructed by crawling
the .uk domain and obtaining a large sample of Web pages that were
automatically part-of-speech tagged using the TreeTagger tool. For this
corpus we considered all the co-occurrences of WordNet lemmas that
appear in the same sentence.
We selected these two corpora for their very different natures, namely: Google
Web1T is a very large corpus, but with very narrow contexts (5-grams) with a mini-
mum occurrence frequency; ukWaC represents a smaller portion of the Web, but with
larger contexts. This enabled us to observe the behavior of WSI algorithms when co-
occurrences were extracted from different kinds of textual source. In Table 5 we show
examples of the contexts available in the two corpora for the same word (i.e., lion) and
the content words that are found to co-occur with it (shown in italics in Table 5).
Table 5
Example of contexts for the word lion in the Web1T and ukWaC corpora (target word in bold,
co-occurring words in italics).
corpus context example
Web1T roar of the lion in
ukWaC Wilson?s Zoo and its sad lion had given way to the brave attempt to create an early
?Safari Park?.
730
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Table 6
The pseudoword data set.
pseudoword
1 pizza*blog
2 banana*plush
3 kalashnikov*mollusk*sky
4 hurricane*glue*modem
5 pistol*stair*yacht*semantics
6 potassium*razor*walrus*calendula
7 monarchy*archery*google*locomotive*beach
8 hyena*helium*soccer*ukulele*wife
9 human*orchid*candela*colosseum*movie*guitar
10 journey*harmonica*vine*mustache*rhino*police
11 glossary*river*dad*kitchen*aikido*geranium*italy
12 microbe*hug*ship*skull*beer*giraffe*mathematics
4.1.3 Tuning Set. Given that our graph construction step and our WSI algorithms have
parameters, we created a data set to perform tuning. In order to fix the parameter values
independently of our application we created this data set by means of pseudowords
(Schu?tze 1992; Yarowsky 1993). A pseudoword is an ambiguous artificial word created
by concatenating two or more monosemous words. Each monosemous word represents
a meaning of the pseudoword. For example, given the words pizza and blog we can
create the pseudoword pizza*blog. The list of pseudowords we used is reported in
Table 6.
The powerful property of pseudowords is that they enable the automatic construc-
tion of sense-tagged corpora with virtually no effort. In fact, we automatically created
our tuning data set as follows:
1. First, we collected the top 100 results retrieved by Yahoo! for each meaning
(i.e., monosemous word) of the pseudoword (e.g., pizza and blog for
pizza*blog).
2. We created a set of 100 snippets for the ?pseudoword? query (e.g.,
pizza*blog) by selecting snippets from each meaning of the pseudoword
in a number that was proportional to their total occurrence count. For
instance, if pizza and blog occur, respectively, 73,000 and 27,000 times in the
reference corpus (e.g., ukWaC), we selected 73 snippets from pizza and 27
from blog. As a result we simulated the distribution of the two senses of
the pseudoword within the retrieved snippets.
3. Finally, within each of the 100 snippets, we replaced each monosemous
word occurrence (e.g., pizza and blog) with the pseudoword itself (i.e.,
pizza*blog). As a result we obtained a set of 100 snippets for each
ambiguous pseudoword.
4.1.4 Parameters. We used our tuning set to select, first, the optimal values of the pa-
rameters needed to perform graph construction, and, second, to choose the parameter
values specific to each graph-based WSI algorithm. To find the best configurations we
performed tuning by combining the three evaluation measures of Adjusted Rand Index,
Jaccard Index, and F1 (introduced in Section 4.2.1).
731
Computational Linguistics Volume 39, Number 3
Table 7
The optimal parameter values for graph creation obtained as a result of tuning.
Web1T ukWaC
parameter C
u
rv
at
u
re
Sq
u
aT
+
+
V
Sq
u
aT
+
+
E
B
-M
ST
H
yp
er
L
ex
C
h
in
es
e
W
h
is
p
er
s
C
u
rv
at
u
re
Sq
u
aT
+
+
V
Sq
u
aT
+
+
E
B
-M
ST
H
yp
er
L
ex
C
h
in
es
e
W
h
is
p
er
s
max comp. length (?) 2 2 2 2 2 2 2 2 2 2 2 2
min co-occurr. (?) 5E-2 5E-2 5E-2 5E-2 5E-2 2E-1 2E-1 5E-1 2E-1 2E-1 2E-1 2E-1
min Dice (??) 1E-2 1E-2 1E-2 1E-2 5E-2 5E-2 1E-4 1E-4 1E-2 1E-2 1E-4 5E-2
min edge weight (?) 9E-4 9E-4 9E-4 9E-4 9E-4 9E-4 6E-3 3E-3 3E-3 3E-3 7E-3 3E-3
co-occurrence order 1 1 1 1 1 1 1 1 1 1 1 1
Graph construction. Because all our WSI algorithms draw on the co-occurrence graph,
we first tuned the parameters for graph construction for each of the two corpora
(cf. Section 3.2.1), namely: the maximum length of the compounds extracted from the
corpus (?), the minimum number of co-occurrences (?) and minimum Dice value (??)
for vertex addition, and the minimum weight for a graph edge (?) and vertex addition
using first versus second-order co-occurrences. In Table 7 we show the values for these
parameters that optimize the performance of each WSI algorithm on the two corpora.11
In all our runs we used the Word Overlap as a similarity measure for Web search result
clustering.
We observed that the optimal values for many of the parameters used for graph
construction were stable across algorithms, whereas they changed across corpora due
to the different scales of the two corpora. Instead, the maximum compound length and
the co-occurrence order were fixed for all configurations. For the former we observed no
performance increase with longer compound lengths. For the latter we found negligible
improvements with second-order co-occurrences, at the cost, however, of increasing the
size of the resulting graph exponentially. Given the large number of experiments that
would be involved, we decided to avoid this additional workload and use first-order
co-occurrences in all our experiments.
WSI algorithms. Next, for each graph-based WSI algorithm, we kept the given optimal
values fixed for building the co-occurrence graphs for the tuning set queries, while
varying the parameter values of the WSI algorithm, using Word Overlap as similar-
ity measure for Web search result clustering. In Table 8 we show the optimal values
for each algorithm when using Web1T (third column) and ukWaC (fourth column)
to build the co-occurrence graph. Chinese Whispers is not shown as it is parameter-
free (cf. Section 3.2.2). For SquaT++, together with the ? threshold, we also tuned the
three coefficient values ?, ?, and ?, that is, we needed to find the best values for the
coefficients in Equation (8). The optimal coefficient combinations are shown in Table 9
for SquaT++ on vertices and edges, when using the two corpora for graph construction.
The values indicate that all the three graph patterns provide a positive contribution to
the algorithm?s performance, with the same coefficients for SquaT++ on vertices and
11 To this end we used empirically chosen parameters for each WSI algorithm, while delaying the optimal
choice of these parameters to the next paragraph.
732
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Table 8
The WSI algorithms? parameters.
Web1T ukWaC
Curvature removal threshold (?) 0.25 0.35
SquaT++ vertex removal threshold (?) 0.07 0.2edge removal threshold (?) 0.2 0.25
B-MST number of clusters (N) 4 4
HyperLex min hub degree (?) 0.05 0.06min edge weight (??) 0.004 0.01
Table 9
Optimal values for the three graph patterns used in SquaT++.
Web1T ukWaC
? ? ? ? ? ?
SquaT++V 0.34 0.16 0.50 0.34 0.50 0.16
SquaT++E 0.34 0.16 0.50 0.34 0.50 0.16
edges. Interestingly, we observe that, whereas the contribution of triangles (weighted
by ?) is the same across corpora, the respective weights of squares (?) and diamonds
(?) are flipped. After inspection we found that the graphs obtained with Web1T are less
interconnected than those produced with ukWac. Consequently, diamonds are sparser
but more reliable in the Web1T setting, whereas they are much more frequent, and thus
noisier, in the ukWaC setting.
4.1.5 Test Sets. We conducted our in vivo experiments on two test sets of ambiguous
queries:
 AMBIENT (AMBIguous ENTries), a data set that contains 44 ambiguous
queries.12 The sense inventory for the meanings (i.e., subtopics)13 of
queries is given by Wikipedia disambiguation pages. For instance, given
the beagle query, its disambiguation page in Wikipedia provides the
meanings of dog, Mars lander, computer search service, beer brand, and so
forth. The top 100 Web results of each query returned by the Yahoo! search
engine were tagged with the most appropriate query senses according to
Wikipedia (amounting to 4,400 sense-annotated search results). To our
knowledge, this is currently the largest data set of ambiguous queries
available on-line. In fact, other existing data sets, such as those from the
TREC Interactive Tracks, are not focused on distinguishing the subtopics
of a query.
12 http://credo.fub.it/ambient.
13 In the following we use the terms subtopic and word sense interchangeably. As stated in the Introduction,
this work focuses on query disambiguation along the lines of Word Sense Induction and Disambiguation
(Navigli 2009), rather than aspect identification, which concerns subtle distinctions within the same
meaning of a query.
733
Computational Linguistics Volume 39, Number 3
Table 10
Statistics on the AMBIENT and MORESQUE data sets.
data set queries queries by length average1 2 3 4 subtopics
AMBIENT 44 35 6 3 0 17.9
MORESQUE 114 0 47 36 31 6.6
 MORESQUE (MORE Sense-tagged QUEry results), a data set that we
developed as an integration of AMBIENT following guidelines provided
by its authors.14 In fact, our aim was to study the behavior of Web search
algorithms on queries of different lengths, ranging from one to four words.
The AMBIENT data set, however, is composed in the main of one-word
queries. MORESQUE provides dozens of queries of length 2, 3, and 4,
together with the top 100 results from Yahoo! for each query annotated
precisely as was done in the AMBIENT data set. We decided not to
discontinue the use of Yahoo! mainly for homogeneity reasons.
Wikipedia has already been used as a sense inventory by, among others, Bunescu
and Pasca (2006), Mihalcea (2007), and Gabrilovich and Markovitch (2009). Santamar??a,
Gonzalo, and Artiles (2010) have investigated in depth the benefit of using Wikipedia
as the sense inventory for diversifying search results, showing that Wikipedia offers
much more sense coverage for search results than other resources such as WordNet.
We report the statistics on the composition of the two data sets in Table 10. Given
that the snippets could possibly be annotated with more than one Wikipedia subtopic,
we also determined the average number of subtopics per snippet. This amounted to
1.01 for AMBIENT and 1.04 for MORESQUE for snippets with at least one subtopic
annotation. We can thus conclude that multiple subtopic annotations are infrequent.
Finally, we analyzed how the different subtopics are distributed over the snippet results
for each query. To do this we calculated the standard deviation of the subtopic popula-
tion for each individual query, which we show in Figure 5. We observed a considerable
difference in the standard deviations of shorter and longer queries (e.g., between those
from the AMBIENT data set [from 1 to 44 in the figure] and the MORESQUE data set
[from 45 to 158]). We further calculated the average standard deviation over the two
data sets? queries, obtaining 6.5 for AMBIENT and 13.1 for MORESQUE. Therefore we
anticipate that the longer the query length, the more unbalanced will be the distribution
of its subtopics over the top-ranking results.
In line with previous experiments on search result clustering, our data set does
not contain monosemous queries for two reasons: (i) we are interested in queries with
multiple meanings, and (ii) monosemous queries would increase the performance of
our experiments because no diversification would be needed for them.
4.1.6 Systems. We performed a comparison of our semantically enhanced search result
clustering systems with nonsemantic ones.
14 http://lcl.uniroma1.it/moresque.
734
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100  120  140  160
st
d 
de
v
query ID
Figure 5
Standard deviations for the subtopic population of the AMBIENT queries (1?44) and the
MORESQUE queries (45?158).
Semantically enhanced systems. We integrated our graph-based WSI algorithms (Curva-
ture, SquaT++, B-MST, HyperLex, and Chinese Whispers; cf. Section 3.2) into our search
result clustering framework. We tested each algorithm when combined with any of the
snippet-to-sense similarity measures introduced in Section 3.3.
Nonsemantic systems. We compared our semantically enhanced systems with four Web
clustering engines, namely:
 Lingo (Osinski and Weiss 2005): A Web clustering engine implemented
in the Carrot open-source framework15 that clusters the most frequent
phrases extracted using suffix arrays.
 Suffix Tree Clustering (STC) (Zamir and Etzioni 1998): The original Web
search clustering approach based on suffix trees.
 KeySRC (Bernardini, Carpineto, and D?Amico 2009): A state-of-the-art
Web clustering engine built on top of STC with part-of-speech pruning
and dynamic selection of the cut-off level of the clustering dendrogram.
 Yippy16 (formerly Clusty): A state-of-the-art metasearch engine developed
by Viv??simo aimed at clustering search results into meaningful topics.
For Lingo and STC we used the Carrot implementation which we integrated into
our framework. Conversely, for Yippy we used the on-line output provided by the Web
search engine.
15 http://project.carrot2.org.
16 http://search.yippy.com.
735
Computational Linguistics Volume 39, Number 3
4.1.7 Baselines. We compared the four systems against three baselines:
 Singletons: Each snippet is clustered as a separate singleton (i.e., the
cardinality of the resulting clustering C is |C| = |R|).
 All-in-one: All the snippets are clustered into a single cluster (i.e., |C| = 1).
 Wikipedia clustering: Given an input query q, we apply Equation (13) to
match the bag of content words of each snippet against that of each
Wikipedia page representing a meaning of q (we use the disambiguation
page of q as its sense inventory). The snippet is then added to the cluster
corresponding to the best-matching Wikipedia page. Given q, we obtain a
clustering whose size is determined by the number of meanings in the
Wikipedia disambiguation page of q.
The first two baselines help us determine whether the evaluation measures have
a bias towards very small (singletons) or big clusters (all-in-one). The third baseline,
based on Wikipedia, is a tough one in that?in contrast to our systems?it relies on a
predefined sense inventory (which is the same as that used in the manual classification
of the test set) to cluster the snippets. Consequently the baseline does not ?induce?
the senses, but just classifies (or labels) each snippet with the best-matching Wikipedia
sense of the input query.
4.2 Experiment 1: Evaluation of the Clustering Quality
4.2.1 Evaluation Measures. In this first experiment our goal is to evaluate the quality
of the output produced by our search result clustering systems. Unfortunately, the
clustering evaluation problem is a notably hard issue, and one for which there exists no
unequivocal solution. Many evaluation measures have been proposed in the literature
(Rand 1971; Zhao and Karypis 2004; Rosenberg and Hirschberg 2007; Geiss 2009, inter
alia) so, in order to get exhaustive results, we tested three different clustering quality
measures, namely, Adjusted Rand Index, Jaccard Index, and F1-measure, which we
introduce hereafter. Each of these measures M(C,G) calculates the quality of a clustering
C, output for a given query q, against the gold standard clustering G for that query. We
then determine the overall results on the entire set of queries Q in the test set according
to the measure M by averaging the values of M(C,G) obtained for each single test
query q ? Q.
Adjusted Rand Index. Given a gold standard clustering G, the Rand Index (RI; Rand 1971)
of a clustering C is a measure of clustering agreement commonly used in the literature,
calculated as follows:
RI(C,G) = TP + TN
TP + FP + FN + TN
(17)
where TP is the number of true positives (i.e., snippet pairs) that are in the same cluster
both in C and G, TN is the number of true negatives (i.e., pairs which are in different
clusters in both clusterings), and FP and FN are, respectively, the number of false
positives and false negatives. For the gold standard G we use the clustering induced
by the sense annotations provided in our data sets for each snippet (i.e., each cluster
contains the snippets manually associated with a particular Wikipedia page, that is,
subtopic, of the query).
736
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Rand Index determines the percentage of snippet pairs that are in the same con-
figuration in both C and G, but its main weakness is that it does not take chance into
account. In fact, the expected value of the RI of two random clusterings is not a constant
value (e.g., 0). This issue is addressed by the Adjusted Rand Index (ARI; Hubert and
Arabie 1985), which corrects the RI for chance agreement and makes it vary according
to expectation:
ARI(C,G) = RI(C,G) ? E(RI(C,G))
max RI(C,G) ? E(RI(C,G)) (18)
where E(RI(C,G)) is the expected value of the RI. Given two clusterings C = (C1, . . . , Cm)
and G = (G1, G2, . . . , Gg), we first quantify the degree of overlap between C and G using
the contingency table reported in Table 11, where nij denotes the number of objects in
common between Gi and Cj (i.e., nij = |Gi ? Cj|) and ai and bj represent, respectively,
the number of objects in Gi and Cj. Now, Equation (18) can be reformulated as follows
(Steinley 2004):
ARI(C,G) =
?
ij
(nij
2
)
? [
?
i
(ai
2
)
?
j
(bj
2
)
]/
(
N
2
)
1
2 [
?
i
(ai
2
)
+
?
j
(bj
2
)
] ? [
?
i
(ai
2
)
?
j
(bj
2
)
]/
(
N
2
)
(19)
Differently from the original RI (which ranges between 0 and 1), the ARI ranges
between ?1 and +1 and is 0 when the index equals its expected value. Given the issues
with RI, in our experiments we focused on ARI.
Jaccard Index. The ARI compares a clustering C with a gold standard G both in terms
of the snippets occurring in the same cluster (TP) and those which are assigned to
different clusters (TN). There are typically many TN in a clustering, however; therefore
this measure tends to overweight the usefulness of snippets placed in different clusters.
The Jaccard Index (JI) is a measure that addresses this issue. JI is calculated as follows:
JI(C,G) = TP
TP + FP + FN
(20)
In fact, in contrast to RI (cf. Equation (17)), neither the numerator nor the denomi-
nator of JI include the TN term.
Table 11
Contingency table for the clusterings G and C.




G
C
C1 C2 ? ? ? Cm Sums
G1 n11 n12 ? ? ? n1m a1
G2 n21 n22 ? ? ? n2m a2
...
...
...
. . .
...
...
Gg ng1 ng2 ? ? ? ngm ag
Sums b1 b2 ? ? ? bm N
737
Computational Linguistics Volume 39, Number 3
F1-Measure. The ARI and the JI calculate the clustering quality using snippet pairs as
the basic unit. Instead, a clustering C can be evaluated by focusing on the precision of
the single clusters and the topics recalled by them, that is, we evaluate C according to
its precision (P) and recall (R) against a gold standard G. Precision determines how
accurately the clusters of C represent the topics in the gold standard G, and recall
measures how accurately the topics in G are covered by the clusters in C.
The precision of a cluster Cj ? C can be calculated as follows (Crabtree, Gao, and
Andreae 2005):
P(Cj) =
|Ctj |
|Cj|
(21)
where t is the majority topic in Cj for a given query,17 and Ctj is the set of snippets in
Cj which are tagged with subtopic t in the gold standard G. The recall of a topic t is,
instead, calculated as:
R(t) =
|
?
Cj?Ct C
t
j |
nt (22)
where Ct is the subset of clusters of C whose majority topic is t, and nt is the number of
snippets tagged with subtopic t in the gold standard. The total precision and recall of
the clustering C are then calculated as:
P =
?
Cj?C P(Cj)|Cj|
?
Cj?C |Cj|
; R =
?
t?T R(t)nt
?
t?T nt
(23)
where T is the set of subtopics in the gold standard G for the given query. The two
values of P and R are then combined into their harmonic mean, namely, the F1 measure
(van Rijsbergen 1979):
F1(C,G) = 2PRP + R (24)
Note that, in contrast with ARI, in calculating precision and recall we do not
consider untagged gold standard snippets.
4.2.2 Results and Discussion. We show the results of the WSI algorithms in Table 12.
With few exceptions, the results obtained on the two corpora are comparable. SquaT++,
which extends Curvature with the Square and Diamond patterns, obtains higher perfor-
mance. Although integrating three different graph patterns is beneficial, the difference
between using edges and vertices to do so is mostly marginal.
The first important insight is that the best results, shown in bold in Table 12, are
consistent across corpora and similarity measures (i.e., WO, DO, and TO), thus showing
the robustness of the WSI algorithms when co-occurrences are extracted from different
textual sources. The pairwise evaluation measures (i.e., ARI and JI), however, rank the
17 The majority topic for a cluster Cj ? C is the topic t for which there exists the maximum number of
snippets in Cj tagged with t.
738
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Table 12
Results of graph-based WSI algorithms with Web1T and ukWaC and various similarity measures
(Word Overlap, Degree Overlap, and Token Overlap). The reported measures are Adjusted Rand
Index (ARI), Jaccard Index (JI), and F1 (percentages). We also show the average number of
clusters per query produced by each algorithm.
Algorithm Sim. Web1T ukWaC
ARI JI F1 # cl. ARI JI F1 # cl.
Curvature
WO 67.03 74.10 58.34 2.3 64.86 72.74 58.84 3.5
DO 66.88 73.76 58.67 2.3 64.02 71.04 59.85 3.5
TO 67.14 74.04 58.36 2.3 65.03 72.46 58.73 3.5
SquaT++V
WO 69.65 75.69 59.19 2.1 69.27 75.55 59.18 2.3
DO 69.21 75.45 59.19 2.1 68.73 75.14 59.75 2.3
TO 69.67 75.69 59.19 2.1 69.33 75.55 59.23 2.3
SquaT++E
WO 69.88 75.82 59.39 2.7 69.84 75.74 59.70 3.9
DO 69.63 75.74 60.99 2.7 69.86 75.35 63.00 3.9
TO 69.88 75.83 59.40 2.7 69.86 75.70 59.78 3.9
B-MST
WO 60.76 71.51 64.56 5.0 61.15 72.24 65.57 5.0
DO 66.48 69.37 64.84 5.0 67.60 70.02 67.41 5.0
TO 63.17 71.21 64.04 5.0 64.18 71.93 65.46 5.0
HyperLex
WO 60.86 72.05 65.41 13.0 56.59 72.00 70.69 17.0
DO 66.27 68.00 71.91 13.0 65.92 67.31 76.88 17.0
TO 62.82 70.87 65.08 13.0 61.64 70.61 70.42 17.0
Chinese Whispers
WO 67.75 75.37 60.25 12.5 68.77 75.45 59.66 6.5
DO 65.95 69.49 70.33 12.5 67.86 72.34 66.16 6.5
TO 67.57 74.69 60.50 12.5 68.97 75.28 59.79 6.5
WSI algorithms differently from the F1 measure. In fact, when we focus on pairwise
evaluation measures, SquaT++ outperforms all other systems on both corpora, with
Chinese Whispers ranking second. B-MST and HyperLex obtain lower results. When we
look into the precision of the output clusters and the recall of the gold-standard topics
(i.e., we calculate F1), however, we observe an inverse trend: HyperLex, B-MST and, to
a lesser extent, Chinese Whispers achieve the best performance, whereas Curvature and
SquaT++ obtain lower F1. This is because, assuming comparable precision, producing
more clusters (as is done by HyperLex, B-MST, and Chinese Whispers) implies more
chances to obtain higher recall, thus better diversifying among the topics of the retrieved
search results. More specifically, B-MST and, especially, HyperLex benefit from the use
of ukWaC in terms of F1 performance, with HyperLex gaining around 5% when moving
from Web1T to ukWaC.
Finally, in most cases we observe negligible differences between the three different
similarity measures (i.e., WO, DO, TO, cf. Section 3.3), with some exceptions concerning
B-MST, HyperLex, and Chinese Whispers.
We now report the best results for our WSI algorithms in Table 13, compared against
those of nonsemantic systems (i.e., Lingo, STC, and KeySRC, cf. Section 4.1.6) and our
three baselines (i.e., all-in-one, singleton, and Wikipedia, cf. Section 4.1.7). For the WSI
algorithms we show the results when using the WO measure, because, first, DO uses
graph information and thus cannot be applied to nonsemantic systems, and, second, in
739
Computational Linguistics Volume 39, Number 3
Table 13
A comparison between different search result clustering approaches (percentages). The best
results for each of the three classes of algorithms is shown in bold.
Algorithm Web1T ukWaC
ARI JI F1 #cl. ARI JI F1 # cl.
WSI-based
Curvature 67.03 74.10 58.34 2.3 64.86 72.74 58.84 3.5
SquaT++V 69.65 75.69 59.19 2.1 69.27 75.55 59.18 2.3
SquaT++E 69.88 75.82 59.39 2.7 69.84 75.74 59.70 3.9
B-MST 60.76 71.51 64.56 5.0 61.15 72.24 65.57 5.0
HyperLex 60.86 72.05 65.41 13.0 56.59 72.00 70.69 17.0
Chinese Whispers 67.75 75.37 64.25 12.5 68.77 75.45 59.66 6.5
SRC systems
Lingo ?0.53 36.36 16.73 2.0 ?0.53 36.36 16.73 2.0
STC ?7.90 38.23 14.96 2.0 ?7.90 38.23 14.96 2.0
KeySRC 14.34 27.77 63.11 18.5 14.34 27.77 63.11 18.5
Baselines
All-in-one 0.00 47.12 42.40 1.0 0.00 47.12 42.40 1.0
Singleton 0.00 0.00 68.17 100.0 0.00 0.00 68.17 100.0
Wikipedia 13.83 56.02 14.33 5.7 13.83 56.02 14.33 5.7
most cases (as remarked earlier) there are negligible differences between the two other
similarity measures (i.e., WO and TO, see Table 12).
Our first finding here is that WSI-based search result clustering outperforms all
other approaches across all evaluation measures on the two corpora, except for KeySRC
and the singleton baseline when using the F1 measure. We note, however, that although
KeySRC outperforms the WSI algorithms based on graph patterns in terms of F1, it
attains very low ARI and JI results. Even worse, the singleton baseline produces trivial,
meaningless clusterings, as measured by ARI and JI. The all-in-one baseline, instead,
obtains non-zero JI (thanks to the true positives taken into account), but again zero ARI.
Further, its F1 is lower than singleton, because of its lower recall. The Wikipedia baseline
fares well compared with the other baselines in terms of ARI and JI, but achieves lower
F1, again because of low recall. Finally, KeySRC consistently outperforms the other SRC
systems in terms of ARI and F1.
In order to perform a fair comparison of our systems with Yippy we used a modified
version of our test set that retains only the Yahoo! results that were also returned by
Yippy. The average number of results over all queries in the resulting data set is 24.4,
with a minimum and maximum number of 3 and 56 results per query, respectively.
We report the results on the reduced data set in Table 14. Among the classical search
result clustering systems, Yippy performs worse in terms of ARI and JI. Instead, when
we focus on the precision and recall of the output clusters, Yippy outperforms all other
nonsemantic systems, while lagging behind all WSI algorithms (which use Web1T). One
finding here is that, even in the presence of a smaller number of snippets per query,
semantic systems perform best, whereas other approaches, which rely (like KeySRC) on
the availability of a sufficient number of snippets, fall short.
4.3 Experiment 2: Evaluation of the Clustering Diversity
4.3.1 Evaluation Measure. Most of today?s search engines return a flat list of search results.
We thus performed a second experiment aimed at quantifying the impact of our Web
search result clustering systems on flat-list search engines. In other words, our goal was
740
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Table 14
A comparison of different SRC approaches, including Yippy (on a reduced version of the
AMBIENT+MORESQUE data set; WSI systems use Web1T).
Algorithm ARI JI F1 # cl.
WSI-based
Curvature 63.21 75.22 64.86 2.1
SquaT++V 64.25 75.29 64.99 2.0
SquaT++E 64.13 75.96 65.30 2.3
B-MST 53.72 76.51 70.82 5.0
HyperLex 55.93 76.63 72.04 7.9
Chinese Whispers 55.41 74.83 70.52 8.4
SRC systems
Lingo ?1.58 35.65 17.00 2.0
STC 7.91 38.34 15.04 2.0
KeySRC ?0.01 31.80 32.90 3.3
Yippy 3.80 14.81 64.52 14.9
Baselines
All-in-one 0.00 45.66 48.30 1.0
Singleton 0.00 0.00 72.19 24.4?
Wikipedia 10.00 52.05 15.60 5.8
? Corresponding to the average number of snippet results in the reduced data set.
to determine how many different meanings of a query are covered in the top-ranking
results shown to the user. One natural way of measuring such performance is given
by S-recall@K (Subtopic recall at rank K) and S-precision@r (Subtopic precision at
recall r) (Zhai, Cohen, and Lafferty 2003). S-recall@K counts the number of different sub-
topics retrieved for q in the top K results returned:
S-recall@K =
| ?Ki=1 subtopics(ri)|
m (25)
where subtopics(ri) is the set of subtopics manually assigned to the search result ri and
m is the number of subtopics for query q in the gold standard. In order to cut out some
noise, we calculated the S-recall@K considering only the subtopics assigned to at least
two snippets.
S-precision@r instead determines the ratio of different subtopics retrieved for q in
the first Kr documents, where Kr is the minimum number of top results for which the
system achieves recall r. Formally:
S-precision@r =
| ?Kri=1 subtopics(ri)|
Kr
(26)
So whereas S-recall@K aims at determining the performance of a system at
retrieving the largest number of topics for the query q in the K top-ranking results,
S-precision@r quantifies the ratio of distinct subtopics covered by the minimal set of
results returned for which the system obtains a specific recall r. Note that unambiguous
queries would perform with S-precision@r = S-recall@K = 1 for all values of r and K.18
18 Here again we focus on the polysemy of queries in the traditional (computational) linguistic sense. See
Section 2.6 for a discussion.
741
Computational Linguistics Volume 39, Number 3
Table 15
S-recall@K on all queries (percentages). The results for WSI algorithms are reported for both
Web1T and ukWaC.
K
System 3 4 5 6 7 8 9 10 15 20
W
eb
1T
Curvature 48.2 53.5 57.1 60.5 64.6 67.4 69.4 72.6 81.5 86.2
SquaT++V 47.1 52.0 55.5 59.3 61.9 65.6 68.4 70.4 79.4 86.2
SquaT++E 47.6 51.9 56.2 59.6 62.6 64.5 67.0 69.0 78.5 84.4
B-MST 49.1 55.8 59.4 62.5 65.6 67.8 70.0 72.3 80.0 85.5
HyperLex 50.4 55.5 60.5 63.4 66.2 69.3 71.2 72.9 78.9 84.9
Chinese Whispers 48.8 53.3 58.3 62.2 65.4 68.5 70.8 72.8 78.9 84.5
u
kW
aC
Curvature 47.2 51.8 56.8 59.5 62.5 65.4 67.0 68.4 76.3 83.4
SquaT++V 47.1 51.9 56.7 59.4 62.9 65.6 68.1 70.3 78.8 84.4
SquaT++E 47.9 51.2 55.1 58.6 61.5 64.8 67.8 69.6 78.9 84.9
B-MST 49.9 55.3 61.0 63.9 66.9 70.7 73.7 75.6 83.3 87.5
HyperLex 51.1 59.3 64.6 67.3 71.3 73.9 74.9 76.6 83.4 87.6
Chinese Whispers 49.7 54.5 57.7 61.2 64.0 66.7 69.5 71.4 79.4 84.0
KeySRC 39.5 46.1 48.7 51.4 54.3 57.1 59.6 61.7 68.2 72.5
EP 36.1 41.4 44.6 50.8 53.5 55.0 57.1 59.2 67.9 73.3
Yahoo! 42.3 47.6 51.4 54.6 57.3 59.4 61.0 63.4 69.1 73.3
These two measures are only suitable, however, for systems returning ranked lists
(such as Yahoo! and Essential Pages). In order to apply them to search result clustering
systems, we flatten each clustering to a list of search results. To do so, given a clustering
C = (C1, C2, . . . , Cm), we add to the initially empty list the first element19 of each cluster
Cj (j = 1, . . . , m); then we iterate the process by selecting the second element of each
cluster Cj such that |Cj| ? 2, and so on. The remaining elements returned by the search
engine, but not included in any cluster of C, are appended to the bottom of the list in
their original order.
4.3.2 Results and Discussion. The results in terms of S-recall@K are shown in Table 15. The
first key finding is that, independently of the adopted corpus for graph construction,
each of the WSI algorithms outperforms all nonsemantic systems, including the state-
of-the-art search resulting clustering engine (KeySRC), Essential Pages (see Section 2.4),
and the Yahoo! baseline. This result provides strong evidence that inducing senses for
a given ambiguous query is beneficial for the diversification of snippet results. Not
all WSI algorithms perform the same, however: In fact, we observe that exploiting
local graph patterns (as done by Curvature and SquaT++) typically leads to worse
results compared with other graph-based approaches. We do not observe substantial
differences between Curvature and SquaT++ on edges and vertices. We hypothesize
that the lack of significant difference in the diversification performance of the three
pattern-based WSI algorithms is due to the lower number of clusters they produce
(in the order of around two to three clusters, cf. Table 12).
19 Recall that the snippets within a cluster are sorted by relevance, cf. Section 3.4.
742
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
The best performance on both corpora is, instead, obtained by HyperLex, tailed
by B-MST and Chinese Whispers. HyperLex is more complex and requires the tuning
of many parameters (Agirre et al 2006a), however. Interestingly, we observe that the
ranking of WSI algorithms according to S-recall@K closely matches that obtained with
the F1 measure for clustering quality. Finally, among the nonsemantic alternatives,
Yahoo! fares well and surpasses KeySRC and EP.
To get futher insights into the performance of the best semantic systems, in Fig-
ure 6 we graphed the values of S-recall@K for representative systems, namely, B-MST,
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25
S-
re
ca
ll@
K
K
B-MST
SquaT++V
KeySRC
Essential Pages
Yahoo!
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25
S-
re
ca
ll@
K
K
B-MST
SquaT++V
KeySRC
Essential Pages
Yahoo!
Figure 6
S-recall@K trend for B-MST, SquaT++V , KeySRC, Essential Pages, and Yahoo! on Web1T (top)
and ukWaC (bottom).
743
Computational Linguistics Volume 39, Number 3
SquaT++V, and the three nonsemantic systems. The results shown in the figure are
those obtained with Web1T (top) and ukWaC (bottom). We can see that SquaT++V lags
behind B-MST especially for low values of K. As also remarked previously, Yahoo! tends
to perform better than KeySRC.
As regards S-precision@r, shown in Table 16, again all WSI algorithms outperform
nonsemantic systems. The general trend observed for S-recall@K is confirmed here:
HyperLex generally achieves the best values of S-precision@r, with good performance
for all other semantic systems. All in all, HyperLex has the best balance between recall
and precision, with better diversification performance on ukWaC, and therefore looks
like the most suitable choice. B-MST, however, is much simpler and requires just one
parameter (i.e., the number of clusters), which can also be exploited by the user to get
finer- or coarser-grained search result groups. As was previously done for S-recall@K,
we also graphed the values of S-precision@r for the same representative systems in
Figure 7.
5. In Vitro Experiment: Evaluating the Induced Senses
Although the primary aim of this work was to demonstrate a relevant, end-to-end appli-
cation of sense discovery techniques, we performed an additional in vitro experiment
aimed at verifying the quality of the discovered senses independently of the task in
which they are used.
When performing in vitro evaluations, no single intrinsic measure provides a
clear hint as to which algorithm performs best (Manandhar et al 2010). In fact, some
measures favor large clusters, whereas others are based on the expectaction that the
WSI algorithm will discover more fine-grained sense distinctions. To provide further
insights into the clusters produced by our graph-based WSI algorithms, we performed
Table 16
S-precision@r on all queries (percentages). The results for WSI algorithms are reported for both
Web1T and ukWaC.
r
System 50 60 70 80 90
W
eb
1T
Curvature 46.0 33.8 30.7 25.2 21.9
SquaT++V 45.9 34.5 28.3 24.0 21.0
SquaT++E 42.8 35.3 29.1 23.6 20.4
B-MST 43.6 35.3 29.3 25.7 21.6
HyperLex 46.5 38.0 31.3 26.5 22.5
Chinese Whispers 49.4 35.2 28.9 24.2 21.9
u
kW
aC
Curvature 37.9 33.2 27.4 24.3 20.4
SquaT++V 45.0 34.4 28.8 25.5 22.2
SquaT++E 42.0 34.0 29.3 25.2 20.5
B-MST 49.3 36.7 33.5 26.3 22.5
HyperLex 51.4 40.0 32.4 27.6 22.6
Chinese Whispers 45.1 36.6 30.1 24.5 20.5
KeySRC 29.3 22.3 17.7 15.4 12.0
EP 33.6 24.9 18.9 16.1 13.2
Yahoo! 36.1 25.7 18.7 15.5 12.6
744
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 50  55  60  65  70  75  80  85  90
S-
pr
ec
isi
on
@
r
r
B-MST
SquaT++V
KeySRC
Essential Pages
Yahoo!
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 50  55  60  65  70  75  80  85  90
S-
pr
ec
isi
on
@
r
r
B-MST
SquaT++V
KeySRC
Essential Pages
Yahoo!
Figure 7
S-precision@r trend for B-MST, SquaT++V , KeySRC, Essential Pages, and Yahoo! on Web1T (top)
and ukWaC (bottom).
a qualitative evaluation of the output clusters. To this end we randomly selected
17 queries from our query data set. For each query, we submitted in random order
the output of three representative WSI algorithms on the ukWaC corpus, namely,
Curvature, HyperLex, and B-MST, to five annotators.
We show an excerpt of the evaluation procedure for the query excalibur in
Table 17. On the left side of the table we propose an example of an anonymized set of
three clusterings (i.e., one for each algorithm, shown in columns 2?4) presented to our
annotators. Each algorithm produced a group of clusters, each of which consisted of a
set of words strictly related to the meaning conveyed by the cluster itself, as discussed in
745
Computational Linguistics Volume 39, Number 3
Table 17
An example of the manual evaluation procedure for the query excalibur: We show a clustering
triple proposed to the evaluator (left side) and an example of produced ranking (right side).
Clustering A Clustering B Clustering C
Cluster 1
movie movie hotel
book DVD movie
casino video review
book offer
Cluster 2
sword sword
stone lineage
artillery stone
Cluster 3
comic hotel
comic strip chicago
casino
rank algorithm
1st B
2nd A
3rd C
Section 3.2.2. The annotators were asked to rank the three clusterings according to their
own preference (ties were allowed). On the right side of Table 17 we show an example
of ranking for the three clusterings. In the example, clustering B was deemed to be more
representative, because it better models three meanings of excalibur, namely: the film-
novel meaning, the sword meaning, and the hotel casino meaning, whereas clustering
A mixes the movie and the casino meaning within cluster 1, and, even worse, clustering
C just provides a singleton cluster.
Finally, for each query, and for the entire set of 17 queries, we calculated the average
ranking obtained by each WSI algorithm. The overall results are shown in Table 18
(last row): 1.7 for HyperLex, 1.8 for B-MST, and 2.4 for Curvature. This experiment
corroborates the findings obtained from our extrinsic experiments: Curvature is the
worst-ranking system (probably because of the low number of induced senses), whereas
HyperLex and B-MST are more apt to discriminate between the meanings of an input
query. It is worth noting that the annotators often assigned the same rank to the clus-
ters produced by B-MST and HyperLex, confirming our extrinsic finding that the two
algorithms tend to have a similar behavior, compared with local graph pattern WSI.
6. Time Performance Analysis
Finally, because we are interested in the real-world application of the WSI techniques
we discussed, we decided to collect statistics about the execution times of each system
on the AMBIENT and MORESQUE data sets. We carried out this performance analysis
on a workstation using Sun Java 1.6 VM running on OpenSuse 11.4 (64 bit) with 16 GB
PC3-15000 RAM, Intel Xeon E3-1240@3.30 GHz, and 1.5 TB hard disk space.
In the graph construction step, in common with all WSI algorithms, most (i.e., about
80%) of the computational load is due to the interaction with the database management
system (DBMS, we used MySQL 5.1), and the remaining CPU time is used for popu-
lating the graph. On average constructing a co-occurrence graph takes 10?12 seconds
per query. We note, however, that our algorithms were not engineered to work in an
enterprise, possibly distributed, environment, with a commercial DBMS. Moreover, a
fully engineered architecture might appropriately precalculate and cache the graphs
concerning the most frequent queries.
746
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Table 18
Results of the manual evaluations. The average scores assigned by the assessors to each one of
the 17 queries are shown in columns 2, 3, and 4. In the last row we report the average results
over all queries.
query id B-MST HyperLex Curvature
1 2.0 1.0 3.0
2 1.4 2.0 2.4
3 2.2 1.6 2.2
4 1.4 2.4 2.2
5 2.6 2.0 1.4
6 1.2 1.8 3.0
7 2.2 1.4 2.1
8 2.2 2.4 1.4
9 1.8 2.4 1.6
10 1.8 1.6 2.2
11 1.6 1.4 3.0
12 1.6 1.8 2.6
13 1.8 1.2 3.0
14 1.8 1.2 3.0
15 1.8 1.8 2.0
16 1.6 1.4 3.0
17 1.8 1.2 3.0
all 1.8 1.7 2.4
The average time performance of WSI algorithms including sense discovery and
snippet clustering (but excluding graph construction) are shown in Table 19, expressed
in average number of seconds per query for both corpora. These numbers are compared
with the time performance of nonsemantic systems (bottom part of the table).
We observe that, among pattern-based algorithms, SquaT++ has a high runtime
cost, due to the heavy calculation of three different graph patterns. SquaT++E is
particularly onerous in the presence of large amounts of edges, which is the case
for ukWaC. Curvature, instead, has a lower cost, because the triangle pattern is less
Table 19
Execution times expressed in seconds. For WSI algorithms, the reported times include the sense
discovery and snippet clustering steps and excludes the graph construction step.
Algorithm Web1T ukWaC
Curvature 0.34 0.34
SquaT++V 28.98 14.45
WSI-based SquaT++E 21.49 169.13
systems B-MST 0.24 0.27
HyperLex 0.16 0.13
Chinese Whispers 0.28 0.35
SRC systems
Lingo 0.27
STC 0.20
KeySRC 1.00?
? Estimated by the authors of KeySRC.
747
Computational Linguistics Volume 39, Number 3
onerous to compute. Interestingly, the algorithms which we experimentally found
to perform best (i.e., B-MST, HyperLex, and Chinese Whispers) have a much lower
computational load compared with graph-pattern based algorithms. We found that
HyperLex is particularly fast, with an average time of 0.1 seconds per query. Finally, we
observe that the cost of the best WSI algorithms is not very far off that of nonsemantic
SRC systems.
7. Conclusions
In this article we have presented a novel approach to Web search result clustering based
on the automatic discovery of word senses from raw text. Key to our approach is the
idea of, first, automatically inducing senses for the target query and, second, clustering
the search results based on their semantic similarity to the word senses induced.
A sizeable body of work looking at the benefit of word senses for Web search
already exists at the intersection between lexical semantics and information retrieval.
That research, however, has focused almost exclusively on classical Word Sense Dis-
ambiguation, with contrasting and often inconclusive results. In this article, instead,
we provide clear indication on the usefulness of a looser notion of sense to cope with
ambiguous queries.
In fact, our experiments on data sets of queries of different lengths show that our
approach outperforms all nonsemantic approaches to Web search result clustering. The
main advantage of using Word Sense Induction lies in its dynamic production of word
senses that cover both concepts (e.g., beagle as a specific breed of dog) and instances (e.g.,
beagle as a specific instance of a space lander). This is in contrast with static dictionaries
such as WordNet that are typically used in Word Sense Disambiguation and which, by
their very nature, mainly encode concepts.
Not only have we shown that graph-based WSI, when applied to search result
clustering, surpasses its nonsemantic alternatives, but we have also provided an end-to-
end evaluation framework that enables fair comparison of WSI algorithms. As a result,
we are able to overcome many of the issues with the evaluation of clustering algorithms
(von Luxburg, Williamson, and Guyon 2012), including the lack of a single unbiased
intrinsic measure (Manandhar et al 2010). Moreover, new WSI algorithms can be added
at any time and compared with those already integrated into the framework. Building
upon this, we are currently organizing a Semeval-2013 task for the extrinsic evaluation
of WSI algorithms.20 As of today, we are releasing a new data set of 114 ambiguous
queries and 11,400 sense-annotated snippets.21 Given the present paucity of ambiguous
query data sets available (Sanderson 2008), we hope our data set will be useful in future
comparative experiments.
Thanks to its modular structure, our framework can easily be extended in many
other ways, including the addition of new snippet similarity measures, text corpora,
query data sets, evaluation measures, and so on. Although our graphs are centered on
words (as vertices), we are also interested in testing new graph construction procedures
based on the use of collocations as vertices, as done by Korkontzelos and Manandhar
(2010). Furthermore, the framework is independent of the target language, in that it just
requires a large-enough corpus for co-occurrence extraction in that language and some
basic tools for processing text (i.e., a stopword list, a lemmatizer, and a compounder).
20 http://www.cs.york.ac.uk/semeval-2013/task11/.
21 The data set is available at http://lcl.uniroma1.it/moresque/.
748
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
As future work, the framework might be integrated with distributional semantics
models and techniques (Baroni and Lenci 2010; Erk, Pado?, and Pado? 2010; Mitchell and
Lapata 2010; Boleda, im Walde, and Badia 2012; Clarke 2012; Silberer and Lapata 2012,
inter alia).
Finally we note that, although in this article our framework was applied to poly-
semous queries only, nothing prevents it from being used to perform experiments at
different levels of sense granularity. A qualitative evaluation of preliminary experiments
in aspect identification (cf. Section 2.6), which requires the detection of very fine-grained
subsenses of possibly monosemous queries, showed that WSI also seems to perform
well in this task. Given the high number of monosemous queries submitted to Web
search engines, we believe that further investigation in this direction may well reveal
additional benefits of WSI for Web Information Retrieval.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI no. 259234 and the
CASPUR High-Performance Computing
Grants 515/2011 and 118/2012.
Thanks go to Google for providing the
Web1T corpus for research purposes,
Claudio Carpineto and Massimiliano
D?Amico for producing the output of
KeySRC and Essential Pages, and Stanislaw
Osinski and Dawid Weiss for their help
with Lingo and STC. Additional thanks go
to Jim McManus and the three anonymous
reviewers for their helpful comments.
References
Agirre, Eneko, David Mart??nez, Oier Lo?pez
de Lacalle, and Aitor Soroa. 2006a.
Evaluating and optimizing the parameters
of an unsupervised graph-based WSD
algorithm. In Proceedings of the 1st
Workshop on Graph-Based Algorithms for
Natural Language Processing, pages 89?96,
New York.
Agirre, Eneko, David Mart??nez, Oier
Lo?pez de Lacalle, and Aitor Soroa.
2006b. Two graph-based algorithms
for state-of-the-art WSD. In Proceedings
of the 2006 Conference on Empirical
Methods in Natural Language Processing,
pages 585?593, Sydney.
Agirre, Eneko and Aitor Soroa. 2007.
UBC-AS: A graph based unsupervised
system for induction and classification.
In Proceedings of the 4th International
Workshop on Semantic Evaluations,
pages 346?349, Prague.
Agrawal, Rakesh, Sreenivas Gollapudi,
Alan Halverson, and Samuel Ieong. 2009.
Diversifying search results. In Proceedings
of the 2nd International Conference on Web
Search and Web Data Mining, pages 5?14,
Barcelona.
Baroni, Marco and Alessandro Lenci.
2010. Distributional memory: A
general framework for corpus-based
semantics. Computational Linguistics,
36(4):673?721.
Basile, Pierpaolo, Annalina Caputo, and
Giovanni Semeraro. 2009. Exploiting
disambiguation and discrimination
in Information Retrieval systems.
In Proceedings of the 2009 IEEE/WIC/
ACM International Joint Conference on
Web Intelligence and Intelligent Agent
Technology - Volume 03, pages 539?542,
Washington, DC.
Bennett, Paul N. and Nam Nguyen. 2009.
Refined experts: Improving classification
in large taxonomies. In Proceedings of the
32nd Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 11?18,
Boston, MA.
Bernardini, Andrea, Claudio Carpineto,
and Massimiliano D?Amico. 2009.
Full-subtopic retrieval with
keyphrase-based search results clustering.
In Proceedings of 2009 IEEE/WIC/ACM
International Conference on Web Intelligence,
pages 206?213, Milan.
Biemann, Chris. 2006. Chinese whispers?an
efficient graph clustering algorithm
and its application to Natural Language
Processing problems. In Proceedings of the
1st Workshop on Graph-Based Algorithms for
Natural Language Processing, pages 73?80,
New York.
Boleda, Gemma, Sabine Schulte im Walde,
and Toni Badia. 2012. Modeling regular
polysemy: A study on the semantic
classification of Catalan adjectives.
Computational Linguistics, 38(3):575?616.
Branson, S. and A. Greenberg. 2002.
Clustering Web search results using suffix
749
Computational Linguistics Volume 39, Number 3
tree methods. In Technical Report CS276A
Final Project, Stanford University.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram, ver. 1, ldc2006t13. In Linguistic
Data Consortium, Philadelphia, PA.
Brody, Samuel and Mirella Lapata. 2009.
Bayesian Word Sense Induction. In
Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 103?111,
Athens.
Bruza, Peter, Robert McArthur, and Simon
Dennis. 2000. Interactive Internet
search: Keyword, directory and query
reformulation mechanisms compared. In
Proceedings of the 23rd Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 280?287, Athens.
Bunescu, Razvan C. and Marius Pasca. 2006.
Using encyclopedic knowledge for named
entity disambiguation. In Proceedings of
11th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 9?16, Trento.
Carbonell, Jaime and Jade Goldstein.
1998. The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Proceedings of
the 21st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 335?336,
Melbourne.
Carmel, David, Haggai Roitman, and
Naama Zwerdling. 2009. Enhancing
cluster labeling using Wikipedia. In
Proceedings of the 32nd Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 139?146, Boston, MA.
Carpineto, Claudio, Massimiliano D?Amico,
and Andrea Bernardini. 2011. Full
discrimination of subtopics in search
results with keyphrase-based clustering.
Web Intelligence and Agent Systems,
9(4):337?349.
Carpineto, Claudio, Stanislaw Osin?ski,
Giovanni Romano, and Dawid Weiss.
2009. A survey of Web clustering engines.
ACM Computing Surveys, 41(3):1?38.
Carpineto, Claudio and Giovanni Romano.
2004. Exploiting the potential of concept
lattices for Information Retrieval with
CREDO. Journal of Universal Computer
Science, 10(8):985?1013.
Chandar, Praveen and Ben Carterette. 2010.
Diversification of search results using
webgraphs. In Proceedings of the 33rd
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 869?870, Geneva.
Chapelle, Olivier, Yi Chang, and Tie-Yan Liu.
2011. Future directions in learning
to rank. Journal of Machine Learning
Research?Proceedings Track, 14:91?100.
Chen, Harr and David R. Karger. 2006. Less
is more: Probabilistic models for retrieving
fewer relevant documents. In Proceedings of
the 29th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 429?436,
Seattle, WA.
Chen, Jiyang, Osmar R. Za??ane, and Randy
Goebel. 2008. An unsupervised approach
to cluster web search results based on
word sense communities. In Proceedings
of 2008 IEEE/WIC/ACM International
Conference on Web Intelligence,
pages 725?729, Sydney.
Cheng, David, Santosh Vempala, Ravi
Kannan, and Grant Wang. 2005. A
divide-and-merge methodology for
clustering. In Proceedings of the 24th
ACM SIGACT-SIGMOD-SIGART
Symposium on Principles of Database
Systems, pages 196?205, Baltimore, MD.
Clarke, Daoud. 2012. A context-theoretic
framework for compositionality in
distributional semantics. Computational
Linguistics, 38(1):41?71.
Clough, Paul, Mark Sanderson, Murad
Abouammoh, Sergio Navarro, and
Monica Lestari Paramita. 2009. Multiple
approaches to analysing query diversity. In
Proceedings of the 32nd Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 734?735, Boston, MA.
Crabtree, Daniel, Xiaoying Gao, and Peter
Andreae. 2005. Improving Web clustering
by cluster selection. In Proceedings of 2005
IEEE/WIC/ACM International Conference on
Web Intelligence, pages 172?178,
Compiegne.
Cutting, Douglass R., David R. Karger,
Jan O. Pedersen, and John W. Tukey. 1992.
Scatter/Gather: A cluster-based approach
to browsing large document collections. In
Proceedings of the 15th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 318?329, Copenhagen.
Di Giacomo, Emilio, Walter Didimo,
Luca Grilli, and Giuseppe Liotta. 2007.
Graph visualization techniques for Web
clustering engines. IEEE Transactions on
Visualization and Computer Graphics,
13(2):294?304.
750
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Di Marco, Antonio and Roberto Navigli.
2011. Clustering Web search results with
maximum spanning trees. In Proceedings of
the XIIth International Conference of the
Italian Association for Artificial Intelligence,
pages 201?212, Palermo.
Dorow, Beate, Dominic Widdows, Katarina
Ling, Jean-Pierre Eckmann, Danilo Sergi,
and Elisha Moses. 2005. Using curvature
and Markov clustering in graphs for
lexical acquisition and word sense
discrimination. In Proceedings of the
Meaning-2005 Workshop, Trento.
Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.
2010. A flexible, corpus-driven model of
regular and inverse selectional preferences.
Computational Linguistics, 36(4):723?763.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Database. MIT Press,
Cambridge, MA.
Ferraresi, Adriano, Eros Zanchetta, Marco
Baroni, and Silvia Bernardini. 2008.
Introducing and evaluating ukWaC,
a very large Web-derived corpus of
English. In Proceedings of the 4th Web as
Corpus Workshop (WAC-4), pages 47?54,
Marrakech.
Furnas, G. W., T. K. Landauer, L. M.
Gomez, and S. T. Dumais. 1987. The
vocabulary problem in human-system
communication. Commununications of
ACM, 30(11):964?971.
Gabrilovich, Evgeniy and Shaul Markovitch.
2009. Wikipedia-based semantic
interpretation for natural language
processing. Journal of Artificial Intelligence
Research (JAIR), 34:443?498.
Geiss, Johanna. 2009. Creating a gold
standard for sentence clustering in
multi-document summarization. In
Proceedings of the 47th Annual Meeting
of the Association for Computational
Linguistics and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 96?104, Singapore.
Gelgi, Fatih, Hasan Davulcu, and Srinivas
Vadrevu. 2007. Term ranking for clustering
Web search results. In Proceedings of
10th International Workshop on the Web
and Databases, Beijing, China.
Gonzalo, Julio, Anselmo Penas, and Felisa
Verdejo. 1999. Lexical ambiguity and
Information Retrieval revisited. In
Proceedings of the Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora,
pages 195?202, College Park, MD.
Harris, Zellig. 1954. Distributional structure.
Word, 10:146?162.
Hubert, L. and P. Arabie. 1985. Comparing
partitions. Journal of Classification,
2(1):193?218.
Kamvar, Maryam and Shumeet Baluja.
2006. A large scale study of wireless
search behavior: Google mobile search.
In Proceedings of the 2006 Conference on
Human Factors in Computing Systems,
pages 701?709, Montre?al.
Ke, Weimao, Cassidy R. Sugimoto, and
Javed Mostafa. 2009. Dynamicity vs.
effectiveness: Studying online clustering
for Scatter/Gather. In Proceedings of the
32nd Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 19?26,
Boston, MA.
Kim, Sang-Bum, Hee-Cheol Seo, and
Hae-Chang Rim. 2004. Information
Retrieval using word senses: Root sense
tagging approach. In Proceedings of the
27th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 258?265,
Sheffield.
Korkontzelos, Ioannis and Suresh
Manandhar. 2010. UoY: Graphs of
unambiguous vertices for word sense
induction and disambiguation.
In Proceedings of the 5th International
Workshop on Semantic Evaluation,
pages 355?358, Uppsala.
Krovetz, Robert and William B. Croft.
1992. Lexical ambiguity and Information
Retrieval. ACM Transactions on Information
Systems, 10(2):115?141.
Kurland, Oren. 2008. The opposite of
smoothing: A language model approach
to ranking query-specific document
clusters. In Proceedings of the 31st Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 171?178, Singapore.
Kurland, Oren and Carmel Domshlak. 2008.
A rank-aggregation approach to searching
for optimal query-specific clusters. In
Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 547?554, Singapore.
Lee, Kyung Soon, W. Bruce Croft, and James
Allan. 2008. A cluster-based resampling
method for pseudo-relevance feedback. In
Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 235?242, Singapore.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
751
Computational Linguistics Volume 39, Number 3
of the 17th International Conference on
Computational Linguistics, pages 768?774,
Montreal.
Lin, Dekang and Patrick Pantel. 2002.
Discovering word senses from text.
In Proceedings of the 8th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 613?619,
Edmonton.
Liu, Shuang, Clement Yu, and Weiyi Meng.
2005. Word Sense Disambiguation
in queries. In Proceedings of the 2005
ACM CIKM International Conference on
Information and Knowledge Management,
pages 525?532, Bremen.
Liu, Tie-Yan, Yiming Yang, Hao Wan,
Hua-Jun Zeng, Zheng Chen, and
Wei-Ying Ma. 2005. Support vector
machines classification with a very
large-scale taxonomy. SIGKDD
Explorations, 7(1):36?43.
Liu, Ying, Wenyuan Li, Yongjing Lin, and
Liping Jing. 2008. Spectral geometry for
simultaneously clustering and ranking
query search results. In Proceedings of the
31st Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 539?546,
Singapore.
Ma, Hao, Michael R. Lyu, and Irwin King.
2010. Diversifying query suggestion
results. In Proceedings of the 24th AAAI
Conference on Artificial Intelligence, AAAI
2010, pages 1,399?1,404, Atlanta, GA.
Maarek, Yoelle, Ron Fagin, Israel Ben-Shaul,
and Dan Pelleg. 2000. Ephemeral
document clustering for Web applications.
IBM Research Report RJ 10186. Haifa.
Manandhar, Suresh, Ioannis P. Klapaftis,
Dmitriy Dligach, and Sameer S. Pradhan.
2010. SemEval-2010 task 14: Word sense
induction & disambiguation. In Proceedings
of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala.
Mandala, Rila, Takenobu Tokunaga, and
Hozumi Tanaka. 1998. The use of WordNet
in Information Retrieval. In Proceedings of
the COLING-ACL workshop on Usage of
Wordnet in Natural Language Processing,
pages 31?37, Montre?al.
Matsuo, Yutaka, Takeshi Sakaki, Ko?ki
Uchiyama, and Mitsuru Ishizuka. 2006.
Graph-based word clustering using a Web
search engine. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 542?550,
Sydney.
Mihalcea, Rada. 2007. Using Wikipedia for
automatic Word Sense Disambiguation.
In Human Language Technology Conference of
the North American Chapter of the Association
of Computational Linguistics, pages 196?203,
Rochester, NY.
Mihalkova, L. and R. Mooney. 2009. Learning
to disambiguate search queries from short
sessions. In Proceedings of Machine Learning
and Knowledge Discovery in Databases (2),
pages 111?127, Bled.
Miller, George A., R. T. Beckwith,
Christiane D. Fellbaum, D. Gross, and
K. Miller. 1990. WordNet: an online
lexical database. International Journal of
Lexicography, 3(4):235?244.
Mitchell, Jeff and Mirella Lapata. 2010.
Composition in distributional models
of semantics. Cognitive Science,
34(8):1388?1429.
Navigli, Roberto. 2009. Word Sense
Disambiguation: A survey. ACM
Computing Surveys, 41(2):1?69.
Navigli, Roberto. 2012. A quick tour of Word
Sense Disambiguation, induction and
related approaches. In Proceedings of the
38th Conference on Current Trends in
Theory and Practice of Computer Science,
pages 115?129, Spindleruv Mly?n.
Navigli, Roberto and Giuseppe Crisafulli.
2010. Inducing word senses to improve
Web search result clustering. In Proceedings
of the 2010 Conference on Empirical
Methods in Natural Language Processing,
pages 116?126, Boston, MA.
Navigli, Roberto and Simone Paolo Ponzetto.
2012. The automatic construction,
evaluation and application of a
wide-coverage multilingual semantic
network. Artificial Intelligence,
193:217?250.
Ngo, Chi Lang and Hung Son Nguyen.
2005. A method of Web search result
clustering based on rough sets. In
Proceedings of 2005 IEEE/WIC/ACM
International Conference on Web Intelligence,
pages 673?679, Compiegne.
Nguyen, Cam-Tu, Xuan-Hieu Phan,
Susumu Horiguchi, Thu-Trang Nguyen,
and Quang-Thuy Ha. 2009. Web search
clustering and labeling with hidden topics.
ACM Transactions on Asian Language
Information Processing, 8(3):1?40.
Osinski, Stanislaw and Dawid Weiss. 2005.
A concept-driven algorithm for clustering
search results. IEEE Intelligent Systems,
20(3):48?54.
Rand, W. M. 1971. Objective criteria for the
evaluation of clustering methods. Journal
of the American Statistical Association,
66(336):846?850.
752
Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI
Reisinger, Joseph and Marius Pasca. 2011.
Fine-grained class label markup of search
queries. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 1,200?1,209, Portland, OR.
Rosenberg, Andrew and Julia Hirschberg.
2007. V-measure: A conditional
entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 410?420,
Prague.
Sanderson, Mark. 1994. Word Sense
Disambiguation and Information
Retrieval. In Proceedings of the 17th Annual
International ACM-SIGIR Conference on
Research and Development in Information
Retrieval, pages 142?151, Dublin.
Sanderson, Mark. 2000. Retrieving with good
sense. Information Retrieval, 2(1):49?69.
Sanderson, Mark. 2008. Ambiguous queries:
Test collections need more sense. In
Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 499?506, Singapore.
Santamar??a, Celina, Julio Gonzalo, and
Javier Artiles. 2010. Wikipedia as sense
inventory to improve diversity in Web
search results. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, ACL 2010,
pages 1,357?1,366, Uppsala.
Schu?tze, Hinrich. 1992. Dimensions of
meaning. In Proceedings of the 1992
ACM/IEEE Conference on Supercomputing,
pages 787?796, Los Alamitos, CA.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
Schu?tze, Hinrich and Jan Pedersen. 1995.
Information Retrieval based on word
senses. In Proceedings of SDAIR?95,
pages 161?175, Las Vegas, NV.
Silberer, Carina and Mirella Lapata.
2012. Grounded models of semantic
representation. In Proceedings of
the 2012 Joint Conference on Empirical
Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 1,423?1,433,
Jeju Island.
Smadja, Frank, Kathleen R. McKeown,
and Vasileios Hatzivassiloglou. 1996.
Translating collocations for bilingual
lexicons: A statistical approach.
Computational Linguistics, 22(1):1?38.
Song, Ruihua, Zhenxiao Luo, Jian-Yun Nie,
Yong Yu, and Hsiao-Wuen Hon. 2009.
Identification of ambiguous queries in
Web search. Information Processing and
Management, 45:216?229.
Steinley, Doug. 2004. Properties of the
Hubert-Arabie adjusted Rand index.
Psychological Methods, 9(3):386?396.
Stokoe, Christopher, Michael J. Oakes,
and John I. Tait. 2003. Word Sense
Disambiguation in Information Retrieval
revisited. In Proceedings of the 26th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 159?166, Toronto.
Swaminathan, Ashwin, Cherian V. Mathew,
and Darko Kirovski. 2009. Essential pages.
In Proceedings of 2009 IEEE/WIC/ACM
International Conference on Web Intelligence,
pages 173?182, Milan.
Udani, Goldee, Shachi Dave, Anthony
Davis, and Tim Sibley. 2005. Noun sense
induction using Web search results. In
Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 657?658, Salvador.
Van de Cruys, Tim and Marianna
Apidianaki. 2011. Latent semantic word
sense induction and disambiguation.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 1,476?1,485, Portland, OR.
van Rijsbergen, C. J. 1979. Information
Retrieval, second edition. Butterworths,
London.
Ve?ronis, Jean. 2004. HyperLex: Lexical
cartography for information retrieval.
Computer, Speech and Language,
18(3):223?252.
von Luxburg, Ulrike, Robert C. Williamson,
and Isabelle Guyon. 2012. Clustering:
Science or art? Journal of Machine
Learning Research?Proceedings Track,
27:65?80.
Voorhees, Ellen M. 1993. Using WordNet
to disambiguate word senses for text
retrieval. In Proceedings of the 16th Annual
International ACM-SIGIR Conference on
Research and Development in Information
Retrieval, pages 171?180, Pittsburgh, PA.
Wang, Xuanhui, Deepayan Chakrabarti,
and Kunal Punera. 2009. Mining broad
latent query aspects from search sessions.
In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 867?876,
Paris.
753
Computational Linguistics Volume 39, Number 3
Wang, Xuanhui and ChengXiang Zhai.
2007. Learn from Web search logs to
organize search results. In Proceedings of
the 30th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 87?94,
Amsterdam.
Widdows, Dominic and Beate Dorow. 2002.
A graph model for unsupervised lexical
acquisition. In Proceedings of the 19th
International Conference on Computational
Linguistics, pages 1?7, Taipei.
Wu, Fei, Jayant Madhavan, and Alon Y.
Halevy. 2011. Identifying aspects for
Web-search queries. Journal of Artificial
Intelligence Research (JAIR), 40:677?700.
Xue, Gui-Rong, Dikan Xing, Qiang Yang,
and Yong Yu. 2008. Deep classification in
large-scale text hierarchies. In Proceedings
of the 31st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 619?626,
Singapore.
Xue, Xiaobing and Xiaoxin Yin. 2011. Topic
modeling for named entity queries. In
Proceedings of the 20th ACM International
Conference on Information and Knowledge
Management, pages 2009?2012, New York.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the ARPA
Workshop on Human Language Technology,
pages 266?271, Princeton, NJ.
Zamir, Oren and Oren Etzioni. 1998. Web
document clustering: A feasibility
demonstration. In Proceedings of the 21st
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 46?54, Melbourne.
Zamir, Oren, Oren Etzioni, Omid Madani,
and Richard M. Karp. 1997. Fast and
intuitive clustering of Web documents.
In Proceedings of the Third International
Conference on Knowledge Discovery and
Data Mining, pages 287?290, Newport
Beach, CA.
Zeng, Hua-Jun, Qi-Cai He, Zheng Chen,
Wei-Ying Ma, and Jinwen Ma. 2004.
Learning to cluster Web search results. In
Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 210?217, Sheffield.
Zhai, ChengXiang, William W. Cohen,
and John Lafferty. 2003. Beyond
independent relevance: Methods and
evaluation metrics for subtopic retrieval.
In Proceedings of the 26th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 10?17, Toronto.
Zhang, Benyu, Hua Li, Yi Liu, Lei Ji,
Wensi Xi, Weiguo Fan, Zheng Chen,
and Wei-Ying Ma. 2005. Improving Web
search results using affinity graph. In
Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
SIGIR 2005, pages 504?511, Salvador.
Zhang, Xiaodan, Xiaohua Hu, and Xiaohua
Zhou. 2008. A comparative evaluation
of different link types on enhancing
document clustering. In Proceedings of the
31st Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 555?562,
Singapore.
Zhao, Ying and George Karypis. 2004.
Empirical and theoretical comparisons of
selected criterion functions for document
clustering. Machine Learning, 55(3):311?331.
754
Proceedings of NAACL-HLT 2013, pages 1100?1109,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Paving the Way to a Large-scale Pseudosense-annotated Dataset
Mohammad Taher Pilehvar and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{pilehvar,navigli}@di.uniroma1.it
Abstract
In this paper we propose a new approach to
the generation of pseudowords, i.e., artificial
words which model real polysemous words.
Our approach simultaneously addresses the
two important issues that hamper the gener-
ation of large pseudosense-annotated datasets:
semantic awareness and coverage. We eval-
uate these pseudowords from three different
perspectives showing that they can be used as
reliable substitutes for their real counterparts.
1 Introduction
A fundamental problem in computational linguis-
tics is the paucity of manually annotated data, such
as part-of-speech tagged sentences, treebanks, and
logical forms, which exist only for few languages
(Ide et al, 2010). A case in point is the lack of
abundant sense annotated data, which hampers the
performance and coverage of lexical semantic tasks
such as Word Sense Disambiguation (Navigli, 2009;
Navigli, 2012, WSD) and semantic role labeling
(Gildea and Jurafsky, 2002). A possible way to
break this bottleneck is to use pseudowords, i.e., arti-
ficial words constructed by conflating a set of unam-
biguous words, with the aim of modeling polysemy
in real ambiguous words. The idea of pseudowords
was originally proposed by Gale et al (1992) and
Schu?tze (1992) for WSD evaluation, but later found
application in other tasks such as selectional prefer-
ences (Erk, 2007; Bergsma et al, 2008; Chambers
and Jurafsky, 2010), Word Sense Induction (Bor-
dag, 2006; Di Marco and Navigli, 2013) or studies
concerning the effects of the amount of data on ma-
chine learning for natural language disambiguation
(Banko and Brill, 2001). Being made up of monose-
mous words, pseudowords can potentially be used to
create large amounts of pseudosense-annotated data
at virtually no cost, hence enabling large-scale stud-
ies in lexical semantics. Unfortunately, though, the
extent of their usability for such a purpose is ham-
pered by two main issues: semantic awareness and
wide coverage.
Semantic awareness corresponds to the constraint
that pseudowords, in order to be realistic, are ex-
pected to have senses which are in a semantic rela-
tionship (thus modeling systematic polysemy). Re-
cent work has focused on this issue and, by exploit-
ing either specific lexical hierarchies (Nakov and
Hearst, 2003; Lu et al, 2006), or the WordNet struc-
ture (Otrusina and Smrz, 2010), have succeeded in
generating pseudowords which are comparable to
real words in terms of disambiguation difficulty. The
second challenge is coverage, which corresponds to
the number of distinct pseudowords an algorithm
can generate. When coupled with the semantic
awareness issue, wide coverage is hampered by the
difficulty in generating thousands of pseudowords
which mimic existing polysemous words.
Unfortunately, none of the existing approaches to
the generation of pseudowords can meet both these
challenges simultaneously, and this has hindered
the generation of a large pseudosense-annotated
dataset. For instance, approaches which exploit the
monosemous neighbors of a target sense in Word-
Net (Otrusina and Smrz, 2010) can be used to gener-
ate pseudowords with good semantic awareness, but
1100
they have low coverage of ambiguous nouns when
many pseudosense-tagged sentences are needed (cf.
Section 2.1.1).
In this paper we propose a new approach, based
on Personalized PageRank, which simultaneously
addresses the two above-mentioned issues concern-
ing the generation of pseudowords (i.e., seman-
tic awareness and coverage), and hence enables
the generation of large-scale pseudosense-annotated
datasets. We perform three different experiments to
show that our pseudowords are good at modeling ex-
isting ambiguous words in terms of disambiguation
difficulty, representativeness of real senses and dis-
tinguishability of the artificial senses. As a byprod-
uct of this work, we generate a large dataset that pro-
vides 1000 tagged sentences for each of the 15,935
pseudowords modeled after real ambiguous nouns in
WordNet 3.0.
2 Pseudowords
A pseudoword p = w1*w2*. . . *wn is an artificially-
generated ambiguous word of polysemy degree n
which is usually created by conflating n unique un-
ambiguous words wi called pseudosenses. For in-
stance, airplane*river is a pseudoword with two
meanings explicitly identified by its pseudosenses:
airplane and river. Pseudowords are particularly in-
teresting as they can be used to introduce controlled
artificial ambiguity into a corpus. Given a pseu-
doword p and an untagged corpus C, this artificial
tagging is achieved by substituting all occurrences of
wi in C with p for each pseudosense i ? {1, . . . , n}.
As a result, each occurrence of the pseudoword p is
tagged with the underlying sense wi. As an example,
consider the following two sentences:
a1. The Wright brothers invented the airplane.
a2. The Nile is the longest river in the world.
If we replace the individual occurrences of air-
plane and river with the pseudoword airplane*river
while noting the replaced term as the corresponding
sense, we obtain the following pseudosense-tagged
sentences:
b1. The Wright brothers invented the airplane*river.
b2. The Nile is the longest airplane*river in the world.
As a result of this procedure, we obtain a corpus
of sentences containing the occurrences of an arti-
ficially ambiguous word p, for each of which we
know its correct sense annotation wi. Virtually any
number of pseudowords can be created, resulting in
a large pseudosense-annotated corpus. An obvious
restriction on the choice of pseudosenses is that they
need to be unambiguous, so as to avoid the introduc-
tion of uncontrolled ambiguity. Another constraint
is that the constituent wi must satisfy a minimum oc-
currence frequency in the corpus C. This minimum
frequency corresponds to the number of annotated
sentences that are requested for the task of interest
which will exploit the resulting annotated corpus.
An immediate way of generating a pseudoword
would be to randomly select its constituents from
the set of all monosemous words given by a lexi-
con (e.g., WordNet). However, constructing a pseu-
doword by merely combining a random set of unam-
biguous words selected on the basis of their falling
in the same range of occurrence frequency (Schu?tze,
1992), or leveraging homophones and OCR ambi-
guities (Yarowsky, 1993), does not provide a suit-
able model of a real polysemous word (Gaustad,
2001; Nakov and Hearst, 2003). This is because
in the real world different senses, unless they are
homonymous, share some semantic or pragmatic re-
lation. Therefore, random pseudowords will typ-
ically model only homonymous distinctions (such
as the centimeter vs. curium senses of cm), while
they will fall short of modeling systematic polysemy
(such as the lack vs. insufficiency senses of defi-
ciency).
2.1 Semantically-aware Pseudowords
In order to cope with the above-mentioned lim-
its of random pseudowords, an artificial word has
to model an existing word by providing a one-to-
one correspondence between each pseudosense and
a corresponding sense of the modeled word. For
instance, the pseudoword lack*shortfall is a good
model of the real word deficiency in that its pseu-
dosenses preserve the meanings of their correspond-
ing real word?s senses. We call this kind of artificial
words semantically-aware pseudowords.
In the next two subsections, we will describe two
techniques (the second of which is presented for
the first time in this paper) for the generation of
1101
Minimum Polysemy
Overall
Frequency 2 3 4 5 6 7 8 9 10 11 12 >12
0 87 82 74 71 67 70 60 64 45 46 44 28 83
500 41 31 24 15 12 13 10 7 7 0 0 0 35
1000 31 20 16 7 4 6 4 3 0 0 0 0 25
Table 1: Ambiguous noun coverage percentage of vicinity-based pseudowords by degree of polysemy for different
values of minimum pseudosense occurrence frequency in Gigaword.
semantically-aware pseudowords. In what follows
we focus on nominal pseudowords, and leave the ex-
tension to other parts of speech to future work.
2.1.1 Vicinity-based Pseudowords
A computational lexicon such as WordNet (Fell-
baum, 1998) can be used as the basis for the
automatic generation of semantically-aware pseu-
dowords, an idea which was first proposed by
Otrusina and Smrz (2010). WordNet can be viewed
as a graph in which synsets act as nodes and the lexi-
cal and semantic relationships among them as edges.
Given a sense, the approach looks into its surround-
ing synsets in the WordNet graph in order to find
a related monosemous term that can represent that
sense. As search space, the approach considers: the
other literals in the same synset, the genus phrase
from its textual definition, direct siblings, and di-
rect hyponyms. If no monosemous candidate can be
found, this space is further extended to hypernyms
and meronyms. Hereafter, we term this approach as
vicinity-based.
For example, consider the generation process of
the vicinity-based pseudoword corresponding to the
term coke, which has three senses in WordNet 3.0.
There exist multiple monosemous candidates for
each sense: dozens of candidates (such as biomass
and butane) in the direct siblings? vicinity of the
first sense, coca cola, pepsi, and pepsi cola for the
second sense, and nose candy and coca cola for
the third sense. Among these candidates Otrusina
and Smrz (2010) select those whose occurrence fre-
quency ratio in a given text corpus is most similar to
that of the senses of the corresponding real word as
given by a sense-annotated corpus. Clearly, a suffi-
ciently large sense-tagged corpus is required for cal-
culating the occurrence frequency of the individual
senses of a word. This is a limitation of the vicinity-
based approach.
In addition, as we mentioned earlier, we need
pseudowords that can enable the generation of large-
scale pseudosense-tagged corpora. For this to be
achieved, each pseudosense is required to occur with
a relatively high frequency in a given text corpus.
The vicinity-based approach can, however, identify
at best only a few representatives for each pseu-
dosense, thus undermining its ability to cover many
ambiguous nouns. Table 1 shows the percentage
of ambiguous nouns in WordNet that can be mod-
eled using the vicinity-based approach when differ-
ent minimum numbers of annotated sentences are
requested, i.e. each pseudosense is required to oc-
cur in at least 0 (i.e., no minimum frequency restric-
tion), 500, or 1000 unique sentences in the reference
corpus (we use Gigaword (Graff and Cieri, 2003)
in our experiments). In the Table, beside the over-
all coverage percentage, we present the coverage by
degree of polysemy and for three different values of
minimum pseudosense occurrence frequency. Even
though the overall coverage is over 80% when no re-
striction on minimum frequency is considered (first
row in the Table), this high coverage drops rapidly
when we request some hundred sentences per sense.
For instance, only 25% of the ambiguous nouns in
WordNet can be modeled using this approach when
a minimum frequency of 1000 noun occurrences is
required (last row of Table 1), with most of the cov-
ered words having low polysemy (in fact about 93%
of them are either 2- or 3-sense nouns). This se-
vere limitation of the vicinity-based approach hin-
ders a wide-coverage modeling of ambiguous nouns
in WordNet, thus preventing it from being an op-
tion for the generation of a large-scale pseudosense-
annotated dataset.
With a view to addressing the above-mentioned
issues and to enable wide coverage, in the next sub-
section we propose a flexible approach for the gen-
eration of semantically-aware pseudowords.
1102
2.1.2 Similarity-based Pseudowords
The vicinity-based pseudoword generation ap-
proach works on local subgraphs of WordNet, con-
sidering mostly all those candidates which are in a
direct relationship with a real sense si, and treating
them as potentially good representatives of si. We
propose an extension to this approach which exploits
the WordNet semantic network in its entirety, hence
enabling us to determine a graded degree of similar-
ity between si and all the senses of all other words
in WordNet.
We chose a graph-based similarity measure for
two reasons: firstly, it comes as a natural exten-
sion of the vicinity-based method, and, secondly, al-
ternative context-based methods such as Lin?s mea-
sure (Lin, 1998) have been shown to require a wide-
coverage sense-tagged dataset in order to calculate
similarities on a sense-by-sense basis for all words in
the lexicon (Otrusina and Smrz, 2010). As our sim-
ilarity measure we selected the Personalized PageR-
ank (Haveliwala, 2002, PPR) algorithm. PPR basi-
cally computes the probability according to which a
random walker at a specific node in a graph would
visit an arbitrary node in the same graph. The al-
gorithm estimates, for a specific node in a graph,
a probability distribution (called PPR vector) which
determines the importance of any given node in the
graph for that specific node. When applied to a
semantic graph, this importance can be interpreted
as semantic similarity. PPR has previously been
used as a core component for semantic similarity1
(Hughes and Ramage, 2007; Agirre et al, 2009)
and Word Sense Disambiguation (Agirre and Soroa,
2009).
Algorithm 1 shows the procedure for the genera-
tion of our similarity-based pseudowords. The algo-
rithm takes an ambiguous word w as input, and out-
puts its corresponding similarity-based pseudoword
Pw whose ith pseudosense models the ith sense of
w, together with a confidence score which we detail
below.
Given w, the algorithm iterates over the synsets
corresponding to its individual senses (lines 4-13)
and finds the most suitable pseudosenses for Pw. For
1Top-ranking synsets will contain words which are most
likely similar to the target sense, whereas we move to a graded
notion of relatedness as far as lower-ranking ones are concerned
(Agirre et al, 2009).
Algorithm 1 Generate a similarity-based pseudoword
Input: an ambiguous word w in WordNet
Output: a ?similarity-based? pseudoword Pw
a confidence score averageRank
1: Pw ? ?
2: totalRank? 0
3: i? 1
4: for each s ? Synsets(w)
5: similarSynsets? PersonalizedPageRank(s)
6: sort similarSynsets in descending order
7: for each s? ? similarSynsets
8: totalRank? totalRank + 1
9: for each w? ? SynsetLiterals(s?)
10: if |Synsets(w?)|=1 & Freq(w?)>minFreq then
11: Pw ? Pw ? {(i, w?)}
12: break
13: i? i + 1
14: averageRank? totalRank/|Synsets(w)|
15: return (Pw, averageRank)
each synset s of w, we start the PPR algorithm from
s (line 5) and collect the probability distribution vec-
tor output by PPR (similarSynsets in the algorithm),
which determines the probability of reaching each
synset in WordNet starting from s. We then sort
this vector (line 6) and check if each of its nomi-
nal synsets (s?) contains a monosemous word (line
10). This search continues until a suitable candi-
date is found that satisfies a certain minimum oc-
currence frequency minFreq. When this occurs, the
selected monosemous candidate w? is saved as the
corresponding pseudosense for the ith sense of Pw
(line 11). We iterate these steps for all synsets of w.
In line 14 we calculate the averageRank, a value
given by the average of synset positions in the simi-
larSynsets lists from which the pseudosenses of Pw
are picked out. We later use this value as a confi-
dence score while evaluating our pseudowords. Fi-
nally, the algorithm returns the corresponding pseu-
doword Pw along with its averageRank score (line
15). We show in Table 2 some examples of ambigu-
ous words together with their similarity-based pseu-
dowords.
Thanks to the large search space of our similarity-
based approach, we are always able to select a
monosemous candidate for each pseudosense, thus
resolving the coverage issue regarding vicinity-
based pseudowords. A question that arises here is
that of how often our algorithm needs to resort to
lower-ranking items in the similarSynsets list. To
1103
Word Similarity-based Pseudoword
bernoulli physicist*mathematician*astronomer
coach football coach*tutor*passenger car*clarence*
public transport
green greenery*common*labor leader*
green party*river*golf course*greens*max
horoscope forecast*diagram
sunray sunbeam*vine*sunlight
lifter athlete*thief
Table 2: Similarity-based pseudowords generated for
six different nouns in WordNet 3.0 (with minimum fre-
quency of 1000 occurrences in Gigaword). Pseudosenses
which could not be modeled using the vicinity-based ap-
proach are shown in bold.
verify this, we analyzed the averageRank values out-
put by Algorithm 1. Table 3 shows for each poly-
semy degree and for three different values of min-
Freq, the mean and mode statistics of the averageR-
ank scores of the generated similarity-based pseu-
dowords for all the 15,935 polysemous nouns in
WordNet 3.0. As expected, the higher the number of
required sentences per pseudosense (minFreq), the
further the algorithm descends through the list simi-
larSynsets to select a pseudosense. However, as can
be seen from the mode statistics in the Table, even
when minFreq is set to a large value, most of the
pseudosenses are picked from the highest-ranking
positions in the similarSynsets list.
3 Evaluation
Our novel similarity-based algorithm for the gen-
eration of pseudowords inherently tackles the cov-
erage issue. To test whether our generated pseu-
dowords also cope with the issue of semantic aware-
ness we carried out three separate evaluations so
as to assess their strength in modeling semantic
properties of their corresponding real senses from
different perspectives. These will be described in
the next three subsections. Since our aim was to
leverage pseudowords for the creation of a large-
scale pseudosense-annotated dataset, we performed
evaluations on pseudowords generated with minFreq
per pseudosense set to 1000 (i.e., we can gener-
ate at least 1000 annotated sentences for each pseu-
dosense).
minFreq 0 500 1000
poly. mean mode mean mode mean mode
2 2.0 1.0 14.8 2.0 25.4 4.0
3 2.3 1.7 13.4 2.7 21.0 5.5
4 2.3 1.8 12.3 5.8 19.8 6.8
5 2.3 1.8 12.9 5.6 20.0 10.0
6 2.4 2.0 13.7 4.5 18.7 8.8
7 2.3 2.1 11.5 6.3 16.0 6.1
8 2.2 1.8 11.3 9.6 17.2 10.8
9 2.4 2.0 10.7 10.9 15.6 15.1
10 2.2 2.0 10.1 7.0 14.3 12.1
11 2.4 2.1 10.2 7.1 14.2 17.3
12 2.5 2.4 11.0 4.4 14.4 14.4
>12 2.6 1.0 9.3 2.0 13.7 4.0
overall 2.1 1.0 14.1 2.0 23.4 4.0
Table 3: Statistics of averageRank scores for the full set
of 15,935 similarity-based pseudowords modeled after
ambiguous nouns in WordNet 3.0: we show mean and
mode statistics for three different values of minimum oc-
currence frequency (0, 500, and 1000). We show the av-
erage value in the case of multiple modes.
3.1 Disambiguation Difficulty of Pseudowords
Our first experiment is an extrinsic evaluation of
pseudowords. Ideally, pseudowords are expected to
show a similar degree of difficulty to real ambigu-
ous words in a disambiguation task (Otrusina and
Smrz, 2010; Lu et al, 2006). We thus experimen-
tally tested this assumption on similarity-based and
random pseudowords. Given its low coverage, we
excluded the vicinity-based approach from this ex-
periment.
Starting from a sense-tagged lexical sample
dataset for a set of ambiguous nouns, for each such
noun and for each kind of pseudoword, we automat-
ically generated a pseudosense-annotated dataset by
enforcing the same sense distribution as the cor-
responding real ambiguous noun. This constraint
was particularly important for random pseudowords
since they do not model the corresponding real am-
biguous words (see Section 2). An analysis was then
performed to compare the disambiguation perfor-
mance of a supervised WSD system on a given am-
biguous word against its corresponding pseudoword.
Specifically, for our manually sense-tagged cor-
pus we used the Senseval-3 English lexical sample
dataset (Mihalcea et al, 2004), which contains 3593
and 1807 sense-tagged sentences for 20 ambiguous
nouns (with an average polysemy degree of 5.8) in
its training and test sets, respectively. We generated,
1104
with minFreq = 1000, the similarity-based pseu-
dowords corresponding to these 20 nouns, as well
as a set of 20 random pseudowords with the same
polysemy degrees. We note that, in this setting, the
vicinity-based approach could only generate pseu-
dowords corresponding to 5 of the 20 nouns.
In order to create the datasets for our experiments,
for each of our similarity-based and random pseu-
dowords, we sampled unique sentences from the En-
glish Gigaword corpus (Graff and Cieri, 2003) ac-
cording to the same sense distributions given by the
Senseval-3 training and test datasets for the corre-
sponding real word. Next, we performed WSD on
our three datasets, namely: the Senseval-3 dataset
of real words, and the two artificially sense-tagged
datasets for the similarity-based and random pseu-
dowords. As our WSD system for this experiment,
we used It Makes Sense (IMS), a state-of-the-art su-
pervised WSD system (Zhong and Ng, 2010).
WSD recall2 performance values on the above-
mentioned datasets are shown in Table 4. For the
random setting, in order to ensure stability, the re-
sults are averaged on a set of 25 different pseu-
dowords modeling a given ambiguous noun. We
can see from the Table that the overall system
performance with the similarity-based pseudowords
(75.14%) is much closer to the real setting (73.26%)
than it is with random pseudowords (78.80%). For
random pseudowords, the overall recall over 25 runs
ranges from 75.40% to 80.80%.
Moreover, the similarity-based approach exhibits
a closer WSD recall performance to that of real data
(|RE?SB| column in the table) for 15 of the 20
nouns (shown in bold in the Table). Accordingly,
the overall sum of the differences (distance) between
the recall values is 129.3 for similarity-based pseu-
dowords, which is considerably lower than the 196.4
for random pseudowords (averaged over 25 runs
whose distances range from 158.3 to 262.0).
To further corroborate our findings, we calculated
the Pearson?s r correlation between recall values on
real words with those obtained on the corresponding
pseudowords. Similarity-based pseudowords obtain
the high correlation of 0.74, whereas this value drops
to 0.54 for random pseudowords. Even worse, we
2Since in our experiments the WSD system always provides
an answer for each item in the test set, the values of precision,
recall and F1 will be equal.
Word RE SB RND |RE?SB| |RE?RND|
argument 50.44 68.79 77.15 18.35 26.71
arm 92.30 85.69 88.11 6.61 4.19
atmosphere 70.52 69.15 80.44 1.37 10.32
audience 81.28 73.74 83.76 7.54 4.22
bank 85.76 83.07 82.46 2.69 3.99
degree 78.42 81.58 80.59 3.16 4.35
difference 62.46 61.43 75.17 1.03 12.90
difficulty 52.72 51.82 67.23 0.90 14.97
disc 78.62 76.48 78.07 2.14 6.18
image 71.78 75.76 81.50 3.98 10.02
interest 77.34 73.19 71.70 4.15 6.85
judgment 55.64 66.87 59.64 11.23 9.01
organization 80.36 72.86 78.65 7.50 3.65
paper 60.84 66.29 73.14 5.45 12.59
party 82.94 80.00 81.04 2.94 3.74
performance 58.56 64.76 73.86 6.20 15.52
plan 88.42 85.41 87.39 3.01 3.12
shelter 58.48 74.75 80.21 16.27 21.73
sort 67.64 88.15 77.37 20.51 9.73
source 63.46 67.74 66.26 4.28 7.03
overall 73.26 75.14 78.80 129.31 196.35
Table 4: Recall percentage of IMS on the 20 nouns of the
Senseval-3 lexical-sample test set (RE) compared to the
corresponding similarity-based (SB) and random (RND)
pseudowords. The last 2 columns show absolute differ-
ences between the real and the two pseudoword settings.
observed a high variation of correlation (in the range
of [0.18, 0.67]) over the 25 sets of random pseu-
dowords (0.54 being the average).
3.2 Representative Power of Pseudosenses
The ideal case for pseudosenses would be that of
being in a synonymous relationship with the cor-
responding real sense, i.e., selected from the same
WordNet synset. But given that many of the Word-
Net synsets do not contain monosemous terms, the
similarity-based approach often needs to look fur-
ther into other related synsets to find a suitable pseu-
dosense. To get a clear idea of the exact statistics, we
went through all our similarity-based pseudowords
and, for each pseudosense wi, checked the relation-
ship in WordNet between the synset containing wi
and the corresponding real sense. Table 5 shows
for three values of minFreq the distribution of pseu-
dosenses across different types of WordNet relation-
ships, also including indirect ones. As can be seen
in the Table, when minFreq is set to 0, a large por-
tion of pseudosenses (around 75%) are selected from
synonyms or generalization/specialization relations
1105
minFreq 0 500 1000
R
el
at
io
n
ty
pe
Synonyms 33.0 7.6 5.4
Hypernyms 33.4 16.1 13.0
Hyponyms 9.1 6.1 4.9
Meronyms 0.2 0.2 0.2
Siblings 8.2 17.2 16.6
Indirect relations 16.1 52.8 59.9
Table 5: Percentage of similarity-based pseudosenses ob-
tained from different types of WordNet relations.
(hypernym and hyponyms). However, this percent-
age drops to about 23% when minFreq = 1000. This
suggests that many of our pseudosenses are mod-
eled from indirect relations when higher values of
minFreq are used. This can potentially increase the
risk of an undesirable modeling in which meanings
are not properly preserved. For this reason, we car-
ried out another experiment to assess the representa-
tive power of similarity-based pseudosenses. To this
end, we randomly sampled 110 pseudowords (from
the entire set of 15,935 pseudowords generated with
minimum frequency of 1000), 10 for each degree of
polysemy, from 2 to 12, totaling 770 pseudosenses.
Then we presented each of these pseudowords3 to
two annotators who were asked to judge the degree
of representativeness of its pseudosenses based on
the following scores: 1: completely unrelated, 2:
somewhat related, 3: good substitute, or 4: perfect
substitute.
As an example, the scores assigned by the two
annotators to different pseudosenses of the pseu-
doword generated for the noun representative are
shown in Table 6. The overall representativeness
score for each pseudoword is calculated by aver-
aging the scores assigned to its individual pseu-
dosenses. For instance, the overall scores calculated
for the pseudoword representative are 3.75 and 3.50
(as given by the two annotators). The first row in
Table 7 shows the average representativeness scores
for each degree of polysemy on the full set of 770
pseudosenses. It can be seen that the score remains
around 3.0 for all polysemy degrees from 2 to 12.
Despite the fact that only one fifth of pseudosenses
are taken from synonyms, hypernyms and hyponyms
(when minFreq is 1000, cf. Table 5), the overall
3For each pseudoword, we provided annotators with the cor-
responding real word, as well as its synsets and glosses as given
by WordNet.
Sense Definition (in short)
Sc
or
e
1
Sc
or
e
2
{Synset}
> Corresponding Pseudosense
a person who represents others
3 3{representative}
> negotiator
an advocate who represents someone else?s policy
4 4{spokesperson, interpreter, representative, voice}
> spokesperson
a member of the U.S. House of Representatives
4 4{congressman, congresswoman, representative}
> congressman
an item of information that is typical of a group
4 3{example, illustration, instance, representative}
> case in point
average score 3.75 3.50
Table 6: Examples of representativeness scores assigned
by the annotators to pseudosenses of the term representa-
tive.
representativeness score of 3.12 shows that most of
these pseudosenses can be considered as good sub-
stitutes for their corresponding real senses. There-
fore we conclude that not only does our similarity-
based pseudoword generation approach extend the
coverage of the vicinity-based method from 25% to
100% (when minFreq = 1000), but also that the
pseudosenses coming from more distant synsets as
ranked by PPR are still good representatives on av-
erage.
3.3 Distinguishability of Pseudosenses
In addition to assessing the representativeness of
pseudosenses, their degree of distinguishability has
to be determined. In other words, we have to de-
termine how easily each pseudosense can be distin-
guished from the others in a pseudoword. Our rea-
son for having such an experiment is readily illus-
trated by way of an example: consider the similarity-
based pseudoword philanthropist*benefactor4 cor-
responding to the noun donor5. Even though both
pseudosenses are good representatives for their cor-
responding senses, the distinguishability of the two
4From WordNet: ?Philanthropist: someone who makes
charitable donations intended to increase human well-being?;
?Benefactor: a person who helps people or institutions (espe-
cially with financial help)?.
5donor has 2 senses according to WordNet 3.0: (1) ?person
who makes a gift of property?; (2) ?(medicine) someone who
gives blood or tissue or an organ to be used in another person?.
1106
Polysemy 2 3 4 5 6 7 8 9 10 11 12 Overall
Representativeness score 3.3 3.4 3.1 3.1 2.9 3.1 2.9 2.8 3.3 3.1 3.3 3.12
Distinguishability score 0.90 0.83 0.83 0.82 0.81 0.77 0.75 0.73 0.80 0.71 0.70 0.79
Table 7: Average representativeness and distinguishability scores for pseudosenses of different polysemy classes
(scores range from 1 to 4 for representativeness and from 0 to 1 for distinguishability evaluation).
real senses is not preserved in the pseudoword. For
instance, benefactor is a suitable pseudosense for
both senses of donor, whereas philanthropist cannot
be used in the blood donation sense.
Therefore we carried out another manual evalua-
tion to test the efficacy of pseudowords in preserving
the distinguishability of senses of real words. To this
end, for each pseudoword Pw (from the same set of
110 sampled pseudowords used in Section 3.2) we
presented its corresponding pseudosenses in random
order to two annotators and asked them to associate
each pseudosense with the most appropriate Word-
Net sense of the real word w. Then we calculated
a distinguishability score for each polysemy degree
by dividing the number of correct mappings by the
total number of senses.
For instance, for the similarity-based pseudoword
corresponding to the word representative (shown
in Table 6), we provided the shuffled list of pseu-
dosenses [spokesperson, case in point, negotiator,
congressman] to each annotator and asked them to
sort the list according to the WordNet sense inven-
tory of representative (i.e., map each pseudosense to
its most suitable real sense). Both annotators cor-
rectly mapped all pseudosenses of this pseudoword;
hence, the distinguishability score given by each an-
notator for this pseudoword was 4/4 = 1.
The average distinguishability scores for each de-
gree of polysemy, as well as the overall score, is
shown in Table 7 (second row). Each value is an
average of the scores obtained from the two an-
notators. It can be seen that the distinguishability
score decreases for higher degrees of polysemy. The
score, however, remains above 0.70 with highly-
polysemous pseudowords. The overall score of 0.79
shows that similarity-based pseudowords effectively
preserve the distinguishability of senses of their real
counterparts. In other words, they do not tend
to have over-generalized pseudosenses which cover
more than one sense.
4 Related Work
The idea of pseudowords dates back to 1992, when
it was first proposed as a means of generating large
amounts of artificially annotated evaluation data for
WSD algorithms (Gale et al, 1992; Schu?tze, 1992).
However, as mentioned earlier in Section 2, con-
structing a pseudoword by combining a random set
of unambiguous words, as was done in these early
works, can not model systematic polysemy (Gaus-
tad, 2001; Nakov and Hearst, 2003), since differ-
ent senses of a real ambiguous word, unless it is
homonymous, share some semantic or pragmatic re-
lation.
Several researchers addressed the issue of produc-
ing semantically-aware pseudowords that can model
semantic relationships between senses. Nakov
and Hearst (2003) used lexical category mem-
bership from a medical term hierarchy (extracted
from MeSH6 (Medical Subject Headings)) to cre-
ate ?more plausibly-motivated? pseudowords. By
considering the frequency distributions from lexi-
cal category co-occurrence, they produced a set of
pseudowords which were closer to real ambiguous
words in terms of disambiguation difficulty than
random pseudowords. However, this approach re-
quires a specific hierarchical lexicon and falls short
of creating many pseudowords with high polysemy
(the authors report generating pseudowords with two
senses only).
More recent work has focused on the identifica-
tion of monosemous representatives in the surround-
ing of a sense, i.e., selected among concepts directly
related to the given sense. Lu et al (2006) mod-
eled senses of a real ambiguous word by picking
out the most similar monosemous morpheme from a
Chinese hierarchical lexicon. Pseudowords are then
constructed by conflating these morphemes accord-
ingly. However, this method leverages a specific
Chinese hierarchical lexicon, in which different lev-
6http://www.nlm.nih.gov/mesh
1107
els of the hierarchy correspond to different levels of
sense granularity. A more flexible technique is pro-
posed by Otrusina and Smrz (2010) who model am-
biguous words in WordNet. Their vicinity-based ap-
proach searches the surroundings of each particular
sense in the WordNet graph in order to find an un-
ambiguous representative for that sense. However,
as we described in Section 2.1.1, while the approach
addresses the semantic awareness issue, it falls short
of providing a high coverage, an issue which we
tackle in our novel similarity-based approach.
5 Conclusion and Future Work
In this paper we proposed a new technique for the
generation of pseudowords which, in contrast to
existing work, can simultaneously tackle the two
major issues associated with pseudowords, i.e., se-
mantic awareness and coverage. Our approach can
be used to model any given ambiguous noun in
WordNet, hence enabling the generation of large-
scale pseudosense-annotated datasets for thousands
of pseudowords. We performed three experiments
to evaluate the reliability of our pseudowords. We
showed that the similarity-based pseudowords are
highly correlated with their real counterparts in
terms of disambiguation difficulty. Further evalua-
tions demonstrated that this approach is able to pro-
vide a good semantic modeling of individual senses
of real words while preserving their distinguishabil-
ity.
We are releasing to the research community
the entire set of 15,935 pseudowords, i.e., for
all WordNet polysemous nouns (http://lcl.
uniroma1.it/pseudowords/). This set of
pseudowords (together with the English Gigaword
corpus) can be used to generate a large pseudosense-
tagged dataset containing ?1000 annotated sen-
tences for every sense of all the pseudowords mod-
eled after real ambiguous nouns in WordNet. The
resulting dataset could be a good complement for
MASC (Ide et al, 2010) which, being human-
created, can provide 1000 sense-annotated sentences
for just a few words.
We hope that the availability of this resource will
enable large-scale experiments in tasks such as se-
mantic role labeling, semantic parsing, and Word
Sense Disambiguation. Specifically, as future work,
we plan to utilize the generated pseudosense-tagged
dataset to perform an in-depth study of different
WSD paradigms. We also plan to extend our work
to other part-of-speech tags.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ?09, pages 33?41, Athens, Greece.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL
?09, pages 19?27, Boulder, Colorado.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting on
Association for Computational Linguistics, ACL ?01,
pages 26?33, Toulouse, France.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 59?68, Honolulu, Hawaii.
Stefan Bordag. 2006. Word Sense Induction: Triplet-
based clustering and automatic evaluation. In Pro-
ceedings of the 11th Conference on European chap-
ter of the Association for Computational Linguistics,
EACL ?06, pages 137?144, Trento, Italy.
Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selectional
preferences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
ACL ?10, pages 445?453, Uppsala, Sweden.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and diversifying Web search results with graph-
based Word Sense Induction. Computational Linguis-
tics, 39(4).
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
1108
Annual Meeting of the Association of Computational
Linguistics, ACL ?07, pages 216?223, Prague, Czech
Republic.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
William Gale, Kenneth Church, and David Yarowsky.
1992. Work on statistical methods for Word Sense
Disambiguation. In Proceedings of the AAAI Fall
Symposium on Probabilistic Approaches to Natural
Language, pages 54?60, Menlo Park, CA.
Tanja Gaustad. 2001. Statistical corpus-based Word
Sense Disambiguation: Pseudowords vs real ambigu-
ous words. In Companion Volume to the Proceed-
ings of the 39th Annual Meeting of the Association
for Computational Linguistics Proceedings ot the Stu-
dent Research Workshop, ACL/EACL ?01, pages 61?
66, Toulouse, France.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
David Graff and Christopher Cieri. 2003. English Giga-
word, LDC2003T05. In Linguistic Data Consortium,
Philadelphia.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of 11th International Conference on
World Wide Web, WWW ?02, pages 517?526, Hon-
olulu, Hawaii, USA.
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, EMNLP-CoNLL
?07, pages 581?589, Prague, Czech Republic.
Nancy Ide, Collin F. Baker, Christiane Fellbaum, and Re-
becca J. Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the peo-
ple. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (Short Pa-
pers), pages 68?73, Uppsala, Sweden.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, ICML ?98,
pages 296?304, Madison, USA.
Zhimao Lu, Haifeng Wang, Jianmin Yao, Ting Liu, and
Sheng Li. 2006. An equivalent pseudoword solution
to Chinese Word Sense Disambiguation. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL ?06,
pages 457?464, Sydney, Australia.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of Senseval-3: The Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 25?28, Barcelona,
Spain.
Preslav I. Nakov and Marti A. Hearst. 2003. Category-
based pseudowords. In Proceedings of the Confer-
ence of the North American Chapter of the Association
of Computational Linguistics ? short papers, HLT-
NAACL ?03, pages 67?69, Edmonton, Canada.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A quick tour of word sense
disambiguation, induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science, SOF-
SEM ?12, pages 115?129, Spindleruv Mlyn, Czech
Republic.
Lubomir Otrusina and Pavel Smrz. 2010. A new ap-
proach to pseudoword generation. In Proceedings of
the International Conference on Language Resources
and Evaluation, LREC?10, pages 1195?1199, Valletta,
Malta.
Hinrich Schu?tze. 1992. Dimensions of meaning.
In Supercomputing ?92: Proceedings of the 1992
ACM/IEEE conference on Supercomputing, pages
787?796, Minneapolis, Minnesota, USA.
David Yarowsky. 1993. One sense per collocation.
In Proceedings of the 3rd DARPA Workshop on Hu-
man Language Technology, pages 266?271, Princeton,
New Jersey.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage Word Sense Disambiguation system
for free text. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
ACL?10, pages 78?83, Uppsala, Sweden.
1109
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216?225,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
BabelNet: Building a Very Large Multilingual Semantic Network
Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
navigli@di.uniroma1.it
Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University
ponzetto@cl.uni-heidelberg.de
Abstract
In this paper we present BabelNet ? a
very large, wide-coverage multilingual se-
mantic network. The resource is automat-
ically constructed by means of a method-
ology that integrates lexicographic and en-
cyclopedic knowledge from WordNet and
Wikipedia. In addition Machine Transla-
tion is also applied to enrich the resource
with lexical information for all languages.
We conduct experiments on new and ex-
isting gold-standard datasets to show the
high quality and coverage of the resource.
1 Introduction
In many research areas of Natural Language Pro-
cessing (NLP) lexical knowledge is exploited to
perform tasks effectively. These include, among
others, text summarization (Nastase, 2008),
Named Entity Recognition (Bunescu and Pas?ca,
2006), Question Answering (Harabagiu et al,
2000) and text categorization (Gabrilovich and
Markovitch, 2006). Recent studies in the diffi-
cult task of Word Sense Disambiguation (Nav-
igli, 2009b, WSD) have shown the impact of the
amount and quality of lexical knowledge (Cuadros
and Rigau, 2006): richer knowledge sources can
be of great benefit to both knowledge-lean systems
(Navigli and Lapata, 2010) and supervised classi-
fiers (Ng and Lee, 1996; Yarowsky and Florian,
2002).
Various projects have been undertaken to make
lexical knowledge available in a machine read-
able format. A pioneering endeavor was Word-
Net (Fellbaum, 1998), a computational lexicon of
English based on psycholinguistic theories. Sub-
sequent projects have also tackled the significant
problem of multilinguality. These include Eu-
roWordNet (Vossen, 1998), MultiWordNet (Pianta
et al, 2002), the Multilingual Central Repository
(Atserias et al, 2004), and many others. How-
ever, manual construction methods inherently suf-
fer from a number of drawbacks. First, maintain-
ing and updating lexical knowledge resources is
expensive and time-consuming. Second, such re-
sources are typically lexicographic, and thus con-
tain mainly concepts and only a few named enti-
ties. Third, resources for non-English languages
often have a much poorer coverage since the con-
struction effort must be repeated for every lan-
guage of interest. As a result, an obvious bias ex-
ists towards conducting research in resource-rich
languages, such as English.
A solution to these issues is to draw upon
a large-scale collaborative resource, namely
Wikipedia1. Wikipedia represents the perfect com-
plement to WordNet, as it provides multilingual
lexical knowledge of a mostly encyclopedic na-
ture. While the contribution of any individual user
might be imprecise or inaccurate, the continual in-
tervention of expert contributors in all domains re-
sults in a resource of the highest quality (Giles,
2005). But while a great deal of work has been re-
cently devoted to the automatic extraction of struc-
tured information from Wikipedia (Wu and Weld,
2007; Ponzetto and Strube, 2007; Suchanek et
al., 2008; Medelyan et al, 2009, inter alia), the
knowledge extracted is organized in a looser way
than in a computational lexicon such as WordNet.
In this paper, we make a major step towards the
vision of a wide-coverage multilingual knowledge
resource. We present a novel methodology that
produces a very large multilingual semantic net-
work: BabelNet. This resource is created by link-
ing Wikipedia to WordNet via an automatic map-
ping and by integrating lexical gaps in resource-
1http://download.wikipedia.org. We use the
English Wikipedia database dump from November 3, 2009,
which includes 3,083,466 articles. Throughout this paper, we
use Sans Serif for words, SMALL CAPS for Wikipedia pages
and CAPITALS for Wikipedia categories.
216
high wind
blow gas
gasbag
windhot-air
balloon
gas
cluster
ballooning
Montgolfier
brothers
Fermi gas
is-
a
has-part
is-a
is-a
Wikipedia WordNet
balloon
BABEL SYNSET
balloonEN, BallonDE,
aerostatoES, globusCA,
pallone aerostaticoIT,
ballonFR, montgolfie`reFR
WIKIPEDIA SENTENCES
...world?s first hydrogen balloon flight.
...an interim balloon altitude record...
...from a British balloon near Be?court...
+
SEMCOR SENTENCES
...look at the balloon and the...
...suspended like a huge balloon, in...
...the balloon would go up...
Machine Translation system
Figure 1: An illustrative overview of BabelNet.
poor languages with the aid of Machine Transla-
tion. The result is an ?encyclopedic dictionary?,
that provides concepts and named entities lexical-
ized in many languages and connected with large
amounts of semantic relations.
2 BabelNet
We encode knowledge as a labeled directed graph
G = (V,E) where V is the set of vertices ? i.e.
concepts2 such as balloon ? andE ? V ?R?V is
the set of edges connecting pairs of concepts. Each
edge is labeled with a semantic relation from R,
e.g. {is-a, part-of , . . . , }, where  denotes an un-
specified semantic relation. Importantly, each ver-
tex v ? V contains a set of lexicalizations of the
concept for different languages, e.g. { balloonEN,
BallonDE, aerostatoES, . . . , montgolfie`reFR }.
Concepts and relations in BabelNet are har-
vested from the largest available semantic lexi-
con of English, WordNet, and a wide-coverage
collaboratively edited encyclopedia, the English
Wikipedia (Section 3.1). We collect (a) from
WordNet, all available word senses (as concepts)
and all the semantic pointers between synsets (as
relations); (b) from Wikipedia, all encyclopedic
entries (i.e. pages, as concepts) and semantically
unspecified relations from hyperlinked text.
In order to provide a unified resource, we merge
the intersection of these two knowledge sources
(i.e. their concepts in common) by establishing a
mapping between Wikipedia pages and WordNet
senses (Section 3.2). This avoids duplicate con-
cepts and allows their inventories of concepts to
complement each other. Finally, to enable mul-
tilinguality, we collect the lexical realizations of
the available concepts in different languages by
2Throughout the paper, unless otherwise stated, we use
the general term concept to denote either a concept or a
named entity.
using (a) the human-generated translations pro-
vided in Wikipedia (the so-called inter-language
links), as well as (b) a machine translation sys-
tem to translate occurrences of the concepts within
sense-tagged corpora, namely SemCor (Miller et
al., 1993) ? a corpus annotated with WordNet
senses ? and Wikipedia itself (Section 3.3). We
call the resulting set of multilingual lexicalizations
of a given concept a babel synset. An overview of
BabelNet is given in Figure 1 (we label vertices
with English lexicalizations): unlabeled edges are
obtained from links in the Wikipedia pages (e.g.
BALLOON (AIRCRAFT) links to WIND), whereas
labeled ones from WordNet3 (e.g. balloon1n has-
part gasbag1n). In this paper we restrict ourselves
to concepts lexicalized as nouns. Nonetheless, our
methodology can be applied to all parts of speech,
but in that case Wikipedia cannot be exploited,
since it mainly contains nominal entities.
3 Methodology
3.1 Knowledge Resources
WordNet. The most popular lexical knowledge
resource in the field of NLP is certainly WordNet,
a computational lexicon of the English language.
A concept in WordNet is represented as a synonym
set (called synset), i.e. the set of words that share
the same meaning. For instance, the concept wind
is expressed by the following synset:
{ wind1n, air current1n, current of air1n },
where each word?s subscripts and superscripts in-
dicate their parts of speech (e.g. n stands for noun)
3We use in the following WordNet version 3.0. We de-
note with wip the i-th sense of a word w with part of speech
p. We use word senses to unambiguously denote the corre-
sponding synsets (e.g. plane1n for { airplane1n, aeroplane1n,
plane1n }). Hereafter, we use word sense and synset inter-
changeably.
217
and sense number, respectively. For each synset,
WordNet provides a textual definition, or gloss.
For example, the gloss of the above synset is: ?air
moving from an area of high pressure to an area of
low pressure?.
Wikipedia. Our second resource, Wikipedia,
is a Web-based collaborative encyclopedia. A
Wikipedia page (henceforth, Wikipage) presents
the knowledge about a specific concept (e.g. BAL-
LOON (AIRCRAFT)) or named entity (e.g. MONT-
GOLFIER BROTHERS). The page typically con-
tains hypertext linked to other relevant Wikipages.
For instance, BALLOON (AIRCRAFT) is linked to
WIND, GAS, and so on. The title of a Wikipage
(e.g. BALLOON (AIRCRAFT)) is composed of
the lemma of the concept defined (e.g. balloon)
plus an optional label in parentheses which speci-
fies its meaning if the lemma is ambiguous (e.g.
AIRCRAFT vs. TOY). Wikipages also provide
inter-language links to their counterparts in other
languages (e.g. BALLOON (AIRCRAFT) links to
the Spanish page AEROSTATO). Finally, some
Wikipages are redirections to other pages, e.g.
the Spanish BALO?N AEROSTA?TICO redirects to
AEROSTATO.
3.2 Mapping Wikipedia to WordNet
The first phase of our methodology aims to estab-
lish links between Wikipages and WordNet senses.
We aim to acquire a mapping ? such that, for each
Wikipage w, we have:
?(w) =
?
?
?
s ? SensesWN(w) if a link can be
established,
 otherwise,
where SensesWN(w) is the set of senses of the
lemma of w in WordNet. For example, if our map-
ping methodology linked BALLOON (AIRCRAFT)
to the corresponding WordNet sense balloon1n,
we would have ?(BALLOON (AIRCRAFT)) = bal-
loon1n.
In order to establish a mapping between the
two resources, we first identify the disambigua-
tion contexts for Wikipages (Section 3.2.1) and
WordNet senses (Section 3.2.2). Next, we inter-
sect these contexts to perform the mapping (see
Section 3.2.3).
3.2.1 Disambiguation Context of a Wikipage
Given a Wikipage w, we use the following infor-
mation as disambiguation context:
? Sense labels: e.g. given the page BALLOON
(AIRCRAFT), the word aircraft is added to the
disambiguation context.
? Links: the titles? lemmas of the pages linked
from the target Wikipage (i.e., outgoing links).
For instance, the links in the Wikipage BAL-
LOON (AIRCRAFT) include wind, gas, etc.
? Categories: Wikipages are typically classi-
fied according to one or more categories.
For example, the Wikipage BALLOON (AIR-
CRAFT) is categorized as BALLOONS, BAL-
LOONING, etc. While many categories are
very specific and do not appear in Word-
Net (e.g., SWEDISH WRITERS or SCIEN-
TISTS WHO COMMITTED SUICIDE), we
use their syntactic heads as disambiguation con-
text (i.e. writer and scientist, respectively).
Given a Wikipage w, we define its disambiguation
context Ctx(w) as the set of words obtained from
all of the three sources above.
3.2.2 Disambiguation Context of a WordNet
Sense
Given a WordNet sense s and its synset S, we col-
lect the following information:
? Synonymy: all synonyms of s in S. For in-
stance, given the sense airplane1n and its cor-
responding synset { airplane1n, aeroplane1n,
plane1n }, the words contained therein are in-
cluded in the context.
? Hypernymy/Hyponymy: all synonyms in the
synsets H such that H is either a hypernym
(i.e., a generalization) or a hyponym (i.e., a
specialization) of S. For example, given bal-
loon1n, we include the words from its hypernym
{ lighter-than-air craft1n } and all its hyponyms
(e.g. { hot-air balloon1n }).
? Sisterhood: words from the sisters of S. A sis-
ter synset S? is such that S and S? have a com-
mon direct hypernym. For example, given bal-
loon1n, it can be found that { balloon1n } and
{ airship1n, dirigible1n } are sisters. Thus air-
ship and dirigible are included in the disam-
biguation context of s.
? Gloss: the set of lemmas of the content words
occurring within the WordNet gloss of S.
We thus define the disambiguation context Ctx(s)
of sense s as the set of words obtained from all of
the four sources above.
218
3.2.3 Mapping Algorithm
In order to link each Wikipedia page to a WordNet
sense, we perform the following steps:
? Initially, our mapping ? is empty, i.e. it links
each Wikipage w to .
? For each Wikipage w whose lemma is monose-
mous both in Wikipedia and WordNet we map
w to its only WordNet sense.
? For each remaining Wikipage w for which no
mapping was previously found (i.e., ?(w) = ),
we assign the most likely sense to w based on
the maximization of the conditional probabili-
ties p(s|w) over the senses s ? SensesWN(w)
(no mapping is established if a tie occurs).
To find the mapping of a Wikipage w, we need
to compute the conditional probability p(s|w) of
selecting the WordNet sense s given w. The sense
s which maximizes this probability is determined
as follows:
?(w) = argmax
s?SensesWN(w)
p(s|w) = argmax
s
p(s, w)
p(w)
= argmax
s
p(s, w)
The latter formula is obtained by observing that
p(w) does not influence our maximization, as it is
a constant independent of s. As a result, determin-
ing the most appropriate sense s consists of find-
ing the sense s that maximizes the joint probability
p(s, w). We estimate p(s, w) as:
p(s, w) =
score(s, w)
?
s??SensesWN(w),
w??SensesWiki(w)
score(s?, w?)
,
where score(s, w) = |Ctx(s)?Ctx(w)|+ 1 (we
add 1 as a smoothing factor). Thus, in our al-
gorithm we determine the best sense s by com-
puting the intersection of the disambiguation con-
texts of s and w, and normalizing by the scores
summed over all senses of w in Wikipedia and
WordNet. More details on the mapping algorithm
can be found in Ponzetto and Navigli (2010).
3.3 Translating Babel Synsets
So far we have linked English Wikipages to Word-
Net senses. Given a Wikipage w, and provided it
is mapped to a sense s (i.e., ?(w) = s), we cre-
ate a babel synset S ?W , where S is the WordNet
synset to which sense s belongs, and W includes:
(i) w; (ii) all its inter-language links (that is, trans-
lations of the Wikipage to other languages); (iii)
the redirections to the inter-language links found
in the Wikipedia of the target language. For in-
stance, given that ?(BALLOON) = balloon1n, the
corresponding babel synset is { balloonEN, Bal-
lonDE, aerostatoES, balo?n aerosta?ticoES, . . . ,
pallone aerostaticoIT }. However, two issues
arise: first, a concept might be covered only in
one of the two resources (either WordNet or
Wikipedia), meaning that no link can be estab-
lished (e.g., FERMI GAS or gasbag1n in Figure
1); second, even if covered in both resources, the
Wikipage for the concept might not provide any
translation for the language of interest (e.g., the
Catalan for BALLOON is missing in Wikipedia).
In order to address the above issues and thus
guarantee high coverage for all languages we de-
veloped a methodology for translating senses in
the babel synset to missing languages. Given a
WordNet word sense in our babel synset of interest
(e.g. balloon1n) we collect its occurrences in Sem-
Cor (Miller et al, 1993), a corpus of more than
200,000 words annotated with WordNet senses.
We do the same for Wikipages by retrieving sen-
tences in Wikipedia with links to the Wikipage of
interest. By repeating this step for each English
lexicalization in a babel synset, we obtain a col-
lection of sentences for the babel synset (see left
part of Figure 1). Next, we apply state-of-the-art
Machine Translation4 and translate the set of sen-
tences in all the languages of interest. Given a spe-
cific term in the initial babel synset, we collect the
set of its translations. We then identify the most
frequent translation in each language and add it to
the babel synset. Note that translations are sense-
specific, as the context in which a term occurs is
provided to the translation system.
3.4 Example
We now illustrate the execution of our method-
ology by way of an example. Let us focus on
the Wikipage BALLOON (AIRCRAFT). The word
is polysemous both in Wikipedia and WordNet.
In the first phase of our methodology we aim
to find a mapping ?(BALLOON (AIRCRAFT)) to
an appropriate WordNet sense of the word. To
4We use the Google Translate API. An initial prototype
used a statistical machine translation system based on Moses
(Koehn et al, 2007) and trained on Europarl (Koehn, 2005).
However, we found such system unable to cope with many
technical names, such as in the domains of sciences, litera-
ture, history, etc.
219
this end we construct the disambiguation context
for the Wikipage by including words from its la-
bel, links and categories (cf. Section 3.2.1). The
context thus includes, among others, the follow-
ing words: aircraft, wind, airship, lighter-than-
air. We now construct the disambiguation context
for the two WordNet senses of balloon (cf. Sec-
tion 3.2.2), namely the aircraft (#1) and the toy
(#2) senses. To do so, we include words from
their synsets, hypernyms, hyponyms, sisters, and
glosses. The context for balloon1n includes: air-
craft, craft, airship, lighter-than-air. The con-
text for balloon2n contains: toy, doll, hobby. The
sense with the largest intersection is #1, so the
following mapping is established: ?(BALLOON
(AIRCRAFT)) = balloon1n. After the first phase,
our babel synset includes the following English
words from WordNet plus the Wikipedia inter-
language links to other languages (we report Ger-
man, Spanish and Italian): { balloonEN, BallonDE,
aerostatoES, balo?n aerosta?ticoES, pallone aero-
staticoIT }.
In the second phase (see Section 3.3), we col-
lect all the sentences in SemCor and Wikipedia in
which the above English word sense occurs. We
translate these sentences with the Google Trans-
late API and select the most frequent transla-
tion in each language. As a result, we can en-
rich the initial babel synset with the following
words: mongolfie`reFR, globusCA, globoES, mon-
golfieraIT. Note that we had no translation for
Catalan and French in the first phase, because the
inter-language link was not available, and we also
obtain new lexicalizations for the Spanish and Ital-
ian languages.
4 Experiment 1: Mapping Evaluation
Experimental setting. We first performed an
evaluation of the quality of our mapping from
Wikipedia to WordNet. To create a gold stan-
dard for evaluation we considered all lemmas
whose senses are contained both in WordNet and
Wikipedia: the intersection between the two re-
sources contains 80,295 lemmas which corre-
spond to 105,797 WordNet senses and 199,735
Wikipedia pages. The average polysemy is 1.3
and 2.5 for WordNet senses and Wikipages, re-
spectively (2.8 and 4.7 when excluding monose-
mous words). We then selected a random sam-
ple of 1,000 Wikipages and asked an annotator
with previous experience in lexicographic annota-
P R F1 A
Mapping algorithm 81.9 77.5 79.6 84.4
MFS BL 24.3 47.8 32.2 24.3
Random BL 23.8 46.8 31.6 23.9
Table 1: Performance of the mapping algorithm.
tion to provide the correct WordNet sense for each
page (an empty sense label was given, if no correct
mapping was possible). The gold-standard dataset
includes 505 non-empty mappings, i.e. Wikipages
with a corresponding WordNet sense. In order to
quantify the quality of the annotations and the dif-
ficulty of the task, a second annotator sense tagged
a subset of 200 pages from the original sample.
Our annotators achieved a ? inter-annotator agree-
ment (Carletta, 1996) of 0.9, indicating almost
perfect agreement.
Results and discussion. Table 1 summarizes the
performance of our mapping algorithm against
the manually annotated dataset. Evaluation is per-
formed in terms of standard measures of preci-
sion, recall, and F1-measure. In addition we calcu-
late accuracy, which also takes into account empty
sense labels. As baselines we use the most fre-
quent WordNet sense (MFS), and a random sense
assignment.
The results show that our method achieves al-
most 80% F1 and it improves over the baselines by
a large margin. The final mapping contains 81,533
pairs of Wikipages and word senses they map to,
covering 55.7% of the noun senses in WordNet.
As for the baselines, the most frequent sense is
just 0.6% and 0.4% above the random baseline in
terms of F1 and accuracy, respectively. A ?2 test
reveals in fact no statistical significant difference
at p < 0.05. This is related to the random distri-
bution of senses in our dataset and the Wikipedia
unbiased coverage of WordNet senses. So select-
ing the first WordNet sense rather than any other
sense for each target page represents a choice as
arbitrary as picking a sense at random.
5 Experiment 2: Translation Evaluation
We perform a second set of experiments concern-
ing the quality of the acquired concepts. This is as-
sessed in terms of coverage against gold-standard
resources (Section 5.1) and against a manually-
validated dataset of translations (Section 5.2).
220
Language Word senses Synsets
German 15,762 9,877
Spanish 83,114 55,365
Catalan 64,171 40,466
Italian 57,255 32,156
French 44,265 31,742
Table 2: Size of the gold-standard wordnets.
5.1 Automatic Evaluation
Datasets. We compare BabelNet against gold-
standard resources for 5 languages, namely: the
subset of GermaNet (Lemnitzer and Kunze, 2002)
included in EuroWordNet for German, Multi-
WordNet (Pianta et al, 2002) for Italian, the Mul-
tilingual Central Repository for Spanish and Cata-
lan (Atserias et al, 2004), and WOrdnet Libre
du Franc?ais (Beno??t and Fis?er, 2008, WOLF) for
French. In Table 2 we report the number of synsets
and word senses available in the gold-standard re-
sources for the 5 languages.
Measures. Let B be BabelNet, F our gold-
standard non-English wordnet (e.g. GermaNet),
and let E be the English WordNet. All the gold-
standard non-English resources, as well as Babel-
Net, are linked to the English WordNet: given a
synset SF ? F , we denote its corresponding babel
synset as SB and its synset in the English Word-
Net as SE . We assess the coverage of BabelNet
against our gold-standard wordnets both in terms
of synsets and word senses. For synsets, we calcu-
late coverage as follows:
SynsetCov(B,F) =
?
SF?F
?(SB, SF )
|{SF ? F}|
,
where ?(SB, SF ) = 1 if the two synsets SB and
SF have a synonym in common, 0 otherwise. That
is, synset coverage is determined as the percentage
of synsets of F that share a term with the corre-
sponding babel synsets. For word senses we cal-
culate a similar measure of coverage:
WordCov(B,F) =
?
SF?F
?
sF?SF
??(sF , SB)
|{sF ? SF : SF ? F}|
,
where sF is a word sense in synset SF and
??(sF , SB) = 1 if sF ? SB, 0 otherwise. That
is we calculate the ratio of word senses in our
gold-standard resource F that also occur in the
corresponding synset SB to the overall number of
senses in F .
However, our gold-standard resources cover
only a portion of the English WordNet, whereas
the overall coverage of BabelNet is much higher.
We calculate extra coverage for synsets as follows:
SynsetExtraCov(B,F) =
?
SE?E\F
?(SB, SE)
|{SF ? F}|
.
Similarly, we calculate extra coverage for word
senses in BabelNet corresponding to WordNet
synsets not covered by the reference resource F .
Results and discussion. We evaluate the cov-
erage and extra coverage of word senses and
synsets at different stages: (a) using only the inter-
language links from Wikipedia (WIKI Links); (b)
and (c) using only the automatic translations of the
sentences from Wikipedia (WIKI Transl.) or Sem-
Cor (WN Transl.); (d) using all available transla-
tions, i.e. BABELNET.
Coverage results are reported in Table 3. The
percentage of word senses covered by BabelNet
ranges from 52.9% (Italian) to 66.4 (Spanish)
and 86.0% (French). Synset coverage ranges from
73.3% (Catalan) to 76.6% (Spanish) and 92.9%
(French). As expected, synset coverage is higher,
because a synset in the reference resource is con-
sidered to be covered if it shares at least one word
with the corresponding synset in BabelNet.
Numbers for the extra coverage, which pro-
vides information about the percentage of word
senses and synsets in BabelNet but not in the gold-
standard resources, are given in Figure 2. The re-
sults show that we provide for all languages a high
extra coverage for both word senses ? between
340.1% (Catalan) and 2,298% (German) ? and
synsets ? between 102.8% (Spanish) and 902.6%
(German).
Table 3 and Figure 2 show that the best results
are obtained when combining all available trans-
lations, i.e. both from Wikipedia and the machine
translation system. The performance figures suf-
fer from the errors of the mapping phase (see Sec-
tion 4). Nonetheless, the results are generally high,
with a peak for French, since WOLF has been cre-
ated semi-automatically by combining several re-
sources, including Wikipedia. The relatively low
word sense coverage for Italian (55.4%) is, in-
stead, due to the lack of many common words in
the Italian gold-standard synsets. Examples in-
clude whipEN translated as staffileIT but not as the
more common frustaIT, playboyEN translated as
vitaioloIT but not gigolo`IT, etc.
221
0%	 ?
500%	 ?
1000%	 ?
1500%	 ?
2000%	 ?
2500%	 ?
German	 ? Spanish	 ? Catalan	 ? Italian	 ? French	 ?
Wiki	 ?Links	 ?
Wiki	 ?Transl.	 ?
WN	 ?	 ?Transl.	 ?
BabelNet	 ?
(a) word senses
0%	 ?
100%	 ?
200%	 ?
300%	 ?
400%	 ?
500%	 ?
600%	 ?
700%	 ?
800%	 ?
900%	 ?
1000%	 ?
German	 ? Spanish	 ? Catalan	 ? Italian	 ? French	 ?
Wiki	 ?Links	 ?
Wiki	 ?Transl.	 ?
WN	 ?	 ?Transl.	 ?
BabelNet	 ?
(b) synsets
Figure 2: Extra coverage against gold-standard wordnets: word senses (a) and synsets (b).
Resource Method SENSES SYNSETS
G
er
m
an WIKI
{ Links 39.6 50.7
Transl. 42.6 58.2
WN Transl. 21.0 28.6
BABELNET All 57.6 73.4
S
pa
ni
sh WIKI
{ Links 34.4 40.7
Transl. 47.9 56.1
WN Transl. 25.2 30.0
BABELNET All 66.4 76.6
C
at
al
an
WIKI
{ Links 20.3 25.2
Transl. 46.9 54.1
WN Transl. 25.0 29.6
BABELNET All 64.0 73.3
It
al
ia
n
WIKI
{ Links 28.1 40.0
Transl. 39.9 58.0
WN Transl. 19.7 28.7
BABELNET All 52.9 73.7
F
re
nc
h WIKI
{ Links 70.0 72.4
Transl. 69.6 79.6
WN Transl. 16.3 19.4
BABELNET All 86.0 92.9
Table 3: Coverage against gold-standard wordnets
(we report percentages).
5.2 Manual Evaluation
Experimental setup. The automatic evaluation
quantifies how much of the gold-standard re-
sources is covered by BabelNet. However, it
does not say anything about the precision of the
additional lexicalizations provided by BabelNet.
Given that our resource has displayed a remark-
ably high extra coverage ? ranging from 340%
to 2,298% of the national wordnets (see Figure
2) ? we performed a second evaluation to assess
its precision. For each of our 5 languages, we
selected a random set of 600 babel synsets com-
posed as follows: 200 synsets whose senses ex-
ist in WordNet only, 200 synsets in the intersec-
tion between WordNet and Wikipedia (i.e. those
mapped with our method illustrated in Section
3.2), 200 synsets whose lexicalizations exist in
Wikipedia only. Therefore, our dataset included
600? 5 = 3,000 babel synsets. None of the synsets
was covered by any of the five reference wordnets.
The babel synsets were manually validated by ex-
pert annotators who decided which senses (i.e.
lexicalizations) were appropriate given the corre-
sponding WordNet gloss and/or Wikipage.
Results and discussion. We report the results in
Table 4. For each language (rows) and for each
of the three regions of BabelNet (columns), we
report precision (i.e. the percentage of synonyms
deemed correct) and, in parentheses, the over-
all number of synonyms evaluated. The results
show that the different regions of BabelNet con-
tain translations of different quality: while on av-
erage translations for WordNet-only synsets have
a precision around 72%, when Wikipedia comes
into play the performance increases considerably
(around 80% in the intersection and 95% with
Wikipedia-only translations). As can be seen from
the figures in parentheses, the number of trans-
lations available in the presence of Wikipedia is
higher. This quantitative difference is due to our
method collecting many translations from the redi-
rections in the Wikipedia of the target language
(Section 3.3), as well as to the paucity of examples
in SemCor for many synsets. In addition, some of
the synsets in WordNet with no Wikipedia coun-
terpart are very difficult to translate. Examples
include terms like stammel, crape fern, base-
ball clinic, and many others for which we could
222
Language WN WN ?Wiki Wiki
German 73.76 (282) 78.37 (777) 97.74 (709)
Spanish 69.45 (275) 78.53 (643) 92.46 (703)
Catalan 75.58 (258) 82.98 (517) 92.71 (398)
Italian 72.32 (271) 80.83 (574) 99.09 (552)
French 67.16 (268) 77.43 (709) 96.44 (758)
Table 4: Precision of BabelNet on synonyms in
WordNet (WN), Wikipedia (Wiki) and their inter-
section (WN ? Wiki): percentage and total num-
ber of words (in parentheses) are reported.
not find translations in major editions of bilingual
dictionaries. In contrast, good translations were
produced using our machine translation method
when enough sentences were available. Examples
are: chaudre?e de poissonFR for fish chowderEN,
grano de cafe?ES for coffee beanEN, etc.
6 Related Work
Previous attempts to manually build multilingual
resources have led to the creation of a multi-
tude of wordnets such as EuroWordNet (Vossen,
1998), MultiWordNet (Pianta et al, 2002), Balka-
Net (Tufis? et al, 2004), Arabic WordNet (Black
et al, 2006), the Multilingual Central Repository
(Atserias et al, 2004), bilingual electronic dic-
tionaries such as EDR (Yokoi, 1995), and fully-
fledged frameworks for the development of multi-
lingual lexicons (Lenci et al, 2000). As it is of-
ten the case with manually assembled resources,
these lexical knowledge repositories are hindered
by high development costs and an insufficient cov-
erage. This barrier has led to proposals that ac-
quire multilingual lexicons from either parallel
text (Gale and Church, 1993; Fung, 1995, inter
alia) or monolingual corpora (Sammer and Soder-
land, 2007; Haghighi et al, 2008). The disam-
biguation of bilingual dictionary glosses has also
been proposed to create a bilingual semantic net-
work from a machine readable dictionary (Nav-
igli, 2009a). Recently, Etzioni et al (2007) and
Mausam et al (2009) presented methods to pro-
duce massive multilingual translation dictionaries
from Web resources such as online lexicons and
Wiktionaries. However, while providing lexical
resources on a very large scale for hundreds of
thousands of language pairs, these do not encode
semantic relations between concepts denoted by
their lexical entries.
The research closest to ours is presented by de
Melo and Weikum (2009), who developed a Uni-
versal WordNet (UWN) by automatically acquir-
ing a semantic network for languages other than
English. UWN is bootstrapped from WordNet and
is built by collecting evidence extracted from ex-
isting wordnets, translation dictionaries, and par-
allel corpora. The result is a graph containing
800,000 words from over 200 languages in a hier-
archically structured semantic network with over
1.5 million links from words to word senses. Our
work goes one step further by (1) developing an
even larger multilingual resource including both
lexical semantic and encyclopedic knowledge, (2)
enriching the structure of the ?core? semantic net-
work (i.e. the semantic pointers from WordNet)
with topical, semantically unspecified relations
from the link structure of Wikipedia. This result
is essentially achieved by complementing Word-
Net with Wikipedia, as well as by leveraging the
multilingual structure of the latter. Previous at-
tempts at linking the two resources have been pro-
posed. These include associating Wikipedia pages
with the most frequent WordNet sense (Suchanek
et al, 2008), extracting domain information from
Wikipedia and providing a manual mapping to
WordNet concepts (Auer et al, 2007), a model
based on vector spaces (Ruiz-Casado et al, 2005),
a supervised approach using keyword extraction
(Reiter et al, 2008), as well as automatically
linking Wikipedia categories to WordNet based
on structural information (Ponzetto and Navigli,
2009). In contrast to previous work, BabelNet
is the first proposal that integrates the relational
structure of WordNet with the semi-structured in-
formation from Wikipedia into a unified, wide-
coverage, multilingual semantic network.
7 Conclusions
In this paper we have presented a novel methodol-
ogy for the automatic construction of a large multi-
lingual lexical knowledge resource. Key to our ap-
proach is the establishment of a mapping between
a multilingual encyclopedic knowledge repository
(Wikipedia) and a computational lexicon of En-
glish (WordNet). This integration process has
several advantages. Firstly, the two resources
contribute different kinds of lexical knowledge,
one is concerned mostly with named entities, the
other with concepts. Secondly, while Wikipedia
is less structured than WordNet, it provides large
223
amounts of semantic relations and can be lever-
aged to enable multilinguality. Thus, even when
they overlap, the two resources provide comple-
mentary information about the same named enti-
ties or concepts. Further, we contribute a large
set of sense occurrences harvested from Wikipedia
and SemCor, a corpus that we input to a state-of-
the-art machine translation system to fill in the gap
between resource-rich languages ? such as English
? and resource-poorer ones. Our hope is that the
availability of such a language-rich resource5 will
enable many non-English and multilingual NLP
applications to be developed.
Our experiments show that our fully-automated
approach produces a large-scale lexical resource
with high accuracy. The resource includes millions
of semantic relations, mainly from Wikipedia
(however, WordNet relations are labeled), and
contains almost 3 million concepts (6.7 labels per
concept on average). As pointed out in Section
5, such coverage is much wider than that of ex-
isting wordnets in non-English languages. While
BabelNet currently includes 6 languages, links to
freely-available wordnets6 can immediately be es-
tablished by utilizing the English WordNet as an
interlanguage index. Indeed, BabelNet can be ex-
tended to virtually any language of interest. In
fact, our translation method allows it to cope with
any resource-poor language.
As future work, we plan to apply our method
to other languages, including Eastern European,
Arabic, and Asian languages. We also intend to
link missing concepts in WordNet, by establish-
ing their most likely hypernyms ? e.g., a` la Snow
et al (2006). We will perform a semi-automatic
validation of BabelNet, e.g. by exploiting Ama-
zon?s Mechanical Turk (Callison-Burch, 2009) or
designing a collaborative game (von Ahn, 2006)
to validate low-ranking mappings and translations.
Finally, we aim to apply BabelNet to a variety of
applications which are known to benefit from a
wide-coverage knowledge resource. We have al-
ready shown that the English-only subset of Ba-
belNet alows simple knowledge-based algorithms
to compete with supervised systems in standard
coarse-grained and domain-specific WSD settings
(Ponzetto and Navigli, 2010). We plan in the near
future to apply BabelNet to the challenging task of
cross-lingual WSD (Lefever and Hoste, 2009).
5BabelNet can be freely downloaded for research pur-
poses at http://lcl.uniroma1.it/babelnet.
6http://www.globalwordnet.org.
References
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The MEANING multilingual central
repository. In Proc. of GWC-04, pages 80?210.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. Dbpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722?735.
Sagot Beno??t and Darja Fis?er. 2008. Building a free
French WordNet from multilingual resources. In
Proceedings of the Ontolex 2008 Workshop.
William Black, Sabri Elkateb Horacio Rodriguez,
Musa Alkhalifa, Piek Vossen, and Adam Pease.
2006. Introducing the Arabic WordNet project. In
Proc. of GWC-06, pages 295?299.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9?16.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. of EMNLP-09, pages 286?
295.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proc. of EMNLP-06, pages 534?541.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proc. of CIKM-09, pages 513?522.
Oren Etzioni, Kobi Reiter, Stephen Soderland, and
Marcus Sammer. 2007. Lexical translation with ap-
plication to image search on the Web. In Proceed-
ings of Machine Translation Summit XI.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proc. of ACL-95, pages
236?243.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
Wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proc. of AAAI-06,
pages 1301?1306.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Jim Giles. 2005. Internet encyclopedias go head to
head. Nature, 438:900?901.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL-08,
pages 771?779.
224
Sanda M. Harabagiu, Dan Moldovan, Marius Pas?ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu.
2000. FALCON: Boosting knowledge for answer
engines. In Proc. of TREC-9, pages 479?488.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Comp. Vol. to Proc. of ACL-07, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X.
Els Lefever and Veronique Hoste. 2009. Semeval-
2010 task 3: Cross-lingual Word Sense Disambigua-
tion. In Proc. of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 82?87, Boulder, Colorado.
Lothar Lemnitzer and Claudia Kunze. 2002. Ger-
maNet ? representation, visualization, application.
In Proc. of LREC ?02, pages 1485?1491.
Alessandro Lenci, Nuria Bel, Federica Busa, Nico-
letta Calzolari, Elisabetta Gola, Monica Monachini,
Antoine Ogonowski, Ivonne Peters, Wim Peters,
Nilda Ruimy, Marta Villegas, and Antonio Zam-
polli. 2000. SIMPLE: A general framework for the
development of multilingual lexicons. International
Journal of Lexicography, 13(4):249?263.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of ACL-IJCNLP-
09, pages 262?270.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716?
754.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In
Proceedings of the 3rd DARPA Workshop on Human
Language Technology, pages 303?308, Plainsboro,
N.J.
Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
activation spreading. In Proc. of EMNLP-08, pages
763?772.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study on graph connectivity for unsuper-
vised Word Sense Disambiguation. IEEE Transac-
tions on Pattern Anaylsis and Machine Intelligence,
32(4):678?692.
Roberto Navigli. 2009a. Using cycles and quasi-
cycles to disambiguate dictionary glosses. In Proc.
of EACL-09, pages 594?602.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In Proc. of
ACL-96, pages 40?47.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing an aligned
multilingual database. In Proc. of GWC-02, pages
21?25.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proc. of IJCAI-09,
pages 2083?2088.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rival-
ing supervised system. In Proc. of ACL-10.
Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440?1445.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381?387. College Publications, Lon-
don, England.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lecture
Notes in Computer Science. Springer Verlag.
Marcus Sammer and Stephen Soderland. 2007. Build-
ing a sense-distinguished multilingual lexicon from
monolingual corpora and bilingual lexicons. In Pro-
ceedings of Machine Translation Summit XI.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proc. of COLING-ACL-06, pages 801?
808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Dan Tufis?, Dan Cristea, and Sofia Stamou. 2004.
BalkaNet: Aims, methods, results and perspectives.
a general overview. Romanian Journal on Science
and Technology of Information, 7(1-2):9?43.
Luis von Ahn. 2006. Games with a purpose. IEEE
Computer, 6(39):92?94.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer, Dordrecht, The Netherlands.
Fei Wu and Daniel Weld. 2007. Automatically se-
mantifying Wikipedia. In Proc. of CIKM-07, pages
41?50.
David Yarowsky and Radu Florian. 2002. Evaluat-
ing sense disambiguation across diverse parameter
spaces. Natural Language Engineering, 9(4):293?
310.
Toshio Yokoi. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
225
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318?1327,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Word-Class Lattices for Definition and Hypernym Extraction
Roberto Navigli and Paola Velardi
Dipartimento di Informatica
Sapienza Universita` di Roma
{navigli,velardi}@di.uniroma1.it
Abstract
Definition extraction is the task of au-
tomatically identifying definitional sen-
tences within texts. The task has proven
useful in many research areas including
ontology learning, relation extraction and
question answering. However, current ap-
proaches ? mostly focused on lexico-
syntactic patterns ? suffer from both low
recall and precision, as definitional sen-
tences occur in highly variable syntactic
structures. In this paper, we propose Word-
Class Lattices (WCLs), a generalization of
word lattices that we use to model tex-
tual definitions. Lattices are learned from
a dataset of definitions from Wikipedia.
Our method is applied to the task of def-
inition and hypernym extraction and com-
pares favorably to other pattern general-
ization methods proposed in the literature.
1 Introduction
Textual definitions constitute a fundamental
source to look up when the meaning of a term is
sought. Definitions are usually collected in dictio-
naries and domain glossaries for consultation pur-
poses. However, manually constructing and up-
dating glossaries requires the cooperative effort of
a team of domain experts. Further, in the presence
of new words or usages, and ? even worse ? new
domains, such resources are of no help. Nonethe-
less, terms are attested in texts and some (usually
few) of the sentences in which a term occurs are
typically definitional, that is they provide a formal
explanation for the term of interest. While it is not
feasible to manually search texts for definitions,
this task can be automatized by means of Machine
Learning (ML) and Natural Language Processing
(NLP) techniques.
Automatic definition extraction is useful not
only in the construction of glossaries, but also
in many other NLP tasks. In ontology learning,
definitions are used to create and enrich concepts
with textual information (Gangemi et al, 2003),
and extract taxonomic and non-taxonomic rela-
tions (Snow et al, 2004; Navigli and Velardi,
2006; Navigli, 2009a). Definitions are also har-
vested in Question Answering to deal with ?what
is? questions (Cui et al, 2007; Saggion, 2004).
In eLearning, they are used to help students as-
similate knowledge (Westerhout and Monachesi,
2007), etc.
Much of the current literature focuses on the use
of lexico-syntactic patterns, inspired by Hearst?s
(1992) seminal work. However, these methods
suffer both from low recall and precision, as defi-
nitional sentences occur in highly variable syntac-
tic structures, and because the most frequent def-
initional pattern ? X is a Y ? is inherently very
noisy.
In this paper we propose a generalized form of
word lattices, called Word-Class Lattices (WCLs),
as an alternative to lexico-syntactic pattern learn-
ing. A lattice is a directed acyclic graph (DAG), a
subclass of non-deterministic finite state automata
(NFA). The lattice structure has the purpose of
preserving the salient differences among distinct
sequences, while eliminating redundant informa-
tion. In computational linguistics, lattices have
been used to model in a compact way many se-
quences of symbols, each representing an alter-
native hypothesis. Lattice-based methods differ
in the types of nodes (words, phonemes, con-
cepts), the interpretation of links (representing ei-
ther a sequential or hierarchical ordering between
nodes), their means of creation, and the scor-
ing method used to extract the best consensus
output from the lattice (Schroeder et al, 2009).
In speech processing, phoneme or word lattices
(Campbell et al, 2007; Mathias and Byrne, 2006;
Collins et al, 2004) are used as an interface be-
tween speech recognition and understanding. Lat-
1318
tices are adopted also in Chinese word segmenta-
tion (Jiang et al, 2008), decompounding in Ger-
man (Dyer, 2009), and to represent classes of
translation models in machine translation (Dyer et
al., 2008; Schroeder et al, 2009). In more com-
plex text processing tasks, such as information re-
trieval, information extraction and summarization,
the use of word lattices has been postulated but is
considered unrealistic because of the dimension of
the hypothesis space.
To reduce this problem, concept lattices have
been proposed (Carpineto and Romano, 2005;
Klein, 2008; Zhong et al, 2008). Here links repre-
sent hierarchical relations, rather than the sequen-
tial order of symbols like in word/phoneme lat-
tices, and nodes are clusters of salient words ag-
gregated using synonymy, similarity, or subtrees
of a thesaurus. However, salient word selection
and aggregation is non-obvious and furthermore
it falls into word sense disambiguation, a notori-
ously AI-hard problem (Navigli, 2009b).
In definition extraction, the variability of pat-
terns is higher than for ?traditional? applications
of lattices, such as translation and speech, how-
ever not as high as in unconstrained sentences.
The methodology that we propose to align patterns
is based on the use of star (wildcard *) charac-
ters to facilitate sentence clustering. Each clus-
ter of sentences is then generalized to a lattice of
word classes (each class being either a frequent
word or a part of speech). A key feature of our
approach is its inherent ability to both identify def-
initions and extract hypernyms. The method is
tested on an annotated corpus of Wikipedia sen-
tences and a large Web corpus, in order to demon-
strate the independence of the method from the
annotated dataset. WCLs are shown to general-
ize over lexico-syntactic patterns, and outperform
well-known approaches to definition and hyper-
nym extraction.
The paper is organized as follows: Section 2
discusses related work, WCLs are introduced in
Section 3 and illustrated by means of an example
in Section 4, experiments are presented in Section
5. We conclude the paper in Section 6.
2 Related Work
Definition Extraction. A great deal of work
is concerned with definition extraction in several
languages (Klavans and Muresan, 2001; Storrer
and Wellinghoff, 2006; Gaudio and Branco, 2007;
Iftene et al, 2007; Westerhout and Monachesi,
2007; Przepio?rkowski et al, 2007; Dego?rski et
al., 2008). The majority of these approaches use
symbolic methods that depend on lexico-syntactic
patterns or features, which are manually crafted
or semi-automatically learned (Zhang and Jiang,
2009; Hovy et al, 2003; Fahmi and Bouma, 2006;
Westerhout, 2009). Patterns are either very sim-
ple sequences of words (e.g. ?refers to?, ?is de-
fined as?, ?is a?) or more complex sequences of
words, parts of speech and chunks. A fully au-
tomated method is instead proposed by Borg et
al. (2009): they use genetic programming to learn
simple features to distinguish between definitions
and non-definitions, and then they apply a genetic
algorithm to learn individual weights of features.
However, rules are learned for only one category
of patterns, namely ?is? patterns. As we already
remarked, most methods suffer from both low re-
call and precision, because definitional sentences
occur in highly variable and potentially noisy syn-
tactic structures. Higher performance (around 60-
70% F1-measure) is obtained only for specific do-
mains (e.g., an ICT corpus) and patterns (Borg et
al., 2009).
Only few papers try to cope with the general-
ity of patterns and domains in real-world corpora
(like the Web). In the GlossExtractor web-based
system (Velardi et al, 2008), to improve precision
while keeping pattern generality, candidates are
pruned using more refined stylistic patterns and
lexical filters. Cui et al (2007) propose the use
of probabilistic lexico-semantic patterns, called
soft patterns, for definitional question answering
in the TREC contest1. The authors describe two
soft matching models: one is based on an n-gram
language model (with the Expectation Maximiza-
tion algorithm used to estimate the model param-
eter), the other on Profile Hidden Markov Mod-
els (PHMM). Soft patterns generalize over lexico-
syntactic ?hard? patterns in that they allow a par-
tial matching by calculating a generative degree
of match probability between the test instance and
the set of training instances. Thanks to its gen-
eralization power, this method is the most closely
related to our work, however the task of defini-
tional question answering to which it is applied is
slightly different from that of definition extraction,
so a direct performance comparison is not possi-
1Text REtrieval Conferences: http://trec.nist.
gov
1319
ble2. In fact, the TREC evaluation datasets cannot
be considered true definitions, but rather text frag-
ments providing some relevant fact about a target
term. For example, sentences like: ?Bollywood is
a Bombay-based film industry? and ?700 or more
films produced by India with 200 or more from
Bollywood? are both ?vital? answers for the ques-
tion ?Bollywood?, according to TREC classifica-
tion, but the second sentence is not a definition.
Hypernym Extraction. The literature on hy-
pernym extraction offers a higher variability of
methods, from simple lexical patterns (Hearst,
1992; Oakes, 2005) to statistical and machine
learning techniques (Agirre et al, 2000; Cara-
ballo, 1999; Dolan et al, 1993; Sanfilippo and
Poznan?ski, 1992; Ritter et al, 2009). One of the
highest-coverage methods is proposed by Snow et
al. (2004). They first search sentences that con-
tain two terms which are known to be in a taxo-
nomic relation (term pairs are taken from Word-
Net (Miller et al, 1990)); then they parse the sen-
tences, and automatically extract patterns from the
parse trees. Finally, they train a hypernym clas-
sifer based on these features. Lexico-syntactic pat-
terns are generated for each sentence relating a
term to its hypernym, and a dependency parser is
used to represent them.
3 Word-Class Lattices
3.1 Preliminaries
Notion of definition. In our work, we rely on
a formal notion of textual definition. Specifically,
given a definition, e.g.: ?In computer science, a
closure is a first-class function with free variables
that are bound in the lexical environment?, we as-
sume that it contains the following fields (Storrer
and Wellinghoff, 2006):
? The DEFINIENDUM field (DF): this part of
the definition includes the definiendum (that
is, the word being defined) and its modifiers
(e.g., ?In computer science, a closure?);
? The DEFINITOR field (VF): it includes the
verb phrase used to introduce the definition
(e.g., ?is?);
2In the paper, a 55% recall and 34% precision is achieved
with the best experiment on TREC-13 data. Furthermore, the
classifier of Cui et al (2007) is based on soft patterns but also
on a bag-of-word relevance heuristic. However, the relative
influence of the two methods on the final performance is not
discussed.
? The DEFINIENS field (GF): it includes the
genus phrase (usually including the hyper-
nym, e.g., ?a first-class function?);
? The REST field (RF): it includes additional
clauses that further specify the differentia of
the definiendum with respect to its genus
(e.g., ?with free variables that are bound in
the lexical environment?).
Further examples of definitional sentences an-
notated with the above fields are shown in Table
1. For each sentence, the definiendum (that is, the
word being defined) and its hypernym are marked
in bold and italic, respectively. Given the lexico-
syntactic nature of the definition extraction mod-
els we experiment with, training and test sentences
are part-of-speech tagged with the TreeTagger sys-
tem, a part-of-speech tagger available for many
languages (Schmid, 1995).
Word Classes and Generalized Sentences. We
now introduce our notion of word class, on which
our learning model is based. Let T be the set
of training sentences, manually bracketed with the
DF, VF, GF and RF fields. We first determine the
set F of words in T whose frequency is above a
threshold ? (e.g., the, a, is, of, refer, etc.). In our
training sentences, we replace the term being de-
fined with ?TARGET?, thus this frequent token is
also included in F .
We use the set of frequent words F to generalize
words to ?word classes?. We define a word class
as either a word itself or its part of speech. Given
a sentence s = w1, w2, . . . , w|s|, where wi is the
i-th word of s, we generalize its words wi to word
classes ?i as follows:
?i =
{
wi if wi ? F
POS(wi) otherwise
that is, a word wi is left unchanged if it occurs
frequently in the training corpus (i.e., wi ? F )
or is transformed to its part of speech (POS(wi))
otherwise. As a result, we obtain a general-
ized sentence s? = ?1, ?2, . . . , ?|s|. For instance,
given the first sentence in Table 1, we obtain the
corresponding generalized sentence: ?In NN, a
?TARGET? is a JJ NN?, where NN and JJ indicate
the noun and adjective classes, respectively.
3.2 Algorithm
We now describe our learning algorithm based
on Word-Class Lattices. The algorithm consists of
three steps:
1320
[In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF.
[In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of . . . ]REST.
[In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]REST.
Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italic).
? Star patterns: each sentence in the training
set is pre-processed and generalized to a star
pattern. For instance, ?In arts, a chiaroscuro
is a monochrome picture? is transformed to
?In *, a ?TARGET? is a *? (Section 3.2.1);
? Sentence clustering: the training sentences
are then clustered based on the star patterns
to which they belong (Section 3.2.2);
? Word-Class Lattice construction: for each
sentence cluster, a WCL is created by means
of a greedy alignment algorithm (Section
3.2.3).
We present two variants of our WCL model,
dealing either globally with the entire sentence or
separately with its definition fields (Section 3.2.4).
The WCL models can then be used to classify any
input sentence of interest (Section 3.2.5).
3.2.1 Star Patterns
Let T be the set of training sentences. In this step,
we associate a star pattern ?(s) with each sentence
s ? T . To do so, let s ? T be a sentence such that
s = w1, w2, . . . , w|s|, where wi is its i-th word.
Given the set F of most frequent words in T (cf.
Section 3.1), the star pattern ?(s) associated with
s is obtained by replacing with * all the words
wi 6? F , that is all the tokens that are non-frequent
words. For instance, given the sentence ?In arts,
a chiaroscuro is a monochrome picture?, the cor-
responding star pattern is ?In *, a ?TARGET? is a
*?, where ?TARGET? is the defined term.
Note that, here and in what follows, we discard
the sentence fragments tagged with the REST field,
which is used only to delimit the core part of defi-
nitional sentences.
3.2.2 Sentence Clustering
In the second step, we cluster the sentences in our
training set T based on their star patterns. For-
mally, let ? = (?1, . . . , ?m) be the set of star
patterns associated with the sentences in T . We
create a clustering C = (C1, . . . , Cm) such that
Ci = {s ? T : ?(s) = ?i}, that is Ci contains all
the sentences whose star pattern is ?i.
As an example, assume ?3 = ?In *, a
?TARGET? is a *?. The sentences reported in Ta-
ble 1 are all grouped into cluster C3. We note that
each cluster Ci contains sentences whose degree
of variability is generally much lower than for any
pair of sentences in T belonging to two different
clusters.
3.2.3 Word-Class Lattice Construction
Finally, the third step consists of the construction
of a Word-Class Lattice for each sentence cluster.
Given such a cluster Ci ? C, we apply a greedy
algorithm that iteratively constructs the WCL.
Let Ci = {s1, s2, . . . , s|Ci|} and consider
its first sentence s1 = w11, w
1
2, . . . , w
1
|s1|
(wji
denotes the i-th token of the j-th sentence).
We first produce the corresponding general-
ized sentence s?1 = ?
1
1, ?
1
2, . . . , ?
1
|s1|
(cf. Sec-
tion 3.1). We then create a directed graph
G = (V,E) such that V = {?11, . . . , ?
1
|s1|
} and
E = {(?11, ?
1
2), (?
1
2, ?
1
3), . . . , (?
1
|s1|?1
, ?1|s1|)}.
Next, for the subsequent sentences in Ci, that
is, for each j = 2, . . . , |Ci|, we determine the
alignment between the sentence sj and each
sentence sk ? Ci such that k < j based on the
following dynamic programming formulation
(Cormen et al, 1990, pp. 314?319):
Ma,b = max {Ma?1,b?1 +Sa,b,Ma,b?1,Ma?1,b}
where a ? {1, . . . , |sk|} and b ? {1, . . . , |sj |},
Sa,b is a score of the matching between the a-th
token of sk and the b-th token of sj , and M0,0,
M0,b and Ma,0 are initially set to 0 for all a and b.
The matching score Sa,b is calculated on the
generalized sentences s?k of sk and s
?
j of sj as fol-
lows:
Sa,b =
{
1 if ?ka = ?
j
b
0 otherwise
where ?ka and ?
j
b are the a-th and b-th word classes
of s?k and s
?
j , respectively. In other words, the
matching score equals 1 if the a-th and the b-th
tokens of the two original sentences have the same
word class.
Finally, the alignment score between sk and sj
is given by M|sk|,|sj |, which calculates the mini-
1321
In
arts
science
mathematics
NN1
NN4
computer
, a ?TARGET?
pixel
graph
chiaroscuro
is a
monochrome
JJ NN2
structure
picture
dot
NN3
data
Figure 1: The Word-Class Lattice for the sentences in Table 1. The support of each word class is reported
beside the corresponding node.
mal number of misalignments between the two to-
ken sequences. We repeat this calculation for each
sentence sk (k = 1, . . . , j ? 1) and choose the
one that maximizes its alignment score with sj .
We then use the best alignment to add sj to the
graph G. Such alignment is obtained by means
of backtracking from M|sk|,|sj | to M0,0. We add
to the set of vertices V the tokens of the gen-
eralized sentence s?j for which there is no align-
ment to s?k and we add to E the edges (?
j
1, ?
j
2),
. . . , (?j|sj |?1, ?
j
|sj |
). Furthermore, in the final lat-
tice, nodes associated with the hypernym words in
the learning sentences are marked as hypernyms
in order to be able to determine the hypernym of a
test sentence at classification time.
3.2.4 Variants of the WCL Model
So far, we have assumed that our WCL model
learns lattices from the training sentences in
their entirety (we call this model WCL-1). We
now propose a second model that learns separate
WCLs for each field of the definition, namely:
the DEFINIENDUM (DF), DEFINITOR (VF) and
DEFINIENS (GF) fields (see Section 3.1). We re-
fer to this latter model as WCL-3. Rather than ap-
plying the WCL algorithm to the entire sentence,
the very same method is applied to the sentence
fragments tagged with one of the three definition
fields. The reason for introducing the WCL-3
model is that, while definitional patterns are highly
variable, DF, VF and GF individually exhibit a
lower variability, thus WCL-3 should improve the
generalization power.
3.2.5 Classification
Once the learning process is over, a set of WCLs is
produced. Given a test sentence s, the classifica-
tion phase for the WCL-1 model consists of deter-
mining whether it exists a lattice that matches s. In
the case of WCL-3, we consider any combination
of DEFINIENDUM, DEFINITOR and DEFINIENS
lattices. While WCL-1 is applied as a yes-no clas-
sifier as there is a single WCL that can possibly
match the input sentence, WCL-3 selects, if any,
the combination of the three WCLs that best fits
the sentence. In fact, choosing the most appro-
priate combination of lattices impacts the perfor-
mance of hypernym extraction. The best combi-
nation of WCLs is selected by maximizing the fol-
lowing confidence score:
score(s, lDF, lVF, lGF) = coverage ? log(support)
where s is the candidate sentence, lDF, lVF and lGF
are three lattices one for each definition field, cov-
erage is the fraction of words of the input sentence
covered by the three lattices, and support is the
sum of the number of sentences in the star patterns
corresponding to the three lattices.
Finally, when a sentence is classified as a def-
inition, its hypernym is extracted by selecting the
words in the input sentence that are marked as ?hy-
pernyms? in the WCL-1 lattice (or in the WCL-3
GF lattice).
4 Example
As an example, consider the definitions in Table
1. As illustrated in Section 3.2.2, their star pat-
tern is ?In *, a ?TARGET? is a *?. The corre-
sponding WCL is built as follows: the first part-
of-speech tagged sentence, ?In/IN arts/NN , a/DT
?TARGET?/NN is/VBZ a/DT monochrome/JJ pic-
ture/NN?, is considered. The corresponding gen-
eralized sentence is ?In NN , a ?TARGET? is a
JJ NN?. The initially empty graph is thus popu-
lated with one node for each word class and one
edge for each pair of consecutive tokens, as shown
in Figure 1 (the central sequence of nodes in the
graph). Note that we draw the hypernym token
NN2 with a rectangle shape. We also add to the
1322
graph a start node ? and an end node ??, and con-
nect them to the corresponding initial and final
sentence tokens. Next, the second sentence, ?In
mathematics, a graph is a data structure that con-
sists of...?, is aligned to the first sentence. The
alignment of the generalized sentence is perfect,
apart from the NN3 node corresponding to ?data?.
The node is added to the graph together with the
edges a? NN3 and NN3 ? NN2 . Finally, the
third sentence in Table 1, ?In computer science, a
pixel is a dot that is part of a computer image?,
is generalized as ?In NN NN , a ?TARGET? is
a NN?. Thus, a new node NN4 is added, corre-
sponding to ?computer? and new edges are added:
In?NN4 and NN4?NN1. Figure 1 shows the re-
sulting WCL-1 lattice.
5 Experiments
5.1 Experimental Setup
Datasets. We conducted experiments on two
different datasets:
? A corpus of 4,619 Wikipedia sentences, that
contains 1,908 definitional and 2,711 non-
definitional sentences. The former were ob-
tained from a random selection of the first
sentences of Wikipedia articles3. The de-
fined terms belong to different Wikipedia
domain categories4, so as to capture a
representative and cross-domain sample of
lexical and syntactic patterns for defini-
tions. These sentences were manually an-
notated with DEFINIENDUM, DEFINITOR,
DEFINIENS and REST fields by an expert
annotator, who also marked the hypernyms.
The associated set of negative examples
(?syntactically plausible? false definitions)
was obtained by extracting from the same
Wikipedia articles sentences in which the
page title occurs.
? A subset of the ukWaC Web corpus (Fer-
raresi et al, 2008), a large corpus of the En-
glish language constructed by crawling the
.uk domain of the Web. The subset includes
over 300,000 sentences in which occur any
of 239 terms selected from the terminology
of four different domains (COMPUTER SCI-
3The first sentence of Wikipedia entries is, in the large
majority of cases, a definition of the page title.
4en.wikipedia.org/wiki/Wikipedia:Cate-
gories
ENCE, ASTRONOMY, CARDIOLOGY, AVIA-
TION).
The reason for using the ukWaC corpus is that, un-
like the ?clean? Wikipedia dataset, in which rel-
atively simple patterns can achieve good results,
ukWaC represents a real-world test, with many
complex cases. For example, there are sentences
that should be classified as definitional according
to Section 3.1 but are rather uninformative, like
?dynamic programming was the brainchild of an
american mathematician?, as well as informative
sentences that are not definitional (e.g., they do not
have a hypernym), like ?cubism was characterised
by muted colours and fragmented images?. Even
more frequently, the dataset includes sentences
which are not definitions but have a definitional
pattern (?A Pacific Northwest tribe?s saga refers to
a young woman who [..]?), or sentences with very
complex definitional patterns (?white body cells
are the body?s clean up squad? and ?joule is also
an expression of electric energy?). These cases can
be correctly handled only with fine-grained pat-
terns. Additional details on the corpus and a more
thorough linguistic analysis of complex cases can
be found in Navigli et al (2010).
Systems. For definition extraction, we experi-
ment with the following systems:
? WCL-1 and WCL-3: these two classifiers
are based on our Word-Class Lattice model.
WCL-1 learns from the training set a lattice
for each cluster of sentences, whereas WCL-
3 identifies clusters (and lattices) separately
for each sentence field (DEFINIENDUM,
DEFINITOR and DEFINIENS) and classifies a
sentence as a definition if any combination
from the three sets of lattices matches (cf.
Section 3.2.4, the best combination is se-
lected).
? Star patterns: a simple classifier based on
the patterns learned as a result of step 1 of our
WCL learning algorithm (cf. Section 3.2.1):
a sentence is classified as a definition if it
matches any of the star patterns in the model.
? Bigrams: an implementation of the bigram
classifier for soft pattern matching proposed
by Cui et al (2007). The classifier selects as
definitions all the sentences whose probabil-
ity is above a specific threshold. The proba-
bility is calculated as a mixture of bigram and
1323
Algorithm P R F1 A
WCL-1 99.88 42.09 59.22 76.06
WCL-3 98.81 60.74 75.23 83.48
Star patterns 86.74 66.14 75.05 81.84
Bigrams 66.70 82.70 73.84 75.80
Random BL 50.00 50.00 50.00 50.00
Table 2: Performance on the Wikipedia dataset.
unigram probabilities, with Laplace smooth-
ing on the latter. We use the very same set-
tings of Cui et al (2007), including threshold
values. While the authors propose a second
soft-pattern approach based on Profile HMM
(cf. Section 2), their results do not show sig-
nificant improvements over the bigram lan-
guage model.
For hypernym extraction, we compared WCL-
1 and WCL-3 with Hearst?s patterns, a system
that extracts hypernyms from sentences based on
the lexico-syntactic patterns specified in Hearst?s
seminal work (1992). These include (hypernym
in italic): ?such NP as {NP ,} {(or | and)} NP?,
?NP {, NP} {,} or other NP?, ?NP {,} includ-
ing { NP ,} {or | and} NP?, ?NP {,} especially {
NP ,} {or | and} NP?, and variants thereof. How-
ever, it should be noted that hypernym extraction
methods in the literature do not extract hypernyms
from definitional sentences, like we do, but rather
from specific patterns like ?X such as Y?. There-
fore a direct comparison with these methods is not
possible. Nonetheless, we decided to implement
Hearst?s patterns for the sake of completeness. We
could not replicate the more refined approach by
Snow et al (2004) because it requires the annota-
tion of a possibly very large dataset of sentence
fragments. In any case Snow et al (2004) re-
ported the following performance figures on a cor-
pus of dimension and complexity comparable with
ukWaC: the recall-precision graph indicates preci-
sion 85% at recall 10% and precision 25% at re-
call of 30% for the hypernym classifier. A variant
of the classifier that includes evidence from coor-
dinate terms (terms with a common ancestor in a
taxonomy) obtains an increased precision of 35%
at recall 30%. We see no reasons why these figures
should vary dramatically on the ukWaC.
Finally, we compare all systems with the ran-
dom baseline, that classifies a sentence as a defi-
nition with probability 12 .
Algorithm P R?
WCL-1 98.33 39.39
WCL-3 94.87 56.57
Star patterns 44.01 63.63
Bigrams 46.60 45.45
Random BL 50.00 50.00
Table 3: Performance on the ukWaC dataset (? Re-
call is estimated).
Measures. To assess the performance of our
systems, we calculated the following measures:
? precision ? the number of definitional sen-
tences correctly retrieved by the system over
the number of sentences marked by the sys-
tem as definitional.
? recall ? the number of definitional sen-
tences correctly retrieved by the system over
the number of definitional sentences in the
dataset.
? the F1-measure ? a harmonic mean of preci-
sion (P) and recall (R) given by 2PRP+R .
? accuracy ? the number of correctly classi-
fied sentences (either as definitional or non-
definitional) over the total number of sen-
tences in the dataset.
5.2 Results and Discussion
Definition Extraction. In Table 2 we report
the results of definition extraction systems on the
Wikipedia dataset. Given this dataset is also used
for training, experiments are performed with 10-
fold cross validation. The results show very high
precision for WCL-1, WCL-3 (around 99%) and
star patterns (86%). As expected, bigrams and star
patterns exhibit a higher recall (82% and 66%, re-
spectively). The lower recall of WCL-1 is due to
its limited ability to generalize compared to WCL-
3 and the other methods. In terms of F1-measure,
star patterns and WCL-3 achieve 75%, and are
thus the best systems. Similar performance is ob-
served when we also account for negative sen-
tences ? that is we calculate accuracy (with WCL-
3 performing better). All the systems perform sig-
nificantly better than the random baseline.
From our Wikipedia corpus, we learned over
1,000 lattices (and star patterns). Using WCL-
3, we learned 381 DF, 252 VF and 395 GF lat-
tices, that then we used to extract definitions from
1324
Algorithm Full Substring
WCL-1 42.75 77.00
WCL-3 40.73 78.58
Table 4: Precision in hypernym extraction on the
Wikipedia dataset
the ukWaC dataset. To calculate precision on this
dataset, we manually validated the definitions out-
put by each system. However, given the large size
of the test set, recall could only be estimated. To
this end, we manually analyzed 50,000 sentences
and identified 99 definitions, against which recall
was calculated. The results are shown in Table 3.
On the ukWaC dataset, WCL-3 performs best, ob-
taining 94.87% precision and 56.57% recall (we
did not calculate F1, as recall is estimated). In-
terestingly, star patterns obtain only 44% preci-
sion and around 63% recall. Bigrams achieve
even lower performance, namely 46.60% preci-
sion, 45.45% recall. The reason for such bad
performance on ukWaC is due to the very dif-
ferent nature of the two datasets: for example, in
Wikipedia most ?is a? sentences are definitional,
whereas this property is not verified in the real
world (that is, on the Web, of which ukWaC is
a sample). Also, while WCL does not need any
parameter tuning5, the same does not hold for bi-
grams6, whose probability threshold and mixture
weights need to be best tuned on the task at hand.
Hypernym Extraction. For hypernym extrac-
tion, we tested WCL-1, WCL-3 and Hearst?s pat-
terns. Precision results are reported in Tables 4
and 5 for the two datasets, respectively. The Sub-
string column refers to the case in which the cap-
tured hypernym is a substring of what the annota-
tor considered to be the correct hypernym. Notice
that this is a complex matter, because often the se-
lection of a hypernym depends on semantic and
contextual issues. For example, ?Fluoroscopy is
an imaging method? and ?the Mosaic was an in-
teresting project? have precisely the same genus
pattern, but (probably depending on the vagueness
of the noun in the first sentence, and of the adjec-
tive in the second) the annotator selected respec-
5WCL has only one threshold value ? to be set for deter-
mining frequent words (cf. Section 3.1). However, no tuning
was made for choosing the best value of ?.
6We had to re-tune the system parameters on ukWaC,
since with the original settings of Cui et al (2007) perfor-
mance was much lower.
Algorithm Full Substring
WCL-1 86.19 (206) 96.23 (230)
WCL-3 89.27 (383) 96.27 (413)
Hearst 65.26 (62) 88.42 (84)
Table 5: Precision in hypernym extraction on the
ukWaC dataset (number of hypernyms in paren-
theses).
tively imaging method and project as hypernyms.
For the above reasons it is difficult to achieve high
performance in capturing the correct hypernym
(e.g. 40.73% with WCL-3 on Wikipedia). How-
ever, our performance of identifying a substring
of the correct hypernym is much higher (around
78.58%). In Table 4 we do not report the preci-
sion of Hearst?s patterns, as only one hypernym
was found, due to the inherently low coverage of
the method.
On the ukWaC dataset, the hypernyms returned
by the three systems were manually validated and
precision was calculated. Both WCL-1 and WCL-
3 obtained a very high precision (86-89% and 96%
in identifying the exact hypernym and a substring
of it, respectively). Both WCL models are thus
equally robust in identifying hypernyms, whereas
WCL-1 suffers from a lack of generalization in
definition extraction (cf. Tables 2 and 3). Also,
given that the ukWaC dataset contains sentences
in which any of 239 domain terms occur, WCL-3
extracts on average 1.6 and 1.7 full and substring
hypernyms per term, respectively. Hearst?s pat-
terns also obtain high precision, especially when
substrings are taken into account. However, the
number of hypernyms returned by this method is
much lower, due to the specificity of the patterns
(62 vs. 383 hypernyms returned by WCL-3).
6 Conclusions
In this paper, we have presented a lattice-based ap-
proach to definition and hypernym extraction. The
novelty of our approach is:
1. The use of a lattice structure to generalize
over lexico-syntactic definitional patterns;
2. The ability of the system to jointly identify
definitions and extract hypernyms;
3. The generality of the method, which applies
to generic Web documents in any domain and
style, and needs no parameter tuning;
1325
4. The high performance as compared with the
best-known methods for both definition and
hypernym extraction. Our approach outper-
forms the other systems particularly where
the task is more complex, as in real-world
documents (i.e., the ukWaC corpus).
Even though definitional patterns are learned
from a manually annotated dataset, the dimension
and heterogeneity of the training dataset ensures
that training needs not to be repeated for specific
domains7, as demonstrated by the cross-domain
evaluation on the ukWaC corpus.
The datasets used in our experiments are avail-
able from http://lcl.uniroma1.it/wcl.
We also plan to release our system to the research
community. In the near future, we aim to apply the
output of our classifiers to the task of automated
taxonomy building, and to test the WCL approach
on other information extraction tasks, like hyper-
nym extraction from generic sentence fragments,
as in Snow et al (2004).
References
Eneko Agirre, Ansa Olatz, Xabier Arregi, Xabier Ar-
tola, Arantza Daz de Ilarraza Snchez, Mikel Ler-
sundi, David Martnez, Kepa Sarasola, and Ruben
Urizar. 2000. Extraction of semantic relations from
a basque monolingual dictionary using constraint
grammar. In Proceedings of Euralex.
Claudia Borg, Mike Rosner, and Gordon Pace. 2009.
Evolutionary algorithms for definition extraction. In
Proceedings of the 1st Workshop on Definition Ex-
traction 2009 (wDE?09).
William M. Campbell, M. F. Richardson, and D. A.
Reynolds. 2007. Language recognition with word
lattices and support vector machines. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP 2007),
pages 989?992, Honolulu, HI.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
120?126, Maryland, USA.
Claudio Carpineto and Giovanni Romano. 2005. Us-
ing concept lattices for text retrieval and mining. In
B. Ganter, G. Stumme, and R. Wille, editors, Formal
Concept Analysis, pages 161?179.
Christopher Collins, Bob Carpenter, and Gerald Penn.
2004. Head-driven parsing for word lattices. In Pro-
ceedings of the 42nd Meeting of the Association for
7Of course, it would need some additional work if applied
to languages other than English. However, the approach does
not need to be adapted to the language of interest.
Computational Linguistics (ACL?04), Main Volume,
pages 231?238, Barcelona, Spain, July.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to algorithms.
the MIT Electrical Engineering and Computer Sci-
ence Series. MIT Press, Cambridge, MA.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Transactions on Information
Systems, 25(2):8.
?ukasz Dego?rski, Micha? Marcinczuk, and Adam
Przepio?rkowski. 2008. Definition extraction us-
ing a sequential combination of baseline grammars
and machine learning classifiers. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco.
William Dolan, Lucy Vanderwende, and Stephen D.
Richardson. 1993. Automatically deriving struc-
tured knowledge bases from on-line dictionaries. In
Proceedings of the First Conference of the Pacific
Association for Computational Linguistics, pages 5?
14.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2008),
pages 1012?1020, Columbus, Ohio, USA.
Christopher Dyer. 2009. Using a maximum en-
tropy model to build segmentation lattices for mt.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL 2009), pages 406?414, Boul-
der, Colorado, USA.
Ismail Fahmi and Gosse Bouma. 2006. Learning to
identify definitions using syntactic features. In Pro-
ceedings of the EACL 2006 workshop on Learning
Structured Information in Natural Language Appli-
cations, pages 64?71, Trento, Italy.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large Web-derived corpus of english.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4), Marrakech, Morocco.
Aldo Gangemi, Roberto Navigli, and Paola Velardi.
2003. The OntoWordNet project: Extension and ax-
iomatization of conceptual relations in WordNet. In
Proceedings of the International Conference on On-
tologies, Databases and Applications of SEmantics
(ODBASE 2003), pages 820?838, Catania, Italy.
Rosa Del Gaudio and Anto?nio Branco. 2007. Auto-
matic extraction of definitions in portuguese: A rule-
based approach. In Proceedings of the TeMa Work-
shop.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceed-
ings of the 14th International Conference on Com-
putational Linguistics (COLING), pages 539?545,
Nantes, France.
1326
Eduard Hovy, Andrew Philpot, Judith Klavans, Ulrich
Germann, and Peter T. Davis. 2003. Extending
metadata definitions by automatically extracting and
organizing glossary definitions. In Proceedings of
the 2003 Annual National Conference on Digital
Government Research, pages 1?6. Digital Govern-
ment Society of North America.
Adrian Iftene, Diana Trandaba?, and Ionut Pistol. 2007.
Natural language processing and knowledge repre-
sentation for elearning environments. In Proc. of
Applications for Romanian. Proceedings of RANLP
workshop, pages 19?25.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word
lattice reranking for chineseword segmentation and
part-of-speech tagging. In Proceedings of the 22nd
International Conference on Computational Lin-
guistics (COLING 2008), pages 385?392, Manch-
ester, UK.
Judith Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully auto-
matic glossary construction. In Proc. of the Amer-
ican Medical Informatics Association (AMIA) Sym-
posium.
Michael Tully Klein. 2008. Understanding English
with Lattice-Learning, Master thesis. MIT, Cam-
bridge, MA, USA.
Lambert Mathias and William Byrne. 2006. Statis-
tical phrase-based speech translation. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP 2006),
Toulouse, France.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet:
an online lexical database. International Journal of
Lexicography, 3(4):235?244.
Roberto Navigli and Paola Velardi. 2006. Ontology
enrichment through automatic semantic annotation
of on-line glossaries. In Proceedings of the 15th In-
ternational Conference on Knowledge Engineering
and Knowledge Management (EKAW 2006), pages
126?140, Podebrady, Czech Republic.
Roberto Navigli, Paola Velardi, and Juana Mar??a Ruiz-
Mart??nez. 2010. An annotated dataset for extract-
ing definitions and hypernyms from the Web. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC 2010),
Valletta, Malta.
Roberto Navigli. 2009a. Using cycles and quasi-cycles
to disambiguate dictionary glosses. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2009), pages 594?602, Athens, Greece.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Michael P. Oakes. 2005. Using hearst?s rules for
the automatic acquisition of hyponyms for mining a
pharmaceutical corpus. In Proceedings of the Work-
shop Text Mining Research.
Adam Przepio?rkowski, Lukasz Dego?rski, Beata
Wo?jtowicz, Miroslav Spousta, Vladislav Kubon?,
Kiril Simov, Petya Osenova, and Lothar Lemnitzer.
2007. Towards the automatic extraction of defini-
tions in slavic. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing (in
ACL ?07), pages 43?50, Prague, Czech Republic.
Association for Computational Linguistics.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88?93.
Horacio Saggion. 2004. Identifying denitions in text
collections for question answering. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation (LREC 2004), Lisbon,
Portugal.
Antonio Sanfilippo and Victor Poznan?ski. 1992. The
acquisition of lexical knowledge from combined
machine-readable dictionary sources. In Proceed-
ings of the third Conference on Applied Natural Lan-
guage Processing, pages 80?87.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation. In
Proceedings of the European Chapter of the Asso-
ciation for Computation Linguistics (EACL 2009),
pages 719?727, Athens, Greece.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of Advances in Neural
Information Processing Systems, pages 1297?1304.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
german text corpora. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation (LREC 2006), Genova, Italy.
Paola Velardi, Roberto Navigli, and Pierluigi
D?Amadio. 2008. Mining the Web to create
specialized glossaries. IEEE Intelligent Systems,
23(5):18?25.
Eline Westerhout and Paola Monachesi. 2007. Extrac-
tion of dutch definitory contexts for eLearning pur-
poses. In Proceedings of CLIN.
Eline Westerhout. 2009. Definition extraction using
linguistic and structural features. In Proceedings
of the RANLP 2009 Workshop on Definition Extrac-
tion, pages 61?67.
Chunxia Zhang and Peng Jiang. 2009. Automatic ex-
traction of definitions. In Proceedings of 2nd IEEE
International Conference on Computer Science and
Information Technology, pages 364?368.
Zhao-man Zhong, Zong-tian Liu, and Yan Guan. 2008.
Precise information extraction from text based on
two-level concept lattice. In Proceedings of the
2008 International Symposiums on Information Pro-
cessing (ISIP ?08), pages 275?279, Washington,
DC, USA.
1327
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522?1531,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems
Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University
ponzetto@cl.uni-heidelberg.de
Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
navigli@di.uniroma1.it
Abstract
One of the main obstacles to high-
performance Word Sense Disambigua-
tion (WSD) is the knowledge acquisi-
tion bottleneck. In this paper, we present
a methodology to automatically extend
WordNet with large amounts of seman-
tic relations from an encyclopedic re-
source, namely Wikipedia. We show
that, when provided with a vast amount
of high-quality semantic relations, sim-
ple knowledge-lean disambiguation algo-
rithms compete with state-of-the-art su-
pervisedWSD systems in a coarse-grained
all-words setting and outperform them on
gold-standard domain-specific datasets.
1 Introduction
Knowledge lies at the core of Word Sense Dis-
ambiguation (WSD), the task of computation-
ally identifying the meanings of words in context
(Navigli, 2009b). In the recent years, two main
approaches have been studied that rely on a fixed
sense inventory, i.e., supervised and knowledge-
based methods. In order to achieve high perfor-
mance, supervised approaches require large train-
ing sets where instances (target words in con-
text) are hand-annotated with the most appropri-
ate word senses. Producing this kind of knowl-
edge is extremely costly: at a throughput of one
sense annotation per minute (Edmonds, 2000)
and tagging one thousand examples per word,
dozens of person-years would be required for en-
abling a supervised classifier to disambiguate all
the words in the English lexicon with high accu-
racy. In contrast, knowledge-based approaches ex-
ploit the information contained in wide-coverage
lexical resources, such as WordNet (Fellbaum,
1998). However, it has been demonstrated that
the amount of lexical and semantic information
contained in such resources is typically insuffi-
cient for high-performance WSD (Cuadros and
Rigau, 2006). Several methods have been pro-
posed to automatically extend existing resources
(cf. Section 2) and it has been shown that highly-
interconnected semantic networks have a great im-
pact on WSD (Navigli and Lapata, 2010). How-
ever, to date, the real potential of knowledge-rich
WSD systems has been shown only in the presence
of either a large manually-developed extension of
WordNet (Navigli and Velardi, 2005) or sophisti-
cated WSD algorithms (Agirre et al, 2009).
The contributions of this paper are two-fold.
First, we relieve the knowledge acquisition bot-
tleneck by developing a methodology to extend
WordNet with millions of semantic relations. The
relations are harvested from an encyclopedic re-
source, namely Wikipedia. Wikipedia pages are
automatically associated with WordNet senses,
and topical, semantic associative relations from
Wikipedia are transferred to WordNet, thus pro-
ducing a much richer lexical resource. Sec-
ond, two simple knowledge-based algorithms that
exploit our extended WordNet are applied to
standard WSD datasets. The results show that
the integration of vast amounts of semantic re-
lations in knowledge-based systems yields per-
formance competitive with state-of-the-art super-
vised approaches on open-text WSD. In addition,
we support previous findings from Agirre et al
(2009) that in a domain-specific WSD scenario
knowledge-based systems perform better than su-
pervised ones, and we show that, given enough
knowledge, simple algorithms perform better than
more sophisticated ones.
2 Related Work
In the last three decades, a large body of work
has been presented that concerns the develop-
ment of automatic methods for the enrichment of
existing resources such as WordNet. These in-
1522
clude proposals to extract semantic information
from dictionaries (e.g. Chodorow et al (1985)
and Rigau et al (1998)), approaches using lexico-
syntactic patterns (Hearst, 1992; Cimiano et al,
2004; Girju et al, 2006), heuristic methods based
on lexical and semantic regularities (Harabagiu et
al., 1999), taxonomy-based ontologization (Pen-
nacchiotti and Pantel, 2006; Snow et al, 2006).
Other approaches include the extraction of seman-
tic preferences from sense-annotated (Agirre and
Martinez, 2001) and raw corpora (McCarthy and
Carroll, 2003), as well as the disambiguation of
dictionary glosses based on cyclic graph patterns
(Navigli, 2009a). Other works rely on the dis-
ambiguation of collocations, either obtained from
specialized learner?s dictionaries (Navigli and Ve-
lardi, 2005) or extracted by means of statistical
techniques (Cuadros and Rigau, 2008), e.g. based
on the method proposed by Agirre and de Lacalle
(2004). But while most of these methods represent
state-of-the-art proposals for enriching lexical and
taxonomic resources, none concentrates on aug-
menting WordNet with associative semantic rela-
tions for many domains on a very large scale. To
overcome this limitation, we exploit Wikipedia, a
collaboratively generated Web encyclopedia.
The use of collaborative contributions from vol-
unteers has been previously shown to be beneficial
in the Open Mind Word Expert project (Chklovski
and Mihalcea, 2002). However, its current status
indicates that the project remains a mainly aca-
demic attempt. In contrast, due to its low en-
trance barrier and vast user base, Wikipedia pro-
vides large amounts of information at practically
no cost. Previous work aimed at transforming
its content into a knowledge base includes open-
domain relation extraction (Wu and Weld, 2007),
the acquisition of taxonomic (Ponzetto and Strube,
2007a; Suchanek et al, 2008; Wu andWeld, 2008)
and other semantic relations (Nastase and Strube,
2008), as well as lexical reference rules (Shnarch
et al, 2009). Applications using the knowledge
contained in Wikipedia include, among others,
text categorization (Gabrilovich and Markovitch,
2006), computing semantic similarity of texts
(Gabrilovich and Markovitch, 2007; Ponzetto and
Strube, 2007b; Milne and Witten, 2008a), coref-
erence resolution (Ponzetto and Strube, 2007b),
multi-document summarization (Nastase, 2008),
and text generation (Sauper and Barzilay, 2009).
In our work we follow this line of research and
show that knowledge harvested from Wikipedia
can be used effectively to improve the perfor-
mance of a WSD system. Our proposal builds on
previous insights from Bunescu and Pas?ca (2006)
and Mihalcea (2007) that pages in Wikipedia can
be taken as word senses. Mihalcea (2007) manu-
ally maps Wikipedia pages to WordNet senses to
perform lexical-sample WSD. We extend her pro-
posal in three important ways: (1) we fully autom-
atize the mapping between Wikipedia pages and
WordNet senses; (2) we use the mappings to en-
rich an existing resource, i.e. WordNet, rather than
annotating text with sense labels; (3) we deploy
the knowledge encoded by this mapping to per-
form unrestricted WSD, rather than apply it to a
lexical sample setting.
Knowledge from Wikipedia is injected into a
WSD system by means of a mapping to Word-
Net. Previous efforts aimed at automatically link-
ing Wikipedia to WordNet include full use of the
first WordNet sense heuristic (Suchanek et al,
2008), a graph-based mapping of Wikipedia cat-
egories to WordNet synsets (Ponzetto and Nav-
igli, 2009), a model based on vector spaces (Ruiz-
Casado et al, 2005) and a supervised approach
using keyword extraction (Reiter et al, 2008).
These latter methods rely only on text overlap
techniques and neither they take advantage of the
input from Wikipedia being semi-structured, e.g.
hyperlinked, nor they propose a high-performing
probabilistic formulation of the mapping problem,
a task to which we turn in the next section.
3 Extending WordNet
Our approach consists of two main phases: first,
a mapping is automatically established between
Wikipedia pages and WordNet senses; second, the
relations connecting Wikipedia pages are trans-
ferred to WordNet. As a result, an extended ver-
sion of WordNet is produced, that we call Word-
Net++. We present the two resources used in our
methodology in Section 3.1. Sections 3.2 and 3.3
illustrate the two phases of our approach.
3.1 Knowledge Resources
WordNet. Being the most widely used compu-
tational lexicon of English in Natural Language
Processing, WordNet is an essential resource for
WSD. A concept in WordNet is represented as a
synonym set, or synset, i.e. the set of words which
share a common meaning. For instance, the con-
1523
cept of soda drink is expressed as:
{ pop2n, soda2n, soda pop1n, soda water2n, tonic2n }
where each word?s subscripts and superscripts in-
dicate their parts of speech (e.g. n stands for noun)
and sense number1, respectively. For each synset,
WordNet provides a textual definition, or gloss.
For example, the gloss of the above synset is: ?a
sweet drink containing carbonated water and fla-
voring?.
Wikipedia. Our second resource, Wikipedia, is
a collaborative Web encyclopedia composed of
pages2. A Wikipedia page (henceforth, Wikipage)
presents the knowledge about a specific concept
(e.g. SODA (SOFT DRINK)) or named entity (e.g.
FOOD STANDARDS AGENCY). The page typi-
cally contains hypertext linked to other relevant
Wikipages. For instance, SODA (SOFT DRINK)
is linked to COLA, FLAVORED WATER, LEMON-
ADE, and many others. The title of a Wikipage
(e.g. SODA (SOFT DRINK)) is composed of the
lemma of the concept defined (e.g. soda) plus
an optional label in parentheses which specifies
its meaning in case the lemma is ambiguous
(e.g. SOFT DRINK vs. SODIUM CARBONATE). Fi-
nally, some Wikipages are redirections to other
pages, e.g. SODA (SODIUM CARBONATE) redirects
to SODIUM CARBONATE.
3.2 Mapping Wikipedia to WordNet
During the first phase of our methodology we aim
to establish links between Wikipages and Word-
Net senses. Formally, given the entire set of pages
SensesWiki and WordNet senses SensesWN, we aim
to acquire a mapping:
? : SensesWiki ? SensesWN,
such that, for each Wikipage w ? SensesWiki:
?(w) =
?
??
??
s ? SensesWN(w) if a link can be
established,
 otherwise,
where SensesWN(w) is the set of senses of the
lemma of w in WordNet. For example, if our
1We use WordNet version 3.0. We use word senses to un-
ambiguously denote the corresponding synsets (e.g. plane1n
for { airplane1n, aeroplane1n, plane1n }).
2http://download.wikipedia.org. We use the
English Wikipedia database dump from November 3, 2009,
which includes 3,083,466 articles. Throughout this paper, we
use Sans Serif for words, SMALL CAPS for Wikipedia pages
and CAPITALS for Wikipedia categories.
mapping methodology linked SODA (SOFT DRINK)
to the corresponding WordNet sense soda2n, we
would have ?(SODA (SOFT DRINK)) = soda2n.
In order to establish a mapping between the
two resources, we first identify different kinds of
disambiguation contexts for Wikipages (Section
3.2.1) and WordNet senses (Section 3.2.2). Next,
we intersect these contexts to perform the mapping
(see Section 3.2.3).
3.2.1 Disambiguation Context of a Wikipage
Given a target Wikipage w which we aim to map
to a WordNet sense of w, we use the following
information as a disambiguation context:
? Sense labels: e.g. given the page SODA (SOFT
DRINK), the words soft and drink are added to
the disambiguation context.
? Links: the titles? lemmas of the pages linked
from the Wikipage w (outgoing links). For in-
stance, the links in the Wikipage SODA (SOFT
DRINK) include soda, lemonade, sugar, etc.
? Categories: Wikipages are classified accord-
ing to one or more categories, which repre-
sent meta-information used to categorize them.
For instance, the Wikipage SODA (SOFT DRINK)
is categorized as SOFT DRINKS. Since many
categories are very specific and do not appear in
WordNet (e.g., SWEDISH WRITERS or SCI-
ENTISTS WHO COMMITTED SUICIDE),
we use the lemmas of their syntactic heads as
disambiguation context (i.e. writer and scien-
tist). To this end, we use the category heads
provided by Ponzetto and Navigli (2009).
Given a Wikipage w, we define its disambiguation
context Ctx(w) as the set of words obtained from
some or all of the three sources above.
3.2.2 Disambiguation Context of a WordNet
Sense
Given a WordNet sense s and its synset S, we use
the following information as disambiguation con-
text to provide evidence for a potential link in our
mapping ?:
? Synonymy: all synonyms of s in synset S. For
instance, given the synset of soda2n, all its syn-
onyms are included in the context (that is, tonic,
soda pop, pop, etc.).
1524
? Hypernymy/Hyponymy: all synonyms in the
synsets H such that H is either a hypernym
(i.e., a generalization) or a hyponym (i.e., a spe-
cialization) of S. For example, given soda2n,
we include the words from its hypernym { soft
drink1n }.
? Sisterhood: words from the sisters of S. A sister
synset S? is such that S and S? have a common
direct hypernym. For example, given soda2n, it
can be found that bitter lemon1n and soda2n are
sisters. Thus the words bitter and lemon are in-
cluded in the disambiguation context of s.
? Gloss: the set of lemmas of the content words
occurring within the gloss of s. For instance,
given s = soda2n, defined as ?a sweet drink
containing carbonated water and flavoring?, we
add to the disambiguation context of s the fol-
lowing lemmas: sweet, drink, contain, carbon-
ated, water, flavoring.
Given a WordNet sense s, we define its disam-
biguation context Ctx(s) as the set of words ob-
tained from some or all of the four sources above.
3.2.3 Mapping Algorithm
In order to link each Wikipedia page to a Word-
Net sense, we developed a novel algorithm, whose
pseudocode is shown in Algorithm 1. The follow-
ing steps are performed:
? Initially (lines 1-2), our mapping ? is empty, i.e.
it links each Wikipage w to .
? For each Wikipage w whose lemma is monose-
mous both in Wikipedia and WordNet (i.e.
|SensesWiki(w)| = |SensesWN(w)| = 1) we map
w to its only WordNet sense w1n (lines 3-5).
? Finally, for each remaining Wikipage w for
which no mapping was previously found (i.e.,
?(w) = , line 7), we do the following:
? lines 8-10: for each Wikipage d which is a
redirection to w, for which a mapping was
previously found (i.e. ?(d) 6= , that is, d is
monosemous in both Wikipedia and Word-
Net) and such that it maps to a sense ?(d) in
a synset S that also contains a sense of w, we
map w to the corresponding sense in S.
? lines 11-14: if a Wikipage w has not been
linked yet, we assign the most likely sense
to w based on the maximization of the con-
ditional probabilities p(s|w) over the senses
Algorithm 1 The mapping algorithm
Input: SensesWiki, SensesWN
Output: a mapping ? : SensesWiki ? SensesWN
1: for each w ? SensesWiki
2: ?(w) := 
3: for each w ? SensesWiki
4: if |SensesWiki(w)| = |SensesWN(w)| = 1 then
5: ?(w) := w1n
6: for each w ? SensesWiki
7: if ?(w) =  then
8: for each d ? SensesWiki s.t. d redirects to w
9: if ?(d) 6=  and ?(d) is in a synset of w then
10: ?(w) := sense ofw in synset of ?(d); break
11: for each w ? SensesWiki
12: if ?(w) =  then
13: if no tie occurs then
14: ?(w) := argmax
s?SensesWN(w)
p(s|w)
15: return ?
s ? SensesWN(w) (no mapping is established
if a tie occurs, line 13).
As a result of the execution of the algorithm, the
mapping ? is returned (line 15). At the heart of the
mapping algorithm lies the calculation of the con-
ditional probability p(s|w) of selecting the Word-
Net sense s given the Wikipage w. The sense s
which maximizes this probability can be obtained
as follows:
?(w) = argmax
s?SensesWN(w)
p(s|w) = argmax
s
p(s, w)
p(w)
= argmax
s
p(s, w)
The latter formula is obtained by observing that
p(w) does not influence our maximization, as it is
a constant independent of s. As a result, the most
appropriate sense s is determined by maximizing
the joint probability p(s, w) of sense s and page w.
We estimate p(s, w) as:
p(s, w) =
score(s, w)
?
s??SensesWN(w),
w??SensesWiki(w)
score(s?, w?)
,
where score(s, w) = |Ctx(s)?Ctx(w)|+1 (we add
1 as a smoothing factor). Thus, in our algorithm
we determine the best sense s by computing the in-
tersection of the disambiguation contexts of s and
w, and normalizing by the scores summed over all
senses of w in Wikipedia and WordNet.
3.2.4 Example
We illustrate the execution of our mapping algo-
rithm by way of an example. Let us focus on the
1525
Wikipage SODA (SOFT DRINK). The word soda
is polysemous both in Wikipedia and WordNet,
thus lines 3?5 of the algorithm do not concern
this Wikipage. Lines 6?14 aim to find a mapping
?(SODA (SOFT DRINK)) to an appropriateWordNet
sense of the word. First, we check whether a redi-
rection exists to SODA (SOFT DRINK) that was pre-
viously disambiguated (lines 8?10). Next, we con-
struct the disambiguation context for the Wikipage
by including words from its label, links and cate-
gories (cf. Section 3.2.1). The context includes,
among others, the following words: soft, drink,
cola, sugar. We now construct the disambiguation
context for the two WordNet senses of soda (cf.
Section 3.2.2), namely the sodium carbonate (#1)
and the drink (#2) senses. To do so, we include
words from their synsets, hypernyms, hyponyms,
sisters, and glosses. The context for soda1n in-
cludes: salt, acetate, chlorate, benzoate. The
context for soda2n contains instead: soft, drink,
cola, bitter, etc. The sense with the largest inter-
section is #2, so the following mapping is estab-
lished: ?(SODA (SOFT DRINK)) = soda2n.
3.3 Transferring Semantic Relations
The output of the algorithm presented in the previ-
ous section is a mapping between Wikipages and
WordNet senses (that is, implicitly, synsets). Our
insight is to use this alignment to enable the trans-
fer of semantic relations from Wikipedia to Word-
Net. In fact, given a Wikipage w we can collect
all Wikipedia links occurring in that page. For
any such link from w to w?, if the two Wikipages
are mapped to WordNet senses (i.e., ?(w) 6= 
and ?(w?) 6= ), we can transfer the correspond-
ing edge (?(w), ?(w?)) to WordNet. Note that ?(w)
and ?(w?) are noun senses, as Wikipages describe
nominal concepts or named entities. We refer to
this extended resource as WordNet++.
For instance, consider the Wikipage SODA
(SOFT DRINK). This page contains, among oth-
ers, a link to the Wikipage SYRUP. Assuming
?(SODA (SODA DRINK)) = soda2n and ?(SYRUP) =
syrup1n, we can add the corresponding semantic
relation (soda2n, syrup1n) to WordNet3.
Thus, WordNet++ represents an extension of
WordNet which includes semantic associative re-
lations between synsets. These are originally
3Note that such relations are unlabeled. However, for our
purposes this has no impact, since our algorithms do not dis-
tinguish between is-a and other kinds of relations in the lexi-
cal knowledge base (cf. Section 4.2).
found in Wikipedia and then integrated into Word-
Net by means of our mapping. In turn, Word-
Net++ represents the English-only subset of a
larger multilingual resource, BabelNet (Navigli
and Ponzetto, 2010), where lexicalizations of the
synsets are harvested for many languages using
the so-called Wikipedia inter-language links and
applying a machine translation system.
4 Experiments
We perform two sets of experiments: we first eval-
uate the intrinsic quality of our mapping (Section
4.1) and then quantify the impact of WordNet++
for coarse-grained (Section 4.2) and domain-
specific WSD (Section 4.3).
4.1 Evaluation of the Mapping
Experimental setting. We first conducted an
evaluation of the mapping quality. To create
a gold standard for evaluation, we started from
the set of all lemmas contained both in Word-
Net and Wikipedia: the intersection between the
two resources includes 80,295 lemmas which cor-
respond to 105,797 WordNet senses and 199,735
Wikipedia pages. The average polysemy is 1.3 and
2.5 for WordNet senses and Wikipages, respec-
tively (2.8 and 4.7 when excluding monosemous
words). We selected a random sample of 1,000
Wikipages and asked an annotator with previous
experience in lexicographic annotation to provide
the correct WordNet sense for each page title (an
empty sense label was given if no correct mapping
was possible). 505 non-empty mappings were
found, i.e. Wikipedia pages with a corresponding
WordNet sense. In order to quantify the quality
of the annotations and the difficulty of the task,
a second annotator sense tagged a subset of 200
pages from the original sample. We computed the
inter-annotator agreement using the kappa coeffi-
cient (Carletta, 1996) and found out that our anno-
tators achieved an agreement coefficient ? of 0.9,
indicating almost perfect agreement.
Table 1 summarizes the performance of our dis-
ambiguation algorithm against the manually anno-
tated dataset. Evaluation is performed in terms of
standard measures of precision (the ratio of cor-
rect sense labels to the non-empty labels output
by the mapping algorithm), recall (the ratio of
correct sense labels to the total of non-empty la-
bels in the gold standard) and F1-measure ( 2PRP+R ).
We also calculate accuracy, which accounts for
1526
P R F1 A
Structure 82.2 68.1 74.5 81.1
Gloss 81.1 64.2 71.7 78.8
Structure + Gloss 81.9 77.5 79.6 84.4
MFS BL 24.3 47.8 32.2 24.3
Random BL 23.8 46.8 31.6 23.9
Table 1: Performance of the mapping algorithm.
empty sense labels (that is, calculated on all 1,000
test instances). As baseline we use the most fre-
quent WordNet sense (MFS), as well as a ran-
dom sense assignment. We evaluate the map-
ping methodology described in Section 3.2 against
different disambiguation contexts for the Word-
Net senses (cf. Section 3.2.2), i.e. structure-based
(including synonymy, hypernymy/hyponymy and
sisterhood), gloss-derived evidence, and a combi-
nation of the two. As disambiguation context of
a Wikipage (Section 3.2.1) we use all information
available, i.e. sense labels, links and categories4.
Results and discussion. The results show that
our method improves on the baseline by a large
margin and that higher performance can be
achieved by using more disambiguation informa-
tion. That is, using a richer disambiguation con-
text helps to better choose the most appropriate
WordNet sense for a Wikipedia page. The combi-
nation of structural and gloss information attains a
slight variation in terms of precision (?0.3% and
+0.8% compared to Structure and Gloss respec-
tively), but a significantly high increase in recall
(+9.4% and +13.3%). This implies that the differ-
ent disambiguation contexts only partially overlap
and, when used separately, each produces differ-
ent mappings with a similar level of precision. In
the joint approach, the harmonic mean of preci-
sion and recall, i.e. F1, is in fact 5 and 8 points
higher than when separately using structural and
gloss information, respectively.
As for the baselines, the most frequent sense is
just 0.6% and 0.4% above the random baseline in
terms of F1 and accuracy, respectively. A ?2 test
reveals in fact no statistically significant difference
at p < 0.05. This is related to the random distri-
bution of senses in our dataset and the Wikipedia
unbiased coverage of WordNet senses. So select-
4We leave out the evaluation of different contexts for a
Wikipage for the sake of brevity. During prototyping we
found that the best results were given by using the largest
context available, as reported in Table 1.
ing the most frequent sense rather than any other
sense for each target page represents a choice as
arbitrary as picking a sense at random.
The final mapping contains 81,533 pairs of
Wikipages and word senses they map to, covering
55.7% of the noun senses in WordNet.
Using our best performing mapping we are
able to extend WordNet with 1,902,859 semantic
edges: of these, 97.93% are deemed novel, i.e. no
direct edge could previously be found between the
synsets. In addition, we performed a stricter eval-
uation of the novelty of our relations by check-
ing whether these can still be found indirectly by
searching for a connecting path between the two
synsets of interest. Here we found that 91.3%,
87.2% and 78.9% of the relations are novel to
WordNet when performing a graph search of max-
imum depth of 2, 3 and 4, respectively.
4.2 Coarse-grained WSD
Experimental setting. We extrinsically evalu-
ate the impact of WordNet++ on the Semeval-
2007 coarse-grained all-words WSD task (Nav-
igli et al, 2007). Performing experiments in a
coarse-grained setting is a natural choice for sev-
eral reasons: first, it has been argued that the fine
granularity of WordNet is one of the main obsta-
cles to accurate WSD (cf. the discussion in Nav-
igli (2009b)); second, the meanings of Wikipedia
pages are intuitively coarser than those in Word-
Net5. For instance, mapping TRAVEL to the first
or the second sense in WordNet is an arbitrary
choice, as the Wikipage refers to both senses. Fi-
nally, given their different nature, WordNet and
Wikipedia do not fully overlap. Accordingly,
we expect the transfer of semantic relations from
Wikipedia to WordNet to have sometimes the side
effect to penalize some fine-grained senses of a
word.
We experiment with two simple knowledge-
based algorithms that are set to perform coarse-
grained WSD on a sentence-by-sentence basis:
? Simplified Extended Lesk (ExtLesk): The first
algorithm is a simplified version of the Lesk
5Note that our polysemy rates from Section 4.1 also in-
clude Wikipages whose lemma is contained in WordNet, but
which have out-of-domain meanings, i.e. encyclopedic en-
tries referring to specialized named entities such as e.g., DIS-
COVERY (SPACE SHUTTLE) or FIELD ARTILLERY (MAGA-
ZINE). We computed the polysemy rate for a random sample
of 20 polysemous words by manually removing these NEs
and found that Wikipedia?s polysemy rate is indeed lower
than that of WordNet ? i.e. average polysemy of 2.1 vs. 2.8.
1527
algorithm (Lesk, 1986), that performs WSD
based on the overlap between the context sur-
rounding the target word to be disambiguated
and the definitions of its candidate senses (Kil-
garriff and Rosenzweig, 2000). Given a tar-
get word w, this method assigns to w the
sense whose gloss has the highest overlap (i.e.
most words in common) with the context of w,
namely the set of content words co-occurring
with it in a pre-defined window (a sentence in
our case). Due to the limited context provided
by the WordNet glosses, we follow Banerjee
and Pedersen (2003) and expand the gloss of
each sense s to include words from the glosses
of those synsets in a semantic relation with s.
These include all WordNet synsets which are
directly connected to s, either by means of the
semantic pointers found in WordNet or through
the unlabeled links found in WordNet++.
? Degree Centrality (Degree): The second algo-
rithm is a graph-based approach that relies on
the notion of vertex degree (Navigli and Lap-
ata, 2010). Starting from each sense s of the tar-
get word, it performs a depth-first search (DFS)
of the WordNet(++) graph and collects all the
paths connecting s to senses of other words in
context. As a result, a sentence graph is pro-
duced. A maximum search depth is established
to limit the size of this graph. The sense of the
target word with the highest vertex degree is se-
lected. We follow Navigli and Lapata (2010)
and run Degree in a weakly supervised setting
where the system attempts no sense assignment
if the highest degree score is below a certain
(empirically estimated) threshold. The optimal
threshold and maximum search depth are es-
timated by maximizing Degree?s F1 on a de-
velopment set of 1,000 randomly chosen noun
instances from the SemCor corpus (Miller et
al., 1993). Experiments on the development
dataset using Degree on WordNet++ revealed
a performance far lower than expected. Error
analysis showed that many instances were in-
correctly disambiguated, due to the noise from
weak semantic links, e.g. the links from SODA
(SOFT DRINK) to EUROPE or AUSTRALIA. Ac-
cordingly, in order to improve the disambigua-
tion performance, we developed a filter to rule
out weak semantic relations from WordNet++.
Given a WordNet++ edge (?(w), ?(w?)) where
w and w? are both Wikipages and w links to w?,
Resource Algorithm
Nouns only
P R F1
WordNet
ExtLesk 83.6 57.7 68.3
Degree 86.3 65.5 74.5
Wikipedia
ExtLesk 82.3 64.1 72.0
Degree 96.2 40.1 57.4
WordNet++
ExtLesk 82.7 69.2 75.4
Degree 87.3 72.7 79.4
MFS BL 77.4 77.4 77.4
Random BL 63.5 63.5 63.5
Table 2: Performance on Semeval-2007 coarse-
grained all-words WSD (nouns only subset).
we first collect all words from the category la-
bels of w and w? into two bags of words. We re-
move stopwords and lemmatize the remaining
words. We then compute the degree of overlap
between the two sets of categories as the num-
ber of words in common between the two bags
of words, normalized in the [0, 1] interval. We fi-
nally retain the link for the DFS if such score is
above an empirically determined threshold. The
optimal value for this category overlap thresh-
old was again estimated by maximizing De-
gree?s F1 on the development set. The final
graph used by Degree consists of WordNet, to-
gether with 152,944 relations from our semantic
relation enrichment method (cf. Section 3.3).
Results and discussion. We report our results in
terms of precision, recall and F1-measure on the
Semeval-2007 coarse-grained all-words dataset
(Navigli et al, 2007). We first evaluated ExtLesk
and Degree using three different resources: (1)
WordNet only; (2) Wikipedia only, i.e. only those
relations harvested from the links found within
Wikipedia pages; (3) their union, i.e. WordNet++.
In Table 2 we report the results on nouns only. As
common practice, we compare with random sense
assignment and the most frequent sense (MFS)
from SemCor as baselines. Enriching WordNet
with encyclopedic relations from Wikipedia yields
a consistent improvement against using WordNet
(+7.1% and +4.9% F1 for ExtLesk and Degree)
or Wikipedia (+3.4% and +22.0%) alone. The
best results are obtained by using Degree with
WordNet++. The better performance of Wikipedia
against WordNet when using ExtLesk (+3.7%)
highlights the quality of the relations extracted.
However, no such improvement is found with De-
1528
Algorithm
Nouns only All words
P/R/F1 P/R/F1
ExtLesk 81.0 79.1
Degree 85.5 81.7
SUSSX-FR 81.1 77.0
TreeMatch N/A 73.6
NUS-PT 82.3 82.5
SSI 84.1 83.2
MFS BL 77.4 78.9
Random BL 63.5 62.7
Table 3: Performance on Semeval-2007 coarse-
grained all-words WSD with MFS as a back-off
strategy when no sense assignment is attempted.
gree, due to its lower recall. Interestingly, Degree
on WordNet++ beats the MFS baseline, which is
notably a difficult competitor for unsupervised and
knowledge-lean systems.
We finally compare our two algorithms using
WordNet++ with state-of-the-art WSD systems,
namely the best unsupervised (Koeling and Mc-
Carthy, 2007, SUSSX-FR) and supervised (Chan
et al, 2007, NUS-PT) systems participating in
the Semeval-2007 coarse-grained all-words task.
We also compare with SSI (Navigli and Velardi,
2005) ? a knowledge-based system that partici-
pated out of competition ? and the unsupervised
proposal from Chen et al (2009, TreeMatch). Ta-
ble 3 shows the results for nouns (1,108) and
all words (2,269 words): we use the MFS as a
back-off strategy when no sense assignment is at-
tempted. Degree with WordNet++ achieves the
best performance in the literature6. On the noun-
only subset of the data, its performance is com-
parable with SSI and significantly better than the
best supervised and unsupervised systems (+3.2%
and +4.4% F1 against NUS-PT and SUSSX-FR).
On the entire dataset, it outperforms SUSSX-FR
and TreeMatch (+4.7% and +8.1%) and its re-
call is not statistically different from that of SSI
and NUS-PT. This result is particularly interest-
ing, given that WordNet++ is extended only with
relations between nominals, and, in contrast to
SSI, it does not rely on a costly annotation effort
to engineer the set of semantic relations. Last but
not least, we achieve state-of-the-art performance
with a much simpler algorithm that is based on the
notion of vertex degree in a graph.
6The differences between the results in bold in each col-
umn of the table are not statistically significant at p < 0.05.
Algorithm
Sports Finance
P/R/F1 P/R/F1
k-NN ? 30.3 43.4
Static PR ? 20.1 39.6
Personalized PR ? 35.6 46.9
ExtLesk 40.1 45.6
Degree 42.0 47.8
MFS BL 19.6 37.1
Random BL 19.5 19.6
Table 4: Performance on the Sports and Finance
sections of the dataset from Koeling et al (2005):
? indicates results from Agirre et al (2009).
4.3 Domain WSD
The main strength of Wikipedia is to provide wide
coverage for many specific domains. Accord-
ingly, on the Semeval dataset our system achieves
the best performance on a domain-specific text,
namely d004, a document on computer science
where we achieve 82.9% F1 (+6.8% when com-
pared with the best supervised system, namely
NUS-PT). To test whether our performance on the
Semeval dataset is an artifact of the data, i.e. d004
coming from Wikipedia itself, we evaluated our
system on the Sports and Finance sections of the
domain corpora from Koeling et al (2005). In Ta-
ble 4 we report our results on these datasets and
compare them with Personalized PageRank, the
state-of-the-art system from Agirre et al (2009)7,
as well as Static PageRank and a k-NN supervised
WSD system trained on SemCor.
The results we obtain on the two domains with
our best configuration (Degree using WordNet++)
outperform by a large margin k-NN, thus sup-
porting the findings from Agirre et al (2009)
that knowledge-based systems exhibit a more ro-
bust performance than their supervised alterna-
tives when evaluated across different domains. In
addition, our system achieves better results than
Static and Personalized PageRank, indicating that
competitive disambiguation performance can still
be achieved by a less sophisticated knowledge-
based WSD algorithm when provided with a rich
amount of high-quality knowledge. Finally, the
results show that WordNet++ enables competitive
performance also in a fine-grained domain setting.
7We compare only with those system configurations per-
forming token-based WSD, i.e. disambiguating each instance
of a target word separately, since our aim is not to perform
type-based disambiguation.
1529
5 Conclusions
In this paper, we have presented a large-scale
method for the automatic enrichment of a com-
putational lexicon with encyclopedic relational
knowledge8. Our experiments show that the large
amount of knowledge injected into WordNet is of
high quality and, more importantly, it enables sim-
ple knowledge-based WSD systems to perform as
well as the highest-performing supervised ones in
a coarse-grained setting and to outperform them
on domain-specific text. Thus, our results go
one step beyond previous findings (Cuadros and
Rigau, 2006; Agirre et al, 2009; Navigli and La-
pata, 2010) and prove that knowledge-rich dis-
ambiguation is a competitive alternative to super-
vised systems, even when relying on a simple al-
gorithm. We note, however, that the present con-
tribution does not show which knowledge-rich al-
gorithm performs best with WordNet++. In fact,
more sophisticated approaches, such as Personal-
ized PageRank (Agirre and Soroa, 2009), could be
still applied to yield even higher performance. We
leave such exploration to future work. Moreover,
while the mapping has been used to enrich Word-
Net with a large amount of semantic edges, the
method can be reversed and applied to the ency-
clopedic resource itself, that is Wikipedia, to per-
form disambiguation with the corresponding sense
inventory (cf. the task of wikification proposed
by Mihalcea and Csomai (2007) and Milne and
Witten (2008b)). In this paper, we focused on
English Word Sense Disambiguation. However,
since WordNet++ is part of a multilingual seman-
tic network (Navigli and Ponzetto, 2010), we plan
to explore the impact of this knowledge in a mul-
tilingual setting.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proc. of LREC ?04.
Eneko Agirre and David Martinez. 2001. Learning
class-to-class selectional preferences. In Proceed-
ings of CoNLL-01, pages 15?22.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank forWord Sense Disambiguation. In Proc.
of EACL-09, pages 33?41.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
8The resulting resource, WordNet++, is freely available at
http://lcl.uniroma1.it/wordnetplusplus for
research purposes.
performing better than generic supervised WSD. In
Proc. of IJCAI-09, pages 1501?1506.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proc. of IJCAI-03, pages 805?810.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9?16.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-ML: Exploiting parallel texts for Word Sense
Disambiguation in the English all-words tasks. In
Proc. of SemEval-2007, pages 253?256.
Ping Chen, Wei Ding, Chris Bowes, and David Brown.
2009. A fully unsupervised Word Sense Disam-
biguation method using dependency knowledge. In
Proc. of NAACL-HLT-09, pages 28?36.
Tim Chklovski and Rada Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Expert.
In Proceedings of the ACL-02 Workshop on WSD:
Recent Successes and Future Directions at ACL-02.
Martin Chodorow, Roy Byrd, and George E. Heidorn.
1985. Extracting semantic hierarchies from a large
on-line dictionary. In Proc. of ACL-85, pages 299?
304.
Philipp Cimiano, Siegfried Handschuh, and Steffen
Staab. 2004. Towards the self-annotating Web. In
Proc. of WWW-04, pages 462?471.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proc. of EMNLP-06, pages 534?541.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the Web. In
Proc. of COLING-08, pages 161?168.
Philip Edmonds. 2000. Designing a task for
SENSEVAL-2. Technical report, University of
Brighton, U.K.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
Wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proc. of AAAI-06,
pages 1301?1306.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proc. of IJCAI-
07, pages 1606?1611.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Sanda M. Harabagiu, George A. Miller, and Dan I.
Moldovan. 1999. WordNet 2 ? a morphologically
and semantically enhanced resource. In Proceed-
ings of the SIGLEX99 Workshop on Standardizing
Lexical Resources, pages 1?8.
1530
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proc. of
COLING-92, pages 539?545.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for English SENSEVAL.
Computers and the Humanities, 34(1-2).
Rob Koeling and Diana McCarthy. 2007. Sussx: WSD
using automatically acquired predominant senses.
In Proc. of SemEval-2007, pages 314?317.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proc. of HLT-
EMNLP-05, pages 419?426.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th Annual Conference on Systems Documen-
tation, Toronto, Ontario, Canada, pages 24?26.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
Linking documents to encyclopedic knowledge. In
Proc. of CIKM-07, pages 233?242.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proc. of NAACL-
HLT-07, pages 196?203.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In
Proceedings of the 3rd DARPA Workshop on Human
Language Technology, pages 303?308, Plainsboro,
N.J.
David Milne and Ian H. Witten. 2008a. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the Work-
shop on Wikipedia and Artificial Intelligence: An
Evolving Synergy at AAAI-08, pages 25?30.
David Milne and Ian H. Witten. 2008b. Learning to
link with Wikipedia. In Proc. of CIKM-08, pages
509?518.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia category names for knowledge acquisi-
tion. In Proc. of AAAI-08, pages 1219?1224.
Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
activation spreading. In Proc. of EMNLP-08, pages
763?772.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study on graph connectivity for unsuper-
vised Word Sense Disambiguation. IEEE Transac-
tions on Pattern Anaylsis and Machine Intelligence,
32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual seman-
tic network. In Proc. of ACL-10.
Roberto Navigli and Paola Velardi. 2005. Struc-
tural Semantic Interconnections: a knowledge-based
approach to Word Sense Disambiguation. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 27(7):1075?1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained English all-words task. In Proc. of SemEval-
2007, pages 30?35.
Roberto Navigli. 2009a. Using cycles and quasi-
cycles to disambiguate dictionary glosses. In Proc.
of EACL-09, pages 594?602.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proc. of COLING-
ACL-06, pages 793?800.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proc. of IJCAI-09,
pages 2083?2088.
Simone Paolo Ponzetto and Michael Strube. 2007a.
Deriving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2007b.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181?212.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381?387. College Publications, Lon-
don, England.
German Rigau, Horacio Rodr??guez, and Eneko Agirre.
1998. Building accurate semantic taxonomies from
monolingual MRDs. In Proc. of COLING-ACL-98,
pages 1103?1109.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lecture
Notes in Computer Science. Springer Verlag.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proc. of ACL-IJCNLP-09, pages
208?216.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proc. of ACL-IJCNLP-09, pages 450?458.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proc. of COLING-ACL-06, pages 801?
808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Fei Wu and Daniel Weld. 2007. Automatically se-
mantifying Wikipedia. In Proc. of CIKM-07, pages
41?50.
FeiWu and Daniel Weld. 2008. Automatically refining
the Wikipedia infobox ontology. In Proc. of WWW-
08, pages 635?644.
1531
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 67?72,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Multilingual WSD with Just a Few Lines of Code: the BabelNet API
Roberto Navigli and Simone Paolo Ponzetto
Dipartimento di Informatica
Sapienza Universita` di Roma
{navigli,ponzetto}@di.uniroma1.it
Abstract
In this paper we present an API for program-
matic access to BabelNet ? a wide-coverage
multilingual lexical knowledge base ? and
multilingual knowledge-rich Word Sense Dis-
ambiguation (WSD). Our aim is to provide the
research community with easy-to-use tools to
perform multilingual lexical semantic analysis
and foster further research in this direction.
1 Introduction
In recent years research in Natural Language Pro-
cessing (NLP) has been steadily moving towards
multilingual processing: the availability of ever
growing amounts of text in different languages, in
fact, has been a major driving force behind re-
search on multilingual approaches, from morpho-
syntactic (Das and Petrov, 2011) and syntactico-
semantic (Peirsman and Pado?, 2010) phenomena to
high-end tasks like textual entailment (Mehdad et
al., 2011) and sentiment analysis (Lu et al, 2011).
These research trends would seem to indicate the
time is ripe for developing methods capable of per-
forming semantic analysis of texts written in any
language: however, this objective is still far from be-
ing attained, as is demonstrated by research in a core
language understanding task such as Word Sense
Disambiguation (Navigli, 2009, WSD) continuing to
be focused primarily on English. While the lack of
resources has hampered the development of effec-
tive multilingual approaches to WSD, recently this
idea has been revamped with the organization of
SemEval tasks on cross-lingual WSD (Lefever and
Hoste, 2010) and cross-lingual lexical substitution
(Mihalcea et al, 2010). In addition, new research on
the topic has explored the translation of sentences
into many languages (Navigli and Ponzetto, 2010;
Lefever et al, 2011; Banea and Mihalcea, 2011),
as well as the projection of monolingual knowledge
onto another language (Khapra et al, 2011).
In our research we focus on knowledge-based
methods and tools for multilingual WSD, since
knowledge-rich WSD has been shown to achieve
high performance across domains (Agirre et al,
2009; Navigli et al, 2011) and to compete with su-
pervised methods on a variety of lexical disambigua-
tion tasks (Ponzetto and Navigli, 2010). Our vi-
sion of knowledge-rich multilingual WSD requires
two fundamental components: first, a wide-coverage
multilingual lexical knowledge base; second, tools
to effectively query, retrieve and exploit its informa-
tion for disambiguation. Nevertheless, to date, no
integrated resources and tools exist that are freely
available to the research community on a multi-
lingual scale. Previous endeavors are either not
freely available (EuroWordNet (Vossen, 1998)), or
are only accessible via a Web interface (cf. the Mul-
tilingual Research Repository (Atserias et al, 2004)
and MENTA (de Melo and Weikum, 2010)), thus
providing no programmatic access. And this is de-
spite the fact that the availability of easy-to-use li-
braries for efficient information access is known to
foster top-level research ? cf. the widespread use of
semantic similarity measures in NLP, thanks to the
availability of WordNet::Similarity (Peder-
sen et al, 2004).
With the present contribution we aim to fill this
gap in multilingual tools, providing a multi-tiered
contribution consisting of (a) an Application Pro-
gramming Interface (API) for efficiently accessing
the information available in BabelNet (Navigli and
67
bn:00008364n WIKIWN 08420278n 85 WN:EN:bank WIKI:EN:Bank WIKI:DE:Bank WIKI:IT:Banca
WIKIRED:DE:Finanzinstitut WN:EN:banking_company
WNTR:ES:banco WNTR:FR:socie?te?_bancaire WIKI:FR:Banque ...
35 1_7 2_3,4,9 6_8 ...
228 r bn:02945246n r bn:02854884n|FROM_IT @ bn:00034537n ...
Figure 1: The Babel synset for bank2n, i.e. its ?financial? sense (excerpt, formatted for ease of readability).
Ponzetto, 2010), a very large knowledge repository
with concept lexicalizations in 6 languages (Cata-
lan, English, French, German, Italian and Spanish),
at the lexicographic (i.e., word senses), encyclope-
dic (i.e., named entities) and conceptual (i.e., con-
cepts and semantic relations) levels; (b) an API to
perform graph-based WSD with BabelNet, thus pro-
viding, for the first time, a freely-available toolkit for
performing knowledge-basedWSD in a multilingual
and cross-lingual setting.
2 BabelNet
BabelNet follows the structure of a traditional lex-
ical knowledge base and accordingly consists of a
labeled directed graph where nodes represent con-
cepts and named entities and edges express semantic
relations between them. Concepts and relations are
harvested from the largest available semantic lexi-
con of English, i.e., WordNet (Fellbaum, 1998), and
a wide-coverage collaboratively-edited encyclope-
dia, i.e., Wikipedia1, thus making BabelNet a mul-
tilingual ?encyclopedic dictionary? which automati-
cally integrates fine-grained lexicographic informa-
tion with large amounts of encyclopedic knowledge
by means of a high-performing mapping algorithm
(Navigli and Ponzetto, 2010). In addition to this
conceptual backbone, BabelNet provides a multilin-
gual lexical dimension. Each of its nodes, called
Babel synsets, contains a set of lexicalizations of
the concept for different languages, e.g., { bankEN,
BankDE, bancaIT, . . . , bancoES }.
Similar in spirit to WordNet, BabelNet consists,
at its lowest level, of a plain text file. An ex-
cerpt of the entry for the Babel synset containing
bank2n is shown in Figure 12. The record contains
(a) the synset?s id; (b) the region of BabelNet
where it lies (e.g., WIKIWN means at the intersec-
1http://www.wikipedia.org
2We denote with wip the i-th WordNet sense of a word w
with part of speech p.
tion of WordNet and Wikipedia); (c) the correspond-
ing (possibly empty) WordNet 3.0 synset offset;
(d) the number of senses in all languages and
their full listing; (e) the number of translation re-
lations and their full listing; (f) the number of se-
mantic pointers (i.e., relations) to other Babel
synsets and their full listing. Senses encode in-
formation about their source ? i.e., whether they
come from WordNet (WN), Wikipedia pages (WIKI)
or their redirections (WIKIRED), or are automatic
translations (WNTR / WIKITR) ? and about their
language and lemma. In addition, translation rela-
tions among lexical items are represented as a map-
ping from source to target senses ? e.g., 2 3,4,9
means that the second element in the list of senses
(the English word bank) translates into items #3
(German Bank), #4 (Italian banca), and #9 (French
banque). Finally, semantic relations are encoded
using WordNet?s pointers and an additional sym-
bol for Wikipedia relations (r), which can also
specify the source of the relation (e.g., FROM IT
means that the relation was harvested from the Ital-
ian Wikipedia). In Figure 1, the Babel synset in-
herits the WordNet hypernym (@) relation to finan-
cial institution1n (offset bn:00034537n), as well
as Wikipedia relations to the synsets of FINAN-
CIAL INSTRUMENT (bn:02945246n) and ETH-
ICAL BANKING (bn:02854884n, from Italian).
3 An API for multilingual WSD
BabelNet API. BabelNet can be effectively ac-
cessed and automatically embedded within applica-
tions by means of a programmatic access. In order
to achieve this, we developed a Java API, based on
Apache Lucene3, which indexes the BabelNet tex-
tual dump and includes a variety of methods to ac-
cess the four main levels of information encoded in
BabelNet, namely: (a) lexicographic (information
about word senses), (b) encyclopedic (i.e. named en-
3http://lucene.apache.org
68
1 BabelNet bn = BabelNet.getInstance();
2 System.out.println("SYNSETS WITH English word: \"bank\"");
3 List<BabelSynset> synsets = bn.getSynsets(Language.EN, "bank");
4 for (BabelSynset synset : synsets) {
5 System.out.print(" =>(" + synset.getId() + ") SOURCE: " + synset.getSource() +
6 "; WN SYNSET: " + synset.getWordNetOffsets() + ";\n" +
7 " MAIN LEMMA: " + synset.getMainLemma() + ";\n SENSES (IT): { ");
8 for (BabelSense sense : synset.getSenses(Language.IT))
9 System.out.print(sense.toString()+" ");
10 System.out.println("}\n -----");
11 Map<IPointer, List<BabelSynset>> relatedSynsets = synset.getRelatedMap();
12 for (IPointer relationType : relatedSynsets.keySet()) {
13 List<BabelSynset> relationSynsets = relatedSynsets.get(relationType);
14 for (BabelSynset relationSynset : relationSynsets) {
15 System.out.println(" EDGE " + relationType.getSymbol() +
16 " " + relationSynset.getId() +
17 " " + relationSynset.toString(Language.EN));
18 }
19 }
20 System.out.println(" -----");
21 }
Figure 2: Sample BabelNet API usage.
tities), (c) conceptual (the semantic network made
up of its concepts), (d) and multilingual level (in-
formation about word translations). Figure 2 shows
a usage example of the BabelNet API. In the code
snippet we start by querying the Babel synsets for
the English word bank (line 3). Next, we access dif-
ferent kinds of information for each synset: first, we
print their id, source (WordNet, Wikipedia, or both),
the corresponding, possibly empty, WordNet offsets,
and ?main lemma? ? namely, a compact string rep-
resentation of the Babel synset consisting of its cor-
responding WordNet synset in stringified form, or
the first non-redirection Wikipedia page found in it
(lines 5?7). Then, we access and print the Italian
word senses they contain (lines 8?10), and finally
the synsets they are related to (lines 11?19). Thanks
to carefully designed Java classes, we are able to ac-
complish all of this in about 20 lines of code.
Multilingual WSD API. We use the BabelNet API
as a framework to build a toolkit that allows the
user to performmultilingual graph-based lexical dis-
ambiguation ? namely, to identify the most suitable
meanings of the input words on the basis of the se-
mantic connections found in the lexical knowledge
base, along the lines of Navigli and Lapata (2010).
At its core, the API leverages an in-house Java li-
brary to query paths and create semantic graphs
with BabelNet. The latter works by pre-computing
off-line paths connecting any pair of Babel synsets,
which are collected by iterating through each synset
in turn, and performing a depth-first search up to a
maximum depth ? which we set to 3, on the basis of
experimental evidence from a variety of knowledge
base linking and lexical disambiguation tasks (Nav-
igli and Lapata, 2010; Ponzetto and Navigli, 2010).
Next, these paths are stored within a Lucene index,
which ensures efficient lookups for querying those
paths starting and ending in a specific synset. Given
a set of words as input, a semantic graph factory
class searches for their meanings within BabelNet,
looks for their connecting paths, and merges such
paths within a single graph. Optionally, the paths
making up the graph can be filtered ? e.g., it is possi-
ble to remove loops, weighted edges below a certain
threshold, etc. ? and the graph nodes can be scored
using a variety of methods ? such as, for instance,
their outdegree or PageRank value in the context of
the semantic graph. These graph connectivity mea-
sures can be used to rank senses of the input words,
thus performing graph-based WSD on the basis of
the structure of the underlying knowledge base.
We show in Figure 3 a usage example of our
disambiguation API. The method which performs
WSD (disambiguate) takes as input a col-
lection of words (i.e., typically a sentence), a
KnowledgeBase with which to perform dis-
69
1 public static void disambiguate(Collection<Word> words,
2 KnowledgeBase kb, KnowledgeGraphScorer scorer) {
3 KnowledgeGraphFactory factory = KnowledgeGraphFactory.getInstance(kb);
4 KnowledgeGraph kGraph = factory.getKnowledgeGraph(words);
5 Map<String, Double> scores = scorer.score(kGraph);
6 for (String concept : scores.keySet()) {
7 double score = scores.get(concept);
8 for (Word word : kGraph.wordsForConcept(concept))
9 word.addLabel(concept, score);
10 }
11 for (Word word : words) {
12 System.out.println("\n\t" + word.getWord() + " -- ID " + word.getId() +
13 " => SENSE DISTRIBUTION: ");
14 for (ScoredItem<String> label : word.getLabels()) {
15 System.out.println("\t [" + label.getItem() + "]:" +
16 Strings.format(label.getScore()));
17 }
18 }
19 }
20
21 public static void main(String[] args) {
22 List<Word> sentence = Arrays.asList(
23 new Word[]{new Word("bank", ?n?, Language.EN), new Word("bonus", ?n?, Language.EN),
24 new Word("pay", ?v?, Language.EN), new Word("stock", ?n?, Language.EN)});
25 disambiguate(sentence, KnowledgeBase.BABELNET, KnowledgeGraphScorer.DEGREE);
26 }
Figure 3: Sample Word Sense Disambiguation API usage.
ambiguation, and a KnowledgeGraphScorer,
namely a value from an enumeration of different
graph connectivity measures (e.g., node outdegree),
which are responsible for scoring nodes (i.e., con-
cepts) in the graph. KnowledgeBase is an enu-
meration of supported knowledge bases: currently, it
includes BabelNet, as well as WordNet++ (namely,
an EnglishWordNet-based subset of it (Ponzetto and
Navigli, 2010)) and WordNet. Note that, while Ba-
belNet is presently the only lexical knowledge base
which allows for multilingual processing, our frame-
work can easily be extended to work with other ex-
isting lexical knowledge resources, provided they
can be wrapped around Java classes and implement
interface methods for querying senses, concepts, and
their semantic relations. In the snippet we start in
line 3 by obtaining an instance of the factory class
which creates the semantic graphs for a given knowl-
edge base. Next, we use this factory to create the
graph for the input words (line 4). We then score the
senses of the input words occurring within this graph
(line 5?10). Finally, we output the sense distribu-
tions of each word in lines 11?18. The disambigua-
tion method, in turn, can be called by any other Java
program in a way similar to the one highlighted by
the main method of lines 21?26, where we disam-
biguate the sample sentence ?bank bonuses are paid
in stocks? (note that each input word can be written
in any of the 6 languages, i.e. we could mix lan-
guages).
4 Experiments
We benchmark our API by performing knowledge-
based WSD with BabelNet on standard SemEval
datasets, namely the SemEval-2007 coarse-grained
all-words (Navigli et al, 2007, Coarse-WSD, hence-
forth) and the SemEval-2010 cross-lingual (Lefever
and Hoste, 2010, CL-WSD) WSD tasks. For
both experimental settings we use a standard graph-
based algorithm, Degree (Navigli and Lapata, 2010),
which has been previously shown to yield a highly
competitive performance on different lexical disam-
biguation tasks (Ponzetto and Navigli, 2010). Given
a semantic graph for the input context, Degree se-
lects the sense of the target word with the highest
vertex degree. In addition, in the CL-WSD setting
we need to output appropriate lexicalization(s) in
different languages. Since the selected Babel synset
can contain multiple translations in a target language
for the given English word, we use for this task an
70
Algorithm Nouns only All words
NUS-PT 82.3 82.5
SUSSX-FR 81.1 77.0
Degree 84.7 82.3
MFS BL 77.4 78.9
Random BL 63.5 62.7
Table 1: Performance on SemEval-2007 coarse-grained
all-words WSD (Navigli et al, 2007).
unsupervised approach where we return for each test
instance only the most frequent translation found in
the synset, as given by its frequency of alignment
obtained from the Europarl corpus (Koehn, 2005).
Tables 1 and 2 summarize our results in terms
of recall (the primary metric for WSD tasks): for
each SemEval task, we benchmark our disambigua-
tion API against the best unsupervised and super-
vised systems, namely SUSSX-FR (Koeling and
McCarthy, 2007) and NUS-PT (Chan et al, 2007)
for Coarse-WSD, and T3-COLEUR (Guo and Diab,
2010) and UvT-v (van Gompel, 2010) for CL-WSD.
In the Coarse-WSD task our API achieves the best
overall performance on the nouns-only subset of
the data, thus supporting previous findings indicat-
ing the benefits of using rich knowledge bases like
BabelNet. In the CL-WSD evaluation, instead, us-
ing BabelNet alows us to surpass the best unsuper-
vised system by a substantial margin, thus indicating
the viability of high-performing WSD with a multi-
lingual lexical knowledge base. While our perfor-
mance still lags behind the application of supervised
techniques to this task (cf. also results from Lefever
and Hoste (2010)), we argue that further improve-
ments can still be obtained by exploiting more com-
plex disambiguation strategies. In general, using our
toolkit we are able to achieve a performance which
is competitive with the state of the art for these tasks,
thus supporting previous findings on knowledge-rich
WSD, and confirming the robustness of our toolkit.
5 Related Work
Our work complements recent efforts focused on vi-
sual browsing of wide-coverage knowledge bases
(Tylenda et al, 2011; Navigli and Ponzetto, 2012)
by means of an API which allows the user to pro-
grammatically query and search BabelNet. This
knowledge resource, in turn, can be used for eas-
Degree T3-Coleur UvT-v
Dutch 15.52 10.56 17.70
French 22.94 21.75 ?
German 17.15 13.05 ?
Italian 18.03 14.67 ?
Spanish 22.48 19.64 23.39
Table 2: Performance on SemEval-2010 cross-lingual
WSD (Lefever and Hoste, 2010).
ily performing multilingual and cross-lingual WSD
out-of-the-box. In comparison with other contribu-
tions, our toolkit for multilingual WSD takes pre-
vious work from Navigli (2006), in which an on-
line interface for graph-based monolingual WSD is
presented, one step further by adding a multilin-
gual dimension as well as a full-fledged API. Our
work also complements previous attempts by NLP
researchers to provide the community with freely
available tools to perform state-of-the-art WSD us-
ing WordNet-based measures of semantic related-
ness (Patwardhan et al, 2005), as well as supervised
WSD techniques (Zhong and Ng, 2010). We achieve
this by building upon BabelNet, a multilingual ?en-
cyclopedic dictionary? bringing together the lexico-
graphic and encyclopedic knowledge from WordNet
and Wikipedia. Other recent projects on creating
multilingual knowledge bases from Wikipedia in-
clude WikiNet (Nastase et al, 2010) and MENTA
(de Melo and Weikum, 2010): both these resources
offer structured information complementary to Ba-
belNet ? i.e., large amounts of facts about entities
(MENTA), and explicit semantic relations harvested
from Wikipedia categories (WikiNet).
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting Grant
MultiJEDI No. 259234.
BabelNet and its API are available for download at
http://lcl.uniroma1.it/babelnet.
References
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proc. of IJCAI-09, pages 1501?1506.
71
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The MEANING multilingual central
repository. In Proc. of GWC-04, pages 22?31.
Carmen Banea and Rada Mihalcea. 2011. Word Sense
Disambiguation with multilingual features. In Proc.
of IWCS-11, pages 25?34.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-ML: Exploiting parallel texts for Word Sense
Disambiguation in the English all-words tasks. In
Proc. of SemEval-2007, pages 253?256.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL-11, pages 600?609.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
inducing multilingual taxonomies from Wikipedia. In
Proc. of CIKM-10, pages 1099?1108.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Weiwei Guo and Mona Diab. 2010. COLEPL and COL-
SLM: An unsupervised WSD approach to multilingual
lexical substitution, tasks 2 and 3 SemEval 2010. In
Proc. of SemEval-2010, pages 129?133.
Mitesh M. Khapra, Salil Joshi, Arindam Chatterjee, and
Pushpak Bhattacharyya. 2011. Together we can:
Bilingual bootstrapping for WSD. In Proc. of ACL-
11, pages 561?569.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of Ma-
chine Translation Summit X.
Rob Koeling and Diana McCarthy. 2007. Sussx: WSD
using automatically acquired predominant senses. In
Proc. of SemEval-2007, pages 314?317.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation. In
Proc. of SemEval-2010, pages 15?20.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
Word Sense Disambiguation. In Proc. of ACL-11,
pages 317?322.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin
K. Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proc. of ACL-11,
pages 320?330.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proc. of ACL-11, pages
1336?1345.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-lingual lexical substitu-
tion. In Proc. of SemEval-2010, pages 9?14.
Vivi Nastase, Michael Strube, Benjamin Bo?rschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network. In
Proc. of LREC ?10.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis andMachine Intelligence, 32(4):678?
692.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belNet: Building a very large multilingual semantic
network. In Proc. of ACL-10, pages 216?225.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNetXplorer: a platform for multilingual lexical
knowledge base access and exploration. In Comp. Vol.
to Proc. of WWW-12, pages 393?396.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-grained
English all-words task. In Proc. of SemEval-2007,
pages 30?35.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier Lopez
de Lacalle, and Eneko Agirre. 2011. Two birds with
one stone: learning semantic models for Text Catego-
rization and Word Sense Disambiguation. In Proc. of
CIKM-11, pages 2317?2320.
Roberto Navigli. 2006. Online word sense disambigua-
tion with structural semantic interconnections. In
Proc. of EACL-06, pages 107?110.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. SenseRelate::TargetWord ? a generalized
framework for Word Sense Disambiguation. In Comp.
Vol. to Proc. of ACL-05, pages 73?76.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity ? Measuring the re-
latedness of concepts. In Comp. Vol. to Proc. of HLT-
NAACL-04, pages 267?270.
Yves Peirsman and Sebastian Pado?. 2010. Cross-
lingual induction of selectional preferences with bilin-
gual vector spaces. In Proc. of NAACL-HLT-10, pages
921?929.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised system. In Proc. of ACL-10, pages 1522?
1531.
Tomasz Tylenda, Mauro Sozio, and Gerhard Weikum.
2011. Einstein: physicist or vegetarian? Summariz-
ing semantic type graphs for knowledge discovery. In
Proc. of WWW-11, pages 273?276.
Maarten van Gompel. 2010. UvT-WSD1: A cross-
lingual word sense disambiguation system. In Proc.
of SemEval-2010, pages 238?241.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer,
Dordrecht, The Netherlands.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage Word Sense Disambiguation system
for free text. In Proc. of ACL-10 System Demonstra-
tions, pages 78?83.
72
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 528?538,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web
Flavio De Benedictis, Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
flavio.debene@gmail.com,{faralli,navigli}@di.uniroma1.it
Abstract
We present GlossBoot, an effective
minimally-supervised approach to ac-
quiring wide-coverage domain glossaries
for many languages. For each language
of interest, given a small number of
hypernymy relation seeds concerning a
target domain, we bootstrap a glossary
from the Web for that domain by means of
iteratively acquired term/gloss extraction
patterns. Our experiments show high
performance in the acquisition of domain
terminologies and glossaries for three
different languages.
1 Introduction
Much textual content, such as that available on
the Web, contains a great deal of information fo-
cused on specific areas of knowledge. However,
it is not infrequent that, when reading a domain-
specific text, we humans do not know the mean-
ing of one or more terms. To help the human
understanding of specialized texts, repositories of
textual definitions for technical terms, called glos-
saries, are compiled as reference resources within
each domain of interest. Interestingly, electronic
glossaries have been shown to be key resources
not only for humans, but also in Natural Language
Processing (NLP) tasks such as Question Answer-
ing (Cui et al, 2007), Word Sense Disambiguation
(Duan and Yates, 2010; Faralli and Navigli, 2012)
and ontology learning (Navigli et al, 2011; Ve-
lardi et al, 2013).
Today large numbers of glossaries are available
on the Web. However most such glossaries are
small-scale, being made up of just some hundreds
of definitions. Consequently, individual glossaries
typically provide a partial view of a given domain.
Moreover, there is no easy way of retrieving the
subset of Web glossaries which appertains to a do-
main of interest. Although online services such
as Google Define allow the user to retrieve defi-
nitions for an input term, such definitions are ex-
tracted from Web glossaries and put together for
the given term regardless of their domain. As a re-
sult, gathering a large-scale, full-fledged domain
glossary is not a speedy operation.
Collaborative efforts are currently producing
large-scale encyclopedias, such as Wikipedia,
which are proving very useful in NLP (Hovy et al,
2013). Interestingly, wikipedias also include man-
ually compiled glossaries. However, such glos-
saries still suffer from the same above-mentioned
problems, i.e., being incomplete or over-specific,1
and hard to customize according to a user?s needs.
To automatically obtain large domain glos-
saries, over recent years computational ap-
proaches have been developed which extract tex-
tual definitions from corpora (Navigli and Velardi,
2010; Reiplinger et al, 2012) or the Web (Fujii
and Ishikawa, 2000). The former methods start
from a given set of terms (possibly automatically
extracted from a domain corpus) and then har-
vest textual definitions for these terms from the
input corpus using a supervised system. Web-
based methods, instead, extract text snippets from
Web pages which match pre-defined lexical pat-
terns, such as ?X is a Y?, along the lines of Hearst
(1992). These approaches typically perform with
high precision and low recall, because they fall
short of detecting the high variability of the syn-
tactic structure of textual definitions. To address
the low-recall issue, recurring cue terms occurring
within dictionary and encyclopedic resources can
be automatically extracted and incorporated into
lexical patterns (Saggion, 2004). However, this
approach is term-specific and does not scale to ar-
bitrary terminologies and domains.
In this paper we propose GlossBoot, a novel
approach which reduces human intervention to a
bare minimum and exploits the Web to learn a
1http://en.wikipedia.org/wiki/Portal:Contents/Glossaries
528
  
Pattern and glossary extractionInitial seedselection  Gloss ranking and filteringSeedqueries Seedselection
initialseeds
newseeds
 search results domainglossary Gk
finalglossary 1 2 3 4 5
Figure 1: The GlossBoot bootstrapping process for glossary learning.
full-fledged domain glossary. Given a domain and
a language of interest, we bootstrap the glossary
learning process with just a few hypernymy rela-
tions (such as computer is-a device), with the only
condition that the (term, hypernym) pairs must be
specific enough to implicitly identify the domain
in the target language. Hence we drop the require-
ment of a large domain corpus, and also avoid the
use of training data or a manually defined set of
lexical patterns. To the best of our knowledge, this
is the first approach which jointly acquires large
amounts of terms and glosses from the Web with
minimal supervision for any target domain and
language.
2 GlossBoot
Our objective is to harvest a domain glossary G
containing pairs of terms/glosses in a given lan-
guage. To this end, we automatically populate a
set of HTML patterns P which we use to extract
definitions from Web glossaries. Initially, both
P := ? and G := ?. We incrementally populate
the two sets by means of an initial seed selection
step and four iterative steps (cf. Figure 1):
Step 1. Initial seed selection: first, we manu-
ally select a set of K hypernymy relation seeds
S = {(t1, h1), . . . , (tK , hK)}, where the pair (ti,
hi) contains a term ti and its generalization hi
(e.g., (firewall, security system)). This is the only
human input to the entire glossary learning pro-
cess. The selection of the input seeds plays a key
role in the bootstrapping process, in that the pat-
tern and gloss extraction process will be driven by
these seeds. The chosen hypernymy relations thus
have to be as topical and representative as pos-
sible for the domain of interest (e.g., (compiler,
computer program) is an appropriate pair for com-
puter science, while (byte, unit of measurement)
is not, as it might cause the extraction of several
glossaries of units and measures).
We now set the iteration counter k to 1 and start
the first iteration of the glossary bootstrapping pro-
cess (steps 2-5). After each iteration k, we keep
track of the set of glosses Gk, acquired during it-
eration k.
Step 2. Seed queries: for each seed pair (ti, hi),
we submit the following query to a Web search
engine: ?ti? ?hi? glossaryKeyword2 (where
glossaryKeyword is the term in the target lan-
guage referring to glossary (i.e., glossary for En-
glish, glossaire for French etc.)) and collect the
top-ranking results for each query.3 Each result-
ing page is a candidate glossary for the domain
implicitly identified by our relation seeds S.
Step 3. Pattern and glossary extraction: we
initialize the glossary for iteration k as follows:
Gk := ?. Next, from each resulting page, we har-
vest all the text snippets s starting with ti and end-
ing with hi (e.g., ?firewall</b> ? a <i>security
system? where ti = firewall and hi = security sys-
tem), i.e., s = ti . . . hi. For each such text snippet
s, we perform the following substeps:
(a) extraction of the term/gloss separator: we
start from ti and move right until we extract
the longest sequence pM of HTML tags and
non-alphanumeric characters, which we call the
term/gloss separator, between ti and the glossary
definition (e.g., ?</b> -? between ?firewall? and
?a? in the above example).
(b) gloss extraction: we expand the snippet s
to the right of hi in search of the entire gloss
of ti, i.e., until we reach a block element (e.g.,
<span>, <p>, <div>), while ignoring format-
ting elements such as <b>, <i> and <a> which
are typically included within a definition sen-
tence. As a result, we obtain the sequence
ti pM glosss(ti) pR, where glosss(ti) is our gloss
for seed term ti in snippet s (which includes hi by
construction) and pR is the HTML block element
2In what follows we use the typewriter font for
keywords and term/gloss separators.
3We use the Google Ajax APIs, which return the 64 top-
ranking search results.
529
Generalized pattern HTML text snippet
<strong> * </strong> - * </span> <strong>Interrupt</strong> - The suspension of normal
program execution to perform a higher priority service rou-
tine as requested by a peripheral device. </span>
<dt> * </dt><dd> * </dd> <dt>Netiquette</dt><dd>The established conventions
of online politeness are called netiquette.</dd>
<h3> * </h3><p> * </p> <h3>Compiler</h3><p>A program that translates
source code, such as C++ or Pascal, into directly executable
machine code.</p>
<span> * </span> - * </p> <span>Signature</span> - A function?s name and param-
eter list. </p>
<span> * </span>: * <span> <span>Blog</span>: Short for ?web log?, a blog is an
online journal. <span>
Table 1: Examples of generalized patterns together with matching HTML text snippets.
Figure 2: An example of decomposition during pattern extraction for a snippet matching the seed pair
(firewall, security system).
to the right of the extracted gloss. In Figure 2 we
show the decomposition of our example snippet
matching the seed (firewall, security system).
(c) pattern instance extraction: we extract the
following pattern instance:
pL ti pM glosss(ti) pR,
where pL is the longest sequence of HTML tags
and non-alphanumeric characters obtained when
moving to the left of ti (see Figure 2).
(d) pattern extraction: we generalize the above
pattern instance to the following pattern:
pL ? pM ? pR,
i.e., we replace ti and glosss(ti) with *. For the
above example, we obtain the following pattern:
<p><b> * </b> - * </p>.
Finally, we add the generalized pattern to the set
of patterns P , i.e., P := P ? {pL ? pM ? pR}.
We also add the first sentence of the retrieved gloss
glosss(ti) to our glossary Gk, i.e., Gk := Gk ?
{(ti, first(glosss(ti)))}, where first(g) returns
the first sentence of gloss g.
(e) pattern matching: finally, we look for addi-
tional pairs of terms/glosses in the Web page con-
taining the snippet s by matching the page against
the generalized pattern pL ? pM ? pR. We then
add toGk the new (term, gloss) pairs matching the
generalized pattern. In Table 1 we show some non-
trivial generalized patterns together with matching
HTML text snippets.
As a result of step 3, we obtain a glossary Gk
for the terms discovered at iteration k.
Step 4. Gloss ranking and filtering: impor-
tantly, not all the extracted definitions pertain to
the domain of interest. In order to rank the glosses
obtained at iteration k by domain pertinence, we
assume that the terms acquired at previous itera-
tions belong to the target domain, i.e., they are do-
main terms at iteration k. Formally, we define the
terminology T k?11 of the domain terms accumu-
lated up until iteration k ? 1 as follows: T k?11 :=?k?1
i=1 T i, where T i := {t : ?(t, g) ? Gi}. For thebase step k = 1, we define T 01 := {t : ?(t, g) ?
G1}, i.e., we use the first-iteration terminology it-
self.
To rank the glosses, we first transform each ac-
quired gloss g to its bag-of-word representation
Bag(g), which contains all the single- and multi-
word expressions in g. We use the lexicon of the
target language?s Wikipedia together with T k?11 in
order to obtain the bag of content words.4 Then we
4In fact Wikipedia is only utilized in the multi-word iden-
tification phase. We do not use Wikipedia for discovering
new terms.
530
Term Gloss Hypernym # Seeds Score
dynamic packet filter A firewall facility that can monitor the state of ac-
tive connections and use this information to determine
which network packets to allow through the firewall
firewall 2 0.75
die An integrated circuit chip cut from a finished wafer. integrated circuit 1 0.75
constructor a method used to help create a new object and ini-
tialise its data
method 0 1.00
Table 2: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms, i.e., in
T k?11 , are underlined, non-domain terms in italics).
calculate the domain score of a gloss g as follows:
score(g) = |Bag(g) ? T
k?1
1 |
|Bag(g)| . (1)
Finally, we use a threshold ? (whose tuning is
described in the experimental section) to remove
from Gk those glosses g whose score(g) < ?.
In Table 2 we show some glosses in the com-
puter science domain (second column, domain
terms are underlined) together with their scores
(last column).
Step 5. Seed selection for next iteration: we
now aim at selecting the new set of hypernymy
relation seeds to be used to start the next iteration.
We perform three substeps:
(a) Hypernym extraction: for each newly-
acquired term/gloss pair (t, g) ? Gk, we automati-
cally extract a candidate hypernym h from the tex-
tual gloss g. To do this we use a simple unsuper-
vised heuristic which just selects the first term in
the gloss.5 We show an example of hypernym ex-
traction for some terms in Table 2 (we report the
term in column 1, the gloss in column 2 and the
hypernyms extracted by the first term hypernym
extraction heuristic in column 3).
(b) (Term, Hypernym)-ranking: we sort all the
glosses in Gk by the number of seed terms found
in each gloss. In the case of ties (i.e., glosses with
the same number of seed terms), we further sort
the glosses by the score given in Formula 1. We
show an example of rank for some glosses in Table
2, where seed terms are in bold, domain terms (i.e.,
in T k?11 ) are underlined, and non-domain terms
are shown in italics.
5While more complex strategies could be used, such as
supervised classifiers (Navigli and Velardi, 2010), we found
that this heuristic works well because, even when it is not a
hypernym, the first term plays the role of a cue word for the
defined term.
(c) New seed selection: we select the (term, hy-
pernym) pairs corresponding to the K top-ranking
glosses.
Finally, if k equals the maximum number of it-
erations, we stop. Else, we increment the iteration
counter (i.e., k := k + 1) and jump to step (2) of
our glossary bootstrapping algorithm after replac-
ing S with the new set of seeds.
The output of glossary bootstrapping is a do-
main glossary G := ?i=1,...,maxGi, whichincludes a domain terminology T := {t :
?(t, g) ? G} (i.e., T := Tmax1 ) and a set of
glosses glosses(t) for each term t ? T (i.e.,
glosses(t) := {g : ?(t, g) ? G}).
3 Experimental Setup
3.1 Domains and Gold Standards
For our experiments we focused on four differ-
ent domains, namely, Computing, Botany, Envi-
ronment, and Finance, and on three languages,
namely, English, French and Italian. Note that not
all the four domains are clear-cut. For instance, the
Environment domain is quite interdisciplinary, in-
cluding terms from fields such as Chemistry, Biol-
ogy, Law, Politics, etc.
For each domain and language we selected
as gold standards well-reputed glossaries on
the Web, such as: the Utah computing glos-
sary,6 the Wikipedia glossary of botanical terms,7
a set of Wikipedia glossaries about environ-
ment,8 and the Reuters glossary for Finance9
(full list at http://lcl.uniroma1.it/
glossboot/). We report the size of the four
gold-standard datasets in Table 4.
6http://www.math.utah.edu/?wisnia/glossary.html
7http://en.wikipedia.org/wiki/Glossary of botanical terms
8http://en.wikipedia.org/wiki/List of environmental issues,
http://en.wikipedia.org/wiki/Glossary of environmental science,
http://en.wikipedia.org/wiki/Glossary of climate change
9http://glossary.reuters.com/index.php/Main Page
531
Computing Botany Environment Financechip circuit leaf organ sewage waste eurobond bonddestructor method grass plant acid rain rain asset play stockcompiler program cultivar variety ecosystem system income stock securityscanner device gymnosperm plant air monitoring sampling financial intermediary institutionfirewall security system flower reproductive organ global warming temperature derivative financial product
Table 3: Hypernymy relation seeds used to bootstrap glossary learning in the four domains for the English
language.
3.2 Seed Selection
For each domain and language we manually se-
lected five seed hypernymy relations, shown for
the English language in Table 3. The seeds
were selected by the authors on the basis of
just two conditions: i) the seeds should cover
different aspects of the domain and should, in-
deed, identify the domain implicitly, ii) at least
10,000 results should be returned by the search
engine when querying it with the seeds plus the
glossaryKeyword (see step (2) of GlossBoot).
The seed selection was not fine-tuned (i.e., it was
not adjusted to improve performance), so it might
well be that better seeds would provide better
results (see, e.g., (Kozareva and Hovy, 2010b)).
However, this type of consideration is beyond the
scope of this paper.
3.2.1 Evaluation measures
We performed experiments to evaluate the quality
of both terms and glosses, as jointly extracted by
GlossBoot.
Terms. For each domain and language we cal-
culated coverage, extra-coverage and precision of
the acquired terms T . Coverage is the ratio of ex-
tracted terms in T also contained in the gold stan-
dard T? to the size of T? . Extra-coverage is calcu-
lated as the ratio of the additional extracted terms
in T \ T? over the number of gold standard terms
T? . Finally, precision is the ratio of extracted terms
in T deemed to be within the domain. To calcu-
late precision we randomly sampled 5% of the re-
trieved terms and asked two human annotators to
manually tag their domain pertinence (with adju-
dication in case of disagreement; ? = .62, indicat-
ing substantial agreement). Note that by sampling
on the entire set T , we calculate the precision of
both terms in T ? T? , i.e., in the gold standard, and
terms in T \ T? , i.e., not in the gold standard, which
are not necessarily outside the domain.
Glosses. We calculated the precision of the ex-
tracted glosses as the ratio of glosses which were
both well-formed textual definitions and specific
Botany Comput. Environ. Finance
EN
Gold std. terms 772 421 713 1777
GlossBoot terms 5598 3738 4120 5294
glosses 11663 4245 5127 6703
FR
Gold std. terms 662 278 117 109
GlossBoot terms 3450 3462 1941 1486
glosses 5649 3812 2095 1692
IT
Gold std. terms 205 244 450 441
GlossBoot terms 1965 3356 1630 3601
glosses 2678 5891 1759 5276
Table 4: Size of the gold-standard and
automatically-acquired glossaries for the four
domains in the three languages of interest.
to the target domain. Precision was determined on
a random sample of 5% of the acquired glosses for
each domain and language. The annotation was
made by two annotators, with ? = .675, indicat-
ing substantial agreement.
3.3 Parameter tuning
We tuned the minimum and maximum length of
both pL and pR (see step (3) of GlossBoot) and
the threshold ? that we use to filter out non-domain
glosses (see step (4) of GlossBoot) using an extra
domain, i.e., the Arts domain. To do this, we cre-
ated a development dataset made up of the full set
of 394 terms from the Tate Gallery glossary,10 and
bootstrapped our glossary extraction method with
just one seed, i.e., (fresco, painting). We chose an
optimal value of ? = 0.1 on the basis of a har-
monic mean of coverage and precision. Note that,
since precision also concerns terms not in the gold
standard, we had to manually validate a sample of
the extracted terms for each of the 21 tested values
of ? ? {0, 0.05, 0.1, . . . , 1.0}.
4 Results and Discussion
4.1 Terms
The size of the extracted terminologies for the four
domains after five iterations are reported in Table
4. In Table 5 we show examples of the possi-
ble scenarios for terms: in-domain extracted terms
10http://www.tate.org.uk/collections/glossary/
532
In-domain In-domain Out-of-domain In-domain
(in gold std, ? T? ? T ) (not in gold std, ? T \ T? ) (not in gold std, ? T \ T? ) (missed, ? T? \ T )
Computing software, inheritance, mi-
croprocessor
clipboard, even parity, su-
doer
gs1-128 label, grayscale,
quantum dots
openwindows, sun mi-
crosystems, hardwired
Botany pollinium, stigma, spore vegetation, dichogamous,
fertilisation
ion, free radicals, mana-
mana
nomenclature, endemism,
insectivorous
Environment carcinogen, footprint, solar
power
frigid soil, biosafety, fire
simulator
epidermis, science park,
alum
g8, best practice,
polystyrene
Finance cash, bond, portfolio trustor, naked option, mar-
ket price
precedent, immigration,
heavy industry
co-location, petrodollars,
euronext
Table 5: Examples of extracted (and missed) terms.
Botany Comput. Environ. Finance
EN
Precision 95% 98% 94% 98%
Coverage 85% 40% 35% 32%
Extra-coverage 640% 848% 542% 266%
FR
Precision 80% 97% 83% 98%
Coverage 97% 27% 14% 26%
Extra-coverage 425% 1219% 1646% 1350%
IT
Precision 89% 98% 76% 99%
Coverage 42% 27% 11% 73%
Extra-coverage 511% 1349% 356% 746%
Table 6: Precision, coverage and extra-coverage of
the term extraction phase after 5 iterations.
which are also found in the gold standard (col-
umn 2), in-domain extracted terms but not in the
gold standard (column 3), out-of-domain extracted
terms (column 4), and domain terms in the gold
standard but not extracted by our approach (col-
umn 5).
A quantitative evaluation is provided in Table
6, which shows the percentage results in terms of
precision, coverage, and extra-coverage after 5 it-
erations of GlossBoot. For the English language
we observe good coverage (between 32% and 40%
on three domains, with a high peak of 85% cover-
age on Botany) and generally very high precision
values. Moreover for the French and the Italian
languages we observe a peak in the Botany and Fi-
nance domains respectively, while the lowest per-
formances in terms of precision and coverage are
observed for Environment, i.e., the most interdis-
ciplinary domain.
In all three languages GlossBoot provides very
high extra coverage of domain terms, i.e., addi-
tional terms which are not in the gold standard but
are returned by our system. The figures, shown in
Table 6, range between 266% (4726/1777) for the
English Finance domain and 1646% (1926/117)
for the French Environment domain. These re-
sults, together with the generally high precision
values, indicate the larger extent of our boot-
strapped glossaries compared to our gold stan-
dards.
Botany Computing Environm. Finance
Min Max Min Max Min Max Min Max
26% 68% 8% 39% 5% 33% 14% 30%
Table 7: Coverage ranges for single-seed term ex-
traction for the English language.
Number of seeds. Although the choice of se-
lecting five hypernymy relation seeds is quite arbi-
trary, it shows that we can acquire a reliable termi-
nology with minimal human intervention. Now, an
obvious question arises: what if we bootstrapped
GlossBoot with fewer hypernym seeds, e.g., just
one seed? To answer this question we replicated
our English experiments on each single (term, hy-
pernym) pair in our seed set. In Table 7 we show
the coverage ranges ? i.e., the minimum and max-
imum coverage values ? for the five seeds on each
domain. We observe that the maximum coverage
can attain values very close to those obtained with
five seeds. However, the minimum coverage val-
ues are much lower. So, if we adopt a 1-seed boot-
strapping policy there is a high risk of acquiring
a poorer terminology unless we select the single
seed very carefully, whereas we have shown that
just a few seeds can cope with domain variabil-
ity. Similar considerations can be made regarding
different seed set sizes (we also tried 2, 3 and 4).
So five is not a magic number, just one which can
guarantee an adequate coverage of the domain.
Number of iterations. In order to study the cov-
erage trend over iterations we selected 5 seeds for
our tuning domain (i.e., Arts, see Section 3.3).
Figure 3 shows the size (left graph), coverage,
extra-coverage and precision (middle graph) of the
acquired glossary after each iteration, from 1 to
20. As expected, (extra-)coverage grows over iter-
ations, while precision drops. Stopping at iteration
5, as we do, is optimal in terms of the harmonic
mean of precision and coverage (right graph in
Figure 3).
533
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 2  4  6  8  10  12  14  16  18  20
iteration
Number of terms and glosses extracted over iterations
termsglosses 10%
100%
1000%
 2  4  6  8  10  12  14  16  18  20
iteration
Coverage, extra-coverage and precision over iterations
precisioncoverageextra-coverage 30%
32%
34%
36%
38%
40%
 2  4  6  8  10  12  14  16  18  20
iteration
Harmonic mean of precision and coverage over iterations
harmonic mean of precision and coverage
Figure 3: Size, coverage and precision trends for Arts (tuning domain) over 20 iterations for English.
Botany Comput. Environm. Finance
EN 96% 94% 97% 97%
FR 88% 89% 88% 95%
IT 94% 98% 83% 99%
Table 8: Precision of the glosses for the four do-
mains and for the three languages.
4.2 Glosses
We show the results of gloss evaluation in Ta-
ble 8. Precision ranges between 83% and 99%,
with three domains performing above 92% on av-
erage across languages, and the Environment do-
main performing relatively worse because of its
highly interdisciplinary nature (89% on average).
We observe that these results are strongly corre-
lated with the precision of the extracted terms (cf.
Table 6), because the retrieved glosses of domain
terms are usually in-domain too, and follow a def-
initional style because they come from glossaries.
Note, however, that the gloss precision can also
be higher than term precision, because many perti-
nent glosses might be extracted for the same term,
cf. Table 4.
5 Comparative Evaluation
5.1 Comparison with Google Define
We performed a comparison with Google De-
fine,11 a state-of-the-art definition search service.
This service inputs a term query and outputs a list
of glosses. First, we randomly sampled 100 terms
from our gold standard for each domain and each
of the three languages. Next, for each domain and
language, we manually calculated the fraction of
terms for which an in-domain definition was pro-
vided by Google Define and GlossBoot. Table 9
shows the coverage results.
Google Define outperforms our system on all
four domains (with a few exceptions). However
11Accessible from Google search by means of the
define: keyword.
Botany Comput. Environm. Finance
EN Google Define 90% 87% 84% 82%GlossBoot 77% 47% 44% 51%
FR Google Define 40% 48% 36% 82%GlossBoot 88% 42% 22% 32%
IT Google Define 52% 74% 78% 80%GlossBoot 64% 38% 44% 92%
Table 9: Number of domain glosses (from a ran-
dom sample of 100 gold standard terms per do-
main) retrieved using Google Define and Gloss-
Boot.
we note that Google Define: i) requires knowing
the domain term to be defined in advance, whereas
we jointly acquire thousands of terms and glosses
starting from just a few seeds; ii) does not discrim-
inate between glosses pertaining to the target do-
main and glosses concerning other fields or senses,
whereas we extract domain-specific glosses.
5.2 Comparison with TaxoLearn
We also compared GlossBoot with a recent ap-
proach to glossary learning embedded into a
framework for graph-based taxonomy learning
from scratch, called TaxoLearn (Navigli et al,
2011). Since this approach requires the manual
selection of a domain corpus to automatically ex-
tract terms and glosses, we decided to keep a level
playing field and experimented with the same do-
main used by the authors, i.e., Artificial Intelli-
gence (AI). TaxoLearn was applied to the entire
set of IJCAI 2009 proceedings, resulting in the ex-
traction of 427 terms and 834 glosses.12 As re-
gards GlossBoot, we selected 10 seeds to cover all
the fields of AI, obtaining 5827 terms and 6716
glosses after 5 iterations, one order of magnitude
greater than TaxoLearn.
As for the precision of the extracted terms, we
randomly sampled 50% of them for each system.
We show in Table 10 (first row) the estimated term
12Available at: http://lcl.uniroma1.it/taxolearn
534
GlossBoot TaxoLearn
Term Precision 82.3% (2398/2913) 77.0% (164/213)
Gloss Precision 82.8% (2780/3358) 78.9% (329/417)
Table 10: Estimated term and gloss precision of
GlossBoot and TaxoLearn for the Artificial Intel-
ligence domain.
precision for GlossBoot and TaxoLearn. The pre-
cision value for GlossBoot is lower than the preci-
sion values of the four domains in Table 6, due
to the AI domain being highly interdisciplinary.
TaxoLearn obtained a lower precision because it
acquires a full-fledged taxonomy for the domain,
thus also including higher-level concepts which do
not necessarily pertain to the domain.
We performed a similar evaluation for the pre-
cision of the acquired glosses, by randomly sam-
pling 50% of them for each system. We show in
Table 10 (second row) the estimated gloss preci-
sion of GlossBoot and TaxoLearn. Again, Gloss-
Boot outperforms TaxoLearn, retrieving a larger
amount of glosses (6716 vs. 834) with higher pre-
cision. We remark, however, that in TaxoLearn
glossary extraction is a by-product of the taxon-
omy learning process.
Finally, we note that we cannot compare with
approaches based on lexical patterns (such as
(Kozareva and Hovy, 2010a)), because they are
not aimed at learning glossaries, but just at re-
trieving sentence snippets which contain pairs of
terms/hypernyms (e.g., ?supervised systems such
as decision trees?).
6 Related Work
There are several techniques in the literature for
the automated acquisition of definitional knowl-
edge. Fujii and Ishikawa (2000) use an n-gram
model to determine the definitional nature of text
fragments, whereas Klavans and Muresan (2001)
apply pattern matching techniques at the lexical
level guided by cue phrases such as ?is called?
and ?is defined as?. Cafarella et al (2005) de-
veloped a Web search engine which handles more
general and complex patterns like ?cities such as
ProperNoun(Head(NP ))? in which it is possi-
ble to constrain the results with syntactic proper-
ties. More recently, a domain-independent super-
vised approach was presented which learns Word-
Class Lattices (WCLs), i.e. lattice-based definition
classifiers that are applied to candidate sentences
containing the input terms (Navigli and Velardi,
2010). WCLs have been shown to perform with
high precision in several domains (Velardi et al,
2013).
To avoid the burden of manually creating a
training dataset, definitional patterns can be ex-
tracted automatically. Reiplinger et al (2012) ex-
perimented with two different approaches for the
acquisition of lexical-syntactic patterns. The first
approach involves bootstrapping patterns from a
domain corpus, and then manually refining the ac-
quired patterns. The second approach, instead,
involves automatically acquiring definitional sen-
tences by using a more sophisticated syntactic and
semantic processing. The results shows high pre-
cision in both cases.
However, these approaches to glossary learning
extract unrestricted textual definitions from open
text. In order to filter out non-domain definitions,
Velardi et al (2008) automatically extract a do-
main terminology from an input corpus which they
later use for assigning a domain score to each har-
vested definition and filtering out non-domain can-
didates. The extraction of domain terms from cor-
pora can be performed either by means of statis-
tical measures such as specificity and cohesion
(Park et al, 2002), or just TF*IDF (Kim et al,
2009).
To avoid the use of a large domain corpus, ter-
minologies can be obtained from the Web by using
Doubly-Anchored Patterns (DAPs) which, given a
(term, hypernym) pair, harvest sentences match-
ing manually-defined patterns like ?<hypernym>
such as <term>, and *? (Kozareva et al, 2008).
Kozareva and Hovy (2010a) further extend this
term extraction process by harvesting new hy-
pernyms using the corresponding inverse patterns
(called DAP?1) like ?* such as <term1>, and
<term2>?. Similarly to our approach, they drop
the requirement of a domain corpus and start
from a small number of (term, hypernym) seeds.
However, while Doubly-Anchored Patterns have
proven useful in the induction of domain tax-
onomies (Kozareva and Hovy, 2010a), they cannot
be applied to the glossary learning task, because
the extracted sentences are not formal definitions.
In contrast, GlossBoot performs the novel task
of multilingual glossary learning from the Web by
bootstrapping the extraction process with a few
(term, hypernym) seeds. Bootstrapping techniques
(Brin, 1998; Agichtein and Gravano, 2000; Pas?ca
et al, 2006) have been successfully applied to
several tasks, including high-precision semantic
lexicon extraction from large corpora (Riloff and
Jones, 1999; Thelen and Riloff, 2002; McIntosh
535
Domain Term Gloss
EN
Botany deciduous losing foliage at the end of the growing season.
Computing information space The abstract concept of everything accessible using networks: the Web.
Finance discount The difference between the lower price paid for a security and the security?s
face amount at issue.
FR
Botany insectivore Qui capture des insectes et en absorbe les matie`res nutritives.
Computing notebook C?est l?appellation d?un petit portable d?une taille proche d?une feuille A4.
Environment e?cosyste`me Ensemble des e?tres vivants et des e?le?ments non vivants d?un milieu qui sont
lie?s vitalement entre eux.
IT
Computing link Collegamento tra diverse pagine web, puo` essere costituito da immagini o
testo.
Environment effetto serra Riscaldamento dell?atmosfera terrestre dovuto alla presenza di gas
nell?atmosfera (anidride carbonica, metano e vapore acqueo) che osta-
colano l?uscita delle radiazioni infrarosse emesse dal suolo terreste verso
l?alto.
Finance spread Indica la differenza tra la quotazione di acquisto e quella di vendita.
Table 11: An excerpt of the domain glossaries acquired for the three languages.
and Curran, 2008; McIntosh and Curran, 2009),
learning semantic relations (Pantel and Pennac-
chiotti, 2006), extracting surface text patterns for
open-domain question answering (Ravichandran
and Hovy, 2002), semantic tagging (Huang and
Riloff, 2010) and unsupervised Word Sense Dis-
ambiguation (Yarowsky, 1995). By exploiting the
(term, hypernym) seeds to bootstrap the itera-
tive acquisition of extraction patterns from Web
glossary pages, we can cover the high variabil-
ity of textual definitions, including both sentences
matching the above-mentioned lexico-syntactic
patterns (e.g., ?a corpus is a collection of docu-
ments?) and glossary-style definitions (e.g., ?cor-
pus: a collection of document?) independently of
the target domain and language.
7 Conclusions
In this paper we have presented GlossBoot, a
new, minimally-supervised approach to multilin-
gual glossary learning. Starting from a few hyper-
nymy relation seeds which implicitly identify the
domain of interest, we apply a bootstrapping ap-
proach which iteratively obtains HTML patterns
from Web glossaries and then applies them to the
extraction of term/gloss pairs. To our knowledge,
GlossBoot is the first approach to large-scale glos-
sary learning which jointly acquires thousands of
terms and glosses for a target domain and language
with minimal supervision.
The gist of GlossBoot is our glossary bootstrap-
ping approach, thanks to which we can drop the
requirements of existing techniques such as the
availability of domain text corpora, which often
do not contain enough definitions, and the man-
ual specification of lexical patterns, which typi-
cally extract sentence snippets, instead of formal
glosses.
GlossBoot will be made available to the re-
search community as open-source software. Be-
yond the immediate usability of its output and
its effective use for domain Word Sense Disam-
biguation (Faralli and Navigli, 2012), we wish
to show the benefit of GlossBoot in gloss-driven
approaches to ontology learning (Navigli et al,
2011; Velardi et al, 2013) and semantic network
enrichment (Navigli and Ponzetto, 2012). In Ta-
ble 11 we show an excerpt of the acquired glos-
saries. All the glossaries and gold standards cre-
ated for our experiments are available from the au-
thors? Web site http://lcl.uniroma1.it/
glossboot/.
We remark that the terminologies covered with
GlossBoot are not only precise, but also one or-
der of magnitude greater than those covered in
individual online glossaries. As future work we
plan to study the ability of GlossBoot to acquire
domain glossaries at different levels of specificity
(i.e., domains vs. subdomains). We also plan to
exploit the acquired HTML patterns for imple-
menting an open-source glossary crawler, along
the lines of Google Define.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
536
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM confer-
ence on Digital Libraries, pages 85?94, San Anto-
nio, Texas, USA.
Sergey Brin. 1998. Extracting patterns and relations
from the World Wide Web. In Proceedings of the
International Workshop on The World Wide Web and
Databases, pages 172?183, London, UK.
Michael J. Cafarella, Doug Downey, Stephen Soder-
land, and Oren Etzioni. 2005. KnowItNow: Fast,
scalable information extraction from the web. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 563?570, Van-
couver, British Columbia, Canada.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Transactions on Information
Systems, 25(2):1?30.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceed-
ings of Human Language Technologies: The 11th
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 627?635, Los Angeles, CA, USA.
Stefano Faralli and Roberto Navigli. 2012. A
New Minimally-supervised Framework for Domain
Word Sense Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1411?
1422, Jeju, Korea.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: extract-
ing term descriptions from semi-structured texts. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 488?495,
Hong Kong.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 539?545, Nantes, France.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Ruihong Huang and Ellen Riloff. 2010. Induc-
ing domain-specific semantic class taggers from (al-
most) nothing. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 275?285, Uppsala, Sweden.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2009. An unsupervised approach to domain-specific
term extraction. In Proceedings of the Australasian
Language Technology Workshop, pages 94?98, Syd-
ney, Australia.
Judith Klavans and Smaranda Muresan. 2001. Evalu-
ation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of the Amer-
ican Medical Informatics Association (AMIA) Sym-
posium, pages 324?328, Washington, D.C., USA.
Zornitsa Kozareva and Eduard Hovy. 2010a. A
semi-supervised method to learn and construct tax-
onomies using the Web. In Proceedings of Empiri-
cal Methods in Natural Language Processing, pages
1110?1118, Cambridge, MA, USA.
Zornitsa Kozareva and Eduard H. Hovy. 2010b. Not
all seeds are equal: Measuring the quality of text
mining seeds. In Proceedings of Human Lan-
guage Technologies: The 11th Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 618?626, Los
Angeles, California, USA.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the Web with
hyponym pattern linkage graphs. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, pages 1048?1056, Colum-
bus, Ohio, USA.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceed-
ings of the Australasian Language Technology Asso-
ciation Workshop, pages 97?105, CSIRO ICT Cen-
tre, Tasmania.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional sim-
ilarity. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 396?404, Suntec,
Singapore.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1318?1327, Uppsala, Sweden.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In Proceedings of the
22th International Joint Conference on Artificial In-
telligence, pages 1872?1877, Barcelona, Spain.
537
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Names and similari-
ties on the web: Fact extraction in the fast lane. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 809?816, Sydney, Australia.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging Generic Patterns for Auto-
matically Harvesting Semantic Relations. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL), Sydney, Australia, pages 113?120,
Sydney, Australia.
Youngja Park, Roy J. Byrd, and Branimir K. Boguraev.
2002. Automatic glossary extraction: beyond termi-
nology identification. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics, pages 1?7, Taipei, Taiwan.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
41?47, Philadelphia, Pennsylvania.
Melanie Reiplinger, Ulrich Scha?fer, and Magdalena
Wolska. 2012. Extracting glossary sentences from
scholarly articles: A comparative evaluation of pat-
tern bootstrapping and deep analysis. In Proceed-
ings of the ACL-2012 Special Workshop on Redis-
covering 50 Years of Discoveries, pages 55?65, Jeju
Island, Korea.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In Proceedings of the sixteenth na-
tional conference on Artificial intelligence and the
eleventh Innovative applications of artificial intelli-
gence conference, pages 474?479, Menlo Park, CA,
USA.
Horacio Saggion. 2004. Identifying definitions in text
collections for question answering. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, pages 1927?1930, Lis-
bon, Portugal.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using
extraction pattern contexts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 214?221, Salt Lake City,
UT, USA.
Paola Velardi, Roberto Navigli, and Pierluigi
D?Amadio. 2008. Mining the Web to create
specialized glossaries. IEEE Intelligent Systems,
23(5):18?25.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3).
David Yarowsky. 1995. Unsupervised Word Sense
Disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 189?
196, Cambridge, MA, USA.
538
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1222?1232,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SPred: Large-scale Harvesting of Semantic Predicates
Tiziano Flati and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{flati,navigli}@di.uniroma1.it
Abstract
We present SPred, a novel method for the
creation of large repositories of semantic
predicates. We start from existing colloca-
tions to form lexical predicates (e.g., break
?) and learn the semantic classes that best
fit the ? argument. To do this, we extract
all the occurrences in Wikipedia which
match the predicate and abstract its argu-
ments to general semantic classes (e.g.,
break BODY PART, break AGREEMENT,
etc.). Our experiments show that we are
able to create a large collection of seman-
tic predicates from the Oxford Advanced
Learner?s Dictionary with high precision
and recall, and perform well against the
most similar approach.
1 Introduction
Acquiring semantic knowledge from text automat-
ically is a long-standing issue in Computational
Linguistics and Artificial Intelligence. Over the
last decade or so the enormous abundance of in-
formation and data that has become available has
made it possible to extract huge amounts of pat-
terns and named entities (Etzioni et al, 2005), se-
mantic lexicons for categories of interest (Thelen
and Riloff, 2002; Igo and Riloff, 2009), large do-
main glossaries (De Benedictis et al, 2013) and
lists of concepts (Katz et al, 2003). Recently,
the availability of Wikipedia and other collabora-
tive resources has considerably boosted research
on several aspects of knowledge acquisition (Hovy
et al, 2013), leading to the creation of several
large-scale knowledge resources, such as DBPe-
dia (Bizer et al, 2009), BabelNet (Navigli and
Ponzetto, 2012), YAGO (Hoffart et al, 2013),
MENTA (de Melo and Weikum, 2010), to name
but a few. This wealth of acquired knowledge
is known to have a positive impact on important
fields such as Information Retrieval (Chu-Carroll
and Prager, 2007), Information Extraction (Krause
et al, 2012), Question Answering (Ferrucci et al,
2010) and Textual Entailment (Berant et al, 2012;
Stern and Dagan, 2012).
Not only are these knowledge resources ob-
tained by acquiring concepts and named entities,
but they also provide semantic relations between
them. These relations are extracted from unstruc-
tured or semi-structured text using ontology learn-
ing from scratch (Velardi et al, 2013) and Open
Information Extraction techniques (Etzioni et al,
2005; Yates et al, 2007; Wu and Weld, 2010;
Fader et al, 2011; Moro and Navigli, 2013) which
mainly stem from seminal work on is-a relation
acquisition (Hearst, 1992) and subsequent devel-
opments (Girju et al, 2003; Pasca, 2004; Snow et
al., 2004, among others).
However, these knowledge resources still lack
semantic information about language units such
as phrases and collocations. For instance, which
semantic classes are expected as a direct object
of the verb break? What kinds of noun does the
adjective amazing collocate with? Recognition of
the need for systems that are aware of the selec-
tional restrictions of verbs and, more in general, of
textual expressions, dates back to several decades
(Wilks, 1975), but today it is more relevant than
ever, as is testified by the current interest in se-
mantic class learning (Kozareva et al, 2008) and
supertype acquisition (Kozareva and Hovy, 2010).
These approaches leverage lexico-syntactic pat-
terns and input seeds to recursively learn the se-
mantic classes of relation arguments. However,
they require the manual selection of one or more
seeds for each pattern of interest, and this selec-
tion influences the amount and kind of semantic
classes to be learned. Furthermore, the learned
classes are not directly linked to existing resources
such as WordNet (Fellbaum, 1998) or Wikipedia.
The goal of our research is to create a large-
scale repository of semantic predicates whose lex-
ical arguments are replaced by their semantic
classes. For example, given the textual expres-
sion break a toe we want to create the correspond-
1222
ing semantic predicate break a BODY PART, where
BODY PART is a class comprising several lexical
realizations, such as leg, arm, foot, etc.
This paper provides three main contributions:
? We propose SPred, a novel approach which
harvests predicates from Wikipedia and gen-
eralizes them by leveraging core concepts
from WordNet.
? We create a large-scale resource made up of
semantic predicates.
? We demonstrate the high quality of our se-
mantic predicates, as well as the generality
of our approach, also in comparison with our
closest competitor.
2 Preliminaries
We introduce two preliminary definitions which
we use in our approach.
Definition 1 (lexical predicate). A lexical pred-
icate w1 w2 . . . wi ? wi+1 . . . wn is a regular
expression, where wj are tokens (j = 1, . . . , n), ?
matches any sequence of one or more tokens, and
i ? {0, . . . , n}. We call the token sequence which
matches ? the filling argument of the predicate.
For example, a * of milk matches occurrences
such as a full bottle of milk, a glass of milk, a car-
ton of milk, etc. While in principle * could match
any sequence of words, since we aim at general-
izing nouns, in what follows we allow ? to match
only noun phrases (e.g., glass, hot cup, very big
bottle, etc.).
Definition 2 (semantic predicate). A semantic
predicate is a sequence w1 w2 . . . wi c wi+1
. . . wn, where wj are tokens (j = 1, . . . , n),
c ? C is a semantic class selected from a fixed
set C of classes, and i ? {0, . . . , n}.
As an example, consider the semantic predicate
cup of BEVERAGE,1 where BEVERAGE is a se-
mantic class representing beverages. This pred-
icate matches phrases like cup of coffee, cup of
tea, etc., but not cup of sky. Other examples in-
clude: MUSICAL INSTRUMENT is played by, a
CONTAINER of milk, break AGREEMENT, etc.
Semantic predicates mix the lexical information
of a given lexical predicate with the explicit se-
mantic modeling of its argument. Importantly, the
same lexical predicate can have different classes as
its argument, like cup of FOOD vs. cup of BEVER-
AGE. Note, however, that different classes might
convey different semantics for the same lexical
1In what follows we denote the SEMANTIC CLASS in
small capitals and the lexical predicate in italics.
predicate, such as cup of COUNTRY, referring to
cup as a prize instead of cup as a container.
3 Large-Scale Harvesting of Semantic
Predicates
The goal of this paper is to provide a fully auto-
matic approach for the creation of a large repos-
itory of semantic predicates in three phases. For
each lexical predicate of interest (e.g., break ?):
1. We extract all its possible filling arguments
from Wikipedia, e.g., lease, contract, leg,
arm, etc. (Section 3.1).
2. We disambiguate as many filling arguments
as possible using Wikipedia, obtaining a
set of corresponding Wikipedia pages, e.g.,
Lease, Contract, etc. (Section 3.2).
3. We create the semantic predicates by general-
izing the Wikipedia pages to their most suit-
able semantic classes, e.g., break AGREE-
MENT, break LIMB, etc. (Section 3.3).
We can then exploit the learned semantic predi-
cates to assign the most suitable semantic class to
new filling arguments for the given lexical predi-
cate (Section 3.4).
3.1 Extraction of Filling Arguments
Let pi be an input lexical predicate (e.g., break ?).
We search the English Wikipedia for all the to-
ken sequences which match pi, resulting in a list
of noun phrases filling the ? argument. We show
an excerpt of the output obtained when searching
Wikipedia for the arguments of the lexical predi-
cate a * of milk in Table 1. As can be seen, a wide
range of noun phrases are extracted, from quanti-
ties such as glass and cup to other aspects, such as
brand and constituent.
The output of this first step is a set Lpi of triples
(a, s, l) of filling arguments a matching the lexi-
cal predicate pi in a sentence s of the Wikipedia
corpus, with a potentially linked to a page l (e.g.,
see the top 3 rows in Table 1; l =  if no link is
provided, see bottom rows of the Table).2 Note
that Wikipedia is the only possible corpus that can
be used here for at least two reasons: first, in or-
der to extract relevant arguments, we need a large
corpus of a definitional nature; second, we need
wide-coverage semantic annotations of filling ar-
guments.
3.2 Disambiguation of Filling Arguments
The objective of the second step is to disambiguate
as many arguments in Lpi as possible for the lex-
2We will also refer to l as the sense of a in sentence s.
1223
a full [[bottle]] of milk
a nice hot [[cup]] of milk
a cold [[glass]] of milk
a very big bottle of milk
a brand of milk
a constituent of milk
Table 1: An excerpt of the token sequences
which match the lexical predicate a * of milk in
Wikipedia (filling argument shown in the second
column; following the Wikipedia convention we
provide links in double square brackets).
ical predicate pi. We denote Dpi = {(a, s, l) :
l 6= } ? Lpi as the set of those arguments origi-
nally linked to the corresponding Wikipedia page
(like the top three linked arguments in Table 1).
Therefore, in the rest of this section we will focus
only on the remaining triples (a, s, ) ? Upi, where
Upi = Lpi \Dpi, i.e., those triples whose arguments
are not semantically annotated. Our goal is to re-
place  with an appropriate sense, i.e., page, for a.
For each such triple (a, s, ) ? Upi, we apply the
following disambiguation heuristics:
? One sense per page: if another occurrence
of a in the same Wikipedia page (indepen-
dent of the lexical predicate) is linked to a
page l, then remove (a, s, ) from Upi and add
(a, s, l) to Dpi. In other words, we propa-
gate an existing annotation of a in the same
Wikipedia page and apply it to our ambigu-
ous item. For instance, cup of coffee appears
in the Wikipedia page Energy drink in the
sentence ?[. . . ] energy drinks contain more
caffeine than a strong cup of coffee?, but this
occurrence of coffee is not linked. How-
ever the second paragraph contains the sen-
tence ?[[Coffee]], tea and other naturally caf-
feinated beverages are usually not considered
energy drinks?, where coffee is linked to the
Coffee page. This heuristic naturally reflects
the broadly known assumption about lexi-
cal ambiguity presented in (Yarowsky, 1995),
namely the one-sense-per-discourse heuris-
tic.
? One sense per lexical predicate: if
?(a, s?, l) ? Dpi, then remove (a, s, ) from
Upi and add (a, s, l) to Dpi. If multiple senses
of a are available, choose the most frequent
one in Dpi. For example, in the page Singa-
porean cuisine the occurrence of coffee in the
sentence ?[. . . ] combined with a cup of cof-
fee and a half-boiled egg? is not linked, but
we have collected many other occurrences,
all linked to the Coffee page, so this link
gets propagated to our ambiguous item as
well. This heuristic mimes the one-sense-per-
collocation heuristic presented in (Yarowsky,
1995).
? Trust the inventory: if Wikipedia provides
only one sense for a, i.e., only one page title
whose lemma is a, link a to that page. Con-
sider the instance ?At that point, Smith threw
down a cup of Gatorade? in page Jimmy
Clausen; there is only one sense for Gatorade
in Wikipedia, so we link the unannotated oc-
currence to it.
As a result, the initial set of disambiguated ar-
guments in Dpi is augmented with all those triples
for which any of the above three heuristics apply.
Note that Dpi might contain the same argument
several times, occurring in different sentences and
linked many times to the same page or to differ-
ent pages. Notably, the discovery of new links is
made through one scan of Wikipedia per heuristic.
The three disambiguation strategies, applied in the
same order as presented above, contribute to pro-
moting the most relevant sense for a given word.
Finally, let A be the set of arguments in Dpi,
i.e., A := {a : ?(a, s, l) ? Dpi}. For each argu-
ment a ? Awe select the majority sense sense(a)
of a and collect the corresponding set of sen-
tences sent(a) marked with that sense. Formally,
sense(a) := argmaxl |{(x, y, z) ? Dpi : x =
a?z = l}| and sent(a) := {s : (a, s, sense(a)) ?
Dpi}.
3.3 Generalization to Semantic Classes
Our final objective is to generalize the annotated
arguments to semantic classes picked out from a
fixed set C of classes. As explained below, we as-
sume the set C to be made up of representative
synsets from WordNet. We perform this in two
substeps: we first link all our disambiguated argu-
ments to WordNet (Section 3.3.1) and then lever-
age the WordNet taxonomy to populate the seman-
tic classes in C (Section 3.3.2).
3.3.1 Linking to WordNet
So far the arguments in Dpi have been semanti-
cally annotated with the Wikipedia pages they re-
fer to. However, using Wikipedia as our sense in-
ventory is not desirable; in fact, contrarily to other
commonly used lexical-semantic networks such
as WordNet, Wikipedia is not formally organized
in a structured, taxonomic hierarchy. While it is
true that attached to each Wikipedia page there are
one or more categories, these categories just pro-
vide shallow information about the class the page
1224
belongs to. Indeed, categories are not ideal for
representing the semantic classes of a Wikipedia
page for at least three reasons: i) many cate-
gories do not express taxonomic information (e.g.,
the English page Albert Einstein provides cate-
gories such as DEATHS FROM ABDOMINAL AOR-
TIC ANEURYSM and INSTITUTE FOR ADVANCED
STUDY FACULTY); ii) categories are mostly struc-
tured in a directed acyclic graph with multiple
parents per category (even worse, cycles are pos-
sible in principle); iii) there is no clear way of
identifying core semantic classes from the large
set of available categories. Although efforts to-
wards the automatic taxonomization of Wikipedia
categories do exist in the literature (Ponzetto and
Strube, 2011; Nastase and Strube, 2013), the re-
sults are of a lower quality than a hand-built lexical
resource. Therefore, as was done in previous work
(Mihalcea and Moldovan, ; Ciaramita and Altun,
2006; Izquierdo et al, 2009; Erk and McCarthy,
2009; Huang and Riloff, 2010), we pick out our
semantic classes C from WordNet and leverage its
manually-curated taxonomy to associate our argu-
ments with the most suitable class. This way we
avoid building a new taxonomy and shift the prob-
lem to that of projecting the Wikipedia pages ?
associated with annotated filling arguments ? to
synsets in WordNet. We address this problem in
two steps:
Wikipedia-WordNet mapping. We exploit an
existing mapping implemented in BabelNet (Nav-
igli and Ponzetto, 2012), a wide-coverage
multilingual semantic network that integrates
Wikipedia and WordNet.3 Based on a disam-
biguation algorithm, BabelNet establishes a map-
ping ? : Wikipages ? Synsets which links
about 50,000 pages to their most suitable Word-
Net senses.4
Mapping extension. Nevertheless, BabelNet is
able to solve the problem only partially, because it
still leaves the vast majority of the 4 million En-
glish Wikipedia pages unmapped. This is mainly
due to the encyclopedic nature of most pages,
which do not have a counterpart in the WordNet
dictionary. To address this issue, for each un-
mapped Wikipedia page p we obtain its textual
definition as the first sentence of the page.5 Next,
3http://babelnet.org
4We follow (Navigli, 2009) and denote with wip the i-th
sense of w in WordNet with part of speech p.
5According to the Wikipedia guidelines, ?The article
should begin with a short declarative sentence, answer-
ing two questions for the nonspecialist reader: What (or
who) is the subject? and Why is this subject notable??,
extracted from http://en.wikipedia.org/wiki/
we extract the hypernym from the textual defini-
tion of p by applying Word-Class Lattices (Navigli
and Velardi, 2010, WCL6), a domain-independent
hypernym extraction system successfully applied
to taxonomy learning from scratch (Velardi et al,
2013) and freely available online (Faralli and Nav-
igli, 2013). If a hypernym h is successfully ex-
tracted and h is linked to a Wikipedia page p?
for which ?(p?) is defined, then we extend the
mapping by setting ?(p) := ?(p?). For instance,
the mapping provided by BabelNet does not pro-
vide any link for the page Peter Spence; thanks to
WCL, though, we are able to set the page Jour-
nalist as its hypernym, and link it to the WordNet
synset journalist1n.This way our mapping extension now covers
539,954 pages, i.e., more than an order of mag-
nitude greater than the number of pages originally
covered by the BabelNet mapping.
3.3.2 Populating the Semantic Classes
We now proceed to populating the semantic
classes in C with the annotated arguments ob-
tained for the lexical predicate pi.
Definition 3 (semantic class of a synset). The
semantic class for a WordNet synset S is the class
c among those in C which is the most specific hy-
pernym of S according to the WordNet taxonomy.
For instance, given the synset tap water1n, its se-mantic class is water1n (while the other more gen-eral subsumers in C are not considered, e.g., com-
pound2n, chemical1n, liquid3n, etc).For each argument a ? A for which a
Wikipedia-to-WordNet mapping ?(sense(a))
could be established as a result of the linking
procedure described above, we associate a with
the semantic class of ?(sense(a)). For example,
consider the case in which a is equal to tap water
and sense(a) is equal to the Wikipedia page Tap
water, in turn mapped to tap water1n via ?; wethus associate tap water with its semantic class
water1n. If more than one class can be found weadd a to each of them.7
Ultimately, for each class c ? C, we obtain
a set support(c) made up of all the arguments
a ? A associated with c. For instance, sup-
port(beverage1n) = { chinese tea, 3.2% beer, hotcocoa, cider, . . . , orange juice }. Note that, thanks
to our extended mapping (cf. Section 3.3.1), the
support of a class can also contain arguments not
covered in WordNet (e.g., hot cocoa and tejuino).
Wikipedia:Writing_better_articles.
6http://lcl.uniroma1.it/wcl
7This can rarely happen due to multiple hypernyms avail-
able in WordNet for the same synset.
1225
Pclass(c|pi) c support(c)
0.1896 wine1n wine, sack, white wine, red wine, wine in china, madeira wine, claret, kosher wine
0.1805 coffee1n turkish coffee, drip coffee, espresso, coffee, cappucino, caffe` latte, decaffeinated coffee, latte
0.1143 herb2n green tea, indian tea, black tea, orange pekoe tea, tea
0.1104 water1n water, seawater
0.0532 beverage1n chinese tea, 3.2% beer, orange soda, boiled water, hot chocolate, hot cocoa, tejuino, cider,
beverage, cocoa, coffee milk, lemonade, orange juice
0.0403 milk1n skim milk, milk, cultured buttermilk, whole milk
0.0351 beer1n 3.2% beer, beer
0.0273 alcohol1n mead, umeshu, kava, rice wine, ja?germeister, kvass, sake, gin, rum
0.0182 poison1n poison
Table 2: Highest-probability semantic classes for the lexical predicate pi = cup of *, according to our set
C of semantic classes.
Since not all classes are equally relevant to the
lexical predicate pi, we estimate the conditional
probability of each class c ? C given pi on the
basis of the number of sentences which contain an
argument in that class. Formally:
Pclass(c|pi) =
?
a?support(c) |sent(a)|
Z , (1)
where Z is a normalization factor calculated as
Z =
?
c??C
?
a?support(c?) |sent(a)|. As an ex-ample, in Table 2 we show the highest-probability
classes for the lexical predicate cup of ?.
As a result of the probabilistic association of
each semantic class c with a target lexical predi-
cate w1 w2 . . . wi ? wi+1 . . . wn, we obtain a
semantic predicate w1 w2 . . . wi c wi+1 . . . wn.
3.4 Classification of new arguments
Once the semantic predicates for the input lexical
predicate pi have been learned, we can classify a
new filling argument a of pi. However, the class
probabilities calculated with Formula 1 might not
provide reliable scores for several classes, includ-
ing unseen ones whose probability would be 0.
To enable wide coverage we estimate a second
conditional probability based on the distributional
semantic profile of each class. To do this, we per-
form three steps:
1. For each WordNet synset S we create a dis-
tributional vector ~S summing the noun occur-
rences within all the Wikipedia pages p such
that ?(p) = S. Next, we create a distribu-
tional vector for each class c ? C as follows:
~c =
?
S?desc(c) ~S,
where desc(c) is the set of all synsets
which are descendants of the semantic class
c in WordNet. As a result we obtain a
predicate-independent distributional descrip-
tion for each semantic class in C.
2. Now, given an argument a of a lexical predi-
cate pi, we create a distributional vector ~a by
summing the noun occurrences of all the sen-
tences s such that (a, s, l) ? Lpi (cf. Section
3.1).
3. Let Ca be the set of candidate semantic
classes for argument a, i.e., Ca contains the
semantic classes for the WordNet synsets of
a as well as the semantic classes associated
with ?(p) for all Wikipedia pages p whose
lemma is a. For each candidate class c ? Ca,
we determine the cosine similarity between
the distributional vectors ~c and ~a as follows:
sim(~c,~a) = ~c ? ~a||~c|| ||~a|| .
Then, we determine the most suitable seman-
tic class c ? Ca of argument a as the class
with the highest distributional probability, es-
timated as:
Pdistr(c|pi, a) =
sim(~c,~a)?
c??Ca sim(~c ?,~a)
. (2)
We can now choose the most suitable class c ?
Ca for argument a which maximizes the proba-
bility mixture of the distributional probability in
Formula 2 and the class probability in Formula 1:
P (c|pi, a) = ?Pdistr(c|pi, a)+(1??)Pclass(c|pi),
(3)
where ? ? [0, 1] is an interpolation factor.
We now illustrate the entire process of our al-
gorithm on a real example. Given a textual ex-
pression such as virus replicate, we: (i) extract
all the filling arguments of the lexical predicate
* replicate; (ii) link and disambiguate the ex-
tracted filling arguments; (iii) query our system for
the available virus semantic classes (i.e., {virus1n,virus3n}); (iv) build the distributional vectors for
1226
the candidate semantic classes and the given in-
put argument; (v) calculate the probability mix-
ture. As a result we obtain the following rank-
ing, virus1n:0.250, virus3n:0.000894, so that the firstsense of virus in WordNet 3.0 is preferred, being
an ?ultramicroscopic infectious agent that repli-
cates itself only within cells of living hosts?.
4 Experiment 1: Oxford Lexical
Predicates
We evaluate on the two forms of output produced
by SPred: (i) the top-ranking semantic classes of a
lexical predicate, as obtained with Formula 1, and
(ii) the classification of a lexical predicate?s argu-
ment with the most suitable semantic class, as pro-
duced using Formula 3. For both evaluations, we
use a lexical predicate dataset built from the Ox-
ford Advanced Learner?s Dictionary (Crowther,
1998).
4.1 Set of Semantic Classes
The selection of which semantic classes to include
in the set C is of great importance. In fact, hav-
ing too many classes will end up in an overly fine-
grained inventory of meanings, whereas an exces-
sively small number of classes will provide lit-
tle discriminatory power. As our set C of seman-
tic classes we selected the standard set of 3,299
core nominal synsets available in WordNet.8 How-
ever, our approach is flexible and can be used with
classes of an arbitrary level of granularity.
4.2 Datasets
The Oxford Advanced Learner?s Dictionary pro-
vides usage notes that contain typical predicates in
various semantic domains in English, e.g., Travel-
ing.9 Each predicate is made up of a fixed part
(e.g., a verb) and a generalizable part which con-
tains one or more nouns.
Examples include fix an election/the vote, bac-
teria/microbes/viruses spread, spend money/sav-
ings/a fortune. In the case that more than one
noun was provided, we split the textual expres-
sion into as many items as the number of nouns.
For instance, from spend money/savings/a fortune
we created three items in our dataset, i.e., spend
money, spend savings, spend a fortune. The split-
ting procedure generated 6,220 instantiated lexical
predicate items overall.
8http://wordnetcode.princeton.edu/
standoff-files/core-wordnet.txt
9http://oald8.oxfordlearnersdictionaries.
com/usage_notes/unbox_colloc/
k Prec@k Correct Total
1 0.94 46 49
2 0.87 85 98
3 0.86 124 145
4 0.83 160 192
5 0.82 194 237
6 0.81 228 282
7 0.80 261 326
8 0.78 288 370
9 0.77 318 414
10 0.76 349 458
11 0.75 379 502
12 0.75 411 546
13 0.75 445 590
14 0.76 479 634
15 0.75 510 678
16 0.75 544 721
17 0.76 577 763
18 0.76 612 806
19 0.76 643 849
20 0.75 671 892
Table 3: Precision@k for ranking the semantic
classes of lexical predicates.
4.3 Evaluating the Semantic Class Ranking
Dataset. Given the above dataset, we general-
ized each item by pairing its fixed verb part with *
(i.e., we keep ?verb predicates? only, since they
are more informative). For instance, the three
items bacteria/microbes/viruses spread were gen-
eralized into the lexical predicate * spread. The to-
tal number of different lexical predicates obtained
was 1,446, totaling 1,429 distinct verbs (note that
the dataset might contain the lexical predicate *
spread as well as spread *).10
Methodology. For each lexical predicate we cal-
culated the conditional probability of each seman-
tic class using Formula 1, resulting in a ranking
of semantic classes. To evaluate the top ranking
classes, we calculated precision@k, with k rang-
ing from 1 to 20, by counting all applicable classes
as correct, e.g., location1n is a valid semantic classfor travel to * while emotion1n is not.
Results. We show in Table 3 the precision@k
calculated over a random sample of 50 lexical
predicates.11 As can be seen, while the classes
quality is pretty high with low values of k, per-
formance gradually degrades as we let k increase.
This is mostly due to the highly polysemous nature
of the predicates selected (e.g., occupy *, leave *,
help *, attain *, live *, etc.). We note that high per-
formance, attaining above 80%, can be achieved
10The low number of items per predicate is due to the orig-
inal Oxford resource.
11One lexical predicate did not have any semantic class
ranking.
1227
by focusing up to the first 7 classes output by our
system, with a 94% precision@1.
4.4 Evaluating Classification Performance
Dataset. Starting from the lexical predicate
items obtained as described in Section 4.2, we se-
lected those items belonging to a random sample
of 20 usage notes among those provided by the
Oxford dictionary, totaling 3,245 items. We then
manually tagged each item?s argument (e.g., virus
in viruses spread) with the most suitable seman-
tic class (e.g., virus1n), obtaining a gold standarddataset for the evaluation of our argument classifi-
cation algorithm (cf. Section 3.4).
Methodology. In this second evaluation we
measure the accuracy of our method at assigning
the most suitable semantic class to the argument
of a lexical predicate item in our gold standard.
We use three customary measures to determine the
quality of the acquired semantic classes, i.e., pre-
cision, recall and F1. Precision is the number of
items which are assigned the correct class (as eval-
uated by a human) over the number of items which
are assigned a class by the system. Recall is the
number of items which are assigned the correct
class over the number of items to be classified. F1
is the harmonic mean of precision and recall.
Tuning. The only parameter to be tuned is the
factor ? that we use to mix the two probabilities
in Formula 3 (cf. Section 3.4). For tuning ? we
used a held-out set of 8 verbs, randomly sampled
from the lexical predicates not used in the dataset.
We created a tuning set using the annotated argu-
ments in Wikipedia for these verbs: we trained the
model on 80% of the annotated lexical predicate
arguments (i.e., the class probability estimates in
Formula 1) and then applied the probability mix-
ture (i.e., Formula 3) for classifying the remain-
ing 20% of arguments. Finally, we calculated the
performance in terms of precision, recall and F1
with 11 different values of ? ? {0, 0.1, . . . , 1.0},
achieving optimal performance with ? = 0.2.
Results. Table 4 shows the results on the seman-
tic class assignments. Our system shows very high
precision, above 85%, while at the same time at-
taining an adequate 68% recall. We also compared
against a random baseline that randomly selects
one out of all the candidate semantic classes for
each item, achieving only moderate results. A sub-
sequent error analysis revealed the common types
of error produced by our system: terms for which
we could not provide (1) any WordNet concept
Method Precision Recall F1
SPred 85.61 68.01 75.80
Random 40.96 40.96 40.96
Table 4: Performance on semantic class assign-
ment.
(e.g., political corruption) or (2) any candidate se-
mantic class (e.g., immune system).
4.5 Disambiguation heuristics impact
As a follow-up analysis, for each dataset we con-
sidered the impact of each disambiguation heuris-
tic described in Section 3.2 according to how many
times it was triggered. Starting from the entire set
of 1,446 lexical predicates from the Oxford dictio-
nary (see Section 4.3), we counted the number of
argument triples (a, s, l) already disambiguated in
Wikipedia (i.e., l 6= ) and those disambiguated
thanks to our disambiguation strategies. Table
5 shows the statistics. We note that, while the
amount of originally linked arguments is very low
(about 2.5% of total), our strategies are able to
considerably increase the size of the initial set of
linked instances. The most effective strategies ap-
pear to be the One sense per page and the Trust the
inventory, which contribute 26.16% and 31.33%
of the total links, respectively.
Even though most of the triples (i.e., 68 out of
almost 74 million) remain unlinked, the ratio of
distinct arguments which we linked to WordNet
is considerably higher, calculated as 3,723,979
linked arguments over 12,431,564 distinct argu-
ments, i.e., about 30%.
5 Experiment 2: Comparison with
Kozareva & Hovy (2010)
Due to the novelty of the task carried out by SPred,
the resulting output may be compared with only a
limited number of existing approaches. The most
similar approach is that of Kozareva and Hovy
(2010, K&H) who assign supertypes to the argu-
ments of arbitrary relations, a task which resem-
bles our semantic predicate ranking. We therefore
performed a comparison on the quality of the most
highly-ranked supertypes (i.e., semantic classes)
using their dataset of 24 relation patterns (i.e., lex-
ical predicates).
Dataset. The dataset contained 14 lexical pred-
icates (e.g., work for * or * fly to), 10 of which
were expanded in order to semantify their left- and
right-side arguments (e.g., * work for and work
for *); for the remaining 4 predicates just a single
1228
Total Linked in One sense One sense per Trust the Not
triples Wikipedia per page lexical predicate inventory linked
73,843,415 1,795,608 1,433,634 533,946 1,716,813 68,363,414
Table 5: Statistics on argument triple linking for all the lexical predicates in the Oxford dataset.
k Prec@k Correct Total
1 0.88 21 24
2 0.90 43 48
3 0.88 63 72
4 0.89 85 96
5 0.91 109 120
6 0.91 131 144
7 0.92 154 168
8 0.91 175 192
9 0.92 198 216
10 0.92 221 240
11 0.92 242 264
12 0.92 264 288
13 0.91 284 312
14 0.90 304 336
15 0.91 327 360
16 0.91 348 384
17 0.90 367 408
18 0.89 386 432
19 0.89 407 456
20 0.89 429 480
Table 6: Precision@k for the semantic classes of
the relations of Kozareva and Hovy (2010).
side was generalized (e.g., * dress). While most of
the relations apply to persons as a supertype, our
method could find arguments for each of them.
Methodology. We carried out the same evalua-
tion as in Section 4.3. We calculated precision@k
of the semantic classes obtained for each relation
in the dataset of K&H. Because the set of appli-
cable classes was potentially unbounded, we were
not able to report recall directly.
Results. K&H reported an overall accuracy of
the top-20 supertypes of 92%. As can be seen in
Table 6 we exhibit very good performance with in-
creasing values of k. A comparison of Table 3 with
Table 6 shows considerable differences in perfor-
mance between the two datasets. We attribute this
difference to the higher average WordNet poly-
semy of the verbal component of the Oxford pred-
icates (on average 2.64 senses for K&H against
6.52 for the Oxford dataset).
Although we cannot report recall, we list the
number of Wikipedia arguments and associated
classes in Table 7, which provides an estimate of
the extraction capability of SPred. The large num-
ber of classes found for the arguments demon-
strates the ability of our method to generalize to
a variety of semantic classes.
Predicate Number of args Number of classes
cause * 181,401 1,339
live in * 143,628 600
go to * 134,712 867
* cause 92,160 1,244
work in * 79,444 770
* go to 71,794 746
* live in 61,074 541
work on * 58,760 840
work for * 58,332 681
work at * 31,904 511
* work in 24,933 528
* celebrate 23,333 408
Table 7: Number of arguments and associated
classes for the 12 most frequent lexical predicates
of Kozareva and Hovy (2010) extracted by SPred
from Wikipedia.
6 Related work
The availability of Web-scale corpora has led to
the production of large resources of relations (Et-
zioni et al, 2005; Yates et al, 2007; Wu and Weld,
2010; Carlson et al, 2010; Fader et al, 2011).
However, these resources often operate purely at
the lexical level, providing no information on the
semantics of their arguments or relations. Several
studies have examined adding semantics through
grouping relations into sets (Yates and Etzioni,
2009), ontologizing the arguments (Chklovski and
Pantel, 2004), or ontologizing the relations them-
selves (Moro and Navigli, 2013). However, analy-
sis has largely been either limited to ontologizing
a small number of relation types with a fixed in-
ventory, which potentially limits coverage, or has
used implicit definitions of semantic categories
(e.g., clusters of arguments), which limits inter-
pretability. For example, Mohamed et al (2011)
use the semantic categories of the NELL system
(Carlson et al, 2010) to learn roughly 400 valid
ontologized relations from over 200M web pages,
whereas WiSeNet (Moro and Navigli, 2012) lever-
ages Wikipedia to acquire relation synsets for an
open set of relations. Despite these efforts, no
large-scale resource has existed to date that con-
tains ontologized lexical predicates. In contrast,
the present work provides a high-coverage method
for learning argument supertypes from a broad-
coverage ontology (WordNet), which can poten-
tially be leveraged in relation extraction to ontolo-
1229
gize relation arguments.
Our method for identifying the different seman-
tic classes of predicate arguments is closely related
to the task of identifying selectional preferences.
The most similar approaches to it are taxonomy-
based ones, which leverage the semantic types
of the relations arguments (Resnik, 1996; Li and
Abe, 1998; Clark and Weir, 2002; Pennacchiotti
and Pantel, 2006). Nevertheless, despite their high
quality sense-tagged data, these methods have of-
ten suffered from lack of coverage. As a result,
alternative approaches have been proposed that es-
chew taxonomies in favor of rating the quality of
potential relation arguments (Erk, 2007; Cham-
bers and Jurafsky, 2010) or generating probabil-
ity distributions over the arguments (Rooth et al,
1999; Pantel et al, 2007; Bergsma et al, 2008;
Ritter et al, 2010; Se?aghdha, 2010; Bouma, 2010;
Jang and Mostow, 2012) in order to obtain higher
coverage of preferences.
In contrast, we overcome the data sparsity of
class-based models by leveraging the large quan-
tity of collaboratively-annotated Wikipedia text in
order to connect predicate arguments with their
semantic class in WordNet using BabelNet (Nav-
igli and Ponzetto, 2012); because we map directly
to WordNet synsets, we provide a more readily-
interpretable collocation preference model than
most similarity-based or probabilistic models.
Verb frame extraction (Green et al, 2004) and
predicate-argument structure analysis (Surdeanu
et al, 2003; Yakushiji et al, 2006) are two areas
that are also related to our work. But their gener-
ality goes beyond our intentions, as we focus on
semantic predicates, which is much simpler and
free from syntactic parsing.
Another closely related work is that of Hanks
(2013) concerning the Theory of Norms and Ex-
ploitations, where norms (exploitations) represent
expected (unexpected) classes for a given lexical
predicate. Although our semantified predicates do,
indeed, provide explicit evidence of norms ob-
tained from collective intelligence and would pro-
vide support for this theory, exploitations present
a more difficult task, different from the one ad-
dressed here, due to its focus on identifying prop-
erty transfer between the semantic class and the
exploited instance.
The closest technical approach to ours is that
of Kozareva and Hovy (2010), who use recursive
patterns to induce semantic classes for the argu-
ments of relational patterns. Whereas their ap-
proach requires both a relation pattern and one
or more seeds, which bias the types of semantic
classes that are learned, our proposed method re-
quires only the pattern itself, and as a result is ca-
pable of learning an unbounded number of differ-
ent semantic classes.
7 Conclusions
In this paper we present SPred, a novel approach
to large-scale harvesting of semantic predicates.
In order to semantify lexical predicates we ex-
ploit the wide coverage of Wikipedia to extract
and disambiguate lexical predicate occurrences,
and leverage WordNet to populate the semantic
classes with suitable predicate arguments. As a re-
sult, we are able to ontologize lexical predicate in-
stances like those available in existing dictionaries
(e.g., break a toe) into semantic predicates (such
as break a BODY PART).
For each lexical predicate (such as break ?),
our method produces a probability distribution
over the set of semantic classes (thus covering the
different expected meanings for the filling argu-
ments) and is able to classify new instances with
the most suitable class. Our experiments show
generally high performance, also in comparison
with previous work on argument supertyping.
We hope that our semantic predicates will en-
able progress in different Natural Language Pro-
cessing tasks such as Word Sense Disambigua-
tion (Navigli, 2009), Semantic Role Labeling
(Fu?rstenau and Lapata, 2012) or even Textual En-
tailment (Stern and Dagan, 2012) ? each of which
is in urgent need of reliable semantics. While we
focused on semantifying lexical predicates, as fu-
ture work we will apply our method to the ontol-
ogization of large amounts of sequences of words,
such as phrases or textual relations (e.g., consid-
ering Google n-grams appearing in Wikipedia).
Notably, our method should, in principle, gener-
alize to any semantically-annotated corpus (e.g.,
Wikipedias in other languages), provided lexical
predicates can be extracted with associated seman-
tic classes.
In order to support future efforts we are releas-
ing our semantic predicates as a freely available
resource.12
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
Thanks go to David A. Jurgens, Silvia Necs?ulescu,
Stefano Faralli and Moreno De Vincenzi for their
help.
12http://lcl.uniroma1.it/spred
1230
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional prefer-
ence from unlabeled text. In Proc. of EMNLP, pages
59?68, Stroudsburg, PA, USA.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crystal-
lization point for the Web of Data. Web Semantics,
7(3):154?165.
Gerlof Bouma. 2010. Collocation Extraction beyond
the Independence Assumption. In Proc. of ACL,
Short Papers, pages 109?114, Uppsala, Sweden.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proc. of AAAI, pages 1306?1313,
Atlanta, Georgia.
Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selec-
tional preferences. In Proc. of ACL, pages 445?453,
Stroudsburg, PA, USA.
Tim Chklovski and Patrick Pantel. 2004. VerbOcean:
Mining the Web for fine-grained semantic verb rela-
tions. In Proc. of EMNLP, pages 33?40, Barcelona,
Spain.
Jennifer Chu-Carroll and John Prager. 2007. An exper-
imental study of the impact of information extraction
accuracy on semantic search performance. In Proc.
of CIKM, pages 505?514, Lisbon, Portugal.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-Coverage Sense Disambiguation and Infor-
mation Extraction with a Supersense Sequence Tag-
ger. In Proc. of EMNLP, pages 594?602, Sydney,
Australia.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Jonathan Crowther, editor. 1998. Oxford Advanced
Learner?s Dictionary. Cornelsen & Oxford, 5th edi-
tion.
Flavio De Benedictis, Stefano Faralli, and Roberto
Navigli. 2013. GlossBoot: Bootstrapping multilin-
gual domain glossaries from the Web. In Proc. of
ACL, Sofia, Bulgaria.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.
In Proc. of CIKM, pages 1099?1108, New York, NY,
USA.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proc. of EMNLP, pages 440?
449, Stroudsburg, PA, USA.
Katrin Erk. 2007. A Simple, Similarity-based Model
for Selectional Preferences. In Proc. of ACL, pages
216?223, Prague, Czech Republic.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proc. of EMNLP, pages 1535?1545,
Edinburgh, UK.
Stefano Faralli and Roberto Navigli. 2013. A Java
framework for multilingual definition and hypernym
extraction. In Proc. of ACL, Comp. Volume, Sofia,
Bulgaria.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: an overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Hagen Fu?rstenau and Mirella Lapata. 2012. Semi-
supervised semantic role labeling via structural
alignment. Computational Linguistics, 38(1):135?
171.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proc. of
HLT-NAACL, pages 1?8, Edmonton, Canada.
Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing Frame Semantic Verb Classes from
WordNet and LDOCE. In Proc. of ACL, pages 375?
382, Barcelona, Spain.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. University Press Group Limited.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of COL-
ING, pages 539?545, Nantes, France.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194:28?61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and artificial intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-Specific Semantic Class Taggers from (Al-
most) Nothing. In Proc. of ACL, pages 275?285,
Uppsala, Sweden.
Sean P. Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with Web-based corrobo-
ration. In Proc. of UMSLLS, pages 18?26, Strouds-
burg, PA, USA.
Rube?n Izquierdo, Armando Sua?rez, and German Rigau.
2009. An Empirical Study on Class-Based Word
Sense Disambiguation. In Proc. of EACL, pages
389?397, Athens, Greece.
Hyeju Jang and Jack Mostow. 2012. Inferring se-
lectional preferences from part-of-speech n-grams.
In Proc. of EACL, pages 377?386, Stroudsburg, PA,
USA.
1231
Boris Katz, Jimmy J. Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew W. Bilotti, Sue Felshin, Aaron
Fernandes, Gregory Marton, and Federico Mora.
2003. Integrating Web-based and Corpus-based
Techniques for Question Answering. In Proc. of
TREC, pages 426?435, Gaithersburg, Maryland.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
Arguments and Supertypes of Semantic Relations
Using Recursive Patterns. In Proc. of ACL, pages
1482?1491, Uppsala, Sweden.
Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy.
2008. Semantic Class Learning from the Web
with Hyponym Pattern Linkage Graphs. In Proc.
ACL/HLT, pages 1048?1056, Columbus, Ohio.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proc. of ISWC 2012, Part I, pages 263?278,
Boston, MA.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational Linguistics, 24(2):217?244.
Rada Mihalcea and Dan Moldovan. eXtended Word-
Net: Progress report. In Proceedings of the NAACL-
01 Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, Penn.
Thahir Mohamed, Estevam Hruschka, and Tom
Mitchell. 2011. Discovering Relations between
Noun Categories. In Proc. of EMNLP, pages 1447?
1455, Edinburgh, Scotland, UK.
Andrea Moro and Roberto Navigli. 2012. WiSeNet:
Building a Wikipedia-based semantic network with
ontologized relations. In Proc. of CIKM, pages
1672?1676, Maui, HI, USA.
Andrea Moro and Roberto Navigli. 2013. Integrating
Syntactic and Semantic Analysis into the Open In-
formation Extraction Paradigm. In Proc. of IJCAI,
Beijing, China.
Vivi Nastase and Michael Strube. 2013. Transform-
ing wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62?85.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Proc. of ACL, pages 1318?1327, Up-
psala, Sweden.
Roberto Navigli. 2009. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: learning inferential selec-
tional preferences. In Proc. of NAACL, pages 564?
571, Rochester, NY.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of CIKM, pages
137?145, New York, NY, USA.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proc. of COLING,
pages 793?800, Sydney, Australia.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. Artificial Intelligence, 175(9-
10):1737?1756.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127?159.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional pref-
erences. In Proc. of ACL, pages 424?434, Uppsala,
Sweden. ACL.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a seman-
tically annotated lexicon via EM-based clustering.
In Proc. of ACL, pages 104?111, Stroudsburg, PA,
USA.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proc. of ACL, pages
435?444, Uppsala, Sweden.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning Syntactic Patterns for Automatic Hyper-
nym Discovery. In NIPS, pages 1297?1304, Cam-
bridge, Mass.
Asher Stern and Ido Dagan. 2012. Biutee: A mod-
ular open-source system for recognizing textual en-
tailment. In Proc. of ACL 2012, System Demonstra-
tions, pages 73?78, Jeju Island, Korea.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proc. ACL,
pages 8?15, Stroudsburg, PA, USA.
M. Thelen and E. Riloff. 2002. A Bootstrapping
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts. In Proc. of EMNLP, pages
214?221, Salt Lake City, UT, USA.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3).
Yorick Wilks. 1975. A preferential, pattern-seeking,
semantics for natural language inference. Artificial
Intelligence, 6(1):53?74.
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction Using Wikipedia. In Proc. of ACL, pages
118?127, Uppsala, Sweden.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka
Tateisi, and Jun?ichi Tsujii. 2006. Automatic con-
struction of predicate-argument structure patterns
for biomedical information extraction. In Proc. of
EMNLP, pages 284?292, Stroudsburg, PA, USA.
David Yarowsky. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proc. of ACL, pages 189?196, Cambridge, MA,
USA.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: open informa-
tion extraction on the web. In Proc. of NAACL-
Demonstrations, pages 25?26, Stroudsburg, PA,
USA.
1232
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341?1351,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Align, Disambiguate and Walk: A Unified Approach for
Measuring Semantic Similarity
Mohammad Taher Pilehvar, David Jurgens and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{pilehvar,jurgens,navigli}@di.uniroma1.it
Abstract
Semantic similarity is an essential com-
ponent of many Natural Language Pro-
cessing applications. However, prior meth-
ods for computing semantic similarity of-
ten operate at different levels, e.g., sin-
gle words or entire documents, which re-
quires adapting the method for each data
type. We present a unified approach to se-
mantic similarity that operates at multiple
levels, all the way from comparing word
senses to comparing text documents. Our
method leverages a common probabilistic
representation over word senses in order to
compare different types of linguistic data.
This unified representation shows state-of-
the-art performance on three tasks: seman-
tic textual similarity, word similarity, and
word sense coarsening.
1 Introduction
Semantic similarity is a core technique for many
topics in Natural Language Processing such as
Textual Entailment (Berant et al, 2012), Seman-
tic Role Labeling (Fu?rstenau and Lapata, 2012),
and Question Answering (Surdeanu et al, 2011).
For example, textual similarity enables relevant
documents to be identified for information re-
trieval (Hliaoutakis et al, 2006), while identifying
similar words enables tasks such as paraphrasing
(Glickman and Dagan, 2003), lexical substitution
(McCarthy and Navigli, 2009), lexical simplifica-
tion (Biran et al, 2011), and Web search result
clustering (Di Marco and Navigli, 2013).
Approaches to semantic similarity have often
operated at separate levels: methods for word sim-
ilarity are rarely applied to documents or even sin-
gle sentences (Budanitsky and Hirst, 2006; Radin-
sky et al, 2011; Halawi et al, 2012), while
document-based similarity methods require more
linguistic features, which often makes them in-
applicable at the word or microtext level (Salton
et al, 1975; Maguitman et al, 2005; Elsayed et
al., 2008; Turney and Pantel, 2010). Despite the
potential advantages, few approaches to semantic
similarity operate at the sense level due to the chal-
lenge in sense-tagging text (Navigli, 2009); for ex-
ample, none of the top four systems in the recent
SemEval-2012 task on textual similarity compared
semantic representations that incorporated sense
information (Agirre et al, 2012).
We propose a unified approach to semantic sim-
ilarity across multiple representation levels from
senses to documents, which offers two signifi-
cant advantages. First, the method is applicable
independently of the input type, which enables
meaningful similarity comparisons across differ-
ent scales of text or lexical levels. Second, by op-
erating at the sense level, a unified approach is able
to identify the semantic similarities that exist in-
dependently of the text?s lexical forms and any se-
mantic ambiguity therein. For example, consider
the sentences:
t1. A manager fired the worker.
t2. An employee was terminated from work by
his boss.
A surface-based approach would label the sen-
tences as dissimilar due to the minimal lexical
overlap. However, a sense-based representation
enables detection of the similarity between the
meanings of the words, e.g., fire and terminate.
Indeed, an accurate, sense-based representation is
essential for cases where different words are used
to convey the same meaning.
The contributions of this paper are threefold.
First, we propose a new unified representation of
the meaning of an arbitrarily-sized piece of text,
referred to as a lexical item, using a sense-based
probability distribution. Second, we propose a
novel alignment-based method for word sense dis-
1341
ambiguation during semantic comparison. Third,
we demonstrate that this single representation can
achieve state-of-the-art performance on three sim-
ilarity tasks, each operating at a different lexical
level: (1) surpassing the highest scores on the
SemEval-2012 task on textual similarity (Agirre
et al, 2012) that compares sentences, (2) achiev-
ing a near-perfect performance on the TOEFL syn-
onym selection task proposed by Landauer and
Dumais (1997), which measures word pair sim-
ilarity, and also obtaining state-of-the-art perfor-
mance in terms of the correlation with human
judgments on the RG-65 dataset (Rubenstein and
Goodenough, 1965), and finally (3) surpassing the
performance of Snow et al (2007) in a sense-
coarsening task that measures sense similarity.
2 A Unified Semantic Representation
We propose a representation of any lexical item as
a distribution over a set of word senses, referred
to as the item?s semantic signature. We begin
with a formal description of the representation at
the sense level (Section 2.1). Following this, we
describe our alignment-based disambiguation al-
gorithm which enables us to produce sense-based
semantic signatures for those lexical items (e.g.,
words or sentences) which are not sense annotated
(Section 2.2). Finally, we propose three methods
for comparing these signatures (Section 2.3). As
our sense inventory, we use WordNet 3.0 (Fell-
baum, 1998).
2.1 Semantic Signatures
The WordNet ontology provides a rich net-
work structure of semantic relatedness, connect-
ing senses directly with their hypernyms, and pro-
viding information on semantically similar senses
by virtue of their nearby locality in the network.
Given a particular node (sense) in the network, re-
peated random walks beginning at that node will
produce a frequency distribution over the nodes
in the graph visited during the walk. To ex-
tend beyond a single sense, the random walk may
be initialized and restarted from a set of senses
(seed nodes), rather than just one; this multi-seed
walk produces a multinomial distribution over all
the senses in WordNet with higher probability as-
signed to senses that are frequently visited from
the seeds. Prior work has demonstrated that multi-
nomials generated from random walks over Word-
Net can be successfully applied to linguistic tasks
such as word similarity (Hughes and Ramage,
2007; Agirre et al, 2009), paraphrase recogni-
tion, textual entailment (Ramage et al, 2009),
and pseudoword generation (Pilehvar and Navigli,
2013).
Formally, we define the semantic signature of
a lexical item as the multinomial distribution gen-
erated from the random walks over WordNet 3.0
where the set of seed nodes is the set of senses
present in the item. This representation encom-
passes both when the item is itself a single sense
and when the item is a sense-tagged sentence.
To construct each semantic signature, we use
the iterative method for calculating topic-sensitive
PageRank (Haveliwala, 2002). Let M be the ad-
jacency matrix for the WordNet network, where
edges connect senses according to the rela-
tions defined in WordNet (e.g., hypernymy and
meronymy). We further enrich M by connecting
a sense with all the other senses that appear in its
disambiguated gloss.1 Let ~v(0) denote the prob-
ability distribution for the starting location of the
random walker in the network. Given the set of
senses S in a lexical item, the probability mass
of ~v(0) is uniformly distributed across the senses
si ? S, with the mass for all sj /? S set to zero.
The PageRank may then be computed using:
~v (t) = (1? ?)M~v (t?1) + ? ~v (0) (1)
where at each iteration the random walker may
jump to any node si ? S with probability ?/|S|.
We follow standard convention and set ? to 0.15.
We repeat the operation in Eq. 1 for 30 itera-
tions, which is sufficient for the distribution to
converge. The resulting probability vector ~v(t) is
the semantic signature of the lexical item, as it
has aggregated its senses? similarities over the en-
tire graph. For our semantic signatures we used
the UKB2 off-the-shelf implementation of topic-
sensitive PageRank.
2.2 Alignment-Based Disambiguation
Commonly, semantic comparisons are between
word pairs or sentence pairs that do not have their
lexical content sense-annotated, despite the poten-
tial utility of sense annotation in making seman-
tic comparisons. However, traditional forms of
word sense disambiguation are difficult for short
texts and single words because little or no con-
textual information is present to perform the dis-
ambiguation task. Therefore, we propose a novel
1http://wordnet.princeton.edu
2http://ixa2.si.ehu.es/ukb/
1342
Figure 1: (a) Example alignments of the first sense of term manager (in sentence t1) to the two first
senses of the word types in sentence t2, along with the similarity of the two senses? semantic signatures;
(b) Alignments which maximize the similarities across words in t1 and t2 (the source side of an alignment
is taken as the disambiguated sense of its corresponding word).
alignment-based sense disambiguation that lever-
ages the content of the paired item in order to dis-
ambiguate each element. Leveraging the paired
item enables our approach to disambiguate where
traditional sense disambiguation methods can not
due to insufficient context.
We view sense disambiguation as an alignment
problem. Given two arbitrarily ordered texts, we
seek the semantic alignment that maximizes the
similarity of the senses of the context words in
both texts. To find this maximum we use an align-
ment procedure which, for each word type wi in
item T1, assigns wi to the sense that has the max-
imal similarity to any sense of the word types in
the compared text T2. Algorithm 1 formalizes the
alignment process, which produces a sense dis-
ambiguated representation as a result. Senses are
compared in terms of their semantic signatures,
denoted as function R. We consider multiple def-
initions ofR, defined later in Section 2.3.
As a part of the disambiguation procedure, we
leverage the one sense per discourse heuristic of
Yarowsky (1995); given all the word types in two
compared lexical items, each type is assigned a
single sense, even if it is used multiple times. Ad-
ditionally, if the same word type appears in both
sentences, both will always be mapped to the same
sense. Although such a sense assignment is poten-
tially incorrect, assigning both types to the same
sense results in a representation that does no worse
than a surface-level comparison.
We illustrate the alignment-based disambigua-
tion procedure using the two example sentences t1
and t2 given in Section 1. Figure 1(a) illustrates
example alignments of the first sense of manager
to the first two senses of the word types in sentence
t2 along with the similarity of the two senses?
Algorithm 1 Alignment-based Sense Disambiguation
Input: T1 and T2, the sets of word types being compared
Output: P , the set of disambiguated senses for T1
1: P ? ?
2: for each token ti ? T1
3: max sim? 0
4: best si? null
5: for each token tj ? T2
6: for each si ? Senses(ti), sj ? Senses(tj)
7: sim?R(si, sj)
8: if sim > max sim then
9: max sim = sim
10: best si = si
11: P ? P ? {best si}
12: return P
semantic signatures. For the senses of manager,
sense manager1n obtains the maximal similarity
value to boss1n among all the possible pairings of
the senses for the word types in sentence t2, and as
a result is selected as the sense labeling for man-
ager in sentence t1.3 Figure 1(b) shows the final,
maximally-similar sense alignment of the word
types in t1 and t2. The resulting alignment pro-
duces the following sets of senses:
Pt1 = {manager1n, fire4v, worker1n}
Pt2 = {employee1n, terminate4v, work3n, boss2n}
where Px denotes the corresponding set of senses
of sentence x.
2.3 Semantic Signature Similarity
Cosine Similarity. In order to compare seman-
tic signatures, we adopt the Cosine similarity mea-
sure as a baseline method. The measure is com-
puted by treating each multinomial as a vector and
then calculating the normalized dot product of the
two signatures? vectors.
3We follow Navigli (2009) and denote with wip the i-th
sense of w in WordNet with part of speech p.
1343
However, a semantic signature is, in essence,
a weighted ranking of the importance of Word-
Net senses for each lexical item. Given that the
WordNet graph has a non-uniform structure, and
also given that different lexical items may be of
different sizes, the magnitudes of the probabilities
obtained may differ significantly between the two
multinomial distributions. Therefore, for com-
puting the similarity of two signatures, we also
consider two nonparametric methods that use the
ranking of the senses, rather than their probability
values, in the multinomial.
Weighted Overlap. Our first measure provides
a nonparametric similarity by comparing the simi-
larity of the rankings for intersection of the senses
in both semantic signatures. However, we addi-
tionally weight the similarity such that differences
in the highest ranks are penalized more than differ-
ences in lower ranks. We refer to this measure as
the Weighted Overlap. Let S denote the intersec-
tion of all senses with non-zero probability in both
signatures and rji denote the rank of sense si ? S
in signature j, where rank 1 denotes the highest
rank. The sum of the two ranks r1i and r2i for a
sense is then inverted, which (1) weights higher
ranks more and (2) when summed, provides the
maximal value when a sense has the same rank in
both signatures. The unnormalized weighted over-
lap is then calculated as?|S|i=1(r1i + r2i )?1. Then,
to bound the similarity value in [0, 1], we normal-
ize the sum by its maximum value, ?|S|i=1(2i)?1,
which occurs when each sense has the same rank
in both signatures.
Top-k Jaccard. Our second measure uses the
ranking to identify the top-k senses in a signa-
ture, which are treated as the best representatives
of the conceptual associates. We hypothesize that
a specific rank ordering may be attributed to small
differences in the multinomial probabilities, which
can lower rank-based similarities when one of the
compared orderings is perturbed due to slightly
different probability values. Therefore, we con-
sider the top-k senses as an unordered set, with
equal importance in the signature. To compare two
signatures, we compute the Jaccard Index of the
two signatures? sets:
RJac(Uk, Vk) =
|Uk ? Vk|
|Uk ? Vk|
(2)
whereUk denotes the set of k senses with the high-
est probability in the semantic signature U .
Dataset MSRvid MSRpar SMTeuroparl OnWN SMTnews
Training 750 750 734 - -
Test 750 750 459 750 399
Table 1: Statistics of the provided datasets for the
SemEval-2012 Semantic Textual Similarity task.
3 Experiment 1: Textual Similarity
Measuring semantic similarity of textual items has
applications in a wide variety of NLP tasks. As
our benchmark, we selected the recent SemEval-
2012 task on Semantic Textual Similarity (STS),
which was concerned with measuring the seman-
tic similarity of sentence pairs. The task received
considerable interest by facilitating a meaningful
comparison between approaches.
3.1 Experimental Setup
Data. We follow the experimental setup used in
the STS task (Agirre et al, 2012), which provided
five test sets, two of which had accompanying
training data sets for tuning system performance.
Each sentence pair in the datasets was given a
score from 0 to 5 (low to high similarity) by hu-
man judges, with a high inter-annotator agreement
of around 0.90 when measured using the Pearson
correlation coefficient. Table 1 lists the number of
sentence pairs in training and test portions of each
dataset.
Comparison Systems. The top-ranking partic-
ipating systems in the SemEval-2012 task were
generally supervised systems utilizing a variety of
lexical resources and similarity measurement tech-
niques. We compare our results against the top
three systems of the 88 submissions: TLsim and
TLsyn, the two systems of S?aric? et al (2012), and
the UKP2 system (Ba?r et al, 2012). UKP2 utilizes
extensive resources among which are a Distribu-
tional Thesaurus computed on 10M dependency-
parsed English sentences. In addition, the sys-
tem utilizes techniques such as Explicit Semantic
Analysis (Gabrilovich and Markovitch, 2007) and
makes use of resources such as Wiktionary and
Wikipedia, a lexical substitution system based on
supervised word sense disambiguation (Biemann,
2013), and a statistical machine translation sys-
tem. The TLsim system uses the New York Times
Annotated Corpus, Wikipedia, and Google Book
Ngrams. The TLsyn system also uses Google
Book Ngrams, as well as dependency parsing and
named entity recognition.
1344
Ranking System Overall Dataset-specificALL ALLnrm Mean ALL ALLnrm Mean Mpar Mvid SMTe OnWN SMTn
1 1 1 ADW 0.866 0.871 0.711 0.694 0.887 0.555 0.706 0.604
2 3 2 UKP2 0.824 0.858 0.677 0.683 0.873 0.528 0.664 0.493
3 4 6 TLsyn 0.814 0.857 0.660 0.698 0.862 0.361 0.704 0.468
4 2 3 TLsim 0.813 0.864 0.675 0.734 0.880 0.477 0.679 0.398
Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) in
the SemEval-2012 Semantic Textual Similarity task. Rightmost columns report the corresponding Pear-
son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),
OnWN (OnWN) and SMTnews (SMTn). We also provide scores according to the three official evalua-
tion metrics (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics.
System Configuration. Here we describe the
configuration of our approach, which we have
called Align, Disambiguate and Walk (ADW). The
STS task uses human similarity judgments on an
ordinal scale from 0 to 5. Therefore, in ADW we
adopted a similar approach to generating similar-
ity values to that adopted by other participating
systems, whereby a supervised system is trained
to combine multiple similarity judgments to pro-
duce a final rating consistent with the human an-
notators. We utilized the WEKA toolkit (Hall et
al., 2009) to train a Gaussian Processes regression
model for each of the three training sets (cf. Table
1). The features discussed hereafter were consid-
ered in our regression model.
Main features. We used the scores calculated
using all three of our semantic signature compar-
ison methods as individual features. Although the
Jaccard comparison is parameterized, we avoided
tuning and instead used four features for distinct
values of k: 250, 500, 1000, and 2500.
String-based features. Additionally, because
the texts often contain named entities which are
not present in WordNet, we incorporated the sim-
ilarity values produced by four string-based mea-
sures, which were used by other teams in the STS
task: (1) longest common substring which takes
into account the length of the longest overlap-
ping contiguous sequence of characters (substring)
across two strings (Gusfield, 1997), (2) longest
common subsequence which, instead, finds the
longest overlapping subsequence of two strings
(Allison and Dix, 1986), (3) Greedy String Tiling
which allows reordering in strings (Wise, 1993),
and (4) the character/word n-gram similarity pro-
posed by Barro?n-Ceden?o et al (2010).
We followed S?aric? et al (2012) and used the
models trained on the SMTeuroparl and MSRpar
datasets for testing on the SMTnews and OnWN
test sets, respectively.
3.2 STS Results
Three evaluation metrics are provided by the or-
ganizers of the SemEval-2012 STS task, all of
which are based on Pearson correlation r of human
judgments with system outputs: (1) the correla-
tion value for the concatenation of all five datasets
(ALL), (2) a correlation value obtained on a con-
catenation of the outputs, separately normalized
by least square (ALLnrm), and (3) the weighted
average of Pearson correlations across datasets
(Mean). Table 2 shows the scores obtained by
ADW for the three evaluation metrics, as well as
the Pearson correlation values obtained on each
of the five test sets (rightmost columns). We also
show the results obtained by the three top-ranking
participating systems (i.e., UKP2, TLsim, and TL-
syn). The leftmost three columns show the system
rankings according to the three metrics.
As can be seen from Table 2, our system (ADW)
outperforms all the 88 participating systems ac-
cording to all the evaluation metrics. Our sys-
tem shows a statistically significant improvement
on the SMTnews dataset, with an increase in the
Pearson correlation of over 0.10. MSRpar (MPar)
is the only dataset in which TLsim (S?aric? et al,
2012) achieves a higher correlation with human
judgments. Named entity features used by the TL-
sim system could be the reason for its better per-
formance on the MSRpar dataset, which contains
a large number of named entities.
3.3 Similarity Measure Analysis
To gain more insight into the impact of our
alignment-based disambiguation approach, we
carried out a 10-fold cross-validation on the three
training datasets (cf. Table 1) using the systems
described hereafter.
ADW-MF. To build this system, we utilized our
main features only; i.e., we did not make use of
additional string-based features.
1345
DW. Similarly to ADW-MF, this system utilized
the main features only. In DW, however, we re-
placed our alignment-based disambiguation phase
with a random walk-based WSD system that dis-
ambiguated the sentences separately, without per-
forming any alignment. As our WSD system,
we used UKB, a state-of-the-art knowledge-based
WSD system that is based on the same topic-
sensitive PageRank algorithm used by our ap-
proach. UKB initializes the algorithm from all
senses of the words in the context of a word to
be disambiguated. It then picks the most relevant
sense of the word according to the resulting prob-
ability vector. As the lexical knowledge base of
UKB, we used the same semantic network as that
utilized by our approach for calculating semantic
signatures.
Table 3 lists the performance values of the two
above-mentioned systems on the three training
sets in terms of Pearson correlation. In addition,
we present in the table correlation scores for four
other similarity measures reported by Ba?r et al
(2012):
? Pairwise Word Similarity that comprises of
a set of WordNet-based similarity measures
proposed by Resnik (1995), Jiang and Con-
rath (1997), and Lin (1998b). The aggre-
gation strategy proposed by Corley and Mi-
halcea (2005) has been utilized for extend-
ing these word-to-word similarity measures
for calculating text-to-text similarities.
? Explicit Semantic Analysis (Gabrilovich
and Markovitch, 2007) where the high-
dimensional vectors are obtained on Word-
Net, Wikipedia and Wiktionary.
? Distributional Thesaurus where a similarity
score is computed similarly to that of Lin
(1998a) using a distributional thesaurus ob-
tained from a 10M dependency-parsed sen-
tences of English newswire.
? Character n-grams which were also used as
one of our additional features.
As can be seen from Table 3, our alignment-
based disambiguation approach (ADW-MF) is
better suited to the task than a conventional WSD
approach (DW). Another interesting point is the
high scores achieved by the Character n-grams
Similarity measure DatasetMpar Mvid SMTe
DW 0.448 0.820 0.660
ADW-MF 0.485 0.842 0.721
Explicit Semantic Analysis 0.427 0.781 0.619
Pairwise Word Similarity 0.564 0.835 0.527
Distributional Thesaurus 0.494 0.481 0.365
Character n-grams 0.658 0.771 0.554
Table 3: Performance of our main-feature sys-
tem with conventional WSD (DW) and with the
alignment-based disambiguation approach (ADW-
MF) vs. four other similarity measures, using 10-
fold cross validation on the training datasets MSR-
par (Mpar), MSRvid (Mvid), and SMTeuroparl
(SMTe).
measure. This confirms that string-based meth-
ods are strong baselines for semantic textual sim-
ilarity. Except for the MSRpar (Mpar) dataset,
our system (ADW-MF) outperforms all other sim-
ilarity measures. The scores obtained by Explicit
Semantic Analysis and Distributional Thesaurus
are not competitive on any dataset. On the other
hand, Pairwise Word Similarity achieves a high
performance on MSRpar and MSRvid datasets,
but performs surprisingly low on the SMTeuroparl
dataset.
4 Experiment 2: Word Similarity
We now proceed from the sentence level to the
word level. Word similarity has been a key prob-
lem for lexical semantics, with significant efforts
being made by approaches in distributional se-
mantics to accurately identify synonymous words
(Turney and Pantel, 2010). Different evaluation
methods exist in the literature for evaluating the
performance of a word-level semantic similarity
measure; we adopted two well-established bench-
marks: synonym recognition and correlating word
similarity judgments with those from human an-
notators.
For synonym recognition, we used the TOEFL
dataset created by Landauer and Dumais (1997).
The dataset consists of 80 multiple-choice syn-
onym questions from the TOEFL test; a word is
paired with four options, one of which is a valid
synonym. Test takers with English as a second
language averaged 64.5% correct. Despite multi-
ple approaches, only recently has the test been an-
swered perfectly (Bullinaria and Levy, 2012), un-
derscoring the challenge of synonym recognition.
1346
Approach Accuracy
PPMIC (Bullinaria and Levy, 2007) 85.00%
GLSA (Matveeva et al, 2005) 86.25%
LSA (Rapp, 2003) 92.50%
ADWJac 93.75?2.5%
ADWWO 95.00%
ADWCos 96.25%
PR (Turney et al, 2003) 97.50%
PCCP (Bullinaria and Levy, 2012) 100.00%
Table 4: Accuracy on the 80-question TOEFL
Synonym test. ADWJac, ADWWO, and ADWCos
correspond to results with the Jaccard, Weighted
Overlap and Cosine signature comparison mea-
sures, respectively.
For the similarity judgment evaluation, we
used as benchmark the RG-65 dataset created by
Rubenstein and Goodenough (1965). The dataset
contains 65 word pairs judged by 51 human sub-
jects on a scale of 0 to 4 according to their seman-
tic similarity. Ideally, a measure?s similarity judg-
ments are expected to be highly correlated with
those of humans. To be consistent with the previ-
ous literature (Hughes and Ramage, 2007; Agirre
et al, 2009), we used Spearman?s rank correlation
in our experiment.
4.1 Experimental Setup
Our alignment-based sense disambiguation trans-
forms the task of comparing individual words
into that of calculating the similarity of the best-
matching sense pair across the two words. As
there is no training data we do not optimize the k
value for computing signature similarity with the
Jaccard index; instead, we report, for the synonym
recognition and the similarity judgment evalua-
tions, the respective range of accuracies and the
average correlation obtained upon using five val-
ues of k randomly selected in the range [50, 2500]:
678, 1412, 1692, 2358, 2387.
4.2 Word Similarity Results: TOEFL dataset
Table 4 lists the accuracy performance of the sys-
tem in comparison to the existing state of the
art on the TOEFL test. ADWWO, ADWCos,
and ADWJac correspond to our approach when
Weighted Overlap, Cosine, and Jaccard signa-
ture comparison measures are used, respectively.
Despite not being tuned for the task, our model
achieves near-perfect performance, answering all
but three questions correctly with the Cosine mea-
sure. Among the top-performing approaches, only
Word Synonym choices (correct in bold)
fanciful familiar apparent? imaginative? logical
verbal oral? overt fitting verbose?
resolved settled? forgotten? publicized examined
percentage volume sample proportion profit??
figure list solve? divide? express
highlight alter? imitate accentuate? restore
Table 5: Questions answered incorrectly by our
approach. Symbols ? and ? correspond to the
choices of our approach with the Weighted Over-
lap and Cosine signature comparisons respec-
tively. We do not include the mistakes made when
the Jaccard measure was used as they vary with
the k value.
that of Rapp (2003) uses word senses, an approach
that is outperformed by our method.
The errors produced by our system were largely
the result of sense locality in the WordNet net-
work. Table 5 highlights the incorrect responses.
The synonym mistakes reveal cases where senses
of the two words are close in WordNet, indicating
some relatedness. For example, percentage may
be interpreted colloquially as monetary value (e.g.,
?give me my percentage?) and elicits the synonym
of profit in the economic domain, which ADW in-
correctly selects as a synonym.
4.3 Word Similarity Results: RG-65 dataset
Table 6 shows the Spearman?s ? rank correlation
coefficients with human judgments on the RG-65
dataset. As can be seen from the Table, our ap-
proach with the Weighted Overlap signature com-
parison improves over the similar approach of
Hughes and Ramage (2007) which, however, does
not involve the disambiguation step and considers
a word as a whole unit as represented by the set of
its senses.
5 Experiment 3: Sense Similarity
WordNet is known to be a fine-grained sense in-
ventory with many related word senses (Palmer et
al., 2007). Accordingly, multiple approaches have
attempted to identify highly similar senses in or-
der to produce a coarse-grained sense inventory.
We adopt this task as a way of evaluating our sim-
ilarity measure at the sense level.
5.1 Coarse-graining Background
Earlier work on reducing the polysemy of sense
inventories has considered WordNet-based sense
relatedness measures (Mihalcea and Moldovan,
2001) and corpus-based vector representations of
1347
Approach Correlation
ADWCos 0.825
Agirre et al (2009) 0.830
Hughes and Ramage (2007) 0.838
Zesch et al (2008) 0.840
ADWJac 0.841
ADWWO 0.868
Table 6: Spearman?s ? correlation coefficients
with human judgments on the RG-65 dataset.
ADWJac, ADWWO, and ADWCos correspond to
results with the Jaccard, Weighted Overlap and
Cosine signature comparison measures respec-
tively.
word senses (Agirre and Lopez, 2003; McCarthy,
2006). Navigli (2006) proposed an automatic ap-
proach for mapping WordNet senses to the coarse-
grained sense distinctions of the Oxford Dictio-
nary of English (ODE). The approach leverages
semantic similarities in gloss definitions and the
hierarchical relations between senses in the ODE
to cluster WordNet senses. As current state of
the art, Snow et al (2007) developed a super-
vised SVM classifier that utilized, as its features,
several earlier sense relatedness techniques such
as those implemented in the WordNet::Similarity
package (Pedersen et al, 2004). The classifier
also made use of resources such as topic signatures
data (Agirre and de Lacalle, 2004), the WordNet
domain dataset (Magnini and Cavaglia`, 2000), and
the mappings of WordNet senses to ODE senses
produced by Navigli (2006).
5.2 Experimental Setup
We benchmark the accuracy of our similarity mea-
sure in grouping word senses against those of Nav-
igli (2006) and Snow et al (2007) on two datasets
of manually-labeled sense groupings of WordNet
senses: (1) sense groupings provided as a part of
the Senseval-2 English Lexical Sample WSD task
(Kilgarriff, 2001) which includes nouns, verbs and
adjectives; (2) sense groupings included in the
OntoNotes project4 (Hovy et al, 2006) for nouns
and verbs. Following the evaluation methodology
of Snow et al (2007), we combine the Senseval-2
and OntoNotes datasets into a third dataset.
Snow et al (2007) considered sense grouping as
a binary classification task whereby for each word
every possible pairing of senses has to be classified
4Sense groupings belong to a pre-version 1.0: http://
cemantix.org/download/sense/ontonotes-sense-groups.tar.gz
Onto SE-2 Onto + SE-2
Method Noun Verb Noun Verb Adj Noun Verb
RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485
RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503
RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493SVM 0.370 0.455 NA NA 0.473 0.423 0.432
ODE 0.218 0.396 NA NA 0.371 0.331 0.288
Table 7: F-score sense merging evaluation on
three hand-labeled datasets: OntoNotes (Onto),
Senseval-2 (SE-2), and combined (Onto+SE-2).
Results are reported for all three of our signature
comparison measures and also for two previous
works (last two rows).
as either merged or not-merged. We constructed
a simple threshold-based classifier to perform the
same binary classification. To this end, we cal-
culated the semantic similarity of each sense pair
and then used a threshold value t to classify the
pair as merged if similarity ? t and not-merged
otherwise. We sampled out 10% of the dataset for
tuning the value of t, thus adapting our classifier
to the fine granularity of the dataset. We used the
same held-out instances to perform a tuning of the
k value used for Jaccard index, over the same val-
ues of k as in Experiment 1 (cf. Section 3).
5.3 Sense Merging Results
For a binary classification task, we can directly
calculate precision, recall and F-score by con-
structing a contingency table. We show in Ta-
ble 7 the F-score performance of our classifier as
obtained by an averaged 10-fold cross-validation.
Results are presented for all three of the mea-
sures of semantic signature comparison and for
the three datasets: OntoNotes, Senseval-2, and
the two combined. In addition, we show in Ta-
ble 7 the F-score results provided by Snow et al
(2007) for their SVM-based system and for the
mapping-based approach of Navigli (2006), de-
noted by ODE.
Table 7 shows that our methodology yields im-
provements over previous work on both datasets
and for all parts of speech, irrespective of
the semantic signature comparison method used.
Among the three methods, Weighted Overlap
achieves the best performance, which demon-
strates that our transformation of semantic signa-
tures into ordered lists of concepts and calculating
similarity by rank comparison has been helpful.
1348
6 Related Work
Due to the wide applicability of semantic similar-
ity, significant efforts have been made at different
lexical levels. Early work on document-level sim-
ilarity was driven by information retrieval. Vector
space methods provided initial successes (Salton
et al, 1975), but often suffer from data spar-
sity when using small documents, or when doc-
uments use different word types, as in the case
of paraphrases. Later efforts such as LSI (Deer-
wester et al, 1990), PLSA (Hofmann, 2001) and
Topic Models (Blei et al, 2003; Steyvers and Grif-
fiths, 2007) overcame these sparsity issues using
dimensionality reduction techniques or modeling
the document using latent variables. However,
such methods were still most suitable for compar-
ing longer texts. Complementary approaches have
been developed specifically for comparing shorter
texts, such as those used in the SemEval-2012
STS task (Agirre et al, 2012). Most similar to
our approach are the methods of Islam and Inkpen
(2008) and Corley and Mihalcea (2005), who per-
formed a word-to-word similarity alignment; how-
ever, they did not operate at the sense level. Ram-
age et al (2009) used a similar semantic represen-
tation of short texts from random walks on Word-
Net, which was applied to paraphrase recognition
and textual entailment. However, unlike our ap-
proach, their method does not perform sense dis-
ambiguation prior to building the representation
and therefore potentially suffers from ambiguity.
A significant amount of effort has also been put
into measuring similarity at the word level, fre-
quently by approaches that use distributional se-
mantics (Turney and Pantel, 2010). These meth-
ods use contextual features to represent semantics
at the word level, whereas our approach represents
word semantics at the sense level. Most similar to
our approach are those of Agirre et al (2009) and
Hughes and Ramage (2007), which represent word
meaning as the multinomials produced from ran-
dom walks on the WordNet graph. However, un-
like our approach, neither of these disambiguates
the two words being compared, which potentially
conflates the meanings and lowers the similarity
judgment.
Measures of sense relatedness have frequently
leveraged the structural properties of WordNet
(e.g., path lengths) to compare senses. Budanit-
sky and Hirst (2006) provided a survey of such
WordNet-based measures. The main drawback
with these approaches lies in the WordNet struc-
ture itself, where frequently two semantically sim-
ilar senses are distant in the WordNet hierar-
chy. Possible solutions include relying on wider-
coverage networks such as WikiNet (Nastase and
Strube, 2013) or multilingual ones such as Babel-
Net (Navigli and Ponzetto, 2012b). Fewer works
have focused on measuring the similarity ? as op-
posed to relatedness ? between senses. The topic
signatures method of Agirre and Lopez (2003)
represents each sense as a vector over corpus-
derived features in order to build comparable sense
representations. However, topic signatures often
produce lower quality representations due to spar-
sity in the local structure of WordNet, especially
for rare senses. In contrast, the random walk
used in our approach provides a denser, and thus
more comparable, representation for all WordNet
senses.
7 Conclusions
This paper presents a unified approach for comput-
ing semantic similarity at multiple lexical levels,
from word senses to texts. Our method leverages
a common probabilistic representation at the sense
level for all types of linguistic data. We demon-
strate that our semantic representation achieves
state-of-the-art performance in three experiments
using semantic similarity at different lexical levels
(i.e., sense, word, and text), surpassing the per-
formance of previous similarity measures that are
often specifically targeted for each level.
In future work, we plan to explore the impact of
the sense inventory-based network used in our se-
mantic signatures. Specifically, we plan to investi-
gate higher coverage inventories such as BabelNet
(Navigli and Ponzetto, 2012a), which will handle
texts with named entities and rare senses that are
not in WordNet, and will also enable cross-lingual
semantic similarity. Second, we plan to evaluate
our method on larger units of text and formalize
comparison methods between different lexical lev-
els.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Sameer S. Pradhan
for providing us with an earlier version of the
OntoNotes dataset.
1349
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly
available topic signatures for all WordNet nominal senses.
In Proceedings of LREC, pages 1123?1126, Lisbon, Por-
tugal.
Eneko Agirre and Oier Lopez. 2003. Clustering WordNet
word senses. In Proceedings of RANLP, pages 121?130,
Borovets, Bulgaria.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, Marius Pas?ca, and Aitor Soroa. 2009. A study
on similarity and relatedness using distributional and
WordNet-based approaches. In Proceedings of NAACL,
pages 19?27, Boulder, Colorado.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proceedings of SemEval-2012, pages
385?393, Montreal, Canada.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string longest-
common-subsequence algorithm. Information Processing
Letters, 23(6):305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual similar-
ity by combining multiple content similarity measures. In
Proceedings of SemEval-2012, pages 435?440, Montreal,
Canada.
Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism detection across distant
language pairs. In Proceedings of COLING, pages 37?45,
Beijing, China.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012.
Learning entailment relations by global graph structure
optimization. Computational Linguistics, 38(1):73?111.
Chris Biemann. 2013. Creating a system for lexical sub-
stitutions from scratch using crowdsourcing. Language
Resources and Evaluation, 47(1):97?122.
Or Biran, Samuel Brody, and Noe?mie Elhadad. 2011.
Putting it simply: a context-aware approach to lexical sim-
plification. In Proceedings of ACL, pages 496?501, Port-
land, Oregon.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent Dirichlet Allocation. The Journal of Machine
Learning Research, 3:993?1022.
Alexander Budanitsky and Graeme Hirst. 2006. Evaluat-
ing WordNet-based measures of Lexical Semantic Relat-
edness. Computational Linguistics, 32(1):13?47.
John A. Bullinaria and Joseph. P. Levy. 2007. Extracting
semantic representations from word co-occurrence statis-
tics: A computational study. Behavior Research Methods,
(3):510.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
semantic representations from word co-occurrence statis-
tics: stop-lists, stemming, and SVD. Behavior Research
Methods, 44:890?907.
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment, pages 13?18, Ann Arbor, Michigan.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of American
Society for Information Science, 41(6):391?407.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and diversifying Web search results with graph-based
Word Sense Induction. Computational Linguistics, 39(3).
Tamer Elsayed, Jimmy Lin, and Douglas W. Oard. 2008.
Pairwise document similarity in large collections with
MapReduce. In Proceedings of ACL-HLT, pages 265?
268, Columbus, Ohio.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Hagen Fu?rstenau and Mirella Lapata. 2012. Semi-supervised
Semantic Role Labeling via structural alignment. Compu-
tational Linguistics, 38(1):135?171.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of IJCAI, pages 1606?
1611, Hyderabad, India.
Oren Glickman and Ido Dagan. 2003. Acquiring lexi-
cal paraphrases from a single corpus. In Proceedings of
RANLP, pages 81?90, Borovets, Bulgaria.
Dan Gusfield. 1997. Algorithms on strings, trees, and se-
quences: computer science and computational biology.
Cambridge University Press.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda
Koren. 2012. Large-scale learning of word relatedness
with constraints. In Proceedings of KDD, pages 1406?
1414, Beijing, China.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The WEKA data mining software: an update. ACM
SIGKDD Explorations Newsletter, 11(1):10?18.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In
Proceedings of WWW, pages 517?526, Hawaii, USA.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,
Euripides GM Petrakis, and Evangelos Milios. 2006.
Information retrieval by semantic similarity. Interna-
tional Journal on Semantic Web and Information Systems,
2(3):55?73.
Thomas Hofmann. 2001. Unsupervised Learning by Prob-
abilistic Latent Semantic Analysis. Machine Learning,
42(1):177?196.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The
90% solution. In Proceedings of NAACL, pages 57?60,
NY, USA.
Thad Hughes and Daniel Ramage. 2007. Lexical semantic
relatedness with random graph walks. In Proceedings of
EMNLP-CoNLL, pages 581?589, Prague, Czech Repub-
lic.
Aminul Islam and Diana Inkpen. 2008. Semantic text sim-
ilarity using corpus-based word similarity and string sim-
ilarity. ACM Transactions on Knowledge Discovery from
Data, 2(2):10:1?10:25.
Jay J. Jiang and David W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy. In
Proceedings of ROCLING X, pages 19?30, Taiwan.
1350
Adam Kilgarriff. 2001. English lexical sample task descrip-
tion. In Proceedings of Senseval, pages 17?20, Toulouse,
France.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review; Psychological Review, 104(2):211.
Dekang Lin. 1998a. Automatic retrieval and clustering of
similar words. In Proceedings of COLING, pages 768?
774, Montreal, Quebec, Canada.
Dekang Lin. 1998b. An information-theoretic definition of
similarity. In Proceedings of ICML, pages 296?304, San
Francisco, CA.
Bernardo Magnini and Gabriela Cavaglia`. 2000. Integrat-
ing subject field codes into WordNet. In Proceedings of
LREC, pages 1413?1418, Athens, Greece.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and
Alessandro Vespignani. 2005. Algorithmic detection of
semantic similarity. In Proceedings of WWW, pages 107?
116, Chiba, Japan.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christiaan Royer. 2005. Terms representation with gener-
alized latent semantic analysis. In Proceedings of RANLP,
Borovets, Bulgaria.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Evalu-
ation, 43(2):139?159.
Diana McCarthy. 2006. Relating WordNet senses for word
sense disambiguation. In Proceedings of the Workshop on
Making Sense of Sense at EACL-06, pages 17?24, Trento,
Italy.
Rada Mihalcea and Dan Moldovan. 2001. Automatic gen-
eration of a coarse grained WordNet. In Proceedings
of NAACL Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, USA.
Vivi Nastase and Michael Strube. 2013. Transforming
Wikipedia into a large scale multilingual concept network.
Artificial Intelligence, 194:62?85.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network.
Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b. Babel-
Relate! a joint multilingual approach to computing seman-
tic relatedness. In Proceedings of AAAI, pages 108?114,
Toronto, Canada.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost Word Sense Disambiguation performance. In
Proceedings of COLING-ACL, pages 105?112, Sydney,
Australia.
Roberto Navigli. 2009. Word Sense Disambiguation: A sur-
vey. ACM Computing Surveys, 41(2):1?69.
Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2007.
Making fine-grained and coarse-grained sense distinc-
tions, both manually and automatically. Natural Lan-
guage Engineering, 13(2):137?163.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity - measuring the relatedness of
concepts. In Proceedings of AAAI, pages 144?152, San
Jose, CA.
Mohammad Taher Pilehvar and Roberto Navigli. 2013.
Paving the way to a large-scale pseudosense-annotated
dataset. In Proceedings of NAACL-HLT, pages 1100?
1109, Atlanta, USA.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and
Shaul Markovitch. 2011. A word at a time: comput-
ing word relatedness using temporal semantic analysis. In
Proceedings of WWW, pages 337?346, Hyderabad, India.
Daniel Ramage, Anna N. Rafferty, and Christopher D. Man-
ning. 2009. Random walks for text semantic similarity. In
Proceedings of the 2009 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 23?31, Sun-
tec, Singapore.
Reinhard Rapp. 2003. Word sense discovery based on sense
descriptor dissimilarity. In Proceedings of the Ninth Ma-
chine Translation Summit, pages 315?322, New Orleans,
LA.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings of
IJCAI, pages 448?453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Gerard Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications of
the ACM, 18(11):613?620.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y.
Ng. 2007. Learning to merge word senses. In EMNLP-
CoNLL, pages 1005?1014, Prague, Czech Republic.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-factoid
questions from Web collections. Computational Linguis-
tics, 37(2):351?383.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent modules
to solve multiple-choice synonym and analogy problems.
In Proceedings of RANLP, pages 482?489, Borovets, Bul-
garia.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder, and
Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems for
measuring semantic text similarity. In Proceedings of
SemEval-2012, pages 441?448, Montreal, Canada.
Michael J. Wise. 1993. String similarity via greedy string
tiling and running Karp-Rabin matching. In Department
of Computer Science Technical Report, Sydney.
David Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation rivaling supervised methods. In Proceedings of
ACL, pages 189?196, Cambridge, Massachusetts.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych. 2008.
Using Wiktionary for computing semantic relatedness. In
Proceedings of AAAI, pages 861?866, Chicago, Illinois.
1351
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 103?108,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Java Framework for Multilingual Definition and Hypernym Extraction
Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{faralli,navigli}@di.uniroma1.it
Abstract
In this paper we present a demonstra-
tion of a multilingual generalization of
Word-Class Lattices (WCLs), a super-
vised lattice-based model used to identify
textual definitions and extract hypernyms
from them. Lattices are learned from a
dataset of automatically-annotated defini-
tions from Wikipedia. We release a Java
API for the programmatic use of multilin-
gual WCLs in three languages (English,
French and Italian), as well as a Web ap-
plication for definition and hypernym ex-
traction from user-provided sentences.
1 Introduction
Electronic dictionaries and domain glossaries are
definition repositories which prove very useful not
only for lookup purposes, but also for automatic
tasks such as Question Answering (Cui et al,
2007; Saggion, 2004), taxonomy learning (Navigli
et al, 2011; Velardi et al, 2013), domain Word
Sense Disambiguation (Duan and Yates, 2010;
Faralli and Navigli, 2012), automatic acquisition
of semantic predicates (Flati and Navigli, 2013),
relation extraction (Yap and Baldwin, 2009) and,
more in general, knowledge acquisition (Hovy et
al., 2013). Unfortunately, constructing and updat-
ing such resources requires the effort of a team of
experts. Moreover, they are of no help when deal-
ing with new words or usages, or, even worse, new
domains. Nonetheless, raw text often contains
several definitional sentences, that is, it provides
within itself formal explanations for terms of inter-
est. Whilst it is not feasible to search texts manu-
ally for definitions in several languages, the task of
extracting definitional information can be autom-
atized by means of Machine Learning (ML) and
Natural Language Processing (NLP) techniques.
Many approaches (Snow et al, 2004; Kozareva
and Hovy, 2010, inter alia) build upon lexico-
syntactic patterns, inspired by the seminal work
of Hearst (1992). However, these methods suf-
fer from two signifiicant drawbacks: on the one
hand, low recall (as definitional sentences occur in
highly variable syntactic structures), and, on the
other hand, noise (because the most frequent def-
initional pattern ? X is a Y ? is inherently very
noisy). A recent approach to definition and hyper-
nym extraction, called Word-Class Lattices (Nav-
igli and Velardi, 2010, WCLs), overcomes these
issues by addressing the variability of definitional
sentences and providing a flexible way of automat-
ically extracting hypernyms from them. To do so,
lattice-based classifiers are learned from a training
set of textual definitions. Training sentences are
automatically clustered by similarity and, for each
such cluster, a lattice classifier is learned which
models the variants of the definition template de-
tected. A lattice is a directed acyclic graph, a
subclass of non-deterministic finite state automata.
The purpose of the lattice structure is to preserve
(in a compact form) the salient differences among
distinct sequences.
In this paper we present a demonstration of
Word-Class Lattices by providing a Java API and
a Web application for online usage. Since multi-
linguality is a key need in today?s information so-
ciety, and because WCLs have been tested over-
whelmingly only with the English language, we
provide experiments for three different languages,
namely English, French and Italian. To do so, in
contrast to Navigli and Velardi (2010), who cre-
ated a manually annotated training set of defini-
tions, we provide a heuristic method for the au-
tomatic acquisition of reliable training sets from
Wikipedia, and use them to determine the robust-
ness and generalization power of WCLs. We show
high performance in definition and hypernym ex-
traction for our three languages.
2 Word-Class Lattices
In this section we briefly summarize Word-Class
Lattices, originally introduced by Navigli and Ve-
lardi (2010).
2.1 Definitional Sentence Generalization
WCL relies on a formal notion of textual defi-
nition. Specifically, given a definition, e.g.: ?In
computer science, a closure is a first-class func-
tion with free variables that are bound in the lex-
ical environment?, we assume that it contains the
103
[In geography, a country]DF [is]V F [a political division]GF .
[In finance, a bond]DF [is]V F [a negotiable certificate]GF [that that acknowledges. . . ]REST .
[In poetry, a foot]DF [is]V F [a measure]GF [, consisting. . . ]REST .
Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italics).
In
geography
finance
poetry
NN1 , a ?TARGET?
foot
bond
country
a
political
negotiable
JJ NN2
division
certificate
measure
Figure 1: The DF and GF Word-Class Lattices for the sentences in Table 1.
following fields (Storrer and Wellinghoff, 2006):
definiendum (DF), definitor (VF), definiens (GF)
and rest (REST), where DF is the part of the
definition including the word being defined (e.g.,
?In computer science, a closure?), VF is the verb
phrase used to introduce the definition (e.g., ?is?),
GF usually includes the hypernym (e.g., ?a first-
class function?, hypernym marked in italics) and
RF includes additional clauses (e.g., ?with free
variables that are bound in the lexical environ-
ment?).
Consider a set of training sentences T , each
of which is automatically part-of-speech tagged
and manually bracketed with the DF, VF, GF and
REST fields (examples are shown in Table 1). We
first identify the set of most frequent words F
(e.g., the, a, is, of, refer, etc.). Then we add
the symbol ?TARGET? to F and replace in T the
terms being defined with ?TARGET?. We then use
the set of frequent words F to generalize words to
?word classes?.
We define a word class as either a word itself
or its part of speech. Given a sentence s =
w1, w2, . . . , w|s|, where wi is the i-th word of s,we generalize its words wi to word classes ?i as
follows:
?i =
{
wi if wi ? F
POS(wi) otherwise
that is, a word wi is left unchanged if it occurs fre-
quently in the training corpus (i.e., wi ? F ) or is
transformed to its part of speech tag (POS(wi))
otherwise. As a result, we obtain a generalized
sentence s? = ?1, ?2, . . . , ?|s|. For instance,given the first sentence in Table 1, we obtain the
corresponding generalized sentence: ?In NN, a
?TARGET? is a JJ NN?, where NN and JJ indicate
the noun and adjective classes, respectively.
2.2 Learning
The WCL learning algorithm consists of 3 steps:
? Star patterns: each sentence in the training
set is preprocessed and generalized to a star
pattern by replacing with * all the words wi 6?
F , i.e., non-frequent words. For instance, ?In
geography, a country is a political division?
is transformed to ?In *, a ?TARGET? is a *?;
? Sentence clustering: the training sentences
are then clustered based on the star patterns
they belong to;
? Word-Class Lattice construction: for each
sentence cluster, a WCL is created separately
for each DF, VF and GF field by means of a
greedy alignment algorithm. In Figure 1 we
show the resulting lattices for the DF and GF
fields built for the cluster of sentences of Ta-
ble 1. Note that during the construction of the
lattice the nodes associated with the hyper-
nym words in the learning sentences (i.e., di-
vision, certificate and measure) are marked as
hypernyms in order to determine the hyper-
nym of a test sentence at classification time
(see (Navigli and Velardi, 2010) for details).
2.3 Classification
Once the learning process is over, a set of WCLs
is produced for the DF, VF and GF fields. Given
a test sentence s, we consider all possible combi-
nations of definiendum, definitor and definiens lat-
tices and select the combination of the three WCLs
that best fits the sentence, if such a combination
exists. In fact, choosing the most appropriate
combination of lattices impacts the performance
of hypernym extraction. The best combination
of WCLs is selected by maximizing the follow-
ing confidence score: score(s, lDF , lV F , lGF ) =
coverage ? log(support+1) where s is the candi-
date sentence, lDF , lV F and lGF are three latticesone for each definition field, coverage is the frac-
tion of words of the input sentence covered by the
three lattices, and support is the sum of the num-
ber of sentences in the star patterns corresponding
to the GF lattice. Finally, when a sentence is clas-
sified as a definition, its hypernym is extracted by
104
# Wikipedia pages # definitions extracted
English (EN) 3,904,360 1,552,493
French (FR) 1,617,359 447,772
Italian (IT) 1,008,044 291,259
Table 2: The number of Wikipedia pages and the
resulting automatically annotated definitions.
selecting the words in the input sentence that are
marked as hypernyms in the WCL selected for GF.
3 Multilingual Word-Class Lattices
In order to enable multilinguality, thereby extract-
ing definitions and hypernyms in many languages,
we provide here a heuristic method for the creation
of multilingual training datasets from Wikipedia,
that we apply to three languages: English, French
and Italian. As a result, we are able to fully au-
tomatize the definition and hypernym extraction
by utilizing collaboratively-curated encyclopedia
content.
3.1 Automatic Learning of Multilingual
WCLs
The method consists of four steps:
1. candidate definition extraction: we iterate
through the collection of Wikipedia pages for
the language of interest. For each article we
extract the first paragraph, which usually, but
not always, contains a definitional sentence
for the concept expressed by the page title.
We discard all those pages for which the title
corresponds to a special page (i.e., title in the
form ?List of [. . . ]?, ?Index of [. . . ]?, ?[. . . ]
(disambiguation)? etc.).
2. part-of-speech tagging and phrase chunk-
ing: for each candidate definition we per-
form part-of-speech tagging and chunking,
thus automatically identifying noun, verb,
and prepositional phrases (we use TreeTag-
ger (Schmid, 1997)).
3. automatic annotation: we replace all the oc-
currences in the candidate definition of the
target term (i.e., the title of the page) with
the marker ?TARGET?, we then tag as hyper-
nym the words associated with the first hy-
perlink occurring to the right of ?TARGET?.
Then we tag as VF (i.e., definitor field,
see Section 2.1) the verb phrase found be-
tween ?TARGET? and the hypernym, if such
a phrase exists. Next we tag as GF (i.e.,
definiens field) the phrase which contains the
hypernym and as DF (i.e., definiendum field)
the phrase which starts at the beginning of
the sentence and ends right before the start
of the VP tag. Finally we mark as REST the
remaining phrases after the phrase already
tagged as GF. For example, given the sen-
tence ?Albert Einstein was a German-born
theoretical physicist.?, we produce the fol-
lowing sentence annotation: ?[Albert Ein-
stein]DF [was]V F [a German-born theoreti-cal physicist]GF .? (target term marked inbold and hypernym in italics).
4. filtering: we finally discard all the candidate
definitions for which not all fields could be
found during the previous step (i.e., either the
?TARGET?, hypernym or any DF, VF, GF,
REST tag is missing).
We applied the above four steps to the En-
glish, French and Italian dumps of Wikipedia1.
The numbers are shown in Table 2: starting with
3,904,360 Wikipedia pages for English, 1,617,359
for French and 1,008,044 for Italian (first column),
we obtained 1,552,493, 447,772, and 291,259 au-
tomatically tagged sentences, respectively, for the
three languages (second column in the Table).
Since we next had to use these sentences for train-
ing our WCLs, we took out a random sample
of 1000 sentences for each language which we
used for testing purposes. We manually annotated
each of these sentences as definitional or non-
definitional2 and, in the case of the former, also
with the correct hypernym.
3.2 Evaluation
We tested the newly acquired training dataset
against two test datasets. The first dataset was
our random sampling of 1000 Wikipedia test sen-
tences which we had set aside for each language
(no intersection with the training set, see previous
section). The second dataset was the same one
used in Navigli and Velardi (2010), made up of
sentences from the ukWaC Web corpus (Ferraresi
et al, 2008) and used to estimate the definition and
hypernym extraction performance on an open text
corpus.
3.3 Results
Table 3 shows the results obtained on definition
(column 2-4) and hypernym extraction (column 5-
7) in terms of precision (P), recall (R) and accu-
racy (A) on our first dataset. Note that accuracy
also takes into account candidate definitions in
the test set which were tagged as non-definitional
(see Section 3.1). In the Table we compare the
performance of our English WCL trained from
Wikipedia sentences using our automatic proce-
dure against the original performance of WCL
1We used the 21-09-2012 (EN), 17-09-2012 (FR), 21-09-
2012 (IT) dumps.
2Note that the first sentence of a Wikipedia page might
seldom be non-definitional, such as ?Basmo fortress is lo-
cated in the north-western part . . . ?.
105
Definition Extraction Hypernym Extraction
P R A P R A
EN 98.5 78.3 81.0 98.5 77.4 80.0
FR 98.7 83.3 84.0 98.6 78.0 79.0
IT 98.8 87.3 87.0 98.7 83.2 83.0
EN (2010) 100.0 59.0 66.0 100.0 58.3 65.0
Table 3: Precision (P), recall (R) and accuracy
(A) of definition and hypernym extraction when
testing on our dataset of 1000 randomly sam-
pled Wikipedia first-paragraph sentences. EN
(2010) refers to the WCL learned from the origi-
nal manually-curated training set from Navigli and
Velardi (2010), while EN, FR and IT refer to WCL
trained, respectively, with one of the three training
sets automatically acquired from Wikipedia.
P R
EN 98.9 57.6
EN (2010) 94.8 56.5
Table 4: Estimated WCL definition extraction
precision (P) and recall (R), testing a sample of
ukWaC sentences.
trained on 1,908 manually-selected training sen-
tences3. It can be seen that the automatically ac-
quired training set considerably improves the per-
formance, as it covers higher variability. We note
that the recall in both definition and hypernym ex-
traction is higher for French and Italian. We at-
tribute this behavior to the higher complexity and,
again, variability of English Wikipedia pages, and
specifically first-sentence definitions. We remark
that the presented results were obtained without
any human effort, except for the independent col-
laborative editing and hyperlinking of Wikipedia
pages, and that the overall performances can be
improved by manually checking the automatically
annotated training datasets.
We also replicated the experiment carried out
by Navigli and Velardi (2010), testing WCLs with
a subset (over 300,000 sentences) of the ukWaC
Web corpus. As can be seen in Table 4, the
estimated precision and recall for WCL defini-
tion extraction with the 2010 training set were
94.8% and 56.5%, respectively, while with our au-
tomatically acquired English training set we ob-
tained a higher precision of 98.9% and a recall of
57.6%. This second experiment shows that learn-
ing WCLs from hundreds of thousands of defini-
tion candidates does not overfit to Wikipedia-style
definitional sentences.
After looking at the automatically acquired
training datasets, we noted some erroneous an-
notations mainly due to the following factors: i)
some Wikipedia pages do not start with defini-
3Available from http://lcl.uniroma1.it/wcl
1 // select the language of interest
2 Language targetLanguage = Language.EN;
3 // open the training set
4 Dataset ts = new AnnotatedDataset(
5 trainingDatasetFile,
6 targetLanguage);
7 // obtain an instance of the WCL classifier
8 WCLClassifier c = new WCLClassifier(targetLanguage);
9 c.train(ts);
10 // create a sentence to be tested
11 Sentence sentence = Sentence.createFromString(
12 "WCL",
13 "WCL is a kind of classifier.",
14 targetLanguage);
15 // test the sentence
16 SentenceAnnotation sa = c.test(sentence);
17 // print the hypernym
18 if (sa.isDefinition())
19 System.out.println(sa.getHyper());
Figure 2: An example of WCL API usage.
tional sentences; ii) they may contain more than
one verbal phrase between the defined term and
the hypernym; iii) the first link after the verbal
phrase does not cover, or partially covers, the
correct hypernym. The elimination of the above
wrongly acquired definitional patterns can be im-
plemented with some language-dependent heuris-
tics or can be done by human annotators. In any
case, given the presence of a high number of cor-
rect annotated sentences, these wrong definitional
patterns have a very low impact on the definition
and hypernym extraction precision as shown in the
above experiments (see Table 3 and Table 4).
4 Multilingual WCL API
Together with the training and test sets of the
above experiments, we also release here our im-
plementation of Word-Class Lattices, available as
a Java API. As a result the WCL classifier can eas-
ily be used programmatically in any Java project.
In Figure 2 we show an example of the API usage.
After the selection of the target language (line 2),
we load the training dataset for the target language
(line 4). Then an instance of WCLClassifier is
created (line 8) and the training phase is launched
on the input training corpora (line 9). Now the
classifier is ready to be tested on any given sen-
tence in the target language (lines 11-16). If the
classifier output is positive (line 18) we can print
the extracted hypernym (line 19). The output of
the presented code is the string ?classifier? which
corresponds to the hypernym extracted by WCL
for the input sentence ?WCL is a kind of classi-
fier?.
4.1 Web user interface
We also release a Web interface to enable online
usage of our WCLs for the English, French and
Italian languages. In Figure 3 we show a screen-
shot of our Web interface. The user can type the
106
Figure 3: A screenshot of the WCL Web interface.
term of interest, the candidate definition, select
the language of interest and, after submission, in
the case of positive response from WCL, obtain
the corresponding hypernym and a graphical rep-
resentation of the lattices matching the given sen-
tence, as shown in the bottom part of the Figure.
The graphical representation shows the concate-
nation of the learned lattices which match the DF,
VF, GF parts of the given sentence (see Section
2). We also allow the user not to provide the term
of interest: in this case all the nouns in the sen-
tence are considered as candidate defined terms.
The Web user interface is part of a client-server ap-
plication, created with the JavaServer Pages tech-
nology. The server side produces an HTML page
(like the one shown in Figure 3), using the WCL
API (see Section 4) to process and test the submit-
ted definition candidate.
5 Related Work
A great deal of work is concerned with the lan-
guage independent extraction of definitions. Much
recent work uses symbolic methods that depend
on lexico-syntactic patterns or features, which are
manually created or semi-automatically learned as
recently done in (Zhang and Jiang, 2009; Wester-
hout, 2009). A fully automated method is, instead,
proposed by Borg et al (2009), where higher
performance (around 60-70% F1-measure) is ob-
tained only for specific domains and patterns. Ve-
lardi et al (2008), in order to improve precision
while keeping pattern generality, prune candidates
using more refined stylistic patterns and lexical fil-
ters. Cui et al (2007) propose the use of prob-
abilistic lexico-semantic patterns, for definitional
question answering in the TREC contest4. How-
ever, the TREC evaluation datasets cannot be con-
sidered true definitions, but rather text fragments
providing some relevant fact about a target term.
4Text REtrieval Conferences: http://trec.nist.
gov
Hypernym extraction methods vary from simple
lexical patterns (Hearst, 1992; Oakes, 2005) to sta-
tistical and machine learning techniques (Agirre
et al, 2000; Caraballo, 1999; Dolan et al, 1993;
Sanfilippo and Poznanski, 1992; Ritter et al,
2009). Extraction heuristics can be adopted in
many languages (De Benedictis et al, 2013),
where given a definitional sentence the hypernym
is identified as the first occuring noun after the
defined term. One of the highest-coverage meth-
ods is proposed by Snow et al (2004). They first
search sentences that contain two terms which are
known to be in a taxonomic relation (term pairs are
taken from WordNet (Miller et al, 1990)); then
they parse the sentences, and automatically ex-
tract patterns from the parse trees. Finally, they
train a hypernym classifier based on these features.
Lexico-syntactic patterns are generated for each
sentence relating a term to its hypernym, and a de-
pendency parser is used to represent them.
6 Conclusion
In this demonstration we provide three main con-
tributions: 1) a general method for obtaining large
training sets of annotated definitional sentences
for many languages from Wikipedia, thanks to
which we can release three new training sets for
English, French and Italian; 2) an API to program-
matically use WCLs in Java projects; 3) a Web ap-
plication which enables online use of multilingual
WCLs: http://lcl.uniroma1.it/wcl/.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
107
References
Eneko Agirre, Olatz Ansa, Eduard H. Hovy, and David
Mart??nez. 2000. Enriching very large ontologies using the
WWW. In ECAI Workshop on Ontology Learning, Berlin,
Germany.
Claudia Borg, Mike Rosner, and Gordon Pace. 2009. Evo-
lutionary algorithms for definition extraction. In Proceed-
ings of the 1st Workshop on Definition Extraction, pages
26?32, Borovets, Bulgaria.
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics: Proceedings of the Confer-
ence, pages 120?126, Maryland, USA.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pat-
tern matching models for definitional question answering.
ACM Transactions on Information Systems, 25(2):1?30.
Flavio De Benedictis, Stefano Faralli, and Roberto Navigli.
2013. GlossBoot: Bootstrapping Multilingual Domain
Glossaries from the Web. In Proceedings of 51st Annual
Meeting of the Association for Computational Linguistics,
Sofia, Bulgaria.
William Dolan, Lucy Vanderwende, and Stephen D. Richard-
son. 1993. Automatically deriving structured knowledge
bases from on-line dictionaries. In Proceedings of the
First Conference of the Pacific Association for Computa-
tional Linguistics, pages 5?14, Vancouver, Canada.
Weisi Duan and Alexander Yates. 2010. Extracting glosses
to disambiguate word senses. In Proceedings of Human
Language Technologies: The 11th Annual Conference of
the North American Chapter of the Association for Com-
putational Linguistics, pages 627?635, Los Angeles, CA,
USA.
Stefano Faralli and Roberto Navigli. 2012. A new
minimally-supervised framework for Domain Word Sense
Disambiguation. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning,
pages 1411?1422, Jeju, Korea.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia
Bernardini. 2008. Introducing and evaluating ukWaC, a
very large web-derived corpus of English. In Proceedings
of the 4th Web as Corpus Workshop (WAC-4), pages 47?
54, Marrakech, Morocco.
Tiziano Flati and Roberto Navigli. 2013. SPred: Large-scale
Harvesting of Semantic Predicates. In Proceedings of 51st
Annual Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics, pages
539?545, Nantes, France.
Eduard Hovy, Roberto Navigli, and Simone Paolo Ponzetto.
2013. Collaboratively built semi-structured content and
artificial intelligence: The story so far. Artificial Intelli-
gence, 194:2?27.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning argu-
ments and supertypes of semantic relations using recur-
sive patterns. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL),
Uppsala, Sweden, pages 1482?1491, Uppsala, Sweden.
George A. Miller, R.T. Beckwith, Christiane D. Fellbaum,
D. Gross, and K. Miller. 1990. WordNet: an online
lexical database. International Journal of Lexicography,
3(4):235?244.
Roberto Navigli and Paola Velardi. 2010. Learning Word-
Class Lattices for definition and hypernym extraction. In
Proceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1318?1327, Up-
psala, Sweden.
Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011.
A graph-based algorithm for inducing lexical taxonomies
from scratch. In Proceedings of the 22th International
Joint Conference on Artificial Intelligence, pages 1872?
1877, Barcelona, Spain.
Michael P. Oakes. 2005. Using Hearst?s rules for the auto-
matic acquisition of hyponyms for mining a pharmaceu-
tical corpus. In RANLP Text Mining Workshop?05, pages
63?67, Borovets, Bulgaria.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium on
Learning by Reading and Learning to Read, pages 88?93,
Palo Alto, California.
Horacio Saggion. 2004. Identifying definitions in text col-
lections for question answering. In Proceedings of the
Fourth International Conference on Language Resources
and Evaluation, pages 1927?1930, Lisbon, Portugal.
Antonio Sanfilippo and Victor Poznanski. 1992. The ac-
quisition of lexical knowledge from combined machine-
readable dictionary sources. In Proceedings of the third
Conference on Applied Natural Language Processing,
pages 80?87, Trento, Italy.
Helmut Schmid. 1997. Probabilistic part-of-speech tagging
using decision trees. In Daniel Jones and Harold Somers,
editors, New Methods in Language Processing, Studies in
Computational Linguistics, pages 154?164. UCL Press,
London, GB.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym dis-
covery. In Lawrence K. Saul, Yair Weiss, and Le?on Bot-
tou, editors, Proc. of NIPS 2004, pages 1297?1304, Cam-
bridge, Mass. MIT Press.
Angelika Storrer and Sandra Wellinghoff. 2006. Automated
detection and annotation of term definitions in German
text corpora. In LREC 2006, pages 275?295, Genoa, Italy.
Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.
2008. Mining the Web to create specialized glossaries.
IEEE Intelligent Systems, 23(5):18?25.
Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013.
OntoLearn Reloaded: A graph-based algorithm for taxon-
omy induction. Computational Linguistics, 39(3).
Eline Westerhout. 2009. Definition extraction using linguis-
tic and structural features. In Proceedings of the RANLP
2009 Workshop on Definition Extraction, page 61?67,
Borovets, Bulgaria.
Willy Yap and Timothy Baldwin. 2009. Experiments on
pattern-based relation learning. In Proceedings of the 18th
ACM Conference on Information and Knowledge Man-
agement (CIKM 2009), pages 1657?1660, Hong Kong,
China, 2009.
Chunxia Zhang and Peng Jiang. 2009. Automatic extraction
of definitions. In Proceedings of 2nd IEEE International
Conference on Computer Science and Information Tech-
nology, pages 364?368, Beijing, China.
108
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 468?478,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Robust Approach to Aligning Heterogeneous Lexical Resources
Mohammad Taher Pilehvar and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{pilehvar,navigli}@di.uniroma1.it
Abstract
Lexical resource alignment has been an
active field of research over the last
decade. However, prior methods for align-
ing lexical resources have been either spe-
cific to a particular pair of resources, or
heavily dependent on the availability of
hand-crafted alignment data for the pair of
resources to be aligned. Here we present a
unified approach that can be applied to an
arbitrary pair of lexical resources, includ-
ing machine-readable dictionaries with no
network structure. Our approach leverages
a similarity measure that enables the struc-
tural comparison of senses across lexical
resources, achieving state-of-the-art per-
formance on the task of aligning WordNet
to three different collaborative resources:
Wikipedia, Wiktionary and OmegaWiki.
1 Introduction
Lexical resources are repositories of machine-
readable knowledge that can be used in virtually
any Natural Language Processing task. Notable
examples are WordNet, Wikipedia and, more re-
cently, collaboratively-curated resources such as
OmegaWiki and Wiktionary (Hovy et al, 2013).
On the one hand, these resources are heteroge-
neous in design, structure and content, but, on
the other hand, they often provide complemen-
tary knowledge which we would like to see inte-
grated. Given the large scale this intrinsic issue
can only be addressed automatically, by means of
lexical resource alignment algorithms. Owing to
its ability to bring together features like multilin-
guality and increasing coverage, over the past few
years resource alignment has proven beneficial to
a wide spectrum of tasks, such as Semantic Pars-
ing (Shi and Mihalcea, 2005), Semantic Role La-
beling (Palmer et al, 2010), and Word Sense Dis-
ambiguation (Navigli and Ponzetto, 2012).
Nevertheless, when it comes to aligning textual
definitions in different resources, the lexical ap-
proach (Ruiz-Casado et al, 2005; de Melo and
Weikum, 2010; Henrich et al, 2011) falls short
because of the potential use of totally different
wordings to define the same concept. Deeper ap-
proaches leverage semantic similarity to go be-
yond the surface realization of definitions (Nav-
igli, 2006; Meyer and Gurevych, 2011; Niemann
and Gurevych, 2011). While providing good re-
sults in general, these approaches fail when the
definitions of a given word are not of adequate
quality and expressiveness to be distinguishable
from one another. When a lexical resource can be
viewed as a semantic graph, as with WordNet or
Wikipedia, this limit can be overcome by means
of alignment algorithms that exploit the network
structure to determine the similarity of concept
pairs. However, not all lexical resources provide
explicit semantic relations between concepts and,
hence, machine-readable dictionaries like Wik-
tionary have first to be transformed into semantic
graphs before such graph-based approaches can be
applied to them. To do this, recent work has pro-
posed graph construction by monosemous linking,
where a concept is linked to all the concepts asso-
ciated with the monosemous words in its definition
(Matuschek and Gurevych, 2013). However, this
alignment method still involves tuning of parame-
ters which are highly dependent on the character-
istics of the generated graphs and, hence, requires
hand-crafted sense alignments for the specific pair
of resources to be aligned, a task which has to be
replicated every time the resources are updated.
In this paper we propose a unified approach
to aligning arbitrary pairs of lexical resources
which is independent of their specific structure.
Thanks to a novel modeling of the sense entries
and an effective ontologization algorithm, our ap-
proach also fares well when resources lack rela-
tional structure or pair-specific training data is ab-
sent, meaning that it is applicable to arbitrary pairs
468
without adaptation. We report state-of-the-art per-
formance when aligning WordNet to Wikipedia,
OmegaWiki and Wiktionary.
2 Resource Alignment
Preliminaries. Our approach for aligning lexi-
cal resources exploits the graph structure of each
resource. Therefore, we assume that a lexical
resource L can be represented as an undirected
graph G = (V,E) where V is the set of nodes,
i.e., the concepts defined in the resource, and
E is the set of undirected edges, i.e., seman-
tic relations between concepts. Each concept
c ? V is associated with a set of lexicalizations
L
G
(c) = {w
1
, w
2
, ..., w
n
}. For instance, Word-
Net can be readily represented as an undirected
graph G whose nodes are synsets and edges are
modeled after the relations between synsets de-
fined in WordNet (e.g., hypernymy, meronymy,
etc.), and L
G
is the mapping between each synset
node and the set of synonyms which express the
concept. However, other resources such as Wik-
tionary do not provide semantic relations between
concepts and, therefore, have first to be trans-
formed into semantic networks before they can be
aligned using our alignment algorithm. We ex-
plain in Section 3 how a semi-structured resource
which does not exhibit a graph structure can be
transformed into a semantic network.
Alignment algorithm. Given a pair of lexical
resources L
1
and L
2
, we align each concept in L
1
by mapping it to its corresponding concept(s) in
the target lexicon L
2
. Algorithm 1 formalizes the
alignment process: the algorithm takes as input the
semantic graphs G
1
and G
2
corresponding to the
two resources, as explained above, and produces
as output an alignment in the form of a set A of
concept pairs. The algorithm iterates over all con-
cepts c
1
? V
1
and, for each of them, obtains the set
of concepts C ? V
2
, which can be considered as
alignment candidates for c
1
(line 3). For a concept
c
1
, alignment candidates in G
2
usually consist of
every concept c
2
? V
2
that shares at least one lex-
icalization with c
1
in the same part of speech tag,
i.e., L
G
1
(c
1
) ? L
G
2
(c
2
) 6= ? (Reiter et al, 2008;
Meyer and Gurevych, 2011). Once the set of target
candidates C for a source concept c
1
is obtained,
the alignment task can be cast as that of identifying
those concepts in C to which c
1
should be aligned.
To do this, the algorithm calculates the similarity
between c
1
and each c
2
? C (line 5). If their sim-
ilarity score exceeds a certain value denoted by ?
Algorithm 1 Lexical Resource Aligner
Input: graphs H = (V
H
, E
H
), G
1
= (V
1
, E
1
) and G
2
=
(V
2
, E
2
), the similarity threshold ?, and the combination
parameter ?
Output: A, the set of all aligned concept pairs
1: A? ?
2: for each concept c
1
? V
1
3: C ? getCandidates(c
1
, V
2
)
4: for each concept c
2
? C
5: sim? calculateSimilarity(H,G
1
, G
2
, c
1
, c
2
, ?)
6: if sim > ? then
7: A? A ? {(c
1
, c
2
)}
8: return A
(line 6), the two concepts c
1
and c
2
are aligned and
the pair (c
1
, c
2
) is added to A (line 7).
Different resource alignment techniques usually
vary in the way they compute the similarity of a
pair of concepts across two resources (line 5 in Al-
gorithm 1). In the following, we present our novel
approach for measuring the similarity of concept
pairs.
2.1 Measuring the Similarity of Concepts
Figure 1 illustrates the procedure underlying our
cross-resource concept similarity measurement
technique. As can be seen, the approach consists
of two main components: definitional similarity
and structural similarity. Each of these compo-
nents gets, as its input, a pair of concepts belong-
ing to two different semantic networks and pro-
duces a similarity score. These two scores are then
combined into an overall score (part (e) of Figure
1) which quantifies the semantic similarity of the
two input concepts c
1
and c
2
.
The definitional similarity component computes
the similarity of two concepts in terms of the simi-
larity of their definitions, a method that has also
been used in previous work for aligning lexical
resources (Niemann and Gurevych, 2011; Hen-
rich et al, 2012). In spite of its simplicity, the
mere calculation of the similarity of concept defi-
nitions provides a strong baseline, especially for
cases where the definitional texts for a pair of
concepts to be aligned are lexically similar, yet
distinguishable from the other definitions. How-
ever, as mentioned in the introduction, definition
similarity-based techniques fail at identifying the
correct alignments in cases where different word-
ings are used or definitions are not of high qual-
ity. The structural similarity component, instead,
is a novel graph-based similarity measurement
technique which calculates the similarity between
a pair of concepts across the semantic networks
of the two resources by leveraging the semantic
469
Figure 1: The process of measuring the similarity of a pair of concepts across two resources. The method
consists of two components: definitional and structural similarities, each measuring a similarity score for
the given concept pair. The two scores are combined by means of parameter ? in the last stage.
structure of those networks. This component goes
beyond the surface realization of concepts, thus
providing a deeper measure of concept similarity.
The two components share the same backbone
(parts (b) and (d) of Figure 1), but differ in some
stages (parts (a) and (c) in Figure 1). In the follow-
ing, we explain all the stages involved in the two
components (gray blocks in the figure).
2.1.1 Semantic signature generation
The aim of this stage is to model a given concept
or set of concepts through a vectorial semantic
representation, which we refer to as the seman-
tic signature of the input. We utilized Person-
alized PageRank (Haveliwala, 2002, PPR), a ran-
dom walk graph algorithm, for calculating seman-
tic signatures. The original PageRank (PR) algo-
rithm (Brin and Page, 1998) computes, for a given
graph, a single vector wherein each node is as-
sociated with a weight denoting its structural im-
portance in that graph. PPR is a variation of PR
where the computation is biased towards a set of
initial nodes in order to capture the notion of im-
portance with respect to those particular nodes.
PPR has been previously used in a wide variety of
tasks such as definition similarity-based resource
alignment (Niemann and Gurevych, 2011), textual
semantic similarity (Hughes and Ramage, 2007;
Pilehvar et al, 2013), Word Sense Disambigua-
tion (Agirre and Soroa, 2009; Faralli and Navigli,
2012) and semantic text categorization (Navigli et
al., 2011). When applied to a semantic graph by
initializing the random walks from a set of con-
cepts (nodes), PPR yields a vector in which each
concept is associated with a weight denoting its
semantic relevance to the initial concepts.
Formally, we first represent a semantic network
consisting of N concepts as a row-stochastic tran-
sition matrix M ? R
N?N
. The cell (i, j) in the
matrix denotes the probability of moving from a
concept i to j in the graph: 0 if no edge exists
from i to j and 1/degree(i) otherwise. Then the
PPR vector, hence the semantic signature S
v
of
vector v is the unique solution to the linear sys-
tem: S
v
= (1 ? ?)v + ?MS
v
, where v is the
personalization vector of size N in which all the
probability mass is put on the concepts for which
a semantic signature is to be computed and ? is the
damping factor, which is usually set to 0.85 (Brin
and Page, 1998). We used the UKB
1
off-the-shelf
implementation of PPR.
Definitional similarity signature. In the defini-
tional similarity component, the two concepts c
1
and c
2
are first represented by their corresponding
definitions d
1
and d
2
in the respective resourcesL
1
and L
2
(Figure 1(a), top). To improve expressive-
ness, we follow Niemann and Gurevych (2011)
and further extend d
i
with all the word forms asso-
ciated with concept c
i
and its neighbours, i.e., the
union of all lexicalizations L
G
i
(x) for all concepts
x ? {c
?
? V
i
: (c, c
?
) ? E
i
}?{c}, where E
i
is the
set of edges in G
i
. In this component the person-
alization vector v
i
is set by uniformly distributing
the probability mass over the nodes correspond-
ing to the senses of all the content words in the
extended definition of d
i
according to the sense
inventory of a semantic network H . We use the
same semantic graph H for computing the seman-
tic signatures of both definitions. Any semantic
network with a dense relational structure, provid-
ing good coverage of the words appearing in the
definitions, is a suitable candidate for H . For this
purpose we used the WordNet (Fellbaum, 1998)
graph which was further enriched by connecting
1
http://ixa2.si.ehu.es/ukb/
470
each concept to all the concepts appearing in its
disambiguated gloss.
2
Structural similarity signature. In the struc-
tural similarity component (Figure 1(b), bottom),
the semantic signature for each concept c
i
is com-
puted by running the PPR algorithm on its corre-
sponding graph G
i
, hence a different M
i
is built
for each of the two concepts.
2.1.2 Signature unification
As mentioned earlier, semantic signatures are vec-
tors with dimension equal to the number of nodes
in the semantic graph. Since the structural similar-
ity signatures S
v
1
and S
v
2
are calculated on differ-
ent graphs and thus have different dimensions, we
need to make them comparable by unifying them.
We therefore propose an approach (part (c) of Fig-
ure 1) that finds a common ground between the
two signatures: to this end we consider all the
concepts associated with monosemous words in
the two signatures as landmarks and restrict the
two signatures exclusively to those common con-
cepts. Leveraging monosemous words as bridges
between two signatures is a particularly reliable
technique as typically a significant portion of all
words in a lexicon are monosemous.
3
Formally, let I
G
(w) be an inventory mapping
function that maps a term w to the set of con-
cepts which are expressed by w in graph G. Then,
given two signatures S
v
1
and S
v
2
, computed on
the respective graphs G
1
and G
2
, we first obtain
the setM of words that are monosemous accord-
ing to both semantic networks, i.e., M = {w :
|I
G
1
(w)|=1 ? |I
G
2
(w)|=1}. We then transform
each of the two signatures S
v
i
into a new sub-
signature S
?
v
i
whose dimension is |M|: the k
th
component of S
?
v
i
corresponds to the weight in S
v
i
of the only concept ofw
k
in I
G
i
(w
k
). As an exam-
ple, assume we are given two semantic signatures
computed for two concepts in WordNet and Wik-
tionary. Also, consider the noun tradeoff which
is monosemous according to both these resources.
Then, each of the two unified sub-signatures will
contain a component whose weight is determined
by the weight of the only concept associated with
tradeoff
n
in the corresponding semantic signature.
As a result of the unification process, we obtain
a pair of equally-sized semantic signatures with
comparable components.
2
http://wordnet.princeton.edu
3
For instance, we calculated that more than 80% of the
words in WordNet are monosemous, with over 60% of all the
synsets containing at least one of them.
2.1.3 Signature comparison
Having at hand the semantic signatures for the
two input concepts, we proceed to comparing
them (part (d) in Figure 1). We leverage a non-
parametric measure proposed by Pilehvar et al
(2013) which first transforms each signature into
a list of sorted elements and then calculates the
similarity on the basis of the average ranking of
elements across the two lists:
Sim(S
v
1
,S
v
2
) =
?
|T |
i=1
(r
1
i
+ r
2
i
)
?1
?
|T |
i=1
(2i)
?1
(1)
where T is the intersection of all concepts with
non-zero probability in the two signatures and r
j
i
is the rank of the i
th
entry in the j
th
sorted list.
The denominator is a normalization factor to guar-
antee a maximum value of one. The method pe-
nalizes the differences in the higher rankings more
than it does for the lower ones. The measure was
shown to outperform the conventional cosine dis-
tance when comparing different semantic signa-
tures in multiple textual similarity tasks (Pilehvar
et al, 2013).
2.1.4 Score combination
Finally (part (e) of Figure 1), we calculate the
overall similarity between two concepts as a lin-
ear combination of their definitional and struc-
tural similarities: ? Sim
def
(S
v
1
,S
v
2
) + (1 ?
?)Sim
str
(S
v
1
,S
v
2
). In Section 4.2.1, we explain
how we set, in our experiments, the values of ?
and the similarity threshold ? (cf. alignment algo-
rithm in Section 2).
3 Lexical Resource Ontologization
In Section 2, we presented our approach for align-
ing lexical resources. However, the approach as-
sumes that the input resources can be viewed as
semantic networks, which seems to limit its ap-
plicability to structured resources only. In or-
der to address this issue and hence generalize our
alignment approach to any given lexical resource,
we propose a method for transforming a given
machine-readable dictionary into a semantic net-
work, a process we refer to as ontologization.
Our ontologization algorithm takes as input a
lexicon L and outputs a semantic graph G =
(V,E) where, as already defined in Section 2, V is
the set of concepts in L and E is the set of seman-
tic relations between these concepts. Introducing
relational links into a lexicon can be achieved in
different ways. A first option is to extract binary
471
relations between pairs of words from raw text.
Both words in these relations, however, should
be disambiguated according to the given lexicon
(Pantel and Pennacchiotti, 2008), making the task
particularly prone to mistakes due to the high num-
ber of possible sense pairings.
Here, we take an alternative approach which
requires disambiguation on the target side only,
hence reducing the size of the search space sig-
nificantly. We first create the empty undirected
graph G
L
= (V,E) such that V is the set of con-
cepts in L and E = ?. For each source con-
cept c ? V we create a bag of content words
W = {w
1
, . . . , w
n
} which includes all the con-
tent words in its definition d and, if available, ad-
ditional related words obtained from lexicon rela-
tions (e.g., synonyms in Wiktionary). The prob-
lem is then cast as a disambiguation task whose
goal is to identify the intended sense of each word
w
i
? W according to the sense inventory of L: if
w
i
is monosemous, i.e., |{I
G
L
(w
i
)}| = 1, we con-
nect our source concept c to the only sense c
w
i
of
w
i
and set E := E ? {{c, c
w
i
}}; else, w
i
has mul-
tiple senses in L. In this latter case, we choose the
most appropriate concept c
i
? I
G
L
(w
i
) by finding
the maximal similarity between the definition of c
and the definitions of each sense of w
i
. To do this,
we apply our definitional similarity measure intro-
duced in Section 2.1. Having found the intended
sense c?
w
i
of w
i
, we add the edge {c, c?
w
i
} to E.
As a result of this procedure, we obtain a semantic
graph representation G for the lexicon L.
As an example, consider the 4
th
sense of the
noun cone in Wiktionary (i.e., cone
4
n
) which is de-
fined as ?The fruit of a conifer?. The definition
contains two content words: fruit
n
and conifer
n
.
The latter word is monosemous in Wiktionary,
hence we directly connect cone
4
n
to the only sense
of conifer
n
. The noun fruit, however, has 5 senses
in Wiktionary. We therefore measure the similar-
ity between the definition of cone
4
n
and all the 5
definitions of fruit and introduce a link from cone
4
n
to the sense of fruit which yields the maximal
similarity value (defined as ?(botany) The seed-
bearing part of a plant...?).
4 Experiments
Lexical resources. To enable a comparison with
the state of the art, we followed Matuschek
and Gurevych (2013) and performed an align-
ment of WordNet synsets (WN) to three different
collaboratively-constructed resources: Wikipedia
(WP), Wiktionary (WT), and OmegaWiki (OW).
We utilized the DKPro software (Zesch et al,
2008; Gurevych et al, 2012) to access the infor-
mation in the foregoing three resources. For WP,
WT, OW we used the dump versions 20090822,
20131002, and 20131115, respectively.
Evaluation measures. We followed previous
work (Navigli and Ponzetto, 2012; Matuschek and
Gurevych, 2013) and evaluated the alignment per-
formance in terms of four measures: precision, re-
call, F1, and accuracy. Precision is the fraction of
correct alignment judgments returned by the sys-
tem and recall is the fraction of alignment judg-
ments in the gold standard dataset that are cor-
rectly returned by the system. F1 is the harmonic
mean of precision and recall. We also report re-
sults for accuracy which, in addition to true posi-
tives, takes into account true negatives, i.e., pairs
which are correctly judged as unaligned.
Lexicons and semantic graphs. Here, we de-
scribe how the four semantic graphs for our four
lexical resources (i.e., WN, WP, WT, OW) were
constructed. As mentioned in Section 2.1.1, we
build the WN graph by including all the synsets
and semantic relations defined in WordNet (e.g.,
hypernymy and meronymy) and further populate
the relation set by connecting a synset to all the
other synsets that appear in its disambiguated
gloss. For WP, we used the graph provided by
Matuschek and Gurevych (2013), constructed by
directly connecting an article (concept) to all the
hyperlinks in its first paragraph, together with the
category links. Our WN and WP graphs have 118K
and 2.8M nodes, respectively, with the average
node degree being roughly 9 in both resources.
The other two resources, i.e., WT and OW, do
not provide a reliable network of semantic rela-
tions, therefore we used our ontologization ap-
proach to construct their corresponding semantic
graphs. We report, in the following subsection,
the experiments carried out to assess the accuracy
of our ontologization method, together with the
statistics of the obtained graphs for WT and OW.
4.1 Ontologization Experiments
For ontologizing WT and OW, the bag of con-
tent words W is given by the content words in
sense definitions and, if available, additional re-
lated words obtained from lexicon relations (see
Section 3). In WT, both of these are in word sur-
face form and hence had to be disambiguated. For
OW, however, the encoded relations, though rela-
472
Source Type WT OW
Definition
Ambiguous 76.6% 50.7%
Unambiguous 18.3% 32.9%
Relation
Ambiguous 2.8% -
Unambiguous 2.3% 16.4%
Total number of edges 2.1M 255K
Table 1: The statistics of the generated graphs
for WT and OW. We report the distribution of
the edges across types (i.e., ambiguous and un-
ambiguous) and sources (i.e., definitions and rela-
tions) from which candidate words were obtained.
tively small in number, are already disambiguated
and, therefore, the ontologization was just per-
formed on the definition?s content words.
The resulting graphs for WT and OW contain
430K and 48K nodes, respectively, each provid-
ing more than 95% coverage of concepts, with the
average node degree being around 10 for both re-
sources. We present in Table 1, for WT and OW,
the total number of edges together with their dis-
tribution across types (i.e., ambiguous and unam-
biguous) and sources (i.e., definitions and rela-
tions) from which candidate words were obtained.
The edges obtained from unambiguous entries
are essentially sense disambiguated on both sides
whereas those obtained from ambiguous terms
are a result of our similarity-based disambigua-
tion. Hence, given that a large portion of edges
came from ambiguous words (see Table 1), we
carried out an experiment to evaluate the accu-
racy of our disambiguation method. To this end,
we took as our benchmark the dataset provided
by Meyer and Gurevych (2010) for evaluating re-
lation disambiguation in WT. The dataset con-
tains 394 manually-disambiguated relations. We
compared our similarity-based disambiguation ap-
proach against the state of the art on this dataset,
i.e., the WKTWSD system, which is a WT rela-
tion disambiguation algorithm based on a series of
rules (Meyer and Gurevych, 2012b).
Table 2 shows the performance of our disam-
biguation method, together with that of WKTWSD,
in terms of Precision (P), Recall (R), F1, and ac-
curacy. The ?Human? row corresponds to the
inter-rater F1 and accuracy scores, i.e., the upper-
bound performance on this dataset, as calculated
by Meyer and Gurevych (2010). As can be seen,
our method proves to be very accurate, surpassing
the performance of the WKTWSD system in terms
of precision, F1, and accuracy. This is particularly
Approach P R F1 A
WKTWSD 0.780 0.800 0.790 0.840
Our method 0.852 0.767 0.807 0.857
Human - - 0.890 0.910
Table 2: The performance of relation disam-
biguation for our similarity-based disambiguation
method, as well as for the WKTWSD system.
interesting as the WKTWSD system uses a rule-
based technique specific to relation disambigua-
tion in WT, whereas our method is resource inde-
pendent and can be applied to arbitrary words in
the definition of any concept. We also note that the
graph constructed by Meyer and Gurevych (2010)
had an average node degree of around 1.
More recently, Matuschek and Gurevych (2013)
leveraged monosemous linking (cf. Section 5) in
order to create denser semantic graphs for OW and
WT. Our approach, however, thanks to the con-
nections obtained through ambiguous words, can
provide graphs with significantly higher coverage.
As an example, for WT, Matuschek and Gurevych
(2013) generated a graph where around 30% of
the nodes were in isolation, whereas this number
drops to around 5% in our corresponding graph.
These results show that our ontologization ap-
proach can be used to obtain dense semantic graph
representations of lexical resources, while at the
same time preserving a high level of accuracy.
Now that all the four resources are transformed
into semantic graphs, we move to our alignment
experiments.
4.2 Alignment Experiments
4.2.1 Experimental setup
Datasets. As our benchmark we tested on
the gold standard datasets used in Matuschek
and Gurevych (2013) for three alignment
tasks: WordNet-Wikipedia (WN-WP), WordNet-
Wiktionary (WN-WT), and WordNet-OmegaWiki
(WN-OW). However, the dataset for WN-OW was
originally built for the German language and,
hence, was missing many English OW concepts
that could be considered as candidate target
alignments. We therefore fixed the dataset for the
English language and reproduced the performance
of previous work on the new dataset. The three
datasets contained 320, 484, and 315 WN concepts
that were manually mapped to their corresponding
concepts in WP, WT, and OW, respectively.
473
Approach Training type
WN-WP WN-WT WN-OW
P R F1 A P R F1 A P R F1 A
SB Cross-val. 0.780 0.780 0.780 0.950 0.670 0.650 0.660 0.910 0.749 0.691 0.716 0.886
DWSA Tuning on subset 0.750 0.670 0.710 0.930 0.680 0.270 0.390 0.890 0.651 0.372 0.473 0.830
SB+DWSA Cross-val. + tuning 0.750 0.870 0.810 0.950 0.680 0.710 0.690 0.920 0.794 0.688 0.735 0.898
SemAlign
Unsupervised 0.709 0.929 0.805 0.943 0.642 0.799 0.712 0.923 0.664 0.761 0.709 0.872
Tuning on subset 0.877 0.792 0.833 0.960 0.672 0.799 0.730 0.930 0.750 0.717 0.733 0.893
Cross-val. 0.852 0.835 0.840 0.965 0.680 0.769 0.722 0.931 0.778 0.725 0.749 0.900
Tuning on WN-WP - - - - 0.754 0.627 0.684 0.931 0.825 0.584 0.684 0.889
Tuning on WN-WT 0.738 0.934 0.824 0.950 - - - - 0.805 0.677 0.736 0.900
Tuning on WN-OW 0.744 0.925 0.824 0.950 0.684 0.766 0.723 0.930 - - - -
Table 3: The performance of different systems on the task of aligning WordNet to Wikipedia (WN-WP),
Wiktionary (WN-WT), and OmegaWiki (WN-OW) in terms of Precision (P), Recall (R), F1, and Accuracy
(A). We present results for different configurations of our system (SemAlign), together with the state of
the art in definition similarity-based alignment approaches (SB) and the best configuration of the state-
of-the-art graph-based system, Dijkstra-WSA (Matuschek and Gurevych, 2013, DWSA).
Configurations. Recall from Section 2 that our
resource alignment technique has two parameters:
the similarity threshold ? and the combination pa-
rameter ?, both defined in [0, 1]. We performed
experiments with three different configurations:
? Unsupervised, where the two parameters are
set to their middle values (i.e., 0.5), hence,
no tuning is performed for either of the pa-
rameters. In this case, both the definitional
and structural similarity scores are treated
as equally important and two concepts are
aligned if their overall similarity exceeds the
middle point of the similarity scale.
? Tuning, where we follow Matuschek and
Gurevych (2013) and tune the parameters on
a subset of the dataset comprising 100 items.
? Cross-validation, where a 5-fold cross vali-
dation is carried out to find the optimal values
for the parameters, a technique used in most
of the recent alignment methods (Niemann
and Gurevych, 2011; Meyer and Gurevych,
2012a; Matuschek and Gurevych, 2013).
4.2.2 Results
We show in Table 3 the alignment performance of
different systems on the task of aligning WN-WP,
WN-WT, and WN-OW in terms of Precision (P), Re-
call (R), F1, and Accuracy. The SB system corre-
sponds to the state-of-the-art definition similarity
approaches for WN-WP (Niemann and Gurevych,
2011), WN-WT (Meyer and Gurevych, 2011), and
WN-OW (Gurevych et al, 2012). DWSA stands
for Dijkstra-WSA, the state-of-the-art graph-based
alignment approach of Matuschek and Gurevych
(2013). The authors also provided results for
SB+Dijkstra-WSA, a hybrid system where DWSA
was tuned for high precision and, in the case when
no alignment target could be found, the algorithm
fell back on SB judgments. We also show the re-
sults for this system as SB+DWSA in the table.
For our approach (SemAlign) we show the re-
sults of six different runs each corresponding to a
different setting. The first three (middle part of the
table) correspond to the results obtained with the
three configurations of SemAlign: unsupervised,
with tuning on subset, and cross-validation (see
Section 4.2.1). In addition to these, we performed
experiments where the two parameters of SemA-
lign were tuned on pair-independent training data,
i.e., a training dataset for a pair of resources dif-
ferent from the one being aligned. For this setting,
we used the whole dataset of the corresponding re-
source pair to tune the two parameters of our sys-
tem. We show the results for this setting in the
bottom part of the table (last three lines).
The main feature worth remarking upon is the
consistency in the results across different resource
pairs: the unsupervised system gains the best re-
call among the three configurations (with the im-
provement over SB+DWSA being always statisti-
cally significant
4
) whereas tuning, both on a subset
or through cross-validation, consistently leads to
the best performance in terms of F1 and accuracy
(with the latter being statistically significant with
respect to SB+DWSA on WN-WP and WN-WT).
Moreover, the unsupervised system proves to be
very robust inasmuch as it provides competitive
results on all the three datasets, while it surpasses
the performance of SB+DWSA on WN-WT. This
4
All significance tests are done using z-test at p < 0.05.
474
Approach
WN-WP WN-WT WN-OW
P R F1 A P R F1 A P R F1 A
Dijkstra-WSA 0.750 0.670 0.710 0.930 0.680 0.270 0.390 0.890 0.651 0.372 0.473 0.830
SemAlign
str
0.877 0.788 0.830 0.959 0.604 0.643 0.623 0.907 0.654 0.602 0.627 0.853
Table 4: Performance of SemAlign when using only the structural similarity component (SemAlign
str
)
compared to the state-of-the-art graph-based alignment approach, Dijkstra-WSA (Matuschek and
Gurevych, 2013) for our three resource pairs: WordNet to Wikipedia (WN-WP), Wiktionary (WN-WT),
and OmegaWiki (WN-OW).
is particularly interesting as the latter system in-
volves tuning of several parameters, whereas Se-
mAlign, in its unsupervised configuration, does
not need any training data nor does it involve any
tuning. In addition, as can be seen in the table,
SemAlign benefits from pair-independent training
data in most cases across the three resource pairs
with performance surpassing that of SB+DWSA, a
system which is dependent on pair-specific train-
ing data. The consistency in the performance of
SemAlign in its different configurations and across
different resource pairs indicates its robustness
and shows that our system can be utilized effec-
tively for aligning any pair of lexical resources, ir-
respective of their structure or availability of train-
ing data.
The system performance is generally higher on
the alignment task for WP compared to WT and
OW. We attribute this difference to the dictionary
nature of the latter two, where sense distinctions
are more fine-grained, as opposed to the relatively
concrete concepts in the WP encyclopedia.
4.3 Similarity Measure Analysis
We explained in Section 2.1 that our concept sim-
ilarity measure consists of two components: the
definitional and the structural similarities. Mea-
suring the similarity of two concepts in terms of
their definitions has been investigated in previ-
ous work (Niemann and Gurevych, 2011; Hen-
rich et al, 2012). The structural similarity compo-
nent of our approach, however, is novel, but at the
same time one of the very few measures which en-
ables the computation of the similarity of concepts
across two resources directly and independently of
the similarity of their definitions. A comparable
approach is the Dijkstra-WSA proposed by Ma-
tuschek and Gurevych (2013) which, as also men-
tioned earlier in the Introduction, first connects the
two resources? graphs by leveraging monosemous
linking and then aligns two concepts across the
two graphs on the basis of their shortest distance.
To gain more insight into the effectiveness of our
structural similarity measure in comparison to the
Dijkstra-WSA method, we carried out an experi-
ment where our alignment system used only the
structural similarity component, a variant of our
system we refer to as SemAlign
str
. Both systems
(i.e., SemAlign
str
and Dijkstra-WSA) were tuned
on 100-item subsets of the corresponding datasets.
We show in Table 4 the performance of the two
systems on our three datasets. As can be seen in
the table, SemAlign
str
consistently improves over
Dijkstra-WSA according to recall, F1 and accu-
racy with all the differences in recall and accu-
racy being statistically significant (p < 0.05). The
improvement is especially noticeable for pairs in-
volving either WT or OW where, thanks to the rel-
atively denser semantic graphs obtained by means
of our ontologization technique, the gap in F1 is
about 0.23 (WN-WT) and 0.15 (WN-OW).
In addition, as we mentioned earlier, for WN-WP
we used the same graph as that of Dijkstra-WSA,
since both WN and WP provide a full-fledged se-
mantic network and thus neither needed to be
ontologized. Therefore, the considerable perfor-
mance improvement over Dijkstra-WSA on this
resource pair shows the effectiveness of our novel
concept similarity measure independently of the
underlying semantic network.
5 Related Work
Resource ontologization. Having lexical re-
sources represented as semantic networks is
highly beneficial. A good example is WordNet,
which has been exploited as a semantic network
in dozens of NLP tasks (Fellbaum, 1998). A re-
cent prominent case is Wikipedia (Medelyan et
al., 2009; Hovy et al, 2013) which, thanks to
its inter-article hyperlink structure, provides a rich
backbone for structuring additional information
(Auer et al, 2007; Suchanek et al, 2008; Moro
and Navigli, 2013; Flati et al, 2014). How-
ever, there are many large-scale resources, such
as Wiktionary for instance, which by their very
nature are not in the form of a graph. This is
475
usually the case with machine-readable dictionar-
ies, where structuring the resource involves the
arduous task of connecting lexicographic senses
by means of semantic relations. Surprisingly,
despite their vast potential, little research has
been conducted on the automatic ontologization of
collaboratively-constructed dictionaries like Wik-
tionary and OmegaWiki. Meyer and Gurevych
(2012a) and Matuschek and Gurevych (2013) pro-
vided approaches for building graph representa-
tions of Wiktionary and OmegaWiki. The result-
ing graphs, however, were either sparse or had a
considerable portion of the nodes left in isolation.
Our approach, in contrast, aims at transforming a
lexical resource into a full-fledged semantic net-
work, hence providing a denser graph with most
of its nodes connected.
Resource alignment. Aligning lexical resources
has been a very active field of research in the
last decade. One of the main objectives in this
area has been to enrich existing ontologies by
means of complementary information from other
resources. As a matter of fact, most efforts have
been concentrated on aligning the de facto com-
munity standard sense inventory, i.e. WordNet, to
other resources. These include: the Roget?s the-
saurus and Longman Dictionary of Contemporary
English (Kwong, 1998), FrameNet (Laparra and
Rigau, 2009), VerbNet (Shi and Mihalcea, 2005)
or domain-specific terminologies such as the Uni-
fied Medical Language System (Burgun and Bo-
denreider, 2001). More recently, the growth
of collaboratively-constructed resources has seen
the development of alignment approaches with
Wikipedia (Ruiz-Casado et al, 2005; Auer et al,
2007; Suchanek et al, 2008; Reiter et al, 2008;
Navigli and Ponzetto, 2012), Wiktionary (Meyer
and Gurevych, 2011) and OmegaWiki (Gurevych
et al, 2012). Last year Matuschek and Gurevych
(2013) proposed Dijkstra-WSA, a graph-based ap-
proach relying on shortest paths between two
concepts when the two corresponding resources
graphs were combined by leveraging monosemous
linking. Their method when backed off with other
definition similarity based approaches (Niemann
and Gurevych, 2011; Meyer and Gurevych, 2011),
achieved state-of-the-art results on the mapping of
WordNet to different collaboratively-constructed
resources. This approach, however, in addition to
setting the threshold for the definition similarity
component by means of cross validation, also re-
quired other parameters to be tuned, such as the
allowed path length (?) and the maximum num-
ber of edges in a graph. The optimal value for the
? parameter varied from one resource pair to an-
other, and even for a specific resource pair it had
to be tuned for each configuration. This made the
approach dependent on the training data for the
specific pair of resources that were to be aligned.
Instead of measuring the similarity of two con-
cepts on the basis of their distance in the com-
bined graph, our approach models each concept
through a rich vectorial representation we refer to
as semantic signature and compares the two con-
cepts in terms of the similarity of their semantic
signatures. This rich representation leads to our
approach having a good degree of robustness such
that it can achieve competitive results even in the
absence of training data. This enables our system
to be applied effectively for aligning new pairs of
resources for which no training data is available,
with state-of-the-art performance.
6 Conclusions
This paper presents a unified approach for align-
ing lexical resources. Our method leverages
a novel similarity measure which enables a di-
rect structural comparison of concepts across dif-
ferent lexical resources. Thanks to an effec-
tive ontologization method, our alignment ap-
proach can be applied to any pair of lexical re-
sources independently of whether they provide
a full-fledged network structure. We demon-
strate that our approach achieves state-of-the-
art performance on aligning WordNet to three
collaboratively-constructed resources with differ-
ent characteristics, i.e., Wikipedia, Wiktionary,
and OmegaWiki. We also show that our approach
is robust across its different configurations, even
when the training data is absent, enabling it to be
used effectively for aligning new pairs of lexical
resources for which no resource-specific training
data is available. In future work, we plan to ex-
tend our concept similarity measure across differ-
ent natural languages. We release all our data at
http://lcl.uniroma1.it/semalign.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Michael Matuschek for
providing us with Wikipedia graphs and alignment
datasets.
476
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?41, Athens, Greece.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722?735, Bu-
san, Korea.
Sergey Brin and Michael Page. 1998. Anatomy of a
large-scale hypertextual Web search engine. In Pro-
ceedings of the 7
th
Conference on World Wide Web,
pages 107?117, Brisbane, Australia.
Anita Burgun and Olivier Bodenreider. 2001. Compar-
ing terms, concepts and semantic classes in WordNet
and the Unified Medical Language System. In Pro-
ceedings of NAACL Workshop, WordNet and Other
Lexical Resources: Applications, Extensions and
Customizations, pages 77?82, Pittsburgh, USA.
Gerard de Melo and Gerhard Weikum. 2010. Pro-
viding multilingual, multimodal answers to lexical
database queries. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), pages 348?355, Val-
letta, Malta.
Stefano Faralli and Roberto Navigli. 2012. A
New Minimally-supervised Framework for Domain
Word Sense Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1411?
1422, Jeju, Korea.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and bet-
ter) than one: the Wikipedia Bitaxonomy Project.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL
2014), Baltimore, Maryland.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY - a large-scale uni-
fied lexical-semantic resource based on LMF. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 580?590, Avignon, France.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th international conference
on World Wide Web, pages 517?526, Hawaii, USA.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2011. Semi-automatic extension of GermaNet
with sense definitions from Wiktionary. In Pro-
ceedings of 5th Language & Technology Conference
(LTC 2011), pages 126?130, Pozna, Poland.
Verena Henrich, Erhard W. Hinrichs, and Klaus Sut-
tner. 2012. Automatically linking GermaNet to
Wikipedia for harvesting corpus examples for Ger-
maNet senses. In Journal for Language Technology
and Computational Linguistics (JLCL), 27(1):1?19.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing,
pages 581?589, Prague, Czech Republic.
Oi Yee Kwong. 1998. Aligning WordNet with
additional lexical resources. In COLING-ACL98
Workshop on Usage of WordNet in Natural Lan-
guage Processing Systems, pages 73?79, Montreal,
Canada.
Egoitzand Laparra and German Rigau. 2009. Inte-
grating WordNet and FrameNet using a knowledge-
based Word Sense Disambiguation algorithm. In
Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP09), pages 1?6, Borovets,
Bulgaria.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-WSA: A graph-based approach to word
sense alignment. Transactions of the Association for
Computational Linguistics (TACL), 1:151?164.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Christian M. Meyer and Iryna Gurevych. 2010. ?worth
its weight in gold or yet another resource?; a com-
parative study of Wiktionary, OpenThesaurus and
GermaNet. In Proceedings of the 11th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing?10, pages 38?49,
Iasi, Romania.
Christian M. Meyer and Iryna Gurevych. 2011. What
psycholinguists know about Chemistry: Aligning
Wiktionary and WordNet for increased domain cov-
erage. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
883?892, Chiang Mai, Thailand.
Christian M. Meyer and Iryna Gurevych. 2012a. On-
toWiktionary: Constructing an ontology from the
collaborative online dictionary Wiktionary. In Semi-
Automatic Ontology Development: Processes and
Resources, pages 131?161. IGI Global.
477
Christian M. Meyer and Iryna Gurevych. 2012b.
To exhibit is not to loiter: A multilingual, sense-
disambiguated Wiktionary for measuring verb sim-
ilarity. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING
2012), pages 1763?1780, Mumbai, India.
Andrea Moro and Roberto Navigli. 2013. Integrating
syntactic and semantic analysis into the Open Infor-
mation Extraction paradigm. In Proceedings of the
23
rd
International Joint Conference on Artificial In-
telligence (IJCAI 2013), pages 2148?2154, Beijing,
China.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier
de Lacalle, and Eneko Agirre. 2011. Two birds
with one stone: Learning semantic models for text
categorization and Word Sense Disambiguation. In
Proceedings of the 20th ACM Conference on Infor-
mation and Knowledge Management (CIKM), pages
2317?2320, Glasgow, UK.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics joint with the 21st International Conference on
Computational Linguistics (COLING-ACL 2006),
pages 105?112, Sydney, Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
people?s web meets linguistic knowledge: Auto-
matic sense alignment of Wikipedia and WordNet.
In Proceedings of the Ninth International Confer-
ence on Computational Semantics, pages 205?214,
Oxford, United Kingdom.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic Role Labeling. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers.
Patrick Pantel and Marco Pennacchiotti. 2008. Auto-
matically harvesting and ontologizing semantic rela-
tions. In Proceedings of the 2008 Conference on On-
tology Learning and Population: Bridging the Gap
Between Text and Knowledge, pages 171?195, Am-
sterdam, The Netherlands.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Semantic
Similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1341?1351, Sofia, Bulgaria.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381?387. College Publications, Lon-
don, England.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Pro-
ceedings of the Third International Conference on
Advances in Web Intelligence, pages 380?386, Lodz,
Poland.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
the 6th International Conference on Computational
Linguistics and Intelligent Text Processing, pages
100?111, Mexico City, Mexico.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using Wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd national con-
ference on Artificial intelligence - Volume 2, pages
861?866, Chicago, Illinois.
478
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 945?955,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project
Tiziano Flati, Daniele Vannella, Tommaso Pasini and Roberto Navigli
Dipartimento di Informatica
Sapienza Universit`a di Roma
{flati,vannella,navigli}@di.uniroma1.it
p.tommaso@gmail.com
Abstract
We present WiBi, an approach to the
automatic creation of a bitaxonomy for
Wikipedia, that is, an integrated taxon-
omy of Wikipage pages and categories.
We leverage the information available in
either one of the taxonomies to reinforce
the creation of the other taxonomy. Our
experiments show higher quality and cov-
erage than state-of-the-art resources like
DBpedia, YAGO, MENTA, WikiNet and
WikiTaxonomy. WiBi is available at
http://wibitaxonomy.org.
1 Introduction
Knowledge has unquestionably become a key
component of current intelligent systems in many
fields of Artificial Intelligence. The creation and
use of machine-readable knowledge has not only
entailed researchers (Mitchell, 2005; Mirkin et al,
2009; Poon et al, 2010) developing huge, broad-
coverage knowledge bases (Hovy et al, 2013;
Suchanek and Weikum, 2013), but it has also
hit big industry players such as Google (Singhal,
2012) and IBM (Ferrucci, 2012), which are mov-
ing fast towards large-scale knowledge-oriented
systems.
The creation of very large knowledge bases
has been made possible by the availability of
collaboratively-curated online resources such as
Wikipedia and Wiktionary. These resources are
increasingly becoming enriched with new con-
tent in many languages and, although they are
only partially structured, they provide a great deal
of valuable knowledge which can be harvested
and transformed into structured form (Medelyan
et al, 2009; Hovy et al, 2013). Prominent
examples include DBpedia (Bizer et al, 2009),
BabelNet (Navigli and Ponzetto, 2012), YAGO
(Hoffart et al, 2013) and WikiNet (Nastase and
Strube, 2013). The types of semantic relation
in these resources range from domain-specific, as
in Freebase (Bollacker et al, 2008), to unspec-
ified relations, as in BabelNet. However, un-
like the case with smaller manually-curated re-
sources such as WordNet (Fellbaum, 1998), in
many large automatically-created resources the
taxonomical information is either missing, mixed
across resources, e.g., linking Wikipedia cate-
gories to WordNet synsets as in YAGO, or coarse-
grained, as in DBpedia whose hypernyms link to a
small upper taxonomy.
Current approaches in the literature have mostly
focused on the extraction of taxonomies from the
network of Wikipedia categories. WikiTaxonomy
(Ponzetto and Strube, 2007), the first approach
of this kind, is based on the use of heuristics
to determine whether is-a relations hold between
a category and its subcategories. Subsequent ap-
proaches have also exploited heuristics, but have
extended them to any kind of semantic relation
expressed in the category names (Nastase and
Strube, 2013). But while the aforementioned at-
tempts provide structure for categories that sup-
ply meta-information for Wikipedia pages, sur-
prisingly little attention has been paid to the ac-
quisition of a full-fledged taxonomy for Wikipedia
pages themselves. For instance, Ruiz-Casado et
al. (2005) provide a general vector-based method
which, however, is incapable of linking pages
which do not have a WordNet counterpart. Higher
coverage is provided by de Melo and Weikum
(2010) thanks to the use of a set of effective heuris-
tics, however, the approach also draws on Word-
Net and sense frequency information.
In this paper we address the task of taxono-
mizing Wikipedia in a way that is fully indepen-
dent of other existing resources such as WordNet.
We present WiBi, a novel approach to the cre-
ation of a Wikipedia bitaxonomy, that is, a tax-
onomy of Wikipedia pages aligned to a taxonomy
of categories. At the core of our approach lies the
idea that the information at the page and category
945
level are mutually beneficial for inducing a wide-
coverage and fine-grained integrated taxonomy.
2 WiBi: A Wikipedia Bitaxonomy
We induce a Wikipedia bitaxonomy, i.e., a taxon-
omy of pages and categories, in 3 phases:
1. Creation of the initial page taxonomy: we
first create a taxonomy for the Wikipedia
pages by parsing textual definitions, ex-
tracting the hypernym(s) and disambiguating
them according to the page inventory.
2. Creation of the bitaxonomy: we leverage
the hypernyms in the page taxonomy, to-
gether with their links to the corresponding
categories, so as to induce a taxonomy over
Wikipedia categories in an iterative way. At
each iteration, the links in the page taxonomy
are used to identify category hypernyms and,
conversely, the new category hypernyms are
used to identify more page hypernyms.
3. Refinement of the category taxonomy: fi-
nally we employ structural heuristics to over-
come inherent problems affecting categories.
The output of our three-phase approach is a bitax-
onomy of millions of pages and hundreds of thou-
sands of categories for the English Wikipedia.
3 Phase 1: Inducing the Page Taxonomy
The goal of the first phase is to induce a taxonomy
of Wikipedia pages. Let P be the set of all the
pages and let T
P
= (P,E) be the page taxonomy
directed graph whose nodes are pages and whose
edge set E is initially empty (E := ?). For each
p ? P our aim is to identify the most suitable gen-
eralization p
h
? P so that we can create the edge
(p, p
h
) and add it to E. For instance, given the
page APPLE, which represents the fruit meaning
of apple, we want to determine that its hypernym
is FRUIT and add the hypernym edge connecting
the two pages (i.e., E := E?{(APPLE, FRUIT)}).
To do this, we perform a syntactic step, in which
the hypernyms are extracted from the page?s tex-
tual definition, and a semantic step, in which the
extracted hypernyms are disambiguated according
to the Wikipedia inventory.
3.1 Syntactic step: hypernym extraction
In the syntactic step, for each page p ? P , we
extract zero, one or more hypernym lemmas, that
is, we output potentially ambiguous hypernyms
for the page. The first assumption, which follows
Julia Fiona Roberts is
an
American actress
NNP NNP NNP VBZ DT JJ
NN
nn
nn
nsubj
cop
det
amod
Figure 1: A dependency tree example with copula.
the Wikipedia guidelines and is validated in the
literature (Navigli and Velardi, 2010; Navigli and
Ponzetto, 2012), is that the first sentence of each
Wikipedia page p provides a textual definition for
the concept represented by p. The second assump-
tion we build upon is the idea that a lexical tax-
onomy can be obtained by extracting hypernyms
from textual definitions. This idea dates back to
the early 1970s (Calzolari et al, 1973), with later
developments in the 1980s (Amsler, 1981; Calzo-
lari, 1982) and the 1990s (Ide and V?eronis, 1993).
To extract hypernym lemmas, we draw on the
notion of copula, that is, the relation between the
complement of a copular verb and the copular verb
itself. Therefore, we apply the Stanford parser
(Klein and Manning, 2003) to the definition of a
page in order to extract all the dependency rela-
tions of the sentence. For example, given the def-
inition of the page JULIA ROBERTS, i.e., ?Julia
Fiona Roberts is an American actress.?, the Stan-
ford parser outputs the set of dependencies shown
in Figure 1. The noun involved in the copula re-
lation is actress and thus it is taken as the page?s
hypernym lemma. However, the extracted hyper-
nym is sometimes overgeneral (one, kind, type,
etc.). For instance, given the definition of the
page APOLLO, ?Apollo is one of the most impor-
tant and complex of the Olympian deities in an-
cient Greek and Roman religion [...].?, the only
copula relation extracted is between is and one.
To cope with this problem we use a list of stop-
words.
1
When such a term is extracted as hyper-
nym, we replace it with the rightmost noun of the
first following noun sequence (e.g., deity in the
above example). If the resulting lemma is again a
stopword we repeat the procedure, until a valid hy-
pernym or no appropriate hypernym can be found.
Finally, to capture multiple hypernyms, we iter-
atively follow the conj and and conj or relations
starting from the initially extracted hypernym. For
example, consider the definition of ARISTOTLE:
?Aristotle was a Greek philosopher and polymath,
a student of Plato and teacher of Alexander the
Great.? Initially, the philosopher hypernym is
selected thanks to the copula relation, then, fol-
1
E.g., species, genus, one, etc. Full list available online.
946
lowing the conjunction relations, also polymath,
student and teacher are extracted as hypernyms.
While more sophisticated approaches like Word-
Class Lattices could be applied (Navigli and Ve-
lardi, 2010), we found that, in practice, our hy-
pernym extraction approach provides higher cov-
erage, which is critical in our case.
3.2 Semantic step: hypernym disambiguation
Since our aim is to connect pairs of pages via
hypernym relations, our second step consists of
disambiguating the obtained hypernym lemmas of
page p by associating the most suitable page with
each hypernym. Following previous work (Ruiz-
Casado et al, 2005; Navigli and Ponzetto, 2012),
as the inventory for a given lemma we consider the
set of pages whose main title is the lemma itself,
except for the sense specification in parenthesis.
For instance, given fruit as the hypernym for AP-
PLE we would like to link APPLE to FRUIT as op-
posed to, e.g., FRUIT (BAND) or FRUIT (ALBUM).
3.2.1 Hypernym linkers
To disambiguate hypernym lemmas, we exploit
the structural features of Wikipedia through a
pipeline of hypernym linkers L = {L
i
}, applied
in cascade order (cf. Section 3.3.1). We start with
the set of page-hypernym pairs H = {(p, h)} as
obtained from the syntactic step. The successful
application of a linker to a pair (p, h) ? H yields
a page p
h
as the most suitable sense of h, result-
ing in setting isa(p, h) = p
h
. At step i, the i-
th linker L
i
? L is applied to H and all the hy-
pernyms which the linker could disambiguate are
removed from H . This prevents lower-precision
linkers from overriding decisions taken by more
accurate ones.
We now describe the hypernym linkers. In what
follows we denote with p
h
? p
h
the fact that the
definition of a Wikipedia page p contains an oc-
currence of h linked to page p
h
. Note that p
h
is
not necessarily a sense of h.
Crowdsourced linker If p
h
? p
h
, i.e., the hyper-
nym h is found to have been manually linked to p
h
in p by Wikipedians, we assign isa(p, h) = p
h
.
For example, because capital was linked in the
BRUSSELS page definition to CAPITAL CITY, we
set isa(BRUSSELS, capital) = CAPITAL CITY.
Category linker Given the set W ? P of
Wikipedia pages which have at least one category
in common with p, we select the majority sense
of h, if there is one, as hyperlinked across all the
definitions of pages in W :
isa(p, h) = argmax
p
h
?
p
?
?W
1(p
?
h
? p
h
)
where 1(p
?
h
? p
h
) is the characteristic function
which equals 1 if h is linked to p
h
in page
p
?
, 0 otherwise. For example, the linker sets
isa(EGGPLANT, plant) = PLANT because most of
the pages associated with TROPICAL FRUIT, a cat-
egory of EGGPLANT, contain in their definitions
the term plant linked to the PLANT page.
Multiword linker If p
m
? p
h
and m is a
multiword expression containing the lemma h
as one of its words, set isa(p, h) = p
h
. For
example, we set isa(PROTEIN, compound) =
CHEMICAL COMPOUND, as chemical compound
is linked to CHEMICAL COMPOUND in the defini-
tion of PROTEIN.
Monosemous linker If h is monosemous in
Wikipedia (i.e., there is only a single page p
h
for
that lemma), link it to its only sense by setting
isa(p, h) = p
h
. For example, we extract the
hypernym businessperson from the definition of
MERCHANT and, as it is unambiguous, we link
it to BUSINESSPERSON.
Distributional linker Finally, we provide a dis-
tributional approach to hypernym disambiguation.
We represent the textual definition of page p as a
distributional vector ~v
p
whose components are all
the English lemmas in Wikipedia. The value of
each component is the occurrence count of the cor-
responding content word in the definition of p.
The goal of this approach is to find the best
link for hypernym h of p among the pages h is
linked to, across the whole set of definitions in
Wikipedia. Formally, for each p
h
such that h
is linked to p
h
in some definition, we define the
set of pages P (p
h
) whose definitions contain a
link to p
h
, i.e., P (p
h
) = {p
?
? P |p
?
h
? p
h
}.
We then build a distributional vector ~v
p
?
for each
p
?
? P (p
h
) as explained above and create an ag-
gregate vector ~v
p
h
=
?
p
?
~v
p
?
. Finally, we de-
termine the similarity of p to each p
h
by calcu-
lating the dot product between the two vectors
sim(p, p
h
) = ~v
p
? ~v
p
h
. If sim(p, p
h
) > 0 for any
p
h
we perform the following association:
isa(p, h) = argmax
p
h
sim(p, p
h
)
For example, thanks to this linker we set
isa(VACUUM CLEANER, device) = MACHINE.
947
Figure 2: Distribution of linked hypernyms.
3.3 Page Taxonomy Evaluation
Statistics We applied the above linkers to the
October 2012 English Wikipedia dump. Out of
the 3,829,058 total pages, 4,270,232 hypernym
lemmas were extracted in the syntactic step for
3,697,113 pages (covering more than 96% of the
total). Due to illformed definitions, though, it
was not always possible to extract the hypernym
lemma: for example, 6 APRIL 2010 BAGHDAD
BOMBINGS is defined as ?A series of bomb ex-
plosions destroyed several buildings in Baghdad?,
which only implicitly provides the hypernym.
The semantic step disambiguated 3,718,612 hy-
pernyms for 3,294,562 Wikipedia pages, i.e., cov-
ering more than 86% of the English pages with at
least one disambiguated hypernym. Figure 2 plots
the number and distribution of hypernyms disam-
biguated by our hypernym linkers.
Taxonomy quality To evaluate the quality of
our page taxonomy we randomly sampled 1,000
Wikipedia pages. For each page we provided: i)
a list of suitable hypernym lemmas for the page,
mainly selected from its definition; ii) for each
lemma the correct hypernym page(s). We calcu-
lated precision as the average ratio of correct hy-
pernym lemmas (senses) to the total number of
lemmas (senses) returned for all the pages in the
dataset, recall as the number of correct lemmas
(senses) over the total number of lemmas (senses)
in the dataset, and coverage as the fraction of
pages for which at least one lemma (sense) was
returned, independently of its correctness. Results,
both at lemma- and sense-level, are reported in Ta-
ble 1. Not only does our taxonomy show high pre-
cision and recall in extracting ambiguous hyper-
nyms, it also disambiguates more than 3/4 of the
hypernyms with high precision.
3.3.1 Hypernym linker order
The optimal order of application of the above
linkers is the same as that presented in Section
3.2.1. It was established by selecting the combina-
tion, among all possible permutations, which max-
imized precision on a tuning set of 100 randomly
sampled pages, disjoint from our page dataset.
Prec. Rec. Cov.
Lemma 94.83 90.20 98.50
Sense 82.77 75.10 89.20
Table 1: Page taxonomy performance.
4 Phase 2: Inducing the Bitaxonomy
The page taxonomy built in Section 3 will serve
as a stable, pivotal input to the second phase, the
aim of which is to build our bitaxonomy, that is, a
taxonomy of pages and categories. Our key idea
is that the generalization-specialization informa-
tion available in each of the two taxonomies is
mutually beneficial. We implement this idea by
exploiting one taxonomy to update the other, and
vice versa, in an iterative way, until a fixed point
is reached. The final output of this phase is, on the
one hand, a page taxonomy augmented with addi-
tional hypernymy relations and, on the other hand,
a category taxonomy which is built from scratch.
4.1 Initialization
Our bitaxonomy B = {T
P
, T
C
} is a pair consist-
ing of the page taxonomy T
P
= (P,E), as ob-
tained in Section 3, and the category taxonomy
T
C
= (C, ?), which initially contains all the cate-
gories as nodes but does not include any hypernym
edge between category nodes. In the following
we describe the core algorithm of our approach,
which iteratively and mutually populates and re-
fines the edge sets E(T
P
) and E(T
C
).
4.2 The Bitaxonomy Algorithm
Preliminaries Before proceeding, we define
some basic concepts that will turn out to be use-
ful for presenting our algorithm. We denote by
super
T
(t) the set of all ancestors of a node t in the
taxonomy T (be it T
P
or T
C
). We further define a
verification function t;
T
t
?
which, in the case of
T
C
, returns true if there is a path in the Wikipedia
category network between t and t
?
, false other-
wise, and, in the case of T
P
, returns true if t
?
is
a sense, i.e., a page, of a hypernym h of t (that
is, (t, h) ? H , cf. Section 3.2.1). For instance,
SPORTSMEN ;
T
C
MEN BY OCCUPATION holds
for categories because the former is a sub-category
of the latter in Wikipedia, and RADIOHEAD ;
T
P
BAND (MUSIC) for pages, because band is a hy-
pernym extracted from the textual definition of
RADIOHEAD and BAND (MUSIC) is a sense of
band in Wikipedia. Note that, while the super
function returns information that we have already
learned, i.e., it is in T
P
and T
C
, the ; operator
948
holds just for candidate is-a relations, as it uses
knowledge from Wikipedia itself which is poten-
tially incorrect. For instance, SPORTSMEN ;
T
C
MEN?S SPORTS in the Wikipedia category net-
work, and RADIOHEAD ;
T
P
BAND (RADIO) be-
tween the two Wikipedia pages, both hold accord-
ing to our definition of ;, while connecting the
wrong hypernym candidates. At the core of our
algorithm, explained below, is the mutual lever-
aging of the super function from one of the two
taxonomies (pages or categories) to decide about
which candidates (for which a ; relation holds)
in the other taxonomy are real hypernyms.
Finally, we define the projection operator pi,
such that pi(c), c ? C, is the set of pages
categorized with c, and pi(p), p ? P , is the
set of categories associated with page p in
Wikipedia. For instance, the pages which belong
to the category OLYMPIC SPORTS are given by
pi(OLYMPIC SPORTS) = {BASEBALL, BOXING,
. . . , TRIATHLON}. Vice versa, pi(TRIATHLON) =
{MULTISPORTS, OLYMPIC SPORTS, . . . , OPEN
WATER SWIMMING}. The projection operator pi
enables us to jump from one taxonomy to the other
and expresses the mutual membership relation be-
tween pages and categories.
Algorithm We now describe in detail the bitax-
onomy algorithm, whose pseudocode is given in
Algorithm 1. The algorithm takes as input the two
taxonomies, initialized as described in Section 4.1.
Starting from the category taxonomy (line 1), the
algorithm updates the two taxonomies in turn, un-
til convergence is reached, i.e., no more edges can
be added to any side of the bitaxonomy. Let T be
the current taxonomy considered at a given mo-
ment and T
?
its dual taxonomy. The algorithm
proceeds by selecting a node t ? V (T ) for which
no hypernym edge (t, t
h
) could be found up until
that moment (line 3), and then tries to infer such
a relation by drawing on the dual taxonomy T
?
(lines 5-12). This is the core of the bitaxonomy al-
gorithm, in which hypernymy knowledge is trans-
ferred from one taxonomy to the other. By apply-
ing the projection operator pi to t, the algorithm
considers those nodes t
?
aligned to t in the dual
taxonomy (line 5) and obtains their hypernyms t
?
h
using the super
T
?
function (line 6). The nodes
reached in T
?
act as a clue for discovering the suit-
able hypernyms for the starting node t ? V (T ).
To perform the discovery, the algorithm projects
each such hypernym node t
?
h
? S and increments
the count of each projection t
h
? pi(t
?
h
) (line
Algorithm 1 The Bitaxonomy Algorithm
Input: T
P
, T
C
1: T := T
C
, T
?
:= T
P
2: repeat
3: for all t ? V (T ) s.t. @(t, t
h
) ? E(T ) do
4: reset count
5: for all t
?
? pi(t) do
6: S := super
T
?
(t
?
)
7: for all t
?
h
? S do
8: for all t
h
? pi(t
?
h
) do count(t
h
)++ end for
9: end for
10: end for
11:
?
t
h
:= argmax
t
h
: t;
T
t
h
count(t
h
)
12: if count(
?
t
h
) > 0 thenE(T ) := E(T )?{(t,
?
t
h
)}
13: end for
14: swap T and T
?
15: until convergence
16: return {T, T
?
}
8). Finally, the node
?
t
h
? V (T ) with maximum
count, and such that t ;
T
?
t
h
holds, if one exists,
is promoted as hypernym of t and a new hypernym
edge (t,
?
t
h
) is added toE(T ) (line 12). Finally, the
role of the two taxonomies is swapped and the pro-
cess is repeated until no more change is possible.
Example Let us illustrate the algorithm by way
of an example. Assume we are in the first iteration
(T = T
C
) and consider the Wikipedia category
t = OLYMPICS (line 3) and its super-categories
{MULTI-SPORT EVENTS, SPORT AND POLITICS,
INTERNATIONAL SPORTS COMPETITIONS}. This
category has 27 pages associated with it (line
5), 23 of which provide a hypernym page in T
P
(line 6): e.g., PARALYMPIC GAMES, associated
with the category OLYMPICS, is a MULTI-SPORT
EVENT and is therefore contained in S. By con-
sidering and counting the categories of each page
in S (lines 7-8), we end up counting the category
MULTI-SPORT EVENTS four times and other
categories, such as AWARDS and SWIMSUITS,
once. As MULTI-SPORT EVENTS has the highest
count and is connected to OLYMPICS by a path
in the Wikipedia category network (line 11),
the hypernym edge (OLYMPICS, MULTI-SPORT
EVENTS) is added to T
C
(line 12).
5 Phase 3: Category taxonomy
refinement
As the final phase, we refine and enrich the cate-
gory taxonomy. The goal of this phase is to pro-
vide broader coverage to the category taxonomy
T
C
created as explained in Section 4. We apply
three enrichment heuristics which add hypernyms
to those categories c for which no hypernym could
be found in phase 2, i.e., @c
?
s.t. (c, c
?
) ? E(T
C
).
949
Single super-category As a first structural re-
finement, we automatically link an uncovered cat-
egory c to c
?
if c
?
is the only direct super-category
of c in Wikipedia.
Sub-categories We increase the hypernym cov-
erage by exploiting the sub-categories of each un-
covered category c (see Figure 3a). In detail,
for each uncovered category c we consider the
set sub(c) of all the Wikipedia subcategories of
c (nodes c
1
, c
2
, . . . , c
n
in Figure 3a) and then let
each category vote, according to its direct hyper-
nym categories in T
C
(the vote is as in Algo-
rithm 1). Then we proceed in decreasing order
of vote and select the highest-ranking category c
?
which is connected to c via a path in T
C
, i.e.,
c ;
T
C
c
?
. We then pick up the direct ancestor
c
??
of c which lies in the path from c to c
?
and
add the hypernym edge (c, c
??
) to E(T
C
). For ex-
ample, consider the category FRENCH TELEVI-
SION PEOPLE; since this category has no asso-
ciated pages, in phase 2 no hypernym could be
found. However, by applying the sub-categories
heuristic, we discover that TELEVISION PEOPLE
BY COUNTRY is the hypernym most voted by our
target category?s descendants, such as FRENCH
TELEVISION ACTORS and FRENCH TELEVISION
DIRECTORS. Since TELEVISION PEOPLE BY
COUNTRY is at distance 1 in the Wikipedia
category network from FRENCH TELEVISION
PEOPLE, we add (FRENCH TELEVISION PEOPLE,
TELEVISION PEOPLE BY COUNTRY) to E(T
C
).
Super-categories We then apply a similar
heuristic involving super-categories (see Figure
3b). Given an uncovered category c, we consider
its direct Wikipedia super-categories and let them
vote, according to their hypernym categories in
T
C
. Then we proceed in decreasing order of vote
and select the highest-ranking category c
?
which is
connected to c in T
C
, i.e., c;
T
C
c
?
. We then pick
up the direct ancestor c
??
of c which lies in the path
from c to c
?
and add the edge (c, c
??
) to E(T
C
).
5.1 Bitaxonomy Evaluation
Category taxonomy statistics We applied
phases 2 and 3 to the output of phase 1, which
was evaluated in Section 3.3. In Figure 4a we
show the increase in category coverage at each
iteration throughout the execution of the two
phases (1SUP, SUB and SUPER correspond to
the three above heuristics of phase 3). The final
outcome is a category taxonomy which includes
594,917 hypernymy links between categories,
c
?
d
e
c
??
c
c
1
c
2
. . .
c
n
(a) Sub categ. heuristic.
hypernym in T
C
Wikipedia super-category
c
?
c
???
c
1
c
??
c
m
. . .
c
(b) Super categ. heuristic.
Figure 3: Heuristic patterns for the coverage re-
finement of the category taxonomy.
covering more than 96% of the 618,641 categories
in the October 2012 English Wikipedia dump.
The graph shows the steepest slope in the first
iterations of phase 2, which converges around
400k categories at iteration 30, and a significant
boost due to phase 3 producing another 175k
hypernymy edges, with the super-category heuris-
tic contributing most. 78.90% of the nodes in
T
C
belong to the same connected component.
The average height of the biggest component of
T
C
is 23.26 edges and the maximum height is
49. We note that the average height of T
C
is
much greater than that of T
P
, which reflects the
category taxonomy distinguishing between very
subtle classes, such as ALBUMS BY ARTISTS,
ALBUMS BY RECORDING LOCATION, etc.
Category taxonomy quality To estimate the
quality of the category taxonomy, we ran-
domly sampled 1,000 categories and, for each of
them, we manually associated the super-categories
which were deemed to be appropriate hypernyms.
Figure 4b shows the performance trend as the al-
gorithm iteratively covers more and more cate-
gories. Phase 2 is particularly robust across it-
erations, as it leads to increased recall while re-
taining very high precision. As regards phase 3,
the super-categories heuristic leads to only a slight
precision decrease, while improving recall consid-
erably. Overall, the final taxonomy T
C
achieves
85.80% precision, 83.40% recall and 97.20% cov-
erage on our dataset.
Page taxonomy improvement As a result of
phase 2, 141,105 additional hypernymy links were
also added to the page taxonomy, resulting in
an overall 82.99% precision, 77.90% recall and
92.10% coverage, with a non-negligible 3% boost
from phase 1 to phase 2 in terms of recall and cov-
erage on our Wikipedia page dataset.
We also calculated some statistics for the result-
ing taxonomy obtained by aggregating the 3.8M
950
Figure 4: Category taxonomy evaluation.
hypernym links in a single directed graph. Over-
all, 99% of nodes belong to the same connected
component, with a maximum height of 29 and an
average height on the biggest component of 6.98.
6 Related Work
Although the extraction of taxonomies from
machine-readable dictionaries was already being
studied in the early 1970s (Calzolari et al, 1973),
pioneering work on large amounts of data only
appeared in the 1990s (Hearst, 1992; Ide and
V?eronis, 1993). Approaches based on hand-
crafted patterns and pattern matching techniques
have been developed to provide a supertype for
the extracted terms (Etzioni et al, 2004; Blohm,
2007; Kozareva and Hovy, 2010; Navigli and Ve-
lardi, 2010; Velardi et al, 2013, inter alia). How-
ever, these methods do not link terms to existing
knowledge resources such as WordNet, whereas
those that explicitly link do so by adding new
leaves to the existing taxonomy instead of acquir-
ing wide-coverage taxonomies from scratch (Pan-
tel and Ravichandran, 2004; Snow et al, 2006).
The recent upsurge of interest in collabo-
rative knowledge curation has enabled several
approaches to large-scale taxonomy acquisition
(Hovy et al, 2013). Most approaches initially
focused on the Wikipedia category network, an
entangled set of generalization-containment rela-
tions between Wikipedia categories, to extract the
hypernymy taxonomy as a subset of the network.
The first approach of this kind was WikiTaxonomy
(Ponzetto and Strube, 2007; Ponzetto and Strube,
2011), based on simple, yet effective lightweight
heuristics, totaling more than 100k is-a relations.
Other approaches, such as YAGO (Suchanek et
al., 2008; Hoffart et al, 2013), yield a taxonom-
ical backbone by linking Wikipedia categories to
WordNet. However, the categories are linked to
the first, i.e., most frequent, sense of the category
head in WordNet, involving only leaf categories in
the linking.
Interest in taxonomizing Wikipedia pages, in-
stead, developed with DBpedia (Auer et al, 2007),
which pioneered the current stream of work aimed
at extracting semi-structured information from
Wikipedia templates and infoboxes. In DBpedia,
entities are mapped to a coarse-grained ontology
which is collaboratively maintained and contains
only about 270 classes corresponding to popular
named entity types, in contrast to our goal of struc-
turing the full set of Wikipedia articles in a larger
and finer-grained taxonomy.
A few notable efforts to reconcile the two sides
of Wikipedia, i.e., pages and categories, have
been put forward very recently: WikiNet (Nas-
tase et al, 2010; Nastase and Strube, 2013) is a
project which heuristically exploits different as-
pects of Wikipedia to obtain a multilingual con-
cept network by deriving not only is-a relations,
but also other types of relations. A second project,
MENTA (de Melo and Weikum, 2010), creates
one of the largest multilingual lexical knowledge
bases by interconnecting more than 13M articles
in 271 languages. In contrast to our work, hy-
pernym extraction is supervised in that decisions
are made on the basis of labelled training exam-
ples and requires a reconciliation step owing to
the heterogeneous nature of the hypernyms, some-
thing that we only do for categories, due to their
noisy network. While WikiNet and MENTA bring
together the knowledge available both at the page
and category level, like we do, they either achieve
low precision and coverage of the taxonomical
structure or exhibit overly general hypernyms, as
we show in our experiments in the next section.
Our work differs from the others in at least three
respects: first, in marked contrast to most other re-
sources, but similarly to WikiNet and WikiTaxon-
omy, our resource is self-contained and does not
depend on other resources such as WordNet; sec-
ond, we address the taxonomization task on both
sides, i.e., pages and categories, by providing an
algorithm which mutually and iteratively transfers
knowledge from one side of the bitaxonomy to the
other; third, we provide a wide coverage bitaxon-
omy closer in structure and granularity to a manual
WordNet-like taxonomy, in contrast, for example,
to DBpedia?s flat entity-focused hierarchy.
2
2
Note that all the competitors on categories have average
height between 1 and 3.69 on their biggest component, while
we have 23.26, while on pages their height is between 1.9 and
4.22, while ours is 6.98. Since WordNet?s average height is
8.07 we deem WiBi to be the resource structurally closest to
WordNet.
951
Dataset System Prec. Rec. Cov.
Pages
WiBi 84.11 79.40 92.57
WikiNet 57.29
??
71.45
??
82.01
DBpedia 87.06 51.50
??
55.93
MENTA 81.52 72.49
?
88.92
Categories
WiBi 85.18 82.88 97.31
WikiTax 88.50 54.83
??
59.43
YAGO 94.13 53.41
??
56.74
MENTA 87.11 84.63 97.15
MENTA
?ENT
85.18 71.95
??
84.47
Table 2: Page and category taxonomy evaluation.
?
(
??
) denotes statistically significant difference,
using ?
2
test, p < 0.02 (p < 0.01) between WiBi
and the daggered resource.
7 Comparative Evaluation
7.1 Experimental Setup
We compared our resource (WiBi) against the
Wikipedia taxonomies of the major knowledge re-
sources in the literature providing hypernym links,
namely DBpedia, WikiNet, MENTA, WikiTax-
onomy and YAGO (see Section 6). As datasets,
we used our gold standards of 1,000 randomly-
sampled pages (see Section 3.3) and categories
(see Section 5.1). In order to ensure a level playing
field, we detected those pages (categories) which
do not exist in any of the above resources and re-
moved them to ensure full coverage of the dataset
across all resources. For each resource we cal-
culated precision, by manually marking each hy-
pernym returned for each page (category) as cor-
rect or not. As regards recall, we note that in
two cases (i.e., DBpedia returning page super-
types from its upper taxonomy, YAGO linking cat-
egories to WordNet synsets) the generalizations
are neither pages nor categories and that MENTA
returns heterogeneous hypernyms as mixed sets of
WordNet synsets, Wikipedia pages and categories.
Given this heterogeneity, standard recall across re-
sources could not be calculated. For this reason we
calculated recall as described in Section 3.3.
7.2 Results
Wikipedia pages We first report the results of
the knowledge resources which provide page hy-
pernyms, i.e., we compare against WikiNet, DB-
pedia and MENTA. We use the original outputs
from the three resources: the first two are based
on dumps which are from the same year as the one
used in WiBi (cf. Section 3.3), while MENTA is
based on a dump dating back to 2010 (consisting
of 3.25M pages and 565k categories). We decided
to include the latter for comparison purposes, as it
uses knowledge from 271 Wikipedias to build the
final taxonomy. However, we recognize its perfor-
mance might be relatively higher on a 2012 dump.
We show the results on our page hypernym
dataset in Table 2 (top). As can be seen, WikiNet
obtains the lowest precision, due to the high num-
ber of hypernyms provided, many of which are
incorrect, with a recall between that of DBpe-
dia and MENTA. WiBi outperforms all other re-
sources with 84.11% precision, 79.40% recall and
92.57% coverage. MENTA seems to be the clos-
est resource to ours, however, we remark that the
hypernyms output by MENTA are very heteroge-
neous: 48% of answers are represented by a Word-
Net synset, 37% by Wikipedia categories and 15%
are Wikipedia pages. In contrast to all other re-
sources, WiBi outputs page hypernyms only.
Wikipedia categories We then compared all the
knowledge resources which deal with categories,
i.e., WikiTaxonomy, YAGO and MENTA. For the
latter two, the above considerations about the 2012
dump hold, whereas we reimplemented WikiTax-
onomy, which was based on a 2009 dump, to run it
on the same dump as WiBi. We excluded WikiNet
from our comparison because it turned out to have
low coverage of categories (i.e., less than 1%).
We show the results on our category dataset
in Table 2 (bottom). Despite other systems ex-
hibiting higher precision, WiBi generally achieves
higher recall, thanks also to its higher category
coverage. YAGO obtains the lowest recall and
coverage, because only leaf categories are consid-
ered. MENTA is the closest system to ours, ob-
taining slightly higher precision and recall. No-
tably, however, MENTA outputs the first WordNet
sense of entity for 13.17% of all the given answers,
which, despite being correct and accounted in pre-
cision and recall, is uninformative. Since a system
which always outputs entity would maximise all
the three measures, we also calculated the perfor-
mance for MENTA when discarding entity as an
answer; as Table 2 shows (bottom, MENTA
?ENT
),
recall drops to 71.95%. Further analysis, pre-
sented below, shows that the specificity of its hy-
pernyms is considerably lower than that of WiBi.
7.3 Analysis of the results
To get further insight into our results we per-
formed two additional analyses of the data. First,
we estimated the level of specialization of the
hypernyms in the different resources on our two
datasets. The idea is that a hypernym should be
952
Dataset System (X) WiBi=X WiBi>X WiBi<X
Pages
WikiNet 33.38 34.94 31.68
DBpedia 31.68 56.71 11.60
MENTA 19.04 50.85 30.12
Categories
WikiTax 43.11 38.51 18.38
YAGO 12.36 81.14 6.50
MENTA 12.36 73.69 13.95
Table 3: Specificity comparison.
valid while at the same time being as specific as
possible (e.g., SINGER should be preferred over
PERSON). We therefore calculated a measure,
which we called specificity, that computes the per-
centage of times a system outputs a more specific
answer than another system. To do this, we anno-
tated each hypernym returned by a system as fol-
lows: ?1 if the answer was wrong, 0 if missing, >
0 if correct; more specific answers were assigned
higher scores. When comparing two systems, we
select the respective most specific answers a
1
, a
2
and say the first system is more specific than the
latter whenever score(a
1
) > score(a
2
). Table 3
shows the results for all the resources and for both
the page and category taxonomies: WiBi consis-
tently provides considerably more specific hyper-
nyms than any other resource (middle column).
A second important aspect that we analyzed was
the granularity of each taxonomy, determined by
drawing each resource on a bidimensional plane
with the number of distinct hypernyms on the
x axis and the total number of hypernyms (i.e.,
edges) in the taxonomy on the y axis. Figures 5a
and 5b show the position of each resource for the
page and the category taxonomies, respectively.
As can be seen, WiBi, as well as the page tax-
onomy of MENTA, is the resource with the best
granularity, as not only does it attain high cover-
age, but it also provides a larger variety of classes
as generalizations of pages and categories. Specif-
ically, WiBi provides over 3M hypernym pages
chosen from a range of 94k distinct hypernyms,
while others exhibit a considerably smaller range
of distinct hypernyms (e.g., DBpedia by design,
but also WikiNet, with around 11k distinct page
hypernyms). The large variety of classes provided
by MENTA, however, is due to including more
than 100k Wikipedia categories (among which,
categories about deaths and births alone repre-
sent about 2% of the distinct hypernyms). As re-
gards categories, while the number of distinct hy-
pernyms of WiBi and WikiTaxonomy is approxi-
mately the same (around 130k), the total number
of hypernyms (around 580k for both taxonomies)
is distributed over half of the categories in Wiki-
(a) Page taxonomies (b) Category taxonomies
Figure 5: Hypernym granularity for the resources.
Taxonomy compared to WiBi, resulting in a dou-
ble number of hypernyms per category, but lower
coverage (cf. Table 2).
8 Conclusions
In this paper we have presented WiBi, an auto-
matic 3-phase approach to the construction of a
bitaxonomy for the English Wikipedia, i.e., a full-
fledged, integrated page and category taxonomy:
first, using a set of high-precision linkers, the page
taxonomy is populated; next, a fixed point algo-
rithm populates the category taxonomy while en-
riching the page taxonomy iteratively; finally, the
category taxonomy undergoes structural refine-
ments. Coverage, quality and granularity of the
bitaxonomy are considerably higher than the tax-
onomy of state-of-the-art resources like DBpedia,
YAGO, MENTA, WikiNet and WikiTaxonomy.
Our contributions are three-fold: i) we propose
a unified, effective approach to the construction of
a Wikipedia bitaxonomy, a richer structure than
those produced in the literature; ii) our method for
building the bitaxonomy is self-contained, thanks
to its independence from external resources (like
WordNet) and the virtual absence of supervision,
making WiBi replicable on any new version of
Wikipedia; iii) the taxonomy provides nearly full
coverage of pages and categories, encompassing
the entire encyclopedic knowledge in Wikipedia.
We will apply our video games with a purpose
(Vannella et al, 2014) to validate WiBi. We also
plan to integrate WiBi into BabelNet (Navigli and
Ponzetto, 2012), so as to fully taxonomize it, and
exploit its high quality for improving semantic
predicates (Flati and Navigli, 2013).
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We thank Luca Telesca for his implementation of
WikiTaxonomy and Jim McManus for his com-
ments on the manuscript.
953
References
Robert A. Amsler. 1981. A Taxonomy for English
Nouns and Verbs. In Proceedings of Association for
Computational Linguistics (ACL ?81), pages 133?
138, Stanford, California, USA.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722?735, Bu-
san, Korea.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crystal-
lization point for the Web of Data. Web Semantics,
7(3):154?165.
Sebastian Blohm. 2007. Using the web to reduce data
sparseness in pattern-based information extraction.
In Proceedings of the 11th European Conference on
Principles and Practice of Knowledge Discovery in
Databases (PKDD), pages 18?29, Warsaw, Poland.
Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the International
Conference on Management of Data (SIGMOD ?08),
SIGMOD ?08, pages 1247?1250, New York, NY,
USA.
Nicoletta Calzolari, Laura Pecchia, and Antonio Zam-
polli. 1973. Working on the Italian Machine Dictio-
nary: a Semantic Approach. In Proceedings of the
5th Conference on Computational Linguistics (COL-
ING ?73), pages 49?70, Pisa, Italy.
Nicoletta Calzolari. 1982. Towards the organization of
lexical definitions on a database structure. In Proc.
of the 9th Conference on Computational Linguistics
(COLING ?82), pages 61?64, Prague, Czechoslo-
vakia.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.
In Proceedings of Conference on Information and
Knowledge Management (CIKM ?10), pages 1099?
1108, New York, NY, USA.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
ItAll: (preliminary results). In Proceedings of the
13th International Conference on World Wide Web
(WWW ?04), pages 100?110, New York, NY, USA.
ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
David A. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3):1.
Tiziano Flati and Roberto Navigli. 2013. SPred:
Large-scale Harvesting of Semantic Predicates. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1222?1232, Sofia, Bulgaria.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the International Conference on Computational
Linguistics (COLING ?92), pages 539?545, Nantes,
France.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from Wikipedia. Artificial Intelligence, 194:28?61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Nancy Ide and Jean V?eronis. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings of
the Workshop on Knowledge Bases and Knowledge
Structures, pages 257?266, Tokyo, Japan.
Dan Klein and Christopher D. Manning. 2003. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), pages 3?10,
Vancouver, British Columbia, Canada.
Zornitsa Kozareva and Eduard H. Hovy. 2010. A
Semi-Supervised Method to Learn and Construct
Taxonomies Using the Web. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?10), pages 1110?1118,
Seattle, WA, USA.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 558?566,
Athens, Greece.
Tom Mitchell. 2005. Reading the Web: A Break-
through Goal for AI. AI Magazine.
Vivi Nastase and Michael Strube. 2013. Transform-
ing Wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62?85.
954
Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A Very Large Scale Multi-Lingual Concept Net-
work. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010), pages 1318?1327, Uppsala, Sweden,
July. Association for Computational Linguistics.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL HLT
2013), Boston, Massachusetts, 2?7 May 2004, pages
321?328.
Simone Paolo Ponzetto and Michael Strube. 2007.
Deriving a large scale taxonomy from Wikipedia.
In Proceedings of the 22nd Conference on the Ad-
vancement of Artificial Intelligence (AAAI ?07), Van-
couver, B.C., Canada, 22?26 July 2007, pages
1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. Artificial Intelligence, 175(9-
10):1737?1756.
Hoifung Poon, Janara Christensen, Pedro Domingos,
Oren Etzioni, Raphael Hoffmann, Chloe Kiddon,
Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Ste-
fan Schoenmackers, Stephen Soderland, Dan Weld,
Fei Wu, and Congle Zhang. 2010. Machine Read-
ing at the University of Washington. In Proceedings
of the 1st International Workshop on Formalisms
and Methodology for Learning by Reading in con-
junction with NAACL-HLT 2010, pages 87?95, Los
Angeles, California, USA.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lec-
ture Notes in Computer Science, pages 380?386.
Springer Verlag.
Amit Singhal. 2012. Introducing the Knowledge
Graph: Things, Not Strings. Technical report, Of-
ficial Blog (of Google). Retrieved May 18, 2012.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL 2006), pages 801?
808.
Fabian Suchanek and Gerhard Weikum. 2013. Knowl-
edge harvesting from text and Web sources. In IEEE
29th International Conference on Data Engineer-
ing (ICDE 2013), pages 1250?1253, Brisbane, Aus-
tralia. IEEE Computer Society.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Daniele Vannella, David Jurgens, Daniele Scarfini,
Domenico Toscani, and Roberto Navigli. 2014.
Validating and Extending Semantic Knowledge
Bases using Video Games with a Purpose. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2014),
Baltimore, USA.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3):665?707.
955
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1294?1304,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Validating and Extending Semantic Knowledge Bases
using Video Games with a Purpose
Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
surname@di.uniroma1.it
Abstract
Large-scale knowledge bases are impor-
tant assets in NLP. Frequently, such re-
sources are constructed through automatic
mergers of complementary resources, such
as WordNet and Wikipedia. However,
manually validating these resources is pro-
hibitively expensive, even when using
methods such as crowdsourcing. We pro-
pose a cost-effective method of validat-
ing and extending knowledge bases using
video games with a purpose. Two video
games were created to validate concept-
concept and concept-image relations. In
experiments comparing with crowdsourc-
ing, we show that video game-based vali-
dation consistently leads to higher-quality
annotations, even when players are not
compensated.
1 Introduction
Large-scale knowledge bases are an essential
component of many approaches in Natural Lan-
guage Processing (NLP). Semantic knowledge
bases such as WordNet (Fellbaum, 1998), YAGO
(Suchanek et al, 2007), and BabelNet (Navigli
and Ponzetto, 2010) provide ontological struc-
ture that enables a wide range of tasks, such as
measuring semantic relatedness (Budanitsky and
Hirst, 2006) and similarity (Pilehvar et al, 2013),
paraphrasing (Kauchak and Barzilay, 2006), and
word sense disambiguation (Navigli and Ponzetto,
2012; Moro et al, 2014). Furthermore, such
knowledge bases are essential for building unsu-
pervised algorithms when training data is sparse
or unavailable. However, constructing and updat-
ing semantic knowledge bases is often limited by
the significant time and human resources required.
Recent approaches have attempted to build or
extend these knowledge bases automatically. For
example, Snow et al (2006) and Navigli (2005)
extend WordNet using distributional or structural
features to identify novel semantic connections
between concepts. The recent advent of large
semi-structured resources has enabled the creation
of new semantic knowledge bases (Medelyan et
al., 2009; Hovy et al, 2013) through automati-
cally merging WordNet and Wikipedia (Suchanek
et al, 2007; Navigli and Ponzetto, 2010; Nie-
mann and Gurevych, 2011). While these auto-
matic approaches offer the scale needed for open-
domain applications, the automatic processes of-
ten introduce errors, which can prove detrimental
to downstream applications. To overcome issues
from fully-automatic construction methods, sev-
eral works have proposed validating or extending
knowledge bases using crowdsourcing (Biemann
and Nygaard, 2010; Eom et al, 2012; Sarasua et
al., 2012). However, these methods, too, are lim-
ited by the resources required for acquiring large
numbers of responses.
In this paper, we propose validating and extend-
ing semantic knowledge bases using video games
with a purpose. Here, the annotation tasks are
transformed into elements of a video game where
players accomplish their jobs by virtue of playing
the game, rather than by performing a more tradi-
tional annotation task. While prior efforts in NLP
have incorporated games for performing annota-
tion and validation (Siorpaes and Hepp, 2008b;
Herda?gdelen and Baroni, 2012; Poesio et al,
2013), these games have largely been text-based,
adding game-like features such as high-scores on
top of an existing annotation task. In contrast,
we introduce two video games with graphical 2D
gameplay that is similar to what game players are
familiar with. The fun nature of the games pro-
vides an intrinsic motivation for players to keep
playing, which can increase the quality of their
work and lower the cost per annotation.
Our work provides the following three contribu-
tions. First, we demonstrate effective video game-
based methods for both validating and extending
1294
semantic networks, using two games that operate
on complementary sources of information: seman-
tic relations and sense-image mappings. In con-
trast to previous work, the annotation quality is
determined in a fully automatic way. Second, we
demonstrate that converting games with a purpose
into more traditional video games creates an in-
creased player incentive such that players annotate
for free, thereby significantly lowering annotation
costs below that of crowdsourcing. Third, for both
games, we show that games produce better quality
annotations than crowdsourcing.
2 Related Work
Multiple works have proposed linguistic
annotation-based games with a purpose for
tasks such as anaphora resolution (Hladk?a et
al., 2009; Poesio et al, 2013), paraphrasing
(Chklovski and Gil, 2005), term associations
(Artignan et al, 2009; Lafourcade and Joubert,
2010), query expansion (Simko et al, 2011), and
word sense disambiguation (Chklovski and Mi-
halcea, 2002; Seemakurty et al, 2010; Venhuizen
et al, 2013). Notably, all of these linguistic games
focus on users interacting with text, in contrast
to other highly successful games with a purpose
in other domains, such as Foldit (Cooper et al,
2010), in which players fold protein sequences,
and the ESP game (von Ahn and Dabbish, 2004),
where players label images with words.
Most similar to our work are games that create
or validate common sense knowledge. Two games
with a purpose have incorporated video game-
like mechanics for annotation. First, Herda?gdelen
and Baroni (2012) validate automatically acquired
common sense relations using a slot machine
game where players must identify valid relations
and arguments from randomly aligned data within
a time limit. Although the validation is embedded
in a game-like setting, players are limited to one
action (pulling the lever) unlike our games, which
feature a variety of actions and rich gameplay ex-
perience to keep players interested longer. Sec-
ond, Kuo et al (2009) describe a pet-raising game
where players must answer common sense ques-
tions in order to obtain pet food. While their game
is among the most video game-like, the annotation
task is a chore the player must perform in order to
return to the game, rather than an integrated, fun
part of the game?s objectives, which potentially
decreases motivation for answering correctly.
Several works have proposed adapting existing
word-based board game designs to create or val-
idate common sense knowledge. von Ahn et al
(2006) generate common sense facts by using a
game similar to Taboo
TM
, where one player must
list facts about a computer-selected lemma and a
second player must guess the original lemma hav-
ing seen only the facts. Similarly, Vickrey et al
(2008) gather free associations to a target word
with the constraint, similar to Taboo
TM
, where
players cannot enter a small set of banned words.
Vickrey et al (2008) also present two games simi-
lar to the Scattergories
TM
, where players are given
a category and then must list things in that cate-
gory. The two variants differ in the constraints im-
posed on the players, such as beginning all items
with a specific letter. For all three games, two
players play the same game under time limits and
then are rewarded if their answers match.
Last, three two-player games have focused
on validating and extending knowledge bases.
Rzeniewicz and Szyma?nski (2013) extend Word-
Net with common-sense knowledge using a 20
Questions-like game. In a rapid-play style game,
OntoPronto attempts to classify Wikipedia pages
as either categories or individuals (Siorpaes and
Hepp, 2008a). SpotTheLink uses a similar rapid
question format to have players align the DBpedia
and PROTON ontologies by agreeing on the dis-
tinctions between classes (Thaler et al, 2011).
Unlike dynamic gaming elements common in
our video games, the above games are all focused
on interacting with textual items. Another major
limitation is their need for always having two play-
ers, which requires them to sustain enough inter-
est to always maintain an active pool of players.
While the computer can potentially act as a second
player, such a simulated player is often limited to
using preexisting knowledge or responses, which
makes it difficult to validate new types of entities
or create novel answers. In contrast, we drop this
requirement thanks to a new strategy for assign-
ing confidence scores to the annotations based on
negative associations.
3 Video Game with a Purpose Design
To create video games, our development process
focused on a common design philosophy and a
common data set.
3.1 Design Objectives
Three design objectives were used to develop the
video games. First, the annotation task should be
a central and natural action with familiar video
game mechanics. That is, the annotation should
1295
be supplied by common actions such as collecting
items, puzzles, or destroying objects, rather than
through extrinsic tasks that players must complete
in order to return to the game. This design has
the benefits of (1) growing the annotator pool with
video games players, and (2) potentially increas-
ing annotator enjoyment.
Second, the game should be playable by a single
player, with reinforcement for correct game play
coming from gold standard examples.
1
We note
that gold standard examples may come from both
true positive and true negative items.
Third, the game design should be sufficiently
general to annotate a variety of linguistic phenom-
ena, such that only the game data need be changed
to accomplish a different annotation task. While
some complex linguistic annotation tasks such as
preposition attachment may be difficult to inte-
grate directly into gameplay, many simpler but still
necessary annotation tasks such as word and im-
age associations can be easily modeled with tradi-
tional video game mechanics.
3.2 Annotation Setup
Tasks We focused on two annotation tasks: (1)
validating associations between two concepts, and
(2) validating associations between a concept and
an image. For each task we developed a video
game with a purpose that integrates the task within
the game, as illustrated in Sections 4 and 5.
Knowledge base As the reference knowledge
base, we chose BabelNet
2
(Navigli and Ponzetto,
2010), a large-scale multilingual semantic ontol-
ogy created by automatically merging WordNet
with other collaboratively-constructed resources
such as Wikipedia and OmegaWiki. BabelNet
data offers two necessary features for generat-
ing the games? datasets. First, by connecting
WordNet synsets to Wikipedia pages, most synsets
are associated with a set of pictures; while often
noisy, these pictures sometimes illustrate the tar-
get concept and are an ideal case for validation.
Second, BabelNet contains the semantic relations
from both WordNet and hyperlinks in Wikipedia;
these relations are again an ideal case of valida-
tion, as not all hyperlinks connect semantically-
related pages in Wikipedia. Last, we stress that
while our games use BabelNet data, they could
easily validate or extend other knowledge bases
such as YAGO (Suchanek et al, 2007) as well.
1
This design is in contrast to two-player games where mu-
tual agreement reinforces correct behavior.
2
http://babelnet.org
Data We created a common set of concepts, C,
used in both games, containing sixty synsets se-
lected from all BabelNet synsets with at least fifty
associated images. Using the same set of synsets,
separate datasets were created for the two valida-
tion tasks. In each dataset, a concept c ? C is
associated with two sets: a set V
c
containing items
to validate, and a setN
c
with examples of true neg-
ative items (i.e., items where the relation to c does
not hold). We use the notation V and N when re-
ferring to the to-validate and true negative sets for
all concepts in a dataset, respectively.
For the concept-concept dataset, V
c
is the union
of V
B
c
, which contains the lemmas of all synsets
incident to c in BabelNet, and V
n
c
, which con-
tains novel lemmas derived from statistical asso-
ciations. Specifically, novel lemmas were selected
by computing the ?
2
statistic for co-occurrences
between the lemmas of c and all other part of
speech-tagged lemmas in Wikipedia. The 30 lem-
mas with the highest ?
2
are included in V
c
. To
enable concept-to-concept annotations, we disam-
biguate novel lemmas using a simple heuristic
based on link co-occurrence count (Navigli and
Ponzetto, 2012). Each set V
c
contains 77.6 lem-
mas on average.
For the concept-image data, V
c
is the union of
V
B
c
, which contains all images associated with c in
BabelNet, and V
n
c
, which contains web-gathered
images using a lemma of c as the query. Web-
gathered images were retrieved using Yahoo! Boss
image search and the first result set (35 images)
was added to V
c
. Each set V
c
contains 77.0 images
on average.
For both datasets, each negative set N
c
is con-
structed as ?
c
?
?C\{c}
V
B
c
?
, i.e., from the items re-
lated in BabelNet to all other concepts in C. By
constructingN
c
directly from the knowledge base,
play actions may be validated based on recogni-
tion of true negatives, removing the heavy burden
for ever manually creating a gold standard test set.
Annotation Aggregation In each game, an item
is annotated when players make a binary choice as
to whether the item?s relation is true (e.g., whether
an image is related to a concept). To produce a
final annotation, a rating of p ? n is computed,
where p and n denote the number of times players
have marked the item?s relation as true or false, re-
spectively. Items with a positive rating after aggre-
gating are marked as true examples of the relation
and false otherwise.
1296
(a) The passphrase shown at the start (b) Main gameplay screen with a close-up of a player?s interaction with two humans
Figure 1: Screenshots of the key elements of Infection
4 Game 1: Infection
The first game, Infection, validates the concept-
concept relation dataset.
Design Infection is designed as a top-down
shooter game in the style of Commando. Infection
features the classic game premise that a virus has
partially infected humanity, turning people into
zombies. The player?s responsibility is to stop
zombies from reaching the city and rescue humans
that are fleeing to the city. Both zombies and hu-
mans appear at the top of the screen, advance to
the bottom and, upon reaching it, enter the city.
In the game, some humans are infected, but
have not yet become zombies; these infected hu-
mans must be stopped before reaching the city.
Because infected and uninfected humans look
identical, the player uses a passphrase call-and-
response mechanism to distinguish between the
two. Each level features a randomly-chosen
passphrase that the player?s character shouts. Un-
infected humans are expected to respond with a
word or phrase related to the passphrase; in con-
trast, infected humans have become confused due
to the infection and will say something completely
unrelated in an attempt to sneak past. When an in-
fected human reaches the city, the city?s total in-
fection level increases; should the infection level
increase beyond a certain threshold, the player
fails the stage and must replay it to advance the
game. Furthermore, if any time after ten humans
have been seen, the player has killed more than
80% of the uninfected humans, the player?s gun is
taken by the survivors and she loses the stage.
Figure 1a shows instructions for the passphrase
?medicine.? In the corresponding gameplay,
shown in the close up of Figure 1b, a hu-
man shouts a valid response, ?radiology? for the
level?s passphrase, while the nearby infected hu-
man shouts an incorrect response ?longitude.?
Gameplay is divided into eight stages, each with
increasing difficulty. Each stage has a goal of
saving a specific number of uninfected humans.
Infection incorporates common game mechanics,
such as unlockable weapons, power-ups that re-
store health, and achievements. Scoring is based
on both the number of zombies killed and the per-
centage of uninfected humans saved, motivating
players to kill infected humans in order to increase
their score. Importantly, Infection also includes a
leaderboard where players compete for top posi-
tions based on their total scores.
Annotation Each human is assigned a response
selected uniformly from V or N . Humans with
responses from N are treated as infected. Players
annotate by selecting which humans are infected:
Allowing a human with a response from V to enter
the city is treated as a positive annotation; killing
that human is treated as a negative annotation.
The design of Infection enables annotating mul-
tiple types of conceptual relations such as syn-
onymy or antonymy by changing only the descrip-
tion of the passphrase and how uninfected humans
are expected to respond.
Quality Enforcement Mechanisms Infection in-
cludes two game mechanics to limit adversarial
players from creating many low quality annota-
tions. Specifically, the game prevents players
from both (1) allowing all humans to live, via the
city infection level and (2) killing all humans, via
survivors taking the player?s gun; these actions
would both generate many false positives and false
negatives, respectively. These mechanics ensure
the game naturally produces better quality anno-
tations; in contrast, common crowdsourcing plat-
forms do not support analogous mechanics for en-
forcing this type of correctness at annotation time.
5 Game 2: The Knowledge Towers
The second game, The Knowledge Towers (TKT),
validates the concept-image dataset.
Design TKT is designed as a single-player role
playing game (RPG) where the player explores a
1297
(a) An example tower?s concept (b) Image selection screen (c) Gameplay
Figure 2: Screenshots of the key elements of The Knowledge Towers.
series of towers to unlock long-forgotten knowl-
edge. At the start of each tower, a target con-
cept is shown, e.g., the tower of ?tango,? along
with a description of the concept (Figure 2a). The
player must then recover the knowledge of the tar-
get concept by acquiring pictures of it. Pictures are
obtained through defeating monsters and opening
treasure chests, such as those shown in Figure 2c.
However, players must distinguish pictures of the
tower?s concept from unrelated pictures. When an
image is picked up, the player may keep or discard
it, as shown in Figure 2b. A player?s inventory is
limited to eight pictures to encourage them to se-
lect the most relevant pictures only.
Once the player has collected enough pictures,
the door to the boss room is unlocked and the
player may enter to defeat the boss and complete
the tower. Pictures may also be deposited in spe-
cial reward chests that grant experience bonuses if
the deposited pictures are from V . Gathering un-
related pictures has adverse effects on the player.
If the player finishes the level with a majority of
unrelated pictures, the player?s journey is unsuc-
cessful and she must replay the tower.
TKT includes RPG game elements commonly
found in game series such as Diablo and the Leg-
end of Zelda: players begin with a specific charac-
ter class that has class-specific skills, such as War-
rior or Thief, but will unlock the ability to play as
other classes by successfully completing the tow-
ers. Last, TKT includes a leaderboard where play-
ers can compete for positions; a player?s score is
based on increasing her character?s abilities and
her accuracy at discarding images from N .
Annotation Players annotate by deciding which
images to keep in their inventory. Images receive
positive rating annotations from: (1) depositing
the image in a reward chest, and (2) ending the
level with the image still in the inventory. Con-
versely, images receive a negative rating when a
player (1) views the image but intentionally avoids
picking it up or (2) drops the image from her in-
ventory.
TKT is designed to assist in the validation and
extension of automatically-created image libraries
that link to semantic concepts, such as ImageNet
(Deng et al, 2009) and that of Torralba et al
(2008). However, its general design allows for
other types of annotations, such as image labeling,
by changing the tower?s instructions and pictures.
Quality Enforcement Mechanisms Similar to
Infection, TKT includes analogous mechanisms
for limiting adversarial player annotations. Play-
ers who collect no images are prevented from en-
tering the boss room, limiting their ability to gen-
erate false negative annotations. Similarly, players
who collect all images are likely to have half of
their images from N and therefore fail the tower?s
quality-check after defeating the boss.
6 Experiments
Two experiments were performed with Infection
and TKT: (1) an evaluation of players? ability to
play accurately and to validate semantic relations
and image associations and (2) a comprehensive
cost comparison. Each experiment compared (a)
free and financially-incentivized versions of each
game, (b) crowdsourcing, and (c) a non-video
game with a purpose.
6.1 Experimental Setup
Gold Standard Data To compare the quality of
annotation from games and crowdsourcing, a gold
standard annotation was produced for a 10% sam-
ple of each dataset (cf. Section 3.2). Two annota-
tors independently rated the items and, in cases of
disagreement, a third expert annotator adjudicated.
Unlike in the game setting, annotators were free to
consult additional resources such as Wikipedia.
To measure inter-annotator agreement (IAA) on
the gold standard annotations, we calculated Krip-
1298
pendorff?s ? (Krippendorff, 2004; Artstein and
Poesio, 2008); ? ranges between [-1,1] where 1
indicates complete agreement, -1 indicates sys-
tematic disagreement, and values near 0 indicate
agreement at chance levels. Gold standard an-
notators had high agreement, 0.774, for concept-
concept relations. However, image-concept agree-
ment was only moderate, 0.549. A further analy-
sis revealed differences in the annotators? thresh-
olds for determining association, with one anno-
tator permitting more abstract relations. However,
the adjudication process resolved these disputes,
resulting in substantial agreement by all annota-
tors on the final gold annotations.
Incentives At the start of each game, players were
shown brief descriptions of the game and a de-
scription of a contest where the top-ranked players
would win either (1) monetary prizes in the form
of gift cards, or (2) a mention and thanks in this
paper. We refer to these as the paid and free ver-
sions of the game, respectively. In the paid setting,
the five top-ranking players were offered gift cards
valued at 25, 15, 15, 10, and 10 USD, starting from
first place (a total of 75 USD per game). To in-
crease competition among players and to perform
a fairer time comparison with crowdsourcing, the
contest period was limited to two weeks.
6.2 Comparison Methods
To compare with the video games, items were
annotated using two additional methods: crowd-
sourcing and a non-video game with a purpose.
Crowdsourcing Setup Crowdsourcing was per-
formed using the CrowdFlower platform. Anno-
tation tasks were designed to closely match each
game?s annotation process. A task begins with a
description of a target synset and its textual def-
inition; following, ten annotation questions are
shown. Separate tasks were used for validat-
ing concept-concept and concept-image relations.
Each tasks? questions were shown as a binary
choice of whether the item is related to the task?s
concept. Workers were paid 0.05 USD per task.
Each question was answered by three workers.
Following common practices for guarding
against adversarial workers (Mason and Suri,
2012), the tasks for concept c include quality
check questions using items from N
c
. Workers
who rate too many relations from N
c
as valid are
removed by CrowdFlower and prevented from par-
ticipating further. One of the ten questions in a
task used an item fromN
c
, resulting in a task mix-
ture of 90% annotation questions and 10% quality-
check questions. However, we note that both of
our video games use data that is 50% annotation,
50% quality-check. While the crowdsourcing task
could be adjusted to use an increased number of
quality-check options, such a design is uncommon
and artificially inflates the cost of the crowdsourc-
ing comparison beyond what would be expected.
Therefore, although the crowdsourcing and game-
based annotation tasks differ slightly, we chose to
use the common setup in order to create a fair cost-
comparison between the two.
Non-video Game with a Purpose To measure
the impact of the video game itself on the anno-
tation process, we developed a non-video game
with a purpose, referred to as SuchGame. Players
perform a single action in SuchGame: after be-
ing shown a concept c and its textual definition, a
player answers whether an item is related to the
concept. Items are drawn equally from V
c
and N
c
,
with players scoring a point each time they select
that an item from N is not related. A round of
gameplay contains ten questions. After the round
ends, players see their score for that round and the
current leaderboard. Two versions of SuchGame
were released, one for each dataset. SuchGame
was promoted with same free recognition incen-
tive as Infection and TKT.
6.3 Game Release
Both video games were released to multiple on-
line forums, social media sites, and Facebook
groups. SuchGame was released to separate Face-
book groups promoting free webgames and groups
for indie games. For each release, we estimated
an upper-bound of the audience sizes using avail-
able statistics such as Facebook group sites, web-
site analytics, and view counts. The free and paid
versions had sizes of 21,546 and 14,842 people,
respectively; SuchGame had an upper bound of
569,131 people. Notices promoting the game were
separated so that audiences saw promotions for
one of either the paid or free incentive version.
Games were also released in such a way as to pre-
serve the anonymity of the study, which limited
our ability to advertise to public venues where the
anonymity might be compromised.
7 Results and Discussion
7.1 Gameplay Analysis
In this section we analyze the games in terms of
participation and player?s ability to correctly play.
Players completed over 1388 games during the
1299
 0 50 100
 150 200 250
 300 350 400
 450
Numb
er of I
tems
Player
CorrectIncorrect
(a) Infection (free)
 0 50
 100 150
 200 250
 300 350
 400
Numb
er of I
tems
Player
CorrectIncorrect
(b) Infection (paid)
 0 100
 200 300
 400 500
 600 700
Numb
er of I
tems
Player
CorrectIncorrect
(c) TKT (free)
 0 200
 400 600
 800 1000
 1200 1400
 1600
Numb
er of I
tems
Player
CorrectIncorrect
(d) TKT (paid)
Figure 3: Accuracy of the top-40 players in rejecting true negative items during gameplay.
G.S. Agreement
# Players # Anno. N -Acc. Krip.?s ? True Pos. True Neg. All Cost per Ann.
TKT free 100 3005 97.0 0.333 82.5 82.5 82.5 $0.000
TKT paid 97 3318 95.4 0.304 69.0 92.1 74.0 $0.023
Crowdflower 290 13854 - 0.478 59.5 93.7 66.2 $0.008
Infection free 89 3150 71.0 0.445 67.8 68.4 68.1 $0.000
Infection paid 163 3355 65.9 0.330 69.1 54.8 61.1 $0.022
Crowdflower 1097 13764 - 0.167 16.9 96.4 59.6 $0.008
Table 1: Annotation statistics from all sources. N -Accuracy denotes accuracy at rejecting items fromN ;
G.S. Agreement denotes percentage agreement of the aggregated annotations with the gold standard.
study period. The paid and free versions of TKT
had similar numbers of players, while the paid ver-
sion of Infection attracted nearly twice the play-
ers compared to the free version, shown in Ta-
ble 1, Column 1. However, both versions created
approximately the same number of annotations,
shown in Column 2. Surprisingly, SuchGame re-
ceived little attention, with only a few players
completing a full round of game play. We believe
this emphasizes the strength of video game-based
annotation; adding incentives and game-like fea-
tures to an annotation task will not necessarily in-
crease its appeal. Given SuchGame?s minimal in-
terest, we omit it from further analysis.
Second, the type of incentive did not change the
percentage of items from N that players correctly
reject, shown for all players as N -accuracy in Ta-
ble 1 Column 3 and per-player in Figure 3. How-
ever, players were much more accurate at reject-
ing items from N in TKT than in Infection. We
attribute this difference to the nature of the items
and the format of the games. The images used
by TKT provide concrete examples of a concept,
which can be easily compared with the game?s cur-
rent concept; in addition, TKT allows players to
inspect items as long as a player prefers. In con-
trast, concept-concept associations require more
background knowledge to determine if a relation
exists; furthermore, Infection gives players limited
time to decide (due to board length) and also con-
tains cognitive distractors (zombies). Neverthe-
less, player accuracy remains high for both games
(Table 1, Col. 3) indicating the games represent a
viable medium for making annotation decisions.
Last, the distribution of player annotation fre-
quencies (Figure 3) suggests that the leaderboard
and incentives motivated players. Especially in the
paid condition, a clear group appears in the top
five positions, which were advertised as receiving
prizes. The close proximity of players in the paid
positions is a result of continued competition as
players jostled for higher-paying prizes.
7.2 Annotation Quality
This section assesses the annotation quality of
both games and of CrowdFlower in terms of (1)
the IAA of the participants, measured using Krip-
pendorff?s ?, and (2) the percentage agreement of
the resulting annotations with the gold standard.
Players in both free and paid games had similar
IAA, though the free version is consistently higher
(Table 1, Col. 4).
3
For images, crowdsourcing
workers have a higher IAA than game players;
however, this increased agreement is due to ad-
versarial workers consistently selecting the same,
incorrect answer. In contrast, both video games
contain mechanisms for limiting such behavior.
The strength of both crowdsourcing and games
with a purpose comes from aggregating multiple
annotations of a single item; i.e., while IAA may
3
In conversations with players after the contest ended,
several mentioned that being aware their play was contribut-
ing to research motivated them to play more accurately.
1300
Lemma Abbreviated Definition Most-selected Items
atom
The smallest possible
particle of a chemical
element
spectrum, nonparticulate radiation, molecule, hydrogen, electron
? ? ?
chord
A combination of three
or more notes
voicing, triad, tonality,? strum, note, harmony
?
color
An attribute from re-
flected or emitted light
orange, brown,? video, sadness, RGB, pigment
? ? ? ?
fire
The state of combustion
in which inflammable
material burns
sprinkler, machine gun, chemical reduction, volcano, organic chemistry
? ? ?
religion
The expression of
man?s belief in and
reverence for a super-
human power
polytheistic,? monotheistic, Jainism, Christianity,? Freedom of religion
? ? ?
Table 2: Examples of the most-selected words and images from the free version of both games. Bolded
words and images with a dashed border denote items not in BabelNet. Only the items marked with a ?
were rated as valid in the aggregated CrowdFlower annotations.
be low, the majority annotation of an item may be
correct. Therefore, in Table 1, we calculate the
percentage agreement of the aggregated annota-
tions with the gold standard annotations for ap-
proving valid relations (true positives; Col. 5), re-
jecting invalid relations (true negatives; Col. 6),
and for both combined (Col. 7). On average, both
video games in all settings produce more accurate
annotations than crowdsourcing. Indeed, despite
having lower IAA for images, the free version of
TKT provides an absolute 16.3% improvement in
gold standard agreement over crowdsourcing.
Examining the difference in annotation quality
for true positives and negatives, we see a strong
bias with crowdsourcing towards rejecting all
items. This bias leads to annotations with few false
positives, but as Column 5 shows, crowdflower
workers consistently performed much worse than
game players at identifying valid relations, pro-
ducing many false negative annotations. Indeed,
for concept-concept relations, workers identified
only 16.9% of the valid relations.
In contrast to crowdsourcing, both games were
effective at identifying valid relations. Table
2 shows examples of the most frequently cho-
sen items from V for the free versions of both
games. For both games, players were equally
likely to select novel items, suggesting the games
can serve a useful purpose of adding these miss-
ing relations in automatically constructed knowl-
edge bases. Highlighting one example, the five
most selected concept-concept relations for chord
were all novel; BabelNet included many relations
to highly-specific concepts (e.g., ?Circle of fifths?)
but did not include relations to more commonly-
associated concepts, like note and harmony.
7.3 Cost Analysis
This section provides a cost-comparison between
the video games and crowdsourcing. The free
versions of both games proved highly success-
ful, yielding high-quality annotations at no direct
cost. Both free and paid conditions produced sim-
ilar volumes of annotations, suggesting that play-
ers do not need financial incentives provided that
the games are fun to play. It could be argued that
the recognition incentive was motivating players
in the free condition and thus some incentive was
required. However, player behavior indicates oth-
erwise: After the contest period ended, no players
in the free setting registered for being acknowl-
edged by name, which strongly suggests the in-
centive was not contributing to their motivation for
playing. Furthermore, a minority of players con-
tinued to play even after the contest period ended,
suggesting that enjoyment was a driving factor.
1301
Last, while crowdsourcing has seen different qual-
ity and volume from workers in paid and unpaid
settings (Rogstadius et al, 2011), in contrast, our
games produced approximately-equivalent results
from players in both settings.
Crowdsourcing was slightly more cost-effective
than both games in the paid condition, as shown
in Table 1, Column 8. However, three additional
factors need to be considered. First, both games
intentionally uniformly sample between V and N
to increase player engagement,
4
which generates a
larger number of annotations for items in N than
are produced by crowdsourcing. When annota-
tions on items in N are included for both games
and crowdsourcing, the costs per annotation drop
to comparable levels: $0.007 for CrowdFlower
tasks, $0.008 for TKT, and $0.011 for Infection.
Second, for both annotation tasks, crowdsourc-
ing produced lower quality annotations, especially
for valid relations. Based on agreement with the
gold standard (Table 1, Col. 5), the estimated cost
for crowdsourcing a correct true positive annota-
tion increases to $0.014 for a concept-image and
a $0.048 for concepts-concept annotation. In con-
trast, the cost when using video games increases
only to $0.033 for concept-image and $0.031 for
concept-concept. These cost increases suggest
that crowdsourcing is not always cheaper with re-
spect to quality.
Third, we note that both video games in the paid
setting incur a fixed cost (for the prizes) and there-
fore additional games played can only further de-
crease the cost per annotation. Indeed, the present
study divided the audience pool into two separate
groups which effectively halved the potential num-
ber of annotations per game. Assuming combining
the audiences would produce the same number of
annotations, both our games? costs per annotation
drop to $0.012.
Last, video games can potentially come with
indirect costs due to software development and
maintenance. Indeed, Poesio et al (2013) report
spending 60,000? in developing their Phrase De-
tectives game with a purpose over a two-year pe-
riod. In contrast, both games here were developed
as a part of student projects using open source soft-
ware and assets and thus incurred no cost; fur-
thermore, games were created in a few months,
rather than years. Given that few online games
attain significant sustained interest, we argue that
4
Earlier versions that used mostly items from V proved
less engaging due to players frequently performing the same
action, e.g., saving most humans or collecting most pictures.
our lightweight model is preferable for producing
video games with a purpose. While using students
is not always possible, the development process
is fast enough to sufficiently reduce costs below
those reported for Phrase Detectives.
8 Conclusion
Two video games have been presented for vali-
dating and extending knowledge bases. The first
game, Infection, validates concept-concept rela-
tions, and the second, The Knowledge Towers,
validates image-concept relations. In experiments
involving online players, we demonstrate three
contributions. First, games were released in two
conditions whereby players either saw financial
incentives for playing or a personal satisfaction
incentive where they were thanked by us. We
demonstrated that both conditions produced nearly
identical numbers of annotations and, moreover,
that players were disinterested in the satisfaction
incentive, suggesting they played out of interest
in the game itself. Furthermore, we demonstrated
the effectiveness of a novel design for games with
a purpose which does not require two players for
validation and instead reinforces behavior only
using true negative items that required no man-
ual annotation. Second, in a comparison with
crowdsourcing, we demonstrate that video game-
based annotations consistently generated higher-
quality annotations. Last, we demonstrate that
video game-based annotation can be more cost-
effective than crowdsourcing or annotation tasks
with game-like features: The significant number
of annotations generated by the satisfaction incen-
tive condition shows that a fun game can generate
high-quality annotations at virtually no cost. All
annotated resources, demos of the games, and a
live version of the top-ranking items for each con-
cept are currently available online.
5
In the future we will apply our video games
to the validation of more data, such as the new
Wikipedia bitaxonomy (Flati et al, 2014).
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
We thank Francesco Cecconi for his support
with the websites and the many video game play-
ers without whose enjoyment this work would not
be possible.
5
http://lcl.uniroma1.it/games/
1302
References
Guillaume Artignan, Mountaz Hasco?et, and Math-
ieu Lafourcade. 2009. Multiscale visual analysis
of lexical networks. In Proceedings of the Inter-
national Conference on Information Visualisation,
pages 685?690.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing wordnet. In Proceedings of the 5th Global
WordNet conference.
Alexander Budanitsky and Graeme Hirst. 2006.
Evaluating WordNet-based measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Timothy Chklovski and Yolanda Gil. 2005. Improv-
ing the design of intelligent acquisition interfaces for
collecting world knowledge from web contributors.
In Proceedings of the International Conference on
Knowledge Capture, pages 35?42. ACM.
Tim Chklovski and Rada Mihalcea. 2002. Building a
Sense Tagged Corpus with Open Mind Word Expert.
In Proceedings of ACL 2002 Workshop on WSD: Re-
cent Successes and Future Directions, Philadelphia,
PA, USA.
Seth Cooper, Firas Khatib, Adrien Treuille, Janos
Barbero, Jeehyung Lee, Michael Beenen, Andrew
Leaver-Fay, David Baker, Zoran Popovi?c, and Foldit
players. 2010. Predicting protein structures with a
multiplayer online game. Nature, 466(7307):756?
760.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. ImageNet: A large-scale
hierarchical image database. In Proceedings of the
Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 248?255.
Soojeong Eom, Markus Dickinson, and Graham Katz.
2012. Using semi-experts to derive judgments on
word sense alignment: a pilot study. In Proceed-
ings of the Conference on Language Resources and
Evaluation (LREC), pages 605?611.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and better)
than one: the Wikipedia Bitaxonomy Project. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Bal-
timore, Maryland.
Amac? Herda?gdelen and Marco Baroni. 2012. Boot-
strapping a game with a purpose for common sense
collection. ACM Transactions on Intelligent Sys-
tems and Technology, 3(4):1?24.
Barbora Hladk?a, Ji?r?? M??rovsk`y, and Pavel Schlesinger.
2009. Play the language: Play coreference. In
Proceedings of the Joint Conference of the Asso-
ciation for Computational Linguistics and Inter-
national Joint Conference of the Asian Federation
of Natural Language Processing (ACL-IJCNLP),
pages 209?212. Association for Computational Lin-
guistics.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Conference of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 455?462.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage, Thousand
Oaks, CA, second edition.
Yen-ling Kuo, Jong-Chuan Lee, Kai-yang Chiang, Rex
Wang, Edward Shen, Cheng-wei Chan, and Jane
Yung-jen Hsu. 2009. Community-based game de-
sign: experiments on social games for common-
sense data collection. In Proceedings of the ACM
SIGKDD Workshop on Human Computation, pages
15?22.
Mathieu Lafourcade and Alain Joubert. 2010. Com-
puting trees of named word usages from a crowd-
sourced lexical network. In Proceedings of the In-
ternational Multiconference on Computer Science
and Information Technology (IMCSIT), pages 439?
446, Wisla, Poland.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on amazons mechanical turk.
Behavior Research Methods, 44(1):1?23.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: A Unified Approach. Transactions of
the Association for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual se-
mantic network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Uppsala, Sweden, pages 216?225.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Joining forces pays off: Multilingual Joint Word
Sense Disambiguation. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1399?
1410, Jeju, Korea.
1303
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research
Symposium Conference, Clearwater Beach, Florida,
15?17 May 2005, pages 548?553.
Elisabeth Niemann and Iryna Gurevych. 2011. The
people?s web meets linguistic knowledge: Auto-
matic sense alignment of Wikipedia and WordNet.
In Proceedings of the International Conference on
Computational Semantics (IWCS), pages 205?214.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Semantic
Similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1341?1351, Sofia, Bulgaria.
Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,
Livio Robaldo, and Luca Ducceschi. 2013. Phrase
detectives: Utilizing collective intelligence for
internet-scale language resource creation. ACM
Transactions on Interactive Intelligent Systems,
3(1):3:1?3:44, April.
Jakob Rogstadius, Vassilis Kostakos, Aniket Kittur,
Boris Smus, Jim Laredo, and Maja Vukovic. 2011.
An assessment of intrinsic and extrinsic motivation
on task performance in crowdsourcing markets. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Jacek Rzeniewicz and Julian Szyma?nski. 2013.
Bringing Common Sense to WordNet with a Word
Game. In Computational Collective Intelligence.
Technologies and Applications, volume 8083 of Lec-
ture Notes in Computer Science, pages 296?305.
Springer.
Cristina Sarasua, Elena Simperl, and Natalya F Noy.
2012. CrowdMap: Crowdsourcing ontology align-
ment with microtasks. In Proceedings of the Inter-
national Semantic Web Conference (ISWC), pages
525?541.
Nitin Seemakurty, Jonathan Chu, Luis Von Ahn, and
Anthony Tomasic. 2010. Word sense disambigua-
tion via human computation. In Proceedings of the
ACM SIGKDD Workshop on Human Computation,
pages 60?63. ACM.
Jakub Simko, Michal Tvarozek, and Maria Bielikova.
2011. Little search game: term network acquisition
via a human computation game. In Proceedings of
the ACM conference on Hypertext and Hypermedia,
pages 57?62.
Katharina Siorpaes and Martin Hepp. 2008a. Games
with a purpose for the semantic web. IEEE Intelli-
gent Systems, 23(3):50?60.
Katharina Siorpaes and Martin Hepp. 2008b. On-
togame: Weaving the semantic web by online
games. In Sean Bechhofer, Manfred Hauswirth, Jrg
Hoffmann, and Manolis Koubarakis, editors, The
Semantic Web: Research and Applications, volume
5021 of Lecture Notes in Computer Science, pages
751?766. Springer Berlin Heidelberg.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogeneous
evidence. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL), Sydney, Aus-
tralia, pages 801?808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. unifying WordNet and Wikipedia. In Proceed-
ings of the 16th World Wide Web Conference, Banff,
Canada, 8?12 May 2007, pages 697?706.
Stefan Thaler, Elena Paslaru Bontas Simperl, and
Katharina Siorpaes. 2011. SpotTheLink: A Game
for Ontology Alignment. In Proceedings of the
6th Conference on Professional Knowledge Man-
agement: From Knowledge to Action, pages 246?
253.
Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 30(11):1958?1970.
Noortje J. Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense la-
beling. In Proceedings of the International Confer-
ence on Computational Semantics (IWCS).
David Vickrey, Aaron Bronzan, William Choi, Aman
Kumar, Jason Turner-Maier, Arthur Wang, and
Daphne Koller. 2008. Online word games for se-
mantic data collection. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 533?542.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
Conference on Human Factors in Computing Sys-
tems (CHI), pages 319?326.
Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006.
Verbosity: a game for collecting common-sense
facts. In Proceedings of the Conference on Human
Factors in Computing Systems (CHI), pages 75?78.
1304
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 67?72,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
WoSIT: A Word Sense Induction Toolkit
for Search Result Clustering and Diversification
Daniele Vannella, Tiziano Flati and Roberto Navigli
Dipartimento di Informatica
Sapienza Universit`a di Roma
{vannella,flati,navigli}@di.uniroma1.it
Abstract
In this demonstration we present WoSIT,
an API for Word Sense Induction (WSI)
algorithms. The toolkit provides imple-
mentations of existing graph-based WSI
algorithms, but can also be extended with
new algorithms. The main mission of
WoSIT is to provide a framework for the
extrinsic evaluation of WSI algorithms,
also within end-user applications such as
Web search result clustering and diversifi-
cation.
1 Introduction
The Web is by far the world?s largest information
archive, whose content ? made up of billions of
Web pages ? is growing exponentially. Unfortu-
nately the retrieval of any given piece of infor-
mation is an arduous task which challenges even
prominent search engines such as those developed
by Google, Yahoo! and Microsoft. Even today,
such systems still find themselves up against the
lexical ambiguity issue, that is, the linguistic prop-
erty due to which a single word may convey dif-
ferent meanings.
It has been estimated that around 4% of Web
queries and 16% of the most frequent queries are
ambiguous (Sanderson, 2008). A major issue as-
sociated with the lexical ambiguity phenomenon
on the Web is the low number of query words sub-
mitted by Web users to search engines. A pos-
sible solution to this issue is the diversification of
search results obtained by maximizing the dissimi-
larity of the top-ranking Web pages returned to the
user (Agrawal et al., 2009; Ashwin Swaminathan
and Kirovski, 2009). Another solution consists of
clustering Web search results by way of clustering
engines such as Carrot
1
and Yippy
2
and presenting
them to the user grouped by topic.
1
http://search.carrot2.org
2
http://yippy.com
Diversification and Web clustering algorithms,
however, do not perform any semantic analysis of
search results, clustering them solely on the basis
of their lexical similarity. Recently, it has been
shown that the automatic acquisition of the mean-
ings of a word of interest, a task referred to as
Word Sense Induction, can be successfully inte-
grated into search result clustering and diversifica-
tion (Navigli and Crisafulli, 2010; Di Marco and
Navigli, 2013) so as to outperform non-semantic
state-of-the-art Web clustering systems.
In this demonstration we describe a new toolkit
for Word Sense Induction, called WoSIT, which
i) provides ready implementations of existing
WSI algorithms; ii) can be extended with addi-
tional WSI algorithms; iii) enables the integration
of WSI algorithms into search result clustering
and diversification, thereby providing an extrinsic
evaluation tool. As a result the toolkit enables the
objective comparison of WSI algorithms within an
end-user application in terms of the degree of di-
versification of the search results of a given am-
biguous query.
2 WoSIT
In Figure 1 we show the workflow of the WoSIT
toolkit, composed of three main phases: WSI;
semantically-enhanced search result clustering
and diversification; evaluation. Given a target
query q whose meanings we want to automati-
cally acquire, the toolkit first builds a graph for q,
obtained either from a co-occurrence database, or
constructed programmatically by using any user-
provided input. The co-occurrence graph is then
input to a WSI algorithm, chosen from among
those available in the toolkit or implemented by
the user. As a result, a set of word clusters
is produced. This concludes the first phase of
the WoSIT workflow. Then, the word clusters
produced are used for assigning meanings to the
search results returned by a search engine for the
query q, i.e. search result disambiguation. The
67
  
+ MANUAL ANNOTATIONS
DBDB
Search resultdisambiguation
w1w3w2w4w5w6 s4s5s1 s3
s2
Eval ResultsWord ClustersWSIAlgorithm WSI EvaluatorAssignment of results to clustersCo-occurrence graph
Co-occurrence Information Web search engine
WSI Semantically EnhancedSearch Result Clustering
Dataset
Evaluation
Figure 1: The WoSIT workflow.
outcome is that we obtain a clustering of search
results. Finally, during the third phase, we apply
the evaluation module which performs an evalua-
tion of the search result clustering quality and the
diversification performance.
We now describe in detail the three main phases
of WoSIT.
2.1 Word Sense Induction
The first phase of WoSIT consists of the automatic
identification of the senses of a query of inter-
est, i.e. the task of Word Sense Induction. Al-
though WoSIT enables the integration of custom
implementations which can potentially work with
any WSI paradigm, the toolkit provides ready-to-
use implementations of several graph-based algo-
rithms that work with word co-occurrences. All
these algorithms carry out WSI in two steps: co-
occurrence graph construction (Section 2.1.1) and
discovery of word senses (Section 2.1.2).
2.1.1 Co-occurrence graph construction
Given a target query q, we build a co-occurrence
graph G
q
= (V,E) such that V is the set of
words co-occurring with q and E is the set of undi-
rected edges, each denoting a co-occurrence be-
tween pairs of words in V . In Figure 2 we show
an example of a co-occurrence graph for the target
word excalibur.
WoSIT enables the creation of the co-
occurrence graph either programmatically, by
adding edges and vertices according to any user-
specific algorithm, or starting from the statis-
tics for co-occurring words obtained from a co-
occurrence database (created, e.g., from a text cor-
pus, as was done by Di Marco and Navigli (2013)).
In either case, weights for edges have to be pro-
vided in terms of the correlation strength between
pairs of words (e.g. using Dice, Jaccard or other
co-occurrence measures).
The information about the co-occurrence
database, e.g. a MySQL database, is provided
programmatically or via parameters in the prop-
erties configuration file (db.properties).
The co-occurrence database has to follow a
given schema provided in the toolkit docu-
mentation. An additional configuration file
(wosit.properties) also allows the user
to specify additional constraints, e.g. the
minimum weight value of co-occurrence (the
wordGraph.minWeight parameter) to be
added as edges to the graph.
The graphs produced can also be saved to binary
(i.e. serialized) or text file:
g.saveToSer(fileName);
g = WordGraph.loadFromSer(fileName);
g.saveToTxt(fileName);
g = WordGraph.loadFromTxt(fileName);
We are now ready to provide our co-occurrence
graph, created with just a few lines of code, as in-
put to a WSI algorithm, as will be explained in the
next section.
2.1.2 Discovery of Word Senses
Once the co-occurrence graph for the query q is
built, it can be input to any WSI algorithm which
extends the GraphClusteringAlgorithm
class in the toolkit. WoSIT comes with a number
of ready-to-use such algorithms, among which:
68
Car
Limousine
King Arthur
Excalibur
Film
Fantasy
Book
0.02
0.015
0.025
0.005
0.04
0.006
0.007
0.01
0.013
0.012
0.02
Figure 2: Example of a co-occurrence graph for
the word excalibur.
? Balanced Maximum Spanning Tree (B-
MST) (Di Marco and Navigli, 2013), an ex-
tension of a WSI algorithm based on the
calculation of a Maximum Spanning Tree
(Di Marco and Navigli, 2011) aimed at bal-
ancing the number of co-occurrences in each
sense cluster.
? HyperLex (V?eronis, 2004), an algorithm
which identifies hubs in co-occurrence
graphs, thereby identifying basic meanings
for the input query.
? Chinese Whispers (Biemann, 2006), a ran-
domized algorithm which partitions nodes by
means of the iterative transfer of word sense
information across the co-occurrence graph
(Biemann, 2006).
? Squares, Triangles and Diamonds
(SquaT++) (Di Marco and Navigli, 2013),
an extension of the SquaT algorithm (Navigli
and Crisafulli, 2010) which exploits three
cyclic graph patterns to determine and
discard those vertices (or edges) with weak
degree of connectivity in the graph.
We also provide an implementation of a word
clustering algorithm, i.e. Lin98 (Lin, 1998),
which does not rely on co-occurrence graphs, but
just on the word co-occurrence information to it-
eratively refine word clusters on the basis of their
?semantic? relationships.
A programmatic example of use of the B-MST
WSI algorithm is as follows:
BMST mst = new BMST(g);
mst.makeClustering();
Clustering wordClusters =
mst.getClustering();
where g is a co-occurrence graph created as ex-
plained in Section 2.1.1, provided as input to
the constructor of the algorithm?s class. The
makeClustering method implements the in-
duction algorithm and creates the word clus-
ters, which can then be retrieved calling the
getClustering method. As a result an in-
stance of the Clustering class is provided.
As mentioned above, WoSIT also enables
the creation of custom WSI implementa-
tions. This can be done by extending the
GraphClusteringAlgorihm abstract class.
The new algorithm just has to implement two
methods:
public void makeClustering();
public Clustering getClustering();
As a result, the new algorithm is readily inte-
grated into the WoSIT toolkit.
2.2 Semantically-enhanced Search Result
Clustering and Diversification
We now move to the use of the induced senses of
our target query q within an application, i.e. search
result clustering and diversification.
Search result clustering. The next step (cf. Fig-
ure 1) is the association of the search results re-
turned by a search engine for query q with the most
suitable word cluster (i.e. meaning of q). This can
be done in two lines:
SnippetAssociator associator =
SnippetAssociator.getInstance();
SnippetClustering clustering =
associator.associateSnippet(
targetWord,
searchResults,
wordClusters,
AssociationMetric.DEGREE_OVERLAP);
The first line obtains an instance of the class
which performs the association between search re-
sult snippets and the word clusters obtained from
the WSI algorithm. The second line calls the asso-
ciation method associateSnippet which in-
puts the target word, the search results obtained
from the search engine, the word clusters and, fi-
nally, the kind of metric to use for the associa-
tion. Three different association metrics are im-
plemented in the toolkit:
? WORD OVERLAP performs the association by
maximizing the size of the intersection be-
tween the word sets in each snippet and the
word clusters;
? DEGREE OVERLAP performs the association
by calculating for each word cluster the sum
69
of the vertex degrees in the co-occurrence
graph of the words occurring in each snippet;
? TOKEN OVERLAP is similar in spirit to
WORD OVERLAP, but takes into account each
token occurrence in the snippet bag of words.
Search result diversification. The above two
lines of code return a set of snippet clusters and, as
a result, semantically-enhanced search result clus-
tering is performed. At the end, the resulting clus-
tering can be used to provide a diversified rerank-
ing of the results:
List<Snippet> snippets =
clustering.diversify(sorter);
The diversify method returns a flat list of
snippet results obtained according to the Sorter
object provided in input. The Sorter abstract
class is designed to rerank the snippet clusters ac-
cording to some predefined rule. For instance, the
CardinalitySorter class, included in the
toolkit, sorts the clusters according to the size of
each cluster. Once a sorting order has been es-
tablished, an element from each snippet cluster is
added to an initially-empty list; next, a second el-
ement from each cluster is added, and so on, until
all snippets are added to the list.
The sorting rules implemented in the toolkit are:
? CardinalitySorter: sorts the clusters
according to their size, i.e. the number of ver-
tices in the cluster;
? MeanSimilaritySorter: sorts the clus-
ters according to the average association
score between the snippets in the cluster and
the backing word cluster (defined by the se-
lected association metrics).
Notably, the end user can then implement his or
her own custom sorting procedure by simply ex-
tending the Sorter class.
2.2.1 Search Result Datasets
The framework comes with two search result
datasets of ambiguous queries: the AMBI-
ENT+MORESQUE dataset made available by
Bernardini et al. (2009) and Navigli and Crisa-
fulli (2010), respectively, and the SemEval-2013-
Task11 dataset.
3
New result datasets can be pro-
vided by users complying with the dataset format
described below.
3
For details visit http://lcl.uniroma1.it/
wosit/.
A search result dataset in WoSIT is made up of
at least two files:
? topics.txt, which contains the queries
(topics) of interest together with their nu-
meric ids. For instance:
id description
1 polaroid
2 kangaroo
3 shakira
... ...
? results.txt, which lists the search re-
sults for each given query, in terms of URL,
page title and page snippet:
ID url title snippet
1.1 http://www.polaroid.com/ Polaroid | Home ...
1.2 http://www.polaroid.com/products products...
1.3 http://en.wikipedia.org/wiki/Polaroid_Cor...
... ...
Therefore, the two files provide the queries and the
corresponding search results returned by a search
engine. In order to enable an automatic evaluation
of the search result clustering and diversification
output, two additional files have to be provided:
? subTopics.txt, which for each query
provides the list of meanings for that query,
e.g.:
ID description
1.1 Polaroid Corporation, a multinational con...
1.2 Instant film photographs are sometimes kn...
1.3 Instant camera (or Land camera), sometime...
... ...
? STRel.txt, which provides the manual as-
sociations between each search result and the
most suitable meaning as provided in the
subTopics.txt file. For instance:
subTopicID resultID
1.1 1.1
1.1 1.2
1.1 1.3
... ...
2.3 WSI Evaluator
As shown in Figure 1 the final component of our
workflow is the evaluation of WSI when integrated
into search result clustering and diversification (al-
ready used by Navigli and Vannella (2013)). This
component, called the WSI Evaluator, takes as
input the snippet clusters obtained for a given
query together with the fully annotated search re-
sult dataset, as described in the previous section.
Two kinds of evaluations are carried out, described
in what follows.
70
1 Dataset searchResults = Dataset.getInstance();
2 DBConfiguration db = DBConfiguration.getInstance();
3 for(String targetWord : dataset.getQueries())
4 {
5 WordGraph g = WordGraph.createWordGraph(targetWord, searchResults, db);
6 BMST mst = new BMST(g);
7 mst.makeClustering();
8 SnippetAssociator snippetAssociator = SnippetAssociator.getInstance();
9 SnippetClustering snippetClustering = snippetAssociator.associateSnippet(
10 targetWord, searchResults, mst.getClustering(), AssociationMetric.WORD_OVERLAP);
11 snippetClustering.export("output/outputMST.txt", true);
12 }
13 WSIEvaluator.evaluate(searchResults, "output/outputMST.txt");
Figure 3: An example of evaluation code for the B-MST clustering algorithm.
2.3.1 Evaluation of the clustering quality
The quality of the output produced by
semantically-enhanced search result cluster-
ing is evaluated in terms of Rand Index (Rand,
1971, RI), Adjusted Rand Index (Hubert and
Arabie, 1985, ARI), Jaccard Index (JI) and,
finally, precision and recall as done by Crabtree et
al. (2005), together with their F1 harmonic mean.
2.3.2 Evaluation of the clustering diversity
To evaluate the snippet clustering diversity the
measures of S-recall@K and S-precision@r (Zhai
et al., 2003) are calculated. These measures de-
termine how many different meanings of a query
are covered in the top-ranking results shown to the
user. We calculate these measures on the output of
the three different association metrics illustrated in
Section 2.2.
3 A Full Example
We now show a full example of usage of the
WoSIT API. The code shown in Figure 3 initially
obtains a search result dataset (line 1), selects a
database (line 2) and iterates over its queries (line
3). Next, a co-occurrence graph for the current
query is created from a co-occurrence database
(line 5) and an instance of the B-MST WSI algo-
rithm is created with the graph as input (line 6).
After executing the algorithm (line 7), the snippets
for the given query are clustered (lines 8-10). The
resulting snippet clustering is appended to an out-
put file (line 11). Finally, the WSI evaluator is run
on the resulting snippet clustering using the given
dataset (line 13).
3.1 Experiments
We applied the WoSIT API to the AMBI-
ENT+MORESQUE dataset using 4 induction al-
Algorithm
Assoc. Web1T
metr. ARI JI F1 # cl.
SquaT++
WO 69.65 75.69 59.19 2.1
DO 69.21 75.45 59.19 2.1
TO 69.67 75.69 59.19 2.1
B-MST
WO 60.76 71.51 64.56 5.0
DO 66.48 69.37 64.84 5.0
TO 63.17 71.21 64.04 5.0
HyperLex
WO 60.86 72.05 65.41 13.0
DO 66.27 68.00 71.91 13.0
TO 62.82 70.87 65.08 13.0
Chinese Whispers
WO 67.75 75.37 60.25 12.5
DO 65.95 69.49 70.33 12.5
TO 67.57 74.69 60.50 12.5
Table 1: Results of WSI algorithms with a Web1T
co-occurrence database and the three association
metrics (Word Overlap, Degree Overlap and To-
ken Overlap). The reported measures are Ad-
justed Rand Index (ARI), Jaccard Index (JI) and
F1. We also show the average number of clusters
per query produced by each algorithm.
gorithms among those available in the toolkit,
where co-occurrences were obtained from the
Google Web1T corpus (Brants and Franz, 2006).
In Table 1 we show the clustering quality results
output by the WoSIT evaluator, whereas in Fig-
ure 4 we show the diversification performance in
terms of S-recall@K.
3.2 Conclusions
In this demonstration we presented WoSIT, a full-
fledged toolkit for Word Sense Induction algo-
rithms and their integration into search result clus-
tering and diversification. The main contributions
are as follows: first, we release a Java API for
performing Word Sense Induction which includes
several ready-to-use implementations of existing
algorithms; second, the API enables the use of the
acquired senses for a given query for enhancing
71
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0 20.0
S-reca
ll-at-K
K
HyperLexBMSTChineseWSquaT++
Figure 4: S-recall@K performance.
search result clustering and diversification; third,
we provide an evaluation component which, given
an annotated dataset of search results, carries out
different kinds of evaluation of the snippet cluster-
ing quality and diversity.
WoSIT is the first available toolkit which pro-
vides an end-to-end approach to the integration of
WSI into a real-world application. The toolkit en-
ables an objective comparison of WSI algorithms
as well as an evaluation of the impact of apply-
ing WSI to clustering and diversifying search re-
sults. As shown by Di Marco and Navigli (2013),
this integration is beneficial and allows outperfor-
mance of non-semantic state-of-the-art Web clus-
tering systems.
The toolkit, licensed under a Creative Com-
mons Attribution-Non Commercial-Share Alike
3.0 License, is available at http://lcl.
uniroma1.it/wosit/.
References
Rakesh Agrawal, Sreenivas Gollapudi, Alan Halver-
son, and Samuel Ieong. 2009. Diversifying search
results. In Proc. of the Second International Confer-
ence on Web Search and Web Data Mining (WSDM
2009), pages 5?14, Barcelona, Spain.
Cherian V. Mathew Ashwin Swaminathan and Darko
Kirovski. 2009. Essential Pages. In Proc. of the
2009 IEEE/WIC/ACM International Joint Confer-
ence on Web Intelligence and Intelligent Agent Tech-
nology, volume 1, pages 173?182.
Andrea Bernardini, Claudio Carpineto, and Massim-
iliano D?Amico. 2009. Full-Subtopic Retrieval
with Keyphrase-Based Search Results Clustering.
In Proc. of Web Intelligence 2009, volume 1, pages
206?213, Los Alamitos, CA, USA.
Chris Biemann. 2006. Chinese Whispers - an Effi-
cient Graph Clustering Algorithm and its Applica-
tion to Natural Language Processing Problems. In
Proc. of TextGraphs: the First Workshop on Graph
Based Methods for Natural Language Processing,
pages 73?80, New York City.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram, ver. 1, LDC2006T13. In Linguistic Data Con-
sortium, Philadelphia, USA.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selec-
tion. In Proc. of the 2005 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 172?
178, Washington, DC, USA.
Antonio Di Marco and Roberto Navigli. 2011. Clus-
tering Web Search Results with Maximum Spanning
Trees. In Proc. of the XIIth International Confer-
ence of the Italian Association for Artificial Intelli-
gence (AI*IA), pages 201?212, Palermo, Italy.
Antonio Di Marco and Roberto Navigli. 2013. Clus-
tering and Diversifying Web Search Results with
Graph-Based Word Sense Induction. Computa-
tional Linguistics, 39(3):709?754.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing Partitions. Journal of Classification, 2(1):193?
218.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proc. of the 17
th
Inter-
national Conference on Computational linguistics
(COLING), pages 768?774, Montreal, Canada.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing Word Senses to Improve Web Search Result
Clustering. In Proc. of the 2010 Conference on Em-
pirical Methods in Natural Language Processing,
pages 116?126, Boston, USA.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 Task 11: Evaluating Word Sense In-
duction & Disambiguation within An End-User Ap-
plication. In Proc. of the 7
th
International Work-
shop on Semantic Evaluation (SemEval 2013), in
conjunction with the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 193?201, Atlanta, USA.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical association, 66(336):846?850.
Mark Sanderson. 2008. Ambiguous queries: test col-
lections need more sense. In Proc. of the 31st an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 499?506, Singapore.
Jean V?eronis. 2004. HyperLex: lexical cartography
for information retrieval. Computer, Speech and
Language, 18(3):223?252.
ChengXiang Zhai, William W. Cohen, and John Laf-
ferty. 2003. Beyond independent relevance: Meth-
ods and evaluation metrics for subtopic retrieval. In
Proc. of the 26th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 10?17, Toronto, Canada.
72
Entity Linking meets Word Sense Disambiguation: a Unified Approach
Andrea Moro, Alessandro Raganato, Roberto Navigli
Dipartimento di Informatica,
Sapienza Universita` di Roma,
Viale Regina Elena 295, 00161 Roma, Italy
{moro,navigli}@di.uniroma1.it
ale.raganato@gmail.com
Abstract
Entity Linking (EL) and Word Sense Disam-
biguation (WSD) both address the lexical am-
biguity of language. But while the two tasks
are pretty similar, they differ in a fundamen-
tal respect: in EL the textual mention can be
linked to a named entity which may or may not
contain the exact mention, while in WSD there
is a perfect match between the word form (bet-
ter, its lemma) and a suitable word sense.
In this paper we present Babelfy, a unified
graph-based approach to EL and WSD based
on a loose identification of candidate mean-
ings coupled with a densest subgraph heuris-
tic which selects high-coherence semantic in-
terpretations. Our experiments show state-of-
the-art performances on both tasks on 6 differ-
ent datasets, including a multilingual setting.
Babelfy is online at http://babelfy.org
1 Introduction
The automatic understanding of the meaning of text
has been a major goal of research in computational
linguistics and related areas for several decades,
with ambitious challenges, such as Machine Read-
ing (Etzioni et al., 2006) and the quest for knowl-
edge (Schubert, 2006). Word Sense Disambiguation
(WSD) (Navigli, 2009; Navigli, 2012) is a historical
task aimed at assigning meanings to single-word and
multi-word occurrences within text, a task which is
more alive than ever in the research community.
Recently, the collaborative creation of large semi-
structured resources, such as Wikipedia, and knowl-
edge resources built from them (Hovy et al., 2013),
such as BabelNet (Navigli and Ponzetto, 2012a),
DBpedia (Auer et al., 2007) and YAGO2 (Hoffart
et al., 2013), has favoured the emergence of new
tasks, such as Entity Linking (EL) (Rao et al., 2013),
and opened up new possibilities for tasks such as
Named Entity Disambiguation (NED) and Wikifi-
cation. The aim of EL is to discover mentions of
entities within a text and to link them to the most
suitable entry in a reference knowledge base. How-
ever, in contrast to WSD, a mention may be partial
while still being unambiguous thanks to the context.
For instance, consider the following sentence:
(1) Thomas and Mario are strikers playing in Munich.
This example makes it clear how intertwined the
two tasks of WSD and EL are. In fact, on the one
hand, striker and play are polysemous words which
can be disambiguated by selecting the game/soccer
playing senses of the two words in a dictionary; on
the other hand, Thomas and Mario are partial men-
tions which have to be linked to the appropriate en-
tries of a knowledge base, that is, Thomas Mu?ller
and Mario Gomez, two well-known soccer players.
The two main differences between WSD and EL
lie, on the one hand, in the kind of inventory used,
i.e., dictionary vs. encyclopedia, and, on the other
hand, in the assumption that the mention is complete
or potentially partial. Notwithstanding these differ-
ences, the tasks are similar in nature, in that they
both involve the disambiguation of textual fragments
according to a reference inventory. However, the re-
search community has so far tackled the two tasks
separately, often duplicating efforts and solutions.
In contrast to this trend, research in knowledge
acquisition is now heading towards the seamless in-
231
Transactions of the Association for Computational Linguistics, 2 (2014) 231?244. Action Editor: Noah Smith.
Submitted 9/2013; Revised 1/2014; Published 5/2014. c?2014 Association for Computational Linguistics.
tegration of encyclopedic and lexicographic knowl-
edge into structured language resources (Hovy et al.,
2013), and the main representative of this new direc-
tion is undoubtedly BabelNet (Navigli and Ponzetto,
2012a). Given such structured language resources it
seems natural to suppose that they might provide a
common ground for the two tasks of WSD and EL.
More precisely, in this paper we explore the hy-
pothesis that the lexicographic knowledge used in
WSD is also useful for tackling the EL task, and,
vice versa, that the encyclopedic information uti-
lized in EL helps disambiguate nominal mentions in
a WSD setting. We propose Babelfy, a novel, uni-
fied graph-based approach to WSD and EL, which
performs two main steps: i) it exploits random walks
with restart, and triangles as a support for reweight-
ing the edges of a large semantic network; ii) it uses
a densest subgraph heuristic on the available seman-
tic interpretations of the input text to perform a joint
disambiguation with both concepts and named enti-
ties. Our experiments show the benefits of our syn-
ergistic approach on six gold-standard datasets.
2 Related Work
2.1 Word Sense Disambiguation
Word Sense Disambiguation (WSD) is the task of
choosing the right sense for a word within a given
context. Typical approaches for this task can be clas-
sified as i) supervised, ii) knowledge-based, and iii)
unsupervised. However, supervised approaches re-
quire huge amounts of annotated data (Zhong and
Ng, 2010; Shen et al., 2013; Pilehvar and Navigli,
2014), an effort which cannot easily be repeated
for new domains and languages, while unsupervised
ones suffer from data sparsity and an intrinsic diffi-
culty in their evaluation (Agirre et al., 2006; Brody
and Lapata, 2009; Manandhar et al., 2010; Van de
Cruys and Apidianaki, 2011; Di Marco and Nav-
igli, 2013). On the other hand, knowledge-based
approaches are able to obtain good performance us-
ing readily-available structured knowledge (Agirre
et al., 2010; Guo and Diab, 2010; Ponzetto and Nav-
igli, 2010; Miller et al., 2012; Agirre et al., 2014).
Some of these approaches marginally take into ac-
count the structural properties of the knowledge base
(Mihalcea, 2005). Other approaches, instead, lever-
age the structural properties of the knowledge base
by exploiting centrality and connectivity measures
(Sinha and Mihalcea, 2007; Tsatsaronis et al., 2007;
Agirre and Soroa, 2009; Navigli and Lapata, 2010).
One of the key steps of many knowledge-based
WSD algorithms is the creation of a graph repre-
senting the semantic interpretations of the input text.
Two main strategies to build this graph have been
proposed: i) exploiting the direct connections, i.e.,
edges, between the considered sense candidates; ii)
populating the graph according to (shortest) paths
between them. In our approach we manage to unify
these two strategies by automatically creating edges
between sense candidates performing Random Walk
with Restart (Tong et al., 2006).
The recent upsurge of interest in multilinguality
has led to the development of cross-lingual and mul-
tilingual approaches to WSD (Lefever and Hoste,
2010; Lefever and Hoste, 2013; Navigli et al., 2013).
Multilinguality has been exploited in different ways,
e.g., by using parallel corpora to build multilingual
contexts (Guo and Diab, 2010; Banea and Mihalcea,
2011; Lefever et al., 2011) or by means of ensemble
methods which exploit complementary sense evi-
dence from translations in different languages (Nav-
igli and Ponzetto, 2012b). In this work, we present
a novel exploitation of the structural properties of a
multilingual semantic network.
2.2 Entity Linking
Entity Linking (Erbs et al., 2011; Rao et al., 2013;
Cornolti et al., 2013) encompasses a set of similar
tasks, which include Named Entity Disambiguation
(NED), that is the task of linking entity mentions
in a text to a knowledge base (Bunescu and Pasca,
2006; Cucerzan, 2007), and Wikification, i.e., the
automatic annotation of text by linking its relevant
fragments of text to the appropriate Wikipedia arti-
cles. Mihalcea and Csomai (2007) were the first to
tackle the Wikification task. In their approach they
disambiguate each word in a sentence independently
by exploiting the context in which it occurs. How-
ever, this approach is local in that it lacks a collective
notion of coherence between the selected Wikipedia
pages. To overcome this problem, Cucerzan (2007)
introduced a global approach based on the simulta-
neous disambiguation of all the terms in a text and
the use of lexical context to disambiguate the men-
tions. To maximize the semantic agreement Milne
232
and Witten (2008) introduced the analysis of the se-
mantic relations between the candidate senses and
the unambiguous context, i.e., words with a single
sense candidate. However, the performance of this
algorithm depends heavily on the number of links
incident to the target senses and on the availabil-
ity of unambiguous words within the input text. To
overcome this issue a novel class of approaches have
been proposed (Kulkarni et al., 2009; Ratinov et al.,
2011; Hoffart et al., 2011) that exploit global and
local features. However, these systems either rely
on a difficult NP-hard formalization of the problem
which is infeasible for long text, or exploit popular-
ity measures which are domain-dependent. In con-
trast, we show that the semantic network structure
can be leveraged to obtain state-of-the-art perfor-
mance by synergistically disambiguating both word
senses and named entities at the same time.
Recently, the explosion of on-line social network-
ing services, such as Twitter and Facebook, have
contributed to the development of new methods for
the efficient disambiguation of short texts (Ferrag-
ina and Scaiella, 2010; Hoffart et al., 2012; Bo?hm et
al., 2012). Thanks to a loose candidate identification
technique coupled with a densest subgraph heuristic,
we show that our approach is particularly suited for
short and highly ambiguous text disambiguation.
2.3 The Best of Two Worlds
Our main goal is to bring together the two worlds of
WSD and EL. On the one hand, this implies relaxing
the constraint of a perfect association between men-
tions and meanings, which is, instead, assumed in
WSD. On the other hand, this relaxation leads to the
inherent difficulty of encoding a full-fledged sense
inventory for EL. Our solution to this problem is to
keep the set of candidate meanings for a given men-
tion as open as possible (see Section 6), so as to en-
able high recall in linking partial mentions, while
providing an effective method for handling this high
ambiguity (see Section 7).
A key assumption of our work is that the lexico-
graphic knowledge used in WSD is also useful for
tackling the EL task, and vice versa the encyclopedic
information utilized in EL helps disambiguate nom-
inal mentions in a WSD setting. We enable the joint
treatment of concepts and named entities by enforc-
ing high coherence in our semantic interpretations.
3 WSD and Entity Linking Together
Task. Our task is to disambiguate and link all
nominal and named entity mentions occurring
within a text. The linking task is performed by asso-
ciating each mention with the most suitable entry of
a given knowledge base.1
We point out that our definition is unconstrained
in terms of what to link, i.e., unlike Wikification and
WSD, we can link overlapping fragments of text.
For instance, given the text fragment Major League
Soccer, we identify and disambiguate several dif-
ferent nominal and entity mentions: Major League
Soccer, major league, league and soccer. In contrast
to EL, we link not only named entity mentions, such
as Major League Soccer, but also nominal mentions,
e.g., major league, to their corresponding meanings
in the knowledge base.
Babelfy. We provide a unified approach to WSD
and entity linking in three steps:
1. Given a lexicalized semantic network, we as-
sociate with each vertex, i.e., either concept or
named entity, a semantic signature, that is, a set
of related vertices (Section 5). This is a prelim-
inary step which needs to be performed only
once, independently of the input text.
2. Given a text, we extract all the linkable frag-
ments from this text and, for each of them, list
the possible meanings according to the seman-
tic network (Section 6).
3. We create a graph-based semantic interpreta-
tion of the whole text by linking the candidate
meanings of the extracted fragments using the
previously-computed semantic signatures. We
then extract a dense subgraph of this represen-
tation and select the best candidate meaning for
each fragment (Section 7).
4 Semantic Network
Our approach requires the availability of a wide-
coverage semantic network which encodes struc-
tural and lexical information both of an encyclope-
dic and of a lexicographic kind. Although in prin-
ciple any semantic network with these properties
1Mentions which are not contained in the reference knowl-
edge base are not taken into account.
233
could be utilized, in our work we used the Babel-
Net2 1.1.1 semantic network (Navigli and Ponzetto,
2012a) since it is the largest multilingual knowl-
edge base, obtained from the automatic seamless
integration of Wikipedia3 and WordNet (Fellbaum,
1998). We consider BabelNet as a directed multi-
graph which contains both concepts and named en-
tities as its vertices and a multiset of semantic rela-
tions as its edges. We leverage the multilingual lex-
icalizations of the vertices of BabelNet to identify
mentions in the input text. For example, the entity
FC Bayern Munich can be lexicalized in different
languages, e.g., F.C. Bayern de Mu?nich in Spanish,
Die Roten in English and Bayern Mu?nchen in Ger-
man, among others. As regards semantic relations,
the only information we use is that of the end points,
i.e., vertices, that these relations connect, while ne-
glecting the relation type.
5 Building Semantic Signatures
One of the major issues affecting both manually-
curated and automatically constructed semantic net-
works is data sparsity. For instance, we calculated
that the average number of incident edges is roughly
10 in WordNet, 50 in BabelNet and 80 in YAGO2,
to mention a few. Although automatically-built re-
sources typically provide larger amounts of edges,
two issues have to be taken into account: concepts
which should be related might not be directly con-
nected despite being structurally close within the
network, and, vice versa, weakly-related or even un-
related concepts can be erroneously connected by an
edge. For instance, in BabelNet we do not have an
edge between playmaker and Thomas Mu?ller, while
we have an incorrect edge connecting FC Bayern
Munich and Yellow Submarine (song). However,
this crisp notion of relatedness can be overcome by
exploiting the global structure of the semantic net-
work, thereby obtaining a more precise and higher-
coverage measure of relatedness. We address this
issue in two steps: first, we provide a structural
weighting of the network?s edges; second, for each
vertex we create a set of related vertices using ran-
dom walks with restart.
2http://babelnet.org
3http://www.wikipedia.org
Structural weighting. Our first objective is to as-
sign higher weights to edges which are involved in
more densely connected areas of the directed net-
work. To this end, inspired by the local cluster-
ing coefficient measure (Watts and Strogatz, 1998)
and its recent success in Word Sense Induction
(Di Marco and Navigli, 2013), we use directed tri-
angles, i.e., directed cycles of length 3, and weight
each edge (v, v?) by the number of directed triangles
it occurs in:
weight(v, v?) := |{(v, v?, v??) : (v, v?), (1)
(v?, v??), (v??, v) ? E}|+ 1
We add one to each weight to ensure the highest de-
gree of reachability in the network.
Random Walk with Restart. Our goal is to cre-
ate a semantic signature (i.e., a set of highly related
vertices) for each concept and named entity of the
semantic network. To do this, we perform a Random
Walk with Restart (RWR) (Tong et al., 2006), that is,
a stochastic process that starts from an initial vertex
of the graph4 and then, for a fixed number n of steps
or until convergence, explores the graph by choos-
ing the next vertex within the current neighborhood
or by restarting from the initial vertex with a given,
fixed restart probability ?. For each edge (v, v?) in
the network, we model the conditional probability
P (v?|v) as the normalized weight of the edge:
P (v?|v) = weight(v, v
?)?
v???V weight(v, v??)
where V is the set of vertices of the semantic net-
work and weight(v, v?) is the function defined in
Equation 1. We then run the RWR from each ver-
tex v of the semantic network for a fixed number n
of steps (we show in Algorithm 1 our RWR pseu-
docode). We keep track of the encountered ver-
tices using the map counts, i.e., we increase the
counter associated with vertex v? in counts every
time we hit v? during a RWR started from v (see
line 11). As a result, we obtain a frequency distri-
bution over the whole set of concepts and entities.
To eliminate weakly-related vertices we keep only
those items that were hit at least ? times (see lines
16?18). Finally, we save the remaining vertices in
the set semSignv which is the semantic signature
of v (see line 19).
4RWR can be used with an initial set of vertices, however in
this paper we use a single initial vertex.
234
Algorithm 1 Random walk with restart.
1: input: v, the starting vertex;
?, the restart probability;
n, the number of steps to be executed;
P , the transition probabilities;
?, the frequency threshold.
2: output: semSignv , set of related vertices for v.
3: function RWR(v, ?, n, P, ?)
4: v? := v
5: counts := newMap < Synset, Integer >
6: while n > 0 do
7: if random() > ? then
8: given the transition probabilities P (?|v?)
9: of v?, choose a random neighbor v??
10: v? := v??
11: counts[v?] + +
12: else
13: restart the walk
14: v? := v
15: n := n? 1
16: for each v? in counts.keys() do
17: if counts[v?] < ? then
18: remove v? from counts.keys()
19: return semSignv = counts.keys()
The creation of our set of semantic signatures, one
for each vertex in the semantic network, is a prelim-
inary step carried out once only before starting pro-
cessing any input text. We now turn to the candidate
identification and disambiguation steps.
6 Candidate Identification
Given a text as input, we apply part-of-speech tag-
ging and identify the set F of all the textual frag-
ments, i.e., all the sequences of words of maximum
length five, which contain at least one noun and that
are substrings of lexicalizations in BabelNet, i.e.,
those fragments that can potentially be linked to an
entry in BabelNet. For each textual fragment f ? F ,
i.e., a single- or multi-word expression of the input
text, we look up the semantic network for candidate
meanings, i.e., vertices that contain f or, only for
named entities, a superstring of f as their lexical-
ization. For instance, for sentence (1) in the intro-
duction, we identify the following textual fragments:
Thomas, Mario, strikers, Munich. This output is ob-
tained thanks to our loose candidate identification
routine, i.e., based on superstring matching instead
of exact matching, which, for instance, enables us to
recognize the right candidate Mario Gomez for the
mention Mario even if this named entity does not
have Mario as one of its lexicalizations (for an anal-
ysis of the impact of this routine against the exact
matching approach see the discussion in Section 9).
Moreover, as we stated in Section 3, we allow
overlapping fragments, e.g., for major league we
recognize league and major league. We denote with
cand(f) the set of all the candidate meanings of
fragment f . For instance, for the noun league we
have that cand(league) contains among others the
sport word sense and the TV series named entity.
7 Candidate Disambiguation
Semantic interpretation graph. After the identi-
fication of fragments (F ) and their candidate mean-
ings (cand(?)), we create a directed graph GI =
(VI , EI) of the semantic interpretations of the input
text. We show the pseudocode in Algorithm 2. VI
contains all the candidate meanings of all fragments,
that is, VI := {(v, f) : v ? cand(f), f ? F}, where
f is a fragment of the input text and v is a candidate
Babel synset that has a lexicalization which is equal
to or is a superstring of f (see lines 4?8). The set
of edges EI connects related meanings and is pop-
ulated as follows: we add an edge from (v, f) to
(v?, f ?) if and only if f 6= f ? and v? ? semSignv
(see lines 9?11). In other words, we connect two
candidate meanings of different fragments if one is
in the semantic signature of the other. For instance,
we add an edge between (Mario Gomez, Mario) and
(Thomas Mu?ller, Thomas), while we do not add one
between (Mario Gomez, Mario) and (Mario Basler,
Mario) since these are two candidate meanings of
the same fragment, i.e., Mario. In Figure 1, we show
an excerpt of our graph for sentence (1).
At this point we have a graph-based representa-
tion of all the possible interpretations of the input
text. In order to drastically reduce the degree of am-
biguity while keeping the interpretation coherence
as high as possible, we apply a novel densest sub-
graph heuristic (see line 12), whose description we
defer to the next paragraph. The result is a sub-
graph which contains those semantic interpretations
that are most coherent to each other. However, this
subgraph might still contain multiple interpretations
for the same fragment, and even unambiguous frag-
ments which are not correct. Therefore, the final
235
(Toma?s Milia?n, Thomas)
(Thomas Mu?ller, Thomas)
(forward, striker)
(striker, striker) (FC Bayern Munich, Munich)
(Munich, Munich)
(Mario Adorf, Mario)
(Mario Basler, Mario)
(Mario Gomez, Mario)
Figure 1: An excerpt of the semantic interpretation graph automatically built for the sentence Thomas and Mario are
strikers playing in Munich (the edges connecting the correct meanings are in bold).
step is the selection of the most suitable candidate
meaning for each fragment f given a threshold ? to
discard semantically unrelated candidate meanings.
We score each meaning v ? cand(f) with its nor-
malized weighted degree5 in the densest subgraph:
score((v, f)) = w(v,f) ? deg((v, f))?
v? ? cand(f)
w(v?,f) ? deg((v?, f))
(2)
where w(v,f) is the fraction of fragments the candi-
date meaning v connects to:
w(v,f) :=
|{f ? ? F : ?v? s.t. ((v, f), (v?, f ?))
or ((v?, f ?), (v, f)) ? EI}|
|F | ? 1
The rationale behind this scoring function is to
take into account both the semantic coherence, us-
ing a graph centrality measure among the candidate
meanings, and the lexical coherence, in terms of the
number of fragments a candidate relates to.
Finally, we link each f to the highest ranking can-
didate meaning v? if score((v?, f)) ? ?, where ? is
a fixed threshold (see lines 14?18 of Algorithm 2).
For instance, in sentence (1) and for the fragment
Mario we select Mario Gomez as our final candidate
meaning and link it to the fragment.
Linking by densest subgraph. We now illustrate
our novel densest subgraph heuristic, used in line 12
of Algorithm 2, for reducing the level of ambiguity
of the initial semantic interpretation graph GI . The
main idea here is that the most suitable meanings of
each text fragment will belong to the densest area of
the graph. For instance, in Figure 1 the (candidate,
fragment) pairs (Thomas Mu?ller, Thomas), (Mario
Gomez, Mario), (striker, striker) and (FC Bayern
5We denote with deg(v) the overall number of incoming and
outgoing edges, i.e., deg(v) := deg+(v) + deg?(v).
Algorithm 2 Candidate Disambiguation.
1: input: F , the fragments in the input text;
semSign, the semantic signatures;
?, ambiguity level to be reached;
cand, fragments to candidate meanings.
2: output: selected, disambiguated fragments.
3: function DISAMB(F, semSign, ?, cand)
4: VI := ?;EI := ?
5: GI := (VI , EI)
6: for each fragment f ? F do
7: for each candidate v ? cand(f) do
8: VI := VI ? {(v, f)}
9: for each ((v, f), (v?, f ?)) ? VI ? VI do
10: if f 6= f ? and v? ? semSignv then
11: EI := EI ? {((v, f), (v?, f ?))}
12: G?I := DENSSUB(F, cand,GI , ?)
13: selected := newMap < String, Synset >
14: for each f ? F s.t. ?(v, f) ? V ?I do
15: cand?(f) := {v : (v, f) ? V ?I }
16: v? := argmaxv?cand?(f) score((v, f))
17: if score((v?, f)) ? ? then
18: selected(f) := v?
19: return selected
Munich, Munich) form a dense subgraph supporting
their relevance for sentence (1).
The problem of identifying the densest subgraph
of size at least k is NP-hard (Feige et al., 1999).
Therefore, we define a heuristic for k-partite graphs
inspired by a 2-approximation greedy algorithm for
arbitrary graphs (Charikar, 2000; Khuller and Saha,
2009). Our adapted strategy for selecting a dense
subgraph of GI is based on the iterative removal
of low-coherence vertices, i.e., fragment interpreta-
tions. We show the pseudocode in Algorithm 3.
We start with the initial graph G(0)I at step t = 0
(see line 5). For each step t (lines 7?16), first, we
identify the most ambiguous fragment fmax, i.e., the
one with the maximum number of candidate mean-
236
Algorithm 3 Densest Subgraph.
1: input: F , the set of all fragments in the input text;
cand, from fragments to candidate meanings;
G(0)I , the full semantic interpretation graph;
?, ambiguity level to be reached.
2: output: G?I , a dense subgraph.
3: function DENSSUB(F, cand,G(0)I , ?)
4: t := 0
5: G?I := G(0)I
6: while true do
7: fmax := argmaxf?F |{v : ?(v, f) ? V (t)I }|
8: if |{v : ?(v, fmax) ? V (t)I }| ? ? then
9: break;
10: vmin:= argmin
v ? cand(fmax)
score((v, fmax))
11: V (t+1)I := V (t)I \ {(vmin, fmax)}
12: E(t+1)I := E(t)I ? V (t+1)I ? V (t+1)I
13: G(t+1)I := (V (t+1)I , E(t+1)I )
14: if avgdeg(G(t+1)I ) > avgdeg(G?I) then
15: G?I := G(t+1)I
16: t := t+ 1
17: return G?I
ings in the graph (see line 7). Next, we discard
the weakest interpretation of the current fragment
fmax. To do so, we determine the lexical and seman-
tic coherence of each candidate meaning (v, fmax)
using Formula 2 (see line 10). We then remove
from our graph G(t)I the lowest-coherence vertex
(vmin, fmax), i.e., the one whose score is minimum
(see lines 11?13). For instance, in Figure 1, fmax
is the fragment Mario and we have: score((Mario
Gomez, Mario)) ? 33 ? 5 = 5, score((Mario Basler,Mario)) ? 13 ? 1 = 0.3 and score((Mario Adorf,Mario))? 23 ? 2 = 1.3, so we remove (Mario Basler,Mario) from the graph since its score is minimum.
We then move to the next step, i.e., we set t :=
t + 1 (see line 16) and repeat the low-coherence re-
moval step. We stop when the number of remaining
candidates for each fragment is below a threshold
?, i.e., |{v : ?(v, f) ? V (t)I }| ? ? ?f ? F (see
lines 8?9). During each iteration step t we com-
pute the average degree of the current graph G(t)I ,
i.e., avgdeg(G(t)I ) = 2|E
(t)
I |
|V (t)I |
. Finally, we select as
the densest subgraph of the initial semantic interpre-
tation graphGI the graphG?I that maximizes the av-
erage degree (see lines 14?15).
8 Experimental Setup
Datasets. We carried out our experiments on six
datasets, four for WSD and two for EL:
? The SemEval-2013 task 12 dataset for multilin-
gual WSD (Navigli et al., 2013), which consists
of 13 documents in different domains, available
in 5 languages. For each language, all noun
occurrences were annotated using BabelNet,
thereby providing Wikipedia and WordNet an-
notations wherever applicable. The number of
mentions to be disambiguated roughly ranges
from 1K to 2K per language in the different se-
tups.
? The SemEval-2007 task 7 dataset for coarse-
grained English all-words WSD (Navigli et al.,
2007). We take into account only nominal men-
tions obtaining a dataset containing 1107 nouns
to be disambiguated using WordNet.
? The SemEval-2007 task 17 dataset for fine-
grained English all-words WSD (Pradhan et al.,
2007). We considered only nominal mentions
resulting in 158 nouns annotated with WordNet
synsets.
? The Senseval-3 dataset for English all-words
WSD (Snyder and Palmer, 2004), which con-
tains 899 nouns to be disambiguated using
WordNet.
? KORE50 (Hoffart et al., 2012), which consists
of 50 short English sentences (mean length of
14 words) with a total number of 144 mentions
manually annotated using YAGO2, for which a
Wikipedia mapping is available. This dataset
was built with the idea of testing against a high
level of ambiguity for the EL task.
? AIDA-CoNLL6 (Hoffart et al., 2011), which
consists of 1392 English articles, for a total
of roughly 35K named entity mentions anno-
tated with YAGO concepts separated in devel-
opment, training and test sets.
We exploited the POS tags already available in the
SemEval and Senseval datasets, while we used the
Stanford POS tagger (Toutanova et al., 2003) for the
English sentences in the last two datasets.
6We used AIDA-CoNLL as it is the most recent and largest
available dataset for EL (Hachey et al., 2013). The TAC KBP
datasets are available only to participants.
237
Parameters. We fixed the parameters of RWR
(Section 5) to the values ? = .85, ? = 100 and
n = 1M which maximize F1 on a manually cre-
ated tuning set made up of 10 gold-standard seman-
tic signatures. We tuned our two disambiguation pa-
rameters ? = 10 and ? = 0.8 by optimizing F1 on
the trial dataset of the SemEval-2013 task on mul-
tilingual WSD (Navigli et al., 2013). We used the
same parameters on all the other WSD datasets. As
for EL, we used the training part of AIDA-CoNLL
(Hoffart et al., 2011) to set ? = 5 and ? = 0.0.
8.1 Systems
Multilingual WSD. We evaluated our system on
the SemEval-2013 task 12 by comparing it with the
participating systems:
? UMCC-DLSI (Gutie?rrez et al., 2013) a state-
of-the-art Personalized PageRank-based ap-
proach that exploits the integration of different
sources of knowledge, such as WordNet Do-
mains/Affect (Strapparava and Valitutti, 2004),
SUMO (Zouaq et al., 2009) and the eXtended
WordNet (Mihalcea and Moldovan, 2001);
? DAEBAK! (Manion and Sainudiin, 2013)
which performs WSD on the basis of periph-
eral diversity within subgraphs of BabelNet;
? GETALP (Schwab et al., 2013) which uses an
Ant Colony Optimization technique together
with the classical measure of Lesk (1986).
We also compared with UKB w2w (Agirre
and Soroa, 2009), a state-of-the-art approach for
knowledge-based WSD, based on Personalized
PageRank (Haveliwala, 2002). We used the same
mapping from words to senses that we used in our
approach, default parameters7 and BabelNet as the
input graph. Moreover, we compared our system
with IMS (Zhong and Ng, 2010), a state-of-the-
art supervised English WSD system which uses an
SVM trained on sense-annotated corpora, such as
SemCor (Miller et al., 1993) and DSO (Ng and
Lee, 1996), among others. We used the IMS model
out-of-the-box with Most Frequent Sense (MFS) as
backoff routine since the model obtained using the
task trial data performed worse.
We followed the original task formulation and
evaluated the synsets in three different settings, i.e.,
7./ukb wsd -D dict.txt -K kb.bin --ppr w2w ctx.txt
when using BabelNet senses, Wikipedia senses and
WordNet senses, thanks to BabelNet being a super-
set of the other two inventories. We ran our sys-
tem on a document-by-document basis, i.e., disam-
biguating each document at once, so as to test its
effectiveness on long coherent texts. Performance
was calculated in terms of F1 score. We also com-
pared the systems with the MFS baseline computed
for the three inventories (Navigli et al., 2013).
Coarse-grained WSD. For the SemEval-2007
task 7 we compared our system with the two top-
ranked approaches, i.e., NUS-PT (Chan et al., 2007)
and UoR-SSI (Navigli, 2008), which respectively
exploited parallel texts and enriched semantic paths
in a semantic network, the previously described
UKB w2w system,8 a knowledge-based WSD ap-
proach (Ponzetto and Navigli, 2010) which exploits
an automatic extension of WordNet, and, as base-
line, the MFS.
Fine-grained WSD. For the remaining fine-
grained WSD datasets, i.e., Senseval-3 and
SemEval-2007 task 17, we compared our approach
with the previously described state-of-the-art
systems UKB and IMS, and, as baseline, the MFS.
KORE50 and AIDA-CoNLL. For the KORE50
and AIDA-CoNLL datasets we compared our sys-
tem with six approaches, including state-of-the-art
ones (Hoffart et al., 2012; Cornolti et al., 2013):
? MW, i.e., the Normalized Google Distance as
defined by Milne and Witten (2008);
? KPCS (Hoffart et al., 2012), which calcu-
lates a Mutual Information weighted vector of
keyphrases for each candidate and then uses the
cosine similarity to obtain candidates? scores;
? KORE and its variants KORELSH?G and
KORELSH?F (Hoffart et al., 2012), based on
similarity measures that exploit the overlap be-
tween phrases associated with the considered
entities (KORE) and a hashing technique to re-
duce the space needed by the keyphrases asso-
ciated with the entities (LSH-G, LSH-F);
? Tagme 2.09 (Ferragina and Scaiella, 2012)
which uses the relatedness measure defined
8We report the results as given by Agirre et al. (2014).
9We used the out-of-the-box RESTful API available at
http://tagme.di.unipi.it
238
Sens3 Sem07 SemEval-2013 English French German Italian Spanish
System WN WN WN Wiki BN Wiki BN Wiki BN Wiki BN Wiki BN
Babelfy 68.3 62.7 65.9 87.4 69.2 71.6 ?56.9 81.6 69.4 84.3 66.6 83.8 69.5
IMS 71.2 63.3 65.7 ? ? ? ? ? ? ? ? ? ?
UKB w2w ?65.3 ?56.0 61.3 ? 60.8 ? 60.8 ? 66.2 ? 67.3 ? 70.0
UMCC-DLSI ? ? 64.7 54.8 68.5 ?60.5 60.5 ?58.1 62.8 ?58.3 65.8 ?61.0 71.0
DAEBAK! ? ? ? ? 60.4 ? 53.8 ? 59.1 ? ?61.3 ? 60.0
GETALP-BN ? ? 51.4 ? 58.3 ? 48.3 ? 52.3 ? 52.8 ? 57.8
MFS 70.3 65.8 ?63.0 ?80.3 ?66.5 69.4 45.3 83.1 ?67.4 82.3 57.5 82.4 ?64.4
Babelfy unif. weights 67.0 65.2 65.0 87.0 68.5 71.9 57.2 81.2 69.8 83.7 66.8 83.8 70.8
Babelfy w/o dens. sub. 68.3 63.3 65.4 87.3 68.7 71.6 57.0 81.7 69.1 84.4 66.5 83.9 69.5
Babelfy only concepts 68.2 62.7 65.5 83.0 68.7 70.2 56.6 79.3 69.3 83.0 66.3 84.0 69.7
Babelfy on sentences 66.0 65.2 63.5 84.0 67.1 70.7 53.6 82.3 68.1 83.8 64.2 83.5 68.7
Table 1: F1 scores (percentages) of the participating systems of SemEval-2013 task 12 together with MFS, UKB w2w,
IMS, our system and its ablated versions on the Senseval-3, SemEval-2007 task 17 and SemEval-2013 datasets. The
first system which has a statistically significant difference from the top system is marked with ? (?2, p < 0.05).
by Milne and Witten (2008) weighted with
the commonness of a sense together with
the keyphraseness measure defined by Mihal-
cea and Csomai (2007) to exploit the context
around the target word;
? Illinois Wikifier10 (Cheng and Roth, 2013)
which combines local features, such as com-
monness and TF-IDF between mentions and
Wikipedia pages, with global coherence fea-
tures based on Wikipedia links and relational
inference;
? DBpedia Spotlight11 (Mendes et al., 2011)
which uses LingPipe?s string matching algo-
rithm implementation together with a weighted
cosine similarity measure to recognize and dis-
ambiguate mentions.
We also compared with UKB w2w, introduced
above. Note that we could not use supervised sys-
tems, as the training data of AIDA-CoNLL covers
less than half of the mentions used in the testing
part and less than 10% of the entities considered in
KORE50. To enable a fair comparison, we ran our
system by restricting the BabelNet sense inventory
of the target mentions to the English Wikipedia. As
is customary in the literature, we calculated the sys-
tems? accuracy for both Entity Linking datasets.
10We used the out-of-the-box Java API available from
http://cogcomp.cs.illinois.edu/page/download view/Wikifier
11We used the 2011 version of DBpedia Spotlight as it ob-
tains better scores on the considered datasets in comparison to
the new version (Daiber et al., 2013). We used the out-of-the-
box RESTful API available at http://spotlight.dbpedia.org
9 Results
Multilingual WSD. In Table 1 we show the F1
performance on the SemEval-2013 task 12 for the
three setups: WordNet, Wikipedia and BabelNet.
Using BabelNet we surpass all systems on English
and German and obtain performance comparable
with the best systems on two other languages (UKB
on Italian and UMCC-DLSI on Spanish). Using the
WordNet sense inventory, our results are on a par
with the best system, i.e., IMS. On Wikipedia our
results range between 71.6% (French) and 87.4% F1
(English), i.e., more than 10 points higher than the
current state of the art (UMCC-DLSI) in all 5 lan-
guages. As for the MFS baseline, which is known
to be very competitive in WSD (Navigli, 2009), we
beat it in all setups except for German on Wikipedia.
Interestingly, we surpass the WordNet MFS by 2.9
points, a significant result for a knowledge-based
system (see also (Pilehvar and Navigli, 2014)).
Coarse- and fine-grained WSD. In Table 2, we
show the results of the systems on the SemEval-
2007 coarse-grained WSD dataset. As can be seen,
we obtain the second best result after Ponzetto and
Navigli (2010). In Table 1 (first two columns), we
show the results of IMS and UKB on the Senseval-3
and SemEval-2007 task 17 datasets. We rank second
on both datasets after IMS. However, the differences
are not statistically significant. Moreover, Agirre et
al. (2014, Table 5) note that using WordNet 3.0, in-
stead of 1.7 or 2.1, to annotate these datasets can
cause a more than one percent drop in performance.
239
System F1
(Ponzetto and Navigli, 2010) 85.5
Babelfy 84.6
UoR-SSI 84.1
UKB w2w 83.6
NUS-PT ?82.3
MFS 77.4
Babelfy unif. weights 85.7
Babelfy w/o dens. sub. 84.9
Babelfy only concepts 85.3
Babelfy on sentences 82.3
Table 2: F1 score (percentages) on the SemEval-2007
task 7. The first system which has a statistically signifi-
cant difference from the top system is marked with ? (?2,
p < 0.05).
Entity Linking. In Table 3 we show the results on
the two Entity Linking datasets, i.e., KORE50 and
AIDA-CoNLL. Our system outperforms all other
approaches, with KORE-LSH-G getting closest, and
Tagme and Wikifier lagging behind on the KORE50
dataset. For the AIDA-CoNLL dataset we obtain the
third best performance after MW and KPCS, how-
ever the difference is not statistically significant.
We note the low performance of DBpedia Spot-
light which, even if it achieves almost 100% preci-
sion on the identified mentions on both datasets, suf-
fers from low recall due to its candidate identifica-
tion step, confirming previous evaluations (Derczyn-
ski et al., 2013; Hakimov et al., 2012; Ludwig and
Sack, 2011). This problem becomes even more ac-
centuated in the latest version of this system (Daiber
et al., 2013). Finally, UKB using BabelNet obtains
low performance on EL, i.e., 19.4-10.5 points below
the state of the art. This result is discussed below.
Discussion. The results obtained by UKB show
that the high performance of our unified approach
to EL and WSD is not just a mere artifact of the use
of a rich multilingual semantic network, that is, Ba-
belNet. In other words, it is not true that any graph-
based algorithm could be applied to perform both
EL and WSD at the same time equally well. This
also shows that BabelNet by itself is not sufficient
for achieving high performances for both tasks and
that, instead, an appropriate processing of the struc-
tural and lexical information of the semantic net-
work is needed. A manual analysis revealed that the
main cause of error for UKB in the EL setup stems
System KORE50 CoNLL
Babelfy 71.5 82.1
KORE-LSH-G 64.6 81.8
KORE 63.9 ?80.7
MW ?57.6 82.3
Tagme 56.3 70.1
KPCS 55.6 82.2
KORE-LSH-F 53.2 81.2
UKB w2w (on BabelNet) 52.1 71.8
Illinois Wikifier 41.7 72.4
DBpedia Spotlight 35.4 34.0
Babelfy unif. weights 69.4 81.7
Babelfy w/o dens. sub. 62.5 78.1
Babelfy only NE 68.1 78.8
Table 3: Accuracy (percentages) of state-of-the-art EL
systems and our system on KORE50 and AIDA-CoNLL.
The first system with a statistically significant difference
from the top system is marked with ? (?2, p < 0.05).
from its inability to enforce high coherence, e.g., by
jointly disambiguating all the words, which is in-
stead needed when considering the high level of am-
biguity that we have in our semantic interpretation
graph (Cucerzan, 2007). For instance, for sentence
(1) in the introduction, UKB disambiguates Thomas
as a cricket player and Mario as the popular video
game rather than the two well-known soccer play-
ers, and Munich as the German city, rather than the
soccer team in which they play. Our approach, in-
stead, by enforcing highly coherent semantic inter-
pretations, correctly identifies all the soccer-related
entities.
In order to determine the need of our loose candi-
date identification heuristic (see Section 6), we com-
pared the percentage of times a candidate set con-
tains the correct entity against that obtained by an
exact string matching between the mention and the
sense inventory. On KORE50, our heuristic retrieves
the correct entity 98.6% of the time vs. 42.4% when
exact matching is used. This demonstrates the inad-
equacy of exact matching for EL, and the need for
a comprehensive sense inventory, as is done in our
approach.
We also performed different ablation tests by ex-
perimenting with the following variants of our sys-
tem (reported at the bottom of Tables 1, 2 and 3):
? Babelfy using uniform distribution during the
RWR to obtain the concepts? semantic sig-
natures; this test assesses the impact of our
weighting and edge creation strategy.
240
? Babelfy without performing the densest sub-
graph heuristic, i.e., when line 12 in Algorithm
2 is G?I = GI , so as to verify the impact of
identifying the most coherent interpretations.
? Babelfy applied to the BabelNet subgraph in-
duced by the entire set of named entity ver-
tices, for the EL task, and that induced by word
senses only, for the WSD task; this test aims to
stress the impact of our unified approach.
? Babelfy applied on sentences instead of on
whole documents.
The component which has a smaller impact on
the performance is our triangle-based weighting
scheme. The main exception is on the smallest
dataset, i.e., SemEval-2007 task 17, for which this
version attains an improvement of 2.5 percentage
points.
Babelfy without the densest subgraph algorithm
is the version which attains the lowest performances
on the EL task, with a 9% performance drop on the
KORE50 dataset, showing the need for a specially
designed approach to cope with the high level of am-
biguity that is encountered on this task. On the other
hand, in the WSD datasets this version attains almost
the same results as the full version, due to the lower
number of candidate word senses.
Babelfy applied on sentences instead of on whole
documents shows a lower performance, confirm-
ing the significance of higher semantic coherence
on whole documents (notwithstanding the two ex-
ceptions on the SemEval-2007 task 17 and on the
SemEval-2013 German Wikipedia datasets).
Finally, the version in which we restrict our
system to named entities only (for EL) and con-
cepts only (for WSD) consistently obtains lower re-
sults (notwithstanding the three exceptions on the
Spanish SemEval-2013 task 12 using BabelNet and
Wikipedia, and on the SemEval 2007 coarse-grained
task). This highlights the benefit of our joint use
of lexicographic and encyclopedic structured knowl-
edge, on each of the two tasks. The 3.4% per-
formance drop attained on KORE50 is of particu-
lar interest, since this dataset aims at testing perfor-
mance on highly ambiguous mentions within short
sentences. This indicates that the semantic analysis
of small contexts can be improved by leveraging the
coherence between concepts and named entities.
10 Conclusion
In this paper we presented Babelfy, a novel,
integrated approach to Entity Linking and
Word Sense Disambiguation, available at
http://babelfy.org. Our joint solution is
based on three key steps: i) the automatic creation
of semantic signatures, i.e., related concepts and
named entities, for each node in the reference
semantic network; ii) the unconstrained identifica-
tion of candidate meanings for all possible textual
fragments; iii) linking based on a high-coherence
densest subgraph algorithm. We used BabelNet
1.1.1 as our multilingual semantic network.
Our graph-based approach exploits the semantic
network structure to its advantage: two key features
of BabelNet, that is, its multilinguality and its in-
tegration of lexicographic and encyclopedic knowl-
edge, make it possible to run our general, unified ap-
proach on the two tasks of Entity Linking and WSD
in any of the languages covered by the semantic net-
work. However, we also demonstrated that Babel-
Net in itself does not lead to state-of-the-art accu-
racy on both tasks, even when used in conjunction
with a high-performance graph-based algorithm like
Personalized PageRank. This shows the need for our
novel unified approach to EL and WSD.
At the core of our approach lies the effective treat-
ment of the high degree of ambiguity of partial tex-
tual mentions by means of a 2-approximation algo-
rithm for the densest subgraph problem, which en-
ables us to output a semantic interpretation of the
input text with drastically reduced ambiguity, as was
previously done with SSI (Navigli, 2008).
Our experiments on six gold-standard datasets
show the state-of-the-art performance of our ap-
proach, as well as its robustness across languages.
Our evaluation also demonstrates that our approach
fares well both on long texts, such as those of the
WSD tasks, and short and highly-ambiguous sen-
tences, such as the ones in KORE50. Finally, abla-
tion tests and further analysis demonstrate that each
component of our system is needed to contribute
state-of-the-art performances on both EL and WSD.
As future work, we plan to use Babelfy for in-
formation extraction, where semantics is taking the
lead (Moro and Navigli, 2013), and for the valida-
tion of semantic annotations (Vannella et al., 2014).
241
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Proc.
of EACL, pages 33?41.
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proc. of EMNLP, pages
585?593.
Eneko Agirre, Aitor Soroa, and Mark Stevenson. 2010.
Graph-based Word Sense Disambiguation of biomedi-
cal documents. Bioinformatics, 26(22):2889?2896.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random Walks for Knowledge-Based Word
Sense Disambiguation. Computational Linguistics,
40(1):57?84.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
DBpedia: A Nucleus for a Web of Open Data. In Proc.
of ISWC/ASWC, pages 722?735.
Carmen Banea and Rada Mihalcea. 2011. Word Sense
Disambiguation with multilingual features. In Proc.
of IWCS, pages 25?34.
Christoph Bo?hm, Gerard de Melo, Felix Naumann, and
Gerhard Weikum. 2012. LINDA: distributed web-of-
data-scale entity matching. In Proc. of CIKM, pages
2104?2108.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proc. of EACL, pages 103?111.
Razvan C. Bunescu and Marius Pasca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL, pages 9?16.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-PT: Exploiting Parallel Texts for Word Sense
Disambiguation in the English All-Words Tasks. In
Proc. of SemEval-2007, pages 253?256.
Moses Charikar. 2000. Greedy approximation algo-
rithms for finding dense components in a graph. In
Proc. of APPROX, pages 84?95.
Xiao Cheng and Dan Roth. 2013. Relational Inference
for Wikification. In Proc. of EMNLP, pages 1787?
1796.
Marco Cornolti, Paolo Ferragina, and Massimiliano Cia-
ramita. 2013. A framework for benchmarking entity-
annotation systems. In Proc. of WWW, pages 249?
260.
Silviu Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. In Proc. of
EMNLP-CoNLL, pages 708?716.
Joachim Daiber, Max Jakob, Chris Hokamp, and
Pablo N. Mendes. 2013. Improving efficiency and
accuracy in multilingual entity extraction. In Proc. of
I-Semantics, pages 121?124.
Leon Derczynski, Diana Maynard, Niraj Aswani, and
Kalina Bontcheva. 2013. Microblog-genre noise and
impact on semantic annotation accuracy. In Proc. of
Hypertext, pages 21?30.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and Diversifying Web Search Results with Graph-
Based Word Sense Induction. Computational Linguis-
tics, 39(3):709?754.
Nicolai Erbs, Torsten Zesch, and Iryna Gurevych. 2011.
Link discovery: A comprehensive analysis. In Proc.
of ICSC, pages 83?86.
Oren Etzioni, Michele Banko, and Michael J Cafarella.
2006. Machine Reading. In Proc. of AAAI, pages
1517?1519.
Uriel Feige, Guy Kortsarz, and David Peleg. 1999. The
dense k-subgraph problem. Algorithmica, 29:2001.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Paolo Ferragina and Ugo Scaiella. 2010. TAGME:
On-the-fly Annotation of Short Text Fragments (by
Wikipedia Entities). In Proc. of CIKM, pages 1625?
1628.
Paolo Ferragina and Ugo Scaiella. 2012. Fast and Accu-
rate Annotation of Short Texts with Wikipedia Pages.
IEEE Software, 29(1):70?75.
Weiwei Guo and Mona T. Diab. 2010. Combining
Orthogonal Monolingual and Multilingual Sources of
Evidence for All Words WSD. In Proc. of ACL, pages
1542?1551.
Yoan Gutie?rrez, Yenier Castan?eda, Andy Gonza?lez,
Rainel Estrada, Dennys D. Piug, Jose I. Abreu,
Roger Pe?rez, Antonio Ferna?ndez Orqu??n, Andre?s
Montoyo, Rafael Mun?oz, and Franc Camara. 2013.
UMCC DLSI: Reinforcing a Ranking Algorithm with
Sense Frequencies and Multidimensional Semantic
Resources to solve Multilingual Word Sense Disam-
biguation. In Proc. of SemEval-2013, pages 241?249.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating En-
tity Linking with Wikipedia. Artificial Intelligence,
194:130?150.
Sherzod Hakimov, Salih Atilay Oto, and Erdogan Dogdu.
2012. Named entity recognition and disambiguation
using linked data and graph-based centrality scoring.
In Proc. of SWIM, pages 4:1?4:7.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proc. of WWW, pages 517?526.
242
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Fu?rstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum.
2011. Robust disambiguation of named entities in text.
In Proc. of EMNLP, pages 782?792.
Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Mar-
tin Theobald, and Gerhard Weikum. 2012. KORE:
keyphrase overlap relatedness for entity disambigua-
tion. In Proc. of CIKM, pages 545?554.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2013. YAGO2: A spatially and
temporally enhanced knowledge base from Wikipedia.
Artificial Intelligence, 194:28?61.
Eduard H. Hovy, Roberto Navigli, and Simone P.
Ponzetto. 2013. Collaboratively built semi-structured
content and Artificial Intelligence: The story so far.
Artificial Intelligence, 194:2?27.
Samir Khuller and Barna Saha. 2009. On finding dense
subgraphs. In Proc. of ICALP, pages 597?608.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective Annotation of
Wikipedia Entities in Web Text. In Proc. of KDD,
pages 457?466.
Els Lefever and Ve?ronique Hoste. 2010. Semeval-2010
task 3: Cross-lingual Word Sense Disambiguation. In
Proc. of SemEval-2010, pages 15?20.
Els Lefever and Ve?ronique Hoste. 2013. SemEval-2013
Task 10: Cross-lingual Word Sense Disambiguation.
In Proc. of SemEval-2013, pages 158?166.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
Word Sense Disambiguation. In Proc. of ACL-HLT,
pages 317?322.
Michael E. Lesk. 1986. Automatic Sense Disambigua-
tion Using Machine Readable Dictionaries: How to
Tell a Pine Cone from an Ice Cream Cone. In Proc.
of the International Conference on Systems Documen-
tation, pages 24?26.
Nadine Ludwig and Harald Sack. 2011. Named entity
recognition for user-generated tags. In Proc. of DEXA,
pages 177?181.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
task 14: Word sense induction & disambiguation. In
Proc. of SemEval-2010, pages 63?68.
Steve L. Manion and Raazesh Sainudiin. 2013. DAE-
BAK!: Peripheral Diversity for Multilingual Word
Sense Disambiguation. In Proc. of SemEval-2013,
pages 250?254.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva, and
Christian Bizer. 2011. DBpedia spotlight: shed-
ding light on the web of documents. In Proc. of I-
Semantics, pages 1?8.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Proc. of
CIKM, pages 233?242.
Rada Mihalcea and Dan I Moldovan. 2001. Extended
WordNet: Progress report. In Proc. of NAACL Work-
shop on WordNet and Other Lexical Resources, pages
95?100.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proc. of
HLT/EMNLP, pages 411?418.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of HLT, pages 303?308.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna
Gurevych. 2012. Using Distributional Similarity for
Lexical Expansion in Knowledge-based Word Sense
Disambiguation. In Proc. of COLING, pages 1781?
1796.
David Milne and Ian H. Witten. 2008. Learning to link
with Wikipedia. In Proc. of CIKM, pages 509?518.
Andrea Moro and Roberto Navigli. 2013. Integrating
Syntactic and Semantic Analysis into the Open Infor-
mation Extraction Paradigm. In Proc. of IJCAI, pages
2148?2154.
Roberto Navigli and Mirella Lapata. 2010. An Experi-
mental Study of Graph Connectivity for Unsupervised
Word Sense Disambiguation. TPAMI, 32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
Joining forces pays off: Multilingual Joint Word Sense
Disambiguation. In Proc. of EMNLP, pages 1399?
1410.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: Coarse-
Grained English All-Words Task. In Proc. of
SemEval-2007, pages 30?35.
Roberto Navigli, David Jurgens, and Daniele Vannella.
2013. SemEval-2013 Task 12: Multilingual Word
Sense Disambiguation. In Proc. of SemEval-2013,
pages 222?231.
Roberto Navigli. 2008. A structural approach to the
automatic adjudication of word sense disagreements.
Natural Language Engineering, 14(4):293?310.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A Quick Tour of Word Sense
Disambiguation, Induction and Related Approaches.
In Proc. of SOFSEM, pages 115?129.
243
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proc. of ACL,
pages 40?47.
Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A Large-scale Pseudoword-based Evaluation Frame-
work for State-of-the-Art Word Sense Disambigua-
tion. Computational Linguistics.
Simone P. Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised system. In Proc. of ACL, pages 1522?1531.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 task 17: En-
glish lexical sample, SRL and all words. In Proc. of
SemEval-2007, pages 87?92. Association for Compu-
tational Linguistics.
Delip Rao, Paul McNamee, and Mark Dredze. 2013. En-
tity Linking: Finding Extracted Entities in a Knowl-
edge Base. In Multi-source, Multilingual Information
Extraction and Summarization, Theory and Applica-
tions of Natural Language Processing, pages 93?115.
Springer Berlin Heidelberg.
Lev-Arie Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and Global Algorithms for
Disambiguation to Wikipedia. In Proc. of ACL, pages
1375?1384.
Lenhart K. Schubert. 2006. Turing?s dream and the
knowledge challenge. In Proc. of NCAI, pages 1534?
1538.
Didier Schwab, Andon Tchechmedjiev, Je?ro?me Goulian,
Mohammad Nasiruddin, Gilles Se?rasset, and Herve?
Blanchon. 2013. GETALP System: Propagation of
a Lesk Measure through an Ant Colony Algorithm. In
Proc. of SemEval-2013, pages 232?240.
Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013.
Coarse to Fine Grained Sense Disambiguation in
Wikipedia. In Proc. of *SEM, pages 22?31.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
Graph-based Word Sense Disambiguation Using Mea-
sures of Word Semantic Similarity. In Proc. of ICSC,
pages 363?369.
Benjamin Snyder and Martha Palmer. 2004. The English
all-words task. In Proc. of Senseval-3, pages 41?43.
Carlo Strapparava and Alessandro Valitutti. 2004. Word-
Net Affect: an Affective Extension of WordNet. In
Proc. of LREC, pages 1083?1086.
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast Random Walk with Restart and Its Appli-
cations. In Proc. of ICDM, pages 613?622.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
NAACL-HLT, pages 173?180.
George Tsatsaronis, Michalis Vazirgiannis, and Ion An-
droutsopoulos. 2007. Word Sense Disambiguation
with Spreading Activation Networks Generated from
Thesauri. In Proc. of IJCAI, pages 1725?1730.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent Semantic Word Sense Induction and Disambigua-
tion. In Proc. of ACL, pages 1476?1485.
Daniele Vannella, David Jurgens, Daniele Scarfini,
Domenico Toscani, and Roberto Navigli. 2014. Vali-
dating and Extending Semantic Knowledge Bases us-
ing Video Games with a Purpose. In Proc. of ACL.
Duncan J. Watts and Steven H. Strogatz. 1998. Col-
lective dynamics of ?small-world? networks. Nature,
393(6684):409?10.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense: A
Wide-Coverage Word Sense Disambiguation System
for Free Text. In Proc. of ACL (Demo), pages 78?83.
Amal Zouaq, Michel Gagnon, and Benoit Ozell. 2009. A
SUMO-based Semantic Analysis for Knowledge Ex-
traction. In Proc of LTC.
244
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 193?201, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 11: Word Sense Induction & Disambiguation
within an End-User Application
Roberto Navigli and Daniele Vannella
Dipartimento di Informatica
Sapienza Universita` di Roma
Viale Regina Elena, 295 ? 00161 Roma Italy
{navigli,vannella}@di.uniroma1.it
Abstract
In this paper we describe our Semeval-2013
task on Word Sense Induction and Dis-
ambiguation within an end-user application,
namely Web search result clustering and diver-
sification. Given a target query, induction and
disambiguation systems are requested to clus-
ter and diversify the search results returned by
a search engine for that query. The task en-
ables the end-to-end evaluation and compari-
son of systems.
1 Introduction
Word ambiguity is a pervasive issue in Natural Lan-
guage Processing. Two main techniques in compu-
tational lexical semantics, i.e., Word Sense Disam-
biguation (WSD) and Word Sense Induction (WSI)
address this issue from different perspectives: the
former is aimed at assigning word senses from a pre-
defined sense inventory to words in context, whereas
the latter automatically identifies the meanings of a
word of interest by clustering the contexts in which
it occurs (see (Navigli, 2009; Navigli, 2012) for a
survey).
Unfortunately, the paradigms of both WSD and
WSI suffer from significant issues which hamper
their success in real-world applications. In fact, the
performance of WSD systems depends heavily on
which sense inventory is chosen. For instance, the
most popular computational lexicon of English, i.e.,
WordNet (Fellbaum, 1998), provides fine-grained
distinctions which make the disambiguation task
quite difficult even for humans (Edmonds and Kil-
garriff, 2002; Snyder and Palmer, 2004), although
disagreements can be solved to some extent with
graph-based methods (Navigli, 2008). On the other
hand, although WSI overcomes this issue by allow-
ing unrestrained sets of senses, its evaluation is par-
ticularly arduous because there is no easy way of
comparing and ranking different representations of
senses. In fact, all the proposed measures in the lit-
erature tend to favour specific cluster shapes (e.g.,
singletons or all-in-one clusters) of the senses pro-
duced as output. Indeed, WSI evaluation is actually
an instance of the more general and difficult problem
of evaluating clustering algorithms.
Nonetheless, many everyday tasks carried out by
online users would benefit from intelligent systems
able to address the lexical ambiguity issue effec-
tively. A case in point is Web information retrieval, a
task which is becoming increasingly difficult given
the continuously growing pool of Web text of the
most wildly disparate kinds. Recent work has ad-
dressed this issue by proposing a general evaluation
framework for injecting WSI into Web search result
clustering and diversification (Navigli and Crisa-
fulli, 2010; Di Marco and Navigli, 2013). In this
task the search results returned by a search engine
for an input query are grouped into clusters, and di-
versified by providing a reranking which maximizes
the meaning heterogeneity of the top ranking results.
The Semeval-2013 task described in this paper1
adopts the evaluation framework of Di Marco and
Navigli (2013), and extends it to both WSD and WSI
systems. The task is aimed at overcoming the well-
known limitations of in vitro evaluations, such as
those of previous SemEval tasks on the topic (Agirre
1http://www.cs.york.ac.uk/semeval-2013/task11/
193
and Soroa, 2007; Manandhar et al, 2010), and en-
abling a fair comparison between the two disam-
biguation paradigms. Key to our framework is the
assumption that search results grouped into a given
cluster are semantically related to each other and
that each cluster is expected to represent a specific
meaning of the input query (even though it is possi-
ble for more than one cluster to represent the same
meaning). For instance, consider the target query
apple and the following 3 search result snippets:
1. Apple Inc., formerly Apple Computer, Inc., is...
2. The science of apple growing is called pomology...
3. Apple designs and creates iPod and iTunes...
Participating systems were requested to produce a
clustering that groups snippets conveying the same
meaning of the input query apple, i.e., ideally {1, 3}
and {2} in the above example.
2 Task setup
For each ambiguous query the task required partic-
ipating systems to cluster the top ranking snippets
returned by a search engine (we used the Google
Search API). WSI systems were required to iden-
tify the meanings of the input query and cluster the
snippets into semantically-related groups according
to their meanings. Instead, WSD systems were re-
quested to sense-tag the given snippets with the ap-
propriate senses of the input query, thereby implic-
itly determining a clustering of snippets (i.e., one
cluster per sense).
2.1 Dataset
We created a dataset of 100 ambiguous queries.
The queries were randomly sampled from the AOL
search logs so as to ensure that they had been used in
real search sessions. Following previous work on the
topic (Bernardini et al, 2009; Di Marco and Navigli,
2013) we selected those queries for which a sense
inventory exists as a disambiguation page in the En-
glish Wikipedia2. This guaranteed that the selected
queries consisted of either a single word or a multi-
word expression for which we had a collaboratively-
edited list of meanings, including lexicographic and
encyclopedic ones. We discarded all queries made
2http://en.wikipedia.org/wiki/Disambiguation page
Figure 1: An example of search result for the apple query,
including: page title, URL and snippet.
query length 1 2 3 4
AOL logs 45.89 40.98 10.98 2.32
our dataset 40.00 40.00 15.00 5.00
Table 1: Percentage distribution of AOL query lengths
(first row) vs. the queries sampled for our task (second
row).
up of > 4 words, since the length of the great ma-
jority of queries lay in the range [1, 4]. In Table
1 we compare the percentage distribution of 1- to
4-word queries in the AOL query logs against our
dataset of queries. Note that we increased the per-
centage of 3- and 4-word queries in order to have
a significant coverage of those lengths. Anyhow,
in both cases most queries contained from 1 to 2
words. Note that the reported percentage distribu-
tions of query length is different from recent statis-
tics for two reasons: first, over the years users have
increased the average number of words per query in
order to refine their searches; second, we selected
only queries which were either single words (e.g.,
apple) or multi-word expressions (e.g., mortal kom-
bat), thereby discarding several long queries com-
posed of different words (such as angelina jolie ac-
tress).
Finally, we submitted each query to Google
search and retrieved the 64 top-ranking results re-
turned for each query. Therefore, overall the dataset
consists of 100 queries and 6,400 results. Each
search result includes the following information:
page title, URL of the page and snippet of the page
text. We show an example of search result for the
apple query in Figure 1.
2.2 Dataset Annotation
For each query q we used Amazon Mechani-
cal Turk3 to annotate each query result with the
3https://www.mturk.com
194
most suitable sense. The sense inventory for q
was obtained by listing the senses available in the
Wikipedia disambiguation page of q augmented
with additional options from the classes obtained
from the section headings of the disambiguation
page plus the OTHER catch-all meaning. For in-
stance, consider the apple query. We show its disam-
biguation page in Figure 2. The sense inventory for
apple was made up of the senses listed in that page
(e.g., MALUS, APPLE INC., APPLE BANK, etc.)
plus the set of generic classes OTHER PLANTS AND
PLANT PARTS, OTHER COMPANIES, OTHER FILMS,
plus OTHER.
For each query we ensured that three annotators
tagged each of the 64 results for that query with
the most suitable sense among those in the sense
inventory (selecting OTHER if no sense was appro-
priate). Specifically, each Turker was provided with
the following instructions: ?The goal is annotating
the search result snippets returned by Google for a
given query with the appropriate meaning among
those available (obtained from the Wikipedia disam-
biguation page for the query). You have to select
the meaning that you consider most appropriate?.
No constraint on the age, gender and citizenship of
the annotators was imposed. However, in order to
avoid random tagging of search results, we provided
3 gold-standard result annotations per query, which
could be shown to the Turker more than once during
the annotation process. In the case (s)he failed to
annotate the gold items, the annotator was automat-
ically excluded.
2.3 Inter-Annotator Agreement and
Adjudication
In order to determine the reliability of the Turkers?
annotations, we calculated the individual values of
Fleiss? kappa ? (Fleiss, 1971) for each query q and
then averaged them:
? =
?
q?Q ?q
|Q|
, (1)
where ?q is the Fleiss? kappa agreement of the three
annotators who tagged the 64 snippets returned by
the Google search engine for the query q ? Q, and
Q is our set of 100 queries. We obtained an average
value of ? = 0.66, which according to Landis and
Figure 2: The Wikipedia disambiguation page of Apple.
Koch (1977) can be seen as substantial agreement,
with a standard deviation ? = 0.185.
In Table 2 we show the agreement distribution
of our 6400 snippets, distinguishing between full
agreement (3 out of 3), majority agreement (2 out of
3), and no agreement. Most of the items were anno-
tated with full or majority agreement, indicating that
the manual annotation task was generally doable for
the layman. We manually checked all the cases of
majority agreement, correcting only 7.92% of the
majority adjudications, and manually adjudicated
all the snippets for which there was no agreement.
We observed during adjudication that in many cases
the disagreement was due to the existence of sub-
tle sense distinctions, like between MORTAL KOM-
BAT (VIDEO GAME) and MORTAL KOMBAT (2011
VIDEO GAME), or between THE DA VINCI CODE
and INACCURACIES IN THE DA VINCI CODE.
The average number of senses associated with
the search results of each query was 7.69
(higher than in previous datasets, such as AMBI-
ENT4+MORESQUE5, which associates 5.07 senses
4http://credo.fub.it/ambient
5http://lcl.uniroma1.it/moresque
195
Full agr. Majority Disagr.
% snippets 66.70 25.85 7.45
Table 2: Percentage of snippets with full agreement, ma-
jority agreement and full disagreement.
per query on average).
3 Scoring
Following Di Marco and Navigli (2013), we eval-
uated the systems? outputs in terms of the snippet
clustering quality (Section 3.1) and the snippet di-
versification quality (Section 3.2). Given a query
q ? Q and the corresponding set of 64 snippet re-
sults, let C be the clustering output by a given system
and let G be the gold-standard clustering for those
results. Each measure M(C,G) presented below is
calculated for the query q using these two cluster-
ings. The overall results on the entire set of queries
Q in the dataset is calculated by averaging the val-
ues of M(C,G) obtained for each single test query
q ? Q.
3.1 Clustering Quality
The first evaluation concerned the quality of the
clusters produced by the participating systems.
Since clustering evaluation is a difficult issue, we
calculated four distinct measures available in the lit-
erature, namely:
? Rand Index (Rand, 1971);
? Adjusted Rand Index (Hubert and Arabie,
1985);
? Jaccard Index (Jaccard, 1901);
? F1 measure (van Rijsbergen, 1979).
The Rand Index (RI) of a clustering C is a mea-
sure of clustering agreement which determines the
percentage of correctly bucketed snippet pairs across
the two clusterings C and G. RI is calculated as fol-
lows:
RI(C,G) =
TP + TN
TP + FP + FN + TN
, (2)
where TP is the number of true positives, i.e., snip-
pet pairs which are in the same cluster both in C and
HHHHHHG
C
C1 C2 ? ? ? Cm Sums
G1 n11 n12 ? ? ? n1m a1
G2 n21 n22 ? ? ? n2m a2
...
...
...
. . .
...
...
Gg ng1 ng2 ? ? ? ngm ag
Sums b1 b2 ? ? ? bm N
Table 3: Contingency table for the clusterings G and C.
G, TN is the number of true negatives, i.e., pairs
which are in different clusters in both clusterings,
and FP and FN are, respectively, the number of false
positives and false negatives. RI ranges between 0
and 1, where 1 indicates perfect correspondence.
Adjusted Rand Index (ARI) is a development of
Rand Index which corrects the RI for chance agree-
ment and makes it vary according to expectaction:
ARI(C,G) =
RI(C,G)? E(RI(C,G))
maxRI(C,G)? E(RI(C,G))
.
(3)
where E(RI(C,G)) is the expected value of the RI.
Using the contingency table reported in Table 3 we
can quantify the degree of overlap between C and G,
where nij denotes the number of snippets in com-
mon between Gi and Cj (namely, nij = |Gi ? Cj |),
ai and bj represent, respectively, the number of snip-
pets inGi and Cj , andN is the total number of snip-
pets, i.e., N = 64. Now, the above equation can be
reformulated as:
ARI(C,G)=
?
ij (nij2 )?[
?
i (ai2 )
?
j (bj2 )]/(
N
2 )
1
2 [
?
i (ai2 )+
?
j (bj2 )]?[
?
i (ai2 )
?
j (bj2 )]/(
N
2 )
.
(4)
The ARI ranges between ?1 and +1 and is 0
when the index equals its expected value.
Jaccard Index (JI) is a measure which takes into
account only the snippet pairs which are in the same
cluster both in C and G, i.e., the true positives (TP),
while neglecting true negatives (TN), which are the
vast majority of cases. JI is calculated as follows:
JI(C,G) =
TP
TP + FP + FN
. (5)
Finally, the F1 measure calculates the harmonic
mean of precision (P) and recall (R). Precision de-
termines how accurately the clusters of C represent
196
the query meanings in the gold standard G, whereas
recall measures how accurately the different mean-
ings in G are covered by the clusters in C. We follow
Crabtree et al (2005) and define the precision of a
cluster Cj ? C as follows:
P (Cj) =
|Csj |
|Cj |
, (6)
whereCsj is the intersection betweenCj ? C and the
gold cluster Gs ? G which maximizes the cardinal-
ity of the intersection. The recall of a query sense s
is instead calculated as:
R(s) =
|
?
Cj?Cs C
s
j |
ns
, (7)
where Cs is the subset of clusters of C whose ma-
jority sense is s, and ns is the number of snippets
tagged with query sense s in the gold standard. The
total precision and recall of the clustering C are then
calculated as:
P =
?
Cj?C P (Cj)|Cj |?
Cj?C |Cj |
; R =
?
s?S R(s)ns?
s?S ns
(8)
where S is the set of senses in the gold standard G
for the given query (i.e., |S| = |G|). The two values
of P and R are then combined into their harmonic
mean, namely the F1 measure:
F1(C,G) =
2PR
P +R
. (9)
3.2 Clustering Diversity
Our second evaluation is aimed at determining the
impact of the output clustering on the diversifica-
tion of the top results shown to a Web user. To
this end, we applied an automatic procedure for flat-
tening the clusterings produced by the participating
systems to a list of search results. Given a clus-
tering C = (C1, C2, . . . , Cm), we add to the ini-
tially empty list the first element of each cluster Cj
(j = 1, . . . ,m); then we iterate the process by se-
lecting the second element of each cluster Cj such
that |Cj | ? 2, and so on. The remaining elements re-
turned by the search engine, but not included in any
cluster of C, are appended to the bottom of the list
in their original order. Note that systems were asked
to sort snippets within clusters, as well as clusters
themselves, by relevance.
Since our goal is to determine how many differ-
ent meanings are covered by the top-ranking search
results according to the output clustering, we used
the measures of S-recall@K (Subtopic recall at rank
K) and S-precision@r (Subtopic precision at recall
r) (Zhai et al, 2003).
S-recall@K determines the ratio of different
meanings for a given query q in the top-K results
returned:
S-recall@K =
|{sense(ri) : i ? {1, . . . ,K}}|
g
,
(10)
where sense(ri) is the gold-standard sense associ-
ated with the i-th snippet returned by the system,
and g is the total number of distinct senses for the
query q in our gold standard.
S-precision@r instead determines the ratio of dif-
ferent senses retrieved for query q in the first Kr
snippets, where Kr is the minimum number of top
results for which the system achieves recall r. The
measure is defined as follows:
S-precision@r =
| ?Kri=1 sense(ri)|
Kr
. (11)
3.3 Baselines
We compared the participating systems with two
simple baselines:
? SINGLETONS: each snippet is clustered as a
separate singleton cluster (i.e., |C| = 64).
? ALL-IN-ONE: all snippets are clustered into a
single cluster (i.e., |C| = 1).
These baselines are important in that they make
explicit the preference of certain quality measures
towards clusterings made up with a small or large
number of clusters.
4 Systems
5 teams submitted 10 systems, out of which 9 were
WSI systems, while 1 was a WSD system, i.e., us-
ing the Wikipedia sense inventory for performing
the disambiguation task. All systems could exploit
the information provided for each search result, i.e.,
URL, page title and result snippet. WSI systems
were requested to use unannotated corpora only.
197
System URLs Snippets Wikipedia YAGO Hierarchy Distr. Thesaurus Other
W
S
I
HDP-CLUSTERS-LEMMA X X
HDP-CLUSTERS-NOLEMMA X X
DULUTH.SYS1.PK2 X
DULUTH.SYS7.PK2 X
DULUTH.SYS9.PK2 Gigaword
UKP-WSI-WP-LLR2 X X X WaCky
UKP-WSI-WP-PMI X X X WaCky
UKP-WSI-WACKY-LLR X X X WaCky
SATTY-APPROACH1 X
WSD RAKESH X DBPedia
Table 4: Resources used for WSI/WSD.
We asked each team to provide information about
their systems. In Table 4 we report the resources
used by each system. The HDP and UKP systems
use Wikipedia as raw text for sampling word counts;
DULUTH-SYS9-PK2 uses the first 10,000 paragraphs
of the Associated Press wire service data from the
English Gigaword Corpus (Graff, 2003, 1st edition),
whereas DULUTH-SYS1-PK2 and DULUTH-SYS7-
PK2 both use the snippets for inducing the query
senses. Finally, the UKP systems were the only ones
to retrieve the Web pages from the corresponding
URLs and exploit them for WSI purposes. They
also use WaCky (Baroni et al, 2009) and a distri-
butional thesaurus obtained from the Leipzig Cor-
pora Collection6 (Biemann et al, 2007). SATTY-
APPROACH1 just uses snippets.
The only participating WSD system, RAKESH,
uses the YAGO hierarchy (Suchanek et al, 2008) to-
gether with DBPedia abstracts (Bizer et al, 2009).
5 Results
We show the results of RI and ARI in Table 5. The
best performing systems are those from the HDP
team, with considerably higher RI and ARI. The
next best systems are SATTY-APPROACH1, which
uses only the words in the snippets, and the only
WSD system, i.e., RAKESH. SINGLETONS perform
well with RI, but badly when chance agreement is
taken into account.
As for F1 and JI, whose values are shown in Table
6, the two HDP systems again perform best in terms
of F1, and are on par with UKP-WSI-WACKY-LLR in
terms of JI. The third best approach in terms of F1
is again SATTY-APPROACH1, which however per-
6http://corpora.uni-leipzig.de/
System RI ARI
W
SI
HDP-CLUSTERS-LEMMA 65.22 21.31
HDP-CLUSTERS-NOLEMMA 64.86 21.49
SATTY-APPROACH1 59.55 7.19
DULUTH.SYS9.PK2 54.63 2.59
DULUTH.SYS1.PK2 52.18 5.74
DULUTH.SYS7.PK2 52.04 6.78
UKP-WSI-WP-LLR2 51.09 3.77
UKP-WSI-WP-PMI 50.50 3.64
UKP-WSI-WACKY-LLR 50.02 2.53
WSD RAKESH 58.76 8.11
B
L SINGLETONS 60.09 0.00
ALL-IN-ONE 39.90 0.00
Table 5: Results for Rand Index (RI) and Adjusted Rand
Index (ARI), sorted by RI.
forms badly in terms of JI. The SINGLETONS base-
line clearly obtains the best F1 performance, but the
worst JI results. The ALL-IN-ONE baseline outper-
forms all other systems with the JI measure, because
TN are not considered, which favours large clusters.
To get more insights into the performance of the
various systems, we calculated the average number
of clusters per clustering produced by each system
and compared it with the gold standard average. We
also computed the average cluster size, i.e., the aver-
age number of snippets per cluster. The statistics are
shown in Table 7. Interestingly, the best performing
systems are those with the cluster number and aver-
age number of clusters closest to the gold standard
ones. This finding is also confirmed by Figure 3,
where we draw each system according to its average
values regarding cluster number and size: again the
distance from the gold standard is meaningful.
We now move to the diversification perfor-
198
System JI F1
W
SI
UKP-WSI-WACKY-LLR 33.94 58.26
HDP-CLUSTERS-NOLEMMA 33.75 68.03
HDP-CLUSTERS-LEMMA 33.02 68.30
DULUTH.SYS1.PK2 31.79 56.83
UKP-WSI-WP-LLR2 31.77 58.64
DULUTH.SYS7.PK2 31.03 58.78
UKP-WSI-WP-PMI 29.32 60.48
DULUTH.SYS9.PK2 22.24 57.02
SATTY-APPROACH1 15.05 67.09
WSD RAKESH 30.52 39.49
B
L SINGLETONS 0.00 100.00
ALL-IN-ONE 39.90 54.42
Table 6: Results for Jaccard Index (JI) and F1 measure.
System # cl. ACS
GOLD STANDARD 7.69 11.56
W
SI
HDP-CLUSTERS-LEMMA 6.63 11.07
HDP-CLUSTERS-NOLEMMA 6.54 11.68
SATTY-APPROACH1 9.90 6.46
UKP-WSI-WP-PMI 5.86 30.30
DULUTH.SYS7.PK2 3.01 25.15
UKP-WSI-WP-LLR2 4.17 21.87
UKP-WSI-WACKY-LLR 3.64 32.34
DULUTH.SYS9.PK2 3.32 19.84
DULUTH.SYS1.PK2 2.53 26.45
WSD RAKESH 9.07 2.94
Table 7: Average number of clusters (# cl.) and average
cluster size (ACS).
5
10
15
20
25
30
35
40
2 4 6 8 10 12
aver
age n
umbe
r of cl
usters
average cluster size (ACS)
gold-standardhdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2rakeshsatty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmi
Figure 3: Average cluster size (ACS) vs. average number
of clusters.
mance, calculated in terms of S-recall@K and S-
precision@r, whose results are shown in Tables 8
System K5 10 20 40
W
SI
HDP-CL.-NOLEMMA 50.80 63.21 79.26 92.48
HDP-CL.-LEMMA 48.13 65.51 78.86 91.68
UKP-WACKY-LLR 41.19 55.41 68.61 83.90
UKP-WP-LLR2 41.07 53.76 68.87 85.87
UKP-WP-PMI 40.45 56.25 68.70 84.92
SATTY-APPROACH1 38.97 48.90 62.72 82.14
DULUTH.SYS7.PK2 38.88 53.79 70.38 86.23
DULUTH.SYS9.PK2 37.15 49.90 68.91 83.65
DULUTH.SYS1.PK2 37.11 53.29 71.24 88.48
WSD RAKESH 46.48 62.36 78.66 90.72
Table 8: S-recall@K.
System r50 60 70 80
W
SI
HDP-CL.-LEMMA 48.85 42.93 35.19 27.62
HDP-CL.-NOLEMMA 48.18 43.88 34.85 29.30
UKP-WP-PMI 42.83 33.40 26.63 22.92
UKP-WACKY-LLR 42.47 31.73 25.39 22.71
UKP-WP-LLR2 42.06 32.04 26.57 22.41
DULUTH.SYS1.PK2 40.08 31.31 26.73 24.51
DULUTH.SYS7.PK2 39.11 30.42 26.54 23.43
DULUTH.SYS9.PK2 35.90 29.72 25.26 21.26
SATTY-APPROACH1 34.94 26.88 23.55 20.40
WSD RAKESH 48.00 39.04 32.72 27.92
Table 9: S-precision@r.
and 9, respectively. Here we find that, again, the
HDP team obtains the best performance, followed by
RAKESH. We note however that not all systems op-
timized the order of clusters and cluster snippets by
relevance.
We also graph the diversification performance
trend of S-recall@K and S-precision@r in Fig-
ures 4 and 5 for K = 1, . . . , 25 and r ?
{40, 50, . . . , 100}.
6 Conclusions and Future Directions
One of the aims of the SemEval-2013 task on Word
Sense Induction & Disambiguation within an End
User Application was to enable an objective compar-
ison of WSI and WSD systems when integrated into
Web search result clustering and diversification. The
task is a hard one, in that it involves clustering, but
provides clear-cut evidence that our end-to-end ap-
plication framework overcomes the limits of previ-
ous in-vitro evaluations. Indeed, the systems which
create good clusters and better diversify search re-
sults, i.e., those from the HDP team, achieve good
performance across all the proposed measures, with
no contradictory evidence.
199
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
5 10 15 20 25
S-re
call-
at-K
K
hdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2satty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmirakesh
Figure 4: S-recall@K.
0.0
0.1
0.2
0.3
0.4
0.5
0.6
40 50 60 70 80 90 100
S-pr
ecis
ion-a
t-r
r
hdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2satty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmirakesh
Figure 5: S-precision@r.
Our annotation experience showed that the
Wikipedia sense inventory, augmented with our
generic classes, is a good choice for semantically
tagging search results, in that it covers most of the
meanings a Web user might be interested in. In fact,
only 20% of the snippets was annotated with the
OTHER class.
Future work might consider large-scale multilin-
gual lexical resources, such as BabelNet (Navigli
and Ponzetto, 2012), both as sense inventory and for
performing the search result clustering and diversi-
fication task.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We thank Antonio Di Marco and David A. Jur-
gens for their help.
200
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
pages 7?12, Prague, Czech Republic.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Andrea Bernardini, Claudio Carpineto, and Massimil-
iano D?Amico. 2009. Full-subtopic retrieval with
keyphrase-based search results clustering. In Proceed-
ings of Web Intelligence 2009, volume 1, pages 206?
213, Los Alamitos, CA, USA.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig corpora collec-
tion - monolingual corpora of standard size. In Pro-
ceedings of Corpus Linguistic 2007, Birmingham, UK.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009. Dbpedia - a crystallization point
for the web of data. J. Web Sem., 7(3):154?165.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selection.
In Proceedings of the 2005 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 172?
178, Washington, DC, USA.
Antonio Di Marco and Roberto Navigli. 2013. Clus-
tering and diversifying web search results with graph-
based word sense induction. Computational Linguis-
tics, 39(4).
Philip Edmonds and Adam Kilgarriff. 2002. Introduc-
tion to the special issue on evaluating word sense dis-
ambiguation systems. Journal of Natural Language
Engineering, 8(4):279?291.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA, USA.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. In Psychological Bulletin,
volume 76, page 378?382.
David Graff. 2003. English Gigaword. In Technical
Report, LDC2003T05, Linguistic Data Consortium,
Philadelphia, PA, USA.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
Partitions. Journal of Classification, 2(1):193?218.
Paul Jaccard. 1901. E?tude comparative de la distribution
florale dans une portion des alpes et des jura. In Bul-
letin de la Socie?te? Vaudoise des Sciences Naturelles,
volume 37, page 547?579.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159?174.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68, Uppsala, Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing
word senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 116?
126, Boston, USA.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli. 2008. A structural approach to the
automatic adjudication of word sense disagreements.
Journal of Natural Language Engineering, 14(4):293?
310.
Roberto Navigli. 2009. Word Sense Disambiguation: a
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A quick tour of word sense
disambiguation, induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science (SOF-
SEM), pages 115?129.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical association, 66(336):846?850.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the 3rd Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text (Senseval-3), pages 41?
43, Barcelona, Spain.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
wikipedia and wordnet. Journal of Web Semantics,
6(3):203?217.
Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Butterworths, second edition.
ChengXiang Zhai, William W. Cohen, and John Lafferty.
2003. Beyond independent relevance: Methods and
evaluation metrics for subtopic retrieval. In Proceed-
ings of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval, pages 10?17, Toronto, Canada.
201
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 222?231, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 12: Multilingual Word Sense Disambiguation
Roberto Navigli, David Jurgens and Daniele Vannella
Dipartimento di Informatica
Sapienza Universita` di Roma
Viale Regina Elena, 295 ? 00161 Roma Italy
{navigli,jurgens,vannella}@di.uniroma1.it
Abstract
This paper presents the SemEval-2013 task on
multilingual Word Sense Disambiguation. We
describe our experience in producing a mul-
tilingual sense-annotated corpus for the task.
The corpus is tagged with BabelNet 1.1.1,
a freely-available multilingual encyclopedic
dictionary and, as a byproduct, WordNet 3.0
and the Wikipedia sense inventory. We present
and analyze the results of participating sys-
tems, and discuss future directions.
1 Introduction
Word Sense Disambiguation (WSD), the task of au-
tomatically assigning predefined meanings to words
occurring in context, is a fundamental task in com-
putational lexical semantics (Navigli, 2009; Navigli,
2012). Several Senseval and SemEval tasks have
been organized in the past to study the performance
and limits of disambiguation systems and, even
more importantly, disambiguation settings. While
an ad-hoc sense inventory was originally chosen for
the first Senseval edition (Kilgarriff, 1998; Kilgarriff
and Palmer, 2000), later tasks (Edmonds and Cot-
ton, 2001; Snyder and Palmer, 2004; Mihalcea et
al., 2004) focused on WordNet (Miller et al, 1990;
Fellbaum, 1998) as a sense inventory. In 2007 the
issue of the fine sense granularity of WordNet was
addressed in two different SemEval disambiguation
tasks, leading to the beneficial creation of coarser-
grained sense inventories from WordNet itself (Nav-
igli et al, 2007) and from OntoNotes (Pradhan et al,
2007).
In recent years, with the exponential growth of
the Web and, consequently, the increase of non-
English speaking surfers, we have witnessed an up-
surge of interest in multilinguality. SemEval-2010
tasks on cross-lingual Word Sense Disambiguation
(Lefever and Hoste, 2010) and cross-lingual lexi-
cal substitution (Mihalcea et al, 2010) were orga-
nized. While these tasks addressed the multilin-
gual aspect of sense-level text understanding, they
departed from the traditional WSD paradigm, i.e.,
the automatic assignment of senses from an existing
inventory, and instead focused on lexical substitu-
tion (McCarthy and Navigli, 2009). The main factor
hampering traditional WSD from going multilingual
was the lack of a freely-available large-scale multi-
lingual dictionary.
The recent availability of huge collaboratively-
built repositories of knowledge such as Wikipedia
has enabled the automated creation of large-scale
lexical knowledge resources (Hovy et al, 2013).
Over the past few years, a wide-coverage multi-
lingual ?encyclopedic? dictionary, called BabelNet,
has been developed (Navigli and Ponzetto, 2012a).
BabelNet1 brings together WordNet and Wikipedia
and provides a multilingual sense inventory that cur-
rently covers 6 languages. We therefore decided to
put the BabelNet 1.1.1 sense inventory to the test
and organize a traditional Word Sense Disambigua-
tion task on a given English test set translated into 4
other languages (namely, French, German, Spanish
and Italian). Not only does BabelNet enable mul-
tilinguality, but it also provides coverage for both
lexicographic (e.g., apple as fruit) and encyclopedic
1http://babelnet.org
222
meanings (e.g., Apple Inc. as company). In this pa-
per we describe our task and disambiguation dataset
and report on the system results.
2 Task Setup
The task required participating systems to annotate
nouns in a test corpus with the most appropriate
sense from the BabelNet sense inventory or, alter-
natively, from two main subsets of it, namely the
WordNet or Wikipedia sense inventories. In contrast
to previous all-words WSD tasks we did not focus
on the other three open classes (i.e., verbs, adjec-
tives and adverbs) since BabelNet does not currently
provide non-English coverage for them.
2.1 Test Corpus
The test set consisted of 13 articles obtained from
the datasets available from the 2010, 2011 and 2012
editions of the workshop on Statistical Machine
Translation (WSMT).2 The articles cover different
domains, ranging from sports to financial news.
The same article was available in 4 different lan-
guages (English, French, German and Spanish). In
order to cover Italian, an Italian native speaker man-
ually translated each article from English into Ital-
ian, with the support of an English mother tongue
advisor. In Table 1 we show for each language the
number of words of running text, together with the
number of multiword expressions and named enti-
ties annotated, from the 13 articles.
2.2 Sense Inventories
2.2.1 BabelNet inventory
To semantically annotate all the single- and multi-
word expressions, as well as the named entities, oc-
curring in our test corpus we used BabelNet 1.1.1
(Navigli and Ponzetto, 2012a). BabelNet is a mul-
tilingual ?encyclopedic dictionary? and a semantic
network currently covering 6 languages, namely:
English, Catalan, French, German, Italian and Span-
ish. BabelNet is obtained as a result of a novel inte-
gration and enrichment methodology. This resource
is created by linking the largest multilingual Web en-
cyclopedia ? i.e., Wikipedia ? to the most popular
computational lexicon ? i.e., WordNet 3.0. The inte-
gration is performed via an automatic mapping and
2http://www.statmt.org/wmt12/
by filling in lexical gaps in resource-poor languages
with the aid of Machine Translation (MT).
Its lexicon includes lemmas which denote both
lexicographic meanings (e.g., balloon) and ency-
clopedic ones (e.g., Montgolfier brothers). The
basic meaning unit in BabelNet is the Babel
synset, modeled after the WordNet synset (Miller
et al, 1990; Fellbaum, 1998). A Babel synset
is a set of synonyms which express a concept
in different languages. For instance, { Globus
aerosta`ticCA, BalloonEN, Ae?rostationFR, BallonDE,
Pallone aerostaticoIT, . . . , Globo aerosta?ticoES } is
the Babel synset for the balloon aerostat, where the
language of each synonym is provided as a subscript
label. Thanks to their multilingual nature, we were
able to use Babel synsets as interlingual concept tags
for nouns occurring within text written in any of the
covered languages.
2.2.2 WordNet and Wikipedia inventories
Since BabelNet 1.1.1 is a superset of the Word-
Net 3.0 and Wikipedia sense inventories,3 once text
is annotated with Babel synsets, it turns out to
be annotated also according to either WordNet or
Wikipedia, or both. In fact, in order to induce the
WordNet annotations, one can restrict to those lex-
ical items annotated with Babel synsets which con-
tain WordNet senses for the target lemma; similarly,
for Wikipedia, we restrict to those items tagged with
Babel synsets which contain Wikipedia pages for the
target lemma.
2.3 BabelNet sense inventory validation
Because BabelNet is an automatic integration of
WordNet and Wikipedia, the resulting Babel synsets
may contain WordNet and Wikipedia entries about
different meanings of the same lemma. The under-
lying cause is a wrong mapping between the two
original resources. For instance, in BabelNet 1.1
the WordNet synset { arsenic, As, atomic number
33 } was mapped to the Wikipedia page AS (RO-
MAN COIN), and therefore the same Babel synset
mixed the two meanings.
In order to avoid an inconsistent semantic tag-
ging of text, we decided to manually check all the
mappings in BabelNet 1.1 between Wikipedia pages
3For version 1.1.1 we used the English Wikipedia database
dump from October 1, 2012.
223
Language Instances Single- Multiword Named Mean senses Mean senses
words expressions Entities per instance per lemma
BabelNet
English 1931 1604 127 200 1.02 1.09
French 1656 1389 89 176 1.05 1.15
German 1467 1267 21 176 1.00 1.05
Italian 1706 1454 211 41 1.22 1.27
Spanish 1481 1103 129 249 1.15 1.19
Wikipedia
English 1242 945 102 195 1.15 1.16
French 1039 790 72 175 1.18 1.14
German 1156 957 21 176 1.07 1.08
Italian 1977 869 85 41 1.20 1.18
Spanish 1103 758 107 248 1.11 1.10
WordNet
English 1644 1502 85 57 1.01 1.10
Table 1: Statistics for the sense annotations of the test set.
and WordNet senses involving lemmas in our En-
glish test set for the task. Overall, we identified 8306
synsets for 978 lemmas to be manually checked. We
recruited 8 annotators in our research group and as-
signed each lemma to two annotators. Each anno-
tator was instructed to check each Babel synset and
determine whether any of the following three opera-
tions was needed:
? Delete a mapping and separate the WordNet
sense from the Wikipedia page (like in the ar-
senic vs. AS (ROMAN COIN) example above);
? Add a mapping between a WordNet sense and a
Wikipedia page (formerly available as two sep-
arate Babel synsets);
? Merge two Babel synsets which express the
same concept.
After disagreement adjudication carried out by
the first author, the number of delete, add and merge
operations was 493, 203 and 43, respectively, for a
total of 739 operations (i.e., 8.8% of synsets cor-
rected). As a result of our validation of BabelNet
1.1, we obtained version 1.1.1, which is currently
available online.
2.4 Sense Annotation
To ensure high quality annotations, the annotation
process was completed in three phases. Because
BabelNet is a superset of both the WordNet and
Wikipedia sense inventories, all annotators used the
BabelNet 1.1.1 sense inventory for their respective
language. These BabelNet annotations were then
projected into WordNet and Wikipedia senses. An-
notation was performed by one native speaker each
for English, French, German and Spanish and, for
Italian, by two native speakers who annotated dif-
ferent subsets of the corpus.
In the first phase, each annotator was instructed
to inspect each instance to check that (1) the lemma
was tagged with the correct part of speech, (2) lem-
mas were correctly annotated as named entity or
multiword expressions, and (3) the meaning of the
instance?s lemma had an associated sense in Ba-
belNet. Based on these criteria, annotators removed
dozens of instances from the original data.
In the second phase, each instance in the En-
glish dataset was annotated using BabelNet senses.
To reduce the time required for annotation in the
other languages, the sense annotations for the En-
glish dataset were then projected onto the other four
224
Language Projected Valid Invalid
instances projections projections
French 1016 791 225
German 592 373 219
Italian 1029 774 255
Spanish 911 669 242
Table 2: Statistics when using the English sense an-
notations to project the correct sense of a lemma in
another language of the sentence-aligned test data.
languages using the sense translation API of Babel-
Net (Navigli and Ponzetto, 2012d). The projection
operated as follows, using the aligned sentences in
the English and non-English texts. For an instance
in the non-English text, all of the senses for that in-
stance?s lemma were compared with the sense an-
notations in the English sentence. If any of that
lemma?s senses was used in the English sentence,
then that sense was selected for the non-English
instance. The matching procedure operates at the
sentence-aligned level because the instances them-
selves are not aligned; i.e., different languages have
different numbers of instances per sentence, which
are potentially ordered differently due to language-
specific construction. Ultimately, this projection la-
beled approximately 50-70% of the instances in the
other four languages. Given the projected senses,
the annotators for the other four languages were then
asked to (1) correct the projected sense labels and
(2) annotate those still without senses.4 These anno-
tations were recorded in text in a stand-off file; no
further annotation tools were used.
The resulting sense projection proved highly use-
ful for selecting the correct sense. Table 2 shows
the number of corrections made by the annotators
to the projected senses, who changed only 22-37%
of the labels. While simple, the projection method
offers significant potential for generating good qual-
ity sense-annotated data from sentence-aligned mul-
tilingual text.
In the third phase, an independent annotator re-
viewed the labels for the high-frequency lemmas for
4During the second phase, annotators were also allowed
to add and remove instances that were missed during the first
phase, which resulted in small number of changes.
all languages to check for systematic errors and dis-
cuss possible changes to the labeling. This review
resulted in only a small number of changes to less
than 5% of the total instances, except for German
which had a slightly higher percentage of changes.
Table 1 summarizes the sense annotation statis-
tics for the test set. Annotators were allowed to use
multiple senses in the case of ambiguity, but en-
couraged to use a single sense whenever possible.
In rare cases, a lemma was annotated with senses
from a different lemma. For example, WordNet does
not contain a sense for ?card? that corresponds to
the penalty card meaning (as used in sports such
as football). In contrast, BabelNet has a sense for
?penalty card? from Wikipedia which, however, is
not mapped to the lemma ?card?. In such cases,
we add both the closest meaning from the original
lemma (e.g., the rectangual piece of paper sense in
WordNet) and the most suitable sense that may have
a different lemma form (e.g., PENALTY CARD).
Previous annotation studies have shown that,
when a fine-grained sense inventory is used, annota-
tors will often label ambiguous instances with multi-
ple senses if allowed (Erk and McCarthy, 2009; Jur-
gens and Klapaftis, 2013). Since BabelNet is a com-
bination of a fine-grained inventory (WordNet) and
contains additional senses from Wikipedia, we ana-
lyzed the average number of BabelNet sense anno-
tations per instance, shown in column six of Table 1.
Surprisingly, Table 1 suggests that the rate of mul-
tiple sense annotation varies significantly between
languages.
BabelNet may combine multiple Wikipedia pages
into a single BabelNet synset. As a result, when
Wikipedia is used as a sense inventory, instances are
annotated with all of the Wikipedia pages associated
with each BabelNet synset. Indeed, Table 1 shows a
markedly increased multi-sense annotation rate for
three languages when using Wikipedia.
As a second analysis, we considered the observed
level of polysemy for each of the unique lemmas.
The last column of Table 1 shows the average num-
ber of different senses seen for each lemma across
the test sets. In all languages, often only a single
sense of a lemma was used. Because the test set is
constructed based on topical documents, infrequent
lemmas mostly occurred within a single document
where they were used with a consistent interpreta-
225
tion. However, we note that in the case of lem-
mas that were only seen with a single sense, this
sense does not always correspond to the most fre-
quent sense as seen in SemCor.
3 Evaluation
Task 12 uses the standard definitions of precision
and recall for WSD evaluation (see, e.g., (Navigli,
2009)). Precision measures the percentage of the
sense assignments provided by the system that are
identical to the gold standard; Recall measures the
percentage of instances that are correctly labeled by
the system. When a system provides sense labels
for all instances, precision and recall are equivalent.
Systems using BabelNet and WordNet senses are
compared against the Most Frequent Sense (MFS)
baseline obtained by using the WordNet most fre-
quent sense. For the Wikipedia sense inventory, we
constructed a pseudo-MFS baseline by selecting (1)
the Wikipedia page associated with the highest rank-
ing WordNet sense, as ranked by SemCor frequency,
or (2) when no synset for a lemma was associ-
ated with a WordNet sense, the first Wikipedia page
sorted using BabelNet?s ordering criteria, i.e., lexi-
cographic sorting. We note that, in the second case,
this procedure frequently selected the page with the
same name as the lemma itself. For instance, the
first sense of Dragon Ball is the cartoon with title
DRAGON BALL, followed by two films (DRAGON
BALL (1990 FILM) and DRAGON BALL EVOLU-
TION).
Systems were scored separately for each sense in-
ventory. We note that because the instances in each
test set are filtered to include only those that can
be labeled with the respective inventory, both the
Wikipedia and WordNet test sets are subsets of the
instances in the BabelNet test set.
4 Participating Systems
Three teams submitted a total of seven systems for
the task, with at least one participant attempting
all of the sense inventory and language combina-
tions. Six systems participated in the WSD task
with BabelNet senses; two teams submitted four sys-
tems using WordNet senses; and one team submitted
three systems for Wikipedia-based senses. Notably,
all systems used graph-based approaches for sense
disambiguation, either using WordNet or BabelNet?s
synset graphs. We summarize the teams? systems as
follows.
DAEBAK! DAEBAK! submitted one system
called PD (Peripheral Diversity) based on BabelNet
path indices from the BabelNet synset graph. Us-
ing a ?5 sentence window around the target word,
a graph is constructed for all senses of co-occurring
lemmas following the procedure proposed by Nav-
igli and Lapata (2010). The final sense is selected
based on measuring connectivity to the synsets of
neighboring lemmas. The MFS is used as a backoff
strategy when no appropriate sense can be picked
out.
GETALP GETALP submitted three systems, two
for BabelNet and one for WordNet, all based on
the ant-colony algorithm of (Schwab et al, 2012),
which uses the sense inventory network structure
to identify paths connecting synsets of the target
lemma to the synsets of other lemmas in context.
The algorithm requires setting several parameters
for the weighting of the structure of the context-
based graph, which vary across the three systems.
The BN1 system optimizes its parameters from the
trial data, while the BN2 and WN1 systems are
completely unsupervised and optimize their param-
eters directly from the structure of the BabelNet and
WordNet graphs.
UMCC-DLSI UMCC-DLSI submitted three
systems based on the ISR-WN resource (Gutie?rrez
et al, 2011), which enriches the WordNet se-
mantic network using edges from multiple lexical
resources, such as WordNet Domains and the
eXtended WordNet. WSD was then performed
using the ISR-WN network in combination with
the algorithm of Gutie?rrez (2012), which is an
extension of the Personalized PageRank algorithm
for WSD (Agirre and Soroa, 2009) which includes
senses frequency. The algorithm requires initial-
izing the PageRank algorithm with a set of seed
synsets (vertices) in the network; this initialization
represents the key variation among UMCC?s three
approaches. The RUN-1 system performs WSD
using all noun instances from the sentence context.
In contrast, the RUN-2 works at the discourse level
and initializes the PageRank using the synsets of all
226
Team System English French German Italian Spanish
DAEBAK! PD 0.604 0.538 0.591 0.613 0.600
GETALP BN-1 0.263 0.261 0.404 0.324 -
GETALP BN-2 0.266 0.257 0.400 0.324 0.371
UMCC-DLSI RUN-1 0.677 0.605 0.618 0.657 0.705
UMCC-DLSI RUN-2 0.685 0.605 0.621 0.658 0.710
UMCC-DLSI RUN-3 0.680 - - - -
MFS 0.665 0.453 0.674 0.575 0.645
Table 3: System performance, reported as F1, for all five languages in the test set when using BabelNet
senses. Top performing systems are marked in bold.
nouns in the document. Finally, the RUN-3 system
initializes using all words in the sentence.
5 Results and Discussion
All teams submitted at least one system using the
BabelNet inventory, shown in Table 3. The UMCC-
DLSI systems were consistently able to outperform
the MFS baseline (a notoriously hard-to-beat heuris-
tic) in all languages except German. Additionally,
the DAEBAK! system outperformed the MFS base-
line on French and Italian. The UMCC-DLSI RUN-
2 system performed the best for all languages. No-
tably, this system leverages the single-sense per dis-
course heuristic (Yarowsky, 1995), which uses the
same sense label for all occurrences of a lemma in a
document.
UMCC-DLSI submitted the only three sys-
tems to use Wikipedia-based senses. Table 4 shows
their performance. Of the three sense inventories,
Wikipedia had the most competitive MFS baseline,
scoring at least 0.694 on all languages. Notably,
the Wikipedia-based system has the lowest recall of
all systems. Despite having superior precision to the
MFS baseline, the low recall brought the resulting
F1 measure below the MFS.
Two teams submitted four total systems for Word-
Net, shown in Table 5. The UMCC-DLSI RUN-2
system was again the top-performing system, under-
scoring the benefit of using discourse information in
selecting senses. The other two UMCC-DLSI sys-
tems also surpassed the MFS baseline. Though still
performing worse than the MFS baseline, when us-
ing the WordNet sense graph, the GETALP system
sees a noticeable improvement of 0.14 over its per-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  5  10  15  20  25  30  35  40  45
WS
D F
1
Number of senses for the instance
DAEBAK! PDGETALP BN-2 UMCC-DLSI Run-2
Figure 1: F1 measure according to the degree of
instance polysemy, reported when at least ten in-
stances have the specified polysemy.
formance on English data when using the WordNet
sense graph.
The disambiguation task encompasses multiple
types of entities. Therefore, we partitioned the Ba-
belNet test data according to the type of instance be-
ing disambiguated; Table 6 highlights the results per
instance type, averaged across all languages.5 Both
multiword expressions and named entities are less
polysemous, resulting in a substantially higher MFS
baseline that no system was able to outperform on
the two classes. However, for instances made of a
single term, both of the UMCC-DLSI systems were
able to outperform the MFS baseline.
BabelNet adds many Wikipedia senses to the ex-
isting WordNet senses, which increases the poly-
5We omit the UMCC-DLSI Run-3 system from analysis, as
it participated in only a single language.
227
English French German Italian Spanish
Team System Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
UMCC-DLSI RUN-1 0.619 0.484 0.543 0.817 0.480 0.605 0.758 0.460 0.572 0.785 0.458 0.578 0.773 0.493 0.602
UMCC-DLSI RUN-2 0.620 0.487 0.546 0.815 0.478 0.603 0.769 0.467 0.581 0.787 0.463 0.583 0.778 0.502 0.610
UMCC-DLSI RUN-3 0.622 0.489 0.548 - - - - - - - - - - - -
MFS 0.860 0.753 0.803 0.698 0.691 0.694 0.836 0.827 0.831 0.833 0.813 0.823 0.830 0.819 0.824
Table 4: The F1 measure for each system across all five languages in the test set when using Wikipedia-based
senses.
Team System Precision Recall F1
GETALP WN-1 0.406 0.406 0.406
UMCC-DLSI RUN-1 0.639 0.635 0.637
UMCC-DLSI RUN-2 0.649 0.645 0.647
UMCC-DLSI RUN-3 0.642 0.639 0.640
MFS 0.630 0.630 0.630
Table 5: System performance when using WordNet senses. Top performing systems are marked in bold.
Team System Single term Multiword expression Named Entity
DAEBAK! PD 0.502 0.801 0.910
GETALP BN-1 0.232 0.724 0.677
GETALP BN-2 0.235 0.740 0.656
UMCC-DLSI RUN-1 0.582 0.806 0.865
UMCC-DLSI RUN-2 0.584 0.809 0.864
MFS 0.511 0.853 0.920
Table 6: System F1 per instance type, averaged across all submitted languages, with the highest system
scores in bold.
English French German Italian Spanish
Team System Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
DAEBAK PD 0.769 0.364 0.494 0.747 0.387 0.510 0.762 0.307 0.438 0.778 0.425 0.550 0.778 0.450 0.570
GETALP BN-2 0.793 0.111 0.195 0.623 0.130 0.215 0.679 0.124 0.210 0.647 0.141 0.231 0.688 0.177 0.282
UMCC-DLSI RUN-1 0.787 0.421 0.549 0.754 0.441 0.557 0.741 0.330 0.457 0.796 0.461 0.584 0.830 0.525 0.643
UMCC-DLSI RUN-2 0.791 0.419 0.548 0.760 0.436 0.554 0.746 0.332 0.460 0.799 0.453 0.578 0.837 0.530 0.649
Table 7: System performance when the system?s annotations are restricted to only those senses that it also
uses in the aligned sentences of at least two other languages.
semy of most instances. As a further analysis, we
consider the relationship between the polysemy of
an instance?s target and system performance. In-
stances were grouped according to the number of
BabelNet senses that their lemma had; following,
systems were scored on each grouping. Figure 1
shows the performance of the best system from each
team on each polysemy-based instance grouping,
with a general trend of performance decay as the
number of senses increases. Indeed, all systems?
performances are negatively correlated with the de-
gree of polysemy, ranging from -0.401 (UMCC-
DLSI RUN-1) to -0.654 (GETALP BN-1) when
measured using Pearson?s correlation. All systems?
228
correlations are significant at p < 0.05.
Last, we note that all systems operated by sense-
annotating each language individually without tak-
ing advantage of either the multilingual structure of
BabelNet or the sentence alignment of the test data.
For example, the sense projection method used to
create the initial set of multilingual annotations on
our test data (cf. Table 2) suggests that the sense
translation API could be used as a reliable source for
estimating the correctness of an annotation; specifi-
cally, given the sense annotations for each language,
the translation API could be used to test whether the
sense is also present in the aligned sentence in the
other languages.
Therefore, we performed a post-hoc analysis of
the benefit of multilingual sense alignment using the
results of the four systems that submitted for all lan-
guages in BabelNet. For each language, we filter
the sense annotations such that an annotation for an
instance is retained only if the system assigned the
same sense to some word in the aligned sentence
from at least two other languages.
Table 7 shows the resulting performance for the
four systems. As expected, the systems exhibit sig-
nificantly lower recall due to omitting all language-
specific instances. However, the resulting precision
is significantly higher than the original performance,
shown in Table 3. Additionally, we analyzed the set
of instances reported for each system and confirmed
that the improvement is not due to selecting only
monosemous lemmas. Despite the GETALP system
having the lower performance of the four systems
when all instances are considered, the system ob-
tains the highest precision for the English dataset.
Furthermore, the UMCC-DLSI systems still obtain
moderate recall, while enjoying 0.106-0.155 abso-
lute improvements in precision across all languages.
While the resulting F1 is lower due to a loss of recall,
we view this result as a solid starting point for other
methods to sense-tag the remaining instances. Over-
all, these results corroborate previous studies sug-
gesting that highly precise sense annotations can be
obtained by leveraging multiple languages (Navigli
and Ponzetto, 2012b; Navigli and Ponzetto, 2012c).
6 Conclusion and Future Directions
Following recent SemEval efforts with word senses
in multilingual settings, we have introduced a new
task on multilingual WSD that uses the recently
released BabelNet 1.1.1 sense inventory. Using a
data set of 13 articles in five languages, all nomi-
nal instances were annotated with BabelNet senses.
Because BabelNet is a superset of WordNet and
Wikipedia, the task also facilitates analysis in those
sense inventories.
Three teams submitted seven systems, with all
systems leveraging the graph-based structure of
WordNet and BabelNet. Several systems were able
to outperform the competitive MFS baseline, except
in the case of Wikipedia, but current performance
leaves significant room for future improvement. In
addition, we believe that future research could lever-
age sense parallelism available in sentence-aligned
multilingual corpora, together with enriched infor-
mation available in future versions of BabelNet. All
of the resources for this task, including the newest
1.1.1 version of BabelNet, were released on the task
website.6
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
A large group of people assisted with SemEval-
2013 Task 12, and without whose help this
task would not have been possible. In particular,
we would like to thank Philipp Cimiano, Maud
Erhmann, Sascha Hinte, Jesu?s Roque Campan?a
Go?mez, and Andreas Soos for their assistance
in sense annotation; our fellow LCL team mem-
bers: Moreno De Vincenzi, Stefano Faralli, Tiziano
Flati, Marc Franco Salvador, Andrea Moro, Silvia
Necs?ulescu, and Taher Pilehvar for their invaluable
assistance in creating BabelNet 1.1.1, preparing and
validating sense annotations, and sense-tagging the
Italian corpus; last, we thank Jim McManus for his
help in producing the Italian test data.
6http://www.cs.york.ac.uk/semeval-2013/
task12/
229
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, Athens, Greece, pages 33?41.
Philip Edmonds and Scott Cotton. 2001. Senseval-2:
Overview. In Proceedings of The Second International
Workshop on Evaluating Word Sense Disambiguation
Systems, pages 1?6, Toulouse, France.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of Empirical Meth-
ods in Natural Language Processing, pages 440?449,
Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Yoan Gutie?rrez, Antonio Ferna?ndez Orqu??n, Sonia
Va?zquez, and Andre?s Montoyo. 2011. Enriching the
integration of semantic resources based on wordnet.
Procesamiento del Lenguaje Natural, 47:249?257.
Yoan Gutie?rrez. 2012. Ana?lisis sema?ntico multidimen-
sional aplicado a la desambiguacio?n del lenguaje nat-
ural. Ph.D. thesis, Universidad de Alicante.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-structured
content and artificial intelligence: The story so far. Ar-
tificial Intelligence, 194:2?27.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation.
Adam Kilgarriff and Martha Palmer. 2000. Introduction
to the special issue on senseval. Computers and the
Humanities, 34(1-2):1?13.
Adam Kilgarriff. 1998. Senseval: An exercise in eval-
uating word sense disambiguation programs. In Pro-
ceedings of the First International Conference on Lan-
guage Resources and Evaluation, pages 1255?1258,
Granada, Spain.
Els Lefever and Veronique Hoste. 2010. Semeval-2010
task 3: Cross-lingual word sense disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 15?20, Uppsala, Sweden.
Association for Computational Linguistics.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of the 3rd International Work-
shop on the Evaluation of Systems for the Semantic
Analysis of Text (SENSEVAL-3) at ACL-04, Barcelona,
Spain, 25?26 July 2004, pages 25?28.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the 5th international workshop
on semantic evaluation, pages 9?14, Uppsala, Sweden.
Association for Computational Linguistics.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235?244.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 32(4):678?
692.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
BabelRelate! a joint multilingual approach to com-
puting semantic relatedness. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence (AAAI), Toronto, Ontario, Canada.
Roberto Navigli and Simone Paolo Ponzetto. 2012c.
Joining forces pays off: Multilingual Joint Word Sense
Disambiguation. In Proceedings of EMNLP-CoNLL,
pages 1399?1410, Jeju Island, Korea.
Roberto Navigli and Simone Paolo Ponzetto. 2012d.
Multilingual WSD with just a few lines of code: the
BabelNet API. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2012), Jeju, Korea.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: Coarse-
grained English all-words task. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, pages 30?
35.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A quick tour of Word Sense
Disambiguation, Induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science (SOF-
SEM), pages 115?129.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech Repub-
lic, pages 87?92.
Didier Schwab, Je?ro?me Goulian, Andon Tchechmedjiev,
and Herve? Blanchon. 2012. Ant colony algorithm for
230
the unsupervised word sense disambiguation of texts:
Comparison and evaluation. In Proceedings of the
24th International Conference on Computational Lin-
guistics (COLING), pages 8?15, Mumbai, India.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Proceedings of ACL 2004
SENSEVAL-3 Workshop, pages 41?43, Barcelona,
Spain.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA, USA.
231
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17?26,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 3: Cross-Level Semantic Similarity
David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{jurgens,pilehvar,navigli}@di.uniroma1.it
Abstract
This paper introduces a new SemEval
task on Cross-Level Semantic Similarity
(CLSS), which measures the degree to
which the meaning of a larger linguistic
item, such as a paragraph, is captured by
a smaller item, such as a sentence. High-
quality data sets were constructed for four
comparison types using multi-stage an-
notation procedures with a graded scale
of similarity. Nineteen teams submitted
38 systems. Most systems surpassed the
baseline performance, with several attain-
ing high performance for multiple com-
parison types. Further, our results show
that comparisons of semantic representa-
tion increase performance beyond what is
possible with text alone.
1 Introduction
Given two linguistic items, semantic similarity
measures the degree to which the two items have
the same meaning. Semantic similarity is an es-
sential component of many applications in Nat-
ural Language Processing (NLP), and similarity
measurements between all types of text as well
as between word senses lend themselves to a va-
riety of NLP tasks such as information retrieval
(Hliaoutakis et al., 2006) or paraphrasing (Glick-
man and Dagan, 2003).
Semantic similarity evaluations have largely fo-
cused on comparing similar types of lexical items.
Most recently, tasks in SemEval (Agirre et al.,
2012) and *SEM (Agirre et al., 2013) have intro-
duced benchmarks for measuring Semantic Tex-
tual Similarity (STS) between similar-sized sen-
tences and phrases. Other data sets such as that
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
of Rubenstein and Goodenough (1965) measure
similarity between word pairs, while the data sets
of Navigli (2006) and Kilgarriff (2001) offer a bi-
nary similar-dissimilar distinction between senses.
Notably, all of these evaluations have focused on
comparisons between a single type, in contrast to
application-based evaluations such as summariza-
tion and compositionality which incorporate tex-
tual items of different sizes, e.g., measuring the
quality of a paragraph?s sentence summarization.
Task 3 introduces a new evaluation where sim-
ilarity is measured between items of different
types: paragraphs, sentences, phrases, words and
senses. Given an item of the lexically-larger type,
a system measures the degree to which the mean-
ing of the larger item is captured in the smaller
type, e.g., comparing a paragraph to a sentence.
We refer to this task as Cross-Level Semantic Sim-
ilarity (CLSS). A major motivation of this task
is to produce semantic similarity systems that are
able to compare all types of text, thereby free-
ing downstream NLP applications from needing to
consider the type of text being compared. Task 3
enables assessing the extent to which the mean-
ing of the sentence ?do u know where i can watch
free older movies online without download?? is
captured in the phrase ?streaming vintage movies
for free?, or how similar is ?circumscribe? to the
phrase ?beating around the bush.? Furthermore,
by incorporating comparisons of a variety of item
sizes, Task 3 unifies in a single task multiple ob-
jectives from different areas of NLP such as para-
phrasing, summarization, and compositionality.
Because CLSS generalizes STS to items of dif-
ferent types, successful CLSS systems can directly
be applied to all STS-based applications. Fur-
thermore, CLSS systems can be used in other
similarity-based applications such as text simpli-
fication (Specia et al., 2012), keyphrase iden-
tification (Kim et al., 2010), lexical substitu-
tion (McCarthy and Navigli, 2009), summariza-
17
tion (Sp?arck Jones, 2007), gloss-to-sense mapping
(Pilehvar and Navigli, 2014b), and modeling the
semantics of multi-word expressions (Marelli et
al., 2014) or polysemous words (Pilehvar and Nav-
igli, 2014a).
Task 3 was designed with three main objectives.
First, the task should include multiple types of
comparison in order to assess each type?s difficulty
and whether specialized resources are needed for
each. Second, the task should incorporate text
from multiple domains and writing styles to en-
sure that system performance is robust across text
types. Third, the similarity methods should be able
to operate at the sense level, thereby potentially
uniting text- and sense-based similarity methods
within a single framework.
2 Task Description
2.1 Objective
Task 3 is intended to serve as an initial task for
evaluating the capabilities of systems at measuring
all types of semantic similarity, independently of
the size of the text. To accomplish this objective,
systems were presented with items from four com-
parison types: (1) paragraph to sentence, (2) sen-
tence to phrase, (3) phrase to word, and (4) word to
sense. Given a pair of items, a system must assess
the degree to which the meaning of the larger item
is captured in the smaller item. WordNet 3.0 was
chosen as the sense inventory (Fellbaum, 1998).
2.2 Rating Scale
Following previous SemEval tasks (Agirre et al.,
2012; Jurgens et al., 2012), Task 3 recognizes that
two items? similarity may fall within a range of
similarity values, rather than having a binary no-
tion of similar or dissimilar. Initially a six-point
(0?5) scale similar to that used in the STS tasks
was considered (Agirre et al., 2012); however, an-
notators found difficulty in deciding between the
lower-similarity options. After multiple revisions
and feedback from a group of initial annotators,
we developed a five-point Likert scale for rating a
pair?s similarity, shown in Table 1.
1
The scale was designed to systematically order
a broad range of semantic relations: synonymy,
similarity, relatedness, topical association, and un-
relatedness. Because items are of different sizes,
the highest rating is defined as very similar rather
1
Annotation materials along with all training and test
data are available on the task website http://alt.qcri.
org/semeval2014/task3/.
than identical to allow for some small loss in the
overall meaning. Furthermore, although the scale
is designed as a Likert scale, annotators were given
flexibility when rating items to use values between
the defined points in the scale, indicating a blend
of two relations. Table 2 provides examples of
pairs for each scale rating for all four comparison
type.
3 Task Data
Though several data sets exist for STS and com-
paring words and senses, no standard data set ex-
ists for CLSS. Therefore, we created a pilot data
set designed to test the capabilities of systems in a
variety of settings. The task data for all compar-
isons but word-to-sense was created using a three-
phase process. First, items of all sizes were se-
lected from publicly-available data sets. Second,
the selected items were used to produce a second
item of the next-smaller level (e.g., a sentence in-
spires a phrase). Third, the pairs of items were
annotated for their similarity. Because of the ex-
pertise required for working with word senses, the
word-to-sense data set was constructed by the or-
ganizers using a separate but similar process. In
the training and test data, each comparison type
had 500 annotated examples, for a total of 2000
pairs each for training and test. We first describe
the corpora used by Task 3 followed by the anno-
tation process. We then describe the construction
of the word-to-sense data set.
3.1 Corpora
Test and training data were constructed by draw-
ing from multiple publicly-available corpora and
then manually generating a paired item for com-
parison. To achieve our second objective for the
task, the data sets used to create item pairs in-
cluded texts from specific domains, social media,
and text with idiomatic or slang language. Table
3 summarizes the corpora and their distribution
across the test and training sets for each compari-
son type, with a high-level description of the genre
of the data. We briefly describe the corpora next.
The WikiNews, Reuters 21578, and Microsoft
Research (MSR) Paraphrase corpora are all drawn
from newswire text, with WikiNews being au-
thored by volunteer writers and the latter two cor-
pora written by professionals. Travel Guides was
drawn from the Berlitz travel guides data in the
Open American National Corpus (Ide and Suder-
man, 2004) and includes very verbose sentences
18
4 ? Very
Similar
The two items have very similar meanings and the most important ideas, concepts, or actions in the larger
text are represented in the smaller text. Some less important information may be missing, but the smaller
text is a very good summary of the larger text.
3 ? Somewhat
Similar
The two items share many of the same important ideas, concepts, or actions, but include slightly different
details. The smaller text may use similar but not identical concepts (e.g., car vs. vehicle), or may omit a
few of the more important ideas present in the larger text.
2 ? Somewhat
related but not
similar
The two items have dissimilar meaning, but share concepts, ideas, and actions that are related. The smaller
text may use related but not necessarily similar concepts (window vs. house) but should still share some
overlapping concepts, ideas, or actions with the larger text.
1 ? Slightly
related
The two items describe dissimilar concepts, ideas and actions, but may share some small details or domain
in common and might be likely to be found together in a longer document on the same topic.
0 ? Unrelated The two items do not mean the same thing and are not on the same topic.
Table 1: The five-point Likert scale used to rate the similarity of item pairs. See Table 2 for examples.
with many named entities. Wikipedia Science
was drawn from articles tagged with the cate-
gory Science on Wikipedia. Food reviews were
drawn from the SNAP Amazon Fine Food Re-
views data set (McAuley and Leskovec, 2013)
and are customer-authored reviews for a variety of
food items. Fables were taken from a collection of
Aesop?s Fables. The Yahoo! Answers corpus was
derived from the Yahoo! Answers data set, which
is a collection of questions and answers from the
Community Question Answering (CQA) site; the
data set is notable for having the highest degree of
ungrammaticality in our test set. SMT Europarl
is a collection of texts from the English-language
proceedings of the European parliament (Koehn,
2005); Europarl data was also used in the PPDB
corpus (Ganitkevitch et al., 2013), from which
phrases were extracted. Wikipedia was used to
generate two phrase data sets from (1) extracting
the definitional portion of an article?s initial sen-
tence, e.g., ?An [article name] is a [definition],?
and (2) captions for an article?s images. Web
queries were gathered from online sources of real-
world queries. Last, the first and second authors
generated slang and idiomatic phrases based on
expressions contained in Wiktionary.
For all comparison types, the test data included
one genre that was not seen in the training data
in order to test the generalizability of the systems
on data from a novel domain. In addition, we
included a new type of challenge genre with Fa-
bles; unlike other domains, the sentences paired
with the fable paragraphs were potentially seman-
tic interpretations of the intent of the fable, i.e.,
the moral of the story. These interpretations often
have little textual overlap with the fable itself and
require a deeper interpretation of the paragraph?s
meaning in order to make the correct similarity
judgment.
Prior to the annotation process, all content was
filtered to ensure its size and format matched the
desired text type. By average, a paragraph in our
dataset consists of 3.8 sentences. Typos and gram-
matical mistakes in the community-produced con-
tent were left unchanged.
3.2 Annotation Process
A two-phase process was used to produce the test
and training data sets for all but word-to-sense.
Phase 1 generates the item pairs from source texts
and Phase 2 rates the pairs? similarity.
Phase 1 In this phase, annotators were shown the
larger text of a comparison type and then asked
to produce the smaller text of the pair at a spec-
ified similarity; for example an annotator may be
shown a paragraph and asked to write a sentence
that is a ?3? rating. Annotators were instructed to
leave the smaller text blank if they had difficulty
understanding the larger text.
The requested similarity ratings were balanced
to create a uniform distribution of similarity val-
ues. Annotators were asked only to generate rat-
ings of 1?4; pairs with a ?0? rating were automat-
ically created by pairing the larger item with ran-
dom selections of text of the appropriate size from
the same corpus. The intent of Phase 1 is to pro-
duce varied item pairs with an expected uniform
distribution of similarity values along the rating
scale.
Four annotators participated in Phase 1 and
were paid a bulk rate of e110 for completing the
work. In addition to the four annotators, the first
two organizers also assisted in Phase 1: Both com-
pleted items from the SCIENTIFIC genre and the
first organizer produced 994 pairs, including all
19
PARAGRAPH TO SENTENCE
Paragraph: Teenagers take aerial shots of their neigh-
bourhood using digital cameras sitting in old bottles which
are launched via kites - a common toy for children liv-
ing in the favelas. They then use GPS-enabled smart-
phones to take pictures of specific danger points - such as
rubbish heaps, which can become a breeding ground for
mosquitoes carrying dengue fever.
Rating Sentence
4 Students use their GPS-enabled cellphones to
take birdview photographs of a land in order
to find specific danger points such as rubbish
heaps.
3 Teenagers are enthusiastic about taking aerial
photograph in order to study their neighbour-
hood.
2 Aerial photography is a great way to identify
terrestrial features that aren?t visible from the
ground level, such as lake contours or river
paths.
1 During the early days of digital SLRs, Canon
was pretty much the undisputed leader in
CMOS image sensor technology.
0 Syrian President Bashar al-Assad tells the US
it will ?pay the price? if it strikes against Syria.
SENTENCE TO PHRASE
Sentence: Schumacher was undoubtedly one of the very
greatest racing drivers there has ever been, a man who was
routinely, on every lap, able to dance on a limit accessible
to almost no-one else.
Rating Phrase
4 the unparalleled greatness of Schumacher?s
driving abilities
3 driving abilities
2 formula one racing
1 north-south highway
0 orthodontic insurance
PHRASE TO WORD
Phrase: loss of air pressure in a tire
Rating Word
4 flat-tire
3 deflation
2 wheel
1 parking
0 butterfly
WORD TO SENSE
Word: automobile
n
Rating Sense
4 car
1
n
(a motor vehicle with four wheels; usually
propelled by an internal combustion engine)
3 vehicle
1
n
(a conveyance that transports people
or objects)
2 bike
1
n
(a motor vehicle with two wheels and a
strong frame)
1 highway
1
n
(a major road for any form of motor
transport)
0 pen
1
n
(a writing implement with a point from
which ink flows)
Table 2: Example pairs and their ratings.
those for the METAPHORIC genre, and those that
the other annotators left blank.
Phase 2 Here, the item pairs produced in Phase
1 were rated for their similarity according to the
scale described in Section 2.2. An initial pilot
study showed that crowdsourcing was only mod-
erately effective for producing these ratings with
high agreement. Furthermore, the texts used in
Task 3 came from a variety of genres, such as
scientific domains, which some workers had dif-
ficulty understanding. While we note that crowd-
sourcing has been used in prior STS tasks for
generating similarity scores (Agirre et al., 2012;
Agirre et al., 2013), both tasks? efforts encoun-
tered lower worker score correlations on some por-
tions of the dataset (Diab, 2013), suggesting that
crowdsourcing may not be reliable for judging the
similarity of certain types of text. See Section 3.5
for additional details.
Therefore, to ensure high quality, the first two
organizers rated all items independently. Because
the sentence-to-phrase and phrase-to-word com-
parisons contain slang and idiomatic language, a
third American English mother tongue annotator
was added for those data sets. The third annotator
was compensated e250 for their assistance.
Annotators were allowed to make finer-grained
distinctions in similarity using multiples of 0.25.
For all items, when any two annotators disagreed
by one or more scale points, we performed an
adjudication to determine the item?s rating in the
gold standard. The adjudication process revealed
that nearly all disagreements were due to annota-
tor mistakes, e.g., where one annotator had over-
looked a part of the text or had misunderstood the
text?s meaning. The final similarity rating for an
unadjudicated item was the average of its ratings.
3.3 Word-to-Sense
Word-to-sense comparison items were generated
in three phases. To increase the diversity and
challenge of the data set, the word-to-sense was
created for four types of words: (1) a word and
its intended meaning are in WordNet, (2) a word
was not in the WordNet vocabulary, e.g., the verb
?zombify,? (3) the word is in WordNet, but has a
novel meaning that is not in WordNet, e.g., the ad-
jective ?red? referring to Communist, and (4) a set
of challenge words where one of the word?s senses
and a second sense are directly connected by an
edge in the WordNet network, but the two senses
are not always highly similar.
20
Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-Word
Corpus Genre Train Test Train Test Train Test
WikiNews Newswire 15.0 10.0 9.2 6.0
Reuters 21578 Newswire 20.2 15.0 5.0
Travel Guides Travel 15.2 10.0 15.0 9.8
Wikipedia Science Scientific ? 25.6 ? 14.8
Food Reviews Review 19.6 20.0
Fables Metaphoric 9.0 5.2
Yahoo! Answers CQA 21.0 14.2 17.6 17.4
SMT Europarl Newswire 35.4 14.4
MSR Paraphrase Newswire 10.0 10.0 8.8 6.0
Idioms Idiomatic 12.8 12.6 20.0 20.0
Slang Slang ? 15.0 ? 25.0
PPDB Newswire 10.0 10.0
Wikipedia Glosses Lexicographic 28.2 17.0
Wikipedia Image Captions Descriptive 23.0 17.0
Web Search Queries Search 5.0 5.0
Table 3: Percentages of the training and test data per source corpus.
In Phase 1, to select the first type of word,
lemmas in WordNet were ranked by frequency
in Wikipedia; the ranking was divided into ten
equally-sized groups, with words sampled evenly
from groups in order to control for word frequency
in the task data. For the second type, words not
present in WordNet were drawn from two sources:
examining words in Wikipedia, which we refer
to as out-of-vocabulary (OOV), and slang words.
For the third type, to identify words with a novel
sense, we examined Wiktionary entries and chose
novel, salient senses that were distinct from those
in WordNet. We refer to words with a novel mean-
ing as out-of-sense (OOS). Words of the fourth
type were chosen by hand. The part-of-speech dis-
tributions for all four types of items were balanced
as 50% noun, 25% verb, 25% adjective.
In Phase 2, each word was associated with a
particular WordNet sense for its intended mean-
ing, or the closest available sense in WordNet
for OOV or OOS items. To select a comparison
sense, we adopted a neighborhood search proce-
dure: All synsets connected by at most three edges
in the WordNet semantic network were shown.
Given a word and its neighborhood, the corre-
sponding sense for the item pair was selected by
matching the sense with an intended similarity for
the pair, much like how text items were gener-
ated in Phase 1. The reason behind using this
neighborhood-based selection process was to min-
imize the potential bias of consistently selecting
lower-similarity items from those further away in
the WordNet semantic network.
In Phase 3, given all word-sense pairs, annota-
tors were shown the definitions associated with the
intended meaning of the word and of the sense.
Definitions were drawn from WordNet or from
Wiktionary, if the word was OOV or OOS. An-
notators had access to the WordNet structure for
the compared sense in order to take into account
its parents and siblings.
3.4 Trial Data
The trial data set was created using a separate
process. Source text was drawn from WikiNews;
we selected the text for the larger item of each
level and then generated the text or sense of the
smaller. A total of 156 items were produced.
After, four fluent annotators independently rated
all items. Inter-annotator agreement rates varied
in 0.734?0.882, using Krippendorff?s ? (Krippen-
dorff, 2004) on the interval scale.
3.5 Data Set Discussion
The resulting annotation process produced a high-
quality data set. First, Table 4 shows the inter-
annotator agreement (IAA) statistics for each
comparison type on both the full and unadjudi-
cated portions of the data set. IAA was measured
using Krippendorff?s ? for interval data. Because
the disagreements that led to lower ? in the full
data were resolved via adjudication, the quality of
the full data set is expected to be on par with that
of the unadjudicated data. The annotation quality
for Task 3 was further improved by manually ad-
judicating all significant disagreements.
In contrast, the data sets of current STS tasks
aggregated data from annotators with moderate
correlation with each other (Diab, 2013); STS-
2012 (Agirre et al., 2012) saw inter-annotator
Pearson correlations of 0.530?0.874 per data set
and STS-2013 (Agirre et al., 2013) had average
21
Training Test
Data All Unadj. All Unadj.
Para.-to-Sent. 0.856 0.916 0.904 0.971
Sent.-to-Phr. 0.773 0.913 0.766 0.980
Phr.-to-Word 0.735 0.895 0.730 0.988
Word-to-Sense 0.681 0.895 0.655 0.952
Table 4: IAA rates for the task data.
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sco
ring
 sc
ale
Paragraph-to-Sentence
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sentence-to-Phrase
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sco
ring
 sc
ale
Phrase-to-Word
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Word-to-Sense
TrainingTest
Figure 1: Similarity ratings distributions.
inter-annotator correlations of 0.377?0.832. How-
ever, we note that Pearson correlation and Krip-
pendorff?s ? are not directly comparable (Artstein
and Poesio, 2008), as annotators? scores may be
correlated, but completely disagree.
Second, the two-phase construction process
produced values that were evenly distributed
across the rating scale, shown in Figure 1 as the
distribution of the values for all data sets. How-
ever, we note that this creation procedure was very
resource intensive and, therefore, semi-automated
or crowdsourcing-based approaches for produc-
ing high-quality data will be needed to expand
the size of the data in future CLSS-based eval-
uations. Nevertheless, as a pilot task, the man-
ual effort was essential for ensuring a rigorously-
constructed data set for the initial evaluation.
4 Evaluation
Participation The ultimate goal of Task 3 is to
produce systems that can measure similarity for
multiple types of items. Therefore, we strongly
encouraged participating teams to submit systems
that were capable of generating similarity judg-
ments for multiple comparison types. However,
to further the analysis, participants were also per-
mitted to submit systems specialized to a single
domain. Teams were allowed at most three system
submissions, regardless of the number of compar-
ison types supported.
Scoring Systems were required to provide sim-
ilarity values for all items within a comparison
type. Following prior STS evaluations, systems
were scored for each comparison type using Pear-
son correlation. Additionally, we include a second
score using Spearman?s rank correlation, which is
only affected by differences in the ranking of items
by similarity, rather than differences in the similar-
ity values. Pearson correlation was chosen as the
official evaluation metric since the goal of the task
is to produce similar scores. However, Spearman?s
rank correlation provides an important metric for
assessing systems whose scores do not match hu-
man scores but whose rankings might, e.g., string-
similarity measures. Ultimately, a global ranking
was produced by ordering systems by the sum of
their Pearson correlation values for each of the
four comparison levels.
Baselines The official baseline system was
based on the Longest Common Substring (LCS),
normalized by the length of items using the
method of Clough and Stevenson (2011). Given
a pair, the similarity is reported as the normalized
length of the LCS. In the case of word-to-sense,
the LCS for a word-sense pair is measured be-
tween the sense?s definition in WordNet and the
definitions of each sense of the pair?s word, report-
ing the maximal LCS. Because OOV and slang
words are not in WordNet, the baseline reports the
average similarity value of non-OOV items. Base-
line scores were made public after the evaluation
period ended.
Because LCS is a simple procedure, a second
baseline based on Greedy String Tiling (GST)
(Wise, 1996) was added after the evaluation pe-
riod concluded. Unlike LCS, GST better handles
the transpositions of tokens across the two texts
and can still report high similarity when encoun-
tering reordered text. The minimum match length
for GST was set to 6.
5 Results
Nineteen teams submitted 38 systems. Of those
systems, 34 produced values for paragraph-to-
sentence and sentence-to-phrase comparisons, 22
for phrase-to-word, and 20 for word-to-sense.
Two teams submitted revised scores for their sys-
tems after the deadline but before the test set had
22
Team System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank Spearman Rank
Meerkat Mafia pairingWords? 0.794 0.704 0.457 0.389
SimCompass run1 0.811 0.742 0.415 0.356 1 1
ECNU run1 0.834 0.771 0.315 0.269 2 2
UNAL-NLP run2 0.837 0.738 0.274 0.256 3 6
SemantiKLUE run1 0.817 0.754 0.215 0.314 4 4
UNAL-NLP run1 0.817 0.739 0.252 0.249 5 7
UNIBA run2 0.784 0.734 0.255 0.180 6 8
RTM-DCU run1? 0.845 0.750 0.305
UNIBA run1 0.769 0.729 0.229 0.165 7 10
UNIBA run3 0.769 0.729 0.229 0.165 8 11
BUAP run1 0.805 0.714 0.162 0.201 9 13
BUAP run2 0.805 0.714 0.142 0.194 10 9
Meerkat Mafia pairingWords 0.794 0.704 -0.044 0.389 11 12
HULTECH run1 0.693 0.665 0.254 0.150 12 16
GST Baseline 0.728 0.662 0.146 0.185
HULTECH run3 0.669 0.671 0.232 0.137 13 15
RTM-DCU run2? 0.785 0.698 0.221
RTM-DCU run3 0.780 0.677 0.208 14 17
HULTECH run2 0.667 0.633 0.180 0.169 15 14
RTM-DCU run1 0.786 0.666 0.171 16 18
RTM-DCU run3? 0.786 0.663 0.171
Meerkat Mafia SuperSaiyan 0.834 0.777 17 19
Meerkat Mafia Hulk2 0.826 0.705 18 20
RTM-DCU run2 0.747 0.588 0.164 19 22
FBK-TR run3 0.759 0.702 20 23
FBK-TR run1 0.751 0.685 21 24
FBK-TR run2 0.770 0.648 22 25
Duluth Duluth2 0.501 0.450 0.241 0.219 23 21
AI-KU run1 0.732 0.680 24 26
LCS Baseline 0.527 0.562 0.165 0.109
UNAL-NLP run3 0.708 0.620 25 27
AI-KU run2 0.698 0.617 26 28
TCDSCSS run2 0.607 0.552 27 29
JU-Evora run1 0.536 0.442 0.090 0.091 28 31
TCDSCSS run1 0.575 0.541 29 30
Duluth Duluth1 0.458 0.440 0.075 0.076 30 5
Duluth Duluth3 0.455 0.426 0.075 0.079 31 3
OPI run1 0.433 0.213 0.152 32 36
SSMT run1 0.789 33 34
DIT run1 0.785 34 32
DIT run2 0.784 35 33
UMCC DLSI SelSim run1 0.760 36 35
UMCC DLSI SelSim run2 0.698 37 37
UMCC DLSI Prob run1 0.023 38 38
Table 5: Task results. Systems marked with a ? were submitted after the deadline but are positioned
where they would have ranked.
been released. These systems were scored and
noted in the results but were not included in the
official ranking.
Table 5 shows the performance of the participat-
ing systems across all the four comparison types in
terms of Pearson correlation. The two right-most
columns show system rankings by Pearson (Offi-
cial Rank) and Spearman?s ranks correlation.
The SimCompass system attained first place,
partially due to its superior performance on
phrase-to-word comparisons, providing an im-
provement of 0.10 over the second-best sys-
tem. The late-submitted version of the Meerkat
Mafia pairingWords? system corrected a bug in
the phrase-to-word comparison, which ultimately
would have attained first place due to large per-
formance improvements over SimCompass on
phrase-to-word and word-to-sense. ENCU and
UNAL-NLP systems rank respectively second and
third while the former being always in top-4 and
the latter being among the top-7 systems across the
four comparison types. Most systems were able
to surpass the naive LCS baseline; however, the
more sophisticated GST baseline (which accounts
for text transposition) outperforms two-thirds of
the systems. Importantly, both baselines perform
23
poorly on smaller text, highlighting the impor-
tance of performing a semantic comparison, as op-
posed to a string-based one.
Within the individual comparison types, spe-
cialized systems performed well for the larger
text sizes. In the paragraph-to-sentence type, the
run1 system of UNAL-NLP provides the best of-
ficial result, with the late RTM-DCU run1? sys-
tem surpassing its performance slightly. Meerkat
Mafia provides the best performance in sentence-
to-phrase with its SuperSaiyan system and the
best performances in phrase-to-word and word-to-
sense with its late pairingWords? system.
Comparison-Type Analysis Performance
across the comparison types varied considerably,
with systems performing best on comparisons
between longer textual items. As a general trend,
both the baselines? and systems? performances
tend to decrease with the size of lexical items
in the comparison types. A main contributing
factor to this is the reliance on textual similarity
measures (such as the baselines), which perform
well when two items? may share content. How-
ever, as the items? content becomes smaller, e.g.,
a word or phrase, the textual similarity does not
necessarily provide a meaningful indication of
the semantic similarity between the two. This
performance discrepancy suggests that, in order
to perform well, CLSS systems must rely on
comparisons between semantic representations
rather than textual representations. The two
top-performing systems on these smaller levels,
Meerkat Mafia and SimCompass, used additional
resources beyond WordNet to expand a word or
sense to its definition or to represent words with
distributional representations.
Per-genre results and discussions Task 3 in-
cludes multiple genres within the data set for each
comparison type. Figure 2 shows the correlation
of each system for each of these genres, with sys-
tems ordered left to right according to their official
ranking in Table 5. An interesting observation is
that a system?s official rank does not always match
the rank from aggregating its correlations for each
genre individually. This difference suggests that
some systems provided good similarity judgments
on individual genres, but their range of similarity
values was not consistent between genres leading
to lower overall Pearson correlation. For instance,
in the phrase-to-word comparison type, the ag-
gregated per-genre performance of Duluth-1 and
Duluth-3 are among the best whereas their over-
all Pearson performance puts these systems among
the worst-performing ones in the comparison type.
Among the genres, CQA, SLANG, and ID-
IOMATIC prove to be the more difficult for sys-
tems to interpret and judge. These genres in-
cluded misspelled, colloquial, or slang language
which required converting the text into semantic
form in order to meaningfully compare it. Fur-
thermore, as expected, the METAPHORIC genre
was the most difficult, with no system perform-
ing well; we view the METAPHORIC genre as an
open challenge for future systems to address when
interpreting larger text. On the other hand, SCI-
ENTIFIC, TRAVEL, and NEWSWIRE tend to be
the easiest genres for paragraph-to-sentence and
sentence-to-phrase. All three genres tend to in-
clude many named entities or highly-specific lan-
guage, which are likely to be more preserved in the
more-similar paired items. Similarly, DESCRIP-
TIVE and SEARCH genres were easiest in phrase-
to-word, which also often featured specific words
that were preserved in highly-similar pairs. In
the case of word-to-sense, REGULAR proves to be
the least difficult genre. Interestingly, in word-
to-sense, most systems attained moderate perfor-
mance for comparisons with words not in Word-
Net (i.e., OOV) but had poor performance for
slang words, which were also OOV. This differ-
ence suggests that systems could be improved with
additional semantic resources for slang.
Spearman Rank Analysis Although the goal of
Task 3 is to have systems produce similarity judg-
ments, some applications may benefit from simply
having a ranking of pairs, e.g., ranking summa-
rizations by goodness. The Spearman rank corre-
lation measures the ability of systems to perform
such a ranking. Surprisingly, with the Spearman-
based ranking, the Duluth1 and Duluth3 systems
attain the third and fifth ranks ? despite being
among the lowest ranked with Pearson. Both sys-
tems were unsupervised and produced similarity
values that did not correlate well with those of
humans. However, their Spearman ranks demon-
strate the systems ability to correctly identify rela-
tive similarity and suggests that such unsupervised
systems could improve their Pearson correlation
by using the training data to tune the range of sim-
ilarity values to match those of humans.
24
 0
 1
 2
 3
 4
 5
UNAL-NLP-2
ECNU-1
Meerkat_Mafia-SS
Meerkat_Mafia-H
SemantiKLUE-1
UNAL-NLP-1
SimCompass-1
BUAP-1
BUAP-2
Meerkat_Mafia-PW
SSMT-1
RTM-DCU-1
DIT-1DIT-2UNIBA-2
RTM-DCU-3
FBK-TR-2
UNIBA-1
UNIBA-3
FBK-TR-3
FBK-TR-1
RTM-DCU-2
AI-KU-1
UNAL-NLP-3
AI-KU-2
HULTECH-1
HULTECH-3
HULTECH-2
TCDSCSS-2
TCDSCSS-1
JU-Evora-1
Duluth-2
Duluth-1
Duluth-3
(a)
 Pa
rag
rap
h-t
o-S
en
ten
ce
  
Co
rre
lat
ion
s
pe
r g
en
re
CQAReview
TravelNewswire
ScientificMetaphoric
 0
 1
 2
 3
 4
 5
Meerkat_Mafia-SS
ECNU-1
UMCC_DLSI_SelSim-1
SemantiKLUE-1
SimCompass-1
UNAL-NLP-1
UNAL-NLP-2
UNIBA-2
UNIBA-1
UNIBA-3
BUAP-1
BUAP-2
Meerkat_Mafia-H
Meerkat_Mafia-PW
FBK-TR-3
UMCC_DLSI_SelSim-2
FBK-TR-1
AI-KU-1
RTM-DCU-3
HULTECH-3
RTM-DCU-1
HULTECH-1
FBK-TR-2
HULTECH-2
UNAL-NLP-3
AI-KU-2
RTM-DCU-2
TCDSCSS-2
TCDSCSS-1
Duluth-2
JU-Evora-1
Duluth-1
OPI-1
Duluth-3
(b)
 Se
nte
nc
e-t
o-P
hra
se
    
Co
rre
lat
ion
s
pe
r g
en
re
ScientificCQA
IdiomaticSlang
TravelNewswire
 0
 1
 2
 3
SimCompass-1
ECNU-1
UNAL-NLP-2
UNIBA-2
HULTECH-1
UNAL-NLP-1
Duluth-2
HULTECH-3
UNIBA-1
UNIBA-3
SemantiKLUE-1
OPI-1
RTM-DCU-3
HULTECH-2
RTM-DCU-1
RTM-DCU-2
BUAP-1
BUAP-2
JU-Evora-1
Duluth-1
Duluth-3
Meerkat_Mafia-PW
(c)
 Ph
ras
e-t
o-W
ord
    
  
Co
rre
lat
ion
s
pe
r g
en
re
SlangNewswire
IdiomaticDescriptive
LexicographicSearch
 0
 1
 2
Meerkat_Mafia-PW
SimCompass-1
SemantiKLUE-1
ECNU-1
UNAL-NLP-2
UNAL-NLP-1
Duluth-2
BUAP-1
BUAP-2
UNIBA-2
HULTECH-2
UNIBA-1
UNIBA-3
OPI-1
HULTECH-1
HULTECH-3
JU-Evora-1
Duluth-3
Duluth-1
UMCC_DLSI_Prob-1
(d)
 W
ord
-to
-Se
ns
e
Co
rre
lat
ion
s
pe
r g
en
re
OOSOOV
regularregular-challenge
Slang
Figure 2: A stacked histogram for each system, showing its Pearson correlations for genre-specific por-
tions of the gold-standard data, which may also be negative.
6 Conclusion
This paper introduces a new similarity task, Cross-
Level Semantic Similarity, for measuring the se-
mantic similarity of lexical items of different
sizes. Using a multi-phase annotation proce-
dure, we have produced a high-quality data set of
4000 items comprising of various genres, evenly-
split between training and test with four types of
comparison: paragraph-to-sentence, sentence-to-
phrase, phrase-to-word, and word-to-sense. Nine-
teen teams submitted 38 systems, with most teams
surpassing the baseline system and several sys-
tems achieving high performance in multiple types
of comparison. However, a clear performance
trend emerged where systems perform well only
when the text itself is similar, rather than its under-
lying meaning. Nevertheless, the results of Task 3
are highly encouraging and point to clear future
objectives for developing CLSS systems that op-
erate on more semantic representations rather than
text. In future work on CLSS evaluation, we first
intend to develop scalable annotation methods to
increase the data sets. Second, we plan to add new
evaluations where systems are tested according to
their performance in an application related to each
comparison-type, such as measuring the quality of
a paraphrase or summary.
Acknowledgments
We would like to thank Tiziano Flati, Marc Franco Salvador,
Maud Erhmann, and Andrea Moro for their help in preparing
the trial data; Gaby Ford, Chelsea Smith, and Eve Atkinson
for their help in generating the training and test data; and
Amy Templin for her help in generating and rating the train-
ing and test data.
The authors gratefully acknowledge the
support of the ERC Starting Grant Multi-
JEDI No. 259234.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval-2012), pages
385?393, Montr?eal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on typed-
similarity. In Proceedings of the Second Joint Confer-
25
ence on Lexical and Computational Semantics (*SEM),
Atlanta, Georgia.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational Lin-
guistics, 34(4):555?596.
Paul Clough and Mark Stevenson. 2011. Developing a cor-
pus of plagiarised short answers. Language Resources
and Evaluation, 45(1):5?24.
Mona Diab. 2013. Semantic textual similarity: past present
and future. In Joint Symposium on Semantic Process-
ing. Keynote address. http://jssp2013.fbk.eu/
sites/jssp2013.fbk.eu/files/Mona.pdf.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-
Burch. 2013. PPDB: The paraphrase database. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-HLT),
pages 758?764, Atlanta, Georgia.
Oren Glickman and Ido Dagan. 2003. Acquiring lexical
paraphrases from a single corpus. In Proceedings of the
International Conference on Recent Advances in Natural
Language Processing (RANLP), pages 81?90, Borovets,
Bulgaria.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,
Euripides GM Petrakis, and Evangelos Milios. 2006.
Information retrieval by semantic similarity. Interna-
tional Journal on Semantic Web and Information Systems,
2(3):55?73.
Nancy Ide and K. Suderman. 2004. The American Na-
tional Corpus First Release. In Proceedings of the 4
th
Language Resources and Evaluation Conference (LREC),
pages 1681?1684, Lisbon, Portugal.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
Degrees of Relational Similarity. In Proceedings of
the 6th International Workshop on Semantic Evaluation
(SemEval-2012), pages 356?364, Montr?eal, Canada.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In The Proceedings of the Second International
Workshop on Evaluating Word Sense Disambiguation Sys-
tems (SENSEVAL-2), pages 17?20, Toulouse, France.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010. SemEval-2010 Task 5: Automatic
Keyphrase Extraction from Scientific Articles. In Pro-
ceedings of the 5th International Workshop on Semantic
Evaluation (SemEval-2010), pages 21?26, Los Angeles,
California.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of Machine
Translation Summit X, pages 79?86, Phuket, Thailand.
Klaus Krippendorff. 2004. Content Analysis: An Introduc-
tion to Its Methodology. Sage, Thousand Oaks, CA, sec-
ond edition.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Ben-
tivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014.
SemEval-2014 Task 1: Evaluation of compositional dis-
tributional semantic models on full sentences through se-
mantic relatedness and textual entailment. In Proceedings
of the 8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Julian John McAuley and Jure Leskovec. 2013. From ama-
teurs to connoisseurs: modeling the evolution of user ex-
pertise through online reviews. In Proceedings of the 22nd
International Conference on World Wide Web (WWW),
pages 897?908, Rio de Janeiro, Brazil.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Evalu-
ation, 43(2):139?159.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost Word Sense Disambiguation performance. In
Proceedings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics (COLING-
ACL), pages 105?112, Sydney, Australia.
Mohammad Taher Pilehvar and Roberto Navigli. 2014a. A
large-scale pseudoword-based evaluation framework for
state-of-the-art Word Sense Disambiguation. Computa-
tional Linguistics, 40(4).
Mohammad Taher Pilehvar and Roberto Navigli. 2014b.
A robust approach to aligning heterogeneous lexical re-
sources. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, pages 468?
478, Baltimore, USA.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Karen Sp?arck Jones. 2007. Automatic summarising: The
state of the art. Information Processing and Management,
43(6):1449?1481.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.
2012. SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval-2012), pages 347?355.
Michael J. Wise. 1996. YAP3: Improved detection of simi-
larities in computer program and other texts. In Proceed-
ings of the twenty-seventh SIGCSE technical symposium
on Computer science education, SIGCSE ?96, pages 130?
134, Philadelphia, Pennsylvania, USA.
26
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 75?76,
Dublin, Ireland, August 23, 2014.
(Digital) Goodies from the ERC Wishing Well: BabelNet, Babelfy, Video
Games with a Purpose and the Wikipedia Bitaxonomy
Roberto Navigli
Dipartimento di Informatica
Sapienza Universit`a di Roma
Viale Regina Elena, 295 ? 00166 Roma Italy
navigli@di.uniroma1.it
Abstract
Multilinguality is a key feature of today?s Web, and it is this feature that we leverage and exploit
in our research work at the Sapienza University of Rome?s Linguistic Computing Laboratory,
which I am going to overview and showcase in this talk.
I will start by presenting BabelNet 2.5 (Navigli and Ponzetto, 2012), available at
http://babelnet.org, a very large multilingual encyclopedic dictionary and semantic net-
work, which covers 50 languages and provides both lexicographic and encyclopedic knowledge
for all the open-class parts of speech, thanks to the seamless integration of WordNet, Wikipedia,
Wiktionary, OmegaWiki, Wikidata and the Open Multilingual WordNet. In order to construct
the BabelNet network, we extract at different stages: from WordNet, all available word senses
(as concepts) and all the lexical and semantic pointers between synsets (as relations); from
Wikipedia, all the Wikipages (i.e., Wikipages, as concepts) and semantically unspecified rela-
tions from their hyperlinks. WordNet and Wikipedia overlap both in terms of concepts and
relations: this overlap makes the merging between the two resources possible, enabling the cre-
ation of a unified knowledge resource. In order to enable multilinguality, we collect the lexical
realizations of the available concepts in different languages. Finally, we connect the multilingual
Babel synsets by establishing semantic relations between them.
Next, I will present Babelfy (Moro et al., 2014), available at http://babelfy.org, a unified
approach that leverages BabelNet to perform Word Sense Disambiguation (WSD) and Entity
Linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those
of task-specific state-of-the-art supervised systems. Babelfy works in three steps: first, given a
lexicalized semantic network, we associate with each vertex, i.e., either concept or named entity,
a semantic signature, that is, a set of related vertices. This is a preliminary step which needs
to be performed only once, independently of the input text. Second, given a text, we extract all
the linkable fragments from this text and, for each of them, list the possible meanings according
to the semantic network. Third, we create a graph-based semantic interpretation of the whole
text by linking the candidate meanings of the extracted fragments using the previously-computed
semantic signatures. We then extract a dense subgraph of this representation and select the best
candidate meaning for each fragment. Our experiments show state-of-the-art performances on
both WSD and EL on 6 different datasets, including a multilingual setting.
In the third part of the talk I will present two novel approaches to large-scale knowledge acqui-
sition and validation developed in my lab. I will first introduce video games with a purpose
(Vannella et al., 2014), a novel, powerful paradigm for the large scale acquisition and validation
of knowledge and data (http://knowledgeforge.org). We demonstrate that converting
games with a purpose into more traditional video games provides a fun component that moti-
vates players to annotate for free, thereby significantly lowering annotation costs below that of
crowdsourcing. Moreover, we show that video games with a purpose produce higher-quality
annotations than crowdsourcing.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
75
Then I will introduce the Wikipedia Bitaxonomy (Flati et al., 2014, WiBi), available at
http://wibitaxonomy.org and now integrated into BabelNet. WiBi is the largest and
most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories,
aligned to each other. WiBi is created in three steps: we first create a taxonomy for the Wikipedia
pages by parsing textual definitions, extracting the hypernym(s) and disambiguating them accord-
ing to the page inventory; next, we leverage the hypernyms in the page taxonomy, together with
their links to the corresponding categories, so as to induce a taxonomy over Wikipedia categories
while at the same time improving the page taxonomy in an iterative way; finally we employ
structural heuristics to overcome inherent problems affecting categories. The output of our three-
phase approach is a bitaxonomy of millions of pages and hundreds of thousands of categories for
the English Wikipedia.
Acknowledgements
The author gratefully acknowledges the support of the ERC Starting Grant Multi-
JEDI No. 259234.
References
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and Roberto Navigli. 2014. Validating and Extending Semantic
Knowledge Bases using Video Games with a Purpose. In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (ACL 2014), pages 945?955, Baltimore, USA.
Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity linking meets word sense disambiguation:
a unified approach. Transactions of the Association for Computational Linguistics (TACL), 2:231?244.
Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217?250.
Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani, and Roberto Navigli. 2014. Validating and
Extending Semantic Knowledge Bases using Video Games with a Purpose. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguistics (ACL 2014), pages 1294?1304, Baltimore, USA.
76
