Dependency Annotation Scheme for Indian Languages 
 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra 
Sharma, Lakshmi Bai and Rajeev Sangal 
Language Technologies Research Center, 
IIIT, Hyderabad, India. 
{rafiya,samar}@research.iiit.ac.in, 
{dipti,lakshmi,sangal}@iiit.ac.in 
 
 
Abstract 
The paper introduces a dependency annota-
tion effort which aims to fully annotate a 
million word Hindi corpus. It is the first at-
tempt of its kind to develop a large scale 
tree-bank for an Indian language. In this 
paper we provide the motivation for fol-
lowing the Paninian framework as the an-
notation scheme and argue that the Pan-
inian framework is better suited to model 
the various linguistic phenomena manifest 
in Indian languages. We present the basic 
annotation scheme. We also show how the 
scheme handles some phenomenon such as 
complex verbs, ellipses, etc. Empirical re-
sults of some experiments done on the cur-
rently annotated sentences are also re-
ported. 
1 Introduction 
A major effort is currently underway to develop a 
large scale tree bank for Indian Languages (IL). 
The lack of such a resource has been a major limit-
ing factor in the development of good natural lan-
guage tools and applications for ILs. Apart from 
that, a rich and large-scale tree bank can be an in-
dispensable resource for linguistic investigations. 
Some notable efforts in this direction for other lan-
guages have been the Penn Tree Bank (Marcus et 
al., 1993) for English and the Prague Dependency 
Bank (Hajicova, 1998) for Czech.  
It is well known that context free grammar 
(CFG) is not well-suited for free-word order 
languages (Shieber, 1985); instead dependency 
framework appears to be better suited (Hudson, 
1984; Mel'Cuk, 1988, Bharati et al, 1995). Also, 
the dependency framework is arguably closer to 
semantics than the phrase structure grammar (PSG) 
if the dependency relations are judiciously chosen. 
In recent times many research groups have been 
shifting to the dependency paradigm due to this 
reason. Modern dependency grammar is attributed 
to Tesni?re (1959). In a dependency analysis, there 
is no hierarchical arrangement of phrases (or 
substrings) like in phrase structure grammar. 
Rather, we just have words connected via 
dependency relations between them.  
Prague Dependency Bank (PDT) for Czech 
(which has relatively free word order) is one such 
large-scale effort which implements a three-tier 
annotation scheme and annotates morphological 
information, analytical and tectogrammatical level 
annotations at these three levels. Out of the three 
levels, the analytical and tectogrammatical level 
are dependency based. The tectogrammatical level 
tries to capture the deep-semantics of the sentence; 
the annotation at this level is very rich and is 
linked to the other two lower levels. Other major 
efforts in the dependency framework are Alpino 
(van der Beek et. al, 2002) for Dutch, (Rambow et. 
al, 2002) for English, TUT (Bosco and Lombardo, 
2004) for Italian, TIGER (Brants et. al, 2002) 
(combines dependency with PSG) for German. In 
this paper we describe an approach to annotate ILs 
using the Paninian1 model. The paper is arranged 
as follows, Section 2 gives a brief overview of the  
           
          1Paninian theory was formulated by Panini about 
two thousand five hundred years ago for Sanskrit. It 
evolved with the contributions of grammarians that fol-
lowed. 
721
grammatical model and the motivation for 
following the framework. Section 3 talks about the 
chosen corpus and the annotation procedure. In 
Section 4 we discuss some dependency relations. 
Section 5 describes the evaluation procedure. We 
report the empirical results of experiments done on 
the annotated data in Section 6. Section 7, 
concludes the paper. 
2 Grammatical Model 
ILs are morphologically rich and have a relatively 
flexible word order. For such languages syntactic 
subject-object positions are not always able to ele-
gantly explain the varied linguistic phenomena. In 
fact, there is a debate in the literature whether the 
notions ?subject? and ?object? can at all be defined 
for ILs (Mohanan, 1982). Behavioral properties are 
the only criteria based on which one can confi-
dently identify grammatical functions in Hindi 
(Mohanan, 1994); it can be difficult to exploit such 
properties computationally. Marking semantic 
properties such as thematic role as dependency 
relation is also problematic. Thematic roles are 
abstract notions and will require higher semantic 
features which are difficult to formulate and to ex-
tract as well. So, thematic roles are not marked at 
this juncture. On the other hand, the notion of ka-
raka relations (explained shortly) provides us a 
level which while being syntactically grounded 
also helps in capturing some semantics. What is 
important to note here is that such a level can be 
exploited computationally with ease. This provides 
us with just the right level of syntactico-semantic 
interface. The experiments conducted on the pre-
sent annotated text provide empirical evidence for 
this claim (section 6). Paninian grammar is basi-
cally a dependency grammar (Kiparsky and Staal, 
1969; Shastri, 1973). In this section we briefly dis-
cuss the Paninian model for ILs and lay down 
some basic concepts inherent to this framework. 
The main problem that the Paninian approach 
addresses is to identify syntactico-semantic rela-
tions in a sentence. The Paninian approach treats a 
sentence as a series of modifier-modified relations. 
A sentence is supposed to have a primary modified 
(the root of the dependency tree) which is gener-
ally the main verb of the sentence. The elements 
modifying the verb participate in the action speci-
fied by the verb. The participant relations with the 
verb are called karaka. The appropriate mapping of 
the syntactic cues helps in identifying the appro-
priate karakas (?participants in an action?). The 
framework is inspired by an inflectionally rich lan-
guage like Sanskrit; it emphasizes the role of case 
endings or markers such as post-positions and ver-
bal inflections. 
There are six basic karakas, namely; adhikarana 
?location?,  apaadaan ?source?, sampradaan 
?recipient?,  karana ?instrument?, karma ?theme?, 
karta ?agent?. We must note here that although one 
can roughly map the first four karaka to their the-
matic role counterpart, karma and karta are very 
different from ?theme? and ?agent? respectively 
(see section 4.1.1).  
In our annotation scheme, we use chunks as a 
device for modularity. A chunk represents a set of 
adjacent words which are in dependency relations 
with each other, and are connected to the rest of 
the words by a single incoming dependency arc. 
The relations among the words in a chunk are not 
marked for now and hence allow us to ignore local 
details while building the sentence level depend-
ency tree. Thus, in our dependency tree each node 
is a chunk and the edge represents the relations 
between the connected nodes labeled with the ka-
raka or other relations. All the modifier-modified 
relations between the heads of the chunks (inter-
chunk relations) are marked in this manner. Intra-
chunk relations can be marked by a set of rules at a 
later point. Experiments have been conducted with 
high performance in automatically marking intra-
chunk dependencies.  
Using information such as karakas based on 
some vibhaktis (post-positions) and other informa-
tion like TAM (tense, aspect and modality) of the 
main verb seems very well suited for handling free 
word order languages. Other works based on this 
scheme like (Bharati et al, 1993; Bharati et al, 
2002; Pedersen et al, 2004) have shown promising 
results. We, therefore, propose the use of depend-
ency annotation based on the Paninian model in the 
Indian context. 
3 Annotation Procedure and Corpus De-
scription 
The annotation task is planned on a million word 
Hindi corpora obtained from CIIL (Central Insti-
tute for Indian Languages), Mysore, India. It is a 
representative corpus which contains texts from 
various domains like newspaper, literature, gov-
722
ernment reports, etc. The present subset on which 
the dependency annotation is being performed has 
already been manually tagged and chunked. Cur-
rently the annotation is being carried out by 2 an-
notators, who are graduate students with linguistic 
knowledge. The tool being used for the annotation 
is part of Sanchay (Singh, 2006) which is a collec-
tion of tools and APIs for South Asian languages. 
4 Scheme 
There are a total of 28 relations (see, 
http://ltrc.deptagset.googlepages.com/home) which 
we encode during the annotation. The total number 
of relations in the framework is few which has a 
direct bearing on the parser based on this frame-
work (both, rule based and statistical). We briefly 
discuss some of these relations in this section. 
4.1 Dependency relations 
As mentioned earlier, the proposed scheme uses 
the dependency relations from Paninian grammar. 
Section 4.1.1 below shows some karaka relations, 
section 4.1.2 shows some other relations; 
4.1.1 karaka relations 
(1) raama phala   khaataa hai 
?Ram?  ?fruit?     ?eat?    ?is? 
      ?Ram eats fruit? 
 
 
Figure1.  
 
(2) raama   chaaku   se     saiv     kaattaa hai 
?Ram?   ?knife?  -inst  ?apple? ?cut?     ?is? 
      ?Ram cuts the apple with a knife? 
 
 
Figure2.  
 
Examples (1), and (2) above show some simple 
cases which have karaka relations such as k3 
(karana; ?instrument?), k1 (karta), k2 (karma) (the 
term karta and karma can be roughly translated as 
?agent? and ?theme?). One must note here that the 
notion of karta, karma, etc, is not equivalent to that 
of the ?agent?, ?theme? thematic roles (although 
they might map to them sometimes). The reason 
for this divergence in the two notions (karaka and 
thematic role) is due to the difference in what they 
convey. Thematic role is purely semantic in nature 
whereas the karaka is syntactico-semantic. Exam-
ples (3), illustrates this point, 
 
(3) chaabhi  ne      darvaazaa   kholaa 
      ?key?     ?-erg?  ?door?      ?opened? 
      ?The key opened the door? 
 
In the above examples chaabhi is k1 (karta), 
whereas it takes instrument thematic role. While 
the karaka relations are primarily motivated via 
verbal semantics, syntactic cues like postpositions 
and verbal morphology play an important role too. 
For example in (3) above, the ergative case ?ne? 
provides a strong cue to identify karta. Panini de-
fines ?karta? as ?svatantra karta? which can be 
translated as ?the participant which is the most in-
dependent in a given action?. In (3) ?key? has such 
a property. When the speaker uses ?key? in (3), 
he/she intends to elevate the role of ?key? in the 
action of opening and does not communicate the 
actual agent of the action. The speaker uses ?key? 
as the independent participant in the act of open-
ing. Hence, ?key? is the karta (see Bharati et al, 
1995, pp. 65-73, for a more detailed discussion). 
4.2 Special Cases 
(a) POF (Part of relation) 
 
Conjunct verbs form a very challenging case for 
analysis in Indian languages. They have been ex-
tensively analyzed in the past. Some notable at-
tempts have been (Greaves, 1983; Kellogg, 1972; 
Mohanan, 1994; Butt, 2004). The example below 
shows a N+V conjunct verb usage; 
 
(4)  raama   ne    mujhase     prashna    kiyaa  
?Ram?  -erg  ?me-inst?  ?question?  ?do? 
           ?Ram asked me a question.? 
 
723
In example (4), prashna kiyaa is a conjunct verb 
and behaves as a single semantic unit. These verbs 
can also be discontiguous as in (5), 
 
(5) raama  ne    mujhase   prashna   pichle saal kiyaa   
     ?Ram? -erg  ?me-inst? ?question? ?last? ?year? ?did? 
     ?Ram asked me a question last year.? 
 
In the above example above a normal conjunct 
verb sequence prashna kiyaa is disjoint, making it 
rather difficult to annotate. In fact, practically 
anything can come between the disjointed 
elements. Ideally, the noun/adjective + verb 
sequence of the conjunct verb is placed in one 
chunk. Keeping this in mind, example (6) below is 
even more problematic, 
 
   (6)  maine     usase       ek       prashna  kiyaa 
   ?I-erg?  ?him-inst? ?one? ?question? ?did? 
         ?I asked him a question? 
 
The noun prashna ?question? within the con-
junct verb sequence prashna kiyaa is being modi-
fied by the adjective ek ?one? and not the entire 
noun-verb sequence; the annotation scheme should 
be able to account for this relation in the depend-
ency tree. If prashna kiyaa is grouped as a single 
verb chunk, it will not be possible to mark the ap-
propriate relation between ek and prashna. To 
overcome this problem it is proposed to break ek 
prashna kiyaa into two separate chunks, [ek 
prashna]/NP2 [kiyaa]/VG3. The dependency rela-
tion of prashna with kiyaa will be POF (?Part OF? 
relation), i.e. the noun or an adjective in the con-
junct verb sequence will have a POF relation with 
the verb. This way, the relation between ek and 
prashna becomes an intra-chunk relation as they 
will now become part of a single NP chunk. What 
makes such a sequence unique is the fact that the 
components which make up a conjunct verb are 
chunked separately, but semantically they consti-
tute a single unit. 
  The proposed scheme has the following advan-
tages: 
 
 (i) It captures the fact that the noun-verb seque 
ence is a conjunct verb by linking them with an 
appropriate tag, this information is extremely cruc- 
 
          2 Noun Phrase 
    3 Verb Group 
ial syntactically. 
 
(ii) It allows us to deal with the modifier-
modified relation between an adjective and its 
modified noun, as in example (6), which is a fre-
quent phenomenon.  
 
The tree below shows the proposed solution, 
where the adjective ek modifies the noun prashna 
instead of the entire prashna kiyaa, which would 
have been the case had we not separated prashna 
kiyaa into two separate chunks. 
 
 
Figure3.  
 
(b) ccof (?conjunct of? relation) and ellipses 
 
In the case of coordinating conjunction like aur 
?and? , the conjunct becomes the root and takes the 
two conjoined elements as children, the relation 
marked on the edges is ccof (conjunct of). This 
analysis captures the fact that neither of the con-
joined elements is the head. (The head of the two 
(or more) conjoined elements lies in the conjunct, 
and may be so computed when needed.) The ele-
ments participating in the coordination can belong 
to various categories, such as, noun, adjective, ad-
verbs etc; they can also be entire clauses, partici-
ples, etc. Other conjunct and punctuations which 
act like conjuncts are annotated similarly. 
When one or more element from a sentence is 
dropped, it is called ellipses. A null element 
marked with a special tag ?NULL? is introduced in 
cases of ellipses, where without inserting it the tree 
cannot be drawn. Null_NP, Null_VG, Null_CCP 
etc mark different kinds of ellipses.  
In this section, we have briefly discussed some 
of the relations and showed their actual usage us-
ing some sentences. The number of tags in the pro-
posed scheme is not very large. A limited set of 
tags helps immensely in developing high-
performance parsers (both rule based and statisti-
cal) and other related applications. We should note 
here that our tag-set, although small, is not a de-
724
limiting factor and is not a compromise on the se-
mantics, as these 28 relations are enough to fully 
parse the sentences in the language.  
5 Evaluation 
To make sure that the quality of the annotated cor-
pus is good, the annotators cross-validate each 
other?s work. A senior team member finally checks 
the annotated corpus (of both the annotators) to 
ensure that the errors are minimized. Note that 
such a setup is only temporary, we need such a 
thorough validation because we are still in the 
process of revising the guidelines. Once the guide-
lines become stable, the annotators won?t need to 
cross-validate. Of course, the task of final valida-
tion will still continue. 
6 Experiments 
Some preliminary experiments were conducted on 
a corpus of 1403 Hindi sentences that have been 
fully annotated. The aim was to access; 
 
1. Whether the syntactic cues can be ex-
ploited for better machine learnability. 
2. Whether certain generalization can be 
made for a constraint parser. 
3. How far would the automatic annotation 
help the annotators? 
 
We found a strong co-relation between most 
vibhakti-karaka occurrences (shaded cells in Table 
1). k7 (?place?) for example, overwhelmingly takes 
mem post-position, k3 (karana) takes se in all the 
cases. Of course, there are some competing rela-
tions which show preference for the same post-
position. In such cases only the post-position in-
formation will not be sufficient and we need to 
take into account other syntactic cues as well. 
These syntactic cues can be TAM (tense, aspect 
and modality) of the verb, verb class information, 
etc. For example, in case of karata karaka (k1), the 
following heuristics help resolve the ambiguities 
seen in Table 1. These heuristics are applied se-
quentially, i.e. if the first fails then the next follows. 
Note that the heuristics mentioned below are meant 
only for illustrative purpose. The cues mentioned 
in the heuristics will finally be used as features by 
an efficient ML technique to automate the task of 
annotation. 
H1: k1 agrees in gender, number and person 
with the verb if it takes a nominative case, 
H2: k1 takes a ko case-marker if the TAM of the 
verb has nA, 
H3: It takes a kaa/ke/ki if the verb is infinitive, 
H4: It takes a se or dvaara if the TAM of the 
verb is passive 
H5: It takes a ne case-marker if the verb is tran-
sitive and the TAM is perfective 
 
Table-2 shows the results when the heuristics 
were tested on the annotated corpus to test their 
effectiveness. 
 
 
Table 1. karaka-vibhakti correlation 
 
 
Table 2. Heuristics for k1 disambiguation 
 
The field ?Total? in Table-2 gives us the number 
of instances where a particular heuristic was ap-
plied. For example, there were 1801 instances 
where k1 appeared in a nominative case and H1 
correctly identified 1461 instances. H1 failed due 
to the errors caused by the morphological analyzer, 
presence of conjuncts, etc. Of particular interest 
are H2 and H3 which didn?t work out for large 
number of cases. It turns out that H2 failed for 
what is understood in the literature as dative sub-
jects. Dative subjects occur with some specific 
verbs, one possible solution could be to use such 
verbs for disambiguation. Automatic identification 
of conjunct verbs is a difficult problem; in fact, 
there isn?t any robust linguistic test which can be  
 
4 karaka relations (see, 
http://ltrc.deptagset.googlepages.com/home) 
    5 vibhakti (post-position) 
725
used to identify such verbs. Similar heuristics can 
be proposed for disambiguating other karaka based 
on some syntactic cues. Based on the above results 
one can safely conclude that arriving at some ro-
bust generalization (like, karaka-vibhakti correla-
tion) based on the syntactic cues is in fact possible. 
This can help us immensely in building an efficient 
parser for Hindi (and other ILs). It goes without 
saying that there exists a lot of scope for automat-
ing the annotation task. 
7 Conclusion 
In this paper we have introduced an ongoing effort 
to annotate Indian languages with dependency rela-
tion. We stated the motivation behind following 
the Paninian framework in the Indian Language 
scenario. We discussed the basic scheme along 
with some new relations such as ccof, POF, etc. 
We also showed the results of some experiments 
conducted on the annotated data which showed 
that there is a strong co-relation between vibhakti-
karaka relations. 
Acknowledgement 
Thanks are due to Prof. Ramakrishnamacharyulu 
who has been guiding us throughout the develop-
ment of the proposed scheme. 
References 
Akshar Bharati and Rajeev Sangal. 1993. Parsing Free 
Word Order Languages in the Paninian Framework, 
ACL93: Proc. of Annual Meeting of Association for 
Computational Linguistics. 
Akshar Bharati, Vineet Chaitanya and Rajeev Sangal. 
1995. Natural Language Processing: A Paninian 
Perspective, Prentice-Hall of India, New Delhi, pp. 
65-106. 
Akshar Bharati, Rajeev Sangal, T Papi Reddy. 2002. A 
Constraint Based Parser Using Integer Programming, 
In Proc. of ICON-2002: International Conference on 
Natural Language Processing, 2002. 
Cristina Bosco and V. Lombardo. 2004. Dependency 
and relational structure in treebank annotation. In 
Proceedings of Workshop on Recent Advances in 
Dependency Grammar at COLING'04. 
S. Brants, S. Dipper, S. Hansen, W. Lezius and G. 
Smith. 2002. The TIGER Treebank. In Proceedings 
of the Workshop on Treebanks and Linguistic Theo-
ries. 
M. Butt. 2004. The Light Verb Jungle. In G. Aygen, C. 
Bowern & C. Quinn eds. Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in Linguistics, 
p. 1-50. 
Edwin Greaves. 1983. Hindi Grammar. Asian Educa-
tional Services, New Delhi, pp. 335-340 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation. In Proc. 
TSD?98. 
R. Hudson. 1984. Word Grammar, Basil Blackwell, 108 
Cowley Rd, Oxford, OX4 1JF, England. 
S. H. Kellogg. 1972. A Grammar of the Hindi Language. 
Munshiram Manoharlal, New Delhi, pp. 271-279. 
P. Kiparsky and J. F. Staal. 1969. ?Syntactic and Rela-
tions in Panini?, Foundations of Language 5, 84-117. 
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 
1993. Building a large annotated corpus of English: 
The Penn Treebank, Computational Linguistics 1993. 
I. A. Mel'cuk. 1988. Dependency Syntax: Theory and 
Practice, State University, Press of New York. 
K. P. Mohanan. 1982. Grammatical relations in Malaya-
lam, In Joan Bresnan (ed.), The Mental Representa-
tion of Grammatical Relations, MIT Press, Cam-
bridge. 
Tara Mohanan, 1994. Arguments in Hindi. CSLI Publi-
cations. 
M. Pedersen, D. Eades, S. K. Amin, and L. Prakash. 
2004. Relative Clauses in Hindi and Arabic: A Pan-
inian Dependency Grammar Analysis. In COLING 
2004 Recent Advances in Dependency Grammar, 
pages 9?16. Geneva, Switzerland. 
O. Rambow, C. Creswell, R. Szekely, H. Taber, and M. 
Walker. 2002. A dependency treebank for English. In 
Proceedings of the 3rd International Conference on 
Language Resources and Evaluation. 
Anil Kumar Singh. 2006. 
http://sourceforge.net/projects/nlp-sanchay 
Charudev Shastri. 1973. Vyakarana Chandrodya (Vol. 1 
to 5). Delhi: Motilal Banarsidass. (In Hindi) 
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. In Linguistics and Phi-
losophy, p. 8, 334?343. 
L. Tesni?re. 1959. El?ments de Syntaxe Structurale. 
Klincksiek, Paris.  
L. van der Beek, G. Bouma, R. Malouf, and G. van 
Noord. 2002. The Alpino dependency treebank. 
Computational Linguistics in the Netherlands. 
726
Towards an Annotated Corpus of Discourse Relations in Hindi 
Rashmi Prasad*, Samar Husain?, Dipti Mishra Sharma? and Aravind Joshi* 
 
Abstract 
We describe our initial efforts towards 
developing a large-scale corpus of Hindi 
texts annotated with discourse relations. 
Adopting the lexically grounded approach 
of the Penn Discourse Treebank (PDTB), 
we present a preliminary analysis of 
discourse connectives in a small corpus. 
We describe how discourse connectives are 
represented in the sentence-level 
dependency annotation in Hindi, and 
discuss how the discourse annotation can 
enrich this level for research and 
applications. The ultimate goal of our work 
is to build a Hindi Discourse Relation Bank 
along the lines of the PDTB. Our work will 
also contribute to the cross-linguistic 
understanding of discourse connectives. 
1 Introduction 
An increasing interest in human language 
technologies such as textual summarization, 
question answering, natural language generation 
has recently led to the development of several 
discourse annotation projects aimed at creating 
large scale resources for natural language 
processing. One of these projects is the Penn 
Discourse Treebank (PDTB Group, 2006),1whose 
goal is to annotate the discourse relations holding 
between eventualities described in a text, for 
example causal and contrastive relations. The 
PDTB is unique in using a lexically grounded 
approach for annotation: discourse relations are 
anchored in lexical items (called ?explicit 
discourse connectives?) whenever they are 
                                                 
* University of Pennsylvania, Philadelphia, PA, USA, 
{rjprasad,joshi}@seas.upenn.edu 
? Language Technologies Research Centre, IIIT, Hyderabad, 
India, samar@research.iiit.ac.in, dipti@iiit.ac.in 
1 http://www.seas.upenn.edu/?pdtb 
 
explicitly realized in the text. For example, in (1), 
the causal relation between ?the federal 
government suspending US savings bonds sales? 
and ?Congress not lifting the ceiling on 
government debt? is expressed with the explicit 
connective ?because?.2 The two arguments of each 
connective are also annotated, and the annotations 
of both connectives and their arguments are 
recorded in terms of their text span offsets.3  
 
(1) The federal government suspended sales of U.S. 
savings bonds because Congress hasn?t lifted the 
ceiling on government debt. 
 
One of the questions that arises is how the 
PDTB style annotation can be carried over to 
languages other than English. It may prove to be a 
challenge cross-linguistically, as the guidelines and 
methodology appropriate for English may not 
apply as well or directly to other languages, 
especially when they differ greatly in syntax and 
morphology. To date, cross-linguistic 
investigations of connectives in this direction have 
been carried out for Chinese (Xue, 2005) and 
Turkish (Deniz and Webber, 2008). This paper 
explores discourse relation annotation in Hindi, a 
language with rich morphology and free word 
order. We describe our study of ?explicit 
connectives? in a small corpus of Hindi texts, 
discussing them from two perspectives. First, we 
consider the type and distribution of Hindi 
connectives, proposing to annotate a wider range 
                                                 
2 The PDTB also annotates implicit discourse relations, but 
only locally, between adjacent sentences. Annotation here 
consists of providing connectives (called ?implicit discourse 
connectives?) to express the inferred relation. Implicit 
connectives are beyond the scope of this paper, but will be 
taken up in future work. 
3 The PDTB also records the senses of the connectives, and 
each connective and its arguments are also marked for their 
attribution. Sense annotation and attribution annotation are not 
discussed in this paper. We will, of course, pursue these 
aspects in our future work concerning the building of a Hindi 
Discourse Relation Bank. 
 
The 6th Workshop on Asian Languae Resources, 2008
73
of connectives than the PDTB. Second, we 
consider how the connectives are represented in 
the Hindi sentence-level dependency annotation, in 
particular discussing how the discourse annotation 
can enrich the sentence-level structures. We also 
briefly discuss issues involved in aligning the 
discourse and sentence-level annotations.  
Section 2 provides a brief description of Hindi 
word order and morphology. In Section 3, we 
present our study of the explicit connectives 
identified in our texts, discussing them in light of 
the PDTB. Section 4 describes how connectives 
are represented in the sentence-level dependency 
annotation in Hindi. Finally, Section 5 concludes 
with a summary and future work. 
2 Brief Overview of Hindi Syntax and 
Morphology 
Hindi is a free word order language with SOV as 
the default order. This can be seen in (2), where 
(2a) shows the constituents in the default order, 
and the remaining examples show some of the 
word order variants of (2a). 
 
(2)  a. malaya       nao         samaIr         kao     iktaba    dI .  
           malay   ERG  sameer    DAT  book   gave 
           ?Malay gave the book to Sameer? (S-IO-DO-V)4 
       b. malaya nao iktaba samaIr kao dI. (S-DO-IO-V) 
       c. samaIr kao malaya nao iktaba dI. (IO-S-DO-V) 
       d. samaIr kao iktaba malaya nao dI. (IO-DO-S-V) 
       e. iktaba malaya nao samaIr kao dI. (DO-S-IO-V) 
        f. iktaba samaIr kao malaya nao dI.  (DO-IO-S-V) 
 
Hindi also has a rich case marking system, 
although case marking is not obligatory. For 
example, in (2), while the subject and indirect 
object are explicitly for the ergative (ERG) and 
dative (DAT) cases, the direct object is unmarked 
for the accusative. 
3 Discourse Connectives in Hindi 
Given the lexically grounded approach adopted for 
discourse annotation, the first question that arises 
is how to identify discourse connectives in Hindi. 
Unlike the case of the English connectives in the 
PDTB, there are no resources that alone or together 
provide an exhaustive list of connectives in the 
                                                 
4 S=Subject; IO=Indirect Object; DO=Direct Object; 
V=Verb; ERG=Ergative; DAT=Dative 
language. We did try to create a list from our own 
knowledge of the language and grammar, and also 
by translating the list of English connectives in the 
PDTB. However, when we started looking at real 
data, this list proved to be incomplete. For 
example, we discovered that the form of the 
complementizer ?ik? also functions as a temporal 
subordinator, as in (3). 
 
(3) [ vah  baalaTI      ko   gaMdo      panaI      sao      ApnaI    caaOklaoT  
      [he   bucket  of  dirty  water  from  his     chocolates 
      inakalanao          hI     vaalaa    qaa]    ik    {]sakI  mammaI       nao  
      taking-out  just doing was]  that  {his   mother ERG 
      ]sao    raok idyaa } 
      him stop did} 
?He was just going to take out the chocolates from 
the dirty water in the bucket when his mother stopped 
him.? 
 
The method of collecting connectives will 
therefore necessarily involve ?discovery during 
annotation?. However, we wanted to get some 
initial ideas about what kinds of connectives were 
likely to occur in real text, and to this end, we 
looked at 9 short stories with approximately 8000 
words. Our goal here is to develop an initial set of 
guidelines for annotation, which will be done on 
the same corpus on which the sentence-level 
dependency annotation is being carried out (see 
Section 4). Table 1 provides the full set of 
connectives we found in our texts, grouped by 
syntactic type. The first four columns give the 
syntactic grouping, the Hindi connective 
expressions, the English gloss, and the English 
equivalent expressions, respectively. The last 
column gives the number of occurrences we found 
of each expression. In the rest of this section, we 
describe the function and distribution of discourse 
connectives in Hindi based on our texts. In the 
discussion, we have noted our points of departure 
from the PDTB where applicable, both with 
respect to the types of relations being annotated as 
well as with respect to terminology. For argument 
naming, we use the PDTB convention: the clause 
with which the connective is syntactically 
associated is called Arg2 and the other clause is 
called Arg1. Two special conventions are followed 
for paired connectives, which we describe below. 
In all Hindi examples in this paper, Arg1 is 
enclosed in square brackets and Arg2 is in braces. 
The 6th Workshop on Asian Languae Resources, 2008
74
Connective Type Hindi Gloss English Num 
Sub. Conj. @yaaoMik 
(@yaaoM)ik..[salaIe 
(Agar|yadI)..tba|tao 
(jaba).. tba|tao 
jaba tk.. tba tk (ko ilae)  
jaOsao hI..(tao)  
[tnaa|eosaa..kI  
taik  
ik 
why-that 
(why)-that..this-for  
(if)..then 
(when)..then  
when till..then till (of for)  
as just..(then)  
so|such..that  
so-that  
that  
 
because  
because 
if..(then)  
when  
until  
as soon as  
so that  
so that  
when
  
2 
3 
15 
50 
2 
5 
12 
1 
5 
Sentential Relatives ijasasao 
jaao 
ijasako karNa 
which-with 
which 
which-of reason 
because of which 
because of which 
because of which 
5 
1 
1 
Subordinator pr 
(-kr|-ko|krko) 
samaya 
hue 
ko baad 
sao 
ko phlao 
ko ilae 
maoM 
ko karNa 
upon 
(do) 
time 
happening 
of later 
with 
of before 
of for 
in 
of reason
 
upon 
after|while 
while 
while 
after 
due to 
before 
in order to 
while 
because of
 
9 
111 
1 
28 
3 
1 
1 
4 
1 
3 
Coord. Conj. laoikna|pr|prntu 
AaOr|tqaa  
yaa  
yaaoM tao..pr  
naa kovala..balaik  
but  
and  
or  
such TOP..but  
not only..but
 
but  
and  
or  
but  
not only..but
 
51 
117 
2 
2 
1 
Adverbial tba  
baad maoM  
ifr  
[saIilae  
nahIM tao  
tBaI tao  
saao  
vahI|yahI nahIM  
then  
later in  
then  
this-for  
not then  
then-only TOP  
so  
that|this-only not  
then  
later  
then  
that is why  
otherwise  
that is why  
so  
not only that  
2 
5 
4 
7 
5 
1 
10 
1 
TOTAL    472 
     Table 1: A Partial List of Discourse Connectives in Hindi. Parentheses are used for optional  
     elements; ?|? is used for alternating elements; TOP = topic marker.
 
3.1 Types of Discourse Connectives 
3.1.1 Subordinating Conjunctions 
Finite adverbial subordinate clauses are 
introduced by independent lexical items called 
?subordinating conjunctions?, such as @yaaoMik 
(?because?), as in (4), and they typically occur as 
right or left attached to the main clause. 
 
(4) [maOM   [sa  saBaI    Qana        kao       rajya      ko   baadSaah  
      [I  this  all   wealth ACC kingdom of  king 
      kao      do      dota],    @yaaoMik       {vahI           samast 
     
 
 
  DAT give would], why-that {he-EMPH all 
      QartI     kI  sampda      ka  svaamaI   hO} 
      earth  of   wealth  of   lord  is} 
?I would give all this wealth to the king, because he 
alone is the lord of this whole world?s wealth.? 
 
As the first group in Table 1 shows, 
subordinating conjunctions in Hindi often come 
paired, with one element in the main clause and 
the other in the subordinate clause (Ex.5). One 
of these elements can also be implicit (Ex.6),  
The 6th Workshop on Asian Languae Resources, 2008
75
and in our texts, this was most often the 
subordinate clause element. 
 
(5)  @yaaoMik       {yah   tumharI  ja,maIna   pr   imalaa     hO},      [sailae 
       because {this  your  land  on  found  has}, this-for 
       [[sa       Qana      pr  tumhara  AiQakar  hO] 
       [this treasure on  your  right  is] 
?Because this was found on your land, you have the 
right to this treasure.? 
 
(6)  []saka  vaSa       calata]     tao    {vah   ]sao   Gar        sao 
       [her   power  walk] then  {she  it   home  from 
       baahr  inakala  dotI}  
       out  take  would} 
?Had it been in her power, she would have banished 
it from the house.? 
 
When both elements of the paired connective are 
explicit, their text spans must be selected 
discontinuously. The main clause argument is 
called Arg1 and the subordinate clause 
argument, Arg2. 
Subordinating conjunctions, whether single or 
paired, can occur in non-initial positions in their 
clause. However, this word order variability is 
not completely unconstrained. First, not all 
conjunctions display this freedom. For example, 
while ?jaba? (?when?) can be clause-medial (Ex. 
7), ?@yaaoMik? (?because?) cannot. Second, when the 
main clause precedes the subordinate clause, the 
main clause element, if explicit, cannot appear 
clause-initially at all. Consider the causal ?@yaaoMik.. 
[salaIe? (Ex.5), which represents the subordinate-
main clause order. In the reverse order, the 
explicit main clause ?[salaIe? (Ex.8) appears 
clause medially. Placing this element in clause-
initial position is not possible. 
 
(7) {lakD,haro         kI  p%naI      kao}    jaba    {yah 
      {woodcutter of  wife DAT} when {this 
       maalaUma            pD,a  ik     [sa  icaiDyaa  ko   karNa,  
       knowledge put  that this bird   of   reason 
       kama     CaoD,kr    Gar       Aa   gayaa     hO}   tao      [vah 
       work leaving home come went is} then  [she 
       ]sa    pr       barsa        pD,I]. 
       him on  anger-rain put] 
 ?When the woodcutter?s wife found out that he had 
left his work and come home to care for the bird, she 
raged at him.? 
 
(8)  [. . . pr  icaraga   kI  ba%tI    ]sakanaa  yaa   daohrI  
       [. . .but lamp of  light  light    or  another 
       ba%tI    lagaanaa]         Saayad [sailae         []icat nahIM  
       light  putting] perhaps this-for  [appropriate not 
       samaJato          qao]  ik     {tola  ka  Apvyaya   haogaa}. 
       Consider did]  that  {oil  of  waste   be-FUT}. 
?. . . but he did not consider it appropriate to light the 
lamp repeatedly or light another lamp, perhaps 
because it would be a waste of oil.? 
3.1.2 Sentential Relative Pronouns 
Since discourse relations are defined as holding 
between eventualities, we have also identified 
relations that are expressed syntactically as 
relative pronouns in sentential relative clauses, 
which modify the main clause verb denoting an 
eventuality, rather than some entity denoting 
noun phrase. For example, in (9), a 
result/purpose relation is conveyed between ?the 
man?s rushing home? and ?the bird being taken 
care of?, and we believe that this relation 
between the eventualities should be captured 
despite it?s syntactic realization as the relative 
pronoun ?ijasasao? (?because of which/so that?). (10) 
gives an example of a modified relative 
pronoun. 
 
(9) [saara  kama     caaoD,kr     vah  ]sa    baImaar   icaiD,yaa 
      [all  work  leaving  he  that  sick  bird 
      kao       ]zakr       dbaa     Gar     kI    Aaor       Baagaa], 
      ACC picking-up fast home of direction ran], 
      ijasasao             {]saka   sahI       [laaja   ikyaa  jaa    sako} 
      from-which {her    proper  care  do   go  able} 
?Leaving all his work, he picked up the bird and ran 
home very fast, so that the bird could be given proper 
care.? 
 
(10) [}M^TaoM       ko   hr     baar    kdma  rKnao       pr 
        [camels of  every time step keeping upon 
        icaiD,yaao M ko  isar     Aapsa          maoM   tqaa   }M^T      kI 
        birds of  head each-other in and camels of 
        gardna   sao    Tkra            rho     qao]   ijasako karNa 
        neck with hit-against be had] of-which reason 
       {]na     pixayaaoM   kI   drdBarI      caIKoM        inakla 
       {those birds  of   painful  screams come-out  
         rhI   qaIM}. 
         be had} 
?With each step of the camels, the birds heads were 
hitting against each other as well as with the camels? 
necks because of which the birds were screaming 
painfully.? 
3.1.3 Subordinators 
In contrast to the subordinating conjunctions, 
elements introducing non-finite subordinate 
clauses are called ?subordinators?. Unlike 
The 6th Workshop on Asian Languae Resources, 2008
76
English, where certain non-finite subordinate 
clauses, called ?free adjuncts?, appear without 
any overt marking so that their relationship with 
the main clause is unspecified, Hindi non-finite 
subordinate clauses almost always appear with 
overt marking. However, also unlike English, 
where the same elements may introduce both 
finite and non-finite clauses (cf. After leaving, 
she caught the bus vs. After she left, she caught 
the bus), different sets of elements are used in 
Hindi. In fact, as can be seen in the subordinator 
group in Table 1, the non-finite clause markers 
are either postpositions (Ex.11), particles 
following verbal participles (Ex.12), or suffixes 
marking serial verbs (Ex.13). 
 
(11) {mammaI          ko     manaa        krnao}      ko karNa     [ramaU 
        {mummy of  warning  doing} of reason [Ramu 
         qaaoD,I  qaaoD,I    caaOklaoT      baD,o     AnaMd        ko   saaqa 
         little little chocolate big  pleasure  of  with  
         Ka  rha      qaa]. 
         eat being be] 
?Because of his mother?s warning, Ramu was eating 
bits of chocolate with a lot of pleasure.? 
 
(12) . . . AaOr    {Kolato}      hue               [yah    BaUla     jaata hO 
        . . . and  {playing} happening [this forget go is 
         ik    yaid  ]saka  ima~        BaI    Apnao  iKlaaOnao    kao 
         that if    his   friends also their  toys     to 
         ]sao     haqa   nahIM   lagaanao         dota,  tao      ]sao 
         him hand not   touching did,  then  he  
           iktnaa        baura     lagata] 
         how-much bad   feel] 
?. . . and while playing, he forgets that if his friends 
too didn?t let him touch their toys, then how bad he 
would feel.? 
 
(13) {ApnaI  p%naI     sao     yah       sauna}kr      [lakD,hara  
        {self  wife from this   listen}-do  [woodcutter 
         bahut    duKI       huAa] 
         much sad  became] 
?Upon hearing this from his wife, the woodcutter 
became very sad.? 
 
While subordinators constitute a frequently-
used way to mark discourse relations, their 
annotation raises at least two difficult problems, 
both of which have implications for the 
reliability of annotation. The first is that these 
markers are used for marking both argument 
clauses and adjunct clauses, so that annotators 
would be required to make difficult decisions for 
distinguishing them: in the former case, the 
marker would not be regarded as a connective, 
while in the latter case, it would. Second, the 
clauses marked by these connectives often seem 
to be semantically weak. This is especially true 
of verbal participles, which are nonfinite verb 
appearing in a modifying relation with another 
finite verb. Whereas in some cases (Ex.12-13) 
the two verbs are perceived as each projecting 
?two distinct events? between which some 
discourse relation can be said to exist, in other 
cases (Ex.14), the two verbs seem to project two 
distinct actions but as part of a ?single complex 
event? (Verma, 1993). These judgments can be 
very subtle, however, and our final decision on 
whether to annotate such constructions will be 
made after some initial annotation and 
evaluation. 
 
(14) {doKto        hI         doKto      saba  baOla             Baagato } 
        {looking EMPH looking all buffalos running} 
         hue              [gaaoSaalaa   phu^Mca     gae] 
         happening [shed   reach  did] 
?Within seconds all the buffalos came running to the 
shed.? 
 
The naming convention for the arguments of 
subordinators is the same as for the 
subordinating conjunctions: the clause 
associated with the subordinator is called Arg2 
while its matrix clause is called Arg1. 
Unlike subordinating conjunctions, 
subordinators do not come paired and they can 
only appear clause-finally. Clause order, while 
not fixed, is restricted in that the nonfinite 
subordinate clause can appear either before the 
main clause or embedded in it, but never after 
the main clause.  
3.1.4 Coordinating Conjunctions 
Coordinating conjunctions in Hindi are found in 
both inter-sentential (Ex.15) and intra-sentential 
(Ex.16) contexts, they always appear as 
independent elements, and they almost always 
appear clause-initially. 5  For these connectives, 
                                                 
5 While the contrastive connectives  ?pr?, ?prntU? appear only 
clause-initially, it seems possible for the contrastive ?laoikna? 
to appear clause-medially, suggesting that these two types 
may correspond to the English ?but? and ?however?, 
respectively. However, we did not find any examples of 
clause-medial ?laoikna? in our texts, and this behavior will 
have to be verified with further annotation. 
The 6th Workshop on Asian Languae Resources, 2008
77
the first clause is called Arg1 and the second, 
Arg2. 
 
(15) [jaba      vah  laaOTta    tao       gaa-gaakr          ]saka  mana 
         [when he  return then sing-singing   his  mind 
         KuSa      kr    dotI].   laoikna  {]sakI p%naI   kao      vah 
         happy do gave].   But   {his wife DAT  the  
         icaiD,yaa   fUTI    AaM^K  nahIM    sauhatI   qaI}. 
         bird    torn  eye  not   bear  did} 
?Upon his return, she would make him happy by 
singing. But his wife could not tolerate the bird even 
a little bit.? 
 
(16) [ tBaI          drvaaja,a      Kulaa]  AaOr    {maalaikna  Aa 
        [then-only door opened]  and  {wife  come 
         ga[- }. 
        went} 
?Just then the door opened and the wife came in.? 
 
We also recognize paired coordinating 
conjunctions, such as ?naa kovala..balaik? (See Table 
1). The argument naming convention for these is 
the same as for the single conjunctions. 
3.1.5 Discourse Adverbials 
Discourse adverbials in Hindi modify their clau- 
ses as independent elements, and some of these 
are free to appear in non-initial positions in the 
clause. Example (17) gives an example of the 
consequence adverb, ?saao?. The Arg2 of discourse 
adverbials is the clause they modify, whereas 
Arg1 is the other argument. 
 
(17) [icaiD,yaa  jabaana      kT   jaanao     AaOr   maalaikna  ko  eosao 
        [bird    tongue  cut  going  and  wife  of  this  
        vyavahar       sao      Dr   ga[-     qaI]. saao    {vah     iksaI 
        behavior with fear go  had]. So  {she  some 
        trh        ]D,kr    calaI        ga[-}. 
        manner flying  walk    went}. 
?The bird was scared due to her tongue being cut and 
because of the wife?s behavior. So she somehow flew 
away.? 
 
As with the PDTB, one of our goals with the 
Hindi discourse annotation is to explore the 
structural distance of Arg1 from the discourse 
adverbial. If the Arg1 clause is found to be snon-
adjacent to the connective and the Arg2 clause, 
it may suggest that adverbials in Hindi behave 
anaphorically. In the texts we looked at, we did 
not find any instances of non-adjacent Arg1s. 
Addtional annotation will provide further 
evidence in this regard. 
4 Hindi Sentence-level Annotation 
andDiscourse Connectives 
The sentence-level annotation task in Hindi is 
an ongoing effort which aims to come up with a 
dependency annotated treebank for the NLP/CL 
community working on Indian languages. 
Presently a million word Hindi corpus is being 
manually annotated (Begum et al, 2008). The 
dependency annotation is being done on top of 
the corpus which has already been marked for 
POS tag and chunk information. The scheme has 
28 tags which capture various dependency 
relations. These relations are largely inspired by 
the Paninian grammatical framework. Given 
below are some relations, reflecting the 
argument structure of the verb. 
 
a) kta- (agent) (k1) 
b) kma- (theme) (k2) 
c) krNa (instrument) (k3) 
d) samp`dana sampradaan (recipient) (k4) 
e) Apadana (source) (k5) 
f) AiQakrNa (location) (k7) 
 
Figure 1 shows how Examples (2a-f) are 
represented in the framework. Note that agent 
and theme are rough translations for ?kta-? and 
?kma-? respectively. Unlike thematic roles, these 
relations are not purely semantic, and are 
motivated not only through verbal semantics but 
also through vibhaktis (postpositions) and TAM 
(Tense, aspect and modality) markers (Bharati et 
al., 1995). The relations are therefore syntactico-
semantic, and unlike thematic roles there is a 
greater binding between these relations and the 
syntactic cues. 
 
 
k1 k4 k2
 
 
 
Figure 1: Dependency Diagram for Example (2) 
Some discourse relations that we have identified 
are already clearly represented in the sentence-
level annotation. But for those that aren?t, the 
   dI 
   malaya    samaIr    iktaba 
The 6th Workshop on Asian Languae Resources, 2008
78
discourse level annotations will enrich the 
sentence-level. In the rest of this section, we 
discuss the representation of the different types 
of connectives at the sentence level, and discuss 
how the discourse annotation will add to the 
information present in the dependency 
structures. 
 
Subordinating Conjunctions Subordinating 
conjunctions are lexically represented in the 
dependency tree, taking the subordinating clause 
as their dependents while themselves attaching 
to the main verb (the root of the tree). Figure 2 
shows the dependency tree for Example (4) 
containing the subordinating conjunction ?@yaaoMik?. 
Note that the edge between the connective and 
the main verb gives us the causal relation 
between the two clauses, the relation label being 
?rh? (relation hetu ?cause?). Thus, the discourse 
level can be taken to be completely represented 
at the sentence-level. 
 
hE
k1 k2 k4 rh
ccofr6
k1sk1
r6
r6 
 
Figure 2: Dependency Tree for Subordinating 
Conjunction in Example (4) 
 
Paired Subordinating Conjunctions Unlike 
Example (4), however, the analysis for the 
paired connective in Example (5), given in 
Figure 3, is insufficient. Despite the lexical 
representation of the connective in the tree, the 
correct interpretation of the paired conjunction 
and the clauses which it relates is only possible 
at the discourse level. In particular, the 
dependencies don?t show that ?@yaaoMik? and ?[salaIe? 
are two parts of the same connective, expressing 
a single relation and taking the same two 
arguments. Thus, the discourse annotation will 
be able to provide the appropriate argument 
structure and semantics for these paired 
connectives.  
 
 
ccof
k2 k1 rh
ccof
k7p
r6
k1
 
 
Figure 3: Dependency Tree for Paired 
Subordinating Conjunction in Example (5) 
 
Subordinators As mentioned earlier, Hindi 
nonfinite subordinate clauses almost always 
appear with overt marking. But unlike the 
subordinating conjunctions, subordinators are 
not lexically represented in the dependency 
trees. Figure 4 gives the dependency 
representation for Example (11) containing a 
postposition subordinator ?ko karNa?, which relates 
the main and subordinate clauses causally. As 
the figure shows, while the causal relation label 
(?rh?) appears on the edge between the main and 
subordinate verbs, the subordinator itself is not 
lexically represented as the mediator of this 
relation. The lexically grounded annotation at 
the discourse level will thus provide the textual 
anchors of such relations, enriching the 
dependency representation. Furthermore, while 
many of the subordinators in Table 1 are fully 
specified in the dependency trees for the 
semantic relation they denote (e.g., ?pr? and ?maoM? 
marked as the ?k7t? (location in time) relation, 
and ?ko karNa? and ?sao? marked as the ?rh? 
(cause/reason) relation), others, like the particle 
?hue? are underspecified for their semantics, being 
marked only as ?vmod? (verbal modifier). The 
discourse-level annotation will thus be the 
source for the semantics of these subordinators. 
 
Coordinating Conjunctions Coordinating 
conjunctions at the sentence level anchor the 
root of the dependency tree. Figure 5 shows the 
 do dotao oo oo o   
   maOMOM O MO M   Qana   baadSaah    @yaaoMikoM oMoM   
rajya  
   vahI   svaamaI  
  sampda  
   QartI  
   [sailae 
    
AiQakar hOO OO 
    Qana  tuuuumhara  @yaaoMikoMoMo M   
  imalaa hOO OO 
    yah 
  
 ja,maIna, ,,  
 
  tumharIuuu  
The 6th Workshop on Asian Languae Resources, 2008
79
dependency representation of Example (16) 
containing a coordinating conjunction. 
 
 
rh k1 k2
vmod
k1
 
 
Figure 4: Dependency Tree for Subordinator in 
Example (11) 
 
 
ccof ccof
k7t k1 k1
 
 
Figure 5: Dependency Tree for Coordinating 
Conjunction in Example (16) 
 
While the sentence-level dependency analysis 
here is similar to the one we get at the discourse 
level, the semantics of these conjunctions are 
again underspecified, being all marked as ?ccof?, 
and can be obtained from the discourse level. 
 
Discourse Adverbials Like subordinating 
conjunctions, discourse adverbials are 
represented lexically in the dependency tree. 
They are attached to the verb of their clause as 
its child node and their denoted semantic 
relation is specified clearly. This can be seen 
with the temporal adverb ?tBaI? (?then-only?) and 
its semantic label ?k7t? in Figure 5. At the same 
time, since the Arg1 discourse argument of 
adverbials is most often in the prior context, the 
discourse annotation will enrich the semantics of 
these connectives by providing the Arg1 
argument. 
5 Summary and Future Work 
In this paper, we have described our study of 
discourse connectives in a small corpus of Hindi 
texts in an effort towards developing an 
annotated corpus of discourse relations in Hindi. 
Adopting the lexically grounded approach of the 
Penn Discourse Treebank, we have identified a 
wide range of connectives, analyzing their types 
and distributions, and discussing some of the 
issues involved in the annotation. We also 
described the representation of the connectives 
in the sentence-level dependency annotation 
being carried out independently for Hindi, and 
discussed how the discourse annotations can 
enrich the information provided at the sentence 
level. While we focused on explicit connectives 
in this paper, future work will investigate the 
annotation of implicit connectives, the semantic 
classification of connectives, and the attribution 
of connectives and their arguments. 
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. In Proceedings of IJCNLP-2008. 
Hyderabad, India. 
Akshar Bharati, Vineet Chaitanya, and Rajeev 
Sangal. 1995. Natural Language Processing: A 
Paninian Perspective. Prentice Hall of India. 
http://ltrc.iiit.ac.in/downloads/nlpbook/nlppanini.p
df. 
Manindra K. Verma (ed.). 1993. Complex Predicates 
in South Asian Languages. New Delhi: Manohar. 
The PDTB-Group. 2006. The Penn Discourse 
TreeBank 1.0 Annotation Manual. Technical 
Report IRCS-06-01, IRCS, University of 
Pennsylvania. 
Bonnie Webber, Aravind Joshi, Matthew Stone, and 
Alistair Knott. 2003. Anaphora and discourse 
structure. Computational Linguistics, 29(4):545?
587. 
Nianwen Xue. 2005. Annotating Discourse 
Connectives in the Chinese Treebank. In 
Proceedings of the ACL Workshop on Frontiers in 
Corpus Annotation II: Pie in the Sky. Ann Arbor, 
Michigan.  
Deniz Zeyrek and Bonnie Webber. 2008. A 
Discourse Resource for Turkish: Annotating 
Discourse Connectives in the METU Corpus. In 
Proceedings of IJCNLP-2008. Hyderabad, India. 
 
 
  Ka rha qaa 
manaa krnaoo oo rama   caaOO OOklaoTooo    AanaMdMMM  
 mammaI 
  AaOrOOO  
 Kulaauuu   Aa ga[-- -- 
maalaikna  drvaaja,a, ,,   tBaI  
The 6th Workshop on Asian Languae Resources, 2008
80
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 99?106,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Comparison, Selection and Use of Sentence Alignment Algorithms for New
Language Pairs
Anil Kumar Singh
LTRC, IIIT
Gachibowli, Hyderabad
India - 500019
anil@research.iiit.net
Samar Husain
LTRC, IIIT
Gachibowli, Hyderabad
India - 500019
s amar@iiit.net
Abstract
Several algorithms are available for sen-
tence alignment, but there is a lack of
systematic evaluation and comparison of
these algorithms under different condi-
tions. In most cases, the factors which
can significantly affect the performance
of a sentence alignment algorithm have
not been considered while evaluating. We
have used a method for evaluation that
can give a better estimate about a sen-
tence alignment algorithm?s performance,
so that the best one can be selected. We
have compared four approaches using this
method. These have mostly been tried
on European language pairs. We have
evaluated manually-checked and validated
English-Hindi aligned parallel corpora un-
der different conditions. We also suggest
some guidelines on actual alignment.
1 Introduction
Aligned parallel corpora are collections of pairs of
sentences where one sentence is a translation of the
other. Sentence alignment means identifying which
sentence in the target language (TL) is a translation
of which one in the source language (SL). Such cor-
pora are useful for statistical NLP, algorithms based
on unsupervised learning, automatic creation of re-
sources, and many other applications.
Over the last fifteen years, several algorithms have
been proposed for sentence alignment. Their perfor-
mance as reported is excellent (in most cases not less
than 95%, and usually 98 to 99% and above). The
evaluation is performed in terms of precision, and
sometimes also recall. The figures are given for one
or (less frequently) more corpus sizes. While this
does give an indication of the performance of an al-
gorithm, the variation in performance under varying
conditions has not been considered in most cases.
Very little information is given about the conditions
under which evaluation was performed. This gives
the impression that the algorithm will perform with
the reported precision and recall under all condi-
tions.
We have tested several algorithms under differ-
ent conditions and our results show that the per-
formance of a sentence alignment algorithm varies
significantly, depending on the conditions of test-
ing. Based on these results, we propose a method
of evaluation that will give a better estimate of the
performance of a sentence alignment algorithm and
will allow a more meaningful comparison. Our view
is that unless this is done, it will not be possible to
pick up the best algorithm for certain set of con-
ditions. Those who want to align parallel corpora
may end up picking up a less suitable algorithm for
their purposes. We have used the proposed method
for comparing four algorithms under different con-
ditions. Finally, we also suggest some guidelines for
using these algorithms for actual alignment.
2 Sentence Alignment Methods
Sentence alignment approaches can be categorized
as based on sentence length, word correspondence,
and composite (where more than one approaches are
combined), though other techniques, such as cog-
99
nate matching (Simard et al, 1992) were also tried.
Word correspondence was used by Kay (Kay, 1991;
Kay and Roscheisen, 1993). It was based on the idea
that words which are translations of each other will
have similar distributions in the SL and TL texts.
Sentence length methods were based on the intuition
that the length of a translated sentence is likely to be
similar to that of the source sentence. Brown, Lai
and Mercer (Brown et al, 1991) used word count as
the sentence length, whereas Gale and Church (Gale
and Church, 1991) used character count. Brown, Lai
and Mercer assumed prior alignment of paragraphs.
Gale and Church relied on some previously aligned
sentences as ?anchors?. Wu (Wu, 1994) also used
lexical cues from corpus-specific bilingual lexicon
for better alignment.
Word correspondence was further developed in
IBM Model-1 (Brown et al, 1993) for statistical
machine translation. Melamed (Melamed, 1996)
also used word correspondence in a different (geo-
metric correspondence) way for sentence alignment.
Simard and Plamondon (Simard and Plamondon,
1998) used a composite method in which the first
pass does alignment at the level of characters as
in (Church, 1993) (itself based on cognate match-
ing) and the second pass uses IBM Model-1, fol-
lowing Chen (Chen, 1993). The method used by
Moore (Moore, 2002) also had two passes, the first
one being based on sentence length (word count) and
the second on IBM Model-1. Composite methods
are used so that different approaches can compli-
ment each other.
3 Factors in Performance
As stated above, the performance of a sentence
alignment algorithm depends on some identifiable
factors. We can even make predictions about
whether the performance will increase or decrease.
However, as the results given later show, the algo-
rithms don?t always behave in a predictable way. For
example, one of the algorithms did worse rather than
better on an ?easier? corpus. This variation in perfor-
mance is quite significant and it cannot be ignored
for actual alignment (table-1). Some of these factors
have been indicated in earlier papers, but these were
not taken into account while evaluating, nor were
their effects studied.
Translation of a text can be fairly literal or it can
be a recreation, with a whole range between these
two extremes. Paragraphs and/or sentences can be
dropped or added. In actual corpora, there can even
be noise (sentences which are not translations at all
and may not even be part of the actual text). This can
happen due to fact that the texts have been extracted
from some other format such as web pages. While
translating, sentences can also be merged or split.
Thus, the SL and TL corpora may differ in size.
All these factors affect the performance of an al-
gorithm in terms of, say, precision, recall and F-
measure. For example, we can expect the perfor-
mance to worsen if there is an increase in additions,
deletions, or noise. And if the texts were translated
fairly literally, statistical algorithms are likely to per-
form better. However, our results show that this does
not happen for all the algorithms.
The linguistic distance between SL and TL can
also play a role in performance. The simplest mea-
sure of this distance is in terms of the distance on
the family tree model. Other measures could be the
number of cognate words or some measure based
on syntactic features. For our purposes, it may not
be necessary to have a quantitative measure of lin-
guistic distance. The important point is that for lan-
guages that are distant, some algorithms may not
perform too well, if they rely on some closeness be-
tween languages. For example, an algorithm based
on cognates is likely to work better for English-
French or English-German than for English-Hindi,
because there are fewer cognates for English-Hindi.
It won?t be without a basis to say that Hindi is
more distant from English than is German. English
and German belong to the Indo-Germanic branch
whereas Hindi belongs to the Indo-Aryan branch.
There are many more cognates between English and
German than between English and Hindi. Similarly,
as compared to French, Hindi is also distant from
English in terms of morphology. The vibhaktis of
Hindi can adversely affect the performance of sen-
tence length (especially word count) as well as word
correspondence based algorithms. From the syntac-
tic point of view, Hindi is a comparatively free word
order language, but with a preference for the SOV
(subject-object-verb) order, whereas English is more
of a fixed word order and SVO type language. For
sentence length and IBM model-1 based sentence
100
alignment, this doesn?t matter since they don?t take
the word order into account. However, Melamed?s
algorithm (Melamed, 1996), though it allows ?non-
monotonic chains? (thus taking care of some differ-
ence in word order), is somewhat sensitive to the
word order. As Melamed states, how it will fare
with languages with more word variation than En-
glish and French is an open question.
Another aspect of the performance which may not
seem important from NLP-research point of view, is
its speed. Someone who has to use these algorithms
for actual alignment of large corpora (say, more than
1000 sentences) will have to realize the importance
of speed. Any algorithm which does worse than
O(n) is bound to create problems for large sizes. Ob-
viously, an algorithm that can align 5000 sentences
in 1 hour is preferable to the one which takes three
days, even if the latter is marginally more accurate.
Similarly, the one which takes 2 minutes for 100 sen-
tences, but 16 minutes for 200 sentences will be dif-
ficult to use for practical purposes. Actual corpora
may be as large as a million sentences. As an esti-
mate of the speed, we also give the runtimes for the
various runs of all the four algorithms tested.
Some algorithms, like those based on cognate
matching, may even be sensitive to the encoding or
notation used for the text. One of the algorithms
tested (Melamed, 1996) gave worse performance
when we used a notation called ITRANS for the
Hindi text, instead of the WX-notation.1
4 Evaluation in Previous Work
There have been attempts to systematically evaluate
and compare word alignment algorithms (Och and
Ney, 2003) but, surprisingly, there has been a lack of
such evaluation for sentence alignment algorithms.
One obvious problem is the lack of manually aligned
and checked parallel corpora.
Two cases where a systematic evaluation was per-
formed are the ARCADE project (Langlais et al,
1996) and Simard et al (Simard et al, 1992). In the
ARCADE project, six alignment systems were eval-
uated on several different text types. Simard et al
performed an evaluation on several corpus types and
1In this notation, capitalization roughly means aspiration for
consonants and longer length for vowels. In addition, ?w? rep-
resents ?t? as in French entre and ?x? means something similar
to ?d? in French de, hence the name of the notation.
corpus sizes. They, also compared the performance
of several (till then known) algorithms.
In most of the other cases, evaluation was per-
formed on only one corpus type and one corpus size.
In some cases, certain other factors were considered,
but not very systematically. In other words, there
wasn?t an attempt to study the effect of various fac-
tors described earlier on the performance. In some
cases, the size used for testing was too small. One
other detail is that size was sometimes mentioned in
terms of number of words, not number of sentences.
5 Evaluation Measures
We have used local (for each run) as well as global
(over all the runs) measures of performance of an
algorithm. These measures are:
? Precision (local and global)
? Recall (local and global)
? F-measure (local and global)
? 95% Confidence interval of F-measure (global)
? Runtime (local)
6 An Evaluation Scheme
Unless sentence alignment is correct, everything
else that uses aligned parallel corpora, such as word
alignment (for automatically creating bilingual dic-
tionaries) or statistical machine translation will be
less reliable. Therefore, it is important that the best
algorithm is selected for sentence alignment. This
requires that there should be a way to systemati-
cally evaluate and compare sentence alignment al-
gorithms.
To take into account the above mentioned factors,
we used an evaluation scheme which can give an
estimate of the performance under different condi-
tions. Under this scheme, we calculate the measures
given in the previous section along the following di-
mensions:
? Corpus type
? Corpus size
? Difference in sizes of SL and TL corpora
? Noise
101
We are also considering the corpus size as a factor
in performance because the second pass in Moore?s
algorithm is based on IBM Model-1, which needs
training. This training is provided at runtime by us-
ing the tentative alignments obtained from the first
pass (a kind of unsupervised learning). This means
that larger corpus sizes (enough training data) are
likely to make word correspondence more effective.
Even for sentence length methods, corpus size may
play a role because they are based on the distribution
of the length variable. The distribution assumption
(whether Gaussian or Poisson) is likely to be more
valid for larger corpus sizes.
The following algorithms/approaches were evalu-
ated:
? Brn: Brown?s sentence length (word count)
based method, but with Poisson distribution
? GC: Church and Gale?s sentence length (char-
acter count) based method, but with Poisson
distribution
? Mmd: Melamed?s geometric correspondence
based method
? Mre: Moore?s two-pass method (word count
plus word correspondence)
For Brn and GC we used our own implemen-
tations. For Mmd we used the GMA alignment
tool and for Mre we used Moore?s implementation.
Only 1-to-1 mappings were extracted from the out-
put for calculating precision, recall and F-measure,
since the test sets had only 1-to-1 alignments. En-
glish and Hindi stop lists and a bilingual lexicon
were also supplied to the GMA tool. The parame-
ter settings for this tool were kept the same as for
English-Malay. For Brn and GC, the search method
was based on the one used by Moore, i.e., searching
within a growing diagonal band. Using this search
method meant that no prior segmentation of the cor-
pora was needed (Moore, 2002), either in terms
of aligned paragraphs (Gale and Church, 1991), or
some aligned sentences as anchors (Brown et al,
1991).
We would have liked to study the effect of linguis-
tic distance more systematically, but we couldn?t get
equivalent manually-checked aligned parallel cor-
pora for other pairs of languages. We have to rely
on the reported results for other language pairs, but
those results, as mentioned before, do not mention
the conditions of testing which we are considering
for our evaluation and, therefore, cannot be directly
compared to our results for English-Hindi. Still, we
did an experiment on the English-French test data
(447 sentences) for the shared task in NAACL 2003
workshop on parallel texts (see table-1).
For all our experiments, the text in Hindi was in
WX-notation.
In the following sub-sections we describe the de-
tails of the data sets that were prepared to study the
variation in performance due to various factors.
6.1 Corpus Type
Three different types of corpora were used for the
same language pair (English-Hindi) and size. These
were EMILLE, ERDC and India Today. We took
2500 sentences from each of these, as this was the
size of the smallest corpus.
6.1.1 EMILLE
EMILLE corpus was constructed by the EMILLE
project (Enabling Minority Language Engineering),
Lancaster University, UK, and the Central Institute
of Indian Languages (CIIL), Mysore, India. It con-
sists of monolingual, parallel and annotated corpora
for fourteen South Asian languages. The parallel
corpus part has a text (200000 words) in English and
its translations in Hindi, Bengali, Punjabi, Gujarati
and Urdu. The text is from many different domains
like education, legal, health, social, and consumer
markets. The documents are mostly in simple, for-
mal language. The translations are quite literal and,
therefore, we expected this corpus to be the ?easiest?.
6.1.2 ERDC
The ERDC corpus was prepared by Electronic
Research and Development Centre, NOIDA, India.
It also has text in different domains but it is an un-
aligned parallel corpus. A project is going on to pre-
pare an aligned and manually checked version of this
corpus. We have used a part of it that has already
been aligned and manually checked. It was our opin-
ion that the translations in this corpus are less literal
and should be more difficult for sentence alignment
than EMILLE. We used this corpus for studying the
effect of corpus size, in addition to corpus type.
102
Table 1: Results for Various Corpus Types (Corpus Size = 2500)
Clean, Same Size Noisy, Same Size Noisy, Different Size
Type Brn GC Mmd Mre Brn GC Mmd Mre Brn GC Mmd Mre
EMILLE P 99.3 99.1 85.0 66.8 85.5 87.4 38.2 66.2 87.2 86.5 48.0 65.5
R 96.0 93.0 80.0 63.2 80.4 80.0 36.2 58.0 81.2 79.1 46.5 57.4
F 97.6 96.0 82.0 64.9 82.8 83.5 37.2 61.8 84.0 82.6 47.3 61.2
T 23 23 261 45 47 44 363 64 25 25 413 47
ERDC P 99.6 99.5 94.2 100.0 85.4 84.4 48.0 96.5 84.6 85.5 50.9 97.7
R 99.0 99.1 92.7 97.0 81.7 80.6 46.7 78.9 80.5 81.3 49.8 79.1
F 99.3 99.3 93.4 98.4 83.5 82.4 47.3 86.8 82.5 83.3 50.3 87.1
T 31 29 1024 85 92 90 2268 124 55 52 3172 101
India P 91.8 93.9 76.4 99.5 71.5 76.7 49.7 94.4 73.6 75.5 51.7 93.4
Today R 81.0 83.0 70.6 81.5 61.0 65.5 47.6 67.5 62.4 64.4 50.1 62.6
F 86.1 88.1 73.4 89.6 65.8 70.7 48.6 78.7 67.6 69.5 50.9 75.0
T 32 32 755 91 96 101 2120 159 60 68 987 134
English- P 100.0 100.0 100.0 100.0 87.4 87.5 77.2 95.2 91.2 93.3 77.7 96.6
French R 100.0 99.3 100.0 99.3 85.5 84.3 81.7 84.6 83.2 83.7 82.6 83.0
P: Precision, R: Recall, F: F-Measure, T: Runtime (seconds)
6.1.3 India Today
India Today is a magazine published in both En-
glish and Hindi. We used some parallel text col-
lected from the Internet versions of this magazine. It
consists of news reports or articles which appeared
in both languages. We expected this corpus to be the
most difficult because the translations are often more
like adaptations. They may even be rewritings of the
English reports or articles in Hindi. This corpus had
2500 sentences.
6.2 Corpus Size
To study the effect of corpus size, the sizes used
were 500, 1000, 5000 and 10000. All these data sets
were from ERDC corpus (which was expected to be
neither very easy nor very difficult).
6.3 Noise and Difference in Sizes of SL and TL
Corpora
To see the effect of noise and the difference in sizes
of SL and TL corpora, we took three cases for each
of the corpus types and sizes:
? Same size without noise
? Same size with noise
? Different size with noise
Three different data sets were prepared for each
corpus type and for each corpus size. To obtain
such data sets from the aligned, manually checked
and validated corpora, we added noise to the cor-
pora. The noise was in the form of sentences from
some other unrelated corpus. The number of such
sentences was 10% each of the corpus size in the
second case and 5% to SL and 15% to the TL in the
third case. The sentences were added at random po-
sitions in the SL and TL corpora and these positions
were recorded so that we could automatically cal-
culate precision, recall and F-measure even for data
sets with noise, as we did for other data sets. Thus,
each algorithm was tested on (3+4)(3) = 21 data sets.
7 A Limitation
One limitation of our work is that we are considering
only 1-to-1 alignments. This is partly due to prac-
tical constraints, but also because 1-to-1 alignments
are the ones that can be most easily and directly used
for linguistic analysis as well as machine learning.
Since we had to prepare a large number of data
sets of sizes up to 10000 sentences, manual check-
ing was a major constraint. We had four options.
The first was to take a raw unaligned corpus and
manually align it. This option would have allowed
consideration of 1-to-many, many-to-1, or partial
103
Table 2: Results for Various Corpus Sizes
Clean, Same Size Noisy, Same Size Noisy, Different Size
Size Brn GC Mmd Mre Brn GC Mmd Mre Brn GC Mmd Mre
500 P 99.2 99.2 93.9 99.8 75.4 78.2 57.4 94.3 83.5 87.2 45.4 92.4
R 98.8 98.8 91.8 95.0 71.0 73.4 56.8 70.0 77.0 80.8 44.8 70.8
F 99.0 99.0 92.8 97.3 73.1 75.7 57.1 80.4 80.1 83.9 45.1 80.2
T 9 9 126 14 10 10 148 13 10 10 181 14
1000 P 99.3 99.6 96.4 100.0 84.6 84.6 67.8 96.8 82.2 84.0 47.3 95.1
R 98.9 99.4 95.1 96.3 81.4 82.2 68.4 73.7 76.3 78.7 46.1 72.7
F 99.1 99.5 95.7 98.1 83.0 83.4 68.1 83.7 79.1 81.2 46.7 82.4
T 13 13 278 29 24 23 335 34 15 15 453 30
5000 P 99.8 99.8 93.2 99.9 88.5 88.6 56.1 98.5 85.9 86.6 57.6 97.8
R 99.4 99.5 91.6 98.2 83.2 83.3 54.9 86.0 81.7 81.3 56.7 86.3
F 99.6 99.7 92.4 99.1 85.7 85.9 55.4 91.8 83.7 83.9 57.2 91.7
T 54 53 3481 186 199 185 5248 274 185 174 3639 275
10000 P 99.8 99.9 93.2 100.0 88.0 88.9 59.6 98.5 86.8 88.7 57.2 98.4
R 99.4 99.6 91.4 98.6 82.9 83.7 58.9 89.9 81.3 82.8 56.2 89.2
F 99.6 99.7 92.3 99.3 85.4 86.2 59.2 94.0 84.0 85.6 56.6 94.0
T 102 96 4356 305 370 346 4477 467 345 322 4351 479
alignments. The second option was to pass the text
through an alignment tool and then manually check
the output for all kinds of alignment. The third op-
tion was to check only for 1-to-1 alignments from
this output. The fourth option was to evaluate on
much smaller sizes.
In terms of time and effort required, there is an
order of difference between the first and the second
and also between the second and the third option. It
is much easier to manually check the output of an
aligner for 1-to-1 alignments than to align a corpus
from the scratch. We couldn?t afford to use the first
two options. The fourth option was affordable, but
we decided to opt for a more thorough evaluation of
1-to-1 alignments, than for evaluation of all kinds of
alignments for smaller sizes. Thus, our starting data
sets had only 1-to-1 alignments.
In future, we might extend the evaluation to all
kinds of alignments, since the manual alignment
currently being done on ERDC corpus includes par-
tial and 1-to-2 or 2-to-1 alignments. Incidentally,
there are rarely any 2-to-1 alignments in English-
Hindi corpus since two English sentences are rarely
combined into one Hindi sentence (when translating
from English to Hindi), whereas the reverse is quite
possible.
8 Evaluation Results
The results for various corpus types are given in
table-1, for corpus sizes in table-2, and the global
measures in table-3. Among the four algorithms
tested, Moore?s (Mre) gives the best results (ex-
cept for the EMILLE corpus). This is as expected,
since Mre combines sentence length based method
with word correspondence. The results for Mmd are
the worst, but it should be noted that the results for
Mmd reported in this paper may not be the best that
can be obtained with it, because its performance de-
pends on some parameters. Perhaps with better tun-
ing for English-Hindi, it might perform better. An-
other expected outcome is that the results for GC
(character count) are better than Brn (word count).
One reason for this is that there are more of charac-
ters than words (Gale and Church, 1991).
Leaving aside the tuning aspect, the low perfor-
mance of Mmd may be due to the fact that it relies
on cognate matching, and there are fewer cognates
between Hindi and English. It might also be due to
the syntactic differences (word order) between Hindi
and English. This could, perhaps be taken care of
by increasing the maximum point dispersal thresh-
old (relaxing the linearity constraint), as suggested
by Melamed (Melamed, 1996).
104
The results of experiment on English-French
(table-1) show that Mmd performs better for this
language pair than for English-Hindi, but it still
seems to be more sensitive to noise than the other
three algorithms. Mre performed the best for
English-French too.
With respect to speed, Brn and GC are the fastest,
Mre is marginally slower, and Mmd is much slower.
The effects of the previously mentioned factors on
performance have been summarized below.
8.1 Corpus Type
Brn, GC, and Mmd performed almost equally well
for EMILLE and ERDC corpora, but not that well
for India Today. However, surprisingly, Mre per-
formed much worse for EMILLE than it did for
the other two corpora. It could be because of the
fact that the EMILLE has a lot of very short (1-3
words) sentences, and word correspondence (in the
second pass) may not be that effective for such sen-
tences. The results don?t support our assumption
that EMILLE is easier than ERDC, but India Today
does turn out to be more difficult than the other two
for all the test cases. This is understandable since
the translations in this corpus are much less literal.
8.2 Corpus Size
Only in the case of Mre, the performance almost
consistently increased with size. This is as expected
since the second pass in Mre needs training from
the results of the first pass. The corpus size has to be
large for this training to be effective. There doesn?t
seem to be a clear relationship between size and per-
formance for the other three algorithms.
8.3 Noise and Difference in Sizes of SL and TL
Corpora
As expected, introducing noise led to a decrease
in performance for all the algorithms (table-1 and
table-2). However (barring EMILLE) Mre seems to
become less sensitive to noise as the corpus size in-
creases. This again could be due to the unsupervised
learning aspect of Mre.
Making the SL and TL corpora differ in size
tended to reduce the performance in most cases, but
sometimes the performance marginally improved.
Table 3: Global Evaluation MeasuresBrn GC Mmd Mre
Clean, L 92.6 93.4 81.4 80.8
Same Size H 100.0 100.0 96.3 100.0
P 98.4 98.7 90.3 95.1
R 96.1 96.1 87.6 90.0
F 97.2 97.3 88.9 92.4
Noisy, L 73.1 75.8 44.1 72.6
Same Size H 87.5 86.4 62.4 92.3
P 82.7 84.1 53.8 92.2
R 77.4 78.4 52.8 74.9
F 79.8 81.1 53.3 82.5
Noisy, L 74.7 76.4 46.2 71.3
Different H 85.6 86.4 55.0 92.0
Size P 83.4 84.9 51.2 91.5
R 77.2 78.3 50.0 74.0
F 80.1 81.4 50.6 81.6
Overall L 81.1 82.4 55.4 80.0
H 90.4 90.8 73.1 91.0
P 88.2 89.2 65.1 92.9
R 83.6 84.3 63.5 79.6
F 85.7 86.6 64.6 85.5
L and H: Lower and higher limits of
95% confidence interval for F-measure
P, R, and F: Average precision,
recall, and F-measure
9 Some Notes on Actual Corpus Alignment
Based on the evaluation results and our experience
while manually checking alignments, we make some
observations below which could be useful to those
who are planning to create aligned parallel corpora.
Contrary to what we believed, sentence length
based algorithms turn out to be quite robust, but also
contrary to the commonly held view, there is scope
for improvement in the performance of these algo-
rithms by combining them with other techniques as
Moore has done. However, as the performance of
Mre on EMILLE shows, these additional techniques
might sometimes decrease the performance.
There is a tradeoff between precision and recall,
just as between robustness and accuracy (Simard and
Plamondon, 1998). If the corpus aligned automati-
cally is to be used without manual checking, then we
should opt for maximum precision. But if it?s going
to be manually checked before being used, then we
105
should opt for maximum recall. It depends on the
application too (Langlais et al, 1996), but if man-
ual checking is to be done, we can as well try to
get the maximum number of alignments, since some
decrease in precision is not going to make manual
checking much more difficult.
If the automatically aligned corpus is not to be
checked manually, it becomes even more important
to perform a systematic evaluation before aligning
a corpus, otherwise the parallel corpus will not be
reliable either for machine learning or for linguistic
analysis.
10 Conclusion
We used a systematic evaluation method for select-
ing a sentence alignment algorithm with English and
Hindi as the language pair. We tested four algo-
rithms for different corpus types and sizes, for the
same and different sizes of SL and TL corpora, as
well as presence and absence of noise. The evalu-
ation scheme we have described can be used for a
more meaningful comparison of sentence alignment
algorithms. The results of the evaluation show that
the performance depends on various factors. The di-
rection of this variation (increase or decrease) was as
predicted in most of the cases, but some results were
unexpected. We also presented some suggestions on
using an algorithm for actual alignment.
References
Brown Peter F., Cocke John, Della Pietra StephenA., Della Pietra Vincent J., Jelinek Frederick, Laf-ferty John D., Mercer Robert L., and Roossin Paul S.
1990. A Statistical Approach to Machine Translation.Computational Linguistics.
Brown Peter F., Della Pietra Stephen A., Della Pietra Vin-cent J., and Mercer Robert L. 1993. Mathematicsof Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Brown Peter F., Lai J. C. and Mercer Robert L. 1991.
Aligning Sentences in Parallel Corpora. Proceedingsof 29th Annual Meeting of the Association for Compu-tational Linguistics, 169?176. Berkeley, CA.
Chen Stanley F. 1993. Aligning Sentences in BilingualCorpora Using Lexical Information. Proceedings ofthe 31st Annual Meeting of the Association for Com-putational Linguistics, 9?16. Columbus, OH.
Church Kenneth W. 1993. Char align: A Program for
Aligning Parallel Texts at the Character Level. Pro-ceedings of the 31st Annual Meeting of the Associationfor Computational Linguistics, 1?8. Columbus, OH.
Church Kenneth W. and Hanks Patrick. 1993b. Aligning
Parallel Texts: Do Methods Developed for English-French Generalize to Asian Languages?. Proceedingsof Rocling.
Gale William A. and Church Kenneth W. 1991. A
Program for Aligning Sentences in Bilingual Corpora.Proceedings of 29th Annual Meeting of the Associa-tion for Computational Linguistics, 177?184. Berke-
ley, CA.
Kay Martin. 1991. Text-Translation Alignment.ACH/ALLC ?91: ?Making Connections? ConferenceHandbook. Tempe, Arizona.
Kay Martin and Roscheisen Martin. 1993. Text-
Translation Alignment. Computational Linguistics,19(1):121?142.
Langlais Phillippe, Simard Michel, and Vronis Jean.1996. Methods and Practical Issues in Evaluat-
ing Alignment Techniques. Proceedings of 16th In-ternational Conference on Computational Linguistics(COLING-96).
Melamed I. Dan. 1996. A Geometric Approach to Map-
ping Bitext Correspondence. IRCS Technical Report,University of Pennsylvania, 96?22.
Moore Robert C. 2002. Fast and Accurate SentenceAlignment of Bilingual Corpora. Proceedings ofAMTA, 135?144.
Och Franz Joseph and Ney Hermann 2003. A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19-51.
Simard Michel, Foster George F., and Isabelle Pierre.1992 Using Cognates to Align Sentences in Bilin-gual Corpora. Proceedings of the Fourth InternationalConference on Theoretical and Methodological Issuesin Machine Translation. Montreal, Canada.
Simard Michel and Plamondon Pierre. 1998 BilingualSentence Alignment: Balancing Robustness and Ac-curacy. Machine Translation, 13(1):59?80.
Wu Dekai. 1994. Aligning a Parallel English-ChineseCorpus Statistically with Lexical Criteria. Proceed-ings of 32nd Annual Meeting of the Association forComputational Linguistics, 80?87. Las Cruces, NM.
106
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 51?58,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Simple Preposition Correspondence: A problem  in English to Indian 
language Machine Translation 
Samar Husain, Dipti Misra Sharma, Manohar Reddy 
{samar@research.iiit.net,dipti@mail.iiit.net, 
manohar@students.iiit.net} 
Language Technologies Research Centre, 
IIIT, Hyderabad, India. 
 
 
Abstract 
The paper describes an approach to auto-
matically select from Indian Language the 
appropriate lexical correspondence of Eng-
lish simple preposition. The paper de-
scribes this task from a Machine Transla-
tion (MT) perspective. We use the proper-
ties of the head and complement of the 
preposition to select the appropriate sense 
in the target language. We later show that 
the results obtained from this approach are 
promising. 
1 Introduction 
The task of identifying the appropriate sense from 
some target language (here, Hindi and Telugu) for 
a given simple preposition in some source lan-
guage (here, English) is rather complex for an MT 
system, and noting that most foreign language 
learners are never able to get a firm hold on prepo-
sitions of a new language (Brala, 2000), this should 
not be surprising. A simple example illustrates the 
problem: 
 
(1a) He bought a shirt with tiny collars. 
       ?with? gets translated to vaalii in Hindi (hnd). 
        and as kaligi unna in Telugu (tlg). 
 (1b) He washed a shirt with soap. 
       ?with? gets translated to se in hnd. 
    and as to (suffixed to head noun) in tlg. 
 
   For the above English sentences, if we try to 
swap the senses of ?with? in their corresponding 
target translation, the resulting sentences either 
become ill-formed or unfaithful to their English 
source. The pervasive use of preposition (or its 
equivalent in a given language) in most of the lan-
guages makes it a crucial element during transla-
tion. Inappropriate sense selection of a preposition 
during machine translation can have a negative 
impact on the quality of the translation, sometimes 
changing the semantics of the sentence drastically,  
thereby making the preposition sense selection 
module a critical component of any reliable MT 
system. 
  Finding the proper attachment site for the prepo-
sition in English, i.e. getting the correct parse for 
the prepositional phrase (PP) is a classic problem 
in MT, and this information can be used to identify 
the sense of a preposition. Figure 1 and Figure 2 
below show the correct attachment site of PPs in 
example (1a) and (1b) respectively. 
 
 
 
51
   The correct parse of the PP helps us in selecting 
the appropriate sense. However, finding the appro-
priate attachment only reduces the problem. It does 
not lead to a ?complete solution?. The following 
examples (2a, 2b and 3a, 3b) have the same at-
tachment site but take different senses in the target 
language: 
 
(2a) He has had fever for two days now. 
       ?for? gets translated as se in hnd. 
    and as nundi in tlg. 
(2b) He had fever for two days. 
       ?for? gets translated as taka in hnd. 
    Not translated in tlg.  
 
(3a) He is going to Delhi. 
?to? gets translated as ko, or preferably left un-
translated in hnd. 
and in tlg as ki (suffixed to the head noun), or 
may be left un-translated. 
(3b) He is going to his mother. 
       ?to? gets translated as ke paasa in hnd. 
   and daggaraku in tlg 
 
After looking at cases such as (2a), (2b) and (3a), 
(3b) where the parse is same i.e., preposition ?for 
and ?to? get attached to the main verb ?have? and 
?go? respectively, it is clear that we need to come 
up with some criterion which can help us in 
achieving our task. 
There has been extensive work on understanding 
prepositions linguistically, often from various an-
gles. Syntactically (Jackendoff, 1977; Emonds, 
1985; Rauh, 1993; Pullum and Huddleton, 2002), 
from a Cognitive perspective (Lakoff and Johnson, 
1980; Langacker, 1987; Brala, 2000), Semantically 
by (Saint-Dizier and Vazquez, 2001; Saint-Dizier, 
2005), and the Pragmatic aspects by (Fauconnier, 
1994). 
   The work of automatically selecting the correct 
sense has also received good amount of attention 
and there have been many attempts to solve the 
problem. (Japkowicz et. al, 1991) attempts to trans-
late locative prepositions between English and 
French. The paper introduces the notion of ?repre-
sentation of conceptualization? based in turn on 
(Grimaud, 1988). The paper synthesizes this idea 
with the thesis of ideal meaning (Herskovits, 1986). 
(Tezuka et. al, 2001) have tried to resolve concep-
tual geographical prepositions using inference rule 
based on cognitive maps which people have of the 
external world. (Hartrumpf et al, 2005) use 
knowledge representation formalism for PP inter-
pretation. 
Some studies pertain to systems which have 
been implemented for MT; (Gustavii, 2005) uses 
aligned parallel corpora to induce automatic rules 
by applying transformation-based learning. (Alam, 
2004) make use of contextual information to de-
termine the meanings of over. (Trujillo, 1992) use 
a transfer rule based approach to translate locative 
PP-phrase, the approach uses the dependency rela-
tions marked as indices with individual word and a 
bilingual lexicon which has mapping between 
source and target lexical item (with indices). 
(Naskar and Bandyopadhyay, 2005) look at the 
semantics of the head noun of the reference object 
(this is their main criterion) to get the lexical 
meaning of prepositions in an English-Bengali MT 
system. 
The current paper presents a study of preposi-
tions at, for, in, on, to and with in context of Eng-
lish to Indian language MT system. The paper is 
arranged as follows; Section 2 describes our ap-
proach to solving the mentioned task, the 3rd sec-
tion shows the performance of our approach along 
with the error analysis during the testing phase, we 
conclude the paper along with some future direc-
tion in section 4. 
2 Our Approach 
All the previous attempts can be broadly classified 
into 3 main categories; one, where the preposition 
is the main focus, concentration is on the semantics 
(cognitive or lexical) of the preposition; second, 
focus on the verb and the PP which the verb takes 
as argument; and lastly, the head noun of the PP 
becomes the deciding factor to get the appropriate 
sense. 
Very few approaches, like (Alam, 2004; Saint-
Dizier and Vazquez, 2001),  consider both, the 
head (modified) and the complement (modifier) 
information, to decide the sense of the preposition. 
The modified (or head) is the head of the phrase to 
which the PP attaches. The modifier (or 
complement) is the head noun of the PP. The 
following examples show very clearly why given a 
preposition we cannot depend only on the modified 
or the modifier separately, and that we must 
consider them both to solve the problem. 
 
52
Considering only the modifier (the complement); 
 
 (4a) He apologized to his mother. 
        ?to? gets translated as se in hnd 
        & ki (suffixed to the head noun) in  tlg 
 (4b) He went to his mother. 
         ?to? gets translated as ke paasa in hnd 
         & as daggaraku in tlg 
 
Considering only the modified (the head); 
 
 (5a) He waits for her at night. 
        ?at? gets translated as meM in hnd 
        & not translated in tlg 
 (5b) He waits for her at the station. 
         ?at? gets translated as par 
         & as lo in tlg 
 
Only considering the modifer ?his mother? in 4a 
and 4b is not sufficient, likewise taking only the 
modified ?waits? in 5a and 5b will be insufficient, 
both the pairs take different senses and have the 
same partial contextual enviornment which is 
misleading. Hence, the combined context of 
complement-head forms a better candidate for 
solving the problem. We come across plenty of 
cases where isolated information of 
modifier/modified can be misleading. 
The task of preposition sense selection can be 
divided into; 
(a) Getting the correct parse (the task of PP at-
tachment, identification of phrasal verb, etc.), 
(b) Context and semantic extraction, 
(c) Sense selection. 
 
This paper describes the algorithm for achieving 
the above mentioned steps. We assume the input to 
our module has the correct parse, i.e. Step (a) 
above is assumed here. The proposed algorithm is 
a component in English to Indian language MT 
system1, therefore, the required input can be pre-
sumed to be available. Steps (b, c) above are rule 
based, which make use of the modifier-modified 
relation, these relations and the properties of modi-
fier/modified form the core of the context in step 
(b). We then apply a series of rules, which specify 
the context and semantics in which a sense  
 
         1 (http://shakti.iiit.ac.in). Note here that the proposed 
algorithm has been tested with Shakti version 0.83x which has 
still not been released. The released version is 0.73. 
is expected to occur. 
2.1 Context and semantic extraction 
Extraction of context and semantic information (of 
modifier/modified) is done automatically by vari-
ous sub-modules which are combined together to 
perform the overall task. We use the word ?con-
text? very loosely. A context for us is a combina-
tion of various properties which can be syntactic or 
lexical, or both; syntactic context can be modifier-
modified relation, lexical properties can be mor-
phological information such as TAM (tense, aspect 
and modality) of a verb, class of the verb (Levin, 
1993), category of the lexical item and in some 
cases the lexical item itself.  
The semantics of the modifier and the modified 
are captured using WordNet (Miller, 1990), and 
certain other resources such as person, place dic-
tionaries, place and time filters (these filters make 
use of syntactic cues to mark basic time and place), 
etc. We use WordNet to get the hypernyms of a 
word. By using this property we can easily get the 
broader, more general class/concept for a modi-
fier/modified. Although effective and very intui-
tive, this method has its own problems. We will 
elaborate these problems in section 3.2. WordNet 
is also used to identify person and place names by 
using the hyponym tree for person and place. 
Along with the WordNet, as mentioned above, 
we use certain other filters such as place and time. 
They are used prior to using WordNet. In case a 
rule requires the modifier to be a place (rules are 
explained in 2.2), this information is acquired from 
the place filter. If the filter?s result is negative we 
use WordNet. Dictionaries and POS tags are 
checked for identifying proper names, we use a 
proper name dictionary as POS taggers tend to 
have a fixed upper limit especially when it comes 
to the identification of named entities. In essence, 
the linguistic resources are used in the following 
order; 
(1) Dictionaries, 
(2) Time & Place filter, 
(3) WordNet. 
 
Preliminary results have shown that certain 
prepositions occurring in the PP complement of 
certain verb classes (Levin, 1993) translate to a 
specific sense in Hindi. For example, preposition 
?at? in the case of peer verbs always translates to 
kii tarapha or kii ora in Hindi. This knowledge can 
53
be very informational and we plan to pursue this 
aspect in the future. 
2.2 Sense Selection 
We have noticed in the previous examples that the 
prepositions from English either get translated as 
suffixes to the head noun of the PP (in Telugu) or 
as postpositions (in Hindi and Telugu). An 
example where a preposition in English gets 
translated as postposition in its Telugu translation 
is shown below; 
 
(6) The book is on the table.  
  ?buka     taibila     paiina   undi? 
   ?Book?  ?table?     ?on?     ?there? 
 
We select the correct sense of the preposition 
based on a series of rules which are applied 
linearly. These rules have been manually 
constructed. We have tried to make the rules 
mutually exclusive, so that there are no clashes. 
Also, by making sure that the rules are mutually 
exclusive we don?t need to worry about the order 
in which the rules are listed out in the rule file, thus 
making the rule file less fragile. These rules 
currently cover around 20 high frequency English 
prepositions, these prepositions vary in their 
degree of ambiguity; some are highly ambiguous 
(e.g. to, by, with, etc.), whereas some are less 
ambiguous (e.g. against, around, as, etc.), hence 
these are easier to handle. 
Various senses on the target side for a given 
English preposition are selected on the basis of 
rules listed out in a file. The rule file comprises of 
tuples, each having 6 attributes.  
 
The attributes are listed below; 
 
a)  Source Language preposition 
b)  Modified category 
c)  Constraints on the modified item 
d)  Modifier category 
e)  Constraints on the modifier item 
f) Dictionary sense id of the source language 
preposition 
 
 
An example of a tuple: 
# at, v, -, n, place_close, at%p%5 
 
 
(7) He has opened a school at his home. 
 ?usane   apne ghara   mem eka  skuula kholaa hei? 
?He erg? ?his? ?house? ?at? ?one? ?school??open? ?is? 
 
The rule above requires the modifier to be a 
noun and places a constraint ?place_close? on it. 
We map this constraint (place_close) with some set 
of lexical items found in a synset of a hypernym 
obtained from WordNet. For example, 
?place_close? might correspond to ?housing?, 
?lodging?, ?building?, etc in a synset. In essence 
?place_close? is place holder for different relations 
which might be present in a synset. The modified 
category and the modifier category can be ex-
tracted after the correct parse of the PP is known; 
the constraints applied on the modified and modi-
fier item (point c, e above) can be of various kinds, 
some of them are; 
 
? Semantic relations corresponding to 
WordNet hypernyms for a given word 
? Presence of the lexical item in some list 
(eg. verb class) 
? Semantic property such as ?time? or ?place? 
? Lexical property such as aspect, negativity 
etc. 
 
 
 
 
The constraints specified in a tuple can be com-
bined together using logical operators such as 
?and?, ?or?, ?negation?. So, for a single rule, multi-
54
ple constraints can be introduced. For a sense, if 
needed, complex constraints can be introduced 
which must be satisfied. 
 
#for, v, L2:for.dat && aspect:continuous, n, time, 
for%p%5 
 
 (8) He has been playing for years. 
     ?vaha   kaii      saalo   se    khela   rahaa   hai ? 
      ?He? ?many? ?years? ?for? ?play? ?cont.?  ?is? 
 
The above rule (for the Hindi translation) has 
two constraints for the modified (which is a verb in 
this case), the two constraints have been combined 
using an ?and? operator (represented using two 
ampersands, ?&&?). Only if the two constraints are 
satisfied, the constraint is considered as satisfied 
else it is considered as failed. The use of different 
logical operator gives a lot of expressive power to 
a single rule. Sometimes it might be desirable to 
place multiple constraints together, because for a 
given sense these constraints always occur together, 
and by listing them as separate rules we will miss 
out the fact that they co-occur.  
It is not always necessary (or possible) to fill the 
constraint fields. In fact, sometimes it is even de-
sirable to leave them unspecified. In such a case 
we place a hyphen in that field, such as the follow-
ing rule; 
 
# at, v, -, n, place_close, at%p%5 
 
In the above rule, the constraint for the modified 
field is unspecified. There are also cases when it is 
not desirable to have a translated preposition corre-
sponding to its source;  
 
# to, L: verbs.txt, -, n, place, ZZ 
 
(9) He went to Delhi. 
      ?vaha dilli     gayaa? (in hnd) 
       ?He? ?Delhi? ?went? 
 
The ?ZZ? in the above rule signifies that the 
translated sentence will have no preposition corre-
sponding to the preposition ?to? when it occurs 
with certain verbs which are specified by 
?L:verbs.txt? (?verbs.txt? is a list of verbs). For the 
above Hindi sentence post-position ?ko? can  
 
         2 List 
perhaps be introduced, i.e. ?vaha dilli ko gayaa?, 
but ?vaha dilli gayaa? is more natural, and the 
translated sentence is better off without a ?ko?.  
Finally, each preposition handled has a default 
rule, which is applied at the end when all the other 
rules for that preposition fail; the sense given by  
the default rule is based on the most frequent usage 
of the preposition at the target side. All the fields 
(except the first and last) in the default rule have 
hyphens. The default rule for ?to? is written below; 
 
to, -, -, -, -, to%p%1 
 
Some of the rules in the rule file are given below, 
for ease of comprehension, we mention the actual 
target sense instead of the dictionary id for the last 
field (the actual rule file has dictionary sense id) 
 
at, v, L:peer_verbs.txt, n, -, kii tarapha 
at, v, L:transaction_verbs.txt, n, price, meM 
for, v, -, n, distance, taka 
in, n, animate, n, place, kaa 
on, v, -, n, time, ko 
to, v, L:go_verbs.txt, n, animate|authority, ke 
paasa 
with, v, -, n, instrument, se 
2.3 Recap 
We briefly describe the various steps of the al-
gorithm again; 
 
(a) Given a raw sentence we feed it to the 
Shakti MT system which performs various 
source language analysis, for our algo-
rithm, information such as PP attachment 
and correct identification of the phrasal 
verb (if present) is crucial. 
(b) The output of step (a) is taken by our 
module which automatically constructs 
the six field tuple described above. At this 
point we can only fill some fields, which 
are field 1 (source language preposition), 
field 2 (modified category) and field 4 
(modifier category). 
(c) We then compare this constructed tuple 
with the appropriate tuples present in the 
rule file. For this constructed tuple to sat-
isfy the various constraints mentioned in 
the tuple with which it is compared re-
sources such as place filter, time filter, 
lists and WordNet are consulted automati-
55
cally. The order in which we use these re-
sources has been already been mentioned 
in section 2.1. The tuple for which all the 
constraints are satisfied is selected, the 
last field of this tuple contains the diction-
ary id of the sense. 
(d) Output the selected sense. 
3 Evaluation 
For the current study, experiments were conducted 
with 6 high frequency prepositions, they are; at, 
for, in, on, to, and with. The algorithm was tested 
on 100 sentences for each preposition in both the 
language pairs, i.e., 600 sentences for English-
Hindi and 600 sentences for English-Telugu. These 
sentences were randomly extracted from the 
ERDC3 corpus. The corpus contains text from dif-
ferent domains such as medicine, sports, history, 
etc. The input to the implemented system was 
manually checked and corrected to make sure that 
there were no errors in the information which is 
expected by the system. The bulk of these correc-
tions involved rectifying the wrong PP attachment 
given by the parser and the mistakes in phrasal 
verb identification.  
Prep4 Precision BL No. of Sense 
At 73.4 51.5 5 
For 84.05 69.5 6 
In 82 65.2 7 
On 85 70 3 
To 65.2 35.4 10 
With 66 50 6 
Table 1{English-Hindi}. 
 
Prep4 Precision BL No. of Sense 
At 68 48 5 
For 72 50 7 
In 82 82 3 
On 76 76 2 
To 80 80 2 
With 94 90 3 
Table 2{English-Telugu}. 
 
         3Electronic Research and Development Centre, NOIDA 
         4 Prepositions 
3.1 Performance 
The tables above show the performance of the sys-
tem and compares it with the baseline score (BL). 
BL is the precision of the system with only the de-
fault sense. The tables also show the number of 
sense which English prepositions can take on the 
target side. Table 1 and Table 2 show English-
Hindi and English-Telugu results respectively. 
The implemented system gives very promising 
results. Certain prepositions give comparably low 
precision. The reasons for the inappropriate sense 
selection are discussed in the next section. The 
English-Telugu results (Table 2)  show same 
system precision and BL for some preposition (?in? 
and ?to?). This is because these prepositions have 
less number of sense on the target side and all the 
instances found in the test data had the default 
sense.   
3.2 Error analysis 
The errors made by the system were analyzed and 
the major reasons for inappropriate sense selection 
were;  
  
(a) Noise generated by WordNet, 
(b) Special constructions, 
(c) Metonymy, 
(d) Ambiguous sentences, 
(e) Presence of very general constraints. 
 
The problem of noise generation by WordNet 
sometimes leads to surprising and unexpected 
sense selection; this is because in WordNet a noun 
or verb will have multiple sense, and each of these 
senses will have various levels of hypernym syn-
sets, so, while finding various concepts/features 
(specified by the rule for a preposition) we need to 
look at each one of these senses. We need to do 
this because we currently don?t have the sense in-
formation. So, an inappropriate sense might some-
times satisfy the constraint(s) and result in inap-
propriate selection. The solution for this will obvi-
ously be to identify the correct sense of modi-
fier/modified prior to getting its semantic property 
from the WordNet.  
There are certain constructions in which the 
head noun of the PP is a pronoun, which refers 
back to a noun. For us this will create a problem, in 
such cases we will first need to get the referent 
56
noun and then apply the constraints on it, take the 
following example; 
 
(10) The rate at which these reactions occur is 
known as rate of metabolism. 
 
In the above example, the head noun of the PP 
(at which) refers to the noun (rate) on which we 
need to apply the constraints. At present the 
coreference information is not available to us, 
therefore in such cases the algorithm fails to give 
the correct output. 
The other reason for failure was the ambiguity 
of the sentence itself which could be interpreted in 
various ways, like the example below; 
 
(11) Andamaan should go to India. 
 
The above sentence can be interpreted (and 
translated) in two ways, the hindi translations for 
the two interpretation are; 
 
(11a) ?andamaan    indiaa  ko   jaanaa  chahiye? 
          ?Andamaan? ?India?  ?to?  ?go?     ?should? 
   India should get Andaman. 
 
(11b) ?andamaan   ko   indiaa   jaanaa  chahiye? 
          ?Andamaan? ?to? ?India?  ?visit?    ?should? 
   Andaman should visit India. 
 
In (11a) we get the sense that the 
possesion/control of ?Andamaan? should go to 
?India?, and in (11b) it is ?Andamaan? (the 
government of ?Andamaan?) which is going to 
?India? (the government of India), as in, The United 
States should go to UK, also in (11b) we can have 
?Andamaan? as somebodys? name, as in, Ram 
should go to India. In such cases we failed to get 
the appropriate translation of the preposition as it 
in turn depends on the correct interpretation of the 
whole sentence. Ambiguity of numerals in a 
sentence is yet another case which lead to faliure, 
like the following example; 
 
(12) At 83, Vajpayee is overweight. 
 
In the above sentence, the number 83 can either 
mean this persons? (Vajpayee) age or his weight. 
The target side translation takes different 
preposition sense for these two interpretation. 
Hindi takes para and in Telugu ?at? is not-
translated when we treat 83 as weight, and when 
treated as age, we get mem and lo/ki in Hindi and 
Telugu respectively. 
We found that certain prepositions occur in large 
number of metonymical usage, like, ?with? and 
?at?. The constraints in a rule have been formulated 
for the general usage and not the extended usage of 
a given word. The example below shows one such 
instance; 
 
(13) Great bowlers spend hours after hours at 
the nets. 
 
While looking in WordNet for the various 
senses of ?net? not a single sense matches with the 
kind of usage in which ?net? is used in the above 
sentence. 
Certain rules for some of the preposition were 
found to be very general, the low performance of 
?for? and ?to? in telugu and hindi respectively are 
mainly due to this reason. In general, formulating 
rules (English-Hindi) for preposition ?to? was very 
difficult. This was because ?to? can have around 10 
senses in Hindi. The rules with very general 
constraints tend to satisfy cases where they should 
have failed. One has to revisit them and revise 
them. 
4 Conclusion and Future Work 
In this paper we described an approach to select 
the appropriate sense for a preposition from an 
English to Indian language MT perspective, we 
discussed the issues involved in the task, we ex-
plained the steps to achieve the required task; 
which are, semantic and context extraction, and 
sense selection. We reported the performance of 
the system, and showed that our approach gives 
promising results. We also discussed the identified 
problems during the error analysis; such as noise 
generation by WordNet. 
One of the pertinent tasks for the future would 
be to come up with a solution to reduce the noise 
generated by WordNet. The scope of rule file in 
terms of handling more prepositions needs to be 
broadened. We would like to extend this work to 
handle complex preposition. Finally, we would like 
to explore if ML techniques can be combined with 
the rule base to exploit the benefits of both the ap-
proaches. 
57
References 
Yukiko Sasaki Alam. 2004. Decision Trees for Sense 
Disambiguation of Prepositions: Case of Over. In 
HLT/NAACL-04. 
Marija M. Brala. 2000. Understanding and translating 
(spatial) prepositions: an exercise in cognitive se-
mantics for lexicographic purposes. 
Joseph Emonds. 1985. A unified theory of syntactic      
categories. Dordrecht: Foris. 
Gilles Fauconnier. 1994. Mental spaces. Cambridge: 
Cambridge University Press. 
M. Grimaud. 1988. Toponyms, Prepositions, and Cog-
native Maps in English and French, Journal of 
American Society of Geolinguistics. 
Ebba Gustavii. 2005. Target Language Preposition Se-
lection - an Experiment with Transformation-based 
Learning and Aligned Bilingual Data. In Proceedings 
of EAMT 2005. 
Sven Hartrumpf, Hermann Helbig and Rainer Osswald. 
2005. Semantic Interpretation of Prepositions for 
NLP Applications. In EACL 2006 Workshop: Third 
ACL-SIGSEM Workshop on Prepositions 
A. Herskovits. 1986. Language and Spatial Cognition, 
An Interdisciplinary Study of Prepositions in English. 
Cambridge University Press. 
Cliffort Hill. 1982. Up/down, front/back, left/right. A 
contrastive study of Hausa and English. In Weissen 
born and Klein, 13-42. 
Ray Jackendoff. 1977. The architecture of the language. 
Cambridge, MA: MIT Press. 
Nathalie Japkowicz and Janyce M. Wiebe. 1991. A Sys-
tem For Translating Locative Preposition From Eng-
lish Into French. In Proc. Association for Computa-
tional Linguistics. 
George Lakoff and Mark Johnson. 1980. Metaphors we 
live by. Chicago: University of Chicago Press. 
Beth Levin. 1993. English verb classes and alterna-
tions. Chicago/London: The University of Chicago-
Press.  
George A. Miller. 1990. WordNet: An online lexical 
database. International Journal of Lexicography. 
Sudip Kumar Naskar and Sivaji Bandyopadhyay. 2005. 
Handling of Prepositions in English to Bengali Ma-
chine Translation. In EACL 2006 Workshop: Third 
ACL-SIGSEM Workshop on Prepositions. 
Geoffrey Pullum and Rodney Huddleston. 2002. Prepo-
sitions and prepositional phrases. In Huddleston and 
Pullum (eds.), 597-661.  
Gisa Rauh. 1993. On the grammar of lexical and 
nonlexical prepositions in English. In Zelinskiy-
Wibbelt (eds.), 99-150. 
Patrick Saint-Dizier and Gloria Vazquez. 2001. A com-
positional framework for prepositions. In IWCS4, 
Tilburg, Springer, lecture notes, p. 165-179. 
Patrick Saint-Dizier. 2005. PrepNet: A framework for 
describing prepositions: Preliminary investigation re-
sults. In Proc. of IWCS 6, Tilburg. 
Joseph M. Sopena, Agusti LLoberas and Joan L. 
Moliner. 1998. A connectionist approach to preposi-
tional phrase attachment for real world texts. In COL-
ING-ACL ?98, 1233-1237. 
T. Tezuka, R. Lee, H. Takakura, and Y. Kambayashi. 
2001. Web-Based Inference Rules for Processing 
Conceptual Geo-graphical Relationships. In Proc. of 
the 2nd Int. Conf. on Web Information Systems Engi-
neering, The 1st Int. Workshop on Web Geographical 
Information Systems. 
Arturo Trujillo. 1992. Locations in the machine transla-
tion of prepositional phrases. In Proc. TMI-92. 
58
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 77?80,
Paris, October 2009. c?2009 Association for Computational Linguistics
Two stage constraint based hybrid approach to free word order lan-
guage dependency parsing
Akshar Bharati, Samar Husain, Dipti Misra and Rajeev Sangal
Language Technologies Research Centre, IIIT-Hyderabad, India
{samar, dipti, sangal}@mail.iiit.ac.in
Abstract
The paper describes the overall design of a 
new two stage constraint based hybrid ap-
proach to dependency parsing. We define 
the  two  stages  and  show  how  different 
grammatical construct are parsed at appro-
priate stages. This division leads to selec-
tive identification and resolution of specif-
ic dependency relations at the two stages. 
Furthermore,  we  show  how  the  use  of 
hard constraints and soft constraints helps 
us  build  an  efficient  and  robust  hybrid 
parser.  Finally,  we  evaluate  the  imple-
mented parser on Hindi and compare the 
results with that of two data driven depen-
dency parsers.
1 Introduction
Due to the availability of annotated corpora for 
various  languages  since  the  past  decade,  data 
driven parsing has proved to be immensely suc-
cessful.  Unlike  English,  however,  most  of  the 
parsers for morphologically rich free word order 
(MoR-FWO) languages (such as Czech, Turkish, 
Hindi, etc.) have adopted the dependency gram-
matical  framework.  It  is  well  known  that  for 
MoR-FWO  languages,  dependency  framework 
provides ease of linguistic analysis and is much 
better  suited to account  for  their  various struc-
tures (Shieber, 1975; Mel'Cuk, 1988; Bharati et 
al., 1995). The state of the art parsing accuracy 
for many MoR-FWO languages is still low com-
pared  to  that  of  English.  Parsing  experiments 
(Nivre et al,  2007; Hall  et  al.,  2007) for  these 
languages have pointed towards various reasons 
for this low performance. For Hindi1, (a) difficul-
ty in extracting relevant linguistic cues, (b) non-
projectivity,  (c)  lack  of  explicit  cues,  (d)  long 
distance  dependencies,  (e)  complex  linguistic  
phenomena,  and (f)  less corpus size, have been 
suggested (Bharati  et  al.,  2008) for  low perfor-
1  Hindi is a verb final language with free word order and 
a rich case marking system. It is one of the official lan-
guages of India, and is spoken by ~800 million people.
mance.  The  approach  proposed  in  this  paper 
shows how one can minimize these adverse ef-
fects and argues that a hybrid approach can prove 
to be a better option to parsing such languages. 
There have been, in the past, many attempts to 
parsing using constraint based approaches. Some 
recent  works include (Debusmann et  al.,  2004; 
Schr?der, 2002; Bharati et al, 1993).
The  paper  describes  the  overall  design  of  a 
new two stage constraint based hybrid approach 
to dependency parsing. We define the two stages 
and  show how different  grammatical  construct 
are  parsed  at  appropriate  stages.  This  division 
leads to selective identification and resolution of 
specific  dependency  relations  at  two  different 
stages.  Furthermore,  we  show  how  the  use  of 
hard  constraints  (H-constraints)  and  soft  con-
straints (S-constraints) helps us build an efficient 
and  robust  hybrid  parser.  Specifically,  H-con-
straints  incorporate  the  knowledge  base  of  the 
language  and  S-constraints  are  weights  corre-
sponding  to  various  constraints.  These  weights 
are automatically learnt from an annotated tree-
bank. Finally, we evaluate the implemented pars-
er on Hindi and compare the results with that of 
two data driven dependency parsers.
2 Two Stage Parsing
The parser tries to analyze the given input sen-
tence, which has already been POS tagged and 
chunked2, in 2 stages; it first tries to extract intra-
clausal3 dependency  relations.  These  relations 
generally correspond to the argument structure of 
the verb, noun-noun genitive relation, infinitive-
verb relation, infinitive-noun relation, adjective-
noun, adverb-verb relations, etc. In the 2nd stage 
it  then  tries  to  handle  more  complex  relations 
such as conjuncts, relative clause, etc. What this 
2  A chunk is a set of adjacent words which are in depen-
dency relation with each other, and are connected to the 
rest of the words by a single incoming arc. The parser 
marks relations between the head of the chunks (inter-
chunk relations); this is done to avoid local details and 
can be thought as a device for modularity.  
3  A clause is a group of word such that the group con-
tains a single finite verb chunk.
77
essentially means is  a 2-stage resolution of de-
pendencies, where the parser selectively resolves 
the dependencies of various lexical heads at their 
appropriate  stage,  for  example  verbs  in  the  1st 
stage  and  conjuncts  and  inter-verb  relations  in 
the 2nd  stage. The key ideas of the proposed lay-
ered  architecture  are:  (1)  There  are  two layers 
stages, (2) the 1st stage handles intra-clausal rela-
tions, and the 2nd stage handles inter-clausal rela-
tions, (3) the output of each layer is a linguisti-
cally valid partial parse that becomes, if neces-
sary, the input to the next layer, and (4) the out-
put of the final layer is the desired full parse.
By following the above approach we are able 
to get 4-fold advantage, (1) Each layer in effect 
does linguistically valid partial parsing, (2) by di-
viding  the  labels  into  different  functional  sets 
(intra-clausal  and  inter-clausal)  we localize  the 
dependencies  that  need  to  be  identified,  hence 
the  problem  of  long  distance  dependencies  is 
minimizes,  (3)  by  attacking  the  problem  in  a 
modular way, i.e. handling only individual claus-
es  at  1st stage,  we reduce non-projective  struc-
tures  significantly,  and  (4)  the  two stage  con-
straint  based approach can easily  capture  com-
plex linguistic cues that are difficult to learn via 
the data-driven parsers. We?ll revisit these points 
in Section 5. The 1st stage output for example 1 is 
shown in figure 1 (a).
Eg. 1: mai   ghar     gayaa   kyomki    mai 
          ?I?   ?home?  ?went?  ?because?  ?I?        
          bimaar   thaa
          ?sick?     ?was?
         ?I went home because I was sick?
Figure 1. Eg 1 (a): 1st stage output, (b): 2nd stage 
final parse
In figure 1a, the parsed matrix clause subtree 
?mai ghar gayaa? and the subordinate clause are 
attached to _ROOT_. The subordinating conjunct 
?kyomki? is  also seen attached to the _ROOT_. 
_ROOT_ ensures that the parse we get after each 
stage is connected and takes all the analyzed 1st 
stage sub-trees along with unprocessed nodes as 
its children. The dependency tree thus obtained 
in the 1st stage is partial, but linguistically sound. 
Later  in  the  2nd stage  the  relationship  between 
various clauses are identified. The 2nd stage parse 
for the above sentences is also shown in figure
1b.  Note  that  under  normal  conditions  the  2nd 
stage  does  not  modify  the  parse  sub-trees  ob-
tained from the 1st stage, it only establishes the 
relations between the clauses.
3 Hard and Soft Constraints
Both 1st and 2nd stage described in the previ-
ous  section  use  linguistically  motivated  con-
straints.  These  hard  constraints  (H-constraints) 
reflect that aspect of the grammar that in general 
cannot be broken. H-constraints comprise of lex-
ical  and  structural  knowledge  of  the  language. 
The H-constraints are converted into integer pro-
gramming  problem  and  solved  (Bharati  et  al., 
1995). The solution(s) is/are valid parse(s). The 
soft  constraints (S-constraints) on the other hand 
are learnt as weights from an annotated treebank. 
They reflect various preferences that a language 
has towards various linguistic phenomena. They 
are  used to  prioritize  the  parses  and select  the 
best parse. Both H & S constraints reflect the lin-
guistic realities of the language and together can 
be thought as the grammar of a language. Figure 
2 shows the overall design of the proposed parser 
schematically.
3.1 Hard Constraints 
The  core  language  knowledge  being  currently 
considered  that  cannot  be  broken  without  the 
sentence  being  called  ungrammatical  is  named 
H-constraints.  There  can  be  multiple  parses 
which can satisfy these H-constraints. This indi-
cates the  ambiguity in  the  sentence if  only the 
limited knowledge base is considered. Stated an-
other  way,  H-constraints  are  insufficient  to  re-
strict  multiple analysis of a given sentence and 
that  more  knowledge  (semantics,  other  prefer-
ences, etc.) is required to curtain the ambiguities. 
Moreover, we know that many sentences are syn-
tactically ambiguous unless one uses some prag-
matic knowledge, etc. For all such constructions 
there  are  multiple  parses.  As  described  earlier, 
H-constraints  are  used  during  intra-clausal  (1st 
stage)  and inter-clausal  (2nd stage)  analysis  (cf. 
Figure  2).  They  are  used  to  form  a  constraint 
graph which is converted into integer program-
ming equalities (or inequalities). These are then 
solved to get the final solution graph(s). Some of 
the H-constraints are: (1)  Structural constraints  
(ensuring the solution graph to be a tree,
78
Figure 2. Overall parser design
removing implausible language specific ungram-
matical  structures,  etc.),  (2)  Lexicon (linguistic 
demands  of various heads), and (3)  Other lexi-
cal constraints (some language specific  charac-
teristics), etc. 
3.2 Soft Constraints
The S-constraints on the other hand are the con-
straints which can be broken, and are used in the 
language as preferences. These are used during 
the prioritization stage. Unlike the H-constraints 
that are derived from a knowledge base and are 
used  to  form  a  constraint  graph,  S-constraints 
have  weights  assigned  to  them.  These  weights 
are automatically learnt using a manually anno-
tated  dependency  treebank.  The  tree  with  the 
maximum overall score is the best parse. Some 
such  S-constraints are,  (1)  Order of the argu-
ments, (2)  Relative position of arguments w.r.t.  
the verb, (3) Agreement principle, (4) Alignment  
of  prominence scale,  and (5)  Structural  prefer-
ences/General  graph properties  (mild  non-pro-
jectivity, valency, dominance, etc.), etc. 
4 Evaluation
Malt Parser (version 0.4) (Nivre et al, 2007), and 
MST  Parser  (version  0.4b)  (McDonald  et  al., 
2005) have been tuned for Hindi by Bharati et al 
(2008). Parsers were trained on a subset of a Hin-
di Treebank (Begum et al, 2008a). We use the 
same  experimental  setup  (parameters,  features, 
etc.) used by them and compare the results of the 
two data driven parsers with that of the proposed 
constraint  based  hybrid  parser  (CBP)  on  the 
same dataset4 in terms of
4 For details on the corpus type, annotation scheme, 
tagset, etc. see Begum et al (2008a).
unlabeled  attachments  (UA),  label  (L)  and  la-
beled  attachment  (LA)  accuracy.  In  Table  1, 
CBP? shows the performance of the system when 
a basic prioritizer is used, while CBP?? shows it 
for the best parse that is available in the first 25 
parses.  CBP  gives  the  accuracy  when  the  1st 
parse is selected. We show CBP?? to show that a 
good parse is available in as few as the first 25 
parses and that once the prioritizer is further im-
proved the overall performance will easily cross 
CBP??.
 UA LA L
CBP 86.1 63 65
CBP? 87.69 69.67 72.39
CBP? 90.1 75 76.9
MST 87.8 70.4 72.3
Malt 86.6 68.0 70.6
Table 1. Parser Evaluation
5 Observations
The initial results show that the proposed parser 
performs  better  than  the  state-of-the-art  data 
driven Hindi parsers. There are various reasons 
why we think that the proposed approach is bet-
ter  suited  to  parsing  MoR-FWO.  (1)  Complex 
linguistic cues can easily be encoded as part of 
various  constraints.  For  example,  it  has  been 
shown by Bharati  et  al.  (2008) that,  for  Hindi, 
complex  agreement  patterns,  though present  in 
the  data,  are  not  being  learnt  by  data  driven 
parsers. Such patterns along with other idiosyn-
cratic language properties can be easily incorpo-
rated as constraints, (2) Making clauses as basic 
parsing  unit  drastically  reduces  non-projective 
79
sentences.  Experiments  in  parsing  MoR-FOW 
have  shown that  such  non-projective  sentences 
impede parser performances (Bharati et al, 2008; 
Hall et al, 2007). Note that there will still remain 
some  intra-clausal  non-projective  structures  in 
the 1st stage, but they will be short distance de-
pendencies, (3) Use of H-constraints and S-con-
straints  together  reflect  the  grammar  of  a  lan-
guage. The rules in the form of H-constraints are 
complemented  by  the  weights  of  S-constraints 
learnt  from  the  annotated  corpus,  (4)  2  stage 
parsing lends  itself  seamlessly to  parsing com-
plex sentences by modularizing the task of over-
all parsing, (5) the problem of label bias (Bharati 
et  al.,  2008)  faced  by  the  data  driven  Hindi 
parsers for some cases does not arise here as con-
textually  similar  entities  are  disambiguated  by 
tapping  in  hard  to  learn  features,  (6)  Use  of 
clauses as basic parsing units reduces the search 
space at both the stages, (7) Parsing closely relat-
ed languages will become easy.
The performance of our parser is affected due 
to the following reasons,  (a)  Small lexicon (lin-
guistic  demands  of  various  heads):  The  total 
number of such demand frames which the parser 
currently uses is very low. There are a total of 
around 300 frames, which have been divided into 
20  verb  classes  (Begum et  al.,  2008b).  As  the 
coverage of this lexicon increases, the efficiency 
will automatically increase. (b)  Unhandled con-
structions: The parser still doesn?t handle some 
constructions, such as the case when a conjunct 
takes another conjunct as its dependent, and (c) 
Prioritization mistakes: As stated earlier the pri-
oritizer being used is basic and is still being im-
proved.  The  overall  performance  will  increase 
with the improvement of the prioritizer.
6 Conclusion
In this paper we proposed a new two stage con-
straint  based  hybrid  approach  to  dependency 
parsing.  We showed  how  by modularizing  the 
task of overall parsing into 2 stages we can over-
come many problems faced by data driven pars-
ing. We showed how in the 1st stage only intra-
clausal dependencies are handled and later in the 
2nd stage the inter-clausal dependencies are iden-
tified.  We also briefly  described the  use  of  H-
constraints  and  S-constraints.  We  argued  that 
such constraints complement each other in get-
ting the best parse and that together they repre-
sent the grammar of the language. We evaluated 
our  system  for  Hindi  with  two  data  driven 
parsers.  Initial  results  show  that  the  proposed 
parser performs better than those parsers. Finally, 
we argued why the proposed hybrid approach is 
better suited to handle the challenges posed by 
MoR-FWO and gave few pointers as how we can 
further improve our performance.
The proposed parser is still being improved at 
various  fronts.  To  begin  with  a  prioritization 
mechanism has to be improved. We need to en-
rich the verb frame lexicon along with handling 
some unhandled constructions. This will be taken 
up as immediate future work.
References 
R. Begum, S.  Husain, A. Dhwaj, D. Sharma, L. Bai, 
and  R.  Sangal.  2008a.  Dependency  annotation 
scheme for Indian languages. Proc. of IJCNLP08.
R. Begum, S. Husain, D. Sharma and L. Bai. 2008b. 
Developing  Verb  Frames  in  Hindi.  Proc.  of  
LREC08.
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma 
and R. Sangal. 2008. Two Semantic features make 
all  the  difference  in  Parsing  accuracy.  Proc.  of  
ICON-08.
A. Bharati and R. Sangal.  1993. Parsing Free Word 
Order  Languages  in  the  Paninian  Framework. 
Proc. of ACL: 93.
A. Bharati, V. Chaitanya and R. Sangal. 1995.  Natu-
ral Language Processing: A Paninian Perspective, 
Prentice-Hall of India, New Delhi. 
R. Debusmann, D. Duchier and G. Kruijff. 2004. Ex-
tensible  dependency grammar: A new methodolo-
gy.  Proceedings  of  the  Workshop on  Recent  Ad-
vances in Dependency Grammar, pp. 78?85.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, 
M.  Nilsson  and  M.  Saers.  2007.  Single  Malt  or 
Blended? A Study in Multilingual Parser Optimiza-
tion. Proc. of EMNLP-CoNLL shared task 2007.
R. McDonald,  F.  Pereira,  K. Ribarov,  and J.  Hajic. 
2005.  Non-projective  dependency  parsing  using 
spanning tree algorithms. Proc. of HLT/EMNLP.
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. NLE.
S.  M.  Shieber.  1985.  Evidence  against  the  context-
freeness  of  natural  language.  In  Linguistics  and 
Philosophy, p. 8, 334?343.
I.  Schr?der.  2002.  Natural  Language  Parsing  with 
Graded Constraints. PhD thesis, Hamburg Univ.
80
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 657?660,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Data Driven Dependency Parsing using Clausal Information 

Phani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, Rajeev Sangal 
Language Technologies Research Centre, IIIT-Hyderabad, India. 
phani.gadde@research.iiit.ac.in, karan_jindal@students.iiit.ac.in, 
{samar,dipti,sangal}@mail.iiit.ac.in 
 
 
 
 
 
Abstract 
The paper describes a data driven dependency 
parsing approach which uses clausal informa-
tion of a sentence to improve the parser per-
formance. The clausal information is added 
automatically during the parsing process. We 
demonstrate the experiments on Hindi, a lan-
guage with relatively rich case marking sys-
tem and free-word-order. All the experiments 
are done using a modified version of 
MSTParser. We did all the experiments on the 
ICON 2009 parsing contest data. We achieved 
an improvement of 0.87% and 0.77% in unla-
beled attachment and labeled attachment accu-
racies respectively over the baseline parsing 
accuracies. 
1 Introduction 
Linguistic analysis of morphologically rich free-
word-order languages (MoRFWO) using depen-
dency framework have been argued to be more 
effective (Shieber, 1985; Mel??uk, 1988, Bharati et 
al., 1993). Not surprisingly, most parsers for such 
languages are dependency based (Nivre et al, 
2007a; Bharati et al, 2008a; Hall et al, 2007). In 
spite of availability of annotated treebanks, state-
of-the-art parsers for MoRFWO have not reached 
the performance obtained for English. Some of the 
reasons stated for the low performance are small 
treebank size, complex linguistic phenomenon, 
long-distance dependencies, and non-projective 
structures (Nivre et al, 2007a, 2007b; Bharati et 
al., 2008a).   
Several approaches have been tried to handle these 
difficulties in such languages. For Hindi, Bharati et   
al. (2008a) and Ambati et al (2009) used semantic 
features in parsing to reduce the negative impact of 
unavailable syntactic features and showed that use 
of minimal semantics can help in identifying cer-
tain core dependency labels. Various attempts have 
proved to simplify the structure by dividing the 
sentence into suitable linguistic units (Attardi and 
Dell?Orletta 2008; Bharati et al, 1993, 2008b, 
2009; Husain et al, 2009). These approaches han-
dle complex structures by breaking the parsing 
process into several steps. Attardi and Dell'Orletta 
(2008) used chunk information as a feature to 
MaltParser (Nivre et al, 2007a) for parsing Eng-
lish. Bharati et al, 1993 used the notion of local 
word groups, while Bharati et al, 2009 and Husain 
et al, 2009 used clauses.   
In this paper, we describe a data driven depen-
dency parsing approach which uses clausal infor-
mation of a sentence to improve the parser 
performance. Previous attempts at data driven 
parsing for Hindi have failed to exploit this feature 
explicitly. The clausal information is added auto-
matically during the parsing process. We demon-
strate the experiments on Hindi1. All the 
experiments are done using a modified version of 
MSTParser (McDonald et al, 2005a and the refer-
ences therein) (henceforth MST) on the ICON 
2009 parsing contest2 (Husain, 2009) data. We 
achieved an improvement of 0.87% and 0.77% in 
unlabeled attachment and labeled attachment accu-
racies respectively over the baseline parsing accu-
racies. 
                                                        
1 Hindi is a verb final language with free word order and a rich 
case marking system. It is an official language of India and is 
spoken by ~800 million people. 
2 http://www.icon2009.in/contests.html 
657
2 Why Clausal Information?  
Traditionally, a clause is defined as a group of 
words having a subject and a predicate. Clause 
boundary identification is the process of dividing 
the given sentence into a set of clauses. It can be 
seen as a partial parsing step after chunking, in 
which one tries to divide the sentence into mea-
ningful units. It is evident that most of the depen-
dents of words in a clause appear inside the same 
clause; in other words the dependencies of the 
words in a clause are mostly localized within the 
clause boundary. 
In the dependency parsing task, a parser has to 
disambiguate between several words in the sen-
tence to find the parent/child of a particular word. 
This work is to see whether the clause boundary 
information can help the parser to reduce the 
search space when it is trying to find the correct 
parent/child for a word. The search space of the 
parser can be reduced by a large extent if we solve 
a relatively small problem of identifying the claus-
es. Interestingly, it has been shown recently that 
most of the non-projective cases in Hindi are inter-
clausal (Mannem et al, 2009). Identifying clausal 
boundaries, therefore, should prove to be helpful in 
parsing non-projective structures. The same holds 
true for many long-distance dependencies. 
3 Experimental Setup 
3.1 Dataset 
The experiments reported in this paper have been 
done on Hindi; the data was released as part of the 
ICON 2009 parsing contest (Husain, 2009). The 
sentences used for this contest are subset of the 
Hyderabad Dependency Treebank (HyDT) devel-
oped for Hindi (Begum et al, 2008). The depen-
dency relations in the treebank are syntactico-
semantic. The dependency tagset in the annotation 
scheme has around 28 relations. The dependency 
trees in the treebank show relations between chunk 
heads. Note, therefore, that the experiments and 
results described in this paper are based on parse 
trees that have chunk head as nodes. 
The data provided in the task contained morpho-
logical features along with the lemma, POS tag, 
and coarse POS tag, for each word. These are six 
morphological features namely category, gender, 
number, person, vibhakti3 or TAM4 markers of the 
node 
3.2 Clause Boundary Identifier 
We used the Stage15 parser of Husain et al (2009), 
to provide the clause boundary information that is 
then incorporated as features during the actual 
parsing process. The Stage1 parser uses MST to 
identify just the intra-clausal relations. To achieve 
this, Husain et al, introduce a special dummy node 
named _ROOT_ which becomes the head of the 
sentence. All the clauses are connected to this 
dummy node with a dummy relation. In effect the 
Stage1 parser gives only intra-clausal relations. In 
the current work, we used MaltParser6 (Nivre et al, 
2007b) (henceforth Malt) to do this task. This is 
because Malt performs better than MST in case of 
intra-clausal relations, which are mostly short dis-
tance dependencies. We use the same algorithm 
and feature setting of Bharati et al, (2008a) to train 
the Stage1 parser. 
Since the above tool parses clauses, therefore 
along with the clause boundary information we 
also know the root of the clausal sub-tree. Several 
experiments were done to identify the most optim-
al set of clausal features available from the partial 
parse. The best results are obtained when the 
clause boundary information, along with the head 
information i.e. head node of a clause, is given as a 
feature to each node. 
We trained the Stage1 parser by converting the 
treebank data into the stage1 format, following the 
steps that were given in Husain et al (2009). This 
conversion depends on the definition of the clause. 
We experimented with different definitions of 
clause in order to tune the tool to give the optimal 
clause boundary and head information required for 
parsing. For the results reported in this paper, a 
clause is a sequence of words, with a single verb, 
unless the verb is a child of another verb. 
 
 
                                                        
3 Vibhakti is a generic term for preposition, post-position and 
suffix. 
4TAM: Tense, Aspect and Modality. 
5Stage1 handles intra-clausal dependency relations. These 
relations generally correspond to the argument structure of the 
verb, noun-noun genitive relation, infinitive-noun relation, 
adjective-noun, adverb-verb relations, etc. 
6 Malt version 1.2 
658
 Precision Recall 
Clause Boundary 84.83% 91.23% 
Head Information 92.42% 99.40% 
Table 1. Accuracies of the features being used 
 
Table 1 gives the accuracy of the clausal informa-
tion being used as features in parsing. It is clear 
from Table1 that the tool being used doesn?t have 
very high clause boundary identification perfor-
mance; nevertheless, the performance is sufficient 
enough to make an improvement in parsing expe-
riments. On the other hand, the head of the clause 
(or, the root head in the clausal sub-tree) is identi-
fied efficiently. All the above experiments for pa-
rameter tuning were done on the development data 
of the ICON 2009 parsing contest. 
3.3 Parser  
We used MSTParser7 for the actual parsing step. 
MST uses Chu-Liu-Edmonds Maximum Spanning 
Tree Algorithm for non-projective parsing and 
Eisner's algorithm for projective parsing (Eisner, 
1996). It uses online large margin learning as the 
learning algorithm (McDonald et al, 2005b). 
We modified MST so that it uses the clause 
boundary. Unlike the normal features that MST 
uses, the clause boundary features span across 
many words. 
. 
4 Experiments and Results 
We experimented with different combinations of 
the information provided in the data (as mentioned 
in 3.1). Vibhakti and TAM fields gave better re-
sults than others. This is consistent with the best 
previous settings for Hindi parsing (Bharati et al, 
2008a, Ambati et al, 2009). We used the results 
obtained using this setting as our baseline (F1). 
We first experimented by giving only the clause 
inclusion (boundary) information to each node 
(F2). This feature should help the parser reduce its 
search space during parsing decisions. Then, we 
provided only the head and non-head information 
(whether that node is the head of the clause or not) 
(F3). The head or non-head information helps in 
handling complex sentences that have more than 
                                                        
7 MST version 0.4b 
one clause and each verb in the sentence has its 
own argument structure. We achieved the best per-
formance by using both as features (F4) during the 
parsing process. 
 
 LA (%) UA (%) L (%) 
F1 73.62 91.00 76.04 
F2 72.66 91.00 74.74 
F3 73.88 91.35 75.78 
F4 74.39 91.87 76.21 
Table 2. Parsing accuracies with different features 
 
Table 2 gives the results for all the settings. It is 
interesting to note that the boundary information 
(F1) alone does not cross the baseline; however 
this feature is reliable enough to give the best per-
formance when combined with F3. 
5 Observations  
We see from the above results (F4 in Table 2) that 
there is a rise of 0.87% in UA (unlabeled 
attachment) and 0.77% in LA (labeled attachment) 
over previous best (F1).  This shows the positive 
effect of using the clausal information during the 
parsing process. 
We analyzed the performance of both the pars-
ers in handling the long distance dependencies and 
non-projective dependencies. We found that the 
non-projective arcs handled by F4 have a precision 
and recall of 41.1% and 50% respectively for UA, 
compared to 30.5% and 39.2% for the same arcs 
during F1. 
 
 
Figure 1. Distance stats 
 
Figure 1 compares the accuracies of the depen-
dencies at various distances. It is clear that the ef-
fect of clausal information become more 
659
pronounced as the distance increases. This means 
F4 does help the parser in effectively handling long 
distance dependencies as well. 
6 Conclusion and Future Work  
The results show that there is a significant 
improvement in the parsing accuracy when the 
clausal information is being used.  
The clausal information is presently being used 
only as attachment features in MST. Experiments 
can be done in future, to find out if there is a label 
bias to the clause boundary, which also helps in 
reducing the search space for specific labels. Im-
proving the feature set for the labeled parse also 
improves the unlabeled attachment accuracy, as 
MST does attachments and labels in a single step, 
and the labels of processed nodes will also be tak-
en in features. 
We can see from Table1 that the precision of the 
clause boundary is 84.83%. Using a tool, targeted 
at getting just the clausal information, instead of 
using a parser can improve the accuracy of the 
clausal information, which helps improving pars-
ing. 
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009. Experi-
ments in Indian Language Dependency Parsing. In 
Proceedings of the ICON09 NLP Tools Contest: In-
dian Language Dependency Parsing, pp 32-37.  
B. R. Ambati, P. Gade and C. GSK. 2009. Effect ofMi-
nimal Semantics on Dependency Parsing. In the Pro-
ceedings of RANLP 2009 Student Research 
Workshop. 
G. Attardi and F. Dell?Orletta. Chunking and Depen-
dency Parsing. LREC Workshop on Partial Parsing: 
Between Chunking and Deep Parsing. Marrakech, 
Morocco. 2008. 
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. In Proceedings of IJCNLP-2008. 
A. Bharati and R. Sangal. 1993. Parsing Free Word Or-
der Languages in the Paninian Framework. Proceed-
ings of ACL:93.  
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma 
and R. Sangal. 2008a. Two Semantic features make 
all the difference in Parsing accuracy. In Proceed-
ings. of International Conference on Natural Lan-
guage Processing-2008. 
A. Bharati, S. Husain, D. Sharma, and R. Sangal. 2008b. 
A two stage constraint based dependency parser for 
free word order languages. In Proceedings. of 
COLIPS International Conference on Asian Lan-
guage Processing. Thailand. 2008. 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
Two stage constraint based hybrid approach to free 
word order language dependency parsing. In the Pro-
ceedings of the 11th International Conference on 
Parsing Technologies (IWPT09). Paris. 2009. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson,M. Saers.2007. Single Malt or Blended? A 
Study in Multilingual Parser Optimization. 
In Proceedings of the CoNLL Shared Task Session of 
EMNLP-CoNLL 2007. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proceedings of ICON09 NLP Tools Con-
test:Indian Language Dependency Parsing. 
Hyderabad, India. 2009. 
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and Ra-
jeev Sangal. 2009. A modular cascaded approach to 
complete parsing. In the Proceedings of COLIPS In-
ternational Conference on Asian Language 
Processing. Singapore. 2009. 
P. Mannem and H. Chaudhry.2009. Insights into Non-
projectivity in Hindi. In ACL-IJCNLP Student paper 
workshop. 2009. 
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 
2005a. Non-projective dependency parsing using 
spanning tree algorithms. In the Proceedings of 
HLT/EMNLP, pp. 523?530. 
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In 
the Proceedings of ACL 2005. pp. 91?98. 
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceedings 
of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 
13(2), 95-135. 
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. In Linguistics and Phi-
losophy, p. 8, 334?343. 
660
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 22?30,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Two methods to incorporate local morphosyntactic features in Hindi de-
pendency parsing 
 
Bharat Ram Ambati, Samar Husain, Sambhav Jain, Dipti Misra Sharma 
and Rajeev Sangal 
Language Technologies Research Centre, IIIT-Hyderabad, India - 500032. 
{ambati,samar}@research.iiit.ac.in, sambhav-
jain@students.iiit.ac.in,{dipti,sangal}@mail.iiit.ac.in 
 
 
Abstract 
In this paper we explore two strategies to in-
corporate local morphosyntactic features in 
Hindi dependency parsing. These features are 
obtained using a shallow parser. We first ex-
plore which information provided by the shal-
low parser is most beneficial and show that 
local morphosyntactic features in the form of 
chunk type, head/non-head information, 
chunk boundary information, distance to the 
end of the chunk and suffix concatenation are 
very crucial in Hindi dependency parsing. We 
then investigate the best way to incorporate 
this information during dependency parsing. 
Further, we compare the results of various ex-
periments based on various criterions and do 
some error analysis. All the experiments were 
done with two data-driven parsers, MaltParser 
and MSTParser, on a part of multi-layered and 
multi-representational Hindi Treebank which 
is under development. This paper is also the 
first attempt at complete sentence level pars-
ing for Hindi. 
1 Introduction 
The dependency parsing community has since a 
few years shown considerable interest in parsing 
morphologically rich languages with flexible word 
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages, 
but it is also motivated by the observation that the 
performance obtained for these languages have not 
been very high (Nivre et al, 2007a). Attempts at 
handling various non-configurational aspects in 
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty 
and Sima'an, 2008; Eryigit et al, 2008; Seddah et 
al., 2009; Husain et al, 2009; Gadde et al, 2010). 
Among other things, it has been pointed out that 
the use of language specific features may play a 
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways, 
and it has been hypothesized that the integration of 
morphological and syntactic information could be 
a key to better accuracy. However, it has also been 
noted that incorporating these language specific 
features in parsing is not always straightforward 
and many intuitive features do not always work in 
expected ways. 
In this paper we explore various strategies to in-
corporate local morphosyntactic features in Hindi 
dependency parsing. These features are obtained 
using a shallow parser. We conducted experiments 
with two data-driven parsers, MaltParser (Nivre et 
al., 2007b) and MSTParser (McDonald et al, 
2006). We first explore which information pro-
vided by the shallow parser is most beneficial and 
show that local morphosyntactic features in the 
form of chunk type, head/non-head information, 
chunk boundary information, distance to the end of 
the chunk and suffix concatenation are very crucial 
in Hindi dependency parsing. We then investigate 
the best way to incorporate this information during 
dependency parsing. All the experiments were 
done on a part of multi-layered and multi-
representational Hindi Treebank (Bhatt et al, 
2009)1.  
The shallow parser performs three tasks, (a) it 
gives the POS tags for each lexical item, (b) pro-
vides morphological features for each lexical item, 
and (c) performs chunking. A chunk is a minimal 
(non-recursive) phrase consisting of correlated, 
inseparable words/entities, such that the intra-
chunk dependencies are not distorted (Bharati et 
                                                          
1 This Treebank is still under development. There are currently 
27k tokens with complete sentence level annotation.  
22
al., 2006). Together, a group of lexical items with 
some POS tag and morphological features within a 
chunk can be utilized to automatically compute 
local morphosyntactic information. For example, 
such information can represent the postposi-
tion/case-marking in the case of noun chunks, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. In 
the experiments conducted for this paper such local 
information is automatically computed and incor-
porated as a feature to the head of a chunk. In gen-
eral, local morphosyntactic features correspond to 
all the parsing relevant local linguistic features that 
can be utilized using the notion of chunk. Previous-
ly, there have been some attempts at using chunk 
information in dependency parsing. Attardi and 
Dell?Orletta (2008) used chunking information in 
parsing English. They got an increase of 0.35% in 
labeled attachment accuracy and 0.47% in unla-
beled attachment accuracy over the state-of-the-art 
dependency parser. 
Among the three components (a-c, above), the 
parsing accuracy obtained using the POS feature is 
taken as baseline. We follow this by experiments 
where we explore how each of morph and chunk 
features help in improving dependency parsing 
accuracy. In particular, we find that local morpho-
syntactic features are the most crucial. These expe-
riments are discussed in section 2. In section 3 we 
will then see an alternative way to incorporate the 
best features obtained in section 2. In all the pars-
ing experiments discussed in section 2 and 3, at 
each step we explore all possible features and ex-
tract the best set of features. Best features of one 
experiment are used when we go to the next set of 
experiments. For example, when we explore the 
effect of chunk information, all the relevant morph 
information from previous set of experiments is 
taken into account.  
This paper is also the first attempt at complete 
sentence level parsing for Hindi. Due to the availa-
bility of dependency treebank for Hindi (Begum et 
al., 2008), there have been some previous attempts 
at Hindi data-driven dependency parsing (Bharati 
et al, 2008; Mannem et al, 2009; Husain et al, 
2009). Recently in ICON-09 NLP Tools Contest 
(Husain, 2009; and the references therein), rule-
based, constraint based, statistical and hybrid ap-
proaches were explored for dependency parsing. 
Previously, constraint based approaches to Indian 
language (IL) dependency parsing have also been 
explored (Bharati et al, 1993, 1995, 2009b, 
2009c). All these attempts, however, were finding 
inter-chunk dependency relations, given gold-
standard POS and chunk tags. Unlike these pre-
vious parsers, the dependencies in this work are 
between lexical items, i.e. the dependency tree is 
complete.  
The paper is arranged as follows, in section 2 
and 3, we discuss the parsing experiments. In sec-
tion 4, we describe the data and parser settings. 
Section 5 gives the results and discusses some re-
lated issues. General discussion and possible future 
work is mentioned in section 6. We conclude the 
paper in section 7. 
2 Getting the best linguistic features  
As mentioned earlier, a shallow parser consists of 
three main components, (a) POS tagger, (b) mor-
phological analyzer and (c) chunker. In this section 
we systematically explore what is the effect of 
each of these components. We?ll see in section 2.3 
that the best features of a-c can be used to compute 
local morphosyntactic features that, as the results 
show, are extremely useful. 
2.1 Using POS as feature (PaF): 
In this experiment we only use the POS tag infor-
mation of individual words during dependency 
parsing. First a raw sentence is POS-tagged. This 
POS-tagged sentence is then given to a parser to 
predict the dependency relations. Figure 1, shows 
the steps involved in this approach for (1). 
 
(1)  raama   ne         eka     seba        khaayaa  
  ?Ram?   ERG    ?one?  ?apple?      ?ate? 
       ?Ram ate an apple? 
 
Figure 1: Dependency parsing using only POS informa-
tion from a shallow parser. 
23
 
In (1) above, ?NN?, ?PSP?, ?QC?, ?NN? and ?VM? 
are the POS tags2 for raama, ne, eka, seba and 
khaayaa respectively. This information is provided 
as a feature to the parser. The result of this experi-
ment forms our baseline accuracy. 
2.2 Using Morph as feature (MaF): 
In addition to POS information, in this experiment 
we also use the morph information for each token. 
This morphological information is provided as a 
feature to the parser. Morph has the following in-
formation 
 
? Root: Root form of the word 
? Category: Course grained POS 
? Gender: Masculine/Feminine/Neuter 
? Number: Singular/Plural 
? Person: First/Second/Third person 
? Case: Oblique/Direct case 
? Suffix: Suffix of the word 
 
Take raama in (1), its morph information com-
prises of root = ?raama?, category = ?noun? gender 
= ?masculine?, number = ?singular?, person = 
?third?, case = ?direct?, suffix = ?0?. Similarly, 
khaayaa (?ate?) has the following morph informa-
tion. root = ?khaa?, category = ?verb? gender = 
?masculine?, numer = ?singular?, person = ?third?, 
case = ?direct?, suffix = ?yaa?. 
Through a series of experiments, the most cru-
cial morph features were selected. Root, case and 
suffix turn out to be the most important features. 
Results are discussed in section 5. 
2.3 Using local morphosyntax as feature 
(LMSaF) 
Along with POS and the most useful morph fea-
tures (root, case and suffix), in this experiment we 
also use local morphosyntactic features that reflect 
various chunk level information. These features 
are: 
? Type of the chunk 
? Head/non-head of the chunk 
                                                          
2 NN: Common noun, PSP: Post position, QC: Cardinal, VM: 
Verb. A list of complete POS tags can be found here: 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/POS-Tag-
List.pdf. The POS/chunk tag scheme followed in the Treebank 
is described in Bharati et al (2006). 
? Chunk boundary information 
? Distance to the end of the chunk 
? Suffix concatenation 
 
In example 1 (see section 2.1), there are two 
noun chunks and one verb chunk. raama and seba 
are the heads of the noun chunks. khaayaa is the 
head of the verb chunk. We follow standard IOB3 
notation for chunk boundary. raama,  eka and 
khaayaa are at the beginning (B) of their respective 
chunks. ne and seba are inside (I) their respective 
chunks. raama is at distance 1 from the end of the 
chunk and ne is at a distance 0 from the end of the 
chunk. 
Once we have a chunk and morph feature like 
suffix, we can perform suffix concatenation auto-
matically. A group of lexical items with some POS 
tags and suffix information within a chunk can be 
utilized to automatically compute this feature. This 
feature can, for example, represent the postposi-
tion/case-marking in the case of noun chunk, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. 
Note that, this feature becomes part of the lexical 
item that is the head of a chunk. Take (2) as a case 
in point: 
 
(2) [NP raama/NNP   ne/PSP]     [NP seba/NN]        
              ?Ram?           ERG                ?apple?   
      [VGF khaa/VM     liyaa/VAUX] 
                 ?eat?           ?PRFT? 
      ?Ram ate an apple? 
 
The suffix concatenation feature for khaa, which 
is the head of the VGF chunk, will be ?0+yaa? and 
is formed by concatenating the suffix of the main 
verb with that of its auxiliary. Similarly, the suffix 
concatenation feature for raama, which is head of 
the NP chunk, will be ?0+ne?. This feature turns 
out to be very important. This is because in Hindi 
(and many other Indian languages) there is a direct 
correlation between the TAM markers and the case 
that appears on some nominals (Bharati et al, 
1995). In (2), for example, khaa liyaa together 
gives the past perfective aspect for the verb khaa-
naa ?to eat?. Since, Hindi is split ergative, the sub-
ject of the transitive verb takes an ergative case 
marker when the verb is past perfective. Similar 
                                                          
3 Inside, Outside, Beginning of the chunk. 
24
correlation between the case markers and TAM 
exist in many other cases. 
3 An alternative approach to use best fea-
tures: A 2-stage setup (2stage) 
So far we have been using various information 
such as POS, chunk, etc. as features. Rather than 
using them as features and doing parsing at one go, 
we can alternatively follow a 2-stage setup. In par-
ticular, we divide the task of parsing into:  
 
? Intra-chunk dependency parsing 
? Inter-chunk dependency parsing 
 
We still use POS, best morphological features 
(case, suffix, root) information as regular features 
during parsing. But unlike LMSaF mentioned in 
section 2.3, where we gave local morphosyntactic 
information as a feature, we divided the task of 
parsing into sub-tasks. A similar approach was also 
proposed by Bharati et al (2009c). During intra-
chunk dependency parsing, we try to find the de-
pendency relations of the words within a chunk. 
Following which, chunk heads of each chunk with-
in a sentence are extracted. On these chunk heads 
we run an inter-chunk dependency parser. For each 
chunk head, in addition to POS tag, useful morpho-
logical features, any useful intra-chunk information 
in the form of lexical item, suffix concatenation, 
dependency relation are also given as a feature. 
 
Figure 2: Dependency parsing using chunk information: 
2-stage approach. 
Figure 2 shows the steps involved in this ap-
proach for (1). There are two noun chunks and one 
verb chunk in this sentence. raama and seba are 
the heads of the noun chunks. khaaya is the head 
of the verb chunk. The intra-chunk parser attaches 
ne to raama and eka to seba with dependency la-
bels ?lwg__psp? and ?nmod__adj?4 respectively. 
Heads of each chunk along with its POS, morpho-
logical features, local morphosyntactic features and 
intra-chunk features are extracted and given to in-
ter-chunk parser. Using this information the inter-
chunk dependency parser marks the dependency 
relations between chunk heads. khaaya becomes 
the root of the dependency tree. raama and seba 
are attached to khaaya with dependency labels ?k1? 
and ?k2?5 respectively. 
4 Experimental Setup 
In this section we describe the data and the parser 
settings used for our experiments.  
4.1 Data 
For our experiments we took 1228 dependency 
annotated sentences (27k tokens), which have 
complete sentence level annotation from the new 
multi-layered and multi-representational Hindi 
Treebank (Bhatt et al, 2009). This treebank is still 
under development. Average length of these sen-
tences is 22 tokens/sentence and 10 
chunks/sentence. We divided the data into two 
sets, 1000 sentences for training and 228 sentences 
for testing.  
4.2 Parsers and settings 
All experiments were performed using two data-
driven parsers, MaltParser6 (Nivre et al, 2007b), 
and MSTParser7 (McDonald et al, 2006).
                                                          
4 nmod__adj is an intra-chunk label for quantifier-noun mod-
ification. lwg__psp is the label for post-position marker. De-
tails of the labels can be seen in the intra-chunk guidelines 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/IntraChunk-
Dependency-Annotation-Guidelines.pdf 
5 k1 (karta) and k2 (karma) are syntactico-semantic labels 
which have some properties of both grammatical roles and 
thematic roles. k1 behaves similar to subject and agent. k2 
behaves similar to object and patient (Bharati et al, 1995; 
Vaidya et al, 2009). For complete tagset, see (Bharati et al, 
2009). 
6 Malt Version 1.3.1 
7 MST Version 0.4b 
25
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 89.4 78.2 80.5 90.4 80.1 82.4 86.3 75.1 77.9 87.9 77.0 79.3 
MaF 89.6 80.5 83.1 90.4 81.7 84.1 89.1 79.2 82.5 90.0 80.9 83.9 
LMSaF 91.5 82.7 84.7 91.8 84.0 86.2 90.8 79.8 82.0 92.0 81.8 83.8 
2stage 91.8 83.3 85.3 92.4 84.4 86.3 92.1 82.2 84.3 92.7 84.0 86.2 
Table 1: Results of all the four approaches using gold-standard shallow parser information. 
 
Malt is a classifier based shift/reduce parser. It 
provides option for six parsing algorithms, namely, 
arc-eager, arc-standard, convington projective, co-
vington non-projective, stack projective, stack ea-
ger and stack lazy. The parser also provides option 
for libsvm and liblinear learning model. It uses 
graph transformation to handle non-projective trees 
(Nivre and Nilsson, 2005). MST uses Chu-Liu-
Edmonds (Chu and Liu, 1965; Edmonds, 1967) 
Maximum Spanning Tree algorithm for non-
projective parsing and Eisner's algorithm for pro-
jective parsing (Eisner, 1996). It uses online large 
margin learning as the learning algorithm (McDo-
nald et al, 2005). In this paper, we use MST only 
for unlabeled dependency tree and use a separate 
maximum entropy model8 (MaxEnt) for labeling. 
Various combination of features such as node, its 
parent, siblings and children were tried out before 
arriving at the best results. 
As the training data size is small we did 5-fold 
cross validation on the training data for tuning the 
parameters of the parsers and for feature selection. 
Best settings obtained using cross-validated data 
are applied on test set. We present the results both 
on cross validated data and on test data.  
For the Malt Parser, arc-eager algorithm gave 
better performance over others in all the approach-
es. Libsvm consistently gave better performance 
over liblinear in all the experiments. For SVM set-
tings, we tried out different combinations of best 
SVM settings of the same parser on different lan-
guages in CoNLL-2007 shared task (Hall et al, 
2007) and applied the best settings. For feature 
model, apart from trying best feature settings of the 
same parser on different languages in CoNLL-
2007 shared task (Hall et al, 2007), we also tried 
out different combinations of linguistically intui-
tive features and applied the best feature model. 
The best feature model is same as the feature mod-
el used in Ambati et al (2009a), which is the best 
                                                          
8 http://maxent.sourceforge.net/ 
performing system in the ICON-2009 NLP Tools 
Contest (Husain, 2009). 
For the MSTParser, non-projective algorithm, 
order=2 and training-k=5 gave best results in all 
the approaches. For the MaxEnt, apart from some 
general useful features, we experimented consider-
ing different combinations of features of node, par-
ent, siblings, and children of the node.  
5 Results and Analysis 
All the experiments discussed in section 2 and 3 
were performed considering both gold-standard 
shallow parser information and automatic shallow 
parser9 information. Automatic shallow parser uses 
a rule based system for morph analysis, a 
CRF+TBL based POS-tagger and chunker. The 
tagger and chunker are 93% and 87% accurate re-
spectively. These accuracies are obtained after us-
ing the approach of PVS and Gali, (2007) on larger 
training data. In addition, while using automatic 
shallow parser information to get the results, we 
also explored using both gold-standard and auto-
matic information during training. As expected, 
using automatic shallow parser information for 
training gave better performance than using gold 
while training.  
Table 1 and Table 2 shows the results of the four 
experiments using gold-standard and automatic 
shallow parser information respectively. We eva-
luated our experiments based on unlabeled attach-
ment score (UAS), labeled attachment score (LAS) 
and labeled score (LS) (Nivre et al, 2007a). Best 
LAS on test data is 84.4% (with 2stage) and 75.4% 
(with LMSaF) using gold and automatic shallow 
parser information respectively. These results are 
obtained using MaltParser. In the following sub-
section we discuss the results based on different 
criterion.
                                                          
9 http://ltrc.iiit.ac.in/analyzer/hindi/ 
26
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 82.2 69.3  73.4  84.6  72.9  76.5  79.4  66.5  70.7  81.6  69.4  73.1  
MaF 82.5 71.6  76.1  84.0  73.6  77.6  82.3  70.4  75.4  83.4  72.7  77.3  
LMSaF 83.2 73.0  77.0  85.5  75.4  78.9  82.6  71.3  76.1  85.0  73.4  77.3  
2stage 79.0 69.5 75.6 79.6 71.1 76.8 78.8  66.6  72.6 80.1  69.7  75.4  
Table 2: Results of all the four experiments using automatic shallow parser information. 
 
POS tags provide very basic linguistic informa-
tion in the form of broad grained categories. The 
best LAS for PaF while using gold and automatic 
tagger were 80.1% and 72.9% respectively. The 
morph information in the form of case, suffix and 
root information proved to be the most important 
features. But surprisingly, gender, number and per-
son features didn?t help. Agreement patterns in 
Hindi are not straightforward. For example, the 
verb agrees with k2 if the k1 has a post-position; it 
may also sometimes take the default features. In a 
passive sentence, the verb agrees only with k2. The 
agreement problem worsens when there is coordi-
nation or when there is a complex verb. It is un-
derstandable then that the parser is unable to learn 
the selective agreement pattern which needs to be 
followed.  
LMSaF on the other hand encode richer infor-
mation and capture some local linguistic patterns. 
The first four features in LMSaF (chunk type, 
chunk boundary, head/non-head of chunk and dis-
tance to the end of chunk) were found to be useful 
consistently. The fifth feature, in the form of suffix 
concatenation, gave us the biggest jump, and cap-
tures the correlation between the TAM markers of 
the verbs and the case markers on the nominals. 
5.1 Feature comparison: PaF, MaF vs. 
LMSaF 
Dependency labels can be classified as two types 
based on their nature, namely, inter-chunk depen-
dency labels and intra-chunk labels. Inter-chunk 
dependency labels are syntacto-semantic in nature. 
Whereas intra-chunk dependency labels are purely 
syntactic in nature.  
Figure 3, shows the f-measure for top six inter-
chunk and intra-chunk dependency labels for PaF, 
MaF, and LMSaF using Maltparser on test data 
using automatic shallow parser information. The 
first six labels (k1, k2, pof, r6, ccof, and k7p) are 
the top six inter-chunk labels and the next six la-
bels (lwg__psp, lwg__aux, lwg__cont, rsym, 
nmod__adj, and pof__cn) are the top six intra-
chunk labels. First six labels (inter-chunk) corres-
pond to 28.41% and next six labels (intra-chunk) 
correspond to 48.81% of the total labels in the test 
data. The figure shows that with POS information 
alone, f-measure for top four intra-chunk labels 
reached more than 90% accuracy. The accuracy 
increases marginally with the addition of morph 
and local morphosytactic features. The results cor-
roborates with our intuition that intra-chunk de-
pendencies are mostly syntactic.  For example, 
consider an intra-chunk label ?lwg__psp?. This is 
the label for postposition marker. A post-position 
marker succeeding a noun is attached to that noun 
with the label ?lwg__psp?. POS tag for post-
position marker is PSP. So, if a NN (common 
noun) or a NNP (proper noun) is followed by a 
PSP (post-position marker), then the PSP will be 
attached to the preceding NN/NNP with the de-
pendency label ?lwg_psp?. As a result, providing 
POS information itself gave an f-measure of 98.3% 
for ?lwg_psp?.  With morph and local morphosy-
tactic features, this got increased to 98.4%. How-
ever, f-measure for some labels like ?nmod__adj? 
is around 80% only. ?nmod__adj? is the label for 
adjective-noun, quantifier-noun modifications. 
Low accuracy for these labels is mainly due to two 
reasons. One is POS tag errors. And the other is 
attachment errors due to genuine ambiguities such 
as compounding. 
For inter-chunk labels (first six columns in the 
figure 3), there is considerable improvement in the 
f-measure using morph and local morphosytactic 
features. As mentioned, local morphosyntactic fea-
tures provide local linguistic information. For ex-
ample, consider the case of verbs. At POS level, 
there are only two tags ?VM? and ?VAUX? for 
main verbs and auxiliary verbs respectively (Bha-
rati et al, 2006). Information about finite/non-
finiteness is not present in the POS tag. But, at 
chunk level there are four different chunk tags for
27
30
40
50
60
70
80
90
100
k1 k2 pof r6 ccof k7p lwg__psp lwg__vaux lwg__cont rsym nmod__adj pof__cn
PaF
MaF
LMaF
Figure 3: F-measure of top 6, inter-chunk and intra-chunk labels for PaF, MaF and LMSaF approaches using Malt-
parser on test data using automatic shallow parser information. 
 
verbs, namely VGF, VGNF, VGINF and VGNN. 
They are respectively, finite, non-finite, infinitival 
and gerundial chunk tags. The difference in the 
verbal chunk tag is a good cue for helping the 
parser in identifying different syntactic behavior of 
these verbs. Moreover, a finite verb can become 
the root of the sentence, whereas a non-finite or 
infinitival verb can?t. Thus, providing chunk in-
formation also helped in improving the correct 
identification of the root of the sentence. 
Similar to Prague Treebank (Hajicova, 1998), 
coordinating conjuncts are heads in the treebank 
that we use. The relation between a conjunct and 
its children is shown using ?ccof? label. A coordi-
nating conjuct takes children of similar type only. 
For example, a coordinating conjuct can have two 
finite verbs or two non-finite verbs as its children, 
but not a finite verb and a non-finite verb. Such 
instances are also handled more effectively if 
chunk information is incorporated. The largest in-
crease in performance, however, was due to the 
?suffix concatenation? feature. Significant im-
provement in the core inter-chunk dependency la-
bels (such as k1, k2, k4, etc.) due to this feature is 
the main reason for the overall improvement in the 
parsing accuracy. As mentioned earlier, this is be-
cause this feature captures the correlation between 
the TAM markers of the verbs and the case mark-
ers on the nominals. 
5.2 Approach comparison: LMSaF vs. 2stage 
Both LMSaF and 2stage use chunk information. In 
LMSaF, chunk information is given as a feature 
whereas in 2stage, sentence parsing is divided into 
intra-chunk and inter-chunk parsing. Both the ap-
proaches have their pros and cons. In LMSaF as 
everything is done in a single stage there is much 
richer context to learn from. In 2stage, we can pro-
vide features specific to each stage which can?t be 
done in a single stage approach (McDonald et al, 
2006). But in 2stage, as we are dividing the task, 
accuracy of the division and the error propagation 
might pose a problem. This is reflected in the re-
sults where the 2-stage performs better than the 
single stage while using gold standard information, 
but lags behind considerably when the features are 
automatically computed.  
During intra-chunk parsing in the 2stage setup, 
we tried out using both a rule-based approach and 
a statistical approach (using MaltParser). The rule 
based system performed slightly better (0.1% 
LAS) than statistical when gold chunks are consi-
dered. But, with automatic chunks, the statistical 
approach outperformed rule-based system with a 
difference of 7% in LAS. This is not surprising 
because, the rules used are very robust and mostly 
based on POS and chunk information. Due to er-
rors induced by the automatic POS tagger and 
chunker, the rule-based system couldn?t perform 
well. Consider a small example chunk given be-
low. 
 ((    NP 
 meraa ?my?   PRP  
 bhaaii ?brother? NN 
)) 
As per the Hindi chunking guidelines (Bharati et 
al., 2006), meraa and bhaaii should be in two sepa-
rate chunks. And as per Hindi dependency annota-
tion guidelines (Bharati et al, 2009), meraa is 
attached to bhaaii with a dependency label ?r6?10. 
When the chunker wrongly chunks them in a single 
                                                          
10?r6? is the dependency label for genitive relation. 
28
chunk, intra-chunk parser will assign the depen-
dency relation for meraa. Rule based system can 
never assign ?r6? relation to meraa as it is an inter-
chunk label and the rules used cannot handle such 
cases. But in a statistical system, if we train the 
parser using automatic chunks instead of gold 
chunks, the system can potentially assign ?r6? la-
bel.  
5.3 Parser comparison: MST vs. Malt 
In all the experiments, results of MaltParser are 
consistently better than MST+MaxEnt. We know 
that Maltparser is good at short distance labeling 
and MST is good at long distance labeling (McDo-
nald and Nivre, 2007). The root of the sentence is 
better identified by MSTParser than MaltParser. 
Our results also confirm this. MST+MaxEnt and 
Malt could identify the root of the sentence with an 
f-measure of 89.7% and 72.3% respectively. Pres-
ence of more short distance labels helped Malt to 
outperform MST. Figure 5, shows the f-measure 
relative to dependency length for both the parsers 
on test data using automatic shallow parser infor-
mation for LMSaF.  
30
40
50
60
70
80
90
100
0 5 10 15+
Dependency Length
f-
m
ea
su
re
Malt
MST+MaxEnt
 
Figure 5: Dependency arc f-measure relative to depen-
dency length. 
6 Discussion and Future Work 
We systematically explored the effect of various 
linguistic features in Hindi dependency parsing. 
Results show that POS, case, suffix, root, along 
with local morphosyntactic features help depen-
dency parsing. We then described 2 methods to 
incorporate such features during the parsing 
process. These methods can be thought as different 
paradigms of modularity. For practical reasons (i.e. 
given the POS tagger/chunker accuracies), it is 
wiser to use this information as features rather than 
dividing the task into two stages.  
As mentioned earlier, this is the first attempt at 
complete sentence level parsing for Hindi. So, we 
cannot compare our results with previous attempts 
at Hindi dependency parsing, due to, (a) The data 
used here is different and (b) we produce complete 
sentence parses rather than chunk level parses. 
As mentioned in section 5.1, accuracies of intra-
chunk dependencies are very high compared to 
inter-chunk dependencies. Inter-chunk dependen-
cies are syntacto-semantic in nature. The parser 
depends on surface syntactic cues to identify such 
relations. But syntactic information alone is always 
not sufficient, either due to unavailability or due to 
ambiguity. In such cases, providing some semantic 
information can help in improving the inter-chunk 
dependency accuracy. There have been attempts at 
using minimal semantic information in dependency 
parsing for Hindi (Bharati et al, 2008). Recently, 
Ambati et al (2009b) used six semantic features 
namely, human, non-human, in-animate, time, 
place, and abstract for Hindi dependency parsing. 
Using gold-standard semantic features, they 
showed considerable improvement in the core in-
ter-chunk dependency accuracy. Some attempts at 
using clause information in dependency parsing for 
Hindi (Gadde et al, 2010) have also been made. 
These attempts were at inter-chunk dependency 
parsing using gold-standard POS tags and chunks. 
We plan to see their effect in complete sentence 
parsing using automatic shallow parser information 
also.  
7 Conclusion 
In this paper we explored two strategies to incorpo-
rate local morphosyntactic features in Hindi de-
pendency parsing. These features were obtained 
using a shallow parser. We first explored which 
information provided by the shallow parser is use-
ful  and showed that local morphosyntactic fea-
tures in the form of chunk type, head/non-head 
info, chunk boundary info, distance to the end of 
the chunk and suffix concatenation are very crucial 
for Hindi dependency parsing. We then investi-
gated the best way to incorporate this information 
during dependency parsing. Further, we compared 
the results of various experiments based on various 
criterions and did some error analysis. This paper 
was also the first attempt at complete sentence lev-
el parsing for Hindi. 
29
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing. In 
Proc of the ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, pp 32-37.  
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. 
Effect of Minimal Semantics on Dependency Pars-
ing. In Proc of RANLP09 student paper workshop. 
G. Attardi and F. Dell?Orletta. 2008. Chunking and De-
pendency Parsing. In Proc of LREC Workshop on 
Partial Parsing: Between Chunking and Deep Pars-
ing. Marrakech, Morocco. 
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. In Proc of IJCNLP-2008. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural 
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi. 
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 
and R. Sangal. 2008. Two semantic features make all 
the difference in parsing accuracy. In Proc of ICON. 
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. 
AnnCorra: Annotating Corpora Guidelines for POS 
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad. 
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam 
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi 
TreeBank. 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
2009b. Two stage constraint based hybrid approach 
to free word order language dependency parsing. In 
Proc. of IWPT. 
A. Bharati, S. Husain, M. Vijay, K. Deepak, D. M. 
Sharma and R. Sangal. 2009c. Constraint Based Hy-
brid Approach to Parsing Indian Languages. In Proc 
of PACLIC 23. Hong Kong. 2009. 
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 
M. Sharma and F. Xia. 2009. Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu. In 
Proc. of the Third LAW at 47th ACL and 4th IJCNLP. 
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400. 
J. Edmonds. 1967. Optimum branchings. Journal of 
Research of the National Bureau of Standards, 
71B:233?240. 
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc of 
COLING-96, pp. 340?345. 
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency 
Parsing of Turkish. Computational Linguistics 34(3), 
357-389. 
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. 
Sangal. 2010. Improving Data Driven Dependency 
Parsing using Clausal Information. In Proc of 
NAACL-HLT 2010, Los Angeles, CA. 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation. In Proc of 
TSD?98. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson and M. Saers. 2007. Single Malt or Blended? 
A Study in Multilingual Parser Optimization. In Proc 
of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, 933?939. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proc of ICON09 NLP Tools Contest: In-
dian Language Dependency Parsing. Hyderabad, 
India. 
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. 
Sangal. 2009. A modular cascaded approach to com-
plete parsing. In Proc. of the COLIPS IALP. 
P. Mannem, A. Abhilash and A. Bharati. 2009. LTAG-
spinal Treebank and Parser for Hindi. In Proc of In-
ternational Conference on NLP, Hyderabad. 2009. 
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In 
Proc of ACL. pp. 91?98. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. In Proc of the Tenth (CoNLL-X), pp. 
216?220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models. In 
Proc. of EMNLP-CoNLL. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proc of 
EMNLP/CoNLL-2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 
13(2), 95-135. 
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL-2005, pp. 99?106. 
Avinesh PVS and K. Gali. 2007. Part-Of-Speech Tag-
ging and Chunking Using Conditional Random 
Fields and Transformation Based Learning. In Proc 
of the SPSAL workshop during IJCAI '07. 
D. Seddah, M. Candito and B. Crabb?. 2009. Cross 
parser evaluation: a French Treebanks study. In Proc. 
of IWPT, 150-161. 
R. Tsarfaty and K. Sima'an. 2008. Relational-
Realizational Parsing. In Proc. of CoLing, 889-896. 
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 
2009. A karaka-based dependency annotation scheme 
for English. In Proc. of CICLing, 41-52. 
30
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 94?102,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
On the Role of Morphosyntactic Features in Hindi Dependency Parsing 
 
Bharat Ram Ambati*, Samar Husain*, Joakim Nivre? and Rajeev Sangal* 
*Language Technologies Research Centre, IIIT-Hyderabad, India. 
?Department of Linguistics and Philology, Uppsala University, Sweden. 
{bharat,samar}@research.iiit.ac.in, joakim.nivre@lingfil.uu.se, san-
gal@mail.iiit.ac.in 
 
 
 
 
Abstract 
This paper analyzes the relative importance of 
different linguistic features for data-driven 
dependency parsing of Hindi, using a feature 
pool derived from two state-of-the-art parsers.  
The analysis shows that the greatest gain in 
accuracy comes from the addition of morpho-
syntactic features related to case, tense, aspect 
and modality. Combining features from the 
two parsers, we achieve a labeled attachment 
score of 76.5%, which is 2 percentage points 
better than the previous state of the art. We fi-
nally provide a detailed error analysis and 
suggest possible improvements to the parsing 
scheme. 
1 Introduction 
The dependency parsing community has since a 
few years shown considerable interest in parsing 
morphologically rich languages with flexible word 
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages, 
but it is also motivated by the observation that the 
performance obtained for these languages has not 
been very high (Nivre et al, 2007a). Attempts at 
handling various non-configurational aspects in 
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty 
and Sima'an, 2008; Eryigit et al, 2008; Seddah et 
al., 2009; Husain et al, 2009; Gadde et al, 2010). 
Among other things, it has been pointed out that 
the use of language specific features may play a 
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways, 
and it has been hypothesized that the integration of 
morphological and syntactic information could be 
a key to better accuracy. However, it has also been 
noted that incorporating these language specific 
features in parsing is not always straightforward 
and many intuitive features do not always work in 
expected ways. 
In this paper, we are concerned with Hindi, an 
Indian language with moderately rich morphology 
and relatively free word order.  There have been 
several previous attempts at parsing Hindi as well 
as other Indian languages (Bharati et al, 1995, 
Bharati et al, 2009b). Many techniques were tried 
out recently at the ICON09 dependency parsing 
tools contest (Husain, 2009). Both the best per-
forming system (Ambati et al, 2009a) and the sys-
tem in second place (Nivre, 2009b) used a 
transition-based approach to dependency parsing, 
as implemented in MaltParser (Nivre et al, 2007b). 
Other data driven parsing efforts for Indian lan-
guages in the past have been Bharati et al (2008), 
Husain et al (2009), Mannem et al (2009b) and 
Gadde et al (2010). 
In this paper, we continue to explore the transi-
tion-based approach to Hindi dependency parsing, 
building on the state-of-the-art results of Ambati et 
al. (2009a) and Nivre (2009b) and exploring the 
common pool of features used by those systems. 
Through a series of experiments we select features 
incrementally to arrive at the best parser features. 
The primary purpose of this investigation is to 
study the role of different morphosyntactic features 
in Hindi dependency parsing, but we also want to 
improve the overall parsing accuracy. Our final 
results are 76.5% labeled and 91.1% unlabeled at-
tachment score, improving previous results by 2 
and 1 percent absolute, respectively. In addition to 
this, we also provide an error analysis, isolating 
specific linguistic phenomena and/or other factors 
that impede the overall parsing performance, and 
suggest possible remedies for these problems. 
94
2 The Hindi Dependency Treebank 
Hindi is a free word order language with SOV as 
the default order. This can be seen in (1), where 
(1a) shows the constituents in the default order, 
and the remaining examples show some of the 
word order variants of (1a). 
 
(1) a. malaya  ne     sameer      ko     kitaba   dii.  
          Malay   ERG  Sameer    DAT   book    gave 
        ?Malay gave the book to Sameer? (S-IO-DO-V)1 
       b. malaya ne kitaba sameer ko dii. (S-DO-IO-V) 
       c. sameer ko malaya ne kitaba dii. (IO-S-DO-V) 
       d. sameer ko kitaba malaya ne dii. (IO-DO-S-V) 
       e. kitaba malaya ne sameer ko dii. (DO-S-IO-V) 
        f. kitaba sameer ko malaya ne dii.  (DO-IO-S-V) 
 
Hindi also has a rich case marking system, al-
though case marking is not obligatory. For exam-
ple, in (1), while the subject and indirect object are 
explicitly marked for the ergative (ERG) and da-
tive (DAT) cases, the direct object is unmarked for 
the accusative.  
The Hindi dependency treebank (Begum et al, 
2008) used for the experiment was released as part 
of the ICON09 dependency parsing tools contest 
(Husain, 2009). The dependency framework (Bha-
rati et al, 1995) used in the treebank is inspired by 
Panini?s grammar of Sanskrit. The core labels, 
called karakas, are syntactico-semantic relations 
that identify the participant in the action denoted 
by the verb. For example, in (1), ?Malay? is the 
agent, ?book? is the theme, and ?Sameer? is the be-
neficiary in the activity of ?give?. In the treebank, 
these three labels are marked as k1, k2, and k4 re-
spectively. Note, however, that the notion of kara-
ka does not capture the ?global? semantics of 
thematic roles; rather it captures the elements of 
the ?local semantics? of a verb, while also taking 
cues from the surface level morpho-syntactic in-
formation (Vaidya et al, 2009).  The syntactic re-
lational cues (such as case markers) help identify 
many of the karakas. In general, the highest availa-
ble karaka,2 if not case-marked, agrees with the 
verb in an active sentence.  In addition, the tense, 
                                                          
1 S=Subject; IO=Indirect Object; DO=Direct Object; 
V=Verb; ERG=Ergative; DAT=Dative 
2 These are the karta karaka (k1) and karma karaka (k2). k1 
and k2 can be roughly translated as ?agent? and ?theme? re-
spectively. For a complete description of the tagset and the 
dependency scheme, see Begum et al (2008) and Bharati et al 
(2009a).  
aspect and modality (TAM) marker can many a 
times control the case markers that appear on k1. 
For example, in (1) ?Malay? takes an ergative case 
because of the past perfective TAM marker (that 
appears as a suffix in this case) of the main verb 
?gave?. Many dependency relations other than ka-
rakas are purely syntactic. These include relations 
such as noun modifier (nmod), verb modifier 
(vmod), conjunct relation (ccof), etc. 
Each sentence is manually chunked and then an-
notated for dependency relations. A chunk is a mi-
nimal, non-recursive structure consisting of 
correlated groups of words (Bharati et al, 2006). A 
node in a dependency tree represents a chunk head. 
Each lexical item in a sentence is also annotated 
with its part-of-speech (POS).  For all the experi-
ments described in this paper we use gold POS and 
chunk tags. Together, a group of lexical items with 
some POS tags within a chunk can be utilized to 
automatically compute coarse grained morphosyn-
tactic information. For example, such information 
can represent the postposition/case-marking in the 
case of noun chunks, or it may represent the TAM 
information in the case of verb chunks. In the ex-
periments conducted for this paper this local in-
formation is automatically computed and 
incorporated as a feature of the head of a chunk. 
As we will see later, such information proves to be 
extremely crucial during dependency parsing. 
For all the experiments discussed in section 4, 
the training and development data size was 1500 
and 150 sentences respectively. The training and 
development data consisted of ~22k and ~1.7k 
words respectively. The test data consisted of 150 
sentences (~1.6k words). The average sentence 
length is 19.85. 
3 Transition-Based Dependency Parsing 
A transition-based dependency parser is built of 
two essential components (Nivre, 2008): 
 
? A transition system for mapping sentences to 
dependency trees 
? A classifier for predicting the next transition for 
every possible system configuration 
 
 
 
 
95
 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS 
Stack:        top 1 5 1 7  9  
Input:        next 1 5 1 7  9  
Input:        next+1 2 5 6 7    
Input:        next+2 2       
Input:        next+3 2       
Stack:       top-1 3       
String:      predecessor of top 3       
Tree:        head of top 4       
Tree:        leftmost dep of next 4 5 6     
Tree:        rightmost dep of top     8   
Tree:        left sibling of rightmost dep of top     8   
Merge:     PTAG of top and next       10 
Merge:     CTAM and DEPREL of top       10 
 
Table 1. Feature pool based on selection from Ambati et al (2009a) and Nivre (2009b). 
 
Given these two components, dependency parsing 
can be realized as deterministic search  through the 
transition system, guided by the classifier. With 
this technique, parsing can be performed in linear 
time for projective dependency trees. Like Ambati 
et al (2009a) and Nivre (2009b), we use MaltPars-
er, an open-source implementation of transition-
based dependency parsing with a variety of transi-
tion systems and customizable classifiers.3 
3.1 Transition System 
Previous work has shown that the arc-eager projec-
tive transition system first described in Nivre 
(2003) works well for Hindi (Ambati et al, 2009a; 
Nivre, 2009b). A parser configuration in this sys-
tem contains a stack holding partially processed 
tokens, an input buffer containing the remaining 
tokens, and a set of arcs representing the partially 
built dependency tree. There are four possible tran-
sitions (where top is the token on top of the stack 
and next is the next token in the input buffer): 
 
? Left-Arc(r): Add an arc labeled r from next to 
top; pop the stack. 
? Right-Arc(r): Add an arc labeled r from top to 
next; push next onto the stack. 
? Reduce: Pop the stack. 
? Shift: Push next onto the stack. 
 
Although this system can only derive projective 
dependency trees, the fact that the trees are labeled 
                                                          
3 MaltParser is available at http://maltparser.org. 
allows non-projective dependencies to be captured 
using the pseudo-projective parsing technique pro-
posed in Nivre and Nilsson (2005). 
3.2 Classifiers 
Classifiers can be induced from treebank data us-
ing a wide variety of different machine learning 
methods, but all experiments reported below use 
support vector machines with a polynomial kernel, 
as implemented in the LIBSVM package (Chang 
and Lin, 2001) included in MaltParser. The task of 
the classifier is to map a high-dimensional feature 
vector representation of a parser configuration to 
the optimal transition out of that configuration. The 
features used in our experiments represent the fol-
lowing attributes of input tokens: 
 
? PTAG: POS tag of chunk head. 
? CTAG: Chunk tag. 
? FORM: Word form of chunk head. 
? LEMMA: Lemma of chunk head. 
? DEPREL: Dependency relation of chunk. 
? CTAM: Case and TAM markers of chunk.  
 
The PTAG corresponds to the POS tag associated 
with the head of the chunk, whereas the CTAG 
represent the chunk tag. The FORM is the word 
form of the chunk head, and the LEMMA is auto-
matically computed with the help of a morphologi-
cal analyzer. CTAM gives the local 
morphosyntactic features such as case markers 
(postpositions/suffixes) for nominals and TAM 
markers for verbs (cf. Section 2). 
96
The pool of features used in the experiments are 
shown in Table 1, where rows denote tokens in a 
parser configuration ? defined relative to the stack, 
the input buffer, the partially built dependency tree 
and the input string ? and columns correspond to 
attributes. Each non-empty cell represents a fea-
ture, and features are numbered for easy reference. 
4 Feature Selection Experiments 
Starting from the union of the feature sets used by 
Ambati et al (2009a and by Nivre (2009b), we 
first used 5-fold cross-validation on the combined 
training and development sets from the ICON09 
tools contest to select the pool of features depicted 
in Table 1, keeping all features that had a positive 
effect on both labeled and unlabeled accuracy. We 
then grouped the features into 10 groups (indicated 
by numbers 1?10 in Table 1) and reran the cross-
validation, incrementally adding different feature 
groups in order to analyze their impact on parsing 
accuracy. The result is shown in Figure 1. 
30
36
42
48
54
60
66
72
78
84
90
Ex
p1
Ex
p2
Ex
p3
Ex
p4
Ex
p5
Ex
p6
Ex
p7
Ex
p8
Ex
p9
Ex
p1
0
UAS
LAS
 
Figure 1. UAS and LAS of experiments 1-10; 5-fold 
cross-validation on training and development data of the 
ICON09 tools contest. 
 
Experiment 1: Experiment 1 uses a baseline 
model with only four basic features: PTAG and 
FORM of top and next. This results in a labeled 
attachment score (LAS) of 41.7% and an unlabeled 
attachment score (UAS) of 68.2%. 
Experiments 2?3: In experiments 2 and 3, the 
PTAG of contextual words of next and top are 
added. Of all the contextual words, next+1, 
next+2, next+3, top-1 and predecessor of top were 
found to be useful.4 Adding these contextual fea-
tures gave a modest improvement to 45.7% LAS 
and 72.7% UAS. 
Experiment 4: In experiment 4, we used the 
PTAG information of nodes in the partially built 
tree, more specifically the syntactic head of top 
and the leftmost dependent of next. Using these 
features gave a large jump in accuracy to 52% 
LAS and 76.8% UAS. This is because partial in-
formation is helpful in making future decisions. 
For example, a coordinating conjunction can have 
a node of any PTAG category as its child. But all 
the children should be of same category. Knowing 
the PTAG of one child therefore helps in identify-
ing other children as well. 
Experiments 5?7: In experiments 5, 6 and 7, 
we explored the usefulness of CTAG, FORM, and 
LEMMA attributes. These features gave small in-
cremental improvements in accuracy; increasing 
LAS to 56.4% and UAS to 78.5%. It is worth not-
ing in particular that the addition of LEMMA 
attributes only had a marginal effect on accuracy, 
given that it is generally believed that this type of 
information should be beneficial for richly in-
flected languages. 
Experiment 8: In experiment 8, the DEPREL of 
nodes in the partially formed tree is used. The 
rightmost child and the left sibling of the rightmost 
child of top were found to be useful. This is be-
cause, if we know the dependency label of one of 
the children, then the search space for other child-
ren gets reduced. For example, a verb cannot have 
more than one k1 or k2. If we know that the parser 
has assigned k1 to one of its children, then it 
should use different labels for the other children. 
The overall effect on parsing accuracy is neverthe-
less very marginal, bringing LAS to 56.5% and 
UAS to 78.6%. 
Experiment 9: In experiment 9, the CTAM 
attribute of top and next is used. This gave by far 
the greatest improvement in accuracy with a huge 
jump of around 10% in LAS (to 66.3%) and 
slightly less in UAS (to 84.7%). Recall that CTAM 
consists of two important morphosyntactic fea-
tures, namely, case markers (as suffixes or postpo-
sitions) and TAM markers. These feature help 
because (a) case markers are important surface  
                                                          
4 The predecessor of top is the word occurring immediately 
before top in the input string, as opposed to top-1, which is the 
word immediately below top in the current stack. 
97
 
Figure 2. Precision and Recall of some important dependency labels. 
 
cues that help identify various dependency rela-
tions, and (b) there exists a direct mapping be-
tween many TAM labels and the nominal case 
markers because TAMs control the case markers of 
some nominals. As expected, our experiments 
show that the parsing decisions are certainly more 
accurate after using these features. In particular, (a) 
and (b) are incorporated easily in the parsing 
process.  
In a separate experiment we also added some 
other morphological features such as gender, num-
ber and person for each node. Through these fea-
tures we expected to capture the agreement in 
Hindi. The verb agrees in gender, number and per-
son with the highest available karaka. However, 
incorporating these features did not improve pars-
ing accuracy and hence these features were not 
used in the final setting. We will have more to say 
about agreement in section 5. 
Experiment 10: In experiment 10, finally, we 
added conjoined features, where the conjunction of 
POS of next and top and of CTAM and DEPREL 
of top gave slight improvements. This is because a 
child-parent pair type can only take certain labels. 
For example, if the child is a noun and the parent is 
a verb, then all the dependency labels reflecting 
noun, adverb and adjective modifications are not 
relevant. Similarly, as noted earlier, certain case-
TAM combinations demand a particular set of la-
bels only. This can be captured by the combination 
tried in this experiment. 
Experiment 10 gave the best results in the cross-
validation experiments. The settings from this ex-
periment were used to get the final performance on 
the test data. Table 2 shows the final results along 
with the results of the first and second best per-
forming systems in the ICON09 tools contest. We 
see that our system achieved an improvement of 2 
percentage points in LAS and 1 percentage point in 
UAS over the previous state of the art reported in 
Ambati et al (2009a). 
 
System LAS UAS 
Ambati et al (2009a) 74.5 90.1 
Nivre (2009b) 73.4 89.8 
Our system 76.5 91.1 
 
Table 2. Final results on the test data from the ICON09 
tools contest. 
5 Error Analysis 
In this section we provide a detailed error analysis 
on the test data and suggest possible remedies for 
problems noted. We note here that other than the 
reasons mentioned in this section, small treebank 
size could be another reason for low accuracy of 
the parser. The training data used for the experi-
ments only had ~28.5k words. With recent work on 
Hindi Treebanking (Bhatt et al, 2009) we expect 
to get more annotated data in the near future. 
Figure 2 shows the precision and recall of some 
important dependency labels in the test data. The 
labels in the treebank are syntacto-semantic in na-
ture. Morph-syntactic features such as case mark-
ers and/or TAM labels help in identifying these 
labels correctly. But lack of nominal postpositions 
can pose problems. Recall that many case mark-
ings in Hindi are optional. Also recall that the verb 
agrees with the highest available karaka. Since 
agreement features do not seem to help, if both k1 
and k2 lack case markers, k1-k2 disambiguation 
becomes difficult (considering that word order in-
formation cannot help in this disambiguation). In 
the case of k1 and k2, error rates for instances that 
lack post-position markers are 60.9% (14/23) and 
65.8% (25/38), respectively. 
 
 
98
 Correct Incorrect 
  k1 k1s k2 pof k7p k7t k7 others 
k1 184 5 3 8 3  1  3 
k1s 12 6  1 6    1 
k2 126 14  1 7 5   11 
pof 54 1 8 4      
k7p 54 3  7   1 2 3 
k7t 27 3  3 3  1  10 
k7 3 2   2 4    
 
Table 3. Confusion matrix for important labels. The 
diagonal under ?Incorrect? represents attachment errors. 
 
Table 3 shows the confusion matrix for some 
important labels in the test data. As the present 
information available for disambiguation is not 
sufficient, we can make use of some semantics to 
resolve these ambiguities. Bharati et al (2008) and 
Ambati et al (2009b) have shown that this ambi-
guity can be reduced using minimal semantics. 
They used six semantic features: human, non-
human, in-animate, time, place and abstract. Using 
these features they showed that k1-k2 and k7p-k7t 
ambiguities can be resolved to a great extent. Of 
course, automatically extracting these semantic 
features is in itself a challenging task, although 
?vrelid (2008) has shown that animacy features 
can be induced automatically from data. 
In section 4 we mentioned that a separate expe-
riment explored the effectiveness of morphological 
features like gender, number and person. Counter 
to our intuitions, these features did not improve the 
overall accuracy. Accuracies on cross-validated 
data while using these features were less than the 
best results with 66.2% LAS and 84.6% UAS. 
Agreement patterns in Hindi are not straightfor-
ward. For example, the verb agrees with k2 if the 
k1 has a post-position; it may also sometimes take 
the default features. In a passive sentence, the verb 
agrees only with k2. The agreement problem wor-
sens when there is coordination or when there is a 
complex verb. It is understandable then that the 
parser is unable to learn the selective agreement 
pattern which needs to be followed. Similar prob-
lems with agreement features have also been noted 
by Goldberg and Elhadad (2009). 
In the following sections, we analyze the errors 
due to different constructions and suggest possible 
remedies.  
 
 
5.1 Simple Sentences 
A simple sentence is one that has only one main 
verb. In these sentences, the root of the dependen-
cy tree is the main verb, which is easily identified 
by the parser. The main problem is the correct 
identification of the argument structure. Although 
the attachments are mostly correct, the dependency 
labels are error prone. Unlike in English and other 
more configurational languages, one of the main 
cues that help us identify the arguments is to be 
found in the nominal postpositions. Also, as noted 
earlier these postpositions are many times con-
trolled by the TAM labels that appear on the verb. 
There are four major reasons for label errors in 
simple sentences: (a) absence of postpositions, (b) 
ambiguous postpositions, (c) ambiguous TAMs, 
and (d) inability of the parser to exploit agreement 
features. For example in (2), raama and phala are 
arguments of the verb khaata. Neither of them has 
any explicit case marker. This makes it difficult for 
the parser to identify the correct label for these 
nodes. In (3a) and (3b) the case marker se is ambi-
guous. It signifies ?instrument? in (3b) and ?agent? 
in (3a). 
 
(2) raama    phala    khaata    hai 
     ?Ram?    ?fruit?    ?eat?       ?is? 
     ?Ram eats a fruit? 
 
(3) a. raama   se     phala  khaayaa nahi    gaya 
         ?Ram? INST ?fruit?  ?eat?      ?not?  ?PAST? 
         ?Ram could not eat the fruit? 
     b. raama  chamach   se     phala    khaata  hai 
         ?Ram?  ?spoon?   INST ?fruit?    ?eat?     ?is? 
          ?Ram eats fruit with spoon? 
5.2 Embedded Clauses 
Two major types of embedded constructions in-
volve participles and relative clause constructions. 
Participles in Hindi are identified through a set of 
TAM markers. In the case of participle embed-
dings, a sentence will have more than one verb, 
i.e., at least one participle and the matrix verb. 
Both the matrix (finite) verb and the participle can 
take their own arguments that can be identified via 
the case-TAM mapping discussed earlier. Howev-
er, there are certain syntactic constraints that limit 
the type of arguments a participle can take.  There 
99
are two sources of errors here: (a) argument shar-
ing, and (b) ambiguous attachment sites.  
Some arguments such as place/time nominals 
can be shared. Shared arguments are assigned to 
only one verb in the dependency tree. So the task 
of identifying the shared arguments, if any, and 
attaching them to the correct parent is a complex 
task. Note that the dependency labels can be identi-
fied based on the morphosyntactic features. The 
task becomes more complex if there is more than 
one participle in a sentence. 12 out of 130 in-
stances (9.23%) of shared arguments has an incor-
rect attachment.  
Many participles are ambiguous and making the 
correct attachment choice is difficult. Similar par-
ticiples, depending on the context, can behave as 
adverbials and attach to a verb, or can behave as 
adjectives and attach to a noun. Take (4) as a case 
in point.  
 
(4) maine     daurte   hue        kutte  ko   dekhaa 
     ?I?-ERG  (while) running   dog   ACC ?saw?             
 
In (4) based on how one interprets ?daurte hue?, 
one gets either the reading that ?I saw a running 
dog? or that ?I saw a dog while running?. In case of 
the adjectival participle construction (VJJ), 2 out of 
3 errors are due to wrong attachment. 
5.3 Coordination 
Coordination poses problems as it often gives rise 
to long-distance dependencies. Moreover, the tree-
bank annotation treats the coordinating conjunction 
as the head of the coordinated structure. Therefore, 
a coordinating conjunction can potentially become 
the root of the entire dependency tree. This is simi-
lar to Prague style dependency annotation (Hajico-
va, 1998). Coordinating conjunctions pose 
additional problems in such a scenario as they can 
appear as the child of different heads. A coordinat-
ing conjunction takes children of similar POS cat-
egory, but the parent of the conjunction depends on 
the type of the children.  
 
 
(5) a. raama  aur  shyaama   ne     khaana khaayaa                                    
        ?Ram? ?and? ?Shyam? ?ERG?  ?food?   ?ate?          
        ?Ram and Shyam ate the food.? 
 
 
     b. raama   ne    khaanaa  khaayaa  aur  paanii    
          ?Ram?  ?ERG? ?food?     ?ate?     ?and? ?water?  
         piyaa 
         ?drank? 
         ?Ram ate food and drank water.? 
 
In (5a), raama and shyaama are children of the 
coordinating conjunction aur, which gets attached 
to the main verb khaayaa with the label k1. In ef-
fect, syntactically aur becomes the argument of the 
main verb. In (5b), however, the verbs khaayaa 
and piyaa are the children of aur. In this case, aur 
becomes the root of the sentence. Identifying the 
nature of the conjunction and its children becomes 
a challenging task for the parser. Note that the 
number of children that a coordinating conjunction 
can take is not fixed either. The parser could iden-
tify the correct head of the conjunctions with an 
accuracy of 75.7% and the correct children with an 
accuracy of 85.7%. 
The nature of the conjunction will also affect the 
dependency relation it has with its head. For ex-
ample, if the children are nouns, then the conjunc-
tion behaves as a noun and can potentially be an 
argument of a verb. By contrast, if the children are 
finite verbs, then it behaves as a finite verb and can 
become the root of the dependency tree. Unlike 
nouns and verbs, however, conjunctions do not 
have morphological features. So a child-to-head 
feature percolation should help make a coordinat-
ing node more transparent. For example, in (5a) the 
Ergative case ne is a strong cue for the dependency 
label k1. If we copy this information from one of 
its children (here shyaama) to the conjunct, then 
the parser can possibly make use of this informa-
tion. 
5.4 Complex Predicates 
Complex predicates are formed by combining a 
noun or an adjective with a verbalizer kar or ho. 
For instance, in taariif karanaa ?to praise?, taariif 
?praise? is a noun and karanaa ?to do? is a verb. 
Together they form the main verb. Complex predi-
cates are highly productive in Hindi. Combination 
of the light verb and the noun/adjective is depen-
dent on not only syntax but also semantics and 
therefore its automatic identification is not always 
straightforward (Butt, 1995).  A noun-verb com-
plex predicate in the treebank is linked via the de-
pendency label pof. The parser makes mistakes in 
100
identifying pof or misclassifies other labels as pof. 
In particular, the confusion is with k2 and k1s 
which are object/theme and noun complements of 
k1, respectively. These labels share similar contex-
tual features like the nominal element in the verb 
complex. Table 3 includes the confusion matrix for 
pof errors.  
5.5 Non-Projectivity 
As noted earlier, MaltParser?s arc-eager parsing 
algorithm can be combined with the pseudo-
projective parsing techniques proposed in Nivre 
and Nilsson (2005), which potentially helps in 
identifying non-projective arcs. The Hindi treebank 
has ~14% non-projective arcs (Mannem et al, 
2009a). In the test set, there were a total of 11 non-
projective arcs, but the parser did not find any of 
them. This is consistent with earlier results show-
ing that pseudo-projective parsing has high preci-
sion but low recall, especially when the percentage 
of non-projective relations is small (Nilsson et al 
2007). 
Non-projectivity has proven to be one of the ma-
jor problems in dependency parsing, especially for 
free word-order languages. In Hindi, the majority 
of non-projective arcs are inter-clausal (Mannem et 
al., 2009a), involving conjunctions and relative 
clauses. There have been some attempts at han-
dling inter-clausal non-projectivity in Hindi. Hu-
sain et al (2009) proposed a two-stage approach 
that can handle some of the inter-clausal non-
projective structures. 
5.6 Long-Distance Dependencies  
Previous results on parsing other languages have 
shown that MaltParser has lower accuracy on long-
distance dependencies. Our results confirm this. 
Errors in the case of relative clauses and coordina-
tion can mainly be explained in this way. For ex-
ample, there are 8 instances of relative clauses in 
the test data. The system could identify only 2 of 
them correctly. These two are at a distance of 1 
from its parent. For the remaining 6 instances the 
distance to the parent of the relative clause ranges 
from 4 to 12. 
Figure 3 shows how parser performance de-
creases with increasing distance between the head 
and the dependent. Recently, Husain et al (2009) 
have proposed a two-stage setup to parse inter-
clausal and intra-clausal dependencies separately. 
They have shown that most long distance relations 
are inter-clausal, and therefore, using such a clause 
motivated parsing setup helps in maximizing both 
short distance and long distance dependency accu-
racy. In a similar spirit, Gadde et al (2010) showed 
that using clausal features helps in identifying long 
distance dependencies. They have shown that pro-
viding clause information in the form of clause 
boundaries and clausal heads can help a parser 
make better predictions about long distance depen-
dencies. 
0
20
40
60
80
100
0 2 4 6 8 10 12
Dependency Length
D
ep
en
de
nc
y 
Pr
ec
is
io
n
 
0
20
40
60
80
100
0 1 2 3 4 5 6 7 8 9 10 11 12
Dependency Length
D
ep
en
de
nc
y 
Re
ca
ll
 
Figure 3. Dependency arc precision/recall relative to 
dependency length, where the length of a dependency 
from wi to wj is |i-j| and roots are assumed to have dis-
tance 0 to their head. 
6 Conclusion 
In this paper we have analyzed the importance of 
different linguistic features in data-driven parsing 
of Hindi and at the same time improved the state of 
the art. Our main finding is that the combination of 
case markers on nominals with TAM markers on 
verbs is crucially important for syntactic disambig-
uation, while the inclusion of features such as per-
son, number gender that help in agreement has not 
yet resulted in any improvement. We have also 
presented a detailed error analysis and discussed 
possible techniques targeting different error 
classes. We plan to use these techniques to im-
prove our results in the near future. 
101
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing. 
Proc. of ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, 32-37.  
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. 
Effect of Minimal Semantics on Dependency Pars-
ing. Proc. of RANLP Student Research Workshop.  
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. Proc. of IJCNLP. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural 
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi. 
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 
and R. Sangal. 2008. Two semantic features make all 
the difference in parsing accuracy. Proc. of ICON. 
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. 
AnnCorra: Annotating Corpora Guidelines for POS 
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad. 
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam 
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi 
TreeBank. 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
2009b. Two stage constraint based hybrid approach 
to free word order language dependency parsing. In 
Proc. of IWPT. 
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 
M. Sharma and F. Xia. 2009. Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu. Proc. 
of the Third LAW at ACL-IJCNLP, 186-189. 
M. Butt. 1995. The Structure of Complex Predicates in 
Urdu. CSLI Publications. 
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency 
Parsing of Turkish. Computational Linguistics 34(3), 
357-389. 
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. 
Sangal. 2010. Improving Data Driven Dependency 
Parsing using Clausal Information. Proc. of NAACL-
HLT. 
Y. Goldberg and M. Elhadad. 2009. Hebrew Dependen-
cy Parsing: Initial Results. Proc. of IWPT, 129-133. 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation.  Proc. of 
TSD.  
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson and M. Saers. 2007. Single Malt or Blended? 
A Study in Multilingual Parser Optimization. Proc. 
of EMNLP-CoNLL, 933-939. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. Proc. of ICON09 NLP Tools Contest: Indian 
Language Dependency Parsing.  
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. 
Sangal. 2009. A modular cascaded approach to com-
plete parsing. Proc. of the COLIPS International 
Conference on Asian Language Processing.  
P. Mannem, H. Chaudhry, and A. Bharati. 2009a. In-
sights into non-projectivity in Hindi. Proc. of ACL-
IJCNLP Student Research Workshop. 
P. Mannem, A. Abhilash and A. Bharati. 2009b. LTAG-
spinal Treebank and Parser for Hindi. Proceedings of 
International Conference on NLP, Hyderabad. 2009. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. Proc. of CoNLL, 216-220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models.  
Proc. of EMNLP-CoNLL, 122-131. 
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York. 
J. Nilsson, J. Nivre and J. Hall. 2007. Generalizing Tree 
Transformations for Inductive Dependency Parsing. 
Proc. of ACL, 968-975. 
J. Nivre. 2008. Algorithms for Deterministic Incremen-
tal Dependency Parsing. Computational Linguistics 
34(4), 513-553. 
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In 
Proceedings of ACL-HLT, pp. 950-958. 
J. Nivre. 2009a.  Non-Projective Dependency Parsing in 
Expected Linear Time. Proc. of ACL-IJCNLP, 351-
359. 
J. Nivre. 2009b. Parsing Indian Languages with Malt-
Parser. Proc. of ICON09 NLP Tools Contest: Indian 
Language Dependency Parsing, 12-18. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. Proc. of 
EMNLP/CoNLL, 915-932. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. NLE, 13(2), 95-135. 
L. ?vrelid. 2008. Argument Differentiation. Soft con-
straints and data-driven models. PhD Thesis, Uni-
versity of Gothenburg. 
D. Seddah, M. Candito and B. Crabb?. 2009. Cross 
parser evaluation: a French Treebanks study. Proc. of 
IWPT, 150-161. 
R. Tsarfaty and K. Sima'an. 2008. Relational-
Realizational Parsing. Proc. of CoLing, 889-896. 
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 
2009. A karaka-based dependency annotation scheme 
for English. Proc. of CICLing, 41-52. 
102
Proceedings of the Fifth Law Workshop (LAW V), pages 134?142,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Empty Categories in Hindi Dependency Treebank: Analysis and Recovery
Chaitanya GSK
Intl Institute of Info. Technology
Hyderabad, India
chaitanya.gsk
@research.iiit.ac.in
Samar Husain
Intl Institute of Info. Technology
Hyderabad, India
samar
@research.iiit.ac.in
Prashanth Mannem
Intl Institute of Info. Technology
Hyderabad, India
prashanth
@research.iiit.ac.in
Abstract
In this paper, we first analyze and classify the
empty categories in a Hindi dependency tree-
bank and then identify various discovery pro-
cedures to automatically detect the existence
of these categories in a sentence. For this we
make use of lexical knowledge along with the
parsed output from a constraint based parser.
Through this work we show that it is possi-
ble to successfully discover certain types of
empty categories while some other types are
more difficult to identify. This work leads to
the state-of-the-art system for automatic inser-
tion of empty categories in the Hindi sentence.
1 Introduction
Empty categories play a crucial role in the annota-
tion framework of the Hindi dependency treebank1
(Begum et al, 2008; Bharati et al, 2009b). They
are inserted in a sentence in case the dependency
analysis does not lead to a fully connected tree. In
the Hindi treebank, an empty category (denoted by
a NULL node) always has at least one child. These
elements have essentially the same properties (e.g.
case-marking, agreement, etc.) as an overtly real-
ized element and they provide valuable information
(such as predicate-argument structure, etc.). A dif-
ferent kind of motivation for postulating empty cate-
gories comes from the demands of natural lan- guage
processing, in particular parsing. There are several
types of empty categories in the Hindi dependency
1The dependency treebank is part of a Multi Representa-
tional and Multi-Layered Treebank for Hindi/Urdu (Palmer et
al., 2009).
treebank serving different purposes. The presence
of these elements can be crucial for correct auto-
matic parsing. Traditional parsing algorithms do
not insert empty categories and require them to be
part of the input. The performance of such parser
will be severely affected if one removes these ele-
ments from the input data. Statistical parsers like
MaltParser (Nivre, 2003), MSTParser (McDonald,
2005), as well as Constraint Based Hybrid Parser
(CBHP) (Bharati et al, 2009a) produce incorrect
parse trees once the empty categories are removed
from the input data. Hence there is a need for auto-
matic detection and insertion of empty categories in
the Hindi data. Additionally, it is evident that suc-
cessful detection of such nodes will help the annota-
tion process as well.
There have been many approaches for the recov-
ery of empty categories in the treebanks like Penn
treebank, both ML based (Collins, 1997; Johnson,
2002; Dienes and Dubey, 2003a,b; Higgins, 2003)
and rule based (R Campbell, 2004). Some ap-
proaches such as Yang and Xue (2010) follow a post
processing step of recovering empty categories after
parsing the text.
In this paper we make use of lexical knowledge
along with the parsed output from a constraint based
parser to successfully insert empty category in the
input sentence, which may further be given for pars-
ing or other applications. Throughout this paper, we
use the term recovery (of empty categories) for the
insertion of different types of empty categories into
the input sentence.
The paper is arranged as follows, Section 2 dis-
cusses the empty nodes in the treebank and classifies
134
NULL NP tokens 69
NULL VG tokens 68
NULL CCP tokens 32
Sentences with more than
one empty category in them 159
Table 1: Empty categories in Hindi Tree bank
them based on their syntactic type. In section 3 we
provide an algorithm to automatically recover these
elements. Section 4 shows the performance of our
system and discusses the results. We conclude the
paper in section 5.
2 An overview of Empty Categories in
Hindi dependency Treebank
Begum et al, (2008) proposed a dependency frame-
work in which an empty node is introduced dur-
ing the annotation process only if its presence is
required to build the dependency tree for the sen-
tence (Figures 1, 2, 3) 2. Empty categories such as
those discussed in Bhatia et al (2010) which would
be leaf nodes in the dependency tree are not part
of the dependency structure and are added during
Propbanking3. Consequently, the empty categories
in Hindi treebank do not mark displacement as in
Penn treebank (Marcus et al, 1993) rather, they rep-
resent undisplaced syntactic elements which happen
to lack phonological realization. In the Hindi depen-
dency treebank, an empty category is represented by
a ?NULL? word. Sentences can have a missing VG
or NP or CCP 4. These are represented by ?NULL?
token and are marked with the appropriate Part-of-
speech tag along with marking the chunk tag such
as NULL NP, NULL VGF, NULL CCP, etc. in Ta-
ble 2
2Due to space constraints, sentences in all the figures only
show chunk heads. Please refer to examples 1 to 6 for entire
sentences with glosses
3These empty categories are either required to correctly cap-
ture the argument structure during propbanking or are required
to successfully convert the dependency structure to phrase struc-
ture (Xia et al, 2009)
4VG is Verb Group, NP is Noun Phrase and CCP is Conjunct
Phrase.
Type of empty Inst- Chunk tag
categories ances (CPOS)
Empty subject 69 NULL NP
Backward gapping 29 NULL VG
Forward gapping 21 NULL VG
Finite verb ellipses 18 NULL VG
Conjunction ellipses
(verbs) 20 NULL CCP
Conjunction ellipses
(nouns) 12 NULL CCP
Total 169
Table 2: Empty category types.
2.1 Empty category types
From the empty categories recovery point of view,
we have divided the empty categories in the treebank
into six types (Table 2).
The first type of empty category is Empty Subject
(Figure 1), example.1 where a clause ?rava ke
kaaran hi manmohan singh rajaneeti me aaye? is
dependent on the missing subject of the verb ?hai?
(is).
(1) NULL gaurtalab hai ki raao
NULL ?noticeable? ?is? ?that? ?Rao?
ke kaaran hi manmohan sing
?because? ?only? ?Manmohan? ?singh?
raajaniiti me aaye
?politics? ?in? ?came.
?it is noticeable that because of Rao, Manmohan
Singh came in politics?
The second type of empty category is due to
Backward Gapping (Figure 2), example.2 where
the verb is absent in the clause that occurs before a
co-ordinating conjunct.
(2) doosare nambara para misa roosa
?second? ?position? ?on? ?miss? ?Russia?
natasha NULL aur tiisare nambara
?Natasha? NULL ?and? ?third? ?position?
para misa lebanan sendra rahiim .
?on? ?miss? ?Lebanan? ?Sandra? were? .
135
Figure 1: Empty Subject.
Figure 2: Backward Gapping.
Figure 3: Forward Gapping.
Figure 4: Finite verb ellipses.
Figure 5: Conjunction ellipses (verbs).
136
Figure 6: Conjunctuon ellipses (nouns).
?Miss Russia stood second and Miss Lebanan
was third?
The third type of empty category is Forward
Gapping (Figure 3), example 3, which is similar to
the second type but with the clause with the missing
verb occurring after the conjunct rather than before.
The reason for a separate class for forward gapping
is explained in the next section.
(3) divaalii ke dina jua Kele magara
?Diwali? ?GEN? ?day? ?gamble? ?play? ?but?
NULL gar me yaa hotala me
?NULL? ?home? ?in? ?or? ?hotel? ?in?
?Played gamble on Diwali day but was it at home
or hotel?
The fourth type of empty category is due to Finite
verb ellipses (Figure4), example 4, where the main
verb for a sentence is missing.
(4) saath me vahii phevareta khadaa pyaaja
?along? ?in? ?that? ?favorite? ?raw? ?onion?
NULL.
NULL
?Along with this, the same favorite semi-cooked
onion?
The fifth type of empty category is Conjunction
ellipses (Verbs), example 5 (Figure 5).
(5) bacce bare ho-ga-ye-hai NULL
?children? ?big? ?become? ?NULL?
kisii ki baat nahiin maante
?anyone? ?gen? ?advice? ?not? ?accept?
?The children have grown big (and) do not listen
to anyone?
The sixth type of empty category is the Conjunc-
tion ellipses (for nouns), example 6 (Figure 6).
(6) yamunaa nadii me uphaana se
?Yamuna? ?river? ?in? ?storm? ?INST?
sekado ekara gannaa, caaraa,
?thousands? ?acre? ?sugarcane? ?straw?
dhana, NULL sabjii kii phasale
?money? ?NULL? ?vegetable? ?GEN? ?crops?
jala-magna ho-gai-hai .
?drowned? ?happened?
?Because of the storm in the Yamuna river, thou-
sand acres of sugarcane, straw, money, vegetable
crops got submerged?
3 Empty categories recovery Algorithm
Given the limited amount of data available (only 159
sentences with at least one empty category in them
out of 2973 sentences in the Hindi treebank, Table
12 ), we follow a rule based approach rather than us-
ing ML to recover the empty catogories discussed in
the previous section. Interestingly, a rule-based ap-
proach was followed by R Campbell, (2004) that re-
covered empty categories in English resulting in bet-
ter performance than previous empirical approaches.
This work can be extended for ML once more data
becomes available.
The techniques that are used for recovering empty
categories in the Penn treebank (Collins, 1997;
Johnson, 2002;) might not be suitable since the Penn
treebank has all the empty categories as leaf nodes in
the tree unlike the Hindi dependency treebank where
137
for each sentence in the input data
try in Empty Subject
try in Forward Gapping
try in Finite Verb ellipses
for each tree in CBHP parse output
try in Backward Gapping
try in Forward Gapping
try in Finite Verb ellipses
try in Conjunction ellipses (for Verbs)
Table 3: Empty categories Recovery Algorithm.
the empty categories are always internal nodes in the
dependency trees (Figure 2).
In this section we describe an algorithm which
recovers empty categories given an input sentence.
Our method makes use of both the lexical cues as
well as the output of the Constraint Based Hybrid
Parser (CBHP). Table 3 presents the recovery algo-
rithm which first runs on the input sentence and then
on the output of the CBHP.
3.1 Empty Subject
Framing rule 1 requires the formation of a set (Cue-
Set) based on our analysis discussed in the previ-
ous section. It contains all the linguistic cues (lex-
ical items such as gaurtalab ?noticeable?, maloom
?known?, etc). We then scan the input sentence
searching for the cue and insert an empty category
(NULL NP)5 if the cue is found. Table 4 illustates
the process where we search for ?CueSet he ki? or
?CueSet ho ki? phrases. In Table 4, W+1 represents
word next to W, W+2 represents word next to W+1.
3.2 Backward Gapping
To handle backward gapping cases, we take the in-
termediate parse output from CBHP 6 for the whole
data. The reason behind choosing CBHP lies in its
rule based approach. CBHP fails (or rather gives
a visibly different parse) for sentences with miss-
ing verbs. And when it fails to find a verb, CBHP
5We insert a token ?NULL? with NULL NP as CPOS
6CBHP is a two-stage parser. In the 1st stage it parses intra-
clausal relations and inter-clausal relations in the 2nd stage. The
1st stage parse is an intermediate parse.
for each word W in the Sentence
if W  CueSet
if W+1 & W+2 = he or ho & ki
Insert NULL with PRP as POS,
NULL NP as CPOS
Table 4: Rule for identifying Empty Subject.
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
for each node X in unattached subtrees[]
while POS(X) is not VG
traverse in the array of unattached subtrees
if ? a conjunct, then recovery=1
if recovery = 1
insert NULL, with VM as POS,
NULL VG as CPOS
Head of NULL = ?
Table 5: Rule for identifying Backward Gapping using
CBHP.
gives unattached subtrees7 (Figure 7, 8, 9 illustrates
the unattached subtrees where the parser is unable to
find a relation between the heads of each unattached
subtree). Similarly whenever the parser expects a
conjunction and the conjunction is absent in the sen-
tence, CBHP again gives the unattached subtrees.
We analyze these unattached sub-trees to see
whether there is a possibility for empty category.
The array, in Table 5 represents all the nodes hav-
ing no heads. POS represents part of speech and
CPOS represents chunk part of speech and ? repre-
sents empty set.
3.3 Forward gapping
The main reason for handling the forward gapping as
a separate case rather than considering it along with
backward gapping is the prototypical SOV word-
order of Hindi, i.e. the verb occurs after subject and
object in a clause or sentence. We take the interme-
diate parse output from the CBHP for the whole data
and when ever a verb is absent in a clause occurring
immediately after a conjunct, we search for a VG af-
7CBHP gives fully connected trees in both the stages. We
have modified the parser so that it gives unattached subtrees
when it fails.
138
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
for each node X in unattached subtrees[]
if !? a verb between two conjuncts
if those conjuncts belongs to conjunct set
insert insert NULL with VM as POS,
NULL VG as CPOS
Table 6: Rule for identifying Forward Gapping using
CBHP.
for each word W in the sentence S
if W  CueSet FG
insert NULL with NULL VG as POS
and CPOS
if W = Conjunct
if POS(W-1) = VG
if !? a VG in S-W
insert NULL with VM as POS,
NULL VG as CPOS
Table 7: Rule for identifying Forward Gapping .
ter the conjunct and insert an empty category if the
VG is absent (an example of such cases can be seen
in Figure 7). This procedure is given in Table 6. In
addition, we use the lexical cues (such as ya nahii ?or
not?, ya ?or?) for recovering certain types of empty
categories. CueSet FG is the set that contains the
lexical cues and conjunct set contains lexical cues
like (ki and ya). This procedure is shown in Table 7.
Figure 7: Unattached sub trees in CBHP parse output of
an input sentence (forward gapping).
3.4 Finite Verb ellipses
In the cases where there is no VG at all in the sen-
tence, we insert a NULL VG before the EOS (End-
Of-Sentence) in the input sentence. For this case,
finite verb ellipses can be recovered directly from
if !? a VG in S-W
insert NULL with VM as POS,
NULL VG as CPOS
Table 8: Rule for identifying Finite Verb ellipses in sen-
tence.
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
if !? a verb in unattached subtrees[]
if those conjuncts belongs to conjunct set
insert insert NULL with VM as POS,
NULL VG as CPOS
Table 9: Rule for identifying Finite Verb ellipses using
CBHP.
the input sentence using the rule in Table 8 .Also,
in a sentence with a VG, we use CBHP to ascertain
if this VG is the root of the sentence. If its not, we
insert an additional NULL VG. This algorithm will
correctly recover VG in the sentence but the position
can be different from the gold input at times not be-
cause the recovery algorithm is wrong, but there is
no strict rule that says the exact position of empty
category in this case of finite verb ellipse and anno-
tators might choose to insert an empty category at
any position. For example, in Figure 8, we can in-
sert an empty category either after first NP sub tree
or second or the third etc, all these possibilities are
accepted syntactically. For simplicity purposes, we
insert the empty category just before the EOS. This
procedure is shown in Table 9.
3.5 Conjunction ellipses (for verbs)
We again use the intermediate parsed output of
CBHP for this type. Whenever there is a miss-
ing conjunction between the two finite clauses, the
clausal sub trees are disconnected from each other
as shown in Figure 9. Hence the rule that should
be applied is to insert a NULL CCP between two
sub trees with VG heads and insert NULL CCP im-
mediately after the first verb in the input sentence.
Table 10 shows this procedure.
139
Figure 8: Unattached Subtrees (Finite verb ellipses).
Figure 9: Unattached Subtrees in the case of conjunction ellipses.
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
for each node X in unattached subtrees[]
if X and X+1 are VG?s
insert insert NULL with CC as POS,
NULL CCP as CPOS
Table 10: Rule for identifying Finite Verb ellipses using
CBHP.
4 Results and Discussion
We have presented two sets of results, the overall
empty categories detection along with the accuracies
of individual types of empty categories in Table 11
and Table 12.
The results in Table 12 show that the precision in
recovering many empty categories is close to 90%.
A high precision value of 89.8 for recovery of Empty
subject type is due to the strong lexical cues that
were found during our analysis. CBHP parse out-
put proved helpful in most of the remaining types.
Few cases such as backward gapping and conjunc-
Type of empty Inst- Prec- Recall
categories ances ision
Empty subject 69 89.8 89.8
Backward gapping 29 77.7 48.3
Forward gapping 21 88.8 72.7
Finite verb ellipses 18 78.5 61.1
Conjunction ellipses 20 88.2 75
(verbs)
Conjunction ellipses 12 0 0
(nouns)
Total 169 91.4 69.8
Table 11: Recovery of empty categories in Hindi tree-
bank.
tion ellipses (for nouns) are very difficult to handle.
We see that although CBHP helps in the recovery
process by providing unattached subtrees in many
instances, there are cases such as those of backward
gapping and nominal conjunction ellipses where it
does not help. It is not difficult to see why this is
so. The presence of the 2nd verb in the case of back-
ward gapping fools CBHP into treating it as the main
verb of a normal finite clause. In such a case, the
140
Type of empty Inst- Prec- Recall
categories ances ision
NULL NP tokens 69 89.8 89.8
NULL VG tokens 68 82 60.2
NULL CCP tokens 32 88.2 46.8
Total 159 91.4 69.8
Table 12: Empty categories in Hindi Tree bank
parser ends up producing a fully formed tree (which
of course is a wrong analysis) that is of no use for
us.
Similar problem is faced while handling conjunc-
tion ellipses (for nouns). Here as in the previous
case, CBHP is fooled into treating two coordinat-
ing nominals as independent nouns. We note here
that both the cases are in fact notoriously difficult
to automatically detect because of the presence (or
absence) of any robust linguistic pattern.
These results show that our system can be used to
supplement the annotators effort during treebanking.
We plan to use our system during the ongoing Hindi
treebanking to ascertain it effect. As mentioned ear-
lier, automatic detection of empty categories/nodes
will prove to be indis pensable for parsing a sen-
tence. We also intend to see the effect of our system
during the task of parsing.
5 Conclusion
In this paper we presented an empty category recov-
ery algorithm by analyzing the empty categories in
the Hindi treebank. This, we noticed, uses lexical
cues and parsed output of a constraint based parser.
The results show that our system performs consid-
erably high ( 90%) for many types of empty cate-
gories. Few types, on the other hand, such as back-
ward gapping and nominal coordinating conjunc-
tions were very difficult to handle. Our approach
and analysis will be useful in automatic insertion of
empty nodes during dependency annotation. It will
also benefit data-driven/statistical approaches either
as a post-processing tool or in recovering empty cat-
egories by helping in feature selection for various
machine learning techniques.
Acknowledgments
We would like to thank Prof. Rajeev Sangal for pro-
viding valuable inputs throughout the work.
References
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,
and R. Sangal. Dependency annotation scheme for
Indian languages. 2008. In proceedings of Third
International Joint Conference on Natural Language
Processing (IJCNLP), Hyderabad, India
A. Bharati, S. Husain, D. Misra, and R. Sangal. Two
stage constraint based hybrid approach to free word
order language dependency parsing. 2009a. In
Proceedings of the 11th International Conference on
Parsing Technologies (IWPT). Paris.
A. Bharati, D. Sharma, S. Husain, L. Bai, R. Begam, and
R. Sangal. Anncorra: Treebanks for indian languages,
guidelines for annotating hindi treebank. 2009b.
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf
A. Bhatia, R. Bhatt, B. Narasimhan, M. Palmer, O. Ram-
bow, D. Sharma, M. Tepper, A. Vaidya, and F. Xia.
Empty Categories in a Hindi Treebank. 2010. In the
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC).
R. Campbell. Using linguistic principles to recover
empty categories. 2004. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics
A. Chanev. Portability of dependency parsing
algorithms?an application for Italian. 2005. In
Proc. of the fourth workshop on Treebanks and
Linguistic Theories (TLT). Citeseer.
M. Collins. Three generative, lexicalised models for
statistical parsing. 1997. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics and Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics.
P. Dienes and A. Dubey. Antecedent recovery: Experi-
ments with a trace tagger. 2003a. In Proceedings of
the 2003 conference on Empirical methods in natural
language processing.
141
P. Dienes and A. Dubey. Deep syntactic processing by
combining shallow methods. 2003b. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics-Volume 1.
D. Higgins. A machine-learning approach to the identifi-
cation of WH gaps. 2003. In Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics-Volume 2.
X. Fei, O. Rambow, R. Bhatt, M. Palmer, and D. Sharma.
Towards a multi-representational treebank. 2008.
Proc. of the 7th Int?lWorkshop on Treebanks and
Linguistic Theories (TLT-7)
M. Johnson. A simple pattern-matching algorithm
for recovering empty nodes and their antecedents.
2002. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics.
M. Marcus, M. Marcinkiewicz, and B. Santorini. Build-
ing a large annotated corpus of English: The Penn
Treebank. 1993. Computational linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
Non-projective dependency parsing using spanning
tree algorithms. 2005. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing.
J. Nivre. An efficient algorithm for projective depen-
dency parsing. 2003. In Proceedings of the 8th
International Workshop on Parsing Technologies
(IWPT).
M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow,
D. Sharma, and F. Xia. Hindi Syntax: Annotating
Dependency, Lexical Predicate-Argument Structure,
and Phrase Structure. 2009. In The 7th International
Conference on Natural Language Processing.
Y. Yang and N. Xue. Chasing the ghost: recovering
empty categories in the Chinese Treebank. 2010. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
142
