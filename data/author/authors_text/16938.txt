First Joint Conference on Lexical and Computational Semantics (*SEM), pages 441?448,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
TakeLab: Systems for Measuring Semantic Text Similarity
Frane S?aric?, Goran Glavas?, Mladen Karan,
Jan S?najder, and Bojana Dalbelo Bas?ic?
University of Zagreb
Faculty of Electrical Engineering and Computing
{frane.saric, goran.glavas, mladen.karan, jan.snajder, bojana.dalbelo}@fer.hr
Abstract
This paper describes the two systems for
determining the semantic similarity of short
texts submitted to the SemEval 2012 Task 6.
Most of the research on semantic similarity
of textual content focuses on large documents.
However, a fair amount of information is con-
densed into short text snippets such as social
media posts, image captions, and scientific ab-
stracts. We predict the human ratings of sen-
tence similarity using a support vector regres-
sion model with multiple features measuring
word-overlap similarity and syntax similarity.
Out of 89 systems submitted, our two systems
ranked in the top 5, for the three overall eval-
uation metrics used (overall Pearson ? 2nd
and 3rd, normalized Pearson ? 1st and 3rd,
weighted mean ? 2nd and 5th).
1 Introduction
Natural language processing tasks such as text clas-
sification (Sebastiani, 2002), text summarization
(Lin and Hovy, 2003; Aliguliyev, 2009), informa-
tion retrieval (Park et al, 2005), and word sense dis-
ambiguation (Schu?tze, 1998) rely on a measure of
semantic similarity of textual documents. Research
predominantly focused either on the document sim-
ilarity (Salton et al, 1975; Maguitman et al, 2005)
or the word similarity (Budanitsky and Hirst, 2006;
Agirre et al, 2009). Evaluating the similarity of
short texts such as sentences or paragraphs (Islam
and Inkpen, 2008; Mihalcea et al, 2006; Oliva et
al., 2011) received less attention from the research
community. The task of recognizing paraphrases
(Michel et al, 2011; Socher et al, 2011; Wan et
al., 2006) is sufficiently similar to reuse some of the
techniques.
This paper presents the two systems for auto-
mated measuring of semantic similarity of short
texts which we submitted to the SemEval-2012 Se-
mantic Text Similarity Task (Agirre et al, 2012). We
propose several sentence similarity measures built
upon knowledge-based and corpus-based similarity
of individual words as well as similarity of depen-
dency parses. Our two systems, simple and syn-
tax, use supervised machine learning, more specif-
ically the support vector regression (SVR), to com-
bine a large amount of features computed from pairs
of sentences. The two systems differ in the set of
features they employ.
Our systems placed in the top 5 (out of 89 sub-
mitted systems) for all three aggregate correlation
measures: 2nd (syntax) and 3rd (simple) for overall
Pearson, 1st (simple) and 3rd (syntax) for normal-
ized Pearson, and 2nd (simple) and 5th (syntax) for
weighted mean.
The rest of the paper is structured as follows. In
Section 2 we describe both knowledge-based and
corpus-based word similarity measures. In Section
3 we describe in detail the features used by our sys-
tems. In Section 4 we report the experimental results
cross-validated on the development set as well as the
official results on all test sets. Conclusions and ideas
for future work are given in Section 5.
2 Word Similarity Measures
Approaches to determining semantic similarity of
sentences commonly use measures of semantic sim-
441
ilarity between individual words. Our systems use
the knowledge-based and the corpus-based (i.e., dis-
tributional lexical semantics) approaches, both of
which are commonly used to measure the semantic
similarity of words.
2.1 Knowledge-based Word Similarity
Knowledge-based word similarity approaches rely
on a semantic network of words, such as Word-
Net. Given two words, their similarity can be esti-
mated by considering their relative positions within
the knowledge base hierarchy.
All of our knowledge-based word similarity mea-
sures are based on WordNet. Some measures use
the concept of a lowest common subsumer (LCS)
of concepts c1 and c2, which represents the lowest
node in the WordNet hierarchy that is a hypernym
of both c1 and c2. We use the NLTK library (Bird,
2006) to compute the PathLen similarity (Leacock
and Chodorow, 1998) and Lin similarity (Lin, 1998)
measures. A single word often denotes several con-
cepts, depending on its context. In order to compute
the similarity score for a pair of words, we take the
maximum similarity score over all possible pairs of
concepts (i.e., WordNet synsets).
2.2 Corpus-based Word Similarity
Distributional lexical semantics models determine
the meaning of a word through the set of all con-
texts in which the word appears. Consequently, we
can model the meaning of a word using its distribu-
tion over all contexts. In the distributional model,
deriving the semantic similarity between two words
corresponds to comparing these distributions. While
many different models of distributional semantics
exist, we employ latent semantic analysis (LSA)
(Turney and Pantel, 2010) over a large corpus to es-
timate the distributions.
For each word wi, we compute a vector xi using
the truncated singular value decomposition (SVD)
of a tf-idf weighted term-document matrix. The co-
sine similarity of vectors xi and xj estimates the
similarity of the corresponding words wi and wj .
Two different word-vector mappings were com-
puted by processing the New York Times Annotated
Corpus (NYT) (Sandhaus, 2008) and Wikipedia.
Aside from lowercasing the documents and remov-
ing punctuation, we perform no further preprocess-
Table 1: Evaluation of word similarity measures
Measure ws353 ws353-sim ws353-rel
PathLen 0.29 0.61 -0.05
Lin 0.33 0.64 -0.01
Dist (NYT) 0.50 0.50 0.51
Dist (Wikipedia) 0.62 0.66 0.55
ing (e.g., no stopwords removal or stemming). Upon
removing the words not occurring in at least two
documents, we compute the tf-idf. The word vec-
tors extracted from NYT corpus and Wikipedia have
a dimension of 200 and 500, respectively.
We compared the measures by computing the
Spearman correlation coefficient on the Word-
Sim3531 data set, as well as its similarity and re-
latedness subsets described in (Agirre et al, 2009).
Table 1 provides the results of the comparison.
3 Semantic Similarity of Sentences
Our systems use supervised regression with SVR as
a learning model, where each system exploits differ-
ent feature sets and SVR hyperparameters.
3.1 Preprocessing
We list all of the preprocessing steps our systems
perform. If a preprocessing step is executed by only
one of our systems, the system?s name is indicated
in parentheses.
1. All hyphens and slashes are removed;
2. The angular brackets (< and >) that enclose the
tokens are stripped (simple);
3. The currency values are simplified, e.g.,
$US1234 to $1234 (simple);
4. Words are tokenized using the Penn Treebank
compatible tokenizer;
5. The tokens n?t and ?m are replaced with not and
am, respectively (simple);
6. The two consecutive words in one sentence that
appear as a compound in the other sentence are
replaced by the said compound. E.g., cater pil-
lar in one sentence is replaced with caterpil-
lar only if caterpillar appears in the other sen-
tence;
1http://www.cs.technion.ac.il/?gabr/
resources/data/wordsim353/wordsim353.html
442
7. Words are POS-tagged using Penn Treebank
compatible POS-taggers: NLTK (Bird, 2006)
for simple, and OpenNLP2 for syntax;
8. Stopwords are removed using a list of 36 stop-
words (simple).
While we acknowledge that some of the prepro-
cessing steps we take may not be common, we did
not have the time to determine the influence of each
individual preprocessing step on the results to either
warrant their removal or justify their presence.
Since, for example, sub-par, sub par and subpar
are treated as equal after preprocessing, we believe it
makes our systems more robust to inputs containing
small orthographic differences.
3.2 Ngram Overlap Features
We use many features previously seen in paraphrase
classification (Michel et al, 2011). Several features
are based on the unigram, bigram, and trigram over-
lap. Before computing the overlap scores, we re-
move punctuation and lowercase the words. We con-
tinue with a detailed description of each individual
feature.
Ngram Overlap
Let S1 and S2 be the sets of consecutive ngrams
(e.g., bigrams) in the first and the second sentence,
respectively. The ngram overlap is defined as fol-
lows:
ngo(S1, S2) = 2 ?
(
|S1|
|S1 ? S2|
+
|S2|
|S1 ? S2|
)?1
(1)
The ngram overlap is the harmonic mean of the de-
gree to which the second sentence covers the first
and the degree to which the first sentence covers the
second. The overlap, defined by (1), is computed for
unigrams, bigrams, and trigrams.
Additionally we observe the content ngram over-
lap ? the overlap of unigrams, bigrams, and tri-
grams exclusively on the content words. The con-
tent words are nouns, verbs, adjectives, and adverbs,
i.e., the lemmas having one of the following part-of-
speech tags: JJ, JJR, JJS, NN, NNP, NNS, NNPS,
RB, RBR, RBS, VB, VBD, VBG, VBN, VBP, and
VBZ. Intuitively, the function words (prepositions,
2http://opennlp.apache.org/
conjunctions, articles) carry less semantics than con-
tent words and thus removing them might eliminate
the noise and provide a more accurate estimate of
semantic similarity.
In addition to the overlap of consecutive ngrams,
we also compute the skip bigram and trigram over-
lap. Skip-ngrams are ngrams that allow arbitrary
gaps, i.e., ngram words need not be consecutive in
the original sentence. By redefining S1 and S2 to
represent the sets of skip ngrams, we employ eq. (1)
to compute the skip-n gram overlap.
3.3 WordNet-Augmented Word Overlap
One can expect a high unigram overlap between very
similar sentences only if exactly the same words (or
lemmas) appear in both sentences. To allow for
some lexical variation, we use WordNet to assign
partial scores to words that are not common to both
sentences. We define the WordNet augmented cov-
erage PWN (?, ?):
PWN (S1, S2) =
1
|S2|
?
w1?S1
score(w1, S2)
score(w, S) =
?
?
?
1 if w ? S
max
w??S
sim(w,w?) otherwise
where sim(?, ?) represents the WordNet path length
similarity. The WordNet-augmented word over-
lap feature is defined as a harmonic mean of
PWN (S1, S2) and PWN (S2, S1).
Weighted Word Overlap
When measuring sentence similarities we give
more importance to words bearing more content, by
using the information content
ic(w) = ln
?
w??C freq(w
?
)
freq(w)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the cor-
pus. We use the Google Books Ngrams (Michel et
al., 2011) to obtain word frequencies because of its
excellent word coverage for English. Let S1 and S2
be the sets of words occurring in the first and second
sentence, respectively. The weighted word cover-
age of the second sentence by the first sentence is
443
given by:
wwc(S1, S2) =
?
w?S1?S2 ic(w)?
w??S2
ic(w?)
The weighted word overlap between two sen-
tences is calculated as the harmonic mean of the
wwc(S1, S2) and wwc(S2, S1).
This measure proved to be very useful, but it
could be improved even further. Misspelled frequent
words are more frequent than some correctly spelled
but rarely used words. Hence dealing with mis-
spelled words would remove the inappropriate heavy
penalty for a mismatch between correctly and incor-
rectly spelled words.
Greedy Lemma Aligning Overlap
This measure computes the similarity between
sentences using the semantic alignment of lem-
mas. First we compute the word similarity be-
tween all pairs of lemmas from the first and the
second sentence, using either the knowledge-based
or the corpus-based semantic similarity. We then
greedily search for a pair of most similar lemmas;
once the lemmas are paired, they are not considered
for further matching. Previous research by Lavie
and Denkowski (2009) proposed a similar alignment
strategy for machine translation evaluation. After
aligning the sentences, the similarity of each lemma
pair is weighted by the larger information content of
the two lemmas:
sim(l1, l2) = max(ic(l1), ic(l2)) ? ssim(l1, l2) (2)
where ssim(l1, l2) is the semantic similarity be-
tween lemmas l1 and l2.
The overall similarity between two sentences is
defined as the sum of similarities of paired lemmas
normalized by the length of the longer sentence:
glao(S1, S2) =
?
(l1,l2)?P sim(l1, l2)
max(length(S1), length(S2))
where P is the set of lemma pairs obtained by greedy
alignment. We take advantage of greedy align over-
lap in two features: one computes glao(?, ?) by us-
ing the Lin similarity for ssim(?, ?) in (2), while the
other feature uses the distributional (LSA) similarity
to calculate ssim(?, ?).
Vector Space Sentence Similarity
This measure is motivated by the idea of composi-
tionality of distributional vectors (Mitchell and La-
pata, 2008). We represent each sentence as a sin-
gle distributional vector u(?) by summing the dis-
tributional (i.e., LSA) vector of each word w in the
sentence S: u(S) =
?
w?S xw, where xw is the
vector representation of the word w. Another sim-
ilar representation uW (?) uses the information con-
tent ic(w) to weigh the LSA vector of each word
before summation: uW (S) =
?
w?S ic(w)xw.
The simple system uses |cos(u(S1), u(S2))| and
|cos(uW (S1), uW (S2))| for the vector space sen-
tence similarity features.
3.4 Syntactic Features
We use dependency parsing to identify the lemmas
with the corresponding syntactic roles in the two
sentences. We also compute the overlap of the de-
pendency relations of the two sentences.
Syntactic Roles Similarity
The similarity of the words or phrases having the
same syntactic roles in the two sentences may be in-
dicative of their overall semantic similarity (Oliva et
al., 2011). For example, two sentences with very dif-
ferent main predicates (e.g., play and eat) probably
have a significant semantic difference.
Using Lin similarity ssim(?, ?), we obtain the sim-
ilarity between the matching lemmas in a sentence
pair for each syntactic role. Additionally, for each
role we compute the similarity of the chunks that
lemmas belong to:
chunksim(C1, C2) =
?
l1?C1
?
l2?C2
ssim(l1, l2)
where C1 and C2 are the sets of chunks of
the first and second sentence, respectively. The
final similarity score of two chunks is the
harmonic mean of chunksim(C1, C2)/|C1| and
chunksim(C1, C2)/|C2| .
Syntactic roles that we consider are predicates (p),
subjects (s), direct (d), and indirect (i) (i.e., preposi-
tional) objects, where we use (o) to mean either (d)
or (i). The Stanford dependency parser (De Marn-
effe et al, 2006) produces the dependency parse of
the sentence. We infer (p), (s), and (d) from the syn-
tactic dependencies of type nsubj (nominal subject),
444
nsubjpass (nominal subject passive), and dobj (di-
rect object). By combining the prep and pobj de-
pendencies (De Marneffe and Manning, 2008), we
identify (i). Since the (d) in one sentence often se-
mantically corresponds to (i) in the other sentence,
we pair all (o) of one sentence with all (o) of the
other sentence and define object similarity between
the two sentences as the maximum similarity among
all (o) pairs. Because the syntactic role might be
absent from both sentences (e.g., the object in sen-
tences ?John sings? and ?John is thinking?), we in-
troduce additional binary features indicating if the
comparison for the syntactic role in question exists.
Many sentences (especially longer ones) have two
or more (p). In such cases it is necessary to align
the corresponding predicate groups (i.e., the (p) with
its corresponding arguments) between the two sen-
tences, while also aggregating the (p), (s), and (o)
similarities of all aligned (p) pairs. The similarity
of two predicate groups is defined as the sum of (p),
(s), and (o) similarities. In each iteration, the greedy
algorithm pairs all predicate groups of the first sen-
tence with all predicate groups of the second sen-
tence and searches for a pair with the maximum sim-
ilarity. Once the predicate groups of two sentences
have been aligned, we compute the (p) similarity as
a weighted sum of (p) similarities for each predicate
pair group. The weight of each predicate group pair
equals the larger information content of two predi-
cates. The (s) and (o) similarities are computed in
the same manner.
Syntactic Dependencies Overlap
Similar to the ngram overlap features, we measure
the overlap between sentences based on matching
dependency relations. A similar measure has been
proposed in (Wan et al, 2006). Two syntactic depen-
dencies are considered equal if they have the same
dependency type, governing lemma, and dependent
lemma. Let S1 and S2 be the set of all dependency
relations in the first and the second sentence, respec-
tively. Dependency overlap is the harmonic mean
between |S1 ? S2|/|S1| and |S1 ? S2|/|S2| . Con-
tent dependency overlap computes the overlap in the
same way, but considers only dependency relations
between content lemmas.
Similarly to weighted word overlap, we com-
pute the weighted dependency relations overlap.
The weighted coverage of the second sentence de-
pendencies with the first sentence dependencies is
given by:
wdrc(S1, S2) =
?
r?S1?S2 max(ic(g(r)), ic(d(r)))?
r?S2 max(ic(g(r)), ic(d(r)))
where g(r) is the governing word of the dependency
relation r, d(r) is the dependent word of the depen-
dency relation r, and ic(l) is the information con-
tent of the lemma l. Finally, the weighted depen-
dency relations overlap is the harmonic mean be-
tween wdrc(S1, S2) and wdrc(S2, S1).
3.5 Other Features
Although we primarily focused on developing the
ngram overlap and syntax-based features, some
other features significantly improve the performance
of our systems.
Normalized Differences
Our systems take advantage of the following fea-
tures that measure normalized differences in a pair
of sentences: (A) sentence length, (B) the noun
chunk, verb chunk, and predicate counts, and (C)
the aggregate word information content (see Nor-
malized differences in Table 2).
Numbers Overlap
The annotators gave low similarity scores to many
sentence pairs that contained different sets of num-
bers, even though their sentence structure was very
similar. Socher et al (2011) improved the perfor-
mance of their paraphrase classifier by adding the
following features that compare the sets of num-
bers N1 and N2 in two sentences: N1 = N2,
N1?N2 6= ?, and N1 ? N2?N2 ? N1. We replace
the first two features with log (1 + |N1|+ |N2|) and
2? |N1 ?N2|/(|N1|+ |N2|) . Additionally, the num-
bers that differ only in the number of decimal places
are treated as equal (e.g., 65, 65.2, and 65.234 are
treated as equal, whereas 65.24 and 65.25 are not).
Named Entity Features
Shallow NE similarity treats capitalized words as
named entities if they are longer than one character.
If a token in all caps begins with a period, it is clas-
sified as a stock index symbol. The simple system
445
Table 2: The usage of feature sets
Feature set simple syntax
Ngram overlap + +
Content-ngram overlap - +
Skip-ngram overlap - +
WordNet-aug. overlap + -
Weighted word overlap + +
Greedy align. overlap - +
Vector space similarity + -
Syntactic roles similarity - +
Syntactic dep. overlap - +
Normalized differences* A,C A,B
Shallow NERC + -
Full NERC - +
Numbers overlap + +
* See Section 3.5
uses the following four features: the overlap of cap-
italized words, the overlap of stock index symbols,
and the two features indicating whether these named
entities were found in either of the two sentences.
In addition to the overlap of capitalized words, the
syntax system uses the OpenNLP named entity rec-
ognizer and classifier to compute the overlap of en-
tities for each entity class separately. We recognize
the following entity classes: persons, organizations,
locations, dates, and rudimentary temporal expres-
sions. The absence of an entity class from both sen-
tences is indicated by a separate binary feature (one
feature for each class).
Feature Usage in TakeLab Systems
Some of the features presented in the previous sec-
tions were used by both of our systems (simple and
syntax), while others were used by only one of the
systems. Table 2 indicates the feature sets used for
the two submitted systems.
4 Results
4.1 Model Training
For each of the provided training sets we trained a
separate Support Vector Regression (SVR) model
using LIBSVM (Chang and Lin, 2011). To ob-
tain the optimal SVR parameters C, g, and p, our
systems employ a grid search with nested cross-
Table 3: Cross-validated results on train sets
MSRvid MSRpar SMTeuroparl
simple 0.8794 0.7566 0.7802
syntax 0.8698 0.7144 0.7308
validation. Table 3 presents the cross-validated per-
formance (in terms of Pearson correlation) on the
training sets. The models tested on the SMTnews
test set were trained on the SMTeuroparl train set.
For the OnWn test set, the syntax model was trained
on the MSRpar set, while the simple system?s model
was trained on the union of all train sets. The final
predictions were trimmed to a 0?5 range.
Our development results indicate that the
weighted word overlap, WordNet-augmented word
overlap, the greedy lemma alignment overlap, and
the vector space sentence similarity individually
obtain high correlations regardless of the devel-
opment set in use. Other features proved to be
useful on individual development sets (e.g., syntax
roles similarity on MSRvid and numbers overlap
on MSRpar). More research remains to be done in
thorough feature analysis and systematic feature
selection.
4.2 Test Set Results
The organizers provided five different test sets to
evaluate the performance of the submitted systems.
Table 4 illustrates the performance of our systems
on individual test sets, accompanied by their rank.
Our systems outperformed most other systems on
MSRvid, MSRpar, and OnWN sets (Agirre et al,
2012). However, they performed poorly on the
SMTeuroparl and SMTnews sets. While the corre-
lation scores on the MSRvid and MSRpar test sets
correspond to those obtained using cross-validation
on the corresponding train sets, the performance on
the SMT test sets is drastically lower than the cross-
validated performance on the corresponding train
set. The sentences in the SMT training set are signif-
icantly longer (30.4 tokens on average) than the sen-
tences in both SMT test sets (12.3 for SMTeuroparl
and 13.5 for SMTnews). Also there are several re-
peated pairs of extremely short and identical sen-
tences (e.g., ?Tunisia? ? ?Tunisia? appears 17 times
446
Table 4: Results on individual test sets
simple syntax
MSRvid 0.8803 (1) 0.8620 (8)
MSRpar 0.7343 (1) 0.6985 (2)
SMTeuroparl 0.4771 (26) 0.3612 (63)
SMTnews 0.3989 (46) 0.4683 (18)
OnWN 0.6797 (9) 0.7049 (6)
Table 5: Aggregate performance on the test sets
All ALLnrm Mean
simple 0.8133 (3) 0.8635 (1) 0.6753 (2)
syntax 0.8138 (2) 0.8569 (3) 0.6601 (5)
in the SMTeuroparl test set). The above measure-
ments indicate that the SMTeuroparl training set was
not representative of the SMTeuroparl test set for our
choice of features.
Table 5 outlines the aggregate performance of our
systems according to the three aggregate evaluation
measures proposed for the task (Agirre et al, 2012).
Both systems performed very favourably compared
to the other systems, achieving very high rankings
regardless of the aggregate evaluation measure.
The implementation of simple system is available
at http://takelab.fer.hr/sts.
5 Conclusion and Future Work
In this paper we described our submission to the
SemEval-2012 Semantic Textual Similarity Task.
We have identified some high performing features
for measuring semantic text similarity. Although
both of the submitted systems performed very well
on all but the two SMT test sets, there is still room
for improvement. The feature selection was ad-hoc
and more systematic feature selection is required
(e.g., wrapper feature selection). Introducing ad-
ditional features for deeper understanding (e.g., se-
mantic role labelling) might also improve perfor-
mance on this task.
Acknowledgments
This work was supported by the Ministry of Science,
Education and Sports, Republic of Croatia under the
Grant 036-1300646-1986. We would like to thank
the organizers for the tremendous effort they put into
formulating this challenging task.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 19?27. As-
sociation for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012). ACL.
Ramiz M. Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, COLING-ACL ?06, pages 69?
72. Association for Computational Linguistics.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine De Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8. Association for Computational
Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449?454.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
447
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine translation, 23(2):105?115.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 71?78. Association for
Computational Linguistics.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th inter-
national conference on Machine Learning, volume 1,
pages 296?304. San Francisco.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad,
and Alessandro Vespignani. 2005. Algorithmic detec-
tion of semantic similarity. In Proceedings of the 14th
international conference on World Wide Web, pages
107?116. ACM.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, et al 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. Proceedings of ACL-
08: HLT, pages 236?244.
Jesu?s Oliva, Jo?se Ignacio Serrano, Mar??a Dolores
Del Castillo, and A?ngel Iglesias. 2011. SyMSS: A
syntax-based measure for short-text semantic similar-
ity. Data & Knowledge Engineering.
Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang. 2005.
Techniques for improving web retrieval effectiveness.
Information processing & management, 41(5):1207?
1223.
Gerard Salton, Andrew Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational linguistics, 24(1):97?123.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1?47.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. Advances in Neural Infor-
mation Processing Systems, 24.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37(1):141?
188.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using dependency-based features to take the
?para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, volume
2006.
448
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 1?9,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Experiments on Hybrid Corpus-Based Sentiment Lexicon Acquisition
Goran Glavas?, Jan S?najder and Bojana Dalbelo Bas?ic?
Faculty of Electrical Engineering and Computing
University of Zagreb
Zagreb, Croatia
{goran.glavas, jan.snajder, bojana.dalbelo}@fer.hr
Abstract
Numerous sentiment analysis applications
make usage of a sentiment lexicon. In
this paper we present experiments on hy-
brid sentiment lexicon acquisition. The ap-
proach is corpus-based and thus suitable
for languages lacking general dictionary-
based resources. The approach is a hy-
brid two-step process that combines semi-
supervised graph-based algorithms and su-
pervised models. We evaluate the perfor-
mance on three tasks that capture differ-
ent aspects of a sentiment lexicon: polar-
ity ranking task, polarity regression task,
and sentiment classification task. Exten-
sive evaluation shows that the results are
comparable to those of a well-known senti-
ment lexicon SentiWordNet on the polarity
ranking task. On the sentiment classifica-
tion task, the results are also comparable to
SentiWordNet when restricted to monosen-
timous (all senses carry the same senti-
ment) words. This is satisfactory, given the
absence of explicit semantic relations be-
tween words in the corpus.
1 Introduction
Knowing someone?s attitude towards events, en-
tities, and phenomena can be very important in
various areas of human activity. Sentiment anal-
ysis is an area of computational linguistics that
aims to recognize the subjectivity and attitude ex-
pressed in natural language texts. Applications
of sentiment analysis are numerous, including
sentiment-based document classification (Riloff
et al, 2006), opinion-oriented information extrac-
tion (Hu and Liu, 2004), and question answering
(Somasundaran et al, 2007).
Sentiment analysis combines subjectivity anal-
ysis and polarity analysis. Subjectivity analy-
sis answers whether the text unit is subjective
or neutral, while polarity analysis determines
whether a subjective text unit is positive or nega-
tive. The majority of research approaches (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Wilson et al, 2009) see subjec-
tivity and polarity as categorical terms (i.e., clas-
sification problems). Intuitively, not all words ex-
press the sentiment with the same intensity. Ac-
cordingly, there has been some research effort in
assessing subjectivity and polarity as graded val-
ues (Baccianella et al, 2010; Andreevskaia and
Bergler, 2006). Most of the work on sentence or
document level sentiment makes usage of senti-
ment annotated lexicon providing subjectivity and
polarity information for individual words (Wilson
et al, 2009; Taboada et al, 2011).
In this paper we present a hybrid approach
for automated acquisition of sentiment lexicon.
The method is language independent and corpus-
based and therefore suitable for languages lack-
ing general lexical resources such as WordNet
(Fellbaum, 2010). The two-step hybrid pro-
cess combines semi-supervised graph-based algo-
rithms and supervised learning models.
We consider three different tasks, each captur-
ing different aspect of a sentiment lexicon:
1. Polarity ranking task ? determine the relative
rankings of words, i.e., order lexicon items
descendingly by positivity and negativity;
2. Polarity regression task ? assign each word
absolute scores (between 0 and 1) for posi-
tivity and negativity;
3. Sentiment classification task ? classify each
1
word into one of the three sentiment classes
(positive, negative, or neutral).
Accordingly, we evaluate our method using three
different measures ? one to evaluate the quality
of the ordering by positivity and negativity, other
to evaluate the absolute sentiment scores assigned
to each corpus word, and another to evaluate the
classification performance.
The rest of the paper is structured as follows.
In Section 2 we present the related work on senti-
ment lexicon acquisition. Section 3 discusses the
semi-supervised step of the hybrid approach. In
Section 4 we explain the supervised step in more
detail. In Section 5 the experimental setup, the
evaluation procedure, and the results of the ap-
proach are discussed. Section 6 concludes the pa-
per and outlines future work.
2 Related Work
Several approaches have been proposed for deter-
mining the prior polarity of words. Most of the
approaches can be classified as either dictionary-
based (Kamps et al, 2004; Esuli and Sebastiani,
2007; Baccianella et al, 2010) or corpus-based
(Hatzivassiloglou and McKeown, 1997; Turney
and Littman, 2003). Regardless of the resource
used, most of the approaches focus on bootstrap-
ping, starting from a small seed set of manually
labeled words (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003; Esuli and Se-
bastiani, 2007). In this paper we also follow this
idea of the semi-supervised bootstrapping as the
first step of the sentiment lexicon acquisition.
Dictionary-based approaches grow the seed
sets according to the explicit paradigmatic seman-
tic relations (synonymy, antonymy, hyponymy,
etc.) between words in the dictionary. Kamps
et al (2004) build a graph of adjectives based
on synonymy relations gathered from WordNet.
They determine the polarity of the adjective based
on its shortest path distances from positive and
negative seed adjectives good and bad. Esuli and
Sebastiani (2007) first build a graph based on a
gloss relation (i.e., definiens ? definiendum rela-
tion) from WordNet. Afterwards they perform a
variation of the PageRank algorithm (Page et al,
1999) in two runs. In the first run positive PageR-
ank value is assigned to the vertices of the synsets
from the positive seed set and zero value to all
other vertices. In the second run the same is done
for the synsets from the negative seed set. Word?s
polarity is then decided based on the difference
between its PageRank values of the two runs. We
also believe that graph is the appropriate struc-
ture for the propagation of sentiment properties of
words. Unfortunately, for many languages a pre-
compiled lexical resource like WordNet does not
exist. In such a case, semantic relations between
words may be extracted from corpus.
In their pioneering work, Hatzivassiloglou and
McKeown (1997) attempt to determine the po-
larity of adjectives based on their co-occurrences
in conjunctions. They start with a small manu-
ally labeled seed set and build on the observa-
tion that adjectives of the same polarity are often
conjoined with the conjunction and, while adjec-
tives of the opposite polarity are conjoined with
the conjunction but. Turney and Littman (2003)
use pointwise mutual information (PMI) (Church
and Hanks, 1990) and latent semantic analysis
(LSA) (Dumais, 2004) to determine the similarity
of the word of unknown polarity with the words
in both positive and negative seed sets. The afore-
mentioned work presumes that there is a corre-
lation between lexical semantics and sentiment.
We base our work on the same assumption, but
instead of directly comparing the words with the
seed sets, we use distributional semantics to build
a word similarity graph. In contrast to the ap-
proaches above, this allows us to potentially ac-
count for similarities between all pairs of words
from corpus. To the best of our knowledge, such
an approach that combines corpus-based lexical
semantics with graph-based propagation has not
yet been applied to the task of building senti-
ment lexicon. However, similar approaches have
been proven rather efficient on other tasks such
as document level sentiment classification (Gold-
berg and Zhu, 2006) and word sense disambigua-
tion (Agirre et al, 2006).
3 Semi-supervised Graph-based
Methods
The structure of a graph in general provides a
good framework for propagation of object proper-
ties, which, in our case, are the sentiment values
of the words. In a word similarity graph, weights
of edges represent the degree of semantic similar-
ity between words.
In the work presented in this paper we build
graphs from corpus, using different notions of
2
word similarity. Each vertex in the graph repre-
sents a word from corpus. Weights of the edges
are calculated in several different ways, using
measures of word co-occurrence (co-occurrence
frequency and pointwise mutual information) and
distributional semantic models (latent semantic
analysis and random indexing). We manually
compiled positive and negative seed sets, each
consisting of 15 words:
positiveSeeds = {good, best, excel-
lent, happy, well, new, great, nice,
smart, beautiful, smile, win, hope, love,
friend}
negativeSeeds = {bad, worst, violence,
die, poor, terrible, death, war, enemy,
accident, murder, lose, wrong, attack,
loss}
In addition to these, we compiled the third seed
set consisting of neutral words to serve as sen-
timent sinks for the employed label propagation
algorithm:
neutralSeeds = {time, place, company,
work, city, house, man, world, woman,
country, building, number, system, ob-
ject, room}
Once we have built the graph, we label the ver-
tices belonging to the words from the polar seed
set with the sentiment score of 1. All other ver-
tices are initially unlabeled (i.e., assigned a sen-
timent score of 0). We then use the structure of
the graph and one of the two random-walk algo-
rithms to propagate the labels from the labeled
seed set vertices to the unlabeled ones. The ran-
dom walk algorithm is executed twice: once with
the words from the positive seed set being ini-
tially labeled and once with the words from the
negative seed set being initially labeled. Once the
random walk algorithm converges, all unlabeled
vertices will be assigned a sentiment label. How-
ever, the final sentiment values obtained after the
convergence of the random-walk algorithm are di-
rectly dependent on the size of the graph (which,
in turn, depends on the size of the corpus), the
size of the seed set, and the choice of the seed set
words. Thus, they should be interpreted as rela-
tive rather than absolute sentiment scores. Nev-
ertheless, the scores obtained from the graph can
be used to rank the words by their positivity and
negativity.
3.1 Similarity Based on Corpus
Co-occurrence
If the two words co-occur in the corpus within a
window of a given size, an edge in the graph be-
tween their corresponding vertices is added. The
weight of the edge should represent the measure
of the degree to which the two words co-occur.
There are many word collocation measures that
may be used to calculate the weights of edges
(Evert, 2008). In this work, we use raw co-
occurrence frequency and pointwise mutual in-
formation (PMI) (Church and Hanks, 1990). In
the former case the edge between two words is
assigned a weight indicating a total number of
co-occurrences of the corresponding words in the
corpus within the window of a given size. In the
latter case, we use PMI to account for the indi-
vidual frequencies of each of the two words along
with their co-occurrence frequency. The most fre-
quent corpus words tend to frequently co-occur
with most other words in the corpus, including
words from both positive and negative seed sets.
PMI compensates for this shortcoming of the raw
co-occurrence frequency measure.
3.2 Similarity Based on Latent Semantic
Analysis
Latent semantic analysis is a well-known tech-
nique for identifying semantically related con-
cepts and dimensionality reduction in large vector
spaces (Dumais, 2004). The first step is to cre-
ate a sparse word-document matrix. Matrix ele-
ments are frequencies of words occurring in docu-
ments, usually transformed using some weighting
scheme (e.g., tf-idf ). The word-document matrix
is then decomposed using singular value decom-
position (SVD), a well-known linear algebra pro-
cedure. Finally, the dimensionality reduction is
performed by approximating the original matrix
using only the top k largest singular values.
We build two different word-document matri-
ces using different weighting schemes. The el-
ements of the first matrix were calculated using
the tf-idf weighting scheme, while for the sec-
ond matrix the log-entropy weighting scheme was
used. In the log-entropy scheme, each matrix ele-
ment, mw,d, is calculated using logarithmic value
of word-document frequency and the global word
entropy (entropy of word frequency across the
documents), as follows:
3
mw,d = log (tfw ,d + 1 ) ? ge(w)
with
ge(w) = 1 +
1
log n
?
d??D
tfw ,d ?
gf w
log
tfw ,d ?
gf w
where tfw ,d represents occurrence frequency of
word w in document d, parameter gf w represents
global frequency of word w in corpus D, and n
is the number of documents in corpus D. Next,
we decompose each of the two matrices using
SVD in order to obtain a vector for each word
in the vector space of reduced dimensionality k
(k  n). LSA vectors tend to express semantic
properties of words. Moreover, the similarity be-
tween the LSA vectors may be used as a measure
of semantic similarity between the corresponding
words. We compute this similarity using the co-
sine between the LSA vectors and use the ob-
tained values as weights of graph edges. Because
running random-walk algorithms on a complete
graph would be computationally intractable, we
decided to reduce the number of edges by thresh-
olding the similarity values.
3.3 Similarity Based on Random Indexing
Random Indexing (RI) is another word space ap-
proach, which presents an efficient and scalable
alternative to more commonly used word space
methods such as LSA. Random indexing is a di-
mensionality reduction technique in which a ran-
dom matrix is used to project the original word-
context matrix into the vector space of lower di-
mensionality. Each context is represented by its
index vector, a sparse vector with a small number
of randomly distributed +1 and ?1 values, the
remaining values being 0 (Sahlgren, 2006). For
each corpus word its context vector is constructed
by summing index vectors of all context elements
occurring within contexts of all of its occurrences
in the corpus. The semantic similarity of the two
words is then expressed as the similarity between
its context vectors.
We use two different definitions for the context
and context relation. In the first case (referred to
as RI with document context), each corpus docu-
ment is considered as a separate context and the
word is considered to be in a context relation if
it occurs in the document. The context vector of
each word is then simply the sum of random in-
dex vectors of the documents in which the word
occurs. In the second case (referred to as RI with
window context), each corpus word is considered
as a context itself, and the two words are consid-
ered to be in a context relation if they co-occur in
the corpus within the window of a given size. The
context vector of each corpus word is then com-
puted as the sum of random index vectors of all
words with which it co-occurs in the corpus in-
side the window of a given size. Like in the LSA
approach, we use the cosine of the angle between
the context vectors as a measure of semantic simi-
larity between the word pairs. To reduce the num-
ber of edges, we again perform the thresholding
of the similarity values.
3.4 Random-Walk Algorithms
Once the graph building phase is done, we start
propagating the sentiment scores from the vertices
of the seed set words to the unlabeled vertices.
To this end, one can use several semi-supervised
learning algorithms. The most commonly used
algorithm for dictionary-based sentiment lexicon
acquisition is PageRank. Along with the PageR-
ank we employ another random-walk algorithm
called harmonic function learning.
PageRank
PageRank (Page et al, 1999) was initially de-
signed for ranking web pages by their relevance.
The intuition behind PageRank is that a vertex
v should have a high score if it has many high-
scoring neighbours and these neighbours do not
have many other neighbours except the vertex
v. Let W be the weighted row-normalized ad-
jacency matrix of graph G. The algorithm itera-
tively computes the vector of vertex scores a in
the following way:
a(k) = ?a(k?1)W + (1? ?)e
where? is the PageRank damping factor. Vector e
models the normalized internal source of score for
all vertices and its elements sum up to 1. We as-
sign the value of ei to be 1|SeedSet | for the vertices
whose corresponding words belong to the seed set
and ei = 0 for all other vertices.
Harmonic Function
The second graph-based semi-supervised
learning algorithm we use is the harmonic func-
4
tion label propagation (also known as absorbing
random walk) (Zhu and Goldberg, 2009). Har-
monic function tries to propagate labels between
sources and sinks of sentiment. We perform two
runs of the algorithm: one for positive sentiment,
in which we use the words from the positive seed
set as sentiment sources, and one for the negative
sentiment, in which we use the words from the
negative seed set as sentiment sources. In both
cases, we use the precompiled seed set of neutral
words as sentiment sinks. Note that we could
not have used positive seed set words as sources
and negative seed set words as sinks (or vice
versa) because we aim to predict the positive and
negative sentiment scores separately.
The value of the harmonic function for a la-
beled vertex remains the same as initially labeled,
whereas for an unlabeled vertex the value is com-
puted as the weighted average of its neighbours?
values (Zhu and Goldberg, 2009):
f(vk) =
?
j?|V |wkj ? f(vj)
?
j?|V |wkj
where V is the set of vertices of graph G and
wkj is the weight of the edge between the ver-
tices vk and vj . If there is no graph edge be-
tween vertices vk and vj , the value of the weight
wkj is 0. This equation also represents the update
rule for the iterative computation of the harmonic
function. However, it can be shown that there is
a closed-form solution of the harmonic function.
Let W be the unnormalized weighted adjacency
matrix of the graph G, and let D be the diagonal
matrix with the element Dii =
?
j?|V |wij be-
ing the weighted degree of the vertex vi. Then
the unnormalized graph Laplacian is defined with
L = D ?W . Assuming that the labeled seed set
vertices are ordered before the unlabeled ones, the
graph Laplacian can be partitioned in the follow-
ing way:
L =
(
Lll Llu
Lul Luu
)
The closed form solution for the harmonic
function of the unlabeled vertices is then given by:
fu = ?L?1uuLulyl
where yl if the vector of labels of the seed set ver-
tices (Zhu and Goldberg, 2009).
4 Supervised Step Hybridization
The sentiment scores obtained by the semi-
supervised graph-based approaches described
above are relative because they depend on the
graph size as well as on the size and content of
the seed sets. As such, these values can be used to
rank the words by positivity or negativity, but not
as absolute positivity and negativity scores. Thus,
in the second step of our hybrid approach, we use
supervised learning to obtain the absolute senti-
ment scores (polarity regression task) and the sen-
timent labels (sentiment classification task).
Each score obtained on each graph represents
a single feature for supervised learning. There
are altogether 24 different semi-supervised fea-
tures used as input for the supervised learners.
These features are both positive and negative la-
bels generated from six different semi-supervised
graphs (co-occurence frequency, co-occurrence
PMI, LSA log-entropy, LSA tf-idf, random in-
dexing with document context, and random in-
dexing with window context) using two different
random-walk algorithms (harmonic function and
PageRank). We used the occurrence frequency of
words in corpus as an additional feature.
For polarity regression, learning must be per-
formed twice: once for the negative and once for
the positive sentiment score. We performed the
regression using SVM with radial-basis kernel.
The same set of features used for regression was
used for sentiment classification, but the goal was
to predict the class of the word (positive, negative,
or neutral) instead of separate positivity or nega-
tivity scores. SVM with radial-basis kernel was
used to perform classification learning as well.
5 Evaluation and Results
All the experiments were performed on the ex-
cerpt of the New York Times corpus (years 2002?
2007), containing 434,494 articles. The corpus
was preprocessed (tokenized, lemmatized, and
POS tagged) and only the content lemmas (nouns,
verbs, adjectives, and adverbs) occurring at least
80 times in the corpus were considered. Lemmas
occurring less than 80 were mainly named entities
or their derivatives. The final sentiment lexicon
consists of 41,359 lemmas annotated with posi-
tivity and negativity scores and sentiment class.1
1Sentiment lexicon is freely available at
http://takelab.fer.hr/sentilex
5
5.1 Sentiment Annotations
To evaluate our methods on the three tasks, we
compare the results against the Micro-WN(Op)
dataset (Cerini et al, 2007). Micro-WN(Op) con-
tains sentiment annotations for 1105 WordNet 2.0
synsets. Each synset s is manually annotated with
the degree of positivity Pos(s) and negativity
Neg(s), where 0 ? Pos(s) ? 1, 0 ? Neg(s) ?
1, and Pos(s) +Neg(s) ? 1. Objectivity score is
defined as Obj (s) = 1? (Pos(s) + Neg(s)).
This gives us a list of 2800 word-sense pairs
with their sentiment annotations. For reasons that
we explain below, we retain from this list only
those words for which all senses from WordNet
have been sentiment-annotated, which leaves us
with a list of 1645 word-sense pairs. From this
list we then filter out all words that occur less
than 80 times in our corpus, leaving us with a list
of 1125 word-sense pairs (365 distinct words, of
which 152 are monosemous). We refer to this set
of 1125 sentiment-annotated word-sense pairs as
Micro-WN(Op)-0.
Because our corpus-based methods are unable
to discriminate among various senses of a pol-
ysemous word, we wish to be able to eliminate
the negative effect of polysemy in our evalua-
tion. The motivation for this is twofold: first, it
gives us a way of measuring how much polysemy
influences our results. Secondly, it provides us
with the answer how well our method could per-
form in an ideal case where all the words from
corpus have been pre-disambiguated. Because
each of the words in Micro-WN(Op)-0 has all its
senses sentiment-annotated, we can determine for
each of these words how sentiment depends on its
sense. Expectedly, there are words whose senti-
ment differs radically across its senses or parts-
of-speech (e.g., catch, nest, shark, or hot), but
also words whose sentiment is constant or simi-
lar across all its senses. To eliminate the effect
of polysemy on sentiment prediction, we further
filter the Micro-WN(Op)-0 list by retaining only
the words whose sentiment is constant or nearly
constant across all their senses. We refer to such
words as monosentimous. We consider a word
to be monosentimous iff (1) pairwise differences
between all sentiment scores across senses are
less than 0.25 (separately for both positive and
negative sentiment) and (2) the sign of the dif-
ference between positive and negative sentiment
score is constant across all senses. Note that ev-
ery monosemous word is by definition monosen-
timous. Out of 365 words in Micro-WN(Op)-
0, 225 of them are monosentimous. To obtain
the sentiment scores of monosentimous words,
we simply average the scores across their senses.
We refer to the so-obtained set of 225 sentiment-
annotated words as Micro-WN(Op)-1.
5.2 Semi-supervised Step Evaluation
The semi-supervised step was designed to prop-
agate sentiment properties of the labeled words,
ordering the words according to their positivity
or negativity. Therefore, we decided to use the
evaluation metric that measures the quality of
the ranking in ordered lists, Kendall ? distance.
The performance of the semi-supervised graph-
based methods was evaluated both on the Micro-
WN(Op)-1 and Micro-WN(Op)-0 sets.
In order to be able to compare our results to
SentiWordNet (Baccianella et al, 2010), the de
facto standard sentiment lexicon for English, we
use the p-normalized Kendall ? distance between
the rankings generated by our semi-supervised
graph-based methods and the gold standard rank-
ings. The p-normalized Kendall ? distance (Fagin
et al, 2004) is a version of the standard Kendall ?
distance that accounts for ties in the ordering:
? =
nd + p ? nt
Z
where nd is the number of pairs in disagreement
(i.e., pairs of words ordered one way in the gold
standard and the opposite way in the ranking un-
der evaluation), nt is the number of pairs which
are ordered in the gold standard and tied in the
ranking under evaluation, p is the penalization
factor to be assigned to each of the nt pairs (usu-
ally set to p = 12 ), and Z is the number of pairs of
words that are ordered in the gold standard. Table
1 presents the results for each of the methods used
to build the sentiment graph and for both random-
walk algorithms. The results were obtained by
evaluating the relative rankings of words against
the Micro-WN(Op)-1 as gold standard. For com-
parison, the p-normalized Kendall ? scores for
SentiWordNet 1.0 and SentiWordNet 3.0 are ex-
tracted from (Baccianella et al, 2010).
Rankings for the negative scores are consis-
tently better across all methods and algorithms.
We believe that the negative rankings are better
6
Table 1: The results on the polarity ranking task
Harmonic function PageRank
Positive Negative Positive Negative
Co-occurrence freq. 0.395 0.298 0.540 0.544
LSA log-entropy 0.425 0.308 0.434 0.370
LSA tf-idf 0.396 0.320 0.417 0.424
Co-occurrence PMI 0.321 0.256 0.550 0.576
Random indexing document context 0.402 0.433 0.534 0.557
Random indexing window context 0.455 0.398 0.491 0.436
Positive Negative
SentiWordNet 1.0 0.349 0.296
SentiWordNet 3.0 0.281 0.231
for two reasons. Firstly, the corpus contains many
more articles describing negative events such as
wars and accidents than the articles describing
positive events such as celebrations and victo-
ries. In short, the distribution of articles is signif-
icantly skewed towards ?negative? events. Sec-
ondly, the lemma new, which was included in
the positive seed set, occurs in the corpus very
frequently as a part of named entity collocations
such as ?New York? and ?New Jersey? in which
it does not reflect its dominant sense. The har-
monic function label propagation generally out-
performs the PageRank algorithm. The best per-
formance on the Micro-WN(Op)-0 set was 0.380
for the positive ranking and 0.270 for the nega-
tive ranking, showing that the performance de-
teriorates when polysemy is present. However,
the drop in performance, especially for the neg-
ative ranking, is not substantial. Our best method
(graph built based on PMI of corpus words used in
combination with harmonic function label prop-
agation) outperforms SentiWordNet 1.0 and per-
forms slightly worse than SentiWordNet 3.0 for
both positive and negative rankings.
5.3 Evaluation of the Supervised Step
Supervised step deals with the polarity regression
task and the sentiment classification task. Polarity
regression maps the ?virtual? sentiment scores ob-
tained on graphs to the absolute sentiment scores
(on a scale from 0 to 1). The regression was per-
formed twice: once for the positive scores and
once for the negative scores. We evaluate the
performance of the polarity regression against the
Micro-WN(Op)-0 gold standard in terms of root
mean square error (RMSE). We used the aver-
age of the labeled polarity scores (positive and
negative) of all monosentimous words in Micro-
WN(Op)-1 as a baseline for this task.
Sentiment classification uses the scores ob-
tained on graphs as features in order to assign
each word with one of the three sentiment la-
bels (positive, negative, and neutral). The clas-
sification performance is evaluated in terms of
micro-F1 measure. The labels for the classifica-
tion are assigned according to the positivity and
negativity scores (the label neutral is assigned if
Obj (s) = 1?Pos(s)?Neg(s) is larger than both
Pos(s) and Neg(s)). The majority class predictor
was used as a baseline for the classification task.
Due to the small size of the labeled sets (e.g.,
225 for Micro-WN(Op)-1) we performed the 10
? 10 CV evaluation (10 cross-validation trials,
each on randomly permuted data) (Bouckaert,
2003) both for regression and classification. For
comparison, we evaluated the SentiWordNet in
the same way ? we averaged the SentiWordNet
scores for all the senses of monosentimous words
from the Micro-WN(Op)-1.
Although the semi-supervised step itself was
not designed to deal with polarity regression task
and sentiment classification task, we decided to
evaluate the results gained from graphs on these
tasks as well. This gives us an insight to how
much the supervised step adds in terms of perfor-
mance. The positivity and negativity scores ob-
tained from graphs were directly evaluated on the
regression task measuring the RMSE against the
gold standard. Classification labels were deter-
7
mined by comparing the positive rank of the word
against the negative rank of the word. The word
was classified as neutral if the absolute difference
between its positive and negative rank was below
the given treshold t. Empirically determined opti-
mal value of the treshold was t = 1000.
Table 2 we present the results of the hybrid
method on both the regression (for both positive
and negative scores) and classification tasks com-
pared with the performance of the SentiWordNet
and the baselines. Additionally, we present the
results obtained using only the semi-supervised
step. On both the regression and classification
task our method outperforms the baseline. The
performance is comparable to SentiWordNet on
the sentiment classification task. However, the
performance of our corpus-based approach is sig-
nificantly lower than SentiWordNet on the polar-
ity regression task ? a more detailed analysis is
required to determine the cause of this. The hy-
brid approach performs significantly better than
the semi-supervised method alone, confirming the
importance of the supervised step.
Models trained on the Micro-WN(Op)-1 were
applied on the set of words from the Micro-
WN(Op)-0 not present in the Micro-WN(Op)-1
(i.e., the difference between the two sets) in order
to test the performance on non-monosentimous
words. The obtained results on this set are, sur-
prisingly, slightly better (positivity regression ?
0.337; negativity regression ? 0.313; and classi-
fication ? 57.55%). This is most likely due to the
fact that, although not all senses have the same
sentiment, most of them have similar sentiment,
which is often also the sentiment of the dominant
sense in the corpus.
6 Conclusion
We have described a hybrid approach to sentiment
lexicon acquisition from corpus. On one hand, the
approach combines corpus-based lexical seman-
tics with graph-based label propagation, while on
the other hand it combines semi-supervised and
supervised learning. We have evaluated the per-
formance on three sentiment prediction tasks: po-
larity ranking task, polarity regression task, and
sentiment classification task. Our experiments
suggest that the results on the polarity ranking
task are comparable to SentiWordNet. On the
sentiment classification task, the results are also
comparable to SentiWordNet when restricted to
monosentimous words. On the polarity regression
task, our results are worse than SentiWordNet, al-
though still above the baseline.
Unlike with the WordNet-based approaches, in
which sentiment is predicted based on sentiment-
preserving semantic relations between synsets,
the corpus-based approach operates at the level
of words and thus suffers from two major limi-
tations. Firstly, the semantic relations extracted
from corpus are inherently unstructured, vague,
and ? besides paradigmatic relations ? also in-
clude syntagmatic and very loose topical rela-
tions. Thus, sentiment labels propagate in a less
controlled manner and get influenced more easily
by the context. For example, words ?understand-
able? and ?justifiable? get labeled as predomi-
nately negative, because they usually occur in
negative contexts. Secondly, in the approach we
described, polysemy is not accounted for, which
introduces sentiment prediction errors for words
that are not monosentimous. It remains to be
seen whether this could be remedied by employ-
ing WSD prior to sentiment lexicon acquisition.
For future work we intend to investigate how
syntax-based information can be used to intro-
duce more semantic structure into the graph.
We will experiment with other hybridization ap-
proaches that combine semantic links from Word-
Net with corpus-derived semantic relations.
Acknowledgments
We thank the anonymous reviewers for their
useful comments. This work has been sup-
ported by the Ministry of Science, Education and
Sports, Republic of Croatia under the Grant 036-
1300646-1986.
References
E. Agirre, D. Mart??nez, O.L. de Lacalle, and A. Soroa.
2006. Two graph-based algorithms for state-of-the-
art wsd. In Proc. of the 2006 Conference on Em-
pirical Methods in Natural Language Processing,
pages 585?593. Association for Computational Lin-
guistics.
A. Andreevskaia and S. Bergler. 2006. Mining word-
net for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In Proc. of EACL, volume 6,
pages 209?216.
S. Baccianella, A. Esuli, and F. Sebastiani. 2010.
Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In
8
Table 2: The performance on the polarity regression task and sentiment classification task
Regression (RMSE) Classification (micro-F1)
Positivity Negativity
Hybrid approach 0.363 ? 0.005 0.387 ? 0.003 0.548 ? 0.126
Baseline 0.383 0.413 0.427
Semi-supervised 0.443 0.466 0.484
SentiWordNet 0.284 0.294 0.582
Proc. of the Seventh International Conference on
Language Resources and Evaluation (LREC?10),
Valletta, Malta. European Language Resources As-
sociation (ELRA).
R.R. Bouckaert. 2003. Choosing between two
learning algorithms based on calibrated tests.
In Machine learning-International workshop then
conference-, volume 20, pages 51?58.
S. Cerini, V. Compagnoni, A. Demontis, M. For-
mentelli, and G. Gandini. 2007. Micro-WNOp:
A gold standard for the evaluation of automati-
cally compiled lexical resources for opinion mining.
Language resources and linguistic theory: Typol-
ogy, second language acquisition, English linguis-
tics, pages 200?210.
K.W. Church and P. Hanks. 1990. Word associa-
tion norms, mutual information, and lexicography.
Computational linguistics, 16(1):22?29.
S.T. Dumais. 2004. Latent semantic analysis. An-
nual Review of Information Science and Technol-
ogy, 38(1):188?230.
A. Esuli and F. Sebastiani. 2007. Pageranking word-
net synsets: An application to opinion mining. In
Annual meeting-association for computational lin-
guistics, volume 45, pages 424?431.
S. Evert. 2008. Corpora and collocations. Cor-
pus Linguistics. An International Handbook, pages
1212?1248.
R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and
E. Vee. 2004. Comparing and aggregating rank-
ings with ties. In Proc. of the twenty-third ACM
SIGMOD-SIGACT-SIGART symposium on Princi-
ples of database systems, pages 47?58. ACM.
C. Fellbaum. 2010. Wordnet. Theory and Applica-
tions of Ontology: Computer Applications, pages
231?243.
A.B. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: graph-based semi-
supervised learning for sentiment categorization. In
Proc. of the First Workshop on Graph Based Meth-
ods for Natural Language Processing, pages 45?52.
Association for Computational Linguistics.
V. Hatzivassiloglou and K.R. McKeown. 1997. Pre-
dicting the semantic orientation of adjectives. In
Proc. of the eighth conference on European chap-
ter of the Association for Computational Linguis-
tics, pages 174?181. Association for Computational
Linguistics.
M. Hu and B. Liu. 2004. Mining opinion features in
customer reviews. In Proc. of the National Confer-
ence on Artificial Intelligence, pages 755?760.
J. Kamps, MJ Marx, R.J. Mokken, and M. De Rijke.
2004. Using WordNet to measure semantic orienta-
tions of adjectives.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1999.
The PageRank citation ranking: Bringing order to
the web.
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature
subsumption for opinion analysis. In Proc. of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 440?448. Association
for Computational Linguistics.
M. Sahlgren. 2006. The Word-Space Model: Us-
ing Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words
in High-Dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
S. Somasundaran, T. Wilson, J. Wiebe, and V. Stoy-
anov. 2007. Qa with attitude: Exploiting opinion
type analysis for improving question answering in
on-line discussions and the news. In Proc. of the In-
ternational Conference on Weblogs and Social Me-
dia (ICWSM). Citeseer.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2011. Lexicon-based methods for sen-
timent analysis. Computational Linguistics, (Early
Access):1?41.
P. Turney and M.L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation
from association. In ACM Transactions on Infor-
mation Systems (TOIS).
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: an exploration of fea-
tures for phrase-level sentiment analysis. Computa-
tional Linguistics, 35(3):399?433.
X. Zhu and A.B. Goldberg. 2009. Introduction to
semi-supervised learning. Synthesis lectures on ar-
tificial intelligence and machine learning, 3(1):1?
130.
9
