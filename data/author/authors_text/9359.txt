R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 498 ? 506, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Answering Definition Questions Using  
Web Knowledge Bases 
Zhushuo Zhang, Yaqian Zhou, Xuanjing Huang, and Lide Wu 
Department of Computer Science and Engineering, Fudan University, Shanghai, China, 200433 
{zs_zhang, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn 
Abstract. This paper presents a definition question answering approach, which 
is capable of mining textual definitions from large collections of documents. In 
order to automatically identify definition sentences from a large collection of 
documents, we utilize the existing definitions in the Web knowledge bases in-
stead of hand-crafted rules or annotated corpus. Effective methods are adopted to 
make full use of Web knowledge bases, and they promise high quality response 
to definition questions. We applied our system in the TREC 2004 definition 
question-answering task and achieved an encouraging performance with the F-
measure score of 0.404, which was ranked second among all the submitted runs. 
1   Introduction 
When people want to learn an unknown concept from a large collection of documents, 
the most commonly used tools are the search engines. They submit a query to a search 
engine system, and the search engine returns a number of pages related to the query 
terms. Usually, the pages returned are ranked mainly based on keywords matching 
rather than their relevance to the query terms. The users have to read a lot of returned 
pages to organize the information they wanted by themselves. This procedure is time-
consuming, and the information acquired is not concentrative. The research of Ques-
tion Answering (QA) intends to resolve this problem by answering user?s questions 
with exact answers.  
Questions like ?Who is Colin Powell?? or ?What is mold?? are definition questions 
[3]. Their relatively frequent occurrences in logs of Web search engines [2] indicate 
that they are an important type of question. The Text REtrieval Conference (TREC) 
provides an entire evaluation for definition question answering from TREC2003. A 
typical definition QA system extracts definition nuggets that contain the most descrip-
tive information about the question target (the concept for which information is being 
sought is called the target term, or simply, the target) from multiple documents.  
Until recently, definition questions remained a largely unexplored area of question 
answering. Standard factoid question answering technology, designed to extract single 
answers, cannot be directly applied to this task. The solution to this interesting re-
search challenge will involve the techniques in related fields such as information 
extraction, multi-document summarization, and answer fusion. 
In order to extract definitional nuggets/sentences, most systems use various pattern 
matching approaches. Kouylekov et al [10] relied on a set of hand-crafted rules to 
 Answering Definition Questions Using Web Knowledge Bases 499 
find definitional sentences. Sasha et al [12] proposed to combine data-driven statisti-
cal method and machine learned rules to generate definitions. Cui et al [7] used soft 
patterns, which were generated by unsupervised learning. Such methods require hu-
man labor to construct patterns or to annotate corpus more or less. 
Prager et al [8] try to solve this problem through existing technology. They de-
compose a definition question into a series of factoid questions. The answers to the 
factoid questions are merged to form the answer to the original question. However, 
the performance of their system on the TREC definition QA task is unsatisfactory. 
They need a more proper framework to determine how to generate these follow-up 
questions [8]. 
Some systems [1] [7] [9] statistically rank the candidate answers based on the ex-
ternal knowledge. They all adopt a centroid-based ranking method. For each question, 
they form one centroid (i.e., vector of words and frequencies) of the information in 
the external knowledge, and then calculate the similarity between the candidate an-
swer and this centroid. The ones that have large similarity are extracted as the answers 
to this question.  
Among the abundant information on the Web, Web knowledge bases (KBs) are 
one kind of most useful resource to acquire information. Dictionary definitions often 
supply knowledge that can be exploited directly. The information from them can 
model the interests of a typical user more reliably than other information. So we go 
further in identifying and selecting definition sentences from document collection 
using Web knowledge bases.  
Our work differs from the above in that we make use of the Web knowledge bases 
in a novel and effective way. Instead of using centroid-based ranking, we try to find 
out more effective methods in ranking the candidate sentences. We consider the rela-
tionship and the difference between the definitions from different knowledge sources. 
In our first algorithm, we calculate the similarity scores between the candidate sen-
tence and the definitions from different knowledge bases respectively, and merge 
these scores to generate the weight of this candidate sentence. In another algorithm, 
we first summarize the definitions from different KBs in order to eliminate the redun-
dant information, and then use this summary to rank the candidate sentences. We have 
applied our approaches to the TREC 2004 definition question-answering task. The 
results reveal that these procedures can make better use of the knowledge in the Web 
KBs, and the extracted sentences contain the most descriptive information about the 
question target. 
The remainder of the paper is organized as follows. In Section 2, we describe the 
system architecture. Then in Section 3 we give the details of our definition extraction 
methods. The evaluation of our system and the concluding remarks are given in Sec-
tion 4 and Section 5 respectively. 
2   System Architecture 
We adopt a general architecture for definition QA. The system consists of five mod-
ules: question processing, document processing, Web knowledge acquisition, defini-
tion extraction, and an optional module corpus information acquisition. The process 
of answering a definition question is briefly described as follows. 
500 Z. Zhang et al 
Firstly, a definition question is input, and the question processing module identifies 
the question target from this question. The so called target or target term is a term for 
which information is being sought (e.g., the target of the question ?What is Hale Bopp 
comet?? is ?Hale Bopp comet?.) The target term is the input for document processing 
module and knowledge acquisition module.  
Secondly, the document processing module generates the candidate sentence set 
according to this target term. This module has three steps, document retrieval, rele-
vant sentence extraction and redundancy removal. In the first step, the documents that 
relevant to the target are retrieved from the corpus. In the second step, the sentences 
that relevant to the target are extracted from these documents. We first cut the docu-
ments into sentences, and delete the irrelevant sentences by a few heuristic rules. In 
the third step, the redundant sentences are deleted by calculating the percentage of 
shared content words between sentences. After these three steps, we get the candidate 
sentence set.  
 
Definition 
Extraction 
Question 
processing 
Web Knowledge 
Acquisition 
Answer 
Question 
Target 
Candidate sentence set 
Definitions from Web KB 
World Wide Web
Question Corpus 
Corpus Information 
Acquisition 
Document 
processing 
 
Fig. 1.  System architecture 
Thirdly, the Web knowledge acquisition module acquires the definitions of the tar-
get term from the Web knowledge base. If we can find definitions from these sources 
(the Web KBs we used will be described in Section 3.1), we use them to rank the 
candidate sentences set.  
At last, the definition extraction module extracts the definition from the candidate 
sentence set based on the knowledge which is got from the Web knowledge base. 
In very few situations, no definitions can be found from the Web KBs, and the 
module named ?corpus information acquisition? is adopted to form the centroid of the 
 Answering Definition Questions Using Web Knowledge Bases 501 
candidate sentence set. We rank candidate sentences based on this centroid. The sen-
tences that have high similarity with this centroid are extracted as the answers to the 
question. The assumption is that words co-occurring frequently with the target in the 
corpus are more important ones for answering the question.  
The system architecture is illustrated in Fig.1. 
In this paper, we focus on how to make use of the Web KBs in extracting definition 
sentences, so we will describe the detail of the definition extraction module below. 
3   Definition Extraction Based on Web Knowledge Bases 
3.1   Web Knowledge Base 
There are lots of specific websites on the Web, such as online biography dictionaries 
or online cyclopaedias. We can get biography of a person, the profile of an organiza-
tion or the definition of a generic term from them. We call this kind of website Web 
knowledge base (KB). The definitions from them often supply knowledge that can be 
exploited directly. So we answer definition questions by utilizing the existing defini-
tions in the Web knowledge bases. The results of our system reveal that the Web 
knowledge bases are quite helpful to answering definition questions. 
Usually, different knowledge bases may pay attention to different kind of concept, 
and they may have different kind of entries. For example, the biography dictionary 
(www.s9.com) is a dictionary that covers widely on biography of people, and other 
KBs may pay attention to other kinds of concept. We choose several authoritative 
KBs that cover different kinds of concept to achieve our goal.  
The Web knowledge bases we used are the Encyclopedia(www.encyclopedia.com), 
the Wikipedia(www.wikipedia.com), the Merriam-Webster dictionary (www.mw. 
com), the WordNetglossaries (www.cogsci.princeton.edu/cgi-bin/webwn) and a biog-
raphies dictionary (www.s9.com). 
0
0. 2
0. 4
0. 6
0. 8
1
TREC2003 TREC2004
pe
rc
en
t
ency
wi ki
mw
wn
s9
al l
 
Fig. 2. The Web KBs? coverage of TREC data 
These Web KBs can cover most of the target terms, and the definitions in them are 
exact and concise.This can be confirmed from the experiment on TREC?s data set. 
Fig.2 gives our experiment results on the TREC 2003 and TREC 2004?s definition 
question sets, which have 50 and 65 target terms respectively. The ?ency?, ?wiki?, 
502 Z. Zhang et al 
?mw?, ?wn? and ?s9? stand for the five online KBs we have used. Each column repre-
sents the percent of the target terms that can be found in the corresponding online 
knowledge base. The column marked ?all? represents the percent of the target terms 
that can be found in at least one of these five online knowledge bases. 
It is easy to see that a high coverage can be got by using these Web knowledge 
bases. In the Section 3.2 and Section 3.3 we will show how to use these KBs, and in 
Section 4 we can see that it boosts the performance of the system significantly. 
3.2   Definition Extraction Based on GDS 
As mentioned above, we may get most of the submitted target terms? definitions by 
utilizing multiple Web KBs. One target may find its definitions in more than one 
knowledge base. Are all of them useful? The experimental data tells that, the different 
definitions belonging to one target differ from each other in some degree. They are 
short or long, concise or detailed. 
Considering the above factor, we try to utilize all of the definitions from different 
Web KBs to accomplish our task. For one target term, the definitions from all Web 
knowledge bases compose its ?general definition set?, which is abbreviated to GDS. 
Each element of this set is a definition from one Web knowledge base, so the number 
of the elements in this set is the same as the number of the Web KBs we used. When 
we cannot find its entry in a certain Web KB, its corresponding element will be an 
empty string.  
For each target, its candidate sentence set is expressed as SA = {A1, A2,?, Am}, 
where Ak (k=1..m) is a candidate sentence in the set and m is the total number of the 
candidate sentences.  
GDS is expressed as SGD = {D1, D2 ,..., Dn}, where Dk (k=1..n) is the definition of 
the target from the kth knowledge base, and n is the number of the knowledge bases. 
Dk may be an empty string when the target has no definition in the knowledge base k. 
In this algorithm, we rank the candidate sentences set SA = {A1, A2,?, Am} using 
SGD. 
Let Sij be the similarity of Ai and Dj. The similarity is the tf.idf score, where the 
candidate sentence Ai and the definition Dj are all treated as a bag of words. The tf.idf 
function we used is described in [5]. 
For each candidate sentence Ai in the set SA, we calculate its score based on the 
GDS as follows: 
ij
n
j
ji Swscore ?
=
=
1
 (?
=
=
n
j
jw
1
1 ) . (1) 
The weights jw are fixed based on experiment, considering the authoritativeness of 
the knowledge base from which Dj comes. The sentences of set SA are ranked based 
on this score, and the top ones are chosen as the definition of the target term. 
3.3   Definition Extraction Based on EDS 
As we have seen, for a target term, different definitions in its ?general definition set? 
may overlap in some degree. We intent to modify this set by merging its elements into 
 Answering Definition Questions Using Web Knowledge Bases 503 
one concise definition. We extract the essential information from the ?general defini-
tion set? to form the ?essential definition set?, which is abbreviated to EDS.  
EDS is expressed as SED = {d1, d2 ,?, dl}, where each element dk (k=1..l) is an es-
sential definition sentence about the target, and l is the number of the essential defini-
tion sentences. We hope that each element can tell one important aspect of the target 
term, and the whole ?essential definition set? may contain as much information as 
GDS but no redundant information. 
We try to use an automatic text summary technique [11] to get EDS. This tech-
nique is based on sentence?s weight and similarity between sentences. Firstly, calcu-
late the weights of all sentences and similarities between any two sentences, and then 
extract sentence based on these weights. After one sentence has been extracted, calcu-
late the new weights of the remained sentences based on their similarities. Iterate the 
above procedure until the extracted sentences reach the required length. More detail 
of this technique can be found in [11]. In this section we will try to use the ?essential 
definition set? to extract definitions from the candidate sentence set. 
1. Initially set the result set A={}, and i=1. 
2. For the element di in the set SED:  
First get the similarity between di and Aj (j=1..m), which is ex-
pressed as Sij.  
Then let },...,,max{ 21 imiiik SSSS = . If Sik>minsim , then add Ak 
to the set A and delete Ak from the set SA . 
3. If lengthAL
m
k
k max_)(
1
>??
=
 or i equals to l, the algorithm ends; 
otherwise, i = i +1, go to step2. 
Fig. 3.   Definition extraction using EDS 
The algorithm was showed in Fig.3. The candidate sentence set is also expressed as 
SA = {A1, A2,?, Am}, where Ak (k=1..m) is a candidate sentence in the set and m is 
the total number of the candidate sentences. The similarity Sij is calculated as the 
same as in Section 3.2. ( )kAL  represents the length of string Ak in character and m? is 
the number of elements in set A. The parameters max_length and minsim were em-
pirically set based on TREC?s definition question set. The last result is set A, where 
A={A1, A2,?, Am?}. 
4   Evaluation 
In order to get comparable evaluation, we apply our approach to TREC2004 defini-
tion QA task. We can see that our approach is an effective one compared with peer 
systems in this competitive evaluation. 
In this section we present the evaluation criterion and system performance on 
TREC task, and discuss the effectiveness of our approach. 
504 Z. Zhang et al 
4.1   Evaluation Criterion  
The TREC evaluation criterion [3] is summarized here for the purpose of discussing 
the evaluation results.  
For an individual definition question, there is a list of essential nuggets and accept-
able nuggets provided by TREC. These given nuggets are used to score the definition 
generated by the system. 
An individual definition question will be scored using nugget recall (R) and an ap-
proximation to nugget precision (P) based on length. In particular, 
R = # essential nuggets returned in response/# essential nuggets 
P is defined as: if    length < allowance,  P = 1 
else    P=1-[(length-allowance)/length] 
where    allowance = 100*(# essential+acceptable nuggets returned) 
length = total # non-white-space characters in answer strings 
The F measure is:  
RP?
PR?
+
)1+(
=F 2
2
 . (2) 
where ? value is fixed three in TREC 2004, and we also use three to get comparable 
result. 
The score of a system is the arithmetic mean of F-measure scores of all the defini-
tion questions output by the system. 
4.2   Effectiveness of Web Knowledge Bases 
To compare the effectiveness of the Web knowledge bases, we experimented on the 
TREC 2004 definition question set. The result can be seen in Table1. 
Table 1 shows the F-measure scores of our two algorithms and the baseline 
method. It also shows the median of the scores of all participating systems in TREC 
2004. The baseline method is: for an input question, form the candidate sentence set 
by using the approach described in Section 2. Then put the sentence of this set into the 
answer set one by one until all the sentences in the candidate sentence set are consid-
ered or the answer length is greater than a pre-fixed length (we set the length 3000 
characters in our experiment). 
We can see that our two algorithms all outperform the median and the baseline 
method which does not use Web knowledge bases. In conclusion, the Web knowledge 
bases are effective resources to definition question answering. 
Table 1. The F- measure score of the baseline method, the median system in TREC2004, and 
our two methods on TREC 2004 data set 
 Baseline 
method 
Median Ranking 
using  GDS 
Ranking 
using EDS 
F-measure 
score (?=3) 
0.231 0.184 0.404 0.367 
 Answering Definition Questions Using Web Knowledge Bases 505 
4.3   Definition Extraction Based on GDS vs. Based on EDS 
As we have mentioned, we have tried two algorithms in the definition extraction mod-
ule, which are based on GDS and EDS respectively. The performance of these algo-
rithms is shown in Table 2. 
Table 2. Performance of our three runs on the three types of quesitions and on the whole 64 
questions of TREC 2004 
 Num Q Run A Run B Run C 
all 64 0.404 0.389 0.367 
PERSON 23 0.366 0.372 0.404 
ORG 25 0.413 0.389 0.326 
THING 16 0.446 0.415 0.379 
We have submitted three runs in TREC2004, which were generated by using dif-
ferent algorithm in the definition extraction module. Run A and run B were generated 
by using GDS with slightly different weights in formula (1), and run C was generated 
by using EDS. All the 64 questions are divided into three classes based on the entity 
types of the targets, which are person, organization and other thing. Table 2 shows the 
three runs? F-measure scores on these three types and their overall score on the whole 
64 questions. 
Two algorithms? F-measure scores are all among the best of total 63 runs. Run C?s 
score on the ?PERSON?, 0.404 is the highest of our three runs on this type. Run A does 
better on the types named ?ORG? and ?THING?. We can say that these two algorithms 
contribute to different kinds of target terms. Dividing definition questions into different 
subclass and processing them with different methods could be a proper direction. 
Considering the score on all the 64 questions, the former algorithm is slightly 
higher than the latter one. However, the result of the latter one is also encouraging. 
Since the ?essential definition set? contain the important information and less redun-
dancy, it has the potential to get the answers, which are not only concise but also have 
wide coverage about the target. We believe it is an appropriate way to extract the high 
quality definitions. A preliminary analysis shows that the major problem is how to 
improve the quality and the coverage of the essential definition set. We believe that 
the performance could be boosted through improving this technique. 
In conclusion, we can say that our methods can make better use of the external 
knowledge in answering definition question. 
5   Conclusions 
This paper proposes a definition QA approach, which makes use of Web knowledge 
bases and several complementary technology components. The experiments reveal that 
the Web knowledge bases are effective resources to definition question answering, and 
the presented method gives an appropriate framework for answering this kind of ques-
tion. Our approach has achieved an encouraging performance with the F-measure score 
of 0.404, which is ranked second among all the submitted runs in TREC2004. 
506 Z. Zhang et al 
Since definitional patterns can not only filter out those statistically highly-ranked 
sentences that are not definitional, but also bring those definition sentences that are 
written in certain styles for definitions but are not statistically significant into the 
answer set. [6] In the future work, we will employ some pattern matching methods to 
reinforce our existing method. 
Acknowledgements 
This research was partly supported by NSF (Beijing, China) under contract of 
60435020, and Key Project of Science and Technology Committee of Shanghai under 
contract of 035115028. 
References 
1. Abdessamad Echihabi, Ulf Hermjakob, Eduard Hovy: Multiple-Engine Question Answer-
ing in TextMap. In Proceedings of the Twelfth Text REtreival Conference. NIST, Gath-
ersburg, MD (2003) 772?781 
2. Ellen M. Voorhees: Overview of the TREC 2001 question answering track. In Proceedings 
of the Tenth Text REtreival Conference. NIST, Gathersburg, MD (2001) 42?51 
3. Ellen M. Voorhees: Overview of the TREC 2003 Question Answering Track. In Proceed-
ings of the Twelfth Text REtreival Conference. NIST, Gathersburg, MD (2003) 54?68 
4. Ellen M. Voorhees: Evaluating answers to definition questions. In Proceedings of the 2003 
Human Language Technology Conference of the North American Chapter of the Associa-
tion for Computational Linguistics (2003) Volume 2: 109?111 
5. G.Salton, C. Buckley: Term weighting approaches in automatic text retrieval. Information 
Processing and Management (1988) 24(5): 513?523 
6. Hang Cui, Min-Yen Kan, Tat-Seng Chua, Jing Xiao: A comparative Study o Sentence Re-
trieval for Definitional Question Answering. In Proceedings of the 27th Annual Interna-
tional ACM SIGIR Conference (2004) 
7. Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, Min-Yen kan: National University of 
Singapore the TREC-13 Question Answering Main Task. In Proceedings of the Thirteenth 
Text REtreival Conference. NIST, Gathersburg, MD (2004) 
8. J. M. Prager, Jennifer Chu-Carroll, Krzysztof Czuba, Christopher Welty, Abraham It-
tycheiach, Ruchi Mahindru: IBM?s PIQUANT in TREC2003. In Proceedings of the 
Twelfth Text REtreival Conference. NIST, Gathersburg, MD (2003) 283?292 
9. Jinxi Xu, Ana Licuanan, Ralph Weischedel: TREC2003 QA at BBN: Answering defini-
tional Questions. In Proceedings of the Twelfth Text REtreival Conference. NIST, Gath-
ersburg, MD (2003) 98~106 
10. Milen Kouylekov, Bernardo Magnini, Matteo Negri, Hristo Tanev: ITC-irst at TREC-
2003: the DIOGENE QA system. In Proceedings of the Twelfth Text REtreival Confer-
ence. NIST, Gathersburg, MD (2003) 349?357 
11. Qi Zhang, Xuanjing Huang, Lide Wu: A New Method for Calculating Similarity between 
Sentences and Application on Automatic Text Summarization. In Proceedings of the first 
National Conference on Information Retrieval and Content Security (2004) 
12. Sasha Blair-Goldensohn, Kathleen R. McKeown, Andrew Hazen Schlaikjer: A hybrid ap-
proach for QA track definitional questions. In Proceedings of the Twelfth Text REtreival 
Conference. NIST, Gathersburg, MD (2003) 185?192 
Transformation Based Chinese Entity Detection and Tracking 
Yaqian Zhou? 
Dept. Computer Science and Enginering 
Fudan Univ. 
Shanghai 200433, China 
ZhouYaqian@fudane.edu.cn 
Changning Huang 
Microsoft Research, Asia 
Beijing 100080, China 
cnhuang@msrchina.research.
microsoft.com 
Jianfeng Gao 
Microsoft Research, Asia 
Beijing 100080, China 
jfgao@microsoft.com 
Lide Wu 
Dept. of CSE., Fudan Univ. 
Shanghai 200433, China 
ldwu@fudane.edu.cn 
                                                          
? This work is done while the first author is visiting Microsoft Research Asia. 
 
 
Abstract 
This paper proposes a unified 
Transformation Based Learning (TBL, 
Brill, 1995) framework for Chinese 
Entity Detection and Tracking (EDT). 
It consists of two sub models: a 
mention detection model and an entity 
tracking/coreference model. The first 
sub-model is used to adapt existing 
Chinese word segmentation and Named 
Entity (NE) recognition results to a 
specific EDT standard to find all the 
mentions. The second sub-model is 
used to find the coreference relation 
between the mentions. In addition, a 
feedback technique is proposed to 
further improve the performance of the 
system. We evaluated our methods on 
the Automatic Content Extraction 
(ACE, NIST, 2003) Chinese EDT 
corpus. Results show that it 
outperforms the baseline, and achieves 
comparable performance with the state-
of-the-art methods. 
1 Introduction 
The task of Entity Detection and Tracking (EDT) 
is suggested by the Automatic Content Extrac-
tion (ACE) project (NIST, 2003). The goal is to 
detect all entities in a given text and track all 
mentions that refer to the same entity. The task 
is a fundamental to many Natural Language 
Processing (NLP) applications, such as informa-
tion retrieval and extraction, text classification, 
summarization, question answering, and ma-
chine translation.  
EDT is an extension of the task of 
coreference resolution in that in EDT we not 
only resolve the coreference between mentions 
but also detect the entities. Each of those entities 
may have one or more mentions. In the ACE 
project, there are five types of entities defined in 
EDT: person (PER), geography political Entity 
(GPE), organization (ORG), location (LOC), 
and facility (FAC). Many traditional coreference 
techniques can be extended to EDT for entity 
tracking. 
Early work on pronoun anaphora resolution 
usually uses rule-based methods (e.g. Hobbs 
1976; Ge et al, 1998; Mitkov, 1998), which try 
to mine the cues of the relation between the pro-
nouns and its antecedents. Recent research 
(Soon et al, 2001; Yang et al, 2003; Ng and 
Cardie, 2002; Ittycherah et al, 2003; Luo et al, 
2004) focuses on the use of statistical machine 
learning methods and tries to resolve references 
among all kinds of noun phases, including name, 
nominal and pronoun phrase. One common ap-
proach applied by them is to first train a binary 
statistical model to measure how likely a pair of 
232
mentions corefer; and then followed by a greedy 
procedure to group the mentions into entities. 
Mention detection is to find all the named en-
tity, noun or noun phrase, pronoun or pronoun 
phrase. Therefore, it needs Named Entity Rec-
ognition, but not only. Though the detection of 
entity mentions is an essential problem for 
EDT/coreference, there has been relatively less 
previous research. Ng and Cardie (2002) shows 
that improving the recall of noun phrase identi-
fication can improve the performance of a 
coreference system. Florian et al (2004) formu-
late the mention detection problem as a charac-
ter-based classification problem. They assign for 
each character in the text a label, indicating 
whether it is the start of a specific mention, in-
side a specific mention, or outside of any men-
tion. 
In this paper, we propose a unified EDT 
model based on the Transformation Based 
Learning (TBL, Brill, 1995) framework for Chi-
nese. The model consists of two sub models: a 
mention detection model and a coreference 
model. The first sub-model is used to adapt ex-
isting Chinese word segmentation and Named 
Entity (NE) recognition system to a specific 
EDT standard. TBL is a widely used machine 
learning method, but it is the first time it is ap-
plied to coreference resolution. In addition, a 
feedback technique is proposed to further im-
prove the performance of the system. 
The rest of the paper is organized as follows. 
In section 2, we propose the unified TBL Chi-
nese EDT model framework. We describe the 
four key techniques of our Chinese EDT, the 
word segmentation adaptation model, the men-
tion detection model, the coreference model and 
the feedback technique in section 3, 4, 5 and 6 
accordingly. The experimental results on the 
ACE Chinese EDT corpus are shown in section 
7. 
2 The Unified System Framework 
Our Chinese EDT system consists of two com-
ponents, mention detection module and corefer-
ence module besides a feedback technique 
between them as illustrated in Figure 1. 
MSRSeg (Gao et al, 2003; Gao et al), Mi-
crosoft Research Asia?s Chinese word segmen-
tation system that is integrated with named 
entity recognition, is used to segment Chinese 
words. However MSRSeg can?t well match the 
standard of ACE EDT evaluation for either 
types or boundaries. The difference of the stan-
dard of named entity between MSRSeg and 
ACE cause more than half of the errors for 
NAME mention detection. In order to overcome 
these problems, we integrate a segmentation 
adapter to mention detection model. 
The EDT system is a unified system that uses 
the TBL scheme. The idea of TBL is to learn a 
list of ordered rules while progressively improve 
upon the current state of the training set. An ini-
tial assignment is made based on simple statis-
tics, and then rules are greedily learned to 
correct the mistakes, until no more improvement 
can be made. There are three main problems in 
the TBL framework: An initial state assignment, 
a set of allowable templates for rules, and an 
objective function for learning. 
Figure 1. Entity detection and tracking system 
flow. 
3 Word Segmentation Adaptation 
The method of applying TBL to adapt the Chi-
nese word segmentation standard has been de-
scribed in Gao et al (2004). Our approach is 
slightly different for not have a correctly seg-
mented corpus according to ACE standard. 
From the un-segmented ACE EDT corpus, 
we can only obtain mention boundary informa-
tion. So the adapting objective is to detect the 
mention boundary instead of all words in text, 
correctly. In the corpus, very few mentions? 
boundaries are crossing1. 
The initial state of the segmentation adapta-
tion model is the output of MSRSeg. And we 
                                                          
1 The mentions? extents are frequently crossing, while 
heads not. 
MSRSeg&POS
Tagging 
Mention 
Detection 
Model 
Coreference 
Model 
Raw 
Document
Mentions Entities
Seg/POS/NE 
Document 
233
define two actions in the model, inserting and 
removing a boundary. The prefix or suffix of 
current word is used to define the boundary of 
inserting. Both inserting and removing action 
consider the combination of POS tag and word 
string of current, left and right words.  
When inserting a boundary, the right part of 
the word keeps the old POS tag, and the left part 
introduces a special POS tag ?new?. When re-
moving a boundary, the new formed word intro-
duces a special POS tag ?new?. The following 
two examples illustrate the strategy. 
????? /nt/court of Russia ? ???
/new/Russia ??/nt/court 
?/nr/Bo ?/nr/Pu ???/new/Bopu 
 
4 Mention Detection 
Since the word segmentation adaptation model 
has corrected the boundaries of mentions, our 
mention detection model bases on word and 
only tagging the entity mention types. The 
model detects the mentions by tagging sixteen 
tags (including the combination of five entity 
types and three mention types and ?OTHER? tag) 
for all the words outputted by segmentation ad-
aptation model. The templates, as illustrated in 
table 1, only refer to local features, such as POS 
tag and word string of left, right, and current 
words; the suffix, and single character feature of 
current word. 
Table 1. Templates for mention detection. 
MT1: P0 MT9: R4,P0 
MT2: W0 MT10: R3,P0 
MT3: P0,W0 MT11: R2,P0 
MT4: P_1,W0 MT12: R1,P0 
MT5: P_1,P0 MT13: S0,P0 
MT6: W0,P1 MT14: T_1,W0 
MT7: P0,P1 MT15: T_1,P0 
MT8: W0,W1 MT16: P0,T1 
Table 2. Examples of transformation rules of 
mention detection. 
MR1: MT13 0 ns GPE 
MR2: MT13 0 nr PER 
MR3: MT13 0 nt ORG 
MR4: MT16 n PER NPER 
MR5: MT16 new ORG GPE 
In table 1, ?MT1? et alrepresent the id of the 
templates; ?R1?, ?R2?, ?R3? and ?R4? represent 
the suffix of current word and the number of 
character is 1, 2, 3 and 4 accordingly; other suf-
fix ?_1?, ?0?, ?1? means the left, current and 
right words? feature; ?W? represent the string of 
word; ?P? represent POS tag; ?T? represent 
mention tag; ?S? represent the binary-value sin-
gle character feature. 
Five best transformation rules are illustrated 
in Table 2. For example, MR3 means ?if current 
word?s POS tag is nt, then it is a ORG?. Follow-
ing example well describe the process of apply-
ing these rules. 
???/new/Russia ??/nt/court  
????/new/Russia [??/nt/court]ORG (MR3)
?[ ? ? ? /new/Russia]GPE [ ? ?
/nt/court]ORG 
(MR5)
5 Entity Tracking 
In our entity tracking/coreference model, the 
initial state is let each mention in a document 
form an entity, as shown in Figure 2 (a). And the 
objective function directs the learning process to 
insert or remove chains between mentions (Fig-
ure 2 b and c) to approach the goal state (Figure 
2 f). 
A list of rules is learned in greedy fashion, 
according to the objective function. When no 
rule that improves the current state of the train-
ing set beyond a pre-set threshold can be found, 
the training phrase ends. The objective function 
in our system is driven by the correctness of the 
binary classification for pair-wise mention pairs. 
The TBL entity tracking model has more 
widely clustering/searching space as compare 
with previous strategies (Soon et al 2001; Ng 
and Cardie, 2002; Luo et al, 2004). For example, 
the state shown in Figure 2 (d) is not reachable 
for them. Because they assume one mention 
should refer to its most confidential mentions or 
entities that before it, while A and B are obvi-
ously not in same entity, as we can see in Figure 
2 (d). Thus C can refer to either A or B, but not 
both. While in TBL model, this state is allowed. 
In order to keep our system robust, the trans-
formation templates refer to only six types of 
simple features, as described below.  
All these features do not need any high level 
tools (i.e. syntactic parser) and little external 
knowledge base. In fact, only a country name 
abbreviation list (171 entrances) and a Chinese 
234
province alias list (34 entrances) are used to de-
tect ?alias? relation for String Match feature. 
String Match feature (STRM): Its possible 
values are exact, alias, abbr, left, right, other. If 
two mentions are exact string matched, then re-
turn exact; else if one mention is an alias of the 
other, then return alias; else if one mention is 
the abbreviation of the other, then return abbr; 
else if one mention is the left substring of the 
other, then return left; else if one mention is the 
right substring of the other, then return right; 
else return other. 
Figure 2. The procedure of TBL entity track-
ing/coreference model 
Edit Distance feature I (ED1): Its possible 
values are true or false. If the edit distance of the 
two mentions are less than or equal to 1, then 
return true, else return false. 
Token Distance feature I (TD1): Its possi-
ble values are true or false. If the edit distance of 
the two mentions are less than or equal to 1(i.e., 
there are not more than one token between the 
two mentions), then return true, else return false. 
Mention Type (MT): Its possible values are 
NAME, NOMINAL, or PRONOUN.. 
Entity Type (ET): Its possible values are 
PER, GPE, ORG, LOC, or FAC. 
Mention String (M): Its possible values are 
the actual mention string. 
These six features can be divided into two 
categories: mention pair features (the first three) 
and single mention features (the other three). 
And the single mention features are suffixed 
with ?L? or ?R? to differentiate for left or right 
mentions (i.e. ETL represent the left mention?s 
entity type). 
Based on the six kinds of basic features, four 
simple transformation templates are used in our 
system, as listed in table 3. 
Table 3. Templates for coreference model. 
CT1: MTL,MTR,STRM 
CT2: MTL,MTR,ETL,ETR,ED1 
CT3: MTL,MTR,ETL,ETR,TD1 
CT4: MTL,MTR,ML,MR 
Table 4. Examples of transformation rules of 
coreference model. 
CR1:CT1 NAME NAME EXACT LINK 
CR2:CT2 NOMINAL NAME PER PER 1 LINK 
CR3:CT1 NAME NAME ALIAS LINK 
CR4:CT1 PRONOUN PRONOUN EXACT LINK 
Though trained on different data set will 
learn different rules, the four rules listed in table 
4 is the best rules that always been learned. For 
example, the first rule means that ?If two NAME 
mentions are exact string matched, then insert a 
chain between them?. The following example 
illustrates the process. 
[??/US]GPE??[???/Russia]GPE?
???????[?? /US]GPE[??
/businessman]NPER[??/Bopu]PER 
? [ ? ? /US]GPE-1 ? ? [ ? ? ?
/Russia]GPE ????????[??
/US]GPE-1[?? /businessman]NPER[??
/Bopu]PER 
(CR1)
? [ ? ? /US]GPE-1 ? ? [ ? ? ?
/Russia]GPE ????????[??
/US]GPE-1[?? /businessman]NPER-2[??
/Bopu]PER-2 
(CR2)
6 Feedback 
There are three reasons push us apply feedback 
technique in the EDT system. The first is to de-
termine whether a signal character is an abbre-
viation is discourse depended. For example, 
Chinese character ??? can represents both a 
country name ?China? and a common preposi-
tion ?in?. If it can links to ??? /China? by 
coreference model, it is likely to represent 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
A    B
 
C    D
 
E 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
(e) 
(a)                     (b)                          (c) 
(d) 
(f) 
235
?China?. The second is the definition of men-
tions is hard to hold, especially the nominal 
mentions. An isolated mention is more likely not 
to be a mention. The third is to pick up lost men-
tion according to its multi-appearance in the dis-
course. In fact, [Ji and Crishman, 2004] has used 
five hubristic rules based on coreference results 
to improve the name recognition result. While in 
this section we will present an automatic method. 
The feedback technique is employed by us-
ing entity features in mention detection model. 
In our model, the transformation templates refer 
to the number of mentions in the entity, the sin-
gle character feature, the entity type feature, the 
mention type feature and mention string, as 
listed follows. 
SDD: Its possible values are the combination 
of the mention type and entity type of the men-
tion string in discourse: PER, GPE, ORG, LOC, 
FAC, NPER (NOMINAL PER), NGPE, NORG, 
NLOC, NFAC, PPER (PROUNOUN PER), 
PGPE, PORG, PLOC, and PFAC. 
SC2, SC3, SC4: Their possible values are 
true or false. If the word string appear not less 
than 2 (3, 4) times in the discourse then return 
true, else return false. 
PDD: presents the combination of the men-
tion type and entity type of the mention in dis-
course. Its possible values are same with ?SDD?. 
PC2: Its possible values are true or false. If 
the mention belong to an entity has not less than 
2 mentions then return true, else return false. 
S0: Its possible values are true or false. If the 
mention is a single character word then return 
true, else return false. 
W0: string of the mention. 
Table 5. Templates for feedback. 
FT1: SDD,SC2 FT4: PDD,PC2,S0 
FT2: SDD,SC3 FT5: PDD,PC2,S0 
FT3: SDD,SC4 FT6: PDD,PC2,W0 
Table 6. Examples of transformation rules of 
feedback. 
FR1: FT1 PER T PER FR4: FT4 NORG F O 
FR2: FT5 GPE F 1 O FR5: FT3 PGPE F O 
FR3: FT4 NFAC F O  
The first rule means that ?if a word in the 
document appears as person name more than 
two times, then it is a person name?. This rule 
can pick up lost person names. The second rule 
means that ?if a GPE mention is isolated and it 
is a single character word, then it is not a men-
tion?. This rule can throw away isolated abbre-
viation of GPE, as illustrated in the following 
example. 
?[??/Bopu]PER-3 ?????[???
/Russia]GPE-2[?? /court]ORG-4[? /by]GPE-6 
????? 20??? ? 
??[??/Bopu]PER-3 ?????[??
?/Russia]GPE-2[??/court]ORG-4?/by ?
???? 20??? ? 
(FR2)
7 Experiments 
Our experiments are conducted on Chinese EDT 
corpus for ACE project from LDC. This corpus 
is the training data for ACE evaluation 2003. 
The corpus has two types, paper news (nwire) 
and broadcast news (bnews). the statistics of the 
corpus is shown in Table 7. 
Table 7. Statistics of the ACE corpus. 
 nwire bnews 
Document 99 122 
Character 55,000 45,000 
Entity 2517 2050 
Mention 5423 4506 
Because the test data for ACE evaluation is 
not public, we randomly and equally divide the 
corpus into 3 subsets: set0, set1, set2. Each con-
sists of about 73 documents and 33K Chinese 
Characters 2 . Cross experiments are conducted 
on these data sets. ACE-value is used to evaluate 
the EDT system; and precision (P), recall (R) 
and F (F=2*P*R/(P+R)) to evaluate the mention 
detection result. 
In the experiments, we first use one data set 
train the mention detection system; then use an-
other set train the coreference model based on 
the output of the mention detection; finally use 
the other set test. In practice, we can retrain the 
mention detection model use the two train set to 
get higher performance. 
Table 8. EDT and mention detection results. 
 EDT Mention Detection 
Method ACE-
value 
R P F 
Tag 55.7?1.6 62.3?1.0 85.0?1.4 71.9?0.6
SegTag 61.6?3.6 70.9?4.5 81.9?1.0 75.9?2.6
SegTag+F 63.3?2.0 68.0?4.8 83.8?1.2 75.0?3.1
                                                          
2 Two of the documents (CTS20001110.1300.0506, and 
XIN20001102.2000.0207) in the corpus are not use for 
serious annotation error. 
236
In Table 8, ?SegTag? represent the mention 
detection system integrated with segmentation 
adaptation, ?Tag? represent the mention detec-
tion system without segmentation adaptation. 
?+F? means with feedback. 
The ACE-value of our Chinese EDT system 
is better than 58.8% of Florian et al (2004). In 
fact, the two systems are not comparable for not 
basing on the same training and test data. How-
ever both corpora are under the same standard 
from ACE project, and our training data (about 
66K) is smaller than Florian et al (2004) (about 
80K). Therefore, it is an encouraging result. 
Segmentation adapting and feedback can im-
prove 7.5% of ACE-value for the whole system. 
As we can see from Table 8, using TBL method 
to adapt standard or correct errors can improve 
the mention detection performance especially 
recall, and word segmentation adapting is essen-
tial for mention detection. Feedback can im-
prove the precision of mention detection with 
loss of recall. The two techniques can signifi-
cantly improve the EDT performance, since the 
p-value of the T-test for the performance of 
?SegTag? to ?Tag? is 96.7%, while for ?Seg-
Tag+F? to ?Tag? is 98.9%. The recall of men-
tion detection is dropped after feedback because 
of the great effect of rule FR2, 3, 4 and 5 as il-
lustrated in table 6. 
8 Conclusion 
In this paper, we integrate the mention detection 
model and entity tracking/coreference model 
into a unified TBL framework. Experimental 
results show segmentation adapting and feed-
back can significantly improve the performance 
of EDT system. And even with very limited 
knowledge and shallow NLP tools, our method 
can reach comparable performance with related 
work. 
References 
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: a case 
study in Part-of-Speech tagging. In: Computa-
tional Lingusitics, 21(4). 
R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A 
statistical model for multilingual entity detection 
and tracking. In Proc. of HLT/NAACL-04, pages 
1-8, Boston Massachusetts, USA. 
Jianfeng Gao, Mu Li and Changning Huang. 2003. 
Improved souce-channel model for Chinese word 
segmentation. In Proc. of ACL2003. 
Jianfeng Gao, Andi Wu, Mu Li, Changning Huang, 
Hongqiao Li, Xinsong and Xia, Haowei Qin. 2004. 
Adaptive Chinese word segmentation. In Proc. of 
ACL2004. 
Niyu Ge, John Hale, and Eugene Charniak. 1998. A 
statistical approach to anaphora resolution. In 
Proc. of the Sixth Workshop on Very Large Cor-
pora. 
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven 
J. Maiorano. 2001. Text and knowledge mining 
for coreference resolution. In Proc. of NAACL. 
J. Hobbs. 1976. Pronoun resolution. Technical report, 
Dept. of Computer Science, CUNY, Technical 
Report TR76-1. 
A Ittycheriah, L Lita, N Kambhatla, N Nicolov, S 
Roukos, and M Stys. 2003. Identifying and track-
ing entity mentions in maximum entropy frame-
work. In HLT-NAACL 2003. 
Heng Ji and Ralph Grishman. 2004. Applying 
Coreference to Improve Name Recognition. In 
ACL04 Reference Resolution and its Application Work-
shop. 
Xiaoqiang Luo, A. Ittycheriah, H. Jing, N. Kamb-
hatla, S. Roukos.2004. A Mention-Synchronous 
Coreference Resolution Aogorithm Based on the 
Bell Tree. In Proc. of ACL2004. 
R. Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. In Proc. of the 17th Interna-
tional Conference on Computational Linguistics, 
pages 869-875. 
MUC. 1996. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). Morgan Kauf-
mann, San Mateo, CA. 
NIST. 2003. The ACE evaluation plan. 
www.nist.gov/speech/tests/ace/index.htm. 
Wee Meng Soon, Hwee Tou Ng, and Chung Yong 
Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521-544. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. 
Hirschman. 1995. A Model-Theoretic coreference 
scoring scheme. In Proc. of MUC-6, page45-52. 
Morgan Kaufmann. 
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew 
Lim Tan. 2003. Coreference resolution using 
competition learning approach. In Proc. of 
ACL2003. 
237
  
CRF-based Hybrid Model for Word Segmentation, NER and even 
POS Tagging 
Zhiting Xu, Xian Qian, Yuejie Zhang,  Yaqian Zhou 
Department of Computer Science & Engineering, 
Shanghai Key Laboratory of Intelligent Information Processing, 
Fudan University, Shanghai 200433, P. R. China 
 {zhiting, qianxian, yjzhang, zhouyaqian}@fudan.edu.cn 
 
  
Abstract 
This paper presents systems submitted to 
the close track of Fourth SIGHAN Bakeoff. 
We built up three systems based on Condi-
tional Random Field for Chinese Word 
Segmentation, Named Entity Recognition 
and Part-Of-Speech Tagging respectively. 
Our systems employed basic features as 
well as a large number of linguistic features. 
For segmentation task, we adjusted the BIO 
tags according to confidence of each char-
acter. Our final system achieve a F-score of 
94.18 at CTB, 92.86 at NCC, 94.59 at SXU 
on Segmentation, 85.26 at MSRA on 
Named Entity Recognition, and 90.65 at 
PKU on Part-Of-Speech Tagging. 
1 Introduction 
Fourth SIGHAN Bakeoff includes three tasks, that 
is, Word Segmentation, Named Entity Recognition 
(NER) and Part-Of-Speech (POS) Tagging. In the 
POS Tagging task, the testing corpora are pre-
segmented. Word Segmentation, NER and POS 
Tagging could be viewed as classification prob-
lems. In a Segmentation task, each character 
should be classified into three classes, B, I, O, in-
dicating whether this character is the Beginning of 
a word, In a word or Out of a word. For NER, each 
character is assigned a tag indicating what kind of 
Named Entity (NE) this character is (Beginning of 
a Person Name (PN), In a PN, Beginning of a Lo-
cation Name (LN), In a LN, Beginning of an Or-
ganization Name (ON), In an ON or not-a-NE). In 
POS tagging task defined by Fourth SIGHAN Ba-
keoff, we only need to give a POS tag for each 
given word in a context. 
We attended the close track of CTB, NCC, SXU 
on Segmentation, MSRA on NER and PKU on 
POS Tagging. In the close track, we cannot use 
any external resource, and thus we extracted sev-
eral word lists from training corpora to form multi-
ple features beside basic features. Then we trained 
CRF models based on these feature sets. In CRF 
models, a margin of each character can be gotten, 
and the margin could be considered as the confi-
dence of that character. For the Segmentation task, 
we performed the Maximum Probability Segmen-
tation first, through which each character is as-
signed a BIO tag (B represents the Beginning of a 
word, I represents In a word and O represents Out 
of a word). If the confidence of a character is lower 
than the threshold, the tag of that character will be 
adjusted to the tag assigned by the Maximum 
Probability Segmentation (R. Zhang et al, 2006). 
2 Conditional Random Fields 
Conditional Random Fields (CRFs) are a class of 
undirected graphical models with exponent distri-
bution (Lafferty et al, 2001). A common used spe-
cial case of CRFs is linear chain, which has a dis-
tribution of: 
)),,,(exp(1)|(
1
1??
=
?? =
T
t k
ttkk
x
txyyf
Z
xyP rrr
r
?  (1) 
where ),,( 1 txyyf ttk
r
? is a function which is usu-
ally an indicator function; k?  is the learned weight 
of feature kf ; and xZ r is the normalization factor. 
The feature function actually consists of two kinds 
of features, that is, the feature of single state and 
the feature of transferring between states. Features 
will be discussed in section 3. 
167
Sixth SIGHAN Workshop on Chinese Language Processing
  
Several methods (e.g. GIS, IIS, L-BFGS) could 
be used to estimate k? , and L-BFGS has been 
showed to converge faster than GIS and IIS. To 
build up our system, we used Pocket CRF1. 
3 Feature Representation 
We used three feature sets for three tasks respec-
tively, and will describe them respectively. 
3.1 Word Segmentation 
We mainly adopted features from (H. T. Ng et al, 
2004, Y. Shi et al, 2007), as following: 
a) Cn(n=-2, -1, 0, 1, 2) 
b) CnCn+1(n=-2,-1,0,1) 
c) C-1C1 
d) CnCn+1Cn+2 (n=-1, 0, 1) 
e) Pu(C0) 
f) T(C-2)T(C-1)T(C0)T(C1)T(C2) 
g) LBegin(C0), Lend(C0) 
h) Single(C0) 
where C0 represents the current character and Cn 
represents the nst character from the current charac-
ter. Pu(C0) indicates whether current word is a 
punctuation. this feature template helps to indicate 
the end of a sentence. T(C) represents the type of 
character C. There are four types we used: (1) Chi-
nese Number (??/one?, ??/two?, ??/ten?); (2) 
Chinese Dates (??/day?, ??/month?, ??/year?); 
(3) English letters; and (4) other characters. The (f) 
feature template is used to recognize the Chinese 
dates for the construction of Chinese dates may 
cause the sparseness problem. LBegin(C0) represents 
the maximum length of the word beginning with 
the character C0, and Lend(C0) presents the maxi-
mum length of the word ending with the character 
C0. The (g) feature template is used to decide the 
boundary of a word. Single(C0) shows whether cur-
rent character can form a word solely. 
3.2 Named Entity Recognition 
Most features described in (Y. Wu et al, 2005) are 
used in our systems. Specifically, the following is 
the feature templates we used: 
a) Surname(C0): Whether current character is in 
a Surname List, which includes all first char-
acters of PNs in the training corpora. 
                                                 
1 
http://sourceforge.net/project/showfiles.php?group_id=201943 
b) PersonName(C0C1C2, C0C1): Whether C0C1C2, 
C0C1 is in the Person Name List, which con-
tains all PNs in the training corpora. 
c) PersonTitle(C-2C-1): Whether C-2C-1 is in the 
Person Title List, which is extracted from the 
previous two characters of each PN in the 
training corpora. 
d) LocationName(C0C1,C0C1C2,C0C1C2C3): 
Whether C0C1,C0C1C2,C0C1C2C3 is in the Lo-
cation Name List, which includes all LNs in 
the training corpora. 
e) LocationSuffix(C0): Whether current character 
is in the Location Suffix List, which is con-
structed using the last character of each LN in 
the training corpora. 
f) OrgSuffix(C0): Whether current character is in 
the Organization Suffix List, which contains 
the last-two-character of each ON in the train-
ing corpora. 
3.3 Part-Of-Speech Tagging 
We employed part of feature templates described 
in (H. T. Ng et al, 2004, Y. Shi et al, 2007). Since 
we are in the close track, we cannot use morpho-
logical features from external resources such as 
HowNet, and we used features that are available 
just from the training corpora. 
a) Wn, (n=-2,-1,0,1,2) 
b) WnWn+1, (n=-2,-1,0,1) 
c) W-1W1 
d) Wn-1WnWn+1 (n=-1, 1) 
e) Cn(W0) (n=0,1,2,3) 
f) Length(W0) 
where Cn represents the nth character of the current 
word, and Length(W0) indicates the length of the 
current word. 
4 Reliability Evaluation 
In the task of Word Segmentation, the label of each 
character is adjusted according to their reliability. 
For each sentence, we perform Maximum Prob-
ability Segmentation first, through which we can 
get a BIO tagging for each character in the sen-
tence. 
After that, the features are extracted according 
to the feature templates, and the weight of each 
feature has already been estimated in the step of 
training. Then marginal probability for each char-
acter can be computed as follows: 
168
Sixth SIGHAN Workshop on Chinese Language Processing
  
)),(exp(
)(
1)|( yxf
xZ
xyp ii
rr ?=     (2) 
The value of )|( xyp
r
 becomes the original re-
liability value of BIO label y for the current char-
acter under the current contexts. If the probability 
of y  with the largest probability is lower than 0.75, 
which is decided according to the experiment re-
sults, the tag given by Maximum Probability Seg-
mentation will be used instead of tag given by CRF. 
The motivation of this method is to use the Maxi-
mum Probability method to enhance the F-measure 
of In-Vocabulary (IV) Words. According to the 
results reported in (R. Zhang et al, 2006), CRF 
performs relatively better on Out-of-Vocabulary 
(OOV) words while Maximum Probability per-
forms well on IV words, so a model combining the 
advantages of these two methods is appealing. One 
simplest way to combine them is the method we 
described. Besides, there are some complex meth-
ods, such as estimation using Support Vector Ma-
chine (SVM) for CRF, CRF combining boosting 
and combining Margin Infused Relaxed Algorithm 
(MIRA) with CRF, that might perform better. 
However, we did not have enough time to imple-
ment these methods, and we will compare them 
detailedly in the future work. 
5 Experiments 
5.1 Results on Fourth SIGHAN Bakeoff 
We participated in the close track on Word Seg-
mentation on CTB, NCC and SXU corpora, NER 
on MSRA corpora and POS Tagging on PKU cor-
pora. 
For Word Segmentation and NER, our memory 
was enough to use all features. However, for POS 
tagging, we did not have enough memory to use all 
features, and we set a frequency cutoff of 10; that 
is, we could only estimate variables for those fea-
tures that occurred more than ten times. 
Our results of Segmentation are listed in the Ta-
bel 1, the results of NER are listed in the Tabel 2, 
and the results of POS Tagging are listed in the 
Tabel 3. 
 R P F Roov Riv 
CTB 0.9459 0.9418 0.9439 0.6589 0.9628 
NCC 0.9396 0.9286 0.9341 0.5007 0.9614 
SXU 0.9554 0.9459 0.9507 0.6206 0.9735 
Tabel 1. Results of Word Segmentation 
MSRA P R F 
PER 0.8084 0.8557 0.8314 
LOC 0.9138 0.8576 0.8848 
ORG 0.8666 0.773 0.8171 
Overall 0.873 0.8331 0.8526 
Tabel 2. Results of NER 
 
 Total-A IV-R OOV-R MT-R 
PKU 0.9065 0.9259 0.5836 0.8903 
Tabel 3. Results of POS Tagging 
5.2 Errors Analysis 
Observing our results of Word Segmentation and 
POS Tagging, we found that the recall of OOV is 
relatively low, this may be improved through in-
troducing features aiming to enhance the perform-
ance of OOV.  
On NER task, we noticed that precision of PN 
recognition is relative low, and we found that our 
system may classify some ONs as PNs, such as ??
??(Guinness)/ORG? and ?????(World Re-
cord)/)?. Besides, the bound of PN is sometimes 
confusing and may cause problems. For example, 
???/PER ?/ ?/ ??? may be segmented as 
????/PER ?/ ???. Further, some words be-
ginning with Chinese surname, such as ????
??, may be classified as PN.  
For List may not be the real suffix. For example, 
?????? should be a LN, but it is very likely 
that ????? is recognized as a LN for its suffix 
???.  Another problem involves the characters in 
the Location Name list may not a LN all the time. 
In the context ???/ ??/?, for example, ??? 
means Chinese rather than China.  
For ONs, the correlative dictionary also exists. 
Consider sequence ??????, which should be a 
single word, ???? is in the Organization Name 
List and thus it is recognized as an ON in our sys-
tem. Another involves the subsequence of a word. 
For example, the sequence ?????????
??, which should be a person title, but ?????
????? is an ON. Besides, our recall of ON is 
low for the length of an ON could be very long. 
6 Conclusions and Future Works 
We built up our systems based on the CRF model 
and employed multiple linguistics features based 
on the knowledge extracted from training corpora. 
169
Sixth SIGHAN Workshop on Chinese Language Processing
  
We found that these features could greatly improve 
the performance of all tasks. Besides, we adjusted 
the tag of segmentation result according to the reli-
ability of each character, which also helped to en-
hance the performance of segmentation.  
As many other NLP applications, feature plays a 
very important role in sequential labeling tasks. In 
our POS tagging task, we could only use features 
with high frequency, but some low-frequency fea-
tures may also play a vital role in the task; good 
non-redundant features could greatly improve clas-
sification performance while save memory re-
quirement of classifiers. In our further research, we 
will focus on feature selection on CRFs. 
Acknowledgement 
This research was sponsored by National Natural 
Science Foundation of China (No. 60773124, No. 
60503070). 
References 
O. Bender, F. J. Och, and H. Ney. 2003. Maximum En-
tropy Models for Named Entity Recognition. Pro-
ceeding of CoNLL-2003. 
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 
1996. A Maximum Entropy Approach to Natural 
Language Processing. Computational Linguistics, 
22(1). 
H. L. Chieu, H. T. Ng. 2002. Named Entity Recognition: 
A Maximum Entropy Approach Using Global Infor-
mation. International Conference on Computational 
Linguistics (COLING). 
J. N. Darroch and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Linear Models. The Annals of 
Mathematical Statistics, 43(5). 
J. Lafferty, A McCallum, and F. Pereira..2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of the 18th International Conf. on Machine 
Learning (ICML). 
R. Li, J. Wang, X. Chen, X. Tao, and Y. Hu. 2004. Us-
ing Maximum Entropy Model for Chinese Text 
Categorization. Computer Research and Develop-
ment, 41(4). 
H. T. Ng and J. K. Low. 2004. Chinese Part-Of-Speech 
Tagging: One-at-a-Time or All-at-Once? Word-Base 
or Character-Based? Proceedings of Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP). 
A. Ratnaparkhi. 1997. A Simple Introduction to Maxi-
mum Entropy Models for Natural Language Process-
ing. Institute for Research in Cognitive Science Re-
port, 97(8). 
F. Sha and F.Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL. 
Y. Shi and M. Wang. 2007. A Dual-Layer CRFs Based 
Joint Decoding Method for Cascaded Segmentation 
and Labeling Tasks. In International Joint Confer-
ences on Artificial Intelligence (IJCAI). 
C. A. Sutton, K. Rohanimanesh, A. McCallum. 2004. 
Dynamic conditional random fields: factorized prob-
abilistic models for labeling and segmenting se-
quence data. In International Conference on Machine 
Learning (ICML). 
M. Volk, and S. Clematide. 2001. Learn - Filter - Apply 
-- Forget Mixed Approaches to Named Entity Rec-
ognition. Proceeding of the 6th International Work-
shop on Applications of Natural Language for Infor-
mation Systems. 
Y. Wu, J. Zhao, B. Xu and H. Yu. 2005. Chinese 
Named Entity Recognition Based on Multiple Fea-
tures. Proceedings of Human Language Technology 
Conference and Conference on Empirical Methods in 
Natural Language Processing (HLT/EMNLP). 
H. Zhang, Q. Liu, H. Zhang, and X. Cheng. 2002. Au-
tomatic Recognition of Chinese Unknown Words 
Based on Roles Tagging. Proceeding of the 19th In-
ternational Conference on Computational Linguistics. 
R. Zhang, G. Kikui and E. Sumita. 2006. Subword-
based tagging by conditional random fields for Chi-
neseword segmentation. Companion volume to the-
proceedings of the North American chapter of the 
Association for Computational Linguistics (NAACL). 
Y. Zhou, Y. Guo, X. Huang, and L. Wu. 2003. Chinese 
and English BaseNP Recognition Based on a Maxi-
mum Entropy Model. Journal of Computer Research 
and Development, 40(3). 
 
170
Sixth SIGHAN Workshop on Chinese Language Processing
A Fast Algorithm for Feature Selection in Conditional
Maximum Entropy Modeling
Yaqian Zhou
Computer Science Department
Fudan University
Shanghai 200433, P.R. China
archzhou@yahoo.com
Fuliang Weng
Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
Fuliang.weng@rtc.bosch.com
Lide Wu
Computer Science Department
Fudan University
Shanghai 200433, P.R. China
ldwu@fudan.edu.cn
Hauke Schmidt
Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
hauke.schmidt@rtc.bosch.com
Abstract
This paper describes a fast algorithm that se-
lects features for conditional maximum en-
tropy modeling. Berger et al (1996) presents
an incremental feature selection (IFS) algo-
rithm, which computes the approximate gains
for all candidate features at each selection
stage, and is very time-consuming for any
problems with large feature spaces. In this
new algorithm, instead, we only compute the
approximate gains for the top-ranked features
based on the models obtained from previous
stages. Experiments on WSJ data in Penn
Treebank are conducted to show that the new
algorithm greatly speeds up the feature selec-
tion process while maintaining the same qual-
ity of selected features. One variant of this
new algorithm with look-ahead functionality
is also tested to further confirm the good
quality of the selected features. The new algo-
rithm is easy to implement, and given a fea-
ture space of size F, it only uses O(F) more
space than the original IFS algorithm.
1 Introduction
Maximum Entropy (ME) modeling has received
a lot of attention in language modeling and natural
language processing for the past few years (e.g.,
Rosenfeld, 1994; Berger et al1996; Ratnaparkhi,
1998; Koeling, 2000). One of the main advantages
using ME modeling is the ability to incorporate
various features in the same framework with a
sound mathematical foundation. There are two
main tasks in ME modeling: the feature selection
process that chooses from a feature space a subset
of good features to be included in the model; and
the parameter estimation process that estimates the
weighting factors for each selected feature in the
exponential model. This paper is primarily con-
cerned with the feature selection process in ME
modeling.
While the majority of the work in ME modeling
has been focusing on parameter estimation, less
effort has been made in feature selection. This is
partly because feature selection may not be neces-
sary for certain tasks when parameter estimate al-
gorithms are fast. However, when a feature space
is large and complex, it is clearly advantageous to
perform feature selection, which not only speeds
up the probability computation and requires
smaller memory size during its application, but
also shortens the cycle of model selection during
the training.
Feature selection is a very difficult optimization
task when the feature space under investigation is
large. This is because we essentially try to find a
best subset from a collection of all the possible
feature subsets, which has a size of 2
|
W
|
, where |W|
is the size of the feature space.
In the past, most researchers resorted to a sim-
ple count cutoff technique for selecting features
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000), where only the
features that occur in a corpus more than a pre-
defined cutoff threshold get selected. Chen and
Rosenfeld (1999) experimented on a feature selec-
tion technique that uses a c
2
 test to see whether a
feature should be included in the ME model, where
the c
2
 test is computed using the count from a prior
distribution and the count from the real training
data. It is a simple and probably effective tech-
nique for language modeling tasks. Since ME
models are optimized using their likelihood or
likelihood gains as the criterion, it is important to
establish the relationship between c
2
 test score and
the likelihood gain, which, however, is absent.
Berger et al (1996) presented an incremental fea-
ture selection (IFS) algorithm where only one fea-
ture is added at each selection and the estimated
parameter values are kept for the features selected
in the previous stages. While this greedy search
assumption is reasonable, the speed of the IFS al-
gorithm is still an issue for complex tasks. For
better understanding its performance, we re-
implemented the algorithm. Given a task of
600,000 training instances, it takes nearly four
days to select 1000 features from a feature space
with a little more than 190,000 features. Berger
and Printz (1998) proposed an f-orthogonal condi-
tion for selecting k features at the same time with-
out affecting much the quality of the selected
features. While this technique is applicable for
certain feature sets, such as word link features re-
ported in their paper, the f-orthogonal condition
usually does not hold if part-of-speech tags are
dominantly present in a feature subset. Past work,
including Ratnaparkhi (1998) and Zhou et al
(2003), has shown that the IFS algorithm utilizes
much fewer features than the count cutoff method,
while maintaining the similar precision and recall
on tasks, such as prepositional phrase attachment,
text categorization and base NP chunking. This
leads us to further explore the possible improve-
ment on the IFS algorithm.
In section 2, we briefly review the IFS algo-
rithm. Then, a fast feature selection algorithm is
described in section 3. Section 4 presents a number
of experiments, which show a massive speed-up
and quality feature selection of the new algorithm.
Finally, we conclude our discussion in section 5.
2  The Incremental Feature Selection Al-
gorithm
For better understanding of our new algorithm, we
start with briefly reviewing the IFS feature selec-
tion algorithm. Suppose the conditional ME model
takes the following form:
? 
p(y | x) =
1
Z (x )
exp( l
j
f
j
(x, y))
j
?
where f
j 
are the features, l
j 
are their corre-
sponding weights, and Z(x) is the normalization
factor.
The algorithm makes the approximation that the
addition of a feature f in an exponential model af-
fects only its associated weight a, leaving un-
changed the l-values associated with the other
features. Here we only present a sketch of the algo-
rithm in Figure 1. Please refer to the original paper
for the details.
In the algorithm, we use I for the number of
training instances, Y for the number of output
classes, and F for the number of candidate features
or the size of the candidate feature set.
0. Initialize: S = ?, sum[1..I, 1..Y] = 1,
z[1..I] = Y
1. Gain computation:
MaxGain = 0
for f in feature space F do
)(maxarg
? aa a fSG ?=
)(max
? aa fSGg ?=
if MaxGain < 
g
?  then
   MaxGain = 
g
?
   f
*
 = f
  a*=a?
2. Feature selection:
S = S ? { f
*
 }
3. if termination condition is met, then stop
4. Model adjustment:
for instance i such that there is y
and f
*
(x
i
, y) = 1 do
z[i] -=sum[i, y]
sum[i, y] ?= exp(a*)
z[i] += sum[i, y]
5. go to  step 1.
Figure 1: A Variant of the IFS Algorithm.
One difference here from the original IFS algo-
rithm is that we adopt a technique in (Goodman,
2002) for optimizing the parameters in the condi-
tional ME training. Specifically, we use array z to
store the normalizing factors, and array sum for all
the un-normalized conditional probabilities sum[i,
y]. Thus, one only needs to modify those sum[i, y]
that satisfy f
*
(x
i
, y)=1, and to make changes to their
corresponding normalizing factors z[i]. In contrast
to what is shown in Berger et al1996?s paper, here
is how the different values in this variant of the IFS
algorithm are computed.
Let us denote
?
=
j
jj
yxfxysum )),(exp()|( l
?
=
y
xysumxZ )|()(
Then, the model can be represented by sum(y|x)
and Z(x) as follows:
)(/)|()|( xZxysumxyp =
where sum(y|x
i
) and Z(x
i
) correspond to sum[i,y]
and z[i] in Figure 1, respectively.
Assume the selected feature set is S, and f is
currently being considered. The goal of each se-
lection stage is to select the feature f that maxi-
mizes the gain of the log likelihood, where the a
and gain of f are derived through following steps:
Let the log likelihood of the model be
?
?
-=
-?
yx
yx
xZxysumyxp
xypyxppL
,
,
))(/)|(log(),(
~
))|(log(),(
~
)(
and the empirical expectation of feature f be
? 
E
? 
p
( f ) =
? 
p (x,y) f (x, y)
x,y
?
With the approximation assumption in Berger
et al(1996)?s paper, the un-normalized component
and the normalization factor of the model have the
following recursive forms:
  )|()|(
aa
exysumxysum
SfS
?=
?
)|(                           
)|()()(
xysum
xysumxZxZ
fS
SSfS aa
?
?
+
-=
The approximate gain of the log likelihood is
computed by
 
? 
G
S? f
(a) ? L(p
S? f
a
) - L(p
S
)
             = -
? 
p (x)(logZ
S? f ,a (x)
x
?
/Z
S
(x))
                      + aE
? 
p
( f )                    (1)
The maximum approximate gain and its corre-
sponding a are represented as:
  )(max),(~ aa fSGfSL ?=D
  )(maxarg),(~ aa a fSGfS ?=
3 A Fast Feature Selection Algorithm
The inefficiency of the IFS algorithm is due to the
following reasons. The algorithm considers all the
candidate features before selecting one from them,
and it has to re-compute the gains for every feature
at each selection stage. In addition, to compute a
parameter using Newton?s method is not always
efficient. Therefore, the total computation for the
whole selection processing can be very expensive.
Let g(j, k) represent the gain due to the addition
of feature f
j
 to the active model at stage k. In our
experiments, it is found even if D (i.e., the addi-
tional number of stages after stage k) is large, for
most j, g(j, k+D) - g(j, k) is a negative number or at
most a very small positive number. This leads us to
use the g(j, k) to approximate the upper bound of
g(j, k+D).
The intuition behind our new algorithm is that
when a new feature is added to a model, the gains
for the other features before the addition and after
the addition do not change much. When there are
changes, their actual amounts will mostly be within
a narrow range across different features from top
ranked ones to the bottom ranked ones. Therefore,
we only compute and compare the gains for the
features from the top-ranked downward until we
reach the one with the gain, based on the new
model, that is bigger than the gains of the remain-
ing features. With a few exceptions, the gains of
the majority of the remaining features were com-
puted based on the previous models.
As in the IFS algorithm, we assume that the ad-
dition of a feature f only affects its weighting fac-
tor a. Because a uniform distribution is assumed as
the prior in the initial stage, we may derive a
closed-form formula for a(j, 0) and g(j, 0) as fol-
lows.
Let
? 
Ed ( f ) = ? p (x)max
y
{ f (x, y)}
x
?
? 
R
e
( f ) = E
? 
p
( f ) / Ed ( f )
Yp /1
0
=
Then
 )log()0,(
)(1
1)(
0
0
fR
p
p
fR
e
e
j
-
-
?=a
? 
g( j,0) = L(p
?? f
a( i,0)
) - L(p
?
)
          = Ed ( f )[Re ( f )log Re ( f )
p
0
                     + (1- R
e
( f ))log
1-R
e
( f )
1- p
0
] 
where ? denotes an empty set, p? is the uni-
form distribution. The other steps for computing
the gains and selecting the features are given in
Figure 2 as a pseudo code. Because we only com-
pute gains for a small number of top-ranked fea-
tures, we call this feature selection algorithm as
Selective Gain Computation (SGC) Algorithm.
In the algorithm, we use array g to keep the
sorted gains and their corresponding feature indi-
ces. In practice, we use a binary search tree to
maintain the order of the array.
The key difference between the IFS algorithm
and the SGC algorithm is that we do not evaluate
all the features for the active model at every stage
(one stage corresponds to the selection of a single
feature). Initially, the feature candidates are or-
dered based on their gains computed on the uni-
form distribution. The feature with the largest gain
gets selected, and it forms the model for the next
stage. In the next stage, the gain of the top feature
in the ordered list is computed based on the model
just formed in the previous stage. This gain is
compared with the gains of the rest features in the
list. If this newly computed gain is still the largest,
this feature is added to form the model at stage 3.
If the gain is not the largest, it is inserted in the
ordered list so that the order is maintained. In this
case, the gain of the next top-ranked feature in the
ordered list is re-computed using the model at the
current stage, i.e., stage 2.
This process continues until the gain of the top-
ranked feature computed under the current model
is still the largest gain in the ordered list. Then, the
model for the next stage is created with the addi-
tion of this newly selected feature. The whole fea-
ture selection process stops either when the
number of the selected features reaches a pre-
defined value in the input, or when the gains be-
come too small to be useful to the model.
0. Initialize: S = ?, sum[1..I, 1..Y] = 1,
z[1..I] = Y, g[1..F] = {g(1,0),?,g(F,0)}
1. Gain computation:
MaxGain = 0
Loop
    
]},...,1[{maxarg
in  
Fgf
Ff
j
=
    if g[j] ? MaxGain then go to step 2
    else
 
)(maxarg
? aa a fSG ?=
 
)(max
? aa fSGg ?=
       g[j]= 
g
?
       if MaxGain < 
g
?  then
          MaxGain = 
g
?
          f
*
 = f
j
         a*=a?
2. Feature selection:
S = S ? { f
*
 }
3. if termination condition is met, then stop
4. Model adjustment:
for instance i such that there is y
and f
*
(x
i
, y) = 1 do
z[i] -=sum[i, y]
sum[i, y] ?= exp(a*)
z[i] += sum[i, y]
5. go to  step 1.
Figure 2: Selective Gain Computation Algo-
rithm for Feature Selection
In addition to this basic version of the SGC al-
gorithm, at each stage, we may also re-compute
additional gains based on the current model for a
pre-defined number of features listed right after
feature f
*
 (obtained in step 2) in the ordered list.
This is to make sure that the selected feature f
*
 is
indeed the feature with the highest gain within the
pre-defined look-ahead distance. We call this vari-
ant the look-ahead version of the SGC algorithm.
4 Experiments
A number of experiments have been conducted to
verify the rationale behind the algorithm. In par-
ticular, we would like to have a good understand-
ing of the quality of the selected features using the
SGC algorithm, as well as the amount of speed-
ups, in comparison with the IFS algorithm.
The first sets of experiments use a dataset {(x,
y)}, derived from the Penn Treebank, where x is a
10 dimension vector including word, POS tag and
grammatical relation tag information from two ad-
jacent regions, and y is the grammatical relation
tag between the two regions. Examples of the
grammatical relation tags are subject and object
with either the right region or the left region as the
head. The total number of different grammatical
tags, i.e., the size of the output space, is 86. There
are a little more than 600,000 training instances
generated from section 02-22 of WSJ in Penn
Treebank, and the test corpus is generated from
section 23.
In our experiments, the feature space is parti-
tioned into sub-spaces, called feature templates,
where only certain dimensions are included. Con-
sidering all the possible combinations in the 10-
dimensional space would lead to 2
10
 feature tem-
plates. To perform a feasible and fair comparison,
we use linguistic knowledge to filter out implausi-
ble subspaces so that only 24 feature templates are
actually used. With this amount of feature tem-
plates, we get more than 1,900,000 candidate fea-
tures from the training data. To speed up the
experiments, which is necessary for the IFS algo-
rithm, we use a cutoff of 5 to reduce the feature
space down to 191,098 features. On average, each
candidate feature covers about 485 instances,
which accounts for 0.083% over the whole training
instance set and is computed through:
???
=
jj yx
j
yxfac 1/),(
,
The first experiment is to compare the speed of
the IFS algorithm with that of SGC algorithm.
Theoretically speaking, the IFS algorithm com-
putes the gains for all the features at every stage.
This means that it requires O(NF) time to select a
feature subset of size N from a candidate feature
set of size F. On the other hand, the SGC algorithm
considers much fewer features, only 24.1 features
on average at each stage, when selecting a feature
from the large feature space in this experiment.
Figure 3 shows the average number of features
computed at the selected points for the SGC algo-
rithm, SGC with 500 look-ahead, as well as the
IFS algorithm. The averaged number of features is
taken over an interval from the initial stage to the
current feature selection point, which is to smooth
out the fluctuation of the numbers of features each
selection stage considers. The second algorithm
looks at an additional fixed number of features,
500 in this experiment, beyond the ones considered
by the basic SGC algorithm. The last algorithm has
a linear decreasing number of features to select,
because the selected features will not be consid-
ered again. In Figure 3, the IFS algorithm stops
after 1000 features are selected. This is because it
takes too long for this algorithm to complete the
entire selection process. The same thing happens in
Figure 4, which is to be explained below.
0
1
2
3
4
5
6
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
A
v
e
r
a
g
e
 
C
o
n
s
i
d
e
r
e
d
 
F
e
a
t
u
r
e
 
N
u
m
b
e
r
Berger SGC-0 SGC-500
log
10
(Y)
Figure 3: The log number of features considered in
SGC algorithm, in comparison with the IFS algo-
rithm.
To see the actual amount of time taken by the
SGC algorithms and the IFS algorithm with the
currently available computing power, we use a
Linux workstation with 1.6Ghz dual Xeon CPUs
and 1 GB memory to run the two experiments si-
multaneously. As it can be expected, excluding the
beginning common part of the code from the two
algorithms, the speedup from using the SGC algo-
rithm is many orders of magnitude, from more than
100 times to thousands, depending on the number
of features selected. The results are shown in Fig-
ure 4.
-2
-1
0
1
2
3
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
A
v
e
r
a
g
e
 
T
i
m
e
f
o
r
 
e
a
c
h
 
s
e
l
e
c
t
i
o
n
 
s
t
e
p
(
s
e
c
o
n
d
)
Berger SGC-0
log
10
(Y)
Figure 4: The log time used by SGC algorithm, in
comparison with the IFS algorithm.
To verify the quality of the selected features
using our SGC algorithm, we conduct four experi-
ments: one uses all the features to build a condi-
tional ME model, the second uses the IFS
algorithm to select 1,000 features, the third uses
our SGC algorithm, the fourth uses the SGC algo-
rithm with 500 look-ahead, and the fifth takes the
top n most frequent features in the training data.
The precisions are computed on section 23 of the
WSJ data set in Penn Treebank. The results are
listed in Figure 5. Three factors can be learned
from this figure. First, the three IFS and SGC algo-
rithms perform similarly. Second, 3000 seems to
be a dividing line: when the models include fewer
than 3000 selected features, the IFS and SGC algo-
rithms do not perform as well as the model with all
the features; when the models include more than
3000 selected features, their performance signifi-
cantly surpass the model with all the features. The
inferior performance of the model with all the fea-
tures at the right side of the chart is likely due to
the data over-fitting problem. Third, the simple
count cutoff algorithm significantly under-
performs the other feature selection algorithms
when feature subsets with no more than 10,000
features are considered.
To further confirm the findings regarding preci-
sion, we conducted another experiment with Base
NP recognition as the task. The experiment uses
section 15-18 of WSJ as the training data, and sec-
tion 20 as the test data. When we select 1,160 fea-
tures from a simple feature space using our SGC
algorithm, we obtain a precision/recall of
92.75%/93.25%. The best reported ME work on
this task includes Koeling (2000) that has the pre-
cision/recall of 92.84%/93.18% with a cutoff of 5,
and Zhou et al (2003) has reached the perform-
ance of 93.04%/93.31% with cutoff of 7 and
reached a performance of 92.46%/92.74% with
615 features using the IFS algorithm. While the
results are not directly comparable due to different
feature spaces used in the above experiments, our
result is competitive to these best numbers. This
shows that our new algorithm is both very effective
in selecting high quality features and very efficient
in performing the task.
5 Comparison and Conclusion
Feature selection has been an important topic in
both ME modeling and linear regression. In the
past, most researchers resorted to count cutoff
technique in selecting features for ME modeling
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000). A more refined
algorithm, the incremental feature selection algo-
rithm by Berger et al(1996), allows one feature
being added at each selection and at the same time
keeps estimated parameter values for the features
selected in the previous stages. As discussed in
(Ratnaparkhi, 1998), the count cutoff technique
works very fast and is easy to implement, but has
the drawback of containing a large number of re-
70
72
74
76
78
80
82
84
86
88
90
92
94
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
P
r
e
c
i
s
i
o
n
 
(
%
)
All (191098) IFS SGC-0
SGC-500 Count Cutoff
Figure 5: Precision results from models using the whole
feature set and the feature subsets through the IFS algo-
rithm, the SGC algorithm, the SGC algorithm with 500
look-ahead, and the count cutoff algorithm.
dundant features. In contrast, the IFS removes the
redundancy in the selected feature set, but the
speed of the algorithm has been a big issue for
complex tasks. Having realized the drawback of
the IFS algorithm, Berger and Printz (1998) pro-
posed an f-orthogonal condition for selecting k
features at the same time without affecting much
the quality of the selected features. While this
technique is applicable for certain feature sets,
such as link features between words, the f -
orthogonal condition usually does not hold if part-
of-speech tags are dominantly present in a feature
subset.
Chen and Rosenfeld (1999) experimented on a
feature selection technique that uses a c
2
 test to see
whether a feature should be included in the ME
model, where the c
2
 test is computed using the
counts from a prior distribution and the counts
from the real training data. It is a simple and
probably effective technique for language model-
ing tasks. Since ME models are optimized using
their likelihood or likelihood gains as the criterion,
it is important to establish the relationship between
c
2
 test score and the likelihood gain, which, how-
ever, is absent.
There is a large amount of literature on feature
selection in linear regression, where least mean
squared errors measure has been the primary opti-
mization criterion. Two issues need to be ad-
dressed in order to effectively use these techniques.
One is the scalability issue since most statistical
literature on feature selection only concerns with
dozens or hundreds of features, while our tasks
usually deal with feature sets with a million of
features. The other is the relationship between
mean squared errors and likelihood, similar to the
concern expressed in the previous paragraph.
These are important issues and require further in-
vestigation.
In summary, this paper presents our new im-
provement to the incremental feature selection al-
gorithm. The new algorithm runs hundreds to
thousands times faster than the original incre-
mental feature selection algorithm. In addition, the
new algorithm selects the features of a similar
quality as the original Berger et alalgorithm,
which has also shown to be better than the simple
cutoff method in some cases.
Acknowledgement
This work is done while the first author is visiting
the Center for Study of Language and Information
(CSLI) at Stanford University and the Research
and Technology Center of Robert Bosch Corpora-
tion. This project is sponsored by the Research and
Technology Center of Robert Bosch Corporation.
We are grateful to the kind support from Prof.
Stanley Peters of CSLI. We also thank the com-
ments from the three anonymous reviewers which
improve the quality of the paper.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistic, 22 (1): 39-71.
Adam L. Berger and Harry Printz. 1998. A Comparison
of Criteria for Maximum Entropy / Minimum Diver-
gence Feature Selection. Proceedings of the 3
rd
 con-
ference on Empirical Methods in Natural Language
Processing. Granda, Spain.
Stanley Chen and Ronald Rosenfeld. 1999. Efficient
Sampling and Feature Selection in Whole Sentence
maximum Entropy Language Models. Proceedings of
ICASSP-1999, Phoenix, Arizona.
Joshua Goodman. 2002. Sequential Conditional Gener-
alized Iterative Scaling. Association for Computa-
tional Linguistics, Philadelphia, Pennsylvania.
Rob Koeling. 2000. Chunking with Maximum Entropy
Models. In: Proceedings of CoNLL-2000 and LLL-
2000, Lisbon, Portugal, 139-141.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ronald Rosenfeld. 1994. Adaptive Statistical Language
Modeling: A Maximum Entropy Approach. Ph.D.
thesis, Carnegie Mellon University, April.
J. Reynar and A. Ratnaparkhi. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries.
In: Proceedings of the Fifth Conference on Applied
Natural Language Processing, Washington D.C., 16-
19.
Zhou Ya-qian, Guo Yi-kun, Huang Xuan-jing, and Wu
Li-de. 2003. Chinese and English BaseNP Recog-
nized by Maximum Entropy. Journal of Computer
Research and Development. 40(3):440-446, Beijin
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 688?697, Dublin, Ireland, August 23-29 2014.
A Generative Model for Identifying Target Companies of Microblogs
Yeyun Gong, Yaqian Zhou, Ya Guo, Qi Zhang, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{12110240006, zhouyaqian, 13210240002, qz, xjhuang}@fudan.edu.cn
Abstract
Microblogging services have attracted hundreds of millions of users to publish their status, ideas
and thoughts, everyday. These microblog posts have also become one of the most attractive and
valuable resources for applications in different areas. The task of identifying the main targets of
microblogs is an important and essential step for these applications. In this paper, to achieve this
task, we propose a novel method which converts the target company identification problem to
the translation process from content to targets. We introduce a topic-specific generative method
to model the translation process. Topic specific trigger words are used to bridge the vocabulary
gap between the words in microblogs and targets. We examine the effectiveness of our approach
via datasets gathered from real world microblogs. Experimental results demonstrate a 20.2%
improvement in terms of F1-score over the state-of-the-art discriminative method.
1 Introduction
With the rapid growth of social media, about 72% of adult internet users are also members of
a social networking site
1
. Over the past few years, microblogging has become one of the most
popular services. Meanwhile, microblogs have also been widely used as sources for analyzing public
opinions (Bermingham and Smeaton, 2010; Jiang et al., 2011), prediction (Asur and Huberman, 2010;
Bollen et al., 2011), reputation management (Pang and Lee, 2008; Otsuka et al., 2012), and many other
applications (Bian et al., 2008; Sakaki et al., 2010; Becker et al., 2010; Guy et al., 2010; Lee and Croft,
2013; Guy et al., 2013). For most of these applications, identifying the microblogs that are relevant to
the targets of interest is one of the basic steps (Lin and He, 2009; Amig?o et al., 2010; Qiu et al., 2011;
Liu et al., 2013). Let us firstly consider the following example:
Example 1: 11? MacBook Air can run for up to five hours on a single charge.
?MacBook Air? can be considered to be the target being discussed on the microblog, and we can also
infer from the microblog that it is related to Apple Inc. The ability to discriminate which company is
being referred to in a microblog is required by many applications.
Previous studies on fine-grained sentiment analysis and aspect-based opinion mining proposed
supervised (Popescu and Etzioni, 2005; Liu et al., 2012a; Liu et al., 2013) and unsupervised methods (Hu
and Liu, 2004; Wu et al., 2009; Zhang et al., 2010) to extract targets of opinion expressions. Based on
the associations between opinion targets and opinion words, some methods were also introduced to
simultaneously solve the opinion expression and target extraction problems (Qiu et al., 2011; Liu et al.,
2012a). However, most of the existing methods in this area only focus on extracting items about which
opinions are expressed in a given domain. The implicated information of targets is rarely considered.
Moreover, domain adaptation is another big challenge for these fine-grained methods in processing
different domains.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
It is reported by the Pew Research Center?s Internet & American Life Project in Aug 5, 2013.
688
The WePS-3
2
(Amig?o et al., 2010) and RepLab 2013
3
(Amig?o et al., 2013) evaluation campaigns also
addressed the problem from the perspective of the disambiguation of company names in microblogs.
Microblogs that contain company names at a lexical level are classified based on whether it refers
to the company or not. Various approaches have been proposed to address the task with different
methods (Pedersen et al., 2006; Yerva et al., 2010; Zhang et al., 2012; Spina et al., 2012; Spina et
al., 2013). However, the microblogs that do not contain company names cannot be correctly processed
using these methods. From analyzing the data, we observe that a variety of microblog posts belong to
this type. They only contain products names, slang terms, and other related company content.
To achieve this task, in this paper, we propose the use of a translation based model to identify the targets
of microblogs. We assume that the microblog posts and targets describe the same topic using different
languages. Hence, the target identification problem can be regarded as a translation process from the
content of the microblogs to the targets. We integrate latent topical information into the translation
model to facilitate the translation process. Because product names, series, and other related information
are important indicators for this task, we also incorporate this background knowledge into the model. To
evaluate the proposed method, we collect a large number of microblogs and manually annotate a subset
of these as golden standards. We compare the proposed method with state-of-the-art methods using the
constructed dataset. Experimental results demonstrate that the proposed approach can achieve better
performance than the other approaches.
2 The Proposed Method
2.1 The Generation Process
Given a corpus D = {d
i
, 1 ? i ? |D|}, which contains a list of microblogs {d
i
}. A microblog is a
sequence of N
d
words denoted by w
d
= {w
d1
, w
d2
, ..., w
dN
d
}. Each microblog contains a set of targets
denoted by c
d
= {c
d1
, c
d2
, ..., c
dM
d
}. A word is defined as an item from a vocabulary with V distinct
words indexed by w = {w
1
, w
2
, ..., w
V
}. The nth word in the dth microblog is associated with not only
one topic z
dn
, but also an indicator variable l
dn
which indicates whether w
dn
belongs to the ontology
(l
dn
= 1), which contains company names, product names, series, and other related information, or is a
common word (l
dn
= 0). Each target is from the vocabulary with C distinct company names indexed by
c = {c
1
, c
2
, ..., c
C
}. The mth target in the dth microblog is associated with a topic z
dm
. The notations
used in this paper are summarized in Table 1. Fig. 1 shows the graphical representation of the generation
process. The generative story for each microblog is as follows:
1. Sample word distribution ?
t,l
fromDir(?
l
) for each topic t = 1, 2, ..., T and each label l = 1, ..., L.
2. For each microblog d=1,2,...,|D|
a. Sample topic distribution ?
d
from Dir(?)
b. For each word n = 1, 2, ..., N
d
i. Sample a topic z
dn
= t from Multinomial(?
d
)
ii. Sample a label l
dn
= l from the distribution over labels, v
d,n
iii. Sample a wordw according to multinomial distribution P (w
dn
= w|z
dn
= t, l
dn
= l, ?
t,l
)
c. For each target m = 1, 2, ...,M
d
i. Sample a topic z
dm
= t from Multinomial(?
d
)
ii. Sample a target c
dm
= c according to probability P (c
dm
= c|w
d
, l
d
, z
dm
= t, B)
As described above, we use l
dn
to incorporate the ontology information into the model. In this work,
we construct an ontology which contains 4,926 company names, 7,632 abbreviations, and 26,732 product
names. These companies names are collected based on the top search queries in different categories
4
.
We propose to use the distribution v
d,n
to indicate the probability of variable l
dn
. We set v
d,n
by applying
2
http://nlp.uned.es/weps/weps-3
3
http://www.limosine-project.eu/events/replab2013
4
http://top.baidu.com/boards
689
wdn
z
dn
?
d
?
c
dm
z
dm
B
?
t,l
?
l
l
dn
v
d,n
f
dn
?
M
d
N
d
|D|
V
T
L
Figure 1: The graphical representation of the proposed model. Shaded circles are observations or
constants. Unshaded ones are hidden variables.
various sources of ontology (presented by ?) and the context features of the word w
dn
(presented by f
dn
).
In this work, we only consider the word itself as its context feature. This information is encoded into
the hyperparameters {?
w
|w ? {w
1
, w
2
, ..., w
V
}}, where ?
w
is hyperparameter for the word w, and
?
w
0
+ ?
w
1
= 1. For each word w in the ontology, we set ?
w
1
to a value 0.9, ?
w
0
to a value 0.1. For each
word w not contained by ontology, we set ?
w
1
to a value 0 and ?
w
0
to a value 1. Based on the ontology,
v
d,n
could be set as follows:
P (l
dn
= l|w
dn
= w) = v
d,n
l
=
?
w
l
?
w
1
+ ?
w
0
, l ? {0, 1}
(1)
2.2 Model Inference
We use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) to obtain samples of hidden variable
assignment and to estimate the model parameters from these samples.
On the microblog content side, the conditional probability of a latent topic and label for the nth word
in the dth microblog is:
Pr(z
dn
= t, l
dn
= l|w
dn
= w,w
?n
, z
?n
, l
?n
) ?
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
?
N
t,?n
d
+ ?
N
?n
d
+ T?
,
(2)
where N
w,?n
t,l
is the number of the word w that are assigned to topic t under the label l; N
?n
t,l
is the
number of all the words that are assigned to topic t under the label l; N
t,?n
d
is the number of topic t in
the microblog d; N
?n
d
is the number of all the topics in the document d; ?n indicates taking no account
of the current position n.
Given the conditional probability of z
dn
= t, l
dn
= l, we formalize the marginal probability of z
dn
= t
as follows:
Pr(z
dn
= t|w
dn
= w,w
?n
, z
?n
, l
?n
) ?
L?1
?
l=0
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
?
N
t,?n
d
+ ?
N
?n
d
+ T?
(3)
690
Table 1: The notation used in the proposed model.
|D| The number of microblogs in the data set
V The number of unique words in the vocabulary
C The number of companies
T The number of topics
L The number of labels
N
d
The number of words in the dth microblog
M
d
The number of companies in the dth microblog
w
d
All the words in the dth microblog
c
d
All the targets in the dth microblog
z
d
The topic of the words in the dth microblog
l
d
The label of the words in the dth microblog
B The topic-specific word alignment table between a word and a target
?
t,l
Distribution of words for each topic t and each label l
?
d
Distribution of topics in microblog d
v
d,n
Distribution of labels for word w
dn
N
w,?n
t,l
The number of the word w that is assigned to topic t under the label l except the position n
N
?n
t,l
The number of all the words that are assigned to topic t under the label l. except the position n
N
t,?n
d
The number of topic t in the microblog d except the position n
N
?n
d
The number of all the topics in the microblog d except the position n
N
c,w
t,l
The number of the target c that co-occurs with the word w labeled as l under topic t
After re-assigning the topic z
dn
= t for the current word, the conditional probability of ontology label
for the nth word in the dth microblog is:
Pr(l
dn
= l|w
dn
= w, z
dn
= t,w
?n
, z
?n
, l
?n
) ?
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
(4)
On the target side, we perform topic assignments for each target as follows:
Pr(z
dm
= t|c
dm
= c, c
?m
,w, l, z
?m
) ?
N
d
?
n=1
?
l
dn
N
c,w
dn
,?m
t,l
dn
N
w
dn
t,l
dn
+ ?C
?
N
t,?m
d
+ ?
N
?m
d
+ T?
,
(5)
where ?
l
dn
is the weight for the label (?
1
> 1, ?
0
= 1); N
c,w
dn
,?m
t,l
dn
is the number of the company c that
co-occurs with the word w
dn
labeled as l
dn
under topic t; ?C is a smoothing part; N
w
dn
t,l
dn
is the number of
the word w
dn
labeled as l
dn
under topic t; N
t,?m
d
is the number of occurrences of topic t in the document
d; N
?m
d
is the number of occurrences of all the topics in the document d; ?m indicates taking no account
of the current position m.
Based on the above equations, after enough sampling iterations, we can estimate word alignment table
B, B
c,w,t,l
= ?
l
N
c,w
t,l
N
w
t,l
+?C
. Some companies just occur few times, and most of the words co-occur with
them also alignment with other companies, for this case, we use ?C to smooth, where C represent the
number of company c. And also we can estimate topic distribution ? for each document, and word
distribution ? for each topic and each label, as follows:
?
t
d
=
N
t
d
+ ?
N
d
+ T?
, ?
t,l
w
=
N
w
t,l
+ ?
l
N
t,l
+ V ?
l
The possibility table B
c,w,t,l
has a potential size of V ?C ?T ?L. The data sparsity may pose a problem
in estimating B
c,w,t,l
. To reduce the data sparsity problem, we introduce the remedy in our model. We
691
employ a linear interpolation with topic-free word alignment probability to avoid data sparsity problem:
B
?
c,w,t,l
= ?B
c,w,t,l
+ (1? ?)P (c|w),
(6)
where P (c|w) is topic-free word alignment probability between the word w and the company c. ? is
trade-off of two probabilities ranging from 0.0 to 1.0.
2.3 Target Company Extraction
Just like standard LDA, the proposed method itself finds a set of topics but does not directly extract
targets. Suppose we have a dataset which contains microblogs without targets, we can use the collapsed
Gibbs sampling to estimate the topic and label for the words in each microblog. The process is the same
as described in Section 3.2.
After the hidden topics and label of the words in each microblog become stable, we can estimate the
distribution of topics for the dth microblog by: P (t|w
d
) = ?
t
d
=
N
t
d
+?
N
d
+T?
. With the word alignment table
B
?
, we can rank companies for the dth microblog in unlabeled data by computing the scores:
Pr(c
dm
|w
d
) ?
T
?
t=1
N
d
?
n=1
P (c
dm
|t, w
dn
, l
dn
, B
?
) ? P (t|w
d
)P (w
dn
|w
d
),
(7)
where P (w
dn
|w
d
) is the weight of the word w
dn
in the microblog content w
d
. In this paper, we use
inverse document frequency (IDF) score to estimate it. Based on the ranking scores calculated by Eq.(7),
we can extract the top-ranked targets for each microblog to users.
3 Experiments
In this section, we will introduce the experimental results and datasets we constructed for training and
evaluation. We will firstly describe the how we construct the datasets and their statistics. Then we
will introduce the experiment configurations and baseline methods. Finally, the evaluation results and
analysis will be given.
3.1 Datasets
We started by using Sina Weibo?s API
5
to collect public microblogs from randomly selected users. The
dataset contains 282.2M microblogs published by 1.1M users. We use RAW-Weibo to represent it in the
following sections. Based on the collected raw microblogs, we constructed three datasets for evaluation
and training.
3.1.1 Training data
Since social media users post thoughts, ideas, or status on various topics in social medias, there are a
huge number of related companies. Manually constructing training data is a time consuming and cost
process. In this work, we propose a weakly manual method based on ontology and hashtag. A hashtag is
a string of characters preceded by the symbol #. In most cases, hashtags can be viewed as an indication
to the context of the tweet or as the core idea expressed in the tweet. Hence, we can use hashtag as the
targets.
We extract the microblogs whose hashtags contain ontology items as training data and the
corresponding ontology items as targets. Obviously, the training data constructed based on this method
is not perfect. However, since this method can effectively generate a great quantity of data, we think
that general characteristics can be modeled with the generated training data. To evaluate the corpus,
we randomly selected 100 microblogs from the training data and manually labeled their targets. The
accuracy of the sampled dataset is 91%. It indicates that the proposed training data generation method
is effective. From the RAW-Weibo dataset, we extracted a total of 1.79M microblogs whose hashtags
contain more than one target. Training instances for 2,574 target companies are included in the training
data.
5
http://open.weibo.com/
692
3.1.2 Test data
For evaluation, we manually constructed a dataset RAN-Weibo, which contains 2,000 microblogs selected
from RAW-Weibo. Three annotators were asked to label the target companies for each microblog. To
evaluate the quality of annotated dataset, we validate the agreements of human annotations using Cohen?s
kappa coefficient. The average ? among all annotators is 0.626. It indicates that the annotations are
reliable.
Since some targets are ambiguous, inspired by the evaluation campaigns WePS-3 and RepLab 2013,
we also constructed a dataset AMB-Weibo, where microblogs include 10 popular company names which
may cause ambiguity. For each target, we randomly selected and annotated 200 microblogs as golden
standards. Three annotators were also asked to label whether the microblog is related the given target or
not. The agreements of human annotations were also validated through Cohen?s kappa coefficient. The
average ? among all annotators is 0.692.
3.2 Experiment Configurations
We use precision (P ), recall (R), and F1-score (F
1
) to evaluate the performance. We ran our model
with 500 iterations of Gibbs sampling. We use 5-fold cross-validation in the training data to optimize
hyperparameters. The number of topics is set to 30. The other settings of hyperparameters are as follows:
? = 50/T , ? = 0.1, ? = 20, ? = 0.5. The smoothing parameter ? is set to 0.8.
For baselines, we compare the proposed model with the following baseline methods.
? Naive Bayes (NB): The target identification task can be easily formalized as a classification task,
where each target is considered as a classification label. Hence, we applied Naive Bayes to model
the posterior probability of each target given a microblog.
? Support Vector Machine (SVM): The content of microblogs are represented as vectors and SVM
is used to model the classification problem.
? IBM1: Translation model (IBM model-1) is applied to obtain the alignment probability between
words and targets.
? TTM: Topical translation model (TTM) was proposed by Ding et al. (2013) to achieve microblog
hashtag suggestion task. We adopted it to estimate the alignment probability between words and
targets.
3.3 Experimental Results
We evaluate the proposed method from the following perspectives: 1) comparing the proposed method
with the state-of-the-art methods on the two evaluation datasets; 2) identifying the impacts of parameters.
Table 2 shows the comparisons of the proposed method with the state-of-the-arts discriminative
and generative methods on the evaluation dataset RAN-Weibo. ?Our? denotes the method proposed
in previous sections. ?Our w/o BG? represents the proposed method without background knowledge.
From the results, we can observe that the proposed method is better than other methods. Discriminative
methods achieve worse results than generative methods. We think that the large number of targets is
one of the main reasons of the low performances. The results of the proposed models with and without
ontology information also show that background knowledge can benefit both the precision and recall.
TTM achieves better performance than IBM1. It indicates that topical information is useful for this
task. The performances of our method are significantly better than TTM. It illustrates that our smoothing
method and incorporation of background knowledge are effective.
From the description of the proposed model, we can know that there are several hyperparameters in
the proposed model. To evaluate the impacts of them, we evaluate two crucial ones among all of them,
the number of topics T and the smoothing factor ?. Table 3 shows the influence of the number of topics.
From the table, we can observe that the proposed model obtains the best performance when T is set to
30. And performance decreases with more number of topics. We think that data sparsity may be one of
the main reasons. With much more topic number, the data sparsity problem will be more serious when
693
Table 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the evaluation dataset RAN-
Weibo.
Methods Precision Recall F
1
NB 0.168 0.154 0.161
SVM 0.312 0,286 0.298
IBM1 0.236 0.214 0.220
TTM 0.356 0.327 0.341
Our w/o BG 0.488 0.448 0.467
Our 0.522 0.479 0.500
Table 3: The influence of the number of topics T of the proposed method.
T Precision Recall F
1
10 0.516 0.473 0.493
30 0.522 0.479 0.500
50 0.508 0.466 0.486
70 0.489 0.449 0.468
100 0.488 0.448 0.467
estimating topic-specific translation probability. Table 4 shows the influence of the translation probability
smoothing parameter ?. When ? is set to 0.0, it means that the topical information is omitted. Comparing
the results of ? = 0.0 and other values, we can observe that the topical information can benefit this task.
When ? is set to 1.0, it represents the method without smoothing. The results indicate that it is necessary
to address the sparsity problem through smoothing.
Figure 2 shows the results of different methods on the dataset AMB-Weibo. All the models are trained
with same dataset as the above experiments. From the results, we can observe that the F1-scores vary
from less than 0.40 up to almost 0.60. The performances? variations of other methods are also huge. We
think that training data size and difficulty level are two main reasons. The size of training data of different
targets vary greatly in the dataset. However, comparing with other method, the proposed method is the
most stable one. Comparing with other methods, the proposed method achieves better performance than
other methods for all targets.
4 Related Work
Organization name disambiguation task is fundamental problems in many NLP applications. The task
aims to distinguish the real world relevant of a given name with the same surface in context. WePS-
3
6
(Amig?o et al., 2010) and RepLab 2013
7
(Amig?o et al., 2013) evaluation campaigns have also addressed
the problem from the perspective of disambiguation organization names in microblogs. Pedersen et
al. (2006) proposed an unsupervised method for name discrimination. Yerva et al. (2010) used support
vector machines (SVM) classifier with various external resources, such as WordNet, metadata profile,
category profile, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet
allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for
this task. However, most of these methods focused on the text with predefined surface words. The
documents which do not contain organization names or person names can not be well processed by these
methods.
To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use
translation model to handle it. They modeled the tag suggestion task as a translation process from
6
http://nlp.uned.es/weps/weps-3
7
http://www.limosine-project.eu/events/replab2013
694
Table 4: The influence of the smoothing parameter ? of the propose method.
? Precision Recall F
1
0.0 0.471 0.432 0.451
0.2 0.490 0.449 0.469
0.4 0.495 0.454 0.474
0.6 0.511 0.468 0.489
0.8 0.522 0.479 0.500
1.0 0.519 0.476 0.496
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10
F1
-S
co
re
NB SVM IBM1 TTM Our w/o BG Our
Figure 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the different companies in
the test dataset AMB-Weibo.
document content to tags. Ding et al. (2013) extended the translation based method and introduced a
topic-specific translation model to process the multiple meanings of words in different topics. Motivated
by these methods, we also propose to use topic-specific translation model to handle vocabulary problem.
Based on the model, in this work, we incorporate the background knowledge information into the model.
5 Conclusions
To identify target companies of microblogs, in this paper, we propose a novel topical translation
model to achieve the task. The main assumption is that the microblog posts and targets describe
the same thing with different languages. We convert the target identification problem to a translation
process from content of microblogs to targets. We integrate latent topical information into translation
model to hand the themes of microblogs in facilitating the translation process. We also incorporate
background knowledge (such as product names, series, et al.) into the generation model. Experimental
results on a large corpus constructed from a real microblog service and a number of manually labeled
golden standards of easily ambiguous entities demonstrate that the proposed method can achieve better
performance than other approaches.
6 Acknowledgement
The authors wish to thank the anonymous reviewers for their helpful comments. This work was
partially funded by 973 Program (2010CB327900), National Natural Science Foundation of China
(61003092,61073069), Shanghai Leading Academic Discipline Project (B114) and ?Chen Guang?
project supported by Shanghai Municipal Education Commission and Shanghai Education Development
Foundation(11CG05).
695
References
Enrique Amig?o, Javier Artiles, Julio Gonzalo, Damiano Spina, Bing Liu, and Adolfo Corujo. 2010.
Weps3 evaluation campaign: Overview of the on-line reputation management task. In CLEF (Notebook
Papers/LABs/Workshops).
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina Chugur, Adolfo Corujo, Julio Gonzalo, Tamara Mart??n, Edgar
Meij, Maarten Rijke, and Damiano Spina. 2013. Overview of replab 2013: Evaluating online reputation
monitoring systems. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization,
volume 8138 of Lecture Notes in Computer Science, pages 333?352. Springer Berlin Heidelberg.
S. Asur and B.A. Huberman. 2010. Predicting the future with social media. In Proceedings of WI-IAT 2010.
Hila Becker, Mor Naaman, and Luis Gravano. 2010. Learning similarity metrics for event identification in social
media. In Proceedings of WSDM ?10.
Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In
Proceedings of CIKM ?10.
Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding the right facts in the crowd:
factoid question answering over social media. In Proceedings of WWW ?08.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Learning topical translation model for
microblog hashtag suggestion. In Proceedings of IJCAI 2013.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy
of Sciences, volume 101, pages 5228?5235.
Ido Guy, Naama Zwerdling, Inbal Ronen, David Carmel, and Erel Uziel. 2010. Social media recommendation
based on people and tags. In Proceedings of SIGIR ?10.
Ido Guy, Uri Avraham, David Carmel, Sigalit Ur, Michal Jacovi, and Inbal Ronen. 2013. Mining expertise and
interests from social media. In Proceedings of WWW ?13.
Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI?04.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment
classification. In Proceedings of ACL-HLT 2011, Portland, Oregon, USA.
Zornitsa Kozareva and Sujith Ravi. 2011. Unsupervised name ambiguity resolution using a generative model.
In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ?11, pages 105?112,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Chia-Jung Lee and W. Bruce Croft. 2013. Building a web test collection using social media. In Proceedings of
SIGIR ?13, SIGIR ?13.
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of CIKM
?09.
Kang Liu, Liheng Xu, and Jun Zhao. 2012a. Opinion target extraction using word-based translation model. In
Proceedings of EMNLP-CoNLL ?12.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012b. Topical word trigger model for keyphrase extraction. In
Proceedings of COLING.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion targets
from online reviews. In Proceedings of ACL 2013, Sofia, Bulgaria.
Takanobu Otsuka, Takuya Yoshimura, and Takayuki Ito. 2012. Evaluation of the reputation network using realistic
distance between facebook data. In Proceedings of WI-IAT ?12, Washington, DC, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
696
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zornitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name discrimination using second order co-occurrence features.
In Computational Linguistics and Intelligent Text Processing, pages 208?222.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In
Proceedings of HL-EMNLP 2005, Vancouver, British Columbia, Canada.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Comput. Linguist., 37(1):9?27, March.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of the 19th international conference on World wide web, WWW
?10, pages 851?860, New York, NY, USA. ACM.
Damiano Spina, Edgar Meij, Maarten de Rijke, Andrei Oghina, Minh Thuong Bui, and Mathias Breuss. 2012.
Identifying entity aspects in microblog posts. In Proceedings of SIGIR ?12.
Damiano Spina, Julio Gonzalo, and Enrique Amig?o. 2013. Discovering filter keywords for company name
disambiguation in twitter. Expert Systems with Applications, 40(12):4986 ? 5003.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP 2009, Singapore.
Surender Reddy Yerva, Zoltn Mikls, and Karl Aberer. 2010. It was easy, when apples and blackberries
were only fruits. In Martin Braschler, Donna Harman, and Emanuele Pianta, editors, CLEF (Notebook
Papers/LABs/Workshops).
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O?Brien-Strain. 2010. Extracting and ranking product features
in opinion documents. In Proceedings of COLING ?10.
Shu Zhang, Jianwei Wu, Dequan Zheng, Yao Meng, and Hao Yu. 2012. An adaptive method for organization name
disambiguation with feature reinforcing. In Proceedings of the 26th Pacific Asia Conference on Language,
Information, and Computation, pages 237?245, Bali,Indonesia, November. Faculty of Computer Science,
Universitas Indonesia.
697
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187?195,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Joint Training and Decoding Using Virtual Nodes for Cascaded
Segmentation and Tagging Tasks
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{qianxian, qz, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn
Abstract
Many sequence labeling tasks in NLP require
solving a cascade of segmentation and tag-
ging subtasks, such as Chinese POS tagging,
named entity recognition, and so on. Tradi-
tional pipeline approaches usually suffer from
error propagation. Joint training/decoding in
the cross-product state space could cause too
many parameters and high inference complex-
ity. In this paper, we present a novel method
which integrates graph structures of two sub-
tasks into one using virtual nodes, and per-
forms joint training and decoding in the fac-
torized state space. Experimental evaluations
on CoNLL 2000 shallow parsing data set and
Fourth SIGHAN Bakeoff CTB POS tagging
data set demonstrate the superiority of our
method over cross-product, pipeline and can-
didate reranking approaches.
1 Introduction
There is a typical class of sequence labeling tasks
in many natural language processing (NLP) applica-
tions, which require solving a cascade of segmenta-
tion and tagging subtasks. For example, many Asian
languages such as Japanese and Chinese which
do not contain explicitly marked word boundaries,
word segmentation is the preliminary step for solv-
ing part-of-speech (POS) tagging problem. Sen-
tences are firstly segmented into words, then each
word is assigned with a part-of-speech tag. Both
syntactic parsing and dependency parsing usually
start with a textual input that is tokenized, and POS
tagged.
The most commonly approach solves cascaded
subtasks in a pipeline, which is very simple to im-
plement and allows for a modular approach. While,
the key disadvantage of such method is that er-
rors propagate between stages, significantly affect-
ing the quality of the final results. To cope with this
problem, Shi and Wang (2007) proposed a rerank-
ing framework in which N-best segment candidates
generated in the first stage are passed to the tag-
ging model, and the final output is the one with the
highest overall segmentation and tagging probabil-
ity score. The main drawback of this method is that
the interaction between tagging and segmentation is
restricted by the number of candidate segmentation
outputs. Razvan C. Bunescu (2008) presented an
improved pipeline model in which upstream subtask
outputs are regarded as hidden variables, together
with their probabilities are used as probabilistic fea-
tures in the downstream subtasks. One shortcom-
ing of this method is that calculation of marginal
probabilities of features may be inefficient and some
approximations are required for fast computation.
Another disadvantage of these two methods is that
they employ separate training and the segmentation
model could not take advantages of tagging infor-
mation in the training procedure.
On the other hand, joint learning and decoding
using cross-product of segmentation states and tag-
ging states does not suffer from error propagation
problem and achieves higher accuracy on both sub-
tasks (Ng and Low, 2004). However, two problems
arises due to the large state space, one is that the
amount of parameters increases rapidly, which is apt
to overfit on the training corpus, the other is that
the inference by dynamic programming could be in-
efficient. Sutton (2004) proposed Dynamic Con-
ditional Random Fields (DCRFs) to perform joint
training/decoding of subtasks using much fewer pa-
rameters than the cross-product approach. How-
187
ever, DCRFs do not guarantee non-violation of hard-
constraints that nodes within the same segment get
a single consistent tagging label. Another draw-
back of DCRFs is that exact inference is generally
time consuming, some approximations are required
to make it tractable.
Recently, perceptron based learning framework
has been well studied for incorporating node level
and segment level features together (Kazama and
Torisawa, 2007; Zhang and Clark, 2008). The main
shortcoming is that exact inference is intractable
for those dynamically generated segment level fea-
tures, so candidate based searching algorithm is
used for approximation. On the other hand, Jiang
(2008) proposed a cascaded linear model which has
a two layer structure, the inside-layer model uses
node level features to generate candidates with their
weights as inputs of the outside layer model which
captures non-local features. As pipeline models, er-
ror propagation problem exists for such method.
In this paper, we present a novel graph structure
that exploits joint training and decoding in the fac-
torized state space. Our method does not suffer
from error propagation, and guards against viola-
tions of those hard-constraints imposed by segmen-
tation subtask. The motivation is to integrate two
Markov chains for segmentation and tagging sub-
tasks into a single chain, which contains two types of
nodes, then standard dynamic programming based
exact inference is employed on the hybrid struc-
ture. Experiments are conducted on two different
tasks, CoNLL 2000 shallow parsing and SIGHAN
2008 Chinese word segmentation and POS tagging.
Evaluation results of shallow parsing task show
the superiority of our proposed method over tradi-
tional joint training/decoding approach using cross-
product state space, and achieves the best reported
results when no additional resources at hand. For
Chinese word segmentation and POS tagging task, a
strong baseline pipeline model is built, experimental
results show that the proposed method yields a more
substantial improvement over the baseline than can-
didate reranking approach.
The rest of this paper is organized as follows: In
Section 2, we describe our novel graph structure. In
Section 3, we analyze complexity of our proposed
method. Experimental results are shown in Section
4. We conclude the work in Section 5.
2 Multi-chain integration using Virtual
Nodes
2.1 Conditional Random Fields
We begin with a brief review of the Conditional Ran-
dom Fields(CRFs). Let x = x1x2 . . . xl denote the
observed sequence, where xi is the ith node in the
sequence, l is sequence length, y = y1y2 . . . yl is a
label sequence over x that we wish to predict. CRFs
(Lafferty et al, 2001) are undirected graphic mod-
els that use Markov network distribution to learn the
conditional probability. For sequence labeling task,
linear chain CRFs are very popular, in which a first
order Markov assumption is made on the labels:
p(y|x) = 1Z(x)
?
i
?(x,y, i)
,where
?(x,y, i) = exp
(
wT f(x, yi?1, yi, i)
)
Z(x) =
?
y
?
i
?(x,y, i)
f(x, yi?1, yi, i) =
[f1(x, yi?1, yi, i), . . .,fm(x, yi?1, yi, i)]T , each ele-
ment fj(x, yi?1, yi, i) is a real valued feature func-
tion, here we simplify the notation of state feature
by writing fj(x, yi, i) = fj(x, yi?1, yi, i), m is the
cardinality of feature set {fj}. w = [w1, . . . , wm]T
is a weight vector to be learned from the training
set. Z(x) is the normalization factor over all label
sequences for x.
In the traditional joint training/decoding approach
for cascaded segmentation and tagging task, each
label yi has the form si-ti, which consists of seg-
mentation label si and tagging label ti. Let s =
s1s2 . . . sl be the segmentation label sequence over
x. There are several commonly used label sets such
as BI, BIO, IOE, BIES, etc. To facilitate our dis-
cussion, in later sections we will use BIES label set,
where B,I,E represents Beginning, Inside and End of
a multi-node segment respectively, S denotes a sin-
gle node segment. Let t = t1t2 . . . tl be the tagging
label sequence over x. For example, in named entity
recognition task, ti ? {PER, LOC, ORG, MISC,
O} represents an entity type (person name, loca-
tion name, organization name, miscellaneous entity
188
x2
s?t
2
2
x1
s?t
1
1
S-P S-O
x3
s?t
3
3
S-O
x4
s?t
4
4
B-P
x5
s?t
5
5
E-P
Hendrix ?s girlfriend Kathy Etchingham
Figure 1: Graphical representation of linear chain CRFs
for traditional joint learning/decoding
name and other). Graphical representation of lin-
ear chain CRFs is shown in Figure 1, where tagging
label ?P? is the simplification of ?PER?. For nodes
that are labeled as other, we define si =S, ti =O.
2.2 Hybrid structure for cascaded labeling
tasks
Different from traditional joint approach, our
method integrates two linear markov chains for seg-
mentation and tagging subtasks into one that con-
tains two types of nodes. Specifically, we first
regard segmentation and tagging as two indepen-
dent sequence labeling tasks, corresponding chain
structures are built, as shown in the top and mid-
dle sub-figures of Figure 2. Then a chain of twice
length of the observed sequence is built, where
nodes x1, . . . , xl on the even positions are original
observed nodes, while nodes v1, . . . , vl on the odd
positions are virtual nodes that have no content in-
formation. For original nodes xi, the state space is
the tagging label set, while for virtual nodes, their
states are segmentation labels. The label sequence
of the hybrid chain is y = y1 . . . y2l = s1t1 . . . sltl,
where combination of consecutive labels siti repre-
sents the full label for node xi.
Then we let si be connected with si?1 and si+1
, so that first order Markov assumption is made
on segmentation states. Similarly, ti is connected
with ti?1 and ti+1. Then neighboring tagging and
segmentation states are connected as shown in the
bottom sub-figure of Figure 2. Non-violation of
hard-constraints that nodes within the same seg-
ment get a single consistent tagging label is guar-
anteed by introducing second order transition fea-
tures f(ti?1, si, ti, i) that are true if ti?1 6= ti and
si ? {I,E}. For example, fj(ti?1, si, ti, i) is de-
fined as true if ti?1 =PER, si =I and ti =LOC.
In other words, it is true, if a segment is partially
tagging as PER, and partially tagged as LOC. Since
such features are always false in the training corpus,
their corresponding weights will be very low so that
inconsistent label assignments impossibly appear in
decoding procedure. The hybrid graph structure can
be regarded as a special case of second order Markov
chain.
Hendrix ?s girlfriend Kathy Etchingham
x1 x2 x3 x4 x5
s1 s2 s3 s4 s5
S S S B E
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
s2s1 s3 s4 s5
S S S B E
v1 v2 v3 v4 v5
Integrate
Figure 2: Multi-chain integration using Virtual Nodes
2.3 Factorized features
Compared with traditional joint model that exploits
cross-product state space, our hybrid structure uses
factorized states, hence could handle more flexible
features. Any state feature g(x, yi, i) defined in
the cross-product state space can be replaced by a
first order transition feature in the factorized space:
f(x, si, ti, i). As for the transition features, we
use f(si?1, ti?1, si, i) and f(ti?1, si, ti, i) instead
of g(yi?1, yi, i) in the conventional joint model.
Features in cross-product state space require that
segmentation label and tagging label take on partic-
ular values simultaneously, however, sometimes we
189
want to specify requirement on only segmentation or
tagging label. For example, ?Smith? may be an end
of a person name, ?Speaker: John Smith?; or a sin-
gle word person name ?Professor Smith will . . . ?. In
such case, our observation is that ?Smith? is likely a
(part of) person name, we do not care about its seg-
mentation label. So we could define state feature
f(x, ti, i) = true, if xi is ?Smith? with tagging la-
bel ti=PER.
Further more, we could define features like
f(x, ti?1, ti, i), f(x, si?1, si, i), f(x, ti?1, si, i),
etc. The hybrid structure facilitates us to use
varieties of features. In the remainder of the
paper, we use notations f(x, ti?1, si, ti, i) and
f(x, si?1, ti?1, si, i) for simplicity.
2.4 Hybrid CRFs
A hybrid CRFs is a conditional distribution that fac-
torizes according to the hybrid graphical model, and
is defined as:
p(s, t|x) = 1Z(x)
?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
Where
?(x, s, t, i) = exp
(
wT1 f(x, si?1, ti?1, si)
)
?(x, s, t, i) = exp
(
wT2 f(x, ti?1, si, ti)
)
Z(x) =
?
s,t
(?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
)
Where w1, w2 are weight vectors.
Luckily, unlike DCRFs, in which graph structure
can be very complex, and the cross-product state
space can be very large, in our cascaded labeling
task, the segmentation label set is often small, so
far as we known, the most complicated segmenta-
tion label set has only 6 labels (Huang and Zhao,
2007). So exact dynamic programming based algo-
rithms can be efficiently performed.
In the training stage, we use second order forward
backward algorithm to compute the marginal proba-
bilities p(x, si?1, ti?1, si) and p(x, ti?1, si, ti), and
the normalization factor Z(x). In decoding stage,
we use second order Viterbi algorithm to find the
best label sequence. The Viterbi decoding can be
Table 1: Time Complexity
Method Training Decoding
Pipeline (|S|2cs + |T |2ct)L (|S|2 + |T |2)U
Cross-Product (|S||T |)2cL (|S||T |)2U
Reranking (|S|2cs + |T |2ct)L (|S|2 + |T |2)NU
Hybrid (|S| + |T |)|S||T |cL (|S| + |T |)|S||T |U
used to label a new sequence, and marginal compu-
tation is used for parameter estimation.
3 Complexity Analysis
The time complexity of the hybrid CRFs train-
ing and decoding procedures is higher than that of
pipeline methods, but lower than traditional cross-
product methods. Let
? |S| = size of the segmentation label set.
? |T | = size of the tagging label set.
? L = total number of nodes in the training data
set.
? U = total number of nodes in the testing data
set.
? c = number of joint training iterations.
? cs = number of segmentation training itera-
tions.
? ct = number of tagging training iterations.
? N = number of candidates in candidate rerank-
ing approach.
Time requirements for pipeline, cross-product, can-
didate reranking and hybrid CRFs are summarized
in Table 1. For Hybrid CRFs, original node xi has
features {fj(ti?1, si, ti)}, accessing all label subse-
quences ti?1siti takes |S||T |2 time, while virtual
node vi has features {fj(si?1, ti?1, si)}, accessing
all label subsequences si?1ti?1si takes |S|2|T | time,
so the final complexity is (|S|+ |T |)|S||T |cL.
In real applications, |S| is small, |T | could be
very large, we assume that |T | >> |S|, so for
each iteration, hybrid CRFs is about |S| times slower
than pipeline and |S| times faster than cross-product
190
Table 2: Feature templates for shallow parsing task
Cross Product CRFs Hybrid CRFs
wi?2yi, wi?1yi, wiyi wi?1si, wisi, wi+1si
wi+1yi, wi+2yi wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
wi?1wiyi, wiwi+1yi wi?1wisi, wiwi+1si
wi?1witi, wiwi+1ti
pi?2yi, pi?1yi, piyi pi?1si, pisi, pi+1si
pi+1yi, pi+2yi pi?2ti, pi?1ti, pi+1ti, pi+2ti
pi?2pi?1yi, pi?1piyi, pipi+1yi,
pi+1pi+2yi
pi?2pi?1si, pi?1pisi, pipi+1si, pi+1pi+2si
pi?3pi?2ti, pi?2pi?1ti, pi?1piti, pipi+1ti,
pi+1pi+2ti, pi+2pi+3ti, pi?1pi+1ti
pi?2pi?1piyi, pi?1pipi+1yi,
pipi+1pi+2yi
pi?2pi?1pisi, pi?1pipi+1si, pipi+1pi+2si
wipiti
wisi?1si
wi?1ti?1ti, witi?1ti, pi?1ti?1ti, piti?1ti
yi?1yi si?1ti?1si, ti?1siti
method. When decoding, candidate reranking ap-
proach requires more time if candidate number N >
|S|.
Though the space complexity could not be com-
pared directly among some of these methods, hybrid
CRFs require less parameters than cross-product
CRFs due to the factorized state space. This is sim-
ilar with factorized CRFs (FCRFs) (Sutton et al,
2004).
4 Experiments
4.1 Shallow Parsing
Our first experiment is the shallow parsing task. We
use corpus from CoNLL 2000 shared task, which
contains 8936 sentences for training and 2012 sen-
tences for testing. There are 11 tagging labels: noun
phrase(NP), verb phrase(VP) , . . . and other (O), the
segmentation state space we used is BIES label set,
since we find that it yields a little improvement over
BIO set.
We use the standard evaluation metrics, which are
precision P (percentage of output phrases that ex-
actly match the reference phrases), recall R (percent-
age of reference phrases returned by our system),
and their harmonic mean, the F1 score F1 = 2PRP+R
(which we call F score in what follows).
We compare our approach with traditional cross-
product method. To find good feature templates,
development data are required. Since CoNLL2000
does not provide development data set, we divide
the training data into 10 folds, of which 9 folds for
training and 1 fold for developing. After selecting
feature templates by cross validation, we extract fea-
tures and learn their weights on the whole training
data set. Feature templates are summarized in Table
2, where wi denotes the ith word, pi denotes the ith
POS tag.
Notice that in the second row, feature templates
of the hybrid CRFs does not contain wi?2si, wi+2si,
since we find that these two templates degrade per-
formance in cross validation. However, wi?2ti,
wi+2ti are useful, which implies that the proper con-
text window size for segmentation is smaller than
tagging. Similarly, for hybrid CRFs, the window
size of POS bigram features for segmentation is 5
(from pi?2 to pi+2, see the eighth row in the sec-
ond column); while for tagging, the size is 7 (from
pi?3 to pi+3, see the ninth row in the second col-
umn). However for cross-product method, their win-
dow sizes must be consistent.
For traditional cross-product CRFs and our hybrid
CRFs, we use fixed gaussian prior ? = 1.0 for both
methods, we find that this parameter does not signifi-
191
Table 3: Results for shallow parsing task, Hybrid CRFs
significantly outperform Cross-Product CRFs (McNe-
mar?s test; p < 0.01)
Method Cross-Product
CRFs
Hybrid
CRFs
Training Time 11.6 hours 6.3 hours
Feature Num-
ber
13 million 10 mil-
lion
Iterations 118 141
F1 93.88 94.31
cantly affect the results when it varies between 1 and
10. LBFGS(Nocedal and Wright, 1999) method is
employed for numerical optimization. Experimen-
tal results are shown in Table 3. Our proposed CRFs
achieve a performance gain of 0.43 points in F-score
over cross-product CRFs that use state space while
require less training time.
For comparison, we also listed the results of pre-
vious top systems, as shown in Table 4. Our pro-
posed method outperforms other systems when no
additional resources at hand. Though recently semi-
supervised learning that incorporates large mounts
of unlabeled data has been shown great improve-
ment over traditional supervised methods, such as
the last row in Table 4, supervised learning is funda-
mental. We believe that combination of our method
and semi-supervised learning will achieve further
improvement.
4.2 Chinese word segmentation and POS
tagging
Our second experiment is the Chinese word seg-
mentation and POS tagging task. To facilitate com-
parison, we focus only on the closed test, which
means that the system is trained only with a des-
ignated training corpus, any extra knowledge is not
allowed, including Chinese and Arabic numbers, let-
ters and so on. We use the Chinese Treebank (CTB)
POS corpus from the Fourth International SIGHAN
Bakeoff data sets (Jin and Chen, 2008). The train-
ing data consist of 23444 sentences, 642246 Chinese
words, 1.05M Chinese characters and testing data
consist of 2079 sentences, 59955 Chinese words,
0.1M Chinese characters.
We compare our hybrid CRFs with pipeline and
candidate reranking methods (Shi and Wang, 2007)
Table 4: Comparison with other systems on shallow pars-
ing task
Method F1 Additional Re-
sources
Cross-Product CRFs 93.88
Hybrid CRFs 94.31
SVM combination 93.91
(Kudo and Mat-
sumoto, 2001)
Voted Perceptrons 93.74 none
(Carreras and Mar-
quez, 2003)
ETL (Milidiu et al,
2008)
92.79
(Wu et al, 2006) 94.21 Extended features
such as token fea-
tures, affixes
HySOL 94.36 17M words unla-
beled
(Suzuki et al, 2007) data
ASO-semi 94.39 15M words unla-
beled
(Ando and Zhang,
2005)
data
(Zhang et al, 2002) 94.17 full parser output
(Suzuki and Isozaki,
2008)
95.15 1G words unla-
beled data
using the same evaluation metrics as shallow pars-
ing. We do not compare with cross-product CRFs
due to large amounts of parameters.
For pipeline method, we built our word segmenter
based on the work of Huang and Zhao (2007),
which uses 6 label representation, 7 feature tem-
plates (listed in Table 5, where ci denotes the ith
Chinese character in the sentence) and CRFs for pa-
rameter learning. We compare our segmentor with
other top systems using SIGHAN CTB corpus and
evaluation metrics. Comparison results are shown
in Table 6, our segmenter achieved 95.12 F-score,
which is ranked 4th of 26 official runs. Except for
the first system which uses extra unlabeled data, dif-
ferences between rest systems are not significant.
Our POS tagging system is based on linear chain
CRFs. Since SIGHAN dose not provide develop-
ment data, we use the 10 fold cross validation de-
scribed in the previous experiment to turning feature
templates and Gaussian prior. Feature templates are
listed in Table 5, where wi denotes the ith word in
192
Table 5: Feature templates for Chinese word segmentation and POS tagging task
Segmentation feature templates
(1.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(1.2) ci?1cisi, cici+1si, ci?1ci+1si
(1.3) si?1si
POS tagging feature templates
(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti, wi+1wi+2ti, wi?1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti, c?1(wi)ti
(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti
(2.5) l(wi)ti
(2.6) ti?1ti
Joint segmentation and POS tagging feature templates
(3.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(3.2) ci?1cisi, cici+1si, ci?1ci+1si
(3.3) ci?3ti, ci?2ti, ci?1ti, citi, ci+1ti, ci+2ti, ci+3ti
(3.4) ci?3ci?2ti, ci?2ci?1ti, ci?1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci?2citi, cici+2ti
(3.5) cisiti
(3.6) citi?1ti
(3.7) si?1ti?1si, ti?1siti
Table 6: Word segmentation results on Fourth SIGHAN
Bakeoff CTB corpus
Rank F1 Description
1/26 95.89? official best, using extra un-
labeled data (Zhao and Kit,
2008)
2/26 95.33 official second
3/26 95.17 official third
4/26 95.12 segmentor in pipeline sys-
tem
Table 7: POS results on Fourth SIGHAN Bakeoff CTB
corpus
Rank Accuracy Description
1/7 94.29 POS tagger in pipeline sys-
tem
2/7 94.28 official best
3/7 94.01 official second
4/7 93.24 official third
the sentence, cj(wi), j > 0 denotes the jth Chinese
character of word wi, cj(wi), j < 0 denotes the jth
last Chinese character, l(wi) denotes the word length
of wi. We compare our POS tagger with other top
systems on Bakeoff CTB POS corpus where sen-
tences are perfectly segmented into words, our POS
tagger achieved 94.29 accuracy, which is the best of
7 official runs. Comparison results are shown in Ta-
ble 7.
For reranking method, we varied candidate num-
bers n among n ? {10, 20, 50, 100}. For hybrid
CRFs, we use the same segmentation label set as
the segmentor in pipeline. Feature templates are
listed in Table 5. Experimental results are shown
in Figure 3. The gain of hybrid CRFs over the
baseline pipeline model is 0.48 points in F-score,
about 3 times higher than 100-best reranking ap-
proach which achieves 0.13 points improvement.
Though larger candidate number can achieve higher
performance, such improvement becomes trivial for
n > 20.
Table 8 shows the comparison between our work
and other relevant work. Notice that, such com-
parison is indirect due to different data sets and re-
193
0 20 40 60 80 10090.3
90.4
90.5
90.6
90.7
90.8
90.9
candidate number
F s
cor
e
 
 
candidate reranking
Hybrid CRFs
Figure 3: Results for Chinese word segmentation and
POS tagging task, Hybrid CRFs significantly outperform
100-Best Reranking (McNemar?s test; p < 0.01)
Table 8: Comparison of word segmentation and POS tag-
ging, such comparison is indirect due to different data
sets and resources.
Model F1
Pipeline (ours) 90.40
100-Best Reranking (ours) 90.53
Hybrid CRFs (ours) 90.88
Pipeline (Shi and Wang, 2007) 91.67
20-Best Reranking (Shi and Wang,
2007)
91.86
Pipeline (Zhang and Clark, 2008) 90.33
Joint Perceptron (Zhang and Clark,
2008)
91.34
Perceptron Only (Jiang et al, 2008) 92.5
Cascaded Linear (Jiang et al, 2008) 93.4
sources. One common conclusion is that joint mod-
els generally outperform pipeline models.
5 Conclusion
We introduced a framework to integrate graph struc-
tures for segmentation and tagging subtasks into one
using virtual nodes, and performs joint training and
decoding in the factorized state space. Our approach
does not suffer from error propagation, and guards
against violations of those hard-constraints imposed
by segmentation subtask. Experiments on shal-
low parsing and Chinese word segmentation tasks
demonstrate our technique.
6 Acknowledgements
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
The National High Technology Research and De-
velopment Program of China (2009AA01A346),
Shanghai Leading Academic Discipline Project
(B114), Doctoral Fund of Ministry of Education of
China (200802460066), National Natural Science
Funds for Distinguished Young Scholar of China
(61003092), and Shanghai Science and Technology
Development Funds (08511500302).
References
R. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Pro-
ceedings of ACL, pages 1?9.
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In Proceedings
of EMNLP, Waikiki, Honolulu, Hawaii.
X Carreras and L Marquez. 2003. Phrase recognition by
filtering and ranking with perceptrons. In Proceedings
of RANLP.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21:8?19.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL, Columbus, Ohio, USA.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing, India.
Junichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP, pages 315?
324, Prague, June.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings of ACL, pages
647?655.
194
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
ofspeech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs
based joint decoding method for cascaded segmenta-
tion and labeling tasks. In Proceedings of IJCAI, pages
1707?1712, Hyderabad, India.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In Proceedings of ICML.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL, pages
665?673.
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Proceedings of EMNLP, Prague.
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006.
A general and multi-lingual phrase chunking model
based on masking method. In Proceedings of Intel-
ligent Text Processing and Computational Linguistics,
pages 144?155.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL, Columbus, Ohio, USA.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. ma-
chine learning research. Machine Learning Research,
2:615?637.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111.
195
