Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
NAACL HLT Demonstration Program, pages 5?6,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Adaptive Tutorial Dialogue Systems Using Deep NLP Techniques
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow,
Manuel Marques-Pita, Colin Matheson and Johanna D. Moore
ICCS-HCRC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, United Kingdom
(mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk ?
Abstract
We present tutorial dialogue systems in
two different domains that demonstrate
the use of dialogue management and deep
natural language processing techniques.
Generation techniques are used to produce
natural sounding feedback adapted to stu-
dent performance and the dialogue his-
tory, and context is used to interpret ten-
tative answers phrased as questions.
1 Introduction
Intelligent tutoring systems help students improve
learning compared to reading textbooks, though not
quite as much as human tutors (Anderson et al,
1995). The specific properties of human-human di-
alogue that help students learn are still being stud-
ied, but the proposed features important for learn-
ing include allowing students to explain their actions
(Chi et al, 1994), adapting tutorial feedback to the
learner?s level, and engagement/affect. Some tuto-
rial dialogue systems use NLP techniques to analyze
student responses to ?why? questions. (Aleven et al,
2001; Jordan et al, 2006). However, for remediation
they revert to scripted dialogue, relying on short-
answer questions and canned feedback. The result-
ing dialogue may be redundant in ways detrimental
to student understanding (Jordan et al, 2005) and
allows for only limited adaptivity (Jordan, 2004).
?This work was supported under the 6th Framework Pro-
gramme of the European Commission, Ref. IST-507826, and
by a grant from The Office of Naval Research N000149910165.
We demonstrate two tutorial dialogue systems
that use techniques from task-oriented dialogue sys-
tems to improve the interaction. The systems are
built using the Information State Update approach
(Larsson and Traum, 2000) for dialogue manage-
ment and generic components for deep natural lan-
guage understanding and generation. Tutorial feed-
back is generated adaptively based on the student
model, and the interpretation is used to process
explanations and to differentiate between student
queries and hedged answers phrased as questions.
The systems are intended for testing hypotheses
about tutoring. By comparing student learning gains
between versions of the same system using different
tutoring strategies, as well as between the systems
and human tutors, we can test hypotheses about the
role of factors such as free natural language input,
adaptivity and student affect.
2 The BEEDIFF Tutor
The BEEDIFF tutor helps students solve symbolic
differentiation problems, a procedural task. Solu-
tion graphs generated by a domain reasoner are used
to interpret student actions and to generate feed-
back.1 Student input is relatively limited and con-
sists mostly of mathematical formulas, but the sys-
tem generates adaptive feedback based on the notion
of student performance and on the dialogue history.
For example, if an average student asks for a hint
on differentiating sin(x2), the first level of feedback
may be ?Think about which rule to apply?, which
1Solution graphs are generated automatically for arbitrary
expressions, with no limit on the complexity of expressions ex-
cept for possible efficiency considerations.
5
can then be specialized to ?Use the chain rule? and
then to giving away the complete answer. For stu-
dents with low performance, more specific feed-
back can be given from the start. The same strat-
egy (based on an initial corpus analysis) is used in
producing feedback after incorrect answers, and we
intend to use the system to evaluate its effectiveness.
The feedback is generated automatically from a
single diagnosis and generation techniques are used
to produce appropriate discourse cues. For example,
when a student repeats the same mistake, the feed-
back may be ?You?ve differentiated the inner layer
correctly, but you?re still missing the minus sign?.
The two clauses are joined by a contrast relationship,
and the second indicates that an error was repeated
by using the adverbial ?still?.
3 The BEETLE Tutor
The BEETLE tutor is designed to teach students ba-
sic electricity and electronics concepts. Unlike the
BEEDIFF tutor, the BEETLE tutor is built around
a pre-planned course where the students alternate
reading with exercises involving answering ?why?
questions and interacting with a circuit simulator.
Since this is a conceptual domain, for most exer-
cises there is no structured sequence of steps that the
students should follow, but students need to name a
correct set of objects and relationships in their re-
sponse. We model the process of building an answer
to an exercise as co-constructing a solution, where
the student and tutor may contribute parts of the an-
swer. For example, consider the question ?For each
circuit, which components are in a closed path?.
The solution can be built up gradually, with the stu-
dent naming different components, and the system
providing feedback until the list is complete. This
generic process of gradually building up a solution is
also applied to giving explanations. For example, in
answer to the question ?What is required for a light
bulb to light? the student may say ?The bulb must be
in a closed path?, which is correct but not complete.
The system may then say ?Correct, but is that every-
thing?? to prompt the student towards mentioning
the battery as well. The diagnosis of the student an-
swer is represented as a set of correctly given objects
or relationships, incorrect parts, and objects and re-
lationships that have yet to be mentioned, and the
system uses the same dialogue strategy of eliciting
the missing parts for all types of questions.
Students often phrase their answers tentatively,
for example ?Is the bulb in a closed path??. In the
context of a tutor question the interpretation process
treats yes-no questions from the student as poten-
tially hedged answers. The dialogue manager at-
tempts to match the objects and relationships in the
student input with those in the question. If a close
match can be found, then the student utterance is
interpreted as giving an answer rather than a true
query. In contrast, if the student said ?Is the bulb
connected to the battery??, this would be interpreted
as a proper query and the system would attempt to
answer it.
Conclusion We demonstrate two tutorial dialogue
systems in different domains built by adapting di-
alogue techniques from task-oriented dialogue sys-
tems. Improved interpretation and generation help
support adaptivity and a wider range of inputs than
possible in scripted dialogue.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In Proc. AI-ED 2001.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons learned.
The Journal of the Learning Sciences, 4(2):167?207.
M. T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. La-
Vancher. 1994. Eliciting self-explanations improves
understanding. Cognitive Science, 18(3):439?477.
P. Jordan, P. Albacete, and K. VanLehn. 2005. Taking
control of redundancy in scripted tutorial dialogue. In
Proc. of AIED2005, pages 314?321.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tutorial
dialogue system for physics. In Proc. of FLAIRS-06.
P. W. Jordan. 2004. Using student explanations as mod-
els for adapting tutorial dialogue. In V. Barr and
Z. Markov, editors, FLAIRS Conference. AAAI Press.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3-
4):323?340.
6
Discourse Annotation in the Monroe Corpus
Joel Tetreault   , Mary Swift   , Preethum Prithviraj   , Myroslava Dzikovska  , James Allen  
 
Department of Computer Science, University of Rochester, Rochester, NY, 14620, USA
tetreaul,swift,prithvir,james@cs.rochester.edu

Human Communications Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
mdzikovs@inf.ed.ac.uk
Abstract
We describe a method for annotating spoken dia-
log corpora using both automatic and manual an-
notation. Our semi-automated method for corpus
development results in a corpus combining rich se-
mantics, discourse information and reference anno-
tation, and allows us to explore issues relating these.
1 Introduction
Discourse information plays an important part in
natural language systems performing tasks such
as text summarization, question-answering systems
and collaborative planning. But the type of dis-
course information that is relevant varies widely de-
pending on domain, genre, number of participants,
whether it is written or spoken, etc. Therefore em-
pirical analysis is necessary to determine common-
alities in the variations of discourse and develop
general purpose algorithms for discourse analysis.
The heightened interest in human language tech-
nologies in the last decade has sparked several dis-
course annotation projects. Though there has been
a lot of research, many of the projects focus on a
few specific areas of discourse relevant to their re-
spective system. For example, a text summarization
system working on texts from the web would not
need to know about dialogue modeling or ground-
ing or prosody. In contrast, for a spoken dialogue
system that collaborates with a user, such informa-
tion is crucial but the organization of web pages is
not.
In this paper we describe our work in the Monroe
Project, an effort targeting the production and use of
a linguistically rich annotated corpus of a series of
task-oriented spoken dialogs in an emergency res-
cue domain. Our project differs from past projects
involving reference annotation and discourse seg-
mentation in that the semantics and discourse infor-
mation is generated automatically. Most other work
in this area has had minimal semantics or speech
act tagging, if anything at all, which can be quite
labor intensive to annotate. In addition, our domain
is spoken language, which is rarely annotated for
the information we are providing. We describe our
research on reference resolution and discourse seg-
mentation using the annotated corpus and the soft-
ware tools we have developed to help us with differ-
ent aspects of the annotation tasks.
2 Aims of Monroe Project
2.1 Parser Development
One of the aims of the Monroe Project was to de-
velop a wide coverage grammar for spoken dia-
logue. Since parsing is just an initial stage of natural
language understanding, the project was focused not
just on obtaining syntactic trees alone (as is done
in many other parsed corpora, for example, Penn
TreeBank (Marcus et al, 1993) or Tiger (Brants
and Plaehn, 2000)). Instead, we aimed to develop a
parser and grammar for the production of syntactic
parses and semantic representations useful in dis-
course processing.
The parser produces a domain-independent se-
mantic representation with information necessary
for referential and discourse processing, in par-
ticular, domain-independent representations of de-
terminers and quantifiers (to be resolved by our
reference module), domain-independent represen-
tations for discourse adverbials, and tense, aspect
and modality information. This necessitated the de-
velopment of a domain-independent logical form
syntax and a domain-independent ontology as a
source of semantic types for our representations
(Dzikovska et al, 2004). In subsequent sections
we discuss how the parser-generated representations
are used as a basis for discourse annotation.
2.2 Reference Resolution Development
In spoken dialogue, choice of referring expression
is influential and influenced by the main entities be-
ing discussed and the intentions of the speaker. If
an entity is mentioned frequently, and thus is very
important to the current topic, it is usually pronom-
inalized. Psycholinguistic studies show that salient
terms are usually evoked as pronouns because of the
lighter inference load they place on the listener. Be-
cause pronouns occur frequently in discourse, it is
very important to know what they resolve to, so the
entire sentence can be processed correctly. A cor-
pus annotated for reference relations allows one to
compare the performance of different reference al-
gorithms.
2.3 Discourse Segmentation
Another research area that can benefit from a
discourse-annotated corpus is discourse structure.
There has been plenty of theoretical work such as
(Grosz and Sidner, 1986), (Moser and Moore, 1996)
which shows that just as sentences can be decom-
posed into smaller constituents, a discourse can be
decomposed into smaller units called discourse seg-
ments. Though there are many different ways to
segment discourse, the common themes are that
some sequences are more closely related than oth-
ers (discourse segments) and that a discourse can be
organized as a tree, with the leaves being the indi-
vidual utterances and the interior nodes being dis-
course segments. The embeddedness of a segment
effects which previous segments, and thus their enti-
ties, are accessible. As a discourse progresses, seg-
ments close and unless they are close to the root of
the tree (have a low embedding) may not be acces-
sible.
Discourse segmentation has implications for spo-
ken dialogue systems. Properly detecting discourse
structure can lead to improved reference resolution
accuracy since competing antecedents in inacces-
sible clauses may be removed from consideration.
Discourse segmentation is often closely related to
plan and intention recognition, so recognizing one
can lead to better detection of the other. Finally,
segmentation reduces the size of the history or con-
text maintained by a spoken dialogue system, thus
decreasing the search space for referents.
3 Monroe Corpus Construction
The Monroe domain is a series of task-oriented di-
alogs between human participants (Stent, 2001) de-
signed to encourage collaborative problem-solving
and mixed-initiative interaction. It is a simulated
rescue operation domain in which a controller re-
ceives emergency calls and is assisted by a system
or another person in formulating a plan to handle
emergencies ranging from requests for medical as-
sistance to civil disorder to snow storms. Available
resources include maps, repair crews, plows, ambu-
lances, helicopters and police.
Each dialog consisted of the execution of one
task which lasted about ten minutes. The two par-
ticipants were told to construct a plan as if they
were in an emergency control center. Each ses-
sion was recorded to audio and video, then broken
up into utterances under the guidelines of (Heeman
and Allen, 1994). Finally, the segmented audio files
were transcribed by hand. The entire Monroe cor-
pus consists of 20 dialogs. The annotation work we
report here is based on 5 dialogs totaling 1756 utter-
ances 1.
Discourse annotation of the Monroe Corpus con-
sisted of three phases: first, a semi-automated anno-
tation loop that resulted in parser-generated syntac-
tic and semantic analyses for each sentence. Sec-
ond, the corpus was manually annotated for refer-
ence information for pronouns and coreferential in-
formation for definite noun phrases. Finally, dis-
course segmentation was conducted manually. In
the following sections we discuss each of the three
phases in more detail.
3.1 Building the Parsed Corpus
To build the annotated corpus, we needed to first
have a parsed corpus as a source of discourse en-
tities. We built a suite of tools to rapidly develop
parsed corpora (Swift et al, 2004). These are Java
GUI for annotating speech repairs, a LISP tool to
parse annotated corpora and merge in changes, and
a Java tool interface to manually check the automat-
ically generated parser analyses (the CorpusTool).
Our goal in building the parsed corpus is to obtain
the output suitable for further annotation for refer-
ence and discourse information. In particular, the
parser achieves the following:
  Identifies the referring expressions. These are
definite noun phrases, but also verb phrases
and propositions which can be referred to by
deictic pronouns such as that. All entities are
assigned a unique variable name which can be
used to identify the referent later.
  Identifies implicit entities. These are implicit
subjects of imperatives, and also some implicit
arguments of relational nouns (e.g., the implied
object in the phrase the weight) and of adver-
bials (e.g., the implied reference time in That
happened before).
  Identifies speech acts. These are based on the
syntactic form of the utterance only, but they
provide an initial analysis which can later be
extended in annotation.
Examples of the logical form representation for
the sentence So the heart attack person can?t go
1The 5 Monroe dialogs are: s2, s4, s12, s16, s17
(TERM :VAR V3283471
:LF (LF::THE V3283471 (:* LF::PERSON PERSON) :ASSOC-WITH (V3283440))
:SEM ($ F::PHYS-OBJ (F::SPATIAL-ABSTRACTION F::SPATIAL-POINT)
(F::GROUP -) (F::MOBILITY F::NON-SELF-MOVING)
(F::FORM F::SOLID-OBJECT) (F::ORIGIN F::HUMAN)
(F::OBJECT-FUNCTION F::OCCUPATION) (F::INTENTIONAL +)
(F::INFORMATION -) (F::CONTAINER -) (F::KR-TYPE KR::PERSON)
(F::TRAJECTORY -))
:INPUT (THE HEART ATTACK PERSON))
Figure 1: Excerpt from full logical form for dialog s2 utterance 173
(UTT :TYPE UTT :SPEAKER :USER :ROOT V3286907
:TERMS
((LF::SPEECHACT V3286907 SA TELL :CONTENT V3283686 :MODS (V3283247))
(LF::F V3283247 (:* LF::CONJUNCT SO) :OF V3286907)
(LF::F V3283686 (:* LF::MOVE GO) :THEME V3283471 :MODS (V3284278)
:TMA ((TENSE PRES) (MODALITY (:* LF::ABILITY CAN)) (NEGATION +)))
(LF::THE V3283471 (:* LF::PERSON PERSON) :ASSOC-WITH (V3283440))
(LF::KIND V3283440 (:* LF::MEDICAL-CONDITION HEART-ATTACK))
(LF::F V3284278 (:* LF::TO-LOC THERE) :OF V3283686 :VAL V3286383)
(LF::IMPRO V3286383 (OR LF::PHYS-OBJECT LF::REFERENTIAL-SEM)
:CONTEXT-REL THERE))
Figure 2: Abbreviated LF representation for So the heart attack person can?t go there
Figure 3: CorpusTool Abbreviated LF View
there (dialog s2, utterance 173) is shown in Fig-
ures 1 and 2. Figure 1 shows the full term for the
noun phrase the heart attack person. It contains
the term identifier :VAR V3283471, the logical
form (:LF), the set of semantic features associated
with the term (:SEM), and the list of words associ-
ated with the term (:INPUT). The semantic features
are the domain-independent semantic properties of
words encoded in our lexicon. We use them to ex-
press selectional restrictions (Dzikovska, 2004) and
we are currently investigating their use in reference
resolution. For discourse annotation, we primarily
rely on the logical forms.
The abbreviated logical form for the sentence is
shown in Figure 2. It contains the speech act for
the utterance, SA TELL, in the first term. There
is a domain-independent term for the discourse
adverbial So2, and the term for the main event,
(LF::Move GO), which contains the tense and
modal information in the :TMA field. The phrase
the heart attack person is represented by two terms
linked together with the :ASSOC-WITH relation-
ship, to be resolved during discourse processing.
Finally, there is a term for the adverbial modifier
there, which also results in the implicit pronoun (the
2So is identified as a conjunct because it is a connective, and
its meaning cannot be identified more specifically by the parser
without pragmatic reasoning
last term in the representation) denoting a place to
which the movement is directed. The terms provide
the basic building blocks to be used in the discourse
annotation, and their unique identifiers are used as
reference indices, as discussed in the next section.
The corpus-building process consists of three
stages: initial annotation, parsing and hand-
checking. The initial annotation prepares the sen-
tences as suitable inputs to the TRIPS parser. It is
necessary because handling speech repairs and ut-
terance segmentation is a difficult task, which our
parser cannot do automatically at this point. There-
fore, we start with segmenting the discourse turns
into utterances and marking the speech repairs us-
ing our tool. We also mark incomplete and ungram-
matical utterances which cannot be successfully in-
terpreted.
Once the corpus is annotated for repairs, we use
our automated LISP testing tool to parse the en-
tire corpus. Our parser skips over the repairs we
marked, and ignores incomplete and ungrammati-
cal utterances. Then, it marks utterances ?AUTO-
GOOD? and ?AUTO-BAD? as a guideline for an-
notators. As a first approximation, the utterances
where there is a parse covering the entire utterance
are marked as ?AUTO-GOOD? and those where
there is not are marked as ?AUTO-BAD?. Then
these results are hand-checked by human annotators
using our CorpusTool to inspect the analyses and ei-
ther mark them as ?GOOD?, or mark the incorrect
parses as ?BAD?, and add a reason code explain-
ing the problem with the parse. Note that we use a
strict criterion for accuracy so only utterances that
have both a correct syntactic structure and a cor-
rect logical form can be marked as ?GOOD?. The
CorpusTool allows annotators to view the syntactic
and semantic representations at different levels of
granularity. The top-level LF tree shown in Figure
3 allows a number of crucial aspects of the repre-
sentation to be checked quickly. Note that the entity
identifiers are color-coded, which is a great help for
checking variable mappings. If everything shown
in the top-level representation is correct, the full LF
with all terms expanded can be viewed. Similarly,
levels of the parse tree can be hidden or expanded
as needed.
After the initial checking stage, we analyze the
utterances marked ?BAD? and make changes in the
grammar and lexicon to address the BAD utterances
whenever possible. Occasionally, when the prob-
lems are due to ambiguity, the parser is able to parse
the utterance, but the interpretation it selects is not
the correct one among possible alternatives. In this
case, we manually select the correct parse and add
it to the gold-standard corpus.
Once the changes have been made, we re-parse
the corpus. Our parsing tool determines automat-
ically which parses have been changed and marks
them to be re-checked by the human annotators.
The CorpusTool has the functionality to quickly
locate the utterances marked as changed for re-
checking. This allows us to quickly conduct several
iterations of re-checking and re-parsing, bringing
the coverage in the completed corpus high enough
so that it may now be annotated for reference infor-
mation. The hand-checking scheme was found to be
quite reliable, with a kappa of 0.79. Currently, 85%
of the grammatical sentences are marked as GOOD
in the gold-standard coverage of the 5 dialogs in the
Monroe corpus.
Several iterations of the check and re-parse cy-
cle were needed to achieve parsing accuracy suit-
able for discourse annotation. Once the suitable ac-
curacy level has been reached, the reference annota-
tion process starts.
3.2 Adding Reference Information
As in the parser development phase, we built a Java
tool for annotating the parsed corpora for reference.
First, the relevant terms were extracted from the
LF representation of the semantic parse. These in-
cluded all verbs, noun phrases, implicit pronouns,
etc. Next, the sentences were manually marked for
reference using the tool (PronounTool).
There are many different ways to mark how en-
tities refer. Our annotation scheme is based on the
GNOME project scheme (Poesio, 2000) which an-
notates referential links between entities as well as
their respective discourse and salience information.
The main difference in our approach is that we do
not annotate discourse units and certain semantic
features, and most of the basic syntactic and seman-
tic features are produced automatically for us in the
parsing phase.
We use standoff annotation to separate our coref-
erence annotation from the syntactic and semantic
parse annotations. The standoff file for pronouns
consists of two fields for each pronoun to handle
the reference information: relation, which specifies
how the entities are related; and refers-to, which
specifies the id of the term the referential entity in
question points to.
The focus for our work has been on coreferential
pronouns and noun phrases, although we also anno-
tated the classes of all other pronouns. Typically,
the non-coreferential pronouns are difficult to an-
notate reliably since there are a myriad of different
categories for bridging relations and for specifying
Figure 4: CorpusTool Parse View
Figure 5: Pronoun Tool
demonstrative relations (Poesio and Viera, 1998).
Because our focus was on coreferential entities, we
had our annotators annotate only the main relation
type for the non-coreferential pronouns since these
could be done more reliably. The relations we used
are listed below:
Identity both entities refer to the same object (corefer-
ence)
Dummy non-referential pronouns (expletive or pleonas-
tic)
Indexicals expressions that refer to the discourse speak-
ers or temporal relations (ie. I, you, us, now)
Action pronouns which refer to an action or event
Demonstrative pronouns that refer to an utterance or se-
ries of utterances
Functional pronouns that are indirectly related to an-
other entity, most commonly bridging and one
anaphora
Set plural pronouns that refer to a collection of men-
tioned entities
Hard pronouns that are too difficult to annotate
Entities in identity, action and functional relations
had refers-to fields that pointed to the id of a spe-
cific term (or terms if the entity was a plural com-
posed of other entities). Dummy had no refers-to
set since they were not included in the evaluation.
Demonstrative pronouns had refers-to fields point-
ing to either utterance numbers or a list of utterance
numbers in the case of a discourse segment. Finally,
there were some pronouns for which it was too dif-
ficult to decide what they referred to, if anything.
These typically were found in incomplete sentences
without a verb to provide semantic information.
After the annotation phase, a post-processing
phase identifies all the noun phrases that refer to
the same entity, and generates a unique chain-id for
this entity. This is similar to the    field in the
GNOME scheme. The advantage of doing this pro-
cessing is that it is possible for a referring expres-
sion to refer to a past instantiation that was not the
last mentioned instantiation, which is usually what
is annotated. As a result, it is necessary to mark all
coreferential instantiations with the same identifica-
tion tag.
Figure 5 shows a snapshot of the PronounTool in
use for the pronoun there in the second utterance of
our example. The top pane has buttons to skip to the
next or previous utterance with a pronoun or noun
phrase. The lower pane has the list of extracted en-
tities for easy viewing. The ?Relation? box is a drop
down menu consisting of the relations listed above.
In this case, the identity relation has been selected
for there. The next step is to select an entity from
the context that the pronoun refers to. By clicking
on the ?Refers To? box, a context window pops up
with all the entities organized in order of appear-
ance in the discourse. The user selects the entity
and clicks ?Select? and the antecedent id is added
to the refers-to field.
Our aim with this part of the project (still in a
preliminary stage) is to investigate whether a shal-
low discourse segmentation (which is generated au-
tomatically) is enough to aid in pronominal refer-
ence resolution. Previous work has focused on us-
ing complex nested tree structures to model dis-
course and dialogue. While this method may be
the best way to go ultimately, empirical work has
shown that it has been difficult to put into practice.
There are many different schemes to choose from,
for example Rhetorical Structure Theory (Mann and
Thompson, 1986) or the stack model (Grosz and
Sidner, 1986) and manually annotating with these
schemes has variable reliability. Finally, annotating
these schemes requires real-world knowledge, rea-
soning, and knowledge of salience and semantics,
all of which make automatic segmentation difficult.
However, past studies such as Tetreault and Allen
(2003) show that for reference resolution, a highly-
structured tree may be too constraining, so a shal-
lower approach may be acceptable for studying the
effect of discourse segmentation on resolution.
3.3 Discourse Segmentation
Our preliminary segmentation scheme is as follows.
In a collaborative domain, participants work on a
task until completion. During the conversation, the
participants raise questions, supply answers, give
orders or suggestions and acknowledge each other?s
information and beliefs. In our corpus, these speech
acts and discourse cues such as so and then are
tagged automatically for reliable annotation. We
use this information to decide when to begin and
end a discourse segment.
Roberts (1996) suggests that questions are good
indicators of the start of a discourse segment be-
UTT1 S so gabriela
UTT2 U yes
UTT3 S at the rochester airport there
has been a bomb attack
UTT4 U oh my goodness
UTT5 S but it?s okay
UTT6 U where is i
UTT7 U just a second
UTT8 U i can?t find the rochester air-
port
UTT9 S [ i ] it?s
UTT10 U i think i have a disability with
maps
UTT11 U have i ever told you that before
UTT12 S it?s located on brooks avenue
UTT13 U oh thank you
UTT14 S [ i ] do you see it
UTT15 U yes
Figure 6: Excerpt from dialog s2
cause they open up a topic under discussion. An an-
swer followed by a series of acknowledgments usu-
ally signal a segment close. Currently we annotate
these segments manually by maintaining a ?hold-
out? file for each dialog which contains a list of all
the segments and their start, end and type informa-
tion.
For example, given the discourse as shown in
Figure 6, the discourse segments would be Figure
7. The starts of both segments are adjacent to sen-
tences that are questions.
(SEGMENT :START utt6
:END utt13
:TYPE clarification
:COMMENTS ?has aside in middle?)
(SEGMENT :START utt10
:END utt11
:TYPE aside
:COMMENTS ?same person aside.?)
Figure 7: Discourse annotation for s2 excerpt
4 Results
Spoken dialogue is a very difficult domain to work
with because utterances are often marred with dis-
fluencies, speech repairs, and are incomplete or un-
grammatical. Speakers will interrupt each other. As
a result, many empirical methods that work well in
very formal, structured domains such as newspaper
texts or manuals tend to suffer. For example, many
leading pronoun resolution methods perform around
80% accuracy over a corpus of syntactically-parsed
Wall Street Journal articles (e.g., (Tetreault, 2001)
and (Ge et al, 1998)), but in spoken dialogue the
performance of these algorithms drops significantly
(Byron, 2002).
However, by including semantic and discourse
information, one is able to improve performance.
Our preliminary results show that using the seman-
tic feature lists associated with each entity as a fil-
ter for reference increases performance to 59% from
44%. Adding discourse segmentation boosts that
figure to 66% over some parts of the corpus.
5 Conclusion
We have presented a description of our corpus an-
notation in the Monroe domain. It is novel in that it
incorporates rich semantic information with refer-
ence and discourse information, a rarity for spoken
dialogue domains which are typically very difficult
to annotate. We expedite the annotation process and
make it more reliable by semi-automating the pars-
ing with checking and also by using two tools tai-
lored for our domain to speed up annotation. The re-
sulting corpus has several applications ranging from
overall system development to the testing of theo-
ries and algorithms of reference and discourse. Our
preliminary results demonstrate the usefulness of
the corpus.
6 Acknowledgments
Partial support for this project was provided by
ONR grant no. N00014-01-1-1015, ?Portable Di-
alog Interfaces? and NSF grant 0328810 ?Continu-
ous Understanding?.
References
T. Brants and O. Plaehn. 2000. Interactive corpus
annotation. In LREC ?00.
D. Byron. 2002. Resolving pronominal reference
to abstract entities. In ACL ?02, pages 80?87,
Philadelphia, USA.
M. O. Dzikovska, M. D. Swift, and J. F. Allen.
2004. Building a computational lexicon and on-
tology with framenet. In LREC workshop on
Building Lexical Resources from Semantically
Annotated Corpora. Lisbon, Portugal, May.
M. Dzikovska. 2004. A Practical Semantic Repre-
sentation for Natural Language Parsing. Ph.D.
thesis, U. Rochester.
N. Ge, J. Hale, and E. Charniak. 1998. A statistical
approach to anaphora resolution. Proceedings of
the Sixth Workshop on Very Large Corpora.
B. Grosz and C. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
P. Heeman and J. Allen. 1994. The TRAINS93 di-
alogues. Technical Report TRAINS TN 94-2, U.
Rochester.
W. Mann and S. Thompson. 1986. Rhetori-
cal structure theory: Descripton and construc-
tion of text. Technical Report ISI/RS-86-174,
USC/Information Sciences Institute, October.
M. P. Marcus, B Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19:313?330.
M. Moser and J.D. Moore. 1996. Toward a synthe-
sis of two accounts of discourse structure. Com-
putational Linguistics, 22(3):409?419.
M. Poesio and R. Viera. 1998. A corpus-based in-
vestigation of definite description use. Computa-
tional Linguistics, 24(2):183?216.
M. Poesio. 2000. Annotating a corpus to develop
and evaluate discourse entity realization algo-
rithms: issues and preliminary results. In LREC
?00, Athens.
C. Roberts. 1996. Information structure in dis-
course. Papers in Semantics, 49:43?70. Ohio
State Working Papers in Linguistics.
A. Stent. 2001. Dialogue Systems as Conversa-
tional Partners. Ph.D. thesis, U. Rochester.
M. Swift, M. Dzikovska, J. Tetreault, and James F.
Allen. 2004. Semi-automatic syntactic and se-
mantic corpus annotation with a deep parser. In
LREC?04, Lisbon.
J. Tetreault and J. F. Allen. 2003. An empiri-
cal evaluation of pronoun resolution and clausal
structure. In 2003 International Symposium on
Reference Resolution and its Applications to
Question Answering and Summarization, pages
1?8, Venice, Italy.
J. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics, 27(4):507?520.
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 194?195,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
TFLEX: Speeding up Deep Parsing with Strategic Pruning
Myroslava O. Dzikovska
Human Communication Research Centre
University of Edinburgh
Edinburgh, EH8 9LW, UK
mdzikovs@inf.ed.ac.uk
Carolyn P. Rose
Carnegie Mellon University
Language Technologies Institute
Pittsburgh PA 15213, USA
cprose@cs.cmu.edu
1 Introduction
This paper presents a method for speeding up a
deep parser through backbone extraction and prun-
ing based on CFG ambiguity packing.1 The TRIPS
grammar is a wide-coverage grammar for deep nat-
ural language understanding in dialogue, utilized in
6 different application domains, and with high cov-
erage and sentence-level accuracy on human-human
task-oriented dialogue corpora (Dzikovska, 2004).
The TRIPS parser uses a best-first beam search al-
gorithm and a chart size limit, both of which are a
form of pruning focused on finding an n-best list of
interpretations. However, for longer sentences lim-
iting the chart size results in failed parses, while in-
creasing the chart size limits significantly impacts
the parsing speed.
It is possible to speed up parsing by implement-
ing faster unification algorithms, but this requires
considerable implementation effort. Instead, we de-
veloped a new parser, TFLEX, which uses a sim-
pler technique to address efficiency issues. TFLEX
combines the TRIPS grammar with the fast parsing
technologies implemented in the LCFLEX parser
(Rose? and Lavie, 2001). LCFLEX is an all-paths
parser which uses left-corner prediction and ambi-
guity packing, and which was shown to be efficient
on other unification augmented context-free gram-
mars. We describe a way to transfer the TRIPS
grammar to LCFLEX, and a pruning method which
achieves significant improvements in both speed and
coverage compared to the original TRIPS parser.
1This material is based on work supported by grants from
the Office of Naval Research under numbers N000140510048
and N000140510043.
2 TFLEX
To use the TRIPS grammar in LCFLEX we first ex-
tracted a CFG backbone from the TRIPS grammar,
with CFG non-terminals corresponding directly to
TRIPS constituent categories. To each CFG rule
we attach a corresponding TRIPS rule. Whenever
a CFG rule completes, a TRIPS unification function
is called to do all the unification operations associ-
ated with the TRIPS rule. If the unification fails, the
constituent built by the CFG is cancelled.
The TFLEX pruning algorithm uses ambiguity
packing to provide good pruning points. For exam-
ple, in the sentence ?we have a heart attack victim
at marketplace mall? the phrase ?a heart attack vic-
tim? has two interpretations depending on whether
?heart? modifies ?attack? or ?attack victim?. These
interpretations will be ambiguity packed in the CFG
structure, which offers an opportunity to make prun-
ing more strategic by focusing specifically on com-
peting interpretations for the same utterance span.
For any constituent where ambiguity-packed non-
head daughters differ only in local features, we
prune the interpretations coming from them to a
specified prune beam width based on their TRIPS
scores. In the example above, pruning will happen
at the point of making a VP ?have a heart attack vic-
tim?. The NP will be ambiguity packed, and we will
prune alternative VP interpretations resulting from
combining the same sense of the verb ?have? and
different interpretations of the NP.
This approach works better than the original
TRIPS best-first algorithm, because for long sen-
tence the TRIPS chart contains a large number
194
of similar constituents, and the parser frequently
reaches the chart size limit before finding the correct
constituent to use. Ambiguity packing in TFLEX
helps chose the best constituents to prune by prun-
ing competing interpretations which cover the same
span and have the same non-local features, thus
making it less likely that a constituent essential for
building a parse will be pruned.
3 Evaluation
Our evaluation data is an excerpt from the Monroe
corpus that has been used in previous TRIPS re-
search on parsing speed and accuracy (Swift et al,
2004). The test contained 1042 utterances, from 1
to 45 words in length (mean 5.38 words/utt, st. dev.
5.7 words/utt). Using a hold-out set, we determined
that a beam width of 3 was an optimal setting for
TFLEX. We then compared TFLEX at beam width
3 to the TRIPS parser with chart size limits of 1500,
5000, and 10000. As our evaluation metrics we re-
port are average parse time per sentence and proba-
bility of finding at least one parse, the latter being a
measure approximating parsing accuracy.
The results are presented in Figure 1. We grouped
sentences into equivalence classes based on length
with a 5-word increment. On sentences greater
than 10 words long, TFLEX is significantly more
likely to produce a parse than any of the TRIPS
parsers (evaluated using a binary logistic regression,
p < .001). Moreover, for sentences greater than
20 words long, no form of TRIPS parser returned
a complete parse. TFLEX is significantly faster
than TRIPS-10000, statistically indistinguishable in
terms of parse time from TRIPS-5000, and signifi-
cantly slower than TRIPS-1500 (p < .001).
Thus, TFLEX presents a superior balance of cov-
erage and efficiency especially for long sentences
(10 words or more) since for these sentences it is
significantly more likely to find a parse than any ver-
sion of TRIPS, even a version where the chart size is
expanded to an extent that it becomes significantly
slower (i.e., TRIPS-10000).
4 Conclusions
In this paper, we described a combination of effi-
cient parsing techniques to improve parsing speed
and coverage with the TRIPS deep parsing grammar.
Figure 1: Parse times and probability of getting a
parse depending on (aggregated) sentence lengths.
5 denotes sentences with 5 or fewer words, 25 sen-
tences with more than 20 words.
The TFLEX system uses an all-paths left-corner
parsing from the LCFLEX parser, made tractable
by a pruning algorithm based on ambiguity packing
and local features, generalizable to other unification
grammars. Our pruning algorithm provides a bet-
ter efficiency-coverage balance than best-first pars-
ing with chart limits as utilised by the TRIPS parser.
References
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
C. P. Rose? and A. Lavie. 2001. Balancing robustness
and efficiency in unification-augmented context-free
parsers for large practical applications. In J.C. Junqua
and G Van Noord, editors, Robustness in Language
and Speech Technology. Kluwer Academic Press.
M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in
the parser: Using a shallow parser to improve deep
parsing. In Proceedings of COLING-04.
J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and J.
Allen. 2004. Discourse annotation in the monroe cor-
pus. In ACL-04 workshop on Discourse Annotation.
195
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 196?197,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Generic parsing for multi-domain semantic interpretation
Myroslava Dzikovska?, Mary Swift?, James Allen?, William de Beaumont?
? Human Communication Research Centre
University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdom
m.dzikovska@ed.ac.uk
? Department of Computer Science University of Rochester, Rochester, NY 14627-0226
{swift, james, wdebeaum}@cs.rochester.edu
1 Introduction
Producing detailed syntactic and semantic represen-
tations of natural language is essential for prac-
tical dialog systems such as plan-based assistants
and tutorial systems. Development of such systems
is time-consuming and costly as they are typically
hand-crafted for each application, and dialog corpus
data is more difcult to obtain than text. The TRIPS
parser and grammar addresses these issues by pro-
viding broad coverage of common constructions in
practical dialog and producing semantic representa-
tions suitable for dialog processing across domains.
Our system bootstraps dialog system development
in new domains and helps build parsed corpora.1
Evaluating deep parsers is a challenge (e.g., (Ka-
plan et al, 2004)). Although common bracketing
accuracy metrics may provide a baseline, they are
insufcient for applications such as ours that require
complete and correct semantic representations pro-
duced by the parser. We evaluate our parser on
bracketing accuracy against a statistical parser as a
baseline, then on a word sense disambiguation task,
and nally on full sentence syntactic and semantic
accuracy in multiple domains as a realistic measure
of system performance and portability.
2 The TRIPS Parser and Logical Form
The TRIPS grammar is a linguistically motivated
unication formalism using attribute-value struc-
1We thank 4 anonymous reviewers for comments.
This material is based on work supported by grants from
ONR #N000149910165, NSF #IIS-0328811, DARPA
#NBCHD030010 via subcontract to SRI #03-000223 and
NSF #E1A-0080124.
(SPEECHACT sa1 SA REQUEST :content e123)
(F e123 (:* LF::Fill-Container Load)
:Agent pro1 :Theme v1 :Goal v2)
(IMPRO pro1 LF::Person :context-rel *YOU*)
(THE v1 (SET-OF (:* LF::Fruit Orange)))
(THE v2 (:* LF::Vehicle Truck))
Figure 1: LF for Load the oranges into the truck.
tures. An unscoped neo-Davidsonian semantic rep-
resentation is built in parallel with the syntactic
representation. A sample logical form (LF) rep-
resentation for Load the oranges into the truck is
shown above. The TRIPS LF provides the neces-
sary information for reference resolution, surface
speech act analysis, and interpretations for a wide
variety of fragmentary utterances and conventional
phrases typical in dialog. The LF content comes
from a domain-independent ontology adapted from
FrameNet (Johnson and Fillmore, 2000; Dzikovska
et al, 2004) and linked to a domain-independent lex-
icon (Dzikovska, 2004).
The parser uses a bottom-up chart algorithm with
beam search. Alternative parses are scored with fac-
tors assigned to grammar rules and lexical entries by
hand, because due to the limited amount of corpus
data we have not yet been able to train a statistical
model that outperforms our hand-tuned factors.
3 Evaluation
As a rough baseline, we compared the bracketing
accuracy of our parser to that of a statistical parser
(Bikel, 2002), Bikel-M, trained on 4294 TRIPS
196
parse trees from the Monroe corpus (Stent, 2001),
task-oriented human dialogs in an emergency res-
cue domain. 100 randomly selected utterances were
held out for testing. The gold standard for evalu-
ation is created with the help of the parser (Swift
et al, 2004). Corpus utterances are parsed, and the
parsed output is checked by trained annotators for
full-sentence syntactic and semantic accuracy, reli-
able with a kappa score 0.79. For test utterances
for which TRIPS failed to produce a correct parse,
gold standard trees were manually constructed inde-
pendently by two linguists and reconciled. Table 1
shows results for the 100 test utterances and for the
subset for which TRIPS nds a spanning parse (74).
Bikel-M performs somewhat better on the bracket-
ing task for the entire test set, which includes utter-
ances for which TRIPS failed to nd a parse, but it
is lower on complete matches, which are crucial for
semantic interpretation.
All test utts (100) Spanning parse utts (74)
R P CM R P CM
BIKEL-M 79 79 42 89 88 54
TRIPS 77 79 65 95 95 86
Table 1: Bracketing results for Monroe test sets (R:
recall, P: precision, CM: complete match).
Word senses are an important part of the LF rep-
resentation, so we also evaluated TRIPS on word
sense tagging against a baseline of the most common
word senses in Monroe. There were 546 instances of
ambiguous words in the 100 test utterances. TRIPS
tagged 90.3% (493) of these correctly, compared to
the baseline model of 75.3% (411) correct.
To evaluate portability to new domains, we com-
pared TRIPS full sentence accuracy on a subset
of Monroe that underwent a fair amount of devel-
opment (Tetreault et al, 2004) to corpora of key-
board tutorial session transcripts from new domains
in basic electronics (BEETLE) and differentiation
(LAM) (Table 2). The only development for these
domains was addition of missing lexical items and
two grammar rules. TRIPS full accuracy requires
correct speech act, word sense and thematic role as-
signment as well as complete constituent match.
Error analysis shows that certain senses and sub-
categorization frames for existing words are still
Domain Utts Acc. Cov. Prec.
Monroe 1576 70% 1301 84.1%
BEETLE 192 50% 129 75%
LAM 934 42% 579 68%
Table 2: TRIPS full sentence syntactic and semantic
accuracy in 3 domains (Acc: full accuracy; Cov.: #
spanning parses; Prec: full acc. on spanning parses).
needed in the new domains, which can be rectied
fairly quickly. Finding and addressing such gaps is
part of bootstrapping a system in a new domain.
4 Conclusion
Our wide-coverage grammar, together with a
domain-independent ontology and lexicon, produces
semantic representations applicable across domains
that are detailed enough for practical dialog applica-
tions. Our generic components reduce development
effort when porting to new dialog domains where
corpus data is difcult to obtain.
References
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In HLT-2002.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004.
Building a computational lexicon and ontology with
framenet. In LREC workshop on Building Lexical Re-
sources from Semantically Annotated Corpora.
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
C. Johnson and C. J. Fillmore. 2000. The FrameNet
tagset for frame-semantic and syntactic coding of
predicate-argument structure. In ANLP-NAACL 2000.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell III,
A. Vasserman, and R. S. Crouch. 2004. Speed and
accuracy in shallow and deep stochastic parsing. In
HLT-NAACL 2004.
A. J. Stent. 2001. Dialogue Systems as Conversational
Partners. Ph.D. thesis, University of Rochester.
M. D. Swift, M. O. Dzikovska, J. R. Tetreault, and J. F.
Allen. 2004. Semi-automatic syntactic and semantic
corpus annotation with a deep parser. In LREC-2004.
J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and
J. Allen. 2004. Discourse annotation in the Monroe
corpus. In ACL workshop on Discourse Annotation.
197
Interpretation and Generation in a Knowledge-Based Tutorial System
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW,
United Kingdom,
{mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk
Abstract
We discuss how deep interpretation and
generation can be integrated with a know-
ledge representation designed for question
answering to build a tutorial dialogue sys-
tem. We use a knowledge representa-
tion known to perform well in answering
exam-type questions and show that to sup-
port tutorial dialogue it needs additional
features, in particular, compositional rep-
resentations for interpretation and struc-
tured explanation representations.
1 Introduction
Human tutoring is known to help students learn
compared with reading textbooks, producing up to
two standard deviations in learning gain (Bloom,
1984). Tutorial systems, in particular cognitive
tutors which model the inner state of a student?s
knowledge, help learning but result in only up to 1
standard deviation learning gain (Anderson et al,
1995). One current research hypothesis is that this
difference is accounted for by interactive dialogue,
which allows students to ask questions freely, and
tutors to adapt their direct feedback and presenta-
tion style to the individual student?s needs.
Adding natural language dialogue to a tutorial
system is a complex task. Many existing tuto-
rial dialogue systems rely on pre-authored curricu-
lum scripts (Person et al, 2000) or finite-state ma-
chines (Rose? et al, 2001) without detailed knowl-
edge representations. These systems are easy to
design for curriculum providers, but offer limited
flexibility because the writer has to predict all pos-
sible student questions and answers.
We argue that the ability to interpret novel,
context-dependent student questions and answers,
and offer tailored feedback and explanations is
important in tutorial dialogue, and that a domain
knowledge representation and reasoning engine is
necessary to support these applications. We dis-
cuss our knowledge representation, and the issues
of integrating it with state-of-the-art interpretation
and generation components to build a knowledge-
based tutorial dialogue system.
Our application domain is in basic electricity
and electronics, specifically teaching a student
how to predict the behavior and interpret measure-
ments in series and parallel circuits. This is a con-
ceptual domain - that is, students are primarily fo-
cused on learning concepts such as voltage and
current, and their relationships with the real world.
The students use a circuit simulator to build cir-
cuits, and their questions and answers depend on
the current context.
There are various sources of context-
dependency in our domain. Students and
tutors refer to specific items in the simulation
(e.g., ?Which lightbulbs will be lit in these
circuits??), and may phrase their answers in an
unexpected way, for example, by saying ?the
lightbulbs in 2 and 4 will be out? instead of
naming the lit lightbulbs. Moreover, students
may build arbitrary circuits not included in the
question, either because they make mistakes, or
because a tutor instructs them to do so as part of
remediation. Thus it would be difficult to produce
and maintain a finite-state machine to predict all
possible situations, both for interpreting the input
and for generating feedback based on the state
of the environment and the previous dialogue
context: a domain reasoner is necessary to handle
such unanticipated situations correctly.
We describe a tutorial system which uses a de-
scription logic-based knowledge representation to
4 KRAQ06
generate intelligent explanations and answers to
a student?s questions, as well as to interpret the
student?s language at all stages of the dialogue.
Our approach relies on using an existing wide-
coverage parser for domain-independent syntactic
parsing and semantic interpretation, as well as a
wide-coverage deep generation system. We dis-
cuss the issues which arise in connecting such re-
sources to a domain knowledge representation in a
practical system.
2 Motivation
A good teaching method for basic electricity
and electronics is eliciting cognitive dissonance
(Schaffer and McDermott, 1992; Arnold and
Millar, 1987) which we are implementing as a
?predict-verify-evaluate? (PVE) cycle. The stu-
dents are asked to make predictions about the be-
havior of a schematic circuit and then build it in a
simulation environment. If the observed results do
not match their predictions, a discussion ensues,
where the computer tutor helps a student learn the
relevant concepts. The PVE exercises are comple-
mented with exercises asking the students to iden-
tify properties of circuits in diagrams and to inter-
pret a circuit?s behavior.
Thus, the system has to answer questions about
circuits which students build and manipulate dy-
namically in a simulation environment, and pro-
duce explanations and feedback tailored to that in-
dividual context. This relies on the following sys-
tem capabilities:
? Understanding and giving explanations.
Since the system relies on inducing cognitive
dissonance, it should be able to explain to the
student why their prediction for a specific cir-
cuit was incorrect, and also verify explana-
tions given by a student.
? Unrestricted language input with reference
resolution. Similar to other conceptual do-
mains (VanLehn et al, 2002) the language
observed in corpus studies is varied and syn-
tactically complex. Additionally, in our do-
main students refer to items on screen, e.g.
?the lightbulb in 5?, which requires the sys-
tem to make the connection between the lan-
guage descriptions and the actual objects in
the environment.
? Tailored generation. The level of detail in
the explanations offered should be sensitive
to student knowledge of the domain. Tutorial
utterances should be natural and use correct
terminology even if a student doesn?t.
To support answering questions and giving ex-
planations, we chose the KM knowledge represen-
tation environment (Clark and Porter, 1999) as a
basis for our implementation. KM is a description-
logic based language which has been used to rep-
resent facts and rules in a HALO system for AP
chemistry tests (Barker et al, 2004). It supports
the generation of explanations and obtained the
highest explanation scores in an independent eval-
uation based on an AP chemistry exam (Friedland
et al, 2004). Thus it is a good choice to provide
reasoning support for explanations and answering
novel questions in a tutorial system. However, KM
has not been used previously in connection with
natural language input for question answering, and
we discuss how the limitations of KM representa-
tions affect the interpretation process in Section 4.
We use a deep domain-independent parser and
grammar to support language interpretation, and
a deep generator to provide natural sounding and
context-dependent text. Both deep parsing and
generation provide the context adaptivity we need,
but they are time-consuming to build for a spe-
cific domain. Now that a number of deep domain-
independent parsing and generation systems are
available in the community, our research goal is to
investigate the issues in integrating them with the
knowledge representation for question answering
to support the requirements of a tutorial dialogue
system. We focus on context-dependent explana-
tion understanding and generation as a primary tu-
toring task in our domain. Section 3 discusses
our representations, Section 4 presents the issues
arising in knowledge representation to support in-
terpretation, and Section 5 discusses the require-
ments for appropriate explanation generation and
how it can be integrated into the system.
3 Representations
From the point of view of tutoring, the most im-
portant requirement on the knowledge representa-
tion is that system reasoning should closely match
human reasoning, so that it can be explained to
students in meaningful terms. Thus, for exam-
ple, a numerical circuit simulator is well suited for
dynamically displaying circuit behaviors, but not
for conceptually tutoring basic circuits, because it
5 KRAQ06
hides physics principles behind complex mathe-
matical equations that are not suitable for learners.
To design our knowledge representation we
started with a set of lessons for our domain de-
signed by psychologists experienced in designing
training courses for physics and simulated envi-
ronments. The lessons were used in a data col-
lection environment with experienced tutors con-
ducting tutoring sessions over a text chat interface.
Each student and tutor were required to go through
the materials presented as a set of slides and solve
pre-defined exercises, but the students asked ques-
tions to get help with problem-solving. The tutor
had complete freedom to choose how to answer
student questions and how to remediate when stu-
dents made mistakes. We are using this data set
to study the types of errors that students make as
well as the language used by both students and tu-
tors. The latter serves as a guide to developing our
interpretation and generation components.
In addition to the set of materials, the course
designers provided a ?glossary? of concepts and
facts that students need to learn and use in expla-
nations, containing approximately 200 concepts
and rules in a form which should be used in model
explanations. We then developed our knowledge
representation so that concepts listed in the glos-
sary were represented as KM concepts, and facts
are represented as rules for computing slots.
An example KM representation for our domain
is shown in Figure 1. It represents the fact that a
lightbulb will be on if it is in a complete path (i.e. a
closed path containing a battery). The explanation
is generated using the comment structure [light-
bulbstate] shown in Figure 2 (slightly simplified
for readability). Explanations are generated sepa-
rately from reasoning in the KM system because
reasoning in general contains too many low-level
details. For example, our rule for computing a
lightbulb state includes two facts: that the light-
bulb has to be in a complete path with a battery,
and that a lightbulb is always in one state only (i.e.
it cannot be broken and on at the same time). The
latter is required for proof completeness, but is too
trivial to be mentioned to students. KM therefore
requires knowledge engineers to explicitly desig-
nate the facts to be used in explanations.
This representation allows KM to generate de-
tailed explanations by using a template string and
then explaining the supporting facts. An example
of a full explanation, together with the adjustments
needed to use it in dialogue rather than as a com-
plete answer, is given in Section 5.
Currently, KM only supports generating expla-
nations, but not verifying them. The explanation
mechanism produces explanations as text directly
from the knowledge representation, as shown in
Figure 2(a). This generation method is not well
suited for a tutorial dialogue system, because it
does not take context into account, as discussed
in Section 5. Therefore, we are designing a struc-
tured representation for explanations to be pro-
duced by the KM explanation mechanism instead
of using English sentences, shown in Figure 2(b).
This will allow us to generate more flexible ex-
planations (Section 5) and also to interpret student
explanations (Section 4).
4 Interpretation
The interpretation process consists of parsing,
reference resolution, dialogue act recognition
and diagnosing student answers. We discuss
reference resolution and diagnosis here as these
are the two steps impacted by the knowledge
representation issues. As a basic example we will
use the student answer from the following pair:1
Problem: For each circuit, which lightbulbs will
be lit? Explain.
Student: the bulbs in 1 and 3 are lit because they
are in a closed path with a battery
To respond to this answer properly, the system
must complete at least the following tasks. First, it
must resolve ?the bulbs in 1 and 3? to correspond-
ing object IDs in the knowledge base, for exam-
ple, LB-13-1-1 and LB-13-3-1. Then, it must ver-
ify that the student statement is factually correct.
This includes verifying that the lightbulbs in 1 and
3 will be lit, and that each of them is in a closed
path with a battery. Finally, it must verify that
the student explanation is correct. This is sepa-
rate from verifying factual correctness. For exam-
ple, a statement ?because they are in a closed path?
is true for both of those lightbulbs, but it is not a
complete explanation, because a lightbulb may be
in a closed path which does not contain a battery,
where it won?t be lit.
1These utterances come from our corpus, though most of
the student answers are not as easy to parse. We are working
on robust parsing methods to address the issues in parsing
less coherent utterances.
6 KRAQ06
(every LightBulb has
(state ((must-be-a Electrical-Usage-State) (exactly 1 Electrical-Usage-State)
(if (the is-damaged of Self) then *Broken-Usage-State
else (if (has-value (oneof ?batt in (the powered-by of Self)))
where ((the state of ?batt) = *Charged-Power-State)))
then *On-Usage-State else *Off-Usage-State) [lightbulbstate])))
Figure 1: The representation of a lightbulb in our KM database
(a)(comment [lightbulbstate]
(:sentence (?a working lightbulb is on if it is in a complete path with a charged battery?))
(:supporting-facts (:triple Self powered-by *) (forall (the powered-by of Self) (:triple It state *)))
(b)(comment [lightbulbstate]
(:rule :object LightBulb :fact (?lb state *On-Usage-State)
:requires ((?lb powered-by ?v1) (?v1 instance-of Battery) (?v1 state *Charged-Power-State))
:bindings ((?lb ? Self ?) (?v1 ? the powered-by of Self ?)) )
Figure 2: A sample comment structure to generate an explanation for a lit lightbulb (a) The KM text
template (b) a new structured representation. Items in angle brackets are computed dynamically
4.1 Interpreting Factual Statements
We use the TRIPS dialogue parser (Dzikovska,
2004) for interpretation. The TRIPS parser pro-
vides a two-layer architecture where the utter-
ance meaning is represented using a domain-
independent semantic ontology and syntax. The
domain-independent representation is used for dis-
course processing tasks such as reference reso-
lution, but it is connected to the domain-specific
knowledge representation by mapping between
the domain-independent and domain-specific on-
tologies (Dzikovska et al, 2003; Dzikovska,
2004). This architecture allows us to separate lin-
guistic and domain-specific knowledge and easily
specialize to new domains.
When applied in our domain, the TRIPS inter-
pretation architecture was helpful in getting the
interpretation started quickly, because we only
needed to extend the lexicon with specific terms
related to basic electricity and electronics (e.g.,
?multimeter?), while other lexical items and syn-
tactic constructions were provided in the domain-
independent part.
The reference resolution module operates on
TRIPS domain-independent representations send-
ing queries to KM as necessary, because the
TRIPS representations offer linguistic features to
guide reference resolution not available in the rep-
resentations used for reasoning. We use a recur-
sive reference resolution algorithm similar to By-
ron (2002) which first resolves ?1 and 3? are re-
solved as names for Circuit-13-1 and Circuit-13-
3,2 and then queries KM to find all lightbulbs in
those circuits. Dialogue context is used to inter-
pret the reference resolution results. In this case,
the context does not matter because the question
sets up all lightbulbs on screen as contextually rel-
evant. But if the student had said ?the ones in 1
and 3?, the query would be for all components in
circuits 1 and 3, and then our algorithm will filter
the query results based on the question context to
retain only lightbulbs.
Once the references are resolved, the whole sen-
tence is converted to a KM statement which repre-
sents the student utterance, in our case (the state
of LB13-1-1) = *On-Usage-State, where LB13-1-
1 is the lightbulb obtained by reference resolution.
This statement is sent to the KM system, which
verifies that it is correct. This procedure allows us
to use dialogue context in understanding, and also
to check correctness of answers easily, even if they
are phrased in an unanticipated way.
However, even with the layer of separation of
linguistic and domain knowledge provided by the
TRIPS architecture, we found that the need to sup-
port interpretation in a compositional way influ-
ences the interaction with knowledge representa-
tion. There are many ways to express the same
query to KM, which differ in efficiency. Two ex-
2This step is not trivial, because on other slides the label
?1? refers to terminals or other components rather than whole
circuits, and therefore there is no 1-to-1 correspondence be-
tween names and objects in the environment.
7 KRAQ06
(a) (allof ?x in (the all-instances of LightBulb) where ((the components of Circuit-13-1) include ?x))
(allof ?x (LightBulb ?x) and (components Circuit-13-1 ?x))
(b) (allof ?comp in (the components of Circuit-13-1) where (?comp isa LightBulb))
(allof ?x (components Circuit-13-1 ?x) and (LightBulb ?x) )
Figure 3: KM Queries to to retrieve all lightbulbs in a circuit with corresponding first-order logic glosses.
ample queries to ask the same question are given in
Figure 3. While their first order logic semantics is
equivalent except for the order of conjuncts, they
are expressed in a very different way in the KM
syntax. Version (b) is more efficient to ask, be-
cause it retrieves the components of circuit 1 first,
a smaller set than the set of all lightbulbs.
This asymmetry presents a challenge to both
language interpretation and knowledge engineer-
ing. Existing reference resolution algorithms (By-
ron, 2002; Bos, 2004) expect the queries for ?the
lightbulb? and ?the lightbulb in 1? to be strictly
compositional in the sense that the phrase ?the
lightbulb? will be represented identically in both
cases, and ?in 1? is represented as an additional
constraint on the lightbulbs. This corresponds to
the query variant (a) in the system. Otherwise
a large amount of query-specific transformations
may be required to produce queries for complex
noun phrase descriptions, diminishing the scala-
bility of the approach.
We had to spend a significant portion of time
in the project developing an efficient and com-
positional knowledge representation. Our cur-
rent solution is to prefer compositionality over ef-
ficiency, even though it impacts performance in
some cases, but we are working on a more gen-
eral solution. Instead of converting directly to
KM from domain-independent language represen-
tations, we will convert all queries in a FOL-like
syntax shown in Figure 3 which uses concepts
from the KM representation, but where all con-
juncts are treated identically in the syntax. The
problem of converting this representation to the
optimal KM form can then be seen as an instance
of query optimization. For example, we can re-
order the conjuncts putting the relations which in-
clude an instance constant (e.g., (the components
of Circuit-13-1)) first in the query, because they
are more likely to limit the search to small sets
of objects. This representation can be easily con-
verted in the KM syntax, and is also useful for
explanation understanding and generation as dis-
cussed below.
4.2 Explanation Understanding
While KM has facilities for generating explana-
tions, it does not have support for reading in a stu-
dent explanation and verifying it. We devised a
method to support this functionality with the aid of
KM explanation generation mechanism. Any time
a student offers an explanation, the KM reasoner
will be called to generate its own explanation for
the same fact, in the structured format shown in
Figure 2(b). Then the student explanation (con-
verted into the same intermediate syntax) can be
matched against the KM-generated explanation to
verify that it is complete, or else that certain parts
are missing.
In our example, the student explanation ?be-
cause they are in a closed path with a battery? will
be represented as (?pa instance-of Path) (?pa is-
closed t) (?b instance-of Battery) (?pa contains
?b) (?pa contains LB-13-1-1).3 This explanation
does not directly match into the explanation struc-
ture from Figure 2(b), because it uses the more
specific term ?in closed path with a battery? rather
than the more general term ?in complete path?
(represented by the powered-by slot). However,
as part of generating the explanation, an explana-
tion structure for the powered-by will be gener-
ated, and it will include the facts (?pa is-closed
t) (?pa contains ?b). This will match the student
explanation. It will be up to the tutorial module to
decide whether to accept the explanation ?as is?,
or lead the student to use the more precise termi-
nology, as discussed in Section 5.
This method can address student explanations
as long as they correspond to parts of typical ex-
planations, and identify missing parts. The biggest
open problem we face is equivalent inferences.
For example, a student may say ?A lightbulb is
not on? instead of ?a lightbulb is off?. KM rea-
soning handles those differences when verifying
factual correctness, but KM does not support sim-
ilar reasoning for matching explanations (which
3Here ?they? would be resolved first to a set of lightbulbs,
and each instance will be treated separately to verify that the
explanation applies.
8 KRAQ06
would correspond to verifying full proofs rather
than individual facts). We are considering bring-
ing a theorem prover to reason over intermediate
representations together with KM axioms to help
interpret explanations, as done in (Makatchev et
al., 2004; Bos, 2005).
5 Generation
The task of the utterance generation component is
to produce tutorial dialogue, such as asking new
questions of the student, conveying the correctness
of their answers, and giving explanations. Expla-
nations may be given in response to a student?s di-
rect ?why? question or when a student has erred
and the pedagogical reasoner has decided that an
explanation is the best remediation strategy. In
each case, the utterance generator must not only
provide a correct, thorough and coherent explana-
tion, but must tailor it so that the student doesn?t
receive too much or too little information. To
be tailorable, explanations must be derived from
the represented domain knowledge and from what
the tutoring system knows about the student (e.g.,
their recent performance).
Directly producing explanations by appending
together pieces of hand-written strings as in Fig-
ure 2(a) usually results in long explanations that
contain little detail of interest to the student. Fig-
ure 4 contains one such example explanation gen-
erated by the KM system in our domain and de-
rived from a query based on the production rule in
Figure 1. This explanation makes sense in answer-
ing an exam question, as intended in the KM sys-
tem, but it is not necessarily helpful in dialogue.
As an example, suppose the student had incor-
rectly answered the question in Section 4, and the
tutoring system decides to correctly explain why
the lightbulbs are lit. Usually, a full explanation
is not necessary in these cases. In the case where
a student gave an incomplete explanation, namely
leaving out the necessary mention of the battery,
a simple response of the form ?Yes, but don?t for-
get the battery? will be infinitely more helpful than
the full explanation. If the student?s explanation is
completely correct, but they have failed to notice
a change in the environment, the more appropriate
explanation is ?The lightbulb is in a closed path,
as well as the battery, but the battery is not oper-
ational?. Furthermore, if a student has shown that
they are knowledgeable about certain fundamen-
tal facts, such as what states a lightbulb may be
in, statements like ?A lightbulb can be on, off or
broken? should be removed.
Adding this reasoning directly to the knowledge
base would make it unwieldy and unmodifiable,
and the string-based generation in KM comments
does not allow for adapting explanations based on
external knowledge such as a student model. To
adapt the KM explanation mechanism to support
such context-dependent generation, instead of cre-
ating explanations via template strings, we have
devised the representation presented in Figure 2(b)
that is based on semantics and allows us to mod-
ify an explanation after it has been produced by
the KM reasoning process but before it has been
converted into a string representation.
Based on this semantic representation, explana-
tion content can be selected more appropriately. If
the interpreter discussed in Section 4.2 determines
that parts of the explanation from the :requires
field are missing, the generation can focus only on
that part of the explanation. The requirements list
would also be used to determine if the student is
not aware of environment properties, such as that a
battery is damaged. Finally, the facts known to the
student can be removed if the corresponding se-
mantic forms were used in previous explanations.
In addition to selecting the explanation content
properly, it is important that the responses given to
the student sound fluid and are easy to understand.
In dialogue, in particular, it is important that pro-
nouns can be generated based on references im-
portant for the student, and avoid repetitiveness in
syntax. Knowledge of linguistic features such as
number and gender, and also knowledge of what
was previously mentioned in the discourse, is nec-
essary to support such natural text generation.
Deep generation utilizes this represented
knowledge along with grammatical and lexical
knowledge of a language, rather than hand-written
strings, to produce utterances. Our current im-
plementation uses a custom utterance generation
component and the STORYBOOK (Callaway and
Lester, 2002) deep text generator modified to
work in a dialogue context. Once the explanation
content is selected, it is passed to the STORYBOOK
system to produce the actual utterance text.
6 Discussion and Related Work
Existing tutorial dialogue systems most often rely
on one of two approaches for interpretation: they
either use wide coverage but shallow language
9 KRAQ06
A lightbulb can be on, off or broken.
A working lightbulb is on if it is in a complete path with a charged battery.
The complete paths of a component are those which are valid, closed, and complete.
A path is complete if it is a closed path with at least one battery and at least...
A path is closed if it is a valid path, a sequence of more than two terminals, ...
A path is valid if it is a single sequence with more than one terminal, all ...
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is valid.
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is closed.
... 6 lines showing that the path contains both L1-13-1-1 and B1-13-1-1 ...
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is complete.
L1-13-1-1 is in a complete path with B1-13-1-1.
A battery is charged unless it is damaged.
B1-13-1-1 is charged.
L1-13-1-1 is on.
Figure 4: Untailored text produced by appending strings from production rules.
interpretation techniques (e.g. LSA (Person et
al., 2000), finite-state parsing (Glass, 2001)) in
combination with shallow knowledge represen-
tations (tutorial scripts or FSA-based knowledge
construction dialogues), or they use deep KR&R
systems but with highly domain-specific parsing
and semantic interpretation (e.g. ATLAS-ANDES
(Rose? et al, 2001), PACT (Aleven et al, 2002)).
The Why2-Atlas system (VanLehn et al, 2002)
makes progress on combining wide coverage in-
terpretation with deep knowledge representation
by utilizing a wide-coverage syntactic grammar
(Rose?, 2000) and a theorem prover to interpret stu-
dent essays (Makatchev et al, 2004). However,
once the misconceptions are diagnosed, the reme-
diation is done via KCDs, with very limited lan-
guage input and pre-authored responses, and with-
out allowing students to ask questions. Our ap-
proach attempts to address issues which arise in
making remediation more flexible and dependent
on context, while still relying on wide-coverage
language interpretation and generation.
The issues we encountered in integrating com-
positional interpretation and reference resolution
with efficient knowledge representation is simi-
lar to a known problem in natural language inter-
faces to databases which may contain slots with
complex meanings. (Stallard, 1986) solves this
problem by providing inference schemas linking
complex-valued slots with compositional repre-
sentations. Our solution in mapping domain-
independent to domain-specific representation is
similar, but stricter compositionality is needed for
reference resolution support, placing additional
constraints on knowledge engineering as we dis-
cussed in Section 4.
We glossed over the interpretation issues related
to metonymy and other imprecise formulations in
questions (Aleven et al, 2002). A taxonomy of
imprecise manual question encodings by domain
experts is presented in (Fan and Porter, 2004).
They also propose an algorithm to address loosely
encoded questions using ontological knowledge.
This algorithm in effect performs question inter-
pretation, and we are planning to incorporate it
into our interpretation mechanism to help inter-
pret question representations obtained automati-
cally during language interpretation.
Text generation of the type that can handle the
necessary linguistic phenomena needed have not
been implemented in tutoring systems that use di-
alogue. The DIAG-NLP tutorial dialogue sys-
tem (Eugenio et al, 2005) shows that structured
explanations from deep generation supported by
knowledge representation and reasoning improve
learning. However, it does not engage in a dia-
logue with the user, and in this paper we showed
that explanations need to be further adjusted in di-
alogue based on previous student responses and
knowledge. Deep generation using context has
been used in some other types of dialogue sys-
tems such as collaborative problem solving (Stent,
2001), and we expect that the approaches used in
content selection and planning in those systems
will also transfer to our deep generation system.
7 Conclusions
We discussed the implementation of a tutorial di-
alogue system which relies on a domain knowl-
edge representation to verify student answers and
offer appropriate explanations. Integration with
domain-independent interpretation and generation
components places additional requirements on
knowledge representation, and we showed how
an existing knowledge representation mechanisms
used in answering exam questions can be adapted
to the more complex task of tutoring, including in-
terpreting student explanations and generating ap-
10 KRAQ06
propriate feedback.
Acknowledgments
This material is based upon work supported by a
grant from The Office of Naval Research number
N000149910165.
References
V. Aleven, O. Popescu, and K. Koedinger. 2002. Pilot-
testing a tutorial dialogue system that supports self-
explanation. Lecture Notes in Computer Science,
2363.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons
learned. The Journal of the Learning Sciences,
4(2):167?207.
M. Arnold and R. Millar. 1987. Being constructive:
An alternative approach to the teaching of introduc-
tory ideas in electricity. International Journal of
Science Education, 9:553?563.
K. Barker, V. K. Chaudhri, S. Y. Chaw, P. Clark, J. Fan,
D. Israel, S. Mishra, B. W. Porter, P. Romero, D.
Tecuci, and P. Z. Yeh. 2004. A question-answering
system for AP chemistry: Assessing KR&R tech-
nologies. In KR, pages 488?497.
B. S. Bloom. 1984. The two sigma problem: The
search for methods of group instruction as effec-
tive as one-to-one tutoring. Educational Researcher,
13:3?16.
Johan Bos. 2004. Computational semantics in dis-
course: Underspecification, resolution, and infer-
ence. Journal of Logic, Language and Information,
13(2):139?157.
Johan Bos. 2005. Towards wide-coverage semantic
interpretation. In Proceedings of Sixth International
Workshop on Computational Semantics (IWCS-6).
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway and James C. Lester. 2002.
Narrative prose generation. Artificial Intelligence,
139(2):213?252, August.
P. Clark and B. Porter, 1999. KM (1.4): Users Manual.
http://www.cs.utexas.edu/users/mfkb/km.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2003.
Integrating linguistic and domain knowledge for
spoken dialogue systems in multiple domains. In
Proceedings of IJCAI-03 Workshop on Knowledge
and Reasoning in Practical Dialogue Systems.
M. O. Dzikovska. 2004. A Practical Semantic Rep-
resentation For Natural Language Parsing. Ph.D.
thesis, University of Rochester.
B. Di Eugenio, D. Fossati, D. Yu, S. Haller, and
M. Glass. 2005. Natural language generation for in-
telligent tutoring systems: A case study. In 12th In-
ternational Conference on Artificial Intelligence in
Education, pages 217?224.
J. Fan and B. W. Porter. 2004. Interpreting loosely en-
coded questions. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence, Six-
teenth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 399?405.
N. S. Friedland, P. G. Allen, M. J. Witbrock, G.
Matthews, N. Salay, P. Miraglia, J. Angele, S.
Staab, D. Israel, V. K. Chaudhri, B. W. Porter, K.
Barker, and P. Clark. 2004. Towards a quanti-
tative, platform-independent analysis of knowledge
systems. In KR, pages 507?515.
M. Glass. 2001. Processing language input in the
CIRCSIM-tutor intelligent tutoring system. In J.
Moore, C. L. Redfield, and W. L. Johnson, editors,
Artificial Intelligence in Education. IOS press.
M. Makatchev, P. W. Jordan, and K. VanLehn. 2004.
Abductive theorem proving for analyzing student
explanations to guide feedback in intelligent tutor-
ing systems. J. Autom. Reasoning, 32(3):187?226.
N. Person, A.C. Graesser, D. Harter, and E. Math-
ews. 2000. Dialog move generation and conversa-
tion management in autotutor. In Workshop Notes of
the AAAI ?00 Fall Symposium on Building Dialogue
Systems for Tutorial Applications.
C. Rose?, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive concep-
tual tutoring in atlas-andes. In Proceedings of AI in
Education 2001 Conference.
C. Rose?. 2000. A framework for robust semantic inter-
pretation. In Proceedings 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
P. S. Schaffer and L. C. McDermott. 1992. Research
as a guide for curriculum development: An example
from introductory electricity. part ii: Design of in-
structional strategies. American Journal of Physics,
60(11):1003?1013.
D. G. Stallard. 1986. A terminological simplifica-
tion transformation for natural language question-
answering systems. In ACL Proceedings, 24th An-
nual Meeting, pages 241?246.
A. Stent. 2001. Dialogue Systems as Conversational
Partners: Applying conversation acts theory to nat-
ural language generation for task-oriented mixed-
initiative spoken dialogue. Ph.D. thesis, University
of Rochester, Rochester, NY, August.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural
Language Tutoring Group. 2002. The architecture
of why2-atlas: a coach for qualitative physics essay
writing. In Proceedings of Intelligent Tutoring Sys-
tems Conference.
11 KRAQ06
Tools for hierarchical annotation of typed dialogue
Myroslava O. Dzikovska, Charles Callaway, Elaine Farrow
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, United Kingdom,
{mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk
1 Introduction
We discuss a set of tools for annotating a complex
hierarchical and linguistic structure of tutorial di-
alogue based on the NITE XML Toolkit (NXT)
(Carletta et al, 2003). The NXT API supports
multi-layered stand-off data annotation and syn-
chronisation with timed and speech data. Using
NXT, we built a set of extensible tools for de-
tailed structure annotation of typed tutorial dia-
logue, collected from a tutor and student typing
via a chat interface. There are several corpora of
tutoring done with such chat-style communication
techniques (Shah et al, 2002; Jordan and Siler,
2002), however, our annotation presents a special
problem because of its detailed hierarchical struc-
ture. We applied our annotation methodology to
annotating corpora in two different tutoring do-
mains: basic electricity and electronics, and sym-
bolic differentiation.
2 Data Structures
Our corpus has two sources of overlapping anno-
tations: the turn structure of the corpus and situ-
ational factors annotation. The data are naturally
split into turns whenever a participant presses their
?submit? button. Timing information is associated
with individual turns, representing the time when
the entire message was sent to the other partici-
pant, rather than with individual words and sounds
as it would be in spoken corpora.
However, turns are too large to be used as units
in the annotation for dialogue phenomena. For
example, the single turn ?Well done. Let?s try a
harder one.? consists of two utterances making
different dialogue contributions: positive tutorial
feedback for the previous student utterance and a
statement of a new tutorial goal. Thus, turns must
be segmented into smaller units which can serve
as a basis for dialogue annotation. We call these
utterances by analogy with spoken language, be-
cause they are often fragments such as ?well done?
rather than complete sentences.
Thus, the corpus has two inherently overlap-
ping layers: the turn segmentation layer, grouping
utterances into turns, and the dialogue structure
layer built up over individual utterances. The NXT
toolkit supports such overlapping annotations, and
we built two individual tools to support corpus an-
notation: an utterance segmentation tool and a tu-
torial annotation tool.
Additionally, the corpus contains annotation
done by the tutor herself at collection time which
we call ?situational factors?. The tutors were
asked to submit a set of these factors after each
turn describing the progress and state of the stu-
dent, such as answer correctness, confidence and
engagement. The factors were submitted sepa-
rately from dialogue contributions and provide an-
other layer of dialogue annotation which has to
be coordinated with other annotations. The fac-
tors are typically related to the preceding student?s
utterance, but the link is implicit in the submis-
sion time.1 Currently we include the factors in the
tool?s transcript display based on the submission
time, so they are displayed after the appropriate
turn in the transcript allowing the annotators to vi-
sually synchronise them with the dialogue. We
also provide an option to annotators for making
them visible or not. In the future we plan to make
factors a separate layer of the annotation linked by
pointers with the preceding student and tutor turns.
1The factor interface was designed to be quick to use and
minimally impact the dialogue flow, so the submission tim-
ings are generally reliable.
57
3 Utterance Segmentation
We process the raw data with an automatic seg-
menter/tokenizer which subdivides turns into indi-
vidual utterances, and utterances into tokens, pro-
viding an initial segmentation for the annotation.
However, perfect automatic segmentation is not
possible, because punctuation is often either in-
consistent or missing in typed dialogue and this
task therefore requires human judgement. The
output of our automatic segmentation algorithm
was verified and corrected by a human annotator.
A screen-shot of the interface we developed for
segmentation verification is displayed in Figure 1.
With the aid of this tool, it took 6 person-hours
to check and correct the automatically segmented
utterances for the 18 dialogues in our corpus.
4 Tutorial Annotation
To provide a detailed analysis of tutorial dialogue
and remediation strategies, we employ a hierarchi-
cal annotation scheme which encodes the recur-
sive dialogue structure. Each tutorial session con-
sists of a sequence of tasks, which may be either
teaching specific domain concepts or doing indi-
vidual exercises. Each task?s structure includes
one or more of the following: giving definitions,
formulating a question, obtaining the student an-
swer and remediation by the tutor.
Generally speaking, the structure of tutorial di-
alogue is governed by the task structure just as in
task-oriented dialogue (Grosz and Sidner, 1986).
However, the specific annotation structure differs
depending on the tutoring method. In our basic
electricity and electronics domain, a tutorial ses-
sion consists of a set of ?teach? segments, and
within each segment a number of ?task? segments.
Task segments usually contain exercises in which
the student is asked a question requiring a simple
(one- or two-line) answer, which may be followed
by a long remediation segment to address the con-
ceptual problems revealed by the answer.
In contrast, in our calculus domain the students
have to do multi-step procedures to differentiate
complex math expressions, but most of the reme-
diations are very short, fixing the immediate prob-
lem and letting the student continue on with the
procedure. Thus even though the dialogue is hier-
archically structured in both cases, the annotation
schemes differ depending on the domain. We de-
veloped a generic tool for annotating hierarchical
dialogue structure which can be configured with
the specific annotation scheme.
The tool interface (Figure 2) consists of a tran-
script of a session and a linked tree representation.
Individual utterances displayed in the transcript
are leaves of the tree. It is not possible to display
them as tree leaves directly as would be done in
syntactic trees, because they are too large to fit in
graphical tree display. Instead, a segment is high-
lighted in a transcript whenever it is selected in the
tutorial structure, and a hotkey is provided to ex-
pand the tree to see all annotations of a particular
utterance in the transcript.
The hierarchical tree structure is supported by a
schema which describes the annotations possible
on each hierarchical tree level. Since the multi-
layered annotation scheme is quite complex, the
tool uses the annotation schema to limit the num-
ber of codes presented to the annotator to be only
those consistent with the tree level. For exam-
ple, in our basic electricity domain annotation de-
scribed above, there are about 20 codes at different
level, but an annotator will only have ?teach? as an
option for assigning a code to a top tree level, and
only ?task? and ?test? (with appropriate subtypes)
for assigning codes immediately below the teach
level, based on the schema defined for the domain.
5 Transcript Segmentation
We had to conduct several simpler data analy-
ses where the utterances in the transcript are seg-
mented according to their purpose. For exam-
ple, in tutorial differentiation the dialogue con-
centrates on 4 main purposes: general discussion,
introducing problems, performing differentiation
proper, or doing algebraic transformations to sim-
plify the resulting expressions. In another analysis
we needed to mark the segments where the student
was making errors and the nature of those errors.
We developed a generic annotation tool to sup-
port such segmentation annotation over the utter-
ance layer. The tool is configured with the name
of the segment tag and colours indicating different
segment types. The annotator can enter a segment
type, and use a freetext field for other information.
A screenshot of the annotation tool with utterance
purposes marked is given in Figure 3.
6 Data Analysis
The NITE query language (NQL) enables us to ac-
cess the data as a directed acyclic graph to cor-
relate simple annotations, such as finding out the
58
Figure 1: Utterance Segmentation Tool.
Figure 2: Tutorial Strategy Annotation Tool.
59
Figure 3: Segmentation tool. The segment labels are shown on the left.
number of turns which contain only mathematical
expressions but no words. We use the NITE query
interface for simpler analysis tasks such as finding
all instances of specific tags and tag combinations.
However, we found the query language less use-
ful for coordinating the situational factors anno-
tated by tutors with other annotation. Each set of
factors submitted is normally associated with the
first student turn which precedes it, but the factors
were not linked to student utterances explicitly.
NQL does not have a ?direct precedence? opera-
tor.2 Thus it is easier derive this information using
the JAVA API. To make the data analysis simpler,
we are planning to add a pointer layer, generated
automatically based on timing information, which
will use explicit pointers between the factor sub-
missions and preceding tutor and student turns.
7 Conclusions
We presented a set of tools for hierarchically an-
notating dialogue structure, suitable for annotating
typed dialogue. The turns in these dialogues are
complex and overlap with dialogue structure, and
our toolset supports segmenting turns into smaller
2It?s possible to express the query in NQL us-
ing its precedence operator ?? as ?($f factor)
($u utterance) (forall $u1 utterance) :
(($f  $u) && ($f  u1)) ? (u  u1)?.
However, this is very inefficient since it must check all
utterance pairs in the corpus to determine direct precedence,
especially if it needs to be included as part of a bigger query.
utterance units and annotating hierarchical dia-
logue structure over the utterances, as well as pro-
viding simpler segmentation annotation.
Acknowledgements
This material is based upon work supported by a
grant from The Office of Naval Research num-
ber N000149910165 and European Union 6th
framework programme grant EC-FP6-2002-IST-
1-507826 (LeActiveMath).
References
Jean Carletta, J. Kilgour, T. O?Donnell, S. Evert, and
H. Voormann. 2003. The NITE object model li-
brary for handling structured linguistic annotation
on multimodal data sets. In Proceedings of the
EACL Workshop on Language Technology and the
Semantic Web.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175?204.
Pamela Jordan and Stephanie Siler. 2002. Student
initiative and questioning strategies in computer-
mediated human tutoring dialogues. In Proceedings
of ITS 2002 Workshop on Empirical Methods for Tu-
torial Dialogue Systems.
Farhana Shah, Martha W. Evens, Joel Michael, and
Allen Rovick. 2002. Classifying student initiatives
and tutor responses in human keyboard-to-keyboard
tutoring sessions. Discourse Processes, 33(1).
60
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 9?16,
New York City, June 2006. c?2006 Association for Computational Linguistics
Backbone Extraction and Pruning for Speeding Up a Deep Parser for
Dialogue Systems
Myroslava O. Dzikovska
Human Communication Research Centre
University of Edinburgh
Edinburgh, EH8 9LW, United Kingdom,
mdzikovs@inf.ed.ac.uk
Carolyn P. Rose?
Carnegie Mellon University
Language Technologies Institute,
Pittsburgh, PA 15213,
cprose@cs.cmu.edu
Abstract
In this paper we discuss issues related to
speeding up parsing with wide-coverage
unification grammars. We demonstrate
that state-of-the-art optimisation tech-
niques based on backbone parsing before
unification do not provide a general so-
lution, because they depend on specific
properties of the grammar formalism that
do not hold for all unification based gram-
mars. As an alternative, we describe an
optimisation technique that combines am-
biguity packing at the constituent structure
level with pruning based on local features.
1 Introduction
In this paper we investigate the problem of scaling
up a deep unification-based parser developed specif-
ically for the purpose of robust interpretation in dia-
logue systems by improving its speed and coverage
for longer utterances. While typical sentences in di-
alogue contexts are shorter than in expository text
domains, longer utterances are important in discus-
sion oriented domains. For example, in educational
applications of dialogue it is important to elicit deep
explanation from students and then offer focused
feedback based on the details of what students say.
The choice of instructional dialogue as a target ap-
plication influenced the choice of parser we needed
to use for interpretation in a dialogue system. Sev-
eral deep, wide-coverage parsers are currently avail-
able (Copestake and Flickinger, 2000; Rose?, 2000;
Baldridge, 2002; Maxwell and Kaplan, 1994), but
many of these have not been designed with issues re-
lated to interpretation in a dialogue context in mind.
The TRIPS grammar (Dzikovska et al, 2005) is a
wide-coverage unification grammar that has been
used very successfully in several task-oriented di-
alogue systems. It supports interpretation of frag-
ments and lexical semantic features (see Section 2
for a more detailed discussion), and provides addi-
tional robustness through ?robust? rules that cover
common grammar mistakes found in dialogue such
as missing articles or incorrect agreement. These
enhancements help parsing dialogue (both spoken
and typed), but they significantly increase grammar
ambiguity, a common concern in building grammars
for robust parsing (Schneider and McCoy, 1998). It
is specifically these robustness-efficiency trade-offs
that we address in this paper.
Much work has been done related to enhanc-
ing the efficiency of deep interpretation systems
(Copestake and Flickinger, 2000; Swift et al, 2004;
Maxwell and Kaplan, 1994), which forms the foun-
dation that we build on in this work. For example,
techniques for speeding up unification in HPSG lead
to dramatic improvements in efficiency (Kiefer et
al., 1999). Likewise ambiguity packing and CFG
backbone parsing (Maxwell and Kaplan, 1994; van
Noord, 1997) are known to increase parsing effi-
ciency. However, as we show in this paper, these
techniques depend on specific grammar properties
that do not hold for all grammars. This claim is con-
sistent with observations of Carroll (1994) that pars-
ing software optimisation techniques tend to be lim-
ited in their applicability to the individual grammars
they were developed for. While we used TRIPS as
our example unification-based grammar, this inves-
tigation is important not only for this project, but in
the general context of speeding up a wide-coverage
unification grammar which incorporates fragment
9
rules and lexical semantics, which may not be im-
mediately provided by other available systems.
In the remainder of the paper, we begin with a
brief description of the TRIPS parser and grammar,
and motivate the choice of LCFLEX parsing algo-
rithm to provide a fast parsing foundation. We then
discuss the backbone extraction and pruning tech-
niques that we used, and evaluate them in compar-
ison with the original parsing algorithm. We con-
clude with discussion of some implications for im-
plementing grammars that build deep syntactic and
semantic representations.
2 Motivation
The work reported in this paper was done as part
of the process of developing a dialogue system that
incorporates deep natural language understanding.
We needed a grammar that provides lexical seman-
tic interpretation, supports parsing fragmentary ut-
terance in dialogue, and could be used to start de-
velopment without large quantities of corpus data.
TRIPS fulfilled our requirements better than sim-
ilar alternatives, such as LINGO ERG (Copestake
and Flickinger, 2000) or XLE (Maxwell and Kaplan,
1994).
TRIPS produces logical forms which include se-
mantic classes and roles in a domain-independent
frame-based formalism derived from FrameNet and
VerbNet (Dzikovska et al, 2004; Kipper et al,
2000). Lexical semantic features are known to be
helpful in both deep (Tetreault, 2005) and shal-
low interpretation tasks (Narayanan and Harabagiu,
2004). Apart from TRIPS, these have not been in-
tegrated into existing deep grammars. While both
LINGO-ERG and XLE include semantic features
related to scoping, in our applications the avail-
ability of semantic classes and semantic role as-
signments was more important to interpretation,
and these features are not currently available from
those parsers. Finally, TRIPS provides a domain-
independent parse selection model, as well as rules
for interpreting discourse fragments (as was also
done in HPSG (Schlangen and Lascarides, 2003), a
feature actively used in interpretation.
While TRIPS provides the capabilities we need,
its parse times for long sentences (above 15 words
long) are intolerably long. We considered two pos-
sible techniques for speeding up parsing: speeding
up unification using the techniques similar to the
LINGO system (Copestake and Flickinger, 2000),
or using backbone extraction (Maxwell and Ka-
plan, 1994; Rose? and Lavie, 2001; Briscoe and Car-
roll, 1994). TRIPS already uses a fast unification
algorithm similar to quasi-destructive unification,
avoiding copying during unification.1 However,
the TRIPS grammar retains the notion of phrase
structure, and thus it was more natural to chose to
use backbone extraction with ambiguity packing to
speed up the parsing.
As a foundation for our optimisation work, we
started with the freely available LCFLEX parser
(Rose? and Lavie, 2001). LCFLEX is an all-paths
parser that uses left-corner prediction and ambigu-
ity packing to make all-paths parsing tractable, and
which was shown to be efficient for long sentences
with somewhat less complex unification augmented
context-free grammars. We show that all-paths pars-
ing with LCFLEX is not tractable for the ambiguity
level in the TRIPS grammar, but that by introduc-
ing a pruning method that uses ambiguity packing to
guide pruning decisions, we can achieve significant
improvements in both speed and coverage compared
to the original TRIPS parser.
3 The TRIPS and LCFLEX algorithms
3.1 The TRIPS parser
The TRIPS parser we use as a baseline is a bottom-
up chart parser with lexical entries and rules repre-
sented as attribute-value structures. To achieve pars-
ing efficiency, TRIPS uses a best-first beam search
algorithm based on the scores from a parse selection
model (Dzikovska et al, 2005; Elsner et al, 2005).
The constituents on the parser?s agenda are grouped
into buckets based on their scores. At each step, the
bucket with the highest scoring constituents is se-
lected to build/extend chart edges. The parsing stops
once N requested analyses are found. This guaran-
tees that the parser returns the N -best list of analyses
according to the parse selection model used, unless
the parser reaches the chart size limit.
1Other enhancements used by LINGO depend on disallow-
ing disjunctive features, and relying instead on the type system.
The TRIPS grammar is untyped and uses disjunctive features,
and converting it to a typed system would require as yet unde-
termined amount of additional work.
10
In addition to best-first parsing, the TRIPS parser
uses a chart size limit, to prevent the parser from
running too long on unparseable utterances, similar
to (Frank et al, 2003). TRIPS is much slower pro-
cessing utterances not covered in the grammar, be-
cause it continues its search until it reaches the chart
limit. Thus, a lower chart limit improves parsing
efficiency. However, we show in our evaluation that
the chart limit necessary to obtain good performance
in most cases is too low to find parses for utterances
with 15 or more words, even if they are covered by
the grammar.
The integration of lexical semantics in the TRIPS
lexicon has a major impact on parsing in TRIPS.
Each word in the TRIPS lexicon is associated with a
semantic type from a domain-independent ontology.
This enables word sense disambiguation and seman-
tic role labelling for the logical form produced by
the grammar. Multiple word senses result in addi-
tional ambiguity on top of syntactic ambiguity, but it
is controlled in part with the use of weak selectional
restrictions, similar to the restrictions employed by
the VerbNet lexicon (Kipper et al, 2000). Check-
ing semantic restrictions is an integral part of TRIPS
parsing, and removing them significantly decreases
speed and increases ambiguity of the TRIPS parser
(Dzikovska, 2004). We show that it also has an im-
pact on parsing with a CFG backbone in Section 4.1.
3.2 LCFLEX
The LCFLEX parser (Rose? and Lavie, 2001) is an
all-paths robust left corner chart parser designed to
incorporate various robustness techniques such as
word skipping, flexible unification, and constituent
insertion. Its left corner chart parsing algorithm
is similar to that described by Briscoe and Carroll
(1994). The system supports grammatical specifi-
cation in a unification framework that consists of
context-free grammar rules augmented with feature
bundles associated with the non-terminals of the
rules. LCFLEX can be used in two parsing modes:
either context-free parsing can be done first, fol-
lowed by applying the unification rules, or unifica-
tion can be done interleaved with context-free pars-
ing. The context free backbone allows for efficient
left corner predictions using a pre-compiled left cor-
ner prediction table, such as that described in (van
Noord, 1997). To enhance its efficiency, it incor-
porates a provably optimal ambiguity packing algo-
rithm (Lavie and Rose?, 2004).
These efficiency techniques make feasible all-
path parsing with the LCFLEX CARMEL grammar
(Rose?, 2000). However, CARMEL was engineered
with fast all-paths parsing in mind, resulting in cer-
tain compromises in terms of coverage. For exam-
ple, it has only very limited coverage for noun-noun
compounding, or headless noun phrases, which are a
major source of ambiguity with the TRIPS grammar.
4 Combining LCFLEX and TRIPS
4.1 Adding CFG Backbone
A simplified TRIPS grammar rule for verb phrases
and a sample verb entry are shown in Figure 1. The
features for building semantic representations are
omitted for brevity. Each constituent has an assigned
category that corresponds to its phrasal type, and a
set of (complex-valued) features.
The backbone extraction algorithm is reason-
ably straightforward, with CFG non-terminals cor-
responding directly to TRIPS constituent categories.
To each CFG rule we attach a corresponding TRIPS
unification rule. After parsing is complete, the
parses found are scored and ordered with the parse
selection model, and therefore parsing accuracy in
all-paths mode is the same or better than TRIPS ac-
curacy for the same model.
For constituents with subcategorized arguments
(verbs, nouns, adverbial prepositions), our back-
bone generation algorithm takes the subcategoriza-
tion frame into account. For example, the TRIPS
VP rule will split into 27 CFG rules corresponding
to different subcategorization frames: VP? V intr,
VP? V NP NP, VP? V NP CP NP CP, etc. For
each lexical entry, its appropriate CFG category is
determined based on the subcategorization frame
from TRIPS lexical representation. This improves
parsing efficiency using the prediction algorithms in
TFLEX operating on the CFG backbone. The ver-
sion of the TRIPS grammar used in testing con-
tained 379 grammar rules with 21 parts of speech
(terminal symbols) and 31 constituent types (non-
terminal symbols), which were expanded into 1121
CFG rules with 85 terminals and 36 non-terminals
during backbone extraction.
We found, however, that the previously used tech-
11
(a) ((VP (SUBJ ?!subj) (CLASS ?lf))
-vp1-role .99
(V (LF ?lf) (SUBJ ?!subj) (DOBJ ?dobj)
(IOBJ ?iobj) (COMP3 ?comp3))
?iobj ?dobj ?comp3)
(b) ((V (agr 3s) (LF LF::Filling)
(SUBJ (NP (agr 3s)))
(DOBJ (NP (case obj))) (IOBJ -) (COMP3 -)))
Figure 1: (a) A simplified VP rule from the TRIPS
grammar; (b) a simplified verb entry for a transitive
verb. Question marks denote variables.
nique of context-free parsing first followed by full
re-unification was not suitable for parsing with the
TRIPS grammar. The CFG structure extracted from
the TRIPS grammar contains 43 loops resulting
from lexical coercion rules or elliptical construc-
tions. A small number of loops from lexical coer-
cion were both obvious and easy to avoid, because
they are in the form N? N. However, there were
longer loops, for example, NP ? SPEC for sen-
tences like ?John?s car? and SPEC ? NP for head-
less noun phrases in sentences like ?I want three?.
LCFLEX uses a re-unification algorithm that asso-
ciates a set of unification rules with each CFG pro-
duction, which are reapplied at a later stage. To
be able to apply a unification rule corresponding to
N? N production, it has to be explicitly present in
the chart, leading to an infinite number of N con-
stituents produced. Applying the extra CFG rules
expanding the loops during re-unification would
complicate the algorithm significantly. Instead, we
implemented loop detection during CFG parsing.
The feature structures prevent loops in unifica-
tion, and we considered including certain grammat-
ical features into backbone extraction as done in
(Briscoe and Carroll, 1994). However, in the TRIPS
grammar the feature values responsible for break-
ing loops belonged to multi-valued features (6 val-
ued in the worst case), with values which may de-
pend on other multiple-valued features in daughter
constituents. Thus adding the extra features resulted
in major backbone size increases because of cate-
gory splitting. This can be remedied with additional
pre-compilation (Kiefer and Krieger, 2004), how-
ever, this requires that all lexical entries be known
in advance. One nice feature of the TRIPS lex-
icon is that it includes a mechanism for dynami-
cally adding lexical entries for unknown words from
wide-coverage lexicons such as VerbNet (Kipper et
al., 2000), which would be impractical to use in pre-
compilation.
Therefore, to use CFG parsing before unification
in our system, we implemented a loop detector that
checked the CFG structure to disallow loops. How-
ever, the next problem that we encountered is mas-
sive ambiguity in the CFG structure. Even a very
short phrase such as ?a train? had over 700 possi-
ble CFG analyses, and took 910 msec to parse com-
pared to 10 msec with interleaved unification. CFG
ambiguity is so high because noun phrase fragments
are allowed as top-level categories, and lexical am-
biguity is compounded with semantic ambiguity and
robust rules normally disallowed by features during
unification. Thus, in our combined algorithm we had
to use unification interleaved with parsing to filter
out the CFG constituents.
4.2 Ambiguity Packing
For building semantic representations in parallel
with parsing, ambiguity packing presents a set of
known problems (Oepen and Carroll, 2000). One
possible solution is to exclude semantic features dur-
ing an initial unification stage, use ambiguity pack-
ing, and re-unify with semantic features in a post-
processing stage. In our case, we found this strategy
difficult to implement, since selectional restrictions
are used to limit the ambiguity created by multiple
word senses during syntactic parsing. Therefore, we
chose to do ambiguity packing on the CFG structure
only, keeping the multiple feature structures associ-
ated with each packed CFG constituent.
To begin to evaluate the contribution of ambiguity
packing on efficiency, we ran a test on the first 39 ut-
terances in a hold out set not used in the formal eval-
uation below. Sentences ranged from 1 to 17 words
in length, 16 of which had 6 or more words. On this
set, the average parse time without ambiguity pack-
ing was 10 seconds per utterance, and 30 seconds per
utterance on utterances with 6 or more words. With
ambiguity packing turned on, the average parse time
decreased to 5 seconds per utterance, and 13.5 sec-
onds per utterance on the utterances with more than
6 words. While this evaluation showed that ambi-
12
guity packing improves parsing efficiency, we deter-
mined that further enhancements were necessary.
4.3 Pruning
We added a pruning technique based on the scor-
ing model discussed above and ambiguity packing
to enhance system performance. As an illustration,
consider an example from a corpus used in our eval-
uation where the TRIPS grammar generates a large
number of analyses, ?we have a heart attack vic-
tim at marketplace mall?. The phrase ?a heart at-
tack victim? has at least two interpretations,?a [N1
heart [N1 attack [N1 victim]]]? and ?a [N1 [N1 heart
[N1 attack]] [N1 victim]]?. The prepositional phrase
?at marketplace mall? can attach either to the noun
phrase or to the verb. Overall, this results in 4 basic
interpretations, with additional ambiguity resulting
from different possible senses of ?have?.
The best-first parsing algorithm in TRIPS uses
parse selection scores to suppress less likely inter-
pretations. In our example, the TRIPS parser will
chose the higher-scoring one of the two interpreta-
tions for ?a heart attack victim?, and use it first. For
this NP the features associated with both interpreta-
tions are identical with respect to further processing,
thus TRIPS will never come back to the other in-
terpretation, effectively pruning it. ?At? also has 2
possible interpretations due to word sense ambigu-
ity: LF::TIME-LOC and LF::SPATIAL-LOC. The
former has a slightly higher preference, and TRIPS
will try it first. But then it will be unable to find an
interpretation for ?at Marketplace Mall?, and back-
track to LF::SPATIAL-LOC to find a correct parse.
Without chart size limits the parser is guaran-
teed to find a parse eventually through backtracking.
However, this algorithm does not work quite as well
with chart size limits. If there are many similarly-
scored constituents in the chart for different parts of
the utterance, the best-first algorithm expands them
first, and the the chart size limit tends to interfere
before TRIPS can backtrack to an appropriate lower-
scoring analysis.
Ambiguity packing offers an opportunity to make
pruning more strategic by focusing specifically on
competing interpretations for the same utterance
span. The simplest pruning idea would be for ev-
ery ambiguity packed constituent to eliminate the in-
terpretations with low TRIPS scores. However, we
need to make sure that we don?t prune constituents
that are required higher up in the tree to make a
parse. Consider our example again.
The constituent for ?at? will be ambiguity
packed with its two meanings. But if we prune
LF::SPATIAL-LOC at that point, the parse for ?at
Marketplace Mall? will fail later. Formally, the com-
peting interpretations for ?at? have non-local fea-
tures, namely, the subcategorized complement (time
versus location) is different for those interpretations,
and is checked higher up in the parse. But for ?a
heart attack victim? the ambiguity-packed interpre-
tations differ only in local features. All features as-
sociated with this NP checked higher up come from
the head noun ?victim? and are identical in all inter-
pretations. Therefore we can eliminate the low scor-
ing interpretations with little risk of discarding those
essential for finding a complete parse. Thus, for
any constituent where ambiguity-packed non-head
daughters differ only in local features, we prune
the interpretations coming from them to a specified
prune beam width based on their TRIPS scores.
This pruning heuristic based on local features
can be generalised to different unification grammars.
For example, in HPSG pruning would be safe at all
points where a head is combined with ambiguity-
packed non-head constituents, due to the locality
principle. In the TRIPS grammar, if a trips rule
uses subcategorization features, the same locality
principle holds. This heuristic has perfect precision
though not complete recall, but, as our evaluation
shows, it is sufficient to significantly improve per-
formance in comparison with the TRIPS parser.
5 Evaluation
The purpose of our evaluation is to explore the ex-
tent to which we can achieve a better balance be-
tween parse time and coverage using backbone pars-
ing with pruning compared to the original best-first
algorithm. For our comparison we used an excerpt
from the Monroe corpus that has been used in previ-
ous TRIPS research on parsing speed and accuracy
(Swift et al, 2004) consisting of dialogues s2, s4,
s16 and s17. Dialogue s2 was a hold out set used for
pilot testing and setting parameters. The other three
dialogues were set aside for testing. Altogether, the
test set contained 1042 utterances, ranging from 1 to
13
45 words in length (mean 5.38 words/utt, st. dev. 5.7
words/utt). Using our hold-out set, we determined
that a beam width of three was an optimal setting.
Thus, we compared TFLEX using a beam width of 3
to three different versions of TRIPS that varied only
in terms of the maximum chart size, giving us a ver-
sion that is significantly faster than TFLEX overall,
one that has parse times that are statistically indis-
tinguishable from TFLEX, and one that is signifi-
cantly slower. We show that while lower chart sizes
in TRIPS yield speed ups in parse time, they come
with a cost in terms of coverage.
5.1 Evaluation Methodology
Because our goal is to explore the parse time versus
coverage trade-offs of two different parsing architec-
tures, the two evaluation measures that we report are
average parse time per sentence and probability of
finding at least one parse, the latter being a measure
estimating the effect of parse algorithm on parsing
coverage.
Since the scoring model is the same in TRIPS and
TFLEX, then as long as TFLEX can find at least one
parse (which happened in all but 1 instances on our
held-out set), the set returned will include the one
produced by TRIPS. We spot-checked the TFLEX
utterances in the test set for which TRIPS could
not find a parse to verify that the parses produced
were reasonable. The parses produced by TFLEX on
these sentences were typically acceptable, with er-
rors mainly stemming from attachment disambigua-
tion problems.
5.2 Results
We first compared parsers in terms of probability of
producing at least one parse (see Figure 2). Since
the distribution of sentence lengths in the test corpus
was heavily skewed toward shorter sentences, we
grouped sentences into equivalence classes based on
a range of sentence lengths with a 5-word increment,
with all of the sentences over 20 words aggregated
in the same class. Given a large number of short sen-
tences, there was no significant difference overall in
likelihood to find a parse. However, on sentences
greater than 10 words long, TFLEX is significantly
more likely to produce a parse than any of the TRIPS
parsers (evaluated using a binary logistic regression,
N = 334, G = 16.8, DF = 1, p < .001). Fur-
Parser <= 20 words >= 6 words
TFLEX 6.2 (20.2) 29.1 (96.3)
TRIPS-1500 2.3 (5.4) 6.9 (8.2)
TRIPS-5000 7.7 (30.2) 28.1 (56.4)
TRIPS-10000 22.7 (134.4) 107.6 (407.4)
Table 1: The average parse times for TRIPS and
TFLEX on utterances 6 words or more.
thermore, for sentences greater than 20 words long,
no form of TRIPS parser ever returned a complete
parse.
Next we compared the parsers in terms of aver-
age parse time on the whole data set across equiva-
lence classes of sentences, assigned based on Aggre-
gated Sentence Length (see Figure 2 and Table 1).
An ANOVA with Parser and Aggregated Sentence
Length as independent variables and Parse Time as
the dependent variable showed a significant effect
of Parser on Parse Time (F (3, 4164) = 270.03,
p < .001). Using a Bonferroni post-hoc analysis, we
determined that TFLEX is significantly faster than
TRIPS-10000 (p < .001), statistically indistinguish-
able in terms of parse time from TRIPS-5000, and
significantly slower than TRIPS-1500 (p < .001).
Since none of the TRIPS parsers ever returned a
parse for sentences greater than 20 words long, we
recomputed this analysis excluding the latter. We
still find a significant effect of Parser on Parse Time
(F (3, 4068) = 18.6, p < .001). However, a post-
hoc analysis reveals that parse times for TFLEX,
TRIPS-1500, and TRIPS-5000 are statistically in-
distinguishable for this subset, whereas TFLEX is
significantly faster than TRIPS-10000 (p < .001).
See Table 1 for for parse times of all four parsers.
Since TFLEX and TRIPS both spent 95% of their
computational effort on sentences with 6 or more
words, we also include results for this subset of the
corpus.
Thus, TFLEX presents a superior balance of cov-
erage and efficiency especially for long sentences
(10 words or more) since for these sentences it is
significantly more likely to find a parse than any ver-
sion of TRIPS, even a version where the chart size is
expanded to an extent that it becomes significantly
slower (i.e., TRIPS-10000). And while TRIPS-1500
is consistently faster than the other parsers, it is
not significantly faster than TFLEX on sentences 20
14
Figure 2: Parse times and probability of getting a parse depending on (aggregated) sentence lengths. 5
denotes sentences with 5 or fewer words, 25 sentences with more than 20 words.
words long or less, which is the subset of sentences
for which it is able to find a parse.
5.3 Discussion and Future Work
The most obvious lesson learned in this experience
is that the speed up techniques developed for specific
grammars and unification formalisms do not transfer
easily to other unification grammars. The features
that make TRIPS interesting ? the inclusion of lex-
ical semantics, and the rules for parsing fragments
? also make it less amenable to using existing effi-
ciency techniques.
Grammars with an explicit CFG backbone nor-
mally restrict the grammar writer from writing
grammar loops, a restriction not imposed by gen-
eral unification grammars. As we showed, there
can be a substantial number of loops in a CFG due
to the need to cover various elliptical constructions,
which makes CFG parsing not interleaved with uni-
fication less attractive in cases where we want to
avoid expensive CFG precompilation. Moreover, as
we found with the TRIPS grammar, in the context
of robust parsing with lexical semantics the ambigu-
ity in a CFG backbone grows large enough to make
CFG parsing followed by unification inefficient. We
described an alternative technique that uses pruning
based on a parse selection model.
Another option for speeding up parsing that we
have not discussed in detail is using a typed gram-
mar without disjunction and speeding up unification
as done in HPSG grammars (Kiefer et al, 1999). In
order to do this, we must first address the issue of
integrating the type of lexical semantics that we re-
quire with HPSG?s type system. Adding lexical se-
mantics while retaining the speed benefits obtained
through this type system would require that the se-
mantic type ontology be expressed in the same for-
malism as the syntactic types. We plan to further
explore this option in our future work.
Though longer sentences were relatively rare
in our test set, using the system in an educa-
tional domain (our ultimate goal) means that the
longer sentences are particularly important, because
they often correspond to significant instructional
events, specifically answers to deep questions such
as ?why? and ?how? questions. Our evaluation has
been designed to show system performance with ut-
terances of different length, which would roughly
correspond to the performance in interpreting short
and long student answers. Since delays in respond-
ing can de-motivate the student and decrease the
quality of the dialogue, improving handling of long
utterances can have an important effect on the sys-
tem performance. Evaluating this in practice is a
possible direction for future work.
6 Conclusions
We described a combination of efficient parsing
techniques to improve parsing speed and coverage
with the TRIPS deep parsing grammar. We showed
that context-free parsing was inefficient on a back-
bone extracted from an existing unification gram-
mar, and demonstrated how to make all-path pars-
ing more tractable by a new pruning algorithm based
15
on ambiguity packing and local features, general-
isable to other unification grammars. We demon-
strated that our pruning algorithm provides better
efficiency-coverage balance than the best-first pars-
ing with chart limits utilised by the TRIPS parser,
and discussed the design implications for other ro-
bust parsing grammars.
Acknowledgements
We thank Mary Swift and James Allen for their
help with the TRIPS code and useful comments.
This material is based on work supported by grants
from the Office of Naval Research under numbers
N000140510048 and N000140510043.
References
J. Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
T. Briscoe and J. Carroll. 1994. Generalized proba-
bilistic LR parsing of natural language (corpora) with
unification-based grammars. Computational Linguis-
tics, 19(1):25?59.
J. Carroll. 1994. Relating complexity to practical per-
formance in parsing with wide-coverage unification
grammars. In Proceedings of ACL-2004.
A. Copestake and D. Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC-2000, Athens, Greece.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004.
Building a computational lexicon and ontology with
framenet. In LREC workshop on Building Lexical Re-
sources from Semantically Annotated Corpora.
M. Dzikovska, M. Swift, J. Allen, and W. de Beaumont.
2005. Generic parsing for multi-domain semantic in-
terpretation. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT-05).
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
M. Elsner, M. Swift, J. Allen, and D. Gildea. 2005. On-
line statistics for a unification-based dialogue parser.
In Proceedings of the 9th International Workshop on
Parsing Technologies (IWPT-05).
A. Frank, B. Kiefer, B. Crysmann, M. Becker, and
U. Schafer. 2003. Integrated shallow and deep pars-
ing: TopP meets HPSG. In Proceedings of ACL 2003.
B. Kiefer and H.-U. Krieger. 2004. A context-free ap-
proximation of head-driven phrase structure grammar.
In H. Bunt, J. Carroll, and G. Satta, editors, New De-
velopments in Parsing Technology. Kluwer.
B. Kiefer, H. Krieger, J. Carroll, and R. Malouf. 1999.
Bag of useful techniques for efficient and robust pars-
ing. In Proceedings of ACL 1999.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of the 7th Conference on Artificial Intelligence and of
the 12th Conference on Innovative Applications of Ar-
tificial Intelligence.
A. Lavie and C. P. Rose?. 2004. Optimal ambiguity pack-
ing in context-free parsers with interleaved unification.
In H. Bunt, J. Carroll, and G. Satta, editors, Current Is-
sues in Parsing Technologies. Kluwer Academic Press.
J. T. Maxwell and R. M. Kaplan. 1994. The interface
between phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?590.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proceedings of
International Conference on Computational Linguis-
tics (COLING 2004), Geneva, Switzerland.
S. Oepen and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing - practical results. In Pro-
ceedings of NAACL?00.
C. P. Rose? and A. Lavie. 2001. Balancing robustness
and efficiency in unification-augmented context-free
parsers for large practical applications. In J. Junqua
and G. Van Noord, editors, Robustness in Language
and Speech Technology. Kluwer Academic Press.
C. Rose?. 2000. A framework for robust semantic in-
terpretation. In Proceedings 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
D. Schlangen and A. Lascarides. 2003. The interpreta-
tion of non-sentential utterances in dialogue. In Pro-
ceedings of the 4th SIGdial Workshop on Discourse
and Dialogue, Japan, May.
D. Schneider and K. F. McCoy. 1998. Recognizing syn-
tactic errors in the writing of second language learners.
In Proceedings of COLING-ACL?98.
M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in
the parser: Using a shallow parser to improve deep
parsing. In Proceedings of COLING-04.
J. Tetreault. 2005. Empirical Evaluations of Pronoun
Resolution. Ph.D. thesis, University of Rochester.
G. van Noord. 1997. An efficient implementation of
the head-corner parser. Computational Linguistics,
23(3):425?456.
16
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 25?32,
New York City, June 2006. c?2006 Association for Computational Linguistics
Increasing the coverage of a domain independent dialogue lexicon with
VERBNET
Benoit Crabbe??, Myroslava O. Dzikovska?, William de Beaumont?, Mary Swift?
? ICCS-HCRC, University of Edinburgh, 2 Buccleuch Place, EH8 9LW, Edinburgh, UK
{bcrabbe,mdzikovs}@inf.ed.ac.uk
?Department of Computer Science, University of Rochester, Rochester NY, USA
{wdebeaum,swift}@cs.rochester.edu
Abstract
This paper investigates how to extend cov-
erage of a domain independent lexicon tai-
lored for natural language understanding.
We introduce two algorithms for adding
lexical entries from VERBNET to the lexi-
con of the TRIPS spoken dialogue system.
We report results on the efficiency of the
method, discussing in particular precision
versus coverage issues and implications
for mapping to other lexical databases.
1 Introduction
This paper explores how different lexicons can be
integrated with the goal of extending coverage of
a deep parser and semantic interpreter. Lexical
semantic databases (Kipper et al, 2000; Johnson
and Fillmore, 2000; Dorr, 1997) use a frame-based
model of lexical semantics. Each database groups
words in classes where predicative words and their
arguments are described. The classes are generally
organised in an inheritance structure. Each such
database can be used, among other things, to per-
form semantic interpretation. However, their actual
structures are quite different, reflecting different un-
derlying methodological approaches to lexical de-
scription, and this results in representation that are
not directly compatible. Since no such database has
full coverage of English, it is worth combining them
in order to get a lexicon with better coverage and a
unified representation for English.
We explore the issues related to merging verb
descriptions from two lexical databases, which
have both syntactic and semantic incompatibilities,
and compare two techniques for aligning semantic
classes and the syntax-semantics mappings between
them. The resulting lexicon is to be used in precise
interpretation tasks, so its consistency and accuracy
are a high priority. Thus, though it is possible to gen-
erate lexical entries automatically (Kwon and Hovy,
2006; Swift, 2005), we use a semi-automatic method
in which an expert hand-checks the automatically
generated entries before adding them to the lexicon.
Therefore, our goal is to maximise the number of
new useful entries added to the lexicon while min-
imising the number of entries that are discarded or
hand-edited.
We take the mapping between the TRIPS lexicon
and the VERBNET lexical database as a case study
for our experiment. The TRIPS lexicon is used to-
gether with a parser to provide a natural language
understanding component for several dialogue ap-
plications in different domains. It outputs highly de-
tailed semantic representations suitable for complex
dialogue tasks such as problem-solving and tutoring
dialogue, inter alia. An essential feature of TRIPS
is the integration of a detailed lexical semantic rep-
resentation, semantic classes and theta role assign-
ments in the parsing process.
Semantic types and role labelling are helpful in
both deep (Tetreault, 2005) and shallow interpreta-
tion tasks (Narayanan and Harabagiu, 2004). TRIPS
provides a convenient test case because its grammar
is already equipped with the formal devices required
to build up a frame-based semantic representation
including this information.1
1While wide coverage grammars such as the English Re-
25
We chose VERBNET to extend the TRIPS lexicon
because it includes a detailed syntax-semantic map-
pings, thus providing a more convenient interface to
the syntactic component of the grammar than lexi-
cons where this connection is left unclear, such as
FRAMENET. However the methods described here
are designed to be reusable for merging other lexi-
cal databases, in particular we intend to experiment
with FRAMENET in the near future.
The plan of the paper is as follows: we first de-
scribe the target lexicon (Section 2) and the source
lexicon (Section 3) for our experiment before de-
scribing the methodology for integration (Section 4).
We finally present an evaluation of the techniques in
Section 5.
2 The TRIPS Lexicon
The TRIPS lexicon (Dzikovska, 2004) is the target
of the mapping procedure we describe in Section
4. It includes syntactic and semantic information
necessary to build semantic representations usable
in dialogue systems. The TRIPS parser is equipped
with a fairly detailed grammar, but a major restric-
tion on coverage in new domains is often lack of
lexical information. The lexicon used in our eval-
uation comprised approximately 700 verb lemmas
with 1010 senses (out of approximately 2500 total
word senses, covering both open- and closed-class
words). The lexicon is designed for incremental
growth, since the lexical representation is domain-
independent and the added words are then re-used
in new domains.
A graphical representation of the information
stored in the TRIPS lexicon and used in parsing is
shown in Figure 1. The lexicon is a list of canon-
ical word entries each of which is made of a set
of sense definitions comprised of a LF type and a
syntax-semantic template.
Semantic classes (LF types) in the TRIPS lexi-
con are organised in a domain-independent ontol-
ogy (the LF ontology). The LF Ontology was orig-
inally based on a simplified version of FRAMENET
source Grammar (Copestake and Flickinger, 2000) build deep
semantic representations which account for scoping and tempo-
ral structure, their lexicons do not provide information related
to word senses and role labels, in part due to the additional dif-
ficulty involved building a wide coverage lexicon with the nec-
essary lexical semantic information.
The tourists admired the paintings
LSUBJ LOBJ
LF::Experiencer-Emotion
LF::Experiencer LF::Theme
Figure 1: Information in the TRIPS word sense def-
inition for mapping between syntactic and semantic
roles.
(Baker et al, 1998; Dzikovska et al, 2004), with
each LF type describing a particular situation, object
or event and its participants. Syntax-Semantics Tem-
plates (or templates) capture the linking between the
syntax and semantics (LF type and semantic roles)
of a word. The semantic properties of an argument
are described by means of a semantic role assigned
to it and selectional restrictions.2
The TRIPS grammar contains a set of indepen-
dently described lexical rules, such as the passive or
dative shift rules, which are designed to create non-
canonical lexical entries automatically, while pre-
serving the linking properties defined in the canoni-
cal entry.
In this context adding an entry to the lexicon re-
quires determining both the list of LF types and
the list of templates for canonical contexts, that is,
the list of mappings between a logical frame and a
canonical subcategorization frame.
3 VERBNET
VERBNET (Kipper et al, 2000) provides an actual
implementation of the descriptive work carried out
by Levin (1993), which has been extended to cover
prepositional constructions and corpus-based sub-
categorization frames (Kipper et al, 2004; Kipper
et al, 2006).
VERBNET is a hierarchical verb lexicon in which
verbs are organised in classes. The fundamental
assumption underlying the classification is that the
members of a given class share a similar syntactic
2The selectional restrictions are domain independent and
specified using features derived from EuroWordNet (Vossen,
1997; Dzikovska et al, to appear).
26
behaviour, that is, they pattern in the same set of al-
ternations, and are further assumed to share common
semantic properties.3
VERBNET classes are organised in an inheritance
hierarchy. Each class includes a set of members
(verbs), a set of (subcategorization) frames and a set
of semantic descriptions. Frames are descriptions of
the linking between syntax and semantics for that
class. Each frame argument contains a syntactic cat-
egory augmented with syntactic features, and a cor-
responding thematic role. Each class also specifies
a set of additional selectional restriction features.
VERBNET further includes for each class a semantic
description stated in terms of event semantics, that
we ignore in this paper.
4 Methodology
The methodology used in the mapping process con-
sists of two steps. First we translate the source,
VERBNET, to an intermediate representation best
suited for parsing purposes. Second this interme-
diate representation is translated to a specific tar-
get, here the TRIPS lexicon. At this stage of our
work, the translation from VERBNET to the inter-
mediate representation mainly concerns normalising
syntactic information coded in VERBNET to make
them easier to handle for parsing purposes, and the
translation from the intermediate representation to
the TRIPS lexicon focuses on translating semantic
information. This architecture is best understood
as a cross compilation scheme: we further expect
to reuse this intermediate representation for produc-
ing outputs for different parsers and to accept inputs
from other lexical databases such as FRAMENET.
4.1 The intermediate representation
The intermediate representation is a lexical repre-
sentation scheme mainly tailored for parsing: in this
context, a lexicon is thus made of a set of words,
each of which consists of a lemma, a syntactic cate-
gory and a list of sense definitions. Each sense def-
inition has a name and a frame. The name of the
sense definition is actually the name of the VERB-
NET class it derives from. The frame of the sense
definition has a list of arguments, each of which con-
3In practice, it turns out that there are exceptions to that hy-
pothesis (Kipper, 2005).
sists of a syntactic category, a syntactic function, a
thematic role and possibly a set of prepositions and
syntactic feature structures.
The content of the intermediate representation
uses the following data categories. Syntactic cate-
gories, thematic roles and features are those used in
VERBNET. We further add the syntactic functions
described in (Carroll et al, 1998). Specifically, two
categories left implicit in VERBNET by the use of
feature structures are made explicit here: preposi-
tional phrases (PP) and sentential arguments (S).
Each argument described in a sense definition
frame is marked with respect to its coreness status.
The coreness status aims to provide the lexicon with
an operational account for common discrepancies
between syntax and semantics descriptions. This
status may be valued as core, non-core or non-sem
and reflects the status of the argument with respect
to the syntax-semantics interface.
Indeed, there is a methodological pitfall concern-
ing the mapping between thematic roles and syntac-
tic arguments: semantic arguments are not defined
following criteria identical to those for syntactic ar-
guments. The main criterion for describing semantic
arguments is their participation in the event, situa-
tion, object described by the frame whereas the cri-
terion for describing syntactic arguments is based on
the obligatoriness or the specificity of the argument
with respect to the verb. The following example il-
lustrates such conflicts:
(1) a. It is raining
b. I am walking to the store
The It in example (1a) plays no role in the seman-
tic representation, but is obligatory in syntax since
it fills a subject position. The locative PP in exam-
ple (1b) is traditionally not treated as an argument
in syntax, rather as a modifier, hence it does not fill
a complement position. Such phrases are, however,
classified in VERBNET as part of the frames. Fol-
lowing this, we distinguish three kinds of arguments:
non-sem as in (1a) are syntactic-only arguments with
no semantic contribution. non-core as in (1b) con-
tribute to the semantics but are not subcategorized.
27
4.2 From VERBNET to the intermediate
representation
Given VERBNET as described in Section 3 and the
intermediate representation we described above, the
translation process requires mainly (1) to turn the
class based representation of VERBNET into a list-
of-word based representation (2) to mark arguments
for coreness (3) to merge some arguments and (4) to
annotate arguments with syntactic functions.
The first step is quite straightforward. Every
member m of every VERBNET class C is associated
with every frame of C yielding a new sense defini-
tion in the intermediate representation for m.
In the second step, each argument receives a core-
ness mark. Arguments marked as non-core are ad-
verbs, and prepositional phrases introduced by a
large class of prepositions (e.g. spatial preposi-
tions). The arguments marked as non-sem are those
with an impersonal it, typically members of the
weather class. All other arguments listed in VERB-
NET frames are marked as core.
In the third step, syntactic arguments are merged
to correspond better to phrase-based syntax.4 For
example, the VERBNET encoding of subcategoriza-
tion frames splits prepositional frames on two slots:
one for the preposition and one for the noun phrase.
We have merged the two arguments, to become a
PP, also merging their syntactic and semantic fea-
tures. Other merges at this stage include merging
possessive arguments such as John?s brother which
are described with three argument slots in VERB-
NET frames. We merged them as a single NP.
The last step in the translation is the inference of
syntactic functions. It is possible to reasonably infer
syntactic functions from positional arguments and
syntactic categories by (a) considering the follow-
ing oblicity order over the set of syntactic functions
used in the intermediate representation:5
(2) NCSUBJ < DOBJ < OBJ2 < {IOBJ, XCOMP,CCOMP}
4We also relabel some categories for convenience without
affecting the process. For instance, VERBNET labels both
clausal arguments and noun phrases with the category NP. The
difference is made with syntactic features. We take advantage
of the features to relabel clausal arguments with the category S.
5This order is partial, such that the 3 last functions are un-
ordered wrt to each other. These functions are the subset of the
functions described in (Carroll et al, 1998) relevant for han-
dling VERBNET data.
and by (b) considering this problem as a transduc-
tion problem over two tapes. One tape being the tape
of syntactic categories and the second the tape of
syntactic functions. Given that, we designed a trans-
ducer that implements a category to function map-
ping. It implements the above oblicity order together
with an additional mapping constraint: nouns can
only map to NCSUBJ, DOBJ, prepositional phrases
can only map to OBJ2, IOBJ, infinitival clauses can
only map to XCOMP and finite clauses to CCOMP.
We further added refinements to account for
frames that do not encode their arguments follow-
ing the canonical oblicity order: for dealing with
dative shift encoded in VERBNET with two differ-
ent frames and for dealing with impersonal contexts,
so that we eventually used the transducer in Figure
2. All states except 0 are meant to be final. The
transduction operates only on core and non-sem ar-
guments, non-core arguments are systematically as-
sociated with an adjunct function. This transducer is
capable of correctly handling the majority of VERB-
NET frames, finding a functional assignment for
more than 99% of the instances.
0 1
2
NP:ncSubj
NP:
? 3
PP: Dobj, Iobj
PP:Iobj
PP:Iobj
S[inf]: Xcomp
S[fin]:Ccomp
S[inf]: Dobj, Xcomp
S[fin]: Dobj, Ccomp
S[fin
]:Cc
omp
S[in
f]:Xc
omp
it[+be]:SUBJ
NP: Iobj,Obj2
4
?:Dobj
Adj:Adj
Adj:Adj
Adj:Adj
Figure 2: A transducer for assigning syntactic func-
tions to ordered subcategorization frames
4.3 From Intermediate representation to TRIPS
Recall that a TRIPS lexical entry is comprised of an
LF type with a set of semantic roles and a template
representing the mappings from syntactic functions
to semantic roles. Converting from our intermedi-
ate representation to the TRIPS format involves two
steps:
28
? For every word sense, determine the appropri-
ate TRIPS LF type
? Establish the correspondence between VERB-
NET and TRIPS syntactic and semantic argu-
ments, and generate the appropriate mapping in
the TRIPS format.
We investigated two strategies to align semantic
classes (VERBNET classes and TRIPS LFs). Both
use a class intersection algorithm as a basis for deci-
sion: two semantic classes are considered a match if
they are associated with the same lexical items.
The intersection algorithm takes advantage of the
fact that both VERBNET and TRIPS contain lexical
sets. A lexical set for VERBNET is a class name
and the set of its members, for TRIPS it is an LF
type and the set of words that are associated with it
in the lexicon. Our intersection algorithm computes
the intersection between every VERBNET lexical set
and every TRIPS lexical set. The sets which intersect
are then considered as candidate mappings from a
VERBNET class to a TRIPS class.
However, this technique produces many 1-word
class intersections, and leads to spurious entries. We
considered two ways of improving precision: first
by requiring a significantly large intersection, sec-
ond by using syntactic structure as a filter. We dis-
cuss them in turn.
4.4 Direct Mapping Between Semantic
Representations
The first technique which we tried for mapping
between TRIPS and VERBNET semantic represen-
tations is to map the classes directly. We con-
sider all candidate mappings between the TRIPS
and VERBNET classes, and take the match with the
largest intersection. We then align the semantic roles
between the two classes and produce all possible
syntax-semantics mappings specified by VERBNET.
This technique has the advantage of providing the
most complete set of syntactic frames and syntax-
semantics mappings which can be retrieved from
VERBNET. However, since VERBNET lists many
possible subcategorization frames for every word,
guessing the class incorrectly is very expensive, re-
sulting in many spurious senses generated. We use a
class intersection threshold to improve reliability.
VERBNET ROLE TRIPS ROLES
Theme LF::THEME, LF::ADDRESSEE,
LF::ALONG, LF::ENTITY
Cause LF::CAUSE, LF::THEME
Experiencer LF::EXPERIENCER, LF::COGNIZER
Source LF::FROM-LOC, LF::SOURCE,
LF::PATH
Destination LF::GOAL, LF::TO-LOC
Recipient LF::RECIPIENT, LF::ADDRESSEE,
LF::GOAL
Instrument LF::INSTRUMENT
Table 1: Sample VERBNET to TRIPS role mappings
At present, we count an LF type match as suc-
cessfully guessed if there is an intersection in lex-
ical entries above the threshold (we determined 3
words as a best value by finding an optimal balance
of precision/recall figures over a small gold-standard
mapping set). Since the classes contain closely re-
lated items, larger intersection means a more reliable
mapping. If the VERBNET class is not successfully
mapped to an LF type then no TRIPS lexical entry is
generated.
Once the correspondence between the LF type
and the VERBNET class has been established, se-
mantic arguments have to be aligned between the
two classes. We established a role mapping table
(a sample is shown in Table 1), which is an extended
version of the mapping from Swift (2005). The role
mapping is one to many (each VERBNET role maps
to 1 to 8 TRIPS roles), however, since the appropriate
LF type has been identified prior to argument map-
ping, we usually have a unique mapping based on
the roles defined by the LF type.6
Once the classes and semantic roles have been
aligned, the mapping of syntactic functions between
the intermediate representation and TRIPS syntax
is quite straightforward. Functional and category
mappings are one to one and do not raise specific
problems. Syntactic features are also translated into
TRIPS representation.
To illustrate the results obtained by the automatic
mapping process, two of the sense definitions gener-
ated for the verb relish are shown in Figure 3. The
TRIPS entries contain references to the class descrip-
tion in the TRIPS LF ontology (line introduced by
6In rare cases where more than 1 correspondence is possible,
we are using the first value in the intersection as the default.
29
;; entries
(relish
(SENSES
((EXAMPLE "The tourists admired the paintings")
(LF-PARENT LF::EXPERIENCER-EMOTION)
(TEMPL VN-EXPERIENCER-THEME-TEMPL-84))
((EXAMPLE "The children liked that the clown had a red nose")
(LF-PARENT LF::EXPERIENCER-EMOTION)
(TEMPL VN-EXPERIENCER-THEME-XP-TEMPL-87))
))
;;Templates
(VN-EXPERIENCER-THEME-TEMPL-84
(ARGUMENTS
(LSUBJ (% NP) LF::EXPERIENCER)
(LOBJ (% NP) LF::THEME)
))
(VN-EXPERIENCER-THEME-XP-TEMPL-87
(ARGUMENTS
(LSUBJ (% NP) LF::EXPERIENCER)
(LCOMP (% CP (vform fin) (ctype s-finite)) LF::THEME)
))
Figure 3: Sample TRIPS generated entries
LF-PARENT) and to a template (line introduced by
TEMPL) generated on the fly by our syntactic con-
version algorithm. The first sense definition and
template in Figure 3 represent the same information
shown graphically in Figure 1. Each argument in a
template is assigned a syntactic function, a feature
structure describing its syntactic properties, and a
mapping to a semantic role defined in the LF type
definition (not depicted here).
4.5 Filtering with syntactic structure
The approach described in the previous section pro-
vides a fairly complete set of subcategorization
frames for each word, provided that the class corre-
spondence has been established successfully. How-
ever, it misses classes with small intersections and
classes for which some but not all members match
(see Section 5 for discussion). To address these is-
sues we tried another approach that automatically
generates all possible class matches between TRIPS
and VERBNET, again using class member intersec-
tion, but using the a TRIPS syntactic template as an
additional filter on the class match. For each poten-
tial match, a human evaluator is presented with the
following:
{confidence score
{verbs in TRIPS-VN class intersection}/
LF-type TRIPS-template
=> VN-class: {VN class members}}
The confidence score is based on the number of
verbs in the intersection, weighted by taking into ac-
count the number of verbs remaining in the respec-
tive TRIPS and VERBNET classes. The template
used for filtering is taken from all templates that oc-
cur with the TRIPS words in this intersection (one
match per template is generated for inspection). For
example:
93.271%
{clutch,grip,clasp,hold,wield,grasp}/
lf::body-manipulation agent-theme-xp-templ
=> hold-15.1-1: {handle}
This gives the evaluator additional syntactic in-
formation to make the judgement on class intersec-
tions. The evaluator can reject entire class matches,
or just individual verbs from the VERBNET class
which don?t quite fit an otherwise good match. We
only used the templates already in TRIPS (those cor-
responding to each of the word senses in the inter-
section) to avoid overwhelming the evaluator with a
large number of possibly spurious template matches
resulting from an incorrect class match. This tech-
nique allows us to pick up class matches based on a
single member intersection, such as:
7.814%
{swallow}/
lf::consume agent-theme-xp-templ
=> gobble-39.3-2: {gulp,guzzle,quaff,swig}
However, the entries obtained are not guaranteed
to cover all frames in VERBNET because if a given
alternation is not already covered in TRIPS, it is not
derived from VERBNET with this method.
5 Evaluation and discussion
Since our goal in this evaluation is to balance the
coverage of VERBNET with precision, we corre-
spondingly evaluate along those two dimensions.
For both techniques, we evaluate how many word
senses were added, and the number of different
words defined and VERBNET classes covered. As a
measure of precision we use, for those entries which
were retrieved, the percentage of those which could
be taken ?as is? (good entries) and the percentage of
entries which could be taken with minor edits (for
example, changing an LF type to a more specific
subclass, or changing a semantic role in a template).
The results of evaluation are shown in Table 2.7
Since for mapping with syntax filtering we con-
sidered all possible TRIPS-VERBNET intersections,
it in effect presents an upper bound the number of
words shared between the two databases. Further
7
?nocos? table rows exclude the other cos VERBNET class,
which is exceptionally broad and skews evaluation results.
30
Class mapping Mapping with syntax filtering
Type Total Good Edit Bad %usable Total Good Edit Bad %usable
Sense 3075 1000 196 1879 0.39 11036 1688 87 9261 0.16
Word 744 274 98 372 0.5 2138 1211 153 714 0.64
Class 15 10 1 4 0.73 198 129 2 67 0.66
Sense-nocos 1136 654 196 286 0.75 7989 1493 87 6409 0.20
Word-nocos 422 218 98 106 0.75 1763 1059 153 491 0.69
Class-nocos 14 9 1 4 0.71 197 128 2 67 0.65
Table 2: Evaluation results for different acquisition techniques. %usable = (good + editable) / bad?.
extension would require extending the TRIPS LF
Ontology with additional types to cover the miss-
ing classes. As can be seen from this table, 65%
of VERBNET classes have an analogous class in
TRIPS. At the same time, there is a very large num-
ber of class intersections possible, so if all possible
intersections are generated, only a very small per-
centage of generated word senses (16%) is usable in
the combined system. Thus developing techniques
to filter out the irrelevant senses and class matches
is important for successful hand-checking.
Our evaluation also shows that while class inter-
section with thresholding provides higher precision,
it does not capture many words and verb senses. One
reason for this is data sparsity. TRIPS is relatively
small, and both TRIPS and VERBNET contain a
number of 1-word classes, which cannot be reliably
mapped without human intervention. This problem
can be alleviated in part as the size of the database
grows. We expect this technique to have better recall
when the combined lexicon is used to merge with a
different lexical database such as FRAMENET.
However, a more difficult issue to resolve is dif-
ferences in class structure. VERBNET was built
around the theory of syntactic alternations, while
TRIPS used FRAMENET structure as a starting point,
simplifying the role structure to make connection
to parsing more straightforward (Dzikovska et al,
2004). Therefore TRIPS does not require that all
words associated with the same LF type share syn-
tactic behaviour, so there are a number of VERB-
NET classes with members which have to be split
between different TRIPS classes based on additional
semantic properties. 70% of all good matches in the
filtering technique were such partial matches. This
significantly disadvantages the thresholding tech-
nique, which provides the mappings on class level,
not allowing for splitting word entries between the
classes.
We believe that the best solution can be found
by combining these two techniques. The thresh-
olding technique could be used to establish reliable
class mappings, providing classes where many en-
tries could be transferred ?as is?. The mapping can
then be examined to determine incorrect class map-
pings as well as the cases where classes should be
split based on individual words. For those entries
judged reliable in the first pass, the syntactic struc-
ture can be transferred fully and quickly, while the
syntactic filtering technique, which requires more
manual checking, can be used to transfer other en-
tries in the intersections where class mapping could
not be established reliably.
Establishing class and member correspondence is
a general problem with merging any two semantic
lexicons. Similar issues have been noted in compar-
ing FRAMENET and VERBNET (Baker and Ruppen-
hofer, 2002). A method recently proposed by Kwon
and Hovy (2006) aligns words in different seman-
tic lexicons to WordNet senses, and then aligns se-
mantic roles based on those matches. Since we are
designing a lexicon for semantic interpretation, it is
important for us that all words should be associated
with frames in a shared hierarchy, to be used in fur-
ther interpretation tasks. We are considering using
this alignment technique to further align semantic
classes, in order to produce a shared database for in-
terpretation covering words from multiple sources.
6 Conclusion
In this paper, we presented a methodology for merg-
ing lexicons including syntactic and lexical semantic
31
information. We developed a model based on cross-
compilation ideas to provide an intermediate repre-
sentation which could be used to generate entries
for different parsing formalisms. Mapping semantic
properties is the most difficult part of the process,
and we evaluated two different techniques for estab-
lishing correspondence between classes and lexical
entries, using TRIPS and VERBNET lexicons as a
case study. We showed that a thresholding technique
has a high precision, but low recall due to inconsis-
tencies in semantic structure, and data sparsity. We
can increase recall by partitioning class intersections
more finely by filtering with syntactic structure. Fur-
ther refining the mapping technique, and using it
to add mappings to other lexical databases such as
FRAMENET is part of our ongoing work.
Acknowledgements
We thank Karin Kipper for providing us useful doc-
umentation on the VERBNET feature system, and
Charles Callaway for technical help with the final
version. This material is based on work supported
by grants from the Office of Naval Research un-
der numbers N000140510048 and N000140510043,
from NSF #IIS-0328811, DARPA #NBCHD030010
via subcontract to SRI #03-000223 and NSF #E1A-
0080124.
References
C. F. Baker and J. Ruppenhofer. 2002. Framenet?s
frames vs. Levin?s verb classes. In Proceedings of the
28th Annual Meeting of the Berkeley Linguistics Soci-
ety, pages 27?38.
C. F. Baker, C. Fillmore, and J. B. Lowe. 1998. The
Berkeley Framenet project. In Proceedings of CoLing-
ACL, Montreal.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998. Parser
evaluation: A survey and a new proposal. In Proceed-
ings of LREC-98.
A. Copestake and D. Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC-2000, Athens, Greece.
B. Dorr. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?375.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004.
Building a computational lexicon and ontology with
FrameNet. In Proceedings of LREC workshop on
Building Lexical Resources from Semantically Anno-
tated Corpora, Lisbon.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. to ap-
pear. Customizing meaning: Building domain-specific
semantic representations from a generic lexicon. In
H. Bunt, editor, Computing Meaning, Volume 3, Stud-
ies in Linguistics and Philosophy. Kluwer.
M. O. Dzikovska. 2004. A Practical Semantic Repre-
sentation for Natural Language Parsing. Ph.D. thesis,
University of Rochester, Rochester NY.
C. Johnson and C. J. Fillmore. 2000. The FrameNet
tagset for frame-semantic and syntactic coding of
predicate-argument structure. In Proceedings ANLP-
NAACL 2000, Seattle, WA.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of AAAI, Austin.
K. Kipper, B. Snyder, and M. Palmer. 2004. Us-
ing prepositions to extend a verb lexicon. In Pro-
ceedings of HLT-NAACL 2004 Workshop on Compu-
tational Lexical Semantics, pages 23?29, Boston, MA.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer. 2006.
Extending Verbnet with novel verb classes. In Pro-
ceedings of LREC-2006.
K. Kipper. 2005. Verbnet: A broad coverage, compre-
hensive verb lexicon. Ph.D. thesis, University of Penn-
sylvania.
N. Kwon and E. H. Hovy. 2006. Integrating semantic
frames from multiple sources. In A. F. Gelbukh, edi-
tor, CICLing, volume 3878 of Lecture Notes in Com-
puter Science, pages 1?12. Springer.
B. Levin. 1993. English Verb Classes and Alternations.
The University of Chicago Press.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proceedings of
International Conference on Computational Linguis-
tics (COLING 2004), Geneva, Switzerland.
M. Swift. 2005. Towards automatic verb acquisition
from Verbnet for spoken dialog processing. In Pro-
ceedings of Interdisciplinary Workshop on the Identi-
fication and Representation of Verb Features and Verb
Classes, Saarbruecken, Germany.
J. Tetreault. 2005. Empirical Evaluations of Pronoun
Resolution. Ph.D. thesis, University of Rochester.
P. Vossen. 1997. Eurowordnet: A multilingual database
for information retrieval. In Proceedings of the Delos
workshop on Cross-language Information Retrieval.
32
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 49?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Deep Linguistic Processing for Spoken Dialogue Systems
James Allen
Department of Computer Science
University of Rochester
james@cs.rochester.edu
Myroslava Dzikovska
ICCS-HCRC
University of Edinburgh
mdzikovs@inf.ed.ac.uk
Mehdi Manshadi
Department of Computer Science
University of Rochester
mehdih@cs.rochester.edu
Mary Swift
Department of Computer Science
University of Rochester
swift@cs.rochester.edu
Abstract
We describe a framework for deep linguis-
tic processing for natural language under-
standing in task-oriented spoken dialogue
systems. The goal is to create domain-
general processing techniques that can be
shared across all domains and dialogue
tasks, combined with domain-specific op-
timization based on an ontology mapping
from the generic LF to the application  on-
tology. This framework has been tested in
six domains that involve tasks such as in-
teractive planning, coordination operations,
tutoring, and learning.
1 Introduction
Deep linguistic processing is essential for spoken
dialogue systems designed to collaborate with us-
ers to perform collaborative tasks. We describe the
TRIPS natural language understanding system,
which is designed for this purpose. As we develop
the system, we are constantly balancing two com-
peting needs: (1) deep semantic accuracy: the need
to produce the semantically and pragmatically deep
interpretations for a specific application; and (2)
portability: the need to reuse our grammar, lexicon
and discourse interpretation processes across do-
mains.
We work to accomplish portability by using a
multi-level representation. The central components
are all based on domain general representations,
including a linguistically based detailed semantic
representation (the Logical Form, or LF), illocu-
tionary acts, and a collaborative problem-solving
model. Each application then involves using a do-
main-specific ontology and reasoning components.
The generic LF is linked to the domain-specific
representations by a set of ontology mapping rules
that must be defined for each domain. Once the
ontology mapping is defined, we then can auto-
matically specialize the generic grammar to use the
stronger semantic restrictions that arise from the
specific domain. In this paper we mainly focus on
the generic components for deep processing. The
work on ontology mapping and rapid grammar ad-
aptation is described elsewhere (Dzikovska et al
2003; forthcoming).
2 Parsing for deep linguistic processing
The parser uses a broad coverage, domain-
independent lexicon and grammar to produce the
LF. The LF is a flat, unscoped representation that
includes surface speech act analysis, dependency
information, word senses (semantic types) with
semantic roles derived from the domain-
independent language ontology, tense, aspect, mo-
dality, and implicit pronouns. The LF supports
fragment and ellipsis interpretation, discussed in
Section 5.2
2.1 Semantic Lexicon
The content of our semantic representation comes
from a domain-independent ontology linked to a
domain-independent lexicon.  Our syntax relies on
a frame-based design in the LF ontology, a com-
mon representation in semantic lexicons (Baker et
al., 1998, Kipper et al, 2000). The LF type hierar-
chy is influenced by argument structure, but pro-
vides a more detailed level of semantic analysis
than found in most broad coverage parsers as it
distinguishes senses even if the senses take the
same argument structure, and may collapse lexical
entries with different argument structures to the
same sense. As a very simple example, the generic
lexicon includes the senses for the verb take shown
49
in Figure 1. Our generic senses have been inspired
by FrameNet (Baker et al, 1998).
In addition, types are augmented with semantic
features derived from EuroWordNet (Vossen et al,
1997) and extended. These are used to provide se-
lectional restrictions, similar to VerbNet (Kipper et
al., 2000). The constraints are intentionally weak,
excluding utterances unsuitable in most contexts
(the idea slept) but not attempting to eliminate
borderline combinations.
The generic selectional restrictions are effective
in improving overall parsing accuracy, while re-
maining valid across multiple domains. An
evaluation with an earlier version of the grammar
showed that if generic selectional restrictions were
removed, full sentence semantic accuracy de-
creased from 77.8% to 62.6% in an emergency
rescue domain, and from 67.9 to 52.5% in a medi-
cal domain (using the same versions of grammar
and lexicon) (Dzikovska, 2004).
The current version of our generic lexicon con-
tains approximately 6400 entries (excluding mor-
phological variants), and the current language on-
tology has 950 concepts. The lexicon can be sup-
plemented by searching large-scale lexical re-
sources such as WordNet (Fellbaum, 1998) and
Comlex (Grisham et al, 1994). If an unknown
word is encountered, an underspecified entry is
generated on the fly. The entry incorporates as
much information from the resource as possible,
such as part of speech and syntactic frame. It is
assigned an underspecified semantic classification
based on correspondences between our language
ontology and WordNet synsets.
2.2 Grammar
The grammar is context-free, augmented with fea-
ture structures and feature unification, motivated
from X-bar theory, drawing on principles from
GPSG (e.g., head and foot features) and HPSG. A
detailed description of an early non-lexicalized
version of the formalism is in (Allen, 1995). Like
HPSG, our grammar is strongly lexicalized, with
the lexical features defining arguments and com-
plement structures for head words. Unlike HPSG,
however, the features are not typed and rather than
multiple inheritance, the parser supports a set of
orthogonal single inheritance hierarchies to capture
different syntactic and semantic properties. Struc-
tural variants such as passives, dative shifts, ger-
unds, and so on are captured in the context-free
rule base. The grammar has broad coverage of
spoken English, supporting a wide range of con-
versational constructs. It also directly encodes
conventional conversational acts, including stan-
dard surface speech acts such as inform, request
and question, as well as acknowledgments, accep-
tances, rejections, apologies, greetings, corrections,
and other speech acts common in conversation.
To support having both a broad domain-general
grammar and the ability to produce deep domain-
specific semantic representations, the semantic
knowledge is captured in three distinct layers (Fig-
ure 2), which are compiled together before parsing
to create efficient domain-specific interpretation.
The first level is primarily encoded in the gram-
mar, and defines an interpretation of the utterance
in terms of generic grammatical relations. The sec-
ond is encoded in the lexicon and defines an inter-
pretation in terms of a generic language-based on-
tology and generic roles. The third is encoded by a
set of ontology-mapping rules that are defined for
each domain, and defines an interpretation in terms
of the target application ontology. While these lev-
els are defined separately, the parser can produce
all three levels simultaneously, and exploit do-
main-specific semantic restrictions to simultane-
ously improve semantic accuracy and parsing effi-
ciency. In this paper we focus on the middle level,
the generic LF.
CONSUME Take an aspirin
MOVE Take it to the store
ACQUIRE Take a picture
SELECT I?ll take that one
COMPATIBLE
WITH
The projector takes 100 volts
TAKE-TIME It took three hours
 Figure 1: Some generic senses of take in lexicon
50
The rules in the grammar are weighted, and
weights are combined, similar to how probabilities
are computed in a PCFG. The weights, however,
are not strictly probabilities (e.g., it is possible to
have weights greater than 1); rather, they encode
structural preferences. The parser operates in a
best-first manner and as long as weights never ex-
ceed 1.0, is guaranteed to find the highest weighted
parse first. If weights are allowed to exceed 1.0,
then the parser becomes more ?depth-first? and it
is possible to ?garden-path? and find globally sub-
optimal solutions first, although eventually all in-
terpretations can still be found.
The grammar used in all our applications uses
these hand-tuned rule weights, which have proven
to work relatively well across domains. We do not
use a statistical parser based on a trained corpus
because in most dialogue-system projects, suffi-
cient amounts of training data are not available and
would be too time consuming to collect. In the one
domain in which we have a reasonable amount of
training data (about 9300 utterances), we experi-
mented with a PCFG using trained probabilities
with the Collins algorithm, but were not able to
improve on the hand-tuned preferences in overall
performance (Elsner et al, 2005).
Figure 3 summarizes some of the most impor-
tant preferences encoded in our rule weights. Be-
cause we are dealing with speech, which is often
ungrammatical and fragmented, the grammar in-
cludes ?robust? rules (e.g., allowing dropped de-
terminers) that would not be found in a grammar of
written English.
3 The Logical Form Language
The logical form language captures a domain-
independent semantic representation of the utter-
ance. As shown later in this paper, it can be seen as
a variant of MRS (Copestake et al, 2006) but is
expressed in a frame-like notation rather than
predicate calculus. In addition, it has a relatively
simple method of computing possible quantifier
scoping, drawing from the approaches by (Hobbs
& Shieber, 1987) and (Alshawi, 1990).
A logical form is set of terms that can be viewed
as a rooted graph with each term being a node
identified by a unique ID (the variable). There are
three types of terms. The first corresponds to gen-
eralized quantifiers, and is on the form (<quant>
<id> <type> <modifiers>*). As a simple example,
the NP Every dog would be captured by the term
(Every d1 DOG). The second type of term is the
propositional term, which is represented in a neo-
Davidsonian representation (e.g., Parsons, 1990)
using reified events and properties. It has the form
(F <id> <type> <arguments>*). The propositional
terms produced from Every dog hates a cat would
be (F h1 HATE :Experiencer d1 :Theme c1).  The
third type of term is the speech act, which has the
same form as propositional terms except for the
initial indicator SA identifying it as a performed
speech act. The speech act for Every dog hates a
cat would be (SA sa1 INFORM :content h1). Put-
ting this all together, we get the following (con-
densed) LF representation from the parser for
Every large dog hates a cat (shown in graphical
Figure 2: The Levels of Representation computed by the Parser
Prefer
? Interpretations without gaps to those with gaps
? Subcategorized interpretations over adjuncts
? Right attachment of PPs and adverbials
? Fully specified constituents over those with
dropped or ?implicit? arguments
? Adjectival modification over noun-noun modifi-
cation
? Standard rules over ?robust? rules
Figure 3: Some Key Preferences used in Parsing
51
form in Figure 4).
(SA x1 TELL :content x2)
(F x2 HATE :experience x3 :theme x5)
(Every x3 DOG :mods  (x4))
(F x4 LARGE :of x3)
(A x5 CAT)
4 Comparison of LF and MRS
Minimal Recursion Semantics (MRS) (Copestake
et al 2006) is a semantic formalism which has
been widely adopted in the last several years. This
has motivated some research on how this formal-
ism compares to some traditional semantic for-
malisms. For example, Fuchss et al (2004) for-
mally show that the translation from MRS to
Dominance Constraints is feasible. We have also
found that MRS is very similar to LF in its de-
scriptive power. In fact, we can convert every LF
to an equivalent MRS structure with a simple algo-
rithm.
First, consider the sentence Every dog hates a
cat. Figure 5 shows the LF and MRS representa-
tions for this sentence.
Figure 5: The LF (left) and MRS (right) representations
for the sentence ?Every dog hates a cat.?
The first step toward converting LF to MRS is to
express LF terms as n-ary relationships. For exam-
ple we express the LF term (F v1 Hate
:Experiencer x :Theme y) as Hate(x, y). For quanti-
fier terms, we break the LF term into two relations:
one for the quantifier itself and one for the restric-
tion. For example (Every x Dog) is converted to
Every(x) and Dog(x).
There is a small change in the conversion proce-
dure when the sentence contains some modifiers.
Consider the modifier large in the sentence Every
large dog hates a cat. In the LF, we bring the
modifier in the term which defines the semantic
head, using a :MODS slot. In the MRS, however,
modifiers are separate EPs labeled with same han-
dle as the head?s. To cover this, for each LF term T
which has a (:MODS v
k
) slot,  and the LF term T1
which defines the variable v
k
, we assign the same
handle to both T and T1. For example for the terms
(F x Dog :MODS v2) and (F v2 Large :OF x), we
assign the same handle to both Dog(x) and
Large(x). Similar approach applies when the modi-
fier itself is a scopal term, such as in the sentence
Every cat in a room sleeps. Figure 7 shows LF and
MRS representations for this sentence. Figure 8,
summarizes all these steps as an algorithm which
takes a LF representation as the input and gener-
ates its equivalent MRS.
There is a small change in the conversion proce-
dure when the sentence contains some modifiers.
Consider the modifier large in the sentence Every
large dog hates a cat. In the LF, we bring the
modifier in the term which defines the semantic
head, using a :MODS slot. In the MRS, however,
modifiers are separate EPs labeled with same han-
dle as the head?s. To cover this, for each LF term T
which has a (:MODS v
k
) slot,  and the LF term T1
which defines the variable v
k
, we assign the same
handle to both T and T1. For example for the terms
(F x Dog :MODS v2) and (F v2 Large :OF x), we
assign the same handle to both Dog(x) and
Large(x). Similar approach applies when the modi-
fier itself is a scopal term, such as in the sentence
Every cat in a room sleeps. Figure 7 shows LF and
MRS representations for this sentence. Figure 8,
summarizes all these steps as an algorithm which
takes a LF representation as the input and gener-
ates its equivalent MRS.
The next step is to bring handles into the repre-
Figure 4: The LF in graphical form
Figure 6: The steps of converting the LF for
?Every cat hates a cat? to its MRS representation
52
sentation. First, we assign a different handle to
each term. Then, for each quantifier term such as
Every(x), we add two handles as the arguments of
the relation: one for the restriction and one for the
body as in h2: Every(x, h6, h7). Finally, we add the
handle constraints to the MRS. We have two types
of handle constraint. The first type comes from the
restriction of each quantifier. We add a qeq rela-
tionship between the restriction handle argument of
the quantifier term and the handle of the actual re-
striction term. The second type of constraint is the
qeq relationship which defines the top handle of
the MRS. The speech act term in every LF refers to
a formula term as content (:content slot), which is
actually the heart of the LF. We build a qeq rela-
tionship between h0 (the top handle) and the han-
dle of this formula term. Figure 6 shows the effect
of applying these steps to the above example.
Figure 7: The LF and MRS representations for the sen-
tence ?Every cat in a room sleeps.?
Another interesting issue about these two formal-
isms is that the effect of applying the simple scop-
ing algorithms referred in section 3 to generate all
possible interpretations of a LF is the same as ap-
plying MRS axioms and handle constraints to gen-
erate all scope-resolved MRSs. For instance, the
example in (Copestake et al 2006), Every nephew
of some famous politician saw a pony has the same
5 interpretations using either approach.
As the last point here, we need to mention that
the algorithm in Figure 8 does not consider fixed-
scopal terms such as scopal adverbials or negation.
However, we believe that the framework itself is
able to support these types of scopal term and with
a small modification, the scoping algorithm will
work well in assigning different possible interpre-
tations. We leave the full discussion about these
details as well as the detailed proof of the other
claims we made here to another paper.
5 Generic Discourse Interpretation
With a generic semantic representation, we can
then define generic discourse processing capabili-
ties that can be used in any application. All of
these methods have a corresponding capability at
the domain-specific level for an application, but we
will not discuss this further here. We also do not
discuss the support for language generation which
uses the same discourse context.
There are three core discourse interpretation ca-
pabilities that the system provides: reference reso-
lution, ellipsis processing, and speech act interpre-
tation. All our different dialog systems use the
same discourse processing, whether the task in-
volves collaborative problem solving, learning
from instruction or automated tutoring.
5.1 Reference Resolution
Our domain-independent representation supports
reference resolution in two ways. First, the quanti-
fiers and dependency structure extracted from the
sentence allow for implementing reference resolu-
tion algorithms based on extracted syntactic fea-
tures. The system uses different strategies for re-
Figure 8: The LF-MRS conversion algorithm
53
solving each type of referring expression along the
lines described in (Byron, 2002).
Second, domain-independent semantic informa-
tion helps greatly in resolving pronouns and defi-
nite descriptions. The general capability provided
for resolving referring expressions is to search
through the discourse history for the most recent
entity that matches the semantic requirements,
where recency within an utterance may be reor-
dered to reflect focusing heuristics (Tetreault,
2001). For definite descriptions, the semantic in-
formation required is explicit in the lexicon. For
pronouns, the parser can often compute semantic
features from verb argument restrictions.  For in-
stance, the pronoun it carries little semantic infor-
mation by itself, but in the utterance Eat it we
know we are looking for an edible object. This
simple technique performs well in practice.
Because of the knowledge in the lexicon for role
nouns such as author, we can also handle simple
bridging reference. Consider the discourse frag-
ment That book came from the library. The author
?. The semantic representation of the author in-
cludes its implicit argument, e.g., (The x1
AUTHOR :of b1). Furthermore, the term b1 has
the semantic feature INFO-CONTENT, which in-
cludes objects that ?contain? information such as
books, articles, songs, etc.., which allows the pro-
noun to correctly resolve via bridging to the book
in the previous utterance.
5.2 Ellipsis
The parser produces a representation of fragmen-
tary utterances similar to (Schlangen and Las-
carides, 2003). The main difference is that instead
of using a single underspecified unknown_rel
predicate to resolve in discourse context, we use a
speech act term as the underspecified relation, dif-
ferentiating between a number of common rela-
tions such as acknowledgments, politeness expres-
sions, noun phrases and underspecified predicates
(PP, ADJP and VP fragments). The representations
of the underspecified predicates also include an
IMPRO in place of the unspecified argument.
We currently handle only a few key cases of el-
lipsis. The first is question/answer pairs. By re-
taining the logical form of the question in the dis-
course history, it is relatively easy to reconstruct
the full content of short answers (e.g., in Who ate
the pizza? John? the answer maps to the represen-
tation that John ate the pizza).  In addition, we
handle common follow-up questions  (e.g., Did
John buy a book? How about a magazine?) by per-
forming a semantic closeness matching of the
fragment into the previous utterance and substitut-
ing the most similar terms. The resulting term can
then be used to update the context. This process is
similar to the resolution process in (Schlangen and
Lascarides, 2003), though the syntactic parallelism
constraint is not checked. It could also be easily
extended to cover other fragment types, as the
grammar provides all the necessary information.
5.3 Speech Act Interpretation
The presence of domain-independent semantic
classes allows us to encode a large set of these
common conversational pattern independently of
the application task and domain. These include
rules to handle short answers to questions, ac-
knowledgements and common politeness expres-
sions, as well as common inferences such as inter-
preting I need to do X as please do X.
Given our focus on problem solving domains,
we are generally interested in identifying more
than just the illocutionary force of an utterance.
For instance, in a domain for planning how to
evacuate people off an island, the  utterance Can
we remove the people by helicopter? is not only
ambiguous between being a true Y-N question or a
suggestion of a course of action, but at the problem
solving level it might intended to (1) introduce a
new goal, (2)  elaborate or extend the solution to
the current problem, or (3) suggest a modification
to an existing solution (e.g., moving them by
truck). One can only choose between these read-
ings using domain specific reasoning about the
current task. The point here is that the interpreta-
tion rules are still generic across all domains and
expressed using the generic LF, yet the interpreta-
tions produced are evaluated using domain-specific
reasoning. This interleaving of generic interpreta-
tion and domain-specific reasoning is enabled by
our ontology mappings.
Similarly, in tutoring domains students often
phrase their answers as check questions. In an an-
swer to the question Which components are in a
closed path, the student may say Is the bulb in 3 in
a closed path? The domain-independent represen-
tation is used to identify the surface form of this
utterance as a yes-no question. The dialogue man-
ager then formulates two hypotheses: that this is a
hedged answer, or a real question. If a domain-
54
specific tutoring component confirms the former
hypothesis, the dialogue manager will proceed
with verifying answer correctness and carrying on
remediation as necessary. Otherwise (such as for Is
the bulb in 5 connected to a battery in the same
context), the utterance is a question that can be
answered by querying the domain reasoner.
5.4 A Note on Generic Capabilities
A key point is that these generic discourse inter-
pretation capabilities are enabled because of the
detailed generic semantic interpretation produced
by the parser. If the parser produced a more shal-
low representation, then the discourse interpreta-
tion techniques would be significantly degraded.
On the other hand, if we developed a new repre-
sentation for each domain, then we would have to
rebuild all the discourse processing for the domain.
6 Evaluation
Our evaluation is aimed at assessing two main
features of the grammar and lexicon: portability
and accuracy. We use two main evaluation criteria:
full sentence accuracy, that takes into account both
syntactic and semantic accuracy of the system, and
sense tagging accuracy, to demonstrate that the
word senses included in the system can be distin-
guished with a combination of syntactic and do-
main-independent semantic information.
As a measure of the breadth of grammatical
coverage of our system, we have evaluated our
coverage on the CSLI LKB (Linguistic Knowledge
Building) test suite (Copestake, 1999). The test
suite contains approximately 1350 sentences, of
which about 400 are ungrammatical. We use a full-
sentence accuracy measure to evaluate our cover-
age, since this is the most meaningful measure in
terms of what we require as parser output in our
applications. For a sentence representation to be
counted as correct by this measure, both the syn-
tactic structure and the semantic representation
must be correct, which includes the correct as-
signment of word senses, dependency relations
among terms, and speech act type. Our current
coverage for the diverse grammatical phenomena
in the corpus is 64% full-sentence accuracy.
We also report the number of spanning parses
found, because in our system there are cases in
which the syntactic parse is correct, but an incor-
rect word sense may have been assigned, since we
disambiguate senses using not only syntactic
structure but also semantic features as selectional
restrictions on arguments. For example, in The
manager interviewed Browne after working, the
parser assigns working the sense LF::FUNCTION,
used with non-agentive subjects, instead of the cor-
rect sense for agentive subjects, LF::WORKING.
For the grammatical utterances in the test suite, our
parser found spanning parses for 80%.
While the ungrammatical sentences in the set are
an important tool for constraining grammar output,
our grammar is designed to find a reasonable inter-
pretation for natural speech, which often is less
than perfect. For example, we have low preference
grammar rules that allow dropped subjects, miss-
ing determiners, and wrong subject verb agree-
ment. In addition, utterances are often fragmentary,
so even those without spanning parses may be con-
sidered correct. Our grammar allows all major con-
stituents (NP, VP, ADJP, ADVP) as valid utter-
ances. As a result, our system produces spanning
parses for 46% of the ?ungrammatical? utterances.
We have not yet done a detailed error analysis.
As a measure of system portability to new do-
mains, we have evaluated our system coverage on
the ATIS (Airline Travel Information System)
speech corpus, which we have never used before.
For this evaluation, the proper names (cities, air-
ports, airline companies) in the ATIS corpus were
added to our lexicon, but no other development
work was performed. We parsed 116 randomly
selected test sentences and hand-checked the re-
sults using our full-sentence accuracy measure.
Our baseline coverage of these utterances is 53%
full-sentence semantic accuracy. Of the 55 utter-
ances that were not completely correct, we found
spanning parses for 36% (20). Reasons that span-
ning parses were marked as wrong include incor-
rect word senses (e.g., for stop in I would like it to
have a stop in Phoenix) or PP-attachment. Reasons
that no spanning parse was found include missing
senses for existing words (e.g., serve as in Does
that flight serve dinner).
7 Discussion
We presented a deep parser and semantic inter-
preter for use in dialogue systems. An important
question to ask is how it compares to other existing
formalisms. At present there is no easy way to
make such comparison. One possible criterion is
grammatical coverage. Looking at the grammar
coverage/accuracy on the TSNLP suite that was
55
used to evaluate the LINGO ERG grammar, our
grammar demonstrates 80% coverage (number of
spanning parses). The reported figure for LINGO
ERG coverage of CSLI is 77% (Oepen, 1999), but
this number has undoubtedly improved in the  9-
year development period. For example, the current
reported coverage figures on spoken dialogue cor-
pora are  close to 90% (Oepen et al, 2002).
However, the grammar coverage alone is not a
satisfactory measure for a deep NLP system for use
in practical applications, because the logical forms
and therefore the capabilities of deep NLP systems
differ significantly. A major distinguishing feature
of our system is that the logical form it outputs
uses semantically motivated word senses. LINGO
ERG, in contrast, contains only syntactically moti-
vated word senses. For example, the words end and
finish are not related in any obvious way. This re-
flects a difference in underlying philosophy.
LINGO ERG aims for linguistic precision, and as
can be seen from our experiments, requiring the
parser to select correct domain-independent word
senses lowers accuracy.
Our system, however, is built with the goal of
easy portability within the context of dialogue
systems. The availability of word senses simplifies
the design of domain-independent interpretation
components, such as reference resolution and
speech act interpretation components that use do-
main-independent syntactic and semantic informa-
tion to encode conventional interpretation rules.
If the LINGO ERG grammar were to be put in a
dialogue system that requires domain interpretation
and reasoning, an additional lexical interpretation
module would have to be developed to perform
word sense disambiguation as well as interpreta-
tion, something that has not yet been done.
Acknowledgments
We thank 3 reviewers for helpful comments. This
work was supported by NSF IIS-0328811, DARPA
NBCHD30010 via subcontract to SRI #03-000223
and ONR N00014051004-3 and ?8.
References
H. Alshawi. 1990. Resolving Quasi Logical Forms.
Computational Linguistics 16(3):133-144.
W. Baker, C. Fillmore and J. B. Lowe. 1998. The Ber-
keley FrameNet Project. COLING-ACL'98, Montr?al.
D. Byron. 2002. Resolving Pronominal Reference to
Abstract Entities. ACL-02, Philadelphia.
A. Copestake. 1999. The (New) LKB System. CSLI.
A. Copestake, D. Flickinger, C. Pollard and I. Sag.
2006. Minimal Recursion Semantics: An Introduc-
tion. Research on Language and Computation,
3(4):281-332.
M. Dzikovska. 2004. A Practical Semantic Representa-
tion for Natural Language Parsing. Ph.D. Thesis,
University of Rochester.
M. Dzikovska, J. Allen and M. Swift. Forthcoming.
Linking Semantic and Knowledge Representations in
a Multi-domain Dialogue System. Journal of Logic
and Computation.
M. Dzikovska, J. Allen and M. Swift. 2003. Integrating
Linguistic and Domain Knowledge for Spoken Dia-
logue Systems in Multiple Domains. Workshop on
Knowledge and Reasoning in Practical Dialogue
Systems, IJCAI-2003, Acapulco.
M. Elsner, M. Swift, J. Allen and D. Gildea. 2005. On-
line Statistics for a Unification-based Dialogue
Parser. IWPT05, Vancouver.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
R. Fuchss, A. Koller, J. Niehren, S. Thater. 2004.
Minimal Recursion Semantics as Dominance Con-
straints. ACL-04, Barcelona.
R. Grisham, C. Macleod and A. Meyers. 1994. Comlex
Syntax: Building a Computational Lexicon. COLING
94, Kyoto.
J. Hobbs and S. Shieber. 1987. An Algorithm for Gen-
erating Quantifier Scopings. Computational Linguis-
tics 13(1-2):47-63.
K. Kipper, H. T. Dang and M. Palmer. 2000.  Class-
based Construction of a Verb Lexicon. AAAI-2000.
S. Oepen, D. Flickinger, K. Toutanova and C. Manning.
2002. Lingo Redwoods: A Rich and Dynamic Tree-
bank for HPSG. First Workshop on Treebanks and
Linguistic Theories (TLT2002).
S. Oepen (1999). [incr tsdb()] User Manual.
www.delph-in.net/itsdb/publications/manual.ps.gz.
T. Parsons. 1990. Events in the Semantics of English. A
Study in Subatomic Semantics. MIT Press.
D. Schlangen and A. Lascarides 2003. The Interpreta-
tion of Non-Sentential Utterances in Dialogue. SIG-
DIAL-03, Sapporo.
J. Tetreault. 2001. A Corpus-Based Evaluation of Cen-
tering and Pronoun Resolution. Computational Lin-
guistics. 27(4):507-520.
Vossen, P. (1997) EuroWordNet: A Multilingual Data-
base for Information Retrieval. In Proc. of the Delos
workshop on Cross-language Information Retrieval.
56
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 112?119,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Extracting a verb lexicon for deep parsing from FrameNet
Mark McConville and Myroslava O. Dzikovska
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, Scotland
{Mark.McConville,M.Dzikovska}@ed.ac.uk
Abstract
We examine the feasibility of harvesting
a wide-coverage lexicon of English verbs
from the FrameNet semantically annotated
corpus, intended for use in a practical natural
language understanding (NLU) system. We
identify a range of constructions for which
current annotation practice leads to prob-
lems in deriving appropriate lexical entries,
for example imperatives, passives and con-
trol, and discuss potential solutions.
1 Introduction
Although the lexicon is the primary source of infor-
mation in lexicalised formalisms such as HPSG or
CCG, constructing one manually is a highly labour-
intensive task. Syntactic lexicons have been derived
from other resources ? the LinGO ERG lexicon
(Copestake and Flickinger, 2000) contains entries
extracted from ComLex (Grishman et al, 1994),
and Hockenmaier and Steedman (2002) acquire a
CCG lexicon from the Penn Treebank. However,
one thing these resources lack is information on how
the syntactic subcategorisation frames correspond to
meaning.
The output representation of many ?deep? wide
coverage parsers is therefore limited with respect to
argument structure ? sense distinctions are strictly
determined by syntactic generalisations, and are
not always consistent. For example, in the logi-
cal form produced by the LinGO ERG grammar,
the verb end can have one of two senses depend-
ing on its subcategorisation frame: end v 1 rel
or end v cause rel, corresponding to the cel-
ebrations ended and the storm ended the celebra-
tions respectively. Yet a very similar verb, stop, has
a single sense, stop v 1 rel, for both the cele-
brations stopped and the storm stopped the celebra-
tions. There is no direct connection between these
different verbs in the ERG lexicon, even though
they are intuitively related and are listed as belong-
ing to the same or related word classes in semantic
lexicons/ontologies such as VerbNet (Kipper et al,
2000) and FrameNet (Baker et al, 1998).
If the output of a deep parser is to be used with
a knowledge representation and reasoning compo-
nent, for example in a dialogue system, then we need
a more consistent set of word senses, linked by spec-
ified semantic relations. In this paper, we investi-
gate how straightforward it is to harvest a compu-
tational lexicon containing this kind of information
from FrameNet, a semantically annotated corpus of
English. In addition, we consider how the FrameNet
annotation system could be made more transparent
for lexical harvesting.
Section 2 introduces the FrameNet corpus, and
section 3 discusses the lexical information required
by frame-based NLU systems, with particular em-
phasis on linking syntactic and semantic structure.
Section 4 presents the algorithm which converts the
FrameNet corpus into a frame-based lexicon, and
evaluates the kinds of entries harvested in this way.
We then discuss a number of sets of entries which
are inappropriate for inclusion in a frame-based lex-
icon: (a) ?subjectless? entries; (b) entries derived
from passive verbs; (c) entries subcategorising for
modifiers; and (d) entries involving ?control? verbs.
112
2 FrameNet
FrameNet1 is a corpus of English sentences an-
notated with both syntactic and semantic informa-
tion. Underlying the corpus is an ontology of
795 ?frames? (or semantic types), each of which
is associated with a set of ?frame elements? (or
semantic roles). To take a simple example, the
Apply heat frame describes a situation involving
frame elements such as a COOK, some FOOD, and
a HEATING INSTRUMENT. Each frame is, in addi-
tion, associated with a set of ?lexical units? which
are understood as evoking it. For example, the
Apply heat frame is evoked by such verbs as
bake, blanch, boil, broil, brown, simmer, steam, etc.
The FrameNet corpus proper consists of 139,439
sentences (mainly drawn from the British National
Corpus), each of which has been hand-annotated
with respect to a particular target word in the sen-
tence. Take the following example: Matilde fried
the catfish in a heavy iron skillet. The process of an-
notating this sentence runs as follows: (a) identify a
target word for the annotation, for example the main
verb fried; (b) identify the semantic frame which is
evoked by the target word in this particular sentence
? in this case the relevant frame is Apply heat;
(c) identify the sentential constituents which realise
each frame element associated with the frame, i.e.:
[COOK Matilde] [Apply heat fried] [FOOD the
catfish] [HEATING INSTR in a heavy iron skillet]
Finally, some basic syntactic information about the
target word and the constituents realising the vari-
ous frame elements is also added: (a) the part-of-
speech of the target word (e.g. V, N, A, PREP); (b)
the syntactic category of each constituent realising a
frame element (e.g. NP, PP, VPto, Sfin); and (c)
the syntactic role, with respect to the target word,
of each constituent realising a frame element (e.g.
Ext, Obj, Dep). Thus, each sentence in the corpus
can be seen to be annotated on at least three inde-
pendent ?layers?, as exemplified in Figure 1.
3 Frame-based NLU
The core of any frame-based NLU system is a parser
which produces domain-independent semantic rep-
1The version of FrameNet discussed in this paper is
FrameNet II release 1.3 from 22 August 2006.
resentations like the following, for the sentence John
billed the champagne to my account:
?
?
?
?
?
commerce-pay
AGENT John
THEME champagne
SOURCE
[
account
OWNER me
]
?
?
?
?
?
Deep parsers/grammars such as the ERG, OpenCCG
(White, 2006) and TRIPS (Dzikovska, 2004) pro-
duce more sophisticated representations with scop-
ing and referential information, but still contain a
frame-based representation as their core. The lex-
ical entries necessary for constructing such repre-
sentations specify information about orthography,
part-of-speech, semantic type and subcategorisation
properties, including a mapping between a syntactic
subcategorisation frame and the semantic frame.
An example of a TRIPS lexical entry is presented
in Figure 2, representing the entry for the verb bill
as used in the sentence discussed above. Note that
for each subcategorised argument the syntactic role,
syntactic category, and semantic role are specified.
Much the same kind of information is included in
ERG and OpenCCG lexical entries.
When constructing a computational lexicon, there
are a number of issues to take into account, sev-
eral of which are pertinent to the following discus-
sion. Firstly, computational lexicons typically list
only the ?canonical? subcategorisation frames, cor-
responding to a declarative sentence whose main
verb is in the active voice, as in Figure 1. Other vari-
ations, such as passive forms, imperatives and dative
alternations are generated automatically, for exam-
ple by lexical rules. Secondly, parsers that build se-
mantic representations typically make a distinction
between ?complements? and ?modifiers?. Comple-
ments are those dependents whose meaning is com-
pletely determined by the verb, for example the PP
on him in the sentence Mary relied on him, and are
thus listed in lexical entries. Modifiers, on the other
hand, are generally not specified in verb entries ?
although they may be associated with the underlying
verb frame, their meaning is determined indepen-
dently, usually by the preposition, such as the time
adverbial next week in I will see him next week.
Finally, for deep parsers, knowledge about which
argument of a matrix verb ?controls? the implicit
113
Matilde fried the catfish in a heavy iron skillet
target Apply heat
frame element COOK FOOD HEATING INSTR
syntactic category NP V NP PP
syntactic role Ext Obj Dep
Figure 1: A FrameNet annotated sentence
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ORTH ?bill?
SYNCAT v
SEMTYPE
?
?
commerce-pay
ASPECT bounded
TIME-SPAN atomic
?
?
ARGS
?
?
?
SYNROLE subj
SYNCAT np
SEMROLE agent
?
?,
?
?
SYNROLE obj
SYNCAT np
SEMROLE theme
?
?,
?
?
?
?
SYNROLE comp
SYNCAT
[
pp
PTYPE to
]
SEMROLE source
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: A TRIPS lexical entry
subject of an embedded complement verb phrase is
necessary in order to to build the correct semantic
form. In a unification parser such as TRIPS, control
is usually represented by a relation of token-identity
(i.e. feature structure reentrancy) between the sub-
ject or object of a control verb and the subject of a
verbal complement.
4 Harvesting a computational lexicon from
FrameNet
In order to harvest a computational lexicon from the
FrameNet corpus, we took each of the 60,309 an-
notated sentences whose target word is a verb and
derived a lexical entry directly from the annotated
information. For example, from the sentence in Fig-
ure 1, the lexical entry in Figure 3 is derived.2
In order to remove duplicate entries, we made two
assumptions: (a) the value of the ARGS feature is a
set of arguments, rather than, say, a list or multiset;
and (b) two arguments are identical just in case they
specify the same syntactic role and semantic role.
These assumptions prevent a range of inappropriate
entries from being created, for example entries de-
2Our original plan was to use the automatically generated
?lexical entry? files included with the most recent FrameNet re-
lease as a basis for deep parsing. However, these entries contain
so many inappropriate subcategorisation frames, of the types
discussed in this paper, that we decided to start from scratch
with the corpus annotations.
rived from sentences involving a ?split? argument,
both parts of which are annotated independently in
FrameNet, e.g. [Ext Serious concern] arose [Ext
about his motives]. A second group of inappropri-
ate entries which are thus avoided are those deriving
from relative clause constructions, where the rela-
tive pronoun and its antecedent are also annotated
separately:
[Ext Perp The two boys] [Ext Perp who] ab-
ducted [Obj Victim James Bulger] are likely to
have been his murderers
Finally, assuming that the arguments constitute a set
means that entries derived from sentences involving
both canonical3 and non-canonical word order are
treated as equivalent. The kinds of construction im-
plicated here include ?quotative inversion? (e.g. ?Or
Electric Ladyland,? added Bob), and leftwards ex-
traction of objects and dependents, for example:
Are there [Obj any places] [Ext you] want to praise
[Dep for their special facilities]?
In this paper we are mainly interested in extract-
ing the possible syntax-semantics mappings from
FrameNet, rather than the precise details of their rel-
ative ordering. Since dependents in the harvested
3The canonical word order in English involves a pre-verbal
subject, with all other dependents following the verb.
114
??
?
?
?
?
?
ORTH ?fry?
SYNCAT V
SEMTYPE Apply heat
ARGS
?
?
?
SYNROLE Ext
SYNCAT NP
SEMROLE Cook
?
?
?
?
SYNROLE Obj
SYNCAT NP
SEMROLE Food
?
?,
?
?
SYNROLE Dep
SYNCAT PP
SEMROLE Heating Instr
?
?
?
?
?
?
?
?
?
?
Figure 3: The lexical entry derived from Figure 1
lexicon are fully specified for semantic role, syn-
tactic category and syntactic role, post-verbal con-
stituent ordering can-be regulated extra-lexically by
means of precedence rules. For example, for the
TRIPS and LFG formalisms, there is a straightfor-
ward correspondence between their native syntactic
role specifications and the FrameNet syntactic roles.
After duplicate entries were removed from the re-
sulting lexicon, we were left with 26,022 distinct
entries. The harvested lexicon incorporated 2,002
distinct orthographic forms, 358 distinct frames,
and 2,661 distinct orthography-frame pairs, giving
a functionality ratio (average number of lexical en-
tries per orthography-type pair) of 9.8.
Next, we evaluated a random sample of the de-
rived lexical entries by hand. The aim here was to
identify general classes of the harvested verb entries
which are not appropriate for inclusion in a frame-
based verb lexicon, and which would need to be
identified and fixed in some way. The main groups
identified were: (a) entries with no Ext argument
(section 4.1); (b) entries derived from verbs in the
passive voice (section 4.2); (c) entries which subcat-
egorise for modifiers (section 4.3); and (d) entries
for control verbs (section 4.4).
4.1 Subjectless entries
The harvested lexicon contains 2,201 entries (i.e.
9% of the total) which were derived from sentences
which do not contain an argument labelled with the
Ext syntactic role, in contravention of the gener-
ally accepted constraint on English verbs that they
always have a subject.
Three main groups of sentences are implicated
here: (a) those featuring imperative uses of the tar-
get verb, e.g. Always moisturise exposed skin with
an effective emollient like E45; (b) those featuring
other non-finite forms of the target verb whose un-
derstood subject is not controlled by (or even coref-
erential with) some other constituent in the sentence,
e.g. Being accused of not having a sense of humour
is a terrible insult; and (c) those involving a non-
referential subject it, for example It is raining heav-
ily or It is to be regretted that the owner should have
cut down the trees. In FrameNet annotations, non-
referential subjects are not identified on the syntactic
role annotation layer, and this makes it more difficult
to harvest appropriate lexical entries for these verbs
from the corpus.
These entries are easy to locate in the harvested
lexicon, but more difficult to repair. Typically one
would want to discard the entries generated from
(a) and (b) as they will be derived automatically in
the grammar, but keep the entries generated from (c)
while adding a non-referential it as a subject.
Although the FrameNet policy is to annotate the
(a) and (b) sentences as having a ?non-overt? real-
isation of the relevant frame element, this is con-
fined to the frame element annotation layer itself,
with the syntactic role and syntactic category lay-
ers containing no clues whatsoever about understood
subjects. One rather roundabout way of differentiat-
ing between these cases would involve attempting to
identify the syntactic category and semantic role of
the missing Ext argument by looking at other en-
tries with the same orthography and semantic type.
However, this whole problem could be avoided if
understood and expletive subjects were identified on
the syntactic layers in FrameNet annotations.
4.2 ?Passive? entries
Many entries in the harvested lexicon were derived
from sentences where the target verb is used in the
passive voice, for example:
[Ext NP Victim The men] had allegedly been ab-
ducted [Dep PP Perp by Mrs Mandela?s body-
115
guards] [Dep PP Time in 1988]
As discussed above, computational lexicons do not
usually list the kinds of lexical entry derived directly
from such sentences. Thus, it is necessary to identify
and correct or remove them.
In FrameNet annotated sentences, the voice of tar-
get verbs is not marked explicitly.4 We applied the
following simple diagnostic to identify ?passive? en-
tries: (a) there is an Ext argument realising frame
element e; and (b) there is some other entry with the
same orthographic form and semantic frame, which
has an Obj argument realising frame element e.
Initially we applied this diagnostic to the entries
in the harvested lexicon together with a part-of-
speech tag filter. The current FrameNet release in-
cludes standard POS-tag information for each word
in each annotated sentence. We considered only
those lexical entries derived from sentences whose
target verb is tagged as a ?past-participle? form (i.e.
VVN). This technique identified 4,160 entries in the
harvested lexicon (i.e. 16% of the total) as being
?passive?. A random sample of 10% of these was
examined and no false positives were found.
The diagnostic test was then repeated on the re-
maining lexical entries, this time without the POS-
tag filter. This was deemed necessary in order to
pick up false negatives caused by the POS-tagger
having assigned the wrong tag to a passive target
verb (generally the past tense form tag VVD). This
test identified a further 1007 entries as ?passive? (4%
of the total entries). As well as mis-tagged instances
of normal passives, this test picked up a further three
classes of entry derived from target verbs appearing
in passive-related constructions. The first of these
involves cases where the target verb is in the com-
plement of a ?raising adjective? (e.g. tough, difficult,
easy, impossible), for example:
[Ext NP Goal Both planning and control] are dif-
ficult to achieve [Dep PP Circs in this form of
production]
The current FrameNet annotation guidelines (Rup-
penhofer et al, 2006) state that the extracted object
in these cases should be tagged as Obj. However,
in practice, the majority of these instances appear to
have been tagged as Ext.
4Whilst there are dedicated subcorpora containing only pas-
sive targets, it is not the case that all passive targets are in these.
The second group of passive-related entries in-
volve verbs in the need -ing construction5 , e.g.:
[Ext NP Content Many private problems] need
airing [Dep PP Medium in the family]
The third group involved sentences where the target
verb is used in the ?middle? construction:
[Ext Experiencer You] frighten [Dep
Manner easily]
Again, linguistically-motivated grammars generally
treat these three constructions in the rule component
rather than the lexicon. Thus, the lexical entries de-
rived from these sentences need to be located and
repaired, perhaps by comparison with other entries.
Of the 1007 lexical entries identified by the sec-
ond, weaker form of the passive test, 224 (i.e. 22%)
turn out to be false positives. The vast majority
of these involve verbs implicated in the causative-
inchoative alternation (e.g. John?s back arched vs.
John arched his back). The official FrameNet pol-
icy is to distinguish between frames encoding a
change-of-state and those encoding the causation
of such a change, for example Amalgamation
versus Cause to amalgamate, Motion versus
Cause motion etc. In each case, the two frames
are linked by the Causative of frame relation.
Most of the false positives are the result of a fail-
ure to consistently apply this principle in annotation
practice, for example where no causative counterpart
has been defined for a particular inchoative frame,
or where an inchoative target has been assigned to a
causative frame, or a causative target to an inchoa-
tive frame. For example, 94 of the false positives
are accounted for simply by the lack of a causative
counterpart for the Body movement frame, mean-
ing that both inchoative and causative uses of verbs
like arch, flutter and wiggle have all been assigned
to the same frame.
For reasons of data sparsity, it is expected that the
approach to identifying passive uses of target verbs
discussed here will result in false negatives, since it
relies on there being at least one corresponding ac-
tive use in the corpus. We checked a random sample
of 400 of the remaining entries in the harvested lex-
icon and found nine false negatives, suggesting that
5Alternatively merit -ing, bear -ing etc.
116
the test successfully identifies 91% of those lexical
entries derived from passive uses of target verbs.
4.3 Modifiers
General linguistic theory makes a distinction be-
tween two kinds of non-subject dependent of a verb,
depending on the notional ?closeness? of the seman-
tic relation ? complements vs. modifiers. Take for
example the following sentence:
[Ext Performer She]?s [Dep Time currently]
starring [Dep Performance in The Cemetery
Club] [Dep Place at the Wyvern Theatre]
Of the three constituents annotated here as Dep,
only one is an complement (the Performance);
the Time and Place dependents are modifiers.
Frame-based NLU systems do not generally list
modifiers in the argument structure of a verb?s lexi-
cal entry. Thus, we need to find a means of identify-
ing those dependents in the harvested lexicon which
are actually modifiers.
The FrameNet ontology provides some informa-
tion to help differentiate complements and modi-
fiers. A frame element can be marked as Core,
signifying that it ?instantiates a conceptually nec-
essary component of a frame, while making the
frame unique and different from other frames?. The
annotation guidelines state that the distinction be-
tween Core and non-Core frame elements cov-
ers ?the semantic spirit? of the distinction between
complements and modifiers. Thus, for example,
obligatory dependents are always Core, as are:
(a) those which, when omitted, receive a definite
interpretation (e.g. the Goal in John arrived);
and (b) those whose semantics cannot be predicted
from their form. In the Performers and roles
frame used in the example above, the Performer
and Performance frame elements are marked as
Core, whilst Time and Place are not.
However, it is not clear that the notion of on-
tological ?coreness? used in FrameNet corresponds
well with the intuitive distinction between syntactic
complements and modifiers. This is exemplified by
the existence of numerous constituents in the corpus
which have been marked as direct objects, despite
invoking non-Core frame elements, for example:
[Agent I] ripped [Subregion the top]
[Patient from my packet of cigarettes]
The relevant frame here is Damaging, where the
Subregion frame element is marked as non-
Core, based on examples like John ripped his
trousers [below the knee]. Thus in this case, the
decision to retain all senses of the verb rip within
the same frame has led to a situation where seman-
tic and syntactic coreness have become dislocated.
Thus, although the Core vs. non-Core property on
frame elements does yield a certain amount of in-
formation about which arguments are complements
and which are modifiers, greater care needs to be
taken when assigning different subcategorisation al-
ternants to the same frame. For example, it would
have been more convenient to have assigned the verb
rip in the above example to the Removing frame,
where the direct object would then be assigned the
Core frame element Theme.
In the example discussed above, FrameNet does
provide syntactic role information (Obj) allowing
us to infer that a non-Core role is a complement
rather than a modifier. Where the syntactic role is
simply marked as Dep however, it is not possible
to make the decision without recourse to other lexi-
cal resources (e.g. ComLex). Since different parsers
may utilise different criteria for distinguishing com-
plements from modifiers, it might be better to post-
pone this task to the syntactic alignment module.
4.4 Control verbs
Unification-based parsers generally handle the dis-
tinction between subject (John promised Mary to
go) and object (John persuaded Mary to go) con-
trol verbs in the lexicon, using coindexation of the
subject/object of the control verb and the understood
subject of the embedded verb. The parser can use
this lexical information to assign the correct refer-
ent to the understood subject in a sentence like John
asked Mary to go:
?
?
?
?
?
?
command
AGENT John
THEME Mary 1
EFFECT
[
motion
THEME 1
]
?
?
?
?
?
?
Control verbs are annotated in FrameNet in the fol-
lowing manner:
Perhaps [Ext NP Speaker we] can persuade
[Obj NP Addressee Tammuz] [Dep VPto
117
Content to entertain him]
The lexical entries for transitive control verbs that
we can harvest directly from these annotations thus
fail to identify whether it is the subject or the direct
object which controls the understood subject of the
embedded verb.
We attempted to automatically distinguish subject
from object control in FrameNet by looking for the
annotated sentences that contain independently an-
notated argument structures for both the control verb
and embedded verb. For example, let?s assume the
following annotation also exists in the corpus:
Perhaps we can persuade [Ext NP Agent Tam-
muz] to entertain [Obj NP Experiencer him]
We can then use the fact that it is the object of the
control verb which is coextensive with the Ext of
the embedded verb to successfully identify persuade
as an object-control verb.
The problem with this approach is data sparsity.
The harvested lexicon contains 135 distinct verbs
which subcategorise for both a direct object and
a controlled VP complement. In a random sam-
ple of ten of these none of the annotated sentences
had been annotated independently from the perspec-
tive of the governed verb. As the proportion of the
FrameNet corpus which involves annotation of run-
ning text, rather than cherry-picked example sen-
tences, increases, we would expect this to improve.6
5 Implementation and discussion
The revised version of the harvested lexicon con-
tains 9,019 entries for 2,626 orthography-frame
pairs, yielding a functionality ratio of 3.4.
This lexicon still requires a certain amount of
cleaning up. For example, the verb accompany is
assigned to a number of distinct lexical entries de-
pending on the semantic role associated with the PP
complement (i.e. Goal, Path or Source). Cases
like this, where the role name is determined by the
particular choice of preposition, could be handled
outside the lexicon. Alternatively, it may be possible
to use the ?core set? feature of the FrameNet ontol-
ogy (which groups together roles that are judged to
6An alternative approach would be to consult an external
lexical resource, e.g. the LinGO ERG lexicon, which has good
coverage of control verbs.
be equivalent in some sense) to locate this kind of re-
dundancy. Other problems involve sentences where
a possessive determiner has been annotated as the
subject of a verb, e.g. It was [his] intention to aid
Larsen, resulting in numerous spurious entries.
The harvested lexical entries are encoded ac-
cording to a framework-independent XML schema,
which we developed with the aim of deriving lexi-
cons for use with a diverse range of parsers. At the
moment, several additional steps are required to con-
vert the entries we extracted into a format suitable
for a particular parser.
Firstly, the syntactic categories used by FrameNet
and the target lexicon have to be reconciled. While
basic constituent types such as noun and adjective
phrases do not change between the theories, small
differences may still exist. For example, the TRIPS
parser classifies all wh-clauses such as what he did
in I saw what he did and What he did was good as
noun phrases, the LinGO ERG grammar interprets
them as either noun phrases or clauses depending on
the context, and FrameNet annotation classifies all
of them as clauses. The alignment, however, should
be relative straightforward as there is, in general,
good agreement on the basic syntactic categories.7
Secondly, the information relevant to constituent
ordering may need to be derived, as discussed in
Section 4. Finally, the more abstract features such as
control have to be converted into feature structures
appropriate for the unification parsers. Our schema
incorporates the possibility for embedded category
structure, as in the treatment of control verbs in CCG
and HPSG where the verbal complement is an ?un-
saturated? category. We plan to use our schema
as a platform for deriving richer lexical represen-
tations from the ?flatter? entries harvested directly
from FrameNet.
As part of our future work, we expect to create
generic algorithms that help automate these steps. In
particular, we plan to include a domain-independent
set of constituent categories and syntactic role la-
bels, and add algorithms that convert between a lin-
ear ordering and a set of functional labels, for exam-
ple (Crabbe? et al, 2006). We also plan to develop
algorithms to import information from other seman-
7http://www.cl.cam.ac.uk/users/alk23/classes/Classes2.txt
contains a list of mappings between three different deep parsers
and ComLex subcategorisation frames
118
tic lexicons such as VerbNet into the same schema.
Currently, we have implemented an algorithm for
converting the harvested entries into the TRIPS lex-
icon format, resulting in a 6133 entry verb lexicon
involving 2654 distinct orthography-type pairs. This
lexicon has been successfully used with the TRIPS
parser, but additional work remains to be done be-
fore the conversion process is complete. For exam-
ple, we need a more sophisticated approach to re-
solving the complement-modifier distinction, along
with a means of integrating the FrameNet semantic
types with the TRIPS ontology so the parser can use
selectional restrictions to disambiguate.
The discussion in this paper has been mainly fo-
cused on extracting entries for a deep lexicons us-
ing frame-based NLU, but similar issues have been
faced also by the developers of shallow semantic
parsers from semantically annotated corpora. For
example, Gildea and Jurafsky (2002) found that
identifying passives was important in training a se-
mantic role classifier from FrameNet, using a parser
trained on the Penn Treebank along with a set of
templates to distinguish passive constructions from
active ones. Similarly, Chen and Rambow (2003)
argue that the kind of deep linguistic features we
harvest from FrameNet is beneficial for the success-
ful assignment of PropBank roles to constituents, in
this case using TAGs generated from PropBank to
generate the relevant features. From this perspec-
tive, our harvested lexicon can be seen as providing a
?cleaned-up?, filtered version of FrameNet for train-
ing semantic interpreters. It may also be utilised to
provide information for a separate lexical interpreta-
tion and disambiguation module to be built on top of
a syntactic parser.
6 Conclusion
We have developed both a procedure and a
framework-independent representation schema for
harvesting lexical information for deep NLP systems
from the FrameNet semantically annotated corpus.
In examining the feasibility of this approach to in-
creasing lexical coverage, we have identified a num-
ber of constructions for which current FrameNet an-
notation practice leads to problems in deriving ap-
propriate lexical entries, for example imperatives,
passives and control.
7 Acknowledgements
The work reported here was supported by grants
N000140510043 and N000140510048 from the Of-
fice of Naval Research.
References
C. F. Baker, C. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
COLING-ACL?98, Montreal, pages 86?90.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In Proceedings of EMNLP?03, Sapporo,
Japan.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC?00, Athens, Greece, pages 591?600.
B. Crabbe?, M. O. Dzikovska, W. de Beaumont, and
M. Swift. 2006. Increasing the coverage of a domain
independent dialogue lexicon with VerbNet. In Pro-
ceedings of ScaNaLU?06, New York City.
M. O. Dzikovska. 2004. A Practical Semantic Repre-
sentation for Natural Language Parsing. Ph.D. thesis,
University of Rochester, Rochester NY.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
R. Grishman, C. MacLeod, and A. Meyers. 1994. Com-
lex syntax: Building a computational lexicon. In Pro-
ceedings of COLING?94, Kyoto, Japan, pages 268?
272.
J. Hockenmaier and M. Steedman. 2002. Acquiring
Compact Lexicalized Grammars from a Cleaner Tree-
bank. In Proceedings of LREC?02, Las Palmas, Spain.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of AAAI?00, Austin TX.
J. Ruppenhofer, M. Ellsworth, M. R. L. Petruck, C. R.
Johnson, and J. Scheffczyk, 2006. FrameNet II: Ex-
tended Theory and Practice. The Berkeley FrameNet
Project, August.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research
on Language and Computation, 4(1):39?75.
119
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 5?13,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Context-Dependent Regression Testing for Natural Language Processing
Elaine Farrow
Human Communication Research Centre
School of Informatics
University of Edinburgh
Edinburgh, UK
Elaine.Farrow@ed.ac.uk
Myroslava O. Dzikovska
Human Communication Research Centre
School of Informatics
University of Edinburgh
Edinburgh, UK
M.Dzikovska@ed.ac.uk
Abstract
Regression testing of natural language sys-
tems is problematic for two main reasons:
component input and output is complex, and
system behaviour is context-dependent. We
have developed a generic approach which
solves both of these issues. We describe our
regression tool, CONTEST, which supports
context-dependent testing of dialogue system
components, and discuss the regression test
sets we developed, designed to effectively iso-
late components from changes and problems
earlier in the pipeline. We believe that the
same approach can be used in regression test-
ing for other dialogue systems, as well as in
testing any complex NLP system containing
multiple components.
1 Introduction
Natural language processing systems, and dialogue
systems in particular, often consist of large sets of
components operating as a pipeline, including pars-
ing, semantic interpretation, dialogue management,
planning, and generation. Testing such a system can
be a difficult task for several reasons. First, the com-
ponent output may be context-dependent. This is
particularly true for a dialogue system ? reference
resolution, ellipsis, and sometimes generation typi-
cally have to query the system state to produce their
output, which depends both on the state of the world
(propositions defined in a knowledge base) and on
the dialogue history (object salience). Under these
conditions, unit testing using the input and output of
a single component in isolation is of limited value
? the entire system state needs to be preserved to
check that context-dependent components are func-
tioning as expected.
Second, the inputs and outputs of most system
components are usually very complex and often
change over time as the system develops. When
two complex representations are compared it may
be difficult to determine what impact any change is
likely to have on system performance (far-reaching
or relatively trivial). Further, if we test components
in isolation by saving their inputs, and these inputs
are reasonably complex, then it will become difficult
to maintain the test sets for the components further
along the pipeline (such as diagnosis and generation)
as the output of the earlier components changes dur-
ing development.
The simplest way to deal with both of these is-
sues would be to save a set of test dialogues as a
gold standard, checking that the final system out-
put is correct given the system input. However, this
presents another problem. If a single component
(generation, for example) malfunctions, it becomes
impossible to verify that a component earlier in the
pipeline (for example, reference resolution) is work-
ing properly. In principle we could also save the
messages passing between components and compare
their content, but then we are faced again with the
problems arising from the complexity of component
input and output which we described above.
To solve these problems, we developed a regres-
sion tool called CONTEST (for CONtext-dependent
TESTing). CONTEST allows the authors of individ-
ual system components to control what information
to record for regression testing. Test dialogues are
5
saved and replayed through the system, and individ-
ual components are tested by comparing only their
specific regression output, ignoring the outputs gen-
erated by other components. The components are
isolated by maintaining a minimal set of inputs that
are guaranteed to be processed correctly.
To deal with issues of output complexity we ex-
tend the approach of de Paiva and King (2008) for
testing a deep parser. They created test sets at dif-
ferent levels of granularity, some including detailed
representations, but some just saving very simple
output of a textual entailment component. They
showed that, given a carefully selected test set, test-
ing on the final system output can be a fast and effec-
tive way to discover problems in the interpretation
pipeline.
We show how the same idea can be used to test
other dialogue system components as well. We de-
scribe the design of three different test sets that
effectively isolate the interpretation, tutorial plan-
ning and generation components of our system. Us-
ing CONTEST allows us to detect system errors and
maintain consistent test sets even as the underlying
representations change, and gives us much greater
confidence that the results of our testing are relevant
to the performance of the system with real users.
The rest of this paper is organised as follows. In
Section 2 we describe our system and its compo-
nents in more detail. The design of the CONTEST
tool and the test sets are described in Sections 3 and
4. Finally, in Section 5 we discuss how the inter-
active nature of the dialogue influences the design
of the test sets and the process of verifying the an-
swers; and we discuss features that we would like to
implement in the future.
2 Background
This work has been carried out to support the devel-
opment of BEETLE (Callaway et al, 2007), a tuto-
rial dialogue system for basic electricity and elec-
tronics. The goal of the BEETLE system is to teach
conceptual knowledge using natural language dia-
logue. Students interact with the system through a
graphical user interface (GUI) which includes a chat
interface,1 a window to browse through slides con-
1The student input is currently typed to avoid issues with
automated speech recognition of complex utterances.
taining reading material and diagrams, and an inter-
face to a circuit simulator where students can build
and manipulate circuits.
The system consists of twelve components alto-
gether, including a knowledge base representing the
state of the world, a curriculum planner responsible
for the lesson structure, and dialogue management
and NLP components. We developed CONTEST so
that it could be used to test any system component,
though our testing focuses on the natural language
understanding and generation components.2
BEETLE uses a standard natural language pro-
cessing pipeline, starting with a parser, lexical in-
terpreter, and dialogue manager. The dialogue man-
ager handles all input from the GUI (text, button
presses and circuits) and also supports generic di-
alogue processing, such as dealing with interpreta-
tion failures and moving the lesson along. Student
answers are processed by the diagnosis and tuto-
rial planning components (discussed below), which
function similarly to planning and execution com-
ponents in task oriented dialogue systems. Finally,
a generation subsystem converts the semantic repre-
sentations output by the tutorial planner into the final
text to be presented to the student.
The components communicate with each other
using the Open Agent Architecture (Martin et al,
1998). CONTEST is implemented as an OAA agent,
accepting requests to record messages. However,
OAA is not essential for the system design ? any
communication architecture which supports adding
extra agents into a system would work equally well.
BEETLE aims to get students to support their rea-
soning using natural language, since explanations
and contentful talk are associated with learning gain
(Purandare and Litman, 2008). This requires de-
tailed analyses of student answers in terms of cor-
rect, incorrect and missing parts (Dzikovska et al,
2008; Nielsen et al, 2008). Thus, we use the TRIPS
parser (Allen et al, 2007), a deep parser which pro-
duces detailed analyses of student input. The lexical
interpreter extracts a list of objects and relationships
mentioned, which are checked against the expected
answer. These lists are fairly long ? many expected
answers have ten or more relations in them. The
2All our components are rule-based, but we expect the same
approach would work for components of a statistical nature.
6
diagnoser categorises each of the objects and rela-
tionships as correct, contradictory or irrelevant. The
tutorial planner makes decisions about the remedi-
ation strategy, choosing one strategy from a set of
about thirteen depending on the question type and
tutorial context. Finally, the generation system uses
the FUF/SURGE (Elhadad, 1991) deep generator to
generate feedback automatically.
Obviously, the output from the deep parser and
the input to the tutorial planner and generator are
quite complex, giving rise to the types of problems
that we discussed in the introduction. We already
had a tool for unit-testing the parser output (Swift et
al., 2004), plus some separate tools to test the diag-
noser and the generation component, but the com-
plexity of the representations made it impractical to
maintain large test sets that depended on such com-
plex inputs and outputs. We also wanted a unified
way to test all the components in the context of the
entire system. This led to the creation of CONTEST,
which we describe in the rest of the paper.
3 The CONTEST Tool
Figure 1: The regression testing process.
In this section we describe the process for creat-
ing and using test cases, illustrated in Figure 1. The
first step in building a useful regression tool is to
make it possible to run the same dialogue through
the system many times without retyping the student
answers. We added a wrapper around the GUI to in-
tercept and record the user actions and system calls
for later playback, thus creating a complete record
of the session. Every time our system runs, a new
saved session file is automatically created and saved
in a standard location. This file forms the basis for
our test cases. It uses an XML format, which is
human-readable and hand-editable, easily extensible
and amenable to automatic processing. A (slightly
simplified) example of a saved session file is shown
in Figure 2. Here we can see that a slide was dis-
played, the tutor asked the question ?Which compo-
nents (if any) are in a closed path in circuit 1?? and
the student answered ?the battery and the lightbulb?.
Creating a new test case is then a simple matter of
starting the system and performing the desired ac-
tions, such as entering text and building circuits in
the circuit simulator. If the system is behaving as it
should, the saved session file can be used directly as
a test case. If the system output is not as desired, the
file can be edited in any text editor.
Of course, this only allows the final output of the
system to be tested, and we have already discussed
the shortcomings of such an approach: if a com-
ponent late in the pipeline has problems, there is
no way to tell if earlier components behaved as ex-
pected. To remedy this, we added a mechanism for
components other than the GUI to record their own
information in the saved session file.
Components can be tested in effective isolation by
combining two mechanisms: carefully designed test
sets which focus on a single component and (impor-
tantly) are expected to succeed even if some other
component is having problems; and a regression tool
which allows us to test the output of an individual
component. Our test sets are discussed in detail in
Section 4. The remainder of this section describes
the design of the tool.
CONTEST reads in a saved session file and re-
produces the user actions (such as typing answers
or building circuits), producing a new saved ses-
sion file as its output. If there have been changes to
the system since the test was created, replaying the
same actions may lead to new slides and tutor mes-
sages being displayed, and different recorded output
from intermediate components. For example, given
the same student answers, the diagnosis may have
changed, leading to different tutor feedback. We
compare the newly generated output file against the
input file. If there are no differences, the test is con-
sidered to have passed. As the input and output files
7
<test>
<action agent="tutor" method="showSlide">
lesson1-oe/exercise/img1.html
</action>
<action agent="tutor" method="showOutput">
Which components (if any) are in a closed path in circuit 1?
</action>
<action agent="student" method="submitText">
the battery and the lightbulb
</action>
</test>
Figure 2: A saved session file showing a single interaction between tutor and student.
are identical in format, the comparison can be done
using a ?diff? command.
With each component recording its own output, it
can be the case that there are many differences be-
tween old and new files. It is therefore important to
be able to choose the level of detail we want when
comparing saved session files, so that the output of
a single component can be checked independently
of other system behaviour. We solved this problem
by creating a set of standard XSLT filters. One fil-
ter picks out just the dialogue between student and
tutor to produce a transcript of the session. Other
filters select the output from one particular compo-
nent, for example the tutorial planner, with the tutor
questions included to provide context. In general,
we wrote one filter for each component.
CONTEST creates a test report by comparing the
expected and actual outputs of the system on each
test run. We specify which filter to use (based on
which component we are testing). If the test fails,
we can examine the relevant differences using the
?ediff? mode in the emacs text editor. More sophis-
ticated approaches are possible, such as using a fur-
ther XSL transform to count all the errors of a partic-
ular type, but we have found ediff to be good enough
for our purposes. With the filters in place we only
see the differences for the component we are testing.
Since component regression output is designed to be
small and human-readable, checking the differences
is a very quick process.
Test cases can be run individually or in groups.3
3Test cases are usually grouped by directory, but symbolic
links allow us to use the same case in several groups.
Using CONTEST, we can create a single report for a
group of test cases: the individual outputs are com-
bined to create a new output file for the group and
this is compared to the (combined) input file, with
filters applied in the usual way. This is a very use-
ful feature, allowing us to create a report for all the
?good answer? cases (for example) in one step.
Differences do not always indicate errors; some-
times they are simply changes or additions to the
recorded information. After satisfying ourselves that
the reported differences are intentional changes, we
can update the test cases to reflect the output of the
latest run. Subsequent runs will test against the new
behaviour. CONTEST includes an update tool which
can update a group of cases with a single command.
This is simpler and less error-prone than editing po-
tentially hundreds of files by hand.
4 Test Cases
We have built several test sets for each component,
amounting to more than 400 individual test cases.
We describe examples of the test sets for three of our
components in more detail below, to demonstrate
how we use CONTEST.
4.1 Interpretation Test Cases
We have a test set consisting of ?good answers? for
each of the questions in our system which we use to
test the interpretation component. The regression in-
formation recorded by the interpretation component
includes the internal ID code of the matched answer
and a code indicating whether it is a ?best?, ?good? or
?minimal? answer. This is enough to allow us to de-
8
<test name="closed_path_discussion">
<action agent="tutor" method="showOutput">
What are the conditions that are required to make a bulb light up?
</action>
<action agent="student" method="submitText">
a bulb must be in a closed path with the battery
</action>
<action agent="simpleDiagnosis" method="logForRegression">
student-act: answer atype: diagnosis consistency: []
code: complete subcode: best
answer_id: conditions_for_bulb_to_light_ans1
</action>
</test>
Figure 3: A sample test case from our ?good answers? set showing the diagnosis produced for the student?s answer.
tect many possible errors in interpretation. We can
run this test set after every change to the parsing or
interpretation components.
A (slightly simplified) example of our XML test
case format is shown in Figure 3, with the tutor ques-
tion ?What are the conditions that are required to
make a bulb light up?? and the student answer ?a
bulb must be in a closed path with the battery?. The
answer diagnosis shows that the system recognised
that the student was attempting to answer the ques-
tion (rather than asking for help), that the answer
match was complete, with no missing or incorrect
parts, and the answer was consistent with the state of
the world as perceived by the system.4 The matched
answer is marked as the best one for that question.
While the recorded information does not supply
the full interpretation, it can suggest the source of
various possible errors. If interpretation fails, the
student act will be set to uninterpretable,
and the code will correspond to the reason
for failed interpretation: unknown input
if the parse failed, unknown mapping or
restriction failure if lexical interpretation
failed, and unresolvable if reference resolution
failed. If interpretation worked, but took incorrect
scoping or attachment decisions, the resulting
proposition is likely to be inconsistent with the
4Sometimes students are unable to interpret diagrams, or
are lacking essential background knowledge, and therefore say
things that contradict the information in the domain model. The
system detects and remediates such cases differently from gen-
eral errors in explanations (Dzikovska et al, 2006).
current knowledge base, and an inconsistency code
will be reported. In addition, verifying the matched
answer ID provides some information in case only
a partial interpretation was produced. Sometimes
different answer IDs correspond to answers that are
very complete versus answers that are acceptable
because they address the key point of the question,
but miss some small details. Thus if a different
answer ID has matched, it indicates that some
information was probably lost in interpretation.
The codes we report were not devised specifically
for the regression tests. They are used internally to
allow the system to produce accurate feedback about
misunderstandings. However, because they indicate
where the error is likely to originate (parsing, lexi-
cal interpretation, scoping and disambiguation), they
can help us to track it down.
We have another test set for ?special cases?, such
as the student requesting a hint or giving up. An ex-
ample is shown in Figure 4. Here the student gives
up completely on the first question, then asks for
help with the second. We use this test case to check
that the set phrases ?I give up? and ?help? are un-
derstood by the system. The ?special cases? test set
includes a variety of help request phrasings observed
in the corpora we collected. Note that this example
was recorded while using a tutorial policy that re-
sponds to help requests by simply providing the an-
swer. This does not matter for testing interpretation,
since the information recorded in the test case will
distinguish help requests from give ups, regardless
9
T: Which components (if any) are in a closed path
in circuit 1?
S: I give up
T: The answer is the battery and the bulb in 1.
T: Which components (if any) are in a closed path
in circuit 2?
S: help
T: Here?s the answer. The bulb in 2.
Figure 4: The transcript of a test case for ?I give up? and
?help?. T: is the tutor, S: is the student.
of the specific tutorial policy used by the system.
Finally, we have a test set for detection of un-
interpretable utterances. In a tutoring system, stu-
dents often use incorrect terminology or unex-
pected phrasings which the system cannot under-
stand. While we expect coverage and robustness of
parsing and interpretation components to improve
as the system develops, some percentage of errors
is to be expected simply because even state-of-the-
art parsers are not 100% accurate. Thus the sys-
tem needs to detect problems whenever possible,
and produce useful help messages. We have col-
lected a set of student utterances that correspond
to typical interpretation problems ? failed parses,
unknown words, sentences that are parsed but the
parser makes an incorrect attachment decision, sen-
tences where the student used terminology incor-
rectly (violating sortal constraints encoded in the
knowledge base), a pronominal referent that could
not be resolved. Identifying the problem type cor-
rectly is important because it affects later processing
in the dialogue manager and tutorial planner. Thus
we have a set of test cases using the collected sen-
tences, which we use to ensure that the problems are
flagged correctly.
4.2 Tutoring Test Cases
The BEETLE tutorial planner makes decisions on
system behaviour and corresponds to planning and
execution components in other dialogue systems.
Here, we are attempting to test that the tutorial plan-
ner selected the right system action. Therefore, since
we are not testing the interpretation component, we
don?t vary the form of the language we use. Instead,
T: Predict the state (on, off) of each bulb in dia-
gram 1.
S: all bulbs are off
T: OK, we?ll check your prediction in a moment.
T: Predict the state (on, off) of each bulb in dia-
gram 2.
S: all bulbs are off
T: OK, we?ll check your prediction in a moment.
T: Test your prediction for diagram 1 by opening
both switches and observing the bulbs.
T: Test your prediction for diagram 2 by closing
both switches and observing the bulbs.
T: Did the bulbs behave the way you expected?
S: no
T: Right. In your second prediction, you said that
Bulb A, Bulb B and Bulb C would be off. But
Bulb A, Bulb B and Bulb C are lit.
Figure 5: A tutorial planner test case transcript. T: is the
tutor, S: is the student.
we vary the combination of good and bad student an-
swers, and record the action chosen by the system.
The tutorial planner chooses feedback in 2 stages.
First, a general algorithm is chosen depending on the
exercise type and student input type: there are sepa-
rate algorithms for addressing, for example, what to
do if the student input was not interpreted, and for
correct and incorrect answers. Choosing the algo-
rithm requires some computation depending on the
question context. Once the main algorithm is cho-
sen, different tutorial strategies can be selected, and
this is reflected in the regression output: the system
records a keyword corresponding to the chosen algo-
rithm, and then the name of the strategy along with
key strategy parameters.
For example, Figure 5 shows the transcript from
a test case for a common exercise type from our
lessons: a so called predict-verify-evaluate se-
quence. In this example, the student is asked to
predict the behaviour of three light bulbs in a cir-
cuit, test it by manipulating the circuit in the simu-
lation environment, and then evaluate whether their
predictions matched the circuit behaviour. The sys-
tem reinforces the point of the exercise by producing
a summary of discrepancies between the student?s
10
<action agent="tutor" method="showOutput">
Did the bulbs behave the way you expected?
</action>
<action agent="student" method="submitText">
no
</action>
<action agent="tc-bee" method="logForRegression">
EVALUATE (INCORRECT-PREDICTION NO_NO)
</action>
Figure 6: An excerpt from a tutorial planner test case showing the recorded summary output.
predictions and the observed outcomes.
An excerpt from the corresponding test case is
shown in Figure 6. Here we can see the tutor ask
the evaluation question ?Did the bulbs behave the
way you expected?? and the student answer ?no?.
The EVALUATE algorithm was chosen to handle the
student answer, and from the set of available strate-
gies the INCORRECT-PREDICTION strategy was
chosen. That strategy takes a parameter indicating
if there was a discrepancy when the student evalu-
ated the results (here NO NO, corresponding to the
expected and actual evaluation result inputs).
In contrast, in the first example in Figure 4, where
the student gives up and doesn?t provide an an-
swer, the tutorial planner output is REMEDIATE
(BOTTOM-OUT Q IDENTIFY). This shows that
the system has chosen to use a REMEDIATE algo-
rithm, and a ?bottom-out? (giving away the answer)
strategy for remediation. The strategy parameter
Q IDENTIFY (which depends on the question type)
determines the phrasing to be used in the generator
to verbalise the tutor?s feedback.
The saved output allows us to see that the cor-
rect algorithm was chosen to handle the student in-
put (for example, that the REMEDIATE algorithm
is correctly chosen after an incorrect student answer
to an explanation question), and that the algorithm
chooses a strategy appropriate for the tutorial con-
text. Certain errors can still go undetected here, for
example, if the algorithm for verbalising the chosen
strategy in the generator is broken. Developing sum-
mary inputs to detect such errors is part of planned
future work.
In order to isolate the tutorial planner from inter-
pretation, we use standard fixed phrasings for stu-
dent answers. The answer phrasings in the ?good
answers? test set for interpretation (described in Sec-
tion 4.1) are guaranteed to be understood correctly,
so we use only these phrasings in our tutorial planner
test cases. Thus, we are able to construct tests which
will not be affected by problems in the interpretation
pipeline.
4.3 Generation Test Cases
To test generation, we have a set of test cases where
the student immediately says ?I give up? in response
to each question. This phrase is used in our system
to prevent the students getting stuck ? the tutorial
policy is to immediately stop and give the answer to
the question. The answers given are generated by
a deep generator from internal semantic representa-
tions, so this test set gives us the assurance that all
relevant domain content is being generated properly.
This is not a complete test for the generation capabil-
ities of our system, since each explanation question
can have several possible answers of varying degrees
of quality (suggested by experienced human tutors
(Dzikovska et al, 2008)), and we always choose
the best possible answer when the student gives up.
However, it gives us confidence that the student can
give up at any point and receive an answer which can
be used as a template for future answers.
5 Discussion and Future Work
We have created more than 400 individual test cases
so far. There are more than 50 for the interpretation
component, more than 150 for the tutorial planner
and more than 200 for the generation component.
We are developing new test sets based on other sce-
narios, such as responding to each question with a
11
help request. We are also refining the summary in-
formation recorded by each component.
An important feature of our testing approach is
the use of short summaries rather than the inter-
nal representations of component inputs and outputs.
Well-designed summaries provide key information
in an easy-to-read format that can remain constant as
internal formats change and develop over time. We
believe that this approach would be useful for other
language processing systems, since at present there
are few standardised formats in the community and
representations are typically developed and refined
together with the algorithms that use them.
The decision about what information to include in
the summary is vital to the success and overall use-
fulness of the regression tool. If too much detail is
recorded, there will be many spurious changes and
it will be burdensome to keep a large regression set
updated. If too little detail is recorded, unwanted
changes in the system may go undetected. The con-
tent of the test cases we discussed in Section 4 rep-
resents our approach to such decisions.
Interpretation was perhaps the most difficult, be-
cause it has a particularly complex output. In deter-
mining the information to record, we were following
the solution of de Paiva and King (2008) who use the
decision result of the textual entailment system as a
way to efficiently test parser output. For our sys-
tem, the information output by the diagnoser about
answer correctness proved to have a similar function
? it effectively provides information about whether
the output of the interpretation component was us-
able, without the need to check details carefully.
The main challenge for our tutorial planner and
generation components (corresponding to planning
and execution components in a task-oriented dia-
logue system) was to ensure that they were suffi-
ciently isolated so as to be unaffected by errors in in-
terpretation. We achieve this by maintaining a small
set of known phrasings which are guaranteed to be
interpreted correctly; this ensures that in practice,
the downstream components are isolated from un-
wanted changes in interpretation.
Our overall methodology of recording and test-
ing summary information for individual components
can be used with any complex NLP system. The spe-
cific details of what information to record obviously
depends on the domain, but our experience suggests
some general principles. For testing the interpreta-
tion pipeline, it is useful to record pre-existing error
codes and a qualitative summary of the information
used to decide on the next system action. Where we
record the code output by the diagnoser, an informa-
tion seeking system could record, for example, the
number of slots filled and the number of items re-
trieved from a database. It is also useful to record
decisions taken by the system, or actions performed
in response to user input; so, just as we record infor-
mation about the chosen tutorial policy, other sys-
tems can record the action taken ? whether it is to
search the database, query a new slot, or confirm a
slot value.
One major improvement that we have planned for
the future is adding another layer of test case man-
agement to CONTEST, to enable us to produce sum-
maries and statistics about the total number of test
cases that have passed and failed, instead of check-
ing reports individually. Such statistics can be im-
plemented easily using another XSL transform on
top of the existing filters to count the number of
test cases with no differences and produce summary
counts of each type of error detected.
6 Conclusion
The regression tool we developed, CONTEST, solves
two of the major issues faced when testing dia-
logue systems: context-dependence of component
behaviour and complexity of component output. We
developed a generic approach based on running
saved dialogues through the system, and checking
summary information recorded by different compo-
nents against separate gold standards. We demon-
strated that test sets can be designed in such a way as
to effectively isolate downstream components from
changes and problems earlier in the pipeline. We be-
lieve that the same approach can be used in regres-
sion testing for other dialogue systems, as well as in
testing any complex NLP system containing multi-
ple components.
Acknowledgements
This work has been supported in part by Office of
Naval Research grant N000140810043. We thank
Charles Callaway for help with generation and tu-
toring tests.
12
References
James Allen, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL-07 Workshop on Deep Linguistic Processing.
Charles B. Callaway, Myroslava Dzikovska, Elaine Far-
row, Manuel Marques-Pita, Colin Matheson, and Jo-
hanna D. Moore. 2007. The Beetle and BeeDiff tutor-
ing systems. In Proceedings of the SLaTE-2007 Work-
shop, Farmington, Pennsylvania, USA, September.
Valeria de Paiva and Tracy Holloway King. 2008. De-
signing testsuites for grammar-based systems in appli-
cations. In Coling 2008: Proceedings of the workshop
on Grammar Engineering Across Frameworks, pages
49?56, Manchester, England, August. Coling 2008 Or-
ganizing Committee.
Myroslava O. Dzikovska, Charles B. Callaway, and
Elaine Farrow. 2006. Interpretation and generation in
a knowledge-based tutorial system. In Proceedings of
EACL-06 workshop on knowledge and reasoning for
language processing, Trento, Italy, April.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008. Diagnosing natural language
answers to support adaptive tutoring. In Proceed-
ings 21st International FLAIRS Conference, Coconut
Grove, Florida, May.
Michael Elhadad. 1991. FUF: The universal unifier user
manual version 5.0. Technical Report CUCS-038-91,
Dept. of Computer Science, Columbia University.
D. Martin, A. Cheyer, and D. Moran. 1998. Building
distributed software systems with the open agent ar-
chitecture. In Proceedings of the Third International
Conference on the Practical Application of Intelligent
Agents and Multi-Agent Technology, Blackpool, Lan-
cashire, UK.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Learning to assess low-level conceptual under-
standing. In Proceedings 21st International FLAIRS
Conference, Coconut Grove, Florida, May.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
Mary D. Swift, Joel Tetreault, and Myroslava O.
Dzikovska. 2004. Semi-automatic syntactic and se-
mantic corpus annotation with a deep parser. In Pro-
ceedings of LREC-2004.
13
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 38?45,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Dealing with Interpretation Errors in Tutorial Dialogue
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow, Johanna D. Moore
School of Informatics
University of Edinburgh, Edinburgh, United Kingdom
mdzikovs,ccallawa,efarrow,jmoore@inf.ed.ac.uk
Natalie Steinhauser, Gwendolyn Campbell
Naval Air Warfare Training Systems Division
Orlando, Florida, USA
Abstract
We describe an approach to dealing with
interpretation errors in a tutorial dialogue
system. Allowing students to provide ex-
planations and generate contentful talk can
be helpful for learning, but the language
that can be understood by a computer sys-
tem is limited by the current technology.
Techniques for dealing with understanding
problems have been developed primarily for
spoken dialogue systems in information-
seeking domains, and are not always appro-
priate for tutorial dialogue. We present a
classification of interpretation errors and our
approach for dealing with them within an
implemented tutorial dialogue system.
1 Introduction
Error detection and recovery is a known problem in
the spoken dialogue community, with much research
devoted to determining the best strategies, and learn-
ing how to choose an appropriate strategy from data.
Most existing research is focused on dealing with
problems in an interaction resulting from speech
recognition errors. This focus is justified, since the
majority of understanding problems observed in cur-
rent spoken dialogue systems (SDS) are indeed due
to speech recognition errors.
Recovery strategies, therefore, are sometimes de-
vised specifically to target speech recognition prob-
lems - for example, asking the user to repeat the ut-
terance, or to speak more softly, which only makes
sense if speech recognition is the source of trouble.
However, errors can occur at all levels of process-
ing, including parsing, semantic interpretation, in-
tention recognition, etc. As speech recognition im-
proves and more sophisticated systems are devel-
oped, strategies for dealing with errors coming from
higher (and potentially more complex) levels of pro-
cessing will have to be developed.
This paper presents a classification of non-
understandings, defined as the errors where the sys-
tem fails to arrive at an interpretation of the user?s
utterance (Bohus and Rudnicky, 2005), and a set of
strategies for dealing with them in an implemented
tutorial dialogue system. Our system differs from
many existing systems in two ways. First, all di-
alogue is typed. This was done in part to avoid
speech recognition issues and allow for more com-
plex language input than would otherwise be pos-
sible. But it is also a valid modality for tutoring -
there are now many GUI-based tutoring systems in
existence, and as distance and online learning have
become more popular, students are increasingly fa-
miliar with typed dialogue in chat rooms and discus-
sion boards. Second, different genres impose dif-
ferent constraints on the set of applicable recovery
strategies - as we discuss in Section 2, certain help
strategies developed for task-oriented dialogue sys-
tems are not suitable for tutorial dialogue, because
tutoring systems should not give away the answer.
We propose a targeted help approach for dealing
with interpretation problems in tutorial dialogue by
providing help messages that target errors at differ-
ent points in the pipeline. In our system they are
combined with hints as a way to lead the student
to an answer that can be understood. While some
38
parts of the system response are specific to tutorial
dialogue, the targeted help messages themselves can
serve as a starting point for developing appropriate
recovery strategies in other systems where errors at
higher levels of interpretation are a problem.
The rest of this paper is organized as follows. In
Section 2, we motivate the need for error handling
strategies in tutorial dialogue. In Section 3 we de-
scribe the design of our system. Section 4 discusses
a classification of interpretation problems and our
targeted help strategy. Section 5 provides a prelim-
inary evaluation based on a set of system tests con-
ducted to date. Finally, we discuss how the approach
taken by our system compares to other systems.
2 Background and Motivation
Tutorial dialogue systems aim to improve learning
by engaging students in contentful dialogue. There
is a mounting body of evidence that dialogue which
encourages students to explain their actions (Aleven
and Koedinger, 2000), or to generate contentful talk
(Purandare and Litman, 2008), results in improved
learning. However, the systems? ability to under-
stand student language, and therefore to encourage
contentful talk, is limited by the state of current lan-
guage technology. Moreover, student language may
be particularly difficult to interpret since students
are often unaware of proper terminology, and may
phrase their answers in unexpected ways. For exam-
ple, a recent error analysis for a domain-independent
diagnoser trained on a large corpus showed that a
high proportion of errors were due to unexpected
paraphrases (Nielsen et al, 2008).
In small domains, domain-specific grammars and
lexicons can cover most common phrasings used
by students to ensure robust interpretation (Aleven,
2003; Glass, 2000). However, as the size of the
domain and the range of possible questions and an-
swers grows, achieving complete coverage becomes
more difficult. For essays in large domains, sta-
tistical methods can be used to identify problems
with the answer (Jordan et al, 2006; Graesser et
al., 1999), but these approaches do not perform well
on relatively short single-sentence explanations, and
such systems often revert to short-answer questions
during remediation to ensure robustness.
To the best of our knowledge, none of these tu-
torial systems use sophisticated error handling tech-
niques. They rely on the small size of the domain
or simplicity of expected answers to limit the range
of student input. They reject utterances they cannot
interpret, asking the user to repeat or rephrase, or
tolerate the possibility that interpretation problems
will lead to repetitive or confusing feedback.
We are developing a tutorial dialogue system that
behaves more like human tutors by supporting open-
ended questions, as well as remediations that allow
for open-ended answers, and gives students detailed
feedback on their answers, similar to what we ob-
served with human tutors. This paper takes the first
step towards addressing the problem of handling er-
rors in tutorial dialogue by developing a set of non-
understanding recovery strategies - i.e. strategies
used where the system cannot find an interpretation
for an utterance.
In early pilot experiments we observed that if the
system simply rejects a problematic student utter-
ance, saying that it was not understood, then stu-
dents are unable to determine the reason for this
rejection. They either resubmit their answer mak-
ing only minimal changes, or else they rephrase the
sentence in a progressively more complicated fash-
ion, causing even more interpretation errors. Even
after interacting with the system for over an hour,
our students did not have an accurate picture as to
which phrasings are well understood by the system
and which should be avoided. Previous research also
shows that users are rarely able to perceive the true
causes of ASR errors, and tend to form incorrect the-
ories about the types of input a system is able to ac-
cept (Karsenty, 2001).
A common approach for dealing with these is-
sues in spoken dialogue systems is to either change
to system initiative with short-answer questions (?Is
your destination London??), or provide targeted help
(?You can say plane, car or hotel?). Neither of these
is suitable for our system. The expected utterances
in our system are often more complex (e.g., ?The
bulb must be in a closed path with the battery?), and
therefore suggesting an utterance may be equivalent
to giving away the entire answer. Giving students
short-answer questions such as ?Are the terminals
connected or not connected?? is a valid tutoring
strategy sometimes used by the tutors. However,
it changes the nature of the question from a recall
39
task to a recognition task, which may affect the stu-
dent?s ability to remember the correct solution in-
dependently. Therefore, we decided to implement
strategies that give the student information about the
nature of the mistake without directly giving infor-
mation about the expected answer, and encourage
them to rephrase their answers in ways that can be
understood by the system.
We currently focus on strategies for dealing
with non-understanding rather than misunderstand-
ing strategies (i.e. cases where the system finds an
interpretation, but an incorrect one). It is less clear
in tutorial dialogue what it means for a misunder-
standing to be corrected. In task-oriented dialogue,
if the system gets a slot value different from what
the user intended, it should make immediate correc-
tions at the user?s request. In tutoring, however, it
is the system which knows the expected correct an-
swer. So if the student gives an answer that does not
match the expected answer, when they try to correct
it later, it may not always be obvious whether the
correction is due to a true misunderstanding, or due
to the student arriving at a better understanding of
the question. Obviously, true misunderstandings can
and will still occur - for example, when the system
resolves a pronoun incorrectly. Dealing with such
situations is planned as part of future work.
3 System Architecture
Our target application is a system for tutoring ba-
sic electricity and electronics. The students read
some introductory material, and interact with a sim-
ulator where they can build circuits using batteries,
bulbs and switches, and measure voltage and cur-
rent. They are then asked two types of questions:
factual questions, like ?If the switch is open, will
bulb A be on or off??, and explanation questions.
The explanation questions ask the student to explain
what they observed in a circuit simulation, for exam-
ple, ?Explain why you got the voltage of 1.5 here?,
or define generic concepts, such as ?What is volt-
age??. The expected answers are fairly short, one or
two sentences, but they involve complex linguistic
phenomena, including conjunction, negation, rela-
tive clauses, anaphora and ellipsis.
The system is connected to a knowledge base
which serves as a model for the domain and a rea-
soning engine. It represents the objects and rela-
tionships the system can reason about, and is used
to compute answers to factual questions.1 The stu-
dent answers are processed using a standard NLP
pipeline. All utterances are parsed to obtain syntac-
tic analyses.2 The lexical-semantic interpreter takes
analyses from the parser and maps them to seman-
tic representations using concepts from the domain
model. A reference resolution algorithm similar to
(Byron, 2002) is used to find referents for named ob-
jects such as ?bulb A? and for pronouns.
Once an interpretation of a student utterance has
been obtained, it is checked in two ways. First, its
internal consistency is verified. For example, if the
student says ?Bulb A will be on because it is in a
closed path?, we first must ensure that their answer
is consistent with what is on the screen - that bulb A
is indeed in a closed path. Otherwise the student
probably has a problem either with understanding
the diagrams or with understanding concepts such as
?closed path?. These problems indicate lack of basic
background knowledge, and need to be remediated
using a separate tutorial strategy.
Assuming that the utterance is consistent with the
state of the world, the explanation is then checked
for correctness. Even though the student utterance
may be factually correct (Bulb A is indeed in a
closed path), it may still be incomplete or irrelevant.
In the example above, the full answer is ?Bulb A
is in a closed path with the battery?, hence the stu-
dent explanation is factually correct but incomplete,
missing the mention of the battery.
In the current version of our system, we are partic-
ularly concerned about avoiding misunderstandings,
since they can result in misleading tutorial feedback.
Consider an example of what can happen if there is
a misunderstanding due to a lexical coverage gap.
The student sentence ?the path is broken? should be
interpreted as ?the path is no longer closed?, corre-
sponding to the is-open relation. However, the
1Answers to explanation questions are hand-coded by tutors
because they are not always required to be logically complete
(Dzikovska et al, 2008). However, they are checked for consis-
tency as described later, so they have to be expressed in terms
that the knowledge base can reason about.
2We are using a deep parser that produces semantic analyses
of student?s input (Allen et al, 2007). However, these have to
undergo further lexical interpretation, so we are treating them
as syntactic analyses for purposes of this paper.
40
most frequent sense of ?broken? is is-damaged,
as in ?the bulb is broken?. Ideally, the system lex-
icon would define ?broken? as ambiguous between
those two senses. If only the ?damaged? sense is
defined, the system will arrive at an incorrect inter-
pretation (misunderstanding), which is false by defi-
nition, as the is-damaged relation applies only to
bulbs in our domain. Thus the system will say ?you
said that the path is damaged, but that?s not true?.
Since the students who used this phrasing were un-
aware of the proper terminology in the first instance,
they dismissed such feedback as a system error. A
more helpful feedback message is to say that the sys-
tem does not know about damaged paths, and the
sentence needs to be rephrased.3
Obviously, frequent non-understanding messages
can also lead to communication breakdowns and im-
pair tutoring. Thus we aim to balance the need to
avoid misunderstandings with the need to avoid stu-
dent frustration due to a large number of sentences
which are not understood. We approach this by us-
ing robust parsing and interpretation tools, but bal-
ancing them with a set of checks that indicate poten-
tial problems. These include checking that the stu-
dent answer fits with the sortal constraints encoded
in the domain model, that it can be interpreted un-
ambiguously, and that pronouns can be resolved.
4 Error Handling Policies
All interpretation problems in our system are han-
dled with a unified tutorial policy. Each message to
the user consists of three parts: a social response,
the explanation of the problem, and the tutorial re-
sponse. The social response is currently a simple
apology, as in ?I?m sorry, I?m having trouble under-
standing.? Research on spoken dialogue shows that
users are less frustrated if systems apologize for er-
rors (Bulyko et al, 2005).
The explanation of the problem depends on the
problem itself, and is discussed in more detail below.
The tutorial response depends on the general tu-
torial situation. If this is the first misunderstanding,
the student will be asked to rephrase/try again. If
3This was a real coverage problem we encountered early on.
While we extended the coverage of the lexical interpreter based
on corpus data, other gaps in coverage may remain. We discuss
the issues related to the treatment of vague or incorrect termi-
nology in Section 4.
they continue to phrase things in a way that is mis-
understood, they will be given up to two different
hints (a less specific hint followed by a more spe-
cific hint); and finally the system will bottom out
with a correct answer. Correct answers produced by
the generator are guaranteed to be parsed and under-
stood by the interpretation module, so they can serve
as templates for future student answers.
The tutorial policy is also adjusted depending
on the interaction history. For example, if a non-
understanding comes after a few incorrect answers,
the system may decide to bottom out immediately in
order to avoid student frustration due to multiple er-
rors. At present we are using a heuristic policy based
on the total number of incorrect or uninterpretable
answers. In the future, such policy could be learned
from data, using, for example, reinforcement learn-
ing (Williams and Young, 2007).
In the rest of this section we discuss the explana-
tions used for different problems. For brevity, we
omit the tutorial response from our examples.
4.1 Parse Failures
An utterance that cannot be parsed represents the
worst possible outcome for the system, since detect-
ing the reason for a syntactic parse failure isn?t pos-
sible for complex parsers and grammars. Thus, in
this instance the system does not give any descrip-
tion of the problem at all, saying simply ?I?m sorry,
I didn?t understand.?
Since we are unable to explain the source of the
problem, we try hard to avoid such failures. We use
a spelling corrector and a robust parser that outputs
a set of fragments covering the student?s input when
a full parse cannot be found. The downstream com-
ponents are designed to merge interpretations of the
fragments into a single representation that is sent to
the reasoning components.
Our policy is to allow the system to use such frag-
mentary parses when handling explanation ques-
tions, where students tend to use complex language.
However, we require full parses for factual ques-
tions, such as ?Which bulbs will be off?? We found
that for those simpler questions students are able to
easily phrase an acceptable answer, and the lack of
a full parse signals some unusually complex lan-
guage that downstream components are likely to
have problems with as well.
41
One risk associated with using fragmentary parses
is that relationships between objects from different
fragments would be missed by the parser. Our cur-
rent policy is to confirm the correct part of the stu-
dent?s answer, and prompt for the missing parts, e.g.,
? Right. The battery is contained in a closed path.
And then?? We can do this because we use a diag-
noser that explicitly identifies the correct objects and
relationships in the answer (Dzikovska et al, 2008),
and we are using a deep generation system that can
take those relationships and automatically generate
a rephrasing of the correct portion of the content.
4.2 Lexical Interpretation Errors
Errors in lexical interpretation typically come from
three main sources: unknown words which the lex-
ical interpreter cannot map into domain concepts,
unexpected word combinations, and incorrect uses
of terminology that violate the sortal constraints en-
coded in the domain model.
Unknown words are the simplest to deal with in
the context of our lexical interpretation policy. We
do not require that every single word of an utter-
ance should be interpreted, because we want the
system to be able to skip over irrelevant asides.
However, we require that if a predicate is inter-
preted, all its arguments should be interpreted as
well. To illustrate, in our system the interpretation of
?the bulb is still lit? is (LightBulb Bulb-1-1)
(is-lit Bulb-1-1 true). The adverbial
?still? is not interpreted because the system is un-
able to reason about time.4 But since all arguments
of the is-lit predicate are defined, we consider
the interpretation complete.
In contrast, in the sentence ?voltage is the mea-
surement of the power available in a battery?, ?mea-
surement? is known to the system. Thus, its argu-
ment ?power? should also be interpreted. However,
the reading material in the lessons never talks about
power (the expected answer is ?Voltage is a mea-
surement of the difference in electrical states be-
tween two terminals?). Therefore the unknown word
detector marks ?power? as an unknown word, and
tells the student ?I?m sorry, I?m having a problem
understanding. I don?t know the word power.?
4The lexical interpretation algorithm makes sure that fre-
quency and negation adverbs are accounted for.
The system can still have trouble interpreting sen-
tences with words which are known to the lexical
interpreter, but which appear in unexpected combi-
nations. This involves two possible scenarios. First,
unambiguous words could be used in a way that
contradicts the system?s domain model. For exam-
ple, the students often mention ?closed circuit? in-
stead of the correct term ?closed path?. The former
is valid in colloquial usage, but is not well defined
for parallel circuits which can contain many differ-
ent paths, and therefore cannot be represented in a
consistent knowledge base. Thus, the system con-
sults its knowledge base to tell the student about the
appropriate arguments for a relation with which the
failure occurred. In this instance, the feedback will
be ?I?m sorry, I?m having a problem understanding.
I don?t understand it when you say that circuits can
be closed. Only paths and switches can be closed.?5
The second case arises when a highly ambiguous
word is used in an unexpected combination. The
knowledge base uses a number of fine-grained rela-
tions, and therefore some words can map to a large
number of relations. For example, the word ?has?
means circuit-component in ?The circuit has
2 bulbs?, terminals-of in ?The bulb has ter-
minals? and voltage-property in ?The bat-
tery has voltage?. The last relation only applies to
batteries, but not to other components. These dis-
tinctions are common for knowledge representation
and reasoning systems, since they improve reason-
ing efficiency, but this adds to the difficulty of lex-
ical interpretation. If a student says ?Bulb A has a
voltage of 0.5?, we cannot determine the concept to
which the word ?has? corresponds. It could be either
terminals-of or voltage-property, since
each of those relations uses one possible argument
from the student?s utterance. Thus, we cannot sug-
gest appropriate argument types and instead we in-
dicate the problematic word combination, for exam-
ple, ?I?m sorry, I?m having trouble understanding. I
didn?t understand bulb has voltage.?
Finally, certain syntactic constructions involving
comparatives or ellipsis are known to be difficult
5Note that these error messages are based strictly on the fact
that sortal constraints from the knowledge base for the relation
that the student used were violated. In the future, we may also
want to adjust the recovery strategy depending on whether the
problematic relation is relevant to the expected answer.
42
open problems for interpretation. While we are
working on interpretation algorithms to be included
in future system versions, the system currently de-
tects these special relations, and produces a mes-
sage telling the student to rephrase without the prob-
lematic construction, e.g., ?I?m sorry. I?m having a
problem understanding. I do not understand same
as. Please try rephrasing without the word as.?
4.3 Reference Errors
Reference errors arise when a student uses an am-
biguous pronoun, and the system cannot find a suit-
able object in the knowledge base to match, or on
certain occasions when an attachment error in a
parse causes an incorrect interpretation. We use a
generic message that indicates the type of the ob-
ject the system perceived, and the actual word used,
for example, ?I?m sorry. I don?t know which switch
you?re referring to with it.?
To some extent, reference errors are instances of
misunderstandings rather than non-understandings.
There are actually 2 underlying cases for reference
failure: either the system cannot find any referent at
all, or it is finding too many referents. In the future
a better policy would be to ask the student which of
the ambiguous referents was intended. We expect to
pilot this policy in one of our future system tests.
5 Evaluation
So far, we have run 13 pilot sessions with our sys-
tem. Each pilot consisted of a student going through
1 or 2 lessons with the system. Each lesson lasts
about 2 hours and has 100-150 student utterances
(additional time is taken with building circuits and
reading material). Both the coverage of the interpre-
tation component and the specificity of error mes-
sages were improved between each set of pilots, thus
it does not make sense to aggregate the data from
them. However, over time we observed the trend
that students are more likely to change their behav-
ior when the system issues more specific messages.
Examples of successful and unsuccessful interac-
tions are shown in Figure 1. In (a), the student used
incorrect terminology, and a reminder about how the
word ?complete? is interpreted was enough to get
the conversation back on track.
The dialogue fragment in (b) shows how mes-
sages which are not specific enough can cause a
breakdown in conversation. The system used an in-
sufficiently specific message at the beginning (omit-
ting the part that says that only switches and paths
can be closed). This led the student away from an
answer which was nearly correct with slightly im-
perfect terminology to an answer which was insuffi-
cient (it?s not enough for the components to be con-
nected, they have to be in a closed path), and then
to rephrase it in a more complicated way that was
impossible for the system to understand (consistent
with findings of Bulyko et al (2005)).
The next step would be to conduct a formal evalu-
ation of our policy. We are planning to do this using
recovery rate after different strategies as our evalu-
ation measure (Bohus and Rudnicky, 2005), which
is the percentage of times when the utterance imme-
diately following a non-understanding was success-
fully interpreted by the system.
6 Discussion
Our error handling policy attempts to balance ro-
bustness in interpretation with the need for pre-
cise tutorial feedback. Human tutors accept a wide
range of variation in terminology and phrasing.
However, there is a known trade-off between non-
understandings and misunderstandings. We decided
that misunderstandings are likely to cause more seri-
ous problems with respect to tutoring. Therefore we
set strict understanding requirements and focused on
developing a policy that would encourage students
to use constructions that the system can interpret re-
liably. This is also supported by our observations in
the human-human data ? tutors believe that teaching
correct terminology is important, and we observed
them saying things like ?It?s good to put it into your
own words like that. Just for the sake of the lesson,
could you also use the term closed path??.
Our approach is similar to the TargetedHelp ap-
proach of (Hockey et al, 2003), where in the case of
an interpretation failure the task assistant produces
a report of what the recognizer heard, a description
of the problem, and a possible in-domain utterance
as an alternative. In a user evaluation such targeted
help messages resulted in significantly lower task
completion times compared to a system that pro-
duced a generic ?I didn?t understand? message, and
43
(a)
S: the open switch makes the path incomplete
T: I?m sorry, I?m having a problem understanding. Paths can?t be complete. Only circuits can be
complete. Please try again.
S: the open switch makes the path open
T: Right. An open switch creates a gap.
(b)
T: What are the conditions that are required to make a bulb light up?
S: The lightbulb has to be in a closed circuit with a battery
T: I?m sorry, I?m having a problem understanding. I don?t understand when you say that circuits
can be closed. Please try again.
S: The lightbulb has to be connected to a battery
T: Yes. You correctly mentioned a battery and a bulb. Keep trying.
S: Both the positive and negative connectors have to be connected to
T: I?m sorry, I?m having a problem understanding. Try rephrasing your answer.
S: The lightbulb has to be connected to the positive and negative connectors of the battery
T: I?m sorry, I didn?t understand. Here?s the answer. The battery and the bulb must be contained
in the same closed path.
Figure 1: Sample student-system interactions. S: is student, T: is tutor (system). (a) A successful interaction
where the student changes their language; (b) a failed interaction where system feedback was unhelpful.
subjects gradually learned how to talk to the sys-
tem, reducing the number of misunderstandings over
time. This gives us reason to believe that our sys-
tem can achieve similar effects in tutorial dialogue.
While we don?t suggest alternative domain utter-
ances due to the tutoring reasons described earlier,
the progressively more specific hints serve a simi-
lar function. To what extent this impacts learning
and interaction with the system will have to be de-
termined in future evaluations.
The error handling in our system is significantly
different from systems that analyze user essays be-
cause it needs to focus on a single sentence at a time.
In a system that does essay analysis, such as AUTO-
TUTOR (Graesser et al, 1999) or Why2-Atlas (Jor-
dan et al, 2006) a single essay can have many flaws.
So it doesn?t matter if some sentences are not fully
understood as long as the essay is understood well
enough to identify at least one flaw. Then that par-
ticular flaw can be remediated, and the student can
resubmit the essay. However, this can also cause stu-
dent frustration and potentially affect learning if the
student is asked to re-write an essay many times due
to interpretation errors.
Previous systems in the circuit domain focused on
troubleshooting rather than conceptual knowledge.
The SHERLOCK tutor (Katz et al, 1998) used only
menu-based input, limiting possible dialogue. Cir-
cuit Fix-It Shop (Smith and Gordon, 1997) was a
task-oriented system which allowed for speech in-
put, but with very limited vocabulary. Our system?s
larger vocabulary and complex input result in differ-
ent types of non-understandings that cannot be re-
solved with simple confirmation messages.
A number of researchers have developed er-
ror taxonomies for spoken dialogue systems (Paek,
2003; Mo?ller et al, 2007). Our classification does
not have speech recognition errors (since we are us-
ing typed dialogue), and we have a more complex
interpretation stack than the domain-specific pars-
ing utilized by many SDSs. However, some types
of errors are shared, in particular, our ?no parse?,
?unknown word? and ?unknown attachment? errors
correspond to command-level errors, and our sor-
tal constraint and reference errors correspond to
concept-level errors in the taxonomy of Mo?ller et al
(2007). This correspondence is not perfect because
of the nature of the task - there are no commands in
a tutoring system. However, the underlying causes
are very similar, and so research on the best way
44
to communicate about system failures would benefit
both tutoring and task-oriented dialogue systems. In
the long run, we would like to reconcile these differ-
ent taxonomies, leading to a unified classification of
system errors and recovery strategies.
7 Conclusion
In this paper we described our approach to handling
non-understanding errors in a tutorial dialogue sys-
tem. Explaining the source of errors, without giving
away the full answer, is crucial to establishing ef-
fective communication between the system and the
student. We described a classification of common
problems and our approach to dealing with different
classes of errors. Our experience with pilot studies,
as well as evidence from spoken dialogue systems,
indicates that our approach can help improve dia-
logue efficiency. We will be evaluating its impact on
both student learning and on dialogue efficiency in
the future.
8 Acknowledgments
This work has been supported in part by Office of
Naval Research grant N000140810043.
References
V. A. Aleven and K. R. Koedinger. 2000. The need for
tutorial dialog to support self-explanation. In Proc. of
AAAI Fall Symposion on Building Dialogue Systems
for Tutorial Applications.
O. P. V. Aleven. 2003. A knowledge-based approach
to understanding students? explanations. In School of
Information Technologies, University of Sydney.
J. Allen, M. Dzikovska, M. Manshadi, and M. Swift.
2007. Deep linguistic processing for spoken dialogue
systems. In Proceedings of the ACL-07 Workshop on
Deep Linguistic Processing.
D. Bohus and A. Rudnicky. 2005. Sorry, i didn?t catch
that! - an investigation of non-understanding errors
and recovery strategies. In Proceedings of SIGdial-
2005, Lisbon, Portugal.
I. Bulyko, K. Kirchhoff, M. Ostendorf, and J. Goldberg.
2005. Error-correction detection and response gener-
ation in a spoken dialogue system. Speech Communi-
cation, 45(3):271?288.
D. K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
M. O. Dzikovska, G. E. Campbell, C. B. Callaway, N. B.
Steinhauser, E. Farrow, J. D. Moore, L. A. Butler, and
C. Matheson. 2008. Diagnosing natural language an-
swers to support adaptive tutoring. In Proceedings
21st International FLAIRS Conference.
M. Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Proc.
of the AAAI Fall Symposium on Building Dialogue Sys-
tems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-Hastings,
and R. Kreuz. 1999. Autotutor: A simulation of a
human tutor. Cognitive Systems Research, 1:35?51.
B. A. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist,
J. Hieronymus, A. Gruenstein, and J. Dowding. 2003.
Targeted help for spoken dialogue systems: intelligent
feedback improves naive users? performance. In Pro-
ceedings of EACL.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tuto-
rial dialogue system for physics. In Proceedings of
FLAIRS?06.
L. Karsenty. 2001. Adapting verbal protocol methods to
investigate speech systems use. Applied Ergonomics,
32:15?22.
S. Katz, A. Lesgold, E. Hughes, D. Peters, G. Eggan,
M. Gordin, and L. Greenberg. 1998. Sherlock 2: An
intelligent tutoring system built on the lrdc framework.
In C. Bloom and R. Loftin, editors, Facilitating the
development and use of interactive learning environ-
ments. ERLBAUM.
S. Mo?ller, K.-P. Engelbrecht, and A. Oulasvirta. 2007.
Analysis of communication failures for spoken dia-
logue systems. In Proceedings of Interspeech.
R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Clas-
sification errors in a domain-independent assessment
system. In Proc. of the Third Workshop on Innovative
Use of NLP for Building Educational Applications.
T. Paek. 2003. Toward a taxonomy of communication
errors. In Proceedings of ISCA Workshop on Error
Handling in Spoken Dialogue Systems.
A. Purandare and D. Litman. 2008. Content-learning
correlations in spoken tutoring dialogs at word, turn
and discourse levels. In Proc.of FLAIRS.
R. W. Smith and S. A. Gordon. 1997. Effects of variable
initiative on linguistic behavior in human-computer
spoken natural language dialogue. Computational
Linguistics.
J. D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
45
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471?481,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Evaluating language understanding accuracy with respect to objective
outcomes in a dialogue system
Myroslava O. Dzikovska and Peter Bell and Amy Isard and Johanna D. Moore
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh, United Kingdom
{m.dzikovska,peter.bell,amy.isard,j.moore}@ed.ac.uk
Abstract
It is not always clear how the differences
in intrinsic evaluation metrics for a parser
or classifier will affect the performance of
the system that uses it. We investigate the
relationship between the intrinsic evalua-
tion scores of an interpretation component
in a tutorial dialogue system and the learn-
ing outcomes in an experiment with human
users. Following the PARADISE method-
ology, we use multiple linear regression to
build predictive models of learning gain,
an important objective outcome metric in
tutorial dialogue. We show that standard
intrinsic metrics such as F-score alone do
not predict the outcomes well. However,
we can build predictive performance func-
tions that account for up to 50% of the vari-
ance in learning gain by combining fea-
tures based on standard evaluation scores
and on the confusion matrix entries. We
argue that building such predictive mod-
els can help us better evaluate performance
of NLP components that cannot be distin-
guished based on F-score alone, and illus-
trate our approach by comparing the cur-
rent interpretation component in the system
to a new classifier trained on the evaluation
data.
1 Introduction
Much of the work in natural language processing
relies on intrinsic evaluation: computing standard
evaluation metrics such as precision, recall and F-
score on the same data set to compare the perfor-
mance of different approaches to the same NLP
problem. However, once a component, such as
a parser, is included in a larger system, it is not
always clear that improvements in intrinsic eval-
uation scores will translate into improved over-
all system performance. Therefore, extrinsic or
task-based evaluation can be used to complement
intrinsic evaluations. For example, NLP com-
ponents such as parsers and co-reference resolu-
tion algorithms could be compared in terms of
how much they contribute to the performance of
a textual entailment (RTE) system (Sammons et
al., 2010; Yuret et al 2010); parser performance
could be evaluated by how well it contributes to
an information retrieval task (Miyao et al 2008).
However, task-based evaluation can be difficult
and expensive for interactive applications. Specif-
ically, task-based evaluation for dialogue systems
typically involves collecting data from a number
of people interacting with the system, which is
time-consuming and labor-intensive. Thus, it is
desirable to develop an off-line evaluation pro-
cedure that relates intrinsic evaluation metrics to
predicted interaction outcomes, reducing the need
to conduct experiments with human participants.
This problem can be addressed via the use of
the PARADISE evaluation methodology for spo-
ken dialogue systems (Walker et al 2000). In a
PARADISE study, after an initial data collection
with users, a performance function is created to
predict an outcome metric (e.g., user satisfaction)
which can normally only be measured through
user surveys. Typically, a multiple linear regres-
sion is used to fit a predictive model of the desired
metric based on the values of interaction param-
eters that can be derived from system logs with-
out additional user studies (e.g., dialogue length,
word error rate, number of misunderstandings).
PARADISE models have been used extensively
in task-oriented spoken dialogue systems to estab-
lish which components of the system most need
improvement, with user satisfaction as the out-
come metric (Mo?ller et al 2007; Mo?ller et al
2008; Walker et al 2000; Larsen, 2003). In tu-
torial dialogue, PARADISE studies investigated
471
which manually annotated features predict learn-
ing outcomes, to justify new features needed in
the system (Forbes-Riley et al 2007; Rotaru and
Litman, 2006; Forbes-Riley and Litman, 2006).
We adapt the PARADISE methodology to eval-
uating individual NLP components, linking com-
monly used intrinsic evaluation scores with ex-
trinsic outcome metrics. We describe an evalua-
tion of an interpretation component of a tutorial
dialogue system, with student learning gain as the
target outcome measure. We first describe the
evaluation setup, which uses standard classifica-
tion accuracy metrics for system evaluation (Sec-
tion 2). We discuss the results of the intrinsic sys-
tem evaluation in Section 3. We then show that
standard evaluation metrics do not serve as good
predictors of system performance for the system
we evaluated. However, adding confusion matrix
features improves the predictive model (Section
4). We argue that in practical applications such
predictive metrics should be used alongside stan-
dard metrics for component evaluations, to bet-
ter predict how different components will perform
in the context of a specific task. We demonstrate
how this technique can help differentiate the out-
put quality between a majority class baseline, the
system?s output, and the output of a new classifier
we trained on our data (Section 5). Finally, we
discuss some limitations and possible extensions
to this approach (Section 6).
2 Evaluation Procedure
2.1 Data Collection
We collected transcripts of students interacting
with BEETLE II (Dzikovska et al 2010b), a tu-
torial dialogue system for teaching conceptual
knowledge in the basic electricity and electron-
ics domain. The system is a learning environment
with a self-contained curriculum targeted at stu-
dents with no knowledge of high school physics.
When interacting with the system, students spend
3-5 hours going through pre-prepared reading ma-
terial, building and observing circuits in a simula-
tor, and talking with a dialogue-based computer
tutor via a text-based chat interface.
During the interaction, students can be asked
two types of questions. Factual questions require
them to name a set of objects or a simple prop-
erty, e.g., ?Which components in circuit 1 are in
a closed path?? or ?Are bulbs A and B wired
in series or in parallel?. Explanation and defi-
nition questions require longer answers that con-
sist of 1-2 sentences, e.g., ?Why was bulb A on
when switch Z was open?? (expected answer ?Be-
cause it was still in a closed path with the bat-
tery?) or ?What is voltage?? (expected answer
?Voltage is the difference in states between two
terminals?). We focus on the performance of the
system on these long-answer questions, since re-
acting to them appropriately requires processing
more complex input than factual questions.
We collected a corpus of 35 dialogues from
paid undergraduate volunteers interacting with the
system as part of a formative system evaluation.
Each student completed a multiple-choice test as-
sessing their knowledge of the material before and
after the session. In addition, system logs con-
tained information about how each student?s utter-
ance was interpreted. The resulting data set con-
tains 3426 student answers grouped into 35 sub-
sets, paired with test results. The answers were
then manually annotated to create a gold standard
evaluation corpus.
2.2 BEETLE II Interpretation Output
The interpretation component of BEETLE II uses
a syntactic parser and a set of hand-authored rules
to extract the domain-specific semantic represen-
tations of student utterances from the text. The
student answer is first classified with respect to its
domain-specific speech act, as follows:
? Answer: a contentful expression to which
the system responds with a tutoring action,
either accepting it as correct or remediating
the problems as discussed in (Dzikovska et
al., 2010a).
? Help request: any expression indicating that
the student does not know the answer and
without domain content.
? Social: any expression such as ?sorry? which
appears to relate to social interaction and has
no recognizable domain content.
? Uninterpretable: the system could not arrive
at any interpretation of the utterance. It will
respond by identifying the likely source of
error, if possible (e.g., a word it does not un-
derstand) and asking the student to rephrase
their utterance (Dzikovska et al 2009).
472
If the student utterance was determined to be an
answer, it is further diagnosed for correctness as
discussed in (Dzikovska et al 2010b), using a do-
main reasoner together with semantic representa-
tions of expected correct answers supplied by hu-
man tutors. The resulting diagnosis contains the
following information:
? Consistency: whether the student statement
correctly describes the facts mentioned in
the question and the simulation environment:
e.g., student saying ?Switch X is closed? is
labeled inconsistent if the question stipulated
that this switch is open.
? Diagnosis: an analysis of how well the stu-
dent?s explanation matches the expected an-
swer. It consists of 4 parts
? Matched: parts of the student utterance
that matched the expected answer
? Contradictory: parts of the student ut-
terance that contradict the expected an-
swer
? Extra: parts of the student utterance that
do not appear in the expected answer
? Not-mentioned: parts of the expected
answer missing from the student utter-
ance.
The speech act and the diagnosis are passed to
the tutorial planner which makes decisions about
feedback. They constitute the output of the inter-
pretation component, and its quality is likely to
affect the learning outcomes, therefore we need
an effective way to evaluate it. In future work,
performance of individual pipeline components
could also be evaluated in a similar fashion.
2.3 Data Annotation
The general idea of breaking down the student an-
swer into correct, incorrect and missing parts is
common in tutorial dialogue systems (Nielsen et
al., 2008; Dzikovska et al 2010b; Jordan et al
2006). However, representation details are highly
system specific, and difficult and time-consuming
to annotate. Therefore we implemented a simpli-
fied annotation scheme which classifies whole an-
swers as correct, partially correct but incomplete,
or contradictory, without explicitly identifying the
correct and incorrect parts. This makes it easier to
create the gold standard and still retains useful in-
formation, because tutoring systems often choose
the tutoring strategy based on the general answer
class (correct, incomplete, or contradictory). In
addition, this allows us to cast the problem in
terms of classifier evaluation, and to use standard
classifier evaluation metrics. If more detailed an-
notations were available, this approach could eas-
ily be extended, as discussed in Section 6.
We employed a hierarchical annotation scheme
shown in Figure 1, which is a simplification of
the DeMAND coding scheme (Campbell et al
2009). Student utterances were first annotated
as either related to domain content, or not con-
taining any domain content, but expressing the
student?s metacognitive state or attitudes. Utter-
ances expressing domain content were then coded
with respect to their correctness, as being fully
correct, partially correct but incomplete, contain-
ing some errors (rather than just omissions) or
irrelevant1. The ?irrelevant? category was used
for utterances which were correct in general but
which did not directly answer the question. Inter-
annotator agreement for this annotation scheme
on the corpus was ? = 0.69.
The speech acts and diagnoses logged by the
system can be automatically mapped into our an-
notation labels. Help requests and social acts
are assigned the ?non-content? label; answers
are assigned a label based on which diagnosis
fields were filled: ?contradictory? for those an-
swers labeled as either inconsistent, or contain-
ing something in the contradictory field; ?incom-
plete? if there is something not mentioned, but
something matched as well, and ?irrelevant? if
nothing matched (i.e., the entire expected answer
is in not-mentioned). Finally, uninterpretable ut-
terances are treated as unclassified, analogous to a
situation where a statistical classifier does not out-
put a label for an input because the classification
probability is below its confidence threshold.
This mapping was then compared against the
manually annotated labels to compute the intrin-
sic evaluation scores for the BEETLE II interpreter
described in Section 3.
3 Intrinsic Evaluation Results
The interpretation component of BEETLE II was
developed based on the transcripts of 8 sessions
1Several different subcategories of non-content utter-
ances, and of contradictory utterances, were recorded. How-
ever, they resulting classes were too small and so were col-
lapsed into a single category for purposes of this study.
473
Category Subcategory Description
Non-content Metacognitive and social expressions without domain content, e.g., ?I
don?t know?, ?I need help?, ?you are stupid?
Content The utterance includes domain content.
correct The student answer is fully correct
pc incomplete The student said something correct, but incomplete, with some parts of
the expected answer missing
contradictory The student?s answer contains something incorrect or contradicting the
expected answer, rather than just an omission
irrelevant The student?s statement is correct in general, but it does not answer the
question.
Figure 1: Annotation scheme used in creating the gold standard
Label Count Frequency
correct 1438 0.43
pc incomplete 796 0.24
contradictory 808 0.24
irrelevant 105 0.03
non content 232 0.07
Table 1: Distribution of annotated labels in the evalu-
ation corpus
of students interacting with earlier versions of the
system. These sessions were completed prior to
the beginning of the experiment during which our
evaluation corpus was collected, and are not in-
cluded in the corpus. Thus, the corpus constitutes
unseen testing data for the BEETLE II interpreter.
Table 1 shows the distribution of codes in
the annotated data. The distribution is unbal-
anced, and therefore in our evaluation results we
use two different ways to average over per-class
evaluation scores. Macro-average combines per-
class scores disregarding the class sizes; micro-
average weighs the per-class scores by class size.
The overall classification accuracy (defined as the
number of correctly classified instances out of all
instances) is mathematically equivalent to micro-
averaged recall; however, macro-averaging better
reflects performance on small classes, and is com-
monly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)).
The detailed evaluation results are presented
in Table 2. We will focus on two metrics: the
overall classification accuracy (listed as ?micro-
averaged recall? as discussed above), and the
macro-averaged F score.
The majority class baseline is to assign ?cor-
rect? to every instance. Its overall accuracy is
43%, the same as BEETLE II. However, this is
obviously not a good choice for a tutoring sys-
tem, since students who make mistakes will never
get tutoring feedback. This is reflected in a much
lower value of the F score (0.12 macroaverage F
score for baseline vs. 0.44 for BEETLE II). Note
also that there is a large difference in the micro-
and macro- averaged scores. It is not immediately
clear which of these metrics is the most important,
and how they relate to actual system performance.
We discuss machine learning models to help an-
swer this question in the next section.
4 Linking Evaluation Measures to
Outcome Measures
Although the intrinsic evaluation shows that the
BEETLE II interpreter performs better than the
baseline on the F score, ultimately system devel-
opers are not interested in improving interpreta-
tion for its own sake: they want to know whether
the time spent on improvements, and the compli-
cations in system design which may accompany
them, are worth the effort. Specifically, do such
changes translate into improvement in overall sys-
tem performance?
To answer this question without running expen-
sive user studies we can build a model which pre-
dicts likely outcomes based on the data observed
so far, and then use the model?s predictions as an
additional evaluation metric. We chose a multiple
linear regression model for this task, linking the
classification scores with learning gain as mea-
sured during the data collection. This approach
follows the general PARADISE approach (Walker
et al 2000), but while PARADISE is typically
used to determine which system components need
474
Label baseline BEETLE II
prec. recall F1 prec. recall F1
correct 0.43 1.00 0.60 0.93 0.52 0.67
pc incomplete 0.00 0.00 0.00 0.42 0.53 0.47
contradictory 0.00 0.00 0.00 0.57 0.22 0.31
irrelevant 0.00 0.00 0.00 0.17 0.15 0.16
non-content 0.00 0.00 0.00 0.91 0.41 0.57
macroaverage 0.09 0.20 0.12 0.60 0.37 0.44
microaverage 0.18 0.43 0.25 0.70 0.43 0.51
Table 2: Intrinsic Evaluation Results for the BEETLE II and a majority class baseline
the most improvement, we focus on finding a bet-
ter performance metric for a single component
(interpretation), using standard evaluation scores
as features.
Recall from Section 2.1 that each participant
in our data collection was given a pre-test and
a post-test, measuring their knowledge of course
material. The test score was equal to the propor-
tion of correctly answered questions. The normal-
ized learning gain, post?pre1?pre is a metric typically
used to assess system quality in intelligent tutor-
ing, and this is the metric we are trying to model.
Thus, the training data for our model consists of
35 instances, each corresponding to a single dia-
logue and the learning gain associated with it. We
can compute intrinsic evaluation scores for each
dialogue, in order to build a model that predicts
that student?s learning gain based on these scores.
If the model?s predictions are sufficiently reliable,
we can also use them for predicting the learning
gain that a student could achieve when interacting
with a new version of the interpretation compo-
nent for the system, not yet tested with users. We
can then use the predicted score to compare dif-
ferent implementations and choose the one with
the highest predicted learning gain.
4.1 Features
Table 4 lists the feature sets we used. We tried two
basic types of features. First, we used the eval-
uation scores reported in the previous section as
features. Second, we hypothesized that some er-
rors that the system makes are likely to be worse
than others from a tutoring perspective. For ex-
ample, if the student gives a contradictory answer,
accepting it as correct may lead to student miscon-
ceptions; on the other hand, calling an irrelevant
answer ?partially correct but incomplete? may be
less of a problem. Therefore, we computed sepa-
rate confusion matrices for each student. We nor-
malized each confusion matrix cell by the total
number of incorrect classifications for that stu-
dent. We then added features based on confusion
frequencies to our feature set.2
Ideally, we should add 20 different features to
our model, corresponding to every possible con-
fusion. However, we are facing a sparse data
problem, illustrated by the overall confusion ma-
trix for the corpus in Table 3. For example,
we only observed 25 instances where a contra-
dictory utterance was miscategorized as correct
(compared to 200 ?contradictory?pc incomplete?
confusions), and so for many students this mis-
classification was never observed, and predictions
based on this feature are not likely to be reliable.
Therefore, we limited our features to those mis-
classifications that occurred at least twice for each
student (i.e., at least 70 times in the entire cor-
pus). The list of resulting features is shown in the
?conf? row of Table 4. Since only a small num-
ber of features was included, this limits the appli-
cability of the model we derived from this data
set to the systems which make similar types of
confusions. However, it is still interesting to in-
vestigate whether confusion probabilities provide
additional information compared to standard eval-
uation metrics. We discuss how better coverage
could be obtained in Section 6.
4.2 Regression Models
Table 5 shows the regression models we obtained
using different feature sets. All models were ob-
tained using stepwise linear regression, using the
Akaike information criterion (AIC) for variable
2We also experimented with using % unclassified as an
additional feature, since % of rejections is known to be a
problem for spoken dialogue systems. However, it did not
improve the models, and we do not report it here for brevity.
475
Actual
Predicted contradictory correct irrelevant non-content pc incomplete
contradictory 175 86 3 0 43
correct 25 752 1 4 26
irrelevant 31 12 16 4 29
non-content 1 3 2 95 3
pc incomplete 200 317 40 28 419
Table 3: Confusion matrix for BEETLE II. System predicted values are in rows; actual values in columns.
selection implemented in the R stepwise regres-
sion library. As measures of model quality, we re-
port R2, the percentage of variance accounted for
by the models (a typical measure of fit in regres-
sion modeling), and mean squared error (MSE).
These were estimated using leave-one-out cross-
validation, since our data set is small.
We used feature ablation to evaluate the contri-
bution of different features. First, we investigated
models using precision, recall or F-score alone.
As can be seen from the table, precision is not pre-
dictive of learning gain, while F-score and recall
perform similarly to one another, withR2 = 0.12.
In comparison, the model using only confusion
frequencies has substantially higher estimated R2
and a lower MSE.3 In addition, out of the 3 con-
fusion features, only one is selected as predictive.
This supports our hypothesis that different types
of errors may have different importance within a
practical system.
The confusion frequency feature chosen by
the stepwise model (?predicted-pc incomplete-
actual-contradictory?) has a reasonable theoret-
ical justification. Previous research shows that
students who give more correct or partially cor-
rect answers, either in human-human or human-
computer dialogue, exhibit higher learning gains,
and this has been established for different sys-
tems and tutoring domains (Litman et al 2009).
Consequently, % of contradictory answers is neg-
atively predictive of learning gain. It is reasonable
to suppose, as predicted by our model, that sys-
tems that do not identify such answers well, and
therefore do not remediate them correctly, will do
worse in terms of learning outcomes.
Based on this initial finding, we investigated
the models that combined either F scores or the
3The decrease in MSE is not statistically significant, pos-
sibly because of the small data set. However, since we ob-
serve the same pattern of results across our models, it is still
useful to examine.
full set of intrinsic evaluation scores with confu-
sion frequencies. Note that if the full set of met-
rics (precision, recall, F score) is used, the model
derives a more complex formula which covers
about 33% of the variance. Our best models,
however, combine the averaged scores with con-
fusion frequencies, resulting in a higher R2 and
a lower MSE (22% relative decrease between the
?scores.f? and ?conf+scores.f? models in the ta-
ble). This shows that these features have comple-
mentary information, and that combining them in
an application-specific way may help to predict
how the components will behave in practice.
5 Using prediction models in evaluation
The models from Table 5 can be used to compare
different possible implementations of the inter-
pretation component, under the assumption that
the component with a higher predicted learning
gain score is more appropriate to use in an ITS.
To show how our predictive models can be used
in making implementation decisions, we compare
three possible choices for an interpretation com-
ponent: the original BEETLE II interpreter, the
baseline classifier described earlier, and a new de-
cision tree classifier trained on our data.
We built a decision tree classifier using the
Weka implementation of C4.5 pruned decision
trees, with default parameters. As features, we
used lexical similarity scores computed by the
Text::Similarity package4. We computed
8 features: the similarity between student answer
and either the expected answer text or the question
text, using 4 different scores: raw number of over-
lapping words, F1 score, lesk score and cosine
score. Its intrinsic evaluation scores are shown in
Table 6, estimated using 10-fold cross-validation.
We can compare BEETLE II and baseline clas-
sifier using the ?scores.all? model. The predicted
4http://search.cpan.org/dist/Text-Similarity/
476
Name Variables
scores.fm fmeasure.microaverage, fmeasure.macroaverage, fmeasure.correct,
fmeasure.contradictory, fmeasure.pc incomplete,fmeasure.non-content,
fmeasure.irrelevant
scores.precision precision.microaverage, precision.macroaverage, precision.correct,
precision.contradictory, precision.pc incomplete,precision.non-content,
precision.irrelevant
scores.recall recall.microaverage, recall.macroaverage, recall.correct, recall.contradictory,
recall.pc incomplete,recall.non-content, recall.irrelevant
scores.all scores.fm + scores.precision + scores.recall
conf Freq.predicted.contradictory.actual.correct,
Freq.predicted.pc incomplete.actual.correct,
Freq.predicted.pc incomplete.actual.contradictory
Table 4: Feature sets for regression models
Variables Cross-
validation
R2
Cross-
validation
MSE
Formula
scores.f 0.12
(0.02)
0.0232
(0.0302)
0.32
+ 0.56 ? fmeasure.microaverage
scores.precision 0.00
(0.00)
0.0242
(0.0370)
0.61
scores.recall 0.12
(0.02)
0.0232
(0.0310)
0.37
+ 0.56 ? recall.microaverage
conf 0.25
(0.03)
0.0197
(0.0262)
0.74
? 0.56 ?
Freq.predicted.pc incomplete.actual.contradictory
scores.all 0.33
(0.03)
0.0218
(0.0264)
0.63
+ 4.20 ? fmeasure.microaverage
? 1.30 ? precision.microaverage
? 2.79 ? recall.microaverage
? 0.07 ? recall.non? content
conf+scores.f 0.36
(0.03)
0.0179
(0.0281)
0.52
? 0.66 ?
Freq.predicted.pc incomplete.actual.contradictory
+ 0.42 ? fmeasure.correct
? 0.07 ? fmeasure.non? content
full
(conf+scores.all)
0.49
(0.02)
0.0189
(0.0248)
0.88
? 0.68 ?
Freq.predicted.pc incomplete.actual.contradictory
? 0.06 ? precision.non domain
+ 0.28 ? recall.correct
? 0.79 ? precision.microaverage
+ 0.65 ? fmeasure.microaverage
Table 5: Regression models for learning gain. R2 and MSE estimated with leave-one-out cross-validation.
Standard deviation in parentheses.
477
score for BEETLE II is 0.66. The predicted
score for the baseline is 0.28. We cannot use
the models based on confusion scores (?conf?,
?conf+scores.f? or ?full?) for evaluating the base-
line, because the confusions it makes are always
to predict that the answer is correct when the
actual label is ?incomplete? or ?contradictory?.
Such situations were too rare in our training data,
and therefore were not included in the models (as
discussed in Section 4.1). Additional data will
need to be collected before this model can rea-
sonably predict baseline behavior.
Compared to our new classifier, BEETLE II has
lower overall accuracy (0.43 vs. 0.53), but per-
forms micro- and macro- averaged scores. BEE-
TLE II precision is higher than that of the classi-
fier. This is not unexpected given how the system
was designed: since misunderstandings caused
dialogue breakdown in pilot tests, the interpreter
was built to prefer rejecting utterances as uninter-
pretable rather than assigning them to an incorrect
class, leading to high precision but lower recall.
However, we can use all our predictive models
to evaluate the classifier. We checked the the con-
fusion matrix (not shown here due to space lim-
itations), and saw that the classifier made some
of the same types of confusions that BEETLE II
interpreter made. On the ?scores.all? model, the
predicted learning gain score for the classifier is
0.63, also very close to BEETLE II. But with the
?conf+scores.all? model, the predicted score is
0.89, compared to 0.59 for BEETLE II, indicating
that we should prefer the newly built classifier.
Looking at individual class performance, the
classifier performs better than the BEETLE II in-
terpreter on identifying ?correct? and ?contradic-
tory? answers, but does not do as well for par-
tially correct but incomplete, and for irrelevant an-
swers. Using our predictive performance metric
highlights the differences between the classifiers
and effectively helps determine which confusion
types are the most important.
One limitation of this prediction, however, is
that the original system?s output is considerably
more complex: the BEETLE II interpreter explic-
itly identifies correct, incorrect and missing parts
of the student answer which are then used by the
system to formulate adaptive feedback. This is
an important feature of the system because it al-
lows for implementation of strategies such as ac-
knowledging and restating correct parts of the an-
Label prec. recall F1
correct 0.66 0.76 0.71
pc incomplete 0.38 0.34 0.36
contradictory 0.40 0.35 0.37
irrelevant 0.07 0.04 0.05
non-content 0.62 0.76 0.68
macroaverage 0.43 0.45 0.43
microaverage 0.51 0.53 0.52
Table 6: Intrinsic evaluation scores for our newly built
classifier.
swer. However, we could still use a classifier to
?double-check? the interpreter?s output. If the
predictions made by the original interpreter and
the classifier differ, and in particular when the
classifier assigns the ?contradictory? label to an
answer, BEETLE II may choose to use a generic
strategy for contradictory utterances, e.g. telling
the student that their answer is incorrect without
specifying the exact problem, or asking them to
re-read portions of the material.
6 Discussion and Future Work
In this paper, we proposed an approach for cost-
sensitive evaluation of language interpretation
within practical applications. Our approach is
based on the PARADISE methodology for dia-
logue system evaluation (Walker et al 2000).
We followed the typical pattern of a PARADISE
study, but instead of relying on a variety of fea-
tures that characterize the interaction, we used
scores that reflect only the performance of the
interpretation component. For BEETLE II we
could build regression models that account for
nearly 50% variance in the desired outcomes, on
par with models reported in earlier PARADISE
studies (Mo?ller et al 2007; Mo?ller et al 2008;
Walker et al 2000; Larsen, 2003). More impor-
tantly, we demonstrated that combining averaged
scores with features based on confusion frequen-
cies improves prediction quality and allows us to
see differences between systems which are not ob-
vious from the scores alone.
Previous work on task-based evaluation of NLP
components used RTE or information extraction
as target tasks (Sammons et al 2010; Yuret et al
2010; Miyao et al 2008), based on standard cor-
pora. We specifically targeted applications which
involve human-computer interaction, where run-
ning task-based evaluations is particularly expen-
478
sive, and building a predictive model of system
performance can simplify system development.
Our evaluation data limited the set of features
that we could use in our models. For most con-
fusion features, there were not enough instances
in the data to build a model that would reliably
predict learning gain for those cases. One way
to solve this problem would be to conduct a user
study in which the system simulates random er-
rors appearing some of the time. This could pro-
vide the data needed for more accurate models.
The general pattern we observed in our data
is that a model based on F-scores alone predicts
only a small proportion of the variance. If a full
set of metrics (including F-score, precision and
recall) is used, linear regression derives a more
complex equation, with different weights for pre-
cision and recall. Instead of the linear model, we
may consider using a model based on F? score,
F? = (1 + ?2) PR?2P+R , and fitting it to the data to
derive the ? weight rather than using the standard
F1 score. We plan to investigate this in the future.
Our method would apply to a wide range of
systems. It can be used straightforwardly with
many current spoken dialogue systems which rely
on classifiers to support language understanding
in domains such as call routing and technical sup-
port (Gupta et al 2006; Acomb et al 2007).
We applied it to a system that outputs more com-
plex logical forms, but we showed that we could
simplify its output to a set of labels which still
allowed us to make informed decisions. Simi-
lar simplifications could be derived for other sys-
tems based on domain-specific dialogue acts typ-
ically used in dialogue management. For slot-
based systems, it may be useful to consider con-
cept accuracy for recognizing individual slot val-
ues. Finally, for tutoring systems it is possible
to annotate the answers on a more fine-grained
level. Nielsen et al(2008) proposed an annota-
tion scheme based on the output of a dependency
parser, and trained a classifier to identify individ-
ual dependencies as ?expressed?, ?contradicted?
or ?unaddressed?. Their system could be evalu-
ated using the same approach.
The specific formulas we derived are not likely
to be highly generalizable. It is a well-known
limitation of PARADISE evaluations that models
built based on one system often do not perform
well when applied to different systems (Mo?ller et
al., 2008). But using them to compare implemen-
tation variants during the system development,
without re-running user evaluations, can provide
important information, as we illustrated with an
example of evaluating a new classifier we built for
our interpretation task. Moreover, the confusion
frequency feature that our models picked is con-
sistent with earlier results from a different tutor-
ing domain (see Section 4.2). Thus, these models
could provide a starting point when making sys-
tem development choices, which can then be con-
firmed by user evaluations in new domains.
The models we built do not fully account for
the variance in the training data. This is expected,
since interpretation performance is not the only
factor influencing the objective outcome: other
factors, such choosing the the appropriate tutor-
ing strategy, are also important. Similar models
could be built for other system components to ac-
count for their contribution to the variance. Fi-
nally, we could consider using different learning
algorithms. Mo?ller et al(2008) examined deci-
sion trees and neural networks in addition to mul-
tiple linear regression for predicting user satisfac-
tion in spoken dialogue. They found that neural
networks had the best prediction performance for
their task. We plan to explore other learning algo-
rithms for this task as part of our future work.
7 Conclusion
In this paper, we described an evaluation of an
interpretation component of a tutorial dialogue
system using predictive models that link intrin-
sic evaluation scores with learning outcomes. We
showed that adding features based on confusion
frequencies for individual classes significantly
improves the prediction. This approach can be
used to compare different implementations of lan-
guage interpretation components, and to decide
which option to use, based on the predicted im-
provement in a task-specific target outcome met-
ric trained on previous evaluation data.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation; and Christo-
pher Brew for helpful comments and discussion.
This work has been supported in part by the US
ONR award N000141010085.
479
References
Kate Acomb, Jonathan Bloom, Krishna Dayanidhi,
Phillip Hunter, Peter Krogh, Esther Levin, and
Roberto Pieraccini. 2007. Technical support dia-
log systems: Issues, problems, and solutions. In
Proceedings of the Workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Tech-
nologies, pages 25?31, Rochester, NY, April.
Gwendolyn C. Campbell, Natalie B. Steinhauser,
Myroslava O. Dzikovska, Johanna D. Moore,
Charles B. Callaway, and Elaine Farrow. 2009. The
DeMAND coding scheme: A ?common language?
for representing and analyzing student discourse. In
Proceedings of 14th International Conference on
Artificial Intelligence in Education (AIED), poster
session, Brighton, UK, July.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn E. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of the SIGDIAL 2009 Conference, pages
38?45, London, UK, September.
Myroslava Dzikovska, Diana Bental, Johanna D.
Moore, Natalie B. Steinhauser, Gwendolyn E.
Campbell, Elaine Farrow, and Charles B. Callaway.
2010a. Intelligent tutoring with natural language
support in the Beetle II system. In Sustaining TEL:
From Innovation to Learning and Practice - 5th Eu-
ropean Conference on Technology Enhanced Learn-
ing, (EC-TEL 2010), Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a sys-
tem for tutoring and computational linguistics ex-
perimentation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2010) demo session, Uppsala, Swe-
den, July.
Kate Forbes-Riley and Diane J. Litman. 2006. Mod-
elling user satisfaction and student learning in a
spoken dialogue tutoring system with generic, tu-
toring, and user affect parameters. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT-NAACL
?06), pages 264?271, Stroudsburg, PA, USA.
Kate Forbes-Riley, Diane Litman, Amruta Purandare,
Mihai Rotaru, and Joel Tetreault. 2007. Compar-
ing linguistic features for modeling learning in com-
puter tutoring. In Proceedings of the 2007 confer-
ence on Artificial Intelligence in Education: Build-
ing Technology Rich Learning Contexts That Work,
pages 270?277, Amsterdam, The Netherlands. IOS
Press.
Narendra K. Gupta, Go?khan Tu?r, Dilek Hakkani-Tu?r,
Srinivas Bangalore, Giuseppe Riccardi, and Mazin
Gilbert. 2006. The AT&T spoken language un-
derstanding system. IEEE Transactions on Audio,
Speech & Language Processing, 14(1):213?222.
Pamela W. Jordan, Maxim Makatchev, and Umarani
Pappuswamy. 2006. Understanding complex nat-
ural language explanations in tutorial applications.
In Proceedings of the Third Workshop on Scalable
Natural Language Understanding, ScaNaLU ?06,
pages 17?24.
Lars Bo Larsen. 2003. Issues in the evaluation of spo-
ken dialogue systems using objective and subjective
measures. In Proceedings of the 2003 IEEE Work-
shop on Automatic Speech Recognition and Under-
standing, pages 209?214.
David D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the workshop on Speech and Nat-
ural Language, HLT ?91, pages 312?318, Strouds-
burg, PA, USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural lan-
guage processing to analyze tutorial dialogue cor-
pora across domains and modalities. In Proceed-
ings of 14th International Conference on Artificial
Intelligence in Education (AIED), Brighton, UK,
July.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of ACL-08: HLT, pages 46?
54, Columbus, Ohio, June.
Sebastian Mo?ller, Paula Smeele, Heleen Boland, and
Jan Krebber. 2007. Evaluating spoken dialogue
systems according to de-facto standards: A case
study. Computer Speech & Language, 21(1):26 ?
53.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, and
Robert Schleicher. 2008. Predicting the quality and
usability of spoken dialogue services. Speech Com-
munication, pages 730?744.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Mihai Rotaru and Diane J. Litman. 2006. Exploit-
ing discourse structure for spoken dialogue perfor-
mance analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?06, pages 85?93, Strouds-
burg, PA, USA.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan
Roth. 2010. ?Ask not what textual entailment can
do for you...?. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1199?1208, Uppsala, Sweden, July.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
480
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
51?56, Uppsala, Sweden, July.
481
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200?210,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Towards Effective Tutorial Feedback for Explanation Questions:
A Dataset and Baselines
Myroslava O. Dzikovska? and Rodney D. Nielsen? and Chris Brew?
?School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, UK
?Computational Language & Education Research Center
University of Colorado at Boulder, Boulder, CO 80309-0594, USA
?Educational Testing Service, Princeton, NJ 08451, USA
m.dzikovska@ed.ac.uk, rodney.nielsen@colorado.edu, cbrew@ets.org
Abstract
We propose a new shared task on grading stu-
dent answers with the goal of enabling well-
targeted and flexible feedback in a tutorial di-
alogue setting. We provide an annotated cor-
pus designed for the purpose, a precise speci-
fication for a prediction task and an associated
evaluation methodology. The task is feasible
but non-trivial, which is demonstrated by cre-
ating and comparing three alternative baseline
systems. We believe that this corpus will be
of interest to the researchers working in tex-
tual entailment and will stimulate new devel-
opments both in natural language processing
in tutorial dialogue systems and textual entail-
ment, contradiction detection and other tech-
niques of interest for a variety of computa-
tional linguistics tasks.
1 Introduction
In human-human tutoring, it is an effective strategy
to ask students to explain instructional material in
their own words. Self-explanation (Chi et al, 1994)
and contentful talk focused on the domain are cor-
related with better learning outcomes (Litman et al,
2009; Chi et al, 1994). There has therefore been
much interest in developing automated tutorial dia-
logue systems that ask students open-ended expla-
nation questions (Graesser et al, 1999; Aleven et
al., 2001; Jordan et al, 2006; VanLehn et al, 2007;
Nielsen et al, 2009; Dzikovska et al, 2010a). In
order to do this well, it is not enough to simply
ask the initiating question, because students need
the experience of engaging in meaningful dialogue
about the instructional content. Thus, systems must
respond appropriately to student explanations, and
must provide detailed, flexible and appropriate feed-
back (Aleven et al, 2002; Jordan et al, 2004).
In simple domains, we can adopt a knowledge en-
gineering approach and build a domain model and a
diagnoser, together with a natural language parser to
produce detailed semantic representations of student
input (Glass, 2000; Aleven et al, 2002; Pon-Barry
et al, 2004; Callaway et al, 2006; Dzikovska et al,
2010a). The advantage of this approach is that it
allows for flexible adaptation of feedback to a va-
riety of factors such as student performance. For
example, it is easy for the system to know if the
student made the same error before, and adjust its
feedback to reflect it. Moreover, this approach al-
lows for easy addition of new exercises : as long as
an exercise relies on the concepts covered by the do-
main model, the system can apply standard instruc-
tional strategies to each new question automatically.
However, this approach is significantly limited by
the requirement that the domain be small enough to
allow comprehensive knowledge engineering, and it
is very labor-intensive even for small domains.
Alternatively, we can adopt a data-driven ap-
proach, asking human tutors to anticipate in ad-
vance a range of possible correct and incorrect an-
swers, and associating each answer with an appro-
priate remediation (Graesser et al, 1999; Jordan et
al., 2004; VanLehn et al, 2007). The advantage
of this approach is that it allows more complex and
interesting domains and provides a good framework
for eliciting the necessary information from the hu-
man experts. A weakness of this approach, which
200
also arises in content-scoring applications such as
ETS?s c-rater (Leacock and Chodorow, 2003), is that
human experts find it extremely difficult to predict
with any certainty what the full range of student re-
sponses will be. This leads to a lack of adaptivity
and generality ? if the system designers have failed
to predict the full range of possibilities, students will
often receive the default feedback. It is frustrating
and confusing for students to repeatedly receive the
same feedback, regardless of their past performance
or dialogue context (Jordan, 2004).
Our goal is to address the weaknesses of the data-
driven approach by creating a framework for sup-
porting more flexible and systematic feedback. Our
approach identifies general classes of error, such as
omissions, incorrect statements and off-topic state-
ments, then aims to develop general remediation
strategies for each error type. This has the potential
to free system designers from the need to pre-author
separate remediations for each individual question.
A precondition for the success of this approach is
that the system be able to identify error types based
on the student response and the model answers.
A contribution of this paper is to provide a new
dataset that will enable researchers to develop clas-
sifiers specifically for this purpose. The hope is that
with an appropriate dataset the data-driven approach
will be flexible and responsive enough to maintain
student engagement. We provide a corpus that is la-
beled for a set of five student response types, develop
a precise definition of the corresponding supervised
classification task, and report results for a variety of
simple baseline classifiers. This will provide a ba-
sis for the development, comparison and evaluation
of alternative approaches to the error classification
task. We believe that the natural language capabil-
ities needed for this task will be directly applicable
to a far wider range of tasks in educational assess-
ment, information extraction and computational se-
mantics. This dataset is publicly available and will
be used in a community-wide shared task.
2 Corpus
The data set we developed draws on two established
sources ? a data set collected and annotated during
an evaluation of the BEETLE II tutorial dialogue sys-
tem (Dzikovska et al, 2010a) (henceforth, BEETLE
corpus) and a set of student answers to questions
from 16 science modules in the Assessing Science
Knowledge (ASK) assessment inventory (Lawrence
Hall of Science, 2006) (henceforth, the Science En-
tailments Bank or SCIENTSBANK).
In both corpora, each question was associated
with one or more reference answers provided by
the experts. Student answers were evaluated against
these reference answers and, using corpus-specific
annotation schemes, assigned labels for correct-
ness. In order to reconcile the two different schemes
and to cast the task in terms of standard supervised
machine classification at the sentence level, we de-
rived a new set of annotations, using the annotation
scheme shown in Figure 1.
Our label set has some similarity to the RTE5 3-
way task (Bentivogli et al, 2009), which used ?en-
tailment?, ?contradiction? and ?unknown? labels.
The additional distinctions in our labels reflect typi-
cal distinctions made by tutorial dialogue systems.
They match our human tutors? intuitions about
the general error types observed in student answers
and corresponding teaching tactics. For example,
a likely response to ?partially correct incomplete?
would be to tell the student that what they said so far
was correct but it had some gaps, and to encourage
them to fill in those gaps. In contrast, the response
to ?contradictory? would emphasize that there is a
mistake and the student needs to change their an-
swer rather than just expand it. Finally, the response
to ?irrelevant? may encourage the student to address
relevant concepts. The ?non domain? content could
be an indicator that the student is frustrated or con-
fused, and may require special attention.
The annotations in the source corpora make some
more fine-grained distinctions based on the needs of
the corresponding systems. In principle, it is possi-
ble to have answers that have both correct and con-
tradictory parts, and acknowledge correct parts be-
fore pointing out mistakes. There are also distinct
classes of ?non domain? utterances, e.g., social and
metacognitive statements, to which an ITS may want
to react differently (described in Section 2.1). How-
ever, these situations were rare in our corpora, and
we decided to use a single class for all contradictory
answers and a single non-domain class. This may be
expanded in the future as more data becomes avail-
able for new versions of this challenge task.
201
Label Definition
non domain does not contain domain content, e.g., a help request or ?I don?t know?
correct the student answer is correct
partially correct incomplete the answer does not contradict the reference answer and includes some
correct nuggets, but parts are missing
contradictory an answer that contradicts some part of the reference answer
irrelevant contains domain content, but does not answer the question
Figure 1: The set of answer labels used in our task
We further discuss the relationship with the task
of recognizing textual entailment in Section 5. In
the rest of this section, we describe our corpora and
discuss how we obtained these labels from the raw
data available in our datasets.
2.1 BEETLE II data
The BEETLE corpus consists of the interactions be-
tween students and the BEETLE II tutorial dialogue
system (Dzikovska et al, 2010b). The BEETLE II
system is an intelligent tutoring system that teaches
students with no knowledge of high-school physics
concepts in basic electricity and electronics. In the
first system evaluation, students spend 3-5 hours go-
ing through prepared reading material, building and
observing circuits in the simulator and interacting
with a dialogue-based tutor. The interaction was
by keyboard, with the computer tutor asking ques-
tions, receiving replies and providing feedback via a
text-based chat interface. The data from 73 under-
graduate volunteer participants at southeastern US
university were recorded and annotated to form the
BEETLE human-computer dialogue corpus.
The BEETLE II lesson material contains two types
of questions. Factual questions require them to name
a set of objects or a simple property, e.g., ?Which
components in circuit 1 are in a closed path?? or
?Are bulbs A and B wired in series or in parallel?.
Explanation and definition questions require longer
answers that consist of 1-2 sentences, e.g., ?Why
was bulb A on when switch Z was open?? (expected
answer ?Because it was still in a closed path with the
battery?) or ?What is voltage?? (expected answer
?Voltage is the difference in states between two ter-
minals?). From the full BEETLE evaluation corpus,
we automatically extracted only the students? an-
swers to explanation and definition questions, since
reacting to them appropriately requires processing
more complex input than factual questions.
The extracted answers were filtered to remove du-
plicates. In the BEETLE II lesson material there
are a number of similar questions and the tutor ef-
fectively had a template answer such as ?Terminal
X is connected to the negative/positive battery ter-
minal?. A number of students picked up on this
and used the same pattern in their responses (Stein-
hauser et al, 2011). This resulted in a number of an-
swers to certain questions that came from different
speakers but which were exact copies of each other.
We removed such answers from the data set, since
they were likely to be in both the training and test
set, thus inflating our results. Note that only exact
matches were removed: for example, answers that
were nearly identical but contained spelling errors
were retained, since they would need to be handled
in a practical system.
Student utterances were manually labeled using a
simplified version of the DEMAND coding scheme
(Campbell et al, 2009) shown in Figure 2. The utter-
ances were first classified as related to domain con-
tent, student?s metacognitive state, or social inter-
action. Utterances addressing domain content were
further classified with respect to their correctness as
described in the table. The Kappa value for this
annotation effort was ? = 0.69.
This annotation maps straightforwardly into our
set of labels. The social and metacognitive state-
ments are mapped to the ?non domain? label;
?pc some error?, ?pc? and ?incorrect? are mapped
to the ?contradictory? label; and the other classes
have a one-to-one correspondence with our task la-
bels.
2.2 SCIENTSBANK data
The SCIENTSBANK corpus (Nielsen et al, 2008)
consists of student responses to science assessment
202
Category Subcategory Description
Metacognitive positive
negative
content-free expressions describing student knowledge, e.g., ?I don?t
know?
Social positive
negative
neutral
expressions describing student?s attitudes towards themselves and
the computer (mostly negative in this data, e.g., ?You are stupid?)
Content the utterance addresses domain content.
correct the student answer is fully correct
pc some missing the student said something correct, but incomplete
incorrect the student?s answer is completely incorrect
pc some error the student?s answer contains correct parts, but some errors as well
pc the answer contains a mixture of correct, incorrect and missing parts
irrelevant the answer may be correct or incorrect, but it is not answering the
question.
Figure 2: Annotation scheme used in the BEETLE corpus
questions. Specifically, around 16k answers were
collected spanning 16 distinct science subject ar-
eas within physical sciences, life sciences, earth
sciences, space sciences, scientific reasoning and
technology. The tests were part of the Berke-
ley Lawrence Hall of Science Assessing Science
Knowledge (ASK) standardized assessments cover-
ing material from their Full Option Science System
(FOSS) (Lawrence Hall of Science, 2011). The an-
swers came from students in grades 3-6 in schools
across North America.
The tests included a variety of questions includ-
ing ?fill in the blank? and multiple choice, but the
SCIENTSBANK corpus only used a subset that re-
quired students to explain their beliefs about top-
ics, typically in one to two sentences. We reviewed
the questions and a sample of the responses and
decided to filter the following types of questions
from the corpus, because they did not mesh with
our goals. First, we removed questions whose ex-
pected answer was more than two full sentences
(typically multi-step procedures), which were be-
yond the scope of our task. Second, we removed
questions where the expected answer was ill-defined
or very open-ended. Finally, the most frequent rea-
son for removing questions was an extreme imbal-
ance in the answer classifications (e.g., for many
questions, almost all of the answers were labeled
?partially correct incomplete?). Specifically, we re-
moved questions where more than 80% of the an-
swers had the same label and questions with fewer
than three correct answers, since these questions
were unlikely to be useful in differentiating between
the quality of assessment systems.
The SCIENTSBANK corpus was developed for the
purpose of assessing student responses at a very fine-
grained level. The reference answers were broken
down into several facets, which consisted roughly
of two key terms and the relation connecting them.
Nielsen et al annotated student responses to indicate
for each reference answer facet whether the response
1) implied the student understood the facet, 2) im-
plied they held a contradictory belief, 3) included a
related, non-contradicting facet, or 4) left the facet
unaddressed. Reported agreement was 86.2% with
a kappa statistic (Cohen, 1960) of 0.728, which is in
the range of substantial agreement.1
Because our task focuses on answer classifica-
tion rather than facet classification, we developed a
set of rules indicating which combinations of facets
constituted a correct answer. We were then able
to compute an answer label from the gold-standard
facet annotations, as follows. First, if any facet
was annotated as contradictory, the answer was also
labeled ?contradictory?. Second, if all of the ex-
pected facets for any valid answer were annotated
as being understood, the answer was labeled ?cor-
1These statistics were actually based on five labels, but we
chose to combine the fifth, a self-contradiction, with other con-
tradictions for the purposes of our task.
203
rect?. Third, the remaining answers that included
some but not all of the expected facets were la-
beled ?partially correct incomplete?. Fourth, if an
answer matched none of the expected facets, and
had not been previously labeled as ?contradictory? it
was given the label ?irrelevant?. Finally, all ?irrele-
vant? answers were reviewed manually to determine
whether they should be relabeled as ?non domain?.
However, since Nielsen et al had already removed
most of the responses that originally fell into this
category, we only found 24 ?non domain? answers.
3 Baselines
We established three baselines for our data set ? a
straightforward majority class baseline, an existing
system baseline (BEETLE II system performance,
which we report only for the BEETLE portion of the
dataset), and the performance of a simple classifier
based on lexical similarity, which we report in order
to offer a substantial example of applying the same
classifier to both portions of the dataset.
3.1 BEETLE II system baseline
The interpretation component of the BEETLE II
system uses a syntactic parser and a set of hand-
authored rules to extract the domain-specific se-
mantic representations of student utterances from
the text. These representations were then matched
against the semantic representations of expected cor-
rect answers supplied by tutors. The resulting sys-
tem output was automatically mapped into our target
labels as discussed in (Dzikovska et al, 2012).
3.2 Lexical similarity baseline
To provide a higher baseline that is compara-
ble across both subsets of the data, we built
a simple decision tree classifier using the Weka
3.6.2 implementation of C4.5 pruned decision trees
(weka.classifiers.trees.J48 class), with default pa-
rameters. As features, we used lexical similar-
ity scores computed by the Text::Similarity
package with default parameters2. The code com-
putes four similarity metrics ? the raw number of
overlapping words, F1 score, Lesk score and cosine
score. We compared the learner response to the ex-
pected answer(s) and the question, resulting in eight
2http://search.cpan.org/dist/Text-Similarity/
total features (the four values indicated above for
the comparison with the question and the highest of
each value from the comparisons with each possible
expected answer).
This baseline is based on the lexical overlap base-
line used in RTE tasks (Bentivogli et al, 2009).
However, we measured overlap with the question
text in addition to the overlap with the expected
answers. Students often repeat parts of the ques-
tion in their answer and this needs to be taken
into account to differentiate, for example, ?par-
tially correct incomplete? and ?correct? answers.
4 Results
4.1 Experimental Setup
We held back part of the data set for use as standard
test data in the future challenge tasks. For BEETLE,
this consisted of all student answers to 9 out of 56
explanation questions asked by the system, plus ap-
proximately 15% of the student answers to the re-
maining 47 questions, sampling so that the distribu-
tion of labels in test data was similar to the training
data. For SCIENTSBANK, we used a previous train-
test split (Nielsen et al, 2009). For both data sets,
the data was split so that in the future we can test
how well the different systems generalize: i.e., how
well they perform on answers to questions for which
they have some sample student answers vs. how
well they perform on answers to questions that were
not in the training data (e.g., newly created questions
in a deployed system). We discuss this in more detail
in Section 5.
In this paper, we report baseline performance on
the training set to demonstrate that the task is suf-
ficiently challenging to be interesting and that sys-
tems can be compared using our evaluation met-
rics. We preserve the true test data for use in the
planned large-scale system comparisons in a com-
munity shared task.
For the lexical similarity baseline, we use 10-fold
cross-validation.3 For the BEETLE II system base-
line, the language understanding module was de-
3We did not take the student id into account explicitly during
cross-validation. While there is some risk that the classifiers
will learn features specific to the student, we concluded (based
on our understanding of data collection specifics for both data
sets) that there is little enough overlap in cross-validation on the
training data that this should not have a big effect on the results.
204
veloped based on eight transcripts, each taken from
the interaction of a different student with an earlier
version of the system. These sessions were com-
pleted prior to the beginning of the experiment dur-
ing which the BEETLE corpus was collected, and are
not included in the corpus presented here. Thus, the
dataset used in the paper constitutes unseen data for
the BEETLE II system.
We process the two corpora separately because
the additional system baseline is available for bee-
tle, and because the corpora may be different enough
that it will be helpful for shared task participants to
devise processing strategies that are sensitive to the
provenance of the data.
4.2 Evaluation Metrics
Table 1 shows the distribution of codes in the anno-
tated data. The distribution is unbalanced, and there-
fore in our evaluation results we report per-class pre-
cision, recall and F1 scores, plus the averaged scores
using two different ways to average over per-class
evaluation scores, micro- and macro- averaging.
For a set of classes C, each represented with Nc
instances in the test set, the macro-averaged recall is
defined as
Rmacro =
1
|C|
?
cC
R(c)
and the micro-averaged recall as
Rmicro =
?
cC
1
Nc
R(c)
Micro- and macro-averaged precision and F1 are de-
fined similarly.
Micro-averaging takes class sizes into account, so
a system that performs well on the most common
classes will have a high micro-average score. This is
the most commonly used classifier evaluation met-
ric. Note that, in particular, overall classification
accuracy (defined as the number of correctly clas-
sified instances out of all instances) is mathemat-
ically equivalent to micro-averaged recall (Abuda-
wood and Flach, 2011). However, macro-averaging
better reflects performance on small classes, and is
commonly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)). We report both val-
ues in our results.
BEETLE SCIENTSBANK
Label Count Freq. Count Freq.
correct 1157 0.42 2095 0.40
partially correct
incomplete
626 0.23 1431 0.27
contradictory 656 0.24 526 0.10
irrelevant 86 0.03 1175 0.22
non domain 204 0.07 24 0.005
total 2729 5251
Table 1: Distribution of annotated labels in the data
In addition, we report the system scores on the bi-
nary decision of whether or not the corrective feed-
back should be issued (denoted ?corrective feed-
back? in the results table). It assumes that a tutoring
system using a classifier will give corrective feed-
back if the classifiers returns any label other than
?correct?. Thus, every instance classified as ?par-
tially correct incomplete?, ?contradictory?, ?irrele-
vant? or ?non domain? is counted as true positive
if the hand-annotated label also belongs to this set
(even if the classifier disagrees with the annotation);
and as false positive if the hand-annotated label is
?correct?. This reflects the idea that students are
likely to be frustrated if the system gives corrective
feedback when their answer is in fact a fully accurate
paraphrase of a correct answer.
4.3 BEETLE baseline performance
The detailed evaluation results for all baselines are
presented in Table 2.
The majority class baseline is to assign ?correct?
to every test instance. It achieves 42% overall ac-
curacy. However, this is obviously at the expense
of serious errors; for example, such a system would
tell the students that they are correct if they are say-
ing something contradictory. This is reflected in a
much lower macro-averaged F1 score.
The BEETLE II system performs only slightly bet-
ter than the baseline on the overall accuracy (0.44
vs. 0.42 micro-averaged recall). However, the
macro-averaged F1 score of the BEETLE II system
is substantially higher (0.46 vs. 0.12). The micro-
averaged results show a similar pattern, although the
majority-class baseline performs slightly better than
in the macro-averaged case, as expected.
Comparing the BEETLE II parser to our lexical
205
similarity baseline, BEETLE II has lower overall ac-
curacy, but performs similarly on micro- and macro-
averaged scores. BEETLE II precision is higher than
that of the classifier in all cases except for the binary
decision as to whether corrective feedback should
be issued. This is not unexpected given how the sys-
tem was designed ? since misunderstandings caused
dialogue breakdown in pilot tests, the parser was
built to prefer rejecting utterances as uninterpretable
rather than assigning them an incorrect class, lead-
ing to a considerably lower recall. Around 31% of
utterances could not be interpreted.
Our recent analysis shows that both incorrect
interpretations (in particular, confusions between
?partially correct incomplete? and ?contradictory?)
and rejections have significant negative effects on
learning gain (Dzikovska et al, 2012). Classifiers
can be tuned to reject examples where classification
confidence falls below a given threshold, resulting
in precision-recall trade-offs. Our baseline classifier
classified all answer instances; exploring the possi-
bilities for rejecting some low-confidence answers is
planned for future work.
4.4 SCIENTSBANK baseline performance
The accuracy of the majority class baseline (which
assumes all answers are ?correct?) is 40% for SCI-
ENTSBANK, about the same as it was for BEE-
TLE. The evaluation results, based on 10-fold cross-
validation, for our simple lexical similarity classi-
fier are presented in Table 3. The lexical similar-
ity based classifier outperforms the majority class
baseline by 0.18 and 3% on the macro-averaged
F1-measure and accuracy, respectively. The F1-
measure for the two-way classification detecting an-
swers which need corrective feedback is 0.66.
The scores on SCIENTSBANK are noticeably
lower than those for BEETLE. The SCIENTSBANK
includes questions from 12 distinct science subject
areas, rather than a single area as in BEETLE. This
decision tree classifier learns a function from the
eight text similarity features to the desired answer
label. Because the features do not mention particular
words, the model can be applied to items other than
the ones on which it was trained, and even to items
from different subject areas. However, the correct
weighting of the textual similarity features depends
on the extent and nature of the expected textual over-
Predictn correct pc inc contra irrlvnt nondom
correct 1213 553 209 392 2
pc inc 432 497 128 241 2
contra 115 109 58 74 3
irrlvnt 335 272 131 468 17
nondom 0 0 0 0 0
Figure 4: Confusion matrix for lexical classifier on SCI-
ENTSBANK. Predictions in rows, gold labels in columns
lap, which does vary from subject-area to subject-
area. We suspect that the differences between sub-
ject areas made it hard for the decision-tree classi-
fier to find a single, globally appropriate strategy.
Nielsen (2009) reported the best results for classify-
ing facets when training separate question-specific
or even facet-specific classifiers. Although separate
training for each item reduces the amount of relevant
training data for each classifier, it allows each clas-
sifier to learn the specifics of how that item works.
A comparison using this style of training would be a
reasonable next step,
5 Discussion and Future Work
The results presented satisfy two critical require-
ments for a challenge task. First, we have shown that
it is feasible to develop a system that performs sig-
nificantly better than the majority class baseline. On
the macro-averaged F1-measure, our lexical clas-
sifier outperformed the majority-class baseline by
0.33 (on BEETLE) and 0.18 (on SCIENTSBANK)
and by 13% and 3% on accuracy. Second, we have
also shown, as is desired for a challenge task, that
the task is not trivial. With a system specifically
designed to parse the BEETLE corpus answers, the
macro-averaged F1-measure was just 0.46 and on
the binary decision regarding whether the response
needed corrective feedback, it achieved just 0.63.
One contribution of this work was to define a gen-
eral classification scheme for student responses that
allows more specific learner feedback. Another key
contribution was to unify two, previously incom-
patible, large student response corpora under this
common annotation scheme. The resultant corpus
will enable researchers to train learning algorithms
to classify student responses. These classifications
can then be used by a dialogue manager to generate
targeted learner feedback. The corpus is available
206
Classifier: majority lexical similarity BEETLE II
Predicted label prec. recall F1 prec. recall F1 prec. recall F1
correct 0.42 1.00 0.60 0.68 0.75 0.72 0.93 0.53 0.68
partially correct incomplete 0.00 0.00 0.00 0.41 0.38 0.39 0.43 0.53 0.47
contradictory 0.00 0.00 0.00 0.39 0.34 0.36 0.58 0.23 0.33
irrelevant 0.00 0.00 0.00 0.05 0.02 0.03 0.23 0.17 0.20
non domain 0.00 0.00 0.00 0.66 0.82 0.73 0.92 0.46 0.61
macroaverage 0.09 0.20 0.12 0.44 0.46 0.45 0.62 0.39 0.46
microaverage 0.18 0.42 0.25 0.53 0.55 0.54 0.71 0.44 0.53
corrective feedback 0.00 0.00 0.00 0.80 0.74 0.77 0.73 0.56 0.63
Table 2: Evaluation results for BEETLE corpus
Classifier: lexical similarity BEETLE II
Predicted label corrct pc inc contra irrlvnt nondom corrct pc inc contra irrlvnt nondom
correct 870 187 199 20 2 617 20 23 0 3
part corr incmp 138 239 178 24 11 249 332 146 29 20
contradictory 139 153 221 33 22 68 38 149 3 0
irrelevant 3 20 12 2 1 4 22 23 15 1
non domain 7 27 46 7 168 3 3 1 1 94
uninterpretable n/a n/a n/a n/a n/a 216 211 314 38 86
Figure 3: Confusion matrix for BEETLE corpus. Predictions in rows, gold labels in columns
Classifier: baseline lexical similarity
Predicted label prec. recall F1 prec. recall F1
correct 0.40 1.00 0.57 0.51 0.58 0.54
partially correct incomplete 0.00 0.00 0.00 0.38 0.35 0.36
contradictory 0.00 0.00 0.00 0.16 0.11 0.13
irrelevant 0.00 0.00 0.00 0.38 0.40 0.39
non domain 0.00 0.00 0.00 0.00 0.00 0.00
macroaverage 0.08 0.20 0.11 0.29 0.29 0.29
microaverage 0.16 0.40 0.23 0.41 0.43 0.42
corrective feedback 0.00 0.00 0.00 0.69 0.63 0.66
Table 3: Evaluation results for SCIENTSBANK baselines
207
for general research purposes and forms the basis
of SEMEVAL-2013 shared task ?Textual entailment
and paraphrasing for student input assessment?.4
A third contribution of this work was to provide
basic evaluation benchmark metrics and the corre-
sponding evaluation scripts (downloadable from the
site above) for other researchers, including shared
task participants. This will facilitate the comparison
and, hence, the progress, of research.
The work reported here is based on approximately
8000 student responses to questions covering 12 dis-
tinct science subjects and coming from a wide range
of student ages. These responses comprise the train-
ing data for our task. The vast majority of prior
work, including BEETLE II, which was included as
a benchmark here, has been designed to provide ITS
feedback for relatively small, well-defined domains.
The corpus presented in this paper is intended to en-
courage research into more generalizable, domain-
independent techniques. Following Nielsen (2009),
from whom the SCIENTSBANK corpus was adapted,
our shared task evaluation corpus will be composed
of three types of data: additional student responses
for all of the questions in the training data (Un-
seen Answers), student responses to questions that
were not seen in the training data, but that are from
the same subject areas (Unseen Questions), and re-
sponses to questions from three entirely different
subject areas (Unseen Domains), though in this case
the questions are still from the same general domain
? science. Unseen Answers is the typical scenario
for the vast majority of prior work ? training and
testing on responses to the same questions. Unseen
Questions and Unseen Domains allow researchers to
evaluate how well their systems generalize to near
and far domains, respectively.
The primary target application for this work is in-
telligent tutoring systems, where the classification of
responses is intended to facilitate specific pedagogic
feedback. Beneath the surface, the baseline systems
reported here are more similar to grading systems
that use the approach of (Leacock and Chodorow,
2003), which uses classifier technology to detect ex-
pressions of facet-like concepts, then converts the
result to a numerical score, than to grading systems
like (Mohler et al, 2011), which directly produces a
4See http://www.cs.york.ac.uk/semeval-2013/task4/
numerical score, using support vector regression and
similar techniques. Either approach is reasonable,
but we think that feedback is the more challeng-
ing test of a system?s ultimate abilities, and there-
fore a better candidate for the shared task. The cor-
pora from those systems, alongside with new cor-
pora currently being collected in BEETLE and SCI-
ENTSBANK domains, can serve as sources of data
for future tasks extensions.
Future systems developed for this task can benefit
from the large amount of existing work on recog-
nizing textual entailment (Giampiccolo et al, 2007;
Giampiccolo et al, 2008; Bentivogli et al, 2009)
and on detecting contradiction (Ritter et al, 2008;
De Marneffe et al, 2008). However, there are sub-
stantial challenges in applying the RTE tools directly
to this data set. Our set of labels is more fine-grained
than RTE labels to reflect the needs of intelligent tu-
toring systems (see Section 2). In addition, the top-
performing systems in RTE5 3-way task, as well as
contradiction detection methods, rely on NLP tools
such as dependency parsers and semantic role la-
belers; these do not perform well on specialized
terminology and language constructs coming from
(typed) dialogue context. We chose to use lexical
similarity as a baseline specifically because a simi-
lar measure was used as a standard baseline in RTE
tasks, and we expect that adapting the more complex
RTE approaches for purposes of this task will result
in both improved results on our data set and new de-
velopments in computational linguistics algorithms
used for RTE and related tasks.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation. The research
reported here was supported by the US ONR award
N000141010085 and by the Institute of Education
Sciences, U.S. Department of Education, through
Grant R305A110811 to Boulder Language Tech-
nologies Inc. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education.
208
References
Tarek Abudawood and Peter Flach. 2011. Learn-
ing multi-class theories in ilp. In The 20th Interna-
tional Conference on Inductive Logic Programming
(ILP?10). Springer, June.
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Vincent Aleven, Octav Popescu, and Koedinger
Koedinger. 2002. Pilot-testing a tutorial dialogue
system that supports self-explanation. Lecture Notes
in Computer Science, 2363:344?354.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Charles Callaway, Myroslava Dzikovska, Colin Mathe-
son, Johanna Moore, and Claus Zinn. 2006. Using
dialogue to learn math in the LeActiveMath project.
In Proceedings of the ECAI Workshop on Language-
Enhanced Educational Technology, pages 1?8, Au-
gust.
Gwendolyn C. Campbell, Natalie B. Steinhauser, My-
roslava O. Dzikovska, Johanna D. Moore, Charles B.
Callaway, and Elaine Farrow. 2009. The DeMAND
coding scheme: A ?common language? for represent-
ing and analyzing student discourse. In Proceedings
of 14th International Conference on Artificial Intelli-
gence in Education (AIED), poster session, Brighton,
UK, July.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):3746.
M.C. De Marneffe, A.N. Rafferty, and C.D. Manning.
2008. Finding contradictions in text. Proceedings of
ACL-08: HLT, pages 1039?1047.
Myroslava Dzikovska, Diana Bental, Johanna D. Moore,
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Elaine Farrow, and Charles B. Callaway. 2010a. In-
telligent tutoring with natural language support in the
Beetle II system. In Sustaining TEL: From Innovation
to Learning and Practice - 5th European Conference
on Technology Enhanced Learning, (EC-TEL 2010),
Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-
2010) demo session, Uppsala, Sweden, July.
Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-
hanna D. Moore. 2012. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
computational Linguistics, Avignon, France, April.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9, Prague, June.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pro-
ceedings of the AAAI Fall Symposium on Building Di-
alogue Systems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, K. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language un-
derstanding approaches in an intelligent tutoring sys-
tem. In James C. Lester, Rosa Maria Vicari, and
Fa?bio Paraguac?u, editors, Intelligent Tutoring Systems,
volume 3220 of Lecture Notes in Computer Science,
pages 346?357. Springer.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Pamela W. Jordan. 2004. Using student explanations
as models for adapting tutorial dialogue. In Valerie
Barr and Zdravko Markov, editors, FLAIRS Confer-
ence. AAAI Press.
Lawrence Hall of Science. 2006. Assessing Science
Knowledge (ask). University of California at Berke-
ley, NSF-0242510.
Lawrence Hall of Science. 2011. Full option science
system.
209
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
David D. Lewis. 1991. Evaluating text categorization. In
Proceedings of the workshop on Speech and Natural
Language, HLT ?91, pages 312?318, Stroudsburg, PA,
USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proceedings of 14th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Brighton, UK, July.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. The Journal of Natural Language Engineer-
ing, 15:479?501.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proceedings of ITS-
2004, pages 390?400.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction?no, it?s not: a case study
using functional relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 11?20.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proceed-
ings of the 15th International Conference on Artificial
Intelligence in Education (AIED-2011).
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technol-
ogy in Education, Farmington, PA, October.
210
Proceedings of the ACL 2010 Conference Short Papers, pages 43?48,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The impact of interpretation problems on tutorial dialogue
Myroslava O. Dzikovska and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{natalie.steihauser,gwendolyn.campbell}@navy.mil
Abstract
Supporting natural language input may
improve learning in intelligent tutoring
systems. However, interpretation errors
are unavoidable and require an effective
recovery policy. We describe an evaluation
of an error recovery policy in the BEE-
TLE II tutorial dialogue system and dis-
cuss how different types of interpretation
problems affect learning gain and user sat-
isfaction. In particular, the problems aris-
ing from student use of non-standard ter-
minology appear to have negative conse-
quences. We argue that existing strategies
for dealing with terminology problems are
insufficient and that improving such strate-
gies is important in future ITS research.
1 Introduction
There is a mounting body of evidence that student
self-explanation and contentful talk in human-
human tutorial dialogue are correlated with in-
creased learning gain (Chi et al, 1994; Purandare
and Litman, 2008; Litman et al, 2009). Thus,
computer tutors that understand student explana-
tions have the potential to improve student learn-
ing (Graesser et al, 1999; Jordan et al, 2006;
Aleven et al, 2001; Dzikovska et al, 2008). How-
ever, understanding and correctly assessing the
student?s contributions is a difficult problem due
to the wide range of variation observed in student
input, and especially due to students? sometimes
vague and incorrect use of domain terminology.
Many tutorial dialogue systems limit the range
of student input by asking short-answer questions.
This provides a measure of robustness, and previ-
ous evaluations of ASR in spoken tutorial dialogue
systems indicate that neither word error rate nor
concept error rate in such systems affect learning
gain (Litman and Forbes-Riley, 2005; Pon-Barry
et al, 2004). However, limiting the range of pos-
sible input limits the contentful talk that the stu-
dents are expected to produce, and therefore may
limit the overall effectiveness of the system.
Most of the existing tutoring systems that accept
unrestricted language input use classifiers based
on statistical text similarity measures to match
student answers to open-ended questions with
pre-authored anticipated answers (Graesser et al,
1999; Jordan et al, 2004; McCarthy et al, 2008).
While such systems are robust to unexpected ter-
minology, they provide only a very coarse-grained
assessment of student answers. Recent research
aims to develop methods that produce detailed
analyses of student input, including correct, in-
correct and missing parts (Nielsen et al, 2008;
Dzikovska et al, 2008), because the more detailed
assessments can help tailor tutoring to the needs of
individual students.
While the detailed assessments of answers to
open-ended questions are intended to improve po-
tential learning, they also increase the probabil-
ity of misunderstandings, which negatively impact
tutoring and therefore negatively impact student
learning (Jordan et al, 2009). Thus, appropri-
ate error recovery strategies are crucially impor-
tant for tutorial dialogue applications. We describe
an evaluation of an implemented tutorial dialogue
system which aims to accept unrestricted student
input and limit misunderstandings by rejecting low
confidence interpretations and employing a range
of error recovery strategies depending on the cause
of interpretation failure.
By comparing two different system policies, we
demonstrate that with less restricted language in-
put the rate of non-understanding errors impacts
both learning gain and user satisfaction, and that
problems arising from incorrect use of terminol-
ogy have a particularly negative impact. A more
detailed analysis of the results indicates that, even
though we based our policy on an approach ef-
43
fective in task-oriented dialogue (Hockey et al,
2003), many of our strategies were not success-
ful in improving learning gain. At the same time,
students appear to be aware that the system does
not fully understand them even if it accepts their
input without indicating that it is having interpre-
tation problems, and this is reflected in decreased
user satisfaction. We argue that this indicates that
we need better strategies for dealing with termi-
nology problems, and that accepting non-standard
terminology without explicitly addressing the dif-
ference in acceptable phrasing may not be suffi-
cient for effective tutoring.
In Section 2 we describe our tutoring system,
and the two tutoring policies implemented for the
experiment. In Section 3 we present experimen-
tal results and an analysis of correlations between
different types of interpretation problems, learning
gain and user satisfaction. Finally, in Section 4 we
discuss the implications of our results for error re-
covery policies in tutorial dialogue systems.
2 Tutorial Dialogue System and Error
Recovery Policies
This work is based on evaluation of BEETLE II
(Dzikovska et al, 2010), a tutorial dialogue sys-
tem which provides tutoring in basic electricity
and electronics. Students read pre-authored mate-
rials, experiment with a circuit simulator, and then
are asked to explain their observations. BEETLE II
uses a deep parser together with a domain-specific
diagnoser to process student input, and a deep gen-
erator to produce tutorial feedback automatically
depending on the current tutorial policy. It also
implements an error recovery policy to deal with
interpretation problems.
Students currently communicate with the sys-
tem via a typed chat interface. While typing
removes the uncertainty and errors involved in
speech recognition, expected student answers are
considerably more complex and varied than in
a typical spoken dialogue system. Therefore, a
significant number of interpretation errors arise,
primarily during the semantic interpretation pro-
cess. These errors can lead to non-understandings,
when the system cannot produce a syntactic parse
(or a reasonable fragmentary parse), or when it
does not know how to interpret an out-of-domain
word; and misunderstandings, where a system ar-
rives at an incorrect interpretation, due to either
an incorrect attachment in the parse, an incorrect
word sense assigned to an ambiguous word, or an
incorrectly resolved referential expression.
Our approach to selecting an error recovery pol-
icy is to prefer non-understandings to misunder-
standings. There is a known trade-off in spoken di-
alogue systems between allowing misunderstand-
ings, i.e., cases in which a system accepts and
acts on an incorrect interpretation of an utterance,
and non-understandings, i.e., cases in which a sys-
tem rejects an utterance as uninterpretable (Bo-
hus and Rudnicky, 2005). Since misunderstand-
ings on the part of a computer tutor are known
to negatively impact student learning, and since
in human-human tutorial dialogue the majority of
student responses using unexpected terminology
are classified as incorrect (Jordan et al, 2009),
it would be a reasonable approach for a tutorial
dialogue system to deal with potential interpreta-
tion problems by treating low-confidence interpre-
tations as non-understandings and focusing on an
effective non-understanding recovery policy.1
We implemented two different policies for com-
parison. Our baseline policy does not attempt any
remediation or error recovery. All student utter-
ances are passed through the standard interpreta-
tion pipeline, so that the results can be analyzed
later. However, the system does not attempt to ad-
dress the student content. Instead, regardless of
the answer analysis, the system always uses a neu-
tral acceptance and bottom out strategy, giving the
student the correct answer every time, e.g., ?OK.
One way to phrase the correct answer is: the open
switch creates a gap in the circuit?. Thus, the stu-
dents are never given any indication of whether
they have been understood or not.
The full policy acts differently depending on the
analysis of the student answer. For correct an-
swers, it acknowledges the answer as correct and
optionally restates it (see (Dzikovska et al, 2008)
for details). For incorrect answers, it restates the
correct portion of the answer (if any) and provides
a hint to guide the student towards the completely
correct answer. If the student?s utterance cannot be
interpreted, the system responds with a help mes-
sage indicating the cause of the problem together
with a hint. In both cases, after 3 unsuccessful at-
tempts to address the problem the system uses the
bottom out strategy and gives away the answer.
1While there is no confidence score from a speech recog-
nizer, our system uses a combination of a parse quality score
assigned by the parser and a set of consistency checks to de-
termine whether an interpretation is sufficiently reliable.
44
The content of the bottom out is the same as in
the baseline, except that the full system indicates
clearly that the answer was incorrect or was not
understood, e.g., ?Not quite. Here is the answer:
the open switch creates a gap in the circuit?.
The help messages are based on the Targeted-
Help approach successfully used in spoken dia-
logue (Hockey et al, 2003), together with the error
classification we developed for tutorial dialogue
(Dzikovska et al, 2009). There are 9 different er-
ror types, each associated with a different targeted
help message. The goal of the help messages is to
give the student as much information as possible
as to why the system failed to understand them but
without giving away the answer.
In comparing the two policies, we would expect
that the students in both conditions would learn
something, but that the learning gain and user sat-
isfaction would be affected by the difference in
policies. We hypothesized that students who re-
ceive feedback on their errors in the full condition
would learn more compared to those in the base-
line condition.
3 Evaluation
We collected data from 76 subjects interacting
with the system. The subjects were randomly as-
signed to either the baseline (BASE) or the full
(FULL) policy condition. Each subject took a pre-
test, then worked through a lesson with the system,
and then took a post-test and filled in a user satis-
faction survey. Each session lasted approximately
4 hours, with 232 student language turns in FULL
(SD = 25.6) and 156 in BASE (SD = 2.02). Ad-
ditional time was taken by reading and interact-
ing with the simulation environment. The students
had little prior knowledge of the domain. The sur-
vey consisted of 63 questions on the 5-point Lik-
ert scale covering the lesson content, the graphical
user interface, and tutor?s understanding and feed-
back. For purposes of this study, we are using an
averaged tutor score.
The average learning gain was 0.57 (SD =
0.23) in FULL, and 0.63 (SD = 0.26) in BASE.
There was no significant difference in learning
gain between conditions. Students liked BASE bet-
ter: the average tutor evaluation score for FULL
was 2.56 out of 5 (SD = 0.65), compared to 3.32
(SD = 0.65) in BASE. These results are signif-
icantly different (t-test, p < 0.05). In informal
comments after the session many students said that
they were frustrated when the system said that it
did not understand them. However, some students
in BASE also mentioned that they sometimes were
not sure if the system?s answer was correcting a
problem with their answer, or simply phrasing it
in a different way.
We used mean frequency of non-interpretable
utterances (out of all student utterances in
each session) to evaluate the effectiveness of
the two different policies. On average, 14%
of utterances in both conditions resulted in
non-understandings.2 The frequency of non-
understandings was negatively correlated with
learning gain in FULL: r = ?0.47, p < 0.005,
but not significantly correlated with learning gain
in BASE: r = ?0.09, p = 0.59. However, in both
conditions the frequency of non-understandings
was negatively correlated with user satisfaction:
FULL r = ?0.36, p = 0.03, BASE r = ?0.4, p =
0.01. Thus, even though in BASE the system
did not indicate non-understanding, students were
negatively affected. That is, they were not satis-
fied with the policy that did not directly address
the interpretation problems. We discuss possible
reasons for this below.
We investigated the effect of different types of
interpretation errors using two criteria. First, we
checked whether the mean frequency of errors was
reduced between BASE and FULL for each individ-
ual strategy. The reduced frequency means that
the recovery strategy for this particular error type
is effective in reducing the error frequency. Sec-
ond, we looked for the cases where the frequency
of a given error type is negatively correlated with
either learning gain or user satisfaction. This is
provides evidence that such errors are negatively
impacting the learning process, and therefore im-
proving recovery strategies for those error types is
likely to improve overall system effectiveness,
The results, shown in Table 1, indicate that the
majority of interpretation problems are not sig-
nificantly correlated with learning gain. How-
ever, several types of problems appear to be
particularly significant, and are all related to
improper use of domain terminology. These
were irrelevant answer, no appr terms, selec-
tional restriction failure and program error.
An irrelevant answer error occurs when the stu-
dent makes a statement that uses domain termi-
2We do not know the percentage of misunderstandings or
concept error rate as yet. We are currently annotating the data
with the goal to evaluate interpretation correctness.
45
full baseline
error type
mean freq.
(std. dev)
satisfac-
tion r
gain
r
mean freq
(std. dev)
satisfac-
tion r
gain
r
irrelevant answer 0.008 (0.01) -0.08 -0.19 0.012 (0.01) -0.07 -0.47**
no appr terms 0.005 (0.01) -0.57** -0.42** 0.003 (0.01) -0.38** -0.01
selectional restr failure 0.032 (0.02) -0.12 -0.55** 0.040 (0.03) 0.13 0.26*
program error 0.002 (0.003) 0.02 0.26 0.003 (0.003) 0 -0.35**
unknown word 0.023 (0.01) 0.05 -0.21 0.024 (0.02) -0.15 -0.09
disambiguation failure 0.013 (0.01) -0.04 0.02 0.007 (0.01) -0.18 0.19
no parse 0.019 (0.01) -0.14 -0.08 0.022(0.02) -0.3* 0.01
partial interpretation 0.004 (0.004) -0.11 -0.01 0.004 (0.005) -0.19 0.22
reference failure 0.012 (0.02) -0.31* -0.09 0.017 (0.01) -0.15 -0.23
Overall 0.134 (0.05) -0.36** -0.47** 0.139 (0.04) -0.4** -0.09
Table 1: Correlations between frequency of different error types and student learning gain and satisfac-
tion. ** - correlation is significant with p < 0.05, * - with p <= 0.1.
nology but does not appear to answer the system?s
question directly. For example, the expected an-
swer to ?In circuit 1, which components are in a
closed path?? is ?the bulb?. Some students mis-
read the question and say ?Circuit 1 is closed.? If
that happens, in FULL the system says ?Sorry, this
isn?t the form of answer that I expected. I am look-
ing for a component?, pointing out to the student
the kind of information it is looking for. The BASE
system for this error, and for all other errors dis-
cussed below, gives away the correct answer with-
out indicating that there was a problem with in-
terpreting the student?s utterance, e.g., ?OK, the
correct answer is the bulb.?
The no appr terms error happens when the stu-
dent is using terminology inappropriate for the les-
son in general. Students are expected to learn to
explain everything in terms of connections and ter-
minal states. For example, the expected answer to
?What is voltage?? is ?the difference in states be-
tween two terminals?. If instead the student says
?Voltage is electricity?, FULL responds with ?I am
sorry, I am having trouble understanding. I see no
domain concepts in your answer. Here?s a hint:
your answer should mention a terminal.? The mo-
tivation behind this strategy is that in general, it is
very difficult to reason about vaguely used domain
terminology. We had hoped that by telling the stu-
dent that the content of their utterance is outside
the domain as understood by the system, and hint-
ing at the correct terms to use, the system would
guide students towards a better answer.
Selectional restr failure errors are typically due
to incorrect terminology, when the students
phrased answers in a way that contradicted the sys-
tem?s domain knowledge. For example, the sys-
tem can reason about damaged bulbs and batter-
ies, and open and closed paths. So if the stu-
dent says ?The path is damaged?, the FULL sys-
tem would respond with ?I am sorry, I am having
trouble understanding. Paths cannot be damaged.
Only bulbs and batteries can be damaged.?
Program error were caused by faults in the un-
derlying network software, but usually occurred
when the student was using extremely long and
complicated utterances.
Out of the four important error types described
above, only the strategy for irrelevant answer was
effective: the frequency of irrelevant answer er-
rors is significantly higher in BASE (t-test, p <
0.05), and it is negatively correlated with learning
gain in BASE. The frequencies of other error types
did not significantly differ between conditions.
However, one other finding is particularly in-
teresting: the frequency of no appr terms errors
is negatively correlated with user satisfaction in
BASE. This indicates that simply accepting the stu-
dent?s answer when they are using incorrect termi-
nology and exposing them to the correct answer is
not the best strategy, possibly because the students
are noticing the unexplained lack of alignment be-
tween their utterance and the system?s answer.
4 Discussion and Future Work
As discussed in Section 1, previous studies of
short-answer tutorial dialogue systems produced a
counter-intuitive result: measures of interpretation
accuracy were not correlated with learning gain.
With less restricted language, misunderstandings
46
negatively affected learning. Our study provides
further evidence that interpretation quality signif-
icantly affects learning gain in tutorial dialogue.
Moreover, while it has long been known that user
satisfaction is negatively correlated with interpre-
tation error rates in spoken dialogue, this is the
first attempt to evaluate the impact of different
types of interpretation errors on task success and
usability of a tutoring system.
Our results demonstrate that different types of
errors may matter to a different degree. In our
system, all of the error types negatively correlated
with learning gain stem from the same underlying
problem: the use of incorrect or vague terminol-
ogy by the student. With the exception of the ir-
relevant answer strategy, the targeted help strate-
gies we implemented were not effective in reduc-
ing error frequency or improving learning gain.
Additional research is needed to understand why.
One possibility is that irrelevant answer was eas-
ier to remediate compared to other error types. It
usually happened in situations where there was a
clear expectation of the answer type (e.g., a list of
component names, a yes/no answer). Therefore,
it was easier to design an effective prompt. Help
messages for other error types were more frequent
when the expected answer was a complex sen-
tence, and multiple possible ways of phrasing the
correct answer were acceptable. Therefore, it was
more difficult to formulate a prompt that would
clearly describe the problem in all contexts.
One way to improve the help messages may be
to have the system indicate more clearly when user
terminology is a problem. Our system apologized
each time there was a non-understanding, leading
students to believe that they may be answering cor-
rectly but the answer is not being understood. A
different approach would be to say something like
?I am sorry, you are not using the correct termi-
nology in your answer. Here?s a hint: your answer
should mention a terminal?. Together with an ap-
propriate mechanism to detect paraphrases of cor-
rect answers (as opposed to vague answers whose
correctness is difficult to determine), this approach
could be more beneficial in helping students learn.
We are considering implementing and evaluating
this as part of our future work.
Some of the errors, in particular instances of
no appr terms and selectional restr failure, also
stemmed from unrecognized paraphrases with
non-standard terminology. Those answers could
conceivably be accepted by a system using seman-
tic similarity as a metric (e.g., using LSA with pre-
authored answers). However, our results also indi-
cate that simply accepting the incorrect terminol-
ogy may not be the best strategy. Users appear to
be sensitive when the system?s language does not
align with their terminology, as reflected in the de-
creased satisfaction ratings associated with higher
rates of incorrect terminology problems in BASE.
Moreover, prior analysis of human-human data
indicates that tutors use different restate strate-
gies depending on the ?quality? of the student an-
swers, even if they are accepting them as correct
(Dzikovska et al, 2008). Together, these point at
an important unaddressed issue: existing systems
are often built on the assumption that only incor-
rect and missing parts of the student answer should
be remediated, and a wide range of terminology
should be accepted (Graesser et al, 1999; Jordan
et al, 2006). While it is obviously important for
the system to accept a range of different phrasings,
our analysis indicates that this may not be suffi-
cient by itself, and students could potentially ben-
efit from addressing the terminology issues with a
specifically devised strategy.
Finally, it could also be possible that some
differences between strategy effectiveness were
caused by incorrect error type classification. Man-
ual examination of several dialogues suggests that
most of the errors are assigned to the appropri-
ate type, though in some cases incorrect syntac-
tic parses resulted in unexpected interpretation er-
rors, causing the system to give a confusing help
message. These misclassifications appear to be
evenly split between different error types, though
a more formal evaluation is planned in the fu-
ture. However from our initial examination, we
believe that the differences in strategy effective-
ness that we observed are due to the actual differ-
ences in the help messages. Therefore, designing
better prompts would be the key factor in improv-
ing learning and user satisfaction.
Acknowledgments
This work has been supported in part by US Office
of Naval Research grants N000140810043 and
N0001410WX20278. We thank Katherine Harri-
son, Leanne Taylor, Charles Callaway, and Elaine
Farrow for help with setting up the system and
running the evaluation. We would like to thank
anonymous reviewers for their detailed feedback.
47
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Dan Bohus and Alexander Rudnicky. 2005. Sorry,
I didn?t catch that! - An investigation of non-
understanding errors and recovery strategies. In
Proceedings of SIGdial-2005, Lisbon, Portugal.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting
self-explanations improves understanding. Cogni-
tive Science, 18(3):439?477.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008. Diagnosing natural lan-
guage answers to support adaptive tutoring. In
Proceedings 21st International FLAIRS Conference,
Coconut Grove, Florida, May.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn C. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of SIGDIAL-09, London, UK, Sep.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a sys-
tem for tutoring and computational linguistics ex-
perimentation. In Proceedings of ACL-2010 demo
session.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simula-
tion of a human tutor. Cognitive Systems Research,
1:35?51.
Beth Ann Hockey, Oliver Lemon, Ellen Campana,
Laura Hiatt, Gregory Aist, James Hieronymus,
Alexander Gruenstein, and John Dowding. 2003.
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics, pages 147?154, Morristown, NJ, USA.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring sys-
tem. In James C. Lester, Rosa Maria Vicari, and
Fa?bio Paraguac?u, editors, Intelligent Tutoring Sys-
tems, volume 3220 of Lecture Notes in Computer
Science, pages 346?357. Springer.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Pamela Jordan, Diane Litman, Michael Lipschultz, and
Joanna Drummond. 2009. Evidence of misunder-
standings in tutorial dialogue and their impact on
learning. In Proceedings of the 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Diane Litman and Kate Forbes-Riley. 2005. Speech
recognition performance and learning in spoken di-
alogue tutoring. In Proceedings of EUROSPEECH-
2005, page 1427.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Generalizing tutorial dia-
logue results. In Proceedings of 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Philip M. McCarthy, Vasile Rus, Scott Crossley,
Arthur C. Graesser, and Danielle S. McNamara.
2008. Assessing forward-, reverse-, and average-
entailment indeces on natural language input from
the intelligent tutoring system, iSTART. In Proceed-
ings of the 21st International FLAIRS conference,
pages 165?170.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Heather Pon-Barry, Brady Clark, Elizabeth Owen
Bratt, Karl Schultz, and Stanley Peters. 2004. Eval-
uating the effectiveness of SCoT: A spoken conver-
sational tutor. In J. Mostow and P. Tedesco, editors,
Proceedings of the ITS 2004 Workshop on Dialog-
based Intelligent Tutoring Systems, pages 23?32.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
48
Proceedings of the ACL 2010 System Demonstrations, pages 13?18,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
BEETLE II: a system for tutoring and computational linguistics
experimentation
Myroslava O. Dzikovska and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{gwendolyn.campbell,natalie.steihauser}@navy.mil
Elaine Farrow
Heriot-Watt University
Edinburgh, United Kingdom
e.farrow@hw.ac.uk
Charles B. Callaway
University of Haifa
Mount Carmel, Haifa, Israel
ccallawa@gmail.com
Abstract
We present BEETLE II, a tutorial dia-
logue system designed to accept unre-
stricted language input and support exper-
imentation with different tutorial planning
and dialogue strategies. Our first system
evaluation used two different tutorial poli-
cies and demonstrated that the system can
be successfully used to study the impact
of different approaches to tutoring. In the
future, the system can also be used to ex-
periment with a variety of natural language
interpretation and generation techniques.
1 Introduction
Over the last decade there has been a lot of inter-
est in developing tutorial dialogue systems that un-
derstand student explanations (Jordan et al, 2006;
Graesser et al, 1999; Aleven et al, 2001; Buckley
and Wolska, 2007; Nielsen et al, 2008; VanLehn
et al, 2007), because high percentages of self-
explanation and student contentful talk are known
to be correlated with better learning in human-
human tutoring (Chi et al, 1994; Litman et al,
2009; Purandare and Litman, 2008; Steinhauser et
al., 2007). However, most existing systems use
pre-authored tutor responses for addressing stu-
dent errors. The advantage of this approach is that
tutors can devise remediation dialogues that are
highly tailored to specific misconceptions many
students share, providing step-by-step scaffolding
and potentially suggesting additional problems.
The disadvantage is a lack of adaptivity and gen-
erality: students often get the same remediation
for the same error regardless of their past perfor-
mance or dialogue context, as it is infeasible to
author a different remediation dialogue for every
possible dialogue state. It also becomes more dif-
ficult to experiment with different tutorial policies
within the system due to the inherent completixites
in applying tutoring strategies consistently across
a large number of individual hand-authored reme-
diations.
The BEETLE II system architecture is designed
to overcome these limitations (Callaway et al,
2007). It uses a deep parser and generator, to-
gether with a domain reasoner and a diagnoser,
to produce detailed analyses of student utterances
and generate feedback automatically. This allows
the system to consistently apply the same tutorial
policy across a range of questions. To some extent,
this comes at the expense of being able to address
individual student misconceptions. However, the
system?s modular setup and extensibility make it
a suitable testbed for both computational linguis-
tics algorithms and more general questions about
theories of learning.
A distinguishing feature of the system is that it
is based on an introductory electricity and elec-
tronics course developed by experienced instruc-
tional designers. The course was first created for
use in a human-human tutoring study, without tak-
ing into account possible limitations of computer
tutoring. The exercises were then transferred into
a computer system with only minor adjustments
(e.g., breaking down compound questions into in-
dividual questions). This resulted in a realistic tu-
toring setup, which presents interesting challenges
to language processing components, involving a
wide variety of language phenomena.
We demonstrate a version of the system that
has undergone a successful user evaluation in
13
2009. The evaluation results indicate that addi-
tional improvements to remediation strategies, and
especially to strategies dealing with interpretation
problems, are necessary for effective tutoring. At
the same time, the successful large-scale evalua-
tion shows that BEETLE II can be used as a plat-
form for future experimentation.
The rest of this paper discusses the BEETLE II
system architecture (Section 2), system evaluation
(Section 3), and the range of computational lin-
guistics problems that can be investigated using
BEETLE II (Section 4).
2 System Architecture
The BEETLE II system delivers basic electricity
and electronics tutoring to students with no prior
knowledge of the subject. A screenshot of the sys-
tem is shown in Figure 1. The student interface in-
cludes an area to display reading material, a circuit
simulator, and a dialogue history window. All in-
teractions with the system are typed. Students read
pre-authored curriculum slides and carry out exer-
cises which involve experimenting with the circuit
simulator and explaining the observed behavior.
The system also asks some high-level questions,
such as ?What is voltage??.
The system architecture is shown in Figure 2.
The system uses a standard interpretation pipeline,
with domain-independent parsing and generation
components supported by domain specific reason-
ers for decision making. The architecture is dis-
cussed in detail in the rest of this section.
2.1 Interpretation Components
We use the TRIPS dialogue parser (Allen et al,
2007) to parse the utterances. The parser provides
a domain-independent semantic representation in-
cluding high-level word senses and semantic role
labels. The contextual interpreter then uses a refer-
ence resolution approach similar to Byron (2002),
and an ontology mapping mechanism (Dzikovska
et al, 2008a) to produce a domain-specific seman-
tic representation of the student?s output. Utter-
ance content is represented as a set of extracted
objects and relations between them. Negation is
supported, together with a heuristic scoping algo-
rithm. The interpreter also performs basic ellipsis
resolution. For example, it can determine that in
the answer to the question ?Which bulbs will be
on and which bulbs will be off in this diagram??,
?off? can be taken to mean ?all bulbs in the di-
agram will be off.? The resulting output is then
passed on to the domain reasoning and diagnosis
components.
2.2 Domain Reasoning and Diagnosis
The system uses a knowledge base implemented in
the KM representation language (Clark and Porter,
1999; Dzikovska et al, 2006) to represent the state
of the world. At present, the knowledge base rep-
resents 14 object types and supports the curricu-
lum containing over 200 questions and 40 differ-
ent circuits.
Student explanations are checked on two levels,
verifying factual and explanation correctness. For
example, for a question ?Why is bulb A lit??, if
the student says ?it is in a closed path?, the system
checks two things: a) is the bulb indeed in a closed
path? and b) is being in a closed path a reason-
able explanation for the bulb being lit? Different
remediation strategies need to be used depending
on whether the student made a factual error (i.e.,
they misread the diagram and the bulb is not in a
closed path) or produced an incorrect explanation
(i.e., the bulb is indeed in a closed path, but they
failed to mention that a battery needs to be in the
same closed path for the bulb to light).
The knowledge base is used to check the fac-
tual correctness of the answers first, and then a di-
agnoser checks the explanation correctness. The
diagnoser, based on Dzikovska et al (2008b), out-
puts a diagnosis which consists of lists of correct,
contradictory and non-mentioned objects and re-
lations from the student?s answer. At present, the
system uses a heuristic matching algorithm to clas-
sify relations into the appropriate category, though
in the future we may consider a classifier similar
to Nielsen et al (2008).
2.3 Tutorial Planner
The tutorial planner implements a set of generic
tutoring strategies, as well as a policy to choose
an appropriate strategy at each point of the inter-
action. It is designed so that different policies can
be defined for the system. The currently imple-
mented strategies are: acknowledging the correct
part of the answer; suggesting a slide to read with
background material; prompting for missing parts
of the answer; hinting (low- and high- specificity);
and giving away the answer. Two or more strate-
gies can be used together if necessary.
The hint selection mechanism generates hints
automatically. For a low specificity hint it selects
14
Figure 1: Screenshot of the BEETLE II system
Dialogue ManagerParserContextualInterpreter
Interpretation
CurriculumPlanner
KnowledgeBase
Content Planner & Generator
TutorialPlanner
Tutoring
GUI
Diagnoser
Figure 2: System architecture diagram
15
an as-yet unmentioned object and hints at it, for
example, ?Here?s a hint: Your answer should men-
tion a battery.? For high-specificity, it attempts to
hint at a two-place relation, for example, ?Here?s
a hint: the battery is connected to something.?
The tutorial policy makes a high-level decision
as to which strategy to use (for example, ?ac-
knowledge the correct part and give a high speci-
ficity hint?) based on the answer analysis and di-
alogue context. At present, the system takes into
consideration the number of incorrect answers re-
ceived in response to the current question and the
number of uninterpretable answers.1
In addition to a remediation policy, the tuto-
rial planner implements an error recovery policy
(Dzikovska et al, 2009). Since the system ac-
cepts unrestricted input, interpretation errors are
unavoidable. Our recovery policy is modeled on
the TargetedHelp (Hockey et al, 2003) policy used
in task-oriented dialogue. If the system cannot
find an interpretation for an utterance, it attempts
to produce a message that describes the problem
but without giving away the answer, for example,
?I?m sorry, I?m having a problem understanding. I
don?t know the word power.? The help message is
accompanied with a hint at the appropriate level,
also depending on the number of previous incor-
rect and non-interpretable answers.
2.4 Generation
The strategy decision made by the tutorial plan-
ner, together with relevant semantic content from
the student?s answer (e.g., part of the answer to
confirm), is passed to content planning and gen-
eration. The system uses a domain-specific con-
tent planner to produce input to the surface realizer
based on the strategy decision, and a FUF/SURGE
(Elhadad and Robin, 1992) generation system to
produce the appropriate text. Templates are used
to generate some stock phrases such as ?When you
are ready, go on to the next slide.?
2.5 Dialogue Management
Interaction between components is coordinated by
the dialogue manager which uses the information-
state approach (Larsson and Traum, 2000). The
dialogue state is represented by a cumulative an-
swer analysis which tracks, over multiple turns,
the correct, incorrect, and not-yet-mentioned parts
1Other factors such as student confidence could be con-
sidered as well (Callaway et al, 2007).
of the answer. Once the complete answer has been
accumulated, the system accepts it and moves on.
Tutor hints can contribute parts of the answer to
the cumulative state as well, allowing the system
to jointly construct the solution with the student.
3 Evaluation
The first experimental evaluation involving 81 par-
ticipants (undergraduates recruited from a South-
eastern University in the USA) was completed in
2009. Participants had little or no prior knowledge
of the domain. Each participant took a pre-test,
worked through a lesson with the system, took a
post-test, and completed a user satisfaction survey.
Each session lasted approximately 4 hours.
We implemented two different tutoring policies
in the system for this evaluation. The baseline
policy used an ?accept and bottom out? strategy
for all student answers, regardless of their con-
tent. The students were always given the correct
answer, but the system made no attempt at reme-
diation, and never indicated whether the student
was understood (or correct). In comparison, the
full adaptive policy attempted to select an appro-
priate strategy based on student answer analysis
and dialogue context, as described above. Stu-
dents were randomly assigned to a condition. On
average, each student session contained 232 stu-
dent language turns in the fully adaptive condition
(SD = 25.6) and 156 turns in the baseline condi-
tion (SD = 2.02). Additional time was taken by
reading and interacting with the simulation envi-
ronment.
Out of 81 students, 76 successfully completed
the evaluation (data from 5 participants had to be
discarded due to system crashes). There was no
significant difference in learning gain between the
two conditions (baseline: M = 0.63, SD = 0.26,
full: M = 0.57, SD = 0.23, t-test p = 0.32),
and students gave higher satisfaction scores to the
baseline condition (baseline: M = 3.32, SD =
0.65, full: M = 2.56, SD = 0.65, t-test, p <
0.01). We are currently analyzing the data to in-
vestigate the reasons for this outcome. However,
the evaluation demonstrated that the system can be
successfully deployed in experiments with naive
users and therefore can be used to study the im-
pact of different natural language processing algo-
rithms on learning gain and user satisfaction.
An example dialogue with the system, taken
from the evaluation corpus, is shown in Figure 3.
16
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I?m sorry, I?m having a problem un-
derstanding. I don?t understand the
word ?effect? here. Here?s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
Tutor: You?re on the right track. Here?s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 3: Example interaction with the system
from our corpus
It shows three key system properties: after the stu-
dent?s first turn, the system rephrases its under-
standing of the correct part of the student answer
and prompts the student to supply the missing in-
formation. In the second turn, the student utter-
ance could not be interpreted and the system re-
sponds with a targeted help message and a hint
about the object that needs to be mentioned. Fi-
nally, in the last turn the system combines the in-
formation from the tutor?s hint and the student?s
answers and restates the complete answer since the
current answer was completed over multiple turns.
4 Conclusions and Future Work
The BEETLE II system we present was built to
serve as a platform for research in computational
linguistics and tutoring, and can be used for task-
based evaluation of algorithms developed for other
domains. We are currently developing an annota-
tion scheme for the data we collected to identify
student paraphrases of correct answers. The an-
notated data will be used to evaluate the accuracy
of existing paraphrasing and textual entailment ap-
proaches and to investigate how to combine such
algorithms with the current deep linguistic analy-
sis to improve system robustness. We also plan
to annotate the data we collected for evidence of
misunderstandings, i.e., situations where the sys-
tem arrived at an incorrect interpretation of a stu-
dent utterance and took action on it. Such annota-
tion can provide useful input for statistical learn-
ing algorithms to detect and recover from misun-
derstandings.
In dialogue management and generation, the
key issue we are planning to investigate is that of
linguistic alignment. The analysis of the data we
have collected indicates that student satisfaction
may be affected if the system rephrases student
answers using different words (for example, using
better terminology) but doesn?t explicitly explain
the reason why different terminology is needed
(Dzikovska et al, 2010). Results from other sys-
tems show that measures of semantic coherence
between a student and a system were positively as-
sociated with higher learning gain (Ward and Lit-
man, 2006). Using a deep generator to automati-
cally generate system feedback gives us a level of
control over the output and will allow us to devise
experiments to study those issues in more detail.
From the point of view of tutoring research,
we are planning to use the system to answer
questions about the effectiveness of different ap-
proaches to tutoring, and the differences between
human-human and human-computer tutoring. Pre-
vious comparisons of human-human and human-
computer dialogue were limited to systems that
asked short-answer questions (Litman et al, 2006;
Rose? and Torrey, 2005). Having a system that al-
lows more unrestricted language input will pro-
vide a more balanced comparison. We are also
planning experiments that will allow us to eval-
uate the effectiveness of individual strategies im-
plemented in the system by comparing system ver-
sions using different tutoring policies.
Acknowledgments
This work has been supported in part by US Office
of Naval Research grants N000140810043 and
N0001410WX20278. We thank Katherine Harri-
son and Leanne Taylor for their help running the
evaluation.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
James Allen, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL-07 Workshop on Deep Linguistic Processing.
17
Mark Buckley and Magdalena Wolska. 2007. To-
wards modelling and using common ground in tu-
torial dialogue. In Proceedings of DECALOG, the
2007 Workshop on the Semantics and Pragmatics of
Dialogue, pages 41?48.
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway, Myroslava Dzikovska, Elaine
Farrow, Manuel Marques-Pita, Colin Matheson, and
Johanna D. Moore. 2007. The Beetle and BeeD-
iff tutoring systems. In Proceedings of SLaTE?07
(Speech and Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting
self-explanations improves understanding. Cogni-
tive Science, 18(3):439?477.
Peter Clark and Bruce Porter, 1999. KM (1.4): Users
Manual. http://www.cs.utexas.edu/users/mfkb/km.
Myroslava O. Dzikovska, Charles B. Callaway, and
Elaine Farrow. 2006. Interpretation and generation
in a knowledge-based tutorial system. In Proceed-
ings of EACL-06 workshop on knowledge and rea-
soning for language processing, Trento, Italy, April.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2008a. Linking semantic and knowledge
representations in a multi-domain dialogue system.
Journal of Logic and Computation, 18(3):405?430.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008b. Diagnosing natural lan-
guage answers to support adaptive tutoring. In
Proceedings 21st International FLAIRS Conference,
Coconut Grove, Florida, May.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn C. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of SIGDIAL-09, London, UK, Sep.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010. The
impact of interpretation problems on tutorial dia-
logue. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics(ACL-
2010).
Michael Elhadad and Jacques Robin. 1992. Control-
ling content realization with functional unification
grammars. In R. Dale, E. Hovy, D. Ro?sner, and
O. Stock, editors, Proceedings of the Sixth Interna-
tional Workshop on Natural Language Generation,
pages 89?104, Berlin, April. Springer-Verlag.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simula-
tion of a human tutor. Cognitive Systems Research,
1:35?51.
Beth Ann Hockey, Oliver Lemon, Ellen Campana,
Laura Hiatt, Gregory Aist, James Hieronymus,
Alexander Gruenstein, and John Dowding. 2003.
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics, pages 147?154, Morristown, NJ, USA.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323?340.
Diane Litman, Carolyn P. Rose?, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Sil-
liman. 2006. Spoken versus typed human and com-
puter dialogue tutoring. International Journal of Ar-
tificial Intelligence in Education, 16:145?170.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Generalizing tutorial dia-
logue results. In Proceedings of 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
C.P. Rose? and C. Torrey. 2005. Interactivity versus ex-
pectation: Eliciting learning oriented behavior with
tutorial dialogue systems. In Proceedings of Inter-
act?05.
N. B. Steinhauser, L. A. Butler, and G. E. Campbell.
2007. Simulated tutors in immersive learning envi-
ronments: Empirically-derived design principles. In
Proceedings of the 2007 Interservice/Industry Train-
ing, Simulation and Education Conference, Orlando,
FL.
Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007. Developing pedagogically effective tutorial
dialogue tactics: Experiments and a testbed. In Pro-
ceedings of SLaTE Workshop on Speech and Lan-
guage Technology in Education, Farmington, PA,
October.
Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of 19th International FLAIRS (Florida Ar-
tificial Intelligence Research Society) Conference,
Melbourne Beach, FL.
18
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
gency dispatcher, cooperating with the system to 
dynamically allocate resources to and make 
plans for solving problems as they arise in the 
world. The setting, Monroe County, NY, is con- 
siderably more complex than our previous do- 
mains (e.g. Pacifica, TRAINS), and raises new 
issues in knowledge representation a d refer- 
ence. Emergencies include requests for medical 
assistance, car accidents, civil disorder, and 
larger problems such as flooding and snow 
storms. Resources at the user's disposal may 
include road crews, electric crews, ambulances, 
police units and helicopters. Some of the in- 
crease in mixed-initiative interaction comes 
from givi-n~ the_ system more knowledge of the 
tasks being solved. Some comes from the fact 
that the solution to one problem may conflict 
with the solution to another, either because of 
scheduling conflicts, scarce resources, or aspects 
of the physical world (e.g. an ambulance can't go 
down a road that has not been plowed). The 
range of tasks and complexity of the world allow 
for problem solving at different levels of granu- 
larity, making it possible for the system to take 
as much control over the task as the user per- 
mits. 
4. Important  Contr ibut ions  
While a number of robust dialogue systems have 
been built in recent years, they mostly have op- 
erated in domains that require little if any rea- 
soning. Rather, the task is hard-coded into the 
system operation. One of the major goals of the 
TRIPS project has been to develop dialogue 
models and system architectures that support 
conversational interaction in domains where 
complex reasoning systems are required. One 
goal has been to build a fairly generic model in 
which different domains can then be specified 
fairly easily. On this front, we are seeing some 
success as we have now constructed versions of 
TRIPS in three different domains, and TRIPS? 
911 will be the fourth. In developing the system 
for new domains, the bulk of the work by far has 
been in system enhancements rather than in 
developing the domain models. 
The TRIPS-911 domain has forced a rethinking 
of the relationship between dialogue- 
management, problem-solving, the system's 
Figure 1: Monroe County map used in TRIPS-911 
own goal-pursuit and generation. The new ar- 
chitecture is designed to support research into 
mixed-initiative interactions, incremental gen- 
eration of content (in which the user might in- 
tervene before the system completes all it has to 
say), rich reference resolution models, and the 
introduction of plan monitoring and plan repair 
into the suite of plan management operations 
supported. The domain also can support longer 
and richer dialogues than in previous domains. 
More complex domains mean even more com- 
plex dialogues. The complexity arises from 
many factors. First, more complex dialogues 
will involve topic progression, development and 
resumption, and more complex referential phe- 
nomena. On the problem solving front, there will 
be more complex corrections, elaborations and 
modifications--forcing us to develop richer 
discourse models. In addition, the complexity of 
the domain demonstrates a need for better 
grounding behavior and a need for incremental 
dialogue-based generation. 
We have by no means solved these problems. 
Rather we have built a rich testbed, designed and 
implemented a plausible architecture, and have 
constructed an initial system to demonstrate 
basic capabilities in each of the problem areas. 
34 
5. Limitations 
TRIPS-911 is a first attempt at handling a do- 
main of this complexity. As such there are many 
capabilities that people have in such situations 
that are beyond the system's current capabilities. 
Some of the most important are: 
? Scale - we can only handle small domains 
and the existing techniques would not ex- 
tend directly to a realistic size 911 operation. 
To scale up we must face some difficult 
problems including reasoning about quanti- 
ties and aggregates, planning in large-scale 
domains (i.e., the real domains are beyond 
the capabilities of current plan technology), 
and performing intention recognition as the 
number of options increases. In addition, for 
an effective dialogue system, all this must be 
done in real-time. 
? Meta-talk - when faced with complex prob- 
lems, people often first generally discuss the 
problem and possible strategies for solving 
it, and later may explicitly direct attention to 
specific subproblems. The current TRIPS 
system does not support such discussion. 
? Time - in the 911 domain there are at least 
two temporal contexts that can be "used" by 
the conversants: there is the actual time (i.e., 
when they are talking), but there also is the 
time relative to a point of focus in a plan, or 
even simply talking about the past or the 
future. TRIPS-911 can currently interpret 
expressions with respect to the actual time. 
? Interleaved generation - when people are 
discussing complex issues, they often have 
to plan to communicate heir content across 
several different utterances. There is no 
guarantee that the other conversant will not 
"interrupt" (e.g., to clarify, correct, suggest 
alternatives, etc) before the entire content is 
conveyed. This requires a rethinking of cur- 
rent practice in generation to make it incre- 
mental and interactive. 
? True interruptions - people may interrupt the 
system while it is talking. It is unclear at this 
stage what the system should assume was 
conveyed. The strategies of assuming noth- 
ing was conveyed, or that all was conveyed 
have obvious faults. We are pursuing alter- 
natives based on knowing when speech was 
interrupted, but using this ififormation suc- 
cessfully remains adifficult problem. 
References 
Allen, James et al An Architecture for a Generic 
Dialogue Shell, to appear, J. Natural Language 
Engineering, 2000. 
Ferguson, George and J. Allen,-TRIPS: An Integrated 
Intelligent Problem-Solving Assistant, Proc. Na- 
tional Conference on AI (AAAI-98), Madison, WI, 
1998. 
35 
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 51?58
Manchester, August 2008
?Deep? Grammatical Relations for Semantic Interpretation
Mark McConville and Myroslava O. Dzikovska
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
Informatics Forum, 10 Crichton Street, Edinburgh, EH8 9AB, Scotland
{Mark.McConville,M.Dzikovska}@ed.ac.uk
Abstract
In this paper, we evaluate five distinct sys-
tems of labelled grammatical dependency
against the kind of input we require for se-
mantic interpretation, in particular for the
deep semantic interpreter underlying a tu-
torial dialogue system. We focus on the
following linguistic phenomena: passive,
control and raising, noun modifiers, and
meaningful vs. non-meaningful preposi-
tions. We conclude that no one system
provides all the features that we require,
although each such feature is contained
within at least one of the competing sys-
tems.
1 Introduction
The aim of the work reported in this paper is to
evaluate the extent to which proposed systems of
grammatical relations (GRs) reflect the kinds of
deep linguistic knowledge required for semantic
interpretation, in particular for deriving semantic
representations suitable for domain reasoning in
dialogue systems.
Grammatical relations either produced by or ex-
tracted from the output of wide-coverage syntactic
parsers are currently used as input to shallow se-
mantic parsers, which identify semantic relations
that exist between predicators (typically verbs) and
their dependents (Gildea and Jurafsky, 2002; Erk
and Pad?o, 2006). Predicate-argument structure
identified in this way can then be used in tasks like
information extraction (Surdeanu et al, 2003) and
question answering (Kaisser and Webber, 2007).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
However, wide-coverage stochastic parsers are
only rarely used in dialogue systems. Tradi-
tionally, interpretation modules of dialogue sys-
tems utilise specialised parsers and semantic in-
terpreters handcrafted to a small domain (Seneff,
1992; Chang et al, 2002), or wide coverage deep
parsers (Allen et al, 2007; Jordan et al, 2006;
Wolska and Kruijff-Korbayov?a, 2003; Callaway et
al., 2007; Kay et al, 1994). Unlike in information
retrieval and question answering tasks, the system
often needs to be connected to a knowledge base
which represents the state of the world, and must
be able to convert user utterances into knowledge
base queries. In addition to identifying predicate-
argument relationships, such systems need to sup-
port a variety of tasks, for example resolution of
pronouns and anaphors, and interpreting negation,
quantification, tense and modality.
While deep parsers produce precise seman-
tic representations appropriate for such reason-
ing, they suffer from robustness problems. Wide-
coverage dependency parsers could potentially
provide a more robust alternative, provided that
their output is easy to convert into semantic rep-
resentations for reasoning.
Section 2 introduces the kind of deep linguis-
tic processing application which motivates our ap-
proach to grammatical relations. Section 3 de-
fines some underlying principles behind the kind
of ?deep? GR systemwe have in mind. The remain-
der of the paper discusses a number of linguistic
phenomena in detail, and evaluates how well vari-
ous systems of GR representation from the depen-
dency parsing literature capture the kind of linguis-
tic insights required for interface with reasoning?
passive (section 4), raising and control (section 5),
noun modification (section 6) and syntactic versus
semantic prepositions (section 7).
51
2 Motivation
As an example application that requires deep pars-
ing consider a tutorial dialogue system that inter-
prets students? answers to factual questions (e.g.
Which bulbs will be lit in this circuit?) as well
as explanation questions (e.g. Explain your rea-
soning!). It has been argued previously (Wolska
and Kruijff-Korbayov?a, 2004; Ros?e et al, 2003)
that tutorial dialogue systems require deep under-
standing of student explanations, which can have
significantly more complex structure than database
queries in the information-seeking domain. In our
application, if a student is asked for an explana-
tion, his or her input has to be passed through the
domain knowledge base to verify its factual cor-
rectness, and a separate process verifies that all
relations mentioned in the explanation are correct
and relevant. For example, imagine that the stu-
dent says the following:
(1) The bulbs in circuits 1 and 3 will be lit
because they are in closed paths with the
batteries.
Here, the system has to verify two things: (a) that
the facts are correct (bulbs in circuits 1 and 3 will
be lit, and each of those bulbs is in a closed path
with a battery); and (b) that the reason is valid ?
being in a closed path with a battery is a necessary
and sufficient condition for a bulb to be lit.
This task is particularly interesting because it
combines characteristics of deep and shallow inter-
pretation tasks. On the one hand, the fact-checking
mechanism requires a connection to the database.
Thus, both pronouns and definite noun phrases
need to be resolved to the objects they represent in
the knowledge base, and first-order logic formulas
representing utterance content need to be checked
against the system knowledge. This task is simi-
lar to natural language interfaces to databases, or
knowledge acquisition interfaces that convert lan-
guage into knowledge base statements (Yeh et al,
2005). On the other hand, with respect to rea-
son checking, human tutors have indicated that
they would accept an answer simply if a student
produces the key concepts and relations between
them, even if the answer is not strictly logically
equivalent to the ideal answer (Dzikovska et al,
2008). Human tutors tend to be especially lenient
if a student is asked a generic question, like What
is the definition of voltage?, which does not refer
to specific objects in the knowledge base. Thus, a
simpler matching mechanism is used to check the
reasons, making this task more similar to an infor-
mation retrieval task requiring shallower process-
ing, i.e. that the predicate-argument relations are
retrieved correctly (though negation still remains
important).
Thus, while a specific task is used to motivate
our evaluation, the conclusions would be applica-
ble to a variety of systems, including both deep and
shallow semantic interpreters.
For the purposes of this evaluation, we discuss
features of grammatical representation relevant to
two subtasks critical for the system: (a) identify-
ing predicate-argument structure; and (b) resolving
anaphora.
The extraction of predicate-argument relations
is a common requirement for both shallow and
deep semantic tasks. For example, for the stu-
dent input in example (1) we may expect some-
thing like:
1
(2) (LightBulb b1) (LightBulb b2)
(lit b1 true) (lit b2 true)
(Path P3) (closed P3 true)
(contains P3 b1) (Path P4)
(closed P4 true) (contains P4 b2)
Resolving anaphora, on the other hand, is par-
ticularly important for the kind of deep seman-
tic processing used in dialogue systems. Implicit
in the above representation is the fact that the
definite noun phrase the bulbs in circuits 1 and
3 was resolved to domain constants b1 and b3,
and indefinite references to paths were replaced by
Skolem constants P3 and P4. The reference reso-
lution process requires detailed knowledge of noun
phrase structure, including information about re-
strictive modification, and this is the second focus
of our evaluation.
Ideally, we would like a dependency parser to
produce grammatical relations that can be con-
verted into such semantic representations with
minimal effort, thus minimising the number of spe-
cific rules used to convert individual relations. We
discuss the principles underlying such representa-
tions in more detail in the next section.
1
We used a simplified representation of quantifiers that as-
sumes no scope ambiguity and uses skolem constants to rep-
resent existential quantification. This is sufficient for our par-
ticular application. In general, a more sophisticated quantifier
representation would be necessary, for example that proposed
in Copestake et al (2005) or Bos and Oka (2002), but we
leave the relevant evaluation for future work.
52
3 Deep grammatical relations
We formulated four principles for deep grammati-
cal relations representation.
Firstly, grammatical relations should, whenever
possible, reflect relations between the predicators
(i.e. content words as opposed to function words)
in a sentence. In addition, the same relation should
correspond to the same role assignment. For exam-
ple, the deep GRs in passive constructions should
be the same as those in the active equivalents
(see section 4), and the analysis of a control verb
construction like John persuaded Mary to dance
should make it clear that there is a ?subject? GR
from dance to Mary similar to that in the implied
sentence Mary danced (see section 5).
Secondly, a GR should, whenever possible, ap-
pear only if there is a an explicit selectional restric-
tion link between the words. For example, in a
raising verb construction like John expects Mary to
dance, there should be noGR from the raising verb
expects to its object Mary (see section 5). Also,
where a preposition functions strictly as a syntac-
tic role marker, as in the construction John relies
on Mary, it should have no place in the GR anal-
ysis; rather there should be a direct link from the
verb to the embedded noun phrase (see section 7).
Thirdly, the GRs should preserve evidence of
syntactic modification to enable reference resolu-
tion. To understand why this is important, take the
following two examples:
(3) The lit bulb is in a closed path.
The bulb in a closed path is lit.
From a pure predicate-argument structure perspec-
tive, these two sentences share exactly the same
deep GRs:
2
(4) ext(lit,bulb)
ext(in-closed-path,bulb)
However, from the perspective of reference resolu-
tion, the two sentences are very different. For the
first example, this process involves first finding the
lit bulb and then verifying that it is in a closed path,
whereas for the second we need to find the bulb in
a closed path and verify that it is lit. This differ-
ence can be captured by assigning the following
additional deep GRs to the first example:
2
The representation is simplified for reasons of exposition.
The GRs should be interpreted as follows: ext denotes the
external argument of an adjective or preposition, ncmod a
non-clausal restrictive modifier, and det the determiner of a
noun.
(5) det(bulb,the)
ncmod(bulb,lit)
And the following GRs are added to the analysis
of the second example:
(6) det(bulb,the)
ncmod(bulb,in-closed-path)
Now the two analyses are formally distinct: (a) the
first is rooted at predicate in a closed path and the
second at lit; and (b) the definite external argument
the bulb takes scope over the modifier lit in the first
but over in a closed path in the second. Noun mod-
ification is discussed in section 6.
Finally, the set of grammatical relations should
make it easy to identify and separate out con-
structions which are largely dependent on seman-
tic/world knowledge, such as N-N modification, so
that separate models and evaluations can be con-
ducted as necessary.
4 Passive
The shared task dataset contains numerous passive
participles, most of which can be classified into the
following four groups depending on how the par-
ticiple is used: (a) complement of passive auxiliary
e.g. Tax induction is activated by the RelA subunit;
(b) complement of raising verb e.g. The adminis-
tration doesn?t seem moved by the arguments; (c)
nominal postmodifier e.g. the genes involved in T-
cell growth; and (d) nominal premodifier e.g. the
proposed rules.
In all these cases, our system for deep gram-
matical relation annotation requires: (a) that
there is a relation from the passive partici-
ple to the deep object; and (b) that this rela-
tion be the same as in the corresponding ac-
tive declarative construction, so that predicate-
argument structure can be straightforwardly de-
rived. Thus, for example, the analysis of Tax in-
duction is activated by the RelA subunit will con-
tain the GR dobj(activated,induction),
and that of the proposed rules will include
dobj(proposed,rules), where dobj is the
relation between a transitive verb and its (deep) di-
rect object.
We evaluated five GR-based output formats ac-
cording to these two features. The results are pre-
sented in Table 1, where for each representation
format (the rows) and each usage class of pas-
sive participles (the columns), we provide the GR
which goes from the participle to its deep object,
53
complement of complement of nominal nominal
passive auxiliary raising verb postmodifier premodifier active
HPSG ARG2 (of verb arg12)
RASP ncsubj:obj dobj
CCGBank Spss\NP N/N S\NP/[NP]
Stanford nsubjpass - dobj
PARC subj - obj
Table 1: Representation of deep objects in passive and active
if such a GR exists.
3
The five GR representations
compared are:
HPSG predicate-argument structures extracted
from the University of Tokyo HPSG Treebank
(Miyao, 2006)
RASP grammatical relations as output by the
RASP parser (Briscoe et al, 2006)
CCGBank predicate-argument dependencies ex-
tracted from CCGBank (Hockenmaier and
Steedman, 2007)
Stanford grammatical relations output by the
Stanford Parser (de Marneffe et al, 2006)
PARC dependency structures used in the annota-
tion of DepBank (King et al, 2003)
The first four columns in Table 1 represent, for
each of the four uses of passive participles listed
above, the grammatical relation, if any, which typ-
ically joins a passive participle to its deep object.
The rightmost column presents the label used for
this relation in equivalent active clauses. Adjacent
columns have been collapsed where the same GR
is used for both uses. The ideal system would have
the same GR listed in each of the five columns.
The grammatical relations used in the Stan-
ford, PARC and RASP systems are atomic labels
like subj, obj etc, although the latter system
does allow for a limited range of composite GRs
like ncsubj:obj (a non-clausal surface subject
which realises a deep object). In the HPSG sys-
tem, verbal subjects and objects are represented
as ARG1 and ARG2 respectively of strict transi-
tive verb type verb arg12. Finally, the GRs as-
sumed in CCGBank consist of a lexical category
(e.g. the strict transitive verb category S\NP/NP)
with one argument emphasised. I assume the
3
The relations presented for HPSG and CCG are those for
passive participle of strict transitive verbs.
following notational convenience for those cate-
gories which contain specify more than one argu-
ment ? the emphasised argument is surrounded
by square brackets. Thus, subject and object of a
strict transitive verb are denoted S\[NP]/NP and
S\NP/[NP] respectively.
With respect to Table 1, note that: (a) in the
CCGbank dependency representation, although
prenominal passive participles are linked to their
deep object (i.e. the modified noun), this relation
is just one of generic noun premodification (i.e.
N/N) and is thus irrelevant to the kind of predicate-
argument relation we are interested in; (b) in the
PARC and Stanford dependency representations,
there is no GR from noun-modifying passive par-
ticiples to their deep objects, just generic modifica-
tion relations in the opposite direction; and (c) in
PARC, passive participles are themselves marked
as being passive, thus allowing a subsequent inter-
pretation module to normalise the deep grammati-
cal relations if desired.
If we are interested in a system of deep gram-
matical role annotation which allows for the rep-
resentation of normalised GRs for passive partici-
ples in all their uses, then the HPSG Treebank for-
mat is more appropriate than the other schemes,
since it uniformly uses deep GRs for both ac-
tive and passive verb constructions. The RASP
representation comes a close second, only requir-
ing a small amount of postprocessing to convert
ncsubj:obj relations into dobj ones. In addi-
tion, both the CCGBank and the Stanford notation
distinguish two kinds of surface subject ? those
which realise deep subjects, and those which re-
alise passivised deep objects.
5 Control
The shared task dataset contains a number of in-
finitives or participles which are dependents of
non-auxiliary verbs or adjectives (rather than be-
ing nounmodifiers for example). Most of these can
54
complements adjuncts raising
HPSG 3 3 5
RASP 3 3 5
CCGbank 3 3 5
Stanford 3 5 3
PARC 5 5 5
Table 2: Representation of controlled subjects and
raising
be partitioned into the following three classes: (a)
complements of subject control verbs e.g. The ac-
cumulation of nuclear c-Rel acts to inhibit its own
continued production; (b) complements of subject
raising verbs e.g. The administration seems moved
by arguments that . . . ; and (c) subject controlled
adjuncts e.g. Alex de Castro has stopped by to slip
six cards to the Great Man Himself.
In all these cases, our deep grammatical role an-
notation requires that there be a subject relation
(or an object relation in the case of a passive par-
ticiple) from the infinitive/participle to the surface
subject (or surface object in the case of object con-
trol) of the controlling verb/adjective. For exam-
ple, the analysis of Tax acts indirectly by induc-
ing the action of various host transcription fac-
tors will contain both the GRs sbj(acts,Tax)
and sbj(inducing,Tax). In addition, we also
want to distinguish ?raising? verbs and adjectives
from control structures. Thus, in the analysis of
The administration seems moved by arguments
that . . . , we want a (deep) object relation from
moved to administration, but we don?t want any
relation from seems to administration.
We again evaluated the various GR-based output
formats according to these features. The results are
presented in Table 2, where for each representation
format (the rows) we determine: (a) whether a verb
with an understood subject which is a complement
of the matrix verb is linked directly to its relevant
subject (column 1); (b) whether a verb with an un-
derstood subject which is a controlled adjunct of
the matrix verb is linked directly to its relevant
subject (column 2); and (c) whether raising verbs
are non-linked to their surface subjects (column
3). Note that the Stanford dependency represen-
tation is the only format which distinguishes be-
tween raising and control. This distinction is made
both structurally and in terms of the name assigned
to the relevant dependent ? controlled subjects
are distinguished from all other subjects (includ-
ing raised ones) by having the label xsubj rather
than just nsubj.
4
The ideal GR representation format would have
a tick in each of the three columns in Table 2. It is
clear that no single representation covers all of our
desiderata for a deep grammatical relation treat-
ment of control/raising, but each feature we require
is provided by at least one format.
6 Nominal modifiers
The dataset contains numerous prenominal modi-
fiers
5
, subdivided into the following three groups:
(a) attributive adjectives e.g. a few notable excep-
tions; (b) verb participles e.g. the proposed rules;
and (c) nouns e.g. a car salesman.
In order to ensure an adequate representation of
basic predicate-argument structure, our system of
deep grammatical annotation first of all requires
that, from each prenominal adjective or verb, there
is an appropriate relation to the modified noun, of
the same type as in the corresponding predicative
usage. For example, assuming that He proposed
the rules has a direct object relation from proposed
to rules, the same relation should occur in the anal-
ysis of the proposed rules. Similarly, if The excep-
tions are notable is analysed as having an external
argument relation from notable to exceptions, then
the same should happen in the case of a few no-
table exceptions. However, this does not appear to
hold for prenominal nouns, since the relation be-
tween the two is not simply one of predication ?
a car salesman is not a salesman who ?is? a car,
but rather a salesman who is ?associated? with cars
in some way. Thus we would not want the same
relation to be used here.
6
Secondly, in order to ensure a straightforward
interface with reference resolution, we need a
modification relation going in the opposite direc-
4
We have judged that CCGBank does not make the rele-
vant distinction between raising and control verbs based on
the dependency representations contained in the shared task
dataset. For example, for the example sentence The adminis-
tration seemmoved by the fact that . . . , a CCG subject relation
is specified from the raising verb seem to its surface subject
administration.
5
We focus on prenominal modifiers in order to keep the
exposition simple. Similar remarks are valid for postnominal
restrictive modifiers as well.
6
Presumably the same goes for attributive adjectives
which lack corresponding predicative uses, e.g. the former
president.
55
tion, from the modified noun to each (restrictive)
modifier, as argued in section 2. Thus, a complete
GR representation of a noun phrase like notable
exceptions would be cyclical, for example:
(7) ext(notable,exceptions)
ncmod(exceptions,notable)
We evaluated the various GR-based output formats
according to these desiderata. The results are pre-
sented in Table 3. For each annotation scheme (the
rows), we first present the relation (if any) which
goes from the modified noun to each kind of pre-
modifier (adjective, verb participle and noun re-
spectively).
7
Themiddle three columns contain the
relation (if any) which goes to the noun from each
kind of modifier. Finally, the last three columns
give the corresponding predicative relation used in
the annotation scheme, for example in construc-
tions like The exceptions are notable, He proposed
the rules, or Herbie is a car. Where it is un-
clear whether a particular format encodes the re-
lation between a predicative noun and its subject,
we mark this as ??? in the last column.
Ideally, what we want is a representation where:
(a) there is a GR in all nine columns (with the pos-
sible exception of the ?noun modifier to noun? one
(column 6)); (b) the corresponding relations in the
middle and righthand sections are identical, except
for ?noun modifier to noun? (column 6) and ?pred-
icative noun? (the last column) which should be
distinct, since the relation between a noun modifier
and its head noun is not simply one of predication.
It is clear that no one representation is perfect,
though every feature we require is present in at
least one representation system. Note in particu-
lar that the HPSG, PARC and Stanford systems are
acyclic ? the former only has ?modifier to noun?
links, while the latter two only have ?noun to mod-
ifier? ones. The RASP format is cyclic, at least for
prenominal participles ? in the proposed rules,
there is a modifier relation from rules to proposed,
as well as a deep object relation from proposed to
rules, the same relation that would be found in the
corresponding predicative the rules were proposed.
Note finally that the PARC and Stanford repre-
sentations distinguish between prenominal adjec-
tives and nouns, in terms of the name of the rele-
vant modifier GR. This corresponds well with our
7
Note that the N/N links in the CCG representation actu-
ally go from the modifier to the noun. However, they have
been included in the set of ?noun to modifier? relations since
they are formally modifier categories (i.e. of the form X/X).
preference for a GR system where we can evalu-
ate modules of N-N disambiguation (e.g. luxury
car salesman) in isolation from other aspects of
prenominal structure.
7 Prepositions
All five grammatical relations formats treat prepo-
sition phrases in pretty much the same way: (a)
there is a GR link from the head of which the PP
is a complement or modifier to the preposition it-
self (the HPSG representation has this link going
in the opposite direction for PP modifiers, but the
principle is the same); and (b) there is a link from
the preposition to its complement NP. For example,
the noun phrase experts in Congress is annotated as
follows:
(8) ncmod(experts,in)
dobj(in,Congress)
The only PPs which have been handled differently
are agentive by-PPs of passive participles, which
are either normalised or treated using a special,
construction-specific GR.
Note however that all prepositions are not equal
when it comes down to representing the predicate-
argument structure of a sentence. In a nutshell,
some prepositions are predicators (e.g. experts
in Congress) whereas others are simply syntactic
role markers (e.g. a workout of the Suns). Ide-
ally, we would want a GR system which marks
this distinction, for example by annotating pred-
icator prepositions as lexical heads and ignoring
role-marking prepositions altogether. The only
GR scheme which attempts to make this distinc-
tion is the PARC system, which has a ptype fea-
ture for every preposition with two possible val-
ues, semantic and non-semantic. However,
this does not appear to have been annotated consis-
tently in the PARC dataset ? the only examples of
non-semantic prepositions are agentive by-PPs of
passive participles.
8 Conclusion
We have proposed a set of principles for devel-
oping a grammatical relation annotation system
for use with both shallow and deep semantic in-
terpretation systems, in particular a tutorial dia-
logue system. We then evaluated five different GR
schemes from the dependency parsing literature
based on how well they handle a number of ?deep?
syntactic phenomena implied by these principles,
56
noun to modifier modifier to noun predicative
A V N A V N A V N
RASP ncmod - ncsubj etc - - ncsubj etc -
HPSG - a arg1 v arg1 etc n arg1 a arg1 v arg1 etc n arg1
CCG N/N - N/N - S\NP etc - Sadj\NP S\NP etc ?
PARC adjunct mod - subj subj ?
Stanf amod nn - nsubj nsubj ?
Table 3: Representation of prenominal modifiers
i.e. passive, control and raising, noun modifica-
tion, and meaningful vs. non-meaningful prepo-
sitions. We conclude that none of the proposed
GR annotation schemes contains everything we re-
quire for deep semantic processing, although each
of the features/distinctions we included in our list
of desiderata is provided by at least one system.
Many of the deep syntactic phenomena dis-
cussed here are known issues for shallow seman-
tic tasks like semantic role labelling. For exam-
ple, passive constructions are a recognised source
of noise in semantic role labelling systems (Gildea
and Jurafsky, 2002), and resolving controlled sub-
jects provides more data for training models of se-
lectional restrictions, which are known to be useful
features for role labelling. More generally, Chen
and Rambow (2003) demonstrate that a focus on
?deep? syntactic features results in a more accurate
stochastic semantic role labeller than using surface
information alone.
Note also that the deep grammatical role rep-
resentation proposed here is meant to be ?theory-
neutral?, in the sense that it was not influenced by
any one of the competing grammar formalisms to
the exclusion of the others. Indeed, it should be
a straightforward task to write a grammar using
either the HPSG, LFG, CCG or RASP-style un-
derlying formalism which can produce an output
representation consisting of deep relations, con-
structed in a purely compositional manner. Indeed,
the syntactic phenomena discussed in this paper
are those which form the basis of numerous in-
troductory textbooks on English generative syntax
(Haegeman, 1994; Sag and Wasow, 1999; Bres-
nan, 2000). In addition, the phenomena which
form the basis of the analysis in this paper were
among those which had been the focus of a sig-
nificant amount of attention in the development
of the semantic interpretation system underlying
our domain-independent tutorial dialogue system.
Other issues which were considered, but for which
we lack space to discuss in detail include: (a) ex-
pletive pronouns should be ignored, i.e. the subject
pronouns in ?impersonal? verb constructions like It
is raining or It?s great that John loves Mary should
not be seen as the target of deep grammatical re-
lations; (b) unbounded dependencies should be re-
solved, i.e. in the relative clause the woman Bill
thinks John loves there should be an object relation
between the embedded verb loves and its extracted
object woman; (c) restrictive and non-restrictive
modification (including apposition) should be dis-
tinguished, since the latter is not relevant for refer-
ence resolution; and (d) certain subsentential con-
junctions need to be compiled out (for examples
like electronic, computer and building products).
Finally, we recognise that, in many cases, it is
possible to transform parser representations into
our desired format. For example, if the parser out-
put tells us that a given verb form is a passive
participle, we can use this information to remap
the surface relations, thus retrieving the underlying
predicate-argument structure. However, we pre-
fer a system where this kind of post-processing
is not needed. Reasons for this include the in-
creased potential for error in a system relying on
post-processing rules, as well as the need to have
both detailed documentation for how each parser
output format handles particular constructions, as
well as a comprehensive mapping schema between
representations. Having a community standard for
GR-based parser output is an essential element of
future parsing technology, and to be practically
useful in a range of semantic interpretation tasks,
this standard should involve ?deep? syntactic dis-
tinctions of the kind discussed in this paper.
9 Acknowledgements
The work reported here was supported by grants
N00014-08-1-0179 and N00014-08-1-0043 from
the Office of Naval Research.
57
References
Allen, James, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL?07 Workshop on Deep Linguistic Processing.
Bos, Johan and Tetsushi Oka. 2002. An inference-
based approach to dialogue system design. In Pro-
ceedings of COLING?02.
Bresnan, Joan. 2000. Lexical-Functional Syntax. Basil
Blackwell.
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL?06 Interactive Presen-
tation Sessions.
Callaway, Charles B., Myroslava Dzikovska, Elaine
Farrow, Manuel Marques-Pita, Colin Matheson, and
Johanna D. Moore. 2007. The Beetle and BeeDiff
tutoring systems. In Proceedings of SLaTE?07.
Chang, N., J. Feldman, R. Porzel, and K. Sanders.
2002. Scaling cognitive linguistics: Formalisms
for language understanding. In Proceedings of
ScaNaLU?02.
Chen, John and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling of
semantic arguments. In Proceedings of EMNLP?03.
Copestake, Ann, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Semantics:
An Introduction. Research on Language and Com-
putation, 3:281?332.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC?06.
Dzikovska, Myroslava O., Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008. Diagnosing natural lan-
guage answers to support adaptive tutoring. In Pro-
ceedings of FLAIRS?08 special track on Intelligent
Tutoring Systems.
Erk, Katrin and Sebastian Pad?o. 2006. SHAL-
MANESER - a toolchain for shallow semantic pars-
ing. In Proceedings of LREC?06.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3).
Haegeman, Liliane. 1994. Introduction to Government
and Binding Theory. Basil Blackwell, 2nd edition
edition.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Jordan, Pamela, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of FLAIRS?06.
Kaisser, Michael and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings
of the ACL?07 Workshop on Deep Linguistic Pro-
cessing.
Kay, Martin, Jean Mark Gawron, and Peter Norvig.
1994. Verbmobil: A Translation System for Face-
To-Face Dialog. CSLI Press, Stanford, CA.
King, Tracy Holloway, Richard Crouch, Stefan Rie-
zler, Mary Dalrymple, and Ronald M. Kaplan. 2003.
The PARC 700 dependency bank. In Proceedings of
EACL?03.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis, Uni-
versity of Tokyo.
Ros?e, C. P., D. Bhembe, S. Siler, R. Srivastava, and
K. VanLehn. 2003. The role of why questions in ef-
fective human tutoring. In Proceedings of AIED?03.
Sag, Ivan A. and Thomas Wasow. 1999. Syntactic The-
ory: A Formal Introduction. CSLI.
Seneff, Stephanie. 1992. TINA: A natural language
system for spoken language applications. Computa-
tional Linguistics, 18(1).
Surdeanu, Mihai, Sanda M. Harabagiu, John Williams,
and Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceedings
of ACL?03.
Wolska, Magdalena and Ivana Kruijff-Korbayov?a.
2003. Issues in the interpretation of input in mathe-
matical dialogs. In Duchier, Denys, editor, Prospects
and advances in the syntax/semantics interface.
Lorraine-Saarland Workshop Series proceedings.
Wolska, Magdalena and Ivana Kruijff-Korbayov?a.
2004. Analysis of mixed natural and symbolic lan-
guage input in mathematical dialogs. In Proceedings
of ACL?04.
Yeh, Peter Z., Bruce Porter, and Ken Barker. 2005.
Matching utterances to rich knowledge structures to
acquire a model of the speaker?s goal. In Proceed-
ings of K-CAP?05.
58
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 162?172,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Exploring User Satisfaction in a Tutorial Dialogue System
Myroslava O. Dzikovska, Johanna D. Moore
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska,j.moore@ed.ac.uk
Natalie Steinhauser, Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division
Orlando, Florida, USA
gwendolyn.campbell,natalie.steinhauser@navy.mil
Abstract
User satisfaction is a common evaluation met-
ric in task-oriented dialogue systems, whereas
tutorial dialogue systems are often evaluated
in terms of student learning gain. However,
user satisfaction is also important for such
systems, since it may predict technology ac-
ceptance. We present a detailed satisfaction
questionnaire used in evaluating the BEETLE
II system (REVU-NL), and explore the un-
derlying components of user satisfaction us-
ing factor analysis. We demonstrate interest-
ing patterns of interaction between interpreta-
tion quality, satisfaction and the dialogue pol-
icy, highlighting the importance of more fine-
grained evaluation of user satisfaction.
1 Introduction
User satisfaction is one of the primary evaluation
measures for task-oriented spoken dialogue systems
(SDS): the goal of an SDS is to accomplish the task,
and to keep the user satisfied, so that they will want
to continue using the system. Typically, the PAR-
ADISE methodology (Walker et al, 2000) is used to
establish a performance function which relates user
satisfaction measured through questionnaires to in-
teraction parameters that can be derived from sys-
tem logs. This function can then be used to better
understand which properties of the interaction have
the most impact on the users, and to compare differ-
ent system versions.
In contrast, tutorial dialogue systems are typically
evaluated in terms of student learning gain, by com-
paring student scores on standardized tests before
and after interacting with the system. This is clearly
an important evaluation metric, since it directly as-
sesses the benefit students obtain from using the sys-
tem. However, it is also important to evaluate user
satisfaction, since it can influence students? willing-
ness to use computer tutors in a long run. Thus,
recent studies have looked at factors that could in-
fluence user satisfaction in tutorial dialogue, such as
different tutoring policies (Forbes-Riley and Litman,
2011), quality of speech output (Forbes-Riley et al,
2006), and students? prior attitudes towards technol-
ogy (Jackson et al, 2009).
Assessing user satisfaction, however, is not a
straightforward task. As we discuss in more detail in
Section 2, user satisfaction is known to be a complex
multi-dimensional construct, composed of largely
independent factors such as perceived ease of use
and perceived usefulness. Therefore, questionnaires
used for assessing satisfaction need to be validated
through user studies, and different satisfaction di-
mensions should be assessed independently. There-
fore, SDS researchers are now starting to use tech-
niques from psychometrics for this purpose (Hone
and Graham, 2000; Mo?ller et al, 2007). However,
user satisfaction studies tutorial dialogue currently
rely on simple questionnaires adapted from either
task-oriented SDS or non-dialogue intelligent tutor-
ing systems (Michael et al, 2003; Forbes-Riley et
al., 2006; Forbes-Riley and Litman, 2011; Jackson
et al, 2009), and these questionnaires have not been
validated for tutorial dialogue systems.
In this paper, we make the first step towards de-
veloping a better user satisfaction questionnaire for
tutorial dialogue systems. We present a user satis-
162
faction evaluation of the BEETLE II tutorial dialogue
system. Starting with a detailed user satisfaction
questionnaire, we employ exploratory factor analy-
sis to discover a set of dimensions for the students?
satisfaction with a dialogue-based tutor. We then
use the factors we derived to compare user satisfac-
tion between two versions of our computer tutor that
use different policies for generating the tutor?s feed-
back. We investigate the relationships between the
subjective satisfaction dimensions and the objective
learning gain metric for the two systems. Finally, we
carry out a more detailed investigation of our prior
results on the relationship between user satisfaction
and interpretation quality in tutorial dialogue. Our
analysis also provides insights for further improving
the questionnaire we developed and gives an exam-
ple of how user satisfaction metrics developed for
task-oriented dialogue can be adapted to different
dialogue applications. It also opens new questions
about how different properties of the interaction af-
fect user satisfaction in tutorial dialogue, which can
be investigated in future work.
The rest of the paper is organized as follows. We
discuss the approaches for assessing user satisfac-
tion with SDS in Section 2. In Section 3 we describe
the BEETLE II tutorial dialogue system used in this
evaluation. We describe our questionnaire design in
Section 4, and describe its use in BEETLE II evalu-
ation in Section 5. We conclude by discussing the
implication of our analysis for tutorial dialogue sys-
tem evaluation in Section 6.
2 Background
A typical approach to assessing user satisfaction in
dialogue systems is collecting user survey data by
asking users to rate their agreement with statements
such as ?the system was easy to use?. In the simplest
case of early PARADISE studies, the questionnaires
contained 5 items assessing different dimensions of
satisfaction, which were then summed to produce a
total satisfaction score.
However, using simple questionnaires has draw-
backs now recognized by the SDS community. First,
if individual questions are expected to assess differ-
ent dimensions of user satisfaction, they need to be
validated first, or else they may be ambiguous and
mean different things to different users. Second,
summing or averaging over questions measuring dif-
ferent satisfaction components may not be the best
approach, since it may conflate unrelated judgments
(Hone and Graham, 2000).
To address this problem, SDS researchers have
started using more complex questionnaires, where
each underlying dimension of user satisfaction is as-
sessed through multiple questions. Factor analysis is
then used to determine which questions are related
to one another (and therefore are likely to be assess-
ing the same underlying satisfaction dimension), and
to discard possibly ambiguous questions. Then, the
PARADISE methodology can be used to relate dif-
ferent interaction parameters to individual compo-
nents of user satisfaction.
Several such studies have been conducted recently
(Hone and Graham, 2000; Larsen, 2003; Mo?ller et
al., 2007; Wolters et al, 2009), covering command-
and-control and information-seeking dialogue. The
questionnaires in those studies contained 25 to 50
items, and factor analyses typically resulted in 6- or
7-factor solutions, with dimensions such as accept-
ability, affect, system response accuracy and cogni-
tive demand. The underlying factors found by those
analyses tend to match up well, but not to over-
lap perfectly. In comparison, all user satisfaction
questionnaires for tutorial dialogue systems that we
are aware of contain 10-15 items which are either
summed up for PARADISE studies, or compared
individually to track system improvement (Michael
et al, 2003; Forbes-Riley et al, 2006; Forbes-Riley
and Litman, 2011; Jackson et al, 2009).
In this paper, we apply the more sophisticated
SDS evaluation methodology to the BEETLE II tu-
torial dialogue system. We devise a more sophis-
ticated user satisfaction questionnaire using SDS
questionnaires for guidance and then apply factor
analysis to investigate the underlying dimensions.
We compare our results to analyses from two pre-
vious studies: SASSI (Hone and Graham, 2000),
which is a validated questionnaire intended for use
with a variety of task-oriented dialogue systems,
and a more recent ?modified SASSI? questionnaire
which is a version of SASSI adapted for use with the
INSPIRE home control system (Mo?ller et al, 2007).
Henceforth we will refer to this as INSPIRE.
163
3 BEETLE II Tutorial Dialogue System
The goal of BEETLE II (Dzikovska et al, 2010c)
is to teach students conceptual knowledge in the do-
main of basic electricity and electronics. The system
is built on the premise that encouraging students to
explain their answers and to talk about the domain
will lead to improved learning, a finding consistent
with analyses of human-human tutoring in several
domains (Purandare and Litman, 2008; Litman et
al., 2009). BEETLE II has been engineered to test
this hypothesis by eliciting contentful talk through
explanation questions.
The BEETLE II learning material consists of two
self-contained lessons suitable for college-level stu-
dents with no prior knowledge of basic electricity
and electronics. The lessons take 4 to 5 hours to
complete, and consist of reading materials and inter-
active exercises. During the exercises, the students
interact with a circuit simulator, building electrical
circuits containing bulbs, batteries and switches, and
using a multimeter to measure voltage. Then the
tutor asks students to explain circuit behavior, for
example, ?Why was bulb A on when switch Y was
open and switch Z was closed?? In addition, at dif-
ferent points in the lesson the tutor asks ?summary?
questions, asking students to define concepts such
as voltage, and verbalize general patterns such as
?What are the conditions that are required for a bulb
to light??. At present, students use a typed chat in-
terface to communicate with the system.1
We built and evaluated two versions of the sys-
tem (Dzikovska et al, 2010a). The baseline non-
adaptive tutor (BASE) requires students to produce
answers, but does not provide any remediation and
immediately states the correct answer. The fully
adaptive version (FULL) engages in dialogue with
the student, and tailors its feedback to the student?s
answer by confirming its correct parts and giving
hints in order to help students fix missing or incor-
rect parts. The FULL system generates feedback au-
tomatically based on a detailed analysis of the stu-
dent?s input, and is capable of giving hints at differ-
ent levels of specificity depending on the student?s
previous performance.
1A speech interface is being developed, but typed communi-
cation is common in online and distance learning, and therefore
is an acceptable choice for tutorial dialogue as well.
These two system versions were designed to eval-
uate the impact of adaptive feedback (within the lim-
itations of current language interpretation technol-
ogy) on student learning and satisfaction. Our initial
data analysis focused on the differences in student
language depending on the condition (Dzikovska et
al., 2010a), and on the impact of different types of
interpretation errors on learning gain and user sat-
isfaction (Dzikovska et al, 2010b). However, these
initial results were based on an aggregate satisfac-
tion score obtained by averaging over scores for all
questions in our user satisfaction questionnaire. In
this analysis, we take a more detailed look at the dif-
ferent factors that contribute to students satisfaction
with the system, and their relationship with learning
gain and interpretation quality.
4 Data Collection
4.1 Questionnaire Design
To support user satisfaction evaluation we developed
a satisfaction questionnaire, REVU-IT (Report on
the Enjoyment, Value, and Usability of an Intelli-
gent Tutor). It consists of 63 items which cover all
aspects of interaction with the tutoring system: the
clarity and usefulness of the reading material; the
graphical user interface to the circuit simulator; in-
teraction with the dialogue tutor; and the overall im-
pression of the BEETLE II system as a whole. The
reading material, graphical user interface and inter-
action with the tutor sections are complementary,
because they cover separate parts of the BEETLE II
interface. We expect that all of these three compo-
nents contribute to the overall impression score. For
purposes of this paper, we will focus on the part of
the questionnaire that relates to the natural language
interaction with the tutor (REVU-NL), and its re-
lationship to the overall impression score (REVU-
OVERALL).
The REVU-IT questionnaire was developed by
experienced cognitive psychologists (two of the au-
thors of this paper). The REVU-NL section con-
sists of 35 items shown in Appendix A. Its design
was guided by questionnaires used in previous re-
search, including INSPIRE and a questionnaire used
to evaluate the ITSPOKE tutorial dialogue system
(Forbes-Riley et al, 2006). REVU-NL contains a
number of items from these, but omits items that are
164
not relevant to the BEETLE II domain (e.g, ?Domes-
tic devices can be operated efficiently with the sys-
tem? or ?The tutor responded effectively after I was
uncertain?), and adds extra questions related to tu-
toring (e.g., ?Our dialogues quickly led to me hav-
ing a deeper understanding of the material?), based
on the authors? previous experience in human factors
research. We also slightly rephrased all questions to
refer to ?the tutor? rather than ?the system?.
The REVU-OVERALL section of REVU-IT
consists of 5 items assessing the student?s satis-
faction with their learning as a whole. The ques-
tions are: ?Overall, I am satisfied with my experi-
ence learning about electricity from this system.?;
?Working in this learning environment was just like
working one-on-one with a human tutor?; ?I would
have preferred to learn about electricity in a different
way.?; ?I would use this system again in the future to
continue to learn about electricity.?; ?I would like to
be able to use a system like this to learn about other
topics in the future.?. We use the averaged score over
these 5 items to represent the student?s overall satis-
faction with the learning environment, referring to it
as ?overall satisfaction?.
Adding new questions to the REVU-NL ques-
tionnaire on top of already existing questions is the
initial step in addressing the issues discussed in Sec-
tion 2: validating the individual questions and dis-
covering the underlying dimensions of user satis-
faction. Having a large number of questions ask-
ing about the same aspects of the interaction will
allow us to group related questions together into di-
mensions (?factors?), and also to discover ambigu-
ous questions that will need to be improved in future
studies. The detailed discussion of the technique and
issues involved is presented in Hone and Graham
(2000).
4.2 Participants
We used REVU-IT as part of a controlled experi-
ment comparing the BASE and FULL versions of the
system. We recruited 87 participants from a uni-
versity in the Southern US, paid for participation.
Participants had little knowledge of the domain.
Each participant signed consent forms and com-
pleted a pre-test, then worked through both lessons
(with breaks), and then completed a post-test and a
REVU-IT questionnaire. Each session lasted 3.5
hours on average.
Out of 87 participants that completed the study, 13
had an inordinate amount of trouble with interface:
they typed utterances that could not be interpreted
by the tutor (defined as having more than 3 standard
deviations in interpretation errors compared to the
rest), did not follow tutor?s instructions or experi-
enced system crashes. In addition, two participants
were learning gain outliers (again, more than 3 stan-
dard deviations from average). These participants
were removed from the analysis. The questionnaires
from the remaining 72 participants are used in our
data analysis.
5 Analysis
5.1 Underlying satisfaction dimensions
Each item in the REVU-NL questionnaire used a
5-point Likert scale, from ?completely disagree? (1)
to ?fully agree? (5). Most of the items were phrased
so that the agreement with the statement meant a
positive evaluation of the system. For a few items,
however, the polarity was reversed (e.g., ?The tutor
was not helpful?). Those items were reverse-coded,
with 1 meaning ?fully agree? and 5 ?completely dis-
agree?, to ensure that a lower score on all questions
corresponds to a negative assessment.
Following Hone and Graham (2000), we used
exploratory factor analysis to group questionnaire
items into clusters representing different dimen-
sions. One of the standard approaches in determin-
ing how many factors (?question clusters?) to use
is the scree test which checks the number of eigen-
values in the question covariance matrix which are
greater than 1. These typically correspond to prin-
cipal components which reflect the underlying ques-
tionnaire structure. The scree test showed 7 eigen-
values greater than 1, resulting in the 7-factor solu-
tion presented in Table 1.
The loadings in the table are the correlation coef-
ficients between the individual question scores and
the variables representing the factors. Most of the
correlations are quite high, indicating that the ques-
tions are strongly correlated both among themselves
and the underlying factor. However, the last two fac-
tors contain only non-loading questions according to
the criteria in (Hone and Graham, 2000), i.e., ques-
tions for which the correlations are too weak to be
165
# Question Load-
ing
1 t29: Knew what to say at each point 0.82
1 t22: Easy to interact with the tutor. 0.79
1 t9: Not sure what was expected. 0.73
1 t18: Knew what to say to the tutor. 0.70
1 t14: The tutor was too inflexible. 0.69
1 t19: Able to recover easily from errors 0.69
1 t24: Easy to learn to speak to tutor. 0.69
1 t16: Tutor didn?t do what I wanted. 0.65
1 t3: Tutor understood me well. 0.65
1 t15: Working as easy as with a human. 0.64
1 t13: Had to concentrate when talking. 0.62
2 t31 Tutor was an efficient way to learn. 0.79
2 t32: Easy to learn from the tutor. 0.78
2 t34: Tutor was worthwhile 0.72
3 t28: Tutor was irritating. 0.76
3 t10: Tutor was fun. 0.74
3 t7: Enjoyed talking with tutor. 0.72
3 t30: Dialogues were boring. 0.66
4 t2: Tutor took too long to respond 0.84
4 t33: Tutor responded quickly 0.84
5 t26: Didn?t always understand tutor 0.89
6 (t3: The tutor understood me well) 0.4
7 (t25: Comfortable talking with tutor) 0.59
Table 1: Factors derived from the REVU-NL question-
naire, with question loadings for the factor to which each
question was assigned. Question text shortened due to
space limitations, full text presented in the appendix.
Non-loading questions in parentheses.
reliable. In addition, factors 4 and 5 had fewer than
3 questions. Since the number of subjects in our data
set is small, such factors may not be reliable. There-
fore, we focus our remaining analysis on the top 3
factors from the questionnaire, each of which con-
tains 3 or more questions.
Twelve questions in REVU-NL were ?cross-
loading? according to criteria in Hone and Graham
(2000), that is, their two top loadings differed by
less than 0.2. This indicates questions that are likely
to be ambiguous, since they are strongly correlated
with two (theoretically independent) variables. Such
questions should be refined and re-designed in future
surveys. These were questions t1, t4, t6, t11, t12,
t17, t20, t21, t23, t25, t27, t35 from the appendix.
We removed them from our solution, and discuss the
implications for survey design in Section 6.
The first component in our analysis lines up well
with the Transparency and Cognitive load factors
from INSPIRE, and Response accuracy, Cognitive
demand and Habitability from SASSI, though it was
not split into individual factors as in those analyses.
We will refer to this factor as Transparency. The
second component contains questions specific to tu-
toring. However, it is similar to the Acceptability
dimension from INSPIRE (the original SASSI ques-
tionnaire did not include similar questions), which
asked users to rate statements such as ?domestic de-
vices can be operated efficiently with the system?.
Thus, we will refer to it as Acceptability. Finally,
our third dimension lines up best with the Affect and
Annoyance items from SASSI.2 We will refer to it as
Affect.
Although the correspondences between our fac-
tors and those derived from SASSI and INSPIRE
are not perfect, the fact that similar underlying fac-
tors are derived from different user groups and sys-
tems indicates that they are likely to be measuring
the same underlying constructs.
5.2 Comparing satisfaction in different systems
Recall that in this study we combined the data from
two systems: FULL, where the system provided stu-
dents with adaptive feedback and hints, and BASE,
where the system simply acknowledged the stu-
dent?s answers and then provided a correct answer
without engaging in dialogue. Table 2 separates out
the average factor scores for these two conditions,
where a factor score is computed by averaging over
scores of all questions assigned to that factor.
When comparing learning gain and overall satis-
faction between the two systems (which is the over-
all impression of the system behavior as a whole,
including circuit simulation and lesson design), the
difference is not statistically significant (learning
gain t(69) = ?0.95, p = 0.35, overall satisfac-
tion t(69) = ?1.52, p = 0.13). In contrast, on
individual dimensions related to tutoring the scores
for BASE is significantly higher than the score for
FULL (Transparency, t(69) = ?7.19, p < 0.0001;
Acceptability: t(69) = ?3.24, p < 0.01; Affect:
2The acceptability dimension from INSPIRE is split be-
tween our factors 2 and 3, but most of the questions correspond
to our factor 2 questions.
166
FULL BASE
Transparency 2.15 (0.56) 3.36 (0.81)
Acceptability 3.11 (1.02) 3.80 (0.77)
Affect 2.43 (0.80) 2.86 (0.996)
Overall 3.39 (0.88) 3.70 (0.83)
Learning gain 0.61 (0.15) 0.65 (0.22)
Table 2: Average scores for different satisfaction dimen-
sions in FULL and BASE (standard deviation in parenthe-
ses)
t(69) = ?1.97, p = 0.05). Comparing the means,
the biggest difference in student ratings shows on the
Transparency scale, while the affective reaction for
the two systems is more similar (though still rated
higher for BASE).
It is somewhat unexpected to see that the students
were equally satisfied overall with both systems but
rated the tutor in BASE more highly than in FULL,
since the tutor behavior was the only thing different
between conditions. We are at present investigating
the reasons for this result. One possibility is that
when students did not get much feedback from the
tutor (as in BASE), other factors became more im-
portant to overall satisfaction, such as course design
and quality of user simulation.
5.3 Relationships between subjective and
objective outcome measures
We investigated the correlations between learning
gain and different user satisfaction factors for the
two system versions. Results are presented in Table
3. As can be seen from the table, learning gain and
user satisfaction are only significantly correlated in
FULL, and only for the acceptability and overall sat-
isfaction factors. None of the factors in the BASE
system correlate with learning gain. This indicates
that the student?s affective reaction to the system is
not necessarily linked directly to its objective bene-
fits. We discuss these results further in Section 6
5.4 Impact of interpretation quality on user
satisfaction
It is generally known in SDS research that measures
of interpretation quality such as word error rate and
concept accuracy are strongly correlated with user
FULL BASE
Transparency 0.32 (0.07) 0.06 (0.69)
Acceptability 0.38 (0.03) 0.23 (0.16)
Affect 0.29 (0.08) -0.10 (0.53)
Overall 0.38 (0.02) 0.18 (0.28)
Table 3: Correlations between satisfaction factors and
learning gain for two dialogue policies. Significance level
in parentheses. Bold indicates significance at p < 0.05
level.
satisfaction (e.g., (Walker et al, 2000; Mo?ller et al,
2007)). Our system uses typed input and produces
complex logical representations (rather than sim-
ple slot-value pairs), thus, these measures cannot be
computed directly. However, in an earlier study we
showed that another measure of interpretation qual-
ity, namely, percentage of utterances that could not
be interpreted by the system (?uninterpretable utter-
ances?) is negatively correlated with learning gain
and user satisfaction (Dzikovska et al, 2010b).3
That study revealed an unexpected pattern. Al-
though the system recorded the number of utter-
ances it could not interpret in both FULL and BASE,
students in BASE were never informed of any in-
terpretation problems. Nevertheless, the proportion
of such uninterpretable utterances was still signifi-
cantly negatively correlated with user satisfaction in
BASE. After analyzing correlations between differ-
ent types of errors and user satisfaction, we hypoth-
esized that this can be explained by the lack of align-
ment between the system and the student, in partic-
ular when students used terminology different from
that used by the system (Dzikovska et al, 2010b).
We can now analyze this relationship in more de-
tail, looking at correlations between interpretation
problems and different components of user satisfac-
tion. The results are presented in Table 4.
As can be seen from the table, the proportion
of uninterpretable answers is significantly correlated
with Acceptability in FULL, but not in BASE. This
is not surprising, indicating that students who were
told that they were not understood perceived the
system as less useful for them. More surprisingly,
Transparency, which is related to perceived ease of
3In that study, we computed user satisfaction with the tutor
by averaging over the entire 35 questions in our questionnaire
as an initial approximation.
167
FULL BASE
Transparency -0.28 (0.1) -0.25 (0.10)
Acceptability -0.58 (< 0.001) -0.29 (0.07)
Affect -0.35 (0.04) -0.34 (0.04)
Overall -0.38 (0.03) -0.27 (0.11)
Learning gain -0.38 (0.03) -0.09(0.60)
Table 4: Correlations between satisfaction factors and un-
interpretable utterances for two different policies. Signif-
icance level in parentheses.
use for the system, was not correlated with uninter-
pretable utterances. Finally, the proportion of unin-
terpretable utterances is significantly correlated with
Affect for both systems. Moreover, the unexpected
negative correlation we observed in the earlier study
between satisfaction with the tutor and interpretation
problems in BASE can be primarily attributed to the
negative correlation with the Affect score.
6 Discussion
In this study, we attempted to apply insights from
studies of user satisfaction in spoken dialogue sys-
tems to a different type of dialogue application: tu-
torial dialogue. We were looking to develop a better
user satisfaction questionnaire for evaluating tutorial
dialogue systems, and to implement an evaluation
methodology which takes into account different un-
derlying dimensions of user satisfaction.
The three dimensions we obtained based on ex-
ploratory factor analysis of REVU-NL align well
with the dimensions reported in the SDS litera-
ture, which provides some evidence of their valid-
ity. However, the results are preliminary because
of the small number of participants involved, and
need to be replicated with additional participants and
different tutoring systems. Regardless, our analysis
highlighted important issues in designing satisfac-
tion surveys for different dialogue genres.
When choosing which questions to include in a
satisfaction questionnaire for a new system type,
SASSI is a very attractive starting point, because
it was validated across multiple SDS in two gen-
res (command and control and information seeking).
This also means that SASSI items are phrased very
generally and therefore easier to adapt. In contrast,
INSPIRE contains a number of questions specific to
the command and control domain, asking whether
the user thinks the system is useful in achieving their
goals (i.e., operating the domestic devices). SASSI
includes only one similar item, ?The system was
useful?. It was classed as Affect, most likely be-
cause there were no other similar items. However,
we think that such questions represent an important
separate dimension, namely the ?perceived useful-
ness? factor known to predict technology acceptance
(Adams et al, 1989). Therefore we included sev-
eral items in REVU-NL with similar intent, asking
whether users thought the system was beneficial to
their goal (i.e., learning the material). These items
were clustered into a separate dimension by factor
analysis, indicating that they should be included in
other satisfaction surveys.
Moreover, some of the questions that appeared
genre-independent to us proved to be cross-loading
in our analysis, which is an indicator of ambiguity.
Apparently, some of the items from task-oriented di-
alogue questionnaires did not transfer well. For ex-
ample, statements like ?The system didn?t always do
what I expected? are unambiguous for task-oriented
dialogue, where the user is supposed to be in control
of the interaction, and therefore has clear expecta-
tions of what the system should do. In contrast, in
tutorial dialogue the tutor has control over the learn-
ing material. Thus, it may be more ambiguous as
to what, if anything, students are expecting from the
interaction.
Overall, our experience shows that it may not
be possible, or indeed useful, to create completely
generic surveys. However, we believe that question-
naires can be phrased generally enough to apply to a
range of systems with similar goals, and REVU-NL
in particular is useful starting point for comparing
dialogue-based tutoring systems. We believe that the
18 questions that we retained as unambiguous in our
analysis provide adequate assessment of user satis-
faction, and are grouped into factors consistent with
results of previous research. However, the question-
naire could be further improved by revisiting the
cross-loading items we rejected as ambiguous, and
seeing if their wording could be improved. We are
also intending to use REVU-IT in evaluating a spo-
ken version of BEETLE II, thus providing additional
validation data on a different version of the interface.
With respect to evaluation methodology, our re-
sults highlight the need to look at different satis-
168
faction dimensions separately. We used our fac-
tors to further investigate a pattern that we discov-
ered in previous research, namely, that students who
speak in a way that is difficult for the system to in-
terpret tend to be less satisfied with the tutor, even
when they are not told of the interpretation prob-
lems. Looking at correlations with individual di-
mensions shows that this relationship is primarily
explained by the Affect dimension. Our working hy-
pothesis is that the lack of alignment between in-
correct student answers and the answers supplied by
the system caused students to perceive the system as
a less likeable or cooperative conversational partner.
We also observed that Acceptability, but no other
dimensions, were correlated with learning gain in
FULL. One possible explanation is that students who
are learning more believe that the system is help-
ing them reach their goals (our definition of Accept-
ability). The FULL condition provides students with
more explicit feedback as to their learning; whereas
in BASE students may have a less accurate estimate
of how well they are doing, and hence no satisfaction
dimensions are correlated with learning gain.
It is worth noting that an earlier study investigat-
ing the relationship between user satisfaction and
learning in two different tutorial dialogue systems
(Forbes-Riley and Litman, 2009) found little corre-
lation between the answers to individual questions
on their satisfaction questionnaire and learning gain.
Only one correlation, with the question ?The tutor
helped me to concentrate?, reached significance in
only one of the 4 conditions they investigated. This
adds further evidence that the relationship between
learning gain and satisfaction is not straightforward.
However, our results are difficult to compare since
the questionnaires used are different, and Forbes-
Riley and Litman (2009) are studying correlations
with individual questions rather than grouping re-
lated questions together. Developing better validated
questionnaires will make such results easier to com-
pare and interpret, and we believe that REVU-NL
makes a significant step in that direction.
7 Conclusion and Future Work
In this paper, we proposed an improved question-
naire (REVU-NL) for evaluating user satisfaction
in tutorial dialogue systems, which is an important
evaluation metric alongside learning gain. We used
the methodology from SDS evaluations to investi-
gate different dimensions of user satisfaction, and
their relationship to learning gain and different in-
teraction properties. Next, we are planning to use
the PARADISE methodology to establish predictive
models that relate satisfaction dimensions to mea-
surable interaction properties, so that we can de-
termine development priorities, and make it eas-
ier to compare different system versions. We are
also planning to collect additional questionnaire data
with a speech-enabled version of the system, and
verify our analyses on this extended data set.
Acknowledgments
This work has been supported in part by US Of-
fice of Naval Research grants N000141010085 and
N0001410WX20278. We would like to thank our
sponsors from the Office of Naval Research, Dr. Su-
san Chipman and Dr. Ray Perez, and the Research
Associates who worked on this project, Kather-
ine Harrison, Leanne Taylor, Charles Scott, Simon
Caine, Elaine Farrow and Charles Callaway for their
contribution to this effort.
References
Dennis A. Adams, R. Ryan Nelson, and Peter A. Todd.
1989. Perceived usefulness, ease of use, and usage of
information technology. MIS Quarterly., 13:319?339.
Myroslava Dzikovska, Natalie B. Steinhauser, Jo-
hanna D. Moore, Gwendolyn E. Campbell, Kather-
ine M. Harrison, and Leanne S. Taylor. 2010a. Con-
tent, social, and metacognitive statements: An em-
pirical study comparing human-human and human-
computer tutorial dialogue. In Sustaining TEL: From
Innovation to Learning and Practice - 5th European
Conference on Technology Enhanced Learning (EC-
TEL 2010), pages 93?108, Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010b. The
impact of interpretation problems on tutorial dialogue.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics(ACL-2010),
Uppsala, Sweden, July.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010c. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proceedings of the 48th Annual Meeting of
169
the Association for Computational Linguistics (ACL-
2010) demo session, Uppsala, Sweden, July.
Katherine Forbes-Riley and Diane J. Litman. 2009.
Adapting to student uncertainty improves tutoring dia-
logues. In Artificial Intelligence in Education: Build-
ing Learning Systems that Care: From Knowledge
Representation to Affective Modelling, Proceedings
of the 14th International Conference on Artificial In-
telligence in Education (AIED 2009), pages 33?40,
Brighton, UK, July.
Katherine Forbes-Riley and Diane J. Litman. 2011.
Designing and evaluating a wizarded uncertainty-
adaptive spoken dialogue tutoring system. Computer
Speech & Language, 25(1):105?126.
Katherine Forbes-Riley, Diane J. Litman, Scott Silliman,
and Joel R. Tetreault. 2006. Comparing synthesized
versus pre-recorded tutor speech in an intelligent tu-
toring spoken dialogue system. In Proceedings of
the Nineteenth International Florida Artificial Intelli-
gence Research Society Conference, pages 509?514,
Melbourne Beach, Florida, USA, May.
Kate S. Hone and Robert Graham. 2000. Towards a
tool for the subjective assessment of speech system
interfaces (SASSI). Natural Language Engineering,
6(3&4):287?303.
G. Tanner Jackson, Arthur C. Graesser, and Danielle S.
McNamara. 2009. What students expect may have
more impact than what they know or feel. In Proceed-
ings 14th International Conference on Artificial Intel-
ligence in Education (AIED), Brighton, UK.
Lars Bo Larsen. 2003. Issues in the evaluation of spo-
ken dialogue systems using objective and subjective
measures. In Proceedings of 2003 IEEE Workshop
on Automatic Speech Recognition and Understanding
(ASRU?03), pages 209 ? 214, December.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proceedings of 14th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Brighton, UK, July.
Joel Michael, Allen Rovick, Michael Glass, Yujian Zhou,
and Martha Evens. 2003. Learning from a computer
tutor with natural language capabilities. Interactive
Learning Environments, 11:233?262(30).
Sebastian Mo?ller, Paula Smeele, Heleen Boland, and Jan
Krebber. 2007. Evaluating spoken dialogue systems
according to de-facto standards: A case study. Com-
puter Speech & Language, 21(1):26 ? 53.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings of
the 21st International FLAIRS Conference, Coconut
Grove, Florida, May.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards Developing General Models of
Usability with PARADISE. Natural Language Engi-
neering, 6(3).
Maria Wolters, Kallirroi Georgila, Robert Logie, Sarah
MacPherson, Johanna Moore, and Matt Watson. 2009.
Reducing working memory load in spoken dialogue
systems. Interacting with Computers, 21(4):276?287.
170
A REVU-NL Questions
t1 I felt in control of my conversations with the tutor.
t2 It took the tutor too long to respond to my statements.
t3 I felt that the tutor understood me well.
t4 The tutor didn?t always do what I expected.
t5 The information that the tutor provided to me was incomplete.
t6 It was easy for me to become confused during our dialogue.
t7 I enjoyed talking with the tutor.
t8 The tutor interfered with my understanding of the topics in electricity and circuits.
t9 I was not always sure what the tutor expected of me.
t10 Conversing with the tutor was fun.
t11 It was easy to understand the things that the tutor said.
t12 The dialogue between me and the tutor was very repetitive.
t13 I had to really concentrate when I was talking with the tutor.
t14 The tutor was too inflexible.
t15 Working through the lessons with the computer tutor was as easy as working through the lessons
with a human tutor.
t16 The tutor didn?t always do what I wanted.
t17 I felt confident when talking with the tutor.
t18 I always knew what to say to the tutor.
t19 I was able to recover easily from errors during our dialogues.
t20 Talking with the tutor was frustrating.
t21 The information provided by the tutor was clear.
t22 It was easy to interact with the tutor.
t23 The tutor?s dialogue was clumsy and unnatural.
t24 It was easy to learn how to speak to the tutor in a way that the tutor understood.
t25 I felt comfortable talking with the tutor.
t26 I didn?t always understand what the tutor meant.
t27 The tutor was not helpful.
t28 I found conversing with the tutor to be irritating.
t29 I knew what I could say or do at each point in the conversation with the tutor.
t30 I found our dialogues to be boring.
t31 Having the tutor help me with the material was an efficient way to learn.
t32 It was easy to learn from the tutor.
t33 The tutor responded quickly.
t34 Having the tutor was worthwhile
t35 Our dialogues quickly led to me having a deeper understanding of the material.
B REVU-OVERALL questions
o1 Overall, I am satisfied with my experience learning about electricity from this system.
o2 Working in this learning environment was just like working one-on-one with a human tutor.
o3 I would have preferred to learn about electricity in a different way.
o4 I would use this system again in the future to continue to learn about electricity.
o5 I would like to be able to use a system like this to learn about other topics in the future.
171
C REVU-IT questions related to GUI and reading material (mentioned but not analyzed
in the paper)
sl1 It was easy to navigate through the slides.
sl2 It took a long time for each new slide to be displayed.
sl3 The material on the slides was easy to understand.
sl4 The material on the slides was poorly written.
sl5 I would have benefited from more instrucion on how to move through the slides.
sl6 The material on the slides was interesting.
sl7 The slide navigation buttons didn?t always work the way I expected them to.
sl8 The slides were annoying.
sl9 The material on the slides was written at a level far beneath my abilities.
sl10 I would prefer reading a text book over reading these slides.
e1 I found it difficult to learn how to build circuits and take measurements in the workspace.
e2 Completing exercises in the workspace was fun.
e3 Before beginning the lesson, I received the right amount of instruction on how to build circuits in
the workspace and take measurements.
e4 The exercises were well designed to illustrate the important lesson concepts.
e5 Sometimes I didn?t understand what I was supposed to do for an exercise.
e6 The method for connecting components with wires was counter-intuitive.
e7 Having to build all those circuits was annoying.
e8 I always knew exactly what to build and/or measure in the workspace, and how to do it.
e9 Circuits loaded quickly.
e10 Even if I didn?t predict the outcome correctly ahead of time, once I completed an exercise, I
always understood the point.
e11 It was easy to use the meter.
e12 There were more exercises than necessary to cover the lesson topics.
e13 I would have learned more if I had been able to build circuits with actual light bulbs and batteries.
172
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 338?340,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
BEETLE II: an adaptable tutorial dialogue system
Myroslava O. Dzikovska and Amy Isard and Peter Bell and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore,amy.isard,peter.bell}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{gwendolyn.campbell,natalie.steihauser}@navy.mil
Abstract
We present BEETLE II, a tutorial dialogue sys-
tem which accepts unrestricted language in-
put and supports experimentation with differ-
ent dialogue strategies. Our first system eval-
uation compared two dialogue policies. The
resulting corpus was used to study the impact
of different tutoring and error recovery strate-
gies on user satisfaction and student interac-
tion style. It can also be used in the future to
study a wide range of research issues in dia-
logue systems.
1 Introduction
There has recently been much interest in develop-
ing tutorial dialogue systems that understand student
explanations (Graesser et al, 1999; Aleven et al,
2001; Nielsen et al, 2008; VanLehn et al, 2007),
because it has been shown that high percentages of
self-explanation and student contentful talk are cor-
related with better learning in human-human tutor-
ing (Chi et al, 1994; Litman et al, 2009). How-
ever, most existing systems use pre-authored tutor
responses for addressing student errors. The advan-
tage of this approach is that tutors can devise reme-
diation dialogues that are highly tailored to specific
misconceptions, providing step-by-step scaffolding
and potentially suggesting additional exercises. The
disadvantage is a lack of adaptivity and generality:
students often get the same remediation for the same
error regardless of their past performance or dia-
logue context. It also becomes more difficult to ex-
periment with different dialogue policies (including
error recovery and tutorial policies determining the
most appropriate feedback), due to the complexities
in applying tutoring strategies consistently in a large
number of hand-authored remediations.
The BEETLE II system architecture is designed to
overcome these limitations (Callaway et al, 2007).
It uses a deep parser and generator, together with
a domain reasoner and a diagnoser, to produce de-
tailed analyses of student utterances and to generate
feedback automatically. This allows the system to
consistently apply the same tutorial policy across a
range of questions. The system?s modular setup and
extensibility also make it a suitable testbed for both
computational linguistics algorithms and more gen-
eral questions about theories of learning.
The system is based on an introductory electric-
ity and electronics course developed by experienced
instructional designers, originally created for use in
a human-human tutoring study. The exercises were
then transferred into a computer system with only
minor adjustments (e.g., breaking down compound
questions into individual questions). This resulted
in a realistic tutoring setup, which presents interest-
ing challenges to language processing components,
involving a wide variety of language phenomena.
We demonstrate a version of the system that un-
derwent a user evaluation in 2009, which found sig-
nificant learning gains for students interacting with
the system. The experimental data collection com-
pared two different dialogue policies implemented
in the system, and resulted in a corpus supporting
research into a variety of questions about human-
computer dialogue interaction (Dzikovska et al,
2010a).
338
Figure 1: Screenshot of the BEETLE II system
2 Example Interaction
The BEETLE II system delivers basic electricity and
electronics tutoring to students with no prior knowl-
edge of the subject. A screenshot is shown in Figure
1. The student interface includes an area to display
reading material, a circuit simulator, and a dialogue
history window. Currently all interactions with the
system are typed. Students read pre-authored cur-
riculum slides and carry out exercises which in-
volve experimenting with the circuit simulator and
explaining the observed behaviour. The system also
asks some high-level questions, such as ?What is
voltage??.
An example dialogue with the system, taken from
the evaluation corpus, is shown in Figure 2. It shows
three key system properties: after the student?s first
turn, which was correct but incomplete, the system
rephrases the correct part of the student answer and
prompts for the missing information. In the second
turn, the system cannot interpret the student utter-
ance, so it responds with a targeted help message
and a hint about the object that needs to be men-
tioned. Finally, in the last turn the system com-
bines the information from the tutor?s hint and the
student?s answers and restates the complete answer
since the current answer was completed over multi-
ple turns.
3 Data Analysis and Future Work
The data collected with the BEETLE II system has
been used to investigate several research questions
regarding discourse and dialogue: the effectiveness
of different error recovery strategies (Dzikovska et
al., 2010b); the underlying dimensions of user sat-
isfaction and their relationship with learning gain
(Dzikovska et al, 2011); the relationship between
(student) alignment in dialogue and learning gain
(Steinhauser et al, 2011); and the differences be-
tween students? social and metacognitive statements
depending on the interaction style (Dzikovska et al,
2010a). We are currently annotating the data with
additional interaction parameters, including correct-
ness of student answers and appropriateness of sys-
tem hints. This will allow us to apply PARADISE
339
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I?m sorry, I?m having a problem un-
derstanding. I don?t understand the
word ?effect? here. Here?s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
Tutor: You?re on the right track. Here?s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 2: Example interaction with the system from our
corpus
methodology to get additional insight into which pa-
rameters of tutorial dialogue affect learning gain and
user satisfaction.
We are also adding a speech interface to the sys-
tem, which will open new and interesting research
questions. Students often fail to use domain termi-
nology correctly, and in the tutoring domain it is
important to teach students to use proper terminol-
ogy, so incorrect usage must be detected and cor-
rected. This means that grammar-based language
models are not appropriate for the language mod-
elling, and opens new questions about robust ASR
and language interpretation in such domains.
Acknowledgements
This work has been supported in part by US Of-
fice of Naval Research grants N000141010085 and
N0001410WX20278. We thank Katherine Harrison,
Leanne Taylor, Charles Scott, Simon Caine, Charles
Callaway and Elaine Farrow for their contributions
to this effort.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Charles B. Callaway, Myroslava Dzikovska, Elaine Far-
row, Manuel Marques-Pita, Colin Matheson, and Jo-
hanna D. Moore. 2007. The Beetle and BeeDiff tutor-
ing systems. In Proceedings of SLaTE?07 (Speech and
Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Myroslava Dzikovska, Natalie B. Steinhauser, Jo-
hanna D. Moore, Gwendolyn E. Campbell, Kather-
ine M. Harrison, and Leanne S. Taylor. 2010a. Con-
tent, social, and metacognitive statements: An em-
pirical study comparing human-human and human-
computer tutorial dialogue. In Proceedings of ECTEL-
2010, pages 93?108.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010b. The
impact of interpretation problems on tutorial dialogue.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics(ACL-2010).
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2011. Ex-
ploring user satisfaction in a tutorial dialogue system.
In Proceedings of the 12th annual SIGdial Meeting on
Discourse and Dialogue.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proc. of 14th International
Conference on Artificial Intelligence in Education.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Learning to assess low-level conceptual under-
standing. In Proceedings 21st International FLAIRS
Conference, Coconut Grove, Florida, May.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proceed-
ings of the 15th International Conference on Artificial
Intelligence in Education (AIED-2011).
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technol-
ogy in Education, Farmington, PA, October.
340
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 293?299,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Improving interpretation robustness in a tutorial dialogue system
Myroslava O. Dzikovska and Elaine Farrow and Johanna D. Moore
School of Informatics, University of Edinburgh
Edinburgh, EH8 9AB, United Kingdom
{m.dzikovska,elaine.farrow,j.moore}@ed.ac.uk
Abstract
We present an experiment aimed at improv-
ing interpretation robustness of a tutorial dia-
logue system that relies on detailed semantic
interpretation and dynamic natural language
feedback generation. We show that we can
improve overall interpretation quality by com-
bining the output of a semantic interpreter
with that of a statistical classifier trained on
the subset of student utterances where seman-
tic interpretation fails. This improves on a pre-
vious result which used a similar approach but
trained the classifier on a substantially larger
data set containing all student utterances. Fi-
nally, we discuss how the labels from the sta-
tistical classifier can be integrated effectively
with the dialogue system?s existing error re-
covery policies.
1 Introduction
Giving students formative feedback as they inter-
act with educational applications, such as simu-
lated training environments, problem-solving tutors,
serious games, and exploratory learning environ-
ments, is known to be important for effective learn-
ing (Shute, 2008). Suitable feedback can include
context-appropriate confirmations, hints, and sug-
gestions to help students refine their answers and
increase their understanding of the subject. Pro-
viding this type of feedback automatically, in nat-
ural language, is the goal of tutorial dialogue sys-
tems (Aleven et al, 2002; Dzikovska et al, 2010b;
Graesser et al, 1999; Jordan et al, 2006; Litman and
Silliman, 2004; Khuwaja et al, 1994; Pon-Barry et
al., 2004; VanLehn et al, 2007).
Much work in NLP for educational applications
has focused on automated answer grading (Leacock
and Chodorow, 2003; Pulman and Sukkarieh, 2005;
Mohler et al, 2011). Automated answer assess-
ment systems are commonly trained on large text
corpora. They compare the text of a student answer
with the text of one or more reference answers sup-
plied by human instructors and calculate a score re-
flecting the quality of the match. Automated grad-
ing methods are integrated into intelligent tutoring
systems (ITS) by having system developers antic-
ipate both correct and incorrect responses to each
question, with the system choosing the best match
(Graesser et al, 1999; Jordan et al, 2006; Litman
and Silliman, 2004; VanLehn et al, 2007). Such
systems have wide domain coverage and are robust
to ill-formed input. However, as matching relies on
shallow features and does not provide semantic rep-
resentations of student answers, this approach is less
suitable for dynamically generating adaptive natural
language feedback (Dzikovska et al, 2013).
Real-time simulations and serious games are
commonly used in STEM learning environments
to increase student engagement and support ex-
ploratory learning (Rutten et al, 2012; Mayo, 2007).
Natural language dialogue can help improve learn-
ing in such systems by asking students to explain
their reasoning, either directly during interaction, or
during post-problem reflection (Aleven et al, 2002;
Pon-Barry et al, 2004; Dzikovska et al, 2010b).
Interpretation of student answers in such systems
needs to be grounded in the current state of a dynam-
ically changing environment, and feedback may also
be generated dynamically to reflect the changing
system state. This is typically achieved by employ-
ing hand-crafted parsers and semantic interpreters to
produce structured semantic representations of stu-
dent input, which are then used to instantiate ab-
293
stract tutorial strategies with the help of a natural
language generation system (Freedman, 2000; Clark
et al, 2005; Dzikovska et al, 2010b).
Rule-based semantic interpreters are known to
suffer from robustness and coverage problems, fail-
ing to interpret out-of-grammar student utterances.
In the event of an interpretation failure, most sys-
tems have little information on which to base a feed-
back decision and typically respond by asking the
student to rephrase, or simply give away the answer
(though more sophisticated strategies are sometimes
possible, see Section 4). While statistical scoring ap-
proaches are more robust, they may still suffer from
coverage issues when system designers fail to antic-
ipate the full range of expected student answers. In
one study of a statistical system, a human judge la-
beled 33% of student utterances as not matching any
of the anticipated responses, meaning that the sys-
tem had no information to use as a basis for choos-
ing the next action and fell back on a single strategy,
giving away the answer (Jordan et al, 2009).
Recently, Dzikovska et al (2012b) developed an
annotated corpus of student responses (henceforth,
the SRA corpus) with the goal of facilitating dy-
namic generation of tutorial feedback.1 Student re-
sponses are assigned to one of 5 domain- and task-
independent classes that correspond to typical flaws
found in student answers. These classes can be used
to help a system choose a feedback strategy based
only on the student answer and a single reference
answer. Dzikovska et al (2013) showed that a sta-
tistical classifier trained on this data set can be used
in combination with a semantic interpreter to sig-
nificantly improve the overall quality of natural lan-
guage interpretation in a dialogue-based ITS. The
best results were obtained by using the classifier
to label the utterances that the semantic interpreter
failed to process.
In this paper we further extend this result by
showing that we can obtain similar results by train-
ing the classifier directly on the subset of utterances
that cannot be processed by the interpreter. The
distribution of labels across the classes is differ-
ent in this subset compared to the rest of the cor-
pus. Therefore we can train a subset-specific classi-
1http://www.cs.york.ac.uk/semeval-2013/
task7/index.php?id=data
fier, reducing the amount of annotated training data
needed without compromising performance of the
combined system.
The rest of the paper is organized as follows. In
Section 2 we describe an architecture for combining
semantic interpretation and classification in a sys-
tem with dynamic natural language feedback gener-
ation. In Section 3 we describe an experiment to im-
prove combined system performance using a classi-
fier trained only on non-interpretable utterances. We
discuss future improvements in Section 4.
2 Background
The SRA corpus is made up of two subsets: (1)
the SciEntsBank subset, consisting of written re-
sponses to assessment questions (Nielsen et al,
2008b), and (2) the Beetle subset consisting of ut-
terances collected from student interactions with the
BEETLE II tutorial dialogue system (Dzikovska et
al., 2010b). The SRA corpus annotation scheme
defines 5 classes of student answers (?correct?,
?partially-correct-incomplete?, ?contradictory?, ?ir-
relevant? and ?non-domain?). Each utterance is as-
signed to one of the 5 classes based on pre-existing
manual annotations (Dzikovska et al, 2012b).
We focus on the Beetle subset because the Beetle
data comes from an implemented system, meaning
that we also have access to the semantic interpreta-
tions of student utterances produced by the BEETLE
II interpretation component. The system uses fine-
grained semantic analysis to produce detailed diag-
noses of student answers in terms of correct, incor-
rect, missing and irrelevant parts. We developed a
set of rules to map these diagnoses onto the SRA
corpus 5-class annotation scheme to support system
evaluation (Dzikovska et al, 2012a).
In our previous work (Dzikovska et al, 2013), we
used this mapping as the basis for combining the
output of the BEETLE II semantic interpreter with
the output of a statistical classifier, using a rule-
based policy to determine which label to use for
each instance. If the label from the semantic in-
terpreter is chosen, then the full range of detailed
feedback strategies can be used, based on the corre-
sponding semantic representation. If the classifier?s
label is chosen, then the system can fall back to us-
ing content-free prompts, choosing an appropriate
294
prompt based on the SRA corpus label.
We evaluated 3 rule-based combination policies,
chosen to reduce the effects of the errors that the
semantic interpreter makes, and taking into account
tutoring goals such as reducing student frustration.
The best performing policy takes the classifier?s out-
put if and only if the semantic interpreter is unable
to process the utterance.2 This allows the system to
choose from a wider set of content-free prompts in-
stead of always telling the student that the utterance
was not understood.
As discussed earlier, non-interpretable utterances
present a problem for both rule-based and statistical
approaches. Therefore, we carried out an additional
set of experiments, focusing on the performance of
system combinations that use policies designed to
address non-interpretable utterances. We discuss our
results and future directions in the rest of the paper.
3 Improving Interpretation Robustness
3.1 Experimental Setup
The Beetle portion of the SRA corpus contains 3941
unique student answers to 47 different explanation
questions. Each question is associated with one or
more reference answers provided by expert tutors,
and each student answer is manually annotated with
the label assigned by the BEETLE II interpreter and
a gold-standard correctness label.
In our experiments, we follow the procedure de-
scribed in (Dzikovska et al, 2013), using 10-fold
cross-validation to evaluate the performance of the
various stand-alone and combined systems. We re-
port the per-class F1 scores as evaluation metrics,
using the macro-averaged F1 score as the primary
evaluation metric.
Dzikovska et al (2013) used a statistical classi-
fier based on lexical overlap, taken from (Dzikovska
et al, 2012a), and evaluated 3 different rule-based
policies for combining its output with that of the se-
mantic interpreter. In two of those policies the inter-
preter?s output is always used if it is available, and
the classifier?s label is used for a (subset of) non-
interpretable utterances:
1. NoReject: the classifier?s label is used in all
cases where semantic interpretation fails, thus
2We will refer to such utterances as ?non-interpretable? fol-
lowing (Bohus and Rudnicky, 2005).
creating a system that never rejects student in-
put as non-interpretable
2. NoRejectCorrect: the classifier?s label is
used for non-interpretable utterances which are
labeled as ?correct? by the classifier. This more
conservative policy aims to ensure that correct
student answers are always accepted, but incor-
rect answers may still be rejected with a request
to rephrase.
We conducted a new experiment to evaluate these
two policies together with an enhanced classifier,
discussed in the next section.
3.2 Classifier
For this paper, we extended the classifier from the
previous study (Dzikovska et al, 2013), which we
will call Sim8, with additional features to improve
handling of lexical variability and negation.
Sim8 uses the Weka 3.6.2 implementation of
C4.5 pruned decision trees, with default parameters.
It uses 8 features based on lexical overlap similarity
metrics provided by Perl?s Text::Similarity
package v.0.09: 4 metrics measuring overlap be-
tween the student answer and the expected answer,
and the same 4 metrics applied to the student?s an-
swer and the question text.
In our enhanced classifier, Sim20, we extended
the baseline feature set with 12 additional features.
8 of these are direct analogs of the baseline features,
this time computed on the stemmed text to reduce
the impact of syntactic variation, using the Porter
stemmer from the Lingua::Stem package.3 In
addition, 4 features were added to improve negation
handling and thus detection of contradictions. These
are:
? QuestionNeg, AnswerNeg: features in-
dicating the presence of a negation marker
in the question and the student?s answer re-
spectively, detected using a regular expression.
We distinguish three cases: a negation marker
3We also experimented with features that involve removing
stop words before computing similarity scores, and with using
SVMs for classification, but failed to obtain better performance.
We continue to investigate different SVM kernels and alterna-
tive classification algorithms such as random forests for our fu-
ture work.
295
Standalone Sem. Interp. + Sim20 Sem. Interp. + Sim20NI
Sem. Interp. Sim8 Sim20 no rej no rej corr no rej no rej corr
correct 0.66 0.71 0.71 0.70 0.70 0.70 0.70
pc inc 0.48 0.38 0.40 0.51 0.48 0.50 0.48
contra 0.27 0.40 0.45 0.47 0.27 0.51 0.27
irrlvnt 0.21 0.05 0.08 0.22 0.21 0.22 0.21
nondom 0.65 0.73 0.78 0.83 0.65 0.83 0.65
macro avg 0.45 0.45 0.48 0.55 0.46 0.55 0.46
Table 1: F1 scores for three stand-alone systems, and for combination systems using the Sim20 and Sim20NI
classifiers together with the semantic interpreter. Stand-alone performance for Sim20NI is not shown since it was
trained only on the non-interpretable data subset and is therefore not applicable for the complete data set.
likely to be associated with domain content
(e.g., ?not connected?); a negation marker more
likely to be associated with general expressions
of confusion (such as ?don?t know?); and no
negation marker present.
? BestOverlapNeg: true if the reference an-
swer that has the highest F1 overlap with the
student answer includes a negation marker.
? BestOverlapPolarityMatch: a flag
computed from the values of AnswerNeg and
BestOverlapNeg. Again, we distinguish
three cases: they have the same polarity (both
the student answer and the reference answer
contain negation markers, or both have no
negation markers); they have opposite polar-
ity; or the student answer contains a negation
marker associated with an expression of confu-
sion, as described above.
3.3 Evaluation
Evaluation results are shown in Table 1. Unless
otherwise specified, all performance differences dis-
cussed in the text are significant on an approximate
randomization significance test with 10,000 itera-
tions (Yeh, 2000).
Adding the new features to create the Sim20
classifier resulted in a performance improvement
compared to the Sim8 classifier, raising macro-
averaged F1 from 0.45 to 0.48, with an improvement
in contradiction detection as intended. But these im-
provements did not translate into improvements in
the combined systems. Combinations using Sim20
performed exactly the same as the combinations us-
ing Sim8 (not shown due to space limitations, see
(Dzikovska et al, 2013)). Clearly, more sophisti-
cated features are needed to obtain further perfor-
mance gains in the combined systems.
However, we noted that the subset of non-
interpretable utterances in the corpus has a differ-
ent distribution of labels compared to the full data
set. In the complete data set, 1665 utterances (42%)
are labeled as correct and 1049 (27%) as contradic-
tory. Among the 1416 utterances considered non-
interpretable by the semantic interpreter, 371 (26%)
belong to the ?correct? class, and 598 (42%) to ?con-
tradictory? (other classes have similar distributions
in both subsets). We therefore hypothesized that a
combination system that uses the classifier output
only if an utterance is non-interpretable, may ben-
efit from employing a classifier trained specifically
on this subset rather than on the whole data set.
If our hypothesis is true, it offers an interesting
possibility for combining rule-based and statistical
classifiers in similar setups: if the classifier can be
trained using only the examples that are problematic
for the rule-based system, it can provide improved
robustness at a significantly lower annotation cost.
We therefore trained another classifier,
Sim20NI, using the same feature set as Sim20,
but this time using only the instances rejected
as non-interpretable by the semantic interpreter
in each cross-validation fold (1416 utterances,
36% of all data instances). We again used the
NoReject and NoRejectCorrect policies to
combine the output of Sim20NI with that of the
semantic interpreter. Evaluation results confirmed
our hypothesis. The system combinations that
use Sim20 and Sim20NI perform identically on
296
macro-averaged F1, with NoReject being the best
combination policy in both cases and significantly
outperforming the semantic interpreter alone. How-
ever, the Sim20NI classifier has the advantage of
needing significantly less annotated data to achieve
this performance.
4 Discussion and Future Work
Our research focuses on combining deep and shal-
low processing by supplementing fine-grained se-
mantic interpretations from a rule-based system
with more coarse-grained classification labels. Al-
ternatively, we could try to learn structured se-
mantic representations from annotated text (Zettle-
moyer and Collins, 2005; Wong and Mooney, 2007;
Kwiatkowski et al, 2010), or to learn more fine-
grained assessment labels (Nielsen et al, 2008a).
However, such approaches require substantially
larger annotation effort. Therefore, we believe it is
worth exploring the use of the simpler 5-label anno-
tation scheme from the SRA corpus. We previously
showed that it is possible to improve system perfor-
mance by combining the output of a symbolic inter-
preter with that of a statistical classifier (Dzikovska
et al, 2013). The best combination policy used the
statistical classifier to label utterances rejected as
non-interpretable by the rule-based interpreter.
In this paper, we showed that similar results can
be achieved by training the classifier only on non-
interpretable utterances, rather than on the whole la-
beled corpus. The student answers that the inter-
preter has difficulty with have a distinct distribution,
which is effectively utilized by training a classifier
only on this subset. This reduces the amount of an-
notated training data needed, reducing the amount of
manual labor required.
In future, we will further investigate the best com-
bination of parsing and statistical classification in
systems that offer sophisticated error recovery poli-
cies for non-understandings. Our top-performing
policy, NoReject, uses deep parsing and semantic
interpretation to produce a detailed semantic analy-
sis for the majority of utterances, and falls back on a
shallower statistical classifier for utterances that are
difficult for the interpreter. This policy assumes that
it is always better to use a content-free prompt than
to reject a non-interpretable student utterance. How-
ever, interpretation problems can arise from incor-
rect uses of terminology, and learning to speak in
the language of the domain has been positively cor-
related with learning outcomes (Steinhauser et al,
2011). Therefore, rejecting some non-interpretable
answers as incorrect could be a valid tutoring strat-
egy (Sagae et al, 2010; Dzikovska et al, 2010a).
The BEETLE II system offers several error re-
covery strategies intended to help students phrase
their answers in more acceptable ways by giving a
targeted help message, e.g., ?I am sorry, I?m hav-
ing trouble understanding. Paths cannot be broken,
only components can be broken? (Dzikovska et al,
2010a). Therefore, it may be worthwhile to con-
sider other combination policies. We evaluated the
NoRejectCorrect policy, which uses the statis-
tical classifier to identify correct answers rejected
by the semantic interpreter and asks for rephrasings
in other cases. Using this policy resulted in only a
small improvement in system performance. A dif-
ferent classifier geared towards more accurate iden-
tification of correct answers may help, and we are
planning to investigate this option in the future.
Alternatively, we could consider a combination
policy which looks for rejected answers that the
classifier identifies as contradictory and changes the
wording of the targeted help message to indicate that
the student may have made a mistake, instead of
apologizing for the misunderstanding. This has the
potential to help students learn correct terminology
rather than presenting the issue as strictly an inter-
pretation failure.
Ultimately, all combination policies must be
tested with users to ensure that improved robust-
ness translates into improved system effectiveness.
We have previously studied the effectiveness of our
targeted help strategies with respect to improving
learning outcomes (Dzikovska et al, 2010a). A sim-
ilar study is required to evaluate our combination
strategies.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine and Sarah Denhe
for help with data collection and preparation. The
research reported here was supported by the US
ONR award N000141010085.
297
References
Vincent Aleven, Octav Popescu, and Kenneth R.
Koedinger. 2002. Pilot-testing a tutorial dialogue sys-
tem that supports self-explanation. In Proc. of ITS-02
conference, pages 344?354.
Dan Bohus and Alexander Rudnicky. 2005. Sorry,
I didn?t catch that! - An investigation of non-
understanding errors and recovery strategies. In Pro-
ceedings of SIGdial-2005, Lisbon, Portugal.
Brady Clark, Oliver Lemon, Alexander Gruenstein, Eliz-
abethOwen Bratt, John Fry, Stanley Peters, Heather
Pon-Barry, Karl Schultz, Zack Thomsen-Gray, and
Pucktada Treeratpituk. 2005. A general purpose ar-
chitecture for intelligent tutoring systems. In JanC.J.
Kuppevelt, Laila Dybkjr, and NielsOle Bernsen, edi-
tors, Advances in Natural Multimodal Dialogue Sys-
tems, volume 30 of Text, Speech and Language Tech-
nology, pages 287?305. Springer Netherlands.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010a. The
impact of interpretation problems on tutorial dialogue.
In Proc. of ACL 2010 Conference Short Papers, pages
43?48.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-
hanna D. Moore. 2012a. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. In Proc. of EACL-12 Confer-
ence, pages 471?481.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012b. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Myroslava O. Dzikovska, Elaine Farrow, and Johanna D.
Moore. 2013. Combining semantic interpretation and
statistical classification for improved explanation pro-
cessing in a tutorial dialogue system. In In Proceed-
ings of the The 16th International Conference on Ar-
tificial Intelligence in Education (AIED 2013), Mem-
phis, TN, USA, July.
Reva Freedman. 2000. Using a reactive planner as the
basis for a dialogue agent. In Proceedings of the Thir-
teenth Florida Artificial Intelligence Research Sympo-
sium (FLAIRS 2000), pages 203?208.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Pamela Jordan, Diane Litman, Michael Lipschultz, and
Joanna Drummond. 2009. Evidence of misunder-
standings in tutorial dialogue and their impact on
learning. In Proc. of 14th International Conference on
Artificial Intelligence in Education, pages 125?132.
Ramzan A. Khuwaja, Martha W. Evens, Joel A. Michael,
and Allen A. Rovick. 1994. Architecture of
CIRCSIM-tutor (v.3): A smart cardiovascular physi-
ology tutor. In Proc. of 7th Annual IEEE Computer-
Based Medical Systems Symposium.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proc. of EMNLP-2010 Conference,
pages 1223?1233.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Diane J. Litman and Scott Silliman. 2004. ITSPOKE:
an intelligent tutoring spoken dialogue system. In
Demonstration Papers at HLT-NAACL 2004, pages 5?
8, Boston, Massachusetts.
Merrilea J. Mayo. 2007. Games for science and engi-
neering education. Commun. ACM, 50(7):30?35, July.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
298
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Nico Rutten, Wouter R. van Joolingen, and Jan T. van der
Veen. 2012. The learning effects of computer simula-
tions in science education. Computers and Education,
58(1):136 ? 153.
Alicia Sagae, W. Lewis Johnson, and Stephen Bodnar.
2010. Validation of a dialog system for language
learners. In Proceedings of the 11th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
SIGDIAL ?10, pages 241?244, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Valerie J Shute. 2008. Focus on formative feedback.
Review of educational research, 78(1):153?189.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proc. of
15th international conference on Artificial Intelligence
in Education, pages 361?368.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2007), Prague, Czech Republic, June.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Grammars.
In Proceedings of the 21th Annual Conference on
Uncertainty in Artificial Intelligence (UAI-05), pages
658?666, Arlington, Virginia. AUAI Press.
299
