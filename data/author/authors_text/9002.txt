Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81?88,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Detecting Erroneous Sentences using Automatically Mined Sequential
Patterns
Guihua Sun ? Xiaohua Liu Gao Cong Ming Zhou
Chongqing University Microsoft Research Asia
sunguihua5018@163.com {xiaoliu, gaocong, mingzhou}@microsoft.com
Zhongyang Xiong John Lee ? Chin-Yew Lin
Chongqing University MIT Microsoft Research Asia
zyxiong@cqu.edu.cn jsylee@mit.edu cyl@microsoft.com
Abstract
This paper studies the problem of identify-
ing erroneous/correct sentences. The prob-
lem has important applications, e.g., pro-
viding feedback for writers of English as
a Second Language, controlling the quality
of parallel bilingual sentences mined from
the Web, and evaluating machine translation
results. In this paper, we propose a new
approach to detecting erroneous sentences
by integrating pattern discovery with super-
vised learning models. Experimental results
show that our techniques are promising.
1 Introduction
Detecting erroneous/correct sentences has the fol-
lowing applications. First, it can provide feedback
for writers of English as a Second Language (ESL)
as to whether a sentence contains errors. Second, it
can be applied to control the quality of parallel bilin-
gual sentences mined from the Web, which are criti-
cal sources for a wide range of applications, such as
statistical machine translation (Brown et al, 1993)
and cross-lingual information retrieval (Nie et al,
1999). Third, it can be used to evaluate machine
translation results. As demonstrated in (Corston-
Oliver et al, 2001; Gamon et al, 2005), the better
human reference translations can be distinguished
from machine translations by a classification model,
the worse the machine translation system is.
?Work done while the author was a visiting student at MSRA
?Work done while the author was a visiting student at MSRA
The previous work on identifying erroneous sen-
tences mainly aims to find errors from the writing of
ESL learners. The common mistakes (Yukio et al,
2001; Gui and Yang, 2003) made by ESL learners
include spelling, lexical collocation, sentence struc-
ture, tense, agreement, verb formation, wrong Part-
Of-Speech (POS), article usage, etc. The previous
work focuses on grammar errors, including tense,
agreement, verb formation, article usage, etc. How-
ever, little work has been done to detect sentence
structure and lexical collocation errors.
Some methods of detecting erroneous sentences
are based on manual rules. These methods (Hei-
dorn, 2000; Michaud et al, 2000; Bender et al,
2004) have been shown to be effective in detect-
ing certain kinds of grammatical errors in the writ-
ing of English learners. However, it could be ex-
pensive to write rules manually. Linguistic experts
are needed to write rules of high quality; Also, it
is difficult to produce and maintain a large num-
ber of non-conflicting rules to cover a wide range of
grammatical errors. Moreover, ESL writers of differ-
ent first-language backgrounds and skill levels may
make different errors, and thus different sets of rules
may be required. Worse still, it is hard to write rules
for some grammatical errors, for example, detecting
errors concerning the articles and singular plural us-
age (Nagata et al, 2006).
Instead of asking experts to write hand-crafted
rules, statistical approaches (Chodorow and Lea-
cock, 2000; Izumi et al, 2003; Brockett et al, 2006;
Nagata et al, 2006) build statistical models to iden-
tify sentences containing errors. However, existing
81
statistical approaches focus on some pre-defined er-
rors and the reported results are not attractive. More-
over, these approaches, e.g., (Izumi et al, 2003;
Brockett et al, 2006) usually need errors to be spec-
ified and tagged in the training sentences, which re-
quires expert help to be recruited and is time con-
suming and labor intensive.
Considering the limitations of the previous work,
in this paper we propose a novel approach that is
based on pattern discovery and supervised learn-
ing to successfully identify erroneous/correct sen-
tences. The basic idea of our approach is to build
a machine learning model to automatically classify
each sentence into one of the two classes, ?erro-
neous? and ?correct.? To build the learning model,
we automatically extract labeled sequential patterns
(LSPs) from both erroneous sentences and correct
sentences, and use them as input features for classi-
fication models. Our main contributions are:
? We mine labeled sequential patterns(LSPs)
from the preprocessed training data to build
leaning models. Note that LSPs are also very
different from N-gram language models that
only consider continuous sequences.
? We also enrich the LSP features with other auto-
matically computed linguistic features, includ-
ing lexical collocation, language model, syn-
tactic score, and function word density. In con-
trast with previous work focusing on (a spe-
cific type of) grammatical errors, our model can
handle a wide range of errors, including gram-
mar, sentence structure, and lexical choice.
? We empirically evaluate our methods on two
datasets consisting of sentences written by
Japanese and Chinese, respectively. Experi-
mental results show that labeled sequential pat-
terns are highly useful for the classification
results, and greatly outperform other features.
Our method outperforms Microsoft Word03
and ALEK (Chodorow and Leacock, 2000)
from Educational Testing Service (ETS) in
some cases. We also apply our learning model
to machine translation (MT) data as a comple-
mentary measure to evaluate MT results.
The rest of this paper is organized as follows.
The next section discusses related work. Section 3
presents the proposed technique. We evaluate our
proposed technique in Section 4. Section 5 con-
cludes this paper and discusses future work.
2 Related Work
Research on detecting erroneous sentences can be
classified into two categories. The first category
makes use of hand-crafted rules, e.g., template
rules (Heidorn, 2000) and mal-rules in context-free
grammars (Michaud et al, 2000; Bender et al,
2004). As discussed in Section 1, manual rule based
methods have some shortcomings.
The second category uses statistical techniques
to detect erroneous sentences. An unsupervised
method (Chodorow and Leacock, 2000) is em-
ployed to detect grammatical errors by inferring
negative evidence from TOEFL administrated by
ETS. The method (Izumi et al, 2003) aims to de-
tect omission-type and replacement-type errors and
transformation-based leaning is employed in (Shi
and Zhou, 2005) to learn rules to detect errors for
speech recognition outputs. They also require spec-
ifying error tags that can tell the specific errors
and their corrections in the training corpus. The
phrasal Statistical Machine Translation (SMT) tech-
nique is employed to identify and correct writing er-
rors (Brockett et al, 2006). This method must col-
lect a large number of parallel corpora (pairs of er-
roneous sentences and their corrections) and perfor-
mance depends on SMT techniques that are not yet
mature. The work in (Nagata et al, 2006) focuses
on a type of error, namely mass vs. count nouns.
In contrast to existing statistical methods, our tech-
nique needs neither errors tagged nor parallel cor-
pora, and is not limited to a specific type of gram-
matical error.
There are also studies on automatic essay scoring
at document-level. For example, E-rater (Burstein
et al, 1998), developed by the ETS, and Intelligent
Essay Assessor (Foltz et al, 1999). The evaluation
criteria for documents are different from those for
sentences. A document is evaluated mainly by its or-
ganization, topic, diversity of vocabulary, and gram-
mar while a sentence is done by grammar, sentence
structure, and lexical choice.
Another related work is Machine Translation (MT)
evaluation. Classification models are employed
in (Corston-Oliver et al, 2001; Gamon et al, 2005)
82
to evaluate the well-formedness of machine transla-
tion outputs. The writers of ESL and MT normally
make different mistakes: in general, ESL writers can
write overall grammatically correct sentences with
some local mistakes while MT outputs normally pro-
duce locally well-formed phrases with overall gram-
matically wrong sentences. Hence, the manual fea-
tures designed for MT evaluation are not applicable
to detect erroneous sentences from ESL learners.
LSPs differ from the traditional sequential pat-
terns, e.g., (Agrawal and Srikant, 1995; Pei et al,
2001) in that LSPs are attached with class labels and
we prefer those with discriminating ability to build
classification model. In our other work (Sun et al,
2007), labeled sequential patterns, together with la-
beled tree patterns, are used to build pattern-based
classifier to detect erroneous sentences. The clas-
sification method in (Sun et al, 2007) is different
from those used in this paper. Moreover, instead of
labeled sequential patterns, in (Sun et al, 2007) the
most significant k labeled sequential patterns with
constraints for each training sentence are mined to
build classifiers. Another related work is (Jindal and
Liu, 2006), where sequential patterns with labels are
used to identify comparative sentences.
3 Proposed Technique
This section first gives our problem statement and
then presents our proposed technique to build learn-
ing models.
3.1 Problem Statement
In this paper we study the problem of identifying
erroneous/correct sentences. A set of training data
containing correct and erroneous sentences is given.
Unlike some previous work, our technique requires
neither that the erroneous sentences are tagged with
detailed errors, nor that the training data consist of
parallel pairs of sentences (an error sentence and its
correction). The erroneous sentence contains a wide
range of errors on grammar, sentence structure, and
lexical choice. We do not consider spelling errors in
this paper.
We address the problem by building classifica-
tion models. The main challenge is to automatically
extract representative features for both correct and
erroneous sentences to build effective classification
models. We illustrate the challenge with an exam-
ple. Consider an erroneous sentence, ?If Maggie will
go to supermarket, she will buy a bag for you.? It is
difficult for previous methods using statistical tech-
niques to capture such an error. For example, N-
gram language model is considered to be effective
in writing evaluation (Burstein et al, 1998; Corston-
Oliver et al, 2001). However, it becomes very ex-
pensive if N > 3 and N-grams only consider contin-
uous sequence of words, which is unable to detect
the above error ?if...will...will?.
We propose labeled sequential patterns to effec-
tively characterize the features of correct and er-
roneous sentences (Section 3.2), and design some
complementary features ( Section 3.3).
3.2 Mining Labeled Sequential Patterns ( LSP )
Labeled Sequential Patterns (LSP). A labeled se-
quential pattern, p, is in the form of LHS? c, where
LHS is a sequence and c is a class label. Let I be a
set of items and L be a set of class labels. Let D be a
sequence database in which each tuple is composed
of a list of items in I and a class label in L. We say
that a sequence s1 =< a1, ..., am > is contained in
a sequence s2 =< b1, ..., bn > if there exist integers
i1, ...im such that 1 ? i1 < i2 < ... < im ? n and
aj = bij for all j ? 1, ...,m. Similarly, we say that
a LSP p1 is contained by p2 if the sequence p1.LHS
is contained by p2.LHS and p1.c = p2.c. Note that
it is not required that s1 appears continuously in s2.
We will further refine the definition of ?contain? by
imposing some constraints (to be explained soon).
A LSP p is attached with two measures, support and
confidence. The support of p, denoted by sup(p),
is the percentage of tuples in database D that con-
tain the LSP p. The probability of the LSP p being
true is referred to as ?the confidence of p ?, denoted
by conf(p), and is computed as sup(p)sup(p.LHS) . The
support is to measure the generality of the pattern p
and minimum confidence is a statement of predictive
ability of p.
Example 1: Consider a sequence database contain-
ing three tuples t1 = (< a, d, e, f >,E), t2 = (<
a, f, e, f >,E) and t3 = (< d, a, f >,C). One
example LSP p1 = < a, e, f >? E, which is con-
tained in tuples t1 and t2. Its support is 66.7% and
its confidence is 100%. As another example, LSP p2
83
= < a, f >? E with support 66.7% and confidence
66.7%. p1 is a better indication of class E than p2.
2
Generating Sequence Database. We generate the
database by applying Part-Of-Speech (POS) tagger
to tag each training sentence while keeping func-
tion words1 and time words2. After the process-
ing, each sentence together with its label becomes
a database tuple. The function words and POS tags
play important roles in both grammars and sentence
structures. In addition, the time words are key
clues in detecting errors of tense usage. The com-
bination of them allows us to capture representative
features for correct/erroneous sentences by mining
LSPs. Some example LSPs include ?<a, NNS> ?
Error?(singular determiner preceding plural noun),
and ?<yesterday, is>?Error?. Note that the con-
fidences of these LSPs are not necessary 100%.
First, we use MXPOST-Maximum Entropy Part of
Speech Tagger Toolkit3 for POS tags. The MXPOST
tagger can provide fine-grained tag information. For
example, noun can be tagged with ?NN?(singular
noun) and ?NNS?(plural noun); verb can be tagged
with ?VB?, ?VBG?, ?VBN?, ?VBP?, ?VBD? and
?VBZ?. Second, the function words and time words
that we use form a key word list. If a word in a
training sentence is not contained in the key word
list, then the word will be replaced by its POS. The
processed sentence consists of POS and the words of
key word list. For example, after the processing, the
sentence ?In the past, John was kind to his sister? is
converted into ?In the past, NNP was JJ to his NN?,
where the words ?in?, ?the?, ?was?, ?to? and ?his?
are function words, the word ?past? is time word,
and ?NNP?, ?JJ?, and ?NN? are POS tags.
Mining LSPs. The length of the discovered LSPs
is flexible and they can be composed of contiguous
or distant words/tags. Existing frequent sequential
pattern mining algorithms (e.g. (Pei et al, 2001))
use minimum support threshold to mine frequent se-
quential patterns whose support is larger than the
threshold. These algorithms are not sufficient for our
problem of mining LSPs. In order to ensure that all
our discovered LSPs are discriminating and are capa-
1http://www.marlodge.supanet.com/museum/funcword.html
2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html
3http://www.cogsci.ed.ac.uk/?jamesc/taggers/MXPOST.html
ble of predicting correct or erroneous sentences, we
impose another constraint minimum confidence. Re-
call that the higher the confidence of a pattern is, the
better it can distinguish between correct sentences
and erroneous sentences. In our experiments, we
empirically set minimum support at 0.1% and mini-
mum confidence at 75%.
Mining LSPs is nontrivial since its search space
is exponential, althought there have been a host of
algorithms for mining frequent sequential patterns.
We adapt the frequent sequence mining algorithm
in (Pei et al, 2001) for mining LSPs with constraints.
Converting LSPs to Features. Each discovered LSP
forms a binary feature as the input for classification
model. If a sentence includes a LSP, the correspond-
ing feature is set at 1.
The LSPs can characterize the correct/erroneous
sentence structure and grammar. We give some ex-
amples of the discovered LSPs. (1) LSPs for erro-
neous sentences. For example, ?<this, NNS>?(e.g.
contained in ?this books is stolen.?), ?<past,
is>?(e.g. contained in ?in the past, John is kind to
his sister.?), ?<one, of, NN>?(e.g. contained in ?it is
one of important working language?, ?<although,
but>?(e.g. contained in ?although he likes it, but
he can?t buy it.?), and ?<only, if, I, am>?(e.g. con-
tained in ?only if my teacher has given permission,
I am allowed to enter this room?). (2) LSPs for cor-
rect sentences. For instance, ?<would, VB>?(e.g.
contained in ?he would buy it.?), and ?<VBD,
yeserday>?(e.g. contained in ?I bought this book
yesterday.?).
3.3 Other Linguistic Features
We use some linguistic features that can be com-
puted automatically as complementary features.
Lexical Collocation (LC) Lexical collocation er-
ror (Yukio et al, 2001; Gui and Yang, 2003) is com-
mon in the writing of ESL learners, such as ?strong
tea? but not ?powerful tea.? Our LSP features can-
not capture all LCs since we replace some words
with POS tags in mining LSPs. We collect five types
of collocations: verb-object, adjective-noun, verb-
adverb, subject-verb, and preposition-object from a
general English corpus4. Correct LCs are collected
4The general English corpus consists of about 4.4 million
native sentences.
84
by extracting collocations of high frequency from
the general English corpus. Erroneous LC candi-
dates are generated by replacing the word in correct
collocations with its confusion words, obtained from
WordNet, including synonyms and words with sim-
ilar spelling or pronunciation. Experts are consulted
to see if a candidate is a true erroneous collocation.
We compute three statistical features for each sen-
tence below. (1) The first feature is computed by
m?
i=1
p(coi)/n, where m is the number of CLs, n is
the number of collocations in each sentence, and
probability p(coi) of each CL coi is calculated us-
ing the method (Lu? and Zhou, 2004). (2) The sec-
ond feature is computed by the ratio of the number
of unknown collocations (neither correct LCs nor er-
roneous LCs) to the number of collocations in each
sentence. (3) The last feature is computed by the ra-
tio of the number of erroneous LCs to the number of
collocations in each sentence.
Perplexity from Language Model (PLM) Perplex-
ity measures are extracted from a trigram language
model trained on a general English corpus using
the SRILM-SRI Language Modeling Toolkit (Stolcke,
2002). We calculate two values for each sentence:
lexicalized trigram perplexity and part of speech
(POS) trigram perplexity. The erroneous sentences
would have higher perplexity.
Syntactic Score (SC) Some erroneous sentences of-
ten contain words and concepts that are locally cor-
rect but cannot form coherent sentences (Liu and
Gildea, 2005). To measure the coherence of sen-
tences, we use a statistical parser Toolkit (Collins,
1997) to assign each sentence a parser?s score that
is the related log probability of parsing. We assume
that erroneous sentences with undesirable sentence
structures are more likely to receive lower scores.
Function Word Density (FWD) We consider the
density of function words (Corston-Oliver et al,
2001), i.e. the ratio of function words to content
words. This is inspired by the work (Corston-Oliver
et al, 2001) showing that function word density can
be effective in distinguishing between human refer-
ences and machine outputs. In this paper, we calcu-
late the densities of seven kinds of function words 5
5including determiners/quantifiers, all pronouns, different
pronoun types: Wh, 1st, 2nd, and 3rd person pronouns, prepo-
Dataset Type Source Number
JC
(+) the Japan Times newspaperand Model English Essay 16,857
(-)
HEL (Hiroshima English
Learners? Corpus) and JLE
(Japanese Learners of En-
glish Corpus)
17,301
CC (+) the 21st Century newspaper 3,200
(-)
CLEC (Chinese Learner Er-
ror Corpus) 3,199
Table 1: Corpora ((+): correct; (-): erroneous)
respectively as 7 features.
4 Experimental Evaluation
We evaluated the performance of our techniques
with support vector machine (SVM) and Naive
Bayesian (NB) classification models. We also com-
pared the effectiveness of various features. In ad-
dition, we compared our technique with two other
methods of checking errors, Microsoft Word03 and
ALEK method (Chodorow and Leacock, 2000). Fi-
nally, we also applied our technique to evaluate the
Machine Translation outputs.
4.1 Experimental Setup
Classification Models. We used two classification
models, SVM6 and NB classification model.
Data. We collected two datasets from different do-
mains, Japanese Corpus (JC) and Chinese Corpus
(CC). Table 1 gives the details of our corpora. In
the learner?s corpora, all of the sentences are erro-
neous. Note that our data does not consist of parallel
pairs of sentences (one error sentence and its correc-
tion). The erroneous sentences includes grammar,
sentence structure and lexical choice errors, but not
spelling errors.
For each sentence, we generated five kinds of fea-
tures as presented in Section 3. For a non-binary
feature X , its value x is normalized by z-score,
norm(x) = x?mean(X)?var(X) , where mean(x) is the em-
pirical mean of X and var(X) is the variance of X .
Thus each sentence is represented by a vector.
Metrics We calculated the precision, recall,
and F-score for correct and erroneous sentences,
respectively, and also report the overall accuracy.
sitions and adverbs, auxiliary verbs, and conjunctions.
6http://svmlight.joachims.org/
85
All the experimental results are obtained thorough
10-fold cross-validation.
4.2 Experimental Results
The Effectiveness of Various Features. The exper-
iment is to evaluate the contribution of each feature
to the classification. The results of SVM are given in
Table 2. We can see that the performance of labeled
sequential patterns (LSP) feature consistently out-
performs those of all the other individual features. It
also performs better even if we use all the other fea-
tures together. This is because other features only
provide some relatively abstract and simple linguis-
tic information, whereas the discovered LSP s char-
acterize significant linguistic features as discussed
before. We also found that the results of NB are a
little worse than those of SVM. However, all the fea-
tures perform consistently on the two classification
models and we can observe the same trend. Due to
space limitation, we do not give results of NB.
In addition, the discovered LSPs themselves are
intuitive and meaningful since they are intuitive fea-
tures that can distinguish correct sentences from er-
roneous sentences. We discovered 6309 LSPs in
JC data and 3742 LSPs in CC data. Some exam-
ple LSPs discovered from erroneous sentences are
<a, NNS> (support:0.39%, confidence:85.71%),
<to, VBD> (support:0.11%, confidence:84.21%),
and <the, more, the, JJ> (support:0.19%, confi-
dence:0.93%) 7; Similarly, we also give some exam-
ple LSPs mined from correct sentences: <NN, VBZ>
(support:2.29%, confidence:75.23%), and <have,
VBN, since> (support:0.11%, confidence:85.71%)
8. However, other features are abstract and it is hard
to derive some intuitive knowledge from the opaque
statistical values of these features.
As shown in Table 2, our technique achieves
the highest accuracy, e.g. 81.75% on the Japanese
dataset, when we use all the features. However, we
also notice that the improvement is not very signif-
icant compared with using LSP feature individually
(e.g. 79.63% on the Japanese dataset). The similar
results are observed when we combined the features
PLM, SC, FWD, and LC. This could be explained
7a + plural noun; to + past tense format; the more + the +
base form of adjective
8singular or mass noun + the 3rd person singular present
format; have + past participle format + since
by two reasons: (1) A sentence may contain sev-
eral kinds of errors. A sentence detected to be er-
roneous by one feature may also be detected by an-
other feature; and (2) Various features give conflict-
ing results. The two aspects suggest the directions
of our future efforts to improve the performance of
our models.
Comparing with Other Methods. It is difficult
to find benchmark methods to compare with our
technique because, as discussed in Section 2, exist-
ing methods often require error tagged corpora or
parallel corpora, or focus on a specific type of er-
rors. In this paper, we compare our technique with
the grammar checker of Microsoft Word03 and the
ALEK (Chodorow and Leacock, 2000) method used
by ETS. ALEK is used to detect inappropriate usage
of specific vocabulary words. Note that we do not
consider spelling errors. Due to space limitation, we
only report the precision, recall, F-score
for erroneous sentences, and the overall accuracy.
As can be seen from Table 3, our method out-
performs the other two methods in terms of over-
all accuracy, F-score, and recall, while the three
methods achieve comparable precision. We realize
that the grammar checker of Word is a general tool
and the performance of ALEK (Chodorow and Lea-
cock, 2000) can be improved if larger training data is
used. We found that Word and ALEK usually cannot
find sentence structure and lexical collocation errors,
e.g., ?The more you listen to English, the easy it be-
comes.? contains the discovered LSP <the, more, the,
JJ>? Error.
Cross-domain Results. To study the performance
of our method on cross-domain data from writers
of the same first-language background, we collected
two datasets from Japanese writers, one is composed
of 694 parallel sentences (+:347, -:347), and the
other 1,671 non-parallel sentences (+:795, -:876).
The two datasets are used as test data while we use
JC dataset for training. Note that the test sentences
come from different domains from the JC data. The
results are given in the first two rows of Table 4. This
experiment shows that our leaning model trained for
one domain can be effectively applied to indepen-
dent data in the other domains from the writes of the
same first-language background, no matter whether
the test data is parallel or not. We also noticed that
86
Dataset Feature A (-)F (-)R (-)P (+)F (+)R (+)P
JC
LSP 79.63 80.65 85.56 76.29 78.49 73.79 83.85
LC 69.55 71.72 77.87 66.47 67.02 61.36 73.82
PLM 61.60 55.46 50.81 64.91 62 70.28 58.43
SC 53.66 57.29 68.40 56.12 34.18 39.04 32.22
FWD 68.01 72.82 86.37 62.95 61.14 49.94 78.82
LC + PLM + SC + FWD 71.64 73.52 79.38 68.46 69.48 64.03 75.94
LSP + LC + PLM + SC + FWD 81.75 81.60 81.46 81.74 81.90 82.04 81.76
CC
LSP 78.19 76.40 70.64 83.20 79.71 85.72 74.50
LC 63.82 62.36 60.12 64.77 65.17 67.49 63.01
PLM 55.46 64.41 80.72 53.61 40.41 30.22 61.30
SC 50.52 62.58 87.31 50.64 13.75 14.33 13.22
FWD 61.36 60.80 60.70 60.90 61.90 61.99 61.80
LC + PLM + SC + FWD 67.69 67.62 67.51 67.77 67.74 67.87 67.64
LSP + LC + PLM + SC + FWD 79.81 78.33 72.76 84.84 81.10 86.92 76.02
Table 2: The Experimental Results (A: overall accuracy; (-): erroneous sentences; (+): correct sentences; F:
F-score; R: recall; P: precision)
Dataset Model A (-)F (-)R (-)P
JC
Ours 81.39 81.25 81.24 81.28
Word 58.87 33.67 21.03 84.73
ALEK 54.69 20.33 11.67 78.95
CC
Ours 79.14 77.81 73.17 83.09
Word 58.47 32.02 19.81 84.22
ALEK 55.21 22.83 13.42 76.36
Table 3: The Comparison Results
LSPs play dominating role in achieving the results.
Due to space limitation, no details are reported.
To further see the performance of our method
on data written by writers with different first-
language backgrounds, we conducted two experi-
ments. (1) We merge the JC dataset and CC dataset.
The 10-fold cross-validation results on the merged
dataset are given in the third row of Table 4. The
results demonstrate that our models work well when
the training data and test data contain sentences from
different first-language backgrounds. (2) We use the
JC dataset (resp. CC dataset) for training while the
CC dataset (resp. JC dataset) is used as test data. As
shown in the fourth (resp. fifth) row of Table 4, the
results are worse than their corresponding results of
Word given in Table 3. The reason is that the mis-
takes made by Japanese and Chinese are different,
thus the learning model trained on one data does not
work well on the other data. Note that our method is
not designed to work in this scenario.
Application to Machine Translation Evaluation.
Our learning models could be used to evaluate the
MT results as an complementary measure. This is
based on the assumption that if the MT results can
be accurately distinguished from human references
Dataset A (-)F (-)R (-)P
JC(Train)+nonparallel(Test) 72.49 68.55 57.51 84.84
JC(Train)+parallel(Test) 71.33 69.53 65.42 74.18
JC + CC 79.98 79.72 79.24 80.23
JC(Train)+ CC(Test) 55.62 41.71 31.32 62.40
CC(Train)+ JC(Test) 57.57 23.64 16.94 39.11
Table 4: The Cross-domain Results of our Method
by our technique, the MT results are not natural and
may contain errors as well.
The experiment was conducted using 10-fold
cross validation on two LDC data, low-ranked and
high-ranked data9. The results using SVM as classi-
fication model are given in Table 5. As expected, the
classification accuracy on low-ranked data is higher
than that on high-ranked data since low-ranked MT
results are more different from human references
than high-ranked MT results. We also found that
LSPs are the most effective features. In addition, our
discovered LSPs could indicate the common errors
made by the MT systems and provide some sugges-
tions for improving machine translation results.
As a summary, the mined LSPs are indeed effec-
tive for the classification models and our proposed
technique is effective.
5 Conclusions and Future Work
This paper proposed a new approach to identifying
erroneous/correct sentences. Empirical evaluating
using diverse data demonstrated the effectiveness of
9One LDC data contains 14,604 low ranked (score 1-3) ma-
chine translations and the corresponding human references; the
other LDC data contains 808 high ranked (score 3-5) machine
translations and the corresponding human references
87
Data Feature A (-)F (-)R (-)P (+)F (+)R (+)P
Low-ranked data (1-3 score) LSP 84.20 83.95 82.19 85.82 84.44 86.25 82.73
LSP+LC+PLM+SC+FWD 86.60 86.84 88.96 84.83 86.35 84.27 88.56
High-ranked data (3-5 score) LSP 71.74 73.01 79.56 67.59 70.23 64.47 77.40
LSP+LC+PLM+SC+FWD 72.87 73.68 68.95 69.20 71.92 67.22 77.60
Table 5: The Results on Machine Translation Data
our techniques. Moreover, we proposed to mine
LSPs as the input of classification models from a set
of data containing correct and erroneous sentences.
The LSPs were shown to be much more effective than
the other linguistic features although the other fea-
tures were also beneficial.
We will investigate the following problems in the
future: (1) to make use of the discovered LSPs to pro-
vide detailed feedback for ESL learners, e.g. the er-
rors in a sentence and suggested corrections; (2) to
integrate the features effectively to achieve better re-
sults; (3) to further investigate the application of our
techniques for MT evaluation.
References
Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining se-
quential patterns. In ICDE.
Emily M. Bender, Dan Flickinger, Stephan Oepen, Annemarie
Walsh, and Timothy Baldwin. 2004. Arboretum: Using a
precision grammar for grammmar checking in call. In Proc.
InSTIL/ICALL Symposium on Computer Assisted Learning.
Chris Brockett, William Dolan, and Michael Gamon. 2006.
Correcting esl errors using phrasal smt techniques. In ACL.
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Martin
Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998.
Automated scoring using a hybrid feature identification tech-
nique. In Proc. ACL.
Martin Chodorow and Claudia Leacock. 2000. An unsuper-
vised method for detecting grammatical errors. In NAACL.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. ACL.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proc. ACL.
P.W. Foltz, D. Laham, and T.K. Landauer. 1999. Automated
essay scoring: Application to educational technology. In Ed-
Media ?99.
Michael Gamon, Anthony Aue, and Martine Smets. 2005.
Sentence-level mt evaluation without reference translations:
Beyond language modeling. In Proc. EAMT.
Shicun Gui and Huizhong Yang. 2003. Zhongguo Xuexizhe
Yingyu Yuliaohu. (Chinese Learner English Corpus). Shang-
hai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).
George E. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert Dale,
Hermann Moisi and Harold Somers (ed.). Marcel Dekker.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Sup-
nithi, and Hitoshi Isahara. 2003. Automatic error detection
in the japanese learners? english spoken data. In Proc. ACL.
Nitin Jindal and Bing Liu. 2006. Identifying comparative sen-
tences in text documents. In SIGIR.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In Proc. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization.
Yajuan Lu? and Ming Zhou. 2004. Collocation translation ac-
quisition using monolingual corpora. In Proc. ACL.
Lisa N. Michaud, Kathleen F. McCoy, and Christopher A. Pen-
nington. 2000. An intelligent tutoring system for deaf learn-
ers of written english. In Proc. 4th International ACM Con-
ference on Assistive Technologies.
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and Naoki Isu.
2006. A feedback-augmented method for detecting errors in
the writing of learners of english. In Proc. ACL.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Du-
rand. 1999. Cross-language information retrieval based on
parallel texts and automatic mining of parallel texts from the
web. In SIGIR, pages 74?81.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, and Helen Pinto.
2001. Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. ICDE.
Yongmei Shi and Lina Zhou. 2005. Error detection using lin-
guistic features. In HLT/EMNLP.
Andreas Stolcke. 2002. Srilm-an extensible language modeling
toolkit. In Proc. ICSLP.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and Ming
Zhou. 2007. Mining sequential patterns and tree patterns to
detect erroneous sentences. In AAAI.
Tono Yukio, T. Kaneko, H. Isahara, T. Saiga, and E. Izumi.
2001. The standard speaking test corpus: A 1 million-word
spoken corpus of japanese learners of english and its impli-
cations for l2 lexicography. In ASIALEX: Asian Bilingualism
and the Dictionary.
88
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 698?706,
Beijing, August 2010
Semantic Role Labeling for News Tweets 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 3Zhongyang Xiong and 2Changning Huang 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
zyxiong@cqu.edu.cn 
v-cnh@microsoft.com 
 
Abstract 
News tweets that report what is happen-
ing have become an important real-time 
information source. We raise the prob-
lem of Semantic Role Labeling (SRL) 
for news tweets, which is meaningful for 
fine grained information extraction and 
retrieval. We present a self-supervised 
learning approach to train a domain spe-
cific SRL system to resolve the problem. 
A large volume of training data is auto-
matically labeled, by leveraging the ex-
isting SRL system on news domain and 
content similarity between news and 
news tweets. On a human annotated test 
set, our system achieves  state-of-the-art 
performance, outperforming the SRL 
system trained on news. 
1 Introduction 
Tweets are text messages up to 140 characters. 
Every day, more than 50 million tweets are gen-
erated by millions of Twitter users. According to 
the investigation by Pear Analytics (2009), about 
4% tweets are related to news1. 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
1 http://blog.twitter.com/2010/02/measuring-tweets.html 
We divide news related tweets into two cate-
gories: those excerpted from news articles and 
those not. The former kind of tweets, hereafter 
called news excerpt, is formally written while 
the latter, hereafter called news tweet, varies in 
style and often is not grammatically correct. To 
understand the proportion of news tweets, we 
randomly selected 1000 tweets related to news, 
and got 865 news tweets. Following is an exam-
ple of anews tweet, containing oh, yea, which 
usually appear in spoken language, and :-(, an 
emoticon. 
oh yea and Chile earthquake the earth off it's 
axis according to NASA and shorten the day 
by a wee second :-(                                     (S1) 
News tweets arean important information 
source because they keep reporting what is hap-
pening in real time. For example, the earthquake 
near Los Angeles that happened on Tuesday, 
July 29, 2008 was first reported through news 
tweets only seconds later than the outbreak of 
the quake. Official news did not emerge about 
this event until four minutes later. By then, 
"Earthquake" was trending on Twitter Search 
with thousands of updates2. 
However, it is a daunting task for people to 
find out information they are interested in from 
such a huge number of news tweets, thus moti-
vating us to conduct some kind of information 
                                                 
2 http://blog.twitter.com/2008/07/twitter-as-news-wire.html 
698
extraction such as event mining, where SRL 
plays a crucial  role (Surdeanu et al, 2003). 
Considering Sentence 1, suppose the agent 
earthquake and the patient day for the predicate 
shorten are identified. Then it is straightforward 
to output the event Chile earthquake shorten the 
day, which captures the essential information 
encoded in this tweet. 
Following M?rquez (2009), we define SRL 
for news tweets as the task of identifying the 
arguments of a given verb as predicate in a news 
tweet and assigning them semantic labels de-
scribing the roles they play for the predicate. To 
make our method applicable to general infor-
mation extraction tasks,  rather than only to 
some special scenarios such as arresting event 
extraction, we adopt general semantic roles, i.e., 
Agent(A0), Patient(A1), Location(AM-LOC), 
Temporal(AM-TMP),etc., instead of situation-
specific roles (Fillmore et al, 2004) such as 
Suspect, Authorities, and Offense in an arrest 
frame.  
Our first attempt is to directly apply the state-
of-art SRL system (Meza-Ruiz and Riedel, 2009) 
that trained on the CoNLL 08 shared task da-
taset(Surdeanu et al, 2008), hereafter called 
SRL-BS, to news tweets. Not surprisingly, we 
observe its F1 score drops sharply from 75.5% 
on news corpus to 43.3% on our human annotat-
ed news tweets, owing much to the informal 
written style of news tweets. 
Therefore, we have to build a domain specific 
SRL system for news tweets. Given the diversi-
fied styles of news tweets, building such a sys-
tem requires a larger number of annotated news 
tweets, which are not available, and are not af-
fordable for human labeling. We propose a novel 
method to automatically annotate news tweets, 
which leverages the existing resources of SRL 
for news domain, and content similarity between 
news and news tweets. We argue that the same 
event is likely to be reported by both news and 
news tweets, which results in  content similarity 
between the news and news tweet. Further, we 
argue that the news and news tweets reporting 
the same event tend to have similar predicate-
argument structures. We tested our assumptions 
on the event Chile earthquake that happened on 
Match 2nd, 2010. We got 261 news and 722 news 
tweets published on the same day that described 
this event.  Sentence 2 and 3 are two examples 
of the news excerpts and Sentence 1 is one ex-
ample of news tweets for this event.   
Chile Earthquake Shortened Earth Day    (S2) 
Chile Earthquake Shortened Day              (S3) 
Obviously Sentence 1, 2 and 3 all have predi-
FDWH ?shortened? with the same A0 and A1 ar-
guments. Our manually checking showed that in 
average each news tweet in those 993 samples 
had 2.4 news excerpts that had the same predi-
cate-argument structures.  
Our news tweet annotation approach consists 
of four steps. First, we submit hot queries to 
Twitter and for each query we obtain a list of 
tweets. Second, for each list of tweets, we single 
out news excerpts using heuristic rules and re-
move them from the list, conduct SRL on news 
excerpts using SRL-BS, and cluster them in 
terms of the similarity in content and predicate-
argument structures. Third, for each list of 
tweets, we try to merge every remaining tweet 
into one news excerpt cluster according to its 
content similarity to the cluster. Those that can 
be put into one news group are regarded as news 
tweet. Finally, semantic structures of news ex-
cerpts are passed to the news tweet in the same 
group through word alignment. 
Our domain specific SRL system is then 
trained on automatically constructed training 
data using the Conditional Random Field (CRF: 
Lafferty et al, 2001) learning framework. Our 
system is evaluated on a human labeled dataset, 
and achieves state-of-the-art performance, out-
performing the baseline SRL-BS.  
Our contributions can be summarized as fol-
lows: 
1) We propose to conduct SRL for news 
tweets for fine grained information ex-
traction and retrieval;  
2) We present a semi-supervised learning 
approach to train a domain specific SRL 
system for news tweets, which outper-
forms SRL-BS and achieves the state-of-
the-art performance on a human labeled 
dataset. 
The rest of this paper is organized as follows: 
In the next section, we review related work.  In 
Section 3 we detail key components of our ap-
proach. In Section 4, we setup experiments and 
evaluate the effectiveness of our method.  Final-
699
ly, Section 5 concludes and presents the future 
work. 
2 Related Work 
Our related work falls into two categories: SRL 
on news and domain adaption. 
As for SRL on news, most researchers used 
the pipelined approach, i.e., dividing the task 
into several phases such as argument identifica-
tion, argument classification, global inference, 
etc.,  and conquering them individually (Xue and 
Palmer, 2004; Koomen et al, 2005; Cohn and 
Blunsom, 2005; Punyakanok et al, 2008; 
Toutanova et al, 2005; Toutanova et al, 2008). 
Exceptions to the pipelined approach exist.  
M?rquez et al (2005) sequentially labeled the 
words according to their positions relative to an 
argument (i.e., inside, outside or at the beginning 
of it). Carreras et al (2004) and Surdeanu et al 
(2007) jointly labeled all the predicates. Vickrey 
and Koller(2008) simplified the input sentence 
by hand-written and machine learnt rules before 
conducting SRL. Some other approaches simul-
taneously resolved all the sub-tasks by integrat-
ing syntactic parsing and SRL into a single mod-
el (Musillo and Merlo, 2006; Merlo and Musillo, 
2008), or by using Markov Logic Networks 
(MLN, Richardson and Domingos, 2005) as the 
learning framework (Riedel and Meza-Ruiz, 
2008; Meza-Ruiz and Riedel, 2009). 
All the above approaches focus on sentences 
from news articles or other formal documents, 
and depend on human annotated corpus for 
training. To our knowledge, little study has been 
carried out on SRL for news tweets.  
As for domain adaption, some researchers re-
garded the out-of-GRPDLQ GDWD DV ?SULRU
NQRZOHGJH?DQGestimated the model parameters 
by maximizing the posterior under this prior dis-
tribution, and successfully applied their ap-
proach to language modeling (Bacchiani and 
Roark, 2003) and parsing (Roark and Bacchiani, 
2003). Daum? III and Marcu (2006) presented a 
QRYHO IUDPHZRUN E\ GHILQLQJ D ?JHQHUDO Go-
PDLQ?EHWZHHQWKH?WUXO\LQ-GRPDLQ?DQG?WUXO\
out-of-GRPDLQ?   
Unlike existing domain adaption approaches, 
our method is about adapting SRL system on 
news domain to the news tweets domain, two 
domains that differ in writing style but are linked 
through content similarity. 
3 Our Method 
Our method of SRL for news tweets is to train a 
domain specific SRL on automatically annotated 
training data as briefed in Section 1.  
In this section we present details of the five 
crucial components of our method, i.e., news 
excerpt identification, news excerpt clustering, 
news tweets identification, semantic structure 
mapping, and the domain specific SRL system 
constructing. 
3.1 News Excerpt Identification 
We use one heuristic rule to decide whether or 
not a tweet is news excerpt:  if a tweet has a link 
to a news article and its text content is included 
by the news article, it is news excerpt, otherwise 
not. 
Given a tweet, to apply this rule, we first ex-
tract the content link and expand it, if any, into 
the full link with the unshorten service3. This 
step is necessary because content link in tweet is 
usually shortened to reduce the total amount of 
characters. Next, we check if the full link points 
to any of the pre-defined news sites, which, in 
our experiments, are 57 English news websites. 
If yes, we download the web page and check if it 
exactly contains the text content of the input 
tweet. Figure 1 illustrates this process.  
Figure 1. An illustration of news excerpt identi-
fication. 
To test the precision of this approach, while 
preparing for the training data for the experi-
ments, we checked 100 tweets that were identi-
fied as news excerpt by this rule to find out they 
all are excerpted from news. 
                                                 
3 http://unshort.me 
700
3.2 News Excerpt Clustering 
Given as input a list of news excerpts concerning 
the same query and published in the same time 
scope, this component uses the hierarchical ag-
glomerative clustering algorithm (Manning et 
al., 2008) to divide news excerpts into groups in 
terms of the similarity in content and predicate-
argument structures.  
Before clustering, for every news excerpt, we 
remove the content link and other metadata such 
as author, retweet marks (starting with RT @), 
reply marks (starting with @ immediately after 
the author), hash tags (starting with #), etc., and 
keep only the text content; then it is further 
parsed into tokens, POS tags, chunks and syntac-
tic tree using the OpenNLP toolkit4.  After that,  
SRL is conducted with SRL-BS to get predicate-
argument structures. Finally, every news excerpt 
is represented as frequency a vector of terms, 
including tokens, POS tagger, chunks, predicate-
argument structures, etc. A news cluster is re-
garded as a ?macro? news excerpt and is also 
represented as a term frequency vector, i.e., the 
sum of all the term vectors in the cluster.  Noisy 
terms, such as numbers and predefined stop 
words are excluded from the frequency vector. 
To reduce data sparseness, words are stemmed 
by Porter stemmer (Martin F. Porter, 1980). 
The cosine similarity is used to measure the 
relevance between two clusters, as defined in 
Formula 1.  
   ,
'
'
'
CVCV
CVCV
CCCS
u
?               (1) 
Where C, &? denote two clusters, CV, CV? de-
note  the term frequency vectors of C and  &? 
respectively, and CS(C, &?) stands for the  co-
sine similarity between C and  &?. 
Initially, one news excerpt forms one cluster.  
Then the clustering process repeats merging the 
two most similar clusters into one till the simi-
larity between any pair of clusters is below a 
threshold, which is experimentally set to 0.7 in 
our experiments. 
During the training data preparation process, 
we randomly selected 100 clusters, each with 3.2 
pieces of news in average. For every pair of 
news excerpts in the same cluster, we checked if 
                                                 
4 http://opennlp.sourceforge.net/ 
they shared similar contents and semantic struc-
tures, and found out that 91.1% were the cases. 
3.3 News Tweets Identification 
After news excerpts are identified and removed 
from the list, every remaining tweet is checked if 
it is a news tweet. Here we group news excerpts 
and news tweets together in two steps because 1) 
news excerpts count for only a small proportion 
of all the tweets in the list, making our two-step 
clustering algorithm more efficient; and 2) one-
step clustering tends to output meaningless clus-
ters that include no news tweets. 
Intuitively, news tweet, more often than not, 
have news counterparts that report similar con-
tents. Thus we use the following rule to identify 
news tweets: if the content similarity between 
the tweet and any news excerpt cluster is greater 
than a threshold, which is experimentally set to 
0.7 in our experiments, the tweet is a news tweet, 
otherwise it is not. Furthermore, each news 
tweet is merged into the cluster with most simi-
lar content. Finally, we re-label any news tweet 
as news excerpt, which is then process by SRL-
BS, if its content similarity to the cluster exceeds 
a threshold, which is experimentally set to 0.9 in 
our experiments. 
Again, the cosine similarity is used to meas-
ure the content similarity between tweet and 
news excerpt cluster. Each tweet is repressed as 
a term frequency vector. Before extracting terms 
from tweet, tweet metadata is removed and a 
rule-based normalization process is conducted to 
restore abnormal strLQJVVD\?	DSRV?LQWRWKHLU
KXPDQ IULHQGO\ IRUP VD\ ?? ? 1H[W VWHPPLQJ
tools and OpenNLP are applied to get lemmas, 
POS tags, chunks, etc., and noisy terms are fil-
tered.  
We evaluated the performance of this ap-
proach when preparing for the training data. We 
randomly sampled 500 tweets that were identi-
fied as news tweets, to find that 93.8% were true 
news tweets. 
3.4 Semantic Structure Mapping 
Semantic structure mapping is formed as the 
task of word alignment from news excerpt to 
news tweet. A HMM alignment model is trained 
with GIZA++ (Franz and Hermann, 2000) on all 
(news excerpt, news tweet) pairs in the same 
cluster. After word alignment is done, semantic 
701
information attached to a word in a news excerpt 
is passed to the corresponding word in the news 
tweet as illustrated in Figure 2. 
 
Chile Earthquake Shortened Earth Day
A0 predicate A1
NASA and shorten the day by a wee second :-(
oh yea and Chile earthquake the earth off it's axis according to
 
Figure 2. An example of mapping semantic 
structures from news excerpts to news tweets. 
In Figure 2, shorten, earthquake and day in 
two sentences are aligned, respectively; and two 
predicate-argument structures in the first sen-
tence, i.e., (shortened, earthquake, A0), (short-
ened, day, A1), are passed to the second. 
News tweets may receive no semantic infor-
mation from related news excerpts after mapping, 
because of word alignment errors or no news 
excerpt in the cluster with similar semantic 
structures.  Such tweets are dropped. 
Mapping may also introduce cases that violate 
the following two structural constraints in SRL 
(Meza-Ruiz and Riedel, 2009): 1) one (predi-
cate, argument) pair has only one role label in 
one sentence; and 2) for each predicate, each of 
the proper arguments (A0~A5) can occur at most 
once. Those conflicts are largely owing to the 
noisy outputs of SRL trained on news and to the 
alignment errors. While preparing for the train-
ing data for our experiments, we found 38.9% of 
news tweets had such conflicts.  
A majority voting schema and the structural 
constrains are used to resolve the conflicts as 
described below.   
1) Step 1, for every cluster, each (predicate, 
argument, role) is weighted according to 
its frequency in the cluster; 
2) Step 2, for every cluster, detect conflicts 
using the structural constrains; if no con-
flicts exist, stop; otherwise go to Step 3;   
3) Step 3, for every cluster, keep the one 
with higher weight in each conflicting 
(predicate, argument, role) pair; if the 
weights are equal,  drop both; 
Here is an example to show the conflicting 
resolution process.  Consider the cluster includ-
ing Sentence 1, 2 and 3, where (shorten, earth-
quake, A0), (shorten, earthquake, A1), (shorten, 
axis, A0), and (shorten, day, A1) occur 6, 4, 1 
and 3 times, respectively.  This cluster includes 
three conflicting pairs:   
1) (shorten, earthquake, A0) vs. (shorten, 
earthquake, A1); 
2) (shorten, earthquake, A1) vs. (shorten, 
day, A1); 
3) (shorten, earthquake, A0) vs. (shorten, ax-
is, A0); 
The first pair is first resolved, causing (short-
en, earthquake, A0) to be kept and (shorten, 
earthquake, A1) removed, which leads to the 
second pair being resolved as well; then we pro-
cess the third pair resulting in (shorten, earth-
quake, A0) being kept and (shorten, axis, A0) 
dropped; finally (shorten, earthquake, A0) and 
(shorten, day, A1) stay in the cluster. 
The conflicting resolution algorithm is sensi-
tive to the order of conflict resolution in Step 3. 
Still consider the three conflicting pairs listed 
above. If the second pair is first processed, only 
(shorten, earthquake, A0) will be left. Our strat-
egy is to first handle the conflict resolving which 
leads to most conflicts resolved. 
We tested the performance of this semantic 
structure mapping strategy while preparing for 
the training data. We randomly selected 56 news 
tweets with conflicts and manually annotated 
them with SRL. After the conflict resolution 
method was done, we observed that 38 news 
tweets were resolved correctly, 9 resolved but 
incorrectly, and 9 remain unresolved, suggesting 
the high precision of this method, which fits our 
task.  We leave it to our future work to study 
more advanced approach for semantic structure 
mapping. 
3.5 SRL System for News Tweets 
Following M?rquez et al (2005), we regard SRL 
for tweets as a sequential labeling task, because 
of its joint inference ability and its openness to 
support other languages. 
We adopt conventional features for each token 
defined in M?rquez et al(2005),  such as the 
lemma/POS tag of the current/previous/next to-
ken, the lemma of predicate and its combination 
with the lemma/POS tag of the current token, the 
voice of the predicate (active/passive), the dis-
tance between the current token and the predi-
cate, the relative position of the current token to 
702
the predicate, and so on. We do not use features 
related to syntactic parsing trees, to allow our 
system not to rely on any syntactic parser, whose 
performance depends on style and language of 
text, which limits the generality of our system. 
Before extracting features, we perform a pre-
processing step to remove tweet metadata and 
normalize tweet text content, as described in 
Section 3.3. The OpenNLP toolkit is used for 
feature extraction, and the CRF++ toolkit 5  is 
used to train the model. 
4 Experiments 
In this section, we evaluate our SRL system on a 
gold-standard dataset consisting of 1,110 human 
annotated news tweets and show that our system 
achieves the state-of-the-art performance com-
pared with SRL-BS that is trained on news. Fur-
thermore, we study the contribution of automati-
cally generated training data. 
4.1 Evaluation Metric 
We adopt the widely used precision (Pre.), recall 
(Rec.) and F-score (F., the harmonic mean of 
precision and recall) as evaluation metrics.  
4.2 Baseline System 
We use SRL-BS as our baseline because of its 
state-of-art performance on news domain, and its 
readiness to use as well. 
4.3 Data Preparation 
We restrict to English news tweets to test our 
method. Our method can label news tweets of 
other languages, given that the related tools such 
as the SRL system on news domain, the word 
alignment tool, OpenNLP, etc., can support oth-
er languages.  
We build two corpora for our experiments: 
one is the training dataset of 10,000 news tweets 
with semantic roles automatically labeled; the 
other is the gold-standard dataset of 1,110 news 
tweets with semantic roles manually labeled. 
Training Dataset 
We randomly sample 80 queries from 300 
English queries extracted from the top stories of 
Bing news, Google news and Twitter trending 
topics from March 1, 2010 to March 4, 2010.  
                                                 
5 http://crfpp.sourceforge.net/ 
Submitting the 80 queries to Twitter search, 
we retrieve and download 512,000 tweets, from 
which we got 4,785 news excerpts and 11,427 
news tweets, which were automatically annotat-
ed using the method described in Section 3.   
Furthermore, 10,000 tweets are randomly se-
lected from the automatically annotated news 
tweets, forming the training dataset, while the 
other 1,427 news tweets are used to construct the 
gold-standard dataset. 
Gold-standard Dataset 
We ask two people to annotate the 1,427 news 
tweets, following the Annotation guidelines for 
PropBank6 with one exception: for phrasal ar-
guments, only the head word is labeled as the 
argument, because our system and SRL-BS con-
duct word level SRL. 
317 news tweets are dropped because of in-
consistent annotation, and the remaining 1,110 
news tweets form the gold-standard dataset.  
Quality of Training dataset 
Since the news tweets in the gold-standard da-
taset are randomly sampled from the automati-
cally labeled corpus and are labeled by both hu-
man and machine, we use them to estimate the 
quality of training data, i.e., to which degree the 
automatically generated results are similar to 
humans?.   
We find that our method achieves 75.6% F1 
score, much higher than the baseline, suggesting 
the relatively high quality of the training data. 
4.4 Result and Analysis 
Table 1 reports the experimental results of our 
system (SRL-TS) and the baseline on the gold-
standard dataset. 
 
 Precision Recall F-Score 
SRL-BS 36.0 % 54.5% 43.3% 
SRL-TS 78.0% 57.1% 66.0% 
Table 1. Performances of our system and the 
baseline on the gold-standard dataset. 
As shown in Table 1, our system performs 
much better than the baseline on the gold-
standard dataset in terms of all metrics. We ob-
serve two types of errors that are often made by 
                                                 
6 http://verbs.colorado.edu/~mpalmer/projects/ace/PB 
guidelines.pdf 
703
SRL-BS but not so often by our system, which 
largely explains the difference in performance.  
The first type of errors, which accounts for 
25.3% of the total errors made by SRL-BS, is 
caused by the informal written style, such as el-
lipsis, of news tweets. For instance, for the ex-
ample Sentence 1 listed in Section 1, the SRL-
BS incorrectly identify earth as the A0 argument 
of the predicate shorten. The other type of errors, 
which accounts for 10.2% of the total errors 
made by SRL-BS, is related to the discretionary 
combination of news snippets. For example, 
consider the following news tweet: 
The Chile earthquake shifted the earth's axis, 
"shortened the length of an Earth day by 1.26 
miliseconds".                                              (S4) 
We analyze the errors made by our system 
and find that 12.5% errors are attributed to the 
complex syntactic structures, suggesting that 
combining our system with systems on news 
domain is a promising direction. For example, 
our system cannot identify the A0 argument of 
the predicate shortened, because of its blindness 
of attributive clause; in contrast, SRL-BS works 
on this case.  
wow..the earthquake that caused the 2004 In-
dian Ocean tsunami shortened the day by al-
most 3 microseconds..what does that even 
mean?! HOW?                                           (S5) 
We also find that 32.3% of the errors made by 
our system are more or less related to the train-
ing data, which has noise and cannot fully repre-
sent the knowledge of SRL on news tweets. For 
instance, our system fails to label the following 
sentence, partially because the predicate strike 
does not occur in the training set. 
8.8-Magnitude-Earthquake-Strikes-Chile (S6) 
We further study how the size of automatical-
O\ODEHOHGWUDLQLQJGDWDDIIHFWVRXUV\VWHP?VSHr-
formance, as illustrated in Figure 3. We conduct 
two sets of experiments: in the first set, the train-
ing data is automatically labeled and the testing 
data is the gold-standard dataset; in the second 
set, half of the news tweets from the gold-
standard dataset are added to the training data, 
the remaining half forms the testing dataset. 
Curve 1 and 2 represent the experimental results 
of set 1 and 2, respectively. 
From Curve 1, we see that RXUV\VWHP?VSHr-
formance increases sharply when the training 
data size varies from 5,000 to 6,000; then in-
creases relatively slowly with more training data; 
and finally reaches the highest when all training 
data is used.  Curve 2 reveals a similar trend. 
 
 
Figure 3. Performance on training data of vary-
ing size. 
This phenomenon is largely due to the com-
peting between two forces: the noise in the train-
ing data, and the knowledge of SRL encoded in 
the training data.  
Interestingly, from Figure 3, we observe that 
the contribution of human labeled data is no 
longer significant after 6,000 automatically la-
beled training data is used, reaffirming the effec-
tiveness of the training data. 
5 Conclusions and Future Work 
We propose to conduct SRL on news tweets for 
fine grained information extraction and retrieval. 
We present a self-supervised learning approach 
to train a domain specific SRL system for news 
tweets. Leveraging the SRL system on news 
domain and content similarity between news and 
news tweets, our approach automatically labels a 
large volume of training data by mapping SRL-
BS generated results of news excerpts to news 
tweets. Experimental results show that our sys-
tem outperforms the baseline and achieves the 
state-of-the-art performance.  
In the future, we plan to enlarge training data 
size and test our system on a larger dataset; we 
also plan to further boost the performance of our 
system by incorporating tweets specific features 
such as hash tags, reply/re-tweet marks into our 
704
CRF model, and by combining our system with 
SRL systems trained on news.  
 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper-
vised language model adaptation. Proceedings of 
the 2003 International Conference on Acoustics, 
Speech and Signal Processing, volume 1, pages: 
224-227 
Carreras, Xavier, Llu?s M?rquez, and Grzegorz 
&KUXSD?D+LHUDUFKLFDOUHFRJQLWLRQRISURSo-
sitional arguments with Perceptrons. Proceedings 
of the Eighth Conference on Computational Natu-
ral Language Learning, pages: 106-109. 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labeling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Daum?, Hal III and Daniel Marcu. 2006. Domain 
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26(1), 101-126. 
Fillmore, Charles J., Josef Ruppenhofer, Collin F. 
Baker. 2004. FrameNet and Representing the Link 
between Semantic and Syntactic Relations. Com-
putational Linguistics and Beyond, Institute of 
Linguistics, Academia Sinica. 
Kelly, Ryan, ed. 2009. Twitter Study Reveals Inter-
esting Results About Usage. San Antonio, Texas: 
Pear Analytics. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
Lafferty, John D., Andrew McCallum, Fernando C. 
N. Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Labeling 
Sequence Data. Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, 
pages: 282-289. 
Manning, Christopher D., Prabhakar Raghavan and 
Hinrich Schtze. 2008. Introduction to Information 
Retrieval. Cambridge University Press, Cam-
bridge, UK. 
M?rquez, Llu?s, Jesus Gim?nez Pere Comas and 
Neus Catal?. 2005. Semantic Role Labeling as Se-
quential Tagging. Proceedings of the Ninth Con-
ference on Computational Natural Language 
Learning, pages: 193-196. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses us-
ing Markov Logic. Human Language Technolo-
gies: The 2009 Annual Conference of the North 
American Chapter of the ACL, pages: 155-163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Och, Franz Josef, Hermann Ney. Improved Statistical 
Alignment Models. Proceedings of the 38th Annu-
al Meeting of the Association for Computational 
Linguistics, pages: 440-447. 
Porter, Martin F. 1980. An algorithm for suffix strip-
ping. Program, 14(3), 130-137. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and in-
ference in semantic role labeling. Journal of Com-
putational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. Collec-
tive semantic role labelling with Markov Logic. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 193-
197. 
Roark, Brian and Michiel Bacchiani. 2003. Super-
vised and unsupervised PCFG adaptation to novel 
domains. Proceedings of the 2003 Conference of 
the North American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology, volume 1, pages: 126-133. 
Surdeanu, Mihai, Sanda Harabagiu, JohnWilliams 
and Paul Aarseth. 2003. Using predicate-argument 
structures for information extraction. Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics, volume 1, pages: 8-15. 
Surdeanu, Mihai, Llu?s M?rquez, Xavier Carreras and 
Pere R. Comas. 2007. Combination strategies for 
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29(1), 105-151. 
705
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Vickrey, David and Daphne Koller. 2008. Applying 
sentence simplification to the conll-2008 shared 
task. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, pag-
es: 268-272  
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
706
Coling 2010: Poster Volume, pages 725?729,
Beijing, August 2010
Collective Semantic Role Labeling on Open News Corpus  
by Leveraging Redundancy 
 
 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 5Daniel Tse* and 3Zhongyang Xiong 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
5School of Information Technologies 
The University of Sydney 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
dtse6695@it.usyd.edu.au 
zyxiong@cqu.edu.cn 
 
  
Abstract 
We propose a novel MLN-based method 
that collectively conducts SRL on 
groups of news sentences. Our method is 
built upon a baseline SRL, which uses 
no parsers and leverages redundancy. 
We evaluate our method on a manually 
labeled news corpus and demonstrate 
that news redundancy significantly im-
proves the performance of the baseline, 
e.g., it improves the F-score from 
64.13% to 67.66%.  * 
1 Introduction 
Semantic Role Labeling (SRL, M?rquez, 2009) 
is generally understood as the task of identifying 
the arguments of a given predicate and assigning 
them semantic labels describing the roles they 
play. For example, given a sentence The luxury 
auto maker sold 1,214 cars., the goal is to iden-
tify the arguments of sold and produce the fol-
lowing output: [A0 The luxury auto maker] [V 
sold] [A1 1,214 cars]. Here A0 represents the 
seller, and A1 represents the things sold (CoNLL 
2008 shared task, Surdeanu et al, 2008). 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
Gildea and Jurafsky (2002) first tackled SRL 
as an independent task, which is divided into 
several sub-tasks such as argument identifica-
tion, argument classification, global inference, 
etc. Some researchers (Xue and Palmer, 2004; 
Koomen et al, 2005; Cohn and Blunsom, 2005; 
Punyakanok et al, 2008; Toutanova et al, 2005; 
Toutanova et al, 2008) used a pipelined ap-
proach to attack the task. Some others resolved 
the sub-tasks simultaneously. For example, some 
work (Musillo and Merlo, 2006; Merlo and Mu-
sillo, 2008) integrated syntactic parsing and SRL 
into a single model, and another (Riedel and 
Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009) 
jointly handled all sub-tasks using Markov Log-
ic Networks (MLN, Richardson and Domingos, 
2005). 
All the above methods conduct sentence level 
SRL, and rely on parsers. Parsers have showed 
great effects on SRL performance. For example, 
Xue and Palmer (2004) reported that SRL per-
formance dropped more than 10% when they 
used syntactic features from an automatic parser 
instead of the gold standard parsing trees. Even 
worse, parsers are not robust and cannot always 
analyze any input, due to the fact that some in-
puts are not in the language described by the 
parser?s formal grammar, or adequately repre-
sented within the parser?s training data. 
725
We propose a novel MLN-based method that 
collectively conducts SRL on groups of news 
sentences to leverage the content redundancy in 
news. To isolate the negative effect of noise 
from parsers and thus focus on the study of the 
contribution of redundancy to SRL, we use no 
parsers in our approach. We built a baseline SRL, 
which depends on no parsers, and use the MLN 
framework to exploit  redundancy. Our intuition 
is that SRL on one sentence can help that on 
other differently phrased sentences with similar 
meaning. For example, consider the following 
sentence from a news article: 
A suicide bomber blew himself up Sunday in 
market in Pakistan's northwest crowded with 
shoppers ahead of a Muslim holiday, killing 
12 people, including a mayor who once sup-
ported but had turned against the Taliban, of-
ficials said. 
The state-of-art MLN-based system (Meza-Ruiz 
and Riedel, 2009), hereafter referred to as 
MLNBS for brevity, incorrectly labels northwest 
instead of bomber as A0 of killing. Now consider 
another sentence from another news article: 
Police in northwestern Pakistan say that a su-
icide bomber has killed at least 13 people and 
wounded dozens of others. 
Here MLNBS correctly identify bomber as A0 
of killing. When more sentences are observed 
where bomber as A0 of killing is correctly identi-
fied, we will be more confident that bomber 
should be labeled as A0 of killing, and that 
northwest should not be the A0 of killing accord-
ing to the constraint that one predicate has at 
most one A0. 
We manually construct a news corpus to 
evaluate our method. In the corpus, semantic 
role information is annotated and sentences with 
similar meanings are grouped together. Experi-
mental results show that news redundancy can 
significantly improve the performance of the 
baseline system. 
Our contributions can be summarized as fol-
lows: 
1. We present a novel method that conducts 
SRL on a set of sentences collectively, in-
stead of on a single sentence, by extend-
ing MLNBS to leverage redundancy. 
2. We show redundancy can significantly 
improve the performance of the baseline 
system, indicating a promising research 
direction towards open SRL. 
In the next section, we introduce news sen-
tence extraction and clustering. In Section 3, we 
describe our collective inference method. In Sec-
tion 4, we show our experimental results. Finally, 
in Section 5 we conclude our paper with a dis-
cussion of future work. 
2 Extraction and Clustering of News 
Sentences 
To construct a corpus to evaluate our method, 
we extract sentences from clustered news arti-
cles returned by news search engines such as 
Bing and Google, and divide them into groups 
so that sentences in a group have similar mean-
ing. 
News articles in the same cluster are supposed 
to report the same event. Thus we first group 
sentences according to the news cluster they 
come from. Then we split sentences in the same 
cluster into several groups according to the simi-
larity of meaning. We assume that two sentences 
are more similar in meaning if they share more 
synonymous proper nouns and verbs. The syno-
nyms of verbs, like plod and trudge, are mainly 
extracted from the Microsoft Encarta Diction-
ary1, and the proper nouns thesaurus, containing 
synonyms such as U.S. and the United States, is 
manually compiled. 
As examples, below are two sentence groups 
which are extracted from a news cluster describ-
ing Hurricane Ida. 
Group 1: 
? Hurricane Ida, the first Atlantic hurri-
cane to target the U.S. this year, plod-
ded yesterday toward the Gulf Coast? 
? Hurricane Ida trudged toward the Gulf 
Coast? 
? ? 
Group 2: 
? It could make landfall as early as Tues-
day morning, although it was forecast to 
weaken further. 
                                                 
1
http://uk.encarta.msn.com/encnet/features/dictionary/dictio
naryhome.aspx 
726
? Authorities said Ida could make landfall 
as early as Tuesday morning, although 
it was forecast to weaken by then. 
? ? 
3 Collective Inference Based on MLN 
Our method includes two core components: a 
baseline system that conducts SRL on every sen-
tence; and a collective inference system that ac-
cepts as input a group of sentences with prelimi-
nary SRL information provided by the baseline. 
We build the baseline by removing formulas 
involving syntactic parsing information from 
MLNBS (while keeping other rules) and retrain-
ing the system using the tool and scripts provid-
ed by Riedel and Meza-Ruiz (2008) on the man-
ually annotated news corpus described in Sec-
tion 4. 
A collective inference system is constructed 
to leverage redundancy in the SRL information 
from the baseline.  
We first redefine the predicate role and treat it 
as observed: 
predicate role: Int x Int x Int x Role; 
role has four parameters: the first one stands for 
the number of sentence in the input, which is 
necessary to distinguish the sentences in a group; 
the other three are taken from the arguments of 
the role predicate defined by Riedel and Meza-
Ruiz (2008), which denote the positions of the 
predicate and the argument in the sentence and 
the role of the argument, respectively. If the 
predication holds, it returns 1, otherwise 0.  
A hidden predicate final_role is defined to 
present the final output, which has the same pa-
rameters as the predicate role: 
predicate final_role: Int x Int x Int x Role; 
We introduce the following formula, which 
directly passes the semantic role from the base-
line to the final output: 
role(s, p, a, +r)=> final_role (s, p, a, +r)    (1) 
Here s is the sentence number in a group; p and 
a denote the positions of the predicate and ar-
gument in s, respectively; r stands for the role of 
the argument; the ?+? before the variable r indi-
cates that different r has different weight. 
Then we define another formula for collective 
inference: 
s1?s2^lemma(s1,p1,p_lemma)^lemma(s2,p2, 
p_lemma)^lemma(s1,a1,a_lemma)^lemma(s2,
a2,a_lemma)^role(s2,p2,a2,+r)=>final_role 
(s1,p1,a1,+r)                                                 (2) 
Here p_lemma(a_lemma) stands for the lemma 
of the predicate(argument), which is obtained 
from the lemma dictionary. This dictionary is 
extracted from the dataset of CoNLL 2008 
shared task and is normalized using synonym 
dictionary described in Section 2; lemma is an 
observed predicate that states whether or not the 
word has the lemma. 
Formula 2 encodes our basic ideas about col-
lective SRL: given several sentences expressing 
similar meaning, if one sentence has a predicate 
p with an argument a of role r, the other sen-
tences would be likely to have a predicate p? 
with an argument a? of role r, where p? and a? 
are the same or synonymous with p and a, re-
spectively, as illustrated by the example in Sec-
tion 1. 
Besides, we also apply structural constraints 
(Riedel and Meza-Ruiz, 2008) to final_role. 
To learn parameters of the collective infer-
ence system, we use  thebeast (Riedel and Meza-
Ruiz, 2008),  which is an open Markov Logic 
Engine, and train it on manually annotated news 
corpus described in Section 4. 
4 Experiments 
To train and test the collective inference system, 
we extract 1000 sentences from news clusters, 
and group them into 200 clusters using the 
method described in Section 2. For every sen-
tence, POS tagging is conducted with the 
OpenNLP toolkit (Jason Baldridge et al, 2009), 
lemma of each word is obtained through the 
normalized lemma dictionary described in Sec-
tion 3, and SRL is manually labeled. To reduce 
human labeling efforts, we retrain our baseline 
on the WSJ corpus of CoNLL 2008 shared task 
and run it on our news corpus, and then edit the 
SRL outputs by hand. 
We implement the collective inference system 
with the thebeast toolkit. Precision, recall, and 
F-score are used as evaluation metrics.  In both 
training and evaluation, we follow the CoNLL 
2008 shared task and regard only heads of 
phrases as arguments. 
727
Table 1 shows the averaged 10-fold cross val-
idation results of our systems and the baseline, 
where the third and second line report the results 
of using and not using Formula 1 in our collec-
tive inference system, respectively. 
 
Systems Pre. (%) Rec. (%) F-score (%) 
Baseline 69.87 59.26 64.13 
CI-1 62.99 72.96 67.61 
CI 67.01 68.33 67.66 
Table 1. Averaged 10-fold cross validation re-
sults (Pre.: precision; Rec.: recall). 
Experimental results show that the two collec-
tive inference engines (CI-1 and CI) perform 
significantly better than the baseline in terms of 
the recall and F-score, though a little worse in 
the precision. We observe that predicate-
argument relationships in sentences with com-
plex syntax are usually not recognized by the 
baseline, but some of them are correctly identi-
fied by the collective inference systems. This, 
we guess, explains in large part the difference in 
performance. For instance, consider the follow-
ing sentences in a group, where order and tell 
are synonyms: 
? Colombia said on Sunday it will appeal 
to the U.N. Security Council and the 
OAS after Hugo Chavez, the fiery leftist 
president of neighboring Venezuela, or-
dered his army to prepare for war in or-
der to assure peace. 
? President Hugo Chavez ordered Vene-
zuela's military to prepare for a possible 
armed conflict with Colombia, saying 
yesterday that his country's soldiers 
should be ready if the U.S. tries to pro-
voke a war between the South American 
neighbors. 
? Venezuelan President Hugo Chavez told 
his military and civil militias to prepare 
for a possible war with Colombia as ten-
sions mount over an agreement giving 
U.S. troops access to Colombian mili-
tary bases. 
The baseline cannot label (ordered, Chavez, A0) 
for the first sentence, partially owing to the syn-
tactic complexity of the sentence, but can identi-
fy the relationship for the second and third sen-
tence. In contrast, the collective inference sys-
tems can identify Chavez in the first sentence as 
A0 of order because of its occurrence in the oth-
er sentences of the same group. 
As Table 1 shows, the CI system achieves the 
highest F-score (67.66%), and a higher precision 
than the CI-1 system, indicating the effective-
ness of Formula 1. Consider the above three sen-
tences. CI-1 mislabels (ordered, Venezuela, A1) 
for the first sentence because the baseline labels 
it for the second sentence. In contrast, CI does 
not label it for the first sentence because the 
baseline does not and (ordered, Venezuela, A1) 
rarely occurs in the outputs of the baseline for 
this sentence group. 
We also find cases where the collective infer-
ence systems do not but should help. For exam-
ple, consider the following group of sentences: 
? A Brazilian university expelled a woman 
who was heckled by hundreds of fellow 
students when she wore a short, pink 
dress to class, taking out newspaper ads 
Sunday to publicly accuse her of immo-
rality.  
? The university also published newspaper 
ads accusing the student, Geisy Arruda, 
of immorality. 
The baseline has identified (published, univer-
sity, A0) for the second sentence. But neither 
the baseline nor our method labels (taking, uni-
versity, A0) for the first one.  This happens be-
cause publish is not considered as a synonym 
of take, and thus (published, university, A0) in 
the second provides no evidence for (taking, 
university, A0) in the first. We plan to develop 
a context based synonym detection component 
to address this issue in the future. 
5 Conclusions and Future Work 
We present a novel MLN-based method that col-
lectively conducts SRL on groups of sentences. 
To help build training and test corpora, we de-
sign a method to collect news sentences and to 
divide them into groups so that sentences of sim-
ilar meaning fall into the same cluster. Experi-
mental results on a manually labeled news cor-
pus show that collective inference, which lever-
ages redundancy, can effectively improve the 
performance of the baseline. 
728
In the future, we plan to evaluate our method 
on larger news corpora, and to extend our meth-
od to other genres of corpora, such as tweets. 
 
References  
Baldridge, Jason, Tom Morton, and Gann. 2009. 
OpenNLP, http://opennlp.sourceforge.net/ 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labelling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Journal of Computa-
tional Linguistics, 28(3):245?288. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses 
using Markov Logic. Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the ACL, pages: 155-
163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and 
inference in semantic role labeling. Journal of 
Computational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. 
Collective semantic role labelling with Markov 
Logic. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 193-197. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
 
729
