Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 276?285,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Wikipedia as Frame Information Repository
Sara Tonelli
FBK-irst
I-38100, Trento, Italy
satonelli@fbk.eu
Claudio Giuliano
FBK-irst
I-38100, Trento, Italy
giuliano@fbk.eu
Abstract
In this paper, we address the issue of au-
tomatic extending lexical resources by ex-
ploiting existing knowledge repositories.
In particular, we deal with the new task
of linking FrameNet and Wikipedia us-
ing a word sense disambiguation system
that, for a given pair frame ? lexical unit
(F, l), finds the Wikipage that best ex-
presses the the meaning of l. The mapping
can be exploited to straightforwardly ac-
quire new example sentences and new lex-
ical units, both for English and for all lan-
guages available in Wikipedia. In this way,
it is possible to easily acquire good-quality
data as a starting point for the creation of
FrameNet in new languages. The evalua-
tion reported both for the monolingual and
the multilingual expansion of FrameNet
shows that the approach is promising.
1 Introduction
Many applications in the context of natural lan-
guage processing or information retrieval have
proved to convey significant improvement by ex-
ploiting lexical databases with high-quality anno-
tation such as FrameNet (Fillmore et al, 2003)
and WordNet (Fellbaum, 1998). Nevertheless, the
practical use of similar resources is often biased
by their limited coverage because manual anno-
tation is time-consuming and requires a relevant
financial effort. For this reason, some research ac-
tivities have focused on the automatic enrichment
of such resources with annotated information in
(near) manual quality. The main strategy proposed
was the mapping between resources in order to
reciprocally enrich different lexical databases by
linking their information layers. This has proved
to be useful in several tasks, from verb classifica-
tion (Chow and Webster, 2007) to semantic role
labeling (Giuglea and Moschitti, 2006), open text
semantic parsing (Shi and Mihalcea, 2004) and
textual entailment (Burchardt and Frank, 2006).
In this work, we focus on the automatic enrich-
ment of the FrameNet database for English and we
propose a new framework to extend this procedure
to new languages. While similar works in the past
have mainly proposed to automatically extend the
FrameNet database by mapping frames and Word-
Net synsets (Shi and Mihalcea (2005), Johans-
son and Nugues (2007), and Tonelli and Pighin
(2009)), we present an explorative approach that
for the first time exploits Wikipedia to this pur-
pose. In particular, given a lexical unit l belong-
ing to a frame F , we devise a strategy to link
l to the Wikipedia article that best captures the
sense of l in F . This is basically a word disam-
biguation (WSD) problem (Erk, 2004) and to this
purpose we employ a state-of-the-art WSD sys-
tem (Gliozzo et al, 2005). The mapping between
(F, l) pairs and Wikipedia pages could then be ex-
ploited for three further subtasks: (a) automati-
cally extract from Wikipedia all sentences point-
ing to the Wikipage mapped with (F, l) and assign
them to F ; (b) automatically expand the lexical
units sets in the English FrameNet by exploiting
the redirecting and linking strategy of Wikipedia;
and (c) since Wikipedia is available in 260 lan-
guages, use the English Wikipedia article linked to
(F, l) as a bridge to carry out sentence and lexical
unit retrieval in other languages. The set of auto-
matically collected data would represent the start-
ing point for the creation of FrameNet in new lan-
guages. In fact, having a repository of sentences
extracted from Wikipedia which have already been
divided by sense would significantly speed up the
annotation process. In this way, the annotators
would not need to extract all sentences in a cor-
pus containing l and classify them by sense. In-
stead, they should simply validate the given sen-
tences and assign the correct frame elements.
276
In the following, we start by providing a brief
overview of FrameNet and Wikipedia and we
present their structure and organization. Next, we
describe the algorithm for mapping lexical units
and Wikipages and the word sense disambigua-
tion algorithm employed by the system. In Sec-
tion 5 we describe the dataset used in the first ex-
periment and report evaluation results of the map-
ping between (F, l) pairs and Wikipedia senses. In
Section 6 we describe an application of the map-
ping, i.e. the automatic enrichment of English
FrameNet. We describe the data extraction pro-
cess and evaluate the quality of the data. In Section
7 we describe and evaluate another application of
the mapping, i.e. the acquisition of data for the
automatic creation of Italian FrameNet using the
Italian Wikipedia. Finally, we draw conclusions
and present future research directions.
2 FrameNet and Wikipedia
FrameNet (Fillmore et al, 2003) is a lexical re-
source for English based on corpus evidence,
whose conceptual model comprises a set of proto-
typical situations called frames, the frame-evoking
words or expressions called lexical units (LUs)
and the roles or participants involved in these situ-
ations, called frame elements. All lexical units be-
longing to the same frame have similar semantics
but, differently from WordNet synsets, they can
belong to different categories and present differ-
ent parts of speech. For example, the KILLING
frame is described in the FrameNet database
1
as ?A Killer or Cause causes the death of the
Victim?. The elements in capitals are the se-
mantic roles (frame elements) typically involved
in the KILLING situation. The frame definition
comes also with the list of frame-evoking lexical
units, namely annihilate.v, annihilation.n, butch-
ery.n, carnage.n, crucify.v, deadly.a, etc. Since
FrameNet is a corpus-based resource, every lexi-
cal unit should be instantiated by a set of exam-
ple sentences, where the frame elements are anno-
tated as well. Instead, FrameNet is still an ongoing
project and in the latest release (v. 1.3) there are
about 3,380 lexical units out of 10,195 that come
with no example sentences. In this work we focus
on these lexical units and propose how to automat-
ically collect the missing sentences. Anyhow, the
algorithm we propose is suitable also for expand-
ing sentence sets already present in FrameNet.
1
http://framenet.icsi.berkeley.edu
Wikipedia
2
is one of the largest online reposito-
ries of encyclopedic knowledge, with millions of
articles available for a large number of languages
(>2,800,000 for English). The article (or page)
is the basic entry in Wikipedia. Every article has
an unique reference, i.e., one or more words that
identify the page and are present in its URL. For
example, Ball (dance) identifies the page that de-
scribes several types of ball intended as formal
dance, while Dance (musical form) describes the
dance as musical genre. Every Wikipedia article
is linked to others, and in the body of every page
there are plenty of links to connect the most rel-
evant terms to other pages. Another important
attribute is the presence of about 3,000,000 redi-
rection pages, that given an identifier that is not
present in Wikipedia, automatically display the
page with the most semantically similar identi-
fier (for example Killing is redirected to the Mur-
der page). Wikipedia contains also more than
100,000 disambiguation pages listing all senses
(pages) for an ambiguous entity. For example,
Book has 9 senses, which correspond to 9 dif-
ferent articles. Wikipedia structure and quality
make this resource particularly suitable for infor-
mation extraction and word sense disambiguation
tasks (Csomai and Mihalcea (2008) and Milne and
Witten (2008)). In fact, page references can be
seen as senses and Wikipedia as a large sense in-
ventory. From this point of view, also linking
a lexical unit to the correct Wikipedia page is a
word sense disambiguation issue because it im-
plies recognizing what meaning the lexical unit
has in the given frame. For example, dance.n
in the SOCIAL EVENT frame should be linked to
Ball (dance) and not to Dance (musical form).
3 The Mapping Algorithm
In this section, we describe how to map a frame
? lexical unit pair (F, l) into the Wikipedia arti-
cle that best captures the sense of l as defined in
F . The mapping problem is casted as a supervised
WSD problem, in which l must be disambiguated
using F to provide the context and Wikipedia to
provide the sense inventory and the training data.
Even if the idea of using Wikipedia links for dis-
ambiguation is not novel (Cucerzan, 2007), it is
applied for the first time to FrameNet lexical units,
considering a frame as a sense definition. The pro-
posed algorithm is summarized as follows:
2
http://en.wikipedia.org
277
Step 1 For each lexical unit l, we collect from
the English Wikipedia dump
3
all contexts
4
where l
is the anchor of an internal link (wiki link). The set
of targets represents the senses of l in Wikipedia
and the contexts are used as labelled training ex-
amples. For example, the lexical unit building.n in
the frame Buildings is an anchor in 708 different
contexts that point to 42 different Wikipedia pages
(senses).
Step 2 The set of contexts with their correspond-
ing senses is then used to train the WSD system
described in Section 4. For example, the context
?The building, which date from the mid-to-late
19th century, were built in a variety of High Victo-
rian architectural styles.? is a training example for
the sense defined by the Wikipedia page Building.
Step 3 Finally, the disambiguation model
learned in the previous step is used to map a pair
(F, l) to a Wikipedia article. (F, l) is represented
as a fictitious-context derived by aggregating the
frame definition and all lexical units associated to
F . We used the term ?fictitious-context? to re-
mark the slight difference in structure compared
with the training contexts (i.e., the Wikipedia
paragraphs). For example, ?. . . structures form-
ing an enclosure and providing protection from
the elements . . . acropolis arena auditorium bar
building . . . ? is the fictitious-context built for
the pair (Buildings, building.n). The sense, i.e.,
the Wikipedia article, assigned to the fictitious-
context by the disambiguation algorithm uniquely
defines the mapping. The previous example is as-
signed to the Wikipedia page Building.
4 The WSD Algorithm
Gliozzo et al (2005) proposed an elegant approach
to WSD based on kernel methods. The algorithm
proved effective at Senseval-3 (Mihalcea and Ed-
monds, 2004) and, nowadays, it still represents
the state-of-the-art in WSD (Pradhan et al, 2007).
Specifically, they addressed these issues: (i) inde-
pendently modeling domain and syntagmatic as-
pects of sense distinction to improve feature rep-
resentativeness; and (ii) exploiting external knowl-
edge acquired from unlabeled data, with the pur-
pose of drastically reducing the amount of labeled
3
http://download.wikimedia.org/enwiki/
20090306
4
A context corresponds to a line of text in the Wikipedia
dump and it is represented as a paragraph in a Wikipedia ar-
ticle.
training data. The first direction is based on the
linguistic assumption that syntagmatic and domain
(associative) relations are crucial for representing
sense distictions, but they are originated by differ-
ent phenomena. Regarding the second direction, it
is possible to obtain a more accurate prediction by
taking into account unlabeled data relevant for the
learning problem (Chapelle et al, 2006).
On the other hand, kernel methods are theoret-
ically well founded in statistical learning theory
and shown good empirical results in many appli-
cations (Shawe-Taylor and Cristianini, 2004). The
strategy adopted by kernel methods consists of
splitting the learning problem into two parts. They
first embed the input data in a suitable feature
space, and then use a linear algorithm (e.g., sup-
port vector machines) to discover nonlinear pat-
terns in the input space. The kernel function is
the only task-specific component of the learning
algorithm. Thus, to develop a WSD system, one
only needs to define appropriate kernel functions
to represent the domain and syntagmatic aspects
of sense distinction and to exploit the properties
of kernel functions in order to define a composite
kernel that combines and extends individual ker-
nels.
The WSD system described in the following
consists of a composite kernel (Section 4.3) that
combines the domain and syntagmatic kernels.
The former (Section 4.1) models the domain as-
pects of sense distinction, the latter (Section 4.2)
represents the syntagmatic aspects of sense dis-
tinction.
4.1 Domain Kernel
It is been shown that domain information is fun-
damental for WSD (Magnini et al, 2002). For in-
stance, the (domain) polysemy between the com-
puter science and the medicine senses of the word
?virus? can be solved by considering the domain
of the context in which it appears.
In the context of kernel methods, domain infor-
mation can be exploited by defining a kernel func-
tion that estimates the domain similarity between
the contexts of the word to be disambiguated. The
simplest method to estimate the domain similarity
between two texts is to compute the cosine simi-
larity of their vector representations in the vector
space model (VSM). The VSM is a k-dimensional
space R
k
, in which the text t
j
is represented by
a vector
~
t
j
, where the i
th
component is the term
278
frequency of the term w
i
in t
j
. However, such an
approach does not deal well with lexical variabil-
ity and ambiguity. For instance, despite the fact
that the sentences ?he is affected by AIDS? and
?HIV is a virus? express concepts closely related,
their similarity is zero in the VSM because they
have no words in common (they are represented
by orthogonal vectors). On the other hand, due
to the ambiguity of the word ?virus? , the simi-
larity between the sentences ?the laptop has been
infected by a virus? and ?HIV is a virus? is greater
than zero, even though they convey very different
messages.
To overcome this problem, Gliozzo et al (2005)
introduced the domain model (DM) and show how
to define a domain VSM in which texts and terms
are represented in a uniform way. A DM is com-
posed of soft clusters of terms. Each cluster rep-
resents a semantic domain, that is, a set of terms
that often co-occur in texts having similar topics.
A DM is represented by a k ? k
?
rectangular ma-
trix D, containing the degree of association among
terms and domains.
The matrix D is used to define a function D :
R
k
? R
k
?
, that maps the vector
~
t
j
represented in
the standard VSM, into the vector
~
t
?
j
in the domain
VSM. D is defined by
D(
~
t
j
) =
~
t
j
(I
IDF
D) =
~
t
?
j
, (1)
where
~
t
j
is represented as a row vector, I
IDF
is a
k?k diagonal matrix such that i
IDF
i,i
= IDF (w
i
),
and IDF (w
i
) is the inverse document frequency
of w
i
.
In the domain space, the similarity is esti-
mated by taking into account second order rela-
tions among terms. For example, the similarity of
the two sentences ?He is affected by AIDS? and
?HIV is a virus? is very high, because the terms
AIDS, HIV and virus are strongly associated with
the domain medicine.
Singular valued decomposition (SVD) is used to
acquire in a unsupervised way the DM from a cor-
pus represented by its term-by-document matrix
T. SVD decomposes the term-by-document ma-
trix T into three matrixes T ' V?
k
?
U
T
, where
V and U are orthogonal matrices (i.e., V
T
V = I
and U
T
U = I) whose columns are the eigenvec-
tors of TT
T
and T
T
T respectively, and ?
k
?
is
the diagonal k ? k matrix containing the highest
k
?
 k eigenvalues of T, and all the remaining
elements set to 0. The parameter k
?
is the dimen-
sionality of the domain VSM and can be fixed in
advance. Under this setting, the domain matrix D
is defined by
D = I
N
V
p
?
k
?
(2)
where I
N
is a diagonal matrix such that i
N
i,i
=
1
q
?
~
w
?
i
,
~
w
?
i
?
,
~
w
?
i
is the i
th
row of the matrix V
?
?
k
?
.
The domain kernel is explicitly defined by
K
D
(t
i
, t
j
) = ?D(t
i
),D(t
j
)?, (3)
where D is the domain mapping defined in Equa-
tion 1. Finally, the domain kernel is further ex-
tended to include the standard bag-of-word kernel.
4.2 Syntagmatic Kernel
Kernel functions are not restricted to operate on
vectorial objects ~x ? R
k
. In principle, kernels
can be defined for any kind of object representa-
tion, such as strings and trees. As syntagmatic re-
lations hold among words collocated in a partic-
ular temporal order, they can be modeled by ana-
lyzing sequences of words. Therefore, the string
kernel (Shawe-Taylor and Cristianini, 2004) is a
valid tool to represent such relations. It counts
how many times a (non-contiguous) subsequence
of symbols u of length n occurs in the input string
s, and penalizes non-contiguous occurrences ac-
cording to the number of gaps they contain. For-
mally, let V be the vocabulary, the feature space
associated with the string kernel of length n is in-
dexed by a set I of subsequences over V of length
n. The (explicit) mapping function is defined by
?
n
u
(s) =
X
i:u=s(i)
?
l(i)
, u ? V
n
, (4)
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and ? ?]0, 1] is the decay factor used to pe-
nalize non-contiguous subsequences.
The associated string kernel is defined by
K
n
(s
i
, s
j
) = ??
n
(s
i
), ?
n
(s
j
)? =
X
u?V
n
?
n
(s
i
)?
n
(s
j
)
(5)
Gliozzo et al (2005) modified the generic def-
inition of the string kernel in order to take into
account (sparse) collocations. Specifically, they
defined syntagmatic kernels as a combination of
string kernels applied to sequences of words in a
fixed-size window centered on the word to be dis-
ambiguated. This formulation allows estimating
the number of common (sparse) subsequences of
279
words (i.e., collocations) between two examples,
in order to capture syntagmatic similarity. The
syntagmatic kernel is defined by
K
S
(s
i
, s
j
) =
p
X
n=1
K
n
(s
i
, s
j
), (6)
where K
n
is the string kernel defined in Equation
5 and the parameter n represents the length of the
subsequences analyzed when estimating the sim-
ilarity between contexts. Notice that the syntag-
matic kernel is only effective for those fictitious
contexts in which the lexical units do occur in
meaningful sentences, however this is not guaran-
teed for the lexical units without examples.
4.3 Composite Kernel
Finally, to combine domain and syntagmatic infor-
mation, the composite kernel is defined by
K
WSD
(t
i
, t
j
) =
?
K
D
(t
i
, t
j
) +
?
K
S
(t
i
, t
j
), (7)
where
?
K
D
and
?
K
S
are normalized kernels defined
in Equation 3 and 6, respectively.
5
It follows di-
rectly from the explicit construction of the feature
space and from closure properties of kernels that
it is a valid kernel.
5 Mapping task
In this section we report the first experiment,
namely the mapping between (F, l) pairs and a
Wikipedia pages. We describe the experimental
setup and then present the corresponding evalua-
tion.
5.1 Experimental setup
We applied our algorithm to all lexical units that
do not have any example sentence in the FrameNet
database. In principle, the proposed approach can
be applied to every lexical unit, and we expect the
algorithm performance to improve if some exam-
ple sentences are already available because they
could be added to the fictitious-context used to
represent (F, l) in the system. Nevertheless, in this
explorative study we wanted to focus on the harder
cases, even if results are likely to be worse than on
the whole FrameNet database.
In FrameNet, 3,305 (F, l) pairs have no exam-
ple sentences (536 pairs with adjectival LU, 1313
verbal LU, 1456 nominal LU). Since Wikipedia is
basically a resource organized by concepts, which
5
?
K(x
i
, x
j
) =
K(x
i
,x
j
)
?
K(x
j
,x
j
)K(x
i
,x
i
)
are generally expressed by nouns, we decided to
restrict our experiment to nominal lexical units.
Besides, many verbal and adjectival concepts in
Wikipedia are redirected to nominal identifiers.
So, we randomly selected 900 pairs with nominal
lexical units. For the moment, we decided to dis-
card lexical units expressed by multiwords (about
150), which will be taken into account in a future
version of our system. The average ambiguity of
the 900 LUs considered is 1.24 in FrameNet. In-
stead, every LU corresponds to about 35 candidate
senses in Wikipedia.
In order to perform WSD, we built the domain
model from the 200,000 most visited Wikipedia
articles. After removing terms that occur less than
5 times, the resulting dictionaries contain about
300,000 terms. We used the SVDLIBC pack-
age
6
to compute the SVD, truncated to 100 di-
mensions. The experiments were performed using
the SVM package LIBSVM (Chang and Lin, 2001)
customized to embed the kernels described in Sec-
tion 4.
5.2 Evaluation
In this first evaluation step, we focus on the quality
of the mapping between (F, l) pairs and Wikipedia
articles. In order to evaluate the system output,
we created a gold standard where 250 (F, l) pairs
randomly extracted from the nominal subset de-
scribed above have been manually linked to the
Wikipedia page (if available) that best corresponds
to the meaning of l in F . The pairs have been cho-
sen in order to maximize the frame variability, i.e.
every pair corresponds to a different frame. Since
our gold standard contains 34% of all frames in
the FrameNet database, we believe that, despite its
limited size, it is well representative of FrameNet
characteristics. Evaluation was carried out com-
paring the system output against the gold stan-
dard. Results are reported in Table 1. The base-
line was computed considering the most frequent
sense of every lexical unit in Wikipedia. This ele-
ment is obtained by taking into account all occur-
rences in Wikipedia where the lexical unit LU we
consider is anchored to a given page. The most
frequent sense for LU is the page to which LU is
most frequently linked in Wikipedia. Since about
14% of the lexical units in the gold standard are
not present in Wikipedia, we also estimated an up-
per bound accuracy of 0.86. This confirms our in-
6
http://tedlab.mit.edu/
?
dr/svdlibc/
280
tuition that FrameNet and Wikipedia are linkable
resources to a large extent and that our task is well-
founded.
Accuracy
Baseline 0.66
System output 0.71
Upper bound 0.86
Table 1: Accuracy evaluation.
Wrong assignments include also problematic
cases that are not directly connected to proper sys-
tem errors. One of the most relevant issues is the
different granularity between FrameNet frames
and Wikipages. For example, the NETWORK
frame is defined as ?a set of entities of the same
or similar types (Nodes) are linked to each other
by Connections to form a Network allowing for
the flow of information, resources, etc.?. Even
if the listed lexical units (network.n and web.n)
and some examples refer to the informatics do-
main, the situation described in the FrameNet
database is more general. Wikipedia instead lists
several pages that may be seen as subdomains
of NETWORK such as Computer network, So-
cial network, Telecommunications network, etc.
In the future, it may be worth modifying the sys-
tem in order to allow multiple assignments of
Wikipages for every frame.
In other cases, frame definitions seem not to
be very consistent and it is very difficult to dis-
criminate between two frames even for a human
annotator. For example, ESTIMATED VALUE and
ESTIMATING include both estimation.n as lexical
unit, but since their frame definitions are almost
the same and the other lexical units in the same
frame are not discriminative, the system links both
(F, l) pairs to the same Wikipedia article.
6 English FrameNet expansion
In the following part of the experiment, we want
to investigate to what extent the FrameNet ?
Wikipedia mapping can be effectively applied to
automatically expand the FrameNet database with
new example sentences, and eventually to acquire
new lexical units. For every (F, l) pair, we con-
sider the linked Wikipedia sense s and extract all
sentences C
s
in Wikipedia with a reference to
s. In this way, we can assume that, if s was
linked to (F, l), C
s
can be included in the exam-
ple sentences of F . This repository of sentences
is already divided by sense and can significantly
speed-up manual annotation. On the other hand,
the extracted sentences could enrich the training
set of machine learning systems for frame annota-
tion to improve the frame identification step. In
fact, this task has raised growing interest in the
NLP community, with a devoted challenge at the
last SemEval campaign (Baker et al, 2007).
This retrieval process allows also to ex-
tract from C
s
all words W
s
that have an
embedded reference to s in the form <a
href=?/wiki/Wiki Sense?...>word</a>. In this
way, W
s
are automatically included in F as new
lexical units. In this phase, redirecting links are
very useful because they automatically connect a
word or expression to its nearest sense in case
there is no specific page for this word. The infor-
mation about redirecting allows also to account for
orthographic variations of the same lexical unit,
for example collectible is redirected to collectable.
We explain the data extraction process in
the light of an example from our dataset.
Our WSD system assigned to the (F, l) pair
(WORD RELATIONS ? homonym.n) the Wikipage
http://en.wikipedia.org/wiki/Homonym.
So, we extracted from the English Wikipedia
dump all sentences where the anchor <a
href=?/wiki/Homonym?... > appears and as-
sumed that the word or multiword expression that
is linked to the Homonym site may be a good can-
didate as lexical unit for the WORD RELATIONS
frame. In this case, the example sentences were
186. Apart from homonym, the candidate lexical
units are homograph, homophone, homophonous,
homonymic, heteronym, same. Among them, only
the latter is not appropriate, even if the sentence
where it occurs is semantically connected to the
WORD RELATIONS frame: ?In Hebrew the word
?thus? has the same triconsonantal root?. Instead,
homonymic and heteronym can be acquired as
new lexical units for WORD RELATIONS, and
homograph, for which no example sentences
are provided in FrameNet, can be automatically
instantiated by a set of examples.
6.1 Experimental setup
We considered 893 frame ? lexical unit pairs as-
signed to Wikipedia pages following the algorithm
described in Section 3. We discarded 7 pairs for
which the system reported an assignment failure,
i.e. the best sense delivered is the disambigua-
281
tion page. Then we extracted a set of sentences
for every (F, l) pair as described in the previous
paragraph. Statistics about the retrieved data is re-
ported in Table 2.
English Wikipedia
(F, l) pairs 893
N. of extracted sents 964,268
Avg. sents per (F, l) 1,080
Table 2: Extracted data from English Wikipedia
6.2 Evaluation
The dimension of the extracted corpus does not
allow to carry out a comprehensive evaluation.
For this reason, we manually evaluated 1,000 sen-
tences, i.e. we considered 20 (F, l) pairs, and for
each of them we evaluated 50 sentences extracted
from our large repository. Both (F, l) pairs and
the assigned sentences were randomly selected.
In particular, the 20 (F, l) pairs do not contain
only correctly assigned pairs, in fact three of them
are wrong. Anyhow, the 20 pairs seem to be
a representative subset of the 893 pairs consid-
ered in our experiment because they include both
monosemic lexical units (gynaecology.n in MED-
ICAL SPECIALTIES) and more ambiguous ones
(club.n in the WEAPON frame).
Our evaluation shows that 78% of the sentences
were correctly linked to (F, l) pairs. This value is
higher that the mapping accuracy between (F, l)
and Wikipages reported in Section 5.2. In fact,
we noticed that even if the Wikipage assigned to
(F, l) is not the article that best corresponds to the
meaning of l in F , some sentences pointing to it
may be appropriate to express l.
As we already mentioned in Section 5.2,
the different granularity of the information en-
coded by frames and Wikipages impacts on
the output quality. For example, conversion.n
in CAUSE CHANGE has a causative meaning,
while it implies a personal process in UN-
DERGO CHANGE. The mapping, instead, links
(CAUSE CHANGE ? conversion.n) to the Reli-
gious conversion page, and all the sentences col-
lected point to religious conversion, regardless of
their causative form or not. Another characteristic
of this approach is that we can acquire new lexi-
cal units regardless of their part-of-speech, even if
we start from nominal lexical units. This proves
that we do not need to apply the initial mapping to
verbal or adjectival LUs to obtain new data for all
parts of speech. For example, we linked (MEDI-
CAL SPECIALTIES ? gynaecology.n) to the Gynae-
cology Wikipage. Consequently, we could include
the adjective gynaecologic, pointing to the Gy-
naecology page, into the MEDICAL SPECIALTIES
frame for sentences like ?Fellowship training in a
gynaecologic subspeciality can range from one to
four years?. However, this advantage can also turn
into a weakness, because gynaecologist is also
redirected to the Gynaecology page, but it belongs
to MEDICAL PROFESSIONALS and should not be
included into MEDICAL SPECIALTIES.
For the 20 (F, l) pairs considered in the given
sentences, it was possible also to retrieve 8 lex-
ical units that are not present in FrameNet, for
example billy-club for the WEAPON frame. Ex-
ploiting redirections and anchoring strategies, our
induction method can account for orthographical
variations, for example it acquires both memorize
and memorise. On the other hand, also misspelled
words may be collected, for instance gynaecolo-
gial instead of gynaecological.
7 Multilingual FrameNet expansion
One of the great advantages of Wikipedia is its
availability in several languages. The English ver-
sion is by far the most extended, but a considerable
repository of pages is available also for other lan-
guages, esp. European ones. In general, articles on
the same object in different languages are edited
independently and do not have to be translations
of one another, but are linked to each other by their
authors. In this way, the multilingual versions of
Wikipedia can be easily exploited to build compa-
rable corpora, with connected Wikipages in differ-
ent languages dealing with the same contents.
In this research step, we focus on this aspect of
Wikipedia and propose a methodology that, using
the English Wikipages as a bridge, automatically
acquires new lexical units and example sentences
also for other languages. This would represent the
starting point towards the creation of FrameNet
for new languages. Indeed, FrameNet structure
comprises a language-independent level of infor-
mation, namely frame and frame element defini-
tions, and a language-dependent one, i.e. the lex-
ical units and the example sentences. This makes
the resource particularly suitable to corpus-based
(semi) automatic creation of FrameNet for new
languages, because the descriptive part can be pre-
282
served and the language-dependent layer can be
populated with new instances in other languages
(Crespo and Buitelaar, 2008).
We apply our extraction algorithm to the Italian
Wikipedia. Since several approaches have been
experimented to (semi) automatically build Italian
FrameNet using WordNet (De Cao et al (2008)
and Tonelli and Pighin (2009)), we believe that
our new proposal to exploit Wikipedia may be of
interest in the research community. Anyhow, the
approach can be exploited in principle for every
language available in Wikipedia.
7.1 Experimental setup
Similarly to the data extraction process described
in Section 6, we consider for every (F, l) pair in
English the linked Wikipedia sense s, in English
as well. Then, we retrieve the Italian Wikipedia
sense s
i
linked to s and extract all sentences C
i
in the Italian Wikipedia dump
7
with a reference to
s
i
. In this way, we can assume that C
i
are exam-
ple sentences of F and that the words or expres-
sions W
i
in C
i
containing an embedded reference
to s
i
are good candidate lexical units of F in the
Italian FrameNet. For example, if we link http:
//en.wikipedia.org/wiki/Court to the JUDI-
CIAL BODY frame, we first retrieve the Italian
version of the site http://it.wikipedia.org/
wiki/Tribunale. Then, with a top-down strat-
egy, we further extract all Italian sentences point-
ing to the Tribunale page and acquire as lexi-
cal units all words with an embedded reference to
this concept, for example tribunale and corte. In
this way, we can include the extracted lexical units
and the sentences where they occur in the JUDI-
CIAL BODY frame for Italian.
Given the 893 (F, l) pairs in English and the
linked Wikipedia senses described in 6.2, we first
extracted the Italian Wikipages that are linked to
the English ones. Then for every linked Wikipage
in Italian, we retrieved all sentences with a refer-
ence pointing to that page in the Italian Wikipedia
dump. Statistics about the extracted data are re-
ported in Table 3.
Since the Italian Wikipedia is about one fifth of
the English one, it was not possible to map ev-
ery English Wikipage with an Italian article. In
fact, only 371 senses out of 893 in English were
linked to an Italian page. Also the average num-
7
http://download.wikimedia.org/itwiki/
20090203
Italian Wikipedia
Linked Wikipages in Italian 371
N. of extracted sents 23,078
Avg. sents per Italian sense 62
Table 3: Extracted data from Italian Wikipedia
ber of sentences extracted for every sense is much
smaller (62 vs. 1,080). Anyhow, this does not rep-
resent a problem because in the English FrameNet,
the lexical units whose annotation is considered
to be complete are usually instantiated by set of
20 annotated sentences on average. So, according
to the FrameNet standard, 60 sentences are more
than enough to represent the meaning of a lexical
unit in a frame.
7.2 Evaluation
In this evaluation part, we took into account 1,000
sentences, in order to have a comparable dataset
w.r.t. the evaluation for English. However, the sets
of Italian sentences extracted for every (F, l), i.e.
for every Wikipedia article, were much smaller,
so we increased the number of randomly chosen
(F, l) pairs to 80. Our evaluation is focused on the
quality of the sentences and aims at assessing if the
given sentences are correctly assigned to the (F, l)
pairs. We report 69% accuracy, which is 9% lower
than for English. Apart from the same errors and
issues reported for English, a decrease in perfor-
mance can be explained by the fact that, since less
articles are present w.r.t. the English version, redi-
rections and internal links tend to be less precise
and fine-grained. For example, the word ?diritti?
in the sense of ?(human) rights? redirects to the ar-
ticle about Diritto, corresponding to Law as a sys-
tem of rules. On the contrary, Law and Rights have
two different pages in English. Besides, the differ-
ent quality of the two resources can also depend
on the smaller number of users that edit and check
the Italian articles. From the 1,000 sentences eval-
uated we extracted 145 new lexical units: since
Italian FrameNet does not exist yet, every lexical
unit in a sentence that is correct can be straightfor-
wardly included in the first version of the resource.
8 Conclusions and Future work
In this work, we have proposed to apply a
word sense disambiguation system to a new
task, namely the linking between FrameNet and
Wikipedia. Results are promising and show that
283
the task is adequately substantiated. The proposed
approach can help enriching FrameNet with new
example sentences and lexical units and provide a
starting point for the creation of FrameNet-like re-
sources in all Wikipedia languages. On the one
hand, the retrieved data could speed up human
annotation, requiring only a manual validation.
On the other hand, the extracted sentences could
provide enough training data to machine learning
systems for frame assignment, since insufficient
frame attestations in the FrameNet database are a
major problem for such systems.
In the next research step, we plan to carry out an
extended evaluation process in order to compute
inter-annotator agreement and eventually point out
validation problems. Then, we want to extend
the mapping and the data extraction process to all
(F, l) pairs in FrameNet (about 10,000). The re-
trieved sentences will be made available as train-
ing or annotation material. Besides, we want
to create an online resource where the links be-
tween (F, l) pairs and Wikipages are made explicit
and where users can browse the retrieved sen-
tences. The resource can be produced and made
available with a reduced effort for every language
in Wikipedia. Anyway, the English version has
proved to be more precise, while the resource for
new languages would require a more accurate re-
vision.
Acknowledgments
Claudio Giuliano is supported by the ITCH
project (http://itch.fbk.eu), sponsored
by the Italian Ministry of University and Re-
search and by the Autonomous Province of
Trento and the X-Media project (http://www.
x-media-project.org), sponsored by the
European Commission as part of the Information
Society Technologies (IST) programme under EC
grant number IST-FP6-026978.
References
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 10: Frame Semantic
Structure Extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, CZ, June.
Aljoscha Burchardt and Anette Frank. 2006. Approxi-
mating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of the 2nd PASCAL RTE
Workshop, pages 92?97, Venice, Italy.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining Word Sense
and Usage for modeling Frame Semantics. In Pro-
ceedings of STEP 2008, Venice, Italy.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/
?
cjlin/libsvm.
Olivier Chapelle, Bernhard Sch?olkopf, and Alexander
Zien. 2006. Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Ian Chow and Jonathan Webster. 2007. Integra-
tion of Linguistic Resources for Verb Classification:
FrameNet Frame, WordNet Verb and Suggested Up-
per Merged Ontology. Computational Linguistics
and Intelligent Text Processing, pages 1?11.
Mario Crespo and Paul Buitelaar. 2008. Domain-
specific English-to-Spanish Translation of
FrameNet. In Proc. of LREC 2008, Marrakech.
Andras Csomai and Rada Mihalcea. 2008. Linking
Documents to Encyclopedic Knowledge. IEEE In-
telligent Systems, special issue on ?Natural Lan-
guage Processing for the Web?.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June. Association for Computational Linguistics.
Katrin Erk. 2004. Frame assignment as Word
Sense Disambiguation. In Proceedings of IWCS-6,
Tilburg, NL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
C.J. Fillmore, C.R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International
Journal of Lexicography, 16:235?250, September.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual ACL meeting, pages 929?936, Morris-
town, US.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005.
Domain kernels for word sense disambiguation. In
Proceedings of the 43
rd
annual meeting of the As-
sociation for Computational Linguistics (ACL-05),
pages 403?410, Ann Arbor, Michigan, June.
R. Johansson and P. Nugues. 2007. Using Word-
Net to extend FrameNet coverage. In Proc. of the
Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, at NODAL-
IDA, Tartu.
284
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The Role of Domain Information
in Word Sense Disambiguation. Natural Language
Engineering, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3, Barcelona, Spain, July.
David Milne and Ian H. Witten. 2008. Learning to
link with Wikipedia. In CIKM ?08: Proceedings of
the 17th ACM conference on Information and knowl-
edge management, pages 509?518, NY, USA. ACM.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 87?
92, Prague, Czech Republic, June. Association for
Computational Linguistics.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
Lei Shi and Rada Mihalcea. 2004. Open Text Seman-
tic Parsing Using FrameNet and WordNet. In Pro-
ceedings of HLT-NAACL 2004.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings of
CICLing 2005, pages 100?111. Springer.
Sara Tonelli and Daniele Pighin. 2009. New features
for FrameNet - WordNet Mapping. In Proceedings
of the Thirteenth Conference on Computational Nat-
ural Language Learning, Boulder, CO, USA.
285
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 219?227,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
New Features for FrameNet ? WordNet Mapping
Sara Tonelli and Daniele Pighin
FBK-Irst, Human Language Technologies
Via di Sommarive, 18 I-38100 Povo (TN) Italy
{satonelli,pighin}@fbk.eu
Abstract
Many applications in the context of natural
language processing or information retrieval
may be largely improved if they were able to
fully exploit the rich semantic information an-
notated in high-quality, publicly available re-
sources such as the FrameNet and the Word-
Net databases. Nevertheless, the practical use
of similar resources is often biased by the
limited coverage of semantic phenomena that
they provide.
A natural solution to this problem would be to
automatically establish anchors between these
resources that would allow us 1) to jointly use
the encoded information, thus possibly over-
coming limitations of the individual corpora,
and 2) to extend each resource coverage by ex-
ploiting the information encoded in the others.
In this paper, we present a supervised learn-
ing framework for the mapping of FrameNet
lexical units onto WordNet synsets based on
a reduced set of novel and semantically rich
features. The automatically learnt mapping,
which we call MapNet, can be used 1) to ex-
tend frame sets in the English FrameNet, 2)
to populate frame sets in the Italian FrameNet
via MultiWordNet and 3) to add frame labels
to the MultiSemCor corpus. Our evaluation on
these tasks shows that the proposed approach
is viable and can result in accurate automatic
annotations.
1 Introduction
In recent years, the integration of manually-built
lexical resources into NLP systems has received
growing interest. In particular, resources annotated
with the surface realization of semantic roles, like
FrameNet (Baker et al, 1998) or PropBank (Palmer
et al, 2005) have shown to convey an improve-
ment in several NLP tasks, from question answer-
ing (Shen and Lapata, 2007) to textual entailment
(Burchardt et al, 2007) and shallow semantic pars-
ing (Giuglea and Moschitti, 2006). Nonetheless, the
main limitation of such resources is their poor cov-
erage, particularly as regards FrameNet. Indeed, the
latest FrameNet release (v. 1.3) contains 10,195 lex-
ical units (LUs), 3,380 of which are described only
by a lexicographic definition without any example
sentence. In order to cope with this lack of data, it
would be useful to map frame information onto other
lexical resources with a broader coverage. We be-
lieve that WordNet (Fellbaum, 1998), with 210,000
entries in version 3.0, can represent a suitable re-
source for this task. In fact, both FrameNet and
WordNet group together semantically similar words,
and provide a hierarchical representation of the lex-
ical knowledge (in WordNet the relations between
synsets, in FrameNet between frames, see Ruppen-
hofer et al (2006)). On the other hand, WordNet
provides a more extensive coverage particularly for
adjectives and nouns denoting artifacts and natural
kinds, that are mostly neglected in FrameNet.
In this paper, we present an approach using Sup-
port Vector Machines (SVM) to map FrameNet lex-
ical units to WordNet synsets. The proposed ap-
proach addresses some of the limitations of previous
works on the same task (see for example De Cao
et al (2008) and Johansson and Nugues (2007)).
Most notably, as we do not train the SVM on a per-
219
frame basis, our model is able to cope also with
those frames that have little or no annotated sen-
tences to support the frame description. After learn-
ing a very fast model on a small set of annotated
lexical unit-synset pairs, we can automatically es-
tablish new mappings in never-seen-before pairs and
use them for our applications. We will evaluate the
effect of the induced mappings on two tasks: the au-
tomatic enrichment of lexical unit sets in the English
and Italian FrameNet via MultiWordNet (Pianta et
al., 2002), and the annotation of the MultiSemCor
corpus (Bentivogli and Pianta, 2005) with frame la-
bels.
The discussion is structured as follows: in
Section 2 we review the main characteristics of
FrameNet and WordNet; in Section 3 we discuss
previous attempts to establish a mapping between
them; in Section 4 we describe our supervised ap-
proach to map lexical units onto synsets; Section 5
details the dataset that we employed for our experi-
ments; Section 6 describes the novel features that we
used to characterize the mapping; Section 7 details
the results of our experiments; in Section 8 we ap-
ply the mapping to three resource annotation tasks;
finally, in Section 9 we draw our conclusions.
2 FrameNet and WordNet
The FrameNet database (Baker et al (1998), Fill-
more et al (2003)) is an English lexical resource
based on the description of some prototypical sit-
uations, the frames, and the frame-evoking words
or expressions associated to them, the lexical units
(LU). Every frame corresponds to a scenario involv-
ing a set of participants, the frame elements (FEs),
that are typically the semantic arguments shared by
all LUs in a frame.
We report in Table 1 the information recorded
in FrameNet for the CAUSE TO WAKE frame. In
the first row there is the frame definition with the
relevant frame elements, namely AGENT, CAUSE,
SLEEPER and SLEEP STATE. Then there is the list
of all lexical units evoking the frame and the corre-
sponding part of speech. Note that, differently from
WordNet synsets, a frame can contain LUs with dif-
ferent PoS as well as antonymous words. In the
last row, an example for each frame element is re-
ported. The lexical unit is underlined, while the con-
Frame: CAUSE TO WAKE
De
f. An AGENT or CAUSE causes a SLEEPER totransition from the SLEEP STATE to wakeful
consciousness.
LU
s awaken.v, get up.v, rouse.v, wake.v, wake up.v
singe.v, sizzle.v, stew.v
FE
s AGENT We tried to rouse Peter.
CAUSE The rain woke the children.
SLEEPER Neighbors were awakened by screams.
SL STATE He woke Constance from her doze.
Table 1: Frame CAUSE TO WAKE
stituent bearing the FE label is written in italics. The
FrameNet resource is corpus-based, i.e. every lexi-
cal unit should be instantiated by at least one ex-
ample sentence. Besides, every lexical unit comes
with a manual lexicographic definition. The latest
database release contains 795 frame definitions and
10,195 lexical units, instantiated through approxi-
mately 140.000 example sentences. Despite this, the
database shows coverage problems when exploited
for NLP tasks, and is still being extended by the
Berkeley group at ICSI.
WordNet (Fellbaum, 1998) is a lexical resource
for English based on psycholinguistics principles
and developed at Princeton University. It has been
conceived as a computational resource aimed at im-
proving some drawbacks of traditional dictionaries
such as the circularity of definitions and the ambigu-
ity of sense references. At present, it covers the ma-
jority of nouns, verbs, adjectives and adverbs in the
English language, organized in synonym sets called
synsets, which correspond to concepts. WordNet
also includes a rich set of semantic relations across
concepts, such as hyponymy, entailment, antonymy,
similar-to, etc. Each synset is encoded as a set of
synomyms having the same part of speech and de-
scribed by a definition or gloss. In some cases, one
or more example sentences may also be reported.
The Princeton English WordNet has also been aug-
mented with domain labels (Magnini and Cavaglia`,
2000) that group synsets into homogeneous clusters
in order to reduce polysemy in the database.
We believe that mapping FrameNet LUs to Word-
Net synsets would have at least three different ad-
vantages: 1) for the English FrameNet, it would au-
tomatically increase the number of LUs for frame by
220
importing all synonyms from the mapped synset(s),
and would allow to exploit the semantic and lex-
ical relations in WordNet to enrich the informa-
tion encoded in FrameNet. This would help cop-
ing with coverage problems and disambiguating the
LU senses. 2) For WordNet, it would be possible
to add a semantic layer between the synset level
and the domain level represented by frame rela-
tions, and to enrich the synsets with a computa-
tional description of the situation they refer to to-
gether with the semantic roles involved. 3) Since
frames are mostly defined at conceptual level, the
FrameNet model is particularly suitable for cross-
lingual induction (Boas, 2005). In this framework,
the FrameNet-WordNet mapping could help mod-
elling frame-based resources for new languages us-
ing minimal supervision. In fact, the availability of
multilingual resources like MultiWordNet (Pianta et
al., 2002) and EuroWordNet (Vossen, 1998) allows
to easily populate frame sets for new languages with
reduced human effort and near-manual quality by
importing all lemmas from the mapped synsets.
3 Related work
Several experiments have been carried out to de-
velop a FrameNet-WordNet mapping and test its
applications. Shi and Mihalcea (2005) described
a semi-automatic approach to exploit VerbNet as a
bridge between FrameNet and WordNet for verbs,
using synonym and hyponym relations and simi-
larity between Levin?s verb classes and FrameNet
frames. Their mapping was used to develop a rule-
based semantic parser (Shi and Mihalcea, 2004) as
well as to detect target words and assign frames for
verbs in an open text (Honnibal and Hawker, 2005).
Burchardt et al (2005) presented a rule-based
system for the assignment of FrameNet frames by
way of a ?detour via WordNet?. They applied
a WordNet-based WSD system to annotate lexical
units in unseen texts with their contextually de-
termined WordNet synsets and then exploited syn-
onyms and hypernyms information to assign the best
frame to the lexical units. The system was inte-
grated into the SALSA RTE system for textual en-
tailment (Burchardt et al, 2007) to cope with sparse-
data problems in the automatic assignment of frame
labels.
Johansson and Nugues (2007) created a feature
representation for every WordNet lemma and used
it to train an SVM classifier for each frame that tells
whether a lemma belongs to the frame or not. The
best-performing feature representation was built us-
ing the sequence of unique identifiers for each synset
in its hypernym tree and weigthing the synsets ac-
cording to their relative frequency in the SemCor
corpus. They used the mapping in the Semeval-2007
task on frame-semantic structure extraction (Baker
et al, 2007) in order to find target words in open
text and assign frames.
Crespo and Buitelaar (2008) carried out an auto-
matic mapping of medical-oriented frames to Word-
Net synsets applying a Statistical Hypothesis Test-
ing to select synsets attached to a lexical unit that
were statistically significant using a given refer-
ence corpus. The mapping obtained was used
to expand Spanish FrameNet using EuroWordNet
(Vossen, 1998) and evaluation was carried out on the
Spanish lexical units obtained after mapping.
Given a set of lexical units, De Cao et al (2008)
propose a method to detect the set of suitable Word-
Net senses able to evoke a frame by applying a simi-
larity function that exploits different WordNet infor-
mation, namely conceptual density for nouns, syn-
onymy and co-hyponymy for verbs and synonymy
for adjectives. The mapping approach was applied
also to LU induction for the English FrameNet and
for Italian frames via MultiWordNet.
4 Problem formulation
Our objective is to be able to assign to every lex-
ical unit l, belonging to a frame Fi defined in the
FrameNet database, one or more WordNet senses
that best express the meaning of l. More specifically,
for every l ? Fi, we consider the set of all WordNet
senses where l appears, CandSet, and then find the
best WordNet sense(s) bests ? CandSet that express
the meaning of l.
For example, the lexical unit rouse.v belonging to
the CAUSE TO WAKE frame, is defined in FrameNet
as ?bring out of sleep; awaken?. Its CandSet com-
prises 4 senses1: 1# bestir, rouse (become active);
2# rout out, drive out, force out, rouse (force or
drive out); #3 agitate, rouse, turn on, charge, com-
1The gloss is reported between parenthesis
221
move, excite, charge up (cause to be agitated, ex-
cited or roused); #4 awaken, wake, waken, rouse,
wake up, arouse (cause to become awake or con-
scious). In this example, bests = {#4} for rouse.v
in CAUSE TO WAKE.
We aim at creating a mapping system that
can achieve a good accuracy also with poorly-
documented lexical units and frames. In fact, we be-
lieve that under real-usage conditions, the automatic
induction of LUs is typically required for frames
with a smaller LU set, especially for those with only
one element. In the FrameNet database (v. 1.3), 33
frames out of 720 are described only by one lex-
ical unit, and 63 are described by two. Further-
more, more than 3,000 lexical units are character-
ized only by the lexicographic definition and are
not provided with example sentences. For this rea-
son, we suggest an approach that makes also use
of usually unexploited information in the FrameNet
database, namely the definition associated to every
lexical unit, and disregards example sentences.
This is the main point of difference between
our and some previous works, e.g. Johansson and
Nugues (2007) and De Cao et al (2008), where un-
supervised approaches are proposed which strongly
rely either on the number of lexical units in a frame
or on the example sentences available for l in the
FrameNet corpus. We claim that the relative short
time necessary to annotate a small dataset of frame-
synset pairs will result in a more reliable mapping
system and, as a consequence, in consistent time
savings when we actually try to use the mappings
for some tasks. The ability to cope with different
cases while retaining a good accuracy will allow to
bootstrap the mapping process in many cases where
other approaches would have failed due to lack of
training data.
To this end, we can train a binary classifier
that, given l and CandSet, for each pair ?l, s?,
s ? CandSet, delivers a positive answer if s ?
bests, and a negative one otherwise. To follow on
the previous example, for rouse.v we would have
4 classifier examples, i.e. the pairs ?rouse.v,#1?,
?rouse.v,#2?, ?rouse.v,#3? and ?rouse.v,#4?. Of
these, only the last would be considered a positive
instance. As a learning framework, we decided to
use SVMs due to their classification accuracy and
robustness to noisy data (Vapnik, 1998).
5 Dataset description
In order to train and test the classifier, we created
a gold standard by manually annotating 2,158 LU-
synset pairs as positive or negative examples. We
don?t have data about inter-annotator agreement be-
cause the dataset was developed only by one annota-
tor, but De Cao et al (2008) report 0.90 as Cohen?s
Kappa computed over 192 LU-synset pairs for the
same mapping task. This confirms that senses and
lexical units are highly correlated and that the map-
ping is semantically motivated.
The annotation process can be carried out in rea-
sonable time. It took approximately two work days
to an expert annotator to manually annotate the
2,158 pairs that make up our gold standard. The lexi-
cal units were randomly selected from the FrameNet
database regardless of their part of speech or amount
of annotated data in the FrameNet database. For
each lexical unit, we extracted from WordNet the
synsets where the LU appears, and for each of them
we assigned a positive label in case the LU-synset
pairs share the same meaning, and a negative label
otherwise. Statistics about the dataset are reported
in Table 2.
N. of LU-synset pairs 2,158
N. of lexical units 617
Verbal lexical units 39%
Nominal lexical units 51%
Adjectival lexical units 9%
Adverbial lexical units <1%
Targeted frames 386
Pairs annotated as positive 32%
Pairs annotated as negative 68%
Average polysemy 3.49
LUs with one candidate synset 204
LUs with 10 or more cand. synsets 32
Table 2: Statistics on the dataset
The 386 frames that are present in the dataset rep-
resent about one half of all lexicalized frames in
the FrameNet database. This proves that, despite
the limited size of the dataset, it is well representa-
tive of FrameNet characteristics. This is confirmed
by the distribution of the part of speech. In fact,
in the FrameNet database about 41% of the LUs
222
are nouns, 40% are verbs, 17% are adjectives and
<1% are adverbs (the rest are prepositions, which
are not included in our experiment because they are
not present in WordNet). In our dataset, the per-
centage of nouns is higher, but the PoS ranking by
frequency is the same, with nouns being the most
frequent PoS and adverbs the less represented. The
average polysemy corresponds to the average num-
ber of candidate synsets for every LU in the dataset.
Note that the high number of lexical units with only
one candidate does not imply a more straightforward
mapping, because in some cases the only candidate
represents a negative example. In fact, a LU could
be encoded in a frame that does not correspond to
the sense expressed by the synset.
6 Feature description
For every LU-synset pair in the gold standard, we
extracted a set of features that characterize different
aspects of the mapping. In the remainder, we detail
the meaning as well as the feature extraction proce-
dure of each of them.
Stem overlap Both WordNet glosses and LU def-
initions in FrameNet are manually written by lex-
icographers. We noticed that when they share the
same sense, they show high similarity, and some-
times are even identical. For example, the defini-
tion of thicken in the Change of consistency frame
is ?become thick or thicker?, which is identical to
the WordNet gloss of synset n. v#00300319. The
thicken lemma occurs in three WordNet synsets, and
in each of them it is the only lemma available, so no
other information could be exploited for the sense
disambiguation.
We believe that this information could help in the
choice of the best candidate synset, so we stemmed
all the words in the synset gloss and in the lexical
unit definition and measured their overlap. As fea-
tures, we use the ratio between the number of over-
lapping words and the number of words in the defi-
nition, both for the gloss and the LU description.
Prevalent Domain and Synset Since a frame rep-
resents a prototypical situation evoked by the set
of its lexical units, our intuition is that it should
be possible to assign it to a WordNet domain, that
groups homogeneous clusters of semantically simi-
lar synsets (see Section 2).
Given the LU-synset pair ?l, s?, l ? Fi, s ?
CandSet, we extract all the lexical units in Fi and
then build a set AllCandSet of pairs ?sj , cj?, where
sj is a synset in which at least one li ? Fi appears,
and cj is the count of lexical units that are found in
sj .
We exploit the information conveyed by AllCan-
dSet in two ways: i) if there is a prevalent Word-
Net domain that characterizes the majority of the
synsets in AllCandSet, and s ? CandSet belongs
to that same domain, we add a boolean feature to
the feature vector representing ?l, s?; ii) if s is the
synset with the highest count in AllCandSet, i.e. if
s = sj and cj > ci??sj , cj? ? AllCandSet, i 6= j,
then we add another boolean feature to encode this
information.
Cross-lingual parallelism Our idea is that, if an
English lexical unit and its Italian translation belong
to the same frame, they are likely to appear also in
the same MultiWordNet synset, and the latter would
be a good candidate for mapping. In fact, in Multi-
WordNet the Italian WordNet is strictly aligned with
the Princeton WordNet 1.6, with synsets having the
same id for both languages, and also semantic re-
lations are preserved in the multilingual hierarchy.
Since no Italian FrameNet is available yet, we ex-
tended the parallel English-Italian corpus annotated
on both sides with frame information described in
Tonelli and Pianta (2008) by adding and annotating
400 new parallel sentences. The final corpus con-
tains about 1,000 pairs of parallel sentences where
the English and the Italian lexical unit belong to the
same frame.
Given a pair ?l, s?, we check if l appears also in
the corpus with the frame label Fi and extract its
Italian translation lit. If lit appears also in the Italian
version of synset s in MultiWordNet, we consider s
as a good candidate for the mapping of l and encode
this information as a binary feature.
Simple synset-frame overlap Intuitively, the
more lemmas a frame and a synset have in common,
the more semantically similar they are. In order to
take into account this similarity in our feature vec-
tor, given the pair ?l, s?, l ? Fi, we extract all lexical
units in Fi and all lemmas in s and we compute the
number of overlapping elements. Then we divide
223
the value by the number of synsets where the same
overlapping element(s) occur.
As an example, the words tank and tank car in
the Vehicle frame, occur together only in the fourth
synset related to tank, which therefore will have a
higher value for this feature.
Extended synset-frame overlap This feature is a
generalization of overlapping value described above.
In fact, we noticed that the hypernym information
in WordNet can help disambiguating the synsets.
Therefore, we take into account not only the over-
laps according to the previous criterion, but also the
number of overlapping words between the lexical
units in a frame and the hypernyms of a synset. For
example, the party.n lexical unit in the AGGREGATE
frame has 5 senses in WordNet. According to the
previous criterion, there is no overlap between the
LUs in the frame and the lemmas in any of the five
synsets. Instead, if we look at the direct hypernym
relation of party, we find that sense #3 is also de-
scribed as set, circle, band, that are also lexical units
of AGGREGATE.
In those cases where the hypernym relation is not
defined, e.g. adjectives, we used the similar-to rela-
tion.
7 Experimental setup and evaluation
To evaluate our methodology we carried out a 10-
fold cross validation using the available data, split-
ting them in 10 non-overlapping sets. For each itera-
tion, 70% of the data was used for training, 30% for
testing. All the splits were generated so as to main-
tain a balance between positive and negative exam-
ples in the training and test sets.
We used the SVM optimizer SVM-
Light2 (Joachims, 1999), and applied polynomial
kernels (poly) of different degrees (i.e. 1 through
4) in order to select the configuration with the best
generalization capabilities. The accuracy is mea-
sured in terms of Precision, Recall and F1 measure,
i.e. the harmonic average between Precision and
Recall. For the sake of annotation, it is important
that an automatic system be very precise, thus not
producing wrong annotations. On the other hand,
the higher the recall, the larger the amount of data
that the system will be able to annotate.
2Available at http://svmlight.joachims.org/
The macro-average of the classifier accuracy for
the different configurations is shown in Table 3. We
report results for linear kernel (i.e. poly 1), maxi-
mizing recall and f-measure, and for polynomial ker-
nel of degree 2 (i.e. poly 2), scoring the highest pre-
cision. In general , we notice that all our models
have a higher precision than recall, but overall are
quite balanced. Different polynomial kernels (i.e.
conjunction of features) do not produce very rele-
vant differences in the results, suggesting that the
features that we employed encode significant infor-
mation and have a relevance if considered indepen-
dently.
As a comparison, we also carried out the same
evaluation by setting a manual threshold and con-
sidering a LU-synset pair as a positive example if
the sum of the feature values was above the thresh-
old. We chose two different threshold values, the
first (Row 1 in Table 3) selected so as to have com-
parable precision with the most precise SVM model
(i.e. poly2), the second (Row 2) selected to have
recall comparable with poly1, i.e. the SVM model
with highest recall. In the former case, the model has
a recall that is less than half than poly2, i.e. 0.214
vs. 0.569, meaning that such model would establish
a half of the mappings while making the same per-
centage of mistakes. In the latter, the precision of
the SVM classifier is 0.114 points higher, i.e. 0.794
vs. 0.680, meaning the SVM can retrieve as many
mappings but making 15% less errors.
In order to investigate the impact of different fea-
tures on the classifier performance, we also consid-
ered three different groups of features separately:
the ones based on stem overlap, those computed
for prevalent domain and synset, and the features
for simple and extended frame ? synset overlap.
We did not take into account cross-lingual paral-
lelism because it is one single feature whose cover-
age strongly relies on the parallel corpus available.
As a consequence, it is not possible to test the fea-
ture in isolation due to data sparseness.
Results are shown in Table 3, in the second group
of rows. Also in this case, we carried out a 10-
fold cross validation using a polynomial kernel of
degree 2. The stem overlap features, which to our
best knowledge are an original contribution of our
approach, score the highest recall among the three
groups. This confirms our intuition that LU defini-
224
tions and WordNet glosses can help extending the
number of mapped LUs, including those that are
poorly annotated. For instance, if we consider the
KNOT CREATION frame, having only tie.v as LU,
the features about prevalent domain & synset and
about synset-frame overlap would hardly be infor-
mative, while stem overlap generally achieves a con-
sistent performance regardless of the LU set. In
fact, tie.v is correctly mapped to synset v#00095054
based on their similar definition (respectively ?to
form a knot? and ?form a knot or bow in?). Best
precision was scored by the feature group consider-
ing prevalent domain & synset, which are also new
features introduced by our approach. The positive
effect of combining all features is clearly shown by
comparing the results obtained with individual fea-
ture groups against the figures in the row labeled
poly2.
Prec. Recall F1
Man. thresh. (P) 0.789 0.214 0.337
Man. thresh. (F1) 0.680 0.662 0.671
Stem Overlap 0.679 0.487 0.567
Prev.Dom.& Syn. 0.756 0.434 0.551
Syn.- Frame Overlap 0.717 0.388 0.504
poly1 0.761 0.613 0.679
poly2 0.794 0.569 0.663
Table 3: Mapping evaluation
8 MapNet and its applications
Since we aim at assigning at least one synset to ev-
ery lexical unit in FrameNet, we considered all the
frames and for every LU in the database we created
a list of LU-synset pairs. We re-trained the clas-
sifier using the whole annotated gold standard and
classified all the candidate pairs. The mapping pro-
duced between the two resources, that we call Map-
Net, comprises 5,162 pairs. Statistics on MapNet are
reported in table 4.
About one thousand lexical units in FrameNet
have no candidate synsets because the lemma is not
present in WordNet. The remaining LUs have 3.69
candidate synsets each on average, similarly to the
average polysemy reported for the gold standard (see
Table 2). This confirms our hypothesis that the data
used for training are well representative of the char-
N. of LUs in FrameNet 10,100
N. of LUs with at least one syn.cand. 9,120
N. of LU-synset candidate pairs 33,698
N. of mapped pairs 5,162
Table 4: Statistics on the mapping
acteristics of the whole resource. We expect about
80% of these mappings to be correct, i.e. in line
with the precision of the classifier.
8.1 Automatic FrameNet extension
MapNet can be easily exploited to automatically ex-
tend FrameNet coverage, in particular to extend the
set of lexical units for each frame. In fact, we can
assume that all lemmas in the mapped synsets have
the same meaning of the LUs in the corresponding
frames. We use MapNet to extract from WordNet
the lemmas in the mapped synsets and add them to
the frames.
For English FrameNet, we can acquire 4,265 new
lexical units for 521 frames. In this way, we would
extend FrameNet size by almost 42%. In the ran-
dom evaluation of 100 newly acquired LUs belong-
ing to 100 different frames, we assessed a precision
of 78%. For the Italian side, we extract 6,429 lexi-
cal units for 561 frames. Since no Italian FrameNet
has been developed yet, this would represent a first
attempt to create this resource by automatically pop-
ulating the frames. We evaluate the content of 15
complete frames containing 191 Italian LUs. The
assigned LUs are correct in 88% of the considered
cases, which represent a promising result w.r.t. the
unsupervised creation of Italian FrameNet.
The difference in the evaluation for the two lan-
guages most likely lies in the smaller number of
synsets on the Italian side of MultiWordNet if com-
pared to the English, which results in less ambigu-
ity. Furthermore, we should consider that the task
for Italian is easier than for English, since in the for-
mer case we are building a resource from scratch,
while in the latter we are extending an already exist-
ing resource with lexical units which are most likely
peripheral with respect to those already present in
the database.
225
8.2 Frame annotation of MultiSemCor
MultiSemCor (Bentivogli and Pianta, 2005) is an
English/Italian parallel corpus, aligned at word
level and annotated with PoS, lemma and Word-
Net synsets. The parallel corpus was created start-
ing from the SemCor corpus, which is a subset of
the English Brown corpus containing about 700,000
running words. The corpus was first manually trans-
lated into Italian. Then, the procedure of transferring
word sense annotations from English to Italian was
carried out automatically.
We apply MapNet to enrich the corpus with frame
information. We believe that this procedure would
be interesting from different point of views. Not
only we would enrich the resource with a new anno-
tation layer, but we would also automatically acquire
a large set of English and Italian sentences having a
lexical unit with a frame label. For the English side,
it is a good solution to automatically extract a dataset
with frame information and train, for example, a ma-
chine learning system for frame identification. For
the Italian side, it represents a good starting point for
the creation of a large annotated corpus with frame
information, the base for a future Italian FrameNet.
MultiSemCor contains 12,843 parallel sentences.
If we apply MapNet to the corpus, we produce
27,793 annotated instances in English and 23,872 in
Italian, i.e. about two lexical units per sentence. The
different amount of annotated sentences depends on
the fact that in MultiSemCor some synset annota-
tions have not been transferred from English to Ital-
ian. From both sides of the resulting corpus, we
randomly selected 200 sentences labeled with 200
different frames, and evaluated the annotation qual-
ity. As for the English corpus, 75% of the sen-
tences was annotated with the correct frame label,
while on the Italian side they were 70%. This re-
sult is in line with the expectations, since Map-
Net was developed with 0.79 precision. Besides,
synset annotation on the English side of MultiSem-
Cor was carried out by hand, while annotation in
Italian was automatically acquired by transferring
the information from the English corpus (precision
0.86). This explains why the resulting annotation
for English is slightly better than for Italian. In some
cases, the wrongly annotated frame was strictly con-
nected to the right one, i.e. APPLY HEAT instead
of COOKING CREATION and ATTACHING instead
of INCHOATIVE ATTACHING.
9 Conclusions
We proposed a new method to map FrameNet LUs
to WordNet synsets using SVM with minimal super-
vision effort.
To our best knowledge, this is the only approach
to the task that exploits features based on stem over-
lap between LU definition and synset gloss and
that makes use of information about WordNet do-
mains. Differently from other models, the SVM
is not trained on a per-frame basis and we do not
rely on the number of the annotated sentences for a
LU in the FrameNet corpus, thus our mapping al-
gorithm performs well also with poorly-annotated
LUs. After creating MapNet, the mapping be-
tween FrameNet and WordNet, we applied it to three
tasks: the automatic induction of new LUs for En-
glish FrameNet, the population of frames for Italian
FrameNet and the annotation of the MultiSemCor
corpus with frame information. A preliminary eval-
uation shows that the mapping can significantly re-
duce the manual effort for the development and the
extension of FrameNet-like resources, both in the
phase of corpus annotation and of frame population.
In the future, we plan to improve the algorithm
by introducing syntactic features for assessing simi-
larity between LU definitions and WordNet glosses.
We also want to merge all information extracted and
collected for Italian FrameNet and deliver a seed
version of the resource to be validated. Finally, we
plan to extend the mapping to all languages included
in MultiWordNet, i.e. Spanish, Portuguese, Hebrew
and Romanian.
Acknowledgements
We thank Roberto Basili and Diego De Cao for shar-
ing with us their gold standard of frame ? synset
mappings.
226
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 36th ACL Meeting and 17th ICCL Confer-
ence. Morgan Kaufmann.
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 10: Frame Semantic Struc-
ture Extraction. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 99?104, Prague, CZ, June.
Luisa Bentivogli and Emanuele Pianta. 2005. Exploiting
Parallel Texts in the Creation of Multilingual Seman-
tically Annotated Resources: The MultiSemCor Cor-
pus. Natural Language Engineering, Special Issue on
Parallel Texts, 11(03):247?261, September.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases. In-
ternational Journal of Lexicography, 18(4):445?478.
Aljoscha Burchardt, Katrin Erk, and Annette Frank.
2005. A WordNet detour to FrameNet. In B. Fis-
seni, H. Schmitz, B. Schro?der, and P. Wagner, editors,
Sprachtechnologie, mobile Kommunikation und lingis-
tische Resourcen, Frankfurt am Main, Germany. Peter
Lang.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and An-
nette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of Pascal RTE-3 Challenge, Prague, CZ.
Diego De Cao, Danilo Croce, Marco Pennacchiotti, and
Roberto Basili. 2008. Combining Word Sense and
Usage for modeling Frame Semantics. In Proceedings
of STEP 2008, Venice, Italy.
Mario Crespo and Paul Buitelaar. 2008. Domain-specific
English-to-Spanish Translation of FrameNet. In Proc.
of LREC 2008, Marrakech.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
C.J. Fillmore, C.R. Johnson, and M. R. L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250, September.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via FrameNet, VerbNet and Prop-
Bank. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th an-
nual ACL meeting, pages 929?936, Morristown, NJ,
US. Association for Computational Linguistics.
Matthew Honnibal and Tobias Hawker. 2005. Identify-
ing FrameNet frames for verbs from a real-text corpus.
In Proceedings of Australasian Language Technology
Workshop 2005.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Scho?lkopf, Christopher J. C. Burges, and Alexander J
Smola, editors, Advances in kernel methods: support
vector learning, pages 169?184. MIT Press, Cam-
bridge, MA, USA.
R. Johansson and P. Nugues. 2007. Using WordNet to
extend FrameNet coverage. In Proc. of the Workshop
on Building Frame-semantic Resources for Scandina-
vian and Baltic Languages, at NODALIDA, Tartu.
Bernardo Magnini and Gabriela Cavaglia`. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC 2000, pages 1413?1418, Athens,
Greece.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Computational Linguistics, 31.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In First International Confer-
ence on Global WordNet, pages 292?302, Mysore, In-
dia.
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan
Scheffczyk. 2006. FrameNet II: Ex-
tended Theory and Practice. Available at
http://framenet.icsi.berkeley.edu/book/book.html.
Dan Shen and Mirella Lapata. 2007. Using Semantic
Roles to Improve Question Answering. In Proceed-
ings of EMNLP and CONLL, pages 12?21, Prague,
CZ.
Lei Shi and Rada Mihalcea. 2004. Open Text Semantic
Parsing Using FrameNet and WordNet. In Proceed-
ings of HLT-NAACL 2004.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and WordNet
for Robust Semantic Parsing. In Proceedings of CI-
CLing 2005, pages 100?111. Springer.
Sara Tonelli and Emanuele Pianta. 2008. Frame Infor-
mation Transfer from English to Italian. In European
Language Resources Association (ELRA), editor, Pro-
ceedings of LREC 2008, Marrakech, Morocco.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Springer,
October.
227
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 34?41,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Annotating Spoken Dialogs: from Speech Segments to Dialog Acts and
Frame Semantics
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli, Alessandro Moschitti, Giuseppe Riccardi?
University of Trento
38050 Povo - Trento, Italy
{dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.eu
Abstract
We are interested in extracting semantic
structures from spoken utterances gener-
ated within conversational systems. Cur-
rent Spoken Language Understanding sys-
tems rely either on hand-written seman-
tic grammars or on flat attribute-value se-
quence labeling. While the former ap-
proach is known to be limited in coverage
and robustness, the latter lacks detailed re-
lations amongst attribute-value pairs. In
this paper, we describe and analyze the hu-
man annotation process of rich semantic
structures in order to train semantic statis-
tical parsers. We have annotated spoken
conversations from both a human-machine
and a human-human spoken dialog cor-
pus. Given a sentence of the transcribed
corpora, domain concepts and other lin-
guistic features are annotated, ranging
from e.g. part-of-speech tagging and con-
stituent chunking, to more advanced anno-
tations, such as syntactic, dialog act and
predicate argument structure. In particu-
lar, the two latter annotation layers appear
to be promising for the design of complex
dialog systems. Statistics and mutual in-
formation estimates amongst such features
are reported and compared across corpora.
1 Introduction
Spoken language understanding (SLU) addresses
the problem of extracting and annotating the
meaning structure from spoken utterances in the
context of human dialogs (De Mori et al, 2008).
In spoken dialog systems (SDS) most used models
of SLU are based on the identification of slots (en-
?This work was partially funded by the European Com-
mission projects LUNA (contract 33549) and ADAMACH
(contract 022593).
tities) within one or more frames (frame-slot se-
mantics) that is defined by the application. While
this model is simple and clearly insufficient to
cope with interpretation and reasoning, it has sup-
ported the first generation of spoken dialog sys-
tems. Such dialog systems are thus limited by the
ability to parse semantic features such as predi-
cates and to perform logical computation in the
context of a specific dialog act (Bechet et al,
2004). This limitation is reflected in the type of
human-machine interactions which are mostly di-
rected at querying the user for specific slots (e.g.
?What is the departure city??) or implementing
simple dialog acts (e.g. confirmation). We believe
that an important step in overcoming such limita-
tion relies on the study of models of human-human
dialogs at different levels of representation: lexi-
cal, syntactic, semantic and discourse.
In this paper, we present our results in address-
ing the above issues in the context of the LUNA
research project for next-generation spoken dialog
interfaces (De Mori et al, 2008). We propose
models for different levels of annotation of the
LUNA spoken dialog corpus, including attribute-
value, predicate argument structures and dialog
acts. We describe the tools and the adaptation of
off-the-shelf resources to carry out annotation of
the predicate argument structures (PAS) of spoken
utterances. We present a quantitative analysis of
such semantic structures for both human-machine
and human-human conversations.
To the best of our knowledge this is the first
(human-machine and human-human) SDS corpus
denoting a multilayer approach to the annotation
of lexical, semantic and dialog features, which al-
lows us to investigate statistical relations between
the layers such as shallow semantic and discourse
features used by humans or machines. In the fol-
lowing sections we describe the corpus, as well as
a quantitative analysis and statistical correlations
between annotation layers.
34
2 Annotation model
Our corpus is planned to contain 1000 equally
partitioned Human-Human (HH) and Human-
Machine (HM) dialogs. These are recorded by
the customer care and technical support center of
an Italian company. While HH dialogs refer to
real conversations of users engaged in a problem
solving task in the domain of software/hardware
troubleshooting, HM dialogs are acquired with a
Wizard of Oz approach (WOZ). The human agent
(wizard) reacts to user?s spontaneous spoken re-
quests following one of ten possible dialog scenar-
ios inspired by the services provided by the com-
pany.
The above data is organized in transcrip-
tions and annotations of speech based on a new
multi-level protocol studied specifically within the
project, i.e. the annotation levels of words, turns1,
attribute-value pairs, dialog acts, predicate argu-
ment structures. The annotation at word level
is made with part-of-speech and morphosyntac-
tic information following the recommendations of
EAGLES corpora annotation (Leech and Wilson,
2006). The attribute-value annotation uses a pre-
defined domain ontology to specify concepts and
their relations. Dialog acts are used to annotate in-
tention in an utterance and can be useful to find
relations between different utterances as the next
section will show. For predicate structure annota-
tion, we followed the FrameNet model (Baker et
al., 1998) (see Section 2.2).
2.1 Dialog Act annotation
Dialog act annotation is the task of identifying
the function or goal of a given utterance (Sinclair
and Coulthard, 1975): thus, it provides a comple-
mentary information to the identification of do-
main concepts in the utterance, and a domain-
independent dialog act scheme can be applied.
For our corpus, we used a dialog act taxonomy
which follows initiatives such as DAMSL (Core
and Allen, 1997), TRAINS (Traum, 1996) and
DIT++ (Bunt, 2005). Although the level of granu-
larity and coverage varies across such taxonomies,
a careful analysis leads to identifying three main
groups of dialog acts:
1. Core acts, which represent the fundamen-
tal actions performed in the dialog, e.g. re-
1A turn is defined as the interval when a speaker is active,
between two pauses in his/her speech flow.
questing and providing information, or exe-
cuting a task. These include initiatives (often
called forward-looking acts) and responses
(backward-looking acts);
2. Conventional/Discourse management acts,
which maintain dialog cohesion and delimit
specific phases, such as opening, continua-
tion, closing, and apologizing;
3. Feedback/Grounding acts,used to elicit and
provide feedback in order to establish or re-
store a common ground in the conversation.
Our taxonomy, following the same three-fold
partition, is summarized in Table 1.
Table 1: Dialog act taxonomy
Core dialog acts
Info-request Speaker wants information from ad-
dressee
Action-request Speaker wants addressee to perform
an action
Yes-answer Affirmative answer
No-answer Negative answer
Answer Other kinds of answer
Offer Speaker offers or commits to perform
an action
ReportOnAction Speaker notifies an action is being/has
been performed
Inform Speaker provides addressee with in-
formation not explicitly required (via
an Info-request)
Conventional dialog acts
Greet Conversation opening
Quit Conversation closing
Apology Apology
Thank Thanking (and down-playing)
Feedback/turn management dialog acts
Clarif-request Speaker asks addressee for confirma-
tion/repetition of previous utterance
for clarification.
Ack Speaker expresses agreement with
previous utterance, or provides feed-
back to signal understanding of what
the addressee said
Filler Utterance whose main goal is to man-
age conversational time (i.e. dpeaker
taking time while keeping the turn)
Non-interpretable/non-classifiable dialog acts
Other Default tag for non-interpretable and
non-classifiable utterances
It can be noted that we have decided to retain
only the most frequent dialog act types from the
schemes that inspired our work. Rather than as-
piring to the full discriminative power of possible
conversational situations, we have opted for a sim-
ple taxonomy that would cover the vast majority
35
of utterances and at the same time would be able
to generalize them. Its small number of classes is
meant to allow a supervised classification method
to achieve reasonable performance with limited
data. The taxonomy is currently used by the sta-
tistical Dialogue Manager in the ADAMACH EU
project (Varges et al, 2008); the limited number
of classes allows to reduce the number of hypoth-
esized current dialogue acts, thus reducing the di-
alogue state space.
Dialog act annotation was performed manually
by a linguist on speech transcriptions previously
segmented into turns as mentioned above. The an-
notation unit for dialog acts, is the utterance; how-
ever, utterances are complex semantic entities that
do not necessarily correspond to turns. Hence, a
segmentation of the dialog transcription into ut-
terances was performed by the annotator before
dialog act labeling. Both utterance segmentation
and dialog act labeling were performed through
the MMAX tool (Mu?ller and Strube, 2003).
The annotator proceeded according to the fol-
lowing guidelines:
1. by default, a turn is also an utterance;
2. if more than one tag is applicable to an ut-
terance, choose the tag corresponding to its
main function;
3. in case of doubt among several tags, give pri-
ority to tags in core dialog acts group;
4. when needed, split the turn into several utter-
ances or merge several turns into one utter-
ance.
Utterance segmentation provides the basis not
only for dialog act labeling but also for the other
semantic annotations. See Fig. 1 for a dialog sam-
ple where each line represents an utterance anno-
tated according to the three levels.
2.2 Predicate Argument annotation
We carried out predicate argument structure an-
notation applying the FrameNet paradigm as de-
scribed in (Baker et al, 1998). This model
comprises a set of prototypical situations called
frames, the frame-evoking words or expressions
called lexical units and the roles or participants in-
volved in these situations, called frame elements.
The latter are typically the syntactic dependents of
the lexical units. All lexical units belonging to
the same frame have similar semantics and show
                                              PERSON-NAME 
Info: Buongiorno, sono   Paola.  
  
          GREETING    B._NAMED Name 
Good morning, this is Paola. 
 
Info-req: Come la posso aiutare? 
                      
                    Benefitted_party   ASSISTANCE 
How may I help you? 
 
                                                       CONCEPT         HARDWARE-COMPONENT 
Info: Buongiorno. Ho un problema con la stampante.  
 
          GREETING            PR._DESCRIPTION     Affected_device 
Good morning. I have a problem with the printer. 
 
           PART-OF-DAY   NEGAT. ACTION                ACTION 
Info: Da stamattina non   riesco pi? a  stampare 
                                       
                                    Problem 
Since this morning I can?t print. 
 
Info-req:   Mi  pu?  dire   nome e cognome per favore? 
 
              Addressee      TELLING               Message 
Can you tell me your name and surname, please? 
 
                                       PERSON-NAME  PERSON-SURNAME 
Answer: Mi chiamo  Alessandro  Manzoni. 
 
               Entity B._NAMED                   Name 
My name is Alessandro Manzoni. 
Figure 1: Annotated dialog extract. Each utterance
is preceded by dialog act annotation. Attribute-
value annotation appears above the text, PAS an-
notation below the text.
the same valence. A particular feature of the
FrameNet project both for English and for other
languages is its corpus-based nature, i.e. every el-
ement described in the resource has to be instanti-
ated in a corpus. To annotate our SDS corpus, we
adopted where possible the already existing frame
and frame element descriptions defined for the En-
glish FrameNet project, and introduced new def-
initions only in case of missing elements in the
original model.
Figure 1 shows a dialog sample with PAS an-
notation reported below the utterance. All lexi-
cal units are underlined and the frame is written in
capitals, while the other labels refer to frame el-
ements. In particular, ASSISTANCE is evoked by
the lexical unit aiutare and has one attested frame
element (Benefitted party), GREETING has no
frame element, and PROBLEM DESCRIPTION
and TELLING have two frame elements each.
Figure 2 gives a comprehensive view of the an-
notation process, from audio file transcription to
the annotation of three semantic layers. Whereas
36
Figure 2: The annotation process
Audio file 
Turn segmentation & 
Transcription 
Utterance segmentation 
POS tagging Domain attribute 
annotation 
PAS annotation 
Dialog Act 
annotation 
Syntactic parsing 
attribute-value and DA annotation are carried
out on the segmented dialogs at utterance level,
PAS annotation requires POS-tagging and syntac-
tic parsing (via Bikel?s parser trained for Italian
(Corazza et al, 2007)). Finally, a shallow manual
correction is carried out to make sure that the tree
nodes that may carry semantic information have
correct constituent boundaries. For the annotation
of frame information, we used the Salto tool (Bur-
chardt et al, 2006), that stores the dialog file in
TIGER-XML format and allows to easily intro-
duce word tags and frame flags. Frame informa-
tion is recorded on top of parse trees, with target
information pointing to terminal words and frame
elements pointing to tree nodes.
3 Quantitative comparison of the
Annotation
We evaluated the outcome of dialog act and
PAS annotation levels on both the human-human
(henceforth HH) and human-machine (HM) cor-
pora by not only analyzing frequencies and occur-
rences in the separate levels, but also their interac-
tion, as discussed in the following sections.
3.1 Dialog Act annotation
Analyzing the annotation of 50 HM and 50 HH
dialogs at the dialog act level, we note that an
HH dialog is composed in average by 48.9?17.4
(standard deviation) dialog acts, whereas a HM
dialog is composed of 18.9?4.4. The difference
between average lengths shows how HH sponta-
neous speech can be redundant, while HM dialogs
are more limited to an exchange of essential infor-
mation. The standard deviation of a conversation
in terms of dialog acts is considerably higher in
the HH corpus than in the HM one. This can be ex-
plained by the fact that the WOZ follows a unique,
previously defined task-solving strategy that does
not allow for digressions. Utterance segmentation
was also performed differently on the two corpora.
In HH we performed 167 turn mergings and 225
turn splittings; in HM dialogs, only turn splittings
(158) but no turn mergings were performed.
Tables 2 and 3 report the dialog acts occurring
in the HM and HH corpora, respectively, ranked
by their frequencies.
Table 2: Dialog acts ranked by frequency in the
human-machine (HM) corpus
human-machine (HM)
DA count rel. freq.
Info-request 249 26.3%
Answer 171 18.1%
Inform 163 17.2%
Yes-answer 70 7.4%
Quit 60 6.3%
Thank 56 5.9%
Greet 50 5.3%
Offer 49 5.2%
Clarification-request 26 2.7%
Action-request 25 2.6%
Ack 12 1.3%
Filler 6 0.6%
No-answer 5 0.5%
Other, ReportOnAction 2 0.2%
Apology 1 0.1%
TOTAL 947
From a comparative analysis, we note that:
1. info-request is by far the most common dia-
log act in HM, whereas in HH ack and info
share the top ranking position;
2. the most frequently occurring dialog act in
HH, i.e. ack, is only ranked 11th in HM;
3. the relative frequency of clarification-request
(4,7%) is considerably higher in HH than in
HM.
We also analyzed the ranking of the most fre-
quent dialog act bigrams in the two corpora. We
can summarize our comparative analysis, reported
in Table 4, to the following: in both corpora,
most bigram types contain info and info-request,
37
Table 3: Dialog acts ranked by frequency in the
human-human (HH) corpus
human-human (HH)
DA count rel. freq.
Ack 582 23.8%
Inform 562 23.0%
Info-request 303 12.4%
Answer 192 7.8%
Clarification-request 116 4.7%
Offer 114 4.7%
Yes-answer 112 4.6%
Quit 101 4.1%
ReportOnAction 91 3.7%
Other 70 2.9%
Action-request 69 2.8%
Filler 61 2.5%
Thank 33 1.3%
No-answer 26 1.1%
Greet, Apology 7 0.3%
TOTAL 2446
as expected in a troubleshooting system. How-
ever, the bigram info-request answer, which we
expected to form the core of a task-solving dia-
log, is only ranked 5th in the HH corpus, while 5
out of the top 10 bigram types contain ack. We
believe that this is because HH dialogs primarily
contain spontaneous information-providing turns
(e.g. several info info by the same speaker) and
acknowledgements for the purpose of backchan-
nel. Instead, HM dialogs, structured as sequences
of info-request answers pairs, are more minimal
and brittle, showing how users tend to avoid re-
dundancy when addressing a machine.
Table 4: The 10 most frequent dialog act bigrams
human-machine (HM) human-human (HH)
info-req answer ack info
answer info-req info ack
info info-req info info
info-req y-answer ack ack
sentence beginning greet info-req answer
greet info info info-req
info quit info-req y-answer
offer info ack info-req
thank info answer ack
y-answer thank quit sentence end
3.2 Predicate Argument annotation
We annotated 50 HM and 50 HH dialogs with
frame information. Differently from the English
FrameNet database, we didn?t annotate one frame
per sentence. On the contrary, we identified all
lexical units corresponding to ?semantically rele-
vant? verbs, nouns and adjectives with a syntac-
tic subcategorization pattern, eventually skipping
the utterances with empty semantics (e.g. dis-
fluencies). In particular, we annotated all lexical
units that imply an action, introduce the speaker?s
opinion or describe the office environment. We
introduced 20 new frames out of the 174 iden-
tified in the corpus because the original defini-
tion of frames related to hardware/software, data-
handling and customer assistance was sometimes
too coarse-grained. Few new frame elements were
introduced as well, mostly expressing syntactic re-
alizations that are typical of spoken Italian.
Table 5 shows some statistics about the cor-
pus dimension and the results of our annotation.
The human-human dialogs contain less frame in-
stances in average than the human-machine group,
meaning that speech disfluencies, not present in
turns uttered by the WOZ, negatively affect the se-
mantic density of a turn. For the same reason, the
percentage of turns in HH dialogs that were manu-
ally corrected in the pre-processing step (see Sec-
tion 2.2) is lower than for HM turns, since HH di-
alogs have more turns that are semantically empty
and that were skipped in the correction phase. Be-
sides, HH dialogs show a higher frame variabil-
ity than HM, which can be explained by the fact
that spontaneous conversation may concern mi-
nor topics, whereas HM dialogs follow a previ-
ously defined structure, designed to solve soft-
ware/hardware problems.
Tables 6 and 7 report the 10 most frequent
frames occurring in the human-machine resp.
human-human dialogs. The relative frame fre-
quency in HH dialogs is more sparse than in HM
dialogs, meaning that the task-solving strategy fol-
lowed by the WOZ limits the number of digres-
sions, whereas the semantics of HH dialogs is
richer and more variable.
As mentioned above, we had to introduce and
define new frames which were not present in the
original FrameNet database for English in order to
capture all relevant situations described in the di-
alogs. A number of these frames appear in both
tables, suggesting that the latter are indeed rel-
38
Table 5: Dialog turn and frame statistics for the
human-machine (HM) resp. human-human (HH)
corpus
HM HH
Total number of turns 662 1,997
Mean dialog length (turns) 13.2 39.9
Mean turn length (tokens) 11.4 10.8
Corrected turns (%) 50 39
Total number of annotations 923 1951
Mean number of frame annota-
tions per dialog
18.5 39.0
Mean number of frame elements
per frame annotation
1.6 1.7
evant to model the general semantics of the di-
alogs we are approaching. The most frequent
frame group comprises frames relating to infor-
mation exchange that is typical of the help-desk
activity, including Telling, Greeting, Contacting,
Statement, Recording, Communication. Another
relevant group encompasses frames related to the
operational state of a device, for example Be-
ing operational, Change operational state, Oper-
ational testing, Being in operation.
The two groups also show high variability of
lexical units. Telling, Change operational state
and Greeting have the richest lexical unit set,
with 11 verbs/nouns/adjectives each. Arriving
and Awareness are expressed by 10 different lexi-
cal units, while Statement, Being operational, Re-
moving and Undergo change of operational state
have 9 different lexical units each. The informal
nature of the spoken dialogs influences the com-
position of the lexical unit sets. In fact, they are
rich in verbs and multiwords used only in collo-
quial contexts, for which there are generally few
attestations in the English FrameNet database.
Similarly to the dialog act statistics, we also
analyzed the most frequent frame bigrams and
trigrams in HM and HH dialogs. Results are
reported in Tables 8 and 9. Both HH bigrams
and trigrams show a more sparse distribution and
lower relative frequency than HM ones, implying
that HH dialogs follow a more flexible structure
with a richer set of topics, thus the sequence of
themes is less predictable. In particular, 79%
of HH bigrams and 97% of HH trigrams occur
only once (vs. 68% HM bigrams and 82% HM
trigrams). On the contrary, HM dialogs deal with
Table 6: The 10 most frequent frames in the HM
corpus (* =newly introduced)
HM corpus
Frame count freq-%
Greeting* 146 15.8
Telling 134 14.5
Recording 83 8.9
Being named 74 8.0
Contacting 52 5.6
Usefulness 50 5.4
Being operational 28 3.0
Problem description* 24 2.6
Inspecting 24 2.6
Perception experience 21 2.3
Table 7: The 10 most frequent frames in the HH
corpus (* =newly introduced)
HH corpus
Frame count freq-%
Telling 143 7.3
Greeting* 124 6.3
Awareness 74 3.8
Contacting 63 3.2
Giving 62 3.2
Navigation* 61 3.1
Change operational state 51 2.6
Perception experience 46 2.3
Insert data* 46 2.3
Come to sight* 38 1.9
a fix sequence of topics driven by the turns uttered
by the WOZ. For instance, the most frequent
HM bigram and trigram both correspond to the
opening utterance of the WOZ:
Help desk buongiornoGREETING, sonoBEING NAMED
Paola, in cosa posso esserti utileUSEFULNESS?
(Good morning, help-desk service, Paola speaking, how can
I help you?)
3.3 Mutual information between PAS and
dialog acts
A unique feature of our corpus is the availabil-
ity of both a semantic and a dialog act annota-
tion level: it is intuitive to seek relationships in
the purpose of improving the recognition and un-
derstanding of each level by using features from
the other. We considered a subset of 20 HH and
50 HM dialogs and computed an initial analysis
39
Table 8: The 5 most frequent frame bigrams
human-machine (HM) freq-%
Greeting Being named 17.1
Being named Usefulness 15.3
Telling Recording 12.9
Recording Contacting 10.9
Contacting Greeting 10.6
human-human (HH) freq-%
Greeting Greeting 4.7
Navigation Navigation 1.2
Telling Telling 1.0
Change op. state Change op. state 0.9
Telling Problem description 0.8
Table 9: The 5 most frequent frame trigrams
human-machine (HM) freq-%
Greeting Being named Usefulness 9.5
Recording Contacting Greeting 5.7
Being named Usefulness Greeting 3.7
Telling Recording Contacting 3.5
Telling Recording Recording 2.2
human-human (HH) freq-%
Greeting Greeting Greeting 1.6
Greeting Being named Greeting 0.5
Contacting Greeting Greeting 0.3
Navigation Navigation Navigation 0.2
Working on Greeting Greeting 0.2
of the co-occurrences of dialog acts and PAS. We
noted that each PAS tended to co-occur only with a
limited subset of the available dialog act tags, and
moreover in most cases the co-occurrence hap-
pened with only one dialog act. For a more thor-
ough analysis, we computed the weighted condi-
tional entropy between PAS and dialog acts, which
yields a direct estimate of the mutual information
between the two levels of annotation2.
2Let H(yj |xi) be the weighted conditional entropy of ob-
servation yj of variable Y given observation xi of variable
X:
H(yj |xi) = ?p(xi; yj)log
p(xi; yj)
p(xi)
,
where p(xi; yj) is the probability of co-occurrence of xi and
yj , and p(xi) and p(yj) are the marginal probabilities of oc-
currence of xi resp. yj in the corpus. There is an obvious re-
lation with the weighted mutual information between xi and
yj , defined following e.g. (Bechet et al, 2004) as:
wMI(xi; yj) = p(xi; yj)log
p(xi; yj)
p(xi)p(yj)
.
(a) human-machine dialogs (filtering co-occurrences below 3)
(b) human-human dialogs (filtering co-occurrences below 5)
Figure 3: Weighted conditional entropy between
PAS and dialog acts in the HM (a) and HH corpus
(b). To lower entropies correspond higher values
of mutual information (darker color in the scale)
Our results are illustrated in Figure 3. In the
HM corpus (Fig. 3(a)), we noted some interesting
associations between dialog acts and PAS. First,
info-req has the maximal MI with PAS like Be-
ing in operation and Being attached, as requests
are typically used by the operator to get informa-
tion about the status of device. Several PAS de-
note a high MI with the info dialog act, includ-
ing Activity resume, Information, Being named,
Contacting, and Resolve problem. Contacting
refers to the description of the situation and of the
speaker?s point of view (usually the caller). Be-
ing named is primarily employed when the caller
introduces himself, while Activity resume usually
refers to the operator?s description of the sched-
Indeed, the higher is H(yj |xi), the lower is wMI(xi; yj).
We approximate all probabilities using frequency of occur-
rence.
40
uled interventions.
As for the remaining acts, clarif has the high-
est MI with Perception experience and Statement,
used to warn the addressee about understanding
problems and asking him to repeat/rephrase an ut-
terance, respectively. The two strategies can be
combined in the same utterance, as in the utter-
ance: Non ho sentito bene: per favore ripeti cer-
cando di parlare piu` forte. (I haven?t quite heard
that, please repeat trying to speak up.).
The answer tag is highly informative with Suc-
cessful action, Change operational state, Becom-
ing nonfunctional, Being detached, Read data.
These PAS refer to the exchange of infor-
mation (Read data) or to actions performed
by the user after a suggestion of the system
(Change operational state). Action requests (act-
req) seem to be correlated to Replacing as it usu-
ally occurs when the operator requests the caller
to carry out an action to solve a problem, typically
to replace a component with another. Another fre-
quent request may refer to some device that the
operator has to test.
In the HH corpus (Fig. 3(b)), most of the PAS
are highly mutually informative with info: in-
deed, as shown in Table 3, this is the most fre-
quently occurring act in HH except for ack, which
rarely contain verbs that can be annotated by a
frame. As for the remaining acts, there is an easily
explainable high MI between quit and Greeting;
moreover, info-req denote its highest MI with
Giving, as in requests to give information, while
rep-action denotes a strong co-occurrence with
Inchoative attaching: indeed, interlocutors often
report on the action of connecting a device.
These results corroborate our initial observation
that for most PAS, the mutual information tends
to be very high in correspondence of one dialog
act type: this suggests the beneficial effect of in-
cluding shallow semantic information as features
for dialog act classification. The converse is less
clear as the same dialog act can relate to a span
of words covered by multiple PAS and generally,
several PAS co-occur with the same dialog act.
4 Conclusions
In this paper we have proposed an approach to
the annotation of spoken dialogs using seman-
tic and discourse features. Such effort is crucial
to investigate the complex dependencies between
the layers of semantic processing. We have de-
signed the annotation model to incorporate fea-
tures and models developed both in the speech
and language research community and bridging
the gap between the two communities. Our multi-
layer annotation corpus allows the investigation
of cross-layer dependencies and across human-
machine and human-human dialogs as well as
training of semantic models which accounts for
predicate interpretation.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
ACL/Coling?98, pages 86?90.
F. Bechet, G. Riccardi, and D. Hakkani-Tur. 2004.
Mining spoken dialogue corpora for system evalu-
ation and modeling. In Proceedings of EMNLP?04,
pages 134?141.
H. Bunt. 2005. A framework for dialogue act specica-
tion. In Proceedings of SIGSEM WG on Represen-
tation of Multimodal Semantic Information.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. Salto - a versatile multi-
level annotation tool. In Proceedings of LREC 2006,
pages 517?520, Genoa, Italy.
A. Corazza, A. Lavelli, and G. Satta. 2007. Anal-
isi sintattica-statistica basata su costituenti. Intelli-
genza Artificiale, 4(2):38?39.
M. G. Core and J. F. Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communicative
Actions in Humans and Machines.
R. De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,
G. Riccardi, and G. Tur. 2008. Spoken language
understanding: A survey. IEEE Signal Processing
magazine, 25(3):50?58.
G. Leech and A. Wilson. 2006. EAGLES recommen-
dations for the morphosyntactic annotation of cor-
pora. Technical report, ILC-CNR.
C. Mu?ller and M. Strube. 2003. Multi-level annotation
in MMAX. In Proceedings of SIGDIAL?03.
J. M. Sinclair and R. M. Coulthard. 1975. Towards an
Analysis of Discourse: The English Used by Teach-
ers and Pupils. Oxford University Press, Oxford.
D. Traum. 1996. Conversational agency: The
TRAINS-93 dialogue manager. In Proceedings of
TWLT 11: Dialogue Management in Natural Lan-
guage Systems, pages 1?11, June.
S. Varges, G. Riccardi, and S. Quarteroni. 2008. Per-
sistent information state in a data-centric architec-
ture. In Proceedings of SIGDIAL?08.
41
Another Evaluation of Anaphora Resolution Algorithms and a
Comparison with GETARUNS? Knowledge Rich Approach
Rodolfo Delmonte, Antonella Bristot, Marco
Aldo Piccolino Boniforti, Sara Tonelli
Department of Language Sciences
Universit? Ca? Foscari ? Ca? Bembo
30120, Venezia, Italy
delmont@unive.it
Abstract
In this paper we will present an evaluation of
current state-of-the-art algorithms for Anaphora
Resolution based on a segment of Susanne
corpus (itself a portion of Brown Corpus), a
much more comparable text type to what is
usually required at an international level for
s u c h a p p l i c a t i o n d o m a i n s a s
Question/Answering, Information Extraction,
Text Understanding, Language Learning. The
portion of text chosen has an adequate size
which lends itself to significant statistical
measurements: it is portion A, counting 35,000
tokens and some 1000 third person pronominal
expressions. The algorithms will then be
compared to our system, GETARUNS, which
incorporates an AR algorithm at the end of a
pipeline of interconnected modules that
instantiate standard architectures for NLP. F-
measure values reached by our system are
significantly higher (75%) than the other ones.
1 Introduction
The problem of anaphora resolution (hence AR)
looms more and more as a prominent one in
unrestricted text processing due to the need to
recover semantically consistent information in most
current NLP applications. This problem does not
lend itself easily to a statistical approach so that
rule-based approaches seem the only viable
solution.
We present a new evaluation of three state-of-the-art
algorithms for anaphora resolution ? GuiTAR,
JavaRAP, MARS ? on the basis of a portion of
Susan Corpus (derived from Brown Corpus) a much
richer testbed than the ones previously used for
evaluation, and in any case a much more
comparable source with such texts as newspaper
articles and stories. Texts used previously ranged
from scientific manuals to descriptive scientific
texts and were generally poor on pronouns and rich
on nominal descriptions. Two of the algorithms ?
GuiTAR and JavaRAP - use Charniak?s parser
output, which contributes to the homogeneity of the
type of knowledge passed to the resolution
procedure. MARS, on the contrary, uses a more
sophisticated input, the one provided by Connexor
FDG-parser. The algorithms will then be compared
to our system, GETARUNS, which incorporated an
AR algorithm at the end of a pipeline of
interconnected modules that instantiate standard
architectures for NLP. The version of the algorithm
presented here is a newly elaborated one, and is
devoted to unrestricted text processing. It is an
upgraded version from the one discussed in
Delmonte (1999;2002a;2002b) and tries to
incorporate as much as possible of the more
sophisticated version implemented in the complete
GETARUN (see Delmonte 1990;1991;1992;1994;
2003;2004).
The paper is organized as follows: in section 2
below we briefly discuss architectures and criteria
for AR of the three algorithms evaluated. In section
3 we present our system. Section 4 is dedicated to a
compared evaluation and a general discussion.
2 The Anaphora Resolution Algorithms
We start by presenting a brief overview of three
state-of-the-art algorithms for anaphora resolution ?
GuiTAR, JavaRAP, MARS.
2.1 JavaRAP
As reported by the authors (Long Qiu, Min-Yen
Kan, Tat-Seng Chua, 2004) of the JAVA
implementation, head-dependent relations required
by RAP are provided by looking into the structural
?argument domain? for arguments and into the
structural ?adjunct domain? for adjuncts. Domain
information is important to establish disjunction
relations, i.e. to tell whether a third person pronoun
can look for antecedents within a certain structural
domain or not. According to Binding Principles,
Anaphors (i.e. reciprocal and reflexive pronouns),
3
must be bound ? search for their binder-antecedent ?
in their same binding domain ? roughly
corresponding to the notion of structural
?argument/adjunct domain?. Within the same
domains, Pronouns must be free. Head-argument or
head-adjunct relation is determined whenever two or
more NPs are sibling of the same VP.
Additional information is related to agreement
features, which in the case of pronominal
expressions are directly derived. As for nominal
expressions, features are expressed in case they are
either available on the verb ? for SUBJect NPs? or
else if they are expressed on the noun and some
other tricks are performed for conjoined nouns.
Gender is looked up in the list of names available on
the web. This list is also used to provide the
semantic feature of animacy.
RAP is also used to find pleonastic pronouns, i.e.
pronouns which have no referents. To detect
conditions for pleonastic pronouns a list of patterns
is indicated, which used both lexical and structural
information.
Salience weight is produced for each candidate
antecedent from a set of salience factors. These
factors include main Grammatical Relations,
Headedness, non Adverbiality, belonging to the
same sentence. The information is computed again
by RAP, directly on the syntactic structure. The
weight computed for each noun phrase is divided by
two in case the distance from the current sentence
increases. Only NPs contained within a distance of
three sentences preceding the anaphor are
considered by JavaRAP.
2.2 GuiTAR
The authors (Poesio, M. and Mijail A. Kabadjov
2004) present their algorithm as an attempt at
providing a domain independent anaphora
resolution module, ?that developers of NLE
applications can pick off the shelf in the way of
tokenizers, POS taggers, parsers, or Named
Entity classifiers?. For these reasons, GuiTAR has
been designed to be as independent as possible from
other modules, and to be as modular as possible,
thus ?allowing for the possibility of replacing
specific components (e.g., the pronoun resolution
component)?.
The authors have also made an attempt at specifying
what they call the Minimal Anaphoric Syntax
(MAS) and have devised a markup language based
on GNOME mark-up scheme. In MAS, Nominal
Expressions constitute the main processing units,
and are identified with the tag NE <ne>, which have
a CAT attribute, specifying the NP type: the-np,
pronoun etc., as well as Person, Number and Gender
attributes for agreement features. Also the internal
structure of the NP is marked with Mod and
NPHead tags.
The pre-processing phase uses a syntactic guesser
which is a chunker of NPs based on heuristics. All
NEs add up to a discourse model ? or better History
List - which is then used as the basic domain where
Discourse Segments are contained. Each Discourse
Segment in turn may be constituted by one or more
Utterances. Each Utterance in turn contains a list of
forward looking centers Cfs.
The Anaphora Resolution algorithm implemented is
the one proposed by MARS which will be
commented below. The authors also implemented a
simple algorithm for resolving Definite Descriptions
on the basis of the History List by a same head
matching approach.
2.3 MARS
The approach is presented as a knowledge poor
anaphora resolution algorithm (Mitkov R.
[1995;1998]), which makes use of POS and NP
chunking, it tries to individuate pleonastic ?it?
occurrences, and assigns animacy. The weighting
algorithm seems to contain the most original
approach. It is organized with a filtering approach
by a series of indicators that are used to boost or
reduce the score for antecedenthood to a given NP.
The indicators are the following ones:
FNP (First NP); INDEF (Indefinite NP); IV
(Indicating Verbs); REI (Lexical Reiteration); SH
(Section Heading Preference); CM (Collocation
Match); PNP (Prepositional Noun Phrases); IR
(Immediate Reference); SI (Sequential Instructions);
RD (Referential Distance); TP (Term Preference),
As the author comments, antecedent indicators
(preferences) play a decisive role in tracking down
the antecedent from a set of possible candidates.
Candidates are assigned a score (-1, 0, 1 or 2) for
each indicator; the candidate with the highest
aggregate score is proposed as the antecedent.
The authors comment is that antecedent indicators
have been identified empirically and are related
to salience (definiteness, givenness, indicating
verbs, lexical reiteration, section heading
preference, "non- prepositional" noun phrases), to
structural matches (collocation, immediate
reference), to referential distance or to preference
of terms. However it is clear that most of the
indicators have been suggested for lack of better
information, in particular no syntactic constituency
was available.
In a more recent paper (Mitkov et al, 2003) MARS
has been fully reimplemented and the indicators
updated. The authors seem to acknowledge the fact
that anaphora resolution is a much more difficult
task than previous work had suggested, In
4
unrestricted text analysis, the tasks involved in the
anaphora resolution process contribute a lot of
uncertainty and errors that may be the cause for low
performance measures.
The actual algorithm uses the output of Connexor?s
FDG Parser, filters instances of ?it? and eliminates
pleonastic cases, then produces a list of potential
antecedents by extracting nominal and pronominal
heads from NPs preceding the pronoun. Constraints
are then applied to this list in order to produce the
?set of competing candidates? to be considered
further, i.e. those candidates that agree in number
and gender with the pronoun, and also obey
syntactic constraints. They also introduced the use
of Genetic Algorithms in the evaluation phase.
The new version of MARS includes three new
indicators which seem more general and applicable
to any text, so we shall comment on them.
Frequent Candidates (FC) ? this is a boosting score
for most frequent three NPs; Syntactic Parallelism
(SP) ? this is a boosting score for NPs with the same
syntactic role as the pronoun, roles provided by the
FDG-Parser; Boost Pronoun (BP) ? pronoun
candidates are given a bonus (no indication of
conditions for such a bonus).
The authors also reimplemented in a significant way
the indicator First NPs which has been renamed,
?Obliqueness (OBL) ? score grammatical functions,
SUBJect > OBJect > IndirectOBJect > Undefined?.
MARS has a procedure for automatically identifying
pleonastic pronouns: the classification is done by
means of 35 features organized into 6 types and are
expressed by a mixture of lexical and grammatical
heuristics. The output should be a fine-grained
characterization of the phenomenon of the use of
pleonastic pronouns which includes, among others,
discourse anaphora, clause level anaphora and
idiomatic cases.
In the same paper, the authors deal with two more
important topics: syntactic constraints and animacy
identification.
3 GETARUNS
In a number of papers (Delmonte 1990;1991;
1992;1994; 2003;2004) and in a book (Delmonte
1992) we described our algorithms and the
theoretical background which inspired it. Whereas
the old version of the system had a limited
vocabulary and was intended to work only in limited
domains with high precision, the current version of
the system has been created to cope with
unrestricted text. In Delmonte (2002), we reported
preliminary results obtained on a corpus of
anaphorically annotated texts made available by
R.Mitkov on his website. Both definite descriptions
and pronominal expressions were considered,
success rate was at 75% F-measure. In those case
we used a very shallow and robust parser which
produced only NP chunks which were then used to
fire anaphoric processes. However the texts making
up the corpus were technical manuals, where the
scope and usage of pronominal expressions is very
limited.
The current algorithm for anaphora resolution works
on the output of a complete deep robust parser
which builds an indexed linear list of dependency
structures where clause boundaries are clearly
indicated; differently from Connexor, our system
elaborates both grammatical relations and semantic
roles information for arguments and adjuncts.
Semantic roles are very important in the weighting
procedures. Our system also produces implicit
grammatical relations which are either controlled
SUBJects of untensed clauses, arguments or
adjuncts of relative clauses.
As to the anaphoric resolution algorithm, it is based
on the original Sidner?s (1983:Chapter 5) and
Webber?s (1983:Chapter 6) intuitions on Focussing
in Discourse. We find distributed, local approaches
to anaphora resolution more efficient than
monolithic, global ones. In particular we believe
that due to the relevance of structural constraints in
the treatment of locally restricted classes of
pronominal expressions, it is more appropriate to
activate different procedures which by dealing
separately with non-locally restricted classes also
afford separate evaluation procedures. There are
also at least two principled reasons for the
separation into two classes.
The first reason is a theoretical one. Linguistic
theory has long since established without any doubt
the existence in most languages of the world of at
least two classes: the class of pronouns which must
be bound locally in a given domain and the class of
pronouns which must be left free in the same
domain ? as a matter of fact, English also has a third
class of pronominals, the so-called long-distance
subject-of-consciousness bound pronouns (see
Zribi-Hertz A., 1989);
The second reason is empirical. Anaphora resolution
is usually carried out by searching antecedents
backward w.r.t. the position of the current anaphoric
expression. In our approach, we proceed in a clause
by clause fashion, weighting each candidate
antecedent w.r.t. that domain, trying to resolve it
locally. Weighting criteria are amenable on the one
hand to linear precedence constraints, with scores
assigned on a functional/semantic basis. On the
other hand, these criteria may be overrun by a
functional ranking of clauses which requires to treat
main clauses differently from secondary clauses,
5
and these two differently from complement clauses.
On the contrary, global algorithms neglect
altogether such requirements: they weight each
referring expression w.r.t. the utterance, linear
precedence is only physically evaluated, no
functional correction is introduced.
3.1 Referential Policies and Algorithms
There are also two general referential policy
assumption that we adopt in our approach: The first
one is related to pronominal expressions, the second
one to referring expressions or entities to be asserted
in the History List, and are expressed as follows:
- no more than two pronominal expressions are
allowed to refer back in the previous discourse
portion;
- at discourse level, referring expressions are
stored in a push-down stack according to
Persistence principles.
Persistence principles respond to psychological
principles and limit the topicality space available to
user w.r.t. a given text. It has a bidimensional
nature: it is determined both in relation to an overall
topicality frequency value and to an utterance
number proximity value.
Only ?persistent? referring expressions are allowed
to build up the History List, where persistence is
established on the basis of the frequency of
topicality for each referring expression which must
be higher than 1. All referring expression asserted as
Topic (Secondary, Potential) only once are
discarded in case they appeared at a distance
measured in 5 previous utterances. Proximate
referring expressions are allowed to be asserted in
the History List.
In particular, if Mitkov considers the paragraph as
the discourse unit most suitable for coreferring and
cospecifying operation at discourse level, we prefer
to adopt a parameterized procedure which is
definable by the user and activated automatically: it
can be fired within a number that can vary from
every 10 up to 50 sentences. Our procedure has the
task to prune the topicality space and reduce the
number of perspective topic for Main and
Secondary Topic. Thus we garbage-collect all non-
relevant entities. This responds to the empirically
validated fact that as the distance between first and
second mention of the same referring expression
increases, people are obliged to repeat the same
linguistic description, using a definite expression or
a bare NP. Indefinites are unallowed and may only
serve as first mention; they can also be used as
bridging expression within opaque propositions. The
first procedure is organized as follows:
A. For each clause,
1. we collect all referential expressions and
weight them (see B below for criteria) ? this
is followed by an automatic ranking;
2. then we subtract pronominal expressions;
3. at clause level, we try to bind personal and
possessive pronouns obeying specific
structural properties; we also bind reflexive
pronouns and reciprocals if any, which must
be bound obligatorily in this domain;
4. when binding a pronoun, we check for
disjointness w.r.t. a previously bound
pronoun if any;
5. all unbound pronouns and all remaining
personal pronouns are asserted as
?externals?, and are passed up to the higher
clause levels;
B. Weighting is carried out by taking into account
the following linguistic properties associated to each
referring expression:
1. Grammatical Function with usual hierarchy
(SUBJ > ARG_MOD > OBJ > OBJ2 > IOBJ >
NCMOD);
2. Semantic Roles, as they have been labelled in
FrameNet, and in our manually produced
frequency lexicon of English;
3. Animacy: we use 75 semantic features derived
from WordNet, and reward Human and
Institution/Company labelled referring
expressions;
4. Functional Clause Type is further used to
introduce penalties associated to those referring
expressions  which don?t belong to main clause.
C. Then we turn at the higher level ? if any -, and
we proceed as in A., in addition
1. we try to bind pronouns passed up by the lower
clause levels
o if successful, this will activate a retract of the
?external? label and a label of
?antecedenthood? for the current pronoun
with a given antecedent;
o the best antecedent is chosen by recursively
trying to match features of the pronoun with
the first available antecedent previously
ranked by weighting;
o here again whenever a pronoun is bound we
check for disjointness at utterance level.
D. This is repeated until all clauses are examined
and all pronouns are scrutinised and bound or left
free.
E. Pronouns left free ? those asserted as externals ?
will be matched tentatively with the best candidates
provided this time by a ?centering-like? algorithm.
Step A. is identical and is recursively repeated until
all clauses are processed.
6
Then, we move to step B. which in this case will use
all referring expressions present in the utterance,
rather than only those available locally.
Fig. 1 GETARUNS AR algorithm
3.2 Focussing Revisited
Our version of the focussing algorithm follows
Sidner?s proposal (Sidner C., 1983; Grosz B., Sidner
C., 1986), to use a Focus Stack, a certain Focus
Algorithm with Focus movements and data
structures to allow for processing simple inferential
relations between different linguistic descriptions
co-specifying or coreferring to a given entity.
Our Focus Algorithm is organized as follows: for
each utterance, we assert three ?centers? that we call
Main, Secondary and the first Potential Topic,
which represent the best three referring expressions
as they have been weighted in the candidate list
used for pronominal binding; then we also keep a
list of Potential Topics for the remaining best
candidates. These three best candidates repositories
are renovated at each new utterance, and are used
both to resolve pronominal and nominal
cospecification and coreference: this is done both in
case of strict identity of linguistic description and of
non-identity. The second case may occur either
when derivational morphological properties allow
the two referring expressions to be matched
successfully, or when a simple hyponym/hypernym
relation is entertained by two terms, one of which is
contained in the list of referring expressions
collected from the current sentence, and the other is
among one of the entities stored in the focus list.
The Main Topic may be regarded the Forward
Looking Center in the centering terminology or the
Current Focus. All entities are stored in the History
List (HL) which is a stack containing their
morphological and semantic features: this is not to
be confused with a Discourse Model - what we did
in the deep complete system anaphora resolution
module ? which is a highly semantically wrought
elaboration of the current text. In the HL every new
entity is assigned a semantic index which identifies
it uniquely. To allow for Persistence evaluation, we
also assert rhetorical properties associated to each
entity, i.e. we store the information of topicality (i.e.
whether it has been evaluated as Main, Secondary or
Potential Topic), together with the semantic ID and
the number of the current utterance. This is
subsequently used to measure the degree of
Persistence in the overall text of a given entity, as
explained below.
In order to decide which entity has to become Main,
Secondary or Potential Topic we proceed as
follows:
- we collect all entities present in the History List
with their semantic identifier and feature list
and proceed to an additional weighting
procedure;
- nominal expressions, they are divided up into
four semantic types: definite, indefinite, bare
NPs, quantified NPs. Both definite and
indefinite NP may be computed as new or old
entity according to contextual conditions as
will be discussed below and are given a
rewarding score;
- we enumerate for each entity its persistence in
the previous text, and keep entities which have
frequency higher than 1, we discard the others;
- we recover entities which have been asserted in
the HL in proximity to the current utterance, up
to four utterances back;
- we use this list to ?resolve? referring
expressions contained in the current utterance;
- if this succeeds, we use the ?resolved? entities
as new Main, Secondary, and Potential Topics
and assert the rest in the Potential Topics stack;
- if this fails ? also partially ? we use the best
candidates in the weighted list of referring
expressions to assert the new Topics. It may be
the case that both resolved and current best
candidates are used, and this is by far the most
common case.
4. Evaluation and General Discussion
Evaluating anaphora resolution systems calls for a
reformulation of the usual parameters of Precision
and Recall as introduced in IR/IE field: in that case,
there are two levels that are used as valuable results;
a first stage where systems are measured for their
7
capacity to retrieve/extract relevant items from the
corpus/web (coverage-recall). Then a second stage
follows in which systems are evaluated for their
capacity to match the content of the query
(accuracy-precision). In the field of IR/IE items to
be matched are usually constituted by words/phrases
and pattern-matching procedures are the norm.
However, for AR systems this is not sufficient and
NLP heavy techniques are used to get valuable
results. As Mitkov also notes, this phase jeopardizes
the capacity of AR systems to reach satisfactory
accuracy scores simply because of its intrinsic
weakness: none of the off-the-shelf parsers currently
available overcomes 90% accuracy.
To clarify these issues, we present here below two
Tables: in the first one we report data related to the
vexed question of whether pleonastic ?it? should be
regarded as part of the task of anaphora resolution
or rather part of a separate classification task ? as
suggested in a number of papers by Mitkov. In the
former case, they should contribute to the overall
anaphora resolution evaluation metrics; in the latter
case they should be compute separately as a case of
classification over all occurrences of ?it? in the
current dataset and discarded from the overall count.
Even though we don?t agree fully with Mitkov?s
position, we find it useful to deal with ?it? separate,
due to its high inherent ambiguity. Besides, it is true
that the AR task is not like any Information
Retrieval task.
In Table 1 below we reported figures for ?it? in
order to evaluate the three algorithms in relation to
the classification task. Then in Table 2. we report
general data where we computed the two types of
accuracy reported in the literature. In Table 1 we
split results for ?it? into Wrong Reference vs Wrong
Classification: following Mitkov, in case we only
computed anaphora related cases and disregarded
those cases of ?it? which were wrongly classified as
expletives. Expletive ?it? present in the text are 189:
so at first we computed coverage and accuracy with
the usual formula that we report below. Then we
subtracted wrongly classified cases from the number
of total ?it? found in one case (following Mitkov
who claims that wrongly classified ?it? found by the
system should not count; in another case, this
number is subtracted from the total number of ?it?
to be found in the text. Only for MARS we then
computed different measures of Coverage and
Accuracy. If we regard this approach worth
pursuing, we come up with two Adjusted Accuracy
measures which are related to the revised total
numbers of anaphors by the two subtractions
indicated above.
We computed manually all third person pronominal
expressions and came up with a figure 982 which is
Table 1. Expletive ?it? compared results
MARS JavaRAP GuiTAR GETARUNS
Coverage 163  (86.2%) 188  (99.5%) 188  (99.5%) 171  (91%)
Accuracy 1 63  (33.3%) 73  (38.6%) 75  (39.7%) 87  (46 %)
Wrong Classification 44
163-44=119
189-44=145
49
189-49=140
64
189-64=125
53
189-53=136
Wrong Reference 56 66 49 32
Accuracy 2 63  (38.6%)
Adjusted Accuracy 2 63  (52.9%)
Adjusted Accuracy 3 63  (43.4%) 73  (52.1%) 75  (60%) 87 (64 %)
only confirmed by one of the three systems
considered: JavaRAP. Pronouns considered are the
following one, lower case and upper case included:
Possessives ? his, its, her, hers, their, theirs
Personals ? he, she, it, they, him, her, it, them
(where ?it? and ?her? have to be disambiguated)
Reflexives ? himself, itself, herself, themselves
There are 16 different wordforms. As can be seen
from the table below, apart from JavaRAP, none of
the other systems considered comes close to 100%
coverage.
Computing general measures for Precision and
Recall we have three quantities (see also Poesio &
Kabadjov):
? total number of anaphors present in the text;
? anaphors identified by the system;
? correctly resolved anaphors.
Formulas related to Accuracy/Success Rate or
Precision are as follows: Accuracy1 = number of
successfully resolved anaphors/number of all
anaphors; Accuracy2 = number of successfully
resolved anaphors/number of anaphors found
(attempted to be resolved). Recall - which should
correspond to Coverage - we come up with formula:
R= number of anaphors found /number of all
anaphors to be resolved (present in the text). Finally
the formula for F-measure is as follows:
2*P*R/(P+R) where P is chosen as Accuracy 2.
8
Table 2. Overall results Coverage/Accuracy
COVERAGE ACCURACY 1 ACCURACY 2 F-measure
MARS 936  (95.3%) 403/982  (41.5%) 403/903 (43%) 59.26%
JavaRAP 981  (100%) 490/982  (49.9%) 490/981 (50%) 66.7%
GUITAR 824  (84.8%) 445/982  (45.8%) 445/824 (54%) 65.98%
GETARUNS 885  (90.1%) 555/982  (56.5%) 555/885 (62.7%) 73.94%
In absolute terms best accuracy figures have been
obtained by GETARUNS, followed by JavaRAP. So
it is still thanks to the classic Recall formula that
this result stands out clearly. We also produced
another table which can however only be worked
out for our system, which uses a distributed
approach. We managed to separate pronominal
expressions in relation to their contribution at the
different levels of anaphora resolution considered:
clause level, utterance level, discourse level. At
clause level, only those pronouns which must be
bound locally are checked, as is the case with
reflexive pronouns, possessives, some cases of
expletive ?it?: both arguments and adjuncts may
contribute the appropriate antecedent. At utterance
level, in case the sentence is complex or there is
more than one clause, also personal subject/object
pronouns may be bound (if only preferentially so).
Eventually, those pronouns which do not find an
antecedent are regarded discourse level pronouns.
We collapsed under CLAUSE all pronouns bound at
clause and utterance level; DISCOURSE contains
only sentence external pronouns. Expletives have
been computed in a separate column.
Table 3. GETARUNS pronouns collapsed at structural level
CLAUSE DISCOURSE EXPLETIVES TOTALS
Pronouns found 410 366 109 885
Correct 266 222 67 555
Errors made 144 144 42 330
As can be noticed easily, the highest percentage of
pronouns found is at Clause level: this is not
however the best performance of the system, which
on the contrary performs better at discourse level.
Expletives contribute by far the highest correct
result. We also found correctly 47 ?there? expletives
and 6 correctly classified pronominal ?there? which
however have been left unbound. The system also
found 48 occurrences of deictic discourse bound
?this? and ?that?, which corresponds to the full
coverage.
Finally, nominal expressions: the History List (HL)
has been incremented up to 2243 new entities. The
system identified 2773 entities from the HL by
matching their linguistic description. The overall
number of resolution actions taken by the Discourse
Level algorithm is 1861: this includes both cases of
nominal and pronominal expressions. However,
since only 366 can be pronouns, the remaining 1500
resolution actions have been carried out on nominal
expressions present in the HL. If we compare these
results to the ones computed by GuiTAR, which
assign semantic indices to NamedEntities
disregarding their status of anaphora, we can see
that the whole text is made up of 12731 NEs.
GuiTAR finds 1585 cases of identity relations
between a NE and an antecedent. However,
GuiTAR introduces always new indices and creates
local antecedent-referring expression chains rather
than repeating the same index of the chain head. In
this way, it is difficult if not impossible to compute
how many times the text corefers/cospecifies to the
same referring expressions. On the contrary, in our
case, this can be easily computed by counting how
many times the same semantic index is being
repeated in a ?resolution? or ?identity? action of the
anaphora resolution algorithm. For instance, the
Jury is coreferred/cospecified 12 times; Price Daniel
also 12 times and so on.
5. Conclusions
The error rate of both Charniak?s and Connexor?s as
reported in the literature, is approximately the same,
20%; this notwithstanding, MARS has a slightly
reduced coverage when compared with JavaRAP,
96%. GuiTAR has the worst coverage, 85%. As to
accuracy, none of the three algorithms overruns
50%: JavaRAP has the best score 49.9%. However
GETARUNS has 63% correct score, with 90%
coverage.
There are at least three reasons why our system has a
better performance: one is the presence of a richer
functional and semantic information as explained
above, which comes with augmented head-
dependent structures. Second reason is the decision
to split the referential process into two and treat
utterance level pronominal expressions separately
from discourse level ones. Third reason is the way
in which discourse level anaphora resolution is
9
organized: our version of the Centering algorithm
hinges on a record of a list of best antecedents
weighted on the basis of their behaviour in History
List and on their intrinsic semantic properties. These
three properties of our AR algorithm can be dubbed
the Knowledge Rich approach.
F-measures approximates very closely what we
obtained in a previous experiment: however, as a
whole it is an insufficient score to insure adequate
confidence in semantic substitution of anaphoric
items by the head of the antecedent. Improvements
need to come from parsing and the lexical
component.
Acknowledgements
Thanks to three anonymous reviewers who helped us
improve the overall layout of the paper.
References
Delmonte R. 1990. Semantic Parsing with an LFG-based
Lexicon and Conceptual Representations, Computers
& the Humanities, 5-6, pp.461-488.
Delmonte R. and D.Bianchi 1991. Binding Pronominals
with an LFG Parser, Proceeding of the Second
International Workshop on Parsing Technologies,
Cancun(Messico), ACL 1991, pp.59-72.
Delmonte R., D.Bianchi 1992. Quantifiers in Discourse,
in Proc. ALLC/ACH'92, Oxford(UK), OUP, pp. 107-
114.
Delmonte R. 1992. Linguistic and Inferential Processing
in Text Analysis by Computer, UP, Padova.
Delmonte R. and D.Bianchi 1994. Computing Discourse
Anaphora from Grammatical Representation, in
D.Ross & D.Brink(eds.), Research in Humanities
Computing 3, Clarendon Press, Oxford, 179-199.
Delmonte R. and D.Bianchi 1999. Determining Essential
Properties of Linguistic Objects for Unrestricted Text
Anaphora Resolution, Proc. Workshop on Procedures
in Discourse, Pisa, pp.10-24.
Delmonte R., L.Chiran, and C.Bacalu, (2000). Towards
An Annotated Database For Anaphora Resolution,
LREC, Atene, pp.63-67.
Delmonte R. 2002a. From Deep to Shallow Anaphora
Resolution: What Do We Lose, What Do We Gain, in
Proc. International Symposium RRNLP, Alicante,
pp.25-34.
Delmonte R. 2002b. From Deep to Shallow Anaphora
Resolution:, in Proc. DAARC2002 , 4th Discourse
Anaphora and Anaphora Resolution Colloquium,
Lisbona, pp.57-62.
Delmonte, R. 2003. Getaruns: a Hybrid System for
Summarization and Question Answering. In Proc.
Natural Language Processing (NLP) for Question-
Answering, EACL, Budapest, pp. 21-28.
Delmonte R. 2004. Evaluating GETARUNS Parser with
GREVAL Test Suite, In Proc. ROMAND - 20th
COLING, University of Geneva, pp. 32-41.
Di Eugenio B. 1990. Centering Theory and the Italian
pronominal system, COLING, Helsinki.
Grosz B. and C. Sidner 1986. Attention, Intentions, and
the Structure of Discourse, Computational Linguistics
12 (3), 175-204.
Kennedy, C. and B. Boguraev, 1996. Anaphora for
everyone: Pronominal anaphora resolution without a
parser. In Proc. of the 16th COLING, Budapest.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua, 2004. A
Public Reference Implementation of the RAP
Anaphora Resolution Algorithm, In Proceedings of the
Language Resources and Evaluation Conference 2004
(LREC 04), Lisbon, Portugal, pp.1-4.
Mitkov R. 1995. Two Engines are better than one:
Generating more power and confidence in the search
for the antecedent, Proceedings of Recent Advances in
Natural Language Processing, Tzigov Chark, 87-94.
Mitkov, R. 1998. Robust Pronoun Resolution with
limited knowledge. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING?98)/ACL?98 Conference, pp.
869-875, Montreal, Canada.
Mitkov, R., R. Evans, and C. Orasan. 2002. A New, Fully
Automatic Version of Mitkov?s Knowledge-Poor
Pronoun Resolution Method, Proceedings of CICLing-
2002, pp.1-19.
Poesio, M. and R. Vieira, 1998. A corpus-based
investigation of definite description use.
Computational  Linguistics, 24(2):183?216.
Poesio, M. and Mijail A. Kabadjov 2004. A General-
Purpose, off-the-shelf Anaphora Resolution Module:
Implementation and Preliminary Evaluation
Proceedings of the Language Resources and
Evaluation Conference 2004 (LREC 04), Lisbon,
Portugal, pp.1-4.
Sidner C. 1983. Focusing in the Comprehension of
Definite Anaphora, in Brady M., Berwick R.(eds.),
Computational Models of Discourse, MIT Press,
Cambridge, MA, 267-330.
Webber B. 1983. So can we Talk about Now?, in Brady
M., Berwick R.(eds.), Computational Models of
Discourse, MIT Press, Cambridge, MA, 331-371.
Webber B. L. 1991. Structure and Ostension in the
Interpretation of Discourse Deixis, in Language and
Cognitive Processes 6 (2):107-135.
Zribi-Hertz A. 1989. Anaphor Binding and Narrative
Point of View: English reflexive pronouns in sentence
and discourse, Language, 65(4):695-727.
10
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 48?53,
Prague, June 2007. c?2007 Association for Computational Linguistics
Entailment and Anaphora Resolution in RTE3 Rodolfo Delmonte, Antonella Bristot, Marco Aldo Piccolino Boniforti, Sara Tonelli Department of Language Sciences Universit? Ca? Foscari ? Ca? Bembo 30123, Venezia, Italy delmont@unive.it
Abstract We present VENSES, a linguistically-based approach for semantic inference which is built around a neat division of labour between two main components: a grammatically-driven subsystem which is responsible for the level of predicate-arguments well-formedness and works on the output of a deep parser that produces augmented head-dependency structures. A second subsystem fires allowed logical and lexical inferences on the basis of different types of structural transformations intended to produce a semantically valid meaning correspondence. In the current challenge, we produced a new version of the system, where we do away with grammatical relations and only use semantic roles to generate weighted scores. We also added a number of additional modules to cope with fine-grained inferential triggers which were not present in previous dataset. Different levels of argumenthood have been devised in order to cope with semantic uncertainty generated by nearly-inferrable Text-Hypothesis pairs where the interpretation needs reasoning. RTE3 has introduced texts of paragraph length: in turn this has prompted us to upgrade VENSES by the addition of a discourse level anaphora resolution module, which is paramount to allow entailment in pairs where the relevant portion of text contains pronominal expressions. We present the system, its relevance to the task at hand and an evaluation. 1 Introduction The system for semantic evaluation VENSES (Venice Semantic Evaluation System) is organized as a pipeline of two subsystems: the first is a 
reduced version of GETARUN, our system for Text Understanding; the second is the semantic evaluator which was previously created for Summary and Question evaluation and has now been thoroughly revised for the new more comprehensive RTE task. The reduced GETARUN is composed of the usual sequence of sub-modules common in Information Extraction systems,  i.e. a tokenizer, a multiword and NE recognition module, a PoS tagger based on finite state automata; then a multilayered cascaded RTN-based parser which produces c-structures and has an additional module to map them into tentative grammatical functions in LFG terms. The functionally labeled constituents are then passed to an interpretation module that uses subcategorization information to choose final grammatical relations and assign semantic roles. Eventually, the system has a pronominal binding module that works at clause level for lexical personal, possessive and reflexive pronouns, which are substituted by the heads of their antecedents - if available. The output of the binding module can contain one or more ?external? pronouns, which need to be bound in the discourse. This output is passed to the Anaphora Resolution module presented in detail in Delmonte (2006) and outlined below. This module works on the so-called History List of entities present in the text so far. In order to make the output of this module usable by the Semantic Evaluator, we decided to produce a flat list of semantic vectors which contain all semantic related items of the current sentence. Inside these vectors, pronominal expressions are substituted by the heads of their antecedents. Basically, the output of the system is elaborated on top of the output of the parser, which produces a flat list of fully indexed augmented head-dependent structures (AHDS) with Grammatical Relations (GRs) and Semantic Roles (SRs) labels. Notable additions to the usual formalism is the presence of a distinguished Negation relation; we 
48
also mark modals and progressive mood, tense and voice (for similar approaches see Bos et al, Raina et al).  The evaluation system uses a cost model with rewards/penalties for T/H pairs where text entailment is interpreted in terms of semantic similarity: the closest the T/H pairs are in semantic terms the more probable is their entailment. Rewards in terms of scores are assigned for each "similar" semantic element; penalties on the contrary can be expressed in terms of scores or they can determine a local failure and a consequent FALSE decision ? more on scoring below. The evaluation system is made up of two main Modules: the first takes care of paraphrases and idiomatic expressions; the second is a sequence of linguistic rule-based sub-modules. Their work is basically a count of how much similar are linguistic elements in the H/T pair. Similarity may range from identical linguistic descriptions, to synonymous or just morphologically derivable ones. As to GRs and SRs, they are scored higher according to whether they belong to the subset of core relations and roles, i.e. obligatory arguments, or not, that is adjuncts. Both Modules go through General Consistency checks which are targeted to high level semantic attributes like presence of modality, negation, and opacity operators. The latter ones are expressed either by the presence of discourse markers of conditionality or by a secondary level relation intervening between the main predicate and a governing higher predicate belonging to the class of non factual verbs, but see below. All rule-based sub-modules are organized into a sequence of syntactic-semantic transformation rules going from those containing axiomatic-like paraphrase HDSs which are ranked higher, to rules stating conditions for similarity according to a scale of argumentality (more below) and are ranked lower. All rules address HDSs and SRs. Propositional level ones have access to vectors of semantic features which encode propositional level information. 2   The Task of Semantic Inference Evaluation As happened in the previous Challenge, this year?s Challenge is also characterized by the presence of a relatively high number (we counted 
more than 100 True and another 100 False pairs, i.e. 25%) of T/H pairs which require two particularly hard NLP processes to set up: - reasoning  - paraphrases (reformulation) In addition to that, we found a significant number of pairs ? about 150 in the development set and some 100 in the test set ? in which pronominal binding and anaphora resolution are essential to the definition of entailment. In particular, in such cases it is only by virtue of the substitution of a pronominal expression with the linguistic description of its antecedent that the appropriate Predicate-Argument relations required in order to fire the inference is made available ? but see below. Setting up rules for Paraphrase evaluation, requires the system to actually use the lemmas ? and in some cases the wordforms - to be matched together in axiomatic-like structures: in other words, in these cases the actual linguistic expressions play a determining role in the task to derive a semantic inference. In order for these rules to be applied by the SE, we surmise it is important to address a combination of Syntactic and Semantic structures, where Lexical Inference also may play a role: the parser in this case has a fundamental task of recovering the heads of antecedents of all possible pronominal and unexpressed Grammatical relations. It is important to remark that besides pronominal-antecedent relations, our system also recovers all those cases defined as Control in LFG theory, where basically the unexpressed subject of an untensed proposition (infinitival, participial, gerundive) either lexically, syntactically or structurally controlled is bound to some argument of the governing predicate.  2.1   Pronominal Binding and Anaphora Resolution This year RTE introduces as a novelty a certain number (117 in the Test set ? 135 in the Dev set) of long Texts, of paragraph length. This move is justified by the need to address more realistic data, and consequently to tune the whole process of semantic evaluation to the problems related to such data. Thus more relevance is given to empirical issues to be tackled, rather than to the theoretical ones, which however don?t disappear but may assume less importance. 
49
When a system has to cope with paragraph length texts, the basic difference with short texts regards the problem of anaphora resolution. In short texts, pronominal expressions constituted a minor 
problem and all referring expressions were specified fully. Not so in long texts, as can be seen from the Table below: 
  He Him His She Her It Its They Their Them Total Test 80 15 91 19 18 91 68 43 63 15 485 Dev. 113 16 136 27 35 123 76 44 64 18 652 Total 193 31 227 46 53 214 144 87 127 33 1137 Table 1. 3rd person pronominal expressions contained in RTE3 data sets  As can be seen from this table, the problem a system is faced with is not just to cope with an ad hoc solution for single cases where the pronoun is placed, for instance, in sentence first position and it might be easy to recover its antecedent by some empirical ad hoc procedure. The problem needs to be addressed fully and this requires a full-fledged system for anaphora resolution. One such system is shown in Fig. 2 below, where we highlight the architecture and main processes undergoing at the anaphora level. First of all, the subdivision of the system into two levels: Clause level ? intrasentential pronominal phenomena ? where all pronominal expressions contained in modifiers, adjuncts or complement clauses receive their antecedent locally. Possessive pronouns, pronouns contained in relative clauses and complement clauses choose preferentially their antecedents from list of higher level referring expressions. Not so for those pronouns contained in matrix clauses. In particular the ones in subject position are to be coreferred in the discourse. This requires the system to be equipped with a History List of all referring expressions to be used when needed. In the system, three levels are indicated: Clause level, i.e. simple sentences; Utterance level, i.e. complex sentences; Discourse level, i.e. intersententially. Our system computes semantic structures in a sentence by sentence fashion and any information useful to carry out anaphoric processes needs to be made available to the following portion of text, and eventually to the Semantic Evaluation that computes entailment. We will comment a number of significant examples to clarify the way in which our system operates. 3. Anaphora Resolution for RTE Why is it important to implement an anaphora resolution algorithm for RTE? I think the reason is quite straightforward: pronominal expressions do 
not allow any inference to be drawn or any otherwise semantic similarity processes to be fired, being inherently referentially poor. In order to be able to use information made available by the verb in the sentence in which a pronoun is used, the antecedent must be recovered and the pronoun substituted by its head. So, very simply, RTE needs anaphora resolution in order to allow the system to use verb predicates where pronouns have been used to corefer to some antecedent in the previous text. In turn that verb predicate is just what is being searched for in the first place, and in our case it is the one contained in the Hypothesis. The current algorithm for anaphora resolution works on the output of the complete deep robust parser which builds an indexed linear list of dependency structures where clause boundaries are clearly indicated. As said above, our system elaborates both grammatical relations and semantic roles information for arguments and adjuncts. Semantic roles are very important in the weighting procedures.  As to the anaphoric resolution algorithm, it is a distributed, local ? clause-based - approach to anaphora resolution which we regard more efficient than monolithic, global ones. Linguistic theory has long since established without any doubt the existence in most languages of the world of at least two classes of pronouns: the class which must be bound locally in a given domain ? roughly the clause, and the class which must be left free in the same domain. In our approach, we proceed in a clause by clause fashion, weighting each candidate antecedent w.r.t. that domain, trying to resolve it locally. Weighting criteria are amenable on the one hand to linear precedence constraints, with scores assigned on a functional/semantic basis. On the other hand, these criteria may be overrun by a functional ranking of clauses which requires to 
50
treat main clauses differently from secondary clauses, and these two differently from complement clauses.  There are also two general referential policy assumption that we adopt in our approach: The first one is related to pronominal expressions, the second one to referring expressions or entities to be asserted in the History List, and are expressed as follows: - no more than two pronominal expressions are allowed to refer back in the previous discourse portion; - at discourse level, referring expressions are stored in a push-down stack according to Persistence principles. Only ?persistent? referring expressions are allowed to build up the History List, where persistence is established on the basis of the frequency of topicality for each referring expression which must be higher than 1. All referring expression asserted as Topic (Main, Secondary, Potential) only once are discarded in case they appeared at a distance measured in 5 previous utterances. Proximate referring expressions are allowed to be asserted in the History List. The first procedure is organized as follows.  A. For each clause, 1. we collect all referential expressions and weight them ? this is followed by an automatic ranking; 2. then we subtract pronominal expressions; 3. at clause level, we try to bind personal and possessive pronouns obeying specific structural properties; we also bind reflexive pronouns and reciprocals if any, which must be bound obligatorily in this domain; 4. when binding a pronoun, we check for disjointness w.r.t. a previously bound pronoun if any; 5. all unbound pronouns and all remaining personal pronouns are asserted as ?externals?, and are passed up to the higher clause levels; B. Then we turn at the higher level ? if any -, and we proceed as in A., in addition we try to bind pronouns passed up by the lower clause levels o if successful, this will activate a retract of the ?external? label and a label of ?antecedenthood? for the current pronoun with a given antecedent; o the best antecedent is chosen by recursively trying to match features of the pronoun with the 
first available antecedent previously ranked by weighting; o here again whenever a pronoun is bound we check for disjointness at utterance level.  
 Fig. 1. Anaphoric Processes in VENSES  C. This is repeated until all clauses are examined and all pronouns are scrutinised and bound or left free. D. Pronouns left free ? those asserted as externals ? will be matched tentatively with the best candidates provided this time by a ?centering-like? algorithm. Step A. is identical and is recursively repeated until all clauses are processed. 3.1 Focussing Revisited Our version of the focussing algorithm follows Sidner?s proposal (Sidner C., 1983; Grosz B., Sidner C., 1986), to use a Focus Stack, a certain Focus Algorithm with Focus movements and data structures to allow for processing simple inferential relations between different linguistic descriptions co-specifying or coreferring to a given entity.  Our Focus Algorithm is organized as follows: for each utterance, we assert three hierarchically ordered ?centers? that we call Main, Secondary and the first Potential Topic, which represent the best three referring expressions as they have been weighted in the candidate list used for pronominal binding; then we also keep a list of Potential Topics for the remaining best candidates. These 
51
three best candidates repositories are renovated at each new utterance, and are used both to resolve pronominal and nominal cospecification and coreference: this is done both in case of strict identity of linguistic description and of non-identity.  The Main Topic may be regarded the Forward Looking Center in the centering terminology or the Current Focus. All entities are stored in the History List (HL) which is a stack containing their morphological and semantic features. In the HL every new entity is assigned a semantic index which identifies it uniquely. To allow for Persistence evaluation, we also assert rhetorical properties associated to each entity, i.e. we store the information of topicality (i.e. whether it has been evaluated as Main, Secondary or Potential Topic), together with the semantic ID and the number of the current utterance. This is subsequently used to measure the degree of Persistence in the overall text of a given entity, as explained below. In order to decide which entity has to become Main, Secondary or Potential Topic we proceed as follows: - we collect all entities present in the History List with their semantic identifier and feature list and proceed to an additional weighting procedure; - nominal expressions, they are divided up into four semantic types: definite, indefinite, bare NPs, quantified NPs. Both definite and indefinite NP may be computed as new or old entity according to contextual conditions as will be discussed below and are given a rewarding score; - we enumerate for each entity its persistence in the previous text, and keep entities which have frequency higher than 1, we discard the others; - we recover entities which have been asserted in the HL in proximity to the current utterance, up to four utterances back; - we use this list to ?resolve? referring expressions contained in the current utterance; - if this succeeds, we use the ?resolved? entities as new Main, Secondary, and Potential Topics and assert the rest in the Potential Topics stack; - if this fails ? also partially ? we use the best candidates in the weighted list of referring expressions to assert the new Topics. It may be the case that both resolved and current best 
candidates are used, and this is by far the most common case. In example n.3 below, the first possessive pronoun ?his? is met at Utterance level ? the first sentence has two clauses: clause 1, headed by the predicate DIVORCE, and clause 2, headed by MARRY. ?His? will look for a masculine antecedent and Chabrol will be chosen, also for weights associated to it, being the higher subject. This will produce the following semantic structure, which is made of a Head, a Semantic Role and an Index, - Chabrol-poss-sn2 which is the output of the substitution of ?his? present in the same structure by means of information made available by the Anaphoric module. Note that the index of a modifier points to the governing head, in this case ?wife?, the apposition associated to ?Agnes?, which in turn is the OBJect of DIVORCE.  T/H pair n. 3 T: Claude Chabrol divorced Agnes, his first wife, to marry the actress St?phane Audran. His third wife is Aurore Paquiss. H: Aurore Paquiss married Chabrol.  When the first sentence is passed to the semantic interpreter, anaphoric processes have already been completed and the information is then transferred to semantic structure which will register the anaphoric relation by the substitution operation. However this specific relation is not the one that really matters in the current T/H pair. When the system passes to the analysis of the following sentence it has another possessive pronoun which is contained in a SUBJect NP. By definition, these pronouns take their antecedent from the discourse level. To have the system do that, the pronoun has to be left free at sentence level, i.e. it must be computed as ?external? to the current sentence, and not bound locally. Discourse level processes will look for antecedents from the History list and from the socalled Topic Hierarchy, our way to compute centering (but see again Delmonte, 2006). This is shown schematically in the output of the Anaphora Resolution module shown here below, which reports the listing of pronouns, Topic Hierarchy, and Anaphora Resolution processes carried out. In this case, every referring expression will have a semantic index (SI) associated which is unique in the History List. In example n.31, here below, the pronominal expressions are two: an Utterance level 
52
possessive pronoun bound to the local SUBJect; and a Discourse level personal pronoun ?He? which receives its antecedent from the History List. In both cases, substitution with their antecedents? head will take place in the semantic interpretation level.     
 T/H pair n. 31 T: Upon receiving his Ph.D., Wetherill became a staff member at Carnegie's Department of Terrestrial Magnetism (DTM) in Washington, D.C. He originated the concept of the Concordia Diagram for the uranium-lead isotopic system. H: Wetherill was the inventor of the concept of the Concordia Diagram.  Clause No. Main Topic Secondary Topic Potential Topics Pronouns + Features Disjoint-ness Pronominal Binding Anaphora  Resolution id_3_1 Text 'Claude Chabrol'  Agnes     id_3_2 Text Main resolved as Claude Chabrol  -  SI=id1 
wife Aurore Paquiss his-[sem:hum, cat:poss, gen:mas, num:sing, pers:3, pred:he, gov_pred:be] 
disj=[sn1-wife] External pronoun (be, his) his  resolved as  'Claude Chabrol'  id_3_3 Hypo-thesis 
Aurore Paquiss  - SI=id4 Claude Chabrol  - SI=id1 
     
Table 2. Output of the Anaphora Resolution Module  4   Results and Discussion Results for the Test set 485 total pronominal expressions amount to 69% accuracy, 92% recall ? this includes computing It-expletives. The F-measure computed is thus 79%. Overall, we evaluated the contribution of the Anaphora Resolution Module as 15% additional correct results. Of course, the impact of using this module would have been different in case all T/H pairs were constituted by long texts. The RTE task is a hard task: in our case 10-15% mistakes are ascribable to the parser or any other analysis tool; another 5-10% mistakes will certainly come from insufficient semantic information.    ACCURACY AVERAGE PRECISION IE 0.5850 0.5992 IR 0.6050 0.5296 QA 0.5900 0.6327 SUMM 0.5700 0.6132 TOTAL 0.5875 0.5830 Table 3. Results for First Run  
References Bos, J., Clark, S., Steedman, M., Curran, J., Hockenmaier, J.: Wide-coverage semantic representations from a ccg parser. In: Proc. of the 20th International Conference on Computational Linguistics. Geneva, Switzerland (2004) Delmonte, R.: Text Understanding with GETARUNS for Q/A and Summarization, Proc. ACL 2004 - 2nd Workshop on Text Meaning & Interpretation, Barcelona, Columbia University (2004) 97-104 Delmonte R., et al Another Evaluation of Anaphora Resolution Algorithms and a Comparison with GETARUNS' Knowledge Rich Approach. In: ROMAND 2006 - 11th EACL. Geneva, (2006) 3-10 Grosz B. and C.  Sidner 1986. Attention, Intentions, and the Structure of Discourse, Computational Linguistics 12 (3), 175-204. Raina, R., et al: Robust Textual Inference using Diverse Knowledge Sources. In: Proc. of the 1st. PASCAL Recognision Textual Entailment Challenge Workshop, Southampton, U.K., (2005) 57-60 Sidner C. 1983. Focusing in the Comprehension of Definite Anaphora, in Brady M., Berwick R.(eds.), Computational Models of Discourse, MIT Press, Cambridge, MA, 267-330.   
53
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 191?194,
Prague, June 2007. c?2007 Association for Computational Linguistics
IRST-BP: Preposition Disambiguation  
 based on  
 Chain Clarifying Relationships Contexts 
 
Octavian Popescu 
FBK-IRST, Trento (Italy) 
popescu@itc.it  
Sara Tonelli 
FBK-IRST, Trento (Italy) 
satonelli@itc.it 
Emanuele Pianta 
FBK-IRST, Trento (Italy) 
pianta@itc.it 
 
 
Abstract 
We are going to present a technique of 
preposition disambiguation based on 
sense discriminative patterns, which are 
acquired using a variant of Angluin?s al-
gorithm. They represent the essential in-
formation extracted from a particular 
type of local contexts we call Chain 
Clarifying Relationship contexts. The 
data set and the results we present are 
from the Semeval task, WSD of Preposi-
tion (Litkowski 2007). 
1 Introduction 
Word Sense Disambiguation (WSD) is a prob-
lem of finding the relevant clues in a surround-
ing context. Context is used with a wide scope in 
the NLP literature. However, there is a dichot-
omy among two types of contexts, local and 
topical contexts (Leacock et. all 1993), that is 
general enough to encompass the whole notion 
and at the same to represent a relevant distinc-
tion. 
The local context is formed by information on 
word order, distance and syntactic structure and 
it is not restricted to open-class words. A topical 
context is formed by the list of those words that 
are likely to co-occur with a particular sense of a 
word. Generally, the WSD methods have a 
marked predilection for topical context, with the 
consequence that structural clues are rarely, if 
ever, taken into account. However, it has been 
suggested (Stetina&Nagao 1997, Dekang 1997) 
that structural words, especially prepositions and 
particles, play an important role in computing 
the lexical preferences considered to be the most 
important clues for disambiguation. 
Closed class words, prepositions in particular, 
are ambiguous (Litkowski&Hargraves2006). 
Their disambiguation is essential for the correct 
processing of the meaning of a whole phrase. A 
wrong PP-attachment may render the sense of 
the whole sentence unintelligible. Consider for 
example: 
 
(1) Joe heard the gossip about you and me. 
(2) Bob rowed about his old car and his 
mother. 
 
A probabilistic context free grammar most 
likely will parse both (1) and (2) wrongly1. It 
would attach ?about? to ?to hear? in (1) and 
would consider the ?his old car and his mother? 
the object of ?about? in (2).  
The information needed for disambiguation of 
open class words is spread at all linguistics lev-
els, from lexicon to pragmatics, and can be lo-
cated within all discourse levels, from immedi-
ate collocation to paragraphs (Stevenson&Wilks 
1999). Intuitively, prepositions have a different 
behavior. Most likely, their senses are deter-
mined within the government category of their 
                                                 
1
 Indeed, Charniak?s parser, considered to be among 
the most accurate ones for English, parses wrongly 
both of them. 
191
heads. We expect the local context to play the 
most important role in the disambiguation of 
prepositions. 
We are going to present a technique of prepo-
sition disambiguation based on sense discrimina-
tive patterns, which are acquired using a variant 
of Angluin?s algorithm. These patterns represent 
the essential information extracted from a par-
ticular type of local contexts we call Chain 
Clarifying Relationship contexts. The data set 
and the results we present are from the Semeval 
task, WSD of Preposition (Litkowski 2007). 
In Section 2 we introduce the Chain Clarify-
ing Relationships, which represent particular 
types of local contexts. In Section 3 we present 
the main ideas of the Angluin algorithm. We 
show in Section 4 how it can be adapted to ac-
commodate the preposition disambiguation task. 
Section 5 is dedicated to further research. 
2 Chain Clarifying Relationships 
We think of ambiguity of natural language as a 
net - like relationship. Under certain circum-
stances, a string of words represents a unique 
collection of senses. If a different sense for one 
of these words is chosen, the result is an un-
grammatical sentence. Consider (3) below: 
 
(3) Most people do not live in a state of 
high intellectual awareness about their 
every action. 
 
Suppose one chooses the sense of ?to live? to 
be ?to populate?. Then, its complement, ?state?, 
should be synonym with location. The analysis 
crashes when ?awareness? is considered. There 
are two things we notice here: (a) the relation-
ship between ?live? and ?state? ? the only two 
acceptable sense combination out of four are 
(populate, location) and (experience, entity) ? 
and (b) the chain like relationship between 
?awareness?, ?state?, ?live? where the sense of 
any of them determines the sense of all the oth-
ers in a cascade effect, or results in ungrammati-
cality. A third thing, not directly observable in 
(3) is that the syntactic configuration is crucial in 
order for (a) and (b) to arise. Example (4) shows 
that in a different syntactic configuration the 
above sense relationship simply disappears: 
 
(4) The awareness of people about the state insti-
tutions is arguably the first condition to live 
in a democratic state. 
 
We call the relationship between ?live?, 
?state?, ?awareness? a Chain Clarifying Rela-
tionship (CCR). In that specific syntactic con-
figuration their senses are interdependent and 
independent of the rest of the sentence. To each 
CCR corresponds a sense discriminative pattern. 
Our goal is to learn which local contexts are 
CCRs. Each CCR is a pattern of words on a syn-
tactic configuration. Each slot can be filled only 
by words defined by certain lexical features. To 
learn a CCR means to discover the syntactic 
configuration and the respective features. For 
example consider (5) and (6) with their CCRs in 
(CCR5) and (CCR6) respectively:  
 
(5) Some people lived in the same state of 
disappointment/ optimism/ happiness. 
 (CCR5) (vb=live_sense_2, prep1=in_1, 
prep1_obj=state_sense_1,prep2=of_sense_1
a,prep2_obj=[State_of_Spirit])  
 (6) Some people lived in the same state of 
Africa/ Latin America/ Asia. 
(CCR6) (vb=live_sense_1, prep1=in_1, 
prep1_obj=state_sense_1,prep2=of_1b,prep
2_obj = [Location]) 
 
The lexical features of the open class words in 
a specific syntactic configuration trigger the 
senses of each word, if the context is a CCR. In 
(CCR5) any word that has the same lexical trait 
as the one required by prep2_obj slot will deter-
mine a unique sense for all the other words, in-
cluding the preposition. The same holds for 
(CCR6). The difference between (CCR5) and 
(CCR6) is part of the linguistic knowledge 
(which can be clearly shown: ?how? (5) vs. 
?where? (6)). 
The CCR approach proposes a deterministic 
approach to WSD. There are two features of 
CCRs which are interesting from a strictly prac-
tical point of view. Firstly, CCR proposal is a 
way to determine the size of the window where 
the disambiguation clues are searched for (many 
WSD algorithms arbitrarily set it apriori). Sec-
ondly, within a CCR, by construction, the sense 
of one word determines the senses of all the oth-
ers. 
192
3 Angluin Learning Algorithm  
Our working hypothesis is that we can learn the 
CCRs contexts by inferring differences via a 
regular language learning algorithm. What we 
want to learn is which features fulfil each syn-
tactic slot. First we introduce the original An-
gluin?s algorithm and then we mention a variant 
of it admitting unspecified values.  
Angluin proved that a regular set can be 
learned in polynomial time by assuming the ex-
istence of an oracle which can gives ?yes/no? 
answers and counterexamples to two types of 
queries: membership queries and conjecture que-
ries (queries about the form of the regular lan-
guage) (Angluin 1998). 
The algorithm employs an observation table 
built on prefix /suffix closed classes. To each 
word a {1, 0} value is associated, ?1? meaning 
that the word belongs to the target regular lan-
guage. Initially the table is empty and is filled 
incrementally. The table is closed if all prefixes 
of the already seen examples are in the table and 
is consistent if two rows dominated by the same 
prefix have the same value, ?0? or ?1?. 
If the table is not consistent or closed then a 
set of membership queries is made. If the table is 
consistent and closed then a conjecture query is 
made. If the oracle responds ?no?, it has to pro-
vide a counterexample and the previous steps are 
cycled till ?yes? is obtained. 
The role of the oracle for conjecture questions 
can be substituted by a stochastic process. If 
strict equality is not requested, then a probably 
approximately correct identification of language 
can be obtained (PAC identification), which 
guarantees that the two languages (the identified 
one, Li, and the target one, Lt) are equal up to a 
certain extent. The approximation is constrained 
by two parameters ? ? accuracy and ? ? confi-
dence, and the constraint is P(d(Li, Lt) ? ?) ? ?), 
where the distance between two languages is the 
probability to see a word in just one of them. 
The algorithm can be further generalized to 
work with unspecified values. The examples 
may have three values (?yes?, ?no?, ???), as in 
many domains one has to deal with partial 
knowledge The main result is that a variant of 
the above algorithm successfully halts if the 
number of counterexamples provided by the ora-
cle have O(log n) missing attributes, where n is 
the number of attributes (Goldmann et al 2003). 
4 Preposition Disambiguation Task 
The CCR extraction algorithm is supervised. 
Consider that you have a sense annotated cor-
pora. Extract the dependency paths and filter out 
the ones which are not sense discriminative. Try 
to generalize each slot and retain the minimal 
ones. What is left are CCRs. 
Unfortunately, for the preposition disam-
biguation task the training set is sense annotated 
only for prepositions. We have undertaken a dif-
ferent strategy. The training corpus can be used 
as an oracle. The main idea is to start with a set 
of few examples for each sense from the training 
set which are considered to be the most repre-
sentative ones. We try to generalize each of 
them independently and to tackle down the bor-
der cases (the cases that may correspond to two 
different senses) which are considered unspeci-
fied examples. The process stops when the ora-
cle does not bring any new information (the 
training cases have been learned). Below we 
explain this process step by step. 
Step 1. Get the seed examples. For each 
preposition and sense get the seed examples. 
This operation is performed by a human expert. 
It may be the case that the glosses or the diction-
ary definition are a good starting point (with the 
advantage that the intervention of a human is no 
more required). However, we preferred do to it 
manually for better precision. 
Besides the most frequent sense, we have con-
sidered, in average, another two senses. There is 
a practical reason for this limitation: the number 
of examples for the rest of the senses is insuffi-
cient. In total we have considered 149 senses out 
of the 241 senses present in the training set. For 
each an average of three examples has been cho-
sen. 
Step 2. Get the CCRs. For each example we 
read the lex units associated with its frame from 
FrameNet. Our goal is to identify the relevant 
syntactic and lexical features associated with 
each slot. We have undertaken two simplifying 
assumptions. Firstly, only the government cate-
gory of the head of the PP is considered (which 
can be a verb, a noun or an adjective). Secondly, 
193
the lexical features are identified with synsets 
from WordNet.  
We have used the Charniak?s parser to extract 
the structure of the PP-phrases and further we 
have used Collin?s algorithm to implement a 
head recogniser. 
A head can have many synsets. In order to 
understand which sense the word has in the re-
spective construction we look for the synset 
common to the elements extracted from lex. If 
the proposed synset uniquely identifies just one 
sense then it is considered a CCR. If not, we are 
looking for the next synset. This step corre-
sponds to membership queries in Angluin?s al-
gorithm. 
Step 3. Generalize the CCRs. At the end of 
step 2 we have a set of CCRs for each sense. We 
obtained 395 initial CCRs. We tried to extend 
the coverage by taking into account the hypero-
nyms of each synsets. Only approximately 10% 
of these new patterns have received an answer 
from the oracle. Consequently, for our ap-
proach ,a part of the training corpus has not been 
used. It serves only 15 examples in average to 
get a correct CCR. All the instances of the same 
CCR do not bring any new information to our 
approach. 
Posteriori, we have noticed that the initial pat-
terns have an almost 50% (48.57%) coverage in 
the test data. The generalized patterns obtained 
after the third step have 82% test corpus cover-
age. For the rest 18%, which are totally un-
known cases, we have chosen the most frequent 
sense. 
In table 1 we present the performances of our 
system. It achieves 0.65 (FF-score), which com-
pares favourably against baseline ? the most fre-
quent -of 0.53. On the first column of Table 1 
we write the FF score interval - more than 0.75, 
between 0.75 and 0.5, and less than 0.5 respec-
tively, - on the second column we present the 
number of cases within that interval the system 
solved and on the third column we include the 
corresponding number for baseline. 
Table 1 
 
Interval System Baseline 
1.00 - 0.75 18 8 
0.75 - 0.50 15 6 
0.00 ? 0.50 2 20 
5 Conclusion and Further Research 
Our system did not perform very well (third po-
sition out of three). Analyzing the errors, we 
have noticed that our system systematically con-
found two senses in some cases (for example 
?by? 5(2) vs. 15(3), for ?on? 4(1c) vs. 1(1) etc.). 
We would like to see whether these errors are 
due to a misclassification in training. 
 
References 
 
Angluin, D. (1987): ?Learning Regular Sets 
from Queries and Counterexamples?, Infor-
mation and Computation Volume 75 ,  Issue 2 
Goldman, S., Kwek, S., Scott, S. (2003): ?Learn-
ing from examples with unspecified attribute 
values?, Information and Computation, Vol-
ume 180 
Leacock, C., Towell, G., Voorhes, E. (1993): 
?Towards Building Contextual Representa-
tions of Word Senses Using Statistical Mod-
els?, In Proceedings, SIGLEX workshop: Ac-
quisition of Lexical Knowledge from Text 
Lin, D. (1997): ?Using syntactic dependency as 
local context to resolve word sense ambigu-
ity?.ACL/EACL-97,  Madrid 
Litkowski, K. C. (2007):?Word Sense Disam-
biguation of Prepositions? , The Semeval 
2007 WePS Track. In Proceedings of Semeval 
2007, ACL 
Litkowski, K. C., Hargraves O. (2006): ?Cover-
age and Inheritance in the Preposition Project", 
Proceedings of the Third ACL-SIGSEM 
Workshop on Prepositions, Trento, 
Stetina J, Nagao M (1997): ?Corpus based PP 
attachment ambiguity resolution with a se-
mantic dictionary.?, Proc. of the 5th Work-
shop on very large corpora, Beijing and 
Hongkong, pp 66-80 
Stevenson K., Wilks, Y.,(2001): ?The interaction 
of knowledge sources in word sense disam-
biguation?, Computational Linguistics, 
27(3):321?349. 
 
194
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2097?2106, Dublin, Ireland, August 23-29 2014.
An Analysis of Causality between Events and
its Relation to Temporal Information
Paramita Mirza
Fondazione Bruno Kessler,
University of Trento
Trento, Italy
paramita@fbk.eu
Sara Tonelli
Fondazione Bruno Kessler
Trento, Italy
satonelli@fbk.eu
Abstract
In this work we present an annotation framework to capture causality between events, inspired
by TimeML, and a language resource covering both temporal and causal relations. This data set
is then used to build an automatic extraction system for causal signals and causal links between
given event pairs. The evaluation and analysis of the system?s performance provides an insight
into explicit causality in text and the connection between temporal and causal relations.
1 Introduction
Causality is a concept that has been widely investigated from a philosophical, psychological and logical
point of view, but how to model its recognition and representation in NLP-centered applications is
still an open issue. However, information on causality could be beneficial to a number of natural
language processing tasks such as question answering, text summarization, decision support, etc. The
lack of information extraction systems focused on causality may depend also on the lack of unified
annotation guidelines and standard benchmarks, which usually foster the comparison of different systems
performances. Specific phenomena related to causality, such as causal arguments (Bonial et al., 2010),
causal discourse relations (The PDTB Research Group, 2008) or causal relations between nominals (Girju
et al., 2007), have been investigated, but no unified framework has been proposed to capture causal
relations between events, as opposed to the existing TimeML standard for temporal relations (Pustejovsky
et al., 2010).
The work presented in this paper copes with this issue by i) proposing an annotation framework to
model causal relations between events and ii) detailing the development and the evaluation of a supervised
system based on such framework.
We take advantage of the formalization work carried out for the TimeML standard, in which events,
temporal relations and temporal signals have been carefully defined and annotated. We propose to model
causal relations in a similar way to temporal relations, inheriting from TimeML the notion of event,
relation and signal, even though our approach to causality is well rooted in the force dynamic model by
Talmy (1985).
Besides, we focus our preliminary annotation on TimeBank (Pustejovsky et al., 2006), a corpus widely
used by the research community working on temporal processing. This should possibly enable the
adaptation of existing temporal processing systems to the analysis of causal information, given that we
rely on well-known standards and data. On the other hand, this makes it easier for us to straightforwardly
investigate the relation between temporal and causal information, given that a causing event should always
take place before a resulting event.
2 Related Work
Research on the extraction of event relations has concerned both the analysis of the temporal ordering
of events and the recognition of causality relations. However, the two research lines have progressed
quite independently from each other. Recent works on temporal relations mostly revolve around the last
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2097
TempEval-3
1
shared task on temporal and event processing. The task organizers released some data sets
annotated with events, time expressions and temporal relations in TimeML format (Pustejovsky et al.,
2003), mainly used for training and evaluation purposes. The results of TempEval-3 reported by UzZaman
et al. (2013) show that, even though the performance of systems for extracting TimeML events and
time expressions is quite good (>80% F-score), the overall performance of end-to-end event extraction
pipelines is negatively affected by the poor performance of modules for temporal relation extraction. In
fact, the state-of-the-art performance on the temporal relation extraction task yields only around 36%
F-score (Bethard, 2013).
The problem of detecting causality between events is as challenging as recognizing their temporal
order, but less analyzed from an NLP perspective. Besides, it has mostly focused on specific types of
event pairs and causal expressions in text, and has failed to provide a global account of causal phenomena
that can be captured with NLP techniques. SemEval-2007 Task 4 ?Classification of Semantic Relations
between Nominals? (Girju et al., 2007) gives access to a corpus containing nominal causal relations among
others, as causality is one of the considered semantic relations in the task. Bethard et al. (2008) collected
1,000 conjoined event pairs connected by and from the Wall Street Journal corpus. The event pairs
were annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSE,
NO-REL). They use 697 event pairs to train a classification model for causal relations, and use the rest
for evaluating the system, which results in 37.4% F-score. Rink et al. (2010) perform textual graph
classification using the same corpus, and make use of manually annotated temporal relation types as a
feature to build a classification model for causal relations between events. This results in 57.9% F-score,
15% improvement in performance compared with the system without the additional feature of temporal
relations.
The interaction between temporal and causal information, and the contribution of temporal information
to the identification of causal links, are also one of the issues investigated in this paper. However, we aim
at providing a more comprehensive account of how causal relations can be explicitly expressed in a text,
and we do not limit our analysis to specific connectives.
Do et al. (2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowing
the detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs. Causality
between event pairs is measured by taking into account Point-wise Mutual Information (PMI) between
the cause and the effect. They also incorporate discourse information, specifically the connective types
extracted from the Penn Discourse TreeBank (PDTB), and achieve a performance of 46.9% F-score.
Unfortunately, the data set is not freely available, hence, comparing our work with theirs is not possible.
The most recent work of Riaz and Girju (2013) focuses on the identification of causal relations between
verbal events. They rely on the unambiguous discourse markers because and but to automatically collect
training instances of cause and non-cause event pairs, respectively. The result is a knowledge base of
causal associations of verbs, which contains three classes of verb pairs: strongly causal, ambiguous and
strongly non-causal.
The lack of a standard benchmark to evaluate systems for the extraction of causal relations between
events makes it difficult to compare the performance of different systems, and to identify the state-of-the-
art approach to this particular task. For this reason, we annotated TimeBank, a freely available corpus,
with the aim of making it available to the research community for further evaluations.
3 Data annotation
In order to develop a classifier for the detection of causal relations between events, we first define
annotation guidelines for explicit causality and then manually annotate a data set for training and testing.
3.1 Annotation scheme
Since one of the goals of this work is to investigate the interaction between temporal and causal information,
we define an annotation scheme strongly inspired by the TimeML standard for events, time expressions
and temporal relations. First, we inherit from TimeML the definition of events, which includes all types
1
http://www.cs.york.ac.uk/semeval-2013/task1/
2098
of actions (punctual and durative) and states. Hence, we do not limit our annotation only to specific PoS
such as verbal or nominal events.
Similar to the <TLINK> tag in TimeML for temporal relations, we introduce the <CLINK> tag to
mark a causal relation between two events. Both TLINKs and CLINKs mark directional relations, i.e.
they involve a source and a target event. However, while a list of relation types is part of the attributes for
TLINKs (e.g. BEFORE, AFTER, INCLUDES, etc.), for CLINKs only one relation type is foreseen, going
from a source (the cause, indicated with
S
in the examples) to a target (the effect, indicated with
T
).
We also introduce the notion of causal signals through the <C-SIGNAL> tag. <SIGNAL>s have
been introduced in TimeML to annotate temporal prepositions and other temporal connectives and
subordinators. If a SIGNAL marks the presence of a temporal relation in a text, its ID is added to the
attributes of such TLINK. In a similar way, C-SIGNALs are used to mark-up textual elements signalling
the presence of causal relations, which include all causal uses of prepositions (e.g. because of, as a result
of, due to), conjunctions (e.g. because, since, so that), adverbial connectors (e.g. so, therefore, thus)
and clause-integrated expressions (e.g. the reason why, the result is, that is why). Also for CLINKs it
is possible to assign a c-signalID attribute, in case a C-SIGNAL marks the causal relation between two
events in text.
Concerning the notion of causality, it is particularly challenging to provide guidelines that clearly
define how to identify it in text, since causality exists as a psychological tool for understanding the world
independently of language and it is not necessarily grounded in text (van de Koot and Neeleman, 2012).
There have been several attempts in the psychology field to model causality, including the counterfactual
model (Lewis, 1973), the probabilistic contrast model (Cheng and Novick, 1991; Cheng and Novick,
1992) and the dynamics model (Wolff and Song, 2003; Wolff et al., 2005; Wolff, 2007), which is based on
Talmy?s force dynamic account of causality (Talmy, 1985; Talmy, 1988). We choose to lean our guidelines
on the latter model, since it accounts also for different ways in which causal concepts are lexicalized.
Specifically, Wolff (2007) claims that causation covers three main types of causal concepts, i.e. CAUSE,
ENABLE and PREVENT. These causal concepts are lexicalized through three types of verbs listed in
Wolff and Song (2003): i) CAUSE-type verbs, e.g. cause, prompt, force; ii) ENABLE-type verbs, e.g.
allow, enable, help; and iii) PREVENT-type verbs, e.g. block, prevent, restrain. These categories of
causation and the corresponding verbs are taken into account in our guidelines (Tonelli et al., 2014).
We assign a CLINK if, given two annotated events, there is an explicit causal construction linking them.
Such construction can be expressed in one of the following ways:
1. Expressions containing affect verbs (affect, influence, determine, change, etc.), e.g. Ogun ACN
crisis
S
influences the launch
T
of the All Progressive Congress.
2. Expressions containing link verbs (link, lead, depend on, etc.), e.g. An earthquake
T
in North
America was linked to a tsunami
S
in Japan.
3. Basic constructions involving causative verbs of CAUSE, ENABLE and PREVENT type, e.g. The
purchase
S
caused the creation
T
of the current building.
4. Periphrastic constructions involving causative verbs of CAUSE, ENABLE and PREVENT type,
e.g. The blast
S
caused the boat to heel
T
violently. With ?periphrastic? we mean constructions
where a causative verb (caused) takes an embedded clause or predicate as a complement expressing
a particular result (heel).
5. Expressions containing CSIGNALs, e.g. Its shipments declined
T
as a result of a reduction
S
in
inventories by service centers.
We annotate both intra- and inter-sentential causal relations between events, provided that one of
the above constructions is present. We do not annotate causal relations that are implicit and must be
inferred by annotators, because they may be highly ambiguous and would probably affect inter-annotator
agreement.
2099
3.2 Corpus statistics
Based on the guidelines above, we manually annotated causality in the TimeBank corpus taken from
TempEval-3, containing 183 documents with 6,811 annotated events in total.
2
We chose this corpus
because gold events were already present, between which we could add causal links. Besides, one of our
research goals is the analysis of the interaction between temporal and causal information, and TimeBank
already presents full manual annotation of temporal information according to TimeML standard.
However, during annotation, we noticed that some events involved in causal relations were not annotated,
probably because the corpus was originally built focusing on events involved in temporal relations.
Therefore, we annotated also 137 new events, which led to around 56% increase in the number of
annotated CLINKs.
The total number of annotated CSIGNALs is 171 and there are 318 CLINKs, much less than the number
of TLINKs found in the corpus, which is 5,118. Besides, not all documents contain causality relations
between events. From the total number of documents in TimeBank, only 109 (around 60%) of them
contain explicit causal links and only 87 (around 47%) of them contain CSIGNALs. We also found that
there is no temporal signal (marked by <SIGNAL> tag) annotated in TimeBank, which is unfortunate
since it could help in disambiguating causal signals from temporal signals.
Annotation was performed using the CAT tool (Bartalesi Lenzi et al., 2012), a web-based application
with a plugin to import annotated data in TimeML and add information on top of it. The agreement reached
by two annotators on a subset of 5 documents is 0.844 Dice?s coefficient on C-SIGNALs (micro-average
over markables) and of 0.73 on CLINKs. The built corpus is then used as training and test data in the
experiments for the classification of CSIGNALs and CLINKs, as described in Section 4. This preliminary
analysis on the corpus, however, shows that explicit causal relations between events are less frequently
found in texts than temporal ones. This may lead to data sparseness problems.
4 Experiments
Using the 183 documents from TimeBank manually enriched with causal information for training and
testing, we implement two different classifiers: the first one is a CSIGNAL labeler, that takes in input
information on events and temporal expressions as annotated in the original TimeBank, and classifies
whether a token is part of a causal signal or not (Section 4.1). The second one is a CLINK classifier,
which given an event pair detects whether they are connected by an explicit causal link (Section 4.2). Both
experiments are carried out based on five-fold cross-validation. The overall approach is largely inspired
by our existing framework for the classification of temporal relations (Mirza and Tonelli, 2014).
4.1 Automatic Extraction of CSIGNALs
The task of recognizing CSIGNALs can be seen as a text chunking task, i.e. using a classifier to determine
whether a token is part of a causal signal or not. Since the extent of causal signals can be expressed by
multi-word expressions, we employ the IOB tagging convention to annotate the data, where each token
can either be classified into B-CSIGNAL, I-CSIGNAL or O (for other). We build our classification model
using the Support Vector Machine (SVM) implementation provided by YamCha
3
, a generic, customizable,
and open source text chunker. In order to provide the classifier a feature vector to learn from, we perform
the two following steps:
1. Run the TextPro tool (Pianta et al., 2008) to get information on base NP chunking and whether a
token is part of named entity or not.
2. Run Stanford CoreNLP tool
4
to get information on lemma, part-of-speech (PoS) tags and dependency
relations between tokens.
In the end, the feature vector includes token, lemma, PoS tag, NP chunking, dependency path, and
several binary features, indicating whether a token is: i) an event or part of a temporal expression,
2
The annotated data set is available at http://hlt.fbk.eu/technologies/causal-timebank
3
http://chasen.org/?taku/software/yamcha/
4
http://nlp.stanford.edu/software/corenlp.shtml
2100
according to gold TimeML annotation; ii) part of a named entity or not; and iii) part of a specific discourse
connective type.
Dependency information is encoded as the dependency path between the current token and its governor.
For example, in ?He fell because the chair was broken?, there is a dependency relation mark (broken,
because), where mark indicates the presence of a finite clause subordinate to another clause (de Marneffe
and Manning, 2008). Thus, we encode the dependency feature for the token because as mark (broken). If
the governor is an event, e.g. broken is annotated as an event, the dependency feature is represented as
mark (EVENT) instead.
The mentioned binary features are introduced to exclude the corresponding token as a candidate token
for a causal signal. In other words, if a token is part of a named entity or an event, it is very unlikely that it
will be part of a causal signal. The same holds for all connective types that do not express causal relations,
e.g. temporal or concessive ones. In order to obtain this information, we include in the feature vector the
information about discourse connectives acquired using the addDiscourse tool (Pitler and Nenkova, 2009),
which identifies connectives and assigns them to one of four semantic classes in the framework of the
Penn Discourse Treebank (The PDTB Research Group, 2008): TEMPORAL, EXPANSION, CONTINGENCY
and COMPARISON. Note that causality is part of the CONTINGENCY class.
System Precision Recall F-score
Rule-based (baseline) 54.33% 40.35% 46.31%
Supervised chunking 91.03% 41.76% 57.26%
Table 1: Evaluation of CSIGNAL extraction system
Table 1 shows the performance of our classification model in a five-fold cross-validation setting, which
yields a good precision but a poorer recall, summing up into 57.26% F-score. We also compare our
supervised model with a baseline rule-based system, which labels as CSIGNALs all causal connectors
listed in our annotation guidelines and those appearing in specific syntactic constructions. For instance,
from and by are always labeled as CSIGNAL when they are governed by a passive verb annotated as
event and govern another event, as in the sentence ?The building was damaged
T
by the earthquake
S
.?
Note that this is quite a strong baseline, since the rule-based algorithm embeds some of the intuitions on
syntactic dependencies expressed also as features in the supervised approach.
4.2 Automatic Extraction of CLINKs
Similar to causal signal extraction, we approach the problem of detecting causal links between events as a
supervised classification task. Given an ordered pair or events (e
1
,e
2
), the classifier has to decide whether
there is a causal relation between them or not. However, since we also consider the directionality of the
causal link, an event pair (e
1
,e
2
) is classified into 3 classes: CLINK (where e
1
is the source and e
2
is the
target), CLINK-R (with the reverse order or source and target) or NO-REL. Again, we use YamCha to build
the classifier. This time, a feature vector is built for each pair of events and not for each token as in the
previous classification task.
As candidate event pairs, we take into account every possible combination of events in a sentence
in a forward manner. For example, if we have e
1
, e
2
and e
3
in a sentence (in this order), the candidate
event pairs are (e
1
,e
2
), (e
1
,e
3
) and (e
2
,e
3
). We also include as candidate event pairs the combination
of each event in a sentence with events in the following one. This is necessary to account for inter-
sentential causality, under the simplifying assumption that causality may occur only between events in
two consecutive sentences.
We implement a number of features, some of which are computed independently based on either e
1
or
e
2
, e.g. lemma, PoS, while some others are pairwise features, which are computed based on both elements,
e.g. dependency path, signals in between, etc. The implemented features are as follows:
String and grammatical features. The tokens and lemmas of e
1
and e
2
, along with their PoS and a
binary feature indicating whether e
1
and e
2
have the same PoS tags.
Textual context. The sentence distance and event distance of e
1
and e
2
. Sentence distance measures
2101
how far e
1
and e
2
are from each other in terms of sentences, i.e. 0 if they are in the same sentence. The
event distance corresponds to the number of events occurring between e
1
and e
2
(i.e. if they are adjacent,
the distance is 0).
Event attributes. Event attributes as specified in TimeML annotation, which consist of class, tense,
aspect and polarity. Events being a noun, adjective and preposition do not have tense and aspect attributes
in TimeML. Therefore, we retrieve this information by extracting the tense and aspect of the verbs that
govern them, based on their dependency relation. We also include four binary features representing
whether e
1
and e
2
have the same event attributes or not. These features, especially the tense and aspect
one, are very relevant for detecting causality. For instance, if e
1
is in the future tense and e
2
in the past
tense, there cannot be a causal relation connecting e
1
(as source) and e
2
(as target or result).
Dependency information. We include as features i) the dependency path that exists between e
1
and
e
2
, ii) the type of causative verb connecting them (if any) and iii) binary features indicating whether
e
1
/e
2
is the root of the sentence. This information is based on the collapsed representation of dependency
relations provided by the parsing module of Stanford CoreNLP. Consider the sentence ?Profit from coal
fell
T
to $41 million from $58 million, partly because of a miners? strike
S
.? Based on the collapsed
typed dependencies, we would obtain a direct relation between fell and strike, which is prep because of
(fell, strike). This information combined with the classification of because of as a causal signal would
straightforwardly identify the relation connecting the two events as causal.
Causal signals. We take into account the annotated CSIGNALs connecting two candidate events. We
look for causal signals occurring between e
1
and e
2
, or before e
1
. We also include the position of the
signals (between or before) as feature, since it is crucial to determine the direction of the causality of
a given ordered event pair. This is particularly evident if you consider the position of causal signals
in the following examples: i) ?The building collapsed
T
because of the earthquake
S
? vs. ii) ?Because
of the earthquake
S
the building collapsed
T
.? This feature is also very relevant in connection with the
Textual context, since two events being in two different sentences are linked by an explicit causal relation
only in specific cases, for instance if there is a CSIGNAL in between, typically at the beginning of the
second sentence. Note that in case of several CSIGNALs occurring between e
1
and e
2
, we take the
closest CSIGNAL to e
2
, as in the sentence ?The building was damaged
S
by the earthquake
,
thus, people
moved
T
away?. The dependency path between the causal signal and e
1
/e
2
is also important to determine
the correct involved events in the causal relations. For instance, in the sentence ?They decided
T
to move
because of the earthquake
S
?, the involved event is decided instead of move.
Temporal relations (TLINKs). Rink et al. (2010) showed that including temporal relation information
in detecting causal links results in improving classification performance. Nevertheless, they only analyze
this phenomenon when causality is expressed by the conjunction and. We decided to include this
information in the feature set by specifying the temporal relation type connecting e
1
and e
2
, if any, to see
whether TLINKs help in improving causality detection also in a more comprehensive setting.
We evaluate our approach in a five-fold cross-validation setting, and we compare the performance of
our classifier with a baseline rule-based system. This relies on an algorithm that, given a term t belonging
to affect, link, causative verbs (basic and periphrastic constructions) or causal signals (as listed in the
annotation guidelines), looks for specific dependency constructions where t is connected to two events. If
such dependencies are found, a CLINK is automatically set between the two events identifying the source
and the target of the relation. Further details on the baseline system and its evaluation can be found in
Mirza et al. (2014).
In our experimental setting, we evaluate two versions of the CLINK classifier: the first includes as
features the gold annotated CSIGNALs in the classification model, while the second takes in input
the CSIGNALs automatically annotated by the classifier described in Section 4.1. We also evaluate
the contribution of dependency, CSIGNAL and TLINK features by excluding each of them from the
classification model.
Evaluation results are reported in Table 2. We observe that the baseline is always outperformed by the
other classifiers. CSIGNAL is the most important feature, with a particularly high impact on recall. The
2102
intuition behind this result is that, if a CSIGNAL is present, it is a strong indicator of a causal relation being
present in the surrounding context. This is similar to what Derczynski and Gaizauskas (2012) report for
temporal information, showing that temporal signals provide useful information in TLINK classification.
Dependency information contributes to the performance of the classifier, but is less relevant than TLINK
information. A more detailed analysis of the relation between temporal and causal information is reported
in the following section. The significantly decreasing recall of the classifier using the automatic extracted
CSIGNALs as features is most probably caused by the low recall of the CSIGNAL extraction system.
System Precision Recall F-score
Rule-based (baseline) 36.79% 12.26% 18.40%
Supervised classification (with gold CSIGNALs) 74.67% 35.22% 47.86%
- without dependency feature 65.77% 30.82% 41.97%
- without CSIGNAL feature 57.53% 13.21% 21.48%
- without TLINK feature 61.59% 29.25% 39.66%
Supervised classification (with automatic CSIGNALs) 67.29% 22.64% 33.88%
Table 2: Performance of CLINK extraction system
5 Discussion
We further analyse the output of the automatic extraction systems, in order to understand some phenomena
triggering the results.
5.1 Recognizing CSIGNALs
When we manually inspect the output of the CSIGNAL extraction system, we find that the false positives
are actually the causal signals that annotators missed in the corpus, and not ambiguous connectives. The
system surprisingly yields better precision than human annotation, finding new correct signals.
The recall, however, suffers most probably from data sparseness. It is possible that during the cross-
validation experiments some splits do not have enough data to learn from, recalling that only around 47%
of the documents contain annotated CSIGNALs. Furthermore, 20% of the false negative cases are due to
classifier?s mistakes in detecting the causal signal by, which is highly ambiguous. Our assumption with
the rule-based system that ?by is likely to be a causal signal when it is used to modify a passive verb? is
too restrictive, since by can convey a causal meaning even if the target event is not in the passive voice, as
in the example ?The embargo is meant to cripple
T
Iraq by cutting
S
off its exports of oil and imports of
food and military supplies.?
Another ambiguous causal signal that the classifier fails to detect is the conjunction and. We believe
that more training data, and perhaps more lexical information on the tokens connected by the conjunction
and, are needed for the classifier to be able to disambiguate them.
5.2 Detecting CLINKs
We found that most of the mistakes done by the classifier, as well as by the rule-based system, are caused
by the dependency parser output that tends to establish a dependency relation between a causative verb or
causal signal and the closest verb. For example, in the sentence ?StatesWest Airlines withdrew
T
its offer
to acquire Mesa Airlines because the Farmington carrier did not respond
S
to its offer?, the dependency
parser identify because as the mark of acquire instead of withdrew.
Moreover, also for this task data sparseness is definitely an issue. One possible solution would be to
annotate more data, for instance the AQUAINT data set used for TempEval-3 competition (UzZaman et
al., 2013). Another possibility would be to automatically generate additional data from the Penn Discourse
TreeBank corpus, where causality is one of the discourse relations annotated between argument pairs.
However, a further processing step would be needed to identify inside the argument spans the events
between which a relation holds, which may introduce some errors.
2103
Regarding the directionality of causal relations, the classifier is generally quite precise. 112 out of 150
CLINKs detected by the classifier actually match a causal relation present in the gold annotated data.
Only 8 of them have been classified with the wrong direction. We believe that using the TLINK types as
features contributes to this good performance in disambiguating causality direction (CLINK vs. CLINK-R).
5.3 Interaction between temporal and causal information
We provide in Table 3 some statistics on the overlaps between causal links and temporal relation types
from the gold data. The Others class in the table includes SIMULTANEOUS, IS INCLUDED, BEGUN BY
and DURING INV relations. These counts were obtained by overlapping the temporal information in
TimeBank with the causal information manually added for our experiments. In total, only 32% of the gold
causal links have the underlying temporal relations. Note that the annotators could not see the temporal
links already present in the data, therefore they were not biased by TLINKs when assessing causal links.
BEFORE AFTER IBEFORE IAFTER Others Total
CLINK 15 5 0 0 4 24
CLINK-R 1 67 0 3 8 79
Table 3: Statistics of CLINKs overlapping with TLINKs
The data confirm our intuition that temporal information is a strong constraint when detecting causal
relations, with the BEFORE class having the most overlaps with CLINK and AFTER with CLINK-R. This
is in line with the outcome of our feature analysis reported in Table 2, suggesting that feeding temporal
information into a causal relation classifier yields an improvement in performance. However, the converse
would be less effective, since the occurrences of explicit causal relations are by far less frequent than
temporal ones. Besides, we found that the few cases where CLINKs overlap with AFTER relation are not
due to annotation mistakes, as in the example ?But some analysts questioned
T
how much of an impact
the retirement package will have, because few jobs will end
S
up being eliminated.?
Finally, the performance achieved by our system in causal relation extraction (with gold C-SIGNALs)
is 47.86% F-score, which is better than the performance of the state-of-the-art temporal relation extraction
system with 36.26% (Bethard, 2013). This probably depends on the fact that extracting CLINKs is a
simpler task compared with TLINK extraction: in the first case 3 classes are considered, while temporal
relation types are classified into 14 classes.
6 Conclusions
In this paper, we presented a framework for annotating causal signals and causal relations between events.
Besides, we implemented and evaluated two supervised systems, one classifying C-SIGNALs and the
other CLINKs.
With the first task, we showed that while recognizing unambiguous causal signals is very trivial,
ambiguous signals such as by and and are very difficult to identify because they occur in diverse syntactic
constructions. We definitely need more data to learn from, and perhaps use more lexical information
on the words connected by such causal signals as features. The knowledge base of causal associations
between verbs developed by Riaz and Girju (2013) may be a useful resource to provide such information,
and we will explore this possibility in the future.
We found that the low recall achieved by the CLINK classifier is probably affected by wrong dependen-
cies identified by the Stanford parser. In the future, we would like to test also the C&C tool (Curran et al.,
2007) to extract dependency relations, since it has a better coverage of long-range dependencies. We have
also shown that causal signals are very important in detecting explicit causal links holding between two
events. Finally, we showed that temporal relation types help in disambiguating the direction of causality,
i.e. to determine the source and target event. However, the converse may not hold, since the causal links
in the data set are very sparse, and only 2% of the total TLINKs overlap with CLINKs.
2104
Acknowledgements
The research leading to this paper was partially supported by the European Union?s 7th Framework
Programme via the NewsReader Project (ICT-316404). We also thank Rachele Sprugnoli and Manuela
Speranza for their contribution in defining the annotation guidelines.
References
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele Sprugnoli. 2012. CAT: the CELCT Annotation Tool. In
Proceedings of LREC 2012.
Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a Corpus of Temporal-
Causal Structure. In European Language Resources Association (ELRA), editor, Proceedings of the Sixth Inter-
national Language Resources and Evaluation (LREC?08), Marrakech, Morocco, may.
Steven Bethard. 2013. ClearTK-TimeML: A minimalist approach to TempEval 2013. In Proceedings of the Sev-
enth International Workshop on Semantic Evaluation, SemEval ?13, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Claire Bonial, Olga Babko-Malaya, Jinho D. Choi, Jena Hwang, and Martha Palmer. 2010. PropBank An-
notation Guidelines, Version 3.0. Technical report, Center for Computational Language and Education Re-
search, Institute of Cognitive Science, University of Colorado at Boulder. http://clear.colorado.
edu/compsem/documents/propbank_guidelines.pdf.
Patricia W. Cheng and Laura R. Novick. 1991. Causes versus enabling conditions. Cognition, 40(1-2):83 ? 120.
Patricia W. Cheng and Laura R. Novick. 1992. Covariation in natural causal induction. Psychological Review,
99(2):365?382.
James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically Motivated Large-Scale NLP with C&C and
Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions, pages 33?36, Prague, Czech Republic, June. Association
for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages
1?8. Association for Computational Linguistics.
Leon Derczynski and Robert J. Gaizauskas. 2012. Using Signals to Improve Automatic Classification of Temporal
Relations. CoRR, abs/1203.5055.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011. Minimally Supervised Event Causality Identification.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 294?303,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. SemEval-
2007 Task 04: Classification of Semantic Relations between Nominals. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-2007), pages 13?18, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
David Lewis. 1973. Causation. The Journal of Philosophy, 70(17):pp. 556?567.
Paramita Mirza and Sara Tonelli. 2014. Classifying Temporal Relations with Simple Features. In Proceedings of
the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 308?317,
Gothenburg, Sweden, April. Association for Computational Linguistics.
Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating Causality in the
TempEval-3 Corpus. In Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality
in Language (CAtoCL), pages 10?19, Gothenburg, Sweden, April. Association for Computational Linguistics.
Emanuele Pianta, Christian Girardi, and Roberto Zanoli. 2008. The TextPro Tool Suite. In Proceedings of
the 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco.
European Language Resources Association.
2105
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ?09, pages 13?16, Stroudsburg, PA,
USA. Association for Computational Linguistics.
James Pustejovsky, Jos?e Casta?no, Robert Ingria, Roser Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust specification of event and temporal expressions in text. In Proceedings of the
Fifth International Workshop on Computational Semantics (IWCS-5).
James Pustejovsky, Jessica Littman, Roser Saur??, and Marc Verhagen. 2006. Timebank 1.2 documentation. Tech-
nical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent Romary. 2010. ISO-TimeML: An international stan-
dard for semantic annotation. In Proceedings o the Fifth International Workshop on Interoperable Semantic
Annotation.
Mehwish Riaz and Roxana Girju. 2013. Toward a better understanding of causality between verbal events: Ex-
traction and analysis of the causal power of verb-verb associations. In Proceedings of the SIGDIAL 2013
Conference, pages 21?30, Metz, France, August. Association for Computational Linguistics.
Bryan Rink, Cosmin Adrian Bejan, and Sanda M. Harabagiu. 2010. Learning Textual Graph Patterns to Detect
Causal Event Relations. In Proceedings of the Twenty-Third International FLAIRS Conference.
Leonard Talmy. 1985. Force dynamics in language and thought. Chicago Linguistic Society, 21:293?337.
Leonard Talmy. 1988. Force dynamics in language and cognition. Cognitive science, 12(1):49?100.
The PDTB Research Group. 2008. The PDTB 2.0. Annotation Manual. Technical Report IRCS-08-01, Institute
for Research in Cognitive Science, University of Pennsylvania.
Sara Tonelli, Rachele Sprugnoli, and Manuela Speranza. 2014. Newsreader guidelines for annotation
at document level. Technical Report NWR-2014-2, Fondazione Bruno Kessler. http://www.newsreader-
project.eu/files/2013/01/NWR-2014-2.pdf.
Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013.
Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Proceedings
of the Seventh International Workshop on Semantic Evaluation, SemEval ?13, pages 1?9, Atlanta, Georgia,
USA, June. Association for Computational Linguistics.
H. van de Koot and A. Neeleman, 2012. The Theta System: Argument Structure at the Interface, chapter The
Linguistic Expression of Causation, pages 20 ? 51. Oxford University Press: Oxford.
Phillip Wolff and Grace Song. 2003. Models of causation and the semantics of causal verbs. Cognitive Psychology,
47(3):276?332.
Phillip Wolff, Bianca Klettke, Tatyana Ventura, and Grace Song. 2005. Expressing causation in english and other
languages. Categorization inside and outside the laboratory: Essays in honor of Douglas L. Medin, pages
29?48.
Phillip Wolff. 2007. Representing causation. Journal of experimental psychology: General, 136(1):82.
2106
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 308?317,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Classifying Temporal Relations with Simple Features
Paramita Mirza
Fondazione Bruno Kessler
and University of Trento
Trento, Italy
paramita@fbk.eu
Sara Tonelli
Fondazione Bruno Kessler
Trento, Italy
satonelli@fbk.eu
Abstract
Approaching temporal link labelling as a
classification task has already been ex-
plored in several works. However, choos-
ing the right feature vectors to build the
classification model is still an open is-
sue, especially for event-event classifica-
tion, whose accuracy is still under 50%.
We find that using a simple feature set re-
sults in a better performance than using
more sophisticated features based on se-
mantic role labelling and deep semantic
parsing. We also investigate the impact of
extracting new training instances using in-
verse relations and transitive closure, and
gain insight into the impact of this boot-
strapping methodology on classifying the
full set of TempEval-3 relations.
1 Introduction
In recent years, temporal processing has gained in-
creasing attention within the NLP community, in
particular since TempEval evaluation campaigns
have been organized on this topic (Verhagen et
al., 2007; Verhagen et al., 2010; UzZaman et al.,
2013). In particular, the classification of tem-
poral relations holding between entities such as
events and temporal expressions (timex) is crucial
to build event timelines and to reconstruct the plot
of a story. This could be exploited in decision sup-
port systems and document archiving applications,
among others.
In this work we focus on the problem of clas-
sifying temporal relation types, assuming that the
links between events and time expressions are al-
ready established. This task is part of Tempeval-3
evaluation campaign, hence we follow the guide-
lines and the dataset provided by the organizers,
so that we can compare our system with other
systems participating in the challenge. Recent
works have tried to address this complex classifi-
cation task by using sophisticated features, based
on deep parsing, semantic role labelling and dis-
course parsing (D?Souza and Ng, 2013; Laokulrat
et al., 2013). We argue that a simpler approach,
based on lexico-syntactic features, achieves com-
parable results, while reducing the processing time
needed to extract the features. Besides, the perfor-
mance of complex NLP tools may strongly vary
when moving to new domains, affecting in turn the
classification performance, while our approach is
likely to be more stable across different domains.
Our features include some basic information on
the position, the attributes and the PoS tags of
events and timexes, as well as other information
obtained from external lexical resources such as a
list of typical event durations and a list of temporal
signals. The few processing steps required include
PoS-tagging, dependency parsing and the seman-
tic tagging of connectives (based on the parser out-
put).
We also investigate the impact of extending the
number of training instances through inverse rela-
tions and transitive closure, which is a ?simplified?
version of temporal closure covering only entities
connected via the same relation type.
2 Related Work
The task we deal with in this paper was proposed
as part of the TempEval-3 shared task (UzZaman
et al., 2012). Compared to previous TempEval
campaigns, the TempEval-3 task involved recog-
nizing the full set of temporal relations in TimeML
(14 types) instead of a reduced set, increasing the
task complexity. This specific temporal relation
classification task becomes the main focus of this
paper.
Supervised classification of temporal relation
types has already been explored in some earlier
works. Mani et al. (2006) built a MaxEnt classi-
fier to label the temporal links using training data
308
which were bootstrapped by applying temporal
closure. Chambers et al. (2007) focused on clas-
sifying the temporal relation type of event-event
pairs using previously learned event attributes as
features. However, both works use a reduced set
of temporal relations, obtained by collapsing the
relation types that inverse each other into a single
type.
Our work is most similar to the recent work
by D?Souza and Ng (2013). The authors perform
the same task on the full set of temporal rela-
tions, but adopt a much more complex approach.
They utilize lexical relations extracted from the
Merriam-Webster dictionary and WordNet (Fell-
baum, 1998), as well as semantic and discourse
features. They also introduce 437 hand-coded
rules to build a hybrid classification model.
Since we conduct our experiments based on
TempEval-3 task setup, this work is also compa-
rable with the systems participating in the task.
UzZaman et al. (2013) report that three groups
submitted at least one system run to the task.
The best performing one (Laokulrat et al., 2013)
uses, among others, sentence-level semantic in-
formation from a deep syntactic parser, namely
predicate-argument structure features. Another
system (Chambers, 2013) is composed of four
MaxEnt classifiers, two of which have been
trained for event-event links (inter- and intra-
sentence) and two for event-time links. The third-
ranked system (Kolya et al., 2013), instead, im-
plements a much simpler set of features account-
ing for event tense, modality and aspect, event and
timex context, etc.
3 Temporal Link Labelling
In this section we detail the task of temporal re-
lation labelling, the features implemented in our
classification system and the strategy adopted to
bootstrap new training data.
3.1 Task description
The full set of temporal relations specified in
TimeML version 1.2.1 (Saur?? et al., 2006) con-
tains 14 types of relations, as illustrated in Table 1.
Among them there are six pairs of relations that in-
verse each other.
Note that according to TimeML 1.2.1 annota-
tion guidelines, the difference between DURING
and IS INCLUDED (also their inverses) is that
DURING relation is specified when an event per-
sists throughout a temporal duration (e.g. John
drove for 5 hours), while IS INCLUDED relation
is specified when an event happens within a tem-
poral expression (e.g. John arrived on Tuesday).
a |???| a is BEFORE b
b |???| b is AFTER a
a |???| a is IBEFORE b
b |???| b is IAFTER a
a |??| a BEGINS b
b |????| b is BEGUN BY a
a |??| a ENDS b
b |????| b is ENDED BY a
a |??| a is DURING b
b |??????| b is DURING INV a
a |??????| a INCLUDES b
b |??| b IS INCLUDED in a
a |???|
a is SIMULTANEOUS with b
b |???|
a |???| b a is IDENTITY with b
Table 1: Temporal relations in TimeML annota-
tion
In TimeML annotation, temporal links are used
to (i) establish the temporal order of two events
(event-event pair), (ii) anchor an event to a time
expression (event-timex pair) and (iii) establish the
temporal order of two time expressions (timex-
timex pair).
The problem of determining the label of a given
temporal link can be regarded as a classification
problem. Given an ordered pair of entities (e
1
,
e
2
) that could be either event-event, event-timex
or timex-timex pair, the classifier has to assign a
certain label, namely one of the 14 temporal rela-
tion types. We train a classification model for each
category of entity pair, as suggested in several pre-
vious works (Mani et al., 2006; Chambers, 2013).
However, because there are very few examples
of timex-timex pairs in the training corpus, it is not
possible to train the classification model for these
particular pairs. Moreover, they only add up to
3.2% of the total number of extracted entity pairs;
therefore, we decided to disregard these pairs.
3.2 Feature set
We implement a number of features for tempo-
ral relation classification. Some of them are ba-
sic ones which take into account morpho-syntactic
information on events and time expressions, their
textual context and their attributes. Others rely
on semantic information such as typical event du-
rations and connective type. However, we avoid
complex processing of data. Such semantic infor-
mation is based on external lists of lexical items
309
and on the output of the addDiscourse tagger
(Pitler and Nenkova, 2009).
Some features are computed independently
based on either e
1
or e
2
, while some others are
pairwise features, which are computed based on
both elements. Some pairwise features are only
relevant for event-event pairs, for example, the
information on discourse connectives and the
binary features representing whether two events
have the same event attributes or not. Similarly,
the features related to time expression attributes
are only relevant for event-timex pairs, since
this information can only be obtained if e
2
is a
time expression. The selection of features that
contribute to the improvement of event-event
and event-timex classification will be detailed in
Section 4.3.
String features. The tokens and lemmas of
e
1
and e
2
.
Grammatical features. The part of speech
(PoS) tags of e
1
and e
2
, and a binary feature
indicating whether e
1
and e
2
have the same
PoS tag. The binary feature only applies to
event-event pairs since we do not include the
PoS tag of a time expression in the feature set
of event-timex pairs. The grammatical informa-
tion is obtained using the Stanford CoreNLP tool.
1
Textual context. The textual order, sentence
distance and entity distance of e
1
and e
2
. Textual
order is the appearance order of e
1
and e
2
in the
text, while sentence distance measures how far e
1
and e
2
are from each other in terms of sentences,
i.e. 0 if they are in the same sentence. The entity
distance is only measured if e
1
and e
2
are in the
same sentence, and corresponds to the number of
entities occurring between e
1
and e
2
(i.e. if they
are adjacent, the distance is 0).
Entity attributes. Event attributes and time
expression attributes of e
1
and e
2
as specified
in TimeML annotation. Event attributes consist
of class, tense, aspect and polarity, while the
attributes of a time expression are its type, value
and dct (indicating whether a time expression
is the document creation time or not). Events
falling under the category of noun, adjective and
1
http://nlp.stanford.edu/software/
corenlp.shtml
preposition do not have tense and aspect attributes
in TimeML. We retrieve this information by
extracting the tense and aspect of the verbs that
govern them, based on their dependency relation.
For event-event pairs we also include four binary
features representing whether e
1
and e
2
have the
same event attributes or not.
Dependency relations. Similar to D?Souza
and Ng (2013), we use the information related to
the dependency relation between e
1
and e
2
. We
include as features (i) the type of the dependency
relation that exists between them, (ii) the depen-
dency order which is either governor-dependent
or dependent-governor and (iii) binary features
indicating whether e
1
/e
2
is the root of the sen-
tence. This information is based on the collapsed
representation of dependency relations provided
by the parsing module of Stanford CoreNLP.
Consider the sentence ?John left the office and
drove back home for 20 minutes?. Using the
collapsed typed dependencies we could get the di-
rect relations between the existing entities, which
are conj and(left, drove) and prep for(drove,
minutes).
Event durations. To our knowledge, we are
the first to exploit event duration information
as features for temporal relation classification.
In fact, duration can be expressed not only by
a predicate?s tense and aspect but also by its
aktionsart, i.e. the inherent temporal information
connected to the meaning of a predicate. The
typical event duration allows us to infer, for
instance, that a punctual event is more likely to
be contained in a durative one. If we consider the
sentence ?State-run television broadcast footage
of Cuban exiles protesting in Miami?, this feature
would tell us that broadcast lasts for hours while
protesting lasts for days, thus contributing in
determining the direction of DURING relation
between the events.
The approximate duration for an event is
obtained from the list of 1000 most frequent
verbs and their duration distributions compiled
by Gusev et al. (2011).
2
The types of duration
include seconds, minutes, hours, days, weeks,
months, years and decades. We also add the
duration difference between e
1
and e
2
as a feature
2
The list is available at http://cs.stanford.edu/
people/agusev/durations/
310
with the value varied between same, less or more.
Similar to tense and aspect attributes for events,
the duration of events under the category of noun,
adjective and preposition are estimated by the
governing verb. As for time expressions, their
durations are estimated from their type and value
attributes using a set of simple rules, e.g. the
duration of Thursday morning (with the type of
TIME and the value of xxxx-xx-xxTMO) is hours.
Temporal signals. Derczynski and Gaizauskas
(2012) show the importance of temporal signals
in temporal link labelling. We take this into
account by integrating in our features the list of
signals extracted from TimeBank 1.2 corpus
3
. We
believe that the system performance will benefit
from distinguishing between event-related signals
and timex-related signals, therefore we manually
split the signals into two separate lists. Signals
such as when, as and then are commonly used
to temporally connect events, while signals such
as at, for and within more likely occur with time
expressions. There are also signals that are used
in both cases such as before, after and until, and
those kind of signals are added to both lists.
Besides the signal token, the position of the sig-
nal with respect to the events or time expressions
is also an important feature. Consider the position
of a signal in the sentences (i) ?John taught high
school before he worked at a bank? and (ii)
?Before John taught high school, he worked at a
bank?, which is crucial to determine the order of
John?s occupations. We also include in the feature
set whether a signal occurs at the beginning of a
sentence, as it is usually used to temporally relate
events in different sentences, e.g. ?John taught
high school. Previously, he worked at a bank.?
Temporal discourse connectives. Consider
the following sentences:
(i) ?John has been taking that driving course
since the accident that took place last week.?
(ii) ?John has been taking that driving course
since he wants to drive better.?
In order to label the temporal link holding be-
tween two events, it is important to know whether
there are temporal connectives in the surrounding
3
The list is available at http://www.timeml.org/
site/timebank/browser_1.2/displayTags.
php?treshold=1&tagtype=signal&sort=alpha
context, because they may contribute in identify-
ing the relation type. For instance, it may be rele-
vant to distinguish whether since is used as a tem-
poral or a causal cue (example (i) and (ii) resp.).
This information about discourse connectives is
acquired using the addDiscourse tool (Pitler and
Nenkova, 2009), which identifies connectives and
assigns them to one of four semantic classes: Tem-
poral, Expansion, Contingency and Comparison.
Note that this is a much shallower approach than
the one proposed by D?Souza and Ng (2013), who
perform full discourse parsing.
We include as feature whether a discourse con-
nective belonging to the Temporal class occurs in
the textual context of e
1
and e
2
. Similar to tem-
poral signals, we also include in the feature set the
position of the discourse connective with respect
to the events.
3.3 Inverse Relations and Transitive Closure
Since Mani et al. (2006) demonstrate that boot-
strapping training data through temporal closure
results in quite significant improvements, we try
to provide the classifier with more data to learn
from using the inverse relations and closure-based
inferred relations.
There are six pairs of relation types in TimeML
that inverse each other (see Table 1). By switch-
ing the order of the entities in a given pair and la-
belling the pair with the inverse relation type, we
basically multiply the number of training data.
As for temporal closure, there have been at-
tempts to apply it to improve temporal relation
classification. Mani et al. (2006) use SputLink
(Verhagen, 2005), which was developed based on
Allen?s closure inference (Allen, 1983), to infer
the relations based on temporal closure. UzZaman
and Allen (2011b) employ Timegraph (Gerevini et
al., 1995) to implement the scorer for TempEval-3
evaluation, since precision and recall for temporal
relation classification are computed based on the
closure graph.
We use a simpler approach to obtain the closure
graph of temporal relations, by applying the tran-
sitive closure only within the same relation type,
e.g. e
1
BEFORE e
2
? e
2
BEFORE e3 ? e
1
BE-
FORE e3. It can be seen as partial temporal clo-
sure since it produces only a subset of the rela-
tions produced by temporal closure, which covers
more complex cases, e.g. e
1
BEFORE e
2
? e
2
IN-
CLUDES e3? e
1
BEFORE e3.
311
As shown in Fischer and Meyer (1971), the
problem of finding the transitive closure of a di-
rected acyclic graph can be reduced to a boolean
matrix multiplication. For each temporal relation
type, we build its boolean matrix with the size of
n ? n, n being the number of entities in a text.
Given a temporal relation type R and its boolean
matrix M , the transitive closure-based relations of
R can be inferred from the matrix M
2
by extract-
ing its non-zero elements.
4 Experiment Description
4.1 Dataset
Since we want to compare our work with ex-
isting approaches to temporal relation classifica-
tion, we use the same training and test data as
in Tempeval-3 challenge
4
. Two types of train-
ing data were made available in the challenge:
TBAQ-cleaned and TE3-Silver-data. The former
includes a cleaned and improved version of the
AQUAINT TimeML corpus, containing 73 news
report documents, and the TimeBank corpus, with
183 news articles. TE3-Silver-data, instead, is a
600K word corpus annotated by the best perform-
ing systems at Tempeval-2, which we do not use
in our experiments.
Our test data is the newly created TempEval-
3-platinum evaluation corpus that was anno-
tated/reviewed by the Tempeval-3 task organizers.
The distribution of the relation types in all previ-
ously mentioned datasets is shown in Table 2. We
report also the statistics obtained after applying in-
verse relations and transitive closure, that increase
the number of training instances.
It is worth noticing that DURING INV relation
does not exist in the training data but appears in the
test data. In this case, inverse relations help in au-
tomatically acquiring training instances. The BE-
FORE relation corresponds to the majority class
and makes the instance distribution quite unbal-
anced, especially in the TBAQ corpus. Finally,
five event-timex instances in the TBAQ training
data are labeled with IDENTITY relation and can
be assumed to be falsely annotated.
4.2 Experimental Setup
We build our classification models using the Sup-
port Vector Machine (SVM) implementation pro-
4
http://www.cs.york.ac.uk/
semeval-2013/task1/index.php?id=data
vided by YamCha
5
. The experiment involves con-
ducting 5-fold cross validation on the TimeBank
corpus to find the best combination of features for
the event-event and event-timex classifiers. We
first run our experiments using YamCha default
parameters (pairwise method for multi-class clas-
sification and polynomial kernel of degree 2). Af-
ter identifying the best feature sets for the two
classifiers, we evaluate them using different ker-
nel degrees (from 1 to 4).
4.3 Feature Engineering
In order to select from our initial set of features
only those that improve the accuracy of the event-
event and event-timex classifiers, we incremen-
tally add them to the baseline (the model with
string feature only), and compute their contribu-
tion. Table 3 shows the results of this selection
process, by including the average accuracy from
the 5-fold cross validation.
In Table 3, we report the feature contributions of
the entity attributes and dependency relations sets
in more details, because within those categories
only some of the features have a positive impact on
accuracy. Instead, for features within textual con-
text, signal and discourse categories, incremen-
tally adding each feature results in increasing ac-
curacy, therefore we report only the overall accu-
racy of the feature group. Similarly, for duration
features, adding each feature incrementally results
in decreasing accuracy.
Regarding entity attributes, it can be seen that
aspect and class features have no positive im-
pact on the accuracy of event-event classifica-
tion, along with pairwise features same class and
same polarity. As for event-timex classification,
all event attributes except for polarity contribute
to accuracy improvements. Among time expres-
sion attributes, only the information about whether
a time expression is a document creation time or
not (dct feature) helps improving the classifier.
The dependency order feature does not give
positive contribution to the accuracy in both cases.
Besides, information on whether an event is the
root of the sentence (dependency is root feature)
is not relevant for event-timex classification.
Adding the temporal signal feature very slightly
improves the accuracy of event-event classifica-
tion, not as much as its contribution to event-timex
5
http://chasen.org/
?
taku/software/
yamcha/
312
Relation
event-event event-timex
train test train test
TB TBAQ TBAQ-I TBAQ-IC TE3-P TB TBAQ TBAQ-I TBAQ-IC TE3-P
BEFORE 490 2,115 2,938 5,685 226 661 1,417 1,925 2,474 96
AFTER 458 823 2,938 5,685 167 205 509 1,925 2,474 29
IBEFORE 22 60 103 105 1 2 3 8 8 5
IAFTER 27 43 103 105 2 4 5 8 8 6
BEGINS 24 44 86 85 0 20 65 89 89 1
BEGUN BY 24 42 86 85 1 22 24 89 89 1
ENDS 12 17 79 79 1 47 59 120 120 2
ENDED BY 44 62 79 79 0 57 61 120 120 2
DURING 46 80 80 84 1 197 200 200 201 1
DURING INV 0 0 80 84 0 0 0 200 201 1
INCLUDES 170 308 724 7,246 40 288 1,104 2,945 3,404 42
IS INCLUDED 212 416 724 7,246 47 897 1,841 2,945 3,404 125
SIMULTANEOUS 456 519 519 518 81 58 58 58 58 6
IDENTITY 534 742 742 742 15 4 5 5 5 0
Total 2,519 5,271 9,281 27,828 582 2,462 5,351 10,637 12,655 317
Table 2: The distribution of each relation type in the datasets for both event-event and event-timex pairs.
TB stands for TimeBank corpus, TBAQ denotes the combination of TimeBank and AQUAINT corpora,
TBAQ-I denotes the TBAQ corpus augmented with inverse relations, TBAQ-IC is the TBAQ corpus
with both inverse relations and transitive closure, and TE3-P is the TempEval-3-platinum evaluation
corpus.
classification. However, together with the tem-
poral discourse feature, they positively affect ac-
curacy, confirming previous findings (Derczynski
and Gaizauskas, 2012).
Surprisingly, adding event duration feature de-
creases the accuracy in both cases. This might be
caused by the insufficient coverage of the event
duration resource, since around 20% of the train-
ing pairs contain at least an event whose duration
is unknown. Moreover, we employ the approxi-
mate duration of a verb event as a feature without
considering the context and discourse. For exam-
ple, according to the distributions in the duration
resource, the event attack has two likely durations,
minutes and decades, with decades being slightly
more probable than minutes. In the sentence ?Is-
rael has publicly declared that it will respond to
an Iraqi attack on Jordan.?, the classifier fails to
recognize the IBEFORE relation between attack
and respond (attack happens immediately before
respond), because the duration feature of attack is
recognized as decades, while in this context the
attack most probably occurs within seconds.
According to the analysis of the different fea-
ture contributions, we define the best classification
models for both event-event pairs and event-timex
pairs as the models using combinations of features
that have positive impacts on the accuracy, based
on Table 3. Given the best performing sets of fea-
tures, we further experiment with different kernel
degrees in the same 5-fold cross validation sce-
nario.
The best classifier performances are achieved
with the polynomial kernel of degree 4, both for
event-event and event-timex classification. The
accuracy for event-event classification is 43.69%,
while for event-timex classification it is 66.62%.
However, using a high polynomial kernel degree
introduces more complexity in training the classi-
fication model, thus more time is required to train
such models.
D?Souza and Ng (2013) evaluate their system
on the same corpus, but with a slightly different
setting. They also split TimeBank into 5 folds,
but they only use two of them to perform 2-fold
cross validation, while they use another part of the
corpus to develop rules for their hybrid system.
Their best configuration gives 46.8% accuracy for
event-event classification and 65.4% accuracy for
event-timex classification. Although the two ap-
proaches are not directly comparable, we can as-
sume that the systems? performance are likely to
be very similar, with a better accuracy on event-
event classification by D?Souza and Ng (2013) and
a better performance on event-timex pairs by our
system. Probably, the hybrid system by D?Souza
and Ng, which integrates supervised classification
and manual rules, performs better on event-event
classification because it is a more complex task
than event-timex classification, where simple lex-
ical and syntactic features are still very effective.
313
event-event event-timex
Feature Accuracy Feature Accuracy
majority class 22.17% - majority class 36.42% -
string 31.07% - string 58.27% -
+grammatical 36.15% 5.08% +grammatical 61.30% 3.03%
+textual context 39.44% 3.29% +textual context 61.71% 0.41%
+tense 41.10% 1.66% +tense 63.10% 1.39%
+aspect 41.10% 0.00% +aspect 64.51% 1.41%
+class 39.96% -1.14% +class 65.30% 0.79%
+polarity 40.44% 0.48% +polarity 64.88% -0.42%
+same tense 40.55% 0.11% +dct 65.21% 0.33%
+same aspect 40.63% 0.08% +type 64.99% -0.22%
+same class 40.63% 0.00% +value 64.60% -0.39%
+same polarity 40.47% -0.16%
+ dependency 42.15% 1.68% +dependecy 65.60% 1.00%
+dependency order 41.99% -0.16% +dependency order 65.47% -0.13%
+dependency is root 42.63% 0.64% +dependency is root 65.22% -0.25%
+temporal signal 42.66% 0.03% +temporal signal 65.43% 0.21%
+temporal discourse 42.82% 0.16%
+duration 41.47% -1.35% +duration 64.19% -1.24%
Table 3: Feature contributions for event-event and event-timex classification. Features in italics have a
negative impact on accuracy and are not included in the final feature set.
5 Evaluation
We perform two types of evaluation. In the first
one, we evaluate the system performance with the
best feature sets and the best parameter configu-
ration using the four training sets presented in Ta-
ble 2. Our test set is the TempEval-3-platinum cor-
pus. The goal of this first evaluation is to specifi-
cally investigate the effect of enriching the training
data with inverse relations and transitive closure.
We compute the system accuracy as the percent-
age of the correct labels out of all annotated links.
In the second evaluation, we compare our sys-
tem to the systems participating in the task on tem-
poral relation classification at TempEval-3. The
test set is again TempEval-3-platinum, i.e. the
same one used in the competition. The task or-
ganizers introduced an evaluation metric (UzZa-
man and Allen, 2011a) capturing temporal aware-
ness in terms of precision, recall and F1-score. To
compute precision and recall, they verify the cor-
rectness of annotated temporal links using tempo-
ral closure, by checking the existence of the iden-
tified relations in the closure graph. In order to
replicate this type of evaluation, we use the scorer
made available to the task participants.
5.1 Evaluation of the Effects of Inverse
Relations and Transitive Closure
Table 4 shows the classifiers? accuracies achieved
using different training sets. After performing a
randomization test between the best performing
classifier and the others, we notice that on event-
event classification the improvement is significant
(p < 0.005) only between TBAQ and TimeBank.
This shows that only extending the TimeBank cor-
pus by adding AQUAINT is beneficial. In all other
cases, the differences are not significant. Applying
inverse relations and transitive closure extends the
number of training instances but makes the dataset
more unbalanced, thus it does not result in a sig-
nificant improvement.
Training data event-event event-timex
TimeBank 42.61% 71.92%
TBAQ 48.28% 73.82%
TBAQ-I 47.77% 74.45%
TBAQ-IC 46.39% 74.45%
Table 4: Classifier accuracies with different train-
ing data
This result is in contrast with the improvement
brought about by temporal closure reported in
Mani et al. (2006). The difference between our
approach and Mani et al.?s is that (i) we apply only
the transitive closure instead of the full temporal
one, and (ii) our classification task includes 14 re-
lations, while the other authors classify 6 relations.
In our future work, we will investigate whether the
benefits of closure are affected by the number of
relations, or whether our simplified version is ac-
tually outperformed by the full one.
Furthermore, we plan to investigate the effect of
over-sampling to handle highly skewed datasets,
for instance by applying inverse relations and tran-
sitive closure only to minority classes.
314
5.2 Evaluation of the System Performance in
TempEval-3 task
We train our classifiers for event-event pairs and
event-timex pairs by exploiting the best feature
combination and best configuration acquired from
the experiment, and using the best reported dataset
for each classifier as the training data. Even
though it has been shown that inverse relations and
transitive closure do not bring significantly posi-
tive impact to the accuracy, using the TBAQ-IC
corpus as the training set for event-timex classifi-
cation is still the best option. The two classifiers
are part of a temporal classification system called
TRelPro.
We compare in Table 5 the performance of
TRelPro to the other systems participating in
Tempeval-3 task, according to the figures reported
in (UzZaman et al., 2013). TRelPro is the best per-
forming system both in terms of precision and of
recall.
System F1 Precision Recall
TRelPro 58.48% 58.80% 58.17%
UTTime-1, 4 56.45% 55.58% 57.35%
UTTime-3, 5 54.70% 53.85% 55.58%
UTTime-2 54.26% 53.20% 55.36%
NavyTime-1 46.83% 46.59% 47.07%
NavyTime-2 43.92% 43.65% 44.20%
JU-CSE 34.77% 35.07% 34.48%
Table 5: Tempeval-3 evaluation on temporal rela-
tion classification
In order to analyze which are the most com-
mon errors made by TRelPro, we report in Table 6
the number of true positives (tp), false positives
(fp) and false negatives (fn) scored by the system
on each temporal relation. The system generally
fails to recognize IBEFORE, BEGINS, ENDS and
DURING relations, along with their inverse rela-
tions, primarily because of the skewed distribution
of instances in the training data, especially in com-
parison with the majority classes. This explains
also the large number of false positives labelled for
the BEFORE class (event-event pairs) and for the
IS INCLUDED class (event-timex pairs), which
are the majority classes for the two pairs respec-
tively.
6 Conclusion
We have described an approach to temporal link
labelling using simple features based on lexico-
syntactic information, as well as external lexical
resources listing temporal signals and event dura-
Relation
event-event event-timex
tp fp fn tp fp fn
BEFORE 186 186 40 82 17 14
AFTER 63 40 104 14 7 15
IBEFORE 0 0 1 0 0 5
IAFTER 0 0 2 0 0 6
BEGINS 0 0 0 0 0 1
BEGUN BY 0 0 0 0 0 1
ENDS 0 0 1 0 0 2
ENDED BY 1 1 0 0 0 2
DURING 0 0 1 0 2 1
DURING INV 0 0 0 0 0 1
INCLUDES 1 2 39 27 13 15
IS INCLUDED 2 4 45 114 40 11
SIMULTANEOUS 20 33 61 0 0 6
IDENTITY 9 35 6 0 1 0
Table 6: Relation type distribution for TempEval-
3-platinum test data, annotated with TRelPro. The
tp fields indicate the numbers of correctly anno-
tated instances, while the fp/fn fields correspond
to false positives/negatives.
tions. We find that by using a simple feature set we
can build a system that outperforms the systems
built using more sophisticated features, based on
semantic role labelling and deep semantic parsing.
This may depend on the fact that more complex
features are usually extracted from the output of
NLP systems, whose performance impacts on the
quality of such features.
We find that bootstrapping the training data with
inverse relations and transitive closure does not
help improving the classifiers? performances sig-
nificantly as it was reported in previous works, es-
pecially in event-event classification where the ac-
curacy decreases instead. In the future, we will
further investigate the reason of this difference.
We will also explore other variants of closure, as
well as over-sampling techniques to handle the
highly skewed dataset introduced by closure.
Finally, the overall performance of our system,
using the best models for both event-event and
event-timex classification, outperforms the other
systems participating in the TempEval-3 task. This
confirms our intuition that using simple features
and reducing the amount of complex semantic and
discourse information is a valuable alternative to
more sophisticated approaches.
Acknowledgments
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project
(ICT-316404).
315
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?
843, November.
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the ACL on Interactive Poster and Demonstra-
tion Sessions, ACL ?07, pages 173?176, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associ-
ation for Computational Linguistics.
Leon Derczynski and Robert J. Gaizauskas. 2012. Us-
ing Signals to Improve Automatic Classification of
Temporal Relations. CoRR, abs/1203.5055.
Jennifer D?Souza and Vincent Ng. 2013. Classify-
ing Temporal Relations with Rich Linguistic Knowl-
edge. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 918?927.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Michael J. Fischer and Albert R. Meyer. 1971.
Boolean matrix multiplication and transitive closure.
In SWAT (FOCS), pages 129?131. IEEE Computer
Society.
Alfonso Gerevini, Lenhart Schubert, and Stephanie
Schaeffer. 1995. The temporal reasoning tools
Timegraph I-II. International Journal of Artificial
Intelligence Tools, 4(1-2):281?299.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Juraf-
sky. 2011. Using query patterns to learn the dura-
tion of events. In Proceedings of the Ninth Inter-
national Conference on Computational Semantics,
IWCS ?11, pages 145?154, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Anup Kumar Kolya, Amitava Kundu, Rajdeep Gupta,
Asif Ekbal, and Sivaji Bandyopadhyay. 2013.
Ju cse: A crf based approach to annotation of tem-
poral expression, event and temporal relations. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 64?72, Atlanta,
Georgia, USA, June. Association for Computational
Linguistics.
Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Uttime:
Temporal relation classification using deep syntactic
features. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 88?92,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
ACL-44, pages 753?760, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Emily Pitler and Ani Nenkova. 2009. Using syn-
tax to disambiguate explicit discourse connectives
in text. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 13?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Roser Saur??, Jessica Littman, Robert Gaizauskas, An-
drea Setzer, and James Pustejovsky, 2006. TimeML
Annotation Guidelines, Version 1.2.1.
Naushad UzZaman and James Allen. 2011a. Tem-
poral Evaluation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
351?356, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Naushad UzZaman and James F. Allen. 2011b. Tem-
poral evaluation. In ACL (Short Papers), pages 351?
356. The Association for Computer Linguistics.
Naushad UzZaman, Hector Llorens, James F. Allen,
Leon Derczynski, Marc Verhagen, and James Puste-
jovsky. 2012. TempEval-3: Evaluating Events,
Time Expressions, and Temporal Relations. CoRR,
abs/1206.5333.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal
relation identification. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations, Se-
mEval ?07, pages 75?80, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
316
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, SemEval
?10, pages 57?62, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Marc Verhagen. 2005. Temporal closure in an annota-
tion environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
317
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Outsourcing FrameNet to the Crowd
Marco Fossati, Claudio Giuliano, and Sara Tonelli
Fondazione Bruno Kessler
Trento, Italy
{fossati,giuliano,satonelli}@fbk.eu
Abstract
We present the first attempt to perform full
FrameNet annotation with crowdsourcing
techniques. We compare two approaches:
the first one is the standard annotation
methodology of lexical units and frame
elements in two steps, while the second
is a novel approach aimed at acquiring
frames in a bottom-up fashion, starting
from frame element annotation. We show
that our methodology, relying on a single
annotation step and on simplified role defi-
nitions, outperforms the standard one both
in terms of accuracy and time.
1 Introduction
Annotating frame information is a complex task,
usually modeled in two steps: first annotators are
asked to choose the situation (or frame) evoked by
a given predicate (the lexical unit, LU) in a sen-
tence, and then they assign the semantic roles (or
frame elements, FEs) that describe the participants
typically involved in the chosen frame. Existing
frame annotation tools, such as Salto (Burchardt
et al, 2006) and the Berkeley system (Fillmore et
al., 2002) foresee this two-step approach, in which
annotators first select a frame from a large reposi-
tory of possible frames (1,162 frames are currently
listed in the online version of the resource), and
then assign the FE labels constrained by the cho-
sen frame to LU dependents.
In this paper, we argue that such workflow
shows some redundancy which can be addressed
by radically changing the annotation methodology
and performing it in one single step. Our novel an-
notation approach is also more compliant with the
definition of frames proposed in Fillmore (1976):
in his seminal work, Fillmore postulated that the
meanings of words can be understood on the basis
of a semantic frame, i.e. a description of a type
of event or entity and the participants in it. This
implies that frames can be distinguished one from
another on the basis of the participants involved,
thus it seems more cognitively plausible to start
from the FE annotation to identify the frame ex-
pressed in a sentence, and not the contrary.
The goal of our methodology is to provide full
frame annotation in a single step and in a bottom-
up fashion. Instead of choosing the frame first, we
focus on FEs and let the frame emerge based on
the chosen FEs. We believe this approach com-
plies better with the cognitive activity performed
by annotators, while the 2-step methodology is
more artificial and introduces some redundancy
because part of the annotators? choices are repli-
cated in the two steps (i.e. in order to assign a
frame, annotators implicitly identify the partici-
pants also in the first step, even if they are anno-
tated later).
Another issue we investigate in this work is how
semantic roles should be annotated in a crowd-
sourcing framework. This task is particularly
complex, therefore it is usually performed by ex-
pert annotators under the supervision of linguis-
tic experts and lexicographers, as in the case of
FrameNet. In NLP, different annotation efforts
for encoding semantic roles have been carried out,
each applying its own methodology and annota-
tion guidelines (see for instance Ruppenhofer et
al. (2006) for FrameNet and Palmer et al (2005)
for PropBank). In this work, we present a pilot
study in which we assess to what extent role de-
scriptions meant for ?linguistics experts? are also
suitable for annotators from the crowd. Moreover,
we show how a simplified version of these descrip-
tions, less bounded to a specific linguistic theory,
improve the annotation quality.
2 Related work
The construction of annotation datasets for NLP
tasks via non-expert contributors has been ap-
742
proached in different ways, the most prominent
being games with a purpose (GWAP) and micro-
tasks. Verbosity (Von Ahn et al, 2006) was one
of the first attempts in gathering annotations with
a GWAP. Phrase Detectives (Chamberlain et al,
2008; Chamberlain et al, 2009) was meant to
gather a corpus with coreference resolution an-
notations. Snow et al (2008) described design
and evaluation guidelines for five natural language
micro-tasks. However, they explicitly chose a set
of tasks that could be easily understood by non-
expert contributors, thus leaving the recruitment
and training issues open. Negri et al (2011) built
a multilingual textual entailment dataset for statis-
tical machine translation systems.
The semantic role labeling problem has been re-
cently addressed via crowdsourcing by Hong and
Baker (2011). Furthermore, Baker (2012) high-
lighted the crucial role of recruiting people from
the crowd in order to bypass the need for linguis-
tics expert annotations. Nevertheless, Hong and
Baker (2011) focused on the frame discrimination
task, namely selecting the correct frame evoked by
a given lemma. Such task is comparable to the
word sense disambiguation one as per (Snow et
al., 2008), although the complexity increased, due
to lower inter-annotator agreement values.
3 Experiments
In this section, we describe the anatomy and dis-
cuss the results of the tasks we outsourced to the
crowd via the CrowdFlower1 platform.
Golden data Quality control of the collected
judgements is a key factor for the success of
the experiments. Cheating risk is minimized by
adding gold units, namely data for which the re-
quester already knows the answer. If a worker
misses too many gold answers within a given
threshold, he or she will be flagged as untrusted
and his or her judgments will be automatically dis-
carded.
Worker switching effect Depending on their
accuracy in providing answers to gold units, work-
ers may switch from a trusted to an untrusted sta-
tus and vice versa. In practice, a worker submits
his or her responses via a web page. Each page
contains one gold unit and a variable number of
regular units that can be set by the requester dur-
ing the calibration phase. If a worker becomes un-
1https://crowdflower.com
trusted, the platform collects another judgment to
fill the gap. If a worker moves back to the trusted
status, his or her previous contribution is added
to the results as free extra judgments. Such phe-
nomenon typically occurs when the complexity of
gold units is high enough to induce low agree-
ment in workers? answers. Thus, the requester is
constrained to review gold units and to eventually
forgive workers who missed them. This has mas-
sively happened in our experiments and is one of
the main causes of the overall cost decrease and
time increase.
Cost calibration The total cost of a generic
crowdsourcing task is naturally bound to a data
unit. This represents an issue in most of our ex-
periments, as the number of questions per unit
(i.e. a sentence) varies according to the number
of frames and FEs evoked by the LU contained in
a sentence. In order to enable cost comparison, for
each experiment we need to use the average num-
ber of questions per sentence as a multiplier to a
constant cost per sentence. We set the payment
per working page to 5 $ cents and the number of
sentences per page to 3, resulting in 1.83 $ cent
per sentence.
3.1 Assessing task reproducibility and
worker behavior change
Since our overall goal is to compare the perfor-
mance of FrameNet annotation using our novel
workflow to the performance of the standard, 2-
step approach, we first take into account past re-
lated works and try to reproduce them.
To our knowledge, the only attempt to annotate
frame information through crowdsourcing is the
one presented in Hong and Baker (2011), which
however did not include FE annotation.
Modeling The task is designed as follows. (a)
Workers are invited to read a sentence where a
LU is bolded. (b) The question Which is the
correct sense? is combined with the set of
frames evoked by the given LU, as well as the
None choice. Finally, (c) workers must select the
correct frame. A set of example sentences corre-
sponding to each possible frame is provided in the
instructions to facilitate workers.
As a preliminary study, we wanted to assess
to what extent the proposed task could be repro-
duced and if workers reacted in a comparable way
over time. Hong and Baker (2011) did not pub-
lish the input datasets, thus we ignore which sen-
743
LU
2013 2011Sentences Accuracy Accuracy(Gold)
high.a 68 (9) 91.8 92
history.n 72 (9) 84.6 86
range.n 65 (8) 95 93
rip.v 88 (12) 81.9 92
thirst.n 29 (4) 90.4 95
top.a 36 (5) 98.7 96
Table 1: Comparison of the reproduced frame dis-
crimination task as per (Hong and Baker, 2011)
tences were used. Besides, the authors computed
accuracy values directly from the results upon a
majority vote ground truth. Therefore, we de-
cided to consider the same LUs used in Hong
and Baker?s experiments, i.e. high.a, history.n,
range.n, rip.v, thirst.n and top.a, but we lever-
aged the complete sets of FrameNet 1.5 expert-
annotated sentences as gold-standard data for im-
mediate accuracy computation.
Discussion Table 1 displays the results we
achieved, jointly with the experiments by Hong
and Baker (2011). For the latter, we only show ac-
curacy values, as the number of sentences was set
to a constant value of 18, 2 of which were gold.
If we assume that the crowd-based ground truth in
2011 experiments is approximately equivalent to
the expert one, workers seem to have reacted in
a similar manner compared to Hong and Baker?s
values, except for rip.v.
3.2 General task setting
We randomly chose the following LUs among
the set of all verbal LUs in FrameNet evoking 2
frames each: disappear.v [CEASING TO BE, DE-
PARTING], guide.v [COTHEME, INFLUENCE OF -
EVENT ON COGNIZER], heap.v [FILLING, PLAC-
ING], throw.v [BODY MOVEMENT, CAUSE MO-
TION]. We considered verbal LUs as they usually
have more overt arguments in a sentence, so that
we were sure to provide workers with enough can-
didate FEs to annotate. Linguistic tasks in crowd-
sourcing frameworks are usually decomposed to
make them accessible to the crowd. Hence, we
set the polysemy of LUs to 2 to ensure that all
experiments are executed using the smallest-scale
subtask. More frames can then be handled by just
replicating the experiments.
3.3 2-step approach
After observing that we were able to achieve sim-
ilar results on the frame discrimination task as in
previous work, we focused on the comparison be-
tween the 2-step and the 1-step frame annotation
approaches.
We first set up experiments that emulate the for-
mer approach both in frame discrimination and
FEs annotation. This will serve as the baseline
against our methodology. Given the pipeline na-
ture of the approach, errors in the frame discrim-
ination step will affect FE recognition, thus im-
pacting on the final accuracy. The magnitude of
such effect strictly depends on the number of FEs
associated with the wrongly detected frame.
3.3.1 Frame discrimination
Frame discrimination is the first phase of the 2-
step annotation procedure. Hence, we need to
leverage its output as the input for the next step.
Modeling The task is modeled as per Sec-
tion 3.1.
Discussion Table 2 gives an insight into the re-
sults, which confirm the overall good accuracy as
per the experiments discussed in Section 3.1.
3.3.2 Frame elements recognition
We consider all sentences annotated in the previ-
ous subtask with the frame assigned by the work-
ers, even if it is not correct.
Modeling The task is presented as follows. (a)
Workers are invited to read a sentence where a LU
is bolded and the frame that was identified in the
first step is provided as a title. (b) A list of FE def-
initions is then shown together with the FEs text
chunks. Finally, (c) workers must match each def-
inition with the proper FE.
Simplification Since FEs annotation is a very
challenging task, and FE definitions are usually
meant for experts in linguistics, we experimented
with three different types of FE definitions: the
original ones from FrameNet, a manually simpli-
fied version, and an automatically simplified one,
using the tool by Heilman and Smith (2010). The
latter simplifies complex sentences at the syntactic
level and generates a question for each of the ex-
tracted clauses. As an example, we report below
three versions obtained for the Agent definition in
the DAMAGING frame:
744
Approach 2-STEP 1-STEP
Task FD FER
Accuracy .900 .687 .792
Answers 100 160 416
Trusted 100 100 84
Untrusted 21 36 217
Time (h) 102 69 130
Cost/question 1.83 2.74 8.41($ cents)
Table 2: Overview of the experimental results.
FD stands for Frame Discrimination, FER for FEs
Recognition
Original: The conscious entity, generally a per-
son, that performs the intentional action that re-
sults in the damage to the Patient.
Manually simplified: This element describes the
person that performs the intentional action result-
ing in the damage to another person or object.
Automatic system: What that performs the in-
tentional action that results in the damage to the
Patient?
Simplification was performed by a linguistic ex-
pert, and followed a set of straightforward guide-
lines, which can be summarized as follows:
? When the semantic type associated with the
FE is a common concept (e.g. Location),
replace the FE name with the semantic type.
? Make syntactically complex definitions as
simple as possible.
? Avoid variability in FE definitions, try to
make them homogeneous (e.g. they should
all start with ?This element describes...? or
similar).
? Replace technical concepts such as
Artifact or Sentient with com-
mon words such as Object and Person
respectively.
Although these changes (especially the last
item) may make FE definitions less precise from
a lexicographic point of view (for instance, sen-
tient entities are not necessarily persons), annota-
tion became more intuitive and had a positive im-
pact on the overall quality.
After few pilot annotations with the three types
of FE definitions, we noticed that the simplified
one achieved a better accuracy and a lower num-
ber of untrusted annotators compared to the oth-
ers. Therefore, we use the simplified definitions
in both the 2-step and the 1-step approach (Sec-
tion 3.4).
Discussion Table 2 provides an overview of the
results we gathered. The total number of answers
differs from the total number of trusted judgments,
since the average value of questions per sentence
amounts to 1.5.2 First of all, we notice an increase
in the number of untrusted judgments. This is
caused by a generally low inter-worker agreement
on gold sentences due to FE definitions, which still
present a certain degree of complexity, even af-
ter simplification. We inspected the full reports
sentence by sentence and observed a propagation
of incorrect judgments when a sentence involves
an unclear FE definition. As FE definitions may
mutually include mentions of other FEs from the
same frame, we believe this circularity generated
confusion.
3.4 1-step approach
Having set the LU polysemy to 2, in our case a
sentence S always contains a LU with 2 possible
frames (f1, f2), but only conveys one, e.g. f1. We
formulate the approach as follows. S is replicated
in 2 data units (Sa, Sb). Then, Sa is associated to
the set E1 of f1 FE definitions, namely the correct
ones for that sentence. Instead, Sb is associated to
the set E2 of f2 FE definitions. We call Sb a cross-
frame unit. Furthermore, we allow workers to se-
lect the None answer. In practice, we ask a total
amount of |E1 ? E2| + 2 questions per sentence
S. In this way, we let the frame directly emerge
from the FEs. If workers correctly answer None
to a FE definition d ? E2, the probability that S
evokes f1 increases.
Modeling Figure 1 displays a screenshot of
the worker interface. The task is designed as per
Section 3.3.2, but with major differences with
respect to its content. This is better described
by an example. The sentence Karen threw
her arms round my neck, spilling
champagne everywhere contains the LU
throw.v evoking the frame BODY MOVEMENT.
However, throw.v is ambiguous and may also
evoke CAUSE MOTION. We ask to annotate both
the BODY MOVEMENT and the CAUSE MOTION
2Cf. Section 3 for more details
745
Figure 1: 1-step approach worker interface
core FEs, respectively as regular and cross-frame
units.
Discussion We do not interpret the None choice
as an abstention from judgment, since it is a cor-
rect answer for cross-frame units. Instead of pre-
cision and recall, we are thus able to directly com-
pute workers? accuracy upon a majority vote. We
envision an improvement with respect to the 2-
step methodology, as we avoid the proven risk of
error propagation originating from wrongly anno-
tated frames in the first step. Table 2 illustrates
the results we collected. As expected, accuracy
reached a consistent enhancement. This demon-
strates the hypothesis we stated in Section 1 on
the cognitive plausibility of a bottom-up approach
for frame annotation. Furthermore, the execu-
tion time decreases compared to the sum of the
2 steps, namely 130 hours against 171. Neverthe-
less, the cost is sensibly higher due to the higher
number of questions that need to be addressed, in
average 4.6 against 1.5. Untrusted judgments se-
riously grow, mainly because of the cross-frame
gold complexity. Workers seem puzzled by the
presence of None, which is a required answer for
such units. If we consider the English FrameNet
annotation agreement values between experts re-
ported by Pado? and Lapata (2009) as the upper
bound (i.e., .897 for frame discrimination and .949
for FEs recognition), we believe our experimental
setting can be reused as a valid alternative.
4 Conclusion
In this work, we presented an approach to perform
frame annotation with crowdsourcing techniques,
based on a single annotation step and on manu-
ally simplified FE definitions. Since the results
seem promising, we are currently running larger
scale experiments with the full set of FrameNet 1.5
annotated sentences. Input data, interface screen-
shots and full results are available and regularly
updated at http://db.tt/gu2Mj98i.
Future work will include the investigation of a
frame assignment strategy. In fact, we do not take
into account the case of conflicting FE annotations
in cross-frame units. Hence, we need a confidence
score to determine which frame emerges if work-
ers selected contradictory answers in a subset of
cross-frame FE definitions.
Acknowledgements
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project
(ICT-316404).
746
References
[Baker2012] Collin F Baker. 2012. Framenet, cur-
rent collaborations and future goals. Language Re-
sources and Evaluation, pages 1?18.
[Burchardt et al2006] Aljoscha Burchardt, Katrin Erk,
Anette Frank, Andrea Kowalski, Sebastian Pado,
and Manfred Pinkal. 2006. Salto?a versatile multi-
level annotation tool. In Proceedings of LREC 2006,
pages 517?520. Citeseer.
[Chamberlain et al2008] Jon Chamberlain, Massimo
Poesio, and Udo Kruschwitz. 2008. Phrase detec-
tives: A web-based collaborative annotation game.
Proceedings of I-Semantics, Graz.
[Chamberlain et al2009] Jon Chamberlain, Udo Kr-
uschwitz, and Massimo Poesio. 2009. Constructing
an anaphorically annotated corpus with non-experts:
Assessing the quality of collaborative annotations.
In Proceedings of the 2009 Workshop on The Peo-
ple?s Web Meets NLP: Collaboratively Constructed
Semantic Resources, pages 57?62. Association for
Computational Linguistics.
[Fillmore et al2002] Charles J. Fillmore, Collin F.
Baker, and Hiroaki Sato. 2002. The FrameNet
Database and Software Tools. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 1157?
1160, Las Palmas, Spain.
[Fillmore1976] Charles J. Fillmore. 1976. Frame Se-
mantics and the nature of language. In Annals of the
New York Academy of Sciences: Conference on the
Origin and Development of Language, pages 20?32.
Blackwell Publishing.
[Heilman and Smith2010] Michael Heilman and
Noah A. Smith. 2010. Extracting Simplified
Statements for Factual Question Generation. In
Proceedings of QG2010: The Third Workshop on
Question Generation, Pittsburgh, PA, USA.
[Hong and Baker2011] Jisup Hong and Collin F Baker.
2011. How good is the crowd at ?real? wsd? ACL
HLT 2011, page 30.
[Negri et al2011] Matteo Negri, Luisa Bentivogli,
Yashar Mehdad, Danilo Giampiccolo, and Alessan-
dro Marchetti. 2011. Divide and conquer: crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 670?679, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Pado? and Lapata2009] Sebastian Pado? and Mirella La-
pata. 2009. Cross-lingual annotation projection for
semantic roles. Journal of Artificial Intelligence Re-
search, 36(1):307?340.
[Palmer et al2005] Martha Palmer, Dan Gildea, and
Paul Kingsbury. 2005. The Proposition Bank: A
Corpus Annotated with Semantic Roles. Computa-
tional Linguistics, 31(1).
[Ruppenhofer et al2006] Josef Ruppenhofer, Michael
Ellsworth, Miriam R.L. Petruck, Christopher R.
Johnson, and Jan Scheffczyk. 2006. FrameNet
II: Extended Theory and Practice. Available at
http://framenet.icsi.berkeley.edu/book/book.html.
[Snow et al2008] Rion Snow, Brendan O?Connor,
Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap
and fast?but is it good?: evaluating non-expert an-
notations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association
for Computational Linguistics.
[Von Ahn et al2006] Luis Von Ahn, Mihir Kedia, and
Manuel Blum. 2006. Verbosity: a game for col-
lecting common-sense facts. In Proceedings of the
SIGCHI conference on Human Factors in computing
systems, pages 75?78. ACM.
747
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 170?173,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
KX: A flexible system for Keyphrase eXtraction 
 Emanuele Pianta Fondazione Bruno Kessler Trento, Italy. pianta@fbk.eu 
Sara Tonelli Fondazione Bruno Kessler Trento, Italy. satonelli@fbk.eu     Abstract 
In this paper we present KX, a system for key-phrase extraction developed at FBK-IRST, which exploits basic linguistic annotation combined with simple statistical measures to select a list of weighted keywords from a document. The system is flexible in that it of-fers to the user the possibility of setting pa-rameters such as frequency thresholds for col-location extraction and indicators for key-phrase relevance, as well as it allows for do-main adaptation exploiting a corpus of docu-ments in an unsupervised way. KX is also eas-ily adaptable to new languages in that it re-quires only a PoS-Tagger to derive lexical pat-terns. In the SemEval task 5 ?Automatic Key-phrase Extraction from Scientific Articles?, KX performance achieved satisfactory results both in finding reader-assigned keywords and in the combined keywords subtask.  1 Introduction Keyphrases are expressions, either single words or phrases, describing the most important con-cepts of a document. As such, a list of key-phrases provides an approximate but useful char-acterization of the content of a text and can be used in a number of interesting ways both for human and automatic processing. For example, keyphrases provide a sort of quick summary of a document. This can be exploited not only in automatic summarization tasks, but also to en-able quick topic search over a number of docu-ments indexed according to their keywords, which is more precise and efficient than full-text search. Once the keywords of a document collec-tion are known, they can also be used to calculate semantic similarity between documents and to cluster the texts according to such similarity (Ricca et al 2004). Also, keyword extraction can be used as an intermediate step for automatic sense extraction (Jones et al 2002).  
For these reasons, the keyphrase extraction task proposed at SemEval 2010 raised much at-tention among NLP researchers, with 20 groups participating to the competition. In this frame-work, we presented the KX system, specifically tuned to identify keyphrases in scientific articles. In particular, the challenge comprised two sub-tasks: the extraction of reader-assigned and of author-assigned keyphrases in scientific articles from the ACM digital library. The former are assigned to the articles by annotators, who can choose only keyphrases that occur in the docu-ment, while author-assigned keyphrases are not necessarily included in the text. 2 KX architecture A previous version of the KX system, named KXPat (Pianta, 2009), was developed to extract keyphrases from patent documents in the PatEx-pert project (www.patexpert.org). The sys-tem employed in the SemEval task has additional parameters and has been tailored to identify key-phrases in scientific articles. With KX, the identification of keyphrases can be accomplished with or without the help of a reference corpus, from which some statistical measures are computed in an unsupervised way. We present here the general KX architecture, including the corpus-based pre-processing, even if in the SemEval task the information extracted from the corpus did not contribute as expected (see Section 3).  KX keyphrase extraction combines linguistic and statistical information, similar to (Frantzi et al, 2000) and is based on 4 steps. The first three steps are carried out at corpus level, whereas the fourth one extracts information specific to each single document to be processed. This means that the first three steps require a corpus C, preferably sharing the same domain of the document d from which the keyphrases should be extracted. The fourth step, instead, is focused only on the 
170
document d. The steps can be summarized as follows: Step 1: Extract from C the list NG-c of corpus n-grams, where an n-gram is any sequence of tokens in the text, for instance ?the sys-tem?, ?of the?, ?specifically built?. Step 2: Select from the list NG-c a sub-list of multiword terms MW-c, that is combina-tions of words expressing a unitary con-cept, for instance ?light beam? or ?access control?  Step 3: For each document in C, recognize and mark the multiword terms. Calculate the inverse document frequency (IDF) for all words and multiword terms in the cor-pus. Step 4: Given a document d from which a set of relevant keyphrases should be ex-tracted, count all words and multiword terms and rank them. Step 1 is aimed at building a list of all possible n-grams in C. The maximum length of the selected n-grams can be set by the user. For SemEval, beside one-token n-grams, we select 2-, 3- and 4-grams. Since n-grams occurring only a few times are very unlikely to be useful for keyphrase recognition, they are cut off from the extracted list and excluded for further processing. The fre-quency threshold can be set according to the ref-erence corpus dimensions. For SemEval, we fixed the frequency threshold to 4. In this step, a black-list was also used in order to exclude n-grams containing any of the stopwords in the list. Such stopwords include for example ?every-thing?, ?exemplary?, ?preceding?, etc. In Step 2, we select as multiword terms those n-grams that match certain lexical patterns. To this purpose, we first analyze all n-grams with the MorphoPro morphological analyzer of the TextPro toolsuite (Pianta et al, 2006). Then, we filter out the n-grams whose analysis does not correspond to a predefined set of lexical patterns. For example, one of the patterns admitted for 4-grams is the following: [N]~[O]~[ASPGLU]~[NU]. This means that a 4-gram is a candidate multiword term if it is composed by a Noun, fol-lowed by ?of? or ?for? (defined as O), followed by either an Adjective, Singular noun, Past parti-ciple, Gerund, punctuation (L) or Unknown word, followed by either a Noun or Unknown word. This is matched for example by the 4-gram ?subset [S] of [O] parent [S] peers [N]?.  
Both the lexical categories (e.g. S for singular noun) and the admissible lexical patterns can be defined by the user. In Step 3, multiword terms are recognized by combining local (document) and global (corpus) evidence. To this purpose, we do not exploit as-sociation measures such as Log-Likelihood, or Mutual Information, but a simple frequency based criterion. Two thresholds are defined: MinCorpus, which corresponds to the minimum number of occurrences of an n-gram in a refer-ence corpus, and MinDoc, i.e. the minimum number of occurrences in the current document. KX marks an n-gram in a document as a multiword term if it occurs at least MinCorpus times in the corpus or at least MinDoc times in the document. The two parameters depend on the size of the corpus and the document respectively. In SemEval, we found that the best thresholds are MinDoc=4 and MinCorpus=8. A similar, fre-quency-based, strategy is used to solve ambigui-ties in how sequences of contiguous multiwords should be segmented. For instance, given the sequence ?combined storage capability of sen-sors? we need to decide whether we recognize ?combined storage capability? or ?storage capa-bility of sensors?. To this purpose, we calculate the strength of each alternative collocation as docFrequency * corpusFrequency, and then choose the stronger one. To calculate IDF for each word and multiword term, we use the usual formula:   log( TotDocs / DocsContaningTerm ). In step 4, we take into account a new docu-ment d, possibly not included in C, from which the keyphrases should be extracted. First we rec-ognize and mark multiword terms, through the same algorithm used in Step 3. Note that KX can recognize multiwords also in isolated documents, independently of any reference corpus, by acti-vating only the MinDoc parameter (see above). Then, we count the frequency of words and multiword terms in d, obtaining a first list of keyphrases, ranked according to frequency. Thus, frequency is the baseline ranking parame-ter, based on the assumption that important con-cepts are mentioned more frequently than less important ones.  After the creation of a frequency-based list of keyphrases, various techniques are used to re-rank it according to relevance. In order to find the best ranking mechanism for the type of key-phrases we want to extract, different parameters can be set: ? Inverse document frequency (IDF): this parameter takes into account the fact that a 
171
concept that is mentioned in all documents is less relevant to our task than a concept occurring in few documents ? Keyphrase length: number of tokens in a keyphrase. Concepts expressed by longer phrases are expected to be more specific, and thus more relevant. When this pa-rameter is activated, frequency is multi-plied by the keyphrase length. ? Position of first occurrence: important concepts are expected to be mentioned be-fore less relevant ones. If the parameter is activated, the frequency score will be mul-tiplied by the PosFact factor computed as (DistFromEnd / MaxIndex)pwr2, where MaxIndex is the length of the current document and DistFromEnd is MaxIndex minus the position of the first keyphrase occurrence in the text.  ? Shorter concept subsumption: In the key-phrase list, two concepts can occur, such that one is a specification of the other. Concept subsumption and boosting are used to merge or re-rank such couples of concepts. If a keyphrase is (stringwise) in-cluded in a longer keyphrase with a higher frequency, the frequency of the shorter keyphrase is transferred to the count of the longer one. E.g.  ?grid service discov-ery?=6 and ?grid service?=4 are re-ranked as ?grid service discovery?=10 and ?grid service?=0  ? Longer concept boosting: If a keyphrase is included in a longer one with a lower frequency, the average score between the two keyphrase frequency is computed. Such score is assigned to the less frequent keyphrase and subtracted from the fre-quency score of the higher ranked one. For example, if ?grid service discovery?=4 and ?grid service?=6, the average fre-quency is 5, so that ?grid service discov-ery?=5 and ?grid service? = 6?5=1. This parameter can be activated alone or to-gether with another one that modifies the criterion for computing the boosting. With this second option, the longer keyphrase is assigned the frequency of the shorter one. For example, if ?grid service discovery?=4 and ?grid service?=6, the boosting gives ?grid service discovery?=6 and ?grid serv-ice?=6. After the list of ranked keyphrases is extracted for each document, it is finally post-processed in two steps. The post-processing phase has been 
added specifically for SemEval, because key-phrases do not usually need to be stemmed and acronym expansion is relevant only for the spe-cific genre of scientific articles. For this reason, the two processes are not part of the official sys-tem architecture.  First, acronyms are replaced by the extended form, which is automatically extracted from the current document. The algorithm for acronym detection scans for parenthetical expressions in the text and checks if a preceding text span can be considered a suitable correspondence (Nguyen and Kan, 2007). The algorithm should detect cases in which the acronym appears after or before the extended form, like in ?Immediate Predecessors Tracking (IPT)? and ?IPT (Imme-diate Predecessors Tracking)?. If the acronym and the extended form appear both in the key-phrase list, only the extended form is kept and the acronym frequency is added.  The second step is stemming with the (Porter Stemmer). Then, we check if the list of stemmed keyphrases contains duplicate entries. If yes, we sum the frequencies of the double keyphrases and remove one of the two from the list.   3 Experimental Setup In the SemEval task, 144 training files were made available before the test data release. We split them into a training/development set of 100 documents and a test set of 44 documents, in or-der to find the best parameter combination. Key-phrase assignment is a subjective task and crite-ria for keyphrase identification depend on the domain and on the goal for which the keyphrases are needed. For example in scientific articles longer keyphrases are often more informative than shorter ones, so the parameters for boosting longer concepts are particularly relevant.    We first tested all parameters in isolation to compute the improvement over the frequency-based baseline. Results are reported in Table 1. F1 is computed as the harmonic mean of preci-sion and recall over the 15 top-ranked key-phrases after stemming. We report the combined F1, as computed by the task scorer in order to combine reader-assigned and author-assigned keyword sets.   Parameter F1 (combined) Baseline(MinDoc = 2) 13.63 Baseline(MinDoc = 4) 14.84 +CorpusColloc(small) 13.48     +CorpusColloc(big) 13.33 +IDF 17.98 
172
+KeyphraseLength 16.78 +FirstPosition 16.18 +ShortConcSubsumption 16.03 +LongConcBoost(version1) 14.38 +LongConcBoost(version2) 13.93 MinDoc = 4, +FirstPosition,    +IDF,   +KeyphraseLength,   +ShortConcSubsumption,  +LongConcBoost(version1) 
 25.62 
Table 1: Parameter performance over development set  The parameter scoring the highest improvement over the baseline is IDF. Also the parameters boosting longer keyphrases and those that occur at the beginning of the text are effective. Note that the LongConcBoost parameter achieves bet-ter results in the first version, which has a higher impact on the re-ranking. Surprisingly, using a domain corpus to extract information about multiword terms, as described in Section 2, steps 1 - 3, does not achieve any improvement. This means that KX can better recognize keyphrases in single documents without any corpus refer-ence. Besides, the best setting for MinDoc, the minimum number of multiword occurrences in the current document (see Section 2) is 4. We tested the CorpusColloc parameter using two different reference corpora: one contained the 100 articles of the training set (CorpusColloc small), while the other (CorpusColloc big) in-cluded both the 100 training articles and the 200 scientific publications of the NUS Keyphrase Corpus (Nguyen and Kan, 2007). The perform-ance is worse using the larger corpus than the smaller one, and in both cases it is below the baseline obtained without any reference corpus.  In the bottom row of Table 1, the best pa-rameter combination is reported with the score obtained over the development set. The im-provement over the baseline reaches 11.99 F1. 4 Evaluation In the SemEval task, the system was run on the test set (100 articles) with the best performing parameter combination described in the previous section. The results obtained over the 15 top-ranked keyphrases are reported in Table 2.   Keyphrase type P R F1 Reader-assigned 20.33 25.33 22.56 Combined 23.60 24.15 23.87 Table 2: System performance over test set In the competition, the F1 score over reader-assigned keyphrases was ranked 3rd out of 20 
participants, while the combined measure  achieved the 7th best result out of 20. 5 Conclusions In this work we have described KX, a flexible system for keyphrase extraction, which achieved promising results in the SemEval task 5. The good KX performance is due to its adaptable ar-chitecture, based on a set of parameters that can be tailored to the document type, the preferred keyphrase length, etc. The system can also ex-ploit multiword lists (with frequency) extracted from a reference corpus, even if this feature did not improve KX performance in this specific task. However, this proved to be relevant when applied to keyphrase extraction in the patent do-main, using a large domain-specific corpus of 10.000 very long documents (Pianta, 2009). A limitation of KX in the task was that it ex-tracts only keyphrases already present in a given document, while the author-assigned subtask in the SemEval competition included also key-phrases that do not occur in the text. Another improvement, which is now being implemented, is the extraction of the best parameter combina-tion using machine-learning techniques. References  Jones, S., Lundy, S. and Paynter, G. W. 2002. Interac-tive Document Summarization Using Automati-cally Extracted Keyphrases. In Proc. of the 35th Hawaii International Conference on System Sci-ences. Frantzi, K., Ananiadou, S. and Mima, H. 2000. Automatic recognition of multi-word terms: the C-value/NC-value method. Journal on Digital Li-braries. 3 (2), pp.115-130. Thuy Dung Nguyen and Min-Yen Kan. 2007. Key-phrase Extraction in Scientific Documents. In D.H.-L. Goh et al (eds.): ICADL 2007, LNCS 4822, pp. 317-326. Pianta, E., Girardi, C and Zanoli, R. 2006. The TextPro tool suite. In Proc. of LREC. Pianta, E. 2009. Content Distillation from Patent Ma-terial, FBK Technical Report. Ricca, F., Tonella, P., Girardi, C and Pianta, E. 2004. An empirical study on Keyword-based Web Site Clustering. In Proceedings of the 12th IWPC. PorterStemmer: http://tartarus.org/~martin/PorterStemmer/perl.txt.  
173
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 296?299,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
VENSES++: Adapting a deep semantic processing system to the identification of null instantiations 
Sara Tonelli Fondazione Bruno Kessler Trento, Italy. satonelli@fbk.eu 
Rodolfo Delmonte Universit? Ca? Foscari Venezia, Italy. delmont@unive.it    Abstract The system to spot INIs, DNIs and their anteced-ents is an adaptation of VENSES, a system for semantic evaluation that has been used for RTE challenges in the last 6 years. In the following we will briefly describe the system and then the ad-ditions we made to cope with the new task. In particular, we will discuss how we mapped the VENSES analysis to the representation of frame information in order to identify null instantia-tions in the text. 1 Introduction The SemEval-2010 task for linking events and their participants in discourse (Ruppenhofer et al, 2009) introduced a new issue w.r.t. the Se-mEval-2007 task ?Frame Semantic Structure Ex-traction? (Baker et al, 2007), in that it focused on linking local semantic argument structures across sentence boundaries. Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al, 1998), then the identifica-tion of locally uninstantiated roles (NIs). If these roles are indefinite (INI), they have to be marked as such and no antecedent has to be found. On the contrary, if they are definite (DNI), their coreferents have to be found in the wider dis-course context. The challenge comprised two tasks, namely the full task (semantic role recog-nition and labelling + NI linking) and the NIs only task, i.e. the identification of null instantia-tions and their referents given a test set with gold standard local semantic argument structure. We took part to the NIs only task by modify-ing the VENSES system for deep semantic pro-cessing and entailment recognition (Delmonte et al, 2005). In our approach, we assume that the identification of null instantiations is a complex task requiring different levels of semantic know-ledge and several processing steps. For this rea-
son, we believe that the rich analysis performed by the pipeline architecture of VENSES is par-ticularly suitable for the task, also due to the small amount of training data available and the heterogeneity of NI phenomena.  2 The VENSES system VENSES is a reduced version of GETARUNS (Delmonte, 2008), a complete system for text understanding, whose backbone is LFG theory in its original version (Bresnan, 1982 and 2000). The system produces different levels of analysis, from syntax to discourse. However, three of them contribute most to the NI identification task: the lexico-semantic, the anaphora resolution and the deep semantic module. 2.1 The syntactic and lexico-semantic   module The system produces a c(onstituent)-structure representation by means of a cascade of aug-mented FSA, then it uses this output to map lexi-cal information from a number of different lexica which however contain similar information re-lated to verb/adjective and noun subcategoriza-tion. The mapping is done by splitting sentences into main and subordinate clauses. Other clauses are computed in their embedded position and can be either complement or relative clauses.  The system output is an Augmented Head Dependent Structure (AHDS), which is a fully indexed logical form, with Grammatical Rela-tions and Semantic Roles. The inventory of se-mantic roles we use is however very small ? 35, even though it is partly overlapping the one pro-posed in the first FrameNet project. We prefer to use generic roles rather than specific Frame Ele-ments (FEs) because sense disambiguation at this stage of computation may not be effective.  
296
2.2 The anaphora resolution module The AHDS structure is passed to and used by a full-fledged module for pronominal and ana-phora resolution, which is in turn split into two submodules. The resolution procedure takes care only of third person pronouns of all kinds ? re-ciprocals, reflexives, possessive and personal. Its mechanisms are quite complex, as described in (Delmonte et al, 2006). The first submodule basically treats all pronouns at sentence level ? that is, taking into account their position ? and if they are left free, they receive the annotation ?external?. If they are bound, they are associated to an antecedent?s index; else they might also be interpreted as expletives, i.e. they receive a label that prevents the following submodule to con-sider them for further computation. The second submodule receives as input the ex-ternal pronouns, and tries to find an antecedent in the previous stretch of text or discourse. To do that, the systems computes a topic hierarchy that is built following suggestions by (Sidner and Grosz, 1986) and is used in a centering-like manner.  2.3 The semantic module The output of the anaphora resolution module is used by the semantic module to substitute the pronoun?s head with the antecedent?s head. After this operation, the module produces Predicate-Argument Structures or PAS on the basis of a previously produced Logical Form. PAS are pro-duced for each clause and they separate obliga-tory from non-obligatory arguments, and these from adjuncts and modifiers. Some adjuncts, like spatiotemporal locations, are only bound at propositional level.  3 From VENSES output to NIs identifi-cation and binding After computing PAS information for each sen-tence, we first map the test set gold standard an-notation of frame information to VENSES out-put. Starting from the PAS with frames and FE labels attached to the predicates and the argu-ments, we run a module for DNI/INI spotting and DNI binding. It is composed by two different submodules, one for verbal predicates and one for nominal ones.  3.1 NIs identification and binding with ver-bal predicates As pointed out in (Ruppenhofer et al, 2009), the identification of DNI/INIs includes three main 
steps: i) recognizing that a core role is missing ii) ascertaining if it has a definite interpretation and iii) if yes, finding a role filler for it.  For verbal predicates, the two first steps are ac-complished starting from the PAS structure pro-duced by VENSES and trying to map them with the valence patterns in FrameNet. To this pur-pose, we take into account the list of all valence patterns extracted for every LU and every frame from FrameNet 1.4 and from the training data, in which all possible sequences of FEs (both overtly expressed and null instantiated) are listed with their grammatical functions, coreness status and frequencies. For example, the predicate ?barbecue.v? in the APPLY_HEAT frame is char-acterized by two patterns, both occurring once. In the first, Food is the subject (ext) and Cook is constructionally not instantiated (cni). In the sec-ond, the peripheral FE Time is also present:  ssr(barbecue-v,apply_heat,[[[[food-  c,np,ext],[cook-c,cni,null]],1],[[[time-p,pp,dep],[food-c,np,ext],[cook-c,cni,null]],1]]).  The first step in our computation is selecting for the current predicate those patterns or templates that contain the same number of core arguments of the clause under analysis plus one. This is due to the fact that NIs are always core FEs. For ex-ample, if a test sentence contains the ?barbe-cue.v? lexical unit labelled with the AP-PLY_HEAT frame and only the Food FE is overtly annotated, we look in the template list for all pat-terns in which ?barbecue.v? appears with the Food FE and another implicit core FE (either INI or DNI). If ?barbecue.v? is not present in the template list, we consider the templates of the other verbal lexical units in the same frame. The second step is assessing the licensor of the omission, whether lexical or constructional. Here we only distinguish complement governing predicates and passive constructions. For exam-ple, if ?barbecue.v? is attested in the template list both with an indefinite and with a definite instan-tiation of the Cook FE, we check if it occurs in the passive form in the test sentence. If yes, we infer that Cook has to be labelled as an indefinite null instantiation (INI). Another licensor of the omission could be the imperative form of the verb, which however has not been considered yet by our system. If we assess that the null instantiation is not indefinite, we look for an antecedent of the NI and, if we find it, we label it as a DNI. Other-wise, we don?t encode any information about 
297
omitted roles. The strategy devised for searching for possible referential expressions is as follows:  1. Given the current PAS (with frame labels), look in the previous sentence(s) for compa-rable PAS. Comparable means that the predi-cate is the same or semantically related based on WordNet synsets. 2. If a comparable PAS is found, check if they share at least one argument slot ? typically they should share the subject role. 3. If yes, look for the best head available in that PAS by semantic matching with the FE label as a referent for the DNI label in the current sentence. In case that does not produce any matching, we look into the list of all heads in FrameNet associated to the FE label and se-lect the one present in the PAS that matches. 3.2 NIs identification and binding with nominal predicates In order to identify DNI/INIs of nominal predi-cates, we take into account the History List pro-duced by VENSES in the AHDS analysis, where all nominal heads describing Events, Spatial and Temporal Locations and Body Parts in the document are collected together with their cur-rent sentence ID. Such list is derived from WordNet general nouns.  Based on a computational lexicon of Com-mon Sense Reasoning relations made available with ConceptNet 2.0 by MIT AI Lab (Liu and Singh, 2004), we first process the history list in order to identify the relations between nominal heads in different sentences. Such relations in-clude inheritance and inferences. For instance, if the current sentence contains the nominal heads ?door? or ?window?, they are connected to the ?house? head, if it is present in the History List as a spatial location occurring in a previous sen-tence. For instance, sentence 42 of the test document n. 13 contains the noun ?wall? as lexi-cal unit of the ARCHITECTURAL_PART frame. In the History List, it is classified as a place. Also the noun ?house? in sentence n. 7 (token 7) is classified as a place in the History List. Since ConceptNet alows us to infer a meronymy rela-tion between ?wall? and ?house?, we can derive the following information, saying that ?place? in sent. 45, token 25, is related to ?house?, in sent. 7, token 7:  loc(42-25, place, wall, house-[7-7]). Starting from this information, we then check which core FEs are overtly expressed in the test sentence for the ?wall? lexical unit. As encoded in the FrameNet database, the ARCHITEC-
TURAL_PART frame has two core FEs, namely Part and Whole. Since Part is already present in sentence n. 45, we assume that Whole could be a candidate DNI. After looking up the relations between nominal heads identified in the previous step, we make the hypothesis that ?house? be the antecedent of the Whole DNI. We then check if ?house? appears as a head of the Whole FE either in the FrameNet database or in the training data of the SemEval task in order to perform some semantic verification. If this hypothesis is con-firmed, we finally take the syntactic node headed by the antecedent as the best DNI referent. In our example, ?house? is the head of the node 501, so we generate the following output, in which the Whole FE is identified with the node 501 (headed by ?house?) in sentence 7:      <fe id="s42_f5_e2" name="Whole">   <fenode idref="s7_501"/>  <flag name="Definite_Interpretation">  Note that, in case the antecedent does not appear as the head of the candidate FE, it is discarded and no information about NIs is generated. This is clearly a limit of our approach, because nomi-nal predicates are never assigned an INI label. 4 System output and evaluation The SemEval test data comprise two annotated documents extracted from Conan Doyle?s novels. We report some statistics about the test data with gold standard annotation and a comparison with our system output in Table 1.   Text 1 Text 2 N. of sentences 249 276 Gold standard data N. of DNIs 158 191 N. of INIs 115 245 System output N. of DNIs 35 30 N. of INIs 16 20 F-score 0.0121 Table 1: Comparison between gold standard and   system output  The amount of NIs detected by our system is much lower than the gold standard one, particu-larly for INIs. This depends partly on the fact that no specific strategy for INI detection with nominal predicates has been devised so far, as described in Section 3.2. Another problem is that a lot of DNIs in the gold standard don?t get re-solved, while our system always looks for a re-
298
ferent in case of DNIs and if it is not found, the procedure fails.  The issue of detecting which DNIs are liable not to have an explicit antecedent remains an open problem. In general, Ruppenhofer et al (2009) suggest to treat the DNI identification and binding as a coreference resolution task. How-ever, the only information available is in fact the label of the missing FE. The authors propose to obtain information about the likely fillers of a missing FE from annotated data sets, but the task showed that this procedure could be successful only in case all FE labels are semantically well identifiable: in fact many FE labels are devoid of any specific associated meaning. Furthermore, lexical fillers of a given semantic role in the Fra-meNet data sets can be as diverse as possible. For example, a complete search in the FrameNet database for the FE Charges will reveal heads like ?possession, innocent, actions?, where the significant portion of text addressed by the FE would be in the specification - i.e. "possession of a gun" etc. Only in case of highly specialized FEs there will be some help in the semantic characterization of a possible antecedent. An-other open issue is the notion of context where the antecedent should be searched for, which is lacking an appropriate definition. If we take into account our system results on Text 1, we notice that only 3 DNIs have been identified and linked to the correct antecedent, while the overall amount of exact matches in-cluding INIs is 7. However, in 21 other cases the system correctly identifies a null instantiated role and assigns the right FE label, but it either de-tects an INI instead of a DNI (and vice-versa), or it finds the wrong antecedent for the DNI. A similar performance is achieved on Text 2: no DNI has been linked to the correct antecedent, and in only 8 cases there is an exact match be-tween the INIs identified by the system and those in the gold standard. However, in 18 cases a null instantiation is detected and assigned the correct FE label, even if either the referent or the defi-niteness label is wrong. Some evaluation metrics taking into account the different information lay-ers conveyed by the system would help high-lighting such differences and pointing out the NI identification steps that need to be consolidated. 5 Conclusions In this paper, we have introduced VENSES++, a modified version of the VENSES system for deep semantic processing and entailment detection. 
We described two strategies for the identification of null instantiations in a text, depending on the predicate class (either nominal or verbal).     The system took part to the SemEval task for NIs identification and binding. Even if the pre-liminary results are far from satisfactory, we were able to devise a general strategy for dealing with the task. Only 2 teams took part to the competition, and the first ranked system achieved F1 = 0.0140. This confirms that NI identification is a very challenging issue which can be hardly modeled. Anyway, it deserves further efforts, as various NLP applications could benefit from the effective identification of null instantiated roles, from SRL to coreference resolution and informa-tion extraction.  References  Baker, C., Ellsworth, M. and Erk, K. 2007. Frame Semantic Structure Extraction. In Proceedings of the 4th International Workshop on Semantic Evaluations. Prague, Czech Republic. Baker, C. F., Fillmore, C. J., & Lowe, J. B. 1998. The Berkeley FrameNet project. In Proceedings of COLING-ACL-98, Montreal, Canada. Bresnan, J. 2000. Lexical-functional syntax. Oxford: Blackwell. Bresnan, J. (ed.). 1982. The mental representation of grammatical relations, The MIT Press, Cambridge. Delmonte R., 2008. Computational Linguistic Text Processing ? Lexicon, Grammar, Parsing and Anaphora Resolution, Nova Science, New York. Delmonte, R., Tonelli, S., Piccolino Boniforti, M. A., Bristot, A., and Pianta, E. 2005. VENSES ? A Lin-guistically-based System for Semantic Evaluation. In Proc. of the 1st PASCAL RTE Workshop. Delmonte, R., Bristot, A., Piccolino Boniforti, M.A., and Tonelli, S. 2006. Another Evaluation of Anaphora Resolution Algorithms and a Compari-son with GETARUNS' Knowledge Rich Approach, In Proc. of ROMAND 2006, Trento, pp. 3-10. Grosz, B., and Sidner, C. 1986. Attention, intentions and the structure of discourse. Computational Lin-guistics, 12, 175?204. Liu, H., and Singh, P. 2004. ConceptNet: a practical commonsense reasoning toolkit. At http://web.media.mit.edu/~push/ConceptNet.pdf. Ruppenhofer, J., Sporleder, C., Morante, R., Baker, C. and Palmer, M. 2009. SemEval-2010 Task 10: Linking Events and Their Participants in Dis-course. In Proc. of the HLT-NAACL Workshop on Semantic Evaluations: Recent Achievements and Future Directions. Boulder, Colorado. 
299
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 466?470, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBK: Sentiment Analysis in Twitter with Tweetsted
Md. Faisal Mahbub Chowdhury
FBK and University of Trento, Italy
fmchowdhury@gmail.com
Marco Guerini
Trento RISE, Italy
marco.guerini@trentorise.eu
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Alberto Lavelli
FBK, Trento, Italy
lavelli@fbk.eu
Abstract
This paper presents the Tweetsted system im-
plemented for the SemEval 2013 task on Sen-
timent Analysis in Twitter. In particular, we
participated in Task B on Message Polar-
ity Classification in the Constrained setting.
The approach is based on the exploitation of
various resources such as SentiWordNet and
LIWC. Official results show that our approach
yields a F-score of 0.5976 for Twitter mes-
sages (11th out of 35) and a F-score of 0.5487
for SMS messages (8th out of 28 participants).
1 Introduction
Microblogging is currently a very popular commu-
nication tool where millions of users share opinions
on different aspects of life. For this reason it is a
valuable source of data for opinion mining and sen-
timent analysis.
Working with such type of texts presents chal-
lenges for NLP beyond those typically encountered
when dealing with more traditional texts, such as
newswire data. Tweets are short, the language used
is very informal, with creative spelling and punctua-
tion, misspellings, slang, new words, URLs, genre-
specific terminology and abbreviations, and #hash-
tags. These characteristics need to be handled with
specific approaches.
This paper presents the approach adopted for the
SemEval 2013 task on Sentiment Analysis in Twit-
ter, in particular Task B on Message Polarity Clas-
sification in the Constrained setting (i.e., using the
provided training data only).
The goal of Task B on Message Polarity Classi-
fication is the following: given a message, decide
whether it expresses a positive, negative, or neutral
sentiment. For messages conveying both a positive
and a negative sentiment, whichever is the stronger
sentiment should be chosen.
Two modalities are possible: (1) Constrained (us-
ing the provided training data only; other resources,
such as lexica, are allowed; however, it is not al-
lowed to use additional tweets/SMS messages or ad-
ditional sentences with sentiment annotations); and
(2) Unconstrained (using additional data for train-
ing, e.g., additional tweets/SMS messages or addi-
tional sentences annotated for sentiment). We par-
ticipated in the Constrained modality.
We adopted a supervised machine learning (ML)
approach based on various contextual and seman-
tic features. In particular, we exploited resources
such as SentiWordNet (Esuli and Sebastiani, 2006),
LIWC (Pennebaker and Francis, 2001), and the lex-
icons described in Mohammad et al (2009).
Critical features include: whether the mes-
sage contains intensifiers, adjectives, interjections,
presence of positive or negative emoticons, pos-
sible message polarity based on SentiWordNet
scores (Esuli and Sebastiani, 2006; Gatti and
Guerini, 2012), scores based on LIWC cate-
gories (Pennebaker and Francis, 2001), negated
words, etc.
2 System Description
Our supervised ML-based approach relies on Sup-
port Vector Machines (SVMs). The SVM imple-
mentation used in the system is LIBSVM (Chang
466
and Lin, 2001) for training SVM models and test-
ing. Moreover, in the preprocessing phase we used
TweetNLP (Owoputi et al, 2013), a POS tagger ex-
plicitly tailored for working on tweets.
We adopted a 2 stage approach: (1) during stage
1, we performed a binary classification of messages
according to the classes neutral vs subjective; (2)
in stage 2, we performed a binary classification of
subjective messages according to the classes positive
vs negative. We performed various experiments on
the training and development sets exploring the use
of different features (see Section 2.1) to find the best
configurations for the official submission.
2.1 Feature list
We implement several features divided into three
groups: contextual features, semantic features from
context and semantic features from external re-
sources. The complete list is reported in Table 1.
Contextual features are features computed by
considering only the tokens in the tweets/SMS and
the associated part of speech.
Semantic Features from Context are features
based on words polarity. Emoticons were recog-
nized through a list of emoticons extracted from
Wikipedia1 and then manually labeled as positive or
negative. Negated words (feature n. 18) are any to-
ken occurring between n?t, not, no and a comma, ex-
cluding those tagged as function words. Feature n.
19 captures tokens (or sequences of tokens) labeled
with a positive or negative polarity in the resource
described in Mohammad et al (2009). The intensi-
fiers considered for Feature n. 20 have been identi-
fied by implementing a simple algorithm that detects
tokens containing anomalously repeated characters
(e.g. happyyyyy). Feature n. 21 was computed by
training the system on the training data and predict-
ing labels for the test data, and then using these la-
bels as new features to train the system again.
Semantic Features from external resources in-
clude word classes from the Linguistic Inquiry
and Word Count (LIWC), a tool that calculates
the degree to which people use different cate-
gories of words related to psycholinguistic pro-
cesses (Pennebaker and Francis, 2001). LIWC in-
1http://en.wikipedia.org/wiki/List_of_
emoticons
cludes about 2,200 words and stems grouped into 70
broad categories relevant to psychological processes
(e.g., EMOTION, COGNITION). Sample words are
shown in Table 2.
For each non-zero valued LIWC category of a cor-
responding tweet/SMS, we added a feature for that
category and used the category score as the value
of that feature. We call this LWIC string feature.
Alternatively, we also added a separate feature for
each non-zero valued LIWC category and set 1 as
the value of that feature. This feature is called LWIC
boolean.
We also used words prior polarity - i.e. if a word
out of context evokes something positive or nega-
tive. For this, we relied on SentiWordNet, a broad-
coverage resource that provides polarities for (al-
most) every word. Since words can have multi-
ple senses, we compute the prior polarity of a word
starting from the polarity of each sense and returning
its polarity strength as an index between -1 and 1.
We tested 14 formulae that combine posterior polar-
ities in different ways to obtain a word prior polarity,
as reported in (Gatti and Guerini, 2012).
For the SWNscoresMaximum feature, we select
the prior polarity of the word in a tweet/SMS hav-
ing the maximum absolute score among all words
(of that tweet/SMS). For SWNscoresPolarityCount,
we select the polarity (positive, negative or neutral)
that is assigned to the majority of the words. As
for SWNscoresSum, it corresponds to the sum of
the prior polarities associated with all words in the
tweet/SMS.
3 Experimental Setup
In order to select the best performing feature set,
we carried out several 5-fold cross validation ex-
periments on the training data. We report in Table
3 the best performing feature set. In particular, we
adopted a 2 stage approach:
1. during the first stage we performed a binary
classification of messages according to the
classes neutral vs subjective;
2. in the second stage, we performed a binary
classification of subjective messages according
to the classes positive vs negative.
We opted for a two stage binary classification ap-
proach, since we observed that it produces slightly
467
Contextual Features
1. noOfAdjectives num
2. adjective list string
3. interjection list string
4. firstInterj string
5. lastInterj string
6. bigramList string
7. beginsWithRT boolean
8. hasRTinMiddle boolean
9. endsWithLink boolean
10. endsWithHashtag boolean
11. hasQuestion boolean
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
14. beginsWithPosEmoticon boolean
15. beginsWithNegEmoticon boolean
16. endsWithPosEmoticon boolean
17. endsWithNegEmoticon boolean
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
21. labelPredictedBySystem pos./neg./neut.
Semantic Features from External Resources
22. LIWC string string
23. LIWC boolean string
24. SWNscoresMaximum pos./neg./neut.
25. SWNscoresPolarityCount pos./neg./neut.
26. SWNscoresSum pos./neg./neut.
Table 1: Complete feature list.
LABEL Sample words
CERTAIN all, very, fact*, exact*, certain*, completely
DISCREP but, if, expect*, should
TENTAT or, some, may, possib*, probab*
SENSES observ*, discuss*, shows, appears
SELF we, our, I, us
SOCIAL discuss*, interact*, suggest*, argu*
OPTIM best, easy*, enthus*, hope, pride
ANGER hate, kill, annoyed
INHIB block, constrain, stop
Table 2: Word categories along with sample words
better results than a single stage multi-class ap-
proach (i.e. neutral vs positive vs negative).2 Dif-
ferent combinations of classifiers were explored ob-
taining comparable results. Here we will report only
2The average F-scores (pos and neg) for two stage and single
stage approaches obtained using the official scorer, by training
on the training data and testing on the development data, are
0.5682 and 0.5611 respectively.
the best results.
STAGE 1. The best result for stage (1), neutral vs
subjective, obtained with 5-fold cross validation on
training set only, accounts for an accuracy of 69.6%.
Instead, the best result for stage (1), obtained with
training on training data and testing on development
data, accounts for an accuracy of 72.67%.
The list of best features is reported in Table 3.
Feature selection was performed by starting from a
small set of basic features, and then by adding the
remaining features incrementally.
Contextual Features
2. adjective list string
3. interjection list string
5. lastInterj string
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
Semantic Features from external resources
23. LIWC boolean string
24. SWNscoresMaximum posi./neg./neut.
Table 3: Best performing feature set.
STAGE 2. In stage (2), positive vs negative, we
started from the best feature set obtained from stage
(1) and added the remaining features one by one in-
crementally. In this case, we kept SWNscoresMaxi-
mum without testing again other formulae; in partic-
ular, to compute words prior polarity, we also kept
the first sense approach, that assigns to every word
the SWN score of its most frequent sense and proved
to be the most discriminative in the first stage neutral
vs. subjective. We found that none of the feature sets
produced better results than that obtained using the
best feature set selected from stage (1). So, the best
feature set for stage (2) is unchanged. We trained
the system on the training data and tested it on the
development data, achieving an accuracy of 80.67%.
4 Evaluation
The SemEval task organizers (Wilson et al, 2013)
provided two test sets on which the systems were
to be evaluated: one included Twitter messages, i.e.
the same type of texts included in the training set,
468
while the other comprised SMS messages, i.e. texts
having more or less the same length as the Twitter
data but (supposedly) a different style. We applied
the same model, trained both on the training and the
development set, on the two types of data, without
any specific adaptation.
The Twitter test set was composed of 3,813
tweets. Official results show that our approach
yields an F-score of 0.5976 for Twitter messages
(11th out of 35), while the best performing system
obtained an F-score of 0.6902. The confusion ma-
trix is reported in Table 4, while the score details
in Table 5. The latter table shows that our system
achieves the lowest results on negative tweets, both
in terms of precision and of recall.
gs/pred positive negative neutral
positive 946 101 525
negative 90 274 237
neutral 210 70 1360
Table 4: Confusion matrix for Twitter task
class prec recall F-score
positive 0.7592 0.6018 0.6714
negative 0.6157 0.4559 0.5239
neutral 0.6409 0.8293 0.7230
average(pos and neg) 0.5976
Table 5: Detailed results for Twitter task
The SMS test set for the competition was com-
posed of 2,094 SMS. Official results provided by the
task organizers show that our approach yields an F-
score of 0.5487 for SMS messages (8th out of 28
participants), while the best performing system ob-
tained an F-score of 0.6846. The confusion matrix
is reported in Table 6, while the score details in Ta-
ble 7. Also in this case the recognition of negative
messages achieves by far the poorest performance.
A comparison of the results on the two test sets
shows that, as expected, our system performs bet-
ter on tweets than on SMS. However, precision
achieved by the system on neutral SMS is 0.12
points better on text messages than on tweets.
Interestingly, it appears from the results in Ta-
bles 5 and 7 (and from the distribution of the classes
in the data sets) that there may be a correlation be-
tween the number of tweets/SMS for a particular
class and the performance obtained for such class.
We plan to further investigate this issue.
gs/pred positive negative neutral
positive 320 44 128
negative 66 171 157
neutral 208 64 936
Table 6: Confusion matrix for SMS task
class prec recall F-score
positive 0.5387 0.6504 0.5893
negative 0.6129 0.4340 0.5082
neutral 0.7666 0.7748 0.7707
average(pos and neg) 0.5487
Table 7: Detailed results for SMS task
5 Conclusions
In this paper, we presented Tweetsted, the system de-
veloped by FBK for the SemEval 2013 task on Sen-
timent Analysis. We trained a classifier performing
a two-step binary classification, i.e. first neutral vs.
subjective data, and then positive vs. negative ones.
We implemented a set of features including contex-
tual and semantic ones. We also integrated in our
feature representation external knowledge from Sen-
tiWordNet, LIWC and the resource by Mohammad
et al (2009). On both test sets (i.e., Twitter mes-
sages and SMS) of the constrained modality of the
challenge, we achieved a good performance, being
among the top 30% of the competing systems. In
the near future, we plan to perform an error analysis
of the wrongly classified data to investigate possible
classification issues, in particular the lower perfor-
mance on negative tweets and SMS.
Acknowledgments
This work is supported by ?eOnco - Pervasive knowledge
and data management in cancer care? and ?Trento RISE
PerTe? projects.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
469
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
Lorenzo Gatti and Marco Guerini. 2012. Assessing sen-
timent strength in words prior polarities. In Proceed-
ings of COLING 2012: Posters, pages 361?370, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Saif Mohammad, Bonnie Dorr, and Cody Dunne. 2009.
Generating High-Coverage Semantic Orientation Lex-
icons From Overtly Marked Words and a Thesaurus.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013, Atlanta, Georgia, June.
J. Pennebaker and M. Francis. 2001. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
470
Proceedings of the 8th International Conference on Computational Semantics, pages 342?345,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
A novel approach to mapping
FrameNet lexical units to WordNet synsets
Sara Tonelli, Emanuele Pianta
Abstract
In this paper we present a novel approach to mapping FrameNet
lexical units to WordNet synsets in order to automatically enrich the
lexical unit set of a given frame. While the mapping approaches pro-
posed in the past mainly rely on the semantic similarity between lexical
units in a frame and lemmas in a synset, we exploit the definition of
the lexical entries in FrameNet and the WordNet glosses to find the
best candidate synset(s) for the mapping. Evaluation results are also
reported and discussed.
1 FrameNet and the existing mapping approaches
The FrameNet database [1] is a lexical resource of English describing some
prototypical situations, the frames, and the frame-evoking words or expres-
sions associated with them, the lexical units (LU). Every frame corresponds
to a scenario involving a set of participants, the frame elements, that are typ-
ically the syntactic dependents of the lexical units. The FrameNet resource
is corpus-based, i.e. every lexical unit should be instantiated by at least
one example sentence, even if at the moment the definition and annotation
step is still incomplete for several LUs. FrameNet has proved to be useful
in a number of NLP tasks, from textual entailment to question answering,
but its coverage is still a major problem. In order to expand the resource,
it would be a good solution to acquire lexical knowledge encoded in other
existing resources and import it into the FrameNet database. WordNet [4],
for instance, covers the majority of nouns, verbs, adjectives and adverbs in
the English language, organized in synonym sets called synsets. Mapping
FrameNet LUs to WordNet synsets would automatically increase the num-
ber of LUs per frame by importing all synonyms from the mapped synset,
and would allow to exploit the semantic and lexical relations in WordNet to
enrich the information encoded in FrameNet.
342
Several experiments have been carried out in this direction. Johansson and
Nugues [5] created a feature representation for every WordNet lemma and
used it to train an SVM classifier for each frame that tells whether a lemma
belongs to the frame or not. Crespo and Buitelaar [3] carried out an au-
tomatic mapping of medical-oriented frames to WordNet synsets, trying to
select synsets attached to a LU that were statistically significant in a given
reference corpus. De Cao et al [2] proposed a method to detect the set
of suitable WordNet senses able to evoke the same frame by exploiting the
hypernym hierarchies that capture the largest number of LUs in the frame.
For all above mentioned approaches, a real evaluation on randomly selected
frames is missing, and accuracy was mainly computed over the new lexical
units obtained for a frame, not on a gold standard where one or more synsets
are assigned to every lexical unit in a frame. Besides, it seems that the most
common approach to carry out the mapping relies on some similarity mea-
sures that perform better on richer sets of lexical units.
2 The mapping algorithm
2.1 Motivation
We propose a mapping algorithm that is independent of the number of LUs
in a frame and from the example sentences available. In fact, we believe
that under real-usage conditions, the automatic expansion of LUs is typi-
cally required for frames with a smaller LU set, especially for those with
only one element. In the FrameNet database (v. 1.3), 33 frames out of 720
are described only by one lexical unit, and 63 are described by two. Further-
more, almost 3000 lexical units are characterized only by the lexicographic
definition and are not provided with example sentences. For this reason, we
suggest an alternative approach that makes use of usually unexploited infor-
mation collected in the FrameNet database, namely the definition associated
with every lexical unit.
Since both WordNet glosses and FrameNet definitions are manually writ-
ten by lexicographers, they usually show a high degree of similarity, and
sometimes are even identical. For example, the definition of thicken in the
Change of consistency frame is ?become thick or thicker?, which is identical
to the WordNet gloss of synset n. v#00300319. The thicken lemma occurs
in three WordNet synsets, and in each of them it is the only lemma available,
so no synonymy information could be exploited for the sense disambiguation.
343
2.2 The algorithm
We tried to devise a simple method to map a FrameNet Lexical Unit (LU)
into one or more WordNet synsets. Given a LU L from a frame F, we first
find the set of all synsets containing L (L candidate set, LCandSet). If LCan-
dSet contains only one synset, this is assigned to L. Otherwise, we look for
the synsets in LCandSet whose WN gloss has the highest similarity with the
FrameNet definition of L. We tried two baseline similarity algorithms based
respectively on stem overlap and on a modified version of the Levenshtein
algorithm taking stems as comparison unit instead of characters. Stem over-
lap turned out to perform definitely better than Levehnstein. Then we tried
to improve on simple stem overlap baseline by considering also the other
LUs in F. To this extent, we calculate the set of all synsets linked to any
LU in F (FCandSet). This is exploited in two ways. First, we boost the
similarity score of the synsets in LCandSet with the largest number of links
to other LUs in F (according to FCandSet). Secondly we assign to F the
most common WordNet Domain in FCandSet, and then boost the similarity
score of LCandSet synsets belonging to the most frequent WordNet-Domain
in F. We discard any candidate synset with a similarity score below a MIN
threshold; on the other side, we accept more than one candidate synset if
they have a similarity score higher than a MAX threshold.
3 Evaluation
We created a gold standard by manually mapping 380 LUs belonging to
as many frames to the corresponding WordNet synsets. Then, we divided
our dataset into a development set of 100 LUs and a testset of 280 LUs.
We tested the Levenshtein algorithm and the Stem Overlap algorithm (SO),
then we evaluated the improvement in performance of the latter taking into
account information about the most frequent domain (D) and the most
frequent synsets (Syn). Results are reported in Table 1.
Table 1: Mapping evaluation
Precision Recall F-measure
Levenshtein 0.50 0.49 0.49
Stem Overlap (SO) 0.66 0.56 0.61
SO+Domain (D) 0.66 0.57 0.61
SO+D+Syn 0.71 0.62 0.66
344
We carried out several tests to set the MIN and MAX threshold in order
to get the best F-measure, reported in Table 1. As for precision, the best
performance obtained with SO+D+Syn and a stricter MIN/MAX threshold
scored 0.78 (recall 0.36, f-measure 0.49).
4 Conclusions
We proposed a new method to map FrameNet LUs to WordNet synsets
by computing a similarity measure between LU definitions and WordNet
glosses. To our knowledge, this is the only approach to the task based on
this kind of similarity. The only comparable evaluation available is reported
in [5], and shows that our results are promising. De Cao at al. [2] reported a
better performance, particularly for recall, but evaluation of their mapping
algorithm relied on a gold standard of 4 selected frames having at least 10
LUs and a given number of corpus instantiations.
In the future, we plan to improve the algorithm by shallow parsing the
LU definitions and the WordNet glosses. Besides, we will exploit informa-
tion extracted from the WordNet hierarchy. We also want to evaluate the
effectiveness of the approach focusing on the new LUs to be included in the
existing frames.
References
[1] Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The Berkeley
FrameNet Project. In Proceedings of the 36th ACL Meeting and 17th
ICCL Conference. Morgan Kaufmann, 1998.
[2] Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili.
Combining Word Sense and Usage for modeling Frame Semantics. In
Proceedings of STEP 2008, Venice, Italy, 2008.
[3] Mario Crespo and Paul Buitelaar. Domain-specific English-to-Spanish
Translation of FrameNet. In Proc. of LREC 2008, Marrakech, 2008.
[4] Christiane Fellbaum, editor. WordNet: An Electronic Lexical Database.
MIT Press, 1998.
[5] R. Johansson and P. Nugues. Using WordNet to extend FrameNet cov-
erage. In Proc. of the Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, at NODALIDA, Tartu, 2007.
345
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 54?62,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Desperately Seeking Implicit Arguments in Text
Sara Tonelli
Fondazione Bruno Kessler / Trento, Italy
satonelli@fbk.eu
Rodolfo Delmonte
Universit Ca? Foscari / Venezia, Italy
delmont@unive.it
Abstract
In this paper, we address the issue of automat-
ically identifying null instantiated arguments
in text. We refer to Fillmore?s theory of prag-
matically controlled zero anaphora (Fillmore,
1986), which accounts for the phenomenon of
omissible arguments using a lexically-based
approach, and we propose a strategy for iden-
tifying implicit arguments in a text and finding
their antecedents, given the overtly expressed
semantic roles in the form of frame elements.
To this purpose, we primarily rely on linguis-
tic knowledge enriched with role frequency
information collected from a training corpus.
We evaluate our approach using the test set
developed for the SemEval task 10 and we
highlight some issues of our approach. Be-
sides, we also point out some open problems
related to the task definition and to the general
phenomenon of null instantiated arguments,
which needs to be better investigated and de-
scribed in order to be captured from a compu-
tational point of view.
1 Introduction
In natural language, lexically unexpressed linguistic
items are very frequent and indirectly weaken any
attempt at computing the meaning of a text or dis-
course. However, the need to address semantic in-
terpretation is strongly felt in current advanced NLP
tasks, in particular, the issue of transforming a text
or discourse into a set of explicitly interconnected
predicate-argument/adjunct structures (hence PAS).
The aim of this task would be to unambiguously
identify events and participants and their association
to spatiotemporal locations. However, in order to do
that, symbolic and statistical approaches should be
based on the output representation of a deep parser,
which is currently almost never the case. Current
NLP technologies usually address the surface level
linguistic information with good approximation in
dependency or constituency structures, but miss im-
plicit entities (IEs) altogether. The difficulties to
deal with lexically unexpressed items or implicit en-
tities are related on the one hand to recall problems,
i.e. the problem of deciding whether an item is im-
plicit or not, and on the other hand to precision prob-
lems, i.e. if an implicit entity is accessible to the
reader from the discourse or its context, an appropri-
ate antecedent has to be found. However, a system
able to derive the presence of IEs may be a deter-
mining factor in improving performance of QA sys-
tems and, in general, in Informations Retrieval and
Extraction systems.
The current computational scene has witnessed an
increased interest in the creation and use of semanti-
cally annotated computational lexica and their asso-
ciated annotated corpora, like PropBank (Palmer et
al., 2005), FrameNet (Baker et al, 1998) and Nom-
Bank (Meyers, 2007), where the proposed annota-
tion scheme has been applied in real contexts. In all
these cases, what has been addressed is a basic se-
mantic issue, i.e. labeling PAS associated to seman-
tic predicates like adjectives, verbs and nouns. How-
ever, what these corpora have not made available is
information related to IEs. For example, in the case
of eventive deverbal nominals, information about the
subject/object of the nominal predicate is often im-
plicit and has to be understood from the previous
54
discourse or text, e.g. ?the development of a pro-
totype [? implicit subject]?. As reported by Gerber
and Chai (2010), introducing implicit arguments to
nominal predicates in NomBank would increase the
resource coverage of 65%.
Other IEs can be found in agentless passive con-
structions ( e.g.?Our little problem will soon be
solved ? [? unexpressed Agent ]?1) or as unex-
pressed arguments such as addressee with verbs of
commitment, for example ?I can promise ? that one
of you will be troubled [? unexpressed Addressee]?
and ?I dare swear ? that before tomorrow night he
will be fluttering in our net [? unexpressed Ad-
dressee]?.
In this paper we discuss the issues related to the
identification of implicit entities in text, focussing in
particular on omissions of core arguments of pred-
icates. We investigate the topic from the perspec-
tive proposed by (Fillmore, 1986) and base our ob-
servations on null instantiated arguments annotated
for the SemEval 2010 Task 10, ?Linking Events and
Their Participants in Discourse? (Ruppenhofer et al,
2010)2. The paper is structured as follows: in Sec-
tion 2 we detail the task of identifying null instan-
tiated arguments from a theoretical perspective and
describe related work. In Section 3 we briefly in-
troduce the SemEval task 10 for identifying implicit
arguments in text, while in Section 4 we detail our
proposal for NI identification and binding. In Sec-
tion 5 we give a thorough description of the types of
null instantiations annotated in the SemEval data set
and we explain the behavior of our algorithm w.r.t.
such cases. We also compare our results with the
output of the systems participating to the SemEval
task. Finally, we draw some conclusions in Section
6.
2 Related work
In this work, we focus on null complements, also
called pragmatically controlled zero anaphora (Fill-
more, 1986), understood arguments or linguistically
1Unless otherwise specified, the following examples are
taken from the data sets made available in the SemEval 2010
task ?Linking Events and Their Participants in Discourse?.
Some of them have been slightly simplified for purposes of ex-
position.
2http://semeval2.fbk.eu/semeval2.php?
location=tasks&taskid=9
unrealized arguments. We focus on Fillmore?s the-
ory because his approach represents the backbone of
the FrameNet project, which in turn inspired the Se-
mEval task we will describe below. Fillmore (1986)
shows that in English and many other languages
some verbs allow null complements and some oth-
ers don?t. The latter require that, when they ap-
pear in a sentence, all core semantic roles related
to the predicate are expressed. For example, sen-
tences like ?Mary locked ??? or ?John guaranteed
??? are not grammatically well-formed, because they
both require two mandatory linguistically inherent
participants. Fillmore tries to explain why seman-
tic roles can sometimes be left unspoken and what
constraints help the interpreter recover the missing
roles. He introduces different factors that can in-
fluence the licensing of null complements. These
can be lexically-based, (semantically close predi-
cates like ?promise? and ?guarantee? can license the
omission of the theme argument in different cases),
motivated by the interpretation of the predicate (?I
was eating ?? licenses a null object because it has
an existential interpretation) and depending on the
context (see for example the use of impress in an
episodic context like ?She impressed the audience?,
where the null complement is not allowed, compared
to ?She impresses ? every time? in habitual interpre-
tation; examples from Ruppenhofer and Michaelis
(2009)).
The fact that Fillmore explains the phenomenon
of omissible arguments with a lexically-based ap-
proach implies that from his perspective neither a
purely pragmatic nor a purely semantic approach
can account for the behavior of omissible arguments.
For example, he argues that some verbs, such as to
lock will never license a null complement, no matter
in which pragmatic context they are used. Besides,
there are synonymous verbs which behave differ-
ently as regards null complementation, which Fill-
more sees as evidence against a purely semantic ex-
planation of implicit arguments.
Another relevant distinction drawn in Fillmore
(1986) is the typology of omitted arguments, which
depends on the type of licensor and on the interpre-
tation of the null complement. Fillmore claims that
with some verbs the missing complement can be re-
trieved from the context, i.e. it is possible to find a
referent previously mentioned in the text / discourse
55
and bearing a definite, precise meaning. These cases
are labeled as definite null complements or instantia-
tions (DNI) and are lexically specific in that they ap-
ply only to some predicates. We report an example
of DNI in (1), taken from the SemEval task 10 data
set (see Section 3). The predicate ?waiting? has an
omitted object, which we understand from the dis-
course context to refer to ?I?.
(1) I saw him rejoin his guest, and I crept quietly
back to where my companions were waiting ?
to tell them what I had seen.
DNIs can also occur with nominal predicates, as
reported in (2), where the person having a thought,
the baronet, is mentioned in the preceding sentence:
(2) Stapleton was talking with animation, but the
baronet looked pale and distrait. Perhaps the
thought of that lonely walk across the
ill-omened moor was weighing heavily upon
his mind.
In contrast to DNIs, Fillmore claims that with
some verbs and in some interpretations, a core ar-
gument can be omitted without having a referent
expressing the meaning of the null argument. The
identity of the missing argument can be left un-
known or indefinite. These cases are labeled as in-
definite null complements or instantiations (INI) and
are constructionally licensed in that they apply to
any predicate in a particular grammatical construc-
tion. See for example the following cases, where the
omission of the agent is licensed by the passive con-
struction:
(3) One of them was suddenly shut off ?.
(4) I am reckoned fleet of foot ?.
Cases of INI were annotated by the organizers of
the SemEval task 10 also with nominal predicates,
as shown in the example below, where the perceiver
of the odour is left unspecified:
(5) Rank reeds and lush, slimy water-plants sent
an odour ? of decay and a heavy miasmatic
vapour.
Few attempts have been done so far to automati-
cally deal with the recovery of implicit information
in text. One of the earliest systems for identifying
extra-sentential arguments is PUNDIT by Palmer et
al. (1986). This Prolog-based system comprises a
syntactic component for parsing, a semantic compo-
nent, which decomposes predicates into component
meanings and fills their semantic roles with syntactic
constituents based on a domain-specific model, and
a reference resolution component, which is called
both for explicit constituents and for obligatory im-
plicit constituents. The reference resolution process
is based on a focus list with all potential pronominal
referents identified by the semantic component. The
approach, however, has not been evaluated on a data
set, so we cannot directly compare its performance
with other approaches. Furthermore, it is strongly
domain-dependent.
In a case study, Burchardt et al (2005) propose
to identify implicit arguments exploiting contex-
tual relations from deep-parsing and lexico-semantic
frame relations encoded in FrameNet. In particu-
lar, they suggest converting a text into a network of
lexico-semantic predicate-argument relations con-
nected through frame-to-frame relations and recur-
rent anaphoric linking patterns. However, the au-
thors do not implement and evaluate this approach.
Most recently, Gerber and Chai (2010) have pre-
sented a supervised classification model for the re-
covery of implicit arguments of nominal predicates
in NomBank. The model features are quite different
from those usually considered in standard SRL tasks
and include among others information from Verb-
Net classes, pointwise mutual information between
semantic arguments, collocation and frequency in-
formation about the predicates, information about
parent nodes and siblings of the predicates and dis-
course information. The authors show the feasibility
of their approach, which however relies on a selected
group of nominal predicates with a large number of
annotated instances.
The first attempt to evaluate implicit argument
identification over a common test set and consider-
ing different kinds of predicates was made by Rup-
penhofer et al (2010). Further details are given in
the following section.
56
Data set Sentences Frame inst. Frame types Overt FEs DNIs (resolved) INIs
Train 438 1,370 317 2,526 303 (245) 277
Test 525 1,703 452 3,141 349 (259) 361
Table 1: Data set statistics from SemEval task 10
3 SemEval 2010 task 10
The SemEval-2010 task for linking events and their
participants in discourse (Ruppenhofer et al, 2010)
introduced a new issue w.r.t. the SemEval-2007
task ?Frame Semantic Structure Extraction? (Baker
et al, 2007), in that it focused on linking local se-
mantic argument structures across sentence bound-
aries. Specifically, the task included first the identi-
fication of frames and frame elements in a text fol-
lowing the FrameNet paradigm (Baker et al, 1998),
then the identification of locally uninstantiated roles
(NIs). If these roles are indefinite (INI), they have
to be marked as such and no antecedent has to be
found. On the contrary, if they are definite (DNI),
their coreferents have to be found in the wider dis-
course context. The challenge comprised two tasks,
namely the full task (semantic role recognition and
labeling + NI linking) and the NIs only task, i.e. the
identification of null instantiations and their refer-
ents given a test set with gold standard local seman-
tic argument structure. In this work, we focus on the
latter task.
The data provided to the participants included a
training and a test set. The training data comprised
438 sentences from Arthur Conan Doyle?s novel
?The Adventure of Wisteria Lodge?, manually an-
notated with frame and INI/DNI information. The
test set included 2 chapters of the Sherlock Holmes
story ?The Hound of Baskervilles? with a total of
525 sentences, provided with gold standard frame
information. The participants had to i) assess if a lo-
cal argument is implicit; ii) decide whether it is an
INI or a DNI and iii) in the second case, find the
antecedent of the implicit argument. We report in
Table 1 some statistics about the provided data sets
from Ruppenhofer et al (2010). Note that overt FEs
are the explicit frame elements annotated in the data
set.
Although 26 teams downloaded the data sets,
there were only two submissions, probably depend-
ing on the intrinsic difficulties of the task (see dis-
cussion in Section 5). The best performing system
(Chen et al, 2010) is based on a supervised learn-
ing approach using, among others, distributional se-
mantic similarity between the heads of candidate
referents and role fillers in the training data, but
its performance is strongly affected by data sparse-
ness. Indeed, only 438 sentences with annotated
NIs were made available in the training set, which
is clearly insufficient to capture such a multifaceted
phenomenon with a supervised approach. The sec-
ond system participating in the task (Tonelli and
Delmonte, 2010) was an adaptation of an exist-
ing LFG-based system for deep semantic analysis
(Delmonte, 2009), whose output was mapped to
FrameNet-style annotation. In this case, the major
challenge was to cope with the classification of some
NI phenomena which are very much dependent on
frame specific information, and can hardly be gener-
alized in the LFG framework.
4 A linguistically motivated proposal for
NI identification and binding
In this section, we describe our proposal for dealing
with INI/DNI identification and evaluate our output
against SemEval gold standard data. As discussed in
the previous section, existing systems dealing with
this task suffer on the one hand from a lack of train-
ing data and on the other hand from the dependence
of the task on frame annotation, which makes it diffi-
cult to adapt existing unsupervised approaches. We
show that, given this state of the art, better results
can be achieved in the task by simply developing an
algorithm that reflects as much as possible the lin-
guistic motivations behind NI identification in the
FrameNet paradigm. Our approach is divided into
two subtasks: i) identify INIs/DNIs and ii) for each
DNI, find the corresponding referent in text.
We develop an algorithm that incorporates the fol-
lowing linguistic information:
FE coreness status Null instantiated arguments as
defined in FrameNet are always core arguments, i.e.
57
they are central to the meaning of a frame. Since
the coreness status of the arguments is encoded in
FrameNet, we limit our search for an NI only if a
core frame element is not overtly expressed in the
text.
Incorporated FEs Although all lexical units be-
longing to the same frame in the FrameNet database
are characterized by the same set of core FEs, a fur-
ther distinction should be introduced when dealing
with NIs identification. For example, in PERCEP-
TION ACTIVE, several predicates are listed, which
however have a different behavior w.r.t. the core
Body part FE. ?Feel.v?, for instance, is underspec-
ified as regards the body part perceiving the sensa-
tion, so we can assume that when it is not overtly
expressed, we have a case of null instantiation. For
other verbs in the same frame, such as ?glance.v? or
?listen.v?, the coreness status of Body part seems to
be more questionable, because the perceiving organ
is already implied by the verb meaning. For this rea-
son, we argue that if Body part is not expressed with
?glance.v? or ?listen.v?, it is not a case of null instan-
tiation. Such FEs are defined as incorporated in the
lexical unit and are encoded as such in FrameNet.
Excludes and Includes relation In FrameNet,
some information about the relationship between
certain FEs is encoded. In particular, some FEs are
connected by the Excludes relation, which means
that they cannot occur together, and others by the
Requires relation, which means that if a given FE
is present, then also the other must be overtly or
implicitly present. An example of Excludes is the
relationship between the FE Entity 1 / Entity 2 and
Entities, because if Entity 1 and Entity 2 are both
present in a sentence, then Entities cannot be co-
present. Conversely, Entity 1 and Entity 2 stand in a
Requires relationship, because the first cannot occur
without the second. This kind of information can
clearly be helpful in case we have to automatically
decide whether an argument is implicit or is just not
present because it is not required.
INI/DNI preference Ruppenhofer and Michaelis
(2009) suggest that omissible arguments in particu-
lar frames tend to be always interpreted as definite or
indefinite. For example, they report that in a sample
from the British National Corpus, the interpretation
for a null instantiated Goal argument is definite in
97.5% of the observed cases. We take this feature
into account by considering the frequency of an im-
plicit argument being annotated as definite/indefinite
in the training set.
The algorithm incorporating all this linguistic in-
formation is detailed in the following subsection.
4.1 INI/DNI identification
In a preliminary step, we collect for each frame the
list of arguments being annotated as DNI/INI with
the corresponding frequency in the training set. For
example, in the CALENDRIC UNIT frame, the Whole
argument has been annotated 11 times as INI and
5 times as DNI. Some implicit frame elements oc-
cur only as INI or DNI, for example Goal, which is
annotated 14 times as DNI and never as INI in the
ARRIVING frame. This frequency list (FreqList)
is collected in order to decide if candidate null in-
stantiations have to be classified as DNI or INI.
We consider each sentence in the test data pro-
vided with FrameNet annotation, and for each pred-
icate p annotated with a set of overt frame elements
FEs, we run the first module for DNI/INI identi-
fication. The steps followed are reported in Algo-
rithm 1. We first check if the annotated FEs con-
tain all core frame elements C listed in FrameNet for
p. If the two sets are identical, we conclude that no
core frame element can be implicit and we return an
empty set both for DNI and INI . For example, in
the test sentence (6), the BODY MOVEMENT frame
appears in the sentence with its two core frame el-
ements, i.e. Body part and Agent. Therefore, no
implicit argument can be postulated.
(6) Finally [she]Agent openedBODY MOVEMENT [her
eyes]Body part again.
If the core FEs in C are not all overtly expressed
in FEs, we run two routines to check if the miss-
ing FEs CandNIs are likely to be null instantiated
elements. First, we discard all candidate NIs that ap-
pear as incorporated FEs for the given p. Second, we
discard as well candidate NIs if they are excluded by
the overtly annotated FEs.
The last steps of the algorithm are devoted to de-
ciding if the candidate null instantiation is definite
or indefinite. For this step, we rely on the observa-
tions collected in FreqList. In particular, for each
58
candidate c we check if it was already present as INI
or DNI in the training set. If yes, we label c accord-
ingly. In case c was observed both as INI and as
DNI, the most probable label is assigned based on
its frequency in the training set.
Input: TestSet with annotated core FEs;
FreqList
Output: INI and DNI for p
foreach p ? TestSet do
extract annotated core FEs;
extract set C of core FEs for p in FrameNet;
if C ? FEs then
DNI = ?;
INI = ?;
else
C \ FEs = CandNIs;
foreach c ? CandNIs do
if c is incorporated FE of p then
delete c
foreach fe ? FEs do
if fe excludes c then
delete c
end
foreach nip ? FreqListp do
if c = nip then
if nip is only dnip then
c ? DNI
if nip is only inip then
c ? INI
if nip is inip and nip is dnip
then
if Freq(inip) >
Freq(dnip) then
c ? INI
else
c ? DNI
end
end
return(INI);
return(DNI);
end
Algorithm 1: DNI/INI identification
4.2 DNI binding
Given that both the supervised approach exploited
by Chen et al (2010) and the methodology pro-
posed in Tonelli and Delmonte (2010) based on
deep-semantic parsing achieved quite poor results
in the DNI-binding task, we devise a third approach
that relies on the observed heads of each FE in the
training set and assigns a relevance score to each
candidate antecedent.
We first collect for each FE the list of heads
Htrain assigned to FE in the training set, and we ex-
tract for each head htrain ? Htrain the correspond-
ing frequency fhtrain . Then, for each dni ? DNI
identified with Algorithm 1 in the test set, we collect
all nominal heads Htest occurring in a window of
(plus/minus) 5 sentences and we assign to each can-
didate head htest ? Htest a relevance score relhtest
w.r.t. dni computed as follows:
relhtest =
fhtrain
dist(sentdni, senthtest)
(7)
where fhtrain is the number of times h has been
observed in the training set with a FE label, and
dist(sentdni, senthtest) is the distance between the
sentence where the dni has been detected and the
sentence where the candidate head htest occurs (0 ?
dist(sentdni, senthtest) ? 5).
The best candidate head for dni is the one with
the highest relhtest , given that it is (higher) than 0.
The way we compute the relevance score is based on
the intuition that, if a head was frequently observed
for FE in the training set, it is likely that it is a good
candidate. However, the more distant it occurs from
dni, then less probable it is as antecedent.
5 Evaluation and error analysis
We present here an evaluation of the system output
on test data. We further comment on some difficult
aspects of the task and suggest some solutions.
5.1 Results
Evaluation consists of different layers, which we
consider separately. The first task was to decide
whether an argument is implicit or not. We were
able to identify 53.8% of all null instantiated ar-
guments in text, which is lower than the recall of
63.4% achieved by SEMAFOR (Chen et al, 2010),
the best performing system in the challenge. How-
ever, in the following subtask of deciding whether an
implicit argument is an INI or a DNI, we achieved
an accuracy of 74.6% (vs. 54.7% of SEMAFOR,
59
even if our result is based on fewer proposed clas-
sifications). Note that the majority-class accuracy
reported by Ruppenhofer et al (2010) is 50.8%.
In Table 2 we further report precision, recall and
F1 scores computed separately on all DNIs and all
INIs automatically detected. Precision corresponds
to the percentage of null instantiations found (either
INI or DNI) that are correctly labeled as such, while
recall indicates the amount of INI or DNI that were
correctly identified compared to the gold standard
ones. Our approach does not show significant dif-
ferences between the result obtained with INIs and
DNIs, while the evaluation of SEMAFOR (between
parenthesis) shows that its performance suffers from
low recall as regards DNIs and low precision as re-
gards INIs.
P R F1
DNI 0.39 (0.57) 0.43 (0.03) 0.41 (0.06)
INI 0.46 (0.20) 0.38 (0.61) 0.42 (0.30)
Table 2: Evaluation of INI/DNI identification.
SEMAFOR performance between parenthesis.
Another evaluation step concerns the binding of
DNIs with the corresponding antecedents by apply-
ing the equation reported in Section 4.2. Results are
shown in Table 3:
P R F1
DNI 0.13 (0.25) 0.06 (0.01) 0.08 (0.02)
Table 3: Evaluation of DNI resolution. SEMAFOR per-
formance between parenthesis.
Although the binding quality still needs to be im-
proved, two main factors have a negative impact on
our performance, which do not depend on our al-
gorithm: first, 9% of the DNIs we bound to an an-
tecedent don?t have a referent in the gold standard.
Second, 26% of the wrong assignments concern an-
tecedents found for the Topic frame element in test
sentences where the STATEMENT frame has been
annotated together with the overtly expressed core
FE Message. In all these gold cases, Topic is not
considered null instantiated if the Message FE is ex-
plicit in the clause. Therefore, we can conclude that
the mistake done by our algorithm depends on the
missing Excludes relation between Topic and Mes-
sage, i.e. a rule should be introduced saying that
one of the two roles is redundant (and not null in-
stantiated) if the other is overtly expressed.
5.2 Open issues related to our approach
Even if with a small set of rules our approach
achieved state-of-the-art results in the SemEval task,
our performance clearly requires further improve-
ments. Indeed, we currently rely only on the back-
ground knowledge about core FEs from FrameNet,
combined with statistical observations about role
fillers acquired from the training set. Additional
morphological, syntactic, semantic and discourse in-
formation could be exploited in different ways. For
example, since the passive voice of a verb can con-
structionally license INIs, this kind of information
would greatly improve our performance with verbal
predicates (i.e. 46% of all annotated predicates in
the test set).
As for nominal predicates, consider for example
sentence (8) extracted from the test set:
(8) ?Excuse the admirationJUDGMENT [of a
connoisseur]Evaluee,? said [he]Cognizer.
In this case, ?admiration? is a nominal predicate
with two explicit FEs, namely Evaluee and Cog-
nizer. The JUDGMENT frame includes also the Rea-
son core FE, which can be a candidate for a null in-
stantiation. In fact, it is annotated as INI in the gold
standard data, because in the previous sentences a
reason for such admiration is not mentioned. How-
ever, this could have been annotated as DNI as well,
if only some specific quality of the person had been
previously introduced. This shows that the current
sentence does not present any inherent characteris-
tic motivating the presence of a definite instantia-
tion. In this case, a strategy based on some kind of
history list may be very helpful. This could con-
tain, for example, all subjects and direct objects pre-
viously mentioned in text and selected according to
some relevance criteria, as in (Tonelli and Delmonte,
2010). A further improvement may derive from the
integration of an anaphora resolution step, as first
proposed by Palmer et al (1986) and more recently
by Gerber and Chai (2010).
60
5.3 Open issues related to the task
Other open issues are related to the specification of
the task and to the nature of implicit entities, which
make it difficult to account for this phenomenon
from a computational point of view. We report be-
low the main issues that need to be tackled:
INI Linking: Table 1 shows that 28% of DNIs
in the test set are not linked to any referent. This
puts into question one of the main assumptions of
the task, that is the connection between a definite
instantiation and a referent. In the test set, there are
also 14 cases of indefinite null instantiations (out of
361) that are provided with a referent. Consider for
example the following sentence with gold standard
annotation, in which the INI label Path is actually
instantiated and refers to ?we?:
(9) (We)Path allowed [him]Theme to passTRAVERSING
before we had recovered our nerve.
This again may be a controversial annotation choice,
since the annotation guidelines of the task reported
that ?in cases of indefinite omission, there need not
be any overt mention of an indefinite NP in the lin-
guistic context nor does there have to be a referent
of the kind denoted by the omitted argument in the
physical discourse setting? (Ruppenhofer, 2010).
Position of referent: Although we suggested that
the History List may represent a good starting point
for finding antecedents to DNIs, searching only in
the context preceding the current predicate is not
enough because the referent can occur after such
predicate. Also, the predicate with a DNI and the
referent can be divided by a very large text span. In
the test data, 38% of the DNIs referent occur in the
same sentence of the predicate, while 14% are men-
tioned after that (in a text span of max. 4 sentences).
Another 38% of DNIs are resolved in a text span
preceding the current predicate of max. 5 sentences,
while the rest has a very far antecedent (up to 116
sentences before the current predicate). The notion
of context where the antecedent should be searched
for is clearly lacking an appropriate definition.
Diversity of lexical fillers: In general, it is pos-
sible to successfully obtain information about the
likely fillers of a missing FE from annotated data
sets only in case all FE labels are semantically well
identifiable: in fact many FE labels are devoid of
any specific associated meaning. Furthermore, lex-
ical fillers of a given semantic role in the FrameNet
data sets can be as diverse as possible. For exam-
ple, a complete search in the FrameNet database for
the FE Charges will reveal heads like ?possession,
innocent, actions?, where the significant portion of
text addressed by the FE would be in the specifica-
tion - i.e. ?possession of a gun? etc. Only in case of
highly specialized FEs there will be some help in the
semantic characterization of a possible antecedent.
6 Conclusions
In this paper, we have described the phenomenon
of null instantiated arguments according to the
FrameNet paradigm and we have proposed a strat-
egy for identifying implicit arguments and find-
ing their antecedents, if any, using a linguistically-
motivated approach. We have evaluated our system
using the test set developed for the SemEval task
10 and we have discussed some problems in our ap-
proach affecting its performance. Besides, we have
also pointed out some issues related to the task defi-
nition and to the general phenomenon of null instan-
tiated arguments that make the identification task
challenging from a computational point of view. We
have shed some light on the syntactic, semantic and
discourse information that we believe are necessary
to successfully handle the task.
In the future, we plan to improve on our binding
approach by making our model more flexible. More
specifically, we currently treat DNI referents occur-
ring before and after the sentence containing the
predicate as equally probable. Instead, we should
penalize less those preceding the predicate because
they are more frequent in the training set. For this
reason, the number of observations for the candi-
date head and the distance should be represented
as different weighted features. Another direction to
explore is to extend the training set to the whole
FrameNet resource and not just to the SemEval
data set. However, our approach based on the ob-
servations of lexical fillers is very much domain-
dependent, and a larger training set may introduce
too much variability in the heads. An approach ex-
ploiting some kind of generalization, for example by
linking the fillers to WordNet synsets as proposed by
(Gerber and Chai, 2010), may be more appropriate.
61
References
Collin F. Baker, Charles J. Fillmore, and J. B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of Coling/ACL, Montreal, Quebec, Canada.
C. F. Baker, M. Ellsworth, and K. Erk. 2007. Semeval-
2007 task 10: Frame semantic structure extraction. In
Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 99?104,
Prague, CZ, June.
Aljoscha Burchardt, Annette Frank, and Manfred Pinkal.
2005. Building text meaning representations from
contextually related frames - a case study. In Proceed-
ings of the Sixth International Workshop on Computa-
tional Semantics, Tilburg, NL.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame Argument
Resolution with Log-Linear Models. In Proceedings
of SemEval-2010: 5th International Workshop on Se-
mantic Evaluations, pages 264?267, Uppsala, Swe-
den. Association for Computational Linguistics.
Rodolfo Delmonte. 2009. Understanding Implicit Enti-
ties and Events with Getaruns. In Proceedings of the
IEEE International Conference on Semantic Comput-
ing, pages 25?32, Berkeley, California.
Charles J. Fillmore. 1986. Pragmatically Controlled
Zero Anaphora. In V. Nikiforidou, M. Vanllay,
M. Niepokuj, and D. Felder, editors, Proceedings of
the XII Annual Meeting of the Berkeley Linguistics So-
ciety, Berkeley, California. BLS.
Matthew Gerber and Joyce Y. Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nominal
Predicates. In Proceedings of the 48th annual meet-
ing of the Association for Computational Linguistics
(ACL-10), pages 1583?1592, Uppsala, Sweden. Asso-
ciation for Computational Linguistics.
Adam Meyers. 2007. Annotation guidelines for Nom-
Bank - noun argument structure for PropBank. Tech-
nical report, New York University.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, and J. Dowding. 1986. Recovering im-
plicit information. In Proceedings of ACL 1986, pages
96?113.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Computational Linguistics, 31.
Josef Ruppenhofer and Laura A. Michaelis. 2009.
Frames predict the interpretation of lexical omissions.
Submitted.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin F. Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of SemEval-2010: 5th
International Workshop on Semantic Evaluations.
Josef Ruppenhofer, 2010. Annotation guidelines used for
Semeval task 10 - Linking Events and Their Partici-
pants in Discourse. (manuscript).
Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++:
Adapting a deep semantic processing system to the
identification of null instantiations. In Proceedings
of SemEval-2010: 5th International Workshop on Se-
mantic Evaluations, Uppsala, Sweden.
62
JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 15?24,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Key-concept extraction from French articles with KX
Sara Tonelli1 Elena Cabrio2 Emanuele Pianta1
(1) FBK, via Sommarive 18, Povo (Trento), Italy
(2) INRIA, 2004 Route des Lucioles BP93, Sophia Antipolis cedex, France
satonelli@fbk.eu, elena.cabrio@inria.fr, pianta@fbk.eu
R?SUM?
Nous pr?sentons une adaptation du syst?me KX qui accomplit l?extraction non supervis?e et
multilingue des mots-cl?s, pour l?atelier d??valuation francophone en fouille de textes (DEFT
2012). KX s?lectionne une liste de mots-cl?s (avec leur poids) dans un document, en combinant
des annotations linguistiques de base avec des mesures statistiques. Pour l?adapter ? la langue
fran?aise, un analyseur morphologique pour le Fran?ais a ?t? ajout? au syst?me pour d?river les
patrons lexicaux. De plus, des param?tres comme les seuils de fr?quence pour l?extraction de
collocations, et les index de relevance des concepts-cl?s ont ?t? calcul?s et fix?s sur le corpus
d?apprentissage. En concernant les pistes de DEFT 2012, KX a obtenu de bons r?sultats (Piste 1 -
avec terminologie : 0.27 F1 ; Piste 2 : 0.19 F1) en demandant un effort r?duit pour l?adaptation
du domaine et du langage.
ABSTRACT
We present an adaptation for the French text mining challenge (DEFT 2012) of the KX system
for multilingual unsupervised key-concept extraction. KX carries out the selection of a list of
weighted keywords from a document by combining basic linguistic annotations with simple
statistical measures. In order to adapt it to the French language, a French morphological
analyzer (PoS-Tagger) has been added into the extraction pipeline, to derive lexical patterns.
Moreover, parameters such as frequency thresholds for collocation extraction and indicators for
key-concepts relevance have been calculated and set on the training documents. In the DEFT
2012 tasks, KX achieved good results (i.e. 0.27 F1 for Task 1 - with terminological list, and 0.19
F1 for Task 2) with a limited additional effort for domain and language adaptation.
MOTS-CL?S : Extraction de mots-cl?s, patrons linguistiques, terminologie.
KEYWORDS: Key-concept extraction, linguistic patterns, terminology.
1 Introduction
Key-concepts are simple words or phrases that provide an approximate but useful characterization
of the content of a document, and offer a good basis for applying content-based similarity
functions. In general, key-concepts can be used in a number of interesting ways both for human
and automatic processing. For instance, a quick topic search can be carried out over a number
15
of documents indexed according to their key-concepts, which is more precise and efficient
than full-text search. Also, key-concepts can be used to calculate semantic similarity between
documents and to cluster the texts according to such similarity (Ricca et al, 2004). Furthermore,
key-concepts provide a sort of quick summary of a document, thus they can be used as an
intermediate step in extractive summarization to identify the text segments reflecting the content
of a document. (Jones et al, 2002), for example, exploit key-concepts to rank the sentences in a
document by relevance, counting the number of key-concept stems occurring in each sentence.
In the light of the increasing importance of key-concepts in several applications, from search
engines to digital libraries, a recent task for the evaluation of key-concept extraction was also
proposed at SemEval-2010 campaign (Kim et al, 2010)
In this work, we present an adaptation of the KX system for multilingual key-concept extraction
(Pianta et Tonelli, 2010) for the French text mining challenge (DEFT 2012) task. A preliminary
version of KX for French took part in the DEFT 2011 campaign on ?Abstract ? article matching?
(Tonelli et Pianta, 2011), and achieved good performances in both tracks (0.990 and 0.964 F1
respectively).
Compared to the previous version of KX, we have now integrated into the extraction pipeline a
French morphological analyzer (Chrupala et al, 2008). This allows us to exploit morphological
information while selecting candidate key-concepts, while in the version used at DEFT 2011 the
selection was made using regular expressions and black lists.
The paper is structured as follows : in Section 2 we detail the architecture of KX (i.e. our key-
concepts extraction tool), providing an insight into its parameters configuration. In Section 3 we
present the setting defined and adopted for the DEFT 2012 task, while in Section 4 we report the
system performances on the training and on the test sets. Finally, we draw some conclusions, and
discuss future improvements of our approach in Section 5.
2 Key-concept extraction with KX
This section describes in details the basic KX architecture for unsupervised key-concept extraction.
KX can handle texts in several languages (i.e. English, Italian, French, Finnish and Swedish), and
it is distributed with the TextPro NLP Suite1 (Pianta et al, 2008). KX architecture is the same
across all languages, except for the module selecting multiword expressions, that is based on PoS
tags (this is the only language-dependent part of the system). In order to perform this selection,
a morphological analyzer/PoS tagger has been integrated for each of the five languages, and
some selection rules have been manually defined. More details on the French rules are reported
in Section 2.2 and in Section 3.
2.1 Pre-processing of the reference corpus
If a domain corpus is available, the extraction of key-concepts from a single document can be
preceded by a pre-processing step, during which key-concepts are extracted from the corpus and
their inverse document frequency (IDF) at corpus level is computed by applying the standard
formula :
1
http://textpro.fbk.eu/
16
I DFk = log NDFk
where N is the number of documents in the corpus, and DFk is the number of documents in thecorpus that contain the key-concepts k. The I DF of a rare term tends to be high, while the I DF
of a frequent one is likely to be low. Therefore, I DF may be a good indicator for distinguishing
between common, generic words and specific ones, which are good candidates for being a
key-concept. For DEFT 2012, we have used as a reference corpus all the documents contained in
the training and in the test sets (468 documents in total).
2.2 Key-concept extraction
Figure 1 shows KX work-flow for the key-concept extraction process : starting from a document,
a list of key-concepts ranked by relevance is provided as the output of the system. The same
work-flow applies both to i) the extraction of key-concepts from a single document, and to ii) the
extraction of different statistics including IDF from a reference corpus, which can be optionally
used as additional information when processing a single document. For more information, see
above and Section 2.3.
FIG. 1 ? Key-concept extraction workflow with KX
As a first step, the system takes a document in input and tokenize the text. Then, all possible
n-grams composed by any token sequence are extracted, for instance ??clipse de soleil?, ?tous les?,
?ou chacun?. The user can set the max length of the selected n-grams : for DEFT 2012 we set such
length to six.
Then, from the n-gram list a sublist of multiword expressions (MWE) is derived, i.e. combinations
of words expressing a unitary concept, for example ?proc?s de travail? or ??conomie politique?.
17
In the selection step, the user can choose to rely only on local (document) evidence or to make
use also of global (corpus) evidence. As for the first case, a frequency threshold called MinDoc
can be set, which corresponds to the minimum number of occurrences of n-grams in the current
document. If a reference corpus is also available, another threshold can be added, MinCorpus,
which corresponds to the minimum number of occurrences of an n-gram in the corpus. KX marks
an n-gram in a document as a multiword term if it occurs at least MinCorpus times in the
corpus or at least MinDoc times in the document. The two parameters depend on the size of the
reference corpus and the document respectively. In our case, the corpus was the set of documents
used in the training and in the test set (see Section 2.1).
A similar, frequency-based, strategy is used to solve ambiguities in how sequences of contiguous
multiwords should be segmented. For instance, given the sequence ?retour des bonnes mani?res?
we need to decide whether we recognize ?retour des bonnes? or ?bonnes mani?res?. To this
purpose, the strength of each alternative MWE is calculated as follows, and then the stronger
one is selected.
St reng thcol loc = docF requenc y ? corpusF requenc y
In the next step, the single words and the MWEs are ranked by frequency to obtain a first list
of key-concepts. Thus, frequency is the baseline ranking parameter, based on the assumption
that important concepts are mentioned more frequently than less important ones. Frequency is
normalized by dividing the number of key-concept occurrences by the total number of tokens in
the current document.
As shown in Figure 1, the first key-concepts list is obtained by applying black and white lists
almost at every step of the process. A black list is applied to discard n-grams containing one of
the language-specific stopwords defined by the user, for example ?avons?, ?peut?, ?puis?, ?parce?.
Also single words corresponding to stopwords are discarded when the most frequent tokens
are included into the first key-concept list. For example, in French we may want to exclude all
key-concepts containing the words ?toi?, ?tr?s?, ?finalement?, etc.
When deriving multiword expressions (MWEs) from the n-gram list, KX applies another selection
strategy. This selection is crucial because only MWEs are selected as candidate key-concepts, since
they correspond to combinations of words expressing a unitary concept, for example ?r?gime de
despotisme familial? or ?reproduction mat?rielle?. The n-grams are analyzed with the Morfette
morphological analyzer (Chrupala et al, 2008) in order to select as multiword expressions only
the n-grams that match certain lexical patterns (i.e. part-of-speech). This is the so-called linguistic
filter. For example, one of the patterns admitted for 3-grams is the following :
[SP]? [O]? [SP]
This means that a 3-gram is a candidate multiword term if it is composed by a single or plural
noun (S and P respectively), followed by a preposition (defined as O), followed by another noun.
This is matched for example by the 3-gram ?proc?s [S] de [O] travail [S]?.
Finally, black and white lists can be manually compiled also for key-concepts, to define expressions
that should never be selected as relevant key-concepts, as well as terms that should always be
included in the key-concept rank. For example, the preposition ?de? is very frequent in documents,
so it can happen that it is selected as single-word key-concept. In order to avoid this, ?de? can be
included in the key-concept black list.
18
2.3 First key-concept ranking
Different techniques are used to re-rank the frequency-based list of key-concepts obtained in
the previous step according to their relevance. If a reference corpus is available, as in our case,
additional information can be used to understand which key-concepts are more specific to a
document, and therefore are more likely to be relevant for such document.
In order to find the best ranking mechanism, and to tailor it to the type of key-concepts we want
to extract, the following parameters can be set :
Key-concept IDF : This parameter takes into account the fact that, given a data collection, a
concept that is mentioned in many documents is less relevant to our task than a concept oc-
curring in few documents. To activate it, a reference corpus must undergo a pre-processing
step in which the key-concepts are extracted from each document in the corpus, and the
corresponding inverse document frequency (IDF) is computed, as described in Section 2.1.
When this parameter is activated, for each key-concept found in the current document, its
I DF computed over the reference corpus is retrieved and multiplied by the key-concept
frequency at document level.
Key-concept length : Number of tokens in a key-concept. Concepts expressed by longer phrases
are expected to be more specific, and thus more informative. When this parameter is
activated, the frequency is multiplied by the key-concept length. For example, if ?expression
verbale? has frequency 6 and ?expression verbale des ?motions? has frequency 5, the
activation of the key-concept length parameter gives ?expression verbale? = 6 * 2 = 12 and
?expression verbale des ?motions? = 5 * 4 = 20. In this way, the 4-gram is assigned a higher
ranking than the 2-gram.
Position of first occurrence : Important concepts are expected to be mentioned before less
relevant ones. If the parameter is activated, the frequency score will be multiplied by the
PosFact factor computed as :
PosFact =
DistF romEnd
Max Index
2
where Max Index is the length of the current document, and DistF romEnd is Max Index
minus the position of the first key-concept occurrence in the text.
A configuration file allows the user to independently activate such parameters. The key-concept
relevance is then calculated by multiplying the normalized frequency of a key-concept by the
score obtained by each active parameter. We eventually obtain a ranking of key-concepts ordered
by relevance. The user can also set the number of top ranked key-concepts to consider as best
candidates.
2.4 Final key-concept ranking
Section 2.3 described the first set of ranking strategies, that can be optionally followed by another
set of operations to adjust the preliminary ranking. Again, such operations can be independently
activated through a separate configuration file. The parameters have been introduced to deal
19
with the so-called nested key-concepts (Frantzi et al, 2000), i.e. those that appear within other
longer candidate key-concepts. After the first ranking, which is still influenced by the key-concept
frequency, nested (shorter) key-concepts tend to have a higher ranking than the containing
(longer) ones, because the former are usually more frequent than the latter. However, in some
settings, for example in scientific articles, longer key-concepts are generally preferred over
shorter ones because they are more informative and specific. In such cases, the user may want
to adjust the ranking in order to give preference to longer key-concepts and to reduce or set to
zero the score of nested key-concepts. These operations are allowed by activating the following
parameters :
Shorter concept subsumption : It happens that two concepts can occur in the key-concept list,
such that one is a specification of the other. Concept subsumption and boosting (see below)
are used to merge or rerank such couples of concepts. If a key-concept is (stringwise)
included in a longer key-concept with a higher frequency-based score, the score of the
shorter key-concept is transferred to the count of the longer one. For example, if ?expression
verbale? has frequency 4 and ?expression verbale des ?motions? has frequency 6, by activa-
ting this parameter the relevance of ?expression verbale des ?motions? is 6 + 4 = 10, while
the relevance of ?expression verbale? is set to zero. The idea behind this strategy is that
nested key-concepts can be deleted from the final key-concept list without losing relevant
information, since their meaning is nevertheless contained in the longer key-concepts.
Longer concept boosting : This parameter applies in case a key-concept is (stringwise) included
in a longer key-concept with a lower relevance. Its activation should better balance the
ranking in order to take into account that longer n-grams are generally less frequent, but
not less relevant, than shorter ones. The parameter is available in two different versions,
having different criteria for computing such boosting. With the first option, the average
score between the two key-concepts relevance is computed. Such score is assigned to the
less frequent key-concepts and subtracted from the frequency score of the higher ranked
one. With the second option, the longer key-concepts is assigned the frequency of the shorter
one. In none of the two variants key-concepts are deleted from the relevance list, as it
happens by activating the Shorter concept subsumption parameter.
For example, if ?expression verbale? has score 6 and ?expression verbale des ?motions? has
score 4, by activating the first option of this parameter the relevance of ?expression verbale?
becomes 6 - ((6 + 4) / 2) = 1, while the relevance of ?expression verbale des ?motions? is
set to 5, i.e. (6 + 4) / 2 .
With the second option, both the relevance of ?expression verbale des ?motions? and of
?expression verbale? is set to 6.
The examples above show that these parameters set by the user can change the output of the
ranking by deleting some entries and boosting some others. After applying one cycle of subsump-
tion/boosting, the order of the concepts can dramatically change, producing the conditions for
further subsumption/boosting of concepts. The user can set the number of iterations for the
application of this re-ranking mechanism, and each cycle increases the impact of the re-ranking
on the key-concept list. The parameters can be activated together and in different combinations.
If all parameters are set, the short concept subsumption procedure is applied first, then the longer
concept boosting is run on the output of the first re-ranking, so that the initial relevance-based
list goes through two reordering steps.
20
3 KX configuration for the DEFT 2012 task
As introduced before (Section 2.2), to port KX to the French language and, in particular, to adapt
it to the DEFT 2012 task, the Morfette morphological analyzer (Chrupala et al, 2008) has been
integrated into the system, to select as multiword expressions only the n-grams matching certain
lexical patterns (i.e. part-of-speech). Such lexical patterns are learned on the gold standard, and
manually formalized and added into the system as a linguistic filter. In order to speed up this
process, we took advantage of the set of lexical patterns defined for Italian, and we checked
if they could be applied also for French. Moreover, new patterns were added in compliance
with DEFT training data requirements. For example, the following n-grams have been added as
allowed patterns (i.e. candidate multiword terms) :
? 6-grams : [SP]-[O]-[SP]-[O]-[S]-[JK], where S and P correspond to singular or plural nouns,
O to the prepositions (also in combination with the article), and J and K to singular or plural
adjectives (e.g. ?soul?vement [S] des [O] M?tis [S] dans l? [O] Ouest [S] canadien [J]?) ;
? 5-grams : [SP]-[O]-[SP]-[O]-[P], (e.g. ?gestion [S] des [O] troupeaux [P] de [O] rennes [P]?) ;
? 4-grams : [SP]-[JK]-[0]-[S], (e.g. ?histoire [S] canonique [J] de la [0] traduction [S]?) ;
? 3-grams : [S]-[SP]-[JK], (e.g. ?fran?ais [S] langue [S] premi?re [J]?).
We compiled black lists both for common French stopwords (containing e.g. articles, prepositions,
a few numbers, and functional verbs) and stopphrases (prepositional structures such as ?au sujet
de?, ?en dehors de?, ?en face de?), since we do not want them to be selected as key-concepts.
As for the IDF value mentioned in Section 2.1, it has been computed for 86,419 key-concepts
extracted from DEFT 2012 training and test set. Among the key-concepts with the highest
IDF (i.e. best candidates for final selection), we find ?inversions culturelles?, ?hypertextualit??,
?am?nagement terminologique?. These are key-concepts that occur only in one document of the
reference corpus. Among the key-concepts with a low IDF, instead, we find very common terms
and expressions such as ?rapport?, ?partie? and ?exemple?, which are likely to be discarded as
key-concepts.
The standard KX architecture has also been adapted to one of the two tracks of DEFT 2012,
namely the one in which a terminological list was provided. For that track, the set of documents
to be processed was accompanied by a list of domain terminology. By comparing the gold key-
concepts in the training set with this list, we observed that all terms in the terminology were
also gold key-concepts. Therefore, we modified KX so that, in the final re-ranking, the candidate
key-concepts being present in the terminology list were significantly boosted. This adaptation
lead to an improvement of almost 0.8 P/R/F1 on the test set (see Section 4).
4 Evaluation
Since KX does not require supervision, we used the training set to identify the best parameter
setting, which was then applied in the test phase. The results obtained on the training and on the
test set are discussed in the following subsections.
21
4.1 System evaluation on the training set
We report in Table 1 the best parameter setting on the training documents. Note that the reported
evaluation measures have been computed using our own scorer, which counts as correct each
key-concept exactly matching with the gold standard (case-insensitive). The results reported for
the test set, instead, have been computed by the task organizers with another scorer, which may
apply a slightly different strategy.
We extracted for each document the top k key-concepts, with k being the number of key-
concepts assigned to each document in the training set (this number may vary from document to
document). For this reason, Precision and Recall are the same.
Task 1 : Task 2 :
with terminology w/o terminology
KX Parameters
1. MinCorpus 8 8
2. MinDoc 3 3
3. Use corpusId f Yes Yes
4. Multiply relevance by key-concept length Yes Yes
5. Consider position of first occurrence No No
6. Shorter concept subsumption No No
7. Longer concept boosting No No
8. Boost key-concepts in terminology list Yes No
P/R/F1 on training set F1 0.18 F1 0.15
TAB. 1 ? Best parameter combination for training set
The results obtained on the training set suggest that the key-concepts required in this task should
not be too specific, since the parameters aimed at preferring specific (i.e. longer) key-concepts
are not activated in the best performing setting (we refer to parameters n. 6 and 7 in the above
Table). Also the position of the first key-concept occurrence is not relevant, since the parameter
n. 5 is not part of the best setting. This is in contrast with KX setting used for Semeval 2010
(Pianta et Tonelli, 2010). In that case, boosting the relevance of specific key-concepts, and of
those occurring in the article abstract had a positive effect on the final performance. Note also
that the performance measured on French documents in DEFT is around 0.10 points lower
than that achieved at Semeval on English scientific articles. We believe that this is not due to
a different system performance on the two languages, but rather on the evaluation strategy,
because Semeval scorer required the key-concepts to be stemmed and took into account some
syntactic variations of the same key-concept (Kim et al, 2010).
4.2 System evaluation on the test set
For each task, we submitted three system runs, testing different parameter combinations. Specifi-
cally, for Task 1 (with terminological list), the three runs had the following configurations :
1. Parameter setting reported in Section 4.1 (with boosting of key-concepts in terminology
22
list) ;
2. Parameter setting as in Section 4.1 but Consider position of first occurrence activated (with
boosting of key-concepts in terminology list) ;
3. Parameter setting as in Section 4.1 but terminology list is not taken into account.
As for Task 2 (without terminological list), the three runs had the following configurations :
1. Parameter setting reported in Section 4.1 ;
2. Parameter setting as in Section 4.1 but Consider position of first occurrence activated ;
3. Parameter setting reported in Section 4.1 but system run only on article abstracts.
Task 1 : Task 2 :
with terminology w/o terminology
KX Run 1 0.2682 0.1880
KX Run 2 0.2737 0.1901
KX Run 3 0.1976 0.1149
TAB. 2 ? KX performance on test set
We decided to activate the parameter Consider position of first occurrence, even if it was not
part of the best performing setting in the training phase, because it achieved good results in
the Semeval 2010 challenge on English. The results confirm that, in both tasks, this yielded a
(limited) improvement.
In both tasks, the third run was used to exploit configurations that were not tested in the training
phase. In Task 1, the third run was obtained without taking into account the terminology list.
The difference in performance between Run 1 and Run 3 confirms that this information is indeed
very relevant. In Task 2, the third run concerned the extraction of key-concepts only from the
abstracts, and not from the whole articles. Also in this case, the initial hypothesis that the abstract
may contain all relevant key-concepts proved to be wrong.
At DEFT 2012, 10 teams submitted at least one run in Task 1, and 9 teams in Task 2. The best
performing run of KX was ranked 6th out of 10 in Task 1 and 5th out of 9 in Task 2. In Task 1
the mean F1 for the best submission of each team was 0.3575, the median was 0.3321 and the
standard deviation 0.2985, with system performances ranging from 0.0428 (lowest performance)
to 0.9488 (best run). In Task 2 the mean F1 for the best submission of each team was 0.2045,
the median was 0.1901 and the standard deviation 0.1522, with system performances ranging
from 0.0785 (lowest performance) to 0.5874 (best run).
These results show that the use of terminology significantly improves the overall system perfor-
mance, as confirmed in Table 2. However, KX seems to be more competitive in the second task
compared to other systems. This confirms that KX strength lies in its domain-independence and
in the fact that is does not require any additional information to achieve a good performance.
Furthermore, we believe that the second task is more realistic than the first one : in a real
application scenario, it is unlikely that a terminological list, containing only the key-concepts to
be identified, is actually available.
23
5 Conclusions
In this paper, we presented the French version of the KX system, and we described the experiments
we carried out for our participation at DEFT 2012. KX achieved good results with few adjustments
of the parameter setting and a limited additional effort for domain and language adaptation. Our
system requires no supervision and its English and Italian versions are distributed as a standalone
key-concept extractor. Its extension, which takes into account a reference terminological list,
proved to be effective and achieved a moderate improvement in the first task of the evaluation
challenge.
A limitation of our system is that it is not able to identify key-concepts that are not present
in the document. This kind of concepts amounted to around 20% of the gold key-concepts in
the training set, and this feature strongly affected the outcome of our evaluation. A strategy to
exploit external knowledge sources to extract common subsumers of the given key-concepts may
be investigated in the future.
Acknowledgements
The development of KX has been partially funded by the European Commission under the contract
number FP7-248594, PESCaDO project.
R?f?rences
CHRUPALA, G., DINU, G. et van GENABITH, J. (2008). Learning Morphology with Morfette. In
Proceedings of the 6th International Conference on Languages Resources and Evaluations (LREC
2008), Marrakech, Morocco.
FRANTZI, K., ANANIADOU, S. et MIMA, H. (2000). Automatic recognition of multi-word terms :
the C-value/NC-value. Journal of Digital Libraries, 3(2):115?130.
JONES, S., LUNDY, S. et PAYNTER, G. (2002). Interactive Document Summarisation Using Auto-
matically Extracted Keyphrases. In Proceedings of the 35th Hawaii International Conference on
System Sciences, Hawaii.
KIM, S. N., MEDELYAN, O., KAN, M.-Y. et BALDWIN, T. (2010). SemEval-2010 Task 5 : Automatic
keyphrase extraction from scientific articles. In Proceedings of SemEval 2010, Task 5 : Keyword
extraction from Scientific Articles, Uppsala, Sweden.
PIANTA, E., GIRARDI, C. et ZANOLI, R. (2008). The TextPro tool suite. In Proceedings of the 6th
Language Resources and Evaluation Conference (LREC), Marrakech, Morocco.
PIANTA, E. et TONELLI, S. (2010). KX : A flexible system for Keyphrase eXtraction. In Proceedings
of SemEval 2010, Task 5 : Keyword extraction from Scientific Articles, Uppsala, Sweden.
RICCA, F., TONELLA, P., GIRARDI, C. et PIANTA, E. (2004). An empirical study on keyword-based
web site clustering. In Proceedings of the 12th IWPC, Bari, Italy.
TONELLI, S. et PIANTA, E. (2011). Matching documents and summaries using key-concepts. In
Proceedings of DEFT 2011, Montpellier, France.
24
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 40?48,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Making Readability Indices Readable
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Ke Tran Manh
Charles University, Prague, CR
ketranmanh@gmail.com
Emanuele Pianta
FBK, Trento, Italy
pianta@fbk.eu
Abstract
Although many approaches have been pre-
sented to compute and predict readability of
documents in different languages, the infor-
mation provided by readability systems often
fail to show in a clear and understandable way
how difficult a document is and which aspects
contribute to content readability. We address
this issue by presenting a system that, for a
given document in Italian, provides not only
a list of readability indices inspired by Coh-
Metrix, but also a graphical representation
of the difficulty of the text compared to the
three levels of Italian compulsory education,
namely elementary, middle and high-school
level. We believe that this kind of represen-
tation makes readability assessment more in-
tuitive, especially for educators who may not
be familiar with readability predictions via su-
pervised classification. In addition, we present
the first available system for readability as-
sessment of Italian inspired by Coh-Metrix.
1 Introduction
The task of readability assessment consists in quan-
tifying how difficult a text is for a reader. This kind
of assessment has been widely used for several pur-
poses, such as evaluating the reading level of chil-
dren and impaired persons and improving Web con-
tent accessibility for users with low literacy level.
While indices and methodologies for readabil-
ity assessment of English have been widely investi-
gated, and research on English readability has been
continuously progressing thanks to advances in psy-
cholinguistic research and in natural language pro-
cessing, only limited efforts have been made to ex-
tend current approaches to other languages. An
adaptation of the basic Flesch Index (Flesch, 1946)
exists for many languages, but only in few cases
more sophisticated approaches have been adopted,
taking into account recent studies on text cohesion,
readability and simplification.
With this work, we aim at bridging the gap be-
tween the standard approach to Italian readability
based on the Gulpease index (following the same
criteria of the Flesch Index) and the more advanced
approaches to readability currently available for En-
glish and based on psycholinguistic principles. In
particular, we present a set of indices for Ital-
ian readability covering different linguistics aspects,
from the lexical to the discourse level, which are in-
spired by Coh-Metrix (Graesser et al, 2004). We
make this analysis available online, but we differ-
entiate our service from that of Coh-Metrix1 in that
we provide a graphical representation of the aspects
affecting readability, comparing a document with
the average indices of elementary, middle and high-
school level texts. This makes readability analysis
really intuitive, so that a user can straightforwardly
understand how difficult a document is, and see if
some aspects (e.g. lexicon, syntax, discourse) affect
readability more than others.
Our research goals are: i) to analyze the adequacy
of the Gulpease index for discriminating between
the readability levels of texts used for teaching and
testing in the Italian school practice, ii) to implement
an adaptation of Coh-Metrix indices for Italian, iii)
to make the readability analysis available online and
1http://cohmetrix.memphis.edu
40
understandable to naive users.
2 Related work
The first formulas to automatically compute the dif-
ficulty of a text were devised for English, starting
from the Flesch Index (Flesch, 1946), followed by
the Gunning Fog (Gunning, 1952), the SMOG index
(McLaughlin, 1969) and the Fleisch-Kincaid (Kin-
caid et al, 1975). These metrics combine factors,
such as word and sentence length, that are easy to
compute and that should approximate the linguistic
elements that impact on readability. Similar indexes
have been proposed also for other languages such
as German (Bamberger and Vanecek, 1984), French
(Kandel and Moles, 1958) and Spanish (Huerta,
1959).
The first readability formula for Italian, the
Flesch-Vacca (Franchina and Vacca, 1986), was in-
troduced in the early seventies and was based on an
adaptation of the Flesch index (Flesch, 1946). How-
ever, it has been widely replaced by the Gulpease
index (Lucisano and Piemontese, 1988), which was
introduced in the eighties by the Gruppo Universi-
tario Linguistico Pedagogico (GULP) of the Univer-
sity of Rome. The Gulpease index takes into account
the length of a word in characters rather than in syl-
lables, which proved to be more reliable for assess-
ing the readability of Italian texts. The index ranges
from 0 (lowest readability) to 100 (maximum read-
ability).
In recent years, research on English readability
has progressed toward more sophisticated models
that take into account difficulty at syntactic, seman-
tic and discourse level thanks to advances in psy-
cholinguistic accounts of text processing (Graesser
et al, 2004) and to the availability of a wide range
of NPL tools (e.g. dependency and constituency
parsers, anaphora resolution systems, etc.) and re-
sources (e.g. WordNet). However, for many other
languages current approaches for readability assess-
ment still rely on few basic factors. A notable ex-
ception is the Coh-Metrix-PORT tool (Scarton et al,
2009; Aluisio et al, 2010), which includes 60 read-
ability measures for Brazilian Portuguese inspired
by the Coh-Metrix (Graesser et al, 2004).
A different approach has been followed by the de-
velopers of the DeLite system for German (Glo?ckner
et al, 2006; von der Bru?ck et al, 2008): the tool
computes a set of indices measuring the linguistic
complexity of a document through deep parsing and
outputs a final readability score obtained by apply-
ing the k-nearest neighbor algorithm based on 3,000
ratings from 300 users.
As for Italian, the only work aimed at improving
on the performance of standard readability indices
has been proposed by Dell?Orletta et al (2011), who
implement a set of lexical and morpho-syntactic fea-
tures to distinguish between normal and simplified
newspaper articles in a binary classification task.
Our work differs from their approach in that we
choose a different type of corpus for a different au-
dience (i.e. children with different proficiency levels
vs. adults with low literacy skills or mild cognitive
impairment). We also enrich their feature set in that
our indices capture also semantic and discourse as-
pects of a text. In this respect, we take advantage
of cognitive and psycholinguistic evidence support-
ing the idea behind Coh-Metrix that high textual co-
herence and cohesion result in improved readability
with any type of readers (Beck et al, 1984s; Cataldo
and Oakhill, 2000; Linderholm et al, 2000), and that
discourse connectives and spatio-temporal informa-
tion in a text strongly contribute to cohesion.
3 The corpus
Our goal is to develop a system that can be used in
real scenarios, for instance by teachers who want to
assess if a text is understandable by children in a
certain class. Therefore, we avoid collecting a cor-
pus with documents showing different degrees of
simplification according to a ?controlled? scenario.
This strategy was adopted for instance by Crossley
et al (2011), who compared different readability in-
dices using news texts manually simplified into ad-
vanced, intermediate and beginning difficulty level.
Also the experiments on readability assessment of
Portuguese texts by Scarton et al (2009) were con-
ducted on a corpus of news articles manually simpli-
fied by a linguist according to a natural and a strong
simplification level.
Our approach is different in that we take texts
used for teaching and comprehension exercises in
Italian schools and divide them into three classes,
according to the class level in which they are em-
41
Class 1 Class 2 Class 3
(63 docs) (55 docs) (62 docs)
Doc. length 530 776 1085
in tokens (? 273) (? 758) (? 1152)
Gulpease 55.92 53.88 50.54
(? 6.35) (? 6.13) (? 6.98)
Table 1: Corpus statistics. All values are averaged. StDev
is reported between parenthesis.
ployed. This means that in Class 1 we collect
all documents written for children in elementary
schools (aged 6-10), in Class 2 we collect texts
for children in middle schools (aged 11-13), and in
Class 3 we gather documents written for teenagers
in high schools (aged 14-18). The classes contain
respectively 63, 55 and 62 documents.
As shown in Table 1, the average length of the
documents increases with the school level. How-
ever, the single documents show high variability,
especially those in Class 3. Texts have been se-
lected so as to represent the most common genres
and knowledge domains in school texts. Thus, the
corpus contains a balanced selection of both narra-
tive and expository texts. The latter belong mostly to
the following domains: history, literature, biology,
physics, chemistry, geography and philosophy. The
corpus includes also all official text comprehension
tests used in Italy in the INVALSI school proficiency
evaluation campaign2.
4 Readability assessment based on
Gulpease
We first analyze the behaviour of the Gulpease in-
dex in our corpus, in order to assess if this measure
is adequate for capturing the readability of the doc-
uments. We compute the index by applying to each
document the standard formula:
Gulpdoc = 89 +
(300 ? #sentsdoc) ? (10 ? #charsdoc)
#tokensdoc
Average Gulpease and standard deviation for each
class are reported in Table 1.
2National Institute for the Evaluation of the Educational
System by the Ministry of Research and University, http:
//www.invalsi.it/invalsi/index.php
Fig. 1 shows the distribution of the Gulpease in-
dex in the corpus. On the x axis the document id is
reported, with document 1?63 belonging to Class 1
(elementary), document 64?118 to Class 2 (middle)
and 119?180 to Class 3 (high school). On the y axis,
the Gulpease index is reported, ranging from 41 (i.e.
the lowest readability level in the corpus) to 87 (i.e.
highest readability).
Although the highest readability score is obtained
by a document of Class 1, and the lowest scores
concern documents in Class 3, the three classes do
not seem to be separable based solely on Gulpease.
In particular, documents in Class 2, written for stu-
dents in middle school, show scores partly overlap-
ping with Class 1 and partly with Class 3. Further-
more, the great majority of the documents in the cor-
pus have a Gulpease index included between 50 and
60 and the average Gulpease does not differ consis-
tently across the three classes (Table 1).
Figure 1: Distribution of Gulpease index in the corpus.
Document id on x axis, and Gulpease on y axis
For children in the elementary school, a text with
a Gulpease index between 0 and 55 usually corre-
sponds to the frustration level. For children in the
middle school, the frustration level is reached with a
Gulpease index between 0 and 35. For high-school
students, this level is reached with Gulpease being
between 0 and 10.3
3More information on how to interpret Gulpease for each
of the three classes is reported at http://www.eulogos.
net/ActionPagina_1045.do
42
4.1 Coh-Metrix for English
Coh-Metrix is a computational tool available on-
line at http://cohmetrix.memphis.edu that
can analyze an English document and produce a list
of indices expressing the cohesion of the text. These
indices have been devised based on psycholinguistic
studies on the mental representation of textual con-
tent (McNamara et al, 1996) and address various
characteristics of explicit text, from lexicon to syn-
tax, semantics and discourse, that contribute to the
creation of this representation. Although the tool re-
lies on widely used NLP techniques such as PoS tag-
ging and parsing, there have been limited attempts to
employ it in studies on automatic assessment of text
cohesion. Nevertheless, recent works in the NLP
community investigating the impact of entity grids
(Barzilay and Lapata, 2008) or of discourse relations
(Pitler and Nenkova, 2008) on text coherence and
readability go in the same direction as research on
Coh-Metrix, in that they aim at identifying the lin-
guistic features that best express readability at syn-
tactic, semantic and discourse level.
The indices belonging to Coh-Metrix are divided
into five main classes:
? General Word and Text Information: The in-
dices in this class capture the correlation be-
tween brain?s processing time and word-level
information. For example, many syllables in a
word or many words in a sentence are likely to
make a document more difficult for the brain to
process it. Also, if the type/token ratio is high,
the text should be more difficult because there
are many unique words to be decoded.
? Syntactic Indices: The indices in this class as-
sess syntactic complexity and the frequency of
particular syntactic constituents in a text. The
intuition behind this class is that high syntactic
complexity makes a text more difficult to pro-
cess, lowering its readability, because it usually
implies syntactic ambiguity, structural density,
high number of embedded constituents.
? Referential and Semantic Indices: These in-
dices assess the negative impact on readability
of cohesion gaps, which occur when the words
in a sentence do not connect to other sentences
in the text. They are based on coreference and
anaphoric chains as well as on semantic simi-
larity between segments of the same document
exploiting Latent Semantic Analysis (LSA).
? Indices for Situation Model Dimensions: The
indices in this class express the degree of com-
plexity of the mental model evoked by a doc-
ument (Dijk and Kintsch, 1983) and involves
four main dimensions: causality, intentionality,
time and space.
? Standard readability indices: They comprise
traditional indices for readability assessment
including Flesch Reading Ease and Flesch Kin-
caid Grade Level.
Although the developers of Coh-Metrix claim that
the internal version of the tool includes hundreds of
measures, the online demo shows only 60 of them.
This is partly due to the fact that some metrics are
computed using resources protected by copyright,
and partly because the whole framework is still un-
der development. We refer to these 60 metrics in or-
der to implement the Coh-Metrix version for Italian,
that we call Coease.
4.2 Coease: Coh-Metrix for Italian
In the Coh-Metrix adaptation for Italian, we follow
as much as possible the description of the single in-
dices reported on the official Coh-Metrix documen-
tation. However, in some cases, not all implementa-
tion details are given, so that we may have slightly
different versions of single indices. Besides, one
set of indices is based on the MRC Psycholinguis-
tic Database (Wilson, 2003), a resource including
around 150,000 words with concreteness ratings col-
lected through psycholinguistic experiments, which
is not available for Italian. In general terms, how-
ever, we try to have some indices for each of the
classes described in Section 4.1, in order to repre-
sent all relevant aspects of text cohesion.
The list of all indices is reported in Table 2. In-
dices from 1 to 6 capture some information about the
length of the documents in terms of syllables, words,
sentences and paragraphs. Syllables are computed
using the Perl module Lingua::IT::Hyphenate4.
4http://search.cpan.org/?acalpini/
Lingua-IT-Hyphenate-0.14/
43
Indices from 7 to 10 focus on familiarity of con-
tent words (verbs, nouns, adjectives and adverbs)
measured as their frequency in a reference corpus.
While in English the frequency list was the CELEX
database (Baayen et al, 1995), for Italian we ex-
tracted it from the dump of Italian Wikipedia5. The
idea behind these indices is that unfamiliar words or
technical terminology should have a low frequency
in the reference corpus, which is supposed to be
a general corpus representing many domains. In-
dex 8 is the logarithm of raw frequency of content
words, because logarithm proved to be compatible
with reading time (Haberlandt and Graesser, 1985).
Index 9 is obtained by computing first the lowest fre-
quency score among all the content words in each
sentence, and then calculating the mean. Index 10 is
obtained by computing first the lowest log frequency
score among all content words in each sentence, and
then calculating the mean. Content words were ex-
tracted by running the TextPro NLP suite for Italian
(Pianta et al, 2008)6 and keeping only words tagged
with one of WordNet PoS, namely v, a, n and r.
Indices 11 and 12 compute the abstractness of
nouns and verbs by measuring the distance between
the WordNet synset containing the lemma (most fre-
quent sense) and the root. Then, the mean distance
of all nouns and verbs in the text is computed. We
obtain this index using MultiWordNet (Pianta et al,
2002), the Italian version of WordNet, aligned at
synset level with the English one.
Indices from 13 to 17 measure the syntactic com-
plexity of sentences based on parsing output. Indices
13-15 are computed after parsing each sentence with
the Italian version of Berkeley constituency-based
parser (Lavelli and Corazza, 2009)7. NP incidence
is the incidence of atomic NPs (i.e. not containing
any other NPs) per 1000 words. Higher-level con-
stituents index is the mean distance between each
terminal word in the text and the parse tree root.
Main verb information needed for computing index
16 is obtained by parsing each sentence with Malt
parser for Italian (Lavelli et al, 2009) and taking the
sentence root as main verb. The index accounts for
5http://it.wikipedia.org
6TextPro achieved 95% PoS tagging accuracy at Evalita
2009 evaluation campaign for Italian tools.
7The parser achieved 84% F1 at Evalita 2011 evaluation
campaign for Italian tools.
the memory load needed by a reader to understand a
sentence. Index 17 is calculated by comparing each
token to a manual list of negations and computing
the total number of negations per 1000 words.
Indices 18 and 19 are computed again using
TextPro and the output of Berkeley parser. Index 18
is the ratio of words labelled as pronouns to the in-
cidence of all NPs in the text. High pronoun density
implies low readability, because it makes referential
cohesion less explicit.
Indices from 20 to 29 capture the cohesion of
sentences by taking into account different types of
connectives. In order to compute them, we manu-
ally create lists of connectives divided into additive,
causal, logical and temporal. Then, for each list, we
identify positive (i.e. extending events) and negative
(i.e. ceasing to extend expected events) connectives.
For instance, ?inoltre? (?moreover?) is a positive ad-
ditive connective, while ?ma? (?but?) is a negative ad-
ditive connective. We further compute the incidence
of conditional operators by comparing each token to
a manual list. In order to create such lists, we stick
to their English version by first translating them into
Italian and then manually adding some missing con-
nectives. However, this does not avoid ambiguity,
since some connectives with high frequency can ap-
pear in more than one list, for instance ?e? (?and?),
which can be both temporal and additive.
Indices 30 and 31 capture syntactic similarity of
sentences and are based on the assumption that a
document showing high syntactic variability is more
difficult to understand. This index computes the pro-
portion of intersecting nodes between two syntactic
trees by looking for the largest common subtree, so
that every node except terminal node has the same
production rule in both trees. Index 32 calculates
the proportion of adjacent sentences that share at
least one argument expressed by a noun or a pro-
noun, while indices 33 and 34 compute this propor-
tion based on stems and content words. Stems are
obtained by applying the Snowball stemmer8 to the
lemmatized documents.
Indices 35?40 capture the situation model dimen-
sions of the text. Causal and intentional cohesion
corresponds to the ratio between causal or inten-
tional particles (i.e. connectives and adverbs) and
8http://snowball.tartarus.org/
44
causal or intentional verbs. The rationale behind
this is that a text with many causal verbs and few
causal particles is less readable because the con-
nections between events is not explicitly expressed.
Since no details where given on how these particles
and verbs were extracted for English, we devise our
own methodology. First, we produce manual lists
of causal and intentional particles in Italian. As for
causal verbs, we first select all synsets in the En-
glish WordNet containing ?cause to? in their glosses,
and then obtain the corresponding version in Ital-
ian through MultiWordNet. Intentional verbs are
obtained by first extracting all verbs from English
WordNet that belong to the following categories:
cognition, communication, competition, consump-
tion, contact, creation, emotion, motion and percep-
tion, and then mapping them to the Italian corre-
sponding verbs in MultiWordNet. Temporal cohe-
sion is computed as the average of repetitions of
tense and aspect in the document. Repetitions are
calculated by mapping the output of TextPro mor-
phological analysis of verbs to the labels considered
for tense, i.e. past, present and future, and for as-
pect, i.e. static, completed and in progress. Spa-
tial cohesion reflects the extent to which the sen-
tences are related by spatial particles or relations,
and corresponds to the mean of location and mo-
tion ratio score. Location score is the incidence of
locative prepositions (LSP) divided by LPS plus the
incidence of location nouns. Location nouns are ob-
tained from WordNet and from the Entity Recog-
nizer of TextPro. Motion score is the incidence of
motion particles (MSP) divided by MSP plus the in-
cidence of motion verbs. Motion verbs information
is extracted from WordNet as well. As for motion
and locative particles, we first create a manual list,
which however contains particles that can express
both location and motion (for instance ?in?). The dis-
tinction between the two types of particles is based
on the dependency structure of each sentence: if the
particle is headed by a motion verb and dominates
a location noun, then we assume that it is a motion
particle. Instead, if it heads a location noun but is
not dominated by a motion verb, then it is a locative
particle. We are aware of the fact that this selection
process is quite coarse-grained and can be biased by
wrong dependency structures, ambiguity of nouns
and verbs and limited extension of Italian WordNet.
However, it is a viable solution to approximate the
information conveyed by the corresponding indices
in English, given that no clear explanation for their
implementation is given.
4.3 Additional indices
We implement also three additional indices that are
not part of Coh-Metrix for English. They are re-
ported in Table 2 with the ID 41?46.
Indices 41 and 42 are based on the Basic Ital-
ian Vocabulary (de Mauro, 2000). This resource
includes a list of 7,000 words, which were manu-
ally classified as highly familiar to native speakers of
Italian. We introduce these indices because past ex-
periments on Italian readability by Dell?Orletta et al
(2011) showed that, by combining this information
with some basic features such as word and sentence
length, it was possible to achieve 0.95 accuracy in
a binary classification task aimed at distinguishing
standard newspaper articles from simplified articles
for L2 readers. Index 41 corresponds to the percent-
age of tokens whose base form is listed in the Basic
Italian Vocabulary, while index 42 is the percentage
of (unique) lemmas. The latter is the same feature
implemented by Dell?Orletta et al (2011).
Index 43 is Gulpease, computed following the for-
mula reported in Section 4. We add it to our in-
dex list in line with Coh-Metrix, which includes also
standard readability metrics such as Flesch-Reading
Ease and Flesch-Kincaid.
5 The Online System
The Coease indices have been made available
online through a Web interface at http://
readability.fbk.eu. This allows users to
copy and paste a document in the text field and to
compute all available indices, similar to the func-
tionalities of the English Coh-Metrix tool. We have
normalized each index so that it is comprised be-
tween -1 and +1 using the scaling function available
in LIBSVM (Chang and Lin, 2011). Low scores ex-
press low readability for the given index while high
scores correspond to highly readable texts.
In order to identify the indices that are most cor-
related with the readability levels, we computed
Pearson correlation coefficients between each index
and the three classes, similar to Pitler and Nenkova
45
(2008). The ten most correlated indices are marked
with (*) in Table 2. It is interesting to note that 6
out of 10 indices are not part of the standard Coh-
Metrix framework, and account for lexical informa-
tion. In all cases, correlation is moderate, being
0.3 ? r ? 0.6.
Figure 2: Graphical representation of readability as plot-
ted by the Coease web interface. Index id on x axis, and
normalized value on y axis
Coease is designed in order to enable users to
compute readability of a given document and com-
pare it with the average values for the three classes in
our reference corpus (Section 3). Therefore, the av-
erage normalized score of each index for each class
has been computed based on the corpus. Then, every
time a new document is analyzed, the output scores
are plotted together with the average scores for each
of the three classes. This allows a user to compare
different aspects of the current document, such as
the lexicon or the syntax, with the averages of the
three classes. For example, a user may discover that
a document is highly complex from the lexical point
of view, since its lexical indices are in line with those
of high-school texts. However, its syntax may be
rather simple, having syntax-based indices similar to
those of elementary textbooks. This kind of compar-
ison provides information that are generally not cap-
tured via supervised classification. If we trained a
classifier using the indices as features, we would be
able to assign a new document to elementary, mid-
dle or high-school level, but a naive user would not
be able to understand how the single indices affect
classification. Besides, this graphical representation
allows a user to identify documents that should not
be classified into a specific class, because its indices
fall into different classes. Furthermore, we can de-
tect documents with different degrees of readability
within each class.
As an example, we report in Fig. 2 the graphical
representation returned by the system after analyz-
ing an article taken from ?Due Parole?9 (labeled as
?current?), an online newspaper for adult L2 learn-
ers. The scores are compared with the average val-
ues of the 10 most correlated indices, which are re-
ported on the x axis in the same order as they are
described in Table 2. According to the plot, the ar-
ticle has a degree of readability similar to the ?high-
school? class, although some indices show that its
readability is higher (see for instance the index n. 9,
i.e. lexical overlap with Class 3 documents).
The current system version returns only the 10
most correlated indices for the sake of clarity. How-
ever, it easy configurable in order to plot all indices,
or just a subset selected by the user.
6 Conclusions and Future Work
We present Coease, a system for readability assess-
ment of Italian inspired by Coh-Metrix principles.
This set of indices improves on Gulpease index in
that it takes into account discourse coherence, syn-
tactic parsing and semantic complexity in order to
account for the psycholinguistic and cognitive rep-
resentations involved in reading comprehension.
We make Coease available through an online in-
terface. A user can easily analyze a document and
compare its readability to three difficulty levels, cor-
responding to average elementary, middle and high-
school readability level. The graphical representa-
tion returned by the system makes this comparison
straightforward, in that the indices computed for the
current document are plotted together with the 10
most correlated indices in Coease.
In the future, we will analyze the reason why lex-
ical indices are among the most correlated ones with
the three classes. The lower impact of syntactic in-
formation, for instance, could be affected by parsing
performance. However, this could depend also on
how syntactic indices are computed in Coh-Metrix:
9http://www.dueparole.it/
46
we will investigate whether alternative ways to cal-
culate the indices may be more appropriate for Ital-
ian texts.
In addition, we plan to use the indices as features
for predicting the readability of unseen texts. In a
classification setting, it will be interesting to see if
the 10 best indices mentioned in the previous sec-
tions are also the most predictive features, given that
some information may become redundant (for in-
stance, the Gulpease index).
Acknowledgments
The work described in this paper has been partially
funded by the European Commission under the con-
tract number FP7-ICT-2009-5, Terence project.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1?9,
Stroudsburg, PA, USA.
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (release 2). CD-ROM.
Richard Bamberger and Erich Vanecek. 1984. Lesen-
Verstehen-Lernen-Schreiben. Jugend un Volk Verlags-
gesellschaft.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34:1?34, March.
I. L. Beck, M. G. McKeown, G. M. Sinatra, and J. A.
Loxterman. 1984s. Revisiting social studies text
from a text-processing perspective: Evidence of im-
proved comprehensibility. Reading Research Quar-
terly, 26:251?276.
M. G. Cataldo and J. Oakhill. 2000. Why are poor com-
prehenders inefficient searchers? An investigation into
the effects of text representation and spatial memory
on the ability to locate information in text. Journal of
Educational Psychology, 92:791?799.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Scott A. Crossley, David B. Allen, and Danielle S. Mc-
Namara. 2011. Text readability and intuitive simplifi-
cation: A comparison of readability formula. Reading
in a Foreign Language, 23(1):84?101.
Tullio de Mauro. 2000. Il Dizionario della Lingua Ital-
iana. Paravia, Torino, Italy.
Felice Dell?Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. READ?IT: Assessing Readability
of Italian Texts with a View to Text Simplification.
In Proceedings of the Second Workshop on Speech
and Language Processing for Assistive Technologies,
pages 73?83, Edinburgh, Scotland, UK, July. Associa-
tion for Computational Linguistics.
T. A. Van Dijk and W. Kintsch. 1983. Strategies of dis-
course comprehension. Academic Press, New York,
US.
Rudolf Flesch. 1946. The Art of plain talk. Harper.
V. Franchina and R. Vacca. 1986. Adaptation of Flesch
readability index on a bilingual text written by the
same author both in Italian and English languages.
Linguaggi, 3:47?49.
Ingo Glo?ckner, Sven Hartrumpf, Hermann Helbig, Jo-
hannes Leveling, and Rainer Osswald. 2006. An ar-
chitecture for rating and controlling text readability. In
Proceedings of KONVENS 2006, pages 32?35, Kon-
stanz, Germany, October.
A. Graesser, D. McNamara, M. Louwerse, and Z. Cai.
2004. Coh-Metrix: Analysis of text on cohesion and
language. Behavioral Research Methods, Instruments,
and Computers, 36:193?202.
Robert Gunning. 1952. The technique of clear writing.
McGraw-Hill.
Karl F. Haberlandt and Arthur C. Graesser. 1985. Com-
ponent processes in text comprehension and some of
their interactions. Journal of Experimental Psychol-
ogy, 114(3):357?374.
F. Huerta. 1959. Medida sencillas de lecturabilidad.
Consigna, 214:29?32.
L. Kandel and A. Moles. 1958. Application de l?Indice
de Flesch a` la langue franc?aise. Cahiers d?Etudes de
Radio-Television, pages 253?274.
J.P. Kincaid, R.P. Fishburne, R.L. Rogers, and B.S.
Chissom. 1975. Derivation of New Readability For-
mulas for Navy Enlisted Personnel. Research Branch
Report.
Alberto Lavelli and Anna Corazza. 2009. The Berkeley
Parser at EVALITA 2009 Constituency Parsing Task.
In Proceedings of EVALITA Evaluation Campaign.
A. Lavelli, J. Hall, J. Nilsson, and J. Nivre. 2009.
MaltParser at the EVALITA 2009 Dependency Parsing
Task. In Proceedings of EVALITA Evaluation Cam-
paign.
T. Linderholm, M. G. Everson, P. van den Broek,
M. Mischinski, A. Crittenden, and J. Samuels. 2000.
Effects of Causal Text Revisions on More- and Less-
Skilled Readers? Comprehension of Easy and Difficult
Texts. Cognition and Instruction, 18:525?556.
47
Pietro Lucisano and Maria Emanuela Piemontese. 1988.
Gulpease. Una formula per la predizione della diffi-
colta` dei testi in lingua italiana. Scuola e Citta`, 3:57?
68.
G. H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading, 12(8):639?646.
D.S. McNamara, E. Kintsch, N.B. Songer, and
W. Kintsch. 1996. Are good texts always better? Text
coherence, background knowledge, and levels of un-
derstanding in learning from text. Cognition and In-
struction, pages 1?43.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In First International Confer-
ence on Global WordNet, pages 292?302, Mysore, In-
dia.
Emanuele Pianta, Christian Girardi, and Roberto Zanoli.
2008. The TextPro tool suite. In Proc. of the 6th Lan-
guage Resources and Evaluation Conference (LREC),
Marrakech, Morocco.
Emily Pitler and Ani Nenkova. 2008. Revisiting Read-
ability: A Unified Framework for Predicting Text
Quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 186?195, Honolulu.
Caroline E. Scarton, Daniel M. Almeida, and Sandra M.
Alu??sio. 2009. Ana?lise da Inteligibilidade de tex-
tos via ferramentas de Processamento de L??ngua Natu-
ral: adaptando as me?tricas do Coh-Metrix para o Por-
tugue?s. In Proceedings of STIL-2009, Sa?o Carlos,
Brazil.
Tim von der Bru?ck, Sven Hartrumpf, and Hermann Hel-
big. 2008. A Readability Checker with Super-
vised Learning using Deep Architecture. Informatica,
32:429?435.
Michael Wilson. 2003. MRC Psycholinguistic
Database: Machine Usable Dictionary, Version 2.00.
Rutherford Appleton Laboratory, Oxfordshire, Eng-
land.
ID Feature list
General word and text information
Basic Count
1-3 N. of words, sents and parag. in text
4 Mean n. of syllables per content word*
5 Mean n. of words per sentence
6 Mean n. of sentences per paragraph
Frequencies
7 Raw frequency of content words
8 Log of raw frequency of content words
9 Min raw frequency of content words
10 Log min raw frequency of content words
Hypernymy
11 Mean hypernym value of nouns
12 Mean hypernym value of verbs
Syntactic indices
Constituents information
13 Noun phrase incidence
14 Mean n. of modifiers per NP
15 Higher level constituents
16 Mean n. of words before main verb
17 Negation incidence
Pronouns, Types, Tokens
18 Pronoun ratio
19 Type-token ratio
Connectives
20 Incidence of all connectives
21-22 Incidence of pos./neg. additive conn.
23-24 Incidence of pos./neg. temporal conn.
25-26 Incidence of pos./neg. causal conn.
27-28 Incidence of pos./neg.* logical conn.
29 Incidence of conditional operators
Syntactic similarity
30 Tree intersection between adj. sentences
31 Tree intersection between all sentences
Referential and Semantic Indices
Coreference
32 Adjacent argument overlap*
33 Stem overlap between adjacent sentences
34 Content word overlap between adj. sents.
Situation model dimensions
35-36 Causal content and cohesion
37-38 Intentional content and cohesion*
39-40 Temporal and spatial cohesion
Features not included in Coh-Metrix
41 Lemma overlap with VBI (token-based)*
42 Lemma overlap with VBI (type-based)*
43 Gulpease index*
44 Lexical overlap with Class 1*
45 Lexical overlap with Class 2*
46 Lexical overlap with Class 3*
Table 2: Coease indices for readability assessment. (*)
shows the indices with highest Pearson correlation.48
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 11?20,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
GAF: A Grounded Annotation Framework for Events
Antske Fokkens, Marieke van Erp, Piek Vossen
The Network Institute
VU University Amsterdam
antske.fokkens@vu.nl
marieke.van.erp@vu.nl
piek.vossen@vu.nl
Sara Tonelli
FBK
Trento, Italy
satonelli@fbk.eu
Willem Robert van Hage
SynerScope B.V.
Eindhoven, The Netherlands
willem.van.hage
@synerscope.com
Luciano Serafini, Rachele Sprugnoli
FBK
Trento, Italy
serafini@fbk.eu
sprugnoli@fbk.eu
Jesper Hoeksema
The Network Institute
VU University Amsterdam
j.e.hoeksema@vu.nl
Abstract
This paper introduces GAF, a grounded an-
notation framework to represent events in a
formal context that can represent information
from both textual and extra-textual sources.
GAF makes a clear distinction between men-
tions of events in text and their formal rep-
resentation as instances in a semantic layer.
Instances are represented by RDF compliant
URIs that are shared across different research
disciplines. This allows us to complete textual
information with external sources and facili-
tates reasoning. The semantic layer can inte-
grate any linguistic information and is com-
patible with previous event representations in
NLP. Through a use case on earthquakes in
Southeast Asia, we demonstrate GAF flexibil-
ity and ability to reason over events with the
aid of extra-linguistic resources.
1 Introduction
Events are not only described in textual documents,
they are also represented in many other non-textual
sources. These sources include videos, pictures,
sensors or evidence from data registration such as
mobile phone data, financial transactions and hos-
pital registrations. Nevertheless, many approaches
to textual event annotation consider events as text-
internal-affairs, possibly across multiple documents
but seldom across different modalities. It follows
from the above that event representation is not ex-
clusively a concern for the NLP community. It also
plays a major role in several other branches of in-
formation science such as knowledge representation
and the Semantic Web, which have created their own
models for representing events.
We propose a grounded annotation framework
(GAF) that allows us to interconnect different ways
of describing and registering events, including non-
linguistic sources. GAF representations can be used
to reason over the cumulated and linked sources of
knowledge and information to interpret the often in-
complete and fragmented information that is pro-
vided by each source. We make a clear distinction
between mentions of events in text or any other form
of registration and their formal representation as in-
stances in a semantic layer.
Mentions in text are annotated using the Terence
Annotation Format (Moens et al, 2011, TAF) on top
of which the semantic layer is realized using Seman-
tic Web technologies and standards. In this semantic
layer, instances are denoted with Uniform Resource
Identifiers (URIs). Attributes and relations are ex-
pressed according to the Simple Event Model (Van
Hage et al, 2011, SEM) and other established on-
tologies. Statements are grouped in named graphs
based on provenance and (temporal) validity, en-
abling the representation of conflicting information.
External knowledge can be related to instances from
a wide variety of sources such as those found in the
Linked Open Data Cloud (Bizer et al, 2009a).
Instances in the semantic layer can optionally be
linked to one or more mentions in text or to other
sources. Because linking instances is optional, our
11
representation offers a straightforward way to in-
clude information that can be inferred from text,
such as implied participants or whether an event is
part of a series that is not explicitly mentioned. Due
to the fact that each URI is unique, it is clear that
mentions connected to the same URI have a coref-
erential relation. Other relations between instances
(participants, subevents, temporal relations, etc.) are
represented explicitly in the semantic layer.
The remainder of this paper is structured as fol-
lows. In Section 2, we present related work and ex-
plain the motivation behind our approach. Section 3
describes the in-text annotation approach. Our se-
mantic annotation layer is presented in Section 4.
Sections 5-7 present GAF through a use case on
earthquakes in Indonesia. This is followed by our
conclusions and future work in section 8.
2 Motivation and Background
Annotation of events and of relations between them
has a long tradition in NLP. The MUC confer-
ences (Grishman and Sundheim, 1996) in the 90s
did not explicitly annotate events and coreference
relations, but the templates used for evaluating the
information extraction tasks indirectly can be seen
as annotation of events represented in newswires.
Such events are not ordered in time or further related
to each other. In response, Setzer and Gaizauskas
(2000) describe an annotation framework to create
coherent temporal orderings of events represented
in documents using closure rules. They suggest that
reasoning with text independent models, such as a
calendar, helps annotating textual representations.
More recently, generic corpora, such as Prop-
bank (Palmer et al, 2005) and the Framenet cor-
pus (Baker et al, 2003) have been built according to
linguistic principles. The annotations aim at prop-
erly representing verb structures within a sentence
context, focusing on verb arguments, semantic roles
and other elements. In ACE 2004 (Linguistic Data
Consortium, 2004b), event detection and linking is
included as a pilot task for the first time, inspired by
annotation schemes developed for named entities.
They distinguish between event mentions and the
trigger event, which is the mention that most clearly
expresses its occurrence (Linguistic Data Consor-
tium, 2004a). Typically, agreement on the trigger
event is low across annotators (around 55% (Moens
et al, 2011)). Timebank (Pustejovsky et al, 2006b)
is a more recent corpus for representing events and
time-expressions that includes temporal relations in
addition to plain coreference relations.
All these approaches have in common that they
consider the textual representation as a closed world
within which events need to be represented. This
means that mentions are linked to a trigger event
or to each other but not to an independent semantic
representation. More recently, researchers started to
annotate events across multiple documents, such as
the EventCorefBank (Bejan and Harabagiu, 2010).
Cross-document coreference is more challenging for
establishing the trigger event, but it is in essence not
different from annotating textual event coreference
within a single document. Descriptions of events
across documents may complement each other pro-
viding a more complete picture, but still textual de-
scriptions tend to be incomplete and sparse with re-
spect to time, place and participants. At the same
time, the comparison of events becomes more com-
plex. We thus expect even lower agreement in as-
signing trigger events across documents. Nothman
et al (2012) define the trigger as the first new ar-
ticle that mentions an event, which is easier than
to find the clearest description and still report inter-
annotator agreement of .48 and .73, respectively.
Recent approaches to automatically resolve event
coreference (cf. Chambers and Jurafsky (2011a),
Bejan and Harabagiu (2010)) use some background
data to establish coreference and other relations be-
tween events in text. Background information, in-
cluding resources, and models learned from textual
data do not represent mentions of events directly but
are useful to fill gaps of knowledge in the textual
descriptions. They do not alter the model for anno-
tation as such.
We aim to take these recent efforts one step fur-
ther and propose a grounded annotation framework
(GAF). Our main goal is to integrate information
from text analysis in a formal context shared with
researchers across domains. Furthermore, GAF is
flexible enough to contain contradictory informa-
tion. This is both important to represent sources
that (partially) contradict each other and to com-
bine alternative annotations or output of different
NLP tools. Because conflicting information may be
12
present, provenance of information is provided in
our framework, so that we may decide which source
to trust more or use it as a feature to decide which in-
terpretation to follow. Different models of event rep-
resentation exist that can contribute valuable infor-
mation. Therefore our model is compliant with prior
approaches regardless of whether they are manual or
automatic. Finally, GAF makes a clear distinction
between instances and instance mentions avoiding
the problem of determining a trigger event. Addi-
tionally, it facilitates the integration of information
from extra-textual sources and information that can
be inferred from texts, but is not explicitly men-
tioned. Sections 5 to 7 will explain how we can
achieve this with GAF.
3 The TERENCE annotation format
The TERENCE Annotation Format (TAF) is de-
fined within the TERENCE Project1 with the goal
to include event mentions, temporal expressions and
participant mentions in a single annotation proto-
col (Moens et al, 2011). TAF is based on ISO-
TimeML (Pustejovsky et al, 2010), but introduces
several adaptations in order to fit the domain of chil-
dren?s stories for which it was originally developed.
The format has been used to annotate around 30 chil-
dren stories in Italian and 10 in English.
We selected TAF as the basis for our in-text anno-
tation for three reasons. First, it incorporates the (in
our opinion crucial) distinction between instances
and instance mentions. Second, it adapts some con-
solidated paradigms for linguistic annotation such as
TimeML for events and temporal expressions and
ACE for participants and participant mentions (Lin-
guistic Data Consortium, 2005). It is thus compat-
ible with other annotation schemes. Third, it inte-
grates the annotation of event mentions, participants
and temporal expressions into a unified framework.
We will elaborate briefly on these properties below.
As mentioned, TAF makes a clear distinction be-
tween instances and instance mentions. Originally,
this distinction only applied to nominal and named
entities, similar to ACE (Linguistic Data Consor-
tium, 2005), because children?s stories can gener-
ally be treated as a closed world, usually present-
1ICT FP7 Programme, ICT-2010-25410, http://www.
terenceproject.eu/
ing a simple sequence of events that do not corefer.
Event coreference and linking to other sources was
thus not relevant for this domain. In GAF, we ex-
tend the distinction between instances and instance
mentions to events to model event coreference, link
them to other sources and create a consistent model
for all instances.
Children?s stories usually include a small set of
characters, event sequences (mostly in chronologi-
cal order), and a few generic temporal expressions.
In the TERENCE project, modeling characters in
the stories is necessary. This requires an extension
of TimeML to deal with event participants. Puste-
jovsky et al (2006a) address the need to include ar-
guments in TimeML annotations, but that proposal
did not include specific examples and details on how
to perform annotation (e.g., on the participants? at-
tributes). Such guidelines were created for TAF.
The TAF annotation of event mentions largely
follows TimeML in annotating tense, aspect, class,
mood, modality and polarity and temporal expres-
sions. However, there are several differences be-
tween TAF and TimeML. First, temporal expres-
sions are not normalized into the ISO-8601 form,
because most children?s stories are not fixed to a spe-
cific date. In GAF, the normalization of expressions
takes place in the semantic layer as these go beyond
the scope of the text. As a result, temporal vague-
ness of linguistic expressions in text do not need to
be normalized in the textual representation to actual
time points and remain underspecified.2
In TAF, events and participant mentions are linked
through a has participant relation, which is defined
as a directional, one-to-one relation from the event
to the participant mentions. Only mentions corre-
sponding to mandatory arguments of the events in
the story are annotated. Annotators look up each
verb in a reference dictionary providing information
on the predicate-argument structure of each verb.
This makes annotation easier and generally not con-
troversial. However, this kind of information can be
provided only by annotators having a good knowl-
edge of linguistics.
All annotations are performed with the Celct An-
2Note that we can still use existing tools for normalization
at the linguistic level: early normalizations can be integrated
in the semantic layer alongside normalizations carried out at a
later point.
13
sem:sub
EventOf
sem:Event sem:Actor sem:Place sem:Time
sem:hasTime
sem:hasActor
sem:hasPlace
sem:PlaceType
sem:placeType
sem:EventType
sem:eventType
sem:ActorType
sem:actorType
sem:TimeType
sem:Type
sem:timeType
sem:Core
sem:subTypeOf
C
o
r
e
 
C
l
a
s
s
e
s
(
F
o
r
e
i
g
n
)
T
y
p
e
 
S
y
s
t
e
m
Literal sem:hasTimeStamp
Literal sem:hasTimeStamp
Figure 1: The SEM ontology
notation Tool (Bartalesi Lenzi et al, 2012), an online
tool supporting TimeML that can easily be extended
to include participant information. The annotated
file can be exported to various XML formats and im-
ported into the semantic layer. The next section de-
scribes SEM, the event model used in our semantic
layer, and how it complements the TAF annotations.
4 The Simple Event Model
The Simple Event Model (SEM) is an RDF
schema (Carroll and Klyne, 2004; Guha and Brick-
ley, 2004) to express who did what, where, and
when. There are many RDF schemas and OWL on-
tologies (Motik et al, 2009) that describe events,
e.g., Shaw et al (2009), Crofts et al (2008) and
Scherp et al (2009). SEM is among the most
flexible and easiest to adapt to different domains.
SEM describes events and related instances such as
the place, time and participants (called Actors in
SEM) by representing the interactions between the
instances with RDF triples. SEM models are se-
mantic networks that include events, places, times,
participants and all related concepts, such as their
types.
An overview of all the classes in the SEM ontol-
ogy and the relations connecting them is shown in
Figure 1. Nodes can be identified by URIs, which
universally identify them across all RDF models. If
for example one uses the URI used by DBpedia3
(Bizer et al, 2009b) for the 2004 catastrophe in In-
3http://dbpedia.org
donesia, then one really means the same event as ev-
erybody else who uses that URI. SEM does not put
any constraints on the RDF vocabulary, so vocabu-
laries can easily be reused. Places and place types
can for example be imported from GeoNames4 and
event types from the RDF version of WordNet.
SEM supports two types of abstraction: gener-
alization with hierarchical relations from other on-
tologies, such as the subclass relation from RDFS,
and aggregation of events into superevents with the
sem:subEventOf relation, as exemplified in Fig-
ure 2. Other types of abstractions can be represented
using additional schemas or ontologies in combina-
tion with SEM. For instance, temporal aggregation
can be done with constructs from the OWL Time
ontology (Hobbs and Pan, 2004).
Relations between events and other instances,
which could be other events, places, actors, times,
or external concepts, can be modeled using the
sem:eventProperty relation. This relation can
be refined to represent specific relations, such as
specific participation, causality or simultaneity rela-
tions. The provenance of information in the SEM
graph is captured through assigning contexts to
statements using the PROV Data Model (Moreau et
al., 2012). In this manner, all statements derived
from a specific newspaper article are stored in a
named graph that represents that origin. Conflicting
statements can be stored in different named graphs,
and can thus coexist. This gives us the possibility
4http://www.geonames.org/ontology/
14
sem:Event
sem:Place
sem:EventType
sem:Time
dbpedia:2004_Indian_Ocean_
earthquake_and_ tsunami
rdf:type
"December 2004 
Earthquake and 
Tsunami"@en
rdfs:label
rdf:type
rdf:type
"3.316"^^xsd:decimal
"2004-12-26"^^xsd:date
"95.854"^^xsd:decimal
wgs84:long
wgs84:lat
owltime:inXSD
DateTime
sem:hasPlace sem:hasTime
naacl:INSTANCE_186
rdf:type
sem:subEventOf
wn30:synset-
earthquake-noun-1
sem:eventType
rdf:type
naacl:INSTANCE_188
rdf:type
sem:subEventOf
naacl:INSTANCE_198
sem:hasTime
naacl:TIMEX3_81 "2004"str:anchorOfnwr:denotedBy
naacl:INSTANCE_MENTION_118
nwr:denotedBy "temblor"@en
str:anchorOf
nwr:denotedBy
"tsunami"@en
naacl:INSTANCE_MENTION_120
str:anchorOf
naacl:INSTANCE_189
sem:subEventOf
naacl:INSTANCE_MENTION_121
nwr:denotedBy
"swept"@en
str:anchorOf
sem:hasPlace
naacl:INSTANCE_67
naacl:INSTANCE_MENTION_19nwr:denotedBy
"Indian Ocean"@en
str:anchorOf
taf:LOCATION
taf:NSUBJ
geonames:1545739
skos:exactMatch
gaf:G1
gaf:G2
gaf:G3
gaf:G4
gaf:G5
dbpedia:Bloomberg
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
sem:
derived
From
gaf:causes
Figure 2: Partial SEM representation of December 26th 2004 Earthquake
of delaying or ignoring the resolution of the conflict,
which enables use cases that require the analysis of
the conflict itself.
5 The GAF Annotation Framework
This section explains the basic idea behind GAF by
using texts on earthquakes in Indonesia. GAF pro-
vides a general model for event representation (in-
cluding textual and extra-textual mentions) as well
as exact representation of linguistic annotation or
output of NLP tools. Simply put, GAF is the combi-
nation of textual analyses and formal semantic rep-
resentations in RDF.
5.1 A SEM for earthquakes
We selected newspaper texts on the January 2009
West Papua earthquakes from Bejan and Harabagiu
(2010) to illustrate GAF. This choice was made be-
cause the topic ?earthquake? illustrates the advan-
tage of sharing URIs across domains. Gao and
Hunter (2011) propose a Linked Data model to cap-
ture major geological events such as earthquakes,
volcano activity and tsunamis. They combine infor-
mation from different seismological databases with
the intention to provide more complete information
to experts which may help to predict the occurrence
of such events. The information can also be used
in text interpretation. We can verify whether in-
terpretations by NLP tools correspond to the data
and relations defined by geologists or, through gen-
eralization, which interpretation is the most sensi-
ble given what we know about the events. General
information on events obtained from automatic text
processing, such as event templates (Chambers and
Jurafsky, 2011b) or typical event durations (Gusev
et al, 2010) can be integrated in SEM in a similar
manner. Provenance indications can be used to in-
dicate whether information is based on a model cre-
ated by an expert or an automatically derived model
obtained by a particular approach.
Figure 2 provides a fragment of a SEM represen-
tation for the earthquake and tsunami of December
26 2004.5 The model is partially inspired by Gao
and Hunter (2011)?s proposal. It combines infor-
mation extracted from texts with information from
DBpedia. The linking between the two can be es-
tablished either manually or automatically through
5The annotation and a larger representation including the
sentence it represents can be found on the GAF website http:
//wordpress.let.vu.nl/gaf.
15
an entity linking system.6 The combined event of
the earthquake and tsunami is represented by a DB-
pedia URI. The node labeled naacl:INSTANCE 186
represents the earthquake itself. The unambiguous
representation of the 2004 earthquake leads us to ad-
ditional information about it, for instance that the
earthquake is an event (sem:Event) and that the
sem:EventType is an earthquake, in this case
represented by a synset from WordNet, but also the
exact date it occurred and the exact location (cf
sem:hasTime, sem:hasPlace).
5.2 Integrating TAF representations into SEM
TAF annotations are converted to SEM relations.
For example, the TAF as participant relations
are translated to sem:hasActor relations, and
temporal relations are translated to sem:hasTime.
We use the relation nwr:denotedBy to link in-
stances to their mentions in the text which are repre-
sented by their unique identifiers in Figure 2.
Named graphs are used to model the source of
information as discussed in Section 4. The re-
lation sem:accordingTo indicates provenance
of information in the graph.7 For instance, the
mentions from the text in named graph gaf:G1
come from the source dbpedia:Bloomberg.
Relations between instances (e.g. between IN-
STANCE 189 and INSTANCE 188) are derived
from a specific grammatical relation in the text
(here, that tsunami is subject of swept) indicated
by the nwr:derivedFrom relation from gaf:G5
to gaf:G4. The grammatical relations included
in graph gaf:G5 come from a TAF annotation
(tag:annotation 2013 03 24).
6 GAF Earthquake Examples
This section takes a closer look at a few selected sen-
tences from the text that illustrate different aspects
of GAF. Figure 2 showed how a URI can provide a
formal context including important background in-
6Entity linking is the task of associating a mention to an
instance in a knowledge base. Several approaches and tools for
entity linking w.r.t. DBpedia and other data sets in the Linked
Open Data cloud are available and achieve good performances,
such as DBpedia Spotlight (Mendes et al, 2011); see (Rizzo
and Troncy, 2011) for a comparison of tools.
7The use of named graphs in this way to denote context is
compatible with the method used by Bozzato et al (2012).
formation on the event. Several texts in the corpus
refer to the tsunami of December 26, 2004, a 9.1
temblor in 2004 caused a tsunami and The catastro-
phe four years ago, among others. Compared to time
expressions such as 2004 and four years ago, time
indications extracted from external sources like DB-
pedia are not only more precise, but also permit us to
correctly establish the fact that these expressions re-
fer to the same event and thus indicate the same time.
The articles were published in January 2009: a direct
normalization of time indications would have placed
the catastrophe in 2005. The flexibility to combine
these seemingly conflicting time indications and de-
lay normalization can be used to correctly interpret
that four years ago early January 2009 refers to an
event taking place at the end of December 2004.
A fragment relating to one of the earthquakes of
January 2009: The quake struck off the coast [...] 75
kilometers (50 miles) west of [....] Manokwari pro-
vides a similar example. The expressions 75 kilo-
meters and 50 miles are clearly meant to express
the same distance, but not identical. The location
is most likely neither exactly 75 km nor 50 miles.
SEM can represent an underspecified location that
is included in the correct region. The exact location
of the earthquake can be found in external resources.
We can include both distances as expressions of the
location and decide whether they denote the general
location or include the normalized locations as alter-
natives to those from external resources.
Different sources may report different details.
Details may only be known later, or sources may
report from a different perspective. As provenance
information can be incorporated into the semantic
layer, we can represent different perspectives, and
choose which one to use when reasoning over the
information. For example, the following phrases
indicate the magnitude of the earthquakes that
struck Manokwari on January 4, 2009:
the 7.7 magnitude quake (source: Xinhuanet)
two quakes, measuring 7.6 and 7.4 (source: Bloomberg)
One 7.3-magnitude tremor (source: Jakartapost)
The first two magnitude indicators (7.7, 7.6)
are likely to pertain to the same earthquake, just as
the second two (7.4, 7.3) are. Trust indicators can
be found through the provenance trace of each men-
16
tion. Trust indicators can include the date on which
it was published, properties of the creation process,
the author, or publisher (Ceolin et al, 2010).
Furthermore, because the URIs are shared across
domains, we can link the information from the text
to information from seismological databases, which
may contain the exact measurement for the quake.
Similarly, external information obtained through
shared links can help us establish coreference. Con-
sider the sentences in Figure 3. There are several
ways to establish that the same event is meant in all
three sentences by using shared URIs and reasoning.
All sentences give us approximate time indications,
location of the affected area and casualties. Rea-
soning over these sentences combined with external
knowledge allows us to infer facts such as that un-
dersea [...] off [...] Aceh will be in the Indian Ocean,
or that the affected countries listed in the first sen-
tence are countries around the Indian Ocean, which
constitutes the Indian Ocean Community. The num-
ber of casualties in combination of the approximate
time indication or approximate location suffices to
identify the earthquake and tsunami in Indonesia on
December 26, 2004. The DBpedia representation
contains additional information such as the magni-
tude, exact location of the quake and a list of affected
countries, which can be used for additional verifica-
tion. This example illustrates how a formal context
using URIs that are shared across disciplines of in-
formation science can help to determine exact refer-
ents from limited or imprecise information.
7 Creating GAF
GAF entails integrating linguistic information
(e.g. TAF annotations) into RDF models (e.g. SEM).
The information in the model includes provenance
that points back to specific annotations. There are
two approaches to annotate text according to GAF.
The first approach is bottom-up. Mentions are
marked in the text as well as relations between them
(participants, time, causal relations, basically any-
thing except coreference). Consequently, these an-
notations are converted to SEM representations as
explained above. Coreference is established by link-
ing mentions to the same instance in SEM. The sec-
ond approach is top-down. Here, annotators mark
relations between instances (events, their partici-
pants, time relations, etc.) directly into SEM and
then link these to mentions in the text.
As mention in Section 2, inter-annotator agree-
ment on event annotation is generally low showing
that it is challenging. The task is somewhat simpli-
fied in GAF, since it removes the problem of identi-
fying an event trigger in the text. The GAF equiva-
lent of the event trigger in other linguistic annotation
approaches is an instance in SEM. However, other
challenges such as which mentions to select are in
principle not addressed by GAF, though differences
in inter-annotator agreement may be found depend-
ing on whether the bottom-up approach or the top-
down approach is selected. The formal context of
SEM may help frame annotations, especially for do-
mains such as earthquakes, where expert knowledge
was used to create basic event models. This may
help annotators while defining the correct relations
between events. On the other hand, the top-down
approach may lead to additional challenges, because
annotators are forced to link events to unambiguous
instances leading to hesitations as to when new in-
stances should be introduced.
Currently, we only use the bottom-up approach.
The main reason is the lack of an appropriate anno-
tation tool to directly annotate information in SEM.
We plan to perform comparative studies between the
two annotation approaches in future work.
8 Conclusion and Future Work
We presented GAF, an event annotation framework
in which textual mentions of events are grounded in
a semantic model that facilitates linking these events
to mentions in external (possibly non-textual) re-
sources and thereby reasoning. We illustrated how
GAF combines TAF and SEM through a use case
on earthquakes. We explained that we aim for a
representation that can combine textual and extra-
linguistic information, provides a clear distinction
between instances and instance mentions, is flexi-
ble enough to include conflicting information and
clearly marks the provenance of information.
GAF ticks all these boxes. All instances are rep-
resented by URIs in a semantic layer following stan-
dard RDF representations that are shared across re-
search disciplines. They are thus represented com-
pletely independent of the source and clearly distin-
17
There have been hundreds of earthquakes in Indonesia since a 9.1 temblor in 2004 caused a
tsunami that swept across the Indian Ocean, devastating coastal communities and leaving more
than 220,000 people dead in Indonesia, Sri Lanka, India, Thailand and other countries.
(Bloomberg, 2009-01-07 01:55 EST)
The catastrophe four years ago devastated Indian Ocean community and killed more than 230,000
people, over 170,000 of them in Aceh at northern tip of Sumatra Island of Indonesia.
(Xinhuanet, 2009-01-05 13:25:46 GMT)
In December 2004, a massive undersea quake off the western Indonesian province of Aceh
triggered a giant tsunami that left at least 230,000 people dead and missing in a dozen
countries facing the Indian Ocean. (Aljazeera, 2009-01-05 08:49 GMT)
Figure 3: Sample sentences mentioning the December 2004 Indonesian earthquake from sample texts
guished from mentions in text or mentions in other
sources. The Terence Annotation Format (TAF) pro-
vides a unified framework to annotate events, par-
ticipants and temporal expressions (and the corre-
sponding relations) by leaning on past, consolidated
annotation experiences such TimeML and ACE. We
will harmonize TAF, the Kyoto Annotation Format
(Bosma et al, 2009, KAF) and the NLP Interchange
Format (Hellmann et al, 2012, NIF) with respect
to the textual representation in the near future. The
NAF format includes the lessons learned from these
predecessors: layered standoff representations using
URI as identifiers and where possible standardized
data categories. The formal semantic model (SEM)
provides the flexibility to include conflicting infor-
mation as well as indications of the provenance of
this information. This allows us to use inferencing
and reasoning over the cumulated and aggregated
information, possibly exploiting the provenance of
the type of information source. This flexibility also
makes our representation compatible with all ap-
proaches dealing with event representation and de-
tections mentioned in Section 2. It can include au-
tomatically learned templates as well as specific re-
lations between events and time expressed in text.
Moreover, it may simultaneously contain output of
different NLP tools.
The proposed semantic layer may be simple, its
flexibility in importing external knowledge may in-
crease complexity in usage as it can model events in
every thinkable domain. To resolve this issue, it is
important to scope the domain by importing the ap-
propriate vocabularies, but no more. When keeping
this in mind, reasoning with SEM is shown to be rich
but still versatile (Van Hage et al, 2012).
While GAF provides us with the desired granu-
larity and flexibility for the event annotation tasks
we envision, a thorough evaluation still needs to be
carried out. This includes an evaluation of the anno-
tations created with GAF compared to other anno-
tation formats, as well as testing it within a greater
application. A comparative study of top-down and
bottom-up annotation will also be carried out. As al-
ready mentioned in Section 7, there is no appropriate
modeling tool for SEM yet. We are currently using
the CAT tool to create TAF annotations and convert
those to SEM, but will develop a tool to annotate the
semantic layer directly for this comparative study.
The most interesting effect of the GAF annota-
tions is that it provides us with relatively simple ac-
cess to a vast wealth of extra-linguistic information,
which we can utilize in a variety of NLP tasks; some
of the reasoning options that are made available by
the pairing up with Semantic Web technology may
for example aid us in identifying coreference rela-
tions between events. Investigating the implications
of this combination of NLP and Semantic Web tech-
nologies lies at the heart of our future work.
Acknowledgements
We thank Francesco Corcoglioniti for his helpful
comments and suggestions. The research lead-
ing to this paper was supported by the European
Union?s 7th Framework Programme via the News-
Reader Project (ICT-316404) and by the Biogra-
phyNed project, funded by the Netherlands eScience
Center (http://esciencecenter.nl/). Partners in Biog-
raphyNed are Huygens/ING Institute of the Dutch
Academy of Sciences and VU University Amster-
dam.
18
References
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal of Lexicography, 16(3):281?296.
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele
Sprugnoli. 2012. CAT: the CELCT Annotation Tool.
In Proceedings of LREC 2012.
Cosmin Bejan and Sandra Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguistic
features. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1412?1422.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009a. Linked data - the story so far. International
Journal on Semantic Web and Information Systems,
5(3):1?22.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009b. DBpedia - A crystallization
point for the Web of Data. Web Semantics: Science,
Services and Agents on the World Wide Web, 7(3):154
? 165.
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic se-
mantic annotation format. In Proceedings of the 5th
International Conference on Generative Approaches
to the Lexicon GL 2009, Pisa, Italy.
Loris Bozzato, Francesco Corcoglioniti, Martin Homola,
Mathew Joseph, and Luciano Serafini. 2012. Manag-
ing contextualized knowledge with the ckr (poster). In
Proceedings of the 9th Extended Semantic Web Con-
ference (ESWC 2012), May 27-31.
Jeremy J. Carroll and Graham Klyne. 2004. Re-
source description framework (RDF): Concepts and
abstract syntax. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-concepts-
20040210/.
Davide Ceolin, Paul Groth, and Willem Robert Van Hage.
2010. Calculating the trust of event descriptions using
provenance. Proceedings Of The SWPM.
Nathanael Chambers and Dan Jurafsky. 2011a.
Template-based information extraction without the
templates. In Proceedings of ACL-2011.
Nathanael Chambers and Dan Jurafsky. 2011b.
Template-based information extraction without the
templates. In Proceedings of ACL-2011, Portland, OR.
Nick Crofts, Martin Doerr, Tony Gill, Stephen Stead,
and Matthew Stiff. 2008. Definition of the CIDOC
Conceptual Reference Model. Technical report,
ICOM/CIDOC CRM Special Interest Group. version
4.2.5.
Lianli Gao and Jane Hunter. 2011. Publishing, link-
ing and annotating events via interactive timelines: an
earth sciences case study. In DeRiVE 2011 (Detec-
tion, Representation, and Exploitation of Events in the
Semantic Web) Workshop in conjunction with ISWC
2011, Bonn, Germany.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics (COLING?96), pages 466?471.
Ramanathan V. Guha and Dan Brickley. 2004.
RDF vocabulary description language 1.0: RDF
schema. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-schema-
20040210/.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Jurafsky.
2010. Using query patterns to learn the duration of
events. In Proceedings of ISWC 2010.
Sebastian Hellmann, Jens Lehmann, and So?ren Auer.
2012. NIF: An ontology-based and linked-data-aware
NLP Interchange Format. Working Draft.
Jerry R Hobbs and Feng Pan. 2004. An ontology of time
for the semantic web. ACM Transactions on Asian
Language Information Processing (TALIP), 3(1):66?
85.
Linguistic Data Consortium. 2004a. Annotation
Guidelines for Event Detection and Characterization
(EDC). http://projects.ldc.upenn.edu/
ace/docs/EnglishEDCV2.0.pdf.
Linguistic Data Consortium. 2004b. The ACE 2004
Evaluation Plan. Technical report, LDC.
Linguistic Data Consortium. 2005. ACE (Automatic
Content Extraction) English annotation guidelines for
entities. Version 6.6, July.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In Proceedings of the
7th International Conference on Semantic Systems, I-
Semantics ?11, pages 1?8.
Marie-Francine Moens, Oleksandr Kolomiyets,
Emanuele Pianta, Sara Tonelli, and Steven Bethard.
2011. D3.1: State-of-the-art and design of novel
annotation languages and technologies: Updated
version. Technical report, TERENCE project ? ICT
FP7 Programme ? ICT-2010-25410.
Luc Moreau, Paolo Missier, Khalid Belhajjame, Reza
B?Far, James Cheney, Sam Coppens, Stephen Cress-
well, Yolanda Gil, Paul Groth, Graham Klyne, Timo-
thy Lebo, Jim McCusker, Simon Miles, James Myers,
Satya Sahoo, and Curt Tilmes. 2012. PROV-DM: The
PROV Data Model. Technical report.
Boris Motik, Bijan Parsia, and Peter F. Patel-
Schneider. 2009. OWL 2 Web Ontology
19
Language structural specification and functional-
style syntax. W3C recommendation, W3C,
October. http://www.w3.org/TR/2009/
REC-owl2-syntax-20091027/.
Joel Nothman, Matthew Honnibal, Ben Hachey, and
James R. Curran. 2012. Event linking: Ground-
ing event reference in a news archive. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 228?232, Jeju Island, Korea, July. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, 2013/03/12.
James Pustejovsky, Jessica Littman, and Roser Saur?`.
2006a. Argument Structure in TimeML. In Dagstuhl
Seminar Proceedings. Internationales Begegnungs-
und Forschungszentrum.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006b. Timebank 1.2 documentation.
Technical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An international
standard for semantic annotation. In Proceedings o
the Fifth International Workshop on Interoperable Se-
mantic Annotation.
Giuseppe Rizzo and Raphae?l Troncy. 2011. NERD:
A framework for evaluating named entity recognition
tools in the Web of data. In Workshop on Web Scale
Knowledge Extraction, colocated with ISWC 2011.
Ansgar Scherp, Thomas Franz, Carsten Saathoff, and
Steffen Staab. 2009. F?a model of events based on
the foundational ontology dolce+ dns ultralight. In
Proceedings of the fifth international conference on
Knowledge capture, pages 137?144. ACM.
Andrea Setzer and Robert J. Gaizauskas. 2000. Annotat-
ing events and temporal information in newswire texts.
In LREC. European Language Resources Association.
Ryan Shaw, Raphae?l Troncy, and Lynda Hardman. 2009.
LODE: Linking Open Descriptions of Events. In 4th
Annual Asian Semantic Web Conference (ASWC?09),
Shanghai, China.
Willem Robert Van Hage, Ve?ronique Malaise?, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011. De-
sign and use of the simple event model (SEM). Jour-
nal of Web Semantics.
Willem Robert Van Hage, Marieke Van Erp, and
Ve?ronique Malaise?. 2012. Linked open piracy: A
story about e-science, linked data, and statistics. Jour-
nal on Data Semantics, 1(3):187?201.
20
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 10?19,
Gothenburg, Sweden, April 26, 2014.
c?2014 Association for Computational Linguistics
Annotating causality in the TempEval-3 corpus
Paramita Mirza
FBK, Trento, Italy
University of Trento
paramita@fbk.eu
Rachele Sprugnoli
FBK, Trento, Italy
University of Trento
sprugnoli@fbk.eu
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Manuela Speranza
FBK, Trento, Italy
manspera@fbk.eu
Abstract
While there is a wide consensus in the NLP
community over the modeling of temporal
relations between events, mainly based on
Allen?s temporal logic, the question on how
to annotate other types of event relations, in
particular causal ones, is still open. In this
work, we present some annotation guide-
lines to capture causality between event
pairs, partly inspired by TimeML. We then
implement a rule-based algorithm to auto-
matically identify explicit causal relations
in the TempEval-3 corpus. Based on this
annotation, we report some statistics on the
behavior of causal cues in text and perform
a preliminary investigation on the interac-
tion between causal and temporal relations.
1 Introduction
The annotation of events and event relations in
natural language texts has gained in recent years in-
creasing attention, especially thanks to the develop-
ment of TimeML annotation scheme (Pustejovsky
et al., 2003), the release of TimeBank (Pustejovsky
et al., 2006) and the organization of several eval-
uation campaigns devoted to automatic temporal
processing (Verhagen et al., 2007; Verhagen et al.,
2010; UzZaman et al., 2013).
However, while there is a wide consensus in the
NLP community over the modeling of temporal
relations between events, mainly based on Allen?s
interval algebra (Allen, 1983), the question on how
to model other types of event relations is still open.
In particular, linguistic annotation of causal rela-
tions, which have been widely investigated from
a philosophical and logical point of view, are still
under debate. This leads, in turn, to the lack of
a standard benchmark to evaluate causal relation
extraction systems, making it difficult to compare
systems performances, and to identify the state-of-
the-art approach for this particular task.
Although several resources exist in which causal-
ity has been annotated, they cover only few aspects
of causality and do not model it in a global way,
comparable to what as been proposed for temporal
relations in TimeML. See for instance the annota-
tion of causal arguments in PropBank (Bonial et al.,
2010) and of causal discourse relations in the Penn
Discourse Treebank (The PDTB Research Group,
2008).
In this work, we propose annotation guidelines
for causality inspired by TimeML, trying to take ad-
vantage of the clear definition of events, signals and
relations proposed by Pustejovsky et al. (2003). Be-
sides, as a preliminary investigation of causality in
the TempEval-3 corpus, we perform an automatic
analysis of causal signals and relations observed in
the corpus. This work is a first step towards the an-
notation of the TempEval-3 corpus with causality,
with the final goal of investigating the strict connec-
tion between temporal and causal relations. In fact,
there is a temporal constraint in causality, i.e. the
cause must occur BEFORE the effect. We believe
that investigating this precondition on a corpus ba-
sis can contribute to improving the performance of
temporal and causal relation extraction systems.
2 Existing resources on Causality
Several attempts have been made to annotate causal
relations in texts. A common approach is to look
for specific cue phrases like because or since or to
look for verbs that contain a cause as part of their
meaning, such as break (cause to be broken) or
kill (cause to die) (Khoo et al., 2000; Sakaji et al.,
2008; Girju et al., 2007). In PropBank (Bonial et
al., 2010), causal relations are annotated in the form
of predicate-argument relations, where ARGM-CAU
is used to annotate ?the reason for an action?, for
example: ?They [
PREDICATE
moved] to London
[
ARGM-CAU
because of the baby].?
Another scheme annotates causal relations be-
tween discourse arguments, in the framework of
10
the Penn Discourse Treebank (PDTB). As opposed
to PropBank, this kind of relations holds only be-
tween clauses and do not involve predicates and
their arguments. In PDTB, the Cause relation type
is classified as a subtype of CONTINGENCY.
Causal relations have also been annotated as re-
lations between events in a restricted set of linguis-
tic constructions (Bethard et al., 2008), between
clauses in text from novels (Grivaz, 2010), or in
noun-noun compounds (Girju et al., 2007).
Several types of annotation guidelines for causal
relations have been presented, with varying de-
grees of reliability. One of the simpler approaches
asks annotators to check whether the sentence they
are reading can be paraphrased using a connective
phrase such as and as a result or and as a conse-
quence (Bethard et al., 2008).
Another approach to annotate causal relations
tries to combine linguistic tests with semantic rea-
soning tests. In Grivaz (2010), the linguistic para-
phrasing suggested by Bethard et al. (2008) is
augmented with rules that take into account other
semantic constraints, for instance if the potential
cause occurs before or after the potential effect.
3 Annotation of causal information
As part of a wider annotation effort aimed to an-
notate texts at the semantic level (Tonelli et al.,
2014), we propose guidelines for the annotation of
causal information. In particular, we define causal
relations between events based on the TimeML def-
inition of events (ISO TimeML Working Group,
2008), as including all types of actions (punctual
and durative) and states. Syntactically, events can
be realized by a wide range of linguistic expres-
sions such as verbs, nouns (which can realize even-
tualities in different ways, for example through a
nominalization process of a verb or by possessing
an eventive meaning), and prepositional construc-
tions.
Following TimeML, our annotation of events in-
volved in causal relations includes the polarity
attribute (see Section 3.3); in addition to this, we
have defined the factuality and certainty
event attributes, which are useful to infer informa-
tion about actual causality between events.
Parallel to the TimeML tag <SIGNAL> as an
indicator for temporal links, we have also intro-
duced the notion of causal signals through the use
of the <C-SIGNAL> tag.
3.1 C-SIGNAL
The <C-SIGNAL> tag is used to mark-up textual
elements that indicate the presence of a causal rela-
tion (i.e. a CLINK, see 3.2). Such elements include
all causal uses of:
? prepositions, e.g. because of, on account of,
as a result of, in response to, due to, from, by;
? conjunctions, e.g. because, since, so that,
hence, thereby;
? adverbial connectors, e.g. as a result, so,
therefore, thus;
? clause-integrated expressions, e.g. the result
is, the reason why, that?s why.
The extent of C-SIGNALs corresponds to the
whole expression, so multi-token extensions are
allowed.
3.2 CLINK (Causal Relations)
For the annotation of causal relations between
events, we use the <CLINK> tag, a directional
one-to-one relation where the causing event is the
source (the first argument, indicated as
S
in the
examples) and the caused event is the target (the
second argument, indicated as
T
). The annotation
of CLINKs includes the c-signalID attribute,
whose value is the ID of the C-SIGNAL indicating
the causal relation (if available).
A seminal research in cognitive psychology
based on the force dynamics theory (Talmy, 1988)
has shown that causation covers three main kinds of
causal concepts (Wolff, 2007), which are CAUSE,
ENABLE, and PREVENT, and that these causal
concepts are lexicalized as verbs (Wolff and Song,
2003): (i) CAUSE-type verbs: bribe, cause, com-
pel, convince, drive, have, impel, incite, induce,
influence, inspire, lead, move, persuade, prompt,
push, force, get, make, rouse, send, set, spur, start,
stimulate; (ii) ENABLE-type verbs: aid, allow, en-
able, help, leave, let, permit; (iii) PREVENT-type
verbs: bar, block, constrain, deter, discourage, dis-
suade, hamper, hinder, hold, impede, keep, prevent,
protect, restrain, restrict, save, stop. CAUSE, EN-
ABLE, and PREVENT categories of causation and
the corresponding verbs are taken into account in
our guidelines.
As causal relations are often not overtly ex-
pressed in text (Wolff et al., 2005), we restrict the
annotation of CLINKs to the presence of an explicit
11
causal construction linking two events in the same
sentence
1
, as detailed below:
? Basic constructions for CAUSE, ENABLE
and PREVENT categories of causation as
shown in the following examples:
The purchase
S
caused the creation
T
of the cur-
rent building
The purchase
S
enabled the diversification
T
of
their business
The purchase
S
prevented a future transfer
T
? Expressions containing affect verbs, such as
affect, influence, determine, and change. They
can be usually rephrased using cause, enable,
or prevent:
Ogun ACN crisis
S
affects the launch
T
of the
All Progressives Congress? Ogun ACN cri-
sis causes/enables/prevents the launch of the
All Progressives Congress
? Expressions containing link verbs, such as
link, lead, and depend on. They can usually
be replaced only with cause and enable:
An earthquake
T
in North America was linked
to a tsunami
S
in Japan ? An earthquake
in North America was caused/enabled by a
tsunami in Japan
*An earthquake in North America was pre-
vented by a tsunami in Japan
? Periphrastic causatives are generally com-
posed of a verb that takes an embedded clause
or predicate as a complement; for example,
in the sentence The blast
S
caused the boat
to heel
T
violently, the verb (i.e. caused) ex-
presses the notion of CAUSE while the em-
bedded verb (i.e. heel) expresses a particular
result. Note that the notion of CAUSE can
be expressed by verbs belonging to the three
categories previously mentioned (which are
CAUSE-type verbs, ENABLE-type verbs and
PREVENT-type verbs).
? Expressions containing causative conjunc-
tions and prepositions as listed in Section
3.1. Causative conjunctions and prepositions
are annotated as C-SIGNALs and their ID is
1
A typical example of implicit causal construction is rep-
resented by lexical causatives; for example, kill has the em-
bedded meaning of causing someone to die (Huang, 2012). In
the present guidelines, these cases are not included.
to be reported in the c-signalID attribute
of the CLINK.
2
In some contexts, the coordinating conjunction
and can imply causation; given the ambiguity of
this construction and the fact that it is not an ex-
plicit causal construction, however, we do not an-
notate CLINKs between two events connected by
and. Similarly, the temporal conjunctions after and
when can also implicitly assert a causal relation
but should not be annotated as C-SIGNALs and no
CLINKs are to be created (temporal relations have
to be created instead).
3.3 Polarity, factuality and certainty
The polarity attribute, present both in TimeML
and in our guidelines, captures the grammatical
category that distinguishes affirmative and negative
events. Its values are NEG for events which are
negated (for instance, the event cause in Serotonin
deficiency
S
may not cause depression
T
) and POS
otherwise.
The annotation of factuality that we added
to our guidelines is based on the situation to which
an event refers. FACTUAL is used for facts, i.e. sit-
uations that have happened, COUNTERFACTUAL
is used for counterfacts, i.e. situations that have no
real counterpart as they did not take place, NON-
FACTUAL is used for possibilities, i.e. speculative
situations, such as future events, events for which
it is not possible to determine whether they have
happened, and general statements.
The certainty attribute expresses the binary
distinction between certain (value CERTAIN) and
uncertain (value UNCERTAIN) events. Uncer-
tain events are typically marked in the text by the
presence of modals or modal adverbs (e.g. per-
haps, maybe) indicating possibility. In the sentence
Drinking
S
may cause memory loss
T
, the causal con-
nector cause is an example of a NON-FACTUAL
and UNCERTAIN event.
In the annotation algorithm presented in the fol-
lowing section, only the polarity attribute is
taken into account, given that information about
factuality and certainty of events is not annotated
in the TempEval-3 corpus. In particular, at the
time of the writing the algorithm considers only the
polarity of causal verbal connectors, because this
information is necessary to extract causal chains
2
The absence of a value for the c-signalID attribute
means that the causal relation is encoded by a verb.
12
between events in a text. However, adding informa-
tion on the polarity of the single events involved in
the relations would make possible also the identifi-
cation of positive and negative causes and effects.
4 Automatic annotation of explicit
causality between events
In order to verify the soundness of our annotation
framework for event causality, we implement some
simple rules based on the categories and linguistic
cues listed in Section 3. Our goal is two-fold: first,
we want to check how accurate rule-based identifi-
cation of (explicit) event causality can be. Second,
we want to have an estimate of how frequently
causality can be explicitly found in text.
The dataset we annotate has been released for
the TempEval-3 shared task
3
on temporal and event
processing. The TBAQ-cleaned corpus is the train-
ing set provided for the task, consisting of the Time-
Bank (Pustejovsky et al., 2006) and the AQUAINT
corpora. It contains around 100K words in total,
with 11K words annotated as events (UzZaman et
al., 2013). We choose this corpus because gold
events are already provided, and because it allows
us to perform further analyses on the interaction
between temporal and causal relations.
Our automatic annotation pipeline takes as in-
put the TBAQ-cleaned corpus with gold annotated
events and tries to automatically recognize whether
there is a causal relation holding between them.
The annotation algorithm performs the following
steps in sequence:
1. The TBAQ-cleaned corpus is PoS-tagged and
parsed using the Stanford dependency parser
(de Marneffe and Manning, 2008).
2. The corpus is further analyzed with the ad-
dDiscourse tagger (Pitler and Nenkova, 2009),
which automatically identifies explicit dis-
course connectives and their sense, i.e. EX-
PANSION, CONTINGENCY, COMPARISON
and TEMPORAL. This is used to disambiguate
causal connectives (e.g. we consider only the
occurrences of since when it is a causal con-
nective, meaning that it falls into CONTIN-
GENCY class instead of TEMPORAL).
3. Given the list of affect, link, causative verbs
(basic and periphrastic constructions) and
causal signals listed in Sections 3.1 and 3.2,
3
http://www.cs.york.ac.uk/semeval-2013/task1/
the algorithm looks for specific dependency
constructions where the causal verb or signal
is connected to two events, as annotated in the
TBAQ-cleaned corpus.
4. If such dependencies are found, a CLINK is
automatically set between the two events iden-
tifying the source (
S
) and the target (
T
) of the
relation.
5. When a causal connector corresponds to an
event, the algorithm uses the polarity of the
event to assign a polarity to the causal link.
Specific approaches to detect when ambiguous
connectors have a causal meaning are implemented,
as in the case of from and by, where the algorithm
looks for specific structures. For instance, in ?The
building was damaged
T
by the earthquake
S
?, by is
governed by a passive verb annotated as event.
Also the preposition due to is ambiguous as
shown in the following sentences where it acts as a
causal connector only in b):
a) It had been due to expire Friday evening.
b) It cut
T
the dividend due to its third-quarter loss
S
of $992,000.
The algorithm performs the disambiguation by
checking the dependency structures: in sentence a)
there is only one dependency relation xcomp(due,
expire), while in sentence b) the dependency rela-
tions are xcomp(cut, due) and prep to(due, loss).
Besides, both cut and loss are annotated as events.
We are aware that this type of automatic anno-
tation may be prone to errors because it takes into
account only a limited list of causal connectors.
Besides, it only partially accounts for possible am-
biguities of causal cues and may suffer from pars-
ing errors. However, this allows us to make some
preliminary remarks on the amount of causal in-
formation found in the TempEval-3 corpus. Some
statistics are reported in the following subsection.
4.1 Statistics of Automatic Annotation
Basic construction. In Table 1 we report some
statistics on the non-periphrastic structures
identified starting from verbs expressing the three
categories of causation. Note that for the verbs
have, start, hold and keep, even though they
connect two events, we cannot say that there
is always a causal relation between them, as
exemplified in the following sentence taken from
the corpus:
a) Gen. Schwarzkopf secretly picked
S
Saturday
13
night as the optimal time to start the offensive
T
.
b) On Tuesday, the National Abortion and
Reproductive Rights Action League plans
S
to hold
a news conference
T
to screen a TV advertisement.
Types Verbs CLINK
CAUSE
have 1
start 2
cause 1
compel 1
PREVENT
hold 1
keep 3
block 7
prevent 1
ENABLE - -
Total 17
Table 1: Statistics of CLINKs with basic construc-
tion
Affect verbs. The algorithm does not annotate
any causal relation containing affect verbs mostly
because the majority of the 36 affect verb occur-
rences found in the corpus connect two elements
that are not events, as in ?These big stocks greatly
influence the Nasdaq Composite Index.?
Link verbs. In total, we found 50 occurrences of
link verbs in the corpus, but the algorithm identifies
only 4 causal links. Similar to affect verbs, this is
mainly due to the fact that two events are not found
to be involved in the relation. For instance, the
system associated only one CLINK to link (out
of 12 occurrences of the verb) and no CLINKs
to depend (which occurs 3 times). Most of the
CLINKs identified are signaled by the verb lead;
for example, ?Pol Pot is considered responsible for
the radical policies
S
that led to the deaths
T
of as
many as 1.7 million Cambodians.?
Periphrastic causative verbs. Overall, there are
around 1K potential occurrences of periphrastic
causative verbs in the corpus. However, the algo-
rithm identifies only around 14% of them as part
of a periphrastic construction, as shown in Table 2.
This is because some verbs are often used in non-
periphrastic structures, e.g. make, have, get, keep
and hold. Among the 144 cases of periphrastic con-
structions, 41 causal links are found by our rules.
In Table 2, for each verb type, we report the list
of verbs that appear in periphrastic constructions
in the corpus, specifying the number of CLINKs
identified by the system for each of them.
Some other CAUSE-type (move, push, drive, in-
fluence, compel, spur), PREVENT-type (hold, save,
impede, deter, discourage, dissuade, restrict) and
ENABLE-type (aid) verbs occur in the corpus but
are not involved in periphrastic structures. Some
others do not appear in the corpus at all (bribe, im-
pel, incite, induce, inspire, rouse, stimulate, hinder,
restrain).
Types Verbs Periphr. CLINK All
CAUSE
have 34 0 239
make 6 2 125
get 1 0 50
lead 2 1 38
send 5 1 34
set 2 0 23
start 1 0 22
force 2 1 15
cause 3 2 12
prompt 3 2 6
persuade 2 1 3
convince 1 1 2
PREVENT
keep 1 1 58
stop 3 0 24
block 2 2 21
protect 2 1 15
prevent 6 2 12
hamper 1 0 2
bar 1 0 1
constrain 1 0 1
ENABLE
help 31 13 45
leave 2 2 45
allow 22 3 39
permit 2 1 6
enable 4 2 5
let 4 3 5
Total 144 41 848
Table 2: Statistics of periphrastic causative verbs
Causal signals. Similar to periphrastic causative
verbs, out of around 1.2K potential causal connec-
tors found in the corpus, only 194 are automatically
recognized as actual causal signals after disam-
biguation, as detailed in Table 3. Based on these
identified causal signals, the algorithm derives 111
CLINKs.
Even though the addDiscourse tool labels 11
occurrences of the adverbial connector so as having
a causal meaning, our algorithm does not annotate
any CLINKs for such connector. In most cases, it
is because it acts as an inter-sentential connector,
while we limit the annotation of CLINKs only to
events occurring within the same sentence.
CLINKs polarity. Table 4 shows the distribution
of the positive and negative polarity of the detected
CLINKs.
Only two cases of negated CLINKs are automat-
ically identified in the corpus. One example is the
following: ?Director of the U.S. Federal Bureau of
14
Types C-SIGNALs Causal CLINK All
prep.
because of 32 11 32
on account of 0 0 0
as a result of 13 9 13
in response to 7 1 7
due to 2 1 6
from 2 2 500
by 23 24 465
conj.
because 58 37 58
since 26 19 72
so that 5 4 5
adverbial
as a result 3 0 3
so 11 0 69
therefore 4 0 4
thus 6 2 6
hence 0 0 0
thereby 1 0 1
consequently 1 1 1
clausal
the result is 0 0 0
the reason why 0 0 0
that is why 0 0 0
Total 194 111 1242
Table 3: Statistics of causal signals in CLINKs
Investigation (FBI) Louis Freeh said here Friday
that U.S. air raid
T
on Afghanistan and Sudan is
not directly linked with the probe
S
into the August
7 bombings in east Africa.?
Connector types POS NEG
Basic
CAUSE 5 0
PREVENT 12 0
ENABLE - -
Affect verbs - -
Link verbs 3 1
Periphrastic
CAUSE 10 1
PREVENT 6 0
ENABLE 24 0
Total 60 2
Table 4: Statistics of CLINKs? polarity
CLINKs vs TLINKs. In total, the algorithm iden-
tifies 173 CLINKs in the TBAQ-cleaned corpus,
while the total number of TLINKs between pairs of
events is around 5.2K. For each detected CLINK
between an event pair, we identify the underlying
temporal relations (TLINKs) if any. We found that
from the total of CLINKs extracted, around 33%
of them have an underlying TLINK, as detailed in
Table 5. Most of them are CLINKs signaled by
causal signals.
For causative verbs, the BEFORE relation is the
only underlying temporal relation type, with the
exception of one SIMULTANEOUS relation.
As for C-SIGNALs, the distribution of temporal
relation types is less homogeneous, as shown in Ta-
ble 6. In most of the cases, the underlying temporal
relation is BEFORE. In few cases, CLINKs sig-
Connector types CLINK TLINK
Basic
CAUSE 5 2
PREVENT 12 0
ENABLE - -
Affect verbs - -
Link verbs 4 1
Periphrastic
CAUSE 11 1
PREVENT 6 0
ENABLE 24 0
C-SIGNALs 111 54
Total 173 58
Table 5: Statistics of CLINKs? overlapping with
TLINKs
naled by the connector because overlap with an AF-
TER relation, as in ?But some analysts questioned
T
how much of an impact the retirement package will
have, because few jobs will end
S
up being elimi-
nated.?
In some cases, CLINKs signaled by the con-
nector since match with a BEGINS relation. This
shows that since expresses merely a temporal and
not a causal link. As it has been discussed before,
the connector since is highly ambiguous and the
CLINK has been wrongly assigned because of a
disambiguation mistake of the addDiscourse tool.
5 Evaluation
We perform two types of evaluation. The first is
a qualitative one, and is carried out by manually
inspecting the 173 CLINKs that have been auto-
matically annotated. The second is a quantitative
evaluation, and is performed by comparing the au-
tomatic annotated data with a gold standard corpus
of 100 documents taken from TimeBank.
5.1 Qualitative Evaluation
The automatically annotated CLINKs have been
manually checked in order to measure the precision
of the adopted procedure. Out of 173 annotated
CLINKs, 105 were correctly identified obtaining a
precision of 0.61.
Details on precision calculated on the different
types of categories and linguistic cues defined in
Section 3.2 are provided in Table 7. Statistics show
that performances vary widely depending on the
category and linguistic cue taken into consideration.
In particular, relations expressing causation of PRE-
VENT type prove to be extremely difficult to be
correctly detected with a rule-based approach: the
algorithm precision is 0.25 for basic constructions
and 0.17 for periphrastic constructions.
During the manual evaluation, two main types
15
C-SIGNALs BEFORE AFTER IS INCLUDED BEGINS others
because of 5 - - - -
as a result of 2 - - - -
in response to 1 - - - -
due to 1 - - - -
by 11 - 1 2 3
because 14 2 1 - 1
since 4 1 - 3 -
so that 1 - - - -
thus 1 - - - -
Total 40 3 2 5 4
Table 6: Statistics of CLINKs triggered by C-SIGNALs overlapping with TLINKs
Connector types Extracted Correct P
Basic
CAUSE 5 3 0.60
PREVENT 12 3 0.25
ENABLE 0 n.a. n.a.
Affect Verbs 0 n.a. n.a.
Link Verbs 4 3 0.75
Periphrastic
CAUSE 11 8 0.73
PREVENT 6 1 0.17
ENABLE 24 17 0.71
C-SIGNALs 111 70 0.63
Total 173 105 0.61
Table 7: Precision of automatically annotated
CLINKs
of mistakes have been observed: the wrong iden-
tification of events involved in CLINKs and the
annotation of sentences that do not contain causal
relations.
The assignment of a wrong source or a wrong
target to a CLINK is primarily caused by the de-
pendency parser output that tends to establish a
connection between a causal verb or signal and the
closest previous verb. For example, in the sentence
?StatesWest Airlines said it withdrew
T
its offer to
acquire Mesa Airlines because the Farmington car-
rier did not respond
S
to its offer?, the CLINK is
annotated between respond and acquire instead of
between respond and withdrew. On the other hand,
dependency structure is very effective in identify-
ing cases where one event is the consequence or
the cause of multiple events, as in ?The president
offered to offset
T
Jordan?s costs because 40% of
its exports go
S
to Iraq and 90% of its oil comes
S
from there.? In this case, the algorithm annotates a
causal link between go and offset, and also between
comes and offset.
The annotation of CLINKs in sentences not con-
taining causal relations is strongly related to the
ambiguous nature of many verbs, prepositions and
conjunctions, which encode a causal meaning or
express a causal relation only in some specific
contexts. For instance, many mistakes are due to
the erroneous disambiguation of the conjunction
since. According to the addDiscourse tool, since is
a causal connector in around one third of the cases,
as in ?For now, though, that would be a theoretical
advantage since the authorities have admitted they
have no idea where Kopp is.? However, there are
many cases where the outcome of the tool is not
perfect, as in ?Since then, 427 fugitives have been
taken into custody or located, 133 of them as a
result of citizen assistance, the FBI said?, where
since acts as a temporal conjunction.
5.2 Quantitative Evaluation
In order to perform also a quantitative evaluation of
our automatic annotation, we manually annotated
100 documents taken from the TimeBank corpus
according to the annotation guidelines discussed
before. We then used this data set as a gold stan-
dard.
The agreement reached by two annotators on a
subset of 5 documents is 0.844 Dice?s coefficient
on C-SIGNALS (micro-average over markables)
and of 0.73 on CLINKS.
We found that there are several cases where the
algorithm failed to recognize causal links due to
events that were originally not annotated in Time-
Bank. Therefore, as we proceed with the manual
annotation, we also annotated missing events that
are involved in causal relations. Table 8 shows that,
in creating the gold standard, we annotated 61 new
events. As a result, we have around 52% increase
in the number of CLINKs. Nevertheless, explicit
causal relations between events are by far less fre-
quent than temporal ones, with an average of 1.4
relations per document.
If we compare the coverage of automatic anno-
tation with the gold standard data (without newly
added events, to be fair), we observe that automatic
annotation covers around 76% of C-SIGNALs and
only around 55% of CLINKs. This is due to the
limitation of the algorithm that only considers a
16
Annotation EVENT C-SIGNAL CLINK
manual 3933 78 144
manual-w/o new events 3872 78 95
automatic 3872 59 52
Table 8: Statistics of causality annotation in manual
versus automatic annotation
precision recall F1-score
C-SIGNAL 0.64 0.49 0.55
CLINK 0.42 0.23 0.30
Table 9: Automatic annotation performance
small list of causal connectors. Some examples of
manually annotated causal signals that are not in
the list used by the algorithm include due mostly
to, thanks in part to and in punishment for.
Finally, we evaluate the performance of the algo-
rithm for automatic annotation (shown in Table 9)
by computing precision, recall and F1 on gold stan-
dard data without newly added events. We observe
that our rule-based approach is too rigid to capture
the causal information present in the data. In partic-
ular, it suffers from low recall as regards CLINKs.
We believe that this issue may be alleviated by
adopting a supervised approach, where the list of
verbs and causal signals would be included in a
larger feature set, considering among others the
events? position, their PoS tags, the dependency
path between the two events, etc.
6 Conclusions
In this paper, we presented our guidelines for an-
notating causality between events. We further tried
to automatically identify in TempEval-3 corpus the
types of causal relations described in the guide-
lines by implementing some simple rules based on
causal cues and dependency structures.
In a manual revision of the annotated causal
links, we observe that the algorithm obtains a pre-
cision of 0.61, with some issues related to the class
of PREVENT verbs. Some mistakes are introduced
by the tools used for parsing and for disambiguat-
ing causal signals, which in turn impact on our
annotation algorithm. Another issue, more related
to recall, is that in the TBAQ-cleaned corpus not all
events are annotated, because it focuses originally
on events involved in temporal relations. There-
fore, the number of causal relations identified auto-
matically would be higher if we did not take into
account this constraint.
From the statistics presented in Section 4.1, we
can observe that widely used verbs such as have or
keep express causality relations only in few cases.
The same holds for affect verbs, which are never
found in the corpus with a causal meaning, and for
link verbs. This shows that the main sense of causal
verbs usually reported in the literature is usually
the non-causal one.
Recognizing CLINKs based on causal signals is
more straightforward, probably because very fre-
quent ones such as because of and as a result are
not ambiguous. Others, such as by, can be identi-
fied based on specific syntactic constructions.
As for the polarity of CLINKs, which is a very
important feature to discriminate between actual
and negated causal relations, this phenomenon is
not very frequent (only 2 cases) and can be easily
identified through dependency relations.
We chose to automatically annotate TBAQ-
cleaned corpus because one of our goals was to
investigate how TLINKs and CLINKs interact.
However, this preliminary study shows that there
are only few overlaps between the two relations,
again with C-SIGNALs being more informative
than causal verbs. This may be biased by the fact
that, according to our annotation guidelines, only
explicit causal relations are annotated. Introducing
also the implicit cases would probably increase the
overlap between TLINKs and CLINKs, because
annotator would be allowed to capture the tempo-
ral constrains existing in causal relations even if
the are not overtly expressed.
In the near future, we will complete the manual
annotation of TempEval-3 corpus with causal in-
formation in order to have enough data for training
a supervised system, in which we will incorpo-
rate the lessons learnt with this first analysis. We
will also investigate the integration of the proposed
guidelines into the Grounded Annotation Format
(Fokkens et al., 2013), a formal framework for cap-
turing semantic information related to events and
participants at a conceptual level.
Acknowledgments
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project (ICT-
316404).
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?
843, November.
17
Steven Bethard, William Corvey, Sara Klingenstein,
and James H. Martin. 2008. Building a corpus of
temporal-causal structure. In European Language
Resources Association (ELRA), editor, Proceedings
of the Sixth International Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, may.
Claire Bonial, Olga Babko-Malaya, Jinho D.
Choi, Jena Hwang, and Martha Palmer.
2010. Propbank annotation guidelines, De-
cember. http://www.ldc.upenn.edu/
Catalog/docs/LDC2011T03/propbank/
english-propbank.pdf.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8. Association for Com-
putational Linguistics.
Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, Luciano Ser-
afini, Rachele Sprugnoli, and Jesper Hoeksema.
2013. GAF: A Grounded Annotation Framework
for Events. In Workshop on Events: Definition, De-
tection, Coreference, and Representation, pages 11?
20, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13?18, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
C?ecile Grivaz. 2010. Human Judgements on Causation
in French Texts. In Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may. Euro-
pean Language Resources Association (ELRA).
Li-szu Agnes Huang. 2012. The Effectiveness of a
Corpus-based Instruction in Deepening EFL Learn-
ers? Knowledge of Periphrastic Causatives. TESOL
Journal, 6:83?108.
ISO TimeML Working Group. 2008. ISO TC37 draft
international standard DIS 24617-1, August 14.
http://semantic-annotation.uvt.nl/
ISO-TimeML-08-13-2008-vankiyong.
pdf.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medi-
cal database using graphical patterns. In In Proceed-
ings of 38th Annual Meeting of the ACL, Hong Kong,
2000, pages 336?343.
Emily Pitler and Ani Nenkova. 2009. Using syn-
tax to disambiguate explicit discourse connectives
in text. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 13?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
James Pustejovsky, J. Castano, R. Ingria, Roser Saur??,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2003. TimeML: Robust specification of event and
temporal expressions in text. In Proceedings of the
Fifth International Workshop on Computational Se-
mantics.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006. Timebank 1.2 documenta-
tion. Technical report, Brandeis University, April.
Hiroki Sakaji, Satoshi Sekine, and Shigeru Masuyama.
2008. Extracting causal knowledge using clue
phrases and syntactic patterns. In Proceedings of the
7th International Conference on Practical Aspects
of Knowledge Management, PAKM ?08, pages 111?
122, Berlin, Heidelberg. Springer-Verlag.
Leonard Talmy. 1988. Force dynamics in language
and cognition. Cognitive science, 12(1):49?100.
The PDTB Research Group. 2008. The PDTB 2.0. An-
notation Manual. Technical Report IRCS-08-01, In-
stitute for Research in Cognitive Science, University
of Pennsylvania.
Sara Tonelli, Rachele Sprugnoli, and Manuela Sper-
anza. 2014. NewsReader Guidelines for Annotation
at Document Level, Extension of Deliverable
D3.1. Technical Report NWR-2014-2, Fondazione
Bruno Kessler. https://docs.google.
com/viewer?url=http%3A%2F%2Fwww.
newsreader-project.eu%2Ffiles%
2F2013%2F01%2FNWR-2014-2.pdf.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating events, time expressions, and temporal
relations. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 75?80, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Phillip Wolff and Grace Song. 2003. Models of cau-
sation and the semantics of causal verbs. Cognitive
Psychology, 47(3):276?332.
18
Phillip Wolff, Bianca Klettke, Tatyana Ventura, and
Grace Song. 2005. Expressing causation in english
and other languages. Categorization inside and out-
side the laboratory: Essays in honor of Douglas L.
Medin, pages 29?48.
Phillip Wolff. 2007. Representing causation. Journal
of experimental psychology: General, 136(1):82.
19
