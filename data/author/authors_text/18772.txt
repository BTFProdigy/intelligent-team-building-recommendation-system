Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 446?456,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Grammatical Relations in Chinese:
GB-Ground Extraction and Data-Driven Parsing
Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding, Xiaojun Wan
?
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,duyantao,kouxin,wanxiaojun}@pku.edu.cn, dsy100@gmail.com
Abstract
This paper is concerned with building linguistic re-
sources and statistical parsers for deep grammatical
relation (GR) analysis of Chinese texts. A set of
linguistic rules is defined to explore implicit phrase
structural information and thus build high-quality
GR annotations that are represented as general di-
rected dependency graphs. The reliability of this
linguistically-motivated GR extraction procedure is
highlighted by manual evaluation. Based on the
converted corpus, we study transition-based, data-
driven models for GR parsing. We present a novel
transition system which suits GR graphs better than
existing systems. The key idea is to introduce a new
type of transition that reorders top k elements in the
memory module. Evaluation gauges how successful
GR parsing for Chinese can be by applying data-
driven models.
1 Introduction
Grammatical relations (GRs) represent functional
relationships between language units in a sen-
tence. They are exemplified in traditional gram-
mars by the notions of subject, direct/indirect
object, etc. GRs have assumed an important
role in linguistic theorizing, within a variety of
approaches ranging from generative grammar to
functional theories. For example, several com-
putational grammar formalisms, such as Lexi-
cal Function Grammar (LFG; Bresnan and Ka-
plan, 1982; Dalrymple, 2001) and Head-driven
Phrase Structure Grammar (HPSG; Pollard and
Sag, 1994) encode grammatical functions directly.
In particular, GRs can be viewed as the depen-
dency backbone of an LFG analysis that provide
general linguistic insights, and have great potential
advantages for NLP applications, (Kaplan et al,
2004; Briscoe and Carroll, 2006; Clark and Cur-
ran, 2007a; Miyao et al, 2007).
?
Email correspondence.
In this paper, we address the question of an-
alyzing Chinese sentences with deep GRs. To
acquire high-quality GR corpus, we propose a
linguistically-motivated algorithm to translate a
Government and Binding (GB; Chomsky, 1981;
Carnie, 2007) grounded phrase structure treebank,
i.e. Chinese Treebank (CTB; Xue et al, 2005)
to a deep dependency bank where GRs are ex-
plicitly represented. Different from popular shal-
low dependency parsing that focus on tree-shaped
structures, our GR annotations are represented as
general directed graphs that express not only lo-
cal but also various long-distance dependencies,
such as coordinations, control/raising construc-
tions, topicalization, relative clauses and many
other complicated linguistic phenomena that goes
beyond shallow syntax (see Fig. 1 for example.).
Manual evaluation highlights the reliability of our
linguistically-motivated GR extraction algorithm:
The overall dependency-based precision and recall
are 99.17 and 98.87. The automatically-converted
corpus would be of use for a wide variety of NLP
tasks.
Recent years have seen the introduction of a
number of treebank-guided statistical parsers ca-
pable of generating considerably accurate parses
for Chinese. With the high-quality GR resource
at hand, we study data-driven GR parsing. Previ-
ous work on dependency parsing mainly focused
on structures that can be represented in terms of
directed trees. We notice two exceptions. Sagae
and Tsujii (2008) and Titov et al (2009) individ-
ually studied two transition systems that can gen-
erate more general graphs rather than trees. In-
spired by their work, we study transition-based
models for building deep dependency structures.
The existence of a large number of crossing arcs in
GR graphs makes left-to-right, incremental graph
spanning computationally hard. Applied to our
data, the two existing systems cover only 51.0%
and 76.5% GR graphs respectively. To better suit
446
?? ?? ? ?? ?? ? ?? ?? ?? ? ??? ??
Pudong recently issue practice involve economic field regulatory document
root root
comp
temp
temp
subj
subj
prt
prt
obj
obj
comp
subj*ldd
obj
nmod
relative
nmod
Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field.
The symbol ?*ldd? indicates long-distance dependencies; ?subj*ldd? between the word ???/involve?
and the word ???/documents? represents a long-range subject-predicate relation. The arguments and
adjuncts of the coordinated verbs, namely ???/issue? and ???/practice,? are separately yet distribu-
tively linked the two heads.
our problem, we extend Titov et al?s work and
study what we call K-permutation transition sys-
tem. The key idea is to introduce a new type of
transition that reorders top k (2 ? k ? K) el-
ements in the memory module of a stack-based
transition system. With the increase of K, the ex-
pressiveness of the corresponding system strictly
increases. We propose an oracle deriving method
which is guaranteed to find a sound transition se-
quence if one exits. Moreover, we introduce an
effective approximation of that oracle, which de-
creases decoding ambiguity but practically covers
almost exactly the same graphs for our data.
Based on the stronger transition system, we
build a GR parser with a discriminative model for
disambiguation and a beam decoder for inference.
We conduct experiments on CTB 6.0 to profile this
parser. With the increase of the K, the parser is
able to utilize more GR graphs for training and
the numeric performance is improved. Evaluation
gauges how successful GR parsing for Chinese
can be by applying data-driven models. Detailed
analysis reveal some important factors that may
possibly boost the performance. To our knowl-
edge, this work provides the first result of exten-
sive experiments of parsing Chinese with GRs.
We release our GR processing kit and gold-
standard annotations for research purposes. These
resources can be downloaded at http://www.
icst.pku.edu.cn/lcwm/omg.
2 GB-grounded GR Extraction
In this section, we discuss the construction of the
GR annotations. Basically, the annotations are au-
tomatically converted from a GB-grounded phrase-
structure treebank, namely CTB. Conceptually,
this conversion is similar to the conversions from
CTB structures to representations in deep gram-
mar formalisms (Tse and Curran, 2010; Yu et al,
2010; Guo et al, 2007; Xia, 2001). However, our
work is grounded in GB, which is the linguistic ba-
sis of the construction of CTB. We argue that this
theoretical choice makes the conversion process
more compatible with the original annotations and
therefore more accurate. We use directed graphs to
explicitly encode bi-lexical dependencies involved
in coordination, raising/control constructions, ex-
traction, topicalization, and many other compli-
cated phenomena. Fig. 1 shows an example of
such a GR graph and its original CTB annotation.
2.1 Linguistic Basis
GRs are encoded in different ways in different lan-
guages. In some languages, e.g. Turkish, gram-
matical function is encoded by means of morpho-
logical marking, while in highly configurational
languages, e.g. Chinese, the grammatical function
of a phrase is heavily determined by its constituent
structure position. Dominant Chomskyan theo-
ries, including GB, have defined GRs as configu-
rations at phrase structures. Following this princi-
ple, CTB groups words into constituents through
the use of a limited set of fundamental grammat-
ical functions. Transformational grammar utilizes
empty categories (ECs) to represent long-distance
dependencies. In CTB, traces are provided by
relating displaced linguistic material to where it
should be interpreted semantically. By exploiting
configurational information, traces and functional
tag annotations, GR information can be hopefully
447
IP
VP
?=?
VP
?=?
NP
?=(? OBJ)
NP
?=?
NN
?=?
??
NN
??(? NMOD)
???
CP
??(? REL)
DEC
?=?
?
IP
?=(? COMP)
VP
?=?
NP
?=(? OBJ)
NN
?=?
??
NP
??(? NMOD)
NN
?=?
??
VV
?=?
??
NP
?=(? SBJ)
-NONE-
*T*
AS
?=(? PRT)
?
VCD
?=?
VV
???
??
VV
???
??
LCP
??(? TMP)
LC
?=?
?
NP
?=(? COMP)
NT
?=?
??
NP
?=(? SBJ)
NR
??
Figure 2: The original CTB annotation augmented with LFG-like f-structure annotations of the running
example.
derived from CTB trees with high accuracy.
2.2 The Extraction Algorithm
Our treebank conversion algorithm borrows
key insights from Lexical Functional Grammar
(LFG; Bresnan and Kaplan, 1982; Dalrymple,
2001). LFG posits two levels of representation:
c(onstituent)-structure and f(unctional)-structure
minimally. C-structure is represented by phrase-
structure trees, and captures surface syntactic con-
figurations such as word order, while f-structure
encodes grammatical functions. It is easy to ex-
tract a dependency backbone which approximates
basic predicate-argument-adjunct structures from
f-structures. The construction of the widely used
PARC DepBank (King et al, 2003) is a good ex-
ample.
LFG relates c-structure and f-structure through
f-structure annotations, which compositionally
map every constituent to a corresponding f-
structure. Borrowing this key idea, we translate
CTB trees to dependency graphs by first augment-
ing each constituency with f-structure annotations,
then propagating the head words of the head or
conjunct daughter(s) upwards to their parents, and
finally creating a dependency graph. The follow-
ing presents details step-by-step.
Tapping implicit information. Xue (2007) in-
troduced a systematic study to tap the implicit
functional information of CTB. This gives us a
very good start to extract GRs. We slightly modify
their method to enrich a CTB tree with f-structure
annotations: Each node in a resulting tree is anno-
tated with one and only one corresponding equa-
tion. See Fig. 2 for example. Comparing the orig-
inal annotation and enriched one, we can see that
the functionality of this step is to explicitly repre-
sent and regulate grammatical functions.
Beyond CTB annotations: tracing more. Nat-
ural languages do not always interpret linguistic
material locally. In order to obtain accurate and
complete GR, predicate-argument, or logical form
representations, a hallmark of deep grammars is
that they usually involve a non-local dependency
resolution mechanism. CTB trees utilize ECs and
coindexed materials to represent long-distance de-
pendencies. An EC is a nominal element that does
not have any phonological content and is therefore
unpronounced. Two kinds of anaphoric ECs, i.e.
big PRO and trace, are annotated in CTB. Theo-
retically speaking, only trace is generated as the
result of movement and therefore annotated with
antecedents in CTB. We carefully check the anno-
tation and find that a considerable amount of an-
tecedents are not labeled, and hence a lot of impor-
448
VP{??,??}
VP{??,??}
NP{??}
NP{??}CP{?}
DEC{?}IP{??}
VP{??}NP{??*ldd}
AS{?}VP{??,??}
LCP{?}
Figure 3: An example of lexicalized tree after
head word upward passing. Only partial result is
shown. The long-distance dependency between
???/involve? and ???/document? is created
through copying the dependent to a coindexed
anaphoric EC position.
tant non-local information is missing. In addition,
since the big PRO is also anaphoric, it is possible
to find coindexed components sometimes. Such
non-local information is also very valuable.
Beyond CTB annotations, we introduce a num-
ber of phrase-structure patterns to extract more
non-local dependencies. The method heavily
leverages linguistic rules to exploit structural in-
formation. We take into account both theoreti-
cal assumptions and analyzing practices to enrich
coindexation information according to phrase-
structure patterns. In particular, we try to link
an anaphoric EC e with its c-commonders if no
non-empty antecedent has already been coindexed
with e. Because the CTB is influenced deeply by
the X-bar syntax, which regulates constituent anal-
ysis much, the number of our linguistic rules is
quite modest. For the development of conversion
rules, we used the first 9 files of CTB, which con-
tains about 100 sentences. Readers can refer to
the well-documented Perl script for details. See
Fig. 2 for example. The noun phrase ????
??/regulatory documents? is related to the trace
?*T*.? This coindexation is not labeled by the
original annotation.
Passing head words and linking ECs. Based
on an enriched tree, our algorithm propagates the
head word of the head daughter upwards to their
parents, linking coindexed units, and finally creat-
ing a GR graph. The partial result after head word
passing of the running example is shown in Fig. 3.
There are two differences of the head word passing
between our GR extraction and a ?normal? depen-
dency tree extraction. First, the GR extraction pro-
cedure may pass multiple head words to its parent,
especially in a coordination construction. Second,
Precision Recall F-score
Unlabeled 99.48 99.17 99.32
Labeled 99.17 98.87 99.02
Table 1: Manual evaluation of 209 sentences.
long-distance dependencies are created by linking
ECs and their coindexed phrases.
2.3 Manual Evaluation
To have a precise understanding of whether our ex-
traction algorithm works well, we have selected 20
files that contains 209 sentences in total for man-
ual evaluation. Linguistic experts carefully exam-
ine the corresponding GR graphs derived by our
extraction algorithm and correct all errors. In other
words, a gold standard GR annotation set is cre-
ated. The measure for comparing two dependency
graphs is precision/recall of GR tokens which are
defined as ?w
h
, w
d
, l? tuples, wherew
h
is the head,
w
d
is the dependent and l is the relation. Labeled
precision/recall (LP/LR) is the ratio of tuples cor-
rectly identified by the automatic generator, while
unlabeled precision/recall (UP/UR) is the ratio re-
gardless of l. F-score is a harmonic mean of pre-
cision and recall. These measures correspond to
attachment scores (LAS/UAS) in dependency tree
parsing. To evaluate our GR parsing models that
will be introduced later, we also report these met-
rics.
The overall performance is summarized in Tab.
1. We can see that the automatical GR extraction
achieves relatively high performance. There are
two sources of errors in treebank conversion: (1)
inadequate conversion rules and (2) wrong or in-
consistent original annotations. During the cre-
ation of the gold standard corpus, we find that
the former is mainly caused by complicated un-
bounded dependencies and the lack of internal
structure for some kinds of phrases. Such prob-
lems are very hard to solve through rules only, if
not possible, since original annotations do not pro-
vide sufficient information. The latter problem is
more scattered and unpredictable.
2.4 Statistics
Allowing non-projective dependencies generally
makes parsing either by graph-based or transition-
based dependency parsing harder. Substantial re-
search effort has been devoted in recent years to
the design of elegant solutions for this problem.
There are much more crossing arcs in the GR
449
graphs than syntactic dependency trees. In the
training data (defined in Section 4.1), there are
558132 arcs and 86534 crossing pairs, About half
of the sentences have crossing arcs (10930 out of
22277). The wide existence of crossing arcs poses
an essential challenge for GR parsing, namely, to
find methods for handling crossing arcs without a
significant loss in accuracy and efficiency.
3 Transition-based GR Parsing
The availability of large-scale treebanks has con-
tributed to the blossoming of statistical approaches
to build accurate shallow constituency and depen-
dency parsers. With high-quality GR resources at
hand, it is possible to study statistical approaches
to automatically parse GR graphs. In this section,
we investigate the feasibility of applying a data-
driven, grammar-free approach to build GRs di-
rectly. In particular, transition-based dependency
parsing method is studied.
3.1 Data-Driven Dependency Parsing
Data-driven, grammar-free dependency parsing
has received an increasing amount of attention in
the past decade. Such approaches, e.g. transition-
based (Yamada and Matsumoto, 2003; Nivre,
2008) and graph-based (McDonald, 2006; Tor-
res Martins et al, 2009) models have attracted
the most attention of dependency parsing in re-
cent years. Transition-based parsers utilize tran-
sition systems to derive dependency trees together
with treebank-induced statistical models for pre-
dicting transitions. This approach was pioneered
by (Yamada and Matsumoto, 2003) and (Nivre
et al, 2004). Most research concentrated on sur-
face syntactic structures, and the majority of ex-
isting approaches are limited to producing only
trees. We notice two exceptions. Sagae and Tsu-
jii (2008) and Titov et al (2009) individually in-
troduced two transition systems that can generate
specific graphs rather than trees. Inspired by their
work, we study transition-based approach to build
GR graphs.
3.2 Transition Systems
Following (Nivre, 2008), we define a transition
system for dependency parsing as a quadruple S =
(C, T, c
s
, C
t
), where
1. C is a set of configurations, each of which
contains a buffer ? of (remaining) words and
a set A of dependency arcs,
Transitions
SHIFT (?, j|?,A)? (?|j, ?,A)
LEFT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(j, l, i)})
RIGHT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(i, l, j)})
POP (?|i, ?, A)? (?, ?,A)
ROTATE
k
(?|i
k
| . . . |i
2
|i
1
, ?, A)? (?|i
1
|i
k
| . . . |i
2
, ?, A)
Table 2: K-permutation System.
2. T is a set of transitions, each of which is a
(partial) function t : C 7? C,
3. c
s
is an initialization function, mapping a
sentence x to a configuration, with ? =
[1, . . . , n],
4. C
t
? C is a set of terminal configurations.
Given a sentence x = w
1
, . . . , w
n
and a graph
G = (V,A) on it, if there is a sequence of tran-
sitions t
1
, . . . , t
m
and a sequence of configura-
tions c
0
, . . . , c
m
such that c
0
= c
s
(x), t
i
(c
i?1
) =
c
i
(i = 1, . . . ,m), c
m
? C
t
, and A
c
m
= A, we say
the sequence of transitions is an oracle sequence.
And we define
?
A
c
i
= A ? A
c
i
for the arcs to be
built in c
i
. In a typical transition-based parsing
process, the input words are put into a queue and
partially built structures are organized by a stack.
A set of SHIFT/REDUCE actions are performed se-
quentially to consume words from the queue and
update the partial parsing results.
3.3 Online Reordering
Among existing systems, Sagae and Tsujii?s is de-
signed for projective graphs (denoted by G
1
in
Definition 1), and Titov et al?s handles only a
specific subset of non-projective graphs as well
as projective graphs (G
2
). Applied to our data,
only 51.0% and 76.5% of the extracted graphs are
parsable with their systems. Obviously, it is nec-
essary to investigate new transition systems for the
parsing task in our study. To deal with crossing
arcs, Titov et al (2009) and Nivre (2009) designed
a SWAP transition that switches the position of the
two topmost nodes on the stack. Inspired by their
work, we extend this approach to parse more gen-
eral graphs. The basic idea is to provide our new
system with an ability to reorder more nodes dur-
ing decoding in an online fashion, which we refer
to as online reordering.
3.4 K-Permutation System
We define a K-permutation transition system
S
K
= (C, T, c
s
, C
t
), where a configuration c =
450
(?, ?,A) ? C contains a stack ? of nodes be-
sides ? and A. We set the initial configuration
for a sentence x = w
1
, . . . , w
n
to be c
s
(x) =
([], [1, . . . , n], {}), and take C
t
to be the set of all
configurations of the form c
t
= (?, [], A) (for any
arc set A). The set of transitions T contains five
types of actions, as shown in Tab. 2:
1. SHIFT removes the front element from ? and
pushes it onto ?.
2. LEFT-ARC
l
/RIGHT-ARC
l
updates a configu-
ration by adding (i, l, j)/(j, l, i) to A where i
is the top of ?, and j is the front of ?.
3. POP deletes the top element of ?.
4. ROTATE
k
updates a configuration with stack
?|i
k
| . . . |i
2
|i
1
by rotating the top k nodes
in stack left by one index, obtaining
?|i
1
|i
k
| . . . |i
2
, with constraint 2 ? k ? K.
We refer to this system as K-permutation because
by rotating the top k (2 ? k ? K) nodes in the
stack, we can obtain all the permutations of the
top K nodes. Note that S
2
is identical to Titov
et al?s; S
?
is complete with respect to the class of
all directed graphs without self-loop, since we can
arbitrarily permute the nodes in the stack. The K-
permutation system exhibits a nice property: The
sets of corresponding graphs are strictly mono-
tonic with respect to the ? operation.
Definition 1. If a graphG can be parsed with tran-
sition system S
K
, we say G is a K-perm graph.
We use G
K
to denote the set of all k-perm graphs.
Specially, G
0
= ?, G
1
is the set of all projective
graphs, and G
?
=
?
?
k=0
G
k
.
Theorem 1. G
i
( G
i+1
,?i ? 0.
Proof. It is obvious that G
i
? G
i+1
and G
0
( G
1
.
Fig. 4 gives an example which is in G
i+1
but not
in G
i
for all i > 0, indicating G
i
6= G
i+1
.
Theorem 2. G
?
is the set of all graphs without
self-loop.
Proof. It follows immediately from the fact that
G ? G
|V |
, ?G = ?V,E?.
The transition systems introduced in (Sagae and
Tsujii, 2008) and (Titov et al, 2009) can be viewed
as S
1
1
and S
2
.
1
Though Sagae and Tsujii (2008) introduced additional
constraints to exclude cyclic path, the fundamental transition
mechanism of their system is the same to S
1
.
w
1
? ? ? w
i
w
i+1
? ? ? w
2i
w
2i+1
w
2i+2
Figure 4: A graph which is in G
i+1
, but not in G
i
.
3.5 Normal Form Oracle
The K-permutation transition system may allow
multiple oracle transition sequences on one graph,
but trying to sum all the possible oracles is usu-
ally computational expensive. Here we give a con-
struction procedure which is guaranteed to find an
oracle sequence if one exits. We refer it as normal
form oracle (NFO).
Let L(j) be the ordered list of nodes connected
to j in
?
A
c
i?1
for j ? ?
c
i?1
, and let L
K
(?
c
i?1
) =
[L(j
1
), . . . , L(j
max{l,K})
]. If ?
c
i?1
is empty, then
we set t
i
to SHIFT; if there is no arc linked to
j
1
in
?
A
c
i?1
, then we set t
i
to POP; if there exits
a ?
?
A
c
i?1
linking j
1
and b, then we set t
i
to LEFT-
ARC or RIGHT-ARC correspondingly. When there
are only SHIFT and ROTATE left, we first apply
a sequence of ROTATE?s to make L
K
(?) com-
plete ordered by lexicographical order, then apply
a SHIFT. Let c
i
= t
i
(c
i?1
), we continue to com-
pute t
i+1
, until ?
c
i
is empty.
Theorem 3. If a graph is parsable with the transi-
tion system S
K
then the construction procedure is
guaranteed to find an oracle transition sequence.
Proof. During the construction, all the arcs are
built by LEFT-ARC or RIGHT-ARC, which links
the top of the stack and the front of the buffer.
Therefore, we prefer L(?) to be as orderly as pos-
sible, to make the words to be linked sooner on the
top of the stack. the construction procedure above
does best within the power of the system S
K
.
3.6 An Approximation for NFO
In the construction of NFO transitions, we ex-
haustively use the ROTATE?s to make L(?) com-
plete ordered. We also observed that the tran-
sition LEFT-ARC, RIGHT-ARC and SHIFT only
change the relative order between the first element
of L(?) and the rest elements. Therefore we ex-
plored an approximate procedure to determine the
ROTATE?s, based on the observation. We call it ap-
proximate NFO (ANFO). Using notation defined
in Section 3.5, the approximate procedure goes as
follows. When it comes to the determination of
451
w1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9
Figure 5: A graph that can be parsed
with S
3
with a transition sequence
SSSSR
3
SR
3
APAPR
2
R
3
SR
3
SR
3
APAPAPAPAP,
where S stands for SHIFT, R for ROTATE, A for
LEFT-ARC, and P for POP. But the approximate
procedure fails to find the oracle, since R
2
R
3
in
bold in the sequence are not to be applied.
the ROTATE sequence, let k be the largest m such
that 0 ? m ? min{K, l} and L(j
m
) strictly pre-
cedes L(j
1
) by the lexicographical order (here we
assume L(j
0
) strictly precedes any L(j), j ? ?).
If k > 0, we set t
i
to ROTATE
k
; else we set t
i
to
SHIFT. The approximation assumes L(?) is com-
pletely ordered except the first element, and insert
the first element to its proper place each time.
Definition 2. We define
?
G
K
as the graphs the ora-
cle of which can be extracted by S
K
with the ap-
proximation procedure.
It can be inferred similarly that Theorem 1 and
Theorem 2 also hold for
?
G?s. However, the
?
G
K
is
not equal to G
K
in non-trivial cases.
Theorem 4.
?
G
i
( G
i
,?i ? 3.
Proof. It is trivial that
?
G
i
? G
i
. An example graph
that is in G
3
but not in
?
G
3
is shown in Figure 5,
examples for arbitrary i > 3 can be constructed
similarly.
The above theorem indicates the inadequacy of
the ANFO deriving procedure. Nevertheless, em-
pirical evaluation (Section 4.2) shows that the cov-
erage of AFO and ANFO deriving procedures are
almost identical when applying to linguistic data.
3.7 Statistical Parsing
When we parse a sentence w
1
w
2
? ? ?w
n
, we start
with the initial configuration c
0
= c
s
(x), and
choose next transition t
i
= C(c
i?1
) iteratively ac-
cording to a discriminative classifier trained on or-
acle sequences. To build a parser, we use a struc-
tured classifier to approximate the oracle, and ap-
ply the Passive-Aggressive (PA) algorithm (Cram-
mer et al, 2006) for parameter estimation. The
PA algorithm is similar to the Perceptron algo-
rithm, the difference from which is the update of
weight vector. We also use parameter averaging
and early update to achieve better training. Devel-
oping features has been shown crucial to advanc-
ing the state-of-the-art in dependency tree parsing
(Koo and Collins, 2010; Zhang and Nivre, 2011).
To build accurate deep dependency parsers, we
utilize a large set of features for disambiguation.
See the notes included in the supplymentary ma-
terial for details. To improve the performance, we
also apply the technique of beam search, which
keep a beam of transition sequences with highest
scores when parsing.
4 Experiments
4.1 Experimental setup
CTB is a segmented, part-of-speech (POS) tagged,
and fully bracketed corpus in the constituency for-
malism, and very popular to evaluate fundamen-
tal NLP tasks, including word segmentation (Sun
and Xu, 2011), POS tagging (Sun and Uszkoreit,
2012), and syntactic parsing (Zhang and Clark,
2009; Sun and Wan, 2013). We use CTB 6.0 and
define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. We use
gold-standard word segmentation and POS tag-
ing results as inputs. All transition-based parsing
models are trained with beam 16 and iteration 30.
Overall precision/recall/f-score with respect to de-
pendency tokens is reported. To evaluate the abil-
ity to recover non-local dependencies, the recall of
such dependencies are reported too.
4.2 Coverage and Accuracy
There is a dual effect of the increase of the param-
eter k to our transition-based dependency parser.
On one hand, the higher k is, the more expres-
sivity the corresponding transition system has. A
system with higher k covers more structures and
allows to use more data for training. On the other
hand, higher k brings more ambiguities to the cor-
responding parser, and the parsing performance
may thus suffer. Note that the ambiguity exists not
only in each step for transition decision, but also
in selecting the training oracle.
The left-most columns of Tab. 3 shows the cov-
erage of K-permutation transition system with re-
spect to different K and different oracle deriving
algorithms. Readers may be surprised that the
coverage of NFO and ANFO deriving procedures
is the same. Actually, all the covered graphs by
the two oracle deriving procedures are exactly the
452
System NFO ANFO UP UR UF LP LR LF UR
L
LR
L
UR
NL
LR
NL
S
2
76.5 76.5 85.88 81.00 83.37 83.98 79.21 81.53 81.93 80.34 58.88 52.17
S
3
89.0 89.0 86.02 81.72 83.82 84.07 79.86 81.91 82.61 80.94 60.46 54.28
S
4
95.6 95.6 86.28 82.06 84.12 84.35 80.22 82.23 82.92 81.29 61.48 54.77
S
5
98.4 98.4 86.44 82.21 84.27 84.51 80.37 82.39 83.15 81.51 59.80 53.30
Table 3: Coverage and accuracy of the GR parser on the development data.
same, except for S
3
. Only 1 from 22277 sen-
tences can find a NFO but not an ANFO. This
number demonstrates the effectiveness of ANFO.
In the following experiments, we use the ANFO?s
to train our parser.
Applied to our data, S
2
, i.e. the exact system in-
troduced by Titov et al (2009), only covers 76.5%
GR graphs. This is very different from the re-
sult obtained on the CoNLL shared task data for
English semantic role labeling (SRL). According
to (Titov et al, 2009), 99% semantic-role-labelled
graphs can be generated by S
2
. We think there are
two main reasons accounting for the differences,
and highlight the importance of the expressiveness
of transition systems to solve deep dependency
parsing problems. First, the SRL task only focuses
on finding arguments and adjuncts of verbal (and
nominal) predicates, while dependencies headed
by other words are not contained in its graph rep-
resentation. On contrast, a deep dependency struc-
ture, like GR graph, approximates deep syntactic
or semantic information of a sentence as a whole,
and therefore is more dense. As a result, permuta-
tion system with a very low k is incapable to han-
dle more cases. Another reason is about the Chi-
nese language. Some language-specific properties
result in complex crossing arcs. For example, se-
rial verb constructions are widely used in Chinese
to describe several separate events without con-
junctions. The verbal heads in such constructions
share subjects and adjuncts, both of which are be-
fore the heads. The distributive dependencies be-
tween verbal heads and subjects/adjuncts usually
produce crossing arcs (see Fig. 6). To test our as-
sumption, we evaluate the coverage of S
2
over the
functor-argument dependency graphs provided by
the English and Chinese CCGBank (Hockenmaier
and Steedman, 2007; Tse and Curran, 2010). The
result is 96.9% vs. 89.0%, which confirms our
linguistic intuition under another grammar formal-
ism.
Tab. 3 summarizes the performance of the
transition-based parser with different configura-
tions to reveal how well data-driven parsing can
subject adjunct verb
1
verb
2
Figure 6: A simplified example to illustrate cross-
ing arcs in serial verbal constructions.
be performed in realistic situations. We can see
that with the increase of K, the overall parsing ac-
curacy incrementally goes up. The high complex-
ity of Chinese deep dependency structures demon-
strates the importance of the expressiveness of a
transition system, while the improved numeric ac-
curacies practically certify the benefits. The two
points merit further exploration to more expressive
transition systems for deep dependency parsing, at
least for Chinese. The labeled evaluation scores
on the final test data are presented in Tab. 4.
Test UP UR UF LR
L
LR
NL
S
5
83.93 79.82 81.82 80.94 54.38
Table 4: Performance on the test data.
4.3 Precision vs. Recall
A noteworthy thing about the overall performance
is that the precision is promising but the recall is
too low behind. This difference is consistent with
the result obtained by a shift-reduce CCG parser
(Zhang and Clark, 2011). The functor-argument
dependencies generated by that parser also has a
relatively high precision but considerably low re-
call. There are two similarities between our parser
and theirs: 1) both parsers produce dependency
graphs rather trees; 2) both parser employ a beam
decoder that does not guarantee global optimality.
To build NLP application, e.g. information extrac-
tion, systems upon GR parsing, such property mer-
its attention. A good trade-off between the preci-
sion and the recall may have a great impact on final
results.
453
4.4 Local vs. Non-local
Although the micro accuracy of all dependencies
are considerably good, the ability of current state-
of-the-art statistical parsers to find difficult non-
local materials is far from satisfactory, even for
English (Rimell et al, 2009; Bender et al, 2011).
We report the accuracy in terms of local and non-
local dependencies respectively to show the diffi-
culty of the recovery of non-local dependencies.
The last four columns of Tab. 3 demonstrates the
labeled/unlabeled recall of local (UR
L
/LR
L
) and
non-local dependencies (UR
NL
/LR
NL
). We can
clearly see that non-local dependency recovery is
extremely difficult for Chinese parsing.
4.5 Deep vs. Deep
CCG and HPSG parsers also favor the dependency-
based metrics for evaluation (Clark and Curran,
2007b; Miyao and Tsujii, 2008). Previous work
on Chinese CCG and HPSG parsing unanimously
agrees that obtaining the deep analysis of Chinese
is more challenging (Yu et al, 2011; Tse and Cur-
ran, 2012). The successful C&C and Enju parsers
provide very inaccurate results for Chinese texts.
Though the numbers profiling the qualities of deep
dependency structures under different formalisms
are not directly comparable, all empirical eval-
uation indicates that the state-of-the-art of deep
linguistic processing for Chinese lag behind very
much.
5 Related Work
Wide-coverage in-depth and accurate linguistic
processing is desirable for many practical NLP ap-
plications, such as machine translation (Wu et al,
2010) and information extraction (Miyao et al,
2008). Parsing in deep formalisms, e.g. CCG,
HPSG, LFG and TAG, provides valuable, richer
linguistic information, and researchers thus draw
more and more attention to it. Very recently, study
on deep linguistic processing for Chinese has been
initialized. Our work is one of them.
To quickly construct deep annotations, corpus-
driven grammar engineering has been studied.
Phrase structure trees in CTB have been semi-
automatically converted to deep derivations in the
CCG (Tse and Curran, 2010), LFG (Guo et al,
2007), TAG (Xia, 2001) and HPSG (Yu et al,
2010) formalisms. Our GR extraction work is sim-
ilar, but grounded in GB, which is more consistent
with the construction of the original annotations.
Based on converted fine-grained linguistic an-
notations, successful English deep parsers, such as
C&C (Clark and Curran, 2007b) and Enju (Miyao
and Tsujii, 2008), have been evaluated (Yu et al,
2011; Tse and Curran, 2012). We also borrow
many ideas from recent advances in deep syntac-
tic or semantic parsing for English. In particular,
Sagae and Tsujii (2008)?s and Titov et al (2009)?s
studies on transition-based deep dependency pars-
ing motivated our work very much. However, sim-
ple adoption of their systems does not resolve Chi-
nese GR parsing well because the GR graphs are
much more complicated. Our investigation on the
K-permutation transition system advances the ca-
pacity of existing methods.
6 Conclusion
Recent years witnessed rapid progress made on
deep linguistic processing for English, and ini-
tial attempts for Chinese. Our work stands in
between traditional dependency tree parsing and
deep linguistic processing. We introduced a sys-
tem for automatically extracting grammatical rela-
tions of Chinese sentences from GB phrase struc-
ture trees. The present work remedies the re-
source gap by facilitating the accurate extraction
of GR annotations from GB trees. Manual evalua-
tion demonstrate the effectiveness of our method.
With the availability of high-quality GR resources,
transition-based methods for GR parsing was stud-
ied. A new formal system, namely K-permutation
system, is well theoretically discussed and prac-
tically implemented as the core module of a deep
dependency parser. Empirical evaluation and anal-
ysis were presented to give better understanding of
the Chinese GR parsing problem. Detailed anal-
ysis reveals some important directions for future
investigation.
Acknowledgement
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&D Program (2012AA011101).
References
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 397?408.
Association for Computational Linguistics, Edinburgh,
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1037.
454
J. Bresnan and R. M. Kaplan. 1982. Introduction: Grammars
as mental representations of language. In J. Bresnan, edi-
tor, The Mental Representation of Grammatical Relations,
pages xvii?lii. MIT Press, Cambridge, MA.
Ted Briscoe and John Carroll. 2006. Evaluating the ac-
curacy of an unlexicalized statistical parser on the parc
depbank. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 41?48. Associ-
ation for Computational Linguistics, Sydney, Australia.
URL http://www.aclweb.org/anthology/P/
P06/P06-2006.
Andrew Carnie. 2007. Syntax: A Generative Introduction.
Blackwell Publishing, Blackwell Publishing 350 Main
Street, Malden, MA 02148-5020, USA, second edition.
Noam Chomsky. 1981. Lectures on Government and Binding.
Foris Publications, Dordecht.
Stephen Clark and James Curran. 2007a. Formalism-
independent parser evaluation with ccg and depbank. In
Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 248?255. Associa-
tion for Computational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/anthology/
P07-1032.
Stephen Clark and James R. Curran. 2007b. Wide-coverage
efficient statistical parsing with CCG and log-linear mod-
els. Comput. Linguist., 33(4):493?552. URL http://
dx.doi.org/10.1162/coli.2007.33.4.493.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JOURNAL OF MACHINE
LEARNING RESEARCH, 7:551?585.
M. Dalrymple. 2001. Lexical-Functional Grammar, vol-
ume 34 of Syntax and Semantics. Academic Press, New
York.
Yuqing Guo, Josef van Genabith, and Haifeng Wang. 2007.
Treebank-based acquisition of lfg resources for Chinese.
In Proceedings of the LFG07 Conference. CSLI Publica-
tions, California, USA.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
corpus of CCG derivations and dependency structures ex-
tracted from the penn treebank. Computational Linguis-
tics, 33(3):355?396.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic pars-
ing. In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages 97?
104. Association for Computational Linguistics, Boston,
Massachusetts, USA.
Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700
dependency bank. In In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Corpora
(LINC-03), pages 1?8.
Terry Koo and Michael Collins. 2010. Efficient third-order
dependency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 1?11. Association for Computational Linguistics,
Uppsala, Sweden. URL http://www.aclweb.org/
anthology/P10-1001.
Ryan McDonald. 2006. Discriminative learning and span-
ning tree algorithms for dependency parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented evalu-
ation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46?54. Associ-
ation for Computational Linguistics, Columbus, Ohio.
URL http://www.aclweb.org/anthology/P/
P08/P08-1006.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007. To-
wards framework-independent evaluation of deep linguis-
tic parsers. In Ann Copestake, editor, Proceedings of
the GEAF 2007 Workshop, CSLI Studies in Computa-
tional Linguistics Online, page 21 pages. CSLI Publica-
tions. URL http://www.cs.cmu.edu/
?
sagae/
docs/geaf07miyaoetal.pdf.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Comput. Lin-
guist., 34(1):35?80. URL http://dx.doi.org/10.
1162/coli.2008.34.1.35.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Comput. Linguist., 34:513?
553. URL http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 351?
359. Association for Computational Linguistics, Sun-
tec, Singapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1040.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-
based dependency parsing. In Hwee Tou Ng and Ellen
Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Con-
ference on Computational Natural Language Learning
(CoNLL-2004), pages 49?56. Association for Computa-
tional Linguistics, Boston, Massachusetts, USA.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The University of Chicago Press,
Chicago.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Un-
bounded dependency recovery for parser evaluation. In
Proceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 813?821.
Association for Computational Linguistics, Singapore.
URL http://www.aclweb.org/anthology/D/
D09/D09-1085.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce de-
pendency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguistics,
pages 753?760. Coling 2008 Organizing Committee,
Manchester, UK. URL http://www.aclweb.org/
anthology/C08-1095.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradig-
matic and syntagmatic lexical relations: Towards accu-
rate Chinese part-of-speech tagging. In Proceedings of the
50th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics.
Weiwei Sun and Xiaojun Wan. 2013. Data-driven, pcfg-based
and pseudo-pcfg-based models for Chinese dependency
parsing. Transactions of the Association for Computa-
tional Linguistics (TACL).
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?979.
Association for Computational Linguistics, Edinburgh,
455
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1090.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of the 21st international jont con-
ference on Artifical intelligence, pages 1562?1567. Mor-
gan Kaufmann Publishers Inc., San Francisco, CA, USA.
URL http://dl.acm.org/citation.cfm?id=
1661445.1661696.
Andre Torres Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350. Asso-
ciation for Computational Linguistics, Suntec, Singapore.
URL http://www.aclweb.org/anthology/P/
P09/P09-1039.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the penn Chi-
nese treebank. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling 2010),
pages 1083?1091. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-1122.
Daniel Tse and James R. Curran. 2012. The challenges
of parsing Chinese with combinatory categorial gram-
mar. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
pages 295?304. Association for Computational Linguis-
tics, Montr?eal, Canada. URL http://www.aclweb.
org/anthology/N12-1030.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages
325?334. Association for Computational Linguistics, Up-
psala, Sweden. URL http://www.aclweb.org/
anthology/P10-1034.
Fei Xia. 2001. Automatic grammar generation from two dif-
ferent perspectives. Ph.D. thesis, University of Pennsylva-
nia.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005. The penn Chinese treebank: Phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11:207?238. URL http://portal.acm.org/
citation.cfm?id=1064781.1064785.
Nianwen Xue. 2007. Tapping the implicit information for the
PS to DS conversion of the Chinese treebank. In Proceed-
ings of the Sixth International Workshop on Treebanks and
Linguistics Theories.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
The 8th International Workshop of Parsing Technologies
(IWPT2003), pages 195?206.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli Wang,
and Junichi Tsujii. 2011. Analysis of the difficul-
ties in Chinese deep parsing. In Proceedings of the
12th International Conference on Parsing Technologies,
pages 48?57. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.org/
anthology/W11-2907.
Kun Yu, Miyao Yusuke, Xiangli Wang, Takuya Matsuzaki,
and Junichi Tsujii. 2010. Semi-automatically devel-
oping Chinese hpsg grammar from the penn Chinese
treebank for deep parsing. In Coling 2010: Posters,
pages 1417?1425. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-2162.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global discrim-
inative model. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT?09),
pages 162?171. Association for Computational Linguis-
tics, Paris, France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 683?692.
Association for Computational Linguistics, Portland,
Oregon, USA. URL http://www.aclweb.org/
anthology/P11-1069.
Yue Zhang and Joakim Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technolo-
gies, pages 188?193. Association for Computational Lin-
guistics, Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
456
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 459?464,
Dublin, Ireland, August 23-24, 2014.
Peking: Profiling Syntactic Tree Parsing Techniques for Semantic Graph
Parsing
Yantao Du, Fan Zhang, Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{duyantao,ws,wanxiaojun}@pku.edu.cn, zhangf717@gmail.com
Abstract
Using the SemEval-2014 Task 8 data, we
profile the syntactic tree parsing tech-
niques for semantic graph parsing. In par-
ticular, we implement different transition-
based and graph-based models, as well as
a parser ensembler, and evaluate their ef-
fectiveness for semantic dependency pars-
ing. Evaluation gauges how successful
data-driven dependency graph parsing can
be by applying existing techniques.
1 Introduction
Bi-lexical dependency representation is quite pow-
erful and popular to encode syntactic or semantic
information, and parsing techniques under the de-
pendency formalism have been well studied and
advanced in the last decade. The major focus is
limited to tree structures, which fortunately corre-
spond to many computationally good properties.
On the other hand, some leading linguistic theo-
ries argue that more general graphs are needed to
encode a wide variety of deep syntactic and se-
mantic phenomena, e.g. topicalization, relative
clauses, etc. However, algorithms for statistical
graph spanning have not been well explored be-
fore, and therefore it is not very clear how good
data-driven parsing techniques developed for tree
parsing can be for graph generating.
Following several well-established syntactic
theories, SemEval-2014 task 8 (Oepen et al.,
2014) proposes using graphs to represent seman-
tics. Considering that semantic dependency pars-
ing is a quite new topic and there is little previ-
ous work, we think it worth appropriately profil-
ing successful tree parsing techniques for graph
parsing. To this end, we build a hybrid system
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
that combines several important data-driven pars-
ing techniques and evaluate their impact with the
given data. In particular, we implement different
transition-based and graph-based models, as well
as a parser ensembler.
Our experiments highlight the following facts:
? Graph-based models are more effective than
transition-based models.
? Parser ensemble is very useful to boost the
parsing accuracy.
2 Architecture
We explore two kinds of basic models: One is
transition-based, and the other is tree approxima-
tion. Transition-based models are widely used for
dependency tree parsing, and they can be adapted
to graph parsing (Sagae and Tsujii, 2008; Titov
et al., 2009). Here we implement 5 transition-
based models for dependency graph parsing, each
of which is based on different transition system.
The motivation of developing tree approxima-
tion models is to apply existing graph-based tree
parsers to generate graphs. At the training time,
we convert the dependency graphs from the train-
ing data into dependency trees, and train second-
order arc-factored models
1
. At the test phase, we
parse sentences using this tree parser, and convert
the output trees back into semantic graphs. We
think tree approximation can appropriately evalu-
ate the possible effectiveness of graph-based mod-
els for graph spanning.
Finally, we integrate the outputs of different
models with a simple voter to boost the perfor-
mance. The motivation of using system combi-
nation and the choice of voting is mainly due to
the experiments presented by (Surdeanu and Man-
ning, 2010). When we obtain all the outputs of
1
The mate parser (code.google.com/p/
mate-tools/) is used.
459
these models, we combine them into a final result,
which is better than any of them. For combination,
we explore various systems for this task, since em-
pirically we know that variety leads to better per-
formance.
3 Transition-Based Models
Transition-based models are usually used for de-
pendency tree parsing. For this task, we exploit it
for dependency graph parsing.
A transition system S contains a set C of con-
figurations and a set T of transitions. A configu-
ration c ? C generally contains a stack ? of nodes,
a buffer ? of nodes, and a set A of arcs. The ele-
ments in A is in the form (x, l, y), which denotes
a arc from x to y labeled l. A transition t ? T can
be applied to a configuration and turn it into a new
one by adding new arcs or manipulating elements
of the stack or the buffer. A statistical transition-
based parser leverages a classifier to approximate
an oracle that is able to generate target graphs by
transforming the initial configuration c
s
(x) into a
terminal configuration c
t
? C
t
.
An oracle of a given graph on sentence x is a
sequence of transitions which transform the initial
configuration to the terminal configuration the arc
set A
c
t
of which is the set of the arcs of the graph.
3.1 Our Transition Systems
We implemented 5 different transition systems for
graph parsing. Here we describe two of them
in detail, one is the Titov system proposed in
(Titov et al., 2009), and the other is our Naive
system. The configurations of the two systems
each contain a stack ?, a buffer ?, and a set A of
arcs, denoted by ??, ?,A?. The initial configura-
tion of a sentence x = w
1
w
2
? ? ?w
n
is c
s
(x) =
?[0], [1, 2, ? ? ? , n], {}?, and the terminal configu-
ration set C
t
is the set of all configurations with
empty buffer. These two transition systems are
shown in 1.
The transitions of the Titov system are:
? LEFT-ARC
l
adds an arc from the front of the
buffer to the top of the stack, labeled l, into
A.
? RIGHT-ARC
l
adds an arc from the top of the
stack to the front of the buffer, labeled l, into
A.
? SHIFT removes the front of the buffer and
push it onto the stack;
? POP pops the top of the stack.
? SWAP swaps the top two elements of the
stack.
This system uses a transition SWAP to change the
node order in the stack, thus allowing some cross-
ing arcs to be built.
The transitions of the Naive system are similar
to the Titov system?s, except that we can directly
manipulate all the nodes in the stack instead of just
the top two. In this case, the transition SWAP is not
needed.
The Titov system can cover a great proportion,
though not all, of graphs in this task. For more
discussion, see (Titov et al., 2009). The Naive
system, by comparison, covers all graphs. That
is to say, with this system, we can find an oracle
for any dependency graph on a sentence x. Other
transition systems we build are also designed for
dependency graph parsing, and they can cover de-
pendency graphs without self loop as well.
3.2 Statistical Disambiguation
First of all, we derive oracle transition sequences
for every sentence, and train Passive-Aggressive
models (Crammer et al., 2006) to predict next tran-
sition given a configuration. When it comes to
parsing, we start with the initial configuration, pre-
dicting next transition and updating the configura-
tion with the transition iteratively. And finally we
will get a terminal configuration, we then stop and
output the arcs of the graph contained in the final
configuration.
We extracted rich feature for we utilize a set
of rich features for disambiguation, referencing to
Zhang and Nivre (2011). We examine the several
tops of the stack and the one or more fronts of the
buffer, and combine the lemmas and POS tags of
them in many ways as the features. Additionally,
we also derive features from partial parses such as
heads and dependents of these nodes.
3.3 Sentence Reversal
Reversing the order the words of a given sentence
is a simple way to yield heterogeneous parsing
models, thus improving parsing accuracy of the
model ensemble (Sagae, 2007). In our experi-
ments, one transition system produces two mod-
els, one trained on the normal corpus, and the other
on the corpus of reversed sentences. Therefore we
can get 10 parse of a sentence based on 5 transition
systems.
460
LEFT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(j, l, i)})
RIGHT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(i, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP (?|i, ?, A)? (?, ?,A)
SWAP (?|i|j, ?,A)? (?|j|i, ?, A)
Titov System
LEFT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(j, l, i
k
)})
RIGHT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(i
k
, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP
k
(?|i
k
|i
k?1
| . . . |i
2
|i
1
, ?, A)? (?|i
k?1
| . . . |i
2
|i
1
, ?, A)
Naive System
Figure 1: Two of our transition systems.
4 Tree Approximation Models
Parsing based on graph spanning is quite challeng-
ing since computational properties of the seman-
tic graphs given by the shared task are less ex-
plored and thus still unknown. On the other hand,
finding the best higher-order spanning for general
graph is NP complete, and therefore it is not easy,
if not impossible, to implement arc-factored mod-
els with exact inference. In our work, we use a
practical idea to indirectly profile the graph-based
parsing techniques for dependency graph parsing.
Inspired by the PCFG approximation idea (Fowler
and Penn, 2010; Zhang and Krieger, 2011) for
deep parsing, we study tree approximation ap-
proaches for graph spanning.
This tree approximation technique can be ap-
plied to both transition-based and graph-based
parsers. However, since transition systems that
can directly handle build graphs have been devel-
oped, we only use this technique to evaluate the
possible effectiveness of graph-based models for
semantic parsing.
4.1 Graph-to-Tree Transformation
In particular, we develop different methods to con-
vert a semantic graph into a tree, and use edge
labels to encode dependency relations as well as
structural information which helps to transform a
converted tree back to its original graph. By the
graph-to-tree transformation, we can train a tree
parser with a graph-annotated corpus, and utilize
the corresponding tree-to-graph transformation to
generate target graphs from the outputs of the tree
parser. Given that the tree-to-graph transformation
is quite trivial, we only describe the graph-to-tree
transformation approach.
We use graph traversal algorithms to convert a
directed graph to a directed tree. The transforma-
tion implies that we may lose, add or modify some
dependency relations in order to make the graph a
tree.
4.2 Auxiliary Labels
In the transformed trees, we use auxiliary labels to
carry out information of the original graphs. To
encode multiple edges to one, we keep the origi-
nal label on the directed edge but may add other
edges? information. On the other hand, through-
out most transformations, some edges must be re-
versed to make a tree, so we need a symbol to in-
dicate a edge on the tree is reversed during trans-
formation. The auxiliary labels are listed below:
? Label with following ?R: The symbol ?R
means this directed edge is reversed from the
original directed graph.
? Separator: Semicolon separates two encoded
original edges.
? [N ] followed by label: The symbol [N ] (N
is an integer) represents the head of the edge.
The dependent is the current one, but the head
is the dependent?s N -th ancestor where 1st
ancestor is its father and 2nd ancestor is its
father?s father.
See Figure 2 for example.
4.3 Traversal Strategies
Given directed graph (V,E), the task is to traverse
all edges on the graph and decide how to change
the labels or not contain the edge on the output.
We use 3 strategies for traversal. Here we use
x ?
g
y to denote the edge on graph, and x ?
t
y
the edge on tree.
461
Mrs Ward was relieved
noun ARG1 verb ARG1 verb ARG2
adj ARG1
root
Mrs Ward was relieved
noun ARG1?R verb ARG1 verb ARG2
root
Mrs Ward was relieved
noun ARG1?R verb ARG2
adj ARG1;[2]verb ARG1
root
Figure 2: One dependency graph and two possible
dependency trees after converting.
Depth-first-search We try graph traversal by
depth-first-search starting from the root on the di-
rected graph ignoring the direction of edges. Dur-
ing the traversal, we add edges to the directed tree
with (perhaps new) labels. We traverse the graph
recursively. Suppose the depth-first-search is run-
ning at the node x and the nodes set A which have
been searched. And suppose we find node y is
linked to x on the graph (x ?
g
y or y ?
g
x).
If y /? A, we add the directed edge x ?
t
y to the
tree immediately. In the case of y ?
g
x, we add
?R to the edge label. If y ? A, then y must be one
of the ancestors of x. In this case, we add this in-
formation to the label of the existing edge z ?
t
x.
Since the distance between two nodes x and y is
sufficient to indicate the node y, we use the dis-
tance to represent the head or dependent of this
directed edge and add the label and the distance to
the label of z ?
t
x. It is clear that the auxiliary
label [N ] can be used for multiple edge encoding.
Under this strategy, all edges can be encoded on
the tree.
Breadth-first-search An alternative traversal
strategy is based on breadth-first-search starting
from the root. This search ignores the direction
of edge too. We regard the search tree as the de-
pendency tree. During the breadth-first-search, if
(x, l, y) exists but node y has been searched, we
just ignore the edge. Under this strategy, we may
lose some edges.
Iterative expanding This strategy is based on
depth-first-search but slightly different. The strat-
egy only searches through the forward edges on
the directed graph at first. When there is no for-
ward edge to expend, a traversed node linked to
some nodes that are not traversed must be the de-
pendent of them. Then we choose an edge and add
it (reversed) to the tree and continue to expand the
tree. Also, we ignore the edges that does not sat-
isfy the tree constraint. We call this strategy iter-
ative expanding. When we need to expand output
tree, we need to design a strategy to decide which
edge to be add. The measure to decide which node
should be expanded first is its possible location on
the tree and the number of nodes it can search dur-
ing depth-first-search. Intuitively, we want the re-
versed edges to be as few as possible. For this
purpose, this strategy is practical but not necessar-
ily the best. Like the Breadth-first-search strategy,
this strategy may also cause edge loss.
4.4 Forest-to-Tree
After a primary searching process, if there is still
edge x ?
g
y that has not been searched yet, we
start a new search procedure from x or y. Even-
tually, we obtain a forest rather than a tree. To
combine disconnected trees in this forest to the fi-
nal dependency tree, we use edges with label None
to link them. Let the node setW be the set of roots
of the trees in the forest, which are not connected
to original graph root. The mission is to assign a
node v /? W for each w ? W . If we assign v
i
for
w
i
, we add the edge v
i
? w
i
labeled by None to
the final dependency tree. We try 3 strategies in
this step:
? For each w ? W we look for the first node
v /?W on the left of w.
? For each w ? W we look for the first node
v /?W on the right of w.
? By defining the distance between two nodes
as how many words are there between the two
words, we can select the nearest node. If the
distances of more than one node are equal,
we choose v randomly.
We also tried to link all of the nodes in W di-
rectly to the root, but it does not work well.
5 Model Ensemble
We have 19 heterogeneous basic models (10
transition-based models, 9 tree approximation
models), and use a simple voter to combine their
outputs.
462
Algorithm DM PAS PCEDT
DFS 0 0 0
BFS 0.0117 0.0320 0.0328
FEF 0.0127 0.0380 0.0328
Table 1: Edge loss of transformation algorithms.
For each pair of words of a sentence, we count
the number of the models that give positive pre-
dictions. If the number is greater than a threshold,
we put this arc to the final graph, and label the arc
with the most common label of what the models
give.
Furthermore, we find that the performance of
the tree approximation models is better than the
transition based models, and therefore we take
weights of individual models too. Instead of just
counting, we sum the weights of the models that
give positive predictions. The tree approximation
models are assigned higher weights.
6 Experiments
There are 3 subtasks in the task, namely DM, PAS,
and PCEDT. For subtask DM, we finally obtained
19 models, just as stated in previous sections.
For subtask PAS and PCEDT, only 17 models are
trained due to the tight schedule.
The tree approximation algorithms may cause
some edge loss, and the statistics are shown in Ta-
ble 1. We can see that DFS does not cause edge
loss, but edge losses of other two algorithm are
not negligible. This may result in a lower recall
and higher precision, but we can tune the final re-
sults during model ensemble. Edge loss in subtask
DM is less than those in subtask PAS and PCEDT.
We present the performance of several repre-
sentative models in Table 2. We can see that the
tree approximation models performs better than
the transition-based models, which highlights the
effective of arc-factored models for semantic de-
pendency parsing. For model ensemble, besides
the accuracy of each single model, it is also im-
portant that the models to be ensembled are very
different. As shown in Table 2, the evaluation be-
tween some of our models indicates that our mod-
els do vary a lot.
Following the suggestion of the task organizers,
we use section 20 of the train data as the devel-
opment set. With the help of development set,
we tune the parameters of the models and ensem-
Models DM PAS PCEDT
Titov 0.8468 0.8754 0.6978
Titov
r
0.8535 0.8928 0.7063
Naive 0.8481 - -
DFS
n
0.8692 0.9034 0.7370
DFS
l
0.8692 0.9015 0.7246
BFS
n
0.8686 0.8818 0.7247
Titov vs. Titov
r
0.8607 0.8831 0.7613
Titov vs. Naive 0.9245 - -
Titov vs. DFS
n
0.8590 0.8865 0.7650
DFS
n
vs. DFS
l
0.9273 0.9579 0.8688
DFS
n
vs. BFS
n
0.9226 0.9169 0.8367
Table 2: Evaluation between some of our models.
Labeled f-score on test set is shown. Titov
r
stands
for reversed Titov, DFS
n
for DFS+nearest, DFS
l
for DFS+left, and BFS
n
for BFS+nearest. The up-
per part gives the performance, and the lower part
gives the agreement between systems.
Format LP LR LF LM
DM 0.9027 0.8854 0.8940 0.2982
PAS 0.9344 0.9069 0.9204 0.3872
PCEDT 0.7875 0.7396 0.7628 0.1120
Table 3: Final results of the ensembled model.
bling. We set the weight of each transition-based
model 1, and tree approximation model 2 in run
1, 3 in run 2. The threshold is set to a half of the
total weight. The final results given by the orga-
nizers are shown in Table 3. Compared to Table 2
demonstrates the effectiveness of parser ensemble.
7 Conclusion
Data-driven dependency parsing techniques have
been greatly advanced during the parst decade.
Two dominant approaches, i.e. transition-based
and graph-based methods, have been well stud-
ied. In addition, parser ensemble has been shown
very effective to take advantages to combine the
strengthes of heterogeneous base parsers. In this
work, we propose different models to profile the
three techniques for semantic dependency pars-
ing. The experimental results suggest several di-
rections for future study.
Acknowledgement
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&D Program (2012AA011101).
463
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JOURNAL OF MA-
CHINE LEARNING RESEARCH, 7:551?585.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cate-
gorial grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, Uppsala, Sweden, July.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Haji?c, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation, Dublin, Ireland.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 753?760, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Kenji Sagae. 2007. Dependency parsing and domain
adaptation with lr models and parser ensembles. In
In Proceedings of the Eleventh Conference on Com-
putational Natural Language Learning.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Los Angeles, California, June.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisa-
tion for synchronous parsing of semantic and syn-
tactic dependencies. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1562?1567, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-scale
corpus-driven PCFG approximation of an hpsg. In
Proceedings of the 12th International Conference on
Parsing Technologies, Dublin, Ireland, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Portland, Oregon, USA, June.
464
