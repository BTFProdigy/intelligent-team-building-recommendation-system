Proceedings of the 12th Conference of the European Chapter of the ACL, pages 523?531,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Correcting a PoS-tagged corpus using three complementary methods
Hrafn Loftsson
School of Computer Science
Reykjavik University
Reykjavik, Iceland
hrafn@ru.is
Abstract
The quality of the part-of-speech (PoS)
annotation in a corpus is crucial for the
development of PoS taggers. In this pa-
per, we experiment with three complemen-
tary methods for automatically detecting
errors in the PoS annotation for the Ice-
landic Frequency Dictionary corpus. The
first two methods are language indepen-
dent and we argue that the third method
can be adapted to other morphologically
complex languages. Once possible errors
have been detected, we examine each er-
ror candidate and hand-correct the cor-
responding PoS tag if necessary. Over-
all, based on the three methods, we hand-
correct the PoS tagging of 1,334 tokens
(0.23% of the tokens) in the corpus. Fur-
thermore, we re-evaluate existing state-of-
the-art PoS taggers on Icelandic text using
the corrected corpus.
1 Introduction
Part-of-speech (PoS) tagged corpora are valuable
resources for developing PoS taggers, i.e. pro-
grams which automatically tag each word in run-
ning text with morphosyntactic information. Cor-
pora in various languages, such as the English
Penn Treebank corpus (Marcus et al, 1993), the
Swedish Stockholm-Ume? corpus (Ejerhed et al,
1992), and the Icelandic Frequency Dictionary
(IFD) corpus (Pind et al, 1991), have been used
to train (in the case of data-driven methods) and
develop (in the case of linguistic rule-based meth-
ods) different taggers, and to evaluate their accu-
racy, e.g. (van Halteren et al, 2001; Megyesi,
2001; Loftsson, 2006). Consequently, the quality
of the PoS annotation in a corpus (the gold stan-
dard annotation) is crucial.
Many corpora are annotated semi-
automatically. First, a PoS tagger is run on the
corpus text, and, then, the text is hand-corrected
by humans. Despite human post-editing, (large)
tagged corpora are almost certain to contain
errors, because humans make mistakes. Thus, it is
important to apply known methods and/or develop
new methods for automatically detecting tagging
errors in corpora. Once an error has been detected
it can be corrected by humans or an automatic
method.
In this paper, we experiment with three differ-
ent methods of PoS error detection using the IFD
corpus. First, we use the variation n-gram method
proposed by Dickinson and Meurers (2003). Sec-
ondly, we run five different taggers on the cor-
pus and examine those cases where all the tag-
gers agree on a tag, but, at the same time, disagree
with the gold standard annotation. Lastly, we use
IceParser (Loftsson and R?gnvaldsson, 2007) to
generate shallow parses of sentences in the corpus
and then develop various patterns, based on fea-
ture agreement, for finding candidates for annota-
tion errors.
Once error candidates have been detected by
each method, we examine the candidates man-
ually and correct the errors. Overall, based on
these methods, we hand-correct the PoS tagging
of 1,334 tokens or 0.23% of the tokens in the IFD
corpus. We are not aware of previous corpus er-
ror detection/correction work applying the last two
methods above. Note that the first two methods are
completely language-independent, and the third
method can be tailored to the language at hand,
assuming the existence of a shallow parser.
Our results show that the three methods are
complementary. A large ratio of the tokens that get
hand-corrected based on each method is uniquely
corrected by that method1.
1To be precise, when we say that an error is corrected by
a method, we mean that the method detected the error candi-
date which was then found to be a true error by the separate
error correction phase.
523
After hand-correcting the corpus, we retrain and
re-evaluate two of the best three performing tag-
gers on Icelandic text, which results in up to 0.18%
higher accuracy than reported previously.
The remainder of this paper is organised as fol-
lows. In Section 2 we describe related work, with
regard to error detection and PoS tagging of Ice-
landic text. Our three methods of error detection
are described in Section 3 and results are provided
in Section 4. We re-evaluate taggers in Section 5
and we conclude with a summary in Section 6.
2 Related work
2.1 Error detection
The field of automatic error detection/correction
in corpora has gained increased interest during the
last few years. Most work in this field has focused
on finding elements in corpora that violate consis-
tency, i.e. finding inconsistent tagging of a word
across comparable occurrences.
The variation n-gram algorithm is of this na-
ture. This method finds identical strings (n-grams
of words) in a corpus that are annotated differently.
The difference in PoS tags between the strings is
called a variation and the word(s) exhibiting the
variation is called a variation nucleus (Dickinson
and Meurers, 2003). A particular variation is thus
a possible candidate for an error. The variation
might be due to an error in the annotation or it
might exhibit different (correct) tagging because
of different contexts. Intuitively, the more similar
the context of a variation, the more likely it is for
the variation to be an error.
When Dickinson and Meurers applied their
variation n-gram algorithm to the Wall Street Jour-
nal (WSJ) corpus of about 1.3 million words, it
produced variations up to length n = 224. Note
that a variation n-gram of length n contains two
variation n-grams of length n ? 1, obtained by
removing either the first or the last word. More-
over, each variation n-gram contains at least two
different annotations of the same string. There-
fore, it is not straightforward to compute the pre-
cision (the ratio of correctly detected errors to all
error candidates) of this method. However, by ig-
noring variation n-grams of length ? 5, Dickinson
and Meurers found that 2436 of the 2495 distinct
variation nuclei (each nucleus is only counted for
the longest n-gram it appears in) were true errors,
i.e. 97.6%. This resulted in 4417 tag corrections,
i.e. about 0.34% of the tokens in the whole corpus
were found to be incorrectly tagged2.
Intuitively, the variation n-gram method is most
suitable for corpora containing specific genres,
e.g. business news like the WSJ, or very large
balanced corpora, because in both types of cor-
pora one can expect the length of the variations to
be quite large. Furthermore, this method may not
be suitable for corpora tagged with a large fine-
grained tagset, because in such cases a large ratio
of the variation n-grams may actually reflect true
ambiguity rather than inconsistent tagging.
Another example of a method, based on find-
ing inconsistent tagging of a word across compara-
ble occurrences, is the one by Nakagawa and Mat-
sumoto (2002). They use support vector machines
(SVMs) to find elements in a corpus that violate
consistency. The SVMs assign a weight to each
training example in a corpus ? a large weight is
assigned to examples that are hard for the SVMs
to classify. The hard examples are thus candi-
dates for errors in the corpus. The result was a
remarkable 99.5% precision when examples from
the WSJ corpus were extracted with a large weight
greater than or equal to a threshold value. How-
ever, the disadvantage with this approach is that a
model of SVMs needs to be trained for each PoS
tag, which makes it unfeasible for large tagsets.
A set of invalid n-grams can be used to search
for annotation errors. The algorithm proposed by
Kve?to?n and Oliva (2002) starts from a known set
of invalid bigrams, [first,second], and incremen-
tally constructs a set of allowed inner tags appear-
ing between the tags first and second. This set is
then used to generate the complement, impossible
inner tags (the set of all tags excluding the set al
lowed inner tags). Now, any n-gram consisting of
the tag first, followed by any number of tags from
the set impossible inner tags, finally followed by
the tag second, is a candidate for an annotation er-
ror in a corpus. When this method was applied on
the NEGRA corpus (containing 350,000 tokens)
it resulted in the hand-correction of 2,661 tokens
or 0.8% of the corpus. The main problem with
this approach is that is presupposes a set of in-
valid bigrams (e.g. constructed by a linguist). For
a large tagset, for example the Icelandic one (see
Section 2.2), constructing this set is a very hard
task. Moreover, this method fails to detect annota-
tion errors where a particular n-gram tag sequence
2In a more recent work, Dickinson (2008) has developed
a method for increasing the recall (the ratio of correctly de-
tected errors to all errors in the corpus).
524
is valid but erroneous in the given context.
PoS taggers have also been used to point to pos-
sible errors in corpora. If the output of a tagger
does not agree with the gold standard then either
the tagger is incorrect or the gold standard is in-
correctly annotated. A human can then look at the
disagreements and correct the gold standard where
necessary. van Halteren (2000) trained a tagger
on the written texts of the British National Corpus
sampler CD (about 1 million words). In a random
sample of 660 disagreements, the tagger was cor-
rect and the gold standard incorrect in 84 cases,
i.e. the precision of this error detection method
was 12.7%. A natural extension of this method is
to use more than one tagger to point to disagree-
ments.
2.2 PoS tagging Icelandic
The IFD corpus is a balanced corpus, consist-
ing of 590,297 tokens. The corpus was semi-
automatically tagged using a tagger based on lin-
guistic rules and probabilities (Briem, 1989). The
main Icelandic tagset, constructed in the compi-
lation of the corpus, is large (700 possible tags)
compared to related languages. In this tagset, each
character in a tag has a particular function. The
first character denotes the word class. For each
word class there is a predefined number of ad-
ditional characters (at most six), which describe
morphological features, like gender, number and
case for nouns; degree and declension for adjec-
tives; voice, mood and tense for verbs, etc. To
illustrate, consider the word ?hestarnir? (?(the)
horses?). The corresponding tag is ?nkfng?, denot-
ing noun (n), masculine (k), plural (f ), nominative
(n), and suffixed definite article (g).
The large tagset mirrors the morphological
complexity of the Icelandic language. This, in
turn, is the main reason for a relatively low tag-
ging accuracy obtained by PoS taggers on Ice-
landic text, so far. The state-of-the art tagging
accuracy, measured against the IFD corpus, is
92.06%, obtained by applying a bidirectional PoS
tagging method (Dredze and Wallenberg, 2008).
We have developed a linguistic rule-based tagger,
IceTagger, achieving about 91.6% tagging accu-
racy (Loftsson, 2008). Evaluation has shown that
the well known statistical tagger, TnT (Brants,
2000), obtains about 90.4% accuracy (Helgad?t-
tir, 2005; Loftsson, 2008). Finally, an accuracy of
about 93.5% has been achieved by using a tagger
combination method using five taggers (Loftsson,
2006).
3 Three methods for error detection
In this section, we describe the three methods we
used to detect (and correct) annotation errors in
the IFD corpus. Each method returns a set of error
candidates, which we then manually inspect and
correct the corresponding tag if necessary.
3.1 Variation n-grams
We used the Decca software (http:
//decca.osu.edu/) to find the variation
n-grams in the corpus. The length of the longest
variation n-gram was short, i.e. it consisted of only
20 words. The longest variation that contained
a true tagging error was 15 words long. As an
example of a tagging error found by this method,
consider the two occurrences of the 4-gram varia-
tion ?henni datt ? hug? (meaning ?she got an idea?):
1) henni/fpve? datt/sfg3e? ?/a? hug/nke?
2) henni/fpve? datt/sfg3e? ?/ao hug/nkeo
In the first occurrence, the substring ?? hug? (the
variation nucleus) is incorrectly tagged as a prepo-
sition governing the dative case (?a??), and a noun
in masculine, singular, dative (?nke??). In the
latter occurrence, the same substring is correctly
tagged as a preposition governing the accusative
case (?ao?), and a noun in masculine, singular, ac-
cusative (?nkeo?). In both cases, note the agree-
ment in case between the preposition and the noun.
As discussed earlier, the longer variation n-
grams are more likely to contain true errors than
the shorter ones. Therefore, we manually in-
spected all the variations of length ? 5 produced
by this method (752 in total), but only ?browsed
through? the variations of length 4 (like the one
above; 2070 variations) and of length 3 (7563 vari-
ations).
3.2 Using five taggers
Instead of using a single tagger to tag the text in
the IFD corpus, and compare the output of the
taggers to the gold standard (as described in Sec-
tion 2.1), we decided to use five taggers. It is
well known that a combined tagger usually ob-
tains higher accuracy than individual taggers in
the combination pool. For example, by using sim-
ple voting (in which each tagger ?votes? for a tag
525
and the tag with the highest number of votes is
selected by the combined tagger), the tagging ac-
curacy can increase significantly (van Halteren et
al., 2001; Loftsson, 2006). Moreover, if all the
taggers in the pool agree on a vote, one would ex-
pect the tagging accuracy for the respective words
to be high. Indeed, we have previously shown that
when five taggers all agree on a tag in the IFD cor-
pus, the corresponding accuracy is 98.9% (Lofts-
son, 2007b). For the remaining 1.1% tokens, one
would expect that the five taggers are actually cor-
rect in some of the cases, but the gold standard
incorrectly annotated. In general, both the preci-
sion and the recall should be higher when relying
on five agreeing taggers as compared to using only
a single tagger.
Thus, we used the five taggers, MBL (Daele-
mans et al, 1996), MXPOST (Ratnaparkhi, 1996),
fnTBL (Ngai and Florian, 2001), TnT, and IceTag-
ger3, in the same manner as described in (Lofts-
son, 2006), but with the following minor changes.
We extended the dictionaries of the TnT tagger
and IceTagger by using data from a full-form mor-
phological database of inflections (Bjarnad?ttir,
2005). The accuracy of the two taggers increases
substantially (because the ratio of unknown words
drops dramatically) and, in turn, the correspond-
ing accuracy when all the taggers agree increases
from 98.9% to 99.1%. Therefore, we only needed
to inspect about 0.9% of the tokens in the corpus.
The following example from the IFD cor-
pus shows a disagreement found between the
five taggers and the gold standard: ?fj?lskylda
sp?konunnar ? gamla h?sinu? (?family (the)
fortune-teller?s in (the) old house?).
3) fj?lskylda/nven sp?konunnar/nveeg ?/ao
gamla/lhe?vf h?sinu/nhe?g
In this case, the disagreement lies in the tagging
of the preposition ???. All the five taggers suggest
the correct tag ?a?? for the preposition (because
case agreement is needed between the preposition
and the following adjective/noun).
3.3 Shallow parsing
In a morphologically complex language like Ice-
landic, feature agreement, for example inside noun
phrases or between a preposition and a noun
3The first four taggers are data-driven, but IceTagger is a
linguistic rule-based tagger.
phrase, plays an important role. Therefore, of the
total number of possible errors existing in an Ice-
landic corpus, feature agreement errors are likely
to be prevalent. A constituent parser is of great
help in finding such error candidates, because it
annotates phrases which are needed by the error
detection mechanism. We used IceParser, a shal-
low parser for parsing Icelandic text, for this pur-
pose.
The input to IceParser is PoS tagged text, using
the IFD tagset. It produces annotation of both
constituent structure and syntactic functions. To
illustrate, consider the output of IceParser when
parsing the input from 3) above:
4) {*SUBJ [NP fj?lskylda nven NP] {*QUAL
[NP sp?konunnar nveeg NP] *QUAL} *SUBJ}
[PP ? ao [NP [AP gamla lhe?vf AP] h?sinu nhe?g
NP] PP]
The constituent labels seen here are: PP=a
preposition phrase, AP=an adjective phrase, and
NP=a noun phrase. The syntactic functions are
*SUBJ=a subject, and *QUAL=a genitive quali-
fier.
This (not so shallow) output makes it relatively
easy to find error candidates. Recall from example
3) that the accusative preposition tag ?ao?, associ-
ated with the word ???, is incorrect (the correct tag
is the dative ?a??). Since a preposition governs the
case of the following noun phrase, the case of the
adjective ?gamla? and the noun ?h?sinu? should
match the case of the preposition. Finding such
error candidates is thus just a matter of writing
regular expression patterns, one for each type of
error.
Furthermore, IceParser makes it even simpler
to write such patterns than it might seem when
examining the output in 4). IceParser is designed
as a sequence of finite-state transducers. The
output of one transducer is used as the input to
the next transducer in the sequence. One of these
transducers marks the case of noun phrases, and
another one the case of adjective phrases. This is
carried out to simplify the annotation of syntactic
functions in the transducers that follow, but is
removed from the final output (Loftsson and
R?gnvaldsson, 2007). Let us illustrate again:
5) {*SUBJ [NPn fj?lskylda nven NP] {*QUAL
[NPg sp?konunnar nveeg NP] *QUAL} *SUBJ}
526
[PP ? ao [NPd [APd gamla lhe?vf AP] h?sinu
nhe?g NP] PP]
In 5), an intermediate output is shown from
one of the transducers of IceParser, for the sen-
tence from 4). Note that letters have been ap-
pended to some of the phrase labels. This letter
denotes the case of the corresponding phrase, e.g.
?n?=nominative, ?a?=accusative, ?d?=dative, and
?g?=genitive.
The case letter attached to the phrase labels can
thus be used when searching for specific types
of errors. Consider, for example, the pattern
PrepAccError (slightly simplified) which is used
for detecting the error shown in 5) (some details
are left out)4:
PrepTagAcc = ao{WhiteSpace}+
PrepAcc = {Word}{PrepTagAcc}
PrepAccError =
"[PP"{PrepAcc}("[NP"[nde]~"NP]")
This pattern searches for a string starting with
?[PP? followed by a preposition governing the
accusative case ({PrepAcc}), followed by a sub-
string starting with a noun phrase ?[NP?, marked
as either nominative, dative or genitive case
(?[nde]?), and ending with ?NP]?.
We have designed three kinds of patterns, one
for PP errors as shown above, one for disagree-
ment errors inside NPs, and one for specific VP
(verb phrase) errors.
The NP patterns are more complicated than the
PP patterns, and due to lack of space we are not
able to describe them here in detail. Briefly, we
extract noun phrases and use string processing
to compare the gender, number and case features
in nouns to, for example, the previous adjective
or pronoun. If a disagreement is found, we print
out the corresponding noun phrase. To illustrate,
consider the sentence ?? ?essum landshluta
voru fj?lm?rg einkasj?krah?s? (?in this part-of-
the-country were numerous private-hospitals?),
annotated by IceParser in the following way:
6) [PP ? a? [NP ?essum fakf? landshluta nke?
NP] PP] [VPb voru sfg3f? VPb] {*SUBJ< [NP
[AP fj?lm?rg lhfnsf AP] einkasj?krah?s nhfn NP]
*SUBJ<}
4For writing regular expression patterns, we used the lex-
ical analyser generator tool JFlex, http://jflex.de/.
In this example, there is a disagreement error in
number between the demonstrative pronoun ??es-
sum? and the following noun ?landshluta?. The
second ?f? letter in the tag ?fakf?? for ??essum?
denotes plural and the letter ?e? in the tag ?nke??
for ?landshluta? denotes singular.
Our VP patterns mainly search for disagree-
ments (in person and number) between a subject
and the following verb5. Consider, for example,
the sentence ??g les meira um v?sindin? (?I read
more about (the) science?), annotated by IceParser
in the following manner:
7) {*SUBJ> [NP ?g fp1en NP] *SUBJ>} [VP
les sfg3en VP] {*OBJ< [AP meira lheovm AP]
*OBJ<} [PP um ao [NP v?sindin nhfog NP] PP]
The subject ??g? is here correctly tagged as
personal pronoun, first person, (?fp1en?), but the
verb ?les? is incorrectly tagged as third person
(?sfg3en?).
By applying these pattern searches to the output
of IceParser for the whole IFD corpus, we needed
to examine 1,489 error candidates, or 0.25% of
the corpus. Since shallow parsers have been de-
veloped for various languages, this error detection
method may be tailored to other morphologically
complex languages.
Notice that the above search patterns could po-
tentially be used in a grammar checking compo-
nent for Icelandic text. In that case, input text
would be PoS tagged with any available tagger,
shallow parsed with IceParser, and then the above
patterns used to find these specific types of feature
agreement error candidates.
4 Results
Table 1 shows the results of applying the three er-
ror detection methods on the IFD corpus. The col-
umn ?Error candidates? shows the number of PoS
tagging error candidates detected by each method.
The column ?Errors corrected? shows the num-
ber of tokens actually corrected, i.e. how many
of the error candidates were true errors. The col-
umn ?Precision? shows the ratio of correctly de-
tected errors to all error candidates. The column
?Ratio of corpus? shows the ratio of tokens cor-
rected to all tokens in the IFD corpus. The column
5Additionally, one VP pattern searches for a substring
containing the infinitive marker (the word ?a?? (?to?)), imme-
diately followed by a verb which is not tagged as an infinitive
verb.
527
Method Sub- Error Errors Precision Ratio of Uniqueness Feature
type candidates corrected (%) corpus (%) rate (%) agreement (%)
variation n-gram 254 0.04 65.0 4.7
5 taggers 5317 883 16.6 0.15 78.0 24.8
shallow parsing
All 1489 448 30.1 0.08 60.0 80.2
PP 511 226 44.2 0.04 51.3 70.4
NP 740 160 21.6 0.03 70.0 95.0
VP 238 62 26.1 0.01 61.3 77.1
Total distinct errors 1334 0.23
Table 1: Results for the three error detection methods
?Uniqueness rate? shows how large a ratio of the
errors corrected by a method were not found by
any other method. Finally, the column ?Feature
agreement? shows the ratio of errors that were fea-
ture agreement errors.
As discussed in Section 2.1, it is not straight-
forward to compute the precision of the variation
n-gram method, and we did not attempt to do so.
However, we can, using our experience from ex-
amining the variations, claim that the precision is
substantially lower than the 96.7% precision ob-
tained by Dickinson and Meurers (2003). We
had, indeed, expected low precision when using
the variation n-gram on the IFD corpus, because
this corpus and the underlying tagset is not as suit-
able for the method as the WSJ corpus (again, see
the discussion in Section 2.1). Note that as a re-
sult of applying the variation n-gram method, only
0.04% of the tokens in the IFD corpus were found
to be incorrectly tagged. This ratio is 8.5 times
lower than the ratio obtained by Dickinson and
Meurers when applying the same method on the
WSJ corpus. On the other hand, the variation n-
gram method nicely complements the other meth-
ods, because 65.0% of the 254 hand-corrected er-
rors were uniquely corrected on the basis of this
method.
Table 1 shows that most errors were detected by
applying the ?5 taggers? method ? 0.15% of the to-
kens in the corpus were found to be incorrectly an-
notated on the basis of this method. The precision
of the method is 16.6%. Recall that by using a sin-
gle tagger for error detection, van Halteren (2000)
obtained a precision of 12.7%. One might have ex-
pected more difference in precision by using five
taggers vs. a single tagger, but note that the lan-
guages used in the two experiments, as well as the
tagsets, are totally different. Therefore, the com-
parison in precision may not be viable. Moreover,
it has been shown that tagging Icelandic text, us-
ing the IFD tagset, is a hard task (see Section 2.2).
Hence, even though five agreeing taggers disagree
with the gold standard, in a large majority of the
disagreements (83.4% in our case) the taggers are
indeed wrong.
Consider, for example, the simple sentence ???
getur ?a? enginn? (?then can it nobody?, meaning
?then nobody can do-it?), which exemplifies the
free word order in Icelandic. Here the subject is
?enginn? and the object is ??a??. Therefore, the
correct tagging (which is the one in the corpus)
is ???/aa getur/sfg3en ?a?/fpheo enginn/foken?, in
which ??a?? is tagged with the accusative case
(the last letter in the tag ?fpheo?). However, all
the five taggers make the mistake of tagging ??a??
with the nominative case (?fphen?), i.e. assuming
it is the subject of the sentence.
The uniqueness ratio for the 5-taggers method
is high or 78.0%, i.e. a large number of the errors
corrected based on this method were not found
(corrected) by any of the other methods. However,
bear in mind, that this method produces most error
candidates.
The error detection method based on shallow
parsing resulted in about twice as many errors
corrected than by applying the variation n-gram
method. Even though the precision of this method
as a whole (the subtype marked ?All? in Table
1) is considerably higher than when applying
the 5-taggers methods (30.1% vs. 16.6%), we
did expect higher precision. Most of the false
positives (error candidates which turned out not to
be errors) are due to incorrect phrase annotation in
IceParser. A common incorrect phrase annotation
is one which includes a genitive qualifier. To
illustrate, consider the following sentence ?sumir
far?eganna voru ? heimlei?? (?some of-the-
passengers were on-their-way home?), matched
528
by one of the NP error patterns:
8) {*QUAL [NP sumir fokfn far?eganna nkfeg
NP] *QUAL} [VPb voru sfg3f? VPb] [PP ? a?
[NP heimlei? nve? NP] PP]
Here ?sumir far?eganna? is annotated as a sin-
gle noun phrase, but should be annotated as two
noun phrases ?[NP sumir fokfn NP]? and ?[NP
far?eganna nkfeg NP]?, where the second one is
the genitive qualifier of the first one. If this was
correctly annotated by IceParser, the NP error pat-
tern would not detect any feature agreement error
for this sentence, because no match is carried out
across phrases.
The last column in Table 1 shows the ratio of
feature agreement errors, which are errors result-
ing from mismatch in gender/person, number or
case between two words (e.g., see examples 6) and
7) above). Examples of errors not resulting from
feature agreement are: a tag denoting the incorrect
word class, and a tag of a an object containing an
incorrect case (verbs govern the case of their ob-
jects).
Recall from Section 3.3 that rules were written
to search for feature agreement errors in the out-
put of IceParser. Therefore, a high ratio of the to-
tal errors corrected by the shallow parsing method
(80.2%) are indeed due to feature agreement mis-
matches. 95.0% and 70.4% of the NP errors and
the PP errors are feature agreement errors, respec-
tively. The reason for a lower ratio in the PP errors
is the fact that in some cases the proposed preposi-
tion should actually have been tagged as an adverb
(the proposed tag therefore denotes an incorrect
word class). In the case of the 5-taggers method,
24.8% of the errors corrected are due to feature
agreement errors but only 4.7% in the case of the
variation n-gram method.
The large difference between the three meth-
ods with regard to the ratio of feature agreement
errors, as well as the uniqueness ratio discussed
above, supports our claim that the methods are in-
deed complementary, i.e. a large ratio of the to-
kens that get hand-corrected based on each method
is uniquely corrected by that method.
Overall, we were able to correct 1,334 distinct
errors, or 0.23% of the IFD corpus, by applying
the three methods (see the last row of Table 1).
Compared to related work, this ratio is, for ex-
ample, lower than the one obtained by applying
the variation n-gram method on the WSJ corpus
(0.34%). The exact ratio is, however, not of prime
importance because the methods have been ap-
plied to different languages, different corpora and
different tagsets. Rather, our work shows that us-
ing a single method which has worked well for an
English corpus (the variation n-gram method) does
not work particularly well for an Icelandic cor-
pus but adding two other complementary methods
helps in finding errors missed by the first method.
5 Re-evaluation of taggers
Earlier work on evaluation of tagging accuracy for
Icelandic text has used the original IFD corpus
(without any error correction attempts). Since we
were able to correct several errors in the corpus,
we were confident that the tagging accuracy pub-
lished hitherto had been underestimated.
To verify this, we used IceTagger and TnT, two
of the three best performing taggers on Icelandic
text. Additionally, we used a changed version of
TnT, which utilises functionality from IceMorphy,
the morphological analyser of IceTagger, and a
changed version of IceTagger which uses a hidden
Markov Model (HMM) to disambiguate words
which can not be further disambiguated by apply-
ing rules (Loftsson, 2007b). In tables 2 and 3 be-
low, Ice denotes IceTagger, Ice* denotes IceTag-
ger+HMM, and TnT* denotes TnT+IceMorphy.
We ran 10-fold cross-validation, using the exact
same data-splits as used in (Loftsson, 2006), both
before error correction (i.e. on the original corpus)
and after the error correction (i.e. on the corrected
corpus). Note that in these two steps we did not re-
train the TnT tagger, i.e. it still used the language
model derived from the original uncorrected cor-
pus.
Using the original corpus, the average tagging
accuracy results (using the first nine splits), for
unknown words, known words, and all words, are
shown in Table 26. The average unknown word
ratio is 6.8%.
Then we repeated the evaluation, now using the
corrected corpus. The results are shown in Ta-
ble 3. By comparing the tagging accuracy for all
words in tables 2 and 3, it can be seen that the
accuracy had be underestimated by 0.13-0.18 per-
centage points. The taggers TnT* and Ice* benefit
the most from the corpus error correction ? their
6The accuracy figures shown in Table 2 are comparable to
the results in (Loftsson, 2006).
529
Words TnT TnT* Ice Ice*
Unknown 71.82 72.98 75.30 75.63
Known 91.82 92.60 92.78 93.01
All 90.45 91.25 91.59 91.83
Table 2: Average tagging accuracy (%) using the
original IFD corpus
Words TnT TnT* Ice Ice*
Unknown 71.88 73.03 75.36 75.70
Known 91.96 92.75 92.95 93.20
All 90.58 91.43 91.76 92.01
Table 3: Average tagging accuracy (%) using the
corrected IFD corpus
accuracy for all words increases by 0.18 percent-
age points. Recall that we hand-corrected 0.23%
of the tokens in the corpus, and therefore TnT*
and Ice* correctly annotate 78.3% (0.18/0.23) of
the corrected tokens.
Since the TnT tagger is a data-driven tagger, it
is interesting to see whether the corrected corpus
changes the language model (to the better) of the
tagger. In other words, does retraining using the
corrected corpus produce better results than using
the language model generated from the original
corpus? The answer is yes, as can be seen by com-
paring the accuracy figures for TnT and TnT* in
tables 3 and 4. The tagging accuracy for all words
increases by 0.10 and 0.07 percentage points for
TnT and TnT*, respectively.
The re-evaluation of the above taggers, with or
without retraining, clearly indicates that the qual-
ity of the PoS annotation in the IFD corpus has
significant effect on the accuracy of the taggers.
6 Conclusion
The work described in this paper consisted of two
stages. In the first stage, we used three error de-
tection methods to hand-correct PoS errors in an
Icelandic corpus. The first two methods are lan-
guage independent, and we argued that the third
method can be adapted to other morphologically
complex languages.
As we expected, the application of the first
method used, the variation n-gram method, did
result in relatively few errors being detected and
corrected (i.e. 254 errors). By adding two new
methods, the first based on the agreement of five
taggers, and the second based on shallow parsing,
we were able to detect and correct 1,334 errors in
Words TnT TnT*
Unknown 71.97 73.10
Known 92.06 92.85
All 90.68 91.50
Table 4: Average tagging accuracy (%) of TnT af-
ter retraining using the corrected IFD corpus
total, or 0.23% of the tokens in the corpus. Our
analysis shows that the three methods are comple-
mentary, i.e. a large ratio of the tokens that get
hand-corrected based on each method is uniquely
corrected by that method.
An interesting side effect of the first stage is
the fact that by inspecting the error candidates re-
sulting from the shallow parsing method, we have
noticed a number of systematic errors made by
IceParser which should, in our opinion, be rela-
tively easy to fix. Moreover, we noted that our
regular expression search patterns, for finding fea-
ture agreement errors in the output of IceParser,
could potentially be used in a grammar checking
tool for Icelandic.
In the second stage, we re-evaluated and re-
trained two PoS taggers for Icelandic based on the
corrected corpus. The results of the second stage
clearly indicate that the quality of the PoS annota-
tion in the IFD corpus has a significant effect on
the accuracy of the taggers.
It is, of course, difficult to estimate the recall
of our methods, i.e. how many of the true errors
in the corpus we actually hand-corrected. In future
work, one could try to increase the recall by a vari-
ant of the 5-taggers method. Instead of demanding
that all five taggers agree on a tag before compar-
ing the result to the gold standard, one could in-
spect those cases in which four out of the five tag-
gers agree. The problem, however, with that ap-
proach is that the number of cases that need to be
inspected grows substantially. By demanding that
all the five taggers agree on the tag, we needed
to inspect 5,317 error candidates. By relaxing the
conditions to four votes out of five, we would need
to inspect an additional 9,120 error candidates.
Acknowledgements
Thanks to the ?rni Magn?sson Institute for Ice-
landic Studies for providing access to the IFD cor-
pus and the morphological database of inflections,
and to all the developers of the software used in
this research for sharing their work.
530
References
Krist?n Bjarnad?ttir. 2005. Modern Icelandic Inflec-
tions. In H. Holmboe, editor, Nordisk Sprogte-
knologi 2005, pages 49?50. Museum Tusculanums
Forlag, Copenhagen.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the 6th Conference
on Applied Natural Language Processing, Seattle,
WA, USA.
Stef?n Briem. 1989. Automatisk morfologisk analyse
af islandsk tekst. In Papers from the Seventh Scan-
dinavian Conference of Computational Linguistics,
Reykjavik, Iceland.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. MBT: a Memory-Based Part
of Speech Tagger-Generator. In Proceedings of the
4th Workshop on Very Large Corpora, Copenhagen,
Denmark.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting Errors in Part-of-Speech Annotation. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Budapest, Hungary.
Markus Dickinson. 2008. Representations for cate-
gory disambiguation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING-08), Manchester, UK.
Mark Dredze and Joel Wallenberg. 2008. Icelandic
Data Driven Part of Speech Tagging. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, Columbus, OH, USA.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and
Magnus ?str?m. 1992. The Linguistic Annotation
System of the Stockholm-Ume? Project. Department
of General Linguistics, University of Ume?.
Sigr?n Helgad?ttir. 2005. Testing Data-Driven
Learning Algorithms for PoS Tagging of Icelandic.
In H. Holmboe, editor, Nordisk Sprogteknologi
2004, pages 257?265. Museum Tusculanums For-
lag, Copenhagen.
Pavel Kve?to?n and Karel Oliva. 2002. Achieving an
Almost Correct PoS-Tagged Corpus. In P. Sojka,
I. Kopec?ek, and K. Pala, editors, Proceedings of
the 5th International Conference on TEXT, SPEECH
and DIALOGUE, Brno, Czech Republic.
Hrafn Loftsson and Eir?kur R?gnvaldsson. 2007.
IceParser: An Incremental Finite-State Parser for
Icelandic. In Proceedings of the 16th Nordic Con-
ference of Computational Linguistics (NoDaLiDa
2007), Tartu, Estonia.
Hrafn Loftsson. 2006. Tagging Icelandic text:
an experiment with integrations and combinations
of taggers. Language Resources and Evaluation,
40(2):175?181.
Hrafn Loftsson. 2007b. Tagging and Parsing Icelandic
Text. Ph.D. thesis, University of Sheffield, Sheffield,
UK.
Hrafn Loftsson. 2008. Tagging Icelandic text: A lin-
guistic rule-based approach. Nordic Journal of Lin-
guistics, 31(1):47?72.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Be?ta Megyesi. 2001. Comparing Data-Driven Learn-
ing Algorithms for PoS Tagging of Swedish. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001), Pitts-
burgh, PA, USA.
Tetsuji Nakagawa and Yuji Matsumoto. 2002. Detect-
ing errors in corpora using support vector machines.
In Proceedings of the 19th International Conference
on Computational Linguistics, Taipei, Taiwan.
Grace Ngai and Radu Florian. 2001. Transformation-
Based Learning in the Fast Lane. In Proceedings of
the 2nd Conference of the North American Chapter
of the ACL, Pittsburgh, PA, USA.
J?rgen Pind, Fri?rik Magn?sson, and Stef?n Briem.
1991. ?slensk or?t??nib?k [The Icelandic Frequency
Dictionary]. The Institute of Lexicography, Univer-
sity of Iceland, Reykjavik.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA, USA.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 2001. Improving Accuracy in Wordclass
Tagging through Combination of Machine Learning
Systems. Computational Linguistics, 27(2):199?
230.
Hans van Halteren. 2000. The Detection of Incon-
sistency in Manually Tagged Text. In A. Abeill?,
T. Brants, and H. Uszkoreit, editors, Proceedings of
the 2nd Workshop on Linguistically Interpreted Cor-
pora, Luxembourg.
531
Proceedings of NAACL HLT 2007, Companion Volume, pages 105?108,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Tagging Icelandic text using a linguistic and a statistical tagger
Hrafn Loftsson?
Department of Computer Science
Reykjavik University
Reykjavik IS-103, Iceland
hrafn@ru.is
Abstract
We describe our linguistic rule-based tag-
ger IceTagger, and compare its tagging ac-
curacy to the TnT tagger, a state-of-the-
art statistical tagger, when tagging Ice-
landic, a morphologically complex lan-
guage. Evaluation shows that the average
tagging accuracy is 91.54% and 90.44%,
obtained by IceTagger and TnT, respec-
tively. When tag profile gaps in the lex-
icon, used by the TnT tagger, are filled
with tags produced by our morphological
analyser IceMorphy, TnT?s tagging accu-
racy increases to 91.18%.
1 Introduction
In this paper, we use a linguistic rule-based method
(LRBM) and a data-driven method (DDM) for tagg-
ing text in the morphologically complex Icelandic
language.
We present a novel LRBM. The tagger based on
this method, hereafter called IceTagger, uses about
175 local rules for initial disambiguation, and a set
of heuristics, to force feature agreement where ap-
propriate, for further disambiguation.
The average tagging accuracy of IceTagger is
91.54%, compared to 90.44% achieved by the TnT
tagger, a state-of-the-art statistical tagger (Brants,
2000). IceTagger makes 11.5% less errors than TnT.
On the other hand, when tag profile gaps in the lex-
icon, used by TnT, are filled with tags produced by
? The author is also affiliated with the Dept. of Computer
Science, University of Sheffield, Sheffield, S1 4DP, UK.
IceMorphy, our morphological analyser, TnT?s tagg-
ing accuracy increases to 91.18%. In that case, Ice-
Tagger makes 4.1% less errors than TnT.
The remainder of this paper is organised as fol-
lows: In Sect. 2, we describe the different tagging
methods in more detail. Sect. 3 briefly describes the
Icelandic language and the tagset. The components
of IceTagger are described in Sect. 4, and evaluation
results are presented in Sect. 5.
2 The tagging methods
DDMs use machine learning to automatically derive
a language model from, usually, hand-annotated cor-
pora. An advantage of the DDMs is their language
and tagset independence property. Their disadvan-
tage is that a tagged corpus is essential for training.
Furthermore, the limited window size used for dis-
ambiguation (e.g. three words) can be responsible
for some of the tagging errors.
One of the better known statistical data-driven
tagger is the TnT tagger (written in C). The tag-
ger uses a second order (trigram) Hidden Markov
model. The probabilities of the model are esti-
mated from a training corpus using maximum like-
lihood estimation. New assignments of part-of-
speech (POS) to words is found by optimising the
product of lexical probabilities (p(wi|tj)) and con-
textual probabilities (p(ti|ti?1, ti?2)) (where wi and
ti are the ith word and tag, respectively).
In contrast to DDMs, LRBMs are developed with
the purpose of tagging a specific language using a
particular tagset. The purpose of the rules is, usu-
ally, to remove illegitimate tags fromwords based on
context. The advantage of LRBMs is that they do not
105
rely (to the same extent as DDMs) on the existence
of a tagged corpus, and rules can be written to refer
to words and tags in the entire sentence. The con-
struction of a linguistic rule-based tagger, however,
has been considered a difficult and time-consuming
task (Voutilainen, 1995).
One of the better known LRBMs is the Con-
straint Grammar (CG) framework (Karlsson et al,
1995), in which both POS and grammatical func-
tions are tagged. The EngCG-2 tagger, developed
over several years and consisting of 3,600 rules, has
been shown to obtain high accuracy (Samuelsson
and Voutilainen, 1997).
The development time of our LRBM (written in
Java; including a tokeniser, IceMorphy and Ice-
Tagger) was 7 man-months, which can be consid-
ered a short development time for a LRBM. This is
mainly due to the emphasis on using heuristics (see
Sect. 4.3) for disambiguation, as opposed to writing
a large number of local rules.
3 The Icelandic language and its tagset
The Icelandic language is one of the Nordic lan-
guages. The language is morphologically rich,
mainly due to inflectional complexity. A thorough
description of the language can, for example, be
found in (?r?insson, 1994).
The main Icelandic tagset, constructed in the
compilation of the tagged corpus Icelandic Fre-
quency Dictionary (IFD) (Pind et al, 1991), is large
(about 660 tags) compared to related languages. In
this tagset, each character in a tag has a particular
function. Table 1 shows the semantics of the noun
and the adjective tags.
To illustrate, consider the phrase ?fallegu hes-
tarnir? (beautiful horses). The corresponding tag
for ?fallegu? is ?lkfnvf ?, denoting adjective, mascu-
line, plural, nominative, weak declension, positive;
and the tag for ?hestarnir? is ?nkfng? denoting noun,
masculine, plural, nominative with suffixed definite
article.
4 IceTagger
IceTagger consists of three main components: an
unknown word guesser, local rules for initial disam-
biguation and heuristics for further disambiguation.
Both the local rules and the heuristics have been de-
Char Category/ Symbol ? semantics
# Feature
1 Word class n?noun, l?adjective
2 Gender k?masculine, v?feminine,
h?neuter, x?unspecified
3 Number e?singular, f?plural,
4 Case n?nominative, o?accusative,
??dative, e?genitive
5 Article g?with suffixed article
5 Declension s?strong, v?weak
6 Proper noun m?person, ??place, s?other
6 Degree f?positive, m?comparative,
e?superlative
Table 1: The semantics of the noun and the adjective
tags.
veloped using linguistic knowledge and tuned using
a development corpus (described in Sect. 5).
4.1 The unknown word guesser
The purpose of our morphological analyser Ice-
Morphy, which is used as an unknown word guesser
by IceTagger, is to generate all appropriate tags
for a given word. It uses a familiar approach to
unknown word guessing, i.e. it performs mor-
phological/compound analysis and ending analysis
(Mikheev, 1997; Nakov et al, 2003). Additionally,
IceMorphy includes an important module for handl-
ing tag profile gaps (for a thorough description of
IceMorphy, consult (Loftsson, 2006a)).
A tag profile gap arises when a particular word,
listed in a lexicon derived from a corpus, has some
missing tags in its tag profile (set of possible tags).
The missing tag(s) might just not have been encoun-
tered during the derivation of the lexicon (e.g. dur-
ing training). For each noun, adjective or verb, of a
particular morphological class, IceMorphy is able to
fill in the gaps for the given word.
To illustrate, consider the word ?konu? (woman),
and let us assume that only the tag ?nveo? (denoting
noun, feminine, singular, accusative) is found in the
lexicon. Based on the ?u? morphological suffix and
the accusative case of the tag, IceMorphy assumes
the word belongs to a particular morphological fem-
inine noun class, in which singular accusative, dative
and genitive cases have the same word form. Con-
106
sequently, IceMorphy generates the correct missing
tags: ?nve?? and ?nvee?.
4.2 Local rules
The purpose of a local rule is to eliminate inappro-
priate tags from words, based on a local context (a
window of 5 words; two words to the left and right
of the focus word). This reductionistic approach is
common in rule-based taggers. It is, for example,
used in the CG systems.
In principle, the local rules are unordered. The fir-
ing of a rule is, however, dependent on the order of
the words in a sentence. A sentence to be tagged is
scanned from left to right and all tags of each word
are checked in a sequence. Depending on the word
class (the first letter of the tag) of the focus word, the
token is sent to the appropriate disambiguation rou-
tine, which checks a variety of disambiguation con-
straints applicable to the particular word class and
the surrounding words. At each step, only tags for
the focus word are eliminated.
The format of a local rule is: If <condition>
eliminate tag t. A <condition> is a boolean
expression, whose individual components can re-
fer to lexical forms or individual characters (word
class/morphological features) of tags. The follow-
ing are examples of <condition> (L1/R1 and L2/R2
denote tokens one and two to the left/right of the fo-
cus word, F , respectively):
L1.isOnlyWordClass(x) AND L2.isOnlyWordClass(y)
R1.isWordClass(x) OR R2.isWordClass(y)
L1.isWordClass(x) AND t.isCase(y) AND t.isGender(z)
R1.lexeme.equals(x) AND F .isWordClass(y)
To exemplify, consider the sentence part: ?vi?
vorum . . . ? (we were . . . ). The word ?vi?? can
have the following five tags (?_? is used as a sep-
arator between tags): ?ao_a?_fp1fn_aa_nkeo?. For
illustration purposes, it is sufficient to point out
that the first two tags denote prepositions govern-
ing the accusative and the dative cases, respec-
tively. Since the following word is a verb (?vo-
rum?) and prepositions only precede nominals, a
rule, with <condition>=R1.isOnlyWordClass(Verb),
eliminates preposition tags in this context, leaving
only the tags ?fp1fn_aa_nkeo?.
The current version of our tagger has 175 local
rules. The rules are written in a separate file and
compiled to Java code.
4.3 The heuristics
Once local disambiguation has been carried out,
each sentence is sent to a global heuristic mod-
ule, consisting of a collection of algorithmic proce-
dures. Its purpose is to perform grammatical func-
tion analysis, guess prepositional phrases (PPs) and
use the acquired knowledge to force feature agree-
ment where appropriate. We call these heuristics
global because, when disambiguating a particular
word, a heuristic can refer to a word which is not
in the nearest neighbourhood.
The heuristics repeatedly scan each sentence and
perform the following: 1) mark PPs, 2) mark verbs,
3) mark subjects, 4) force subject-verb agreement,
5) mark objects, 6) force subject-object agreement,
7) force verb-object agreement, 8) force nominal
agreement, and 9) force PP agreement. Lastly, the
default heuristic is simply to choose the most fre-
quent tag according to frequency information de-
rived from the IFD corpus. A detailed description of
all the heuristics can be found in (Loftsson, 2006b).
5 Evaluation
For evaluation, we used the pairs of ten training and
test corpora of the IFD corpus, produced by Helga-
d?ttir (2004). We used the first nine of these test cor-
pora for evaluation, but the tenth one was set aside
and used as the development corpus for IceTagger.
For each test corpus (10% of the IFD) the corre-
sponding training corpus (90% of the IFD) was used
to deduce the lexicon(s) used by TnT, IceTagger and
IceMorphy. When testing the two taggers, we thus
made sure that the ratio of unknown words was (al-
most) the same.
The accuracy of a base tagger, which assigns each
known word its most frequent tag, and the most fre-
quent noun tag/proper noun tag to lower case/upper
case unknown words, is 76.27% (see table 2).
The average tagging accuracy of IceTagger for all
words is 91.54%, compared to 90.44% for TnT (see
table 2). IceTagger makes 11.5% less errors than
TnT1.
In order to improve the tagging accuracy of TnT,
we used the tag profile gap filling mechanism of Ice-
1TnT is very fast, it tags about 50,000 tokens/sec on a Dell
Optiplex GX620 Pentium 4, 3.20 GHz. IceTagger tags about
2,700 tokens/sec.
107
Words Base TnT TnT* IceTagger
Unkn. 4.39% 71.68% 72.75% 75.09%
Known 81.84% 91.82% 92.53% 92.74%
All 76.27% 90.44% 91.18% 91.54%
Table 2: Average tagging accuracy of the various
taggers.
Morphy in the following manner. Each record in the
lexicon used by TnT consists of a word and the cor-
responding tags found in the training corpus. Addi-
tionally, to facilitate lexical probability calculations,
each tag is marked by its frequency (i.e. how of-
ten the tag appeared as a label for the given word).
We made IceMorphy generate a ?filled? lexicon such
that each generated missing tag was marked with the
frequency 12. We call the resulting tagger TnT*. In-
deed, when testing TnT*, we obtained an overall av-
erage tagging accuracy of 91.18% (see table 2). Ice-
Tagger makes 4.1% less errors than TnT*.
The development of IceTagger/IceMorphy took 7
man-months, but it has been worth the effort. First,
IceTagger does make fewer errors than TnT, and its
accuracy can probably be increased by improving
its individual components. Secondly, we have used
IceTagger in various tagger combination methods to
further increase the tagging accuracy of Icelandic
text (Loftsson, 2006c).
6 Conclusion
In this paper, we have compared the tagging accu-
racy of our linguistic rule-based tagger, IceTagger,
to the accuracy of TnT, a state-of-the-art statistical
tagger.
IceTagger uses only about 175 local rules, but is
able to achieve high accuracy through the use of
global heuristics along with automatic tag profile
gap filling. The average tagging accuracy of Ice-
Tagger is 91.54%, compared to 90.44% obtained by
the TnT tagger. On the other hand, we were able to
obtain 91.18% accuracy using TnT along with the
tag profile gap filling mechanism of IceMorphy.
In future work, we would like to improve individ-
ual components of IceTagger and IceMorphy, with
2This seems logical since the missing tags were not found in
the training corpus and are, hence, infrequent.
the purpose of further increasing the tagging accu-
racy.
References
T. Brants. 2000. TnT: A statistical part-of-speech tag-
ger. In Proceedings of the 6th Conference on Applied
natural language processing, Seattle, WA, USA.
S. Helgad?ttir. 2004. Testing Data-Driven Learning Al-
gorithms for PoS Tagging of Icelandic. In H. Holm-
boe, editor, Nordisk Sprogteknologi 2004. Museum
Tusculanums Forlag.
F. Karlsson, A. Voutilainen, J. Heikkil?, and A. Anttila.
1995. Constraint Grammar: A Language-
Independent System for Parsing Unrestricted Text.
Mouton de Gruyter, Berlin, Germany.
H. Loftsson. 2006a. Tagging Icelandic text: A lin-
guistic rule-based approach. Technical Report CS-06-
04, Department of Computer Science, University of
Sheffield.
H. Loftsson. 2006b. Tagging a Morphologically Com-
plex Language Using Heuristics. In T. Salakoski,
F. Ginter, S. Pyysalo, and T. Pahikkala, editors, Ad-
vances in Natural Language Processing, 5th Interna-
tional Conference on NLP, FinTAL 2006, Proceedings,
Turku, Finland.
H. Loftsson. 2006c. Tagging Icelandic text: An exper-
iment with integrations and combinations of taggers.
Language Resources and Evaluation, 40(2):175?181.
A. Mikheev. 1997. Automatic Rule Induction for Un-
known Word Guessing. Computational Linguistics,
21(4):543?565.
P. Nakov, Y. Bonev, G. Angelova, E. Cius, and W. Hahn.
2003. Guessing Morphological Classes of Unknown
German Nouns. In Proceedings of Recent Advances in
Natural Language Processing, Borovets, Bulgaria.
J. Pind, F. Magn?sson, and S. Briem. 1991. The Ice-
landic Frequency Dictionary. The Institute of Lexi-
cography, University of Iceland, Reykjavik, Iceland.
H. ?r?insson. 1994. Icelandic. In E. K?nig and J. Auw-
era, editors, The Germanic Languages. Routledge,
London.
C. Samuelsson and A Voutilainen. 1997. Comparing
a Linguistic and a Stochastic tagger. In Proceedings
of the 8th Conference of the European Chapter of the
ACL (EACL), Madrid, Spain.
A. Voutilainen. 1995. A syntax-based part-of-speech an-
alyzer. In Proceedings of the 7th Conference of the Eu-
ropean Chapter of the ACL (EACL), Dublin, Ireland.
108
