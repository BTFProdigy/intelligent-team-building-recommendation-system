Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 410?420, Prague, June 2007. c?2007 Association for Computational Linguistics
V-Measure: A conditional entropy-based external cluster evaluation
measure
Andrew Rosenberg and Julia Hirschberg
Department of Computer Science
Columbia University
New York, NY 10027
{amaxwell,julia}@cs.columbia.edu
Abstract
We present V-measure, an external entropy-
based cluster evaluation measure. V-
measure provides an elegant solution to
many problems that affect previously de-
fined cluster evaluation measures includ-
ing 1) dependence on clustering algorithm
or data set, 2) the ?problem of matching?,
where the clustering of only a portion of data
points are evaluated and 3) accurate evalu-
ation and combination of two desirable as-
pects of clustering, homogeneity and com-
pleteness. We compare V-measure to a num-
ber of popular cluster evaluation measures
and demonstrate that it satisfies several de-
sirable properties of clustering solutions, us-
ing simulated clustering results. Finally, we
use V-measure to evaluate two clustering
tasks: document clustering and pitch accent
type clustering.
1 Introduction
Clustering techniques have been used successfully
for many natural language processing tasks, such
as document clustering (Willett, 1988; Zamir and
Etzioni, 1998; Cutting et al, 1992; Vempala and
Wang, 2005), word sense disambiguation (Shin and
Choi, 2004), semantic role labeling (Baldewein et
al., 2004), pitch accent type disambiguation (Levow,
2006). They are particularly appealing for tasks
in which there is an abundance of language data
available, but manual annotation of this data is
very resource-intensive. Unsupervised clustering
can eliminate the need for (full) manual annotation
of the data into desired classes, but often at the cost
of making evaluation of success more difficult.
External evaluation measures for clustering can
be applied when class labels for each data point in
some evaluation set can be determined a priori. The
clustering task is then to assign these data points to
any number of clusters such that each cluster con-
tains all and only those data points that are members
of the same class Given the ground truth class la-
bels, it is trivial to determine whether this perfect
clustering has been achieved. However, evaluating
how far from perfect an incorrect clustering solution
is a more difficult task (Oakes, 1998) and proposed
approaches often lack rigor (Meila, 2007).
In this paper, we describe a new entropy-based
external cluster evaluation measure, V-MEASURE1 ,
designed to address the problem of quantifying such
imperfection. Like all external measures, V-measure
compares a target clustering ? e.g., a manually an-
notated representative subset of the available data ?
against an automatically generated clustering to de-
termine now similar the two are. We introduce two
complementary concepts, completeness and homo-
geneity, to capture desirable properties in clustering
tasks.
In Section 2, we describe V-measure and how it
is calculated in terms of homogeneity and complete-
ness. We describe several popular external cluster
evaluation measures and draw some comparisons to
V-measure in Section 3. In Section 4, we discuss
how some desirable properties for clustering are sat-
isfied by V-measure vs. other measures. In Sec-
tion 5, we present two applications of V-measure, on
document clustering and on pitch accent type clus-
tering.
2 V-Measure and Its Calculation
V-measure is an entropy-based measure which ex-
plicitly measures how successfully the criteria of ho-
mogeneity and completeness have been satisfied. V-
measure is computed as the harmonic mean of dis-
tinct homogeneity and completeness scores, just as
1The ?V? stands for ?validity?, a common term used to de-
scribe the goodness of a clustering solution.
410
precision and recall are commonly combined into
F-measure (Van Rijsbergen, 1979). As F-measure
scores can be weighted, V-measure can be weighted
to favor the contributions of homogeneity or com-
pleteness.
For the purposes of the following discussion, as-
sume a data set comprising N data points, and two
partitions of these: a set of classes, C = {ci|i =
1, . . . , n} and a set of clusters, K = {ki|1, . . . ,m}.
Let A be the contingency table produced by the clus-
tering algorithm representing the clustering solution,
such that A = {aij} where aij is the number of data
points that are members of class ci and elements of
cluster kj .
To discuss cluster evaluation measures we intro-
duce two criteria for a clustering solution: homo-
geneity and completeness. A clustering result sat-
isfies homogeneity if all of its clusters contain only
data points which are members of a single class. A
clustering result satisfies completeness if all the data
points that are members of a given class are elements
of the same cluster. The homogenity and complete-
ness of a clustering solution run roughly in opposi-
tion: Increasing the homogeneity of a clustering so-
lution often results in decreasing its completeness.
Consider, two degenerate clustering solutions. In
one, assigning every datapoint into a single cluster,
guarantees perfect completeness ? all of the data
points that are members of the same class are triv-
ially elements of the same cluster. However, this
cluster is as unhomogeneous as possible, since all
classes are included in this single cluster. In an-
other solution, assigning each data point to a dis-
tinct cluster guarantees perfect homogeneity ? each
cluster trivially contains only members of a single
class. However, in terms of completeness, this so-
lution scores very poorly, unless indeed each class
contains only a single member. We define the dis-
tance from a perfect clustering is measured as the
weighted harmonic mean of measures of homogene-
ity and completeness.
Homogeneity:
In order to satisfy our homogeneity criteria, a
clustering must assign only those datapoints that are
members of a single class to a single cluster. That is,
the class distribution within each cluster should be
skewed to a single class, that is, zero entropy. We de-
termine how close a given clustering is to this ideal
by examining the conditional entropy of the class
distribution given the proposed clustering. In the
perfectly homogeneous case, this value, H(C|K),
is 0. However, in an imperfect situation, the size of
this value, in bits, is dependent on the size of the
dataset and the distribution of class sizes. There-
fore, instead of taking the raw conditional entropy,
we normalize this value by the maximum reduction
in entropy the clustering information could provide,
specifically, H(C).
Note that H(C|K) is maximal (and equals H(C))
when the clustering provides no new information ?
the class distribution within each cluster is equal to
the overall class distribiution. H(C|K) is 0 when
each cluster contains only members of a single class,
a perfectly homogenous clustering. In the degen-
erate case where H(C) = 0, when there is only a
single class, we define homogeneity to be 1. For a
perfectly homogenous solution, this normalization,
H(C|K)
H(C) , equals 0. Thus, to adhere to the convention
of 1 being desirable and 0 undesirable, we define ho-
mogeneity as:
h =
{
1 if H(C,K) = 0
1? H(C|K)H(C) else
(1)
where
H(C|K) = ?
|K|
?
k=1
|C|
?
c=1
ack
N log
ack
?|C|
c=1 ack
H(C) = ?
|C|
?
c=1
?|K|
k=1 ack
n log
?|K|
k=1 ack
n
Completeness:
Completeness is symmetrical to homogeneity. In
order to satisfy the completeness criteria, a cluster-
ing must assign all of those datapoints that are mem-
bers of a single class to a single cluster. To eval-
uate completeness, we examine the distribution of
cluster assignments within each class. In a perfectly
complete clustering solution, each of these distribu-
tions will be completely skewed to a single cluster.
We can evaluate this degree of skew by calculat-
ing the conditional entropy of the proposed cluster
distribution given the class of the component dat-
apoints, H(K|C). In the perfectly complete case,
H(K|C) = 0. However, in the worst case scenario,
411
each class is represented by every cluster with a dis-
tribution equal to the distribution of cluster sizes,
H(K|C) is maximal and equals H(K). Finally, in
the degenerate case where H(K) = 0, when there
is a single cluster, we define completeness to be 1.
Therefore, symmetric to the calculation above, we
define completeness as:
c =
{
1 if H(K,C) = 0
1 ? H(K|C)H(K) else
(2)
where
H(K|C) = ?
|C|
?
c=1
|K|
?
k=1
ack
N log
ack
?|K|
k=1 ack
H(K) = ?
|K|
?
k=1
?|C|
c=1 ack
n log
?|C|
c=1 ack
n
Based upon these calculations of homogeneity
and completeness, we then calculate a clustering
solution?s V-measure by computing the weighted
harmonic mean of homogeneity and completeness,
V? = (1+?)?h?c(??h)+c . Similarly to the familiar F-
measure, if ? is greater than 1 completeness is
weighted more strongly in the calculation, if ? is less
than 1, homogeneity is weighted more strongly.
Notice that the computations of homogeneity,
completeness and V-measure are completely inde-
pendent of the number of classes, the number of
clusters, the size of the data set and the clustering al-
gorithm used. Thus these measures can be applied to
and compared across any clustering solution, regard-
less of the number of data points (n-invariance), the
number of classes or the number of clusters. More-
over, by calculating homogeneity and completeness
separately, a more precise evaluation of the perfor-
mance of the clustering can be obtained.
3 Existing Evaluation Measures
Clustering algorithms divide an input data set into
a number of partitions, or clusters. For tasks where
some target partition can be defined for testing pur-
poses, we define a ?clustering solution? as a map-
ping from each data point to its cluster assignments
in both the target and hypothesized clustering. In the
context of this discussion, we will refer to the target
partitions, or clusters, as CLASSES, referring only to
hypothesized clusters as CLUSTERS.
Two commonly used external measures for as-
sessing clustering success are Purity and Entropy
(Zhao and Karypis, 2001), defined as,
Purity = ?kr=1 1n maxi(nir)
Entropy = ?kr=1 nrn (? 1log q
?q
i=1
nir
nr log
nir
nr )
where q is the number of classes, k the number
of clusters, nr is the size of cluster r, and nir is the
number of data points in class i clustered in cluster
r.
Both these approaches represent plausable ways
to evaluate the homogeneity of a clustering solution.
However, our completeness criterion is not mea-
sured at all. That is, they do not address the ques-
tion of whether all members of a given class are in-
cluded in a single cluster. Therefore the Purity and
Entropy measures are likely to improve (increased
Purity, decreased Entropy) monotonically with
the number of clusters in the result, up to a degen-
erate maximum where there are as many clusters as
data points. However, clustering solutions rated high
by either measure may still be far from ideal.
Another frequently used external clustering eval-
uation measure is commonly refered to as ?cluster-
ing accuracy?. The calculation of this accuracy is
inspired by the information retrieval metric of F-
Measure (Van Rijsbergen, 1979). The formula for
this clustering F-measure as described in (Fung et
al., 2003) is shown in Figure 3.
Let N be the number of data points, C the set of classes, K
the set of clusters and nij be the number of members of class
ci ? C that are elements of cluster kj ? K.
F (C, K) =
X
ci?C
|ci|
N maxkj?K
{F (ci, kj)} (3)
F (ci, kj) =
2 ? R(ci, kj) ? P (ci, kj)
R(ci, kj) + P (ci, kj)
R(ci, kj) =
nij
|ci|
P (ci, kj) =
nij
|kj |
Figure 1: Calculation of clustering F-measure
This measure has a significant advantage over
Purity and Entropy, in that it does measure both
the homogeneity and the completeness of a cluster-
ing solution. Recall is calculated as the portion of
items from class i that are present in cluster j, thus
measuring how complete cluster j is with respect to
class i. Similarly, Precision is calculated as the por-
412
Solution A Solution B
F-Measure=0.5 F-Measure=0.5
V-Measure=0.14 V-Measure=0.39
Solution C Solution D
F-Measure=0.6 F-Measure=0.6
V-Measure=0.30 V-Measure=0.41
Figure 2: Examples of the Problem of Matching
tion of cluster j that is a member of class i, thus mea-
suring how homogenous cluster j is with respect to
class i.
Like some other external cluster evaluation tech-
niques (misclassification index (MI) (Zeng et al,
2002), H (Meila and Heckerman, 2001), L (Larsen
and Aone, 1999), D (van Dongen, 2000), micro-
averaged precision and recall (Dhillon et al, 2003)),
F-measure relies on a post-processing step in which
each cluster is assigned to a class. These techniques
share certain problems. First, they calculate the
goodness not only of the given clustering solution,
but also of the cluster-class matching. Therefore, in
order for the goodness of two clustering solutions to
be compared using one these measures, an identical
post-processing algorithm must be used. This prob-
lem can be trivially addressed by fixing the class-
cluster matching function and including it in the def-
inition of the measure as in H . However, a second
and more critical problem is the ?problem of match-
ing? (Meila, 2007). In calculating the similarity be-
tween a hypothesized clustering and a ?true? cluster-
ing, these measures only consider the contributions
from those clusters that are matched to a target class.
This is a major problem, as two significantly differ-
ent clusterings can result in identical scores.
In figure 2, we present some illustrative examples
of the problem of matching. For the purposes of this
discussion we will be using F-Measure as the mea-
sure to describe the problem of matching, however,
these problems affect any measure which requires a
mapping from clusters to classes for evaluation.
In the figures, the shaded regions represent CLUS-
TERS, the shapes represent CLASSES. In a perfect
clustering, each shaded region would contain all and
only the same shapes. The problem of matching
can manifest itself either by not evaluating the en-
tire membership of a cluster, or by not evaluating
every cluster. The former situation is presented in
the figures A and B in figure 2. The F-Measure of
both of these clustering solutions in 0.6. (The preci-
sion and recall for each class is 35 .) That is, for each
class, the best or ?matched? cluster contains 3 of 5
elements of the class (Recall) and 3 of 5 elements of
the cluster are members of the class (Precision). The
make up of the clusters beyond the majority class is
not evaluated by F-Measure. Solution B is a better
clustering solution than solution A, in terms of both
homogeneity (crudely, ?each cluster contains fewer2
classes?) and completeness (?each class is contained
in fewer clusters?). Indeed, the V-Measure of so-
lution B (0.387) is greater than that of solution A
(0.135). Solutions C and D represent a case in which
not every cluster is considered in the evaluation of
F-Measure. In this example, the F-Measure of both
solutions is 0.5 (the harmonic mean of 35 and 37 ). The
small ?unmatched? clusters are not measured at all
in the calculation of F-Measure. Solution D is a bet-
ter clustering than solution C ? there are no incorrect
clusterings of different classes in the small clusters.
V-Measure reflects this, solution C has a V-measure
of 0.30 while the V-measure of solution D is 0.41.
A second class of clustering evaluation techniques
is based on a combinatorial approach which exam-
ines the number of pairs of data points that are clus-
tered similarly in the target and hypothesized clus-
tering. That is, each pair of points can either be 1)
clustered together in both clusterings (N11), 2) clus-
tered separately in both clusterings (N00), 3) clus-
tered together in the hypothesized but not the tar-
get clustering (N01) or 4) clustered together in the
target but not in the hypothesized clustering (N10).
Based on these 4 values, a number of measures have
been proposed, including Rand Index (Rand, 1971),
2Homogeneity is not measured by V-measure as a count of
the number of classes contained by a cluster but ?fewer? is an
acceptable way to conceptualize this criterion for the purposes
of these examples.
413
Adjusted Rand Index (Hubert and Arabie, 1985), ?
statistic (Hubert and Schultz, 1976), Jaccard (Mil-
ligan et al, 1983), Fowlkes-Mallows (Fowlkes and
Mallows, 1983) and Mirkin (Mirkin, 1996). We il-
lustrate this class of measures with the calculation of
Rand Index. Rand(C,K) = N11+N00n(n?1)/2 Rand Index
can be interpreted as the probability that a pair of
points is clustered similarly (together or separately)
in C and K .
Meila (2007) describes a number of poten-
tial problems of this class of measures posed by
(Fowlkes and Mallows, 1983) and (Wallace, 1983).
The most basic is that these measures tend not to
vary over the interval of [0, 1]. Transformations like
those applied by the adjusted Rand Index and a mi-
nor adjustment to the Mirkin measure (see Section
4) can address this problem. However, pair match-
ing measures also suffer from distributional prob-
lems. The baseline for Fowlkes-Mallows varies sig-
nificantly between 0.6 and 0 when the ratio of data
points to clusters is greater than 3 ? thus includ-
ing nearly all real-world clustering problems. Simi-
larly, the Adjusted Rand Index, as demonstrated us-
ing Monte Carlo simulations in (Fowlkes and Mal-
lows, 1983), varies from 0.5 to 0.95. This variance
in the measure?s baseline prompts Meila to ask if the
assumption of linearity following normalization can
be maintained. If the behavior of the measure is so
unstable before normalization can users reasonably
expect stable behavior following normalization?
A final class of cluster evaluation measures are
based on information theory. These measures an-
alyze the distribution of class and cluster member-
ship in order to determine how successful a given
clustering solution is or how different two parti-
tions of a data set are. We have already examined
one member of this class of measures, Entropy.
From a coding theory perspective, Entropy is the
weighted average of the code lengths of each clus-
ter. Our V-measure is a member of this class of clus-
tering measures. One significant advantage that in-
formation theoretic evaluation measures have is that
they provide an elegant solution to the ?problem of
matching?. By examining the relative sizes of the
classes and clusters being evaluated, these measures
all evaluate the entire membership of each cluster ?
not just a ?matched? portion.
Dom?s Q0 measure (Dom, 2001) uses conditional
entropy, H(C|K) to calculate the goodness of a
clustering solution. That is, given the hypothesized
partition, what is the number of bits necessary to
represent the true clustering?
However, this term ? like the Purity and
Entropy measures ? only evaluates the homogene-
ity of a solution. To measure the completeness of the
hypothesized clustering, Dom includes a model cost
term calculated using a coding theory argument. The
overall clustering quality measure presented is the
sum of the costs of representing the data (H(C|K))
and the model. The motivation for this approach
is an appeal to parsimony: Given identical condi-
tional entropies, H(C|K), the clustering solution
with the fewest clusters should be preferred. Dom
also presents a normalized version of this term, Q2,
which has a range of (0, 1] with greater scores being
representing more preferred clusterings.
Q0(C,K) = H(C|K)+
1
n
|K|
?
k=1
log
(h(k) + |C| ? 1
|C| ? 1
)
where C is the target partition, K is the hypothe-
sized partition and h(k) is the size of cluster k.
Q2(C,K) =
1
n
?|C|
c=1 log
(h(c)+|C|?1
|C|?1
)
Q0(C,K)
We believe that V-measure provides two significant
advantages over Q0 that make it a more useful diag-
nostic tool. First, Q0 does not explicitly calculate the
degree of completeness of the clustering solution.
The cost term captures some of this information,
since a partition with fewer clusters is likely to be
more complete than a clustering solution with more
clusters. However, Q0 does not explicitly address
the interaction between the conditional entropy and
the cost of representing the model. While this is
an application of the minimum description length
(MDL) principle (Rissanen, 1978; Rissanen, 1989),
it does not provide an intuitive manner for assessing
our two competing criteria of homogeneity and com-
pleteness. That is, at what point does an increase in
conditional entropy (homogeneity) justify a reduc-
tion in the number of clusters (completeness).
Another information-based clustering measure
is variation of information (V I) (Meila, 2007),
V I(C,K) = H(C|K)+H(K|C). V I is presented
414
as a distance measure for comparing partitions (or
clusterings) of the same data. It therefore does not
distinguish between hypothesized and target cluster-
ings. V I has a number of useful properties. First,
it satisfies the metric axioms. This quality allows
users to intuitively understand how V I values com-
bine and relate to one another. Secondly, it is ?con-
vexly additive?. That is to say, if a cluster is split,
the distance from the new cluster to the original is
the distance induced by the split times the size of
the cluster. This property guarantees that all changes
to the metric are ?local?: the impact of splitting or
merging clusters is limited to only those clusters in-
volved, and its size is relative to the size of these
clusters. Third, VI is n-invariant: the number of
data points in the cluster do not affect the value of
the measure. V I depends on the relative sizes of the
partitions of C and K , not on the number of points
in these partitions. However, V I is bounded by the
maximum number of clusters in C or K , k?. With-
out manual modification however, k? = n, where
each cluster contains only a single data point. Thus,
while technically n-invariant, the possible values of
V I are heavily dependent on the number of data
points being clustered. Thus, it is difficult to com-
pare V I values across data sets and clustering algo-
rithms without fixing k?, as V I will vary over differ-
ent ranges. It is a trivial modification to modify V I
such that it varies over [0,1]. Normalizing, V I by
log n or 1/2 log k? guarantee this range. However,
Meila (2007) raises two potential problems with this
modification. The normalization should not be ap-
plied if data sets of different sizes are to be com-
pared ? it negates the n-invariance of the measure.
Additionally, if two authors apply the latter normal-
ization and do not use the same value for k?, their
results will not be comparable.
While V I has a number of very useful distance
properties when analyzing a single data set across a
number of settings, it has limited utility as a general
purpose clustering evaluation metric for use across
disparate clusterings of disparate data sets. Our
homogeneity (h) and completeness (c) terms both
range over [0,1] and are completely n-invariant and
k?-invariant. Furthermore, measuring each as a ra-
tio of bit lengths has greater intuitive appeal than a
more opportunistic normalization.
V-measure has another advantage as a clustering
evaluation measure over V I and Q0. By evaluat-
ing homogeneity and completeness in a symmetri-
cal, complementary manner, the calculation of V-
measure makes their relationship clearly observable.
Separate analyses of homogeneity and complete-
ness are not possible with any other cluster evalu-
ation measure. Moreover, by using the harmonic
mean to combine homogeneity and completeness,
V-measure is unique in that it can also prioritize one
criterion over another, depending on the clustering
task and goals.
4 Comparing Evaluation Measures
Dom (2001) describes a parametric technique for
generating example clustering solutions. He then
proceeds to define five ?desirable properties? that
clustering accuracy measures should display, based
on the parameters used to generate the clustering so-
lution. To compare V-measure more directly to alter-
native clustering measures, we evaluate V-measure
and other measures against these and two additional
desirable properties.
The parameters used in generating a clustering so-
lution are as follows.
? |C| The number of classes
? |K| The number of clusters
? |Knoise| Number of ?noise? clusters;
|Knoise| < |K|
? |Cnoise| Number of ?noise? classes; |Cnoise| <
|C|
? ? Error probability; ? = ?1 + ?2 + ?3.
? ?1 The error mass within ?useful? class-cluster
pairs
? ?2 The error mass within noise clusters
? ?3 The error mass within noise classes
The construction of a clustering solution begins
with a matching of ?useful? clusters to ?useful?
classes3. There are |Ku| = |K| ? |Knoise| ?useful?
clusters and |Cu| = |C| ? |Cnoise| ?useful? classes.
The claim is useful classes and clusters are matched
to each other and matched pairs contain more data
points than unmatched pairs. Probability mass of
1 ? ? is evenly distributed across each match. Er-
ror mass of ?1 is evenly distributed across each pair
3The operation of this matching is omitted in the interest of
space. Interested readers should see (Dom, 2001).
415
of non-matching useful class/cluster pairs. Noise
clusters are those that contain data points equally
from each cluster. Error mass of ?2 is distributed
across every ?noise?-cluster/ ?useful?-class pair. We
extend the parameterization technique described in
(Dom, 2001) in with |Cnoise| and ?3. Noise classes
are those that contain data points equally from each
cluster. Error mass of ?3 is distributed across every
?useful?-cluster/?noise?-class pair. An example so-
lution, along with its generating parameters is given
in Figure 3.
C1 C2 C3 Cnoise1
K1 12 12 2 3
K2 2 2 12 3
Knoise1 4 4 4 0
Figure 3: Sample parametric clustering solution
with n = 60, |K| = 3, |Knoise| = 1, |C| =
3, |Cnoise| = 1, ?1 = .1, ?2 = .2, ?3 = .1
The desirable properties proposed by Dom are
given as P1-P5 in Table 1. We include two addi-
tional properties (P6,P7) relating the examined mea-
sure value to the number of ?noise? classes and ?3.
P1 For |Ku| < |C| and ?|Ku| ? (|C| ? |Ku|),
?M
?|Ku| > 0
P2 For |Ku| ? |C|, ?M?|Ku| < 0
P3 ?M?|Knoise| < 0, if ?2 > 0
P4 ?M??1 ? 0, with equality only if |Ku| = 1
P5 ?M??2 ? 0, with equality only if |Knoise| = 0
P6 ?M?|Cnoise| < 0, if ?3 > 0
P7 ?M??3 ? 0, with equality only if |Cnoise| = 0
Table 1: Desirable Properties of a cluster evaluation
measure M
To evaluate how different clustering measures sat-
isfy each of these properties, we systematically var-
ied each parameter, keeping |C| = 5 fixed.
? |Ku|: 10 values: 2, 3,. . . , 11
? |Knoise|: 7 values: 0, 1,. . . , 6
? |Cnoise|: 7 values: 0, 1,. . . , 6
? ?1: 4 values: 0, 0.033, 0.066, 0.1
? ?2: 4 values: 0, 0.066, 0.133, 0.2
? ?3: 4 values: 0, 0.066, 0.133, 0.2
We evaluated the behavior of V-Measure, Rand,
Mirkin, Fowlkes-Mallows, Gamma, Jaccard, VI,
Q0, F-Measure against the desirable properties P1-
P74. Based on the described systematic modification
of each parameter, only V-measure, VI and Q0 em-
pirically satisfy all of P1-P7 in all experimental con-
ditions. Full results reporting how frequently each
evaluated measure satisfied the properties based on
these experiments can be found in table 2.
All evaluated measures satisfy P4 and P7. How-
ever, Rand, Mirkin, Fowlkes-Mallows, Gamma, Jac-
card and F-Measure all fail to satisfy P3 and P6 in
at least one experimental configuration. This indi-
cates that the number of ?noise? classes or clusters
can be increased without reducing any of these mea-
sures. This implies a computational obliviousness to
potentially significant aspects of an evaluated clus-
tering solution.
5 Applying V-measure
In this section, we present two clustering experi-
ments. We describe a document clustering experi-
ment and evaluate its results using V-measure, high-
lighting the interaction between homogeneity and
completeness. Second, we present a pitch accent
type clustering experiment. We present results from
both of these experiments in order to show how V-
measure can be used to drawn comparisons across
data sets.
5.1 Document Clustering
Clustering techniques have been used widely to sort
documents into topic clusters. We reproduce such
an experiment here to demonstrate the usefulness
of V-measure. Using a subset of the TDT-4 cor-
pus (Strassel and Glenn, 2003) (1884 English news
wire and broadcast news documents manually la-
beled with one of 12 topics), we ran clustering
experiments using k-means clustering (McQueen,
1967) and evaluated the results using V-Measure,
VI and Q0 ? those measures that satisfied the de-
sirable properties defined in section 4. The top-
ics and relative distributions are as follows: Acts
4The inequalities in the desirable properties are inverted in
the evaluation of VI, Q0 and Mirkin as they are defined as dis-
tance, as opposed to similarity, measures.
416
Property Rand Mirkin Fowlkes ? Jaccard F-measure Q0 VI V-Measure
P1 0.18 0.22 1.0 1.0 1.0 1.0 1.0 1.0 1.0
P2 1.0 1.0 0.76 1.0 0.89 0.98 1.0 1.0 1.0
P3 0.0 0.0 0.30 0.19 0.21 0.0 1.0 1.0 1.0
P4 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
P5 0.50 0.57 1.0 1.0 1.0 1.0 1.0 1.0 1.0
P6 0.20 0.20 0.41 0.26 0.52 0.87 1.0 1.0 1.0
P7 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
Table 2: Rates of satisfaction of desirable properties
of Violence/War (22.3%), Elections (14.4%), Diplo-
matic Meetings (12.9%), Accidents (8.75%), Natu-
ral Disasters (7.4%), Human Interest (6.7%), Scan-
dals (6.5%), Legal Cases (6.4%), Miscellaneous
(5.3%), Sports (4.7), New Laws (3.2%), Science and
Discovery (1.4%).
We employed stemmed (Porter, 1980), tf*idf-
weighted term vectors extracted for each document
as the clustering space for these experiments, which
yielded a very high dimension space. To reduce
this dimensionality, we performed a simple feature
selection procedure including in the feature vector
only those terms that represented the highest tf*idf
value for at least one data point. This resulted in a
feature vector containing 484 tf*idf values for each
document. Results from k-means clustering are are
shown in Figure 4.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 1  10  100  1000
 3
 3.5
 4
 4.5
 5
 5.5
V-
m
ea
su
re
 a
nd
 Q
2 
va
lue
s
VI
 v
al
ue
s
number of clusters
V-Measure
VI
Q2
Figure 4: Results of document clustering measured
by V-Measure, VI and Q2
The first observation that can be drawn from these
results is the degree to which VI is dependent on the
number of clusters (k). This dependency severely
limits the usefulness of VI: it is inappropriate in se-
lecting an appropriate parameter for k or for evalu-
ating the distance between clustering solutions gen-
erated using different values of k.
V-measure and Q2 demonstrate similar behavior
in evaluating these experimental results. They both
reach a maximal value with 35 clusters, however, Q2
shows a greater descent as the number of clusters in-
creases. We will discuss this quality in greater detail
in section 5.2.
5.2 Pitch Accent Clustering
Pitch accent is how speakers of many languages
make a word intonational prominent. In most
pitch accent languages, words can also be ac-
cented in different ways to convey different mean-
ings (Hirschberg, 2002). In the ToBI labeling con-
ventions for Standard American English (Silverman
et al, 1992), for example, there are five different ac-
cent types (H*, L*, H+!H*, L+H*, L*+H).
We extracted a number of acoustic features from
accented words within the read portion of the Boston
Directions Corpus (BDC) (Nakatani et al, 1995) and
examined how well clustering in these acoustic di-
mensions correlates to manually annotated pitch ac-
cent types. We obtained a very skewed distribution,
with a majority of H* pitch accents.5 We there-
fore included only a randomly selected 10% sample
of H* accents, providing a more even distribution
of pitch accent types for clustering: H* (54.4%),
L*(32.1%), L+H* (26.5%), L*+H (2.8%), H+!H*
(2.1%).
We extracted ten acoustic features from each ac-
cented word to serve as the clustering space for
this experiment. Using Praat?s (Boersma, 2001) Get
Pitch (ac)... function, we calculated the mean F0
and ?F0, as well as z-score speaker normalized ver-
sions of the same. We included in the feature vector
the relative location of the maximum pitch value in
the word as well as the distance between this max-
5Pitch accents containing a high tone may also be down-
stepped, or spoken in a compressed pitch range. Here we col-
lapsed all DOWNSTEPPED instances of each pitch accent with
the corresponding non-downstepped instances.
417
imum and the point of maximum intensity. Finally,
we calculated the raw and speaker normalized slope
from the start of the word to the maximum pitch, and
from the maximum pitch to the end of the word.
Using this feature vector, we performed k-means
clustering and evaluate how successfully these di-
mensions represent differences between pitch accent
types. The resulting V-measure, VI and Q0 calcula-
tions are shown in Figure 5.
 0
 0.05
 0.1
 0.15
 0.2
 1  10  100  1000
 2
 3
 4
 5
 6
 7
 8
V-
m
ea
su
re
 a
nd
 Q
2 
va
lue
s
VI
 v
al
ue
s
number of clusters
VI
V-measure
Q2
Figure 5: Results of pitch accent clustering mea-
sured by V-Measure, VI and Q0
In evaluating the results from these experiments,
Q2 and V-measure reveal considerably different be-
haviors. Q2 shows a maximum at k = 10, and de-
scends at k increases. This is an artifact of the MDL
principle. Q2 makes the claim that a clustering so-
lution based on fewer clusters is preferable to one
using more clusters, and that the balance between
the number of clusters and the conditional entropy,
H(C|K), should be measured in terms of coding
length. With V-measure, we present a different argu-
ment. We contend that the a high value of k does not
inherently reduce the goodness of a clustering solu-
tion. Using these results as an example, we find that
at approximately 30 clusters an increase of clusters
translates to an increase in V-Measure. This is due to
an increased homogeneity (H(C|K)H(C) ) and a relatively
stable completeness (H(K|C)H(K) ). That is, inclusion of
more clusters leads to clusters with a more skewed
within-cluster distribution and a equivalent distribu-
tion of cluster memberships within classes. This is
intuitively preferable ? one criterion is improved, the
other is not reduced ? despite requiring additional
clusters. This is an instance in which the MDL prin-
ciple limits the usefulness of Q2. We again (see sec-
tion 5.1) observe the close dependency of VI and k.
Moreover, in considering figures 5 and 4, simulta-
neously, we see considerably higher values achieved
by the document clustering experiments. Given the
na??ve approaches taken in these experiments, this is
expected ? and even desired ? given the previous
work on these tasks: document clustering has been
notably more successfully applied than pitch accent
clustering. These examples allow us to observe how
transparently V-measure can be used to compare the
behavior across distinct data sets.
6 Conclusion
We have presented a new external cluster evaluation
measure, V-measure, and compared it with existing
clustering evaluation measures. V-measure is based
upon two criteria for clustering usefulness, homo-
geneity and completeness, which capture a cluster-
ing solution?s success in including all and only data-
points from a given class in a given cluster. We have
also demonstrated V-measure?s usefulness in com-
paring clustering success across different domains
by evaluating document and pitch accent cluster-
ing solutions. We believe that V-measure addresses
some of the problems that affect other cluster mea-
sures. 1) It evaluates a clustering solution indepen-
dent of the clustering algorithm, size of the data set,
number of classes and number of clusters. 2) It does
not require its user to map each cluster to a class.
Therefore, it only evaluates the quality of the cluster-
ing, not a post-hoc class-cluster mapping. 3) It eval-
uates the clustering of every data point, avoiding the
?problem of matching?. 4) By evaluating the criteria
of both homogeneity and completeness, V-measure
is more comprehensive than those that evaluate only
one. 5) Moreover, by evaluating these criteria sepa-
rately and explicitly, V-measure can serve as an el-
egant diagnositic tool providing greater insight into
clustering behavior.
Acknowledgments
The authors thank Kapil Thadani, Martin Jansche
and Sasha Blair-Goldensohn and for their feedback.
This work was funded in part by the DARPA GALE
program under a subcontract to SRI International.
418
References
Ulrike Baldewein, Katrin Erk, Sebastian Pado, and Detlef
Prescher. 2004. Semantic role labelling with similarity-
based generalization using EM-based clustering. In Pro-
ceedings of Senseval?04, Barcelona.
Paul Boersma. 2001. Praat, a system for doing phonetics by
computer. Glot International, 5(9-10):341?345.
Douglass R. Cutting, Jan O. Pedersen, David Karger, and
John W. Tukey. 1992. Scatter/gather: A cluster-based ap-
proach to browsing large document collections. In Proceed-
ings of the Fifteenth Annual International ACM SIGIR Con-
ference on Research and Development in Information Re-
trieval, pages 318?329.
I. S. Dhillon, S. Mallela, and D. S. Modha. 2003. Information-
theoretic co-clustering. In Proceedings of The Ninth ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining(KDD-2003), pages 89?98.
Byron E. Dom. 2001. An information-theoretic external
cluster-validity measure. Technical Report RJ10219, IBM,
October.
E. B. Fowlkes and C. L. Mallows. 1983. A method for com-
paring two hierarchical clusterings. Journal of the American
Statistical Association, 78:553?569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester. 2003. Hi-
erarchical document clustering using frequent itemsets. In
Proc. of the SIAM International Conference on Data Min-
ing.
Julia Hirschberg. 2002. The pragmatics of intonational mean-
ing. In Proc. Speech Prosody, pages 65?68.
L. Hubert and P. Arabie. 1985. Comparing partitions. Journal
of Classification, 2:193?218.
L. Hubert and J. Schultz. 1976. Quadratic assignment as a gen-
eral data analysis strategy. British Journal of Mathematical
and Statistical Psychology, 29:190?241.
Bjornar Larsen and Chinatsu Aone. 1999. Fast and effective
text mining using linear-time document clustering. In KDD
?99: Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages
16?22, New York, NY, USA. ACM Press.
Gina-Anne Levow. 2006. Unsupervised and semi-supervised
learning of tone and pitch accent. In Proceedings of the main
conference on Human Language Technology Conference of
the North American Chapter of the Association of Compu-
tational Linguistics, pages 224?231, Morristown, NJ, USA.
Association for Computational Linguistics.
J. McQueen. 1967. Some methods for classification and analy-
sis of multivariate observations. In Proc. of the Fifty Berke-
ley Symposium on Mathematical Statistics and Probability,
pages 281?297.
Marina Meila and David Heckerman. 2001. An experimen-
tal comparison of model-based clustering methods. Mach.
Learn., 42(1/2):9?29.
Marina Meila. 2007. Comparing clusterings ? an information
based distance. Journal of Multivariate Analysis, 98:873?
895.
G. W. Milligan, S. C. Soon, and L. M. Sokol. 1983. The ef-
fect of cluster size, dimensionality and the number of clustes
on recovery of true cluster structure. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 5:40?47.
Boris G. Mirkin. 1996. Mathematical classification and clus-
tering. Kluwer Academic Press.
Christine Nakatani, Julia Hirschberg, and Barbara Grosz. 1995.
Discourse structure in spoken language: Studies on speech
corpora. In Working Notes of AAAI-95 Spring Symposiom
on Empirical Methods in Discourse Interpretation.
Michael P. Oakes. 1998. Statistics for Corpus Linguistics. Ed-
inburgh University Press.
M. Porter. 1980. An algorithm for suffix stripping. Program,
14(3):130?137.
William M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statistical
Association, 66(336):846?850, Dec.
J. Rissanen. 1978. Modeling by shortest data description. Au-
tomatica, 14:465?471.
J. Rissanen. 1989. Stochastic complexity in statistical inquiry.
World Scientific Series in Computer Science, 15.
Sa-Im Shin and Key-Sun Choi. 2004. Automatic word sense
clustering using collocation for sense adaptation. In The Sec-
ond Global Wordnet Conference.
K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf, C. Wight-
man, P. Price, J. Pierrehumbert, and J. Hirschberg. 1992.
Tobi: A standard for labeling english prosody. In Proc. of
the 1992 International Conference on Spoken Language Pro-
cessing, volume 2, pages 12?16.
S. Strassel and M. Glenn. 2003. Creating
the annotated tdt-4 y2003 evaluation corpus.
http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt.
Stijn van Dongen. 2000. Performance criteria for graph cluster-
ing and markov cluster experiments. Technical report, CWI
(Centre for Mathematics and Computer Science), Amster-
dam, The Netherlands, The Netherlands.
C. J. Van Rijsbergen. 1979. Information Retrieval, 2nd edition.
Dept. of Computer Science, University of Glasgow.
Santosh Vempala and Grant Wang. 2005. The benefit of
spectral projection for document clustering. In Workshop
on Clustering High Dimensional Data and its Applications
Held in conjunction with Fifth SIAM International Confer-
ence on Data Mining (SDM 2005).
D. L. Wallace. 1983. Comment. Journal of the American Sta-
tistical Association, 78:569?576.
Peter Willett. 1988. Recent trends in hierarchic document clus-
tering: a critical review. Inf. Process. Manage., 24(5):577?
597.
419
Oren Zamir and Oren Etzioni. 1998. Web document clustering:
A feasibility demonstration. In Research and Development
in Information Retrieval, pages 46?54.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and Guang R.
Gao. 2002. An adaptive meta-clustering approach: Com-
bining the information from different clustering results. csb,
00:276.
Ying Zhao and George Karypis. 2001. Criterion functions for
ducument clustering: Experiments and analysis. Technical
Report TR 01?40, Department of Computer Science, Uni-
versity of Minnesota.
420
Augmenting the kappa statistic to determine interannotator reliability
for multiply labeled data points
Andrew Rosenberg
Department of Computer Science
Columbia University
amaxwell@cs.columbia.edu
Ed Binkowski
Department of Mathematics & Statistics
Hunter College
ebinkowski@juno.com
Abstract
This paper describes a method for evaluating
interannotator reliability in an email corpus
annotated for type (e.g., question, answer, so-
cial chat) when annotators are allowed to as-
sign multiple labels to a message.  An
augmentation is proposed to Cohen?s kappa
statistic which permits all data to be included
in the reliability measure and which further
permits the identification of more or less re-
liably annotated data points.
1 Introduction
Reliable annotated data are necessary for a wide variety
of natural language processing tasks.  Machine learning
algorithms commonly employed to tackle language
problems from syntactic parsing to prosodic analysis
and information retrieval all require annotated data for
training and testing. The reliability of these computa-
tional solutions is intricately tied to the accuracy of the
annotated data used in their development. Human error
and subjectivity make deciding the accuracy of annota-
tions an intractable problem. While the objective cor-
rectness of human annotations cannot be determined
algorithmically, the degree to which the annotators
agree in their labeling of a corpus can be quickly and
simply statistically determined using Cohen?s (1960)
kappa measure.  Because human artifacts are less likely
to co-occur simultaneously in two annotators, the kappa
statistic is used to measure interannotator reliability.
This paper will describe an email classification and
summarization project which presented a problem for
interlabeler reliability computation since annotators
were allowed to label data with one or two labels
(Rambow, et al, 2004). The existing kappa statistic
computation does not obviously extend to accommodate
the presence of a secondary label.  The augmentation to
the algorithm presented in this paper allows for both a
more accurate assessment of interannotator reliability
and a unique insight into the data and how the annota-
tors have employed the optional second label. Section 2
will describe the categorization project. Section 3 will
present a description of the annotated corpus. Section 4
will describe why the kappa statistic for determining
interannotator agreement in its basic form cannot effec-
tively be applied to this corpus. Section 5 will present a
way to augment the algorithm computing kappa statistic
to provide greater insight into user annotations. Section
6 will analyze the results of applying this new algorithm
to the annotated corpus.
2 Project Description
This inquiry into interannotator reliability measure-
ments was spawned by problems encountered during a
project classifying and summarizing email messages.  In
this project email messages are classified into one of ten
classes.  This classification facilitates email thread re-
construction as well as summarization.  Distinct email
categories have distinct structural and linguistic ele-
ments and thus ought to be summarized differently. For
the casual email user, the luxuries of summarization and
automated classification for the dozen or so daily mes-
sages may be rather superfluous, but for those with hun-
dreds of important emails per day, automatic
summarization and categorization can provide an effi-
cient and convenient way to both scan new messages
(e.g., if the sender responds to a question, the category
will be ?answer?, while the summary will contain the
response) and retrieve old ones (e.g., ?Display all
scheduling emails received last week?).  While the pro-
ject intends to apply machine learning techniques to
both facets, this paper will be focusing on the
categorization component.
3 Corpus Description
The corpus used is a collection of 380 email messages
marked by two annotators with either one or two of the
following labels: question, answer, broadcast, at-
tachment transmission, planning-meeting schedul-
ing, planning scheduling, planning, action item,
technical discussion, and social chat.  If two labels are
used, one is designated primary and the other secondary.
These ten categories were selected in order to direct the
automatic summarization of email messages.
This corpus is a subset of a larger corpus of ap-
proximately 1000 messages exchanged between mem-
bers of the Columbia University chapter of the
Association for Computing Machinery (ACM) in 2001.
The annotation of the rest of corpus is in progress.
4 Standard Kappa Shortcomings
Commonly, the kappa statistic is used to measure inter-
annotator agreement.  It determines how strongly two
annotators agree by comparing the probability of the
two agreeing by chance with the observed agreement.  If
the observed agreement is significantly greater than that
expected by chance, then it is safe to say that the two
annotators agree in their judgments.  Mathematically,
)(1
)()(
Ep
EpAp
K
-
-= where K is the kappa value, p(A) is
the probability of the actual outcome and p(E) is  the
probability of the expected outcome as predicted by
chance.
When each data point in a corpus is assigned a single
label, calculating p(A) is straightforward: simply count
up the number of times the two annotators agree and
divide by the total number of annotations.  However, in
labeling this email corpus, labelers were allowed to se-
lect either a single label or two labels designating one as
primary and one as secondary.
The option of a secondary label increases the possi-
ble labeling combinations between two annotators five-
fold.  In the format ?{<A?s labels>, <B?s labels>}? the
possibilities are as follows: {a,a}, {a,b},  {ab,a},
{ab,b},  {ab,c}, {ab,ab}, {ab,ba}, {ab,ac}, {ab,bc},
{ab,cd}.  The algorithm initially used to calculate the
kappa statistic simply discarded the optional secondary
label.  This solution is unacceptable for two reasons. 1)
It makes the reliability metric inconsistent with the an-
notation instructions.  Why offer the option of a secon-
dary label, if it is to be categorically ignored? 2) It
discards useful information regarding partial agreement
by treating situations corresponding to {ab,ba},
{ab,bc} and {ab, b} as simple disagreements.
Despite this complication, the objective in comput-
ing p(A) remains the same, count the agreements and
divide by the number of annotations.  But how should
the partial agreement cases ({ab, a}, {ab, b}, {ab,ba},
{ab,ac}, and {ab,bc}) be counted?  For example, when
considering a message that clearly contained both a
question and an answer, one annotator had labeled the
message as primarily question and secondarily answer,
with another primarily answer and secondarily ques-
tion.  Should such an annotation be considered an
agreement, as the two concur on the content of the mes-
sage? Or disagreement, as they differ in their employ of
primary and secondary?  To what degree do two annota-
tors agree if one labels a message primarily a and
secondarily b and the other labels it simply a or simply
b?  What if there is agreement on the primary label and
discrepancy on the secondary? Or vice versa?  In the
traditional Boolean assignment, each combination
would have to be counted as either agreement or dis-
agreement.  Instead, in order to compute a useful value
of p(A),  we propose to assign a degree of agreement to
each.  This is similar in concept to Krippendorff?s
(1980) alpha measure for multiple observers.
5 Kappa Algorithm Augmentation
To augment the computation of the kappa statistic, we
consider annotations marked with primary and secon-
dary labels not as two distinct selections, but as one
divided selection.1  When an annotator selects a single
label for a message, that label-message pair is assigned
a score of 1.0.  When an annotator selects a primary and
secondary label, a weight p is assigned to the primary
label and (1-p) to the secondary label for the corre-
sponding label-message pair. Before computing the
kappa score for the corpus, a single value p where 0.5 ?
p ? 1.0 must be selected.  If p = 1.0 the secondary labels
are completely ignored, while if p = 0.5, secondary and
primary labels are given equal weight.  By examining
the resulting kappa score at different values of p, insight
into how the annotators are employing the optional sec-
ondary label can be gained. Moreover, single messages
can be trivially isolated in order to reveal how each data
point has been annotated with respect to primary and
secondary labels.  Landis and Koch (1977) present a
method for calculating a weighted kappa measure.  This
method is useful for single annotations where the cate-
gories have an obvious relationship to each other, but
does not extend to multiply labeled data points where
relationships between categories are unknown.
1 Before settling on this approach, we considered count-
ing each annotation equivalently whether primary or secon-
dary.  This made computation of p(A) and p(E) more
complex, and by ignoring the primary/secondary distinction
offered less insight into the use of the labels.
5.1 Compute p(A)
To compute p(A), the observed probability, two annota-
tion matrices are created, one for each annotator.  These
annotation matrices, Mannotator, have N rows and M col-
umns, where n is the number of messages and m is the
number of labels.  These annotation matrices are propa-
gated as follows.
1],[ =yxM A , if A marked only label y for mes-
sage x.
pyxM A =],[ , if A marked label y as the primary
label for message x.
pyxM A -= 1],[ , if A marked label y as the sec-
ondary label for message x.
0],[ =yxM A , otherwise.
Table 1 shows a sample set of annotations on 5 mes-
sages by annotator A. Table 2 shows the resulting MA
based on the annotation data in Table 1 where p=0.6.
Msg1 Msg2 Msg3 Msg4 Msg5
a,b b,a b c c,b
Table 1. Sample annotation data from labeler A
 a  b  c  d
Msg1 0.6 0.4 0 0
Msg2 0.4 0.6 0 0
Msg3 0 1 0 0
Msg4 0 0 1 0
Msg5 0 0.4 0.6 0
Total 1 2.4 1.6 0 5
Table 2. MA based on Table 1 data (p=0.6;N=5).
With the two annotation matrices, MA and MB, an
agreement matrix, Ag, is constructed where
],[*],[],[ yxMyxMyxAg BA= .  A total, ?, is set
to the sum of all cells of Ag.  Finally,
N
Ap
a=)( .
5.2 Compute p(E)
Instead of assuming an even distribution of labels, we
compute p(E), the expected probability, using the rela-
tive frequencies of each annotator?s labeling preference.
Using the above annotation matrices, relative frequency
vectors, Freqannotator, are generated.  Table 3 shows
FreqA based on MA from Table 2.
?
=
=
N
x
A
A N
yxM
yFreq
1
],[
][
a b c d
0.2 0.48 0.32 0
Table 3. FreqA from MA in Table 2 (p=0.6;N=5).
Using these two frequency vectors,
?
=
=
M
y
BA yFreqyFreqEp
1
][*][)( .
5.3 Calculate ??
The equation for the augmented kappa statistic remains
the same in the presence of this augmentation.
)(1
)()(
'
Ep
EpAp
K
-
-=
6 Results
This technique is not meant to inflate the kappa scores,
but rather to provide further insight into how the annota-
tors are using the two labels. Execution of this aug-
mented kappa algorithm on this corpus suggests that the
annotation guidelines need revision before the superset
corpus is completely annotated.  (Only 150 of 380 mes-
sages present a label for use in a machine learning ex-
periment with ??>0.6.) The exact nature of the
adjustments is yet undetermined. However, both a strict
specification of when the secondary label ought to be
used, and reconsideration of the ten available labels
would likely improve the annotation effort.
When we examine our labeled data, we find the
average kappa statistic across the three annotators did
not increase through examination of the secondary la-
bels.  If we ignore the secondary labels (p=1.0), the av-
erage ??=0.299.  When primary and secondary labels
are given equal weight (p=0.5), the average ??=0.281.
By examining the average kappa statistic for each
message individually at different p values, messages can
be quickly categorized into four classes: those that dem-
onstrate greatest agreement at p = 1.0; those with great-
est agreement at p = 0.5; those that yield a nearly
constant low kappa value and those that yield a nearly
constant high kappa value. These classes suggest certain
characteristics about the component messages, and can
be employed to improve the ongoing annotation proc-
ess. Class 1) Those messages that show a constant, high
kappa score are those that are consistently categorized
with a single label.  (92/380 messages.) Class 2) Those
messages with a constant, low kappa are those messages
that are least consistently annotated regardless of
whether a secondary label is used or not.  (183/380 mes-
sages.) Class 3) Messages that show greater agreement
at p = 1.0 than at p = 0.5 demonstrate greater inconsis-
tency when the annotators opt to use the secondary la-
bels but are in (greater) agreement regarding the
primary label. Whether the primary label is more gen-
eral or more specific depends on, hopefully, annotation
standards, but in the absence of rigorous instructions,
individual annotator preference.  (58/380 messages.)
Class 4) Messages that show greater agreement at p =
0.5 than at p = 1.0 are those messages where the pri-
mary and secondary labels are switched by some anno-
tators, the above {ab,ba} case.  From inspection, this
most often occurs when the two features are not in a
general/specific relationship (e.g., planning and ques-
tion being selected for a message that contains a ques-
tion about planning), but are rather concurrent features
(e.g., question and answer being labeled on a message
that obviously includes both a question and an answer).
(47/380 messages.) Each of the four categories of mes-
sages can be utilized to a distinct end towards improve-
ment of annotation instructions and/or annotation
standards.  Class 1 messages are clear examples of the
labels.  Class 2 messages are problematic.  These mes-
sages can be used to redirect the annotators, revise the
annotation manual or reconsider the annotation stan-
dards.  Class 3 messages are those in which annotators
use the optional secondary label, but not consistently.
These messages can be employed to reinstruct the anno-
tators as to the expected use of the secondary label.
Class 4 messages pose a real dilemma.  When these
messages in fact do contain two concurrent features,
they are not going to be good examples for machine
learning experiments.  While representative of both
categories, they will (most likely) at feature analysis
(the critical component of machine learning algorithms)
be poor exemplars of each. While the fate of Class 4
messages is uncertain2, identification of these awkward
examples is an important first step in handling their
automatic classification.
7 Conclusion
Calculating a useful metric for interannotator reliability
when each data point is marked with optionally one or
two labels proved to be a complicated task.  Multiple
labels raise the possibility of partial agreement between
two annotators.  In order to compute the observed prob-
ability (p(A)) component of the kappa statistic a con-
stant weight, p, between 0.5 and 1.0 is selected. Each
singleton annotation is then assigned a weight of 1,
while the primary label of a doubleton annotation is
assigned a weight of p, the secondary 1-p.  These
weights are then used to determine the partial agreement
in the calculation of p(A).  This augmentation to the
algorithm for computing kappa is not meant to inflate
the reliability metric, but rather to allow for a more
thorough view of annotated data.  By examining how
2 One potential solution would be to create a new anno-
tation category for each commonly occurring pair.  While
each Class 4 message would remain a poor exemplar of each
component category, it would be a good exemplar of this
new ?mixed? type.
the annotated components of a corpus demonstrate
agreement at varying levels of p, insight is gained into
how the annotators are viewing these data and how they
employ the optional secondary label.
8 Future Work
The problem that spawned this study has led to further
discussions about how to get the most information out
of apparently unreliably labeled data.  The above proc-
ess shows how it is possible to classify messages into a
few categories by their reliability at different levels of p.
However, even when interlabeler reliability is relatively
low, annotated data can be leveraged to improve the
confidence in assigning labels to messages.  Annotators
can be ranked by ?how well they agree with the group?
using kappa. Messages (or other labeled data) can be
ranked by ?how well the group agrees on its label? us-
ing variance or ?p*ln(p).  Annotator rankings can be
used to weight ?better? annotators greater than ?worse?
annotators. Similarly, message rankings can be used to
weight ?better? messages greater than ?worse? mes-
sages.  The weighted annotator data can be used to re-
compute the message weights.  These new message
weights can then be used to recompute annotator
weights. Repeating this alternation until the weights
show minimal change will minimize the contributions
of unreliable annotators and poorly annotated messages
to the assignment of labels to messages, thereby increas-
ing confidence in the results.  An implementation of this
?sharpening? algorithm is currently under development.
Acknowledgments
Thanks to Becky Passonneau for her insightful com-
ments on an intermediate draft. This work would not
have been possible without the support and advice of
Julia Hirschberg, Owen Rambow and Lokesh Shrestha.
This research was supported by a grant from NSF/KDD
#IIS-98-17434.
References
J. A. Cohen. 1960. Educational and Psychological
Measurement, 20(1):37-46.
Klaus Krippendorff. 1980. Content Analysis, an Intro-
duction to Its Methodology. Thousand Oaks, CA.
J. R. Landis and G.G. Koch. 1977. Biometrics,
33(1):159-174
Owen Rambow and Lokesh Shrestha and John Chen
and Charles Lewis. 2004. Summarizing Email
Threads. Under submission.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 125?128,
New York, June 2006. c?2006 Association for Computational Linguistics
Story Segmentation of Brodcast News in English, Mandarin and Arabic
Andrew Rosenberg
Computer Science Department
Columbia University
New York City, N.Y. 10027
amaxwell@cs.columbia.edu
Julia Hirschberg
Computer Science Department
Columbia University
New York City, N.Y. 10027
julia@cs.columbia.edu
Abstract
In this paper, we present results from a
Broadcast News story segmentation sys-
tem developed for the SRI NIGHTIN-
GALE system operating on English, Ara-
bic and Mandarin news shows to provide
input to subsequent question-answering
processes. Using a rule-induction algo-
rithm with automatically extracted acous-
tic and lexical features, we report success
rates that are competitive with state-of-
the-art systems on each input language.
We further demonstrate that features use-
ful for English and Mandarin are not dis-
criminative for Arabic.
1 Introduction
Broadcast News (BN) shows typically include
multiple unrelated stories, interspersed with anchor
presentations of headlines and commercials. Tran-
sitions between each story are frequently marked
by changes in speaking style, speaker participation,
and lexical choice. Despite receiving a consider-
able amount of attention through the Spoken Doc-
ument Retrieval (SDR), Topic Detection and Track-
ing (TDT), and Text Retrieval Conference: Video
(TRECVID) research programs, automatic detec-
tion of story boundaries remains an elusive prob-
lem. State-of-the-art story segmentation error rates
on English and Mandarin BN remain fairly high and
Arabic is largely unstudied. The NIGHTINGALE
system searches a diverse news corpus to return an-
swers to user queries. For audio sources, the iden-
tification of story boundaries is crucial, to segment
material to be searched and to provide interpretable
results to the user.
2 Related work
Previous approaches to story segmentation have
largely focused lexical features, such as word sim-
ilarily (Kozima, 1993), cue phrases (Passonneau
and Litman, 1997), cosine similarity of lexical win-
dows (Hearst, 1997; Galley et al, 2003), and adap-
tive language modeling (Beeferman et al, 1999).
Segmentation of stories in BN have included some
acoustic features (Shriberg et al, 2000; Tu?r et al,
2001). Work on non-English BN, generally use
this combination of lexical and acoustic measures,
such as (Wayne, 2000; Levow, 2004) on Mandarin.
And (Palmer et al, 2004) report results from feature
selection experiments that include Arabic sources,
though they do not report on accuracy. TRECVID
has also identified visual cues to story segmentation
of video BN (cf. (Hsu et al, 2004; Hsieh et al, 2003;
Chaisorn et al, 2003; Maybury, 1998)).
3 The NIGHTINGALE Corpus
The training data used for NIGHTINGALE in-
cludes the TDT-4 and TDT5 corpora (Strassel and
Glenn, 2003; Strassel et al, 2004). TDT-4 in-
cludes newswire text and broadcast news audio
in English, Arabic and Mandarin; TDT-5 contains
only text data, and is therefore not used by our
system. The TDT-4 audio corpus includes 312.5
hours of English Broadcast News from 450 shows,
88.5 hours of Arabic news from 109 shows, and
134 hours of Mandarin broadcasts from 205 shows.
This material was drawn from six English news
shows ? ABC ?World News Tonight?, CNN ?Head-
line News?, NBC ?Nightly News?, Public Radio
International ?The World?, MS-NBC ?News with
Brian Williams?, and Voice of America, English
three Mandarin newscasts ? China National Ra-
dio, China Television System, and Voice of Amer-
ica, Mandarin Chinese ? and two Arabic newscasts
? Nile TV and Voice of America, Modern Standard
Arabic. All of these shows aired between Oct. 1,
2000 and Jan. 31, 2001.
4 Our Features and Approach
Our story segmentation system procedure is es-
sentially one of binary classification, trained on a
variety of acoustic and lexical cues to the presence
or absence of story boundaries in BN. Our classi-
fier was trained using the JRip machine learning al-
125
gorithm, a Java implementation of the RIPPER al-
gorithm of (Cohen, 1995).1 All of the cues we
use are automatically extracted. We use as input
to our classifier three types of automatic annotation
produced by other components of the NIGHTIN-
GALE system, speech recognition (ASR) transcrip-
tion, speaker diarization, sentence segmentation.
Currently, we assume that story boundaries occur
only at these hypothesized sentence boundaries. For
our English corpus, this assumption is true for only
47% of story boundaries; the average reference story
boundary is 9.88 words from an automatically rec-
ognized sentence boundary2 . This errorful input im-
mediately limits our overall performance.
For each such hypothesized sentence boundary,
we extract a set of features based on the previous
and following hypothesized sentences. The classi-
fier then outputs a prediction of whether or not this
sentence boundary coincides with a story boundary.
The features we use for story boundary prediction
are divided into three types: lexical, acoustic and
speaker-dependent.
The value of even errorful lexical information in
identifying story boundaries has been confirmed for
many previous story segmentation systems (Beefer-
man et al, 1999; Stokes, 2003)). We include some
previously-tested types of lexical features in our own
system, as well as identifying our own ?cue-word?
features from our training corpus. Our lexical fea-
tures are extracted from ASR transcripts produced
by the NIGHTINGALE system. They include lexi-
cal similarity scores calculated from the TextTiling
algorithm.(Hearst, 1997), which determines the lex-
ical similarity of blocks of text by analyzing the co-
sine similarity of a sequence of sentences; this al-
gorithm tests the likelihood of a topic boundary be-
tween blocks, preferring locations between blocks
which have minimal lexical similarity. For En-
glish, we stem the input before calculating these fea-
tures, using an implementation of the Porter stem-
mer (Porter, 1980); we have not yet attempted to
identify root forms for Mandarin or Arabic. We also
calculate scores from (Galley et al, 2003)?s LCseg
1JRip is implemented in the Weka (Witten et al, 1999) ma-
chine learning environment.
2For Mandarin and Arabic respectively, true for 69% and
62% with the average distance between sentence and story
boundary of 1.97 and 2.91 words.
method, a TextTiling-like approach which weights
the cosine-similarity of a text window by an addi-
tional measure of its component LEXICAL CHAINS,
repetitions of stemmed content words. We also iden-
tify ?cue-words? from our training data that we find
to be significantly more likely (determined by ?2) to
occur at story boundaries within a window preceed-
ing or following a story boundary. We include as
features the number of such words observed within
3, 5, 7 and 10 word windows before and after the
candidate sentence boundary. For English, we in-
clude the number of pronouns contained in the sen-
tence, on the assumption that speakers would use
more pronouns at the end of stories than at the be-
ginning. We have not yet obtained reliable part-of-
speech tagging for Arabic or Mandarin. Finally, for
all three languages, we include features that repre-
sent the sentence length in words, and the relative
sentence position in the broadcast.
Acoustic/prosodic information has been shown to
be indicative of topic boundaries in both sponta-
neous dialogs and more structured speech, such as,
broadcast news (cf. (Hirschberg and Nakatani, 1998;
Shriberg et al, 2000; Levow, 2004)). The acous-
tic features we extract include, for the current sen-
tence, the minimum, maximum, mean, and standard
deviation of F0 and intensity, and the median and
mean absolute slope of F0 calculated over the en-
tire sentence. Additionally, we compute the first-
order difference from the previous sentence of each
of these. As a approximation of each sentence?s
speaking rate, we include the ratio of voiced 10ms
frames to the total number of frames in the sentence.
These acoustic values were extracted from the audio
input using Praat speech analysis software(Boersma,
2001). Also, using the phone alignment information
derived from the ASR process, we calculate speak-
ing rate in terms of the number of vowels per second
as an additional feature. Under the hypothesis that
topic-ending sentences may exhibit some additional
phrase-final lenghthening, we compare the length of
the sentence-final vowel and of the sentence-final
rhyme to average durations for that vowel and rhyme
for the speaker, where speaker identify is available
from the NIGHTINGALE diarization component;
otherwise we use unnormalized values.
We also use speaker identification information
from the diarization component to extract some fea-
126
tures indicative of a speaker?s participation in the
broadcast as a whole. We hypothesize that partici-
pants in a broadcast may have different roles, such
as an anchor providing transitions between stories
and reporters beginning new stories (Barzilay et al,
2000) and thus that speaker identity may serve as
a story boundary indicator. To capture such infor-
mation, we include binary features answering the
questions: ?Is the speaker preceeding this boundary
the first speaker in the show??, ?Is this the first time
the speaker has spoken in this broadcast??, ?The last
time??, and ?Does a speaker boundary occur at this
sentence boundary??. Also, we include the percent-
age of sentences in the broadcast spoken by the cur-
rent speaker.
We assumed in the development of this system
that the source of the broadcast is known, specif-
ically the source language and the show identity
(e. g. ABC ?World News Tonight?, CNN ?Head-
line News?). Given this information, we constructed
different classifiers for each show. This type of
source-specific modeling was shown to improve per-
formance by Tu?r (2001).
5 Results and Discussion
We report the results of our system on En-
glish, Mandarin and Arabic in Table 5. All results
use show-specific modeling, which consistently im-
proved our results across all metrics, reducing er-
rors by between 10% and 30%. In these tables, we
report the F-measure of identifying the precise lo-
cation of a story boundary as well as three metrics
designed specifically for this type of segmentation
task: the pk metric (Beeferman et al, 1999), Win-
dowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). All three are derived
from the pk metric (Beeferman et al, 1999), and for
all, lower values imply better performance. For each
of these three metrics we let k = 5, as prescribed in
(Beeferman et al, 1999).
In every system, the best peforming results are
achieved by including all features from the lexical,
acoustic and speaker-dependent feature sets. Across
all languages, our precision?and false alarm rates?
are better than recall?and miss rates. We believe
that inserting erroneous story boundaries will lead
to more serious downstream errors in anaphora res-
olution and summarization than a boundary omis-
sion will. Therefore, high precision is more impor-
tant than high recall for a helpful story segmentation
system. In the English and Mandarin systems, the
lexical and acoustic feature sets perform similarly,
and combine to yield improved results. However,
on the Arabic data, the acoustic feature set performs
quite poorly, suggesting that the use of vocal cues to
topic transitions may be fundamentally different in
Arabic. Moreover, these differences are not simply
differences of degree or direction. Rather, the acous-
tic indicators of topic shifts in English and Man-
darin are, simply, not discriminative when applied
to Arabic. This difference may be due to the style of
Arabic newscasts or to the language itself. Across
configurations, we find that the inclusion of features
derived from automatic speaker identification (fea-
ture set S), errorful as it is, significantly improves
performance. This improvement is particularly pro-
nounced on the Mandarin material; in China News
Radio broadcasts, story boundaries are very strongly
correlated with speaker transitions.
It is difficult to determine how well our system
performs against state-of-the-art story segmentation.
There are no comparable results for the TDT-4 cor-
pus. On the English TDT-2 corpus, (Shriberg et al,
2000) report a Cseg score of 0.1438. While our scoreof .0670 is half that, we hesitate to conclude that
our system is significantly better than this system;
since the (Shriberg et al, 2000) results are based on a
word-level segmentation, the discrepancy may be in-
fluenced by the disparate datasets as well as the per-
formance of the two systems. On CNN and Reuters
stories from the TDT-1 corpus, (Stokes, 2003) re-
port a Pk score of 0.25 and a WD score of 0.253.
Our Pk score is better than this on TDT-4, while
our WD score is worse. (Chaisorn et al, 2003) re-
port an F-measure of 0.532 using only audio-based
features on the TRECVID 2003 corpus , which is
higher than our system, however, this allows for
?correct? boundaries to fall within 5 seconds of ref-
erence boundaries. (Franz et al, 2000) present a sys-
tem which achieves Cseg scores of 0.067 and Man-darin BN and 0.081 on English audio in TDT-3. This
suggests that their system may be better than ours
on Mandarin, and worse on English, although we
trained and tested on different corpora. Finally, we
are unaware of any reported story segmentation re-
sults on Arabic BN.
127
Table 1: TDT-4 segmentation results. (L=lexical feature set, A=acoustic, S=speaker-dependent)
English Mandarin Arabic
F1(p,r) Pk WD Cseg F1(p,r) Pk WD Cseg F1(p,r) Pk WD CsegL+A+S .421(.67,.31) .194 .318 .0670 .592(.73,.50) .179 .245 .0679 .300(.65,.19) .264 .353 .0850
A+S .346(.65,.24) .220 .349 .0721 .586(.72,.49) .178 .252 .0680 .0487(.81,.03) .333 .426 .0999
L+S .342(.66,.23) .231 .362 .074 .575(.72,.48) .200 .278 .0742 .285(.68,.18) .286 .372 .0884
L+A .319(.66,.21) .240 .376 .0787 .294(.72,.18) .277 .354 .0886 .284(.64,.18) .257 .344 .0851
L .257(.68,.16) .261 .399 .0840 .226(.74,.13) .309 .391 .0979 .286(.68,.18) .283 .349 .0849
A .194(.63,.11) .271 .412 .0850 .252(.72,.18) .291 .377 .0904 .0526(.81,.03) .332 .422 .0996
6 Conclusion
In this paper we have presented results of our
story boundary detection procedures on English,
Mandarin, and Arabic Broadcast News from the
TDT-4 corpus. All features are obtained automati-
cally, except for the identity of the news show and
the source language, information which is, however,
available from the data itself, and could be automat-
ically obtained. Our performance on TDT-4 BN ap-
pears to be better than previous work on earlier cor-
pora of BN for English, and slightly worse than pre-
vious efforts on Mandarin, again for a different cor-
pus. We believe our Arabic results to be the first
reported evaluation for BN in that language. One
important observation from our study is that acous-
tic/prosodic features that correlate with story bound-
aries in English and in Mandarin, do not correlate
with Arabic boundaries. Our further research will
adress the study of vocal cues to segmentation in
Arabic BN.
Acknowledgments
This research was partially supported by the De-
fese Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023.
References
R. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker. 2000.
The rules behind roles: Identifying speaker role in radio
broadcasts. In AAAI/IAAI, 679?684.
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statistical mod-
els for text segmentation. Machine Learning, 31:177?210.
P. Boersma. 2001. Praat, a system for doing phonetics by com-
puter. Glot International, 5(9-10):341?345.
L. Chaisorn, T. Chua, C. Koh, Y. Zhao, H. Xu, H. Feng, and
Q. Tian. 2003. A two-level multi-modeal approach for story
segmentation of large news video corpus. In TRECVID.
W. Cohen. 1995. Fast effective rule induction. In Machine
Learning: Proc. of the Twelfth International Conference,
115?123.
G. Doddington. 1998. The topic detection and tracking phase
2 (tdt2) evaluation plan. In Proccedings DARPA Broadcast
News Transcription and Understanding Workshop, 223?229.
M. Franz, J. S. McCarley, T. Ward, and W. J. Zhu. 2000. Seg-
mentation and detection at ibm: Hybrid statstical models and
two-tiered clustering. In Proc. of TDT-3 Workshop.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing. 2003.
Discourse segmentation of multi-party conversation. In 41st
Annual Meeting of ACL, 562?569.M. A. Hearst. 1997. Texttiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguistics,
23(1):33?64.J. Hirschberg and C. Nakatani. 1998. Acoustic indicators of
topic segmentation. In Proc. of ICSLP, 1255?1258.J. H. Hsieh, C. H. Wu, and K. A. Fung. 2003. Two-stage story
segmentation and detection on broadcast news using genetic
algorithm. In Proc. of the 2003 ISCA Workshop on Multilin-
gual Spoken Document Retrieval (MSDR2003), 55?60.W. Hsu, L. Kennedy, C. W. Huang, S. F. Chang, C. Y. Lin, and
G. Iyengar. 2004. News video story segmentation using fu-
sion of multi-level multi-modal features in trecvid 2003. In
ICASSP.H. Kozima. 1993. Text segmentation based on similarity be-
tween words. In 31st Annual Meeting of the ACL, 286?288.G. A. Levow. 2004. Assessing prosodic and text features for
segmentation of mandarin broadcast news. In HLT-NAACL.M. T. Maybury. 1998. Discourse cues for broadcast news seg-
mentation. In COLING-ACL, 819?822.D. D. Palmer, M. Reichman, and E. Yaich. 2004. Feature selec-
tion for trainable multilingual broadcast news segmentation.
In HLT/NAACL.R. J. Passonneau and D. J. Litman. 1997. Discourse segmenta-
tion by human and automated means. Computational Liun-
guistics, 23(1):103?109.L. Pevzner and M. Hearst. 2002. A critique and improvement
of an evaluation metric for text segmentation. Computational
Linguistics, 28(1):19?36.M. Porter. 1980. An algorithm for suffix stripping. Program,
14(3):130?137.E. Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r. 2000.
Prosody based automatic segmentation of speech into sen-
tences and topics. Speech Comm., 32(1-2):127?154.N. Stokes. 2003. Spoken and written news story segmentation
using lexical chains. In Proc. of the Student Workshop at
HLT-NAACL2003, 49?53.S. Strassel and M. Glenn. 2003. Creating
the annotated tdt-4 y2003 evaluation corpus.
http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt.S. Strassel, M. Glenn, and J. Kong. 2004. Creating
the tdt5 corpus and 2004 evalutation topics at ldc.
http://www.nist.gov/speech/tests/tdt/tdt2004/papers/LDC-
TDT5.ppt.G. Tu?r, D. Hakkani-Tu?r, A. Stolcke, and E. Shriberg. 2001. In-
tegrating prosodic and lexical cues for automatic topic seg-
mentation. Computational Linguistics, 27:31?57.C. L. Wayne. 2000. Multilingual topic detection and tracking:
Successful research enabled by corpora and evaluation. In
LREC, 1487?1494.I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine learn-
ing tools and techniques with java implementation. In
ICONIP/ANZIIS/ANNES, 192?196.
128
Proceedings of NAACL HLT 2009: Short Papers, pages 81?84,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Detecting Pitch Accents at the Word, Syllable and Vowel Level
Andrew Rosenberg
Columbia University
Department of Computer Science
amaxwell@cs.columbia.edu
Julia Hirschberg
Columbia University
Department of Computer Science
julia@cs.columbia.edu
Abstract
The automatic identification of prosodic
events such as pitch accent in English has long
been a topic of interest to speech researchers,
with applications to a variety of spoken lan-
guage processing tasks. However, much re-
mains to be understood about the best meth-
ods for obtaining high accuracy detection. We
describe experiments examining the optimal
domain for accent analysis. Specifically, we
compare pitch accent identification at the syl-
lable, vowel or word level as domains for anal-
ysis of acoustic indicators of accent. Our re-
sults indicate that a word-based approach is
superior to syllable- or vowel-based detection,
achieving an accuracy of 84.2%.
1 Introduction
Prosody in a language like Standard American En-
glish can be used by speakers to convey semantic,
pragmatic and paralinguistic information. Words are
made intonationall prominent, or accented to convey
information such as contrast, focus, topic, and in-
formation status. The communicative implications
of accenting influence the interpretation of a word
or phrase. However, the acoustic excursions associ-
ated with accent are typically aligned with the lex-
ically stressed syllable of the accented word. This
disparity between the domains of acoustic proper-
ties and communicative impact has led to different
approaches to pitch accent detection, and to the use
of different domains of analysis.
In this paper, we compare automatic pitch accent
detection at the vowel, syllable, and word level to
determine which approach is optimal. While lex-
ical and syntactic information has been shown to
contribute to the detection of pitch accent, we only
explore acoustic features. This decision allows us
to closely examine the indicators of accent that are
present in the speech signal in isolation from lin-
guistic effects that may indicate that a word or syl-
lable may be accented. The choice of domain for
automatic pitch accent prediction it also related to
how that prediction is to be used and impacts how
it can be evaluated in comparison with other re-
search efforts. While some downstream spoken lan-
guage processing tasks benefit by knowing which
syllable in a word is accented, such as clarifica-
tion of communication misunderstandings, such as
?I said unlock the door ? not lock it!?, most appli-
cations care only about which word is intonation-
ally prominent. For the identification of contrast,
given/new status, or focus, only word-level informa-
tion is required. While the performance of nucleus-
or syllable-based predictions can be translated to
word predictions, such a translation is rarely per-
formed, making it difficult to compare performance
and thus determine which approach is best.
In this paper, we describe experiments in pitch ac-
cent detection comparing the use of vowel nuclei,
syllables and words as units of analysis. In Section
2, we discuss related work. We describe the ma-
terials in Section 3, the experiments themselves in
Section 4 and conclude in Section 5.
2 Related Work
Acoustic-based approaches to pitch accent detection
have explored prediction at the word, syllable, and
81
vowel level, but have rarely compared prediction
accuracies across these different domains. An ex-
ception is the work of Ross and Ostendorf (1996),
who detect accent on the Boston University Radio
News Corpus (BURNC) at both the syllable and
word level. Using CART predictions as input to an
HMM, they detect pitch accents on syllables spoken
by a single speaker from BURNC with 87.7% accu-
racy, corresponding to 82.5% word-based accuracy,
using both lexical and acoustic features. In compar-
ing the discriminative usefulness of syllables vs. syl-
lable nuclei for accent detection, Tamburini (2003)
finds syllable nuclei (vowel) duration to be as useful
to full syllables. Rosenberg and Hirschberg (2007)
used an energy-based ensemble technique to detect
pitch accents with 84.1% accuracy on the read por-
tion of the Boston Directions Corpus, without us-
ing lexical information. Sridhar et al (2008) ob-
tain 86.0% word-based accuracy using maximum
entropy models from acoustic and syntactic infor-
mation on the BURNC. Syllable-based detection
by Ananthakrishnan and Narayanan (2008) com-
bines acoustic, lexical and syntactic FSM models
to achieve a detection rate of 86.75%. Similar
suprasegmental features have also been explored in
work at SRI/ICSI which employs a hidden event
model to model intonational information for a va-
riety of tasks including punctuation and disfluency
detection (Baron et al, 2002). However, while
progress has been made in accent detection perfor-
mance in the past 15 years, with both word and syl-
lable accuracy at about 86%, these accuracies have
been achieved with different methods and some have
included lexico-syntactic as well as acoustic fea-
tures. It is still not clear which domain of acoustic
analysis provides the most accurate cues for accent
prediction. To address this issue, our work compares
accent detection at the syllable nucleus, full syllable,
and word levels, using a common modeling tech-
nique and a common corpus, to focus on the ques-
tion of which domain of acoustic analysis is most
useful for pitch accent prediction.
3 Boston University Radio News Corpus
Our experiments use 157.9 minutes (29,578 words)
from six speakers in the BURNC (Ostendorf et al,
1995) recordings of professionally read radio news.
This corpus has been prosodically annotated with
full ToBI labeling (Silverman et al, 1992), includ-
ing the presence and type of accents; these are an-
notated at the syllable level and 54.7% (16,178) of
words are accented. Time-aligned phone boundaries
generated by forced alignment are used to identify
vowel regions for analysis. There are 48,359 vow-
els in the corpus and 34.8 of these are accented. To
generate time-aligned syllable boundaries, we align
the forced-aligned phones with a syllabified lexicon
included with the corpus.
The use of BURNC for comparative accent pre-
diction in our three domains is not straightforward,
due to anomalies in the corpus. First, the lexicon
and forced-alignment output in BURNC use distinct
phonetic inventories; to align these, we have em-
ployed a minimum edit distance procedure where
aligning any two vowels incurs zero cost. This guar-
antees that, at a minimum the vowels will be aligned
correctly. Also, the number of syllables per word
in the lexicon does not always match the number
of vowels in the forced alignment. This leads to
114 syllables containing two forced-aligned vowels,
and 8 containing none. Instead of performing post
hoc correction of the syllabification results, we in-
clude all of the automatically identified syllables in
the data set. This syllabification approach generates
48,253 syllables, 16,781 (34.8%) bearing accent.
4 Pitch Accent Detection Experiments
We train logistic regression models to detect the
presence of pitch accent using acoustic features
drawn from each word, syllable and vowel, using
Weka (Witten et al, 1999). The features we use in-
cluded pitch (f0), energy and duration, which have
been shown to correlate with pitch accent in En-
glish. To model these, we calculate pitch and en-
ergy contours for each token using Praat (Boersma,
2001). Duration information is derived using the
vowel, syllable or word segmentation described in
Section 3. The feature vectors we construct include
features derived from both raw and speaker z-score
normalized1 pitch and energy contours. The feature
vector used in all three analysis scenarios is com-
prised of minimum, maximum, mean, standard de-
1Z-score normalization:xnorm = x??? , where x is a valueto normalize, ? and ? are mean and standard deviation. These
are estimated from all pitch or intensity values for a speaker.
82
viation and the z-score of the maximum of these raw
and normalized acoustic contours. The duration of
the region in seconds is also included.
The results of ten-fold cross validation classifica-
tion experiments are shown in Table 1. Note that,
when running ten-fold cross validation on syllables
and vowels, we divide the folds by words, so that
each syllable within a word is a member of the
same fold. To allow for direct comparison of the
three approaches, we generate word-based results
from vowel- and syllable-based experiments. If any
syllable or vowel in a word is hypothesized as ac-
cented, the containing word is predicted to be ac-
cented. Vowel/syllable accuracies should be higher
Region Accuracy (%) F-Measure
Vowel 68.5? 0.319 0.651? 0.00329
Syllable 75.6? 0.125 0.756? 0.00188
Word 82.9? 0.168 0.845? 0.00162
Table 1: Word-level accuracy and F-Measure
than word-based accuracies since the baseline is sig-
nificantly higher. However, we find that the F-
measure for detecting accent is consistently higher
for word-based results. A prediction of accented
on any component syllable is sufficient to generate
a correct word prediction.
Our results suggest, first of all, that there is dis-
criminative information beyond the syllable nucleus.
Syllable-based classification is significantly better
than vowel-based classification, whether we com-
pare accuracy or F-measure. It is possible that the
narrow region of analysis offered by syllable and
vowel-based analysis makes the aggregated features
more susceptible to the effects of noise. Moreover,
errors in the forced-alignment phone boundaries
and syllabification may negatively impact the per-
formance of vowel- and syllable-based approaches.
Until automatic phone alignment improves, word-
based prediction appears to be more reliable. An
automatic, acoustic syllable-nucleus detection ap-
proach may be able generate more discriminative re-
gions of analysis for pitch accent detection than the
forced-alignment and lexicon alignment technique
used here. This remains an area for future study.
However, if we accept that the feature represen-
tations accurately model the acoustic information
contained in the regions of analysis and that the
BURNC annotation is accurate, the most likely ex-
planation for the superiority of word-based predic-
tion over syllable- or vowel-based strategiesis is that
the acoustic excursions correlated with accent occur
outside a word?s lexically stressed syllable. In par-
ticular, complex pitch accents in English are gener-
ally realized on multiple syllables. To examine this
possibility, we looked at the distribution of misses
from the three classification scenarios. The distribu-
tion of pitch accent types of missed detections using
evaluation of the three scenarios is shown in Table
2. In the ToBI framework, the complex pitch ac-
cents include L+H*, L*+H, H+!H* and their down-
stepped variants. As we suspected, larger units of
analysis lead to improved performance on complex
tones; ?2 analysis of the difference between the er-
ror distributions yields a ?2 of 42.108, p< 0.0001.
Since accenting is the perception of a word as
more prominent than surrounding words, features
that incorporate local contextual acoustic informa-
tion should improve detection accuracy at all lev-
els. To represent surrounding acoustic context in
feature vectors, we calculate the z-score of the max-
imum and mean pitch and energy over six regions.
Three of these are ?short range? regions: one pre-
vious region, one following region, and both the
previous and following region. The other three are
?long range? regions. For words, these regions
are defined as two previous words, two following
words, and both two previous and two following
words. To give syllable- and vowel-based classifi-
cation scenarios access to a comparable amount of
acoustic context, the ?long range? regions covered
ranges of three syllables or vowels. There are ap-
proximately 1.63 syllables/vowels per word in the
BURNC corpus; thus, on balance, a window of two
words is equivalent to one of three syllables. Du-
ration is also normalized relative to the duration of
regions within the contextual regions. Accuracy and
f-measure results from ten-fold cross validation ex-
periments are shown in Table 3. We find dramatic
Analysis Region Accuracy (%) F-Measure
Vowel 77.4? 0.264 0.774? 0.00370
Syllable 81.9? 0.197 0.829? 0.00195
Word 84.2? 0.247 0.858? 0.00276
Table 3: Word-level accuracy and F-Measure with Con-
textual Features
increases in the performance of vowel- and syllable-
83
Region H* L* Complex Total Misses
Vowel .6825 (3732) .0686 (375) .2489 (1361) 1.0 (5468)
Syllable .7033 (2422) .0851 (293) .2117 (729) 1.0 (3444)
Word .7422 (2002) .0610 (165) .1986 (537) 1.0 (2704)
Table 2: Distribution of missed detections organized by H*, L* and complex pitch accents.
based performance when we include contextual fea-
tures. Vowel-based classification shows nearly 10%
absolute increase accuracy when translated to the
word level. The improvements in word-based clas-
sification, however, are less dramatic. It may be
that word-based analysis already incorporates much
the contextual information that is helpful for detect-
ing pitch accents. The feature representations in
each of these three experiments include a compara-
ble amount of acoustic context. This suggests that
the superiority of word-based detection is not sim-
ply due to the access to more contextual informa-
tion, but rather that there is discriminative informa-
tion outside the accent-bearing syllable.
5 Conclusion and Future Work
In this paper, we describe experiments comparing
the detection of pitch accents on three acoustic do-
mains ? words, syllables and vowels ? using acous-
tic features alone. To permit direct comparison be-
tween accent prediction in these three domains of
analysis, we generate word-, syllable-, and vowel-
based results directly, and then transfer syllable- and
nucleus-based predictions to word predictions.
Our experiments show that word-based accent
detection significantly outperforms syllable- and
vowel-based approaches. Extracting features that
incorporate acoustic information from surrounding
context improves performance in all three domains.
We find that there is, in fact, acoustic information
discriminative to pitch accent that is found within
accented words, outside the accent-bearing sylla-
ble. We achieve 84.2% word-based accuracy ?
significantly below the 86.0% reported by Sridhar
et al (2008) using syntactic and acoustic compo-
nents. However, our experiments use only acoustic
features, since we are concerned with comparing do-
mains of acoustic analysis within the larger task of
accent identification. Our 84.2% accuracy is signifi-
cantly higher than the 80.09% accuracy obtained by
the 10ms frame-based acoustic modeling described
in (Sridhar et al, 2008). Our aggregations of pitch
and energy contours over a region of analysis appear
to be more helpful than short frame modeling.
In future work, we will explore a number of tech-
niques to transfer word based predictions to sylla-
bles. This will allow us to compare word-based de-
tection to published syllable-based results. Prelimi-
nary results suggest that word-based detection is su-
perior regardless of the domain of evaluation.
References
S. Ananthakrishnan and S. Narayanan. 2008. Auto-
matic prosodic event detection using acoustic, lexical
and syntactic evidence. IEEE Transactions on Audio,
Speech & Language Processing, 16(1):216?228.
D. Baron, E. Shriberg, and A. Stolcke. 2002. Auto-
matic punctuation and disfluency detection in multi-
party meetings using prosodic and lexical cues. In IC-
SLP.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9-10):341?345.
M. Ostendorf, P. Price, and S. Shattuck-Hufnagel. 1995.
The boston university radio news corpus. Technical
Report ECS-95-001, Boston University, March.
A. Rosenberg and J. Hirschberg. 2007. Detecting pitch
accent using pitch-corrected energy-based predictors.
In Interspeech.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract prosodic labels for speech synthesis. Computer
Speech & Language, 10(3):155?185.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert, and
J. Hirschberg. 1992. Tobi: A standard for labeling en-
glish prosody. In Proc. of the 1992 International Con-
ference on Spoken Language Processing, volume 2,
pages 12?16.
V. R. Sridhar, S. Bangalore, and S. Narayanan. 2008.
Exploiting acoustic and syntactic features for prosody
labeling in a maximum entropy framework. IEEE
Transactions on Audio, Speech & Language Process-
ing, 16(4):797?811.
F. Tamburini. 2003. Prosodic prominence detection in
speech. In ISSPA2003, pages 385?388.
I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine
learning tools and techniques with java implementa-
tion. In ICONIP/ANZIIS/ANNES International Work-
shop, pages 192?196.
84
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 721?724,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Classification of Prosodic Events using Quantized Contour Modeling
Andrew Rosenberg
Department of Computer Science
Queens College CUNY, New York, USA
andrew@cs.qc.cuny.edu
Abstract
We present Quantized Contour Modeling (QCM), a
Bayesian approach to the classification of acoustic
contours. We evaluate the performance of this tech-
nique in the classification of prosodic events. We
find that, on BURNC, this technique can success-
fully classify pitch accents with 63.99% accuracy
(.4481 CER), and phrase ending tones with 72.91%
accuracy.
1 Introduction
Intonation can significantly vary the intended meaning
of a spoken utterance. In Standard American English,
contrast is frequently indicated with an accent that has
a steeper pitch rise ? ?I went to the store (not the li-
brary)? ? than an accent that is used to indicate focus or
introduce new information ? ?I went to the store (before
going home)? . At phrase boundaries, rising pitch can
indicate uncertainty or that the speaker is asking a ques-
tion ? ?John likes Mary?? vs. ?John likes Mary?. Auto-
matically detecting prosodic events and classifying their
type allows natural language understanding systems ac-
cess to intonational information that would unavailable if
processing transcribed text alone.
The ToBI standard of intonation (Silverman et al,
1992) describes intonational contours as a sequence
of High and Low tones associated with two types of
prosodic events ? pitch accents and phrase boundaries.
The tones describe an inventory of types of prosodic
events. In this work, we present Quantized Contour Mod-
eling, a novel approach to the automatic classification of
prosodic event types.
In Section 2, we describe related work on this task. We
describe Quantized Contour Modeling in Section 3. Our
materials are described in Section 4. Experimental results
are presented and discussed in Section 5. We conclude
and describe future directions for this work in Section 6.
2 Related Work
Five types of pitch accents ? pitch movements that corre-
spond to perceived prominence of an associated word ?
are defined in the ToBI standard(Silverman et al, 1992):
H*, L*, L+H*, L*+H, H+!H*. In addition to these five,
high tones (H) can be produced in a compressed pitch
range indicated by (!H). For the purposes of the experi-
ments described in this paper, we collapse high (H) and
downstepped High (!H) tones into a single class leav-
ing five accent types. The ToBI standard describes two
levels of phrasing, intermediate phrases and intonational
phrases which are comprised of one or more intermedi-
ate phrases. Each intermediate phrase has an associated
phrase accent describing the pitch movement between the
ultimate pitch accent and the phrase boundary. Phrase
accents can have High (H-), downstepped High (!H-) or
low (L-) tones. Intonational phrase boundaries have an
additional boundary tone, to describe a final pitch move-
ment. These can be high (H%) or low (L%). Intona-
tional phrases have five possible phrase ending tone com-
binations, L-L%, L-H%, H-L%, !H-L% and H-H%. In
section 5.3, we describe experiments classifying these
phrase ending tones.
The detection of pitch accents and phrase boundaries
has received significantly more research attention than
the classification of accent types and phrase ending be-
havior. However, one technique that has been used in
a number of research efforts is to simultaneously detect
and classify pitch accent. This is done by represent-
ing pitch accent detection and classfication as a four-way
classification task, where a token may be classified as
UNACCENTED, HIGH, LOW, or DOWNSTEPPED. Both
Ross and Ostendorf (1996) and Sun (2002) used this ap-
proach, reporting 72.4% and 77.0% accuracy respectively
when evaluated on a single speakers. Levow also used
this four-way classification for pitch accent detection and
classification under supervised (2005), and unsupervised
and semi-supervised learning approaches (2006). Using
721
SVMs with only acoustic features, 81.3% accuracy at the
syllable level is achieved. Using unsupervised spectral
clustering, 78.4% accuracy is reported, while using the
semi-supervised technique, Laplacian SVMs, 81.5% ac-
curacy is achieved. Since these approaches simultane-
ously evaluate the detection and classification of pitch
accents, direct comparison with this work is impossible.
Ananthakrishnan and Narayanan (2008) used RFC
(Taylor, 1994) and Tilt (Taylor, 2000) parameters along
with word and part of speech language modeling to clas-
sify pitch accents as H*, !H*, L+H* or L*. When eval-
uated on six BURNC speakers using leave-one-speaker-
out cross-validation, accuracy of 56.4% was obtained. In
the same work, the authors were able to classify L-L%
from L-H% phrase-final tones in the BURNC with 67.7%
accuracy. This performance was obtained using RFC F0
parameterization and a language model trained over cat-
egorical prosodic events.
3 Quantized Contour Modeling
In this section, we present a modeling technique, Quan-
tized Contour Modeling. This technique quantizes the f0
contour of a word in the time and pitch domains, generat-
ing a low-dimensional representation of the contour. The
pitch of the contour is linearly normalized to the range be-
tween the minimum and maximum pitch in the contour,
and quantized into N equally sized bins. The time do-
main is normalized to the range [0,1] and quantized into
M equally sized bins. An example of such a quantiza-
tion is presented in Figure 1 where N = 3 and M = 4.
Using this quantized representation of a pitch contour, we
Figure 1: Quantization with N=3 value and M=4 time bins.
train a multinomial mixture model for each pitch accent
type. Let the quantized contour be an M dimensional
vector C where C = (C1, C2, . . . , CM ), where Ci ?
{0 . . .N ? 1}. We indicate pitch (f0) contours by Cf0
and intensity contours by CI . We train a multinomial
model p(type|Ci, i) for each time bin i ? {0 . . .N ? 1}
with Laplace (add-one) smoothing. When using multi-
nomial models, we quantize the mean of the pitch values
assigned to a time bin. We use these pitch accent type
models to classify a contour using the Bayesian classi-
fication function found in Equation 1. This formulation
assumes that the values at each time are conditionally in-
dependent given the contour type. Also, we can modify
the model incorporating a Markov hypothesis to include
a sequential component by explicitly modeling the cur-
rent and previous quantized values, as in Equation 2. We
extend each of these models to model the energy contour
shape simultaneously with the pitch contour. The clas-
sification technique allows for the number of pitch and
energy value quantization bins to be distinct. However,
in these experiments, we tie these, constraining them to
be equal. The form of the classification functions using
the energy contours are found in Figure 2.
Standard shape modeling
type? = argmax
type
p(type)
M
Y
i
p(Ci|type, i) (1)
Sequential f0 modeling
type? = argmax
type
p(type)
M
Y
i
p(Ci|Ci?1, type, i) (2)
Standard f0 + I modeling
type? = argmax
type
p(type)
M
Y
i
p(Cf0i , C
I
i |type, i) (3)
Sequential f0 + I modeling
type? = argmax
type
p(type)
M
Y
i
p(Cf0i , C
I
i |Cf0i?1, CIi , type, i)
(4)
Figure 2: Quantized contour modeling classification formulae.
4 Materials and Methods
We use two corpora that have been manually annotated
with ToBI labels to evaluate the use of QCM in the clas-
sification of prosodic events. These two corpora are the
Boston University Radio News Corpus (BURNC) (Os-
tendorf et al, 1995) and the Boston Directions Corpus
(BDC) (Nakatani et al, 1995). The BURNC is a cor-
pus of professionally read radio news data. A 2.35 hour,
29,578 word, subset from six speakers (three female and
three male) has been prosodically annotated. The BDC
is made up of elicited monologues spoken by four non-
professional speakers, three male and one female. The
BDC is divided into two subcorpora comprised of spon-
taneous and read speech. The 50 minutes of read speech
contain 10,831 words. There are 60 minutes of annotated
spontaneous material containing 11,627 words. Both
are spoken by the same four speakers. In these experi-
ments we evaluate these subcorpora separately, and refer
to them as BDC-spon and BDC-read, respectively. The
distribution of pitch accents and phrase-ending tones for
these three corpora can be found in Figure 3.
722
Corpus H* L+H* L* L*+H H+!H*
BDC-read 78.24% 13.72% 5.97% 1.36% 0.71%
BDC-spon 84.57% 6.32% 7.70% 0.68% 0.73%
BURNC 69.99% 21.64% 3.67% 0.34% 4.37%
Corpus L-L% L-H% H-L% !H-L% H-H%
BDC-read 49.00% 35.62% 9.66% 4.29% 1.43%
BDC-spon 29.45% 32.57% 30.96% 4.40% 2.61%
BURNC 56.16% 38.38% 3.57% 0.68% 1.20%
Figure 3: Distribution of prosodic event types in BURNC, BDC-
read and BDC-spon corpora.
In order to use QCM classification, we must first
identify the region of an acoustic contour to quantify.
Though there is evidence that acoustic evidence of promi-
nence crosses the syllable boundary (Rosenberg and
Hirschberg, 2009), it is largely held that the acoustic ex-
cursion corresponding to intonational prominence is cen-
tered around a syllable. To identify the region of analysis
for QCM, we identify the accent-bearing syllable from
the manual prosodic annotation, and quantize the contour
extracted from the syllable boundaries. For the BURNC
material, forced alignment syllable boundaries are avail-
able. However, no forced-alignment phone information
is available for the BDC data. Therefore we apply Villing
et al?s (2004) envelope based pseudosyllabification rou-
tine to identify candidate syllabic regions. We use the
pseudosyllable containing the accent annotation as the re-
gion of analysis for the BDC material. For classification
of phrase ending intonation, we use the final syllable (or
pseudosyllable) in the phrase as the region of analysis.
To be clear, the accent and phrase boundary locations are
derived from manual annotations; the intonational tones
associated with these events are classified using QCM.
5 Prosodic Event Classification Results
In this section we present results applying QCM to the
classification of pitch accents and phrase ending intona-
tion. The work described in this section assumes the
presence of prosodic events is known a priori. The ap-
proaches described can be seen as operating on output of
an automatic prosodic event detection system.
5.1 Combined Error Rate
Automatic pitch accent classification poses an interest-
ing problem. Pitrelli, et al (Pitrelli et al, 1994) report
human agreement of only 64.1% on accent classifica-
tion in the ToBI framework. If downstepped variants of
accents are collapsed with their non-downstapped forms
this agreement improves to 76.1%. Second, pitch accents
are overwhelmingly H* in most labeled corpus, includ-
ing the BDC and BURNC material used in this paper.
This skewed class distribution leads to a very high base-
line, at or above the rate of human agreement. Because
of this, we find accuracy an unreliable measure for evalu-
ating the performance of this task. Multiple solutions can
have similar accuracy, but radically different classifica-
tion performance on minority classes. We therefore pro-
pose to use a different measure for the evaluation of pitch
accent type classification. We define the Combined Error
Rate (CER) as the mean of the weighted rates of Type I
and Type II errors. The combination of these measures
results in an increased penalty for errors of the majority
class while being more sensitive to minority class perfor-
mance than accuracy. Throughout this chapter, we will
continue to report accuracy for comparison to other work,
but consider CER to provide a more informative evalua-
tion. To avoid confusion, accuracy will be reported as a
percentage (%) while CER will be reported as a decimal.
CER = p(FP ) + p(FN)2 (5)
The Type I error rate measures the false positive rate for
a given class (cf. Equation 6).
p(FP ) =
?
i
p(Ci)p(FPi) (6)
We combine this measure with the Weighted Type II Er-
ror Rate (cf. Equation 7). The Type II error rate measures
the false negative rate for a given class
p(FN) =
?
i
p(Ci)p(FNi) (7)
5.2 Pitch Accent Classification
The first step in applying Quantized Contour Modeling
is to fix the desired quantization parameters. We do this
by identifying a stratified 10% held out tuning set from
the training data. We evaluate quantization sizes ranging
between 2 and 7 for both the time and value parameters,
leading to 36 candidates. Once we identify the best pa-
rameterization on this tuning data, we run ten-fold cross
validation on the remaining data to evaluate the perfor-
mance of each modeling technique (cf. Figure 2).
The classification accuracy and CER for each model
is reported in Table 1 along with the number of time and
value bins that were used. We first observe that model-
ing intensity information with f0 data does not improve
classification performance. The alignment between pitch
and intensity peaks have been shown to distinguish pitch
accent types (Rosenberg, 2009); this relationship is not
successfully captured by QCM. Moreover, we find that
sequential modeling only leads to improvements in CER
on BDC-read. On all corpora, the classification accuracy
is improved, with statistically insignificant (p > 0.05)
reductions in CER. This leads us to consider sequential
modeling of pitch to be the best performing approach to
the classification of pitch accent using QCM.
723
Method BDC-read BDC-spon BURNC
f0 46.51/.3860(5,3) 55.41/.4103(3,4) 47.56/.4444(4,4)
Seq. f0 73.17/.3667(6,7) 81.20/.4156 (7,5) 63.99/.4481(7,7)
f0+I 37.53/.4094(3,3) 47.96/.4222(4,2) 48.36/.4472(2,2)
Seq. f0+I 74.08/.4032(7,3) 80.60/.4361(5,4) 66.97/.4530(6,5)
Baseline 78.22/.0000 84.57/.0000 70.23/.0000
Table 1: Accuracy (%), CER, time and value bins from QCM pitch accent type classification experiments.
5.3 Phrase-ending Tone Classification
As in Section 5.2, we identify the best performing quanti-
zation parameters on a stratified 10% tuning set, then run
10-fold cross validation on the remaining data. Results
from QCM classification experiments classifying intona-
tional phrase ending tone combinations ? phrase accent
and boundary tone ? can be found in Table 2. We find
Method BDC-read BDC-spon BURNC
f0 48.21(3,6) 40.26(2,2) 70.36 (5,2)
Seq. f0 53.86(2,2) 43.80(4,4) 71.77 (6,2)
f0+I 48.21(6,6) 38.28(6,6) 67.83(2,2)
Seq. f0+I 57.94(6,6) 46.61(6,5) 72.91(7,7)
Baseline 49% 32% 55%
Table 2: Accuracy (%), time and value bins from QCM phrase
ending tone classification experiments.
that the simultaneous modeling of f0 and intensity con-
sistently yields the best performance in the classification
of phrase ending tones. These results all represent signif-
icant improvement over the majority class baseline. The
interaction between pitch and intensity contours in the
classification of phrase-ending intonation has not been
thoroughly investigated and remains an open area for fu-
ture research.
6 Conclusion and Future Work
In this paper we present a novel technique for the clas-
sification of two dimensional contour data, Quantized
Contour Modeling (QCM). QCM operates by quantizing
acoustic data into a pre-determined, fixed number of time
and value bins. From this quantized data, a model of the
value information is constructed for each time bin. The
likelihood of new data fitting these models is then per-
formed using a Bayesian inference.
We have applied QCM to the tasks of classifying pitch
accent types, and phrase-ending intonation. The best
performing parameterizations of QCM are able to clas-
sify pitch accent types on BURNC with 63.99% accuracy
and .4481 Combined Error Rate (CER). QCM classifies
phrase ending tones on this corpus with 72.91% accuracy.
These results do not represent the best performing ap-
proaches to these tasks. The best reported classification
of pitch accent types on BURNC is 59.95% accuracy and
.422 CER, for phrase ending intonation 75.09% (Rosen-
berg, 2009). However, the classification of phrase ending
intonation is accomplished by including QCM posteriors
in an SVM feature vector with other acoustic features.
This technique may be applicable to classifying other
phenomena. Here we have used ToBI tone classifications
as an intermediate representation of intonational phenom-
ena. QCM could be used to directly classify turn-taking
behavior, or dialog acts. Also, previous work has looked
at using the same techniques to classify prosodic events
and lexical tones in tonal languages such as Mandarin
Chinese. QCM could be directly applied to lexical tone
modeling; the only modification required would be a dif-
ferent segmentation routine.
References
S. Ananthakrishnan and S. Narayanan. 2008. Fine-grained
pitch accent and boundary tone labeling with parametric f0
features. In ICASSP.
G.-A. Levow. 2005. Context in multi-lingual tone and pitch
accent recognition. In Interspeech.
G.-A. Levow. 2006. Unsupervised and semi-supervised learn-
ing of tone and pitch accent. In HLT-NAACL.
C. Nakatani, J. Hirschberg, and B. Grosz. 1995. Discourse
structure in spoken language: Studies on speech corpora. In
AAAI Spring Symposium on Empirical Methods in Discourse
Interpretation and Generation.
M. Ostendorf, P. Price, and S. Shattuck-Hufnagel. 1995. The
boston university radio news corpus. Technical Report ECS-
95-001, Boston University, March.
J. Pitrelli, M. Beckman, and J. Hirschberg. 1994. Evaluation of
prosodic transcription labeling reliability in the tobi frame-
work. In ICSLP.
A. Rosenberg and J. Hirschberg. 2009. Detecting pitch accents
at the word, syllable and vowel level. In HLT-NAACL.
A. Rosenberg. 2009. Automatic Detection and Classification
of Prosodic Events. Ph.D. thesis, Columbia University.
K. Ross and M. Ostendorf. 1996. Prediction of abstract
prosodic labels for speech synthesis. Computer Speech &
Language, 10(3):155?185.
K. Silverman, et al 1992. Tobi: A standard for labeling english
prosody. In ICSLP.
X. Sun. 2002. Pitch accent predicting using ensemble machine
learning. In ICSLP.
P. Taylor. 1994. The rise/fall/connection model of intonation.
Speech Commun., 15(1-2):169?186.
P. Taylor. 2000. Analysis and synthesis of intonation using the
tilt model. Journal of the Acoustical Society of America.
R. Villing, et al 2004. Automatic blind syllable segmentation
for continuous speech. In ISSC.
724
