Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 458?467, Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Identification of Important Segments and Expressions for Mining
of Business-Oriented Conversations at Contact Centers
Hironori Takeuchi?, L Venkata Subramaniam?, Tetsuya Nasukawa?, and Shourya Roy?
?IBM Research, Tokyo Research Laboratory ?IBM Research, India Research Laboratory
Shimotsuruma 1623-14, Yamato-shi Institutional Area 4, Block-C, Vasant Kunj
Kanagawa 2428502 Japan New Delhi 110070 India
{hironori, nasukawa}@jp.ibm.com {lvsubram, rshourya}@in.ibm.com
Abstract
Textual records of business-oriented conver-
sations between customers and agents need
to be analyzed properly to acquire useful
business insights that improve productivity.
For such an analysis, it is critical to iden-
tify appropriate textual segments and ex-
pressions to focus on, especially when the
textual data consists of complete transcripts,
which are often lengthy and redundant. In
this paper, we propose a method to iden-
tify important segments from the conversa-
tions by looking for changes in the accuracy
of a categorizer designed to separate differ-
ent business outcomes. We extract effective
expressions from the important segments to
define various viewpoints. In text mining a
viewpoint defines the important associations
between key entities and it is crucial that the
correct viewpoints are identified. We show
the effectiveness of the method by using real
datasets from a car rental service center.
1 Introduction
?Contact center? is a general term for customer ser-
vice centers, help desks, and information phone
lines. Many companies operate contact centers to
sell their products, handle customer issues, and ad-
dress product-related and services-related issues. In
contact centers, analysts try to get insights for im-
proving business processes from stored customer
contact data. Gigabytes of customer contact records
are produced every day in the form of audio record-
ings of speech, transcripts, call summaries, email,
etc. Though analysis by experts results in insights
that are very deep and useful, such analysis usually
covers only a very small (1-2%) fraction of the total
call volume and yet requires significant workload.
The demands for extracting trends and knowledge
from the whole text data collection by using text
mining technology, therefore, are increasing rapidly.
In order to acquire valuable knowledge through
text mining, it is generally critical to identify im-
portant expressions to be monitored and compared
within the textual data. For example, given a large
collection of contact records at the contact center
of a manufacturer, the analysis of expressions for
products and expressions for problems often leads to
business value by identifying specific problems in a
specific product. If 30% of the contact records with
expressions for a specific product such as ?ABC?
contain expressions about a specific trouble such
as ?cracked?, while the expressions about the same
trouble appear in only 5% of the contact records for
similar products, then it should be a clue that the
product ?ABC? may actually have a crack-related
problem. An effective way to facilitate this type
of analysis is to register important expressions in a
lexicon such as ?ABC? and ?cracked? as associated
respectively with their categories such as ?product?
and ?problem? so that the behavior of terms in the
same category can be compared easily. It is actu-
ally one of the most important steps of text mining
to identify such relevant expressions and their cate-
gories that can potentially lead to some valuable in-
sights. A failure in this step often leads to a failure
in the text mining. Also, it has been considered an
artistic task that requires highly experienced consul-
458
tants to define such categories, which are often de-
scribed as the viewpoint for doing the analysis, and
their corresponding expressions through trial and er-
ror.
In this paper, we propose a method to identify im-
portant segments of textual data for analysis from
full transcripts of conversations. Compared to the
written summary of a conversation, a transcription
of an entire conversation tends to be quite lengthy
and contains various forms of redundancy. Many
of the terms appearing in the conversation are not
relevant for specific analysis. For example, the
terms for greeting such as ?Hello? and ?Welcome
to (Company A)? are unlikely to be associated with
specific business results such as purchased-or-not
and satisfied-or-not, especially because the conver-
sation is transcribed without preserving the nonver-
bal moods such as tone of voice, emotion etc. Thus
it is crucial to identify key segments and notable
expressions within conversations for analysis to ac-
quire valuable insights.
We exploit the fact that business conversations
follow set patterns such as an opening followed by a
request and the confirmation of details followed by
a closing, etc. By taking advantage of this feature of
business conversations, we have developed a method
to identify key segments and the notable expressions
within conversations that tend to discriminate be-
tween the business results. Such key segments, the
trigger segments, and the notable expressions asso-
ciated with certain business results lead us to easily
understand appropriate viewpoints for analysis.
Application of our method for analyzing nearly
one thousand conversations from a rental car reser-
vation office enabled us to acquire novel insights for
improving agent productivity and resulted in an ac-
tual increase in revenues.
Organization of the Paper: We start by describ-
ing the properties of the conversation data used in
this paper. Section 3 describes the method for iden-
tifying useful viewpoints and expressions that meet
the specified purpose. Section 4 provides the results
using conversational data. After the discussion in
Section 5, we conclude the paper in Section 6.
2 Business-Oriented Conversation Data
We consider business-oriented conversation data
collected at contact centers handling inbound tele-
phone sales and reservations. Such business oriented
conversations have the following properties.
? Each conversation is a one-to-one interaction
between a customer and an agent.
? For many contact center processes the conver-
sation flow is well defined in advance.
? There are a fixed number of outcomes and each
conversation has one of these outcomes.
For example, in car rentals, the following conversa-
tion flow is pre-defined for the agent. In practice
most calls to a car rental center follow this call flow.
? Opening - contains greeting, brand name, name
of agent
? Pick-up and return details - agent asks location,
dates and times of pick up and return, etc.
? Offering car and rate - agent offers a car spec-
ifying rate and mentions applicable special of-
fers.
? Personal details - agent asks for customer?s in-
formation such as name, address, etc.
? Confirm specifications - agent recaps reserva-
tion information such as name, location, etc.
? Mandatory enquiries - agent verifies clean driv-
ing record, valid license, etc.
? Closing - agent gives confirmation number and
thanks the customer for calling.
In these conversations the participants speak in turns
and the segments can be clearly identified. Figure 1
shows part of a transcribed call.
Each call has a specific outcome. For example,
each car rental transaction has one of two call types,
reservation or unbooked, as an outcome.
Because the call process is pre-defined, the con-
versations look similar in spite of having different
results. In such a situation, finding the differences in
the conversations that have effects on the outcomes
459
is very important, but it is very expensive and dif-
ficult to find such unknown differences by human
analysis. We show that it is possible to define proper
viewpoints and corresponding expressions leading
to insights on how to change the outcomes of the
calls.
AGENT: Welcome to CarCompanyA. My name is Albert. How may I
help you?
.........
AGENT: Allright may i know the location you want to pick the
car from.
CUSTOMER: Aah ok I need it from SFO.
AGENT: For what date and time.
.........
AGENT: Wonderful so let me see ok mam so we have a 12 or 15
passenger van avilable on this location on those dates and
for that your estimated total for those three dates just
300.58$ this is with Taxes with surcharges and with free
unlimited free milleage.
.........
AGENT : alright mam let me recap the dates you want to pick
it up from SFO on 3rd August and drop it off on august 6th in
LA alright
CUSTOMER : and one more questions Is it just in states or
could you travel out of states
.........
AGENT : The confirmation number for your booking is 221 384.
CUSTOMER : ok ok Thank you
Agent : Thank you for calling CarCompanyA and you have a
great day good bye
Figure 1: Transcript of a car rental dialog (partial)
3 Trigger Segment Detection and Effective
Expression Extraction
In this section, we describe a method for automat-
ically identifying valuable segments and concepts
from the data for the user-specified difference anal-
ysis. First, we present a model to represent the con-
versational data. After that we introduce a method
to detect the segments where the useful concepts for
the analysis appear. Finally, we select useful expres-
sions in each detected trigger segment.
3.1 Data Model
Each conversational data record in the collection D
is defined as di. Each di can be seen as a sequence
of conversational turns in the conversational data,
and then di can be divided as
di = d1i + d2i + ? ? ?+ dMii , (1)
where dki is the k-th turn in di and Mi is the total
number of turns in di. The + operator in the above
equation can be seen as an equivalent of the string
concatenation operator. We define d?ji as the por-
tion of di from the beginning to turn j. Using the
same notation, d?ji = d1i + d2i + ? ? ? + dji . The
collection of d?mki constitutes the Chronologically
Cumulative Data up to turn mk (Dk). Dk is repre-
sented as
Dk = (d?mk1 ,d?mk2 , . . . ,d?mkn ). (2)
Figure 2 shows an image of the data model. We set
some mk and prepare the chronologically cumula-
tive data set as shown in Figure 3. We represent bi-
nary mutually exclusive business outcomes such as
success and failure resulting from the conversations
as ?A? and ?not A?.
di= di1+?+diMi
Number of turns0 1 2 3 Mi
di1 di2 di3 diMimk
di~mk= i1+?+dimk
Figure 2: Conversation data model
m5 turnm1 m2 m3 m40 1 2 5 10 15
Ddidi~m5
D5di~m4
D4di~m3
D3
D2
D1
di~m2
di~m1
m1=1, m2=2, m3=5, m4=10, m5=15
Figure 3: Chronologically cumulative conversa-
tional data
3.2 Trigger Segment Detection
Trigger segments can be viewed as portions of the
data which have important features which distin-
guish data of class ?A? from data of class ?not A?.
460
To detect such segments, we divide each chrono-
logically cumulative data set Dk into two data sets,
training data Dtrainingk and test data Dtestk . Start-
ing from D1, for each Dk we trained a classifier
using Dtrainingk and evaluated it on Dtestk . Using
accuracy, the fraction of correctly classified docu-
ments, as a metric of performance (Yang and Liu,
1999), we denote the evaluation result of the cat-
egorization as acc(categorizer(Dk)) for each Dk
and plot it along with its turn. Figure 4 shows the
effect of gradually increasing the training data for
the classification. The distribution of expressions
m1 m2 m3 m4 m5
acc(categorizer(Di))
trigger trigger
D1
D2 D3
D4
D5 D all
turn
Figure 4: Plot of acc(categorizer(Dk))
in a business-oriented conversation will change al-
most synchronously because the call flow is pre-
defined. Therefore acc(categorizer(Dk)) will in-
crease if features that contribute to the categorization
appear in Dk. In contrast, acc(categorizer(Dk))
will decrease if no features that contribute to
the categorization are in Dk. Therefore, from
the transitions of acc(categorizer(Dk)), we can
identify the segments with increases as triggers
where the features that have an effect on the out-
come appear. We denote a trigger segment as
seg(start position, end position). Because the to-
tal numbers of turns can be different, we do not
detect the last section as a trigger. In Figure 4,
seg(m1,m2) and seg(m4,m5) are triggers. It is
important to note that using the cumulative data is
key to the detection of trigger segments. Using non-
cumulative segment data would give us the catego-
rization accuracy for the features within that seg-
ment but would not tell us whether the features of
this segment are improving the accuracy or decreas-
ing it. It is this gradient information between seg-
ments that is key to identifying trigger segments.
Many approaches have been proposed for docu-
ment classification (Yang and Liu, 1999). In this
research, however, we are not interested in the clas-
sification accuracy itself but in the increase and de-
crease of the accuracy within particular segments.
For example, the greeting, or the particular method
of payment may not affect the outcome, but the
mention of a specific feature of the product may
have an effect on the outcome. Therefore in our
research we are interested in identifying the partic-
ular portion of the call where this product feature
is mentioned, along with its mention, which has an
effect on the outcome of the call. In our experi-
ments we used the SVM (Support Vector Machine)
classifier (Joachims, 1998), but almost any classifier
should work because our approach does not depend
on the classification method.
3.3 Effective Expression Extraction
In this section, we describe our method to extract
effective expressions from the detected trigger seg-
ments.
The effective expressions in Dk are those which
are representative in the selected documents and
appear for the first time in the trigger segments
seg(mi,mj). Numerous methods to select features
exist (Hisamitsu and Niwa, 2002) (Yang and Ped-
ersen, 1997). We use the ?2 statistic for each ex-
pression in Dk as a representative metric. For the
two-by-two contingency table of a expression w and
a class ?A? shown in Table 1, the ?2 statistic is cal-
culated as
Table 1: Contingency table for calculating the ?2
statistic
# of documents # of documents
including w not including w
A n11 n12
not-A n21 n22
?2 = N(n11n22 ? n12n21)
2
(n11 + n12)(n11 + n21)(n12 + n22)(n21 + n22) (3)
where N is the number of documents. This statis-
tic can be compared to the ?2 distribution with one
degree of freedom to judge representativeness.
We also want to extract the expressions that have
not had an effect on the outcome before Dk. To de-
tect the new expressions in Dk, we define the metric
461
new(w) = w(Dk)max(w(Dk?1), 1)/
mk
mk?1
?sign(w(DAk )? w(DnotAk )), (4)
where w(Dk) is the frequency of expression w in
the chronologically cumulative data Dk, max(a, b)
selects the larger value in the arguments, mk is the
number of turns in Dk, w(DAk ) is the frequency of
w in Dk with the outcome of the corresponding data
being ?A?, and sign(?) is the signum function. When
w in class ?A? appears in Dk much more frequently
than Dk?1 compared with the ratio of their turns,
this metric will be more than 1. We detect signifi-
cant expressions by considering the combined score
?2(w) ? new(w). Using this combined score, we
can filter out the representative expressions that have
already appeared before Dk and distinguish signifi-
cant expressions that first appear in Dk for each class
?A? and ?not A?.
3.4 Appropriate Viewpoint Selection
In a text mining system, to get an association that
leads to a useful insight, we have to define appro-
priate viewpoints. Viewpoints refer to objects in re-
lation to other objects. In analysis using a conven-
tional text mining system (Nasukawa and Nagano,
2001), the viewpoints are selected based on expres-
sions in user dictionaries prepared by domain ex-
perts. We have identified important segments of the
conversations by seeing changes in the accuracy of a
categorizer designed to segregate different business
outcomes. We have also been able to extract effec-
tive expressions from these trigger segments to de-
fine various viewpoints. Hence, viewpoint selection
is now based on the trigger segments and effective
expressions identified automatically based on speci-
fied business outcomes. In the next section we apply
our technique to a real life dataset and show that we
can successfully select useful viewpoints.
4 Experiments and Results
4.1 Experiment Data and System
We collected 914 recorded calls from the car rental
help desk and manually transcribed them. Figure 1
shows part of a call that has been transcribed.
There are three types of calls:
1. Reservation Calls: Calls which got converted.
Here, ?converted? means the customer made a
reservation for a car. Reserved cars can get
picked-up or not picked-up, so some reserved
cars do not eventually get picked-up by cus-
tomers (no shows and cancellations).
2. Unbooked Calls: Calls which did not get con-
verted.
3. Service Calls: Customers changing or enquir-
ing about a previous booking.
The distribution of the calls is given in Table 2.
Table 2: Distribution of calls
Unbooked Calls 461
Reservation Calls (Picked-Up) 72
Reservation Calls (Not Picked-Up) 65
Service Calls 326
Total Calls 914
The reservation calls are most important in this
context, so we focus on those 137 calls. In the reser-
vation calls, there are two types of outcomes, car
picked-up and car not picked-up. All reservation
calls look similar in spite of having different out-
comes (in terms of pick up). The reservation hap-
pens during the call but the pick up happens at a
later date. If we can find differences in the conver-
sation that affect the outcome, it is expected that we
could improve the agent productivity. Reservation
calls follow the pre-defined reservation call flow that
we mentioned in Section 2 and it is very difficult
to find differences between them manually. In this
experiment, by using the proposed method, we try
to extract trigger segments and expressions to find
viewpoints that affect the outcome of the reservation
calls.
For the analysis, we constructed a text mining sys-
tem for the difference analysis ?picked-up? vs. ?not
picked-up?. The experimental system consists of
two parts, an information extraction part and a text
mining part. In the information extraction part we
define dictionaries and templates to identify useful
expressions. In the text mining part we define appro-
priate viewpoints based on the identified expressions
to get useful associations leading to useful insights.
462
4.2 Results of Trigger Segment Detection and
Effective Expression Extraction
Based on the pre-defined conversation flow de-
scribed in Section 2, we set m1=1, m2=2,
m3=5, m4=10, m5=15, and m6=20 and prepared
D1, . . . , D6 and D. The features of di consist of
nouns, compound nouns, specified noun phrases
(e.g. adjective+noun), and verbs. For each Dk
we calculated acc(categorizer(Dk)) for the classes
?picked-up? and ?not picked-up.? In this process, we
use a SVM-based document categorizer (Joachims,
2002). Of the 137 calls, we used 100 calls for
training the categorizer and 37 calls for trigger
segment detection. Figure 5 shows the results of
acc(categorizer(Dk)) for picked-up. The accuracy
of classification using the data of entire conversa-
tions (acc(categorizer(D)) is 67.6% but we are try-
ing to detect important segments by considering not
the accuracy values themselves but the gradients be-
tween segments. From these results, seg(1, 2) and
0
10
20
30
40
50
60
70
80
0 5 10 15 20 25 30 35 40 45
Turn (m_j)
A
c
c
u
r
a
c
y
 
[
%
]
D1
D2
D3
D4
D5
D
D6
Figure 5: Result of acc(categorizer(Dk))
seg(10, 15) are detected as trigger segments. We
now know that these segments are highly correlated
to the outcome of the call.
For each detected trigger segment, we extract ef-
fective expressions in each class using the metric de-
scribed in Section 3.3. Table 3 shows some expres-
sions with high values for the metric for each trigger.
In this table, ?just NUMERIC dollars? is a canonical
expression and an expression such as ?just 160 dol-
lars? is mapped to this canonical expression in the
information extraction process. From this result, in
seg(1, 2), ?make?, ?reservation? are correlated with
?pick up? and ?rate? and ?check? are correlated with
Table 3: Selected expressions in trigger segments
Trigger Selected expressions
pick up not picked up
seg(1, 2) make, return, tomorrow, rate, check, see
day, airport, look, want, week
assist, reservation, tonight
seg(10, 15) number, corporate program, go, impala
contract, card, have,
tax surcharge,
just NUMERIC dollars,
discount, customer club,
good rate, economy
?not-picked up?. By looking at some documents
containing these expressions, we found customer in-
tention phrases such as ?would like to make a reser-
vation?, ?want to check a rate?, etc. Therefore, it
can be induced that the way a customer starts the
call may have an impact on the outcome. From ex-
pressions in seg(10, 15), it can be said that discount-
related phrases and mentions of the good rates by the
agent can have an effect on the outcome.
We can directly apply the conventional methods
for representative feature selection to D. The fol-
lowing expressions were selected as the top 20 ex-
pressions from whole conversational data by using
the ?2 metric defined in (3).
corporate program, contract, counter, September,
mile, rate, economy, last name,
valid driving license,BRAND NAME, driving,
telephone, midsize, tonight, use, credit, moment,
airline, afternoon
From these results, we see that looking at the call as
a whole does not point us to the fact that discount-
related phrases, or the first customers-utterance, af-
fect the outcome. Detecting trigger segments and
extracting important expressions from each trigger
segment are key to identifying subtle differences be-
tween very similar looking calls that have entirely
opposite outcomes.
4.3 Results of Text Mining Analysis using
Selected Viewpoints and Expressions
From the detected segments and expressions we de-
termined that the customer?s first utterance along
with discount phrases and value selling phrases af-
fected the call outcomes. Under these hypotheses,
we prepared the following semantic categories.
463
? Customer intention at start of call: From the
customer?s first utterance, we extract the fol-
lowing intentions based on the patterns.
? strong start: would like to make a booking,
need to pick up a car, . . .
? weak start: would like to check the rates,
want to know the rate for vans, . . .
Under our hypotheses, the customer with a
strong start has the intention of booking a car
and we classify such a customer as a book-
ing customer. The customer with a weak start
usually just wants to know the rates and is clas-
sified as a rates customer.
? discount-related phrases: discount, corporate
program, motor club, buying club . . . are reg-
istered into the domain dictionary as discount-
related phrases.
? value selling phrases: we extract phrases men-
tioning good rates and good vehicles by match-
ing patterns related to such utterances.
? mentions of good rates: good rate, won-
derful price, save money, just need to pay
this low amount, . . .
? mentions of good vehicles: good car, fan-
tastic car, latest model, . . .
Using these three categories, we tried to find insights
to improve agent productivity.
Table 4 shows the result of two-dimensional as-
sociation analysis for 137 reservation calls. This ta-
ble shows the association between customer types
based on customer intention at the start of a call
and pick up information. From these results, 67%
Table 4: Association between customer types and
pick up information
Customer types extracted from texts Pick up information
based on customer intent at start of call pick up not-picked up
booking customer (w/ strong start) (70) 47 23
rates customer (w/ weak start) (37) 13 24
(47 out of 70) of the booking customers picked up
the reserved car and only 35% (13 out of 37) of the
rates customers picked it up. This supports our hy-
pothesis and means that pick up is predictable from
the customer?s first or second utterance.
It was found that cars booked by rates customers
tend to be ?not picked up,? so if we can find any
actions by agents that convert such customers into
?pick up,? then the revenue will improve. In the
booking customer case, to keep the ?pick up? high,
we need to determine specific agent actions that con-
cretize the customer?s intent.
Table 5 shows how mentioning discount-related
phrases affects the pick up ratios for rates customers
and booking customers. From this table, it can
Table 5: Association between mention of discount
phrases and pick up information
Rates customer Pick up information
Mention of discount phrases by agents pick up not-picked up
yes (21) 10 11
no (16) 3 13
Booking customer Pick up information
Mention of discount phrases by agents pick up not picked up
yes (40) 30 10
no (30) 17 13
be seen that mentioning discount phrases affects
the final status of both types of customers. In the
rates customer case, the probability that the booked
car will be picked up, P (pick-up) is improved to
0.476 by mentioning discount phrases. This means
customers are attracted by offering discounts and
this changes their intention from ?just checking rate?
to ?make a reservation here?. We found similar
trends for the association between mention of value
selling phrases and pick up information.
4.4 Improving Agent Productivity
From the results of the text mining analysis experi-
ment, we derived the following actionable insights:
? There are two types of customers in reservation
calls.
? Booking customer (with strong start)
tends to pick up the reserved car.
? Rates customer (with weak start) tends
not to pick up the reserved car.
? In the rates customer case, ?pick up? is im-
proved by mentioning discount phrases.
By implementing the actionable insights derived
from the analysis in an actual car rental process, we
verified improvements in pick up. We divided the
83 agents in the car rental reservation center into
two groups. One of them, consisting of 22 agents,
was trained based on the insights from the text min-
ing analysis. The remaining 61 agents were not
told about these findings. By comparing these two
464
groups over a period of one month we hoped to see
how the actionable insights contributed to improv-
ing agent performance. As the evaluation metric, we
used the pick up ratio - that is the ratio of the number
of ?pick-ups? to the number of reservations.
Following the training the pick up ratio of the
trained agents increased by 4.75%. The average
pick up ratio for the remaining agents increased by
2.08%. Before training the ratios of both groups
were comparable. The seasonal trends in this indus-
try mean that depending on the month the bookings
and pickups may go up or down. We believe this
is why the average pick up ratio for the remaining
agents also increased. Considering this, it can be es-
timated that by implementing the actionable insights
the pick up ratio for the pilot group was improved by
about 2.67%. We confirmed that this difference is
meaningful because the p-value of the t-test statistic
is 0.0675 and this probability is close to the stan-
dard t-test (?=0.05). Seeing this, the contact center
trained all of its agents based on the insights from
the text mining analysis.
5 Discussion
There has been a lot of work on specific tools for
analyzing the conversational data collected at con-
tact centers. These include call type classification
for the purpose of categorizing calls (Tang et al,
2003) (Zweig et al, 2006), call routing (Kuo and
Lee, 2003) (Haffner et al, 2003), obtaining call log
summaries (Douglas et al, 2005), agent assisting
and monitoring (Mishne et al, 2005), and building
of domain models (Roy and Subramaniam, 2006).
Filtering problematic dialogs automatically from an
automatic speech recognizer has also been studied
(Hastie et al, 2002) (Walker et al, 2002). In con-
trast to these technologies, in this paper we con-
sider the task of trying to find insights from a col-
lection of complete conversations. In (Nasukawa
and Nagano, 2001), such an analysis was attempted
for agent-entered call summaries of customer con-
tacts by extracting phrases based on domain-expert-
specified viewpoints. In our work we have shown
that even for conversational data, which is more
complex, we could identify proper viewpoints and
prepare expressions for each viewpoint. Call sum-
maries by agents tend to mask the customers? inten-
tion at the start of the call. We get more valuable
insights from the text mining analysis of conversa-
tional data. For such an analysis of conversational
data, our proposed method has an important role.
With our method, we find the important segments
in the data for doing analyses. Also our analyses are
closely linked to the desired outcomes.
In trigger detection, we created a chronologically
cumulative data set based on turns. We can also
use the segment information such as the ?opening?
and ?enquiries? described in Section 2. We prepared
data with segment information manually assigned,
made the chronologically cumulative data and ap-
plied our trigger detection method. Figure 6 shows
the results of acc(categorizer(Dk)). The trend in
40
45
50
55
60
call start -->
opening
call start -->
details
call start -->
offering
call start -->
personal
details
call start -->
confirmation,
mandatory
questions,
closing
Conversation flow
A
c
c
u
r
a
c
y
 
[
%
]
Figure 6: Result of acc(categorizer(Dk)) using
segment information
Figure 6 is similar to that in Figure 5. From this
result, it is observed that ?opening? and ?offering?
segments are trigger segments. Usually, segmenta-
tion is not done in advance and to assign such infor-
mation automatically we need data with labeled seg-
mentation information. The results show that even
in the absence of labeled data our trigger detection
method identifies the trigger segments. In the exper-
iments in Section 4, we set turns for each chrono-
logically cumulative data by taking into account the
pre-defined call flow.
In Figure 5 we observe that the accuracy of the
categorizer is decreasing even when using increas-
ing parts of the call. Even the accuracy using the
complete call is less than using only the first turn.
This indicates that the first turn is very informative,
but it also indicates that the features are not being
used judiciously. In a conventional classification
task, the number of features are sometimes restricted
465
when constructing a categorizer. It is known that se-
lecting only significant features improves the clas-
sification accuracy (Yang and Pedersen, 1997). We
used Information Gain for selecting features from
the document collection. This method selects the
most discriminative features between two classes.
As expected the classification accuracy improved
significantly as we reduced the total number of fea-
tures from over 2,000 to the range of 100 to 300.
Figure 7 shows the changes in accuracy. In the pro-
40
45
50
55
60
65
70
75
80
85
90
0 5 10 15 20 25 30 35 40 45
Turn (m_j)
A
c
c
u
r
a
c
y
 
[
%
}
100
200
300
D1
D2
D3
D4 D5 D
D6
Figure 7: Result of acc(categorizer(Dk)) with top
100 to 300 features selected using information gain
posed method, we detect trigger segments using the
increases and decreases of the classification accu-
racy. By selecting features, the noisy features are not
added in the segments. Decreasing portions, there-
fore are not observed. In this situation, as a trigger
segment, we can detect the portion where the gra-
dient of the accuracy curve increases. Also using
feature selection, we find that the classification ac-
curacy is highest when using the entire document,
which is expected. However, we notice that the trig-
ger segments obtained with and without feature se-
lection are almost the same.
In the experiment, we use manually transcribed
data. As future work we would like to use the noisy
output of an automatic speech recognition system to
obtain viewpoints and expressions.
6 Conclusion
In this paper, we have proposed methods for iden-
tifying appropriate segments and expressions auto-
matically from the data for user specified difference
analysis. We detected the trigger segments using the
property that a business-oriented conversation fol-
lows a pre-defined flow. After that, we identified
the appropriate expressions from each trigger seg-
ment. It was found that in a long business-priented
conversation there are important segments affecting
the outcomes that can not been easily detected by
just looking through the conversation, but such seg-
ments can be detected by monitoring the changes
of the categorization accuracy. For the trigger seg-
ment detection, we do not use semantic segment in-
formation but only the positional segment informa-
tion based on the conversational turns. Because our
method does not rely on the semantic information in
the data, therefore our method can be seen as robust.
Through experiments with real conversational data,
using identified segments and expressions we were
able to define appropriate viewpoints and concepts
leading to insights for improving the car rental busi-
ness process.
Acknowledgment
The authors would like to thank Sreeram Balakr-
ishnan, Raghuram Krishnapuram, Hideo Watanabe,
and Koichi Takeda at IBM Research for their sup-
port. The authors also appreciate the efforts of Jatin
Joy Giri at IBM India in providing domain knowl-
edge about the car rental process and thank him for
help in constructing the dictionaries.
References
S. Douglas, D. Agarwal, T. Alonso, R. M. Bell,
M. Gilbert, D. F. Swayne, and C. Volinsky. 2005.
Mining customer care dialogs for ?daily news?.
IEEE Transaction on Speech and Audio Processing,
13(5):652?660.
P. Haffner, G. Tur, and J. H. Wright. 2003. Optimizing
svms for complex call classification. In Proceedings of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 632?635.
H. W. Hastie, R. Prasad, and M. A. Walker. 2002. What?s
the trouble: Automatically identifying problematic di-
alogues in darpa communicator dialogue systems. In
Proceedings of the 40th Annual Meeting of the ACL,
pages 384?391.
T. Hisamitsu and Y. Niwa. 2002. A measure of term rep-
resentativeness based on the number of co-occurring
sailent words. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING),
pages 1?7.
466
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the 10th European Conference on
Machine Learning (ECML), pages 137?142.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142.
H.-K J. Kuo and C.-H. Lee. 2003. Discriminative train-
ing of natural language call routers. IEEE Transaction
on Speech and Audio Processing, 11(1):24?35.
G. Mishne, D. Carmel, R. Hoory, A. Roytman, and
A. Soffer. 2005. Automatic analysis of call-center
conversations. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 453?459.
T. Nasukawa and T. Nagano. 2001. Text analysis and
knowledge mining system. IBM Systems Journal,
pages 967?984.
S. Roy and L. V. Subramaniam. 2006. Automatic
generation of domain models for call centers from
noisy transcriptions. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL (COLING/ACL),
pages 737?744.
M. Tang, B. Pellom, and K. Hacioglu. 2003. Call-
type classification and unsupervised training for the
call center domain. In Proceesings of IEEE Workshop
on Automatic Speech Recognition and Understanding,
pages 204?208.
M. A. Walker, I. Langkilde-Geary, H. W. Hastie,
J. Wright, and A. Gorin. 2002. Automatically train-
ing a problematic dialogue predictor for a spoken di-
alogue system. Journal of Artificial Intelligence Re-
search, 16:393?319.
Y. Yang and X. Liu. 1999. A re-examination of text cate-
gorization methods. In Proceedings of the 22th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 42?
49.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the 14th International Conference on Machine
Learning (ICML), pages 412?420.
G. Zweig, O. Shiohan, G. Saon, B. Ramabhadran,
D. Povey, L. Mangu, and B. Kingsbury. 2006. Au-
tomatic analysis of call-center conversations. In Pro-
ceedings of IEEE Internatinal Conference of Acous-
tics, Speech and Signal Processing (ICASSP), pages
589?592.
467
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 852?860,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
SMS based Interface for FAQ Retrieval
Govind Kothari
IBM India Research Lab
gokothar@in.ibm.com
Sumit Negi
IBM India Research Lab
sumitneg@in.ibm.com
Tanveer A. Faruquie
IBM India Research Lab
ftanveer@in.ibm.com
Venkatesan T. Chakaravarthy
IBM India Research Lab
vechakra@in.ibm.com
L. Venkata Subramaniam
IBM India Research Lab
lvsubram@in.ibm.com
Abstract
Short Messaging Service (SMS) is popu-
larly used to provide information access to
people on the move. This has resulted in
the growth of SMS based Question An-
swering (QA) services. However auto-
matically handling SMS questions poses
significant challenges due to the inherent
noise in SMS questions. In this work we
present an automatic FAQ-based question
answering system for SMS users. We han-
dle the noise in a SMS query by formu-
lating the query similarity over FAQ ques-
tions as a combinatorial search problem.
The search space consists of combinations
of all possible dictionary variations of to-
kens in the noisy query. We present an ef-
ficient search algorithm that does not re-
quire any training data or SMS normaliza-
tion and can handle semantic variations in
question formulation. We demonstrate the
effectiveness of our approach on two real-
life datasets.
1 Introduction
The number of mobile users is growing at an
amazing rate. In India alone a few million sub-
scribers are added each month with the total sub-
scriber base now crossing 370 million. The any-
time anywhere access provided by mobile net-
works and portability of handsets coupled with the
strong human urge to quickly find answers has fu-
eled the growth of information based services on
mobile devices. These services can be simple ad-
vertisements, polls, alerts or complex applications
such as browsing, search and e-commerce. The
latest mobile devices come equipped with high
resolution screen space, inbuilt web browsers and
full message keypads, however a majority of the
users still use cheaper models that have limited
screen space and basic keypad. On such devices,
SMS is the only mode of text communication.
This has encouraged service providers to build in-
formation based services around SMS technology.
Today, a majority of SMS based information ser-
vices require users to type specific codes to re-
trieve information. For example to get a duplicate
bill for a specific month, say June, the user has
to type DUPBILLJUN. This unnecessarily con-
straints users who generally find it easy and intu-
itive to type in a ?texting? language.
Some businesses have recently allowed users to
formulate queries in natural language using SMS.
For example, many contact centers now allow cus-
tomers to ?text? their complaints and requests for
information over SMS. This mode of communica-
tion not only makes economic sense but also saves
the customer from the hassle of waiting in a call
queue. Most of these contact center based services
and other regular services like ?AQA 63336?1 by
Issuebits Ltd, GTIP2 by AlienPant Ltd., ?Tex-
perts?3 by Number UK Ltd. and ?ChaCha?4 use
human agents to understand the SMS text and re-
spond to these SMS queries. The nature of tex-
ting language, which often as a rule rather than ex-
ception, has misspellings, non-standard abbrevia-
tions, transliterations, phonetic substitutions and
omissions, makes it difficult to build automated
question answering systems around SMS technol-
ogy. This is true even for questions whose answers
are well documented like a FAQ database. Un-
like other automatic question answering systems
that focus on generating or searching answers, in
a FAQ database the question and answers are al-
ready provided by an expert. The task is then
to identify the best matching question-answer pair
for a given query.
In this paper we present a FAQ-based ques-
tion answering system over a SMS interface. Our
1http://www.aqa.63336.com/
2http://www.gtip.co.uk/
3http://www.texperts.com/
4http://www.chacha.com/
852
system allows the user to enter a question in
the SMS texting language. Such questions are
noisy and contain spelling mistakes, abbrevia-
tions, deletions, phonetic spellings, translitera-
tions etc. Since mobile handsets have limited
screen space, it necessitates that the system have
high accuracy. We handle the noise in a SMS
query by formulating the query similarity over
FAQ questions as a combinatorial search prob-
lem. The search space consists of combinations
of all possible dictionary variations of tokens in
the noisy query. The quality of the solution, i.e.
the retrieved questions is formalized using a scor-
ing function. Unlike other SMS processing sys-
tems our model does not require training data or
human intervention. Our system handles not only
the noisy variations of SMS query tokens but also
semantic variations. We demonstrate the effective-
ness of our system on real-world data sets.
The rest of the paper is organized as follows.
Section 2 describes the relevant prior work in this
area and talks about our specific contributions.
In Section 3 we give the problem formulation.
Section 4 describes the Pruning Algorithm which
finds the best matching question for a given SMS
query. Section 5 provides system implementation
details. Section 6 provides details about our exper-
iments. Finally we conclude in Section 7.
2 Prior Work
There has been growing interest in providing ac-
cess to applications, traditionally available on In-
ternet, on mobile devices using SMS. Examples
include Search (Schusteritsch et al, 2005), access
to Yellow Page services (Kopparapu et al, 2007),
Email 5, Blog 6 , FAQ retrieval 7 etc. As high-
lighted earlier, these SMS-based FAQ retrieval ser-
vices use human experts to answer questions.
There are other research and commercial sys-
tems which have been developed for general ques-
tion and answering. These systems generally
adopt one of the following three approaches:
Human intervention based, Information Retrieval
based, or Natural language processing based. Hu-
man intervention based systems exploit human
communities to answer questions. These sys-
tems 8 are interesting because they suggest simi-
lar questions resolved in the past. Other systems
5http://www.sms2email.com/
6http://www.letmeparty.com/
7http://www.chacha.com/
8http://www.answers.yahoo.com/
like Chacha and Askme9 use qualified human ex-
perts to answer questions in a timely manner. The
information retrieval based system treat question
answering as an information retrieval problem.
They search large corpus of text for specific text,
phrases or paragraphs relevant to a given question
(Voorhees, 1999). In FAQ based question answer-
ing, where FAQ provide a ready made database of
question-answer, the main task is to find the clos-
est matching question to retrieve the relevant an-
swer (Sneiders, 1999) (Song et al, 2007). The
natural language processing based system tries to
fully parse a question to discover semantic struc-
ture and then apply logic to formulate the answer
(Molla et al, 2003). In another approach the ques-
tions are converted into a template representation
which is then used to extract answers from some
structured representation (Sneiders, 2002) (Katz et
al., 2002). Except for human intervention based
QA systems most of the other QA systems work
in restricted domains and employ techniques such
as named entity recognition, co-reference resolu-
tion, logic form transformation etc which require
the question to be represented in linguistically cor-
rect format. These methods do not work for SMS
based FAQ answering because of the high level of
noise present in SMS text.
There exists some work to remove noise from
SMS (Choudhury et al, 2007) (Byun et al, 2007)
(Aw et al, 2006) (Kobus et al, 2008). How-
ever, all of these techniques require aligned cor-
pus of SMS and conventional language for train-
ing. Building this aligned corpus is a difficult task
and requires considerable human effort. (Acharya
et al, 2008) propose an unsupervised technique
that maps non-standard words to their correspond-
ing conventional frequent form. Their method can
identify non-standard transliteration of a given to-
ken only if the context surrounding that token is
frequent in the corpus. This might not be true in
all domains.
2.1 Our Contribution
To the best of our knowledge we are the first to
handle issues relating to SMS based automatic
question-answering. We address the challenges
in building a FAQ-based question answering sys-
tem over a SMS interface. Our method is unsu-
pervised and does not require aligned corpus or
explicit SMS normalization to handle noise. We
propose an efficient algorithm that handles noisy
9http://www.askmehelpdesk.com/
853
lexical and semantic variations.
3 Problem Formulation
We view the input SMS S as a sequence of tokens
S = s1, s2, . . . , sn. Let Q denote the set of ques-
tions in the FAQ corpus. Each question Q ? Q
is also viewed as a sequence of terms. Our goal
is to find the question Q? from the corpus Q that
best matches the SMS S. As mentioned in the in-
troduction, the SMS string is bound to have mis-
spellings and other distortions, which needs to be
taken care of while performing the match.
In the preprocessing stage, we develop a Do-
main dictionary D consisting of all the terms that
appear in the corpusQ. For each term t in the dic-
tionary and each SMS token si, we define a simi-
larity measure ?(t, si) that measures how closely
the term t matches the SMS token si. We say that
the term t is a variant of si, if ?(t, si) > 0; this is
denoted as t ? si. Combining the similarity mea-
sure and the inverse document frequency (idf) of t
in the corpus, we define a weight function ?(t, si).
The similarity measure and the weight function are
discussed in detail in Section 5.1.
Based on the weight function, we define a scor-
ing function for assigning a score to each question
in the corpus Q. The score measures how closely
the question matches the SMS string S. Consider
a question Q ? Q. For each token si, the scor-
ing function chooses the term from Q having the
maximum weight; then the weight of the n chosen
terms are summed up to get the score.
Score(Q) =
n?
i=1
[
max
t:t?Q and t?si
?(t, si)
]
(1)
Our goal is to efficiently find the question Q? hav-
ing the maximum score.
4 Pruning Algorithm
We now describe algorithms for computing the
maximum scoring question Q?. For each token
si, we create a list Li consisting of all terms from
the dictionary that are variants of si. Consider a
token si. We collect all the variants of si from the
dictionary and compute their weights. The vari-
ants are then sorted in the descending order of
their weights. At the end of the process we have n
ranked lists. As an illustration, consider an SMS
query ?gud plc buy 10s strng on9?. Here, n = 6
and six lists of variants will be created as shown
Figure 1: Ranked List of Variations
in Figure 1. The process of creating the lists is
speeded up using suitable indices, as explained in
detail in Section 5.
Now, we assume that the lists L1, L2, . . . , Ln
are created and explain the algorithms for com-
puting the maximum scoring question Q?. We de-
scribe two algorithms for accomplishing the above
task. The two algorithms have the same function-
ality i.e. they compute Q?, but the second algo-
rithm called the Pruning algorithm has a better
run time efficiency compared to the first algorithm
called the naive algorithm. Both the algorithms re-
quire an index which takes as input a term t from
the dictionary and returns Qt, the set of all ques-
tions in the corpus that contain the term t. We
call the above process as querying the index on
the term t. The details of the index creation is dis-
cussed in Section 5.2.
Naive Algorithm: In this algorithm, we scan
each list Li and query the index on each term ap-
pearing in Li. The returned questions are added to
a collection C. That is,
C =
n?
i=1
?
?
?
t?Li
Qt
?
?
The collection C is called the candidate set. No-
tice that any question not appearing in the candi-
date set has a score 0 and thus can be ignored. It
follows that the candidate set contains the maxi-
mum scoring question Q?. So, we focus on the
questions in the collection C, compute their scores
and find the maximum scoring question Q?. The
scores of the question appearing in C can be com-
puted using Equation 1.
The main disadvantage with the naive algorithm
is that it queries each term appearing in each list
and hence, suffers from high run time cost. We
next explain the Pruning algorithm that avoids this
pitfall and queries only a substantially small subset
of terms appearing in the lists.
Pruning Algorithm: The pruning algorithm
854
is inspired by the Threshold Algorithm (Fagin et
al., 2001). The Pruning algorithm has the prop-
erty that it queries fewer terms and ends up with
a smaller candidate set as compared to the naive
algorithm. The algorithm maintains a candidate
set C of questions that can potentially be the max-
imum scoring question. The algorithm works in
an iterative manner. In each iteration, it picks
the term that has maximum weight among all the
terms appearing in the lists L1, L2, . . . , Ln. As
the lists are sorted in the descending order of the
weights, this amounts to picking the maximum
weight term amongst the first terms of the n lists.
The chosen term t is queried to find the setQt. The
set Qt is added to the candidate set C. For each
question Q ? Qt, we compute its score Score(Q)
and keep it along with Q. The score can be com-
puted by Equation 1 (For each SMS token si, we
choose the term from Q which is a variant of si
and has the maximum weight. The sum of the
weights of chosen terms yields Score(Q)). Next,
the chosen term t is removed from the list. Each
iteration proceeds as above. We shall now develop
a thresholding condition such that when it is sat-
isfied, the candidate set C is guaranteed to contain
the maximum scoring questionQ?. Thus, once the
condition is met, we stop the above iterative pro-
cess and focus only on the questions in C to find
the maximum scoring question.
Consider end of some iteration in the above pro-
cess. Suppose Q is a question not included in C.
We can upperbound the score achievable by Q, as
follows. At best, Q may include the top-most to-
ken from every list L1, L2, . . . , Ln. Thus, score of
Q is bounded by
Score(Q) ?
n?
i=0
?(Li[1]).
(Since the lists are sorted Li[1] is the term having
the maximum weight in Li). We refer to the RHS
of the above inequality as UB.
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set
C cannot be the maximum scoring question. Thus,
the condition ?Q? ? UB? serves as the termination
condition. At the end of each iteration, we check
if the termination condition is satisfied and if so,
we can stop the iterative process. Then, we simply
pick the question in C having the maximum score
and return it. The algorithm is shown in Figure 2.
In this section, we presented the Pruning algo-
Procedure Pruning Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?.
Begin
Construct lists L1, L2, . . . , Ln //(see Section 5.3).
// Li lists variants of si in descending
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(Li[1])
t? = Lj? [1]
// t? is the term having maximum weight among
// all terms appearing in the n lists.
Delete t? from the list Lj? .
Query the index and fetch Qt?
// Qt? : the set of all questions inQ
//having the term t?
For each Q ? Qt?
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(Li[1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 2: Pruning Algorithm
rithm that efficiently finds the best matching ques-
tion for the given SMS query without the need to
go through all the questions in the FAQ corpus.
The next section describes the system implemen-
tation details of the Pruning Algorithm.
5 System Implementation
In this section we describe the weight function,
the preprocessing step and the creation of lists
L1, L2, . . . , Ln.
5.1 Weight Function
We calculate the weight for a term t in the dic-
tionary w.r.t. a given SMS token si. The weight
function is a combination of similarity measure
between t and si and Inverse Document Frequency
(idf) of t. The next two subsections explain the
calculation of the similarity measure and the idf in
detail.
5.1.1 Similarity Measure
Let D be the dictionary of all the terms in the cor-
pus Q. For term t ? D and token si of the SMS,
the similarity measure ?(t, si) between them is
855
?(t, si) =
?
????
????
LCSRatio(t,si)
EditDistanceSMS(t,si)
if t and si share same
starting character *
0 otherwise
(2)
where LCSRatio(t, si) =
length(LCS(t,si))
length(t) and LCS(t, si) is
the Longest common subsequence between t and si.
* The rationale behind this heuristic is that while typing a SMS, people
typically type the first few characters correctly. Also, this heuristic helps limit
the variants possible for a given token.
The Longest Common Subsequence Ratio
(LCSR) (Melamed, 1999) of two strings is the ra-
tio of the length of their LCS and the length of the
longer string. Since in SMS text, the dictionary
term will always be longer than the SMS token,
the denominator of LCSR is taken as the length of
the dictionary term. We call this modified LCSR
as the LCSRatio.
Procedure EditDistanceSMS
Input: term t, token si
Output: Consonant Skeleton Edit distance
Begin
return LevenshteinDistance(CS(si), CS(t)) + 1
// 1 is added to handle the case where
// Levenshtein Distance is 0
End
Consonant Skeleton Generation (CS)
1. remove consecutive repeated characters
// (call? cal)
2. remove all vowels
//(waiting ? wtng, great? grt)
Figure 3: EditDistanceSMS
The EditDistanceSMS shown in Figure 3
compares the Consonant Skeletons (Prochasson et
al., 2007) of the dictionary term and the SMS to-
ken. If the consonant keys are similar, i.e. the Lev-
enshtein distance between them is less, the simi-
larity measure defined in Equation 2 will be high.
We explain the rationale behind using the
EditDistanceSMS in the similarity measure
?(t, si) through an example. For the SMS
token ?gud? the most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result
the similarity measure between ?gud? and ?good?
will be higher than that of ?gud? and ?guided?.
5.1.2 Inverse Document Frequency
If f number of documents in corpus Q contain a
term t and the total number of documents in Q is
N, the Inverse Document Frequency (idf) of t is
idf(t) = log
N
f
(3)
Combining the similarity measure and the idf
of t in the corpus, we define the weight function
?(t, si) as
?(t, si) = ?(t, si) ? idf(t) (4)
The objective behind the weight function is
1. We prefer terms that have high similarity
measure i.e. terms that are similar to the
SMS token. Higher the LCSRatio and lower
the EditDistanceSMS , higher will be the
similarity measure. Thus for example, for a
given SMS token ?byk?, similarity measure
of word ?bike? is higher than that of ?break?.
2. We prefer words that are highly discrimi-
native i.e. words with a high idf score.
The rationale for this stems from the fact
that queries, in general, are composed of in-
formative words. Thus for example, for a
given SMS token ?byk?, idf of ?bike? will
be more than that of commonly occurring
word ?back?. Thus, even though the similar-
ity measure of ?bike? and ?back? are same
w.r.t. ?byk?, ?bike? will get a higher weight
than ?back? due to its idf.
We combine these two objectives into a single
weight function multiplicatively.
5.2 Preprocessing
Preprocessing involves indexing of the FAQ cor-
pus, formation of Domain and Synonym dictionar-
ies and calculation of the Inverse Document Fre-
quency for each term in the Domain dictionary.
As explained earlier the Pruning algorithm re-
quires retrieval of all questions Qt that contains a
given term t. To do this efficiently we index the
FAQ corpus using Lucene10. Each question in the
FAQ corpus is treated as a Document; it is tok-
enized using whitespace as delimiter and indexed.
10http://lucene.apache.org/java/docs/
856
The Domain dictionaryD is built from all terms
that appear in the corpus Q.
The weight calculation for Pruning algorithm
requires the idf for a given term t. For each term t
in the Domain dictionary, we query the Lucene in-
dexer to get the number of Documents containing
t. Using Equation 3, the idf(t) is calculated. The
idf for each term t is stored in a Hashtable, with t
as the key and idf as its value.
Another key step in the preprocessing stage is
the creation of the Synonym dictionary. The Prun-
ing algorithm uses this dictionary to retrieve se-
mantically similar questions. Details of this step is
further elaborated in the List Creation sub-section.
The Synonym Dictionary creation involves map-
ping each word in the Domain dictionary to it?s
corresponding Synset obtained from WordNet11.
5.3 List Creation
Given a SMS S, it is tokenized using white-spaces
to get a sequence of tokens s1, s2, . . . , sn. Digits
occurring in SMS token (e.g ?10s? , ?4get?) are re-
placed by string based on a manually crafted digit-
to-string mapping (?10? ? ?ten?). A list Li is
setup for each token si using terms in the domain
dictionary. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop
word . A term t from domain dictionary is in-
cluded in Li if its first character is same as that of
the token si and it satisfies the threshold condition
length(LCS(t, si)) > 1.
Each term t that is added to the list is assigned a
weight given by Equation 4.
Terms in the list are ranked in descending or-
der of their weights. Henceforth, the term ?list?
implies a ranked list.
For example the SMS query ?gud plc 2 buy 10s
strng on9? (corresponding question ?Where is a
good place to buy tennis strings online??), is to-
kenized to get a set of tokens {?gud?, ?plc?, ?2?,
?buy?, ?10s?, ?strng?, ?on9?}. Single character to-
kens such as ?2? are neglected as they are most
likely to be stop words. From these tokens cor-
responding lists are setup as shown in Figure 1.
5.3.1 Synonym Dictionary Lookup
To retrieve answers for SMS queries that are
semantically similar but lexically different from
questions in the FAQ corpus we use the Synonym
dictionary described in Section 5.2. Figure 4 illus-
trates some examples of such SMS queries.
11http://wordnet.princeton.edu/
Figure 4: Semantically similar SMS and questions
Figure 5: Synonym Dictionary LookUp
For a given SMS token si, the list of variations
Li is further augmented using this Synonym dic-
tionary. For each token si a fuzzy match is per-
formed between si and the terms in the Synonym
dictionary and the best matching term from the
Synonym dictionary, ? is identified. As the map-
pings between the Synonym and the Domain dic-
tionary terms are maintained, we obtain the corre-
sponding Domain dictionary term ? for the Syn-
onym term ? and add that term to the list Li. ? is
assigned a weight given by
?(?, si) = ?(?, si) ? idf(?) (5)
It should be noted that weight for ? is based on
the similarity measure between Synonym dictio-
nary term ? and SMS token si.
For example, the SMS query ?hw2 countr quik
srv?( corresponding question ?How to return a
very fast serve??) has two terms ?countr? ?
?counter? and ?quik? ? ?quick? belonging to
the Synonym dictionary. Their associated map-
pings in the Domain dictionary are ?return? and
?fast? respectively as shown in Figure 5. During
the list setup process the token ?countr? is looked
857
up in the Domain dictionary. Terms from the Do-
main dictionary that begin with the same character
as that of the token ?countr? and have a LCS > 1
such as ?country?,?count?, etc. are added to the
list and assigned a weight given by Equation 4.
After that, the token ?countr? is looked up in the
Synonym dictionary using Fuzzy match. In this
example the term ?counter? from the Synonym
dictionary fuzzy matches the SMS token. The Do-
main dictionary term corresponding to the Syn-
onym dictionary term ?counter? is looked up and
added to the list. In the current example the cor-
responding Domain dictionary term is ?return?.
This term is assigned a weight given by Equation
5 and is added to the list as shown in Figure 5.
5.4 FAQ retrieval
Once the lists are created, the Pruning Algorithm
as shown in Figure 2 is used to find the FAQ ques-
tionQ? that best matches the SMS query. The cor-
responding answer to Q? from the FAQ corpus is
returned to the user.
The next section describes the experimental
setup and results.
6 Experiments
We validated the effectiveness and usability of
our system by carrying out experiments on two
FAQ data sets. The first FAQ data set, referred
to as the Telecom Data-Set, consists of 1500 fre-
quently asked questions, collected from a Telecom
service provider?s website. The questions in this
data set are related to the Telecom providers prod-
ucts or services. For example queries about call
rates/charges, bill drop locations, how to install
caller tunes, how to activate GPRS etc. The sec-
ond FAQ corpus, referred to as the Yahoo DataSet,
consists of 7500 questions from three Yahoo!
Answers12 categories namely Sports.Swimming,
Sports.Tennis, Sports.Running.
To measure the effectiveness of our system, a
user evaluation study was performed. Ten human
evaluators were asked to choose 10 questions ran-
domly from the FAQ data set. None of the eval-
uators were authors of the paper. They were pro-
vided with a mobile keypad interface and asked to
?text? the selected 10 questions as SMS queries.
Through that exercise 100 relevant SMS queries
per FAQ data set were collected. Figure 6 shows
sample SMS queries. In order to validate that the
system was able to handle queries that were out of
12http://answers.yahoo.com/
Figure 6: Sample SMS queries
Data Set Relevant Queries Irrelevant Queries
Telecom 100 50
Yahoo 100 50
Table 1: SMS Data Set.
the FAQ domain, we collected 5 irrelevant SMS
queries from each of the 10 human-evaluators for
both the data sets. Irrelevant queries were (a)
Queries out of the FAQ domain e.g. queries re-
lated to Cricket, Billiards, activating GPS etc (b)
Absurd queries e.g. ?ama ameyu tuem? (sequence
of meaningless words) and (c) General Queries
e.g. ?what is sports?. Table 1 gives the number
of relevant and irrelevant queries used in our ex-
periments.
The average word length of the collected SMS
messages for Telecom and Yahoo datasets was 4
and 7 respectively. We manually cleaned the SMS
query data word by word to create a clean SMS
test-set. For example, the SMS query ?h2 mke a
pdl bke fstr? was manually cleaned to get ?how
to make pedal bike faster?. In order to quantify
the level of noise in the collected SMS data, we
built a character-level language model(LM)13 us-
ing the questions in the FAQ data-set (vocabulary
size is 44 characters) and computed the perplex-
ity14 of the language model on the noisy and the
cleaned SMS test-set. The perplexity of the LM on
a corpus gives an indication of the average num-
ber of bits needed per n-gram to encode the cor-
pus. Noise will result in the introduction of many
previously unseen n-grams in the corpus. Higher
number of bits are needed to encode these improb-
able n-grams which results in increased perplexity.
From Table 2 we can see the difference in perplex-
ity for noisy and clean SMS data for the Yahoo
and Telecom data-set. The high level of perplexity
in the SMS data set indicates the extent of noise
present in the SMS corpus.
To handle irrelevant queries the algorithm de-
scribed in Section 4 is modified. Only if the
Score(Q?) is above a certain threshold, it?s answer
is returned, else we return ?null?. The threshold
13http://en.wikipedia.org/wiki/Language model
14bits = log2(perplexity)
858
Cleaned SMS Noisy SMS
Yahoo bigram 14.92 74.58trigram 8.11 93.13
Telecom bigram 17.62 59.26trigram 10.27 63.21
Table 2: Perplexity for Cleaned and Noisy SMS
Figure 7: Accuracy on Telecom FAQ Dataset
was determined experimentally.
To retrieve the correct answer for the posed
SMS query, the SMS query is matched against
questions in the FAQ data set and the best match-
ing question(Q?) is identified using the Pruning al-
gorithm. The system then returns the answer to
this best matching question to the human evalua-
tor. The evaluator then scores the response on a bi-
nary scale. A score of 1 is given if the returned an-
swer is the correct response to the SMS query, else
it is assigned 0. The scoring procedure is reversed
for irrelevant queries i.e. a score of 0 is assigned
if the system returns an answer and 1 is assigned
if it returns ?null? for an ?irrelevant? query. The
result of this evaluation on both data-sets is shown
in Figure 7 and 8.
Figure 8: Accuracy on Yahoo FAQ Dataset
In order to compare the performance of our sys-
tem, we benchmark our results against Lucene?s
15 Fuzzy match feature. Lucene supports fuzzy
searches based on the Levenshtein Distance, or
Edit Distance algorithm. To do a fuzzy search
15http://lucene.apache.org
we specify the ? symbol at the end of each to-
ken of the SMS query. For example, the SMS
query ?romg actvt? on the FAQ corpus is refor-
mulated as ?romg? 0.3 actvt? 0.3?. The param-
eter after the ? specifies the required similarity.
The parameter value is between 0 and 1, with a
value closer to 1 only terms with higher similar-
ity will be matched. These queries are run on the
indexed FAQs. The results of this evaluation on
both data-sets is shown in Figure 7 and 8. The
results clearly demonstrate that our method per-
forms 2 to 2.5 times better than Lucene?s Fuzzy
match. It was observed that with higher values
of similarity parameter (? 0.6, ? 0.8), the num-
ber of correctly answered queries was even lower.
In Figure 9 we show the runtime performance of
the Naive vs Pruning algorithm on the Yahoo FAQ
Dataset for 150 SMS queries. It is evident from
Figure 9 that not only does the Pruning Algorithm
outperform the Naive one but also gives a near-
constant runtime performance over all the queries.
The substantially better performance of the Prun-
ing algorithm is due to the fact that it queries much
less number of terms and ends up with a smaller
candidate set compared to the Naive algorithm.
Figure 9: Runtime of Pruning vs Naive Algorithm
for Yahoo FAQ Dataset
7 Conclusion
In recent times there has been a rise in SMS based
QA services. However, automating such services
has been a challenge due to the inherent noise in
SMS language. In this paper we gave an efficient
algorithm for answering FAQ questions over an
SMS interface. Results of applying this on two
different FAQ datasets shows that such a system
can be very effective in automating SMS based
FAQ retrieval.
859
References
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Design-
ing the User Experience for Google SMS. CHI,
Portland, Oregon.
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Inter-
face to Yellow Pages Directory, In Proceedings of
the 4th International conference on mobile technol-
ogy, applications, and systems and the 1st Interna-
tional symposium on Computer human interaction
in mobile technology, Singapore.
Monojit Choudhury, Rahul Saraf, Sudeshna Sarkar, Vi-
jit Jain, and Anupam Basu. 2007. Investigation and
Modeling of the Structure of Texting Language, In
Proceedings of IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data, Hyderabad.
E. Voorhees. 1999. The TREC-8 question answering
track report.
D. Molla. 2003. NLP for Answer Extraction in Tech-
nical Domains, In Proceedings of EACL, USA.
E. Sneiders. 2002. Automated question answering
using question templates that cover the conceptual
model of the database, In Proceedings of NLDB,
pages 235?239.
B. Katz, S. Felshin, D. Yuret, A. Ibrahim, J. Lin, G.
Marton, and B. Temelkuran. 2002. Omnibase: Uni-
form access to heterogeneous data for question an-
swering, Natural Language Processing and Infor-
mation Systems, pages 230?234.
E. Sneiders. 1999. Automated FAQ Answering: Con-
tinued Experience with Shallow Language Under-
standing, Question Answering Systems. Papers from
the 1999 AAAI Fall Symposium. Technical Report
FS-99?02, November 5?7, North Falmouth, Mas-
sachusetts, USA, AAAI Press, pp.97?107
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007.
Question similarity calculation for FAQ answering,
In Proceeding of SKG 07, pages 298?301.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization, In Proceedings of COLING/ACL, pages
33?40.
Catherine Kobus, Franois Yvon and Graldine Damnati.
2008. Normalizing SMS: are two metaphors bet-
ter than one?, In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 441?448 Manchester.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement, Association for the Ad-
vancement of Artificial Intelligence. AAAI Workshop
on Enhanced Messaging
Ronald Fagin , Amnon Lotem , Moni Naor. 2001.
Optimal aggregation algorithms for middleware, In
Proceedings of the 20th ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database sys-
tems.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition, Computational Linguistics.
E. Prochasson, Christian Viard-Gaudin, Emmanuel
Morin. 2007. Language Models for Handwritten
Short Message Services, In Proceedings of the 9th
International Conference on Document Analysis and
Recognition.
Sreangsu Acharya, Sumit Negi, L. V. Subramaniam,
Shourya Roy. 2008. Unsupervised learning of mul-
tilingual short message service (SMS) dialect from
noisy examples, In Proceedings of the second work-
shop on Analytics for noisy unstructured text data.
860
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 737?744,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Generation of Domain Models for Call Centers from Noisy
Transcriptions
Shourya Roy and L Venkata Subramaniam
IBM Research
India Research Lab
IIT Delhi, Block-1
New Delhi 110016
India
rshourya,lvsubram@in.ibm.com
Abstract
Call centers handle customer queries from various
domains such as computer sales and support, mo-
bile phones, car rental, etc. Each such domain
generally has a domain model which is essential
to handle customer complaints. These models
contain common problem categories, typical cus-
tomer issues and their solutions, greeting styles.
Currently these models are manually created over
time. Towards this, we propose an unsupervised
technique to generate domain models automati-
cally from call transcriptions. We use a state of
the art Automatic Speech Recognition system to
transcribe the calls between agents and customers,
which still results in high word error rates (40%)
and show that even from these noisy transcrip-
tions of calls we can automatically build a domain
model. The domain model is comprised of pri-
marily a topic taxonomy where every node is char-
acterized by topic(s), typical Questions-Answers
(Q&As), typical actions and call statistics. We
show how such a domain model can be used for
topic identification of unseen calls. We also pro-
pose applications for aiding agents while handling
calls and for agent monitoring based on the do-
main model.
1 Introduction
Call center is a general term for help desks, infor-
mation lines and customer service centers. Many
companies today operate call centers to handle
customer issues. It includes dialog-based (both
voice and online chat) and email support a user
receives from a professional agent. Call centers
have become a central focus of most companies as
they allow them to be in direct contact with their
customers to solve product-related and services-
related issues and also for grievance redress. A
typical call center agent handles over a hundred
calls in a day. Gigabytes of data is produced ev-
ery day in the form of speech audio, speech tran-
scripts, email, etc. This data is valuable for doing
analysis at many levels, e.g., to obtain statistics
about the type of problems and issues associated
with different products and services. This data can
also be used to evaluate agents and train them to
improve their performance.
Today?s call centers handle a wide variety of do-
mains such as computer sales and support, mobile
phones and apparels. To analyze the calls in any
domain, analysts need to identify the key issues
in the domain. Further, there may be variations
within a domain, say mobile phones, based on the
service providers. The analysts generate a domain
model through inspection of the call records (au-
dio, transcripts and emails). Such a model can in-
clude a listing of the call categories, types of prob-
lems solved in each category, listing of the cus-
tomer issues, typical questions-answers, appropri-
ate call opening and closing styles, etc. In essence,
these models provide a structured view of the do-
main. Manually building such models for vari-
ous domains may become prohibitively resource
intensive. Another important point to note is that
these models are dynamic in nature and change
over time. As a new version of a mobile phone
is introduced, software is launched in a country, a
sudden attack of a virus, the model may need to be
refined. Hence, an automated way of creating and
maintaining such a model is important.
In this paper, we have tried to formalize the es-
sential aspects of a domain model. It comprises
of primarily a topic taxonomy where every node
is characterized by topic(s), typical Questions-
737
Answers (Q&As), typical actions and call statis-
tics. To build the model, we first automatically
transcribe the calls. Current automatic speech
recognition technology for telephone calls have
moderate to high word error rates (Padmanabhan
et al, 2002). We applied various feature engi-
neering techniques to combat the noise introduced
by the speech recognition system and applied text
clustering techniques to group topically similar
calls together. Using clustering at different gran-
ularity and identifying the relationship between
groups at different granularity we generate a tax-
onomy of call types. This taxonomy is augmented
with various meta information related to each node
as mentioned above. Such a model can be used
for identification of topics of unseen calls. To-
wards this, we envision an aiding tool for agents
to increase agent effectiveness and an administra-
tive tool for agent appraisal and training.
Organization of the paper: We start by de-
scribing related work in relevant areas. Section 3
talks about the call center dataset and the speech
recognition system used. The following section
contains the definition and describes an unsuper-
vised mechanism for building a topical model
from automatically transcribed calls. Section 5
demonstrates the usability of such a topical model
and proposes possible applications. Section 6 con-
cludes the paper.
2 Background and Related Work
In this work, we are trying to bridge the gap be-
tween a few seemingly unrelated research areas
viz. (1) Automatic Speech Recognition(ASR), (2)
Text Clustering and Automatic Taxonomy Gener-
ation (ATG) and (3) Call Center Analytics. We
present some relevant work done in each of these
areas.
Automatic Speech Recognition(ASR): Auto-
matic transcription of telephonic conversations is
proven to be more difficult than the transcription
of read speech. According to (Padmanabhan et
al., 2002), word-error rates are in the range of 7-
8% for read speech whereas for telephonic speech
it is more than 30%. This degradation is due
to the spontaneity of speech as well as the tele-
phone channel. Most speech recognition systems
perform well when trained for a particular accent
(Lawson et al, 2003). However, with call cen-
ters now being located in different parts of the
world, the requirement of handling different ac-
cents by the same speech recognition system fur-
ther increases word error rates.
Automatic Taxonomy Generation (ATG): In re-
cent years there has been some work relating to
mining domain specific documents to build an on-
tology. Mostly these systems rely on parsing (both
shallow and deep) to extract relationships between
key concepts within the domain. The ontology is
constructed from this by linking the extracted con-
cepts and relations (Jiang and Tan, 2005). How-
ever, the documents contain well formed sentences
which allow for parsers to be used.
Call Center Analytics: A lot of work on auto-
matic call type classification for the purpose of
categorizing calls (Tang et al, 2003), call rout-
ing (Kuo and Lee, 2003; Haffner et al, 2003), ob-
taining call log summaries (Douglas et al, 2005),
agent assisting and monitoring (Mishne et al,
2005) has appeared in the past. In some cases, they
have modeled these as text classification problems
where topic labels are manually obtained (Tang et
al., 2003) and used to put the calls into different
buckets. Extraction of key phrases, which can be
used as features, from the noisy transcribed calls
is an important issue. For manually transcribed
calls, which do not have any noise, in (Mishne et
al., 2005) a phrase level significance estimate is
obtained by combining word level estimates that
were computed by comparing the frequency of a
word in a domain-specific corpus to its frequency
in an open-domain corpus. In (Wright et al, 1997)
phrase level significance was obtained for noisy
transcribed data where the phrases are clustered
and combined into finite state machines. Other
approaches use n-gram features with stop word re-
moval and minimum support (Kuo and Lee, 2003;
Douglas et al, 2005). In (Bechet et al, 2004) call
center dialogs have been clustered to learn about
dialog traces that are similar.
Our Contribution: In the call center scenario, the
authors are not aware of any work that deals with
automatically generating a taxonomy from tran-
scribed calls. In this paper, we have tried to for-
malize the essential aspects of a domain model.
We show an unsupervised method for building a
domain model from noisy unlabeled data, which is
available in abundance. This hierarchical domain
model contains summarized topic specific details
for topics of different granularity. We show how
such a model can be used for topic identification
of unseen calls. We propose two applications for
738
aiding agents while handling calls and for agent
monitoring based on the domain model.
3 Issues with Call Center Data
We obtained telephonic conversation data col-
lected from the internal IT help desk of a com-
pany. The calls correspond to users making spe-
cific queries regarding problems with computer
software such as Lotus Notes, Net Client, MS Of-
fice, MS Windows, etc. Under these broad cate-
gories users faced specific problems e.g. in Lotus
Notes users had problems with their passwords,
mail archiving, replication, installation, etc. It is
possible that many of the sub problem categories
are similar, e.g. password issues can occur with
Lotus Notes, Net Client and MS Windows.
We obtained automatic transcriptions of the di-
alogs using an Automatic Speech Recognition
(ASR) system. The transcription server, used for
transcribing the call center data, is an IBM re-
search prototype. The speech recognition system
was trained on 300 hours of data comprising of
help desk calls sampled at 6KHz. The transcrip-
tion output comprises information about the rec-
ognized words along with their durations, i.e., be-
ginning and ending times of the words. Further,
speaker turns are marked, so the agent and cus-
tomer portions of speech are demarcated without
exactly naming which part is the agent and which
the customer. It should be noted that the call cen-
ter agents and the customers were of different na-
tionalities having varied accents and this further
made the job of the speech recognizer hard. The
resultant transcriptions have a word error rate of
about 40%. This high error rate implies that many
wrong deletions of actual words and wrong inser-
tion of dictionary words have taken place. Also
often speaker turns are not correctly identified and
voice portions of both speakers are assigned to a
single speaker. Apart from speech recognition er-
rors there are other issues related to spontaneous
speech recognition in the transcriptions. There are
no punctuation marks, silence periods are marked
but it is not possible to find sentence boundaries
based on these. There are repeats, false starts, a
lot of pause filling words such as um and uh, etc.
Portion of a transcribed call is shown in figure 1.
Generally, at these noise levels such data is hard
to interpret by a human. We used over 2000 calls
that have been automatically transcribed for our
analysis. The average duration of a call is about 9
SPEAKER 1: windows thanks for calling and you can
learn yes i don?t mind it so then i went to
SPEAKER 2: well and ok bring the machine front
end loaded with a standard um and that?s um it?s
a desktop machine and i did that everything was
working wonderfully um I went ahead connected
into my my network um so i i changed my network
settings to um to my home network so i i can you
know it?s showing me for my workroom um and then
it is said it had to reboot in order for changes
to take effect so i rebooted and now it?s asking
me for a password which i never i never said
anything up
SPEAKER 1: ok just press the escape key i can
doesn?t do anything can you pull up so that i mean
Figure 1: Partial transcript of a help desk dialog
minutes. For 125 of these calls, call topics were
manually assigned.
4 Generation of Domain Model
Fig 2 shows the steps for generating a domain
model in the call center scenario. This section ex-
plains different modules shown in the figure.
4.1 Description of Model
We propose the Domain Model to be comprised
of primarily a topic taxonomy where every node
is characterized by topic(s), typical Questions-
Answers (Q&As), typical actions and call statis-
tics. Generating such a taxonomy manually from
scratch requires significant effort. Further, the
changing nature of customer problems requires
frequent changes to the taxonomy. In the next sub-
section, we show that meaningful taxonomies can
be built without any manual supervision from a
collection of noisy call transcriptions.
4.2 Taxonomy Generation
As mentioned in section 3, automatically tran-
scribed data is noisy and requires a good amount
of feature engineering before applying any text
analytics technique. Each transcription is passed
through a Feature Engineering Component to per-
form noise removal. We performed a sequence of
cleansing operations to remove stopwords such as
the, of, seven, dot, january, hello. We also remove
pause filling words such as um, uh, huh . The re-
maining words in every transcription are passed
through a stemmer (using Porter?s stemming algo-
739
Stopword
Removal
N-gram
Extraction
Datab
ase,
 
archiv
e, 
replic
ate
Ca
n 
you
 
ac
ce
ss
 
yah
oo
? 
Is m
ode
m 
on
?
Call
 statistics
Feature Engineering
ASR
Clusterer TaxonomyBuilder
Model
Builder
Component
Clusters of different 
granularity
Voice help-desk data
1
2
3 4
5
Figure 2: 5 Steps to automatically build domain model from a collection of telephonic conversation
recordings
rithm 1) to extract the root form of every word e.g.
call from called. We extract all n-grams which
occur more frequently than a threshold and do not
contain any stopword. We observed that using
all n-grams without thresholding deteriorates the
quality of the generated taxonomy. a t & t, lotus
notes, and expense reimbursement are some exam-
ples of extracted n-grams.
The Clusterer generates individual levels of
the taxonomy by using text clustering. We used
CLUTO package 2 for doing text clustering. We
experimented with all the available clustering
functions in CLUTO but no one clustering al-
gorithm consistently outperformed others. Also,
there was not much difference between various
algorithms based on the available goodness met-
rics. Hence, we used the default repeated bisec-
tion technique with cosine function as the similar-
ity metric. We ran this algorithm on a collection
of 2000 transcriptions multiple times. First we
generate 5 clusters from the 2000 transcriptions.
Next we generate 10 clusters from the same set
of transcriptions and so on. At the finest level we
split them into 100 clusters. To generate the topic
1http://www.tartarus.org/?martin/PorterStemmer
2http://glaros.dtc.umn.edu/gkhome/views/cluto
taxonomy, these sets containing 5 to 100 clusters
are passed through the Taxonomy Builder compo-
nent. This component (1) removes clusters con-
taining less than n documents (2) introduces di-
rected edges from cluster v1 to v2 if v1 and v2
share at least one document between them, and
where v2 is one level finer than v1. Now v1 and v2
become nodes in adjacent layers in the taxonomy.
Here we found the taxonomy to be a tree but in
general it can be a DAG. Now onwards, each node
in the taxonomy will be referred to as a topic.
This kind of top-down approach was preferred
over a bottom-up approach because it not only
gives the linkage between clusters of various gran-
ularity but also gives the most descriptive and dis-
criminative set of features associated with each
node. CLUTO defines descriptive (and discrimi-
native) features as the set of features which con-
tribute the most to the average similarity (dissim-
ilarity) between documents belonging to the same
cluster (different clusters). In general, there is a
large overlap between descriptive and discrimina-
tive features. These features, topic features, are
later used for generating topic specific informa-
tion. Figure 3 shows a part of the taxonomy ob-
tained from the IT help desk dataset. The labels
740
atandt
connect lotusnot
click client
connect
wireless
network
default
properti
net
netclient
localarea
areaconnect
router
cabl
databas
server folder
copi archiv
replic
mail
slash
folder
file
archiv
databas
servercopi
localcopi
Figure 3: A part of the automatically generated
ontology along with descriptive features.
shown in Figure 3 are the most descriptive and dis-
criminative features of a node given the labels of
its ancestors.
4.3 Topic Specific Information
The Model Builder component in Figure 2 creates
an augmented taxonomy with topic specific infor-
mation extracted from noisy transcriptions. Topic
specific information includes phrases that describe
typical actions, typical Q&As and call statistics
(for each topic in the taxonomy).
Typical Actions: Actions correspond to typical is-
sues raised by the customer, problems and strate-
gies for solving them. We observed that action re-
lated phrases are mostly found around topic fea-
tures. Hence, we start by searching and collect-
ing all the phrases containing topic words from
the documents belonging to the topic. We define
a 10-word window around the topic features and
harvest all phrases from the documents. The set
of collected phrases are then searched for n-grams
with support above a preset threshold. For exam-
ple, both the 10-grams note in click button to set
up for all stops and to action settings and click the
button to set up increase the support count of the
5-gram click button to set up.
The search for the n-grams proceeds based on
a threshold on a distance function that counts the
insertions necessary to match the two phrases. For
example can you is closer to can < ... > you than
to can < ... >< ... > you. Longer n-grams are
allowed a higher distance threshold than shorter n-
grams. After this stage we extracted all the phrases
that frequently occur within the cluster.
In the second step, phrase tiling and ordering,
we prune and merge the extracted phrases and or-
der them. Tiling constructs longer n-grams from
sequences of overlapping shorter n-grams. We
noted that the phrases have more meaning if they
are ordered by their appearance. For example, if
go to the program menu typically appears before
select options from program menu then it is more
thank you for calling this is
problem with our serial number software
Q: may i have your serial number
Q: how may i help you today
A: i?m having trouble with my at&t network
............
............
click on advance log in properties
i want you to right click
create a connection across an existing internet
connection
in d. n. s. use default network
............
............
Q: would you like to have your ticket
A: ticket number is two
thank you for calling and have a great day
thank you for calling bye bye
anything else i can help you with
have a great day you too
Figure 4: Topic specific information
useful to present them in the order of their appear-
ance. We establish this order based on the average
turn number where a phrase occurs.
Typical Questions-Answers: To understand a
customer?s issue the agent needs to ask the right
set of questions. Asking the right questions is the
key to effective call handling. We search for all the
questions within a topic by defining question tem-
plates. The question templates basically look for
all phrases beginning with how, what, can I, can
you, were there, etc. This set comprised of 127
such templates for questions. All 10-word phrases
conforming to the question templates are collected
and phrase harvesting, tiling and ordering is done
on them as described above. For the answers we
search for phrases in the vicinity immediately fol-
lowing the question.
Figure 4 shows a part of the topic specific in-
formation that has been generated for the default
properti node in Fig 3. There are 123 documents
in this node. We have selected phrases that occur
at least 5 times in these 123 documents. We have
captured the general opening and closing styles
used by the agents in addition to typical actions
and Q&As for the topic. In this node the docu-
ments pertain to queries on setting up a new A T &
T network connection. Most of the topic specific
issues that have been captured relate to the agent
741
leading the customer through the steps for setting
up the connection. In the absence of tagged dataset
we could not quantify our observation. However,
when we compared the automatically generated
topic specific information to the extracted infor-
mation from the hand labeled calls, we noted that
almost all the issues have been captured. In fact
there are some issues in the automatically gener-
ated set that are missing from the hand labeled set.
The following observations can be made from the
topic specific information that has been generated:
? The phrases that have been captured turn out
to be quite well formed. Even though the
ASR system introduces a lot of noise, the re-
sulting phrases when collected over the clus-
ters are clean.
? Some phrases appear in multiple forms thank
you for calling how can i help you, how may
i help you today, thanks for calling can i
be of help today. While tiling is able to
merge matching phrases, semantically simi-
lar phrases are not merged.
? The list of topic specific phrases, as already
noted, matched and at times was more ex-
haustive than similar hand generated sets.
Call Statistics: We compute various aggregate
statistics for each node in the topic taxonomy as
part of the model viz. (1) average call duration(in
seconds), (2) average transcription length(number
of words) (3) average number of speaker turns and
(4) number of calls. We observed that call dura-
tions and number of speaker turns varies signifi-
cantly from one topic to another. Figure 5 shows
average call duration and corresponding average
transcription lengths for a few interesting topics. It
can be seen that in topic cluster-1, which is about
expense reimbursement and related stuff, most of
the queries can be answered quickly in standard
ways. However, some connection related issues
(topic cluster-5) require more information from
customers and are generally longer in duration. In-
terestingly, topic cluster-2 and topic cluster-4 have
similar average call durations but quite different
average transcription lengths. On investigation we
found that cluster-4 is primarily about printer re-
lated queries where the customer many a times is
not ready with details like printer name, ip address
of the printer, resulting in long hold time whereas
for cluster-2, which is about online courses, users
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
54321
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
Ca
ll
 D
ur
at
io
n(
se
cs
)
Tr
an
sc
ri
pt
io
n 
Le
ng
th
(n
o.
 o
f 
wo
rd
s)
Topic Cluster
Figure 5: Call duration and transcription length for
some topic clusters
generally have details like course name, etc. ready
with them and are interactive in nature.
We build a hierarchical index of type
{topic?information} based on this automat-
ically generated model for each topic in the topic
taxonomy. An entry of this index contains topic
specific information viz. (1) typical Q&As, (2)
typical actions, and (3) call statistics. As we
go down this hierarchical index the information
associated with each topic becomes more and
more specific. In (Mishne et al, 2005) a manually
developed collection of issues and their solutions
is indexed so that they can be matched to the
call topic. In our work the indexed collection is
automatically obtained from the call transcrip-
tions. Also, our index is more useful because of
its hierarchical nature where information can be
obtained for topics of various granularity unlike
(Mishne et al, 2005) where there is no concept of
topics at all.
5 Application of Domain Model
Information retrieval from spoken dialog data is an
important requirement for call centers. Call cen-
ters constantly endeavor to improve the call han-
dling efficiency and identify key problem areas.
The described model provides a comprehensive
and structured view of the domain that can be used
to do both. It encodes three levels of information
about the domain:
? General: The taxonomy along with the la-
bels gives a general view of the domain. The
general information can be used to monitor
trends on how the number of calls in differ-
ent categories change over time e.g. daily,
weekly, monthly.
742
? Topic level: This includes a listing of the spe-
cific issues related to the topic, typical cus-
tomer questions and problems, usual strate-
gies for solving the problems, average call
durations, etc. It can be used to identify pri-
mary issues, problems and solutions pertain-
ing to any category.
? Dialog level: This includes information on
how agents typically open and close calls, ask
questions and guide customers, average num-
ber of speaker turns, etc. The dialog level
information can be used to monitor whether
agents are using courteous language in their
calls, whether they ask pertinent questions,
etc.
The {topic?information} index requires iden-
tification of the topic for each call to make use
of information available in the model. Below we
show examples of the use of the model for topic
identification.
5.1 Topic Identification
Many of the customer complaints can be catego-
rized into coarse as well as fine topic categories
by listening to only the initial part of the call. Ex-
ploiting this observation we do fast topic identi-
fication using a simple technique based on distri-
bution of topic specific descriptive and discrimi-
native features (Sec 4.2) within the initial portion
of the call. Figure 6 shows variation in prediction
accuracy using this technique as a function of the
fraction of a call observed for 5, 10 and 25 clus-
ters verified over the 125 hand-labeled transcrip-
tions. It can be seen, at coarse level, nearly 70%
prediction accuracy can be achieved by listening
to the initial 30% of the call and more than 80% of
the calls can be correctly categorized by listening
only to the first half of the call. Also calls related
to some categories can be quickly detected com-
pared to some other clusters as shown in Figure 7.
5.2 Aiding and Administrative Tool
Using the techniques presented in this paper so far
it is possible to put together many applications for
a call center. In this section we give some exam-
ple applications and describe ways in which they
can be implemented. Based on the hierarchical
model described in Section 4 and topic identifica-
tion mentioned in the last sub-section we describe
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
100908070605040302010
Pr
ed
ic
ti
on
 a
cc
ur
ac
y(
%)
Fraction of call observed(%)
?5-Clusters?
?10-Clusters?
?25-Clusters?
Figure 6: Variation in prediction accuracy with
fraction of call observed for 5, 10 and 25 clusters
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
10987654321
Pr
ed
ic
ti
on
 a
cc
ur
ac
y(
%)
Cluster ID
25% observed
50% observed
75% observed
100% observed
Figure 7: Cluster wise variation in prediction ac-
curacy for 10 clusters
(1) a tool capable of aiding agents for efficient
handling of calls to improve customer satisfaction
as well as to reduce call handling time, (2) an ad-
ministrative tool for agent appraisal and training.
Agent aiding is done based on the automati-
cally generated domain model. The hierarchical
nature of the model helps to provide generic to
specific information to the agent as the call pro-
gresses. During call handling the agent can be
provided the automatically generated taxonomy
and the agent can get relevant information asso-
ciated with different nodes by say clicking on the
nodes. For example, once the agent identifies a
call to be about {lotusnot} in Fig 3 then he can
see the generic Lotus Notes related Q&As and ac-
tions. By interacting further with the customer the
agent identifies it to be of {copi archiv replic}
topic and typical Q&As and actions change ac-
cordingly. Finally, the agent narrows down to the
topic as {servercopi localcopi} and suggest solu-
tion for replication problem in Lotus Notes.
The concept of administrative tool is primar-
ily driven by Dialog and Topic level information.
We envision this post-processing tool to be used
743
for comparing completed individual calls with cor-
responding topics based on the distribution of
Q&As, actions and call statistics. Based on the
topic level information we can check whether the
agent identified the issues and offered the known
solutions on a given topic. We can use the dialog
level information to check whether the agent used
courteous opening and closing sentences. Calls
that deviate from the topic specific distributions,
can be identified in this way and agents handling
these calls can be offered further training on the
subject matter, courtesy, etc. This kind of post-
processing tool can also help us to catch abnor-
mally long calls, agents with high average call
handle time, etc.
6 Discussion and Future Work
We have shown that it is possible to retrieve use-
ful information from noisy transcriptions of call
center voice conversations. We have shown that
the extracted information can be put in the form of
a model that succinctly captures the domain and
provides a comprehensive view of it. We briefly
showed through experiments that this model is an
accurate description of the domain. We have also
suggested useful scenarios where the model can be
used to aid and improve call center performance.
A call center handles several hundred-thousand
calls per year in various domains. It is very diffi-
cult to monitor the performance based on manual
processing of the calls. The framework presented
in this paper, allows a large part of this work
to be automated. A domain specific model that
is automatically learnt and updated based on the
voice conversations allows the call center to iden-
tify problem areas quickly and allocate resources
more effectively.
In future we would like to semantically clus-
ter the topic specific information so that redundant
topics are eliminated from the list. We can use Au-
tomatic Taxonomy Generation(ATG) algorithms
for document summarization (Kummamuru et al,
2004) to build topic taxonomies. We would also
like to link our model to technical manuals, cata-
logs, etc. already available on the different topics
in the given domain.
Acknowledgements: We thank our colleagues
Raghuram Krishnapuram and Sreeram Balakrish-
nan for helpful discussions. We also thank Olivier
Siohan from the IBM T. J. Watson Research Cen-
ter for providing us with call transcriptions.
References
F. Bechet, G. Riccardi and D. Hakkani-Tur 2004. Min-
ing Spoken Dialogue Corpora for System Evaluation
and Modeling. Conference on Empirical Methods
in Natural Language Processing (EMNLP). July,
Barcelona, Spain.
S. Douglas, D. Agarwal, T. Alonso, R. M. Bell, M.
Gilbert, D. F. Swayne and C. Volinsky. 2005. Min-
ing Customer Care Dialogs for ?Daily News?. IEEE
Trans. on Speech and Audio Processing, 13(5):652?
660.
P. Haffner, G. Tur and J. H. Wright 2003. Optimiz-
ing SVMs for Complex Call Classification. IEEE
International Conference on Acoustics, Speech, and
Signal Processing. April 6-10, Hong Kong.
X. Jiang and A.-H. Tan. 2005. Mining Ontolog-
ical Knowledge from Domain-Specific Text Doc-
uments. IEEE International Conference on Data
Mining, November 26-30, New Orleans, Louisiana,
USA.
K. Kummamuru, R. Lotlikar, S. Roy, K. Singal and R.
Krishnapuram. 2004. A hierarchical monothetic
document clustering algorithm for summarization
and browsing search results. International Confer-
ence on World Wide Web. New York, NY, USA.
H.-K J. Kuo and C.-H. Lee. 2003. Discriminative
Training of Natural Language Call Routers. IEEE
Trans. on Speech and Audio Processing, 11(1):24?
35.
A. D. Lawson, D. M. Harris, J. J. Grieco. 2003. Ef-
fect of Foreign Accent on Speech Recognition in
the NATO N-4 Corpus. Eurospeech. September 1-
4, Geneva, Switzerland.
G. Mishne, D. Carmel, R. Hoory, A. Roytman and A.
Soffer. 2005. Automatic Analysis of Call-center
Conversations. Conference on Information and
Knowledge Management. October 31-November 5,
Bremen, Germany.
M. Padmanabhan, G. Saon, J. Huang, B. Kingsbury
and L. Mangu.. 2002. Automatic Speech Recog-
nition Performance on a Voicemail Transcription
Task. IEEE Trans. on Speech and Audio Process-
ing, 10(7):433?442.
M. Tang, B. Pellom and K. Hacioglu. 2003. Call-
type Classification and Unsupervised Training for
the Call Center Domain. Automatic Speech Recog-
nition and Understanding Workshop. November 30-
December 4, St. Thomas, U S Virgin Islands.
J. Wright, A. Gorin and G. Riccardi. 1997. Auto-
matic Acquisition of Salient Grammar Fragments
for Call-type Classification. Eurospeech. Septem-
ber, Rhodes, Greece.
744
Coling 2010: Poster Volume, pages 189?196,
Beijing, August 2010
Unsupervised cleansing of noisy text
Danish Contractor
IBM India Software Labs
dcontrac@in.ibm.com
Tanveer A. Faruquie
IBM Research India
ftanveer@in.ibm.com
L. Venkata Subramaniam
IBM Research India
lvsubram@in.ibm.com
Abstract
In this paper we look at the problem of
cleansing noisy text using a statistical ma-
chine translation model. Noisy text is pro-
duced in informal communications such
as Short Message Service (SMS), Twit-
ter and chat. A typical Statistical Ma-
chine Translation system is trained on par-
allel text comprising noisy and clean sen-
tences. In this paper we propose an un-
supervised method for the translation of
noisy text to clean text. Our method has
two steps. For a given noisy sentence, a
weighted list of possible clean tokens for
each noisy token are obtained. The clean
sentence is then obtained by maximizing
the product of the weighted lists and the
language model scores.
1 Introduction
Noisy unstructured text data is found in informal
settings such as Short Message Service (SMS),
online chat, email, social message boards, news-
group postings, blogs, wikis and web pages. Such
text may contain spelling errors, abbreviations,
non-standard terminology, missing punctuation,
misleading case information, as well as false
starts, repetitions, and special characters.
We define noise in text as any kind of difference
between the surface form of a coded representa-
tion of the text and the correct text. The SMS ?u
kno whn is d last train of delhi metro? is noisy
because several of the words are not spelled cor-
rectly and there are grammar mistakes. Obviously
the person who wrote this message intended to
write exactly what is there in the SMS. But still it
is considered noisy because the message is coded
using non-standard spellings and grammar.
Current statistical machine translation (SMT)
systems rely on large parallel and monolingual
training corpora to produce high quality transla-
tions (Brown et al, 1993). Most of the large paral-
lel corpora available comprise newswire data that
include well formed sentences. Even when web
sources are used to train a SMT system, noisy por-
tions of the corpora are eliminated (Imamura et
al., 2003) (Imamura and Sumita, 2002) (Khadivi
and Ney, 2005). This is because it is known that
noise in parallel corpora results in incorrect train-
ing of models thus degrading the performance.
We are not aware of sufficiently large paral-
lel datasets comprising noisy and clean sentences.
In fact, even dictionaries comprising of noisy to
clean mappings in one language are very limited
in size.
With the increase in noisy text data generated
in various social communication media, cleans-
ing of such text has become necessary. The lack
of noisy parallel datasets means that this prob-
lem cannot be tackled in the traditional SMT way,
where translation models are learned based on the
parallel dataset. Consider the problem of translat-
ing a noisy English sentence e to a clean English
sentence h. SMT imagines that e was originally
conceived in clean English which when transmit-
ted over the noisy channel got corrupted and be-
came a noisy English sentence. The objective of
SMT is to recover the original clean sentence.
189
The goal of this paper is to analyze how noise
can be tackled. We present techniques to trans-
late noisy text sentences e to clean text sentences
h. We show that it is possible to clean noisy text
in an unsupervised fashion by incorporating steps
to construct ranked lists of possible clean English
tokens and then searching for the best clean sen-
tence. Of course as we will show for a given noisy
sentence, several clean sentences are possible. We
exploit the statistical machine learning paradigm
to let the decoder pick the best alternative from
these possible clean options to give the final trans-
lation for a given noisy sentence.
The rest of the paper is organized as follows.
In section 2 we state our contributions and give
an overview of our approach. In Section 3 we
describe the theory behind clean noisy text using
MT. In Section 4 we explain how we use a weigh-
ing function and a plain text dictionary of clean
tokens to guess possible clean English language
tokens. Section 5 describes our system along with
our results. We have given an analysis of the kind
of noise present in our data set in section 5.2
2 Our Approach
In this paper we describe an unsupervised method
to clean noisy text. We formulate the text cleans-
ing problem in the machine translation framework
using translation model 1 (Brown et al, 1993).
We clean the text using a pseudo-translation
model of clean and noisy words along with a lan-
guage model trained using a large monolingual
corpus. We use a decoder to search for the best
clean sentence for a noisy sentence using these
models.
We generate scores for the pseudo translation
model using a weighing function for each token in
an SMS and use these scores along with language
model probabilities to hypothesize the best clean
sentence for a given noisy SMS. Our approach can
be summarized in the following steps:
? Tokenize noisy SMS S into n tokens s1, s2 ...
sn. For each SMS token si create a weighted
list based on a weighing function. These lists
along with their scores corresponds to the
translation probabilities of the SMT transla-
tion model.
? Use the lists generated in the step above
along with clean text language model scores,
in a decoder to hypothesize the best clean
sentence
? At the end of the search choose the highest
scoring sentence as the clean translation of
the noisy sentence
In the above approach we do not learn the trans-
lation model but emulate the translation model
during decoding by analyzing the noise of the to-
kens in the input sentence.
3 Noisy sentence translation
Statistical Translation models were invented by
Brown, et al(Brown et al, 1993) and are based
on the source-channel paradigm of communica-
tion theory. Consider the problem of translating a
noisy sentence e to a clean sentence h. We imag-
ine that e was originally conceived cleanly which
when transmitted over the noisy communication
channel got corrupted and became a noisy sen-
tence. The goal is to get back the original clean
sentence from the noisy sentence. This can be ex-
pressed mathematically as
h? = argmax
h
Pr(h|e)
By Bayes? Theorem
h? = argmax
h
Pr(e|h)Pr(h)
Conceptually, the probability distribution
P (e|h) is a table which associates a probability
score with every possible pair of clean and noisy
sentences (e, h). Every noisy sentence e is a
candidate translation of a given clean sentence h.
The goodness of the translation h? e is given by
the probability score of the pair (e, h). Similarly,
Pr(h) is a table which associates a probability
score with every possible clean sentence h and
measures how well formed the sentence h is.
It is impractical to construct these tables exactly
by examining individual sentences (and sentence
pairs) since the number of conceivable sentences
in any language is countably infinite. Therefore,
the challenge in Statistical Machine Translation
is to construct approximations to the probability
190
distributions P (e|h) and Pr(h) that give an ac-
ceptable quality of translation. In the next section
we describe a model which is used to approximate
P (e|h).
3.1 IBM Translation Model 2
IBM translation model 2 is a generative model,
i.e., it describes how a noisy sentence e could be
stochastically generated given a clean sentence h.
It works as follows:
? Given a clean sentence h of length l, choose
the length (m) for the noisy sentence from a
distribution (m|l).
? For each position j = 1, 2, . . .m in the noisy
string, choose a position aj in the clean string
from a distribution a(aj |j, l,m). The map-
ping a = (a1, a2, . . . , am) is known as align-
ment between the noisy sentence e and the
clean sentence h. An alignment between e
and h tells which word of e is the corrupted
version of the corresponding word of h.
? For each j = 1, 2, . . .m in the noisy string,
choose an noisy word ej according to the dis-
tribution t(ej |haj ).
It follows from the generative model that prob-
ability of generating e = e1e2 . . . em given h =
h1h2 . . . hl with alignment a = (a1, a2, . . . , am)
is
Pr(e, a|h) = (m|l)
m?
j=1
t(ej |haj )a(aj |j,m, l).
It can be easily seen that a sentence e could be
produced from h employing many alignments and
therefore, the probability of generating e given
h is the sum of the probabilities of generating
e given h under all possible alignments a, i.e.,
Pr(e|h) =?a Pr(e, a|h). Therefore,
Pr(e|h) =
(m|l)
l?
a1=0
..
l?
am=0
m?
j=1
t(ej |haj )a(aj |j,m, l).
The above expression can be rewritten as follows:
Pr(e|h) = (m|l)
m?
j=1
l?
i=0
t(ej |hi)a(i|j,m, l).
Typical statistical machine translation systems
use large parallel corpora to learn the transla-
tion probabilities (Brown et al, 1993). Tradi-
tionally such corpora have consisted of news ar-
ticles and other well written articles. Therefore
in theory P (e|h) should be constructed by ex-
amining sentence pairs of clean and noisy sen-
tences. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training.
Aligned parallel corpora for noisy sentence is
difficult to obtain. This lack of data for a lan-
guage and the domain dependence of noise makes
it impractical to construct corpus from which
P (e|h) can be learnt automatically. This leads
to difficulty in learning P (e|h). Fortunately the
alignment between clean and noisy sentences are
monotonic in nature hence we assume a uniform
distribution for a(i|j,m, l) held fixed at (l+1)?1.
This is equivalent to model 1 of IBM translation
model. The translation models t(ej |haj ) can be
thought of as a ranked list of noisy words given
a clean word. In section 4.2 we show how this
ranked list can be constructed in an unsupervised
fashion.
3.2 Language Model
The problem of estimating the sentence forma-
tion distribution Pr(h) is known as the lan-
guage modeling problem. The language mod-
eling problem is well studied in literature par-
ticularly in the context of speech recognition.
Typically, the probability of a n-word sentence
h = h1h2 . . . hn is modeled as Pr(h) =
Pr(h1|H1)Pr(h2|H2) . . . P r(hn|Hn), where Hi
is the history of the ith word hi. One of the most
popular language models is the n-gram model
(Brown et al, 1993) where the history of a word
consists o f the word and the previous n?1 words
in the sentence, i.e., Hi = hihi?1 . . . hi?n+1. In
our application we use a smoothed trigram model.
3.3 Decoding
The problem of searching for a sentence h which
minimizes the product of translation model prob-
191
ability and the language model probability is
known as the decoding problem. The decoding
problem has been proved to be NP-complete even
when the translation model is IBM model 1 and
the language model is bi-gram (K Knight., 1999).
Effective suboptimal search schemes have been
proposed (F. Jelinek, 1969), (C. Tillman et al,
1997).
4 Pseudo Translation Model
In order to be able to exploit the SMT paradigm
we first construct a pseudo translation model. The
first step in this direction is to create noisy token
to clean token mapping. In order to process the
noisy input we first have to map noisy tokens in
noisy sentence, Se, to the possible correct lexical
representations. We use a similarity measure to
map the noisy tokens to their clean lexical repre-
sentations .
4.1 Similarity Measure
For a term te ? De, where De is a dictionary of
possible clean tokens, and token si of the noisy
input Se, the similarity measure ?(te, si) between
them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si) if te and si share
same starting
character
0 otherwise
(1)
where LCSRatio(te, si) = length(LCS(te,si))length(te) and
LCS(te, si) is the Longest common subsequence
between te and si. The intuition behind this mea-
sure is that people typically type the first few char-
acters of a word in an SMS correctly. This way we
limit the possible variants for a particular noisy to-
ken.
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is
the ratio of the length of their LCS and the length
of the longer string. Since in the SMS scenario,
the dictionary term will always be longer than the
SMS token, the denominator of LCSR is taken as
the length of the dictionary term.
The EditDistanceSMS (Figure 1) compares
the Consonant Skeletons (Prochasson et al, 2007)
of the dictionary term and the SMS token. If the
Levenshtein distance between consonant skele-
tons is small then ?(te, si) will be high. The intu-
ition behind using EditDistanceSMS can be ex-
plained through an example. Consider an SMS
token ?gud? whose most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a re-
sult the similarity measure between ?gud? and
?good? will be higher than that of ?gud? and
?guided?. Higher the LCSRatio and lower the
EditDistanceSMS , higher will be the similarity
measure. Hence, for a given SMS token ?byk?,
the similarity measure of word ?bike? is higher
than that of ?break?.
In the next section we show how we use
this similarity measure to construct ranked lists.
Ranked lists of clean tokens have also been used
in FAQ retrieval based on noisy queries (Kothari
et al, 2009).
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 1: EditDistanceSMS
4.2 List Creation
For a given noisy input string Se, we tokenize it
on white space and replace any occurrence of dig-
its to their string based form (e.g. 4get, 2day) to
get a series of n tokens s1, s2, . . . , sn. A list Lei
is created for each token si using terms in a dic-
192
hv u cmplted ure prj rprt
d ddline fr sbmission of d rprt hs bn xtnded
i wil be lte by 20 mns
d docs shd rech u in 2 days
thnk u for cmg 2 d prty
Figure 2: Sample SMS queries
tionary De consisting of clean english words. A
term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (2)
Heuristics are applied to boost scores of some
words based on positional properties of characters
in noisy and clean tokens. The scores of the fol-
lowing types of tokens are boosted:
1. Tokens that are a substring of a dictionary
words from the first character.
2. Tokens having the same first and last charac-
ter as a dictionary word.
3. Token that are dictionary words themselves
(clean text).
The threshold value ? is determined experimen-
tally. Thus we select only the top scoring possible
clean language tokens to construct the sentence.
Once the list are constructed the similarity mea-
sure along with the language model scores is used
by the decoding algorithm to find the best possi-
ble English sentence. It is to be noted that these
lists are constructed at decoding time since they
depend on the noisy surface forms of words in the
input sentence.
5 Experiments
To evaluate our system we used a set of 800 noisy
English SMSes sourced from the publicly avail-
able National University of Singapore SMS cor-
pus1 and a collection of SMSes available from the
Indian Institute of Technology, Kharagpur. The
SMSes are a collection of day-to-day SMS ex-
changes between different users. We manually
1http://wing.comp.nus.edu.sg/downloads/smsCorpus
Figure 3: System implementation
BLEU scores 1-gram 2-gram 3-gram 4-gram
Noisy text 40.96 63.7 45.1 34.5 28.3
Cleaned text 53.90 77.5 58.7 47.4 39.5
Table 1: BLEU scores
generated a cleaned english version of our test set
to use as a reference.
The noisy SMS tokens were used to generate
clean text candidates as described in section 4.2.
The dictionary De used for our experiments was a
plain text list of 25,000 English words. We cre-
ated a tri-gram language model using a collec-
tion of 100,000 clean text documents. The docu-
ments were a collection of articles on news, sport-
ing events, literature, history etc. For decoding
we used Moses2, which is an open source decoder
for SMT (Hoang et al, 2008), (Koehn et al,
2007). The noisy SMS along with clean candi-
date token lists, for each SMS token and language
model probabilities were used by Moses to hy-
pothesize the best clean english output for a given
noisy SMS. The language model and translation
models weights used by Moses during the decod-
ing phase, were adjusted manually after some ex-
perimentation.
We used BLEU (Bilingual evaluation under-
study) and Word error rate (WER) to evaluate the
performance of our system. BLEU is used to
2http://www.statmt.org/moses/
193
Figure 4: Comparison of BLEU scores
establish similarity between a system translated
and human generated reference text. A noisy
SMS ideally has only one possible clean transla-
tion and all human evaluators are likely to provide
the same translation. Thus, BLEU which makes
use of n-gram comparisons between reference and
system generated text, is very useful to measure
the accuracy of our system. As shown in Fig 4
, our system reported significantly higher BLEU
scores than unprocessed noisy text.
The word error rate is defined as
WER = S +D + IN (3)
where S is the number of substitutions, D is the
number of the deletions, I is the number of the in-
sertions and N is the number of words in the refer-
ence The WER can be thought of as an execution
of the Levenstein Edit distance algorithm at the
token level instead of character level.
Fig 5 shows a comparison of the WER. Sen-
tences generated from our system had 10 % lower
WER as compared to the unprocessed noisy sen-
tences. In addition, the sentences generated by our
system match a higher number of tokens (words)
with the reference sentences, as compared to the
noisy sentences.
5.1 System performance
Unlike standard MT system when P (e|h) is pre-
computed during the training time, list generation
in our system is dynamic because it depends on
the noisy words present in the input sentence. In
this section we evaluate the computation time for
list generation along with the decoding time for
finding the best list. We used an Intel Core 2
Duo 2.2 GHz processor with 3 GB DDR2 RAM
Figure 5: Word error rates
Figure 6: Execution time slices
to implement our system. As shown in Fig 6 the
additional computation involving list creation etc
takes up 56% (90 milliseconds) of total translation
time. 43% of the total execution time is taken by
the decoder, while I/O operations take only 1% of
the total execution time. The decoder execution
time slices reported above exclude the time taken
to load the language model. Moses took approxi-
mately 10 seconds to load our language model.
5.2 Measuring noise level in SMS queries
The noise in the collected SMS corpus can be cat-
egorized as follows
1. Removal of characters : The commonly ob-
served patterns include deletion of vowels
(as in ?msg? for ?message?), deletion of re-
peated character (as in ?happy? for ?hapy?)
and truncation (as in ?tue? for ?tuesday?)
Type of Noise % of Total Noisy Tokens
Deletion of Characters 48%
Phonetic Substitution 33%
Abbreviations 5%
Dialectical Usage 4%
Deletion of Words 1.2%
Table 2: Measure of Types of SMS Noise
194
Clean (Reference) text Noisy text Output text
Perplexity 19.61 34.56 21.77
Table 3: Perplexity for Reference, Noisy Cleaned
SMS
2. Phonetic substitution: For example, ?2? for
?to? or ?too?, ?lyf?? for ?life?, ?lite? for
?light? etc.
3. Abbreviation: Some frequently used abbre-
viations are ?tb? for ?text back?, ?lol? for
?laughs out loud?, ?AFAICT? for ?as far as
i can tell? etc.
4. Dialectal and informal usage: Often multiple
words are combined into a single token fol-
lowing certain dialectal conventions. For ex-
ample, ?gonna? is used for ?going to?, ?aint?
is used for ?are not?, etc.
5. Deletion of words: Function words (e.g. ar-
ticles) and pronouns are commonly deleted.
?I am reading the book? for example may be
typed as ?readin bk?.
Table 2 lists statistics on these noise types from
101 SMSes selected at random from our data set.
The average length of these SMSes was 13 words.
Out of the total number of words in the SMSes,
52% were non standard words. Table 2 lists the
statistics for the types of noise present in these non
standard words.
Measuring character level perplexity can be an-
other way of estimating noise in the SMS lan-
guage.The perplexity of a LM on a corpus gives
an indication of the average number of bits needed
per n-gram to encode the corpus. Noise results
in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits
are needed to encode these improbable n-grams
which results in increased perplexity.
We built a character-level language model (LM)
using a document collection (vocabulary size is
20K) and computed the perplexity of the language
model on the noisy and the cleaned SMS test-set
and the SMS reference data.
From Table 3 we can see the difference in per-
plexity for noisy and clean SMS data. Large per-
plexity values for the SMS dataset indicates a high
level of noise. The perplexity evaluation indicates
that our method is able to remove noise from the
input queries as given by the perplexity and is
close to the human correct reference corpus whose
perplexity is 19.61.
6 Conclusion
We have presented an inexpensive, unsupervised
method to clean noisy text. It does not require
the use of a noisy to clean language parallel cor-
pus for training. We show how a simple weigh-
ing function based on observed heuristics and a
vocabulary file can be used to shortlist clean to-
kens. These tokens and their weights are used
along with language model scores, by a decoder
to select the best clean language sentence.
References
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of tex-
ting language. International Journal on Document
Analysis and Recognition.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. In Proceedings of AAAI
Workshop on Enhanced Messaging.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of COLING-ACL.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS :
Evaluation et bilan quantitatif. In Actes de TALN,
Toulouse, France.
Catherine Kobus, Francois Yvon and Geraldine
Damnati. 2008. Normalizing SMS: Are two
metaphors better than one? In Proceedings of COL-
ING, Manchester.
Sreangsu Acharya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, Evan Herbst 2007.
Moses: Open source toolkit for statistical machine
195
translation. In Proceedings of ACL, Demonstration
Session .
Peter F. Brown, Vincent J.Della Pietra, Stephen A.
Della Pietra, Robert. L. Mercer 1993. The Math-
ematics of Statistical Machine Translation: Parame-
ter Estimation Computational Linguistics.
I. D. Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message
services. In Proceedings of ICDAR.
S. Khadivi and H. Ney. 2005. Automatic filtering of
bilingual corpora for statistical machine translation.
In Proceedings of NLDB, pages 263?274, 2005.
K. Imamura and E. Sumita. 2002. Bilingual corpus
cleaning focusing on translation literality. In In Pro-
ceedings of ICSLP.
K. Imamura, E. Sumita, and Y. Matsumoto. 2003. Au-
tomatic construction of machine translation knowl-
edge using translation literalness. In In Proceedings
of EACL.
K. Knight, 1999. Decoding complexity in word re-
placement translation models. Computational Lin-
guistics.
F. Jelinek, 1969. A fast sequential decoding algorithm
using a stack. IBM Journal of Research and Devel-
opment.
C. Tillman, S. Vogel, H. Ney, and A. Zubiaga. 1997.
A DP-based search using monotone alignments in
statistical translation. In Proceedings of ACL.
Hieu Hoang, Philipp Koehn. 2008. Design of the
Moses decoder for statistical machine translation.
In Proceedings of ACL Workshop on Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing.
Govind Kothari, Sumit Negi, Tanveer A. Faruquie,
Venkatesan T. Chakraverthy, L. Venkata Subrama-
niam. 2009. SMS based interface for FAQ retrieval,
In In Proceedings of ACL-IJCNLP
196
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87?96,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Handling Noisy Queries In Cross Language FAQ Retrieval
Danish Contractor Govind Kothari Tanveer A. Faruquie
L. Venkata Subramaniam Sumit Negi
IBM Research India
Vasant Kunj, Institutional Area
New Delhi, India
{dcontrac,govkotha,ftanveer,lvsubram,sumitneg}@in.ibm.com
Abstract
Recent times have seen a tremendous growth
in mobile based data services that allow peo-
ple to use Short Message Service (SMS) to
access these data services. In a multilin-
gual society it is essential that data services
that were developed for a specific language
be made accessible through other local lan-
guages also. In this paper, we present a ser-
vice that allows a user to query a Frequently-
Asked-Questions (FAQ) database built in a lo-
cal language (Hindi) using Noisy SMS En-
glish queries. The inherent noise in the SMS
queries, along with the language mismatch
makes this a challenging problem. We handle
these two problems by formulating the query
similarity over FAQ questions as a combina-
torial search problem where the search space
consists of combinations of dictionary varia-
tions of the noisy query and its top-N transla-
tions. We demonstrate the effectiveness of our
approach on a real-life dataset.
1 Introduction
There has been a tremendous growth in the number
of new mobile subscribers in the recent past. Most
of these new subscribers are from developing coun-
tries where mobile is the primary information de-
vice. Even for users familiar with computers and the
internet, the mobile provides unmatched portability.
This has encouraged the proliferation of informa-
tion services built around SMS technology. Several
applications, traditionally available on Internet, are
now being made available on mobile devices using
SMS. Examples include SMS short code services.
Short codes are numbers where a short message in
a predesignated format can be sent to get specific
information. For example, to get the closing stock
price of a particular share, the user has to send a
message IBMSTOCKPR. Other examples are search
(Schusteritsch et al, 2005), access to Yellow Page
services (Kopparapu et al, 2007), Email 1, Blog 2 ,
FAQ retrieval 3 etc. The SMS-based FAQ retrieval
services use human experts to answer SMS ques-
tions.
Recent studies have shown that instant messag-
ing is emerging as the preferred mode of commu-
nication after speech and email.4 Millions of users
of instant messaging (IM) services and short mes-
sage service (SMS) generate electronic content in a
dialect that does not adhere to conventional gram-
mar, punctuation and spelling standards. Words are
intentionally compressed by non-standard spellings,
abbreviations and phonetic transliteration are used.
Typical question answering systems are built for use
with languages which are free from such errors. It
is difficult to build an automated question answer-
ing system around SMS technology. This is true
even for questions whose answers are well docu-
mented like in a Frequently-Asked-Questions (FAQ)
database. Unlike other automatic question answer-
ing systems that focus on searching answers from
a given text collection, Q&A archive (Xue et al,
2008) or the Web (Jijkoun et al, 2005), in a FAQ
database the questions and answers are already pro-
1http://www.sms2email.com/
2http://www.letmeparty.com/
3http://www.chacha.com/
4http://www.whyconverge.com/
87
Figure 1: Sample SMS queries with Hindi FAQs
vided by an expert. The main task is then to iden-
tify the best matching question to retrieve the rel-
evant answer (Sneiders, 1999) (Song et al, 2007).
The high level of noise in SMS queries makes this a
difficult problem (Kothari et al, 2009). In a multi-
lingual setting this problem is even more formidable.
Natural language FAQ services built for users in one
language cannot be accessed in another language.
In this paper we present a FAQ-based question an-
swering system over a SMS interface that solves this
problem for two languages. We allow the FAQ to be
in one language and the SMS query to be in another.
Multi-lingual question answering and information
retrieval has been studied in the past (Sekine and
Grishman, 2003)(Cimiano et al, 2009). Such sys-
tems resort to machine translation so that the search
can be performed over a single language space. In
the two language setting, it involves building a ma-
chine translation system engine and using it such
that the question answering system built for a sin-
gle language can be used.
Typical statistical machine translation systems
use large parallel corpora to learn the translation
probabilities (Brown et al, 2007). Traditionally
such corpora have consisted of news articles and
other well written articles. Since the translation sys-
tems are not trained on SMS language they perform
very poorly when translating noisy SMS language.
Parallel corpora comprising noisy sentences in one
language and clean sentences in another language
are not available and it would be hard to build such
large parallel corpora to train a machine translation
system. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training. Such data is extremely hard
to create. Unsupervised techniques require huge
amounts of SMS data to learn mappings of non-
standard words to their corresponding conventional
form (Acharyya et al, 2009).
Removal of noise from SMS without the use of
parallel data has been studied but the methods used
are highly dependent on the language model and the
degree of noise present in the SMS (Contractor et
al., 2010). These systems are not very effective if
the SMSes contain grammatical errors (or the sys-
tem would require large amounts of training data in
the language model to be able to deal with all pos-
sible types of noise) in addition to misspellings etc.
Thus, the translation of a cleaned SMS, into a second
language, will not be very accurate and it would not
give good results if such a translated SMS is used to
query an FAQ collection.
Token based noise-correction techniques (such as
those using edit-distance, LCS etc) cannot be di-
rectly applied to handle the noise present in the SMS
query. These noise-correction methods return a list
of candidate terms for a given noisy token (E.g.
?gud? ? > ?god?,?good?,?guide? ) . Considering all
these candidate terms and their corresponding trans-
lations drastically increase the search space for any
multi-lingual IR system. Also , naively replacing the
noisy token in the SMS query with the top matching
candidate term gives poor performance as shown by
our experiments. Our algorithm handles these and
related issues in an efficient manner.
In this paper we address the challenges arising
when building a cross language FAQ-based ques-
tion answering system over an SMS interface. Our
method handles noisy representation of questions in
a source language to retrieve answers across target
languages. The proposed method does not require
hand corrected data or an aligned corpus for explicit
SMS normalization to mitigate the effects of noise.
It also works well with grammatical noise. To the
best of our knowledge we are the first to address
issues in noisy SMS based cross-language FAQ re-
trieval. We propose an efficient algorithm that can
handle noise in the form of lexical and semantic cor-
ruptions in the source language.
2 Problem formulation
Consider an input SMS Se in a source language
e. We view Se as a sequence of n tokens Se =
s1, s2, . . . , sn. As explained in the introduction, the
input is bound to have misspellings and other lexical
and semantic distortions. Also let Qh denote the set
88
of questions in the FAQ corpus of a target language
h. Each question Qh ? Qh is also viewed as a se-
quence of tokens. We want to find the question Q?h
from the corpus Qh that best matches the SMS Se.
The matching is assisted by a source dictionary
De consisting of clean terms in e constructed from
a general English dictionary and a domain dictio-
nary of target language Dh built from all the terms
appearing in Qh. For a token si in the SMS in-
put, term te in dictionary De and term th in dictio-
nary Dh we define a cross-lingual similarity mea-
sure ?(th, te, si) that measures the extent to which
term si matches th using the clean term te. We con-
sider th a cross lingual variant of si if for any te the
cross language similarity measure ?(th, te, si) > .
We denote this as th ? si.
We define a weight function ?(th, te, si) using the
cross lingual similarity measure and the inverse doc-
ument frequency (idf) of th in the target language
FAQ corpus. We also define a scoring function to as-
sign a score to each question in the corpusQh using
the weight function. Consider a question Qh ? Qh.
For each token si, the scoring function chooses the
term from Qh having the maximum weight using
possible clean representations of si; then the weight
of the n chosen terms are summed up to get the
score. The score measures how closely the question
in FAQ matches the noisy SMS string Se using the
composite weights of individual tokens.
Score(Qh) =
n?
i=1
max
th?Qh,te?De & th?si
?(th, te, si)
Our goal is to efficiently find the question Q?h having
the maximum score.
3 Noise removal from queries
In order to process the noisy SMS input we first have
to map noisy tokens in Se to the possible correct lex-
ical representations. We use a similarity measure to
map the noisy tokens to their clean lexical represen-
tations.
3.1 Similarity Measure
For a term te ? De and token si of the SMS input
Se, the similarity measure ?(te, si) between them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si)
if te and si share
same starting
character *
0 otherwise
(1)
Where LCSRatio(te, si) =
length(LCS(te,si))
length(te)
and LCS(te, si)
is the Longest common subsequence between te and si.
* The intuition behind this measure is that people typically type the
first few characters of a word in an SMS correctly. This way we limit
the possible variants for a particular noisy token
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is the
ratio of the length of their LCS and the length of the
longer string. Since in the SMS scenario, the dictio-
nary term will always be longer than the SMS token,
the denominator of LCSRatio is taken as the length
of the dictionary term.
The EditDistanceSMS (Figure 2) compares the
Consonant Skeletons (Prochasson et al, 2007) of the
dictionary term and the SMS token. If the Leven-
shtein distance between consonant skeletons is small
then ?(te, si) will be high. The intuition behind us-
ing EditDistanceSMS can be explained through
an example. Consider an SMS token ?gud? whose
most likely correct form is ?good?. The longest
common subsequence for ?good? and ?guided? with
?gud? is ?gd?. Hence the two dictionary terms
?good? and ?guided? have the same LCSRatio of 0.5
w.r.t ?gud?, but the EditDistanceSMS of ?good?
is 1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result the
similarity measure between ?gud? and ?good? will
be higher than that of ?gud? and ?guided?. Higher
the LCSRatio and lower the EditDistanceSMS ,
higher will be the similarity measure. Hence, for
a given SMS token ?byk?, the similarity measure of
word ?bike? is higher than that of ?break?.
4 Cross lingual similarity
Once we have potential candidates which are the
likely disambiguated representations of the noisy
89
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 2: EditDistanceSMS
term, we map these candidates to appropriate terms
in the target language. We use a statistical dictionary
to achieve this cross lingual mapping.
4.1 Statistical Dictionary
In order to build a statistical dictionary we use
the statistical translation model proposed in (Brown
et al, 2007). Under IBM model 2 the transla-
tion probability of source language sentence e? =
{t1e, . . . , t
j
e, . . . , t
m
e } and a target language sentence
h? = {t1h, . . . , t
i
h, . . . , t
l
e} is given by
Pr(h?|e?) = ?(l|m)
l?
i=1
m?
j=0
?(tih|t
j
e)a(j|i,m, l).
(2)
Here the word translation model ?(th|te) gives the
probability of translating the source term to target
term and the alignment model a(j|i,m, l) gives the
probability of translating the source term at position
i to a target position j. This model is learnt using an
aligned parallel corpus.
Given a clean term tie in source language we get
all the corresponding terms T = {t1h, . . . , t
k
h, . . .}
from the target language such that word translation
probability ?(tkh|t
i
e) > ?. We rank these terms ac-
cording to the probability given by the word trans-
lation model ?(th|te) and consider only those tar-
get terms that are part of domain dictionary i.e.
tkh ? D
h.
4.2 Cross lingual similarity measure
For each term si in SMS input query, we find all
the clean terms te in source dictionary De for which
similarity measure ?(te, si) > ?. For each of these
term te, we find the cross lingual similar terms Tte
using the word translation model. We compute the
cross lingual similarity measure between these terms
as
?(si, te, th) = ?(te, si).?(th, te) (3)
The measure selects those terms in target lan-
guage that have high probability of being translated
from a noisy term through one or more valid clean
terms.
4.3 Cross lingual similarity weight
We combine the idf and the cross lingual similarity
measure to define the cross lingual weight function
?(th, te, si) as
?(th, te, si) = ?(th, te, si).idf(th) (4)
By using idf we give preference to terms that are
highly discriminative. This is necessary because
queries are distinguished from each other using in-
formative words. For example for a given noisy
token ?bck? if a word translation model produces
a translation output ?wapas? (as in came back) or
?peet? or ?qamar? (as in back pain) then idf will
weigh ?peet? more as it is relatively more discrim-
inative compared to ?wapas? which is used fre-
quently.
5 Pruning and matching
In this section we describe our search algorithm and
the preprocessing needed to find the best question
Q?h for a given SMS query.
5.1 Indexing
Our algorithm operates at a token level and its corre-
sponding cross lingual variants. It is therefore nec-
essary to be able to retrieve all questions Qhth that
contain a given target language term th. To do this
efficiently we index the questions in FAQ corpus us-
ing Lucene5. Each question in FAQ is treated as a
document. It is tokenized using whitespace as de-
limiter before indexing.
5http://lucene.apache.org/java/docs/
90
The cross lingual similarity weight calculation re-
quires the idf for a given term th. We query on this
index to determine the number of documents f that
contain th. The idf of each term in Dh is precom-
puted and stored in a hashtable with th as the key.
The cross lingual similarity measure calculation re-
quires the word translation probability for a given
term te. For every te in dictionary De, we store
Tte in a hashmap that contains a list of terms in the
target language along with their statistically deter-
mined translation probability ?(th|te) > ?, where
th ? Dh.
Since the query and the FAQs use terms from dif-
ferent languages, the computation of IDF becomes a
challenge (Pirkola, 1998) (Oard et al, 2007). Prior
work uses a bilingual dictionary for translations for
calculating the IDF. We on the other hand rely on
a statistical dictionary that has translation probabil-
ities. Applying the method suggested in the prior
work on a statistical dictionary leads to errors as the
translations may themselves be inaccurate.
We therefore calculate IDFs for target language
term (translation) and use it in the weight measure
calculation. The method suggested by Oard et al
(Oard et al, 2007) is more useful in retrieval tasks
for multiple documents, while in our case we need
to retrieve a specific document (FAQ).
5.2 List Creation
Given an SMS input string Se, we tokenize it on
white space and replace any occurrence of digits to
their string based form (e.g. 4get, 2day) to get a se-
ries of n tokens s1, s2, . . . , sn. A list Lei is created
for each token si using terms in the monolingual dic-
tionary De. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop word.
A term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (5)
The threshold value ? is determined experimen-
tally. For every te ? Lei we retrieve Tte and then
retrieve the idf scores for every th ? Tte . Using the
word translation probabilities and the idf score we
compute the cross lingual similarity weight to create
a new list Lhi . A term th is included in the list only
if
?(th|te) > 0.1 (6)
This probability cut-off is used to prevent poor
quality translations from being included in the list.
If more than one term te has the same transla-
tion th, then th can occur more than once in a given
list. If this happens, then we remove repetitive oc-
currences of th and assign it a weight equal to the
maximum weight amongst all occurrences in the list,
multiplied by the number of times it occurs. The
terms th in Lhi are sorted in decreasing order of their
similarity weights. Henceforth, the term ?list? im-
plies a sorted list.
For example given a SMS query ?hw mch ds it cst
to stdy in india? as shown in Fig. 3, for each token
we create a list of possible correct dictionary words
by dictionary look up. Thus for token ?cst? we get
dictionary words lik ?cost, cast, case, close?. For
each dictionary word we get a set of possible words
in Hindi by looking at statistical translation table.
Finally we merged the list obtained to get single list
of Hindi words. The final list is ranked according to
their similarity weights.
5.3 Search algorithm
Given Se containing n tokens, we create n sorted
lists Lh1 , L
h
2 , . . . , L
h
n containing terms from the do-
main dictionary and sorted according to their cross
lingual weights as explained in the previous section.
A naive approach would be to query the index using
each term appearing in all Lhi to build a Collection
set C of questions. The best matching question Q?h
will be contained in this collection. We compute the
score of each question in C using Score(Q) and the
question with highest score is treated as Q?h. How-
ever the naive approach suffers from high runtime
cost.
Inspired by the Threshold Algorithm (Fagin et
al., 2001) we propose using a pruning algorithm
that maintains a much smaller candidate set C of
questions that can potentially contain the maximum
scoring question. The algorithm is shown in Fig-
ure 4. The algorithm works in an iterative manner.
In each iteration, it picks the term that has maxi-
mum weight among all the terms appearing in the
lists Lh1 , L
h
2 , . . . , L
h
n. As the lists are sorted in the
descending order of the weights, this amounts to
picking the maximum weight term amongst the first
terms of the n lists. The chosen term th is queried to
find the set Qth . The set Qth is added to the candi-
91
Figure 3: List creation
date set C. For each question Q ? Qth , we compute
its score Score(Q) and keep it along with Q. After
this the chosen term th is removed from the list and
the next iteration is carried out. We stop the iterative
process when a thresholding condition is met and fo-
cus only on the questions in the candidate set C. The
thresholding condition guarantees that the candidate
set C contains the maximum scoring question Q?h.
Next we develop this thresholding condition.
Let us consider the end of an iteration. Sup-
pose Q is a question not included in C. At
best, Q will include the current top-most tokens
Lh1 [1], L
h
2 [1], . . . , L
h
n[1] from every list. Thus, the
upper bound UB on the score of Q is
Score(Q) ?
n?
i=0
?(Lhi [1]).
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set C
cannot be the maximum scoring question. Thus, the
condition ?Q? ? UB? serves as the termination cri-
terion. At the end of each iteration, we check if the
termination condition is satisfied and if so, we can
stop the iterative process. Then, we simply pick the
question in C having the maximum score and return
it.
Procedure Search Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?h.
Begin
?si, construct Lei for which ?(si, te) > 
// Li lists variants of si
Construct lists Lh1 , L
h
2 , . . . , L
h
n //(see Section 5.2).
// Lhi lists cross lingual variants of si in decreasing
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(L
h
i [1])
t?h = L
h
j? [1]
// t?h is the term having maximum weight among
// all terms appearing in the n lists.
Delete t?h from the list L
h
j? .
Retrieve Qt?h using the index
// Qt?h : the set of all questions in Q
h
//having the term t?h
For each Q ? Qt?h
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(L
h
i [1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 4: Search Algorithm with Pruning
6 Experiments
To evaluate our system we used noisy English SMS
queries to query a collection of 10, 000 Hindi FAQs.
These FAQs were collected from websites of vari-
ous government organizations and other online re-
sources. These FAQs are related to railway reser-
vation, railway enquiry, passport application and
health related issues. For our experiments we asked
6 human evaluators, proficient in both English and
Hindi, to create English SMS queries based on the
general topics that our FAQ collection dealt with.
We found 60 SMS queries created by the evaluators,
had answers in our FAQ collection and we desig-
nated these as the in-domain queries. To measure
the effectiveness of our system in handling out of
domain queries we used a total of 380 SMSes part of
which were taken from the NUS corpus (How et al,
92
whch metro statn z nr pragati maidan ?
dus metro goes frm airpot 2 new delhi rlway statn?
is dere any special metro pas 4 delhi uni students?
whn is d last train of delhi metro?
whr r d auto stands N delhi?
Figure 5: Sample SMS queries
2005) and the rest from the ?out-of-domain? queries
created by the human evaluators. Thus the total SMS
query data size was 440. Fig 5 shows some of the
sample queries.
Our objective was to retrieve the correct Hindi
FAQ response given a noisy English SMS query. A
given English SMS query was matched against the
list of indexed FAQs and the best matching FAQ was
returned by the Pruning Algorithm described in Sec-
tion 5. A score of 1 was assigned if the retrieved
answer was indeed the response to the posed SMS
query else we assigned a score of 0. In case of out
of domain queries a score of 1 was assigned if the
output was NULL else we assigned a score of 0.
6.1 Translation System
We used the Moses toolkit (Koehn et al, 2007) to
build an English-Hindi statistical machine transla-
tion system. The system was trained on a collec-
tion of 150, 000 English and Hindi parallel sentences
sourced from a publishing house. The 150, 000 sen-
tences were on a varied range of subjects such as
news, literature, history etc. Apart from this the
training data also contained an aligned parallel cor-
pus of English and Hindi FAQs. The FAQs were
collected from government websites on topics such
as health, education, travel services etc.
Since an MT system trained solely on a collection
of sentences would not be very accurate in translat-
ing questions, we trained the system on an English-
Hindi parallel question corpus. As it was difficult
to find a large collection of parallel text consisting
of questions, we created a small collection of par-
allel questions using 240 FAQs and multiplied them
to create a parallel corpus of 50, 000 sentences. This
set was added to the training data and this helped fa-
miliarize the language model and phrase tables used
by the MT systems to questions. Thus in total the
MT system was trained on a corpus of 200, 000 sen-
tences.
Experiment 1 and 2 form the baseline against
which we evaluated our system. For our experi-
ments the lexical translation probabilities generated
by Moses toolkit were used to build the word trans-
lation model. In Experiment 1 the threshold ? de-
scribed in Equation 5 is set to 1. In Experiment 2
and 3 this is set to 0.5. The Hindi FAQ collection
was indexed using Lucene and a domain dictionary
Dh was created from the Hindi words in the FAQ
collection.
6.2 System Evaluation
We perform three sets of experiments to show how
each stage of the algorithm contributes in improving
the overall results.
6.2.1 Experiment 1
For Experiment 1 the threshold ? in Equation 5
is set to 1 i.e. we consider only those tokens in the
query which belong to the dictionary. This setup il-
lustrates the case when no noise handling is done.
The results are reported in Figure 6.
6.2.2 Experiment 2
For Experiment 2 the noisy SMS query was
cleaned using the following approach. Given a noisy
token in the SMS query it?s similarity (Equation 1)
with each word in the Dictionary is calculated. The
noisy token is replaced with the Dictionary word
with the maximum similarity score. This gives us
a clean English query.
For each token in the cleaned English SMS query,
we create a list of possible Hindi translations of the
token using the statistical translation table. Each
Hindi word was assigned a weight according to
Equation 4. The Pruning algorithm in Section 5 was
then applied to get the best matching FAQ.
6.2.3 Experiment 3
In this experiment, for each token in the noisy En-
glish SMS we obtain a list of possible English vari-
ations. For each English variation a corresponding
set of Hindi words from the statistical translation ta-
ble was obtained. Each Hindi word was assigned
a weight according to Equation 4. As described in
Section 5.2, all Hindi words obtained from English
variations of a given SMS token are merged to create
93
Experiment 1 Experiment 2 Experiment 3
MRR Score 0.41 0.68 0.83
Table 1: MRR Scores
F1 Score
Expt 1 (Baseline 1) 0.23
Expt 2 (Baseline 2) 0.68
Expt 3 (Proposed Method) 0.72
Table 2: F1 Measure
a list of Hindi words sorted in terms of their weight.
The Pruning algorithm as described in Section 5 was
then applied to get the best matching FAQ.
We evaluated our system using two different cri-
teria. We used MRR (Mean reciprocal rank) and
the best matching accuracy. Mean reciprocal rank
is used to evaluate a system by producing a list of
possible responses to a query, ordered by probabil-
ity of correctness. The reciprocal rank of a query
response is the multiplicative inverse of the rank of
the first correct answer. The mean reciprocal rank
is the average of the reciprocal ranks of results for a
sample of queries Q.
MRR = 1/|Q|
Q?
i=1
1/ranki (7)
Best match accuracy can be considered as a spe-
cial case of MRR where the size of the ranked list is
1. As the SMS based FAQ retrieval system will be
used via mobile phones where screen size is a ma-
jor constraint it is crucial to have the correct result
on the top. Hence in our settings the best match ac-
curacy is a more relevant and stricter performance
evaluation measure than MRR.
Table 1 compares the MRR scores for all three
experiments. Our method reports the highest MRR
of 0.83. Figure 6 shows the performance using the
strict evaluation criterion of the top result returned
being correct.
We also experimented with different values of
the threshold for Score(Q) (Section 5.3). The ROC
curve for various threshold is shown in Figure 7. The
result for both in-domain and out-of-domain queries
for the three experiments are shown in Figure 6 for
Score(Q) = 8. The F1 Score for experiments 1, 2 and
3 are shown in Table 2.
Figure 6: Comparison of results
Figure 7: ROC Curve for Score(Q)
6.3 Measuring noise level in SMS queries
In order to quantify the level of noise in the col-
lected SMS data, we built a character-level language
model(LM) using the questions in the FAQ data-set
(vocabulary size is 70) and computed the perplexity
of the language model on the noisy and the cleaned
SMS test-set. The perplexity of the LM on a cor-
pus gives an indication of the average number of bits
needed per n-gram to encode the corpus. Noise re-
Cleaned SMS Noisy SMS
English FAQ collection
bigram 16.64 55.19
trigram 9.75 69.41
Table 3: Perplexity for Cleaned and Noisy SMS
94
sults in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits are
needed to encode these improbable n-grams which
results in increased perplexity. From Table 3 we can
see the difference in perplexity for noisy and clean
SMS data for the English FAQ data-set. Large per-
plexity values for the SMS dataset indicates a high
level of noise.
For each noisy SMS query e.g. ?hw 2 prvnt ty-
phd? we manually created a clean SMS query ?how
to prevent typhoid?. A character level language
model using the questions in the clean English FAQ
dataset was created to quantify the level of noise in
our SMS dataset. We computed the perplexity of the
language model on clean and noisy SMS queries.
7 Conclusion
There has been a tremendous increase in information
access services using SMS based interfaces. How-
ever, these services are limited to a single language
and fail to scale for multilingual QA needs. The
ability to query a FAQ database in a language other
than the one for which it was developed is of great
practical significance in multilingual societies. Au-
tomatic cross-lingual QA over SMS is challenging
because of inherent noise in the query and the lack
of cross language resources for noisy processing. In
this paper we present a cross-language FAQ retrieval
system that handles the inherent noise in source lan-
guage to retrieve FAQs in a target language. Our sys-
tem does not require an end-to-end machine transla-
tion system and can be implemented using a sim-
ple dictionary which can be static or constructed
statistically using a moderate sized parallel corpus.
This side steps the problem of building full fledged
translation systems but still enabling the system to
be scaled across multiple languages quickly. We
present an efficient algorithm to search and match
the best question in the large FAQ corpus of tar-
get language for a noisy input question. We have
demonstrated the effectiveness of our approach on a
real life FAQ corpus.
References
Sreangsu Acharyya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition, pp. 175-184.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING-ACL, pp. 33-40.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, Robert. L. Mercer 1993. The Mathematics of
Statistical Machine Translation: Parameter Estimation
Computational Linguistics, pp. 263-311.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. AAAI Workshop on En-
hanced Messaging.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of texting
language. International Journal on Document Analy-
sis and Recognition, pp. 157-174.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, Steffen Staab. 2009. Explicit versus latent con-
cept models for cross-language information retrieval.
In Proceeding of IJCAI, pp. 1513-1518.
Danish Contractor, Tanveer A. Faruquie, L. Venkata Sub-
ramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceeding of COLING 2010: Posters, pp.
189-196.
R. Fagin, A. Lotem, and M. Naor. 2001. Optimal aggre-
gation algorithms for middleware. In Proceedings of
the 20th ACM SIGMOD-SIGACT-SIGART symposium
on Principles of database systems, pp. 102-113.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In M. J. Smith and G. Salvendy (Eds.) Proc. of
Human Computer Interfaces International,Lawrence
Erlbaum Associates
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management,CIKM, pp.
76-83.
Catherine Kobus, Francois Yvon and Grraldine Damnati.
2008. Normalizing SMS: Are two metaphors better
than one? In Proceedings of COLING, pp. 441-448.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst 2007. Moses:
Open source toolkit for statistical machine translation.
Annual Meeting of the Association for Computation
Linguistics (ACL), Demonstration Session .
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Interface
95
to Yellow Pages Directory. In Proceedings of the 4th
international conference on mobile technology, appli-
cations, and systems and the 1st international sympo-
sium on Computer human interaction in mobile tech-
nology, pp. 558-563 .
Govind Kothari, Sumit Negi, Tanveer Faruquie, Venkat
Chakravarthy and L V Subramaniam 2009. SMS
based Interface for FAQ Retrieval. Annual Meeting
of the Association for Computation Linguistics (ACL).
I. D. Melamed. 1999. Bitext maps and alignment via pat-
tern recognition. Computational Linguistics, pp. 107-
130.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS : Eval-
uation et bilan quantitatif. In Actes de TALN, pp. 123-
132.
Douglas W. Oard, Funda Ertunc. 2002. Translation-
Based Indexing for Cross-Language Retrieval In Pro-
ceedings of the ECIR, pp. 324-333.
A. Pirkola 1998. The Effects of Query Structure
and Dictionary Setups in Dictionary-Based Cross-
Language Information Retrieval SIGIR ?98: Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval , pp. 55-63.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message ser-
vices. In Proceedings of the 9th International Confer-
ence on Document Analysis and Recognition, pp. 83-
87.
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Designing
the User Experience for Google SMS. In Proceedings
of ACM SIGCHI, pp. 1777-1780.
Satoshi Sekine, Ralph Grishman. 2003. Hindi-English
cross-lingual question-answering system. ACM Trans-
actions on Asian Language Information Processing,
pp. 181-192.
E. Sneiders. 1999. Automated FAQ Answering: Contin-
ued Experience with Shallow Language Understand-
ing Question Answering Systems. Papers from the
1999 AAAI Fall Symposium. Technical Report FS-99-
02, AAAI Press, pp. 97-107.
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007. Ques-
tion similarity calculation for FAQ answering. In Pro-
ceeding of SKG 07, pp. 298-301.
X. Xue, J. Jeon, and W.B Croft. 2008. Retrieval Models
for Question and Answer Archives. In Proceedings of
SIGIR, pp. 475-482.
96
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 5?6,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
 
Noisy Text Analytics
L. Venkata Subramaniam, IBM Research India
Text produced in informal settings (email, blogs, tweet, SMS, chat) and text which 
results from processing (speech recognition, OCR, machine translation, historical text) 
is inherently noisy. This tutorial will cover the efforts of the computational linguistics 
community in moving beyond traditional techniques to contend with the noise.  
1. Overview
Text produced by processing signals intended for human use is often noisy for 
automated computer processing. Digital text produced in informal settings such as 
online chat, SMS, emails, tweets, message boards, newsgroups, blogs, wikis and web 
pages contain considerable noise. Also processing techniques like Automatic Speech 
Recognition, Optical Character Recognition and Machine Translation introduce 
processing noise. People are adept when it comes to pattern recognition tasks involving 
typeset or handwritten documents or recorded speech, machines less-so.  
Noise can manifest itself at the earliest stages of processing in the form of degraded 
inputs that our systems must be prepared to handle. Many downstream applications use 
techniques meant for clean text. It is only recently that with the increase in noisy text, 
these techniques are being adapted to handle noisy text. This tutorial will focus on the 
problems encountered in analyzing such noisy text coming from various sources. Noise 
introduces challenges that need special handling, either through new methods or 
improved versions of existing ones. For example, missing punctuation and the use of 
non-standard words can often hinder standard natural language processing techniques 
such as part-of-speech tagging and parsing. Further downstream applications such as 
Information Retrieval, Information Extraction and Text mining have to explicitly handle 
noise in order to return useful results. Often, depending on the application, the noise 
can be modeled and it may be possible to develop specific strategies to immunize the 
system from the effects of noise and improve performance. This tutorial will cover:
    * Various sources of noise and their characteristics as well as typical metrics used to 
measure noise.
    * Methods to handle noise by moving beyond traditional natural language processing 
techniques.
    * Methods to overcome noise in specific applications like IR, IE, QA, MT, etc.
2. Outline 
The tutorial will have three parts:
    * What is Noise
          o Detecting Noise
5
          o Classifying Noise
          o Quantifying Noise
    * Processing and/or Correcting Noise
          o Spelling Correction
          o Natural Language Processing of Noisy Text: Segmentation, Parsing, POS
          o Learning underlying language models in presence of noise
    * Effect of Noise on Downstream Applications
          o Information Retrieval from Noisy Text
          o Information Extraction from Noisy Text
          o Classification of Noisy Text
          o Summarization of Noisy Text
          o Machine Translation of Noisy Text
3. Target Audience  
This tutorial is designed for students and researchers in Computer Science and 
Computational Linguistics. Elementary knowledge of text processing is assumed. This 
topic is expected to be of wide interest given its relevance to the computational 
linguistics community. Since noisy data is also the main theme of NAACL HLT 2010, 
good audience participation can be expected. 
 4. Speaker?s Bio  
L Venkata Subramaniam manages the information processing and analytics group at 
IBM Research ? India. He received his PhD from IIT Delhi in 1999. His research focuses 
on unstructured information management, statistical natural language processing, noisy 
text analytics, text and data mining, information theory, speech and image processing. 
He often teaches and guides student thesis at IIT Delhi on these topics. He co founded 
the AND (Analytics for Noisy Unstructured Text Data) workshop series and also co-
chaired the first three workshops, 2007-2009. He was guest co-editor of two special 
issues on Noisy Text Analytics in the International Journal of Document Analysis and 
Recognition in 2007 and 2009.
6
Tutorials, NAACL-HLT 2013, pages 7?9,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Towards Reliability-Aware Entity Analytics and
Integration for Noisy Text at Scale
Sameep Mehta, L. Venkata Subramaniam
IBM Research India
sameepmehta,lvsubram@in.ibm.com
1 Outline
Due to easy to use apps (Facebook, Twitter, etc.), higher Internet connectivity and
always on facility allowed by smart phones, the key characteristics of raw data are
changing. This new data can be characterized by 4V?s - Volume, Velocity, Vari-
ety and Veracity. For example during a Football match, some people will Tweet
about goals, penalties, etc., while others may write longer blogs and further there
will be match reports filed in trusted online news media after the match. Although
the sources may be varied, the data describes and provides multiple evidences for
the same event. Such multiple evidences should be used to strengthen the belief
in the underlying physical event as the individual data points may have inherent
uncertainty. The uncertainty can arise from inconsistent, incomplete and ambigu-
ous reports. The uncertainty is also because the trust levels of the different sources
vary and affect the overall reliability. We will summarize various efforts to perform
reliability aware entity integration.
The other problem in text analysis in such setting is posed by presence of noise
in the text. Since the text is produced in several informal settings such as email,
blogs, tweet, SMS, chat and is inherently noisy and has several veracity issues.
For example, missing punctuation and the use of non-standard words can often
hinder standard natural language processing techniques such as part-of-speech tag-
ging and parsing. Further downstream applications such as entity extraction, entity
resolution and entity completion have to explicitly handle noise in order to return
useful results. Often, depending on the application, noise can be modeled and it
may be possible to develop specific strategies to immunize the system from the
effects of noise and improve performance. Also the aspect of reliability is key as a
lot of this data is ambiguous, incomplete, conflicting, untrustworthy and deceptive.
The key goals of this tutorial are:
7
1. Draw the attention of researchers towards methods for doing entity analytics
and integration on data with 4V characteristics.
2. Differentiate between noise and uncertainty in such data.
3. Provide an in-depth discussion on handling noise in NLP based methods.
4. Finally, handling uncertainty through information fusion and integration.
This tutorial builds on two earlier tutorials: NAACL 2010 tutorial on Noisy
Text and COMAD 2012 tutorial on Reliability Aware Data Fusion. In parallel
the authors are also hosting a workshop on related topic ?Reliability Aware Data
Fusion? at SIAM Data Mining Conference, 2013.
2 Outline
2.1 Data with 4V characteristics
? Define Volume, Velocity, Variety and Veracity and metrics to quantify them
? Information extraction on data with 4V characteristics
2.2 Key technical challenges posed by the 4V dimensions and linguis-
tics techniques to address them
? Analyzing streaming text
? Large scale distributed algorithms for NLP
? Integrating structured and unstructured data
? Noisy text analytics
? Reliability
? Use case: Generating single view of entity from social data
2.3 Computing Reliability and Trust
? Computing source reliability
? Identifying Trust Worthy Messages
? Data fusion to improve reliability: Probabilistic data fusion, information
measures, evidential reasoning
? Use case: Event detection using social data, news and online sources
8
3 Speaker Bios
SameepMehta1 is researcher in Information Management Group at IBM Research
India. He received his M.S. and Ph.D. from The Ohio State University, USA in
2006. He also holds an Adjunct Faculty position at the International Institute of
Information Technology, New Delhi. Sameep regularly advises MS and PhD stu-
dents at University of Delhi and IIT Delhi. He regularly delivers Tutorials at CO-
MAD (2009, 2010 and 2011). His current research interests include Data Mining,
Business Analytics, Service Science, Text Mining, and Workforce Optimization.
L. Venkata Subramaniam2 manages the information management analytics
and solutions group at IBM Research India. He received his PhD from IIT Delhi
in 1999. His research focuses on unstructured information management, statistical
natural language processing, noisy text analytics, text and data mining, information
theory, speech and image processing. He often teaches and guides student thesis at
IIT Delhi on these topics. His tutorial titled Noisy Text Analytics was the second
largest at NAACL-HLT 2010. He co founded the AND (Analytics for Noisy Un-
structured Text Data) workshop series and also co-chaired the first four workshops,
2007-2010. He was guest co-editor of two special issues on Noisy Text Analytics
in the International Journal of Document Analysis and Recognition in 2007 and
2009.
1http://in.linkedin.com/in/sameepmehta
2https://sites.google.com/site/lvs004/
9
Proceedings of the ACL 2010 Conference Short Papers, pages 126?131,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatically Generating Term-frequency-induced Taxonomies
Karin Murthy Tanveer A Faruquie L Venkata Subramaniam
K Hima Prasad Mukesh Mohania
IBM Research - India
{karinmur|ftanveer|lvsubram|hkaranam|mkmukesh}@in.ibm.com
Abstract
We propose a novel method to automati-
cally acquire a term-frequency-based tax-
onomy from a corpus using an unsuper-
vised method. A term-frequency-based
taxonomy is useful for application do-
mains where the frequency with which
terms occur on their own and in combi-
nation with other terms imposes a natural
term hierarchy. We highlight an applica-
tion for our approach and demonstrate its
effectiveness and robustness in extracting
knowledge from real-world data.
1 Introduction
Taxonomy deduction is an important task to under-
stand and manage information. However, building
taxonomies manually for specific domains or data
sources is time consuming and expensive. Tech-
niques to automatically deduce a taxonomy in an
unsupervised manner are thus indispensable. Au-
tomatic deduction of taxonomies consist of two
tasks: extracting relevant terms to represent con-
cepts of the taxonomy and discovering relation-
ships between concepts. For unstructured text, the
extraction of relevant terms relies on information
extraction methods (Etzioni et al, 2005).
The relationship extraction task can be classi-
fied into two categories. Approaches in the first
category use lexical-syntactic formulation to de-
fine patterns, either manually (Kozareva et al,
2008) or automatically (Girju et al, 2006), and
apply those patterns to mine instances of the pat-
terns. Though producing accurate results, these
approaches usually have low coverage for many
domains and suffer from the problem of incon-
sistency between terms when connecting the in-
stances as chains to form a taxonomy. The second
category of approaches uses clustering to discover
terms and the relationships between them (Roy
and Subramaniam, 2006), even if those relation-
ships do not explicitly appear in the text. Though
these methods tackle inconsistency by addressing
taxonomy deduction globally, the relationships ex-
tracted are often difficult to interpret by humans.
We show that for certain domains, the frequency
with which terms appear in a corpus on their own
and in conjunction with other terms induces a nat-
ural taxonomy. We formally define the concept
of a term-frequency-based taxonomy and show
its applicability for an example application. We
present an unsupervised method to generate such
a taxonomy from scratch and outline how domain-
specific constraints can easily be integrated into
the generation process. An advantage of the new
method is that it can also be used to extend an ex-
isting taxonomy.
We evaluated our method on a large corpus of
real-life addresses. For addresses from emerging
geographies no standard postal address scheme
exists and our objective was to produce a postal
taxonomy that is useful in standardizing addresses
(Kothari et al, 2010). Specifically, the experi-
ments were designed to investigate the effective-
ness of our approach on noisy terms with lots of
variations. The results show that our method is
able to induce a taxonomy without using any kind
of lexical-semantic patterns.
2 Related Work
One approach for taxonomy deduction is to use
explicit expressions (Iwaska et al, 2000) or lexi-
cal and semantic patterns such as is a (Snow et al,
2004), similar usage (Kozareva et al, 2008), syn-
onyms and antonyms (Lin et al, 2003), purpose
(Cimiano and Wenderoth, 2007), and employed by
(Bunescu and Mooney, 2007) to extract and orga-
nize terms. The quality of extraction is often con-
trolled using statistical measures (Pantel and Pen-
nacchiotti, 2006) and external resources such as
wordnet (Girju et al, 2006). However, there are
126
domains (such as the one introduced in Section
3.2) where the text does not allow the derivation
of linguistic relations.
Supervised methods for taxonomy induction
provide training instances with global seman-
tic information about concepts (Fleischman and
Hovy, 2002) and use bootstrapping to induce new
seeds to extract further patterns (Cimiano et al,
2005). Semi-supervised approaches start with
known terms belonging to a category, construct
context vectors of classified terms, and associate
categories to previously unclassified terms de-
pending on the similarity of their context (Tanev
and Magnini, 2006). However, providing train-
ing data and hand-crafted patterns can be tedious.
Moreover in some domains (such as the one pre-
sented in Section 3.2) it is not possible to construct
a context vector or determine the replacement fit.
Unsupervised methods use clustering of word-
context vectors (Lin, 1998), co-occurrence (Yang
and Callan, 2008), and conjunction features (Cara-
ballo, 1999) to discover implicit relationships.
However, these approaches do not perform well
for small corpora. Also, it is difficult to label the
obtained clusters which poses challenges for eval-
uation. To avoid these problems, incremental clus-
tering approaches have been proposed (Yang and
Callan, 2009). Recently, lexical entailment has
been used where the term is assigned to a cate-
gory if its occurrence in the corpus can be replaced
by the lexicalization of the category (Giuliano and
Gliozzo, 2008). In our method, terms are incre-
mentally added to the taxonomy based on their
support and context.
Association rule mining (Agrawal and Srikant,
1994) discovers interesting relations between
terms, based on the frequency with which terms
appear together. However, the amount of patterns
generated is often huge and constructing a tax-
onomy from all the patterns can be challenging.
In our approach, we employ similar concepts but
make taxonomy construction part of the relation-
ship discovery process.
3 Term-frequency-induced Taxonomies
For some application domains, a taxonomy is in-
duced by the frequency in which terms appear in a
corpus on their own and in combination with other
terms. We first introduce the problem formally and
then motivate it with an example application.
Figure 1: Part of an address taxonomy
3.1 Definition
Let C be a corpus of records r. Each record is
represented as a set of terms t. Let T = {t | t ?
r ? r ? C} be the set of all terms of C. Let f(t)
denote the frequency of term t, that is the number
of records in C that contain t. Let F (t, T+, T?)
denote the frequency of term t given a set of must-
also-appear terms T+ and a set of cannot-also-
appear terms T?. F (t, T+, T?) = | {r ? C |
t ? r? ? t? ? T+ : t? ? r ? ? t? ? T? : t? /? r} |.
A term-frequency-induced taxonomy (TFIT), is
an ordered tree over terms in T . For a node n in
the tree, n.t is the term at n, A(n) the ancestors of
n, and P (n) the predecessors of n.
A TFIT has a root node with the special term ?
and the conditional frequency ?. The following
condition is true for any other node n:
?t ? T, F (n.t, A(n), P (n)) ? F (t, A(n), P (n)).
That is, each node?s term has the highest condi-
tional frequency in the context of the node?s an-
cestors and predecessors. Only terms with a con-
ditional frequency above zero are added to a TFIT.
We show in Section 4 how a TFIT taxonomy
can be automatically induced from a given corpus.
But before that, we show that TFITs are useful in
practice and reflect a natural ordering of terms for
application domains where the concept hierarchy
is expressed through the frequency in which terms
appear.
3.2 Example Domain: Address Data
An address taxonomy is a key enabler for address
standardization. Figure 1 shows part of such an ad-
dress taxonomy where the root contains the most
generic term and leaf-level nodes contain the most
specific terms. For emerging economies building
a standardized address taxonomy is a huge chal-
127
Row Term Part of address Category
1 D-15 house number alphanumerical
2 Rawal building name proper noun
3 Complex building name proper noun
4 Behind landmark marker
5 Hotel landmark marker
6 Ruchira landmark proper noun
7 Katre street proper noun
8 Road street marker
9 Jeevan area proper noun
10 Nagar area marker
11 Andheri city (taluk) proper noun
12 East city (taluk) direction
13 Mumbai district proper noun
14 Maharashtra state proper noun
15 400069 ZIP code 6 digit string
Table 1: Example of a tokenized address
lenge. First, new areas and with it new addresses
constantly emerge. Second, there are very limited
conventions for specifying an address (Faruquie et
al., 2010). However, while many developing coun-
tries do not have a postal taxonomy, there is often
no lack of address data to learn a taxonomy from.
Column 2 of Table 1 shows an example of an
Indian address. Although Indian addresses tend to
follow the general principal that more specific in-
formation is mentioned earlier, there is no fixed or-
der for different elements of an address. For exam-
ple, the ZIP code of an address may be mentioned
before or after the state information and, although
ZIP code information is more specific than city in-
formation, it is generally mentioned later in the
address. Also, while ZIP codes often exist, their
use by people is very limited. Instead, people tend
to mention copious amounts of landmark informa-
tion (see for example rows 4-6 in Table 1).
Taking all this into account, there is often not
enough structure available to automatically infer a
taxonomy purely based on the structural or seman-
tic aspects of an address. However, for address
data, the general-to-specific concept hierarchy is
reflected in the frequency with which terms appear
on their own and together with other terms.
It mostly holds that f(s) > f(d) > f(c) >
f(z) where s is a state name, d is a district name,
c is a city name, and z is a ZIP code. How-
ever, sometimes the name of a large city may be
more frequent than the name of a small state. For
example, in a given corpus, the term ?Houston?
(a populous US city) may appear more frequent
than the term ?Vermont? (a small US state). To
avoid that ?Houston? is picked as a node at the first
level of the taxonomy (which should only contain
states), the conditional-frequency constraint intro-
duced in Section 3.1 is enforced for each node in a
TFIT. ?Houston?s state ?Texas? (which is more fre-
quent) is picked before ?Houston?. After ?Texas? is
picked it appears in the ?cannot-also-appear?? list
for all further siblings on the first level, thus giving
?Houston? has a conditional frequency of zero.
We show in Section 5 that an address taxonomy
can be inferred by generating a TFIT taxonomy.
4 Automatically Generating TFITs
We describe a basic algorithm to generate a TFIT
and then show extensions to adapt to different ap-
plication domains.
4.1 Base Algorithm
Algorithm 1 Algorithm for generating a TFIT.
// For initialization T+, T? are empty
// For initialization l,w are zero
genTFIT(T+, T?, C, l, w)
// select most frequent term
tnext = tj with F (tj , T+, T?) is maximal amongst all
tj ? C;
fnext = F (tnext, T+, T?);
if fnext ? support then
//Output node (tj , l, w)
...
// Generate child node
genTFIT(T+ ? {tnext}, T?, C, l + 1, w)
// Generate sibling node
genTFIT(T+, T? ? {tnext}, C, l, w + 1)
end if
To generate a TFIT taxonomy as defined in Sec-
tion 3.1 we recursively pick the most frequent term
given previously chosen terms. The basic algo-
rithm genTFIT is sketched out in Algorithm 1.
When genTFIT is called the first time, T+ and
T? are empty and both level l and width w are
zero. With each call of genTFIT a new node
n in the taxonomy is created with (t, l, w) where
t is the most frequent term given T+ and T?
and l and w capture the position in the taxonomy.
genTFIT is recursively called to generate a child
of n and a sibling for n.
The only input parameter required by our al-
gorithm is support. Instead of adding all terms
with a conditional frequency above zero, we only
add terms with a conditional frequency equal to or
higher than support. The support parameter con-
trols the precision of the resulting TFIT and also
the runtime of the algorithm. Increasing support
increases the precision but also lowers the recall.
128
4.2 Integrating Constraints
Structural as well as semantic constraints can eas-
ily be integrated into the TFIT generation.
We distinguish between taxonomy-level and
node-level structural constraints. For example,
limiting the depth of the taxonomy by introduc-
ing a maxLevel constraint and checking before
each recursive call if maxLevel is reached, is
a taxonomy-level constraint. A node-level con-
straint applies to each node and affects the way
the frequency of terms is determined.
For our example application, we introduce the
following node-level constraint: at each node we
only count terms that appear at specific positions
in records with respect to the current level of the
node. Specifically, we slide (or incrementally in-
crease) a window over the address records start-
ing from the end. For example, when picking the
term ?Washington? as a state name, occurrences of
?Washington? as city or street name are ignored.
Using a window instead of an exact position ac-
counts for positional variability. Also, to accom-
modate varying amounts of landmark information
we length-normalize the position of terms. That is,
we divide all positions in an address by the average
length of an address (which is 10 for our 40 Mil-
lion addresses). Accordingly, we adjust the size of
the window and use increments of 0.1 for sliding
(or increasing) the window.
In addition to syntactical constraints, semantic
constraints can be integrated by classifying terms
for use when picking the next frequent term. In our
example application, markers tend to appear much
more often than any proper noun. For example,
the term ?Road? appears in almost all addresses,
and might be picked up as the most frequent term
very early in the process. Thus, it is beneficial to
ignore marker terms during taxonomy generation
and adding them as a post-processing step.
4.3 Handling Noise
The approach we propose naturally handles noise
by ignoring it, unless the noise level exceeds the
support threshold. Misspelled terms are generally
infrequent and will as such not become part of
the taxonomy. The same applies to incorrect ad-
dresses. Incomplete addresses partially contribute
to the taxonomy and only cause a problem if the
same information is missing too often. For ex-
ample, if more than support addresses with the
city ?Houston? are missing the state ?Texas?, then
?Houston? may become a node at the first level and
appear to be a state. Generally, such cases only ap-
pear at the far right of the taxonomy.
5 Evaluation
We present an evaluation of our approach for ad-
dress data from an emerging economy. We imple-
mented our algorithm in Java and store the records
in a DB2 database. We rely on the DB2 optimizer
to efficiently retrieve the next frequent term.
5.1 Dataset
The results are based on 40 Million Indian ad-
dresses. Each address record was given to us as
a single string and was first tokenized into a se-
quence of terms as shown in Table 1. In a second
step, we addressed spelling variations. There is no
fixed way of transliterating Indian alphabets to En-
glish and most Indian proper nouns have various
spellings in English. We used tools to detect syn-
onyms with the same context to generate a list of
rules to map terms to a standard form (Lin, 1998).
For example, in Table 1 ?Maharashtra? can also be
spelled ?Maharastra?. We also used a list of key-
words to classify some terms as markers such as
?Road? and ?Nagar? shown in Table 1.
Our evaluation consists of two parts. First, we
show results for constructing a TFIT from scratch.
To evaluate the precision and recall we also re-
trieved post office addresses from India Post1,
cleaned them, and organized them in a tree.
Second, we use our approach to enrich the ex-
isting hierarchy created from post office addresses
with additional area terms. To validate the result,
we also retrieved data about which area names ap-
pear within a ZIP code.2 We also verified whether
Google Maps shows an area on its map.3
5.2 Taxonomy Generation
We generated a taxonomy O using all 40 million
addresses. We compare the terms assigned to
category levels district and taluk4 in O with the
tree P constructed from post office addresses.
Each district and taluk has at least one post office.
Thus P covers all districts and taluks and allows
us to test coverage and precision. We compute the
precision and recall for each category level CL as
1http://www.indiapost.gov.in/Pin/pinsearch.aspx
2http://www.whereincity.com/india/pincode/search
3maps.google.com
4Administrative division in some South-Asian countries.
129
Support Recall % Precision %
100 District 93.9 57.4
Taluk 50.9 60.5
200 District 87.9 64.4
Taluk 49.6 66.1
Table 2: Precision and recall for categorizing
terms belonging to the state Maharashtra
RecallCL = # correct paths from root to CL in O# paths from root to CL in P
PrecisionCL = # correct paths from root to CL in O# paths from root to CL in O
Table 2 shows precision and recall for district
and taluk for the large state Maharashtra. Recall
is good for district. For taluk it is lower because a
major part of the data belongs to urban areas where
taluk information is missing. The precision seems
to be low but it has to be noted that in almost 75%
of the addresses either district or taluk informa-
tion is missing or noisy. Given that, we were able
to recover a significant portion of the knowledge
structure.
We also examined a branch for a smaller state
(Kerala). Again, both districts and taluks appear
at the next level of the taxonomy. For a support
of 200 there are 19 entries in O of which all but
two appear in P as district or taluk. One entry is a
taluk that actually belongs to Maharashtra and one
entry is a name variation of a taluk in P . There
were not enough addresses to get a good coverage
of all districts and taluks.
5.3 Taxonomy Augmentation
We used P and ran our algorithm for each branch
in P to include area information. We focus our
evaluation on the city Mumbai. The recall is low
because many addresses do not mention a ZIP
code or use an incorrect ZIP code. However,
the precision is good implying that our approach
works even in the presence of large amounts of
noise.
Table 3 shows the results for ZIP code 400002
and 400004 for a support of 100. We get simi-
lar results for other ZIP codes. For each detected
area we compared whether the area is also listed
on whereincity.com, part of a post office name
(PO), or shown on google maps. All but four
areas found are confirmed by at least one of the
three external sources. Out of the unconfirmed
terms Fanaswadi and MarineDrive seem to
be genuine area names but we could not confirm
DhakurdwarRoad. The term th is due to our
Area Whereincity PO Google
Bhuleshwar yes no yes
Chira Bazar yes no yes
Dhobi Talao no no yes
Fanaswadi no no no
Kalbadevi Road yes yes yes
Marine Drive no no no
Marine Lines yes yes yes
Princess Street no no yes
th no no no
Thakurdwar Road no no no
Zaveri Bazar yes no yes
Charni Road no yes no
Girgaon yes yes yes
Khadilkar Road yes no yes
Khetwadi Road yes no no
Kumbharwada no no yes
Opera House no yes no
Prathna Samaj yes no no
Table 3: Areas found for ZIP code 400002 (top)
and 400004 (bottom)
tokenization process. 16 correct terms out of 18
terms results in a precision of 89%.
We also ran experiments to measure the cov-
erage of area detection for Mumbai without us-
ing ZIP codes. Initializing our algorithm with
Maharshtra and Mumbai yielded over 100 ar-
eas with a support of 300 and more. However,
again the precision is low because quite a few of
those areas are actually taluk names.
Using a large number of addresses is necessary
to achieve good recall and precision.
6 Conclusion
In this paper, we presented a novel approach to
generate a taxonomy for data where terms ex-
hibit an inherent frequency-based hierarchy. We
showed that term frequency can be used to gener-
ate a meaningful taxonomy from address records.
The presented approach can also be used to extend
an existing taxonomy which is a big advantage
for emerging countries where geographical areas
evolve continuously.
While we have evaluated our approach on ad-
dress data, it is applicable to all data sources where
the inherent hierarchical structure is encoded in
the frequency with which terms appear on their
own and together with other terms. Preliminary
experiments on real-time analyst?s stock market
tips 5 produced a taxonomy of (TV station, An-
alyst, Affiliation) with decent precision and recall.
5See Live Market voices at:
http://money.rediff.com/money/jsp/markets home.jsp
130
References
Rakesh Agrawal and Ramakrishnan Srikant. 1994.
Fast algorithms for mining association rules in large
databases. In Proceedings of the 20th International
Conference on Very Large Data Bases, pages 487?
499.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 576?583.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, pages 120?126.
Philipp Cimiano and Johanna Wenderoth. 2007. Au-
tomatic acquisition of ranked qualia structures from
the web. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 888?895.
Philipp Cimiano, Gu?nter Ladwig, and Steffen Staab.
2005. Gimme? the context: context-driven auto-
matic semantic annotation with c-pankow. In Pro-
ceedings of the 14th International Conference on
World Wide Web, pages 332?341.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134.
Tanveer A. Faruquie, K. Hima Prasad, L. Venkata
Subramaniam, Mukesh K. Mohania, Girish Venkat-
achaliah, Shrinivas Kulkarni, and Pramit Basu.
2010. Data cleansing as a transient service. In
Proceedings of the 26th International Conference on
Data Engineering, pages 1025?1036.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, pages 1?7.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 265?272.
Lucja M. Iwaska, Naveen Mata, and Kellyn Kruger.
2000. Fully automatic acquisition of taxonomic
knowledge from large corpora of texts. In Lucja M.
Iwaska and Stuart C. Shapiro, editors, Natural Lan-
guage Processing and Knowledge Representation:
Language for Knowledge and Knowledge for Lan-
guage, pages 335?345.
Govind Kothari, Tanveer A Faruquie, L V Subrama-
niam, K H Prasad, and Mukesh Mohania. 2010.
Transfer of supervision for improved address stan-
dardization. In Proceedings of the 20th Interna-
tional Conference on Pattern Recognition.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1048?1056.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of the 18th
International Joint Conference on Artificial Intelli-
gence, pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics,
pages 768?774.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 113?120.
Shourya Roy and L Venkata Subramaniam. 2006. Au-
tomatic generation of domain models for call cen-
ters from noisy transcriptions. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 737?
744.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems, pages 1297?1304.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 3?7.
Hui Yang and Jamie Callan. 2008. Learning the dis-
tance metric in a personal ontology. In Proceed-
ing of the 2nd International Workshop on Ontolo-
gies and Information Systems for the Semantic Web,
pages 17?24.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 271?279.
131
