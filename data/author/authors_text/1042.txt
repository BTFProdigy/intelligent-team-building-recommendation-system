Alias-i ThreatTrackers?
    Breck Baldwin  
Alias-i inc. 
10 E 39th Street 
Suite 1124 
New York, NY 10016  
breck@alias-i.com 
Bob Carpenter 
Alias-i inc. 
10 E 39th Street 
Suite 1124 
New York, NY 10016  
carp@alias-i.com 
Aaron Ross 
Alias-i inc. 
10 E 39th Street 
Suite 1124 
New York, NY 10016  
ross@alias-i.com 
 
 
ABSTRACT 
 
Alias-i ThreatTrackers are an advanced information access 
application designed around the needs of analysts working through 
a large daily data feed. ThreatTrackers help analysts decompose an 
information gathering topic like the unfolding political situation in 
Iraq into specifications including people, places, organizations and 
relationships. These specifications are then used to collect and 
browse information on a daily basis. The nearest related 
technologies are information retrieval (search engines), document 
categorization, information extraction and named entity 
detection.ThreatTrackers are currently being used in the Total 
Information Awareness program. 
Keywords 
Search, coreference, anaphora, information access. 
1. Description of Demo 
Alias-i ThreatTrackers are an advanced information access 
application designed around the needs of analysts working 
through a large daily data feed. ThreatTrackers help analysts 
decompose an information gathering topic like the unfolding 
political situation in Iraq into specifications including 
people, places, organizations and relationships. These 
specifications are then used to collect and browse 
information on a daily basis. The nearest related 
technologies are information retrieval (search engines), 
document categorization, information extraction and named 
entity detection.ThreatTrackers are currently being used in 
the Total Information Awareness program. 
ThreatTrackers have two major components, first is a 
mentally natural organization of a data feed we call 
Cognitive Indexing(tm), and second, a user interface which 
combines data triage, browsing and ThreatTracker 
authoring. 
2. Cognitive Indexing? 
Cognitive Indexing organizes text data feeds into mentally 
natural units like people, events, things, organizations and 
relationships. For example, the Cognitive Indexing 
representation for an Entity like Osama bin Laden contains 
pointers to all sentences in the data feed that mention him 
Fig 4, including spelling variation (i.e. Usama) and 
pronouns which refer to him. In future versions of the 
system, the Cognitive Indexing representation of Osama will 
include audio mentions of him as well as database entries for 
him. In addition to coreference, Cognitive Indexing supports 
standard key word lookup, relationship detection between 
entities and analyst authored classes of Entities. 
3. ThreatTracker Interface  
 
The ThreatTracker interface provides complete analyst control over 
what is in a ThreatTracker, and provides multiple views into the 
collected data. Included are sentence excerpt summaries, tables of 
relationships between Entities and easy access to original source 
documents. There is also a redundant information filter on 
documents or sentences. ThreatTrackers allow analysts to create 
new categories of Entities like ?Chemical Weapons Precursors ? 
for use inside ThreatTrackers. Once created, ThreatTrackers serve 
as a form of information triage against the data feed. 
4. Acknowledgements  
 
We would like to acknowledge the assistance of Mitre Corp. in 
providing the data feed for the product as well as many analysts 
who lent input to the design. This research has been funded under 
the TIDES program. 
 
 
Fig. 1 ThreatTracker Top Level Interface 
                                                               Edmonton, May-June 2003
                                                              Demonstrations , pp. 3-4
                                                         Proceedings of HLT-NAACL 2003
 Fig.2 View of Individual ThreatTracker on Terrorist Suspects 
Fig. 3 Association Table view of Entities in the same sentence as Osama bin Laden. 
 
Fig. 4  Excerpt Summary for Osama Bin Laden 
Head-Driven Parsing for Word Lattices
Christopher Collins
Department of Computer Science
University of Toronto
Toronto, ON, Canada
ccollins@cs.utoronto.ca
Bob Carpenter
Alias I, Inc.
Brooklyn, NY, USA
carp@alias-i.com
Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, Canada
gpenn@cs.utoronto.ca
Abstract
We present the first application of the head-driven
statistical parsing model of Collins (1999) as a si-
multaneous language model and parser for large-
vocabulary speech recognition. The model is
adapted to an online left to right chart-parser for
word lattices, integrating acoustic, n-gram, and
parser probabilities. The parser uses structural
and lexical dependencies not considered by n-
gram models, conditioning recognition on more
linguistically-grounded relationships. Experiments
on the Wall Street Journal treebank and lattice cor-
pora show word error rates competitive with the
standard n-gram language model while extracting
additional structural information useful for speech
understanding.
1 Introduction
The question of how to integrate high-level knowl-
edge representations of language with automatic
speech recognition (ASR) is becoming more impor-
tant as (1) speech recognition technology matures,
(2) the rate of improvement of recognition accu-
racy decreases, and (3) the need for additional in-
formation (beyond simple transcriptions) becomes
evident. Most of the currently best ASR systems use
an n-gram language model of the type pioneered by
Bahl et al (1983). Recently, research has begun to
show progress towards application of new and bet-
ter models of spoken language (Hall and Johnson,
2003; Roark, 2001; Chelba and Jelinek, 2000).
Our goal is integration of head-driven lexical-
ized parsing with acoustic and n-gram models for
speech recognition, extracting high-level structure
from speech, while simultaneously selecting the
best path in a word lattice. Parse trees generated
by this process will be useful for automated speech
understanding, such as in higher semantic parsing
(Ng and Zelle, 1997).
Collins (1999) presents three lexicalized models
which consider long-distance dependencies within a
sentence. Grammar productions are conditioned on
headwords. The conditioning context is thus more
focused than that of a large n-gram covering the
same span, so the sparse data problems arising from
the sheer size of the parameter space are less press-
ing. However, sparse data problems arising from
the limited availability of annotated training data be-
come a problem.
We test the head-driven statistical lattice parser
with word lattices from the NIST HUB-1 corpus,
which has been used by others in related work (Hall
and Johnson, 2003; Roark, 2001; Chelba and Je-
linek, 2000). Parse accuracy and word error rates
are reported. We present an analysis of the ef-
fects of pruning and heuristic search on efficiency
and accuracy and note several simplifying assump-
tions common to other reported experiments in this
area, which present challenges for scaling up to real-
world applications.
This work shows the importance of careful al-
gorithm and data structure design and choice of
dynamic programming constraints to the efficiency
and accuracy of a head-driven probabilistic parser
for speech. We find that the parsing model of
Collins (1999) can be successfully adapted as a lan-
guage model for speech recognition.
In the following section, we present a review of
recent works in high-level language modelling for
speech recognition. We describe the word lattice
parser developed in this work in Section 3. Sec-
tion 4 is a description of current evaluation metrics,
and suggestions for new metrics. Experiments on
strings and word lattices are reported in Section 5,
and conclusions and opportunities for future work
are outlined in Section 6.
2 Previous Work
The largest improvements in word error rate (WER)
have been seen with n-best list rescoring. The best
n hypotheses of a simple speech recognizer are pro-
cessed by a more sophisticated language model and
re-ranked. This method is algorithmically simpler
than parsing lattices, as one can use a model de-
veloped for strings, which need not operate strictly
left to right. However, we confirm the observa-
tion of (Ravishankar, 1997; Hall and Johnson, 2003)
that parsing word lattices saves computation time by
only parsing common substrings once.
Chelba (2000) reports WER reduction by rescor-
ing word lattices with scores of a structured lan-
guage model (Chelba and Jelinek, 2000), interpo-
lated with trigram scores. Word predictions of the
structured language model are conditioned on the
two previous phrasal heads not yet contained in a
bigger constituent. This is a computationally inten-
sive process, as the dependencies considered can be
of arbitrarily long distances. All possible sentence
prefixes are considered at each extension step.
Roark (2001) reports on the use of a lexical-
ized probabilistic top-down parser for word lattices,
evaluated both on parse accuracy and WER. Our
work is different from Roark (2001) in that we use
a bottom-up parsing algorithm with dynamic pro-
gramming based on the parsing model II of Collins
(1999).
Bottom-up chart parsing, through various forms
of extensions to the CKY algorithm, has been ap-
plied to word lattices for speech recognition (Hall
and Johnson, 2003; Chappelier and Rajman, 1998;
Chelba and Jelinek, 2000). Full acoustic and n-best
lattices filtered by trigram scores have been parsed.
Hall and Johnson (2003) use a best-first probabilis-
tic context free grammar (PCFG) to parse the input
lattice, pruning to a set of local trees (candidate par-
tial parse trees), which are then passed to a version
of the parser of Charniak (2001) for more refined
parsing. Unlike (Roark, 2001; Chelba, 2000), Hall
and Johnson (2003) achieve improvement in WER
over the trigram model without interpolating its lat-
tice parser probabilities directly with trigram prob-
abilities.
3 Word Lattice Parser
Parsing models based on headword dependency re-
lationships have been reported, such as the struc-
tured language model of Chelba and Jelinek (2000).
These models use much less conditioning informa-
tion than the parsing models of Collins (1999), and
do not provide Penn Treebank format parse trees as
output. In this section we outline the adaptation of
the Collins (1999) parsing model to word lattices.
The intended action of the parser is illustrated
in Figure 1, which shows parse trees built directly
upon a word lattice.
3.1 Parameterization
The parameterization of model II of Collins (1999)
is used in our word lattice parser. Parameters are
* tokyo was the couldthatspeculation
unit
yen
the
rise
arise
NN NNP INAUX DT MD VBNNIN
and
in
CC
S
NP
S*
NP VP
*
Figure 1: Example of a partially-parsed word lat-
tice. Different paths through the lattice are simul-
taneously parsed. The example shows two final
parses, one of low probability (S   ) and one of high
probability (S).
maximum likelihood estimates of conditional prob-
abilities ? the probability of some event of inter-
est (e.g., a left-modifier attachment) given a con-
text (e.g., parent non-terminal, distance, headword).
One notable difference between the word lattice
parser and the original implementation of Collins
(1999) is the handling of part-of-speech (POS) tag-
ging of unknown words (words seen fewer than 5
times in training). The conditioning context of the
parsing model parameters includes POS tagging.
Collins (1999) falls back to the POS tagging of Rat-
naparkhi (1996) for words seen fewer than 5 times
in the training corpus. As the tagger of Ratnaparkhi
(1996) cannot tag a word lattice, we cannot back off
to this tagging. We rely on the tag assigned by the
parsing model in all cases.
Edges created by the bottom-up parsing are as-
signed a score which is the product of the inside and
outside probabilities of the Collins (1999) model.
3.2 Parsing Algorithm
The algorithm is a variation of probabilistic
online, bottom-up, left-to-right Cocke-Kasami-
Younger parsing similar to Chappelier and Rajman
(1998).
Our parser produces trees (bottom-up) in a right-
branching manner, using unary extension and binary
adjunction. Starting with a proposed headword, left
modifiers are added first using right-branching, then
right modifiers using left-branching.
Word lattice edges are iteratively added to the
agenda. Complete closure is carried out, and the
next word edge is added to the agenda. This process
is repeated until all word edges are read from the
lattice, and at least one complete parse is found.
Edges are each assigned a score, used to rank
parse candidates. For parsing of strings, the score
for a chart edge is the product of the scores of any
child edges and the score for the creation of the new
edge, as given by the model parameters. This score,
defined solely by the parsing model, will be referred
to as the parser score. The total score for chart
edges for the lattice parsing task is a combination
of the parser score, an acoustic model score, and a
trigram model score. Scaling factors follow those of
(Chelba and Jelinek, 2000; Roark, 2001).
3.3 Smoothing and Pruning
The parameter estimation techniques (smoothing
and back-off) of Collins (1999) are reimplemented.
Additional techniques are required to prune the
search space of possible parses, due to the com-
plexity of the parsing algorithm and the size of the
word lattices. The main technique we employ is a
variation of the beam search of Collins (1999) to
restrict the chart size by excluding low probability
edges. The total score (combined acoustic and lan-
guage model scores) of candidate edges are com-
pared against edge with the same span and cate-
gory. Proposed edges with score outside the beam
are not added to the chart. The drawback to this
process is that we can no longer guarantee that a
model-optimal solution will be found. In practice,
these heuristics have a negative effect on parse accu-
racy, but the amount of pruning can be tuned to bal-
ance relative time and space savings against preci-
sion and recall degradation (Collins, 1999). Collins
(1999) uses a fixed size beam (10   000). We exper-
iment with several variable beam (?b) sizes, where
the beam is some function of a base beam (b) and
the edge width (the number of terminals dominated
by an edge). The base beam starts at a low beam
size and increases iteratively by a specified incre-
ment if no parse is found. This allows parsing to
operate quickly (with a minimal number of edges
added to the chart). However, if many iterations
are required to obtain a parse, the utility of starting
with a low beam and iterating becomes questionable
(Goodman, 1997). The base beam is limited to con-
trol the increase in the chart size. The selection of
the base beam, beam increment, and variable beam
function is governed by the familiar speed/accuracy
trade-off.1 The variable beam function found to al-
low fast convergence with minimal loss of accuracy
is:
?b  blog

w  2  2 
(1)
1Details of the optimization can be found in Collins (2004).
Charniak et al (1998) introduce overparsing as a
technique to improve parse accuracy by continuing
parsing after the first complete parse tree is found.
The technique is employed by Hall and Johnson
(2003) to ensure that early stages of parsing do not
strongly bias later stages. We adapt this idea to
a single stage process. Due to the restrictions of
beam search and thresholds, the first parse found by
the model may not be the model optimal parse (i.e.,
we cannot guarantee best-first search). We there-
fore employ a form of overparsing ? once a com-
plete parse tree is found, we further extend the base
beam by the beam increment and parse again. We
continue this process as long as extending the beam
results in an improved best parse score.
4 Expanding the Measures of Success
Given the task of simply generating a transcription
of speech, WER is a useful and direct way to mea-
sure language model quality for ASR. WER is the
count of incorrect words in hypothesis ?W per word
in the true string W . For measurement, we must as-
sume prior knowledge of W and the best alignment
of the reference and hypothesis strings.2 Errors are
categorized as insertions, deletions, or substitutions.
Word Error Rate 
100 Insertions  Substitutions  DeletionsTotal Words in Correct Transcript (2)
It is important to note that most models ? Mangu
et al (2000) is an innovative exception ? minimize
sentence error. Sentence error rate is the percent-
age of sentences for which the proposed utterance
has at least one error. Models (such as ours) which
optimize prediction of test sentences Wt , generated
by the source, minimize the sentence error. Thus
even though WER is useful practically, it is formally
not the appropriate measure for the commonly used
language models. Unfortunately, as a practical mea-
sure, sentence error rate is not as useful ? it is not
as fine-grained as WER.
Perplexity is another measure of language model
quality, measurable independent of ASR perfor-
mance (Jelinek, 1997). Perplexity is related to the
entropy of the source model which the language
model attempts to estimate.
These measures, while informative, do not cap-
ture success of extraction of high-level information
from speech. Task-specific measures should be used
in tandem with extensional measures such as per-
plexity and WER. Roark (2002), when reviewing
2SCLITE (http://www.nist.gov/speech/
tools/) by NIST is the most commonly used alignment tool.
parsing for speech recognition, discusses a mod-
elling trade-off between producing parse trees and
producing strings. Most models are evaluated ei-
ther with measures of success for parsing or for
word recognition, but rarely both. Parsing mod-
els are difficult to implement as word-predictive
language models due to their complexity. Gener-
ative random sampling is equally challenging, so
the parsing correlate of perplexity is not easy to
measure. Traditional (i.e., n-gram) language mod-
els do not produce parse trees, so parsing metrics
are not useful. However, Roark (2001) argues for
using parsing metrics, such as labelled precision
and recall,3 along with WER, for parsing applica-
tions in ASR. Weighted WER (Weber et al, 1997)
is also a useful measurement, as the most often
ill-recognized words are short, closed-class words,
which are not as important to speech understanding
as phrasal head words. We will adopt the testing
strategy of Roark (2001), but find that measurement
of parse accuracy and WER on the same data set is
not possible given currently available corpora. Use
of weighted WER and development of methods to
simultaneously measure WER and parse accuracy
remain a topic for future research.
5 Experiments
The word lattice parser was evaluated with sev-
eral metrics ? WER, labelled precision and recall,
crossing brackets, and time and space resource us-
age. Following Roark (2001), we conducted evalu-
ations using two experimental sets ? strings and
word lattices. We optimized settings (thresholds,
variable beam function, base beam value) for pars-
ing using development test data consisting of strings
for which we have annotated parse trees.
The parsing accuracy for parsing word lattices
was not directly evaluated as we did not have an-
notated parse trees for comparison. Furthermore,
standard parsing measures such as labelled preci-
sion and recall are not directly applicable in cases
where the number of words differs between the pro-
posed parse tree and the gold standard. Results
show scores for parsing strings which are lower than
the original implementation of Collins (1999). The
WER scores for this, the first application of the
Collins (1999) model to parsing word lattices, are
comparable to other recent work in syntactic lan-
guage modelling, and better than a simple trigram
model trained on the same data.
3Parse trees are commonly scored with the PARSEVAL set
of metrics (Black et al, 1991).
5.1 Parsing Strings
The lattice parser can parse strings by creating a
single-path lattice from the input (all word transi-
tions are assigned an input score of 1.0). The lat-
tice parser was trained on sections 02-21 of the Wall
Street Journal portion of the Penn Treebank (Tay-
lor et al, 2003) Development testing was carried
out on section 23 in order to select model thresh-
olds and variable beam functions. Final testing was
carried out on section 00, and the PARSEVAL mea-
sures (Black et al, 1991) were used to evaluate the
performance.
The scores for our experiments are lower than the
scores of the original implementation of model II
(Collins, 1999). This difference is likely due in part
to differences in POS tagging. Tag accuracy for our
model was 93.2%, whereas for the original imple-
mentation of Collins (1999), model II achieved tag
accuracy of 96.75%. In addition to different tagging
strategies for unknown words, mentioned above, we
restrict the tag-set considered by the parser for each
word to those suggested by a simple first-stage tag-
ger.4 By reducing the tag-set considered by the pars-
ing model, we reduce the search space and increase
the speed. However, the simple tagger used to nar-
row the search also introduces tagging error.
The utility of the overparsing extension can be
seen in Table 1. Each of the PARSEVAL measures
improves when overparsing is used.
5.2 Parsing Lattices
The success of the parsing model as a language
model for speech recognition was measured both
by parsing accuracy (parsing strings with annotated
reference parses), and by WER. WER is measured
by parsing word lattices and comparing the sentence
yield of the highest scoring parse tree to the refer-
ence transcription (using NIST SCLITE for align-
ment and error calculation).5 We assume the pars-
ing performance achieved by parsing strings carries
over approximately to parsing word lattices.
Two different corpora were used in training the
parsing model on word lattices:
  sections 02-21 of the WSJ Penn Treebank (the
same sections as used to train the model for
parsing strings) [1 million words]
4The original implementation (Collins, 1999) of this model
considered all tags for all words.
5To properly model language using a parser, one should sum
parse tree scores for each sentence hypothesis, and choose the
sentence with the best sum of parse tree scores. We choose the
yield of the parse tree with the highest score. Summation is too
computationally expensive given the model ? we do not even
generate all possible parse trees, but instead restrict generation
using dynamic programming.
Exp. OP LP (%) LR (%) CB 0 CB (%)   2 CB (%)
Ref N 88.7 89.0 0.95 65.7 85.6
1 N 79.4 80.6 1.89 46.2 74.5
2 Y 80.8 81.4 1.70 44.3 80.4
Table 1: Results for parsing section 0 (   40 words) of the WSJ Penn Treebank: OP = overparsing, LP/LR
= labelled precision/recall. CB is the average number of crossing brackets per sentence. 0 CB,   2 CB are
the percentage of sentences with 0 or   2 crossing brackets respectively. Ref is model II of (Collins, 1999).
  section ?1987? of the BLLIP corpus (Charniak
et al, 1999) [20 million words]
The BLLIP corpus is a collection of Penn
Treebank-style parses of the three-year (1987-1989)
Wall Street Journal collection from the ACL/DCI
corpus (approximately 30 million words).6 The
parses were automatically produced by the parser
of Charniak (2001). As the memory usage of our
model corresponds directly to the amount of train-
ing data used, we were restricted by available mem-
ory to use only one section (1987) of the total cor-
pus. Using the BLLIP corpus, we expected to get
lower quality parse results due to the higher parse
error of the corpus, when compared to the manually
annotated Penn Treebank. The WER was expected
to improve, as the BLLIP corpus has much greater
lexical coverage.
The training corpora were modified using a utility
by Brian Roark to convert newspaper text to speech-
like text, before being used as training input to the
model. Specifically, all numbers were converted to
words (60  sixty) and all punctuation was re-
moved.
We tested the performance of our parser on the
word lattices from the NIST HUB-1 evaluation task
of 1993. The lattices are derived from a set of
utterances produced from Wall Street Journal text
? the same domain as the Penn Treebank and the
BLLIP training data. The word lattices were previ-
ously pruned to the 50-best paths by Brian Roark,
using the A* decoding of Chelba (2000). The word
lattices of the HUB-1 corpus are directed acyclic
graphs in the HTK Standard Lattice Format (SLF),
consisting of a set of vertices and a set of edges.
Vertices, or nodes, are defined by a time-stamp and
labelled with a word. The set of labelled, weighted
edges, represents the word utterances. A word w is
hypothesized over edge e if e ends at a vertex v la-
belled w. Edges are associated with transition prob-
abilities and are labelled with an acoustic score and
a language model score. The lattices of the HUB-
6The sentences of the HUB-1 corpus are a subset of those
in BLLIP. We removed all HUB-1 sentences from the BLLIP
corpus used in training.
1 corpus are annotated with trigram scores trained
using a 20 thousand word vocabulary and 40 mil-
lion word training sample. The word lattices have a
unique start and end point, and each complete path
through a lattice represents an utterance hypothesis.
As the parser operates in a left-to-right manner, and
closure is performed at each node, the input lattice
edges must be processed in topological order. Input
lattices were sorted before parsing. This corpus has
been used in other work on syntactic language mod-
elling (Chelba, 2000; Roark, 2001; Hall and John-
son, 2003).
The word lattices of the HUB-1 corpus are anno-
tated with an acoustic score, a, and a trigram proba-
bility, lm, for each edge. The input edge score stored
in the word lattice is:
log

Pinput   ? log

a   ? log  lm  (3)
where a is the acoustic score and lm is the trigram
score stored in the lattice. The total edge weight in
the parser is a scaled combination of these scores
with the parser score derived with the model param-
eters:
log

w   ? log

a   ? log  lm   s (4)
where w is the edge weight, and s is the score as-
signed by the parameters of the parsing model. We
optimized performance on a development subset of
test data, yielding ?  1  16 and ?  1.
There is an important difference in the tokeniza-
tion of the HUB-1 corpus and the Penn Treebank
format. Clitics (i.e., he?s, wasn?t) are split
from their hosts in the Penn Treebank (i.e., he ?s,
was n?t), but not in the word lattices. The Tree-
bank format cannot easily be converted into the lat-
tice format, as often the two parts fall into different
parse constituents. We used the lattices modified by
Chelba (2000) in dealing with this problem ? con-
tracted words are split into two parts and the edge
scores redistributed. We followed Hall and John-
son (2003) and used the Treebank tokenization for
measuring the WER. The model was tested with and
without overparsing.
We see from Table 2 that overparsing has little
effect on the WER. The word sequence most easily
parsed by the model (i.e., generating the first com-
plete parse tree) is likely also the word sequence
found by overparsing. Although overparsing may
have little effect on WER, we know from the exper-
iments on strings that overparsing increases parse
accuracy. This introduces a speed-accuracy trade-
off: depending on what type of output is required
from the model (parse trees or strings), the addi-
tional time and resource requirements of overpars-
ing may or may not be warranted.
5.3 Parsing N-Best Lattices vs. N-Best Lists
The application of the model to 50-best word lat-
tices was compared to rescoring the 50-best paths
individually (50-best list parsing). The results are
presented in Table 2.
The cumulative number of edges added to the
chart per word for n-best lists is an order of mag-
nitude larger than for corresponding n-best lattices,
in all cases. As the WERs are similar, we conclude
that parsing n-best lists requires more work than
parsing n-best lattices, for the same result. There-
fore, parsing lattices is more efficient. This is be-
cause common substrings are only considered once
per lattice. The amount of computational savings is
dependent on the density of the lattices ? for very
dense lattices, the equivalent n-best list parsing will
parse common substrings up to n times. In the limit
of lowest density, a lattice may have paths without
overlap, and the number of edges per word would
be the same for the lattice and lists.
5.4 Time and Space Requirements
The algorithms and data structures were designed to
minimize parameter lookup times and memory us-
age by the chart and parameter set (Collins, 2004).
To increase parameter lookup speed, all parameter
values are calculated for all levels of back-off at
training time. By contrast, (Collins, 1999) calcu-
lates parameter values by looking up event counts
at run-time. The implementation was then opti-
mized using a memory and processor profiler and
debugger. Parsing the complete set of HUB-1 lat-
tices (213 sentences, a total of 3,446 words) on av-
erage takes approximately 8 hours, on an Intel Pen-
tium 4 (1.6GHz) Linux system, using 1GB memory.
Memory requirements for parsing lattices is vastly
greater than equivalent parsing of a single sentence,
as chart size increases with the number of divergent
paths in a lattice. Additional analysis of resource
issues can be found in Collins (2004).
5.5 Comparison to Previous Work
The results of our best experiments for lattice- and
list-parsing are compared with previous results in
Table 3. The oracle WER7 for the HUB-1 corpus
is 3.4%. For the pruned 50-best lattices, the oracle
WER is 7.8%. We see that by pruning the lattices
using the trigram model, we already introduce addi-
tional error. Because of the memory usage and time
required for parsing word lattices, we were unable
to test our model on the original ?acoustic? HUB-1
lattices, and are thus limited by the oracle WER of
the 50-best lattices, and the bias introduced by prun-
ing using a trigram model. Where available, we also
present comparative scores of the sentence error rate
(SER) ? the percentage of sentences in the test set
for which there was at least one recognition error.
Note that due to the small (213 samples) size of the
HUB-1 corpus, the differences seen in SER may not
be significant.
We see an improvement in WER for our pars-
ing model alone (?  ?  0) trained on 1 million
words of the Penn Treebank compared to a trigram
model trained on the same data ? the ?Treebank
Trigram? noted in Table 3. This indicates that the
larger context considered by our model allows for
performance improvements over the trigram model
alone. Further improvement is seen with the com-
bination of acoustic, parsing, and trigram scores
(?  1  16   ?  1). However, the combination of
the parsing model (trained on 1M words) with the
lattice trigram (trained on 40M words) resulted in
a higher WER than the lattice trigram alone. This
indicates that our 1M word training set is not suf-
ficient to permit effective combination with the lat-
tice trigram. When the training of the head-driven
parsing model was extended to the BLLIP 1987
corpus (20M words), the combination of models
(?  1  16   ?  1) achieved additional improvement
in WER over the lattice trigram alone.
The current best-performing models, in terms of
WER, for the HUB-1 corpus, are the models of
Roark (2001), Charniak (2001) (applied to n-best
lists by Hall and Johnson (2003)), and the SLM of
Chelba and Jelinek (2000) (applied to n-best lists by
Xu et al (2002)). However, n-best list parsing, as
seen in our evaluation, requires repeated analysis of
common subsequences, a less efficient process than
directly parsing the word lattice.
The reported results of (Roark, 2001) and
(Chelba, 2000) are for parsing models interpolated
with the lattice trigram probabilities. Hall and John-
7The WER of the hypothesis which best matches the true
utterance, i.e., the lowest WER possible given the hypotheses
set.
Training Size Lattice/List OP WER Number of EdgesS D I T (per word)
1M Lattice N 10.4 3.3 1.5 15.2 1788
1M List N 10.4 3.2 1.4 15.0 10211
1M Lattice Y 10.3 3.2 1.4 14.9 2855
1M List Y 10.2 3.2 1.4 14.8 16821
20M Lattice N 9.0 3.1 1.0 13.1 1735
20M List N 9.0 3.1 1.0 13.1 9999
20M Lattice Y 9.0 3.1 1.0 13.1 2801
20M List Y 9.0 3.3 0.9 13.3 16030
Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%),
D = deletions (%), I = insertions (%), T = total WER (%). Variable beam function: ?b  b  log  w  2  2  .
Training corpora: 1M = Penn Treebank sections 02-21; 20M = BLLIP section 1987.
Model n-best List/Lattice Training Size WER (%) SER (%)
Oracle (50-best lattice) Lattice 7.8
Charniak (2001) List 40M 11.9
Xu (2002) List 20M 12.3
Roark (2001) (with EM) List 2M 12.7
Hall (2003) Lattice 30M 13.0
Chelba (2000) Lattice 20M 13.0
Current (?  1  16   ?  1) List 20M 13.1 71.0
Current (?  1  16   ?  1) Lattice 20M 13.1 70.4
Roark (2001) (no EM) List 1M 13.4
Lattice Trigram Lattice 40M 13.7 69.0
Current (?  1  16   ?  1) List 1M 14.8 74.3
Current (?  1  16   ?  1) Lattice 1M 14.9 74.0
Current (?  ?  0) Lattice 1M 16.0 75.5
Treebank Trigram Lattice 1M 16.5 79.8
No language model Lattice 16.8 84.0
Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER =
sentence error rate. WER = word error rate. ?Speech-like? transformations were applied to all training
corpora. Xu (2002) is an implementation of the model of Chelba (2000) for n-best list parsing. Hall (2003)
is a lattice-parser related to Charniak (2001).
son (2003) does not use the lattice trigram scores
directly. However, as in other works, the lattice
trigram is used to prune the acoustic lattice to the
50 best paths. The difference in WER between
our parser and those of Charniak (2001) and Roark
(2001) applied to word lists may be due in part to the
lower PARSEVAL scores of our system. Xu et al
(2002) report inverse correlation between labelled
precision/recall and WER. We achieve 73.2/76.5%
LP/LR on section 23 of the Penn Treebank, com-
pared to 82.9/82.4% LP/LR of Roark (2001) and
90.1/90.1% LP/LR of Charniak (2000). Another
contributing factor to the accuracy of Charniak
(2001) is the size of the training set ? 20M words
larger than that used in this work. The low WER
of Roark (2001), a top-down probabilistic parsing
model, was achieved by training the model on 1 mil-
lion words of the Penn Treebank, then performing a
single pass of Expectation Maximization (EM) on a
further 1.2 million words.
6 Conclusions
In this work we present an adaptation of the parsing
model of Collins (1999) for application to ASR. The
system was evaluated over two sets of data: strings
and word lattices. As PARSEVAL measures are not
applicable to word lattices, we measured the pars-
ing accuracy using string input. The resulting scores
were lower than that original implementation of the
model. Despite this, the model was successful as a
language model for speech recognition, as measured
by WER and ability to extract high-level informa-
tion. Here, the system performs better than a simple
n-gram model trained on the same data, while si-
multaneously providing syntactic information in the
form of parse trees. WER scores are comparable to
related works in this area.
The large size of the parameter set of this parsing
model necessarily restricts the size of training data
that may be used. In addition, the resource require-
ments currently present a challenge for scaling up
from the relatively sparse word lattices of the NIST
HUB-1 corpus (created in a lab setting by profes-
sional readers) to lattices created with spontaneous
speech in non-ideal conditions. An investigation
into the relevant importance of each parameter for
the speech recognition task may allow a reduction in
the size of the parameter space, with minimal loss of
recognition accuracy. A speedup may be achieved,
and additional training data could be used. Tun-
ing of parameters using EM has lead to improved
WER for other models. We encourage investigation
of this technique for lexicalized head-driven lattice
parsing.
Acknowledgements
This research was funded in part by the Natural Sci-
ences and Engineering Research Council (NSERC)
of Canada. Advice on training and test data was
provided by Keith Hall of Brown University.
References
L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A maxi-
mum likelihood approach to continuous speech recog-
nition. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 5:179?190.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of Fourth DARPA
Speech and Natural Language Workshop, pages 306?
311.
J.-C. Chappelier and M. Rajman. 1998. A practical
bottom-up algorithm for on-line parsing with stochas-
tic context-free grammars. Technical Report 98-284,
Swiss Federal Institute of Technology, July.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-Based Best-First Chart Parsing. In
6th Annual Workshop for Very Large Corpora, pages
127?133.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 1999. BLLIP 1987-89
WSJ Corpus Release 1. Linguistic Data Consortium.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 2000 Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 132?129, New
Brunswick, U.S.A.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the ACL.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14:283?332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure
for Natural Language Modeling. Ph.D. thesis, Johns
Hopkins University.
Christopher Collins. 2004. Head-Driven Probabilistic
Parsing for Word Lattices. M.Sc. thesis, University of
Toronto.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the 2nd Con-
ference on Empirical Methods in Natural Language
Processing.
Keith Hall and Mark Johnson. 2003. Language mod-
eling using efficient best-first bottom-up parsing. In
Proceedings of the IEEE Automatic Speech Recogni-
tion and Understanding Workshop.
Frederick Jelinek. 1997. Information Extraction From
Speech And Text. MIT Press.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Hwee Tou Ng and John Zelle. 1997. Corpus-based
approaches to semantic interpretation in natural lan-
guage processing. AI Magazine, 18:45?54.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Conference on Empirical
Methods in Natural Language Processing, May.
Mosur K. Ravishankar. 1997. Some results on search
complexity vs accuracy. In DARPA Speech Recogni-
tion Workshop, pages 104?107, February.
Brian Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models, and Applica-
tions. Ph.D. thesis, Brown University.
Brian Roark. 2002. Markov parsing: Lattice rescoring
with a statistical parser. In Proceedings of the 40th
Annual Meeting of the ACL, pages 287?294.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini,
2003. The Penn TreeBank: An Overview, chapter 1.
Kluwer, Dordrecht, The Netherlands.
Hans Weber, Jo?rg Spilker, and Gu?nther Go?rz. 1997.
Parsing n best trees from a word lattice. Kunstliche
Intelligenz, pages 279?288.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002.
A study on richer syntactic dependencies in structured
language modeling. In Proceedings of the 40th An-
nual Meeting of the ACL, pages 191?198.
Scaling High-Order Character Language Models to Gigabytes
Bob Carpenter
Alias-i, Inc.
181 North 11th St., #401, Brooklyn, NY 11211
carp@colloquial.com
Abstract
We describe the implementation steps re-
quired to scale high-order character lan-
guage models to gigabytes of training
data without pruning. Our online models
build character-level PAT trie structures on
the fly using heavily data-unfolded imple-
mentations of an mutable daughter maps
with a long integer count interface. Ter-
minal nodes are shared. Character 8-gram
training runs at 200,000 characters per
second and allows online tuning of hy-
perparameters. Our compiled models pre-
compute all probability estimates for ob-
served n-grams and all interpolation pa-
rameters, along with suffix pointers to
speedup context computations from pro-
portional to n-gram length to a constant.
The result is compiled models that are
larger than the training models, but exe-
cute at 2 million characters per second on
a desktop PC. Cross-entropy on held-out
data shows these models to be state of the
art in terms of performance.
1 Introduction
Character n-gram language models have been ap-
plied to just about every problem amenable to sta-
tistical language modeling. The implementation
we describe here has been integrated as the source
model in a general noisy-channel decoder (with ap-
plications to spelling correction, tokenization and
case normalization) and the class models for sta-
tistical classification (with applications including
spam filtering, topic categorization, sentiment analy-
sis and word-sense disambiguation). In addition to
these human language tasks, n-grams are also popu-
lar as estimators for entropy-based compression and
source models for cryptography. (Teahan, 2000)
and (Peng, 2003) contain excellent overviews of
character-level models and their application from a
compression and HMM perspective, respectively.
Our hypothesis was that language-model smooth-
ing would behave very much like the classifiers ex-
plored in (Banko and Brill, 2001), in that more data
trumps better estimation technique. We managed
to show that the better of the interpolation mod-
els used in (Chen and Goodman, 1996), namely
Dirichlet smoothing with or without update exclu-
sion, Witten-Bell smoothing with or without update
exclusion, and absolute discounting with update ex-
clusion converged for 8-grams after 1 billion charac-
ters to cross entropies of 1.43+/-0.01. The absolute
discounting with update exclusion is what Chen and
Goodman refer to as the Kneser-Ney method, and
it was the clear winner in their evaluation. They
only tested non-parametric Witten-Bell with a sub-
optimal hyperparameter setting (1.0, just as in Wit-
ten and Bell?s original implementation). After a bil-
lion characters, roughly 95 percent of the characters
were being estimated from their highest-order (7)
context. The two best models, parametric Witten-
Bell and absolute discounting with update exclu-
sion (aka Kneser-Ney), were even closer in cross-
entropy, and depending on the precise sample (we
kept rolling samples as described below), and after a
million or so characters, the differences even at the
higher variance 12-grams were typically in the +/-
0.01 range. With a roughly 2.0 bit/character devi-
ation, a 10,000 character sample, which is the size
we used, leads to a 2? (95.45%) confidence interval
of +/-0.02, and the conclusion that the differences
between these systems was insignificant.
Unlike in the token-based setting, we are not op-
timistic about the possibility of improving these re-
sults dramatically by clustering character contexts.
The lower-order models are very well trained with
existing quantities of data and do a good job of
this kind of smoothing. We do believe that train-
ing hyperparameters for different model orders in-
dependently might improve cross-entropy fraction-
ally; we found that training them hierarchically,
as in (Samuelsson, 1996), actually increased cross-
entropy. We believe this is a direct correlate of the
effectiveness of update exclusion; the lower-order
models do not need to be the best possible models
of those orders, but need to provide good estimates
when heavily weighted, as in smoothing. The global
optimization allows a single setting to balance these
attributes, but optimizing each dimension individu-
ally should do even better. But with the number of
estimates taking place at the highest possible orders,
we do not believe the amount of smoothing will have
that large an impact overall.
These experiments had a practical goal ? we
needed to choose a language modeling implemen-
tation for LingPipe and we didn?t want to take the
standard Swiss Army Knife approach because most
of our users are not interested in running experi-
ments on language modeling, but rather using lan-
guage models in applications such as information
retrieval, classification, or clustering. These appli-
cations have actually been shown to perform better
on the basis of character language models than to-
ken models ((Peng, 2003)). In addition, character-
level models require no decisions about tokeniza-
tion, token normalization and subtoken modeling (as
in (Klein et al, 2003)).
We chose to include the Witten-Bell method in
our language modeling API because it is derived
from full corpus counts, which we also use for col-
location and relative frequency statistics within and
across corpora, and thus the overall implementation
effort was simpler. For just language modeling, an
update exclusion implementation of Kneser-Ney is
no more complicated than Witten-Bell.
In this paper, we describe the implementation de-
tails behind storing the model counts, how we sam-
ple the training character stream to provide low-cost,
online leave-one-out style hyperparameter estima-
tion, and how we compile the models and evaluate
them over text inputs to achieve linear performance
that is nearly independent of n-gram length. We also
describe some of the design patterns used at the in-
terface level for training and execution. As far as
we know, the online leave-one-out analysis is novel,
though there are epoch-based precursors in the com-
pression literature.
As far as we know, no one has built a charac-
ter language model implementation that will come
close to the one presented here in terms of scala-
bility. This is largely because they have not been
designed for the task rather than any fundamental
limitation. In fact, we take the main contribution
of this paper to be a presentation of simple data
sharing and data unfolding techniques that would
also apply to token-level language models. Before
starting our presentation, we?ll review some of the
limitations of existing systems. For a start, none
of the systems of which we are aware can scale to
64-bit values for counts, which is necessary for the
size models we are considering without pruning or
count scaling. It?s simply easier to find 4 billion in-
stances of a character than of a token. In fact, the
compression models typically use 16 bits for storing
counts and then just scale downward when neces-
sary, thus not even trying to store a full set of counts
for even modest corpora. The standard implemen-
tations of character models in the compression liter-
ature represent ordinary trie nodes as arrays, which
is hugely wasteful for large sparse implementations;
they represent PAT-trie nodes as pointers into the
original text plus counts, which works well for long
n-gram lengths (32) over small data sets (1 MB) but
does not scale well for reasonable n-gram lengths (8-
12) over larger data sets (100MB-1GB). The stan-
dard token-level language models used to restrict
attention to 64K tokens and thus require 16-bit to-
ken representatives per node just as our character-
based approach; with the advent of large vocabu-
lary speech recognition, they now typically use 32-
bits per node just to represent the token. Arrays of
daughter nodes and lack of sharing of low-count ter-
minal nodes were the biggest space hogs in our ex-
periments, and as far as we know, none of the stan-
dard approaches take the immutable data unfolding
approach we adopt to eliminate this overhead. Thus
we would like to stress again that existing character-
level compression and token-level language model-
ing systems were simply not designed for handling
large character-level models.
We would also like to point out that the stan-
dard finite state machine implementations of lan-
guage models do not save any space over the trie-
based implementations, typically only approximate
smoothing using backoff rather than interpolation,
and further suffer from a huge space explosion when
determinized. The main advantage of finite state ap-
proaches is at the interface level in that they work
well with hand-written constraints and can interface
on either side of a given modeling problem. For
instance, typical language models implemented as
trivial finite state transducers interface neatly with
triphone acoustic models on the one side and with
syntactic grammars on the other. When placed in
that context, the constraints from the grammar can
often create an overall win in space after composi-
tion.
2 Online Character Language Models
For generality, we use the 16-bit subset of unicode as
provided by Java 1.4.2 to represent characters. This
presents an additional scaling problem compared to
ASCII or Latin1, which fit in 7 and 8 bits.
Formally, if Char is a set of characters, a language
model is defined to be a mapping P from the set Char?
of character sequences into non-negative real num-
bers. A process language model is normalized over
sequences of length n: ?X?Char?,|X |=n P(X) = 1.0.
We also implement bounded language models which
normalize over all sequences, but their implementa-
tion is close enough to the process models that we
do not discuss them further here. The basic inter-
faces are provided in Figure 1 (with names short-
ened to preserve space). Note that the process and
sequence distribution is represented through marker
interfaces, whereas the cross-cutting dynamic lan-
guage models support training and compilation, as
well as the estimation inherited from the language
interface LM {
double log2Prob(char[] cs,
int start, int end);
}
interface ProcessLM extends LM {
}
interface SequenceLM extends LM {
}
interface DynamicLM extends LM {
double train(char[] cs,
int start, int end);
void compile(ObjectOutput out)
throws IOException;
}
Figure 1: Language Model Interface
model interface.
We now turn to the statistics behind character-
level langauge models. The chain rule factors
P(x0, . . . ,xk?1) = ?i<k P(xi|x0, . . . ,xi?1). An n-
gram language model estimates a character using
only the last n ? 1 symbols, ?P(xk|x0, . . . ,xk?1) =
?P(xk|xk?n+1, . . . ,xk?1); we follow convention in de-
noting generic estimators by ?P.
The maximum likelihood estimator for n-grams
is derived from frequency counts for sequence X
and symbol c, PML(c|X) = count(Xc)/extCount(X),
where count(X) is the number of times the sequence
X was observed in the training data and extCount(X)
is the number of single-symbol extensions of X
observed: extCount(X) = ?c?Char count(Xc). When
training over one or more short samples, the dis-
parity between count(X) and extCount(X) can be
large: for abracadabra, count(a) = 5, count(bra) =
2, extCount(a) = 4, and extCount(bra) = 1.
We actually provide two implementations of lan-
guage models as part of LingPipe. For language
models as random processes, there is no padding.
They correspond to normalizing over sequences of a
given length in that the sum of probabilities for char-
acter sequences of length k will sum to 1.0. With
a model that inserts begin-of-sequence and end-of-
sequence characters and estimates only the end-of-
sequence character, normalization is over all strings.
Statistically, these are very different models. In
practice, they are only going to be distinguishable
if the boundaries are very significant and the to-
tal string length is relatively small. For instance,
they are not going to make much difference in esti-
mating probabilities of abstracts of 1000 characters,
even though the start and ends are significant (e.g.
capitals versus punctuation being preferred at be-
ginning and end of abstracts) because cross-entropy
will be dominated by the other 1000 characters. On
the other hand, for modeling words, for instance
as a smoothing step for token-level part-of-speech,
named-entity or language models, the begin/end of
a word will be significant, representing capitaliza-
tion, prefixes and suffixes in a language. In fact, this
latter motivation is why we provide padded models.
It is straightforward to implement the padded mod-
els on top of the process models, which is why we
discuss the process models here. But note that we
do not pad all the way to maximum n-gram length,
as that would bias the begin/end statistics for short
words.
We use linear interpolation to form a mixture
model of all orders of maximum likelihood es-
timates down to the uniform estimate PU(c) =
1/|Char|. The interpolation ratio ? (dX) ranges be-
tween 0 and 1 depending on the context dX .
?P(c|dX) = ? (dX)PML(c|dX)
+ (1?? (dX)) ?P(c|X)
?P(c) = ? ()PML(c)
+ (1?? ())(1/|Char|)
The Witten-Bell estimator computed the interpo-
lation parameter ? (X) using only overall training
counts. The best performing model that we evalu-
ated is parameterized Witten-Bell interpolation with
hyperparameter K, for which the interpolation ratio
is defined to be:
? (X) = extCount(X)
extCount(X)+K ? numExts(X)
We take numExts(X) = |{c|count(Xc)> 0}| to be the
number of different symbols observed following the
sequence X in the training data. The original Witten-
Bell estimator set K = 1. We optimize the hyperpa-
rameter K online (see the next section).
3 Online Models and Hyperparameter
Estimation
A language model is online if it can be estimated
from symbols as they arrive. An advantage of online
models is that they are easy to use for adaptation to
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 100000  1e+006  1e+007  1e+008
O
pt
im
al
 H
yp
er
pa
ra
m
et
er
 S
et
tin
g
Amount of Data (characters)
2-gram
4-gram
8-gram
12-gram
Figure 2: Optimal Hyperparameter Settings for
Witten-Bell
documents or styles, hence their inclusion in com-
mercial dictation packages such as DragonDictate
and ViaVoice. Another advantage is that they are
easy to integrate into tag-a-little/learn-a-little sys-
tems such as MITRE?s Alembic Workbench.
With online models, we are able to estimate hy-
perparameters using an online form of leave-one-out
analysis (Ney et al, 1995). This can be performed
in a number of ways as long as the model efficiently
estimates likelihoods given a set of hyperparameter
settings. We opted for the simplest technique we
could muster to find the right settings. This was
made easier because we only have a single hyperpa-
rameter whose behavior is fairly flat around the op-
timal setting and because the optimal setting didn?t
change quickly with increasing data. The optimal
settings are shown in Figure 2. Also note that the op-
timal value is rarely at 1 except for very low-order n-
grams. To save the complexity of maintaining an in-
terval around the best estimate do do true hill climb-
ing, we simply kept rolling averages of values log-
arithmically spaced from 1/4 to 32. We also imple-
mented a training method that kept track of the last
10,000 character estimates (made before the charac-
ters were used for training, of course). We used a cir-
cular queue for this data structure because its size is
fixed and it allowed a constant time insert of the last
recorded value. We used one circular queue for each
hyperparameter setting, thus storing around 5MB or
so worth of samples. These samples can be used
to provide an estimate of the best hyperparameter
at any given point in the algorithm?s execution. We
used this explicit method rather than the much less
costly rolling average method so that results would
be easier to report. We actually believe just keep-
ing a rolling average of measured cross-entropies on
online held-out samples is sufficient.
We also sampled the character stream rather than
estimating each character before training. With a gi-
gabyte of characters, we only needed to sample 1
in 100,000 characters to find enough data for esti-
mates. At this rate, online hyperparameter estimate
did not measurably affect training time, which was
dominated by simply constructing the trie.
We only estimated a single hyperparameter rather
than one for each order to avoid having to solve a
multivariate estimation problem; although we can
collect the data online, we would either have to im-
plement an EM-like solution or spend a lot time per
estimate iterating to find optimal parameters. This
may be worthwhile for cases where less data is avail-
able. As the training data increased, the sensitivity to
training parameters decreased. Counterintuitively,
we found that recursively estimating each order from
low to high, as implemented in (Samuelsson, 1996),
actually increased entropy considerably. Clearly the
estimator is using the fact that lower-order estimates
should not necessarily be optimal for use on their
own. This is a running theme of the discounting
methods of smoothing such as absolute discounting
or Kneser-Ney.
Rather than computing each estimate for hyperpa-
rameter and n-gram length separately, we first gather
the counts for each suffix and each context and the
number of outcomes for that context. This is the ex-
pensive step, as it require looking up counts in the
trie structure. Extension counts require a loop over
all the daughters of a context node in the trie be-
cause we did not have enough space to store them on
nodes. With all of these counts, the n-gram etimates
for each n and each hyperparameter setting can be
computed from shortest to longest, with the lower
order estimates contributing the smoothed estimate
for the next higher order.
4 Substring Counters
Our n-gram language models derive estimates from
counts of substrings of length n or less in the training
interface Node {
Node increment(char[] cs,
int start, int end);
long count(char[] cs,
int start, int end);
long extCount(char[] cs,
int start, int end);
int numExts(char[] cs,
int start, int end);
Node prune(long minCount);
}
Figure 3: Trie Node Interface
corpus. Our counter implementation was the trick-
iest component to scale as it essentially holds the
statistics derived from the training data. It contains
statistics sufficient to implement all of the estimators
defined above. The only non-trivial case is Kneser-
Ney, which is typically implemented using the tech-
nique known in the compression literature as ?up-
date exclusion? (Moffat, 1990). Under update ex-
clusion, if a count ?abc? is updated and the context
?ab? was known, then counts for ?a? and ?ab? are
excluded from the update process. We actually com-
pute these counts from the total counts by noting that
the update exclusion count is equal to the number of
unique characters found following a shorter context.
That is, the count for ?ab? for smoothing is equal to
the number of characters ?x? such that ?xab? has a
non-zero count, because these are the situations in
which the count of ?ab? is not excluded. This is not
an efficient way to implement update exclusion, but
merely an expedient so we could share implementa-
tions for experimental purposes. Straight update ex-
clusion is actually more efficient to implement than
full counts, but we wanted the full set of character
substring counts for other purposes, as well as lan-
guage modeling.
Our implementation relies heavily on a data un-
folded object-oriented implementation of Patricia
tries. Unlike the standard suffix tree algorithms for
constructing this trie for all substrings as in (Cleary
and Teahan, 1997), we limit the length and make
copies of characters rather than pointing back into
the original source. This is more space efficient than
the suffix-tree approach for our data set sizes and n-
gram lengths.
The basic node interface is as shown in Figure 3.
Note that the interface is in terms of long integer val-
ues. This was necessary to avoid integer overflow in
our root count when data size exceeded 2 GB and
our 1-gram counts when data sizes exceeded 5 or
6GB. A widely used alternative used for compres-
sion is to just scale all the counts by dividing by
two (and typically pruning those that go to zero);
this allows PPM to use 8-bit counters at the cost of
arithmetic precision ((Moffat, 1990)). We eschew
pruning because we also use the counts to find sig-
nificant collocations. Although most collocation and
significance statistics are not affected by global scal-
ing, cross-entropy suffers tremendously if scaling is
done globally rather than only on the nodes that need
it.
Next note that the interface is defined in terms
of indexed character slices. This obviates a huge
amount of otherwise unnecessary object creation
and garbage collection. It is simply not efficient
enough, even with the newer generational garbage
collectors, to create strings or even lighter character
sequences where needed on the heap; slice indices
can be maintained in local variables.
The increment method increments the count for
each prefix of the specified character slice. The
count method returns the count of a given character
sequence, extensionCount the count of all one-
character extensions, numExtensions the number
of extensions. The extensions method returns
all the observed extensions of a character sequence,
which is useful for enumerating over all the nodes in
the trie.
Global pruning is implemented, but was not nec-
essary for our scalability experiments. It is neces-
sary for compilation; we could not compile models
nearly as large as those kept online. Just the size
of the floating point numbers (two per node for es-
timate and interpolation) lead to 8 bytes per node.
In just about every study every undertaken, includ-
ing our informal ones, unpruned models have out-
performed pruned ones. Unfortunately, applications
will typically not have a gigabyte of memory avail-
able for models. The best performing models for a
given size are those trained on as much data avail-
able and pruned to the specified size. Our prun-
ing is simply a minimum count approach, because
the other methods have not been shown to improve
much on this baseline.
Finally, note that both the increment and prune
ArrayDtrNode
B, S, I, L
OneDtrNode
B, S, I, L
Node
AbstractNode
DtrNode PatNode
TerminalNode
B, S, I, L
TwoDtrNode
B, S, I, L
ThreeDtrNode
B, S, I, L
Pat1Node
1, 2, 3, B, S, I, L
Pat2Node
1, 2, 3, B, S, I, L
Pat3Node
1, 2, 3, B, S, I, L
Pat4Node
1, 2, 3, B, S, I, L
PatArrayNode
1, 2, 3, B, S, I, L
Figure 4: Unfolded Trie Classes
methods return nodes themselves. This is to sup-
port the key implementation technique for scalabil-
ity ? replacing immutable objects during increments.
Rather than having a fixed mutable node representa-
tion, nodes can return results that are essentially re-
placements for themselves. For instance, there is an
implementation of Node that provides a count as a
byte (8 bits) and a single daughter. If that class gets
incremented above the byte range, it returns a node
with a short-based counter (16 bits) and a daughter
that?s the result of incrementing the daughter. If the
class gets incremented for a different daughter path,
then it returns a two-daughter implementation. Of
course, both of these can happen, with a new daugh-
ter that pushes counts beyond the byte range. This
strategy may be familiar to readers with experience
in Prolog (O?Keefe, 1990) or Lisp (Norvig, 1991),
where many standard algorithms are implemented
this way.
A diagram of the implementations of Node is pro-
vided in Figure 4. At the top of the diagram is the
Node interface itself. The other boxes all represent
abstract classes, with the top class, AbstractNode,
forming an abstract adapter for most of the utility
methods in Node (which were not listed in the inter-
face).
The abstract subclass DtrNode is used for nodes
with zero or more daughters. It requires its exten-
sions to return parallel arrays of daughters and char-
acters and counts from which it implements all the
update methods at a generic level.
abstract class TwoDtrNode
extends DtrNode {
final char mC1; final Node mDtr1
final char mC2; final node mDtr2;
TwoDtrNode(char c1, Node dtr1,
char c2, Node dtr2,
mC1 = c1; mDtr1 = dtr1;
mC2 = c2; mDtr2 = dtr2;
}
Node getDtr(char c) {
return c == mC1
? mDaughter1
: ( c == mC2
? mDaughter2
: null );
}
[] chars() {
return new char[] { mC1, mC2 };
}
Node[] dtrs() {
return new Node[] { mDaughter1,
mDaughter2 };
}
int numDtrs() { return 2; }
}
Figure 5: Two Daughter Node Implementation
The subclass TerminalNode is used for nodes
with no daughters. Its implementation is particu-
larly simple because the extension count, the num-
ber of extensions and the count for any non-empty
sequence starting at this node are zero. The nodes
with non-empty daughters are not much more com-
plex. For instance, the two-daughter node abstract
class is shown in Figure 5.
All daughter nodes come with four concrete im-
plementations, based on the size of storage allocated
for counts: byte (8 bits), short (16 bits), int (32
bits), or long (64 bits). The space savings from only
allocating bytes or shorts is huge. These concrete
implementations do nothing more than return their
own counts as long values. For instance, the short
implementation of three-daughter nodes is shown in
Figure 6. Note that because these nodes are not pub-
lic, the factory can be guaranteed to only call the
constructor with a count that can be cast to a short
value and stored.
Increments are performed by the superclass
final class ThreeDtrNodeShort
extends ThreeDtrNode {
final short mCount;
ThreeDtrNodeShort(char c1, Node dtr1,
char c2, Node dtr2,
char c3, Node dtr3,
long count) {
super(c1,dtr1,c2,dtr2,c3,dtr3);
mCount = (short) count;
}
long count() { return mCount; }
}
Figure 6: Three Daughter Short Node
and will call constructors of the appropriate
size. The increment method as defined in
AbstractDtrNode is given in Figure 7. This
method increments all the suffixes of a string.
The first line just increments the local node if the
array slice is empty; this involves taking its charac-
ters, its daughters and calling the factory with one
plus its count to generate a new node. This gener-
ates a new immutable node. If the first character in
the slice is an existing daughter, then the daughter is
incremented and the result is used to increment the
entire node. Note the assignment to dtrs[k] after
the increment; this is to deal with the immutability.
The majority of the code is just dealing with the case
where a new daughter needs to be inserted. Of spe-
cial note here is the factory instance called on the
remaining slice; this will create a PAT node. This
appears prohibitively expensive, but we refactored to
this approach from a binary-tree based method with
almost no noticeable hit in speed; most of the arrays
stabilize after very few characters and the resizings
of big arrays later on is quite rare. We even replaced
the root node implementation which was formerly
a map because it was not providing a measurable
speed boost.
Once the daughter characters and daughters are
marshalled, the factory calls the appropriate con-
structor based on the number of the character and
daughters. The factory then just calls the appropri-
ately sized constructor as shown in Figure 8.
Unlike other nodes, low count terminal nodes are
stored in an array and reused. Thus if the result of
an increment is within the cache bound, the stored
Node increment(char[] cs,
int start, int end) {
// empty slice; incr this node
if (start == end)
return NodeFactory
.createNode(chars(),dtrs(),
count()+1l);
char[] dtrCs = chars();
// search for dtr
int k = Arrays.binarySearch(dtrCs,
cs[start]);
Node[] dtrs = dtrs();
if (k >= 0) { // found dtr
dtrs[k] = dtrs[k]
.increment(cs,start+1,end);
return NodeFactory
.createNode(dtrCs,dtrs,
count()+1l);
}
// insert new dtr
char[] newCs = new char[dtrs.length+1];
Node[] newDtrs = new Node[dtrs.length+1];
int i = 0;
for (; i < dtrs.length
&& dtrCs[i] < cs[start];
++i) {
newCs[i] = dtrCs[i];
newDtrs[i] = dtrs[i];
}
newCs[i] = cs[start];
newDtrs[i] = NodeFactory
.createNode(cs,start+1,
end,1);
for (; i < dtrCs.length; ++i) {
newCs[i+1] = dtrCs[i];
newDtrs[i+1] = dtrs[i];
}
return NodeFactory
.createNode(newCs,newDtrs,
count()+1l);
}
Figure 7: Increment in AbstractDtrNode
version is returned. Because terminal nodes are im-
mutable, this does not cause problems with consis-
tency. In practice, terminal nodes are far and away
the most common type of node, and the greatest sav-
ing in space came from carefully coding terminal
nodes.
The abstract class PatNode implements a so-
called ?Patricia? trie node, which has a single chain
of descendants each of which has the same count.
There are four fixed-length implementations for the
one, two, three and four daughter case. For these
implementations, the daughter characters are stored
in member variables. For the array implementa-
tion, PatArrayNode, the daughter chain is stored
static Node createNode(char c, Node dtr,
long n) {
if (n <= Byte.MAX_VALUE)
return new OneDtrNodeByte(c,dtr,n);
if (n <= Short.MAX_VALUE)
return new OneDtrNodeShort(c,dtr,n);
if (n <= Integer.MAX_VALUE)
return new OneDtrNodeInt(c,dtr,n);
return new OneDtrNodeLong(c,dtr,n);
}
Figure 8: One Daughter Factory Method
as an array. Like the generic daughter nodes, PAT
nodes contain implementations for byte, short, int
and long counters. They also contain constant im-
plementations for one, two and three counts. We
found in profiling that the majority of PAT nodes
had counts below four. By providing constant imple-
mentations, no memory at all is used for the counts
(other than a single static component per class). Pat
nodes themselves are actually more common that
regular daughter nodes in high-order character tries,
because most long contexts are deterministic. As
n-gram order increases, so does the proportion of
PAT nodes. Implementing increments for PAT nodes
is only done once in the abstract class PatNode.
Each PAT node implementation supplied an array in
a standardized interface to the implementations in
PatNode. That array is created as needed and only
lives long enough to carry out the required increment
or lookup. Java?s new generational garbage collector
is fairly efficient at dealing with garbage collection
for short-lived objects such as the trie nodes.
5 Compilation
Our online models are tuned primarily for scalabil-
ity, and secondarily for speed of substring counts.
Even the simplest model, Witten-Bell, requires for
each context length that exists, summing over exten-
sion counts and doing arithmetic including several
divisions and multiplications per order a logarithm
at the end. Thus straightforward estimation from
models is unsuitable for static, high throughput ap-
plications. Instead, models may be compiled to a
less compact but more efficient static representation.
We number trie nodes breadth-first in unicode or-
der beginning from the root and use this indexing for
four parallel arrays following (Whittaker and Raj,
2001). The main difference is that we have not
char int float float int
Idx Ctx C Suf logP log(1-? ) Dtr
0 n/a n/a n/a n/a -0.63 1
1 a 0 -2.60 -0.41 6
2 b 0 -3.89 -0.58 9
3 c 0 -4.84 -0.32 10
4 d 0 -4.84 -0.32 11
5 r 0 -3.89 -0.58 12
6 a b 2 -2.51 -0.58 13
7 a c 3 -3.49 -0.32 14
8 a d 4 -3.49 -0.32 15
9 b r 5 -1.40 -0.58 16
10 c a 1 -1.59 -0.32 17
11 d a 1 -1.59 -0.32 18
12 r a 1 -1.17 -0.32 19
13 ab r 9 -0.77 n/a n/a
14 ac a 10 -1.10 n/a n/a
15 ad a 11 -1.10 n/a n/a
16 br a 12 -0.67 n/a n/a
17 ca d 8 -1.88 n/a n/a
18 da b 6 -1.55 n/a n/a
19 ra c 7 -1.88 n/a n/a
Figure 9: Compiled Representation of 3-grams for
?abracadabra?
coded to a fixed n-gram length, costing us a bit of
space in general, and also that we included context
suffix pointers, costing us more space but saving
lookups for all suffixes during smoothing.
The arrays are (1) the character leading to the
node, (2) the log estimate of the last character in the
path of characters leading to this node given the pre-
vious characters in the path, (3) the log of one mi-
nus the interpolation parameter for the context rep-
resented by the full path of characters leading to this
node, (4) the index of the first daughter of the node,
and (5) index of the suffix of this node. Note that the
daughters of a given node will be contiguous and in
unicode order given the breadth-first nature of the in-
dexing, ranging from the daughter index of the node
to the daughter index of the next node.
We show the full set of parallel arrays for trigram
counts for the string ?abracadabra? in Figure 9. The
first column is for the array index, and is not explic-
itly represented. The second column, labeled ?Ctx?,
is the context, and this is also not explicitly repre-
sented. The remaining columns are explicitly repre-
sented. The third column is for the character. The
fourth column is an integer backoff suffix pointer;
for instance, in the row with index 13, the context
is ?ab?, and the character is ?r?, meaning it repre-
sents ?abr? in the trie. The suffix index is 9, which
is for ?br?, the suffix of ?abr?. The fifth and sixth
columns are 32-bit floating point estimates, the fifth
of log2 P(r|ab), and the sixth is empty because there
is no context for ?abr?, just an outcome. The value
of log2(1 ? ? (ab)) is found in the row indexed 6,
and equal to -0.58. The seventh and final column
is the integer index of the first daughter of a given
node. The value of the daughter pointer for the fol-
lowing node provides an upper bound. For instance,
in the row index 1 for the string ?a?, the daughter in-
dex is 6, and the next row?s daughter index is 9, thus
the daughters of ?a? fall between 6 and 8 inclusively
? these are ?ab?, ?ac? and ?ad? respectively. Note
that the daughter characters are always in alphabeti-
cal order, allowing for a binary search for daughters.
For n-gram estimators, we need to compute
logP(cn|c0 ? ? ?cn?1). We start with the longest se-
quence ck, . . . ,cn?1 that exists in the trie. If binary
search finds the outcome cn among the daughters of
this node, we return its log probability estimate; this
happens in over 90 percent of estimates with rea-
sonably sized training sets. If the outcome character
is not found, we continue with shorter and shorter
contexts, adding log interpolation values from the
context nodes until we find the result or reach the
uniform estimate at the root, at which point we add
its estimate and return it. For instance, the estimate
of log2 P(r|ab) = ?0.77 can be read directly off the
row indexed 13 in Figure 9. But log2 P(a|ab) =
?0.58+ log2 P(a|b)=?0.58+?0.58+ log2 P(a)=
?0.58+?0.58+?2.60, requiring two interpolation
steps.
For implementation purposes, it is significant that
we keep track of where we backed off from. The
row for ?a?, where the final estimate was made, will
be the starting point for lookup next time. This is
the main property of the fast string algorithms ? we
know that the context ?ba? does not exist, so we do
not need to go back to the root and start our search
all over again at the next character. The result is
a linear bound on lookup time because each back-
off of n characters guarantees at least n steps to get
back to the same context length, thus there can?t be
more backoff steps than characters input. The main
bottleneck in run time is memory bandwidth due to
cache misses.
The log estimates can be compressed using as
much precision as needed (Whittaker and Raj,
2001), or even reduced to integral values and integer
arithmetic used for computing log estimates. We use
floats and full characters for simplicity and speed.
6 Corpora and Parsers
Our first corpus is 7 billion characters from the New
York Times section of the Linguistic Data Consor-
tium?s Gigaword corpus. Only the body of docu-
ments of type story were used. Paragraphs indi-
cated by XML markup were begun with a single
tab character. All newlines were converted to single
whitepspaces, and all other data was left unmodi-
fied. The data is problematic in at least two ways.
First, the document set includes repeats of earlier
documents. Language models provide a good way
of filtering these repeated documents out, but we did
not do so for our measurements because there were
few enough of them that it made little difference
and we wanted to simplify other comparative eval-
uations. Second, the document set includes numer-
ical list data with formatting such as stock market
reports. The Times data uses 87 ASCII characters.
Our second corpus is the 5 billion characters
drawn from abstracts in the United States? National
Library of Medicine?s 2004 MEDLINE baseline ci-
tation set. Abstract truncation markers were re-
moved. MEDLINE uses a larger character set of 161
characters, primarily extending ASCII with diacrit-
ics on names and Greek letters.
By comparison, (Banko and Brill, 2001) used one
billion tokens for a disambiguation task, (Brown et
al., 1991) used 583 million tokens for a language
model task, and (Chen and Goodman, 1996) cleverly
sampled from 250 million tokens to evaluate higher-
order models by only training on sequences used in
the held-out and test sets.
Our implementation is based a generic text parser
and text handler interface, much like a simplified
version of XML?s SAX content handler and XML
parser. A text parser is implemented for the various
data sets, including decompressing their zipped and
gzipped forms and parsing their XML, SGML and
tokenized form. A handler is then implemented that
adds data to the online models and polls the model
for results intermittently for generating graphs.
7 Results
We used a 1.4GB Java heap (unfortunately, the max-
imum allowable with Java on 32-bit Intel hardware
without taking drastic measures), which allowed us
to train 6-grams on up to 7 billion characters with
room to spare. Roughly, 8-grams ran out of mem-
ory at 1 billion characters, 12 grams at 100 million
characters, and 32 grams at 10 million characters.
We did not experiment with pruning for this paper,
though our API supports both thresholded and pdi-
visive scaling pruning. Training the counters de-
pends heavily on the length of n-gram, with 5-grams
training at 431,000 characters per second, 8-grams
at 204,000 char/s, 12-grams at 88,000 char/s and 32-
grams at 46,000 char/s, including online hyperpara-
meter estimation (using a $2000 PC running Win-
dows XP and Sun?s 1.4.2 JDK, with a 3.0GHz Pen-
tium 4, 2GB of ECC memory at 800MHz, and two
10K SATA drives in RAID 0).
Our primary results are displayed in Figure 11
and Figure 10, which plot sample cross-entropy
rates against amount of text used to build the mod-
els for various n-gram lengths. Sample cross en-
tropy is simply the average log (base 2) probabil-
ity estimate per character. All entropies are re-
ported for the best hyperparameter settings through
online leave-one-out estimation for parameterized
Witten-Bell smoothing. Each data point in the plot
uses the average entropy rate over a sample size
of up to 10,000 for MEDLINE and 100,000 for
the Times, with the samples being drawn evenly
over the data arriving since the last plot point.
For instance, the point plotted at 200,000 charac-
ters for MEDLINE uses a sample of every 10th
character between character 100,000 and 200,000
whereas the sample at 2,000,000,000 characters
uses every 100,000th character between characters
1,000,000,000 and 2,000,000,000.
Like the Tipster data used by (Chen and Good-
man, 1996), the immediately noticeable feature of
the plots is the jaggedness early on, including some
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 2.6
 2.8
 3
 10000  100000  1e+006  1e+007  1e+008  1e+009  1e+010
Sa
m
pl
e 
Cr
os
s-
En
tro
py
 R
at
e 
(bi
ts 
pe
r c
ha
rac
ter
)
Amount of Data (characters)
 Entropy=1.495
 Entropy=1.570
4-gram
6-gram
8-gram
12-gram
16-gram
Figure 10: NY Times Sample Cross-Entropy Rates
 1
 1.5
 2
 2.5
 3
 3.5
 4
 100  1000  10000  100000  1e+006  1e+007  1e+008  1e+009  1e+010
Sa
m
pl
e 
Cr
os
s 
En
tro
py
 R
at
e 
(bi
ts 
pe
r c
ha
rac
ter
)
Amount of Data (characters)
 Entropy=1.36
 Entropy=1.435
2-gram
4-gram
6-gram
8-gram
12-gram
Figure 11: MEDLINE Sample Cross-Entropy Rates
ridiculously low cross-entropy rates reported for the
Times data. This is largely due to low training
data count, high n-gram models being very good
at matching repeated passages coupled with the fact
that a 2000 word article repeated out of 10,000 sam-
ple characters provides quite a cross-entropy reduc-
tion. For later data points, samples are sparser and
thus less subject to variance.
For applications other than cross-entropy bake-
offs, 5-grams to 8-grams seem to provide the right
 2
 3
 4
 5
 6
 7
 8
 100  1000  10000  100000  1e+006  1e+007  1e+008  1e+009  1e+010
Sa
m
pl
e 
Va
ria
nc
e 
(sq
ua
red
 bi
ts 
pe
r c
ha
rac
ter
)
Amount of Data (characters)
2-gram
4-gram
8-gram
Figure 12: MEDLINE Sample Variances
compromise between accuracy and efficiency.
We were surprised that MEDLINE had lower
n-gram entropy bounds than the Times, especially
given the occurrence of duplication within the Times
data (MEDLINE does not contain duplicates in the
baseline). The best MEDLINE operating point is in-
dicated in the figure, with a sample cross-entropy
rate of 1.36 for 12-grams trained on 100 million
characters of data; 8-gram entropy is 1.435 at nearly
1 billion characters. The best performance for the
Times corpus was also for 12-grams at 100 mil-
lion characters, but the sample cross-entropy was
1.49; with 8-gram sample cross-entropy as low as
1.570 at 1 billion characters. Although MEDLINE
may be full of jargon and mixed-case alphanumeric
acronyms, the way in which they are used is highly
predictable given enough training data. Data in the
Times such as five and six digit stock reports, sports
scores, etc., seem to provide a challenge.
The per-character sample variances for 2-grams,
4-grams and 8-grams for MEDLINE are given in
Figure 12. We did not plot results for higher-order
n-grams, as their variance was almost identical to
that of 8-grams. Standard error is the square root of
variance, or about 2.0 in the range of interest. With
10,000 samples, variance should be 4/10,000, with
standard error the square root of this, or 0.02. This
is in line with measurement variances found at the
tail end of the plots, but not at the beginnings.
Most interestingly, it turned out that smoothing
method did not matter once n-grams were large, thus
bringing the results of (Banko and Brill, 2001) to
bear on those of (Chen and Goodman, 1996). The
comparison for 12-grams and then for the tail of
more data for 8-grams in Figures 13 and 14. Fig-
ure 14 shows the smoothing methods for 8-grams on
an order of magnitude more data.
Conclusions
We have shown that it is possible to use object ori-
ented techniques to scale language model counts to
very high levels without pruning on relatively mod-
est hardware. Even more space could be saved by
unfolding characters to bytes (especially for token
models). Different smoothing models tend to con-
verge to each other after gigabytes of data, making
smoothing much less critical.
Full source with unit tests, javadoc, and applica-
tions is available from the LingPipe web site:
http://www.alias-i.com/lingpipe
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 100000  1e+006  1e+007  1e+008
Sa
m
pl
e 
Cr
os
s 
En
tro
py
 R
at
e 
(bi
ts 
pe
r c
ha
rac
ter
)
Amount of Data (characters)
Absolute Discounting 12-gram
Dirichlet w. Update Exclusion 12-gram
Witten-Bell w. Update Exclusion 12-gram
Dirichlet 12-gram
Absolute Discounting w. Update Exclusion 12-gram
Witten-Bell 12-gram
Figure 13: Comparison of Smoothing for 12-grams
 1.42
 1.44
 1.46
 1.48
 1.5
 1.52
 1e+009 2e+008
Sa
m
pl
e 
Cr
os
s 
En
tro
py
 R
at
e 
(bi
ts 
pe
r c
ha
rac
ter
)
Amount of Data (characters)
Absolute Discounting 8-gram
Dirichlet w. Update Exclusion 8-gram
Witten-Bell w. Update Exclusion 8-gram
Dirichlet 8-gram
Absolute Discounting w. Update Exclusion 8-gram
Witten-Bell 8-gram
Figure 14: Comparison of Smoothing for 8-grams
References
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings of the 39th Meeting of the ACL.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the ACL.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-sense dis-
ambiguation using statistical methods. pages 264?
270.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the ACL, pages 310?318.
John G. Cleary and William J. Teahan. 1997. Un-
bounded length contexts for PPM. The Computer
Journal, 40(2/3):67???
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice. North-Holland.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings the 7th
ConNLL, pages 180?183.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing off for n-gram language modeling. In Pro-
ceedings of ICASSP, pages 181?184.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd Annual Meeting of the ACL.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. tRuEcasIng. In Proceed-
ings of the 41st Annual Meeting of the ACL, pages
152?159.
David J. C. MacKay and Linda C. Peto. 1995. A hier-
archical Dirichlet language model. Natural Language
Engineering, 1(3):1?19.
Alistair Moffat. 1990. Implementing the PPM data com-
pression scheme. IEEE Transactions on Communica-
tions, 38:1917?1921.
Hermann Ney, U. Essen, and Reinhard Kneser. 1995.
On the estimation of ?small? probabilities by leaving-
one-out. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 17:1202?1212.
Peter Norvig. 1991. Paradigms of Artificial Intelligence
Programming: Case Studies in Common Lisp. Mor-
gan Kaufmann.
Richard O?Keefe. 1990. The Craft of Prolog. MIT Press.
Fuchun Peng. 2003. Building Probabilistic Models for
Language Independent Text Classification. Ph.D. the-
sis.
Gerasimos Potamianos and Frederick Jelinek. 1998. A
study of n-gram and decision tree letter language mod-
eling methods. Speech Communication, 24(3):171?
192.
Christer Samuelsson. 1996. Handling sparse data by suc-
cessive abstraction. In Proceedings of COLING-96,
Copenhagen.
William J. Teahan and John G. Cleary. 1996. The en-
tropy of english using PPM-based models. In Data
Compression Conference, pages 53?62.
William J. Teahan. 2000. Text classification and segmen-
tation using minimum cross-entropy. In Proceeding of
RIAO 2000.
Edward Whittaker and Bhiksha Raj. 2001. Quantization-
based language model compression. In Proceedings of
Eurospeech 2001, pages 33?36.
ChengXiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Transactions on Information
Systems, 2(2):179?214.
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 18?29,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Switch Graphs for Parsing Type Logical Grammars?
Bob Carpenter
Alias-i
181 North 11th Street, #401
Brooklyn, NY, 11211
carp@colloquial.com
Glyn Morrill
Universitat Polite`cnica de Catalunya
Departament de Llenguatges i Sistemes Informa`tics
E-08034 Barcelona
morrill@lsi.upc.edu
Abstract
Parsing in type logical grammars amounts
to theorem proving in a substructural
logic. This paper takes the proof net
presentation of Lambek?s associative cal-
culus as a case study. It introduces
switch graphs for online maintenance
of the Danos-Regnier acyclicity condi-
tion on proof nets. Early detection of
Danos-Regnier acyclicity violations sup-
ports early failure in shift-reduce parsers.
Normalized switch graphs represent the
combinatorial potential of a set of anal-
yses derived from lexical and structural
ambiguities. Packing these subanalyses
and memoizing the results leads directly
to a dynamic programming algorithm for
Lambek grammars.
1 Introduction
Following Montague (1970), we take the goal
of a theory of grammar to be that of assign-
ing semantic terms to linguistic expressions.
Type logical grammar is a paradigm for devel-
oping grammatical theories based on a strong
notion of typing for natural language expres-
sions. Specifically, each linguistic expression is
assigned a syntactic type and a semantic term.
For instance, the expression ?John read the book?
of English might be assigned a syntactic type
S and the semantic term read(the(book))(j),
?Supported by CICYT project TIC2002?04019?C03?01.
the expression ?book that John read? the term
that(?x.read(x)(j))(book) and type CN, and
?person that read the book? the type CN and term
that(?y.read(the(book))(y))(person).
2 Lambek?s Associative Calculus
Lambek?s associative calculus L (Lambek 1958)
contains three connectives: concatenation, left divi-
sion, and right division. Logically, concatenation is
conjunction and the divisions are directed implica-
tions. Algebraically, concatenation is a free semi-
group product and the divisions its left and right
residuals. Viewed as a purely syntactic formalism,
L assigns syntactic types to linguistic expressions
modeled as sequences of tokens. From a stipulated
lexical assignment of expressions to syntactic types,
further assignments of expressions to types are de-
rived through purely logical inference, with the logic
representing a sound and complete axiomatization
and inference system over the algebraic structure
(Pentus 1995).
L appears near the bottom of a hierarchy of
substructural logics obtained by dropping structural
rules: Lambek proofs are valid as multiplicative
intuitionistic linear proofs (restoring permutation)
which are valid as conjuntive and implicative rele-
vance proofs (restoring contraction) which are valid
as conjuntive and implicative intuitionistic proofs
(restoring weakening). In type logical grammars,
lexical entries are associated with syntactic types
and intuitionistic (in fact probably relevant) proofs
as semantic representations, notated as terms of the
simply typed ?-calculus with product, under the
Curry-Howard correspondence. The semantics of a
18
derived expression is the result of substituting the
lexical semantics into the reading of the derivation
as an intuitionistic proof.
2.1 Syntactic and Semantic Types
The set of syntactic types is defined recursively on
the basis of a set SynAtom of atomic syntactic types.
The full set SynTyp of syntactic types is the least
set containing the atomic syntactic types SynAtom
and closed under the formation of products (SynTyp?
SynTyp), left divisions (SynTyp\SynTyp), and right
divisions (SynTyp/SynTyp). The two division, or
?slash?, types, A/B, read A over B, and B\A, read B
under A, refine the semantic function types by pro-
viding a directionality of the argument with respect
to the function. A linguistic expression assigned to
type A/B combines with an expression of type B on
its right side to produce an expression of type A. An
expression of type B\A combines with an expression
of syntactic type B on its left to produce an expres-
sion of type A. The product syntactic type A ?B is as-
signed to the concatenation of an expression of type
A to an expression of type B. The distinguishing
feature of Lambek calculus with respect to the ear-
lier categorial grammar of Bar-Hillel is that as well
as the familar cancelation (modus ponens) rules, it
admits also a form of the deduction theorem: if the
result of concatenating an expression e to each B re-
sults in an expression of type A, then it follows that
e is assigned to syntactic type A/B.
Semantic representations in Lambek type logical
grammar are simply typed ?-terms with product. We
assume a set SemAtom of atomic semantic types,
which generate the usual function types ? ? ? and
product types ? ? ?. Terms are grounded on an in-
finite set of distinct variables Var?, along with a set
of distinct contants Con? for each type ?. We as-
sume the usual ?-terms consisting of variables, con-
stants, function applications ?(?), function abstrac-
tions ?x.?, pairs ??, ?? and projections from pairs
pi1? and pi2? onto the first and second element of the
pair respectively. We say that a term ? is closed if
and only if it contains no free variables.
A type map consists of a mapping typ :
SynAtom ? SemTyp. That is, each atomic syn-
tactic type A ? AtomCat is assigned to a (not neces-
sarily atomic) semantic type typ(A) ? SemTyp. Se-
mantic types are assigned to complex syntactic types
as follows:
typ(A ? B) = typ(A) ? typ(B) [Product]
typ(A/B) = typ(B) ? typ(A) [Right Division]
typ(B\A) = typ(B) ? typ(A) [Left Division]
We will often write ? : A where ? is a ?-term of type
typ(A).
2.2 Linguistic Expressions and the Lexicon
In the Lambek calculus, linguistic expressions are
modeled by sequences of atomic symbols. These
atomic symbols are drawn from a finite set Tok of
tokens. The full set of linguistic expressions Tok?
is the set of sequences of tokens. For the sake of
this short version of the paper we admit the empty
sequence; we will address its exclusion (as in the
original definition of L) in a longer version.
The compositional assignment of semantic terms
to linguistic expressions is grounded by a finite set
of assignments of terms and types to expressions.
A lexicon is a finite relation Lex ? Tok? ? Term ?
SynTyp, where all ?w, ?, A? ? Lex are such that the
semantic term ? is of the appropriate type for the
syntactic type A. We assume that the only terms
used in the lexicon are relevant, in the sense of rele-
vance logic, in not containing vacuous abstractions.
Note that the set of atomic semantic types, atomic
syntactic types and the semantic type mapping are
assumed as part of the definition of a lexicon. Type
logical grammar is an example of a fully lexicalized
grammar formalism in that the lexicon is the only
locus of language-specific information.
2.3 Proof Nets
A sequent ? ? ? : A is formed from an antecedent
? consisting of a (possibly empty) sequence of ?-
term and syntactic type pairs, and a consequent pair
? : A, where the terms are of the appropritate type
for the types. Following Roorda (1991), we define
theoremhood with Girard-style proof nets (Girard
1987), a geometric alternative to Lambek?s Gentzen-
style calculus (Lambek 1958).
Proof nets form a graph over nodes labeled by
polar types, where a polar type is the combination
of a syntactic type and one of two polarities, input
(negative) and output (positive). We write A? for the
input polar type, which corresponds to antecedent
types and is thus logicaly negative. We write A? for
19
the output polar type, which is logically positive and
corresponds to a consequent type. A literal is a po-
lar type with an atomic syntactic type. Where A is
an atomic syntactic type, the literals A? and A? are
said to be complementary.
Each polar type defines an ordered binary tree
rooted at that polar type, known as a polar tree. For a
literal, the polar tree is a single node labeled by that
literal. For polar types with complex syntactic types,
the polar tree is rooted at the polar type and unfolded
upwards based on connective and polarity according
to the solid lines in Figure 1, which includes also
other annotation. Examples for some linguistically
motivated types are shown in Figure 2.
The solid edges of the graphs are the edges of
the logical links. Each unfolding is labeled with a
multiplicative linear logic connective, either multi-
plicative conjunction (?) or multiplicative disjunc-
tion (?). This derives from the logical interpretation
of the polar type trees as formula trees in multiplica-
tive linear logic. Unfolding the Lambek connectives
to their linear counterparts, (A/B)? and (B\A)? un-
fold to A??B?; (A/B)? and (B\A)? unfold to A??B?;
(A ? B)? unfolds to A? ? B?; and (A ? B)? unfolds to
A??B?. The type unfoldings correspond to the clas-
sical equivalences between (? ? ?) and (?? ? ?),
between ?(? ? ?) and (? ? ??), and between
?(? ? ?) and (?? ? ??). For atomic syntactic types
A, A? becomes simply A, whereas A? becomes its
linear negation A?; this is the sense in which po-
lar atomic types correspond to logical literals. The
non-commutatitive nature of the Lambek calculus is
reflected in the ordering of the subtrees in the un-
foldings; for commutative logics, the proof trees are
not ordered.
The proof frame for a syntactic sequent
C1, . . . ,Cn ? C0 is the ordered sequence of
polar trees rooted at C?0,C
?
1, . . . ,C
?
n. We convert
sequents to frames in this order, with the output
polar tree first. In general, what follows applies to
any cyclic reordering of these polar trees. Note that
the antecedent types C1, . . .Cn have input (negative)
polarity inputs and the consequent type C0 has
output (positive) polarity. All of our proof frames
are intuitionistic in that they have a single output
conclusion, i.e. a unique polar tree rooted at an
output type.
A partial proof structure consists of a proof frame
with a set of axiom links linking pairs of comple-
mentary literals with at most one link per literal. Ax-
iom links in both directions are shown in Figure 3.
A proof structure is a proof structure in which all
literals are connected to complementary literals by
axiom links.
Proof nets are proof structures meeting certain
conditions. A proof structure is planar if and only if
its axiom links can be drawn in the half-plane with-
out crossing lines; this condition enforces the lack
of commutativity of the Lambek calculus. The fi-
nal condition on proof structures involves switch-
ing. A switching of a proof structure is a subgraph
that arises from the proof structure by removing ex-
actly one edge from each disjunctive (?) link. A
proof structure is said to be Danos-Regnier (DR-)
acyclic if and only if each of its switchings is acyclic
(Danos and Regnier 1989).1A proof net is a planar
DR-acyclic proof structure. A theorem is any se-
quent forming the proof frame of a proof net.
Consider the three proof nets in Figure 4. The first
example has no logical links, and corresponds to the
simplest sequent derivation S ? S . The second ex-
ample represents a determiner, noun and intransitive
verb sequence. Both of these examples are acyclic,
as must be every proof net with no logical ?-links.
The third example corresponds to the type-raising
sequent N ? S/(N\S ). Unlike the other examples,
this proof net involves a ?-link and is cyclic. But
both of its switchings are acyclic, so it satisfies the
Danos-Regnier acyclicity condition.
2.4 Essential Nets and Semantic Trips
A term is said to be pure if and only if it contains
no constants. The linear terms are closed, pure ?-
terms that bind each variable exactly once. Each
proof net in the Lambek calculus corresponds to a
linear (i.e. binding each variable exactly once) ?-
term via the Curry-Howard correspondence. This
term abstracts over variables standing in for the se-
mantics of the inputs in the antecedent of the sequent
and has a body that is determined by the consequent
of the sequent. For instance, the ?-term ?x.?P.P(x)
corresponds to the syntactic sequent x : N, P :
1The full Danos-Regnier condition is that every switching
be acyclic and connected. Fadda and Morrill (2005) show that
for the intuitionistic case (i.e. single output conclusion, as for
L), DR-acyclicity entails the connectedness of every switching.
20


?(?) : A?
{ }
BB
BB
BB
BB
BB
B
**
&
-
5
B
M U
//_______ ? : B?
{ }
  
  
  
  
  
 
OO

?
? : A/B?




xi : B?
{Li}
CC
CC
CC
CC
CC
CC
? : A?
{Ri}
{{
{{
{{
{{
{{
{{
OO

?
?xi.? : A/B?
LL
q
{



OO



? : B?
{ }
>>
>>
>>
>>
>>
>
OO

?(?) : A?
{ }
||
||
||
||
||
|
tt


	
|
qi
oo_ _ _ _ _ _ _
?
? : B\A?




? : A?
{Li}
CC
CC
CC
CC
CC
CC
OO

xi : B?
{Ri}
{{
{{
{{
{{
{{
{{
?
?xi.? : B\A?
RR
M
C
6
-
'
OO





pi1? : A?
{Li}
AA
AA
AA
AA
AA
A
**
&
,
5
A
M
pi2? : B?
{Ri}
}}
}}
}}
}}
}}
}
tt




}
r
?
? : A ? B?


? : B?
{ }
CC
CC
CC
CC
CC
CC
OO

? : A?
{ }
zz
zz
zz
zz
zz
zz
OO

?
??, ?? : A ? B?
RR
N
C
7
.
( LL
p
z



OO

Figure 1: Logical Links with Switch Paths (solid) and Semantic Trip (dashed)
x : S ? x : N? y(x) : S ?
?
y : N\S ?
???? 
u : CN? z(?x.y)(u) : CN? x : N? y : S ?
? ?
z(?x.y)CN\CN?
DDDD
zzzz
?x.y : S/N?
DDDD
zzzz
?
z : (CN\CN)/(S/N)?
RRRRRRR
lllllll
t : S ? v : N? r:N? x(?yw.u)(?v.t)(r):S ? u:S ? w : N? z : N? y(z) : S ?
? ? ? ?
?v.t : N\S ?
>>>>     
x(?yw.u)(?v.t) : N\S ?
NNNNNN
    
?w.u : N\S ?
>>>>     
y : N\S ?
>>>>     
? ?
x(?y.?w.u) : (N\S )\(N\S )?
NNNNNN
pppppp
?y.?w.u : (N\S )\(N\S )?
NNNNNN
pppppp
?
x : ((N\S )\(N\S ))/((N\S )\(N\S ))?
WWWWWWWWWW
gggggggggg
Figure 2: Examples of Polar Type Trees
21
? : A?

 ? : A?
uu V[_ch
{ }
OO

? : A?
))h c _ [ V
? : A?
{ }

OO
Figure 3: Axiom Links with Switch Paths and Semantic Trip
S ? S ?
N? CN? N? S ?
? ?
S ? N/CN?
?? 
CN? N\S ?
?? 
N? S ?
?
N\S ?
?? 
S ?
?
S/(N\S )?
?? 
N?
Figure 4: Three Example Proof Nets
N\S ? P(x) : S and ?x.?P.P(x) corresponds to
the sequent x : N ? ?P.P(x) : S/(N\S ). The ?-
term induced by the Curry-Howard correspondence
can be determined by a unification problem over a
proof net (Roorda 1991). Different proof nets for the
same theorem correspond to different interpretations
through the Curry-Howard correspondence. The es-
sential net of a proof structure is the directed graph
rooted at the root node of the output polar type tree
whose edges are shown as dashed lines in Figures 1
and 3 (LaMarche 1994). Each output division type
introduces a fresh variable on its input subtype (its
argument), as indicated by the labels xi in Figure 1.
The essential nets for the examples in Figure 4 are
shown in Figure 5.
Terms are computed for the polar type trees by
assigning terms to the roots of the polar inputs. The
tree is then unfolded unifying in substitutions as it
goes, as illustrated in the example polar type trees in
Figure 2. The direction of axiom links in the essen-
tial net provide the substitutions necessary to solve
the unification problem of ?-terms in the proof net
established by equating the two halves of each ax-
iom linked complementary pair of literals. A traver-
sal of an essential net carrying out the substitutions
specified by axiom links constitutes a semantic trip
the end result of which is the Curry-Howard ?-term
for the Lambek calculus theorem derived by the
proof net. All ?-terms derived from a semantic trip
with variables or constants assigned to input root po-
lar types will be in ?-? long form. The essential net
directly corresponds to the tree of the semantic term
derived by the Curry-Howard correspondence.
The well-formedness of a set of axiom linkings
over a polar tree may be expressed in terms of the
essential net. Among the conditions are that an es-
sential net must be acyclic and planar. In addition,
essential nets must be connected in two ways. First,
there must be a path from the root of the single out-
put polar tree to the root of each of the input polar
trees. Second, there must be a path from each output
daughter of an output division to the input daugh-
ter. That is, when A/B? is unfolded to B?A?, there
must be a path from A? to B?. These conditions ex-
press the definition of linear semantic terms dictated
through the logic by the Curry-Howard correspon-
dence. The first condition requires each variable (or
term) corresponding to the root of an input polar tree
to occur in the output term, whereas the second con-
dition requires that variables only occur within their
proper scopes so that they are bound. The essen-
tial nets presented in Figure 5 adhere to these con-
ditions and produce well-typed linear ?-terms. The
example presented in Figure 6 shows a set of axiom
links that does not form a proof net; it violates the
condition on variable binding, as is seen from the
lack of path from the N? daughter to the N? daugh-
ter of the N/N? node. The major drawback to us-
ing these conditions directly in parsing is that they
are existential in the sense of requring the existence
of a certain kind of path, and thus difficult to refute
online during parsing. In comparison, the Danos-
22
t(k) : N?

?
??
?
// CN?

N? r(t(k)) : S ?
 

oo
S ?

t : N/CN? k : CN? r : N\S ?
N?

x(j) : S ?

oo
~~||
||
x : N\S ? S ?
?x.x(j) : S/(N\S )?
>>||||
j : N?
Figure 5: Semantic Trips
?(?x.?)(?(x)) : N? //
  B
BB
B
N?

x : N?

N?

?(?x.?) : N/N? //
((PP
PPP
P
?x.? : N/N?
>>||||
N? ?(x) : N?oo
~~||
||
N?

? : (N/N)/(N/N)? ? : N? ? : N\N?
Figure 6: Unbound Variable in Illegal Semantic Trip
N? N?





___________________________








N?



______________






N?
? ?
N/N?
BBBB
|
|
N/N?
B
B
N? N?
? ?
N? (N/N)/(N/N)?
P P P P
nnnn
N? N\N?
B
B
|
|
Figure 7: Switching with Path Violating Danos-Regnier Acyclicity
23
Regnier acyclicity condition is violated by the at-
tempt to close off the binding of the variable. The
path vilolating DR acyclicity is shown in Figure 7,
with the path given in dashed lines and the switch-
ing taking the right daughter of N/N? as the arc to
remove.
3 Parsing with Switch Graphs
The planar connection of all literals into a proof
structure is straightforward to implement. Axiom
links are simply added in such a way that planarity
is maintained until a complete linkage is found. In
our shift-reduce-style parser, planarity is maintained
by a stack in the usual way (Morrill 2000). For dy-
namic programming, we combine switch graphs in
the cells in a Cocke-Kasami-Younger (CKY) parser
(Morrill 1996). The main challenge is enforcing
DR-acyclicity, and this is the main focus of the rest
of the paper. We introduce switch graphs, which not
only maintain DR-acyclicity, but also lead the way
to a normal form for well-formed subsequence frag-
ments of a partial proof structure. This normal form
underlies the packing of ambiguities in subderiva-
tions in exactly the same way as usual in dynamic
programming parsing.
3.1 Switch Graphs
Switch graphs are based on the observation that a
proof structure is DR-acyclic if and only if every cy-
cle contains both edges of a ?-link. If a cycle con-
tains both edges of a ?-link, then any switching re-
moves the cycle. Thus if every cycle in a proof struc-
ture contains both edges of a ?-link, every switching
is acyclic.
The (initial) switch graph of a partial proof struc-
ture is defined as the undirected graph underlying
the partial proof structure with edges labeled with
sets of ?-edge identifiers as indicated in Figures 1
and 3. Each edge in a logical ?-link is labeled with
the singleton set containing an identifier of the link
itself, either Li for the left link of ?-link i or Ri for
the right link of ?-link i. Edges of axiom links and
logical ?-links are labeled with the empty set.
The closure of a switch graph is computed by it-
erating the following operation: if there is an edge
n1 ? n2 labeled with set X1 and an edge edge n2 ? n3
labeled with set X2 such that X1?X2 does not contain
both edges of a ?-link, add an edge n1 ? n3 labeled
with X1?X2. An edge n?m labeled by X is subsumed
by an edge between the same nodes n?m labeled by
Y if Y ? X. The normal switch graph of a partial
proof structure is derived by closing its the initial
switch graph, removing edges that are subsumed by
other edges, and restricting to the literal nodes not
connected by an axiom link. These normal switch
graphs define a unique representation of the combi-
natorial possibilities of a span of polar trees and their
associated links in a partial proof structure. That is,
any partial proof structure substructure that leads to
the same normal switch graph may be substituted in
any proof net while maintaining well-formedness.
The fundamental insight explored in this paper is
that two literals may be connected by an axiom link
in a partial proof structure without violating DR-
acyclicity if and only if they are not connected in
the normal switch graph for the partial proof struc-
ture. The normal switch graph arising from the ad-
dition of an axiom link is easily computed. It is just
the closure generated by adding the new axiom link,
with the two literals being linked removed.
3.2 Shift-Reduce Parsing
In this section, we present the search space for a
shift-reduce-style parsing algorithm based on switch
graphs. The states in the search consist of a global
stack of literals, a lexical stack of literals, the re-
maining tokens to be processed, and the set of links
among nodes on the stacks in the switch graph. The
shift-reduce search space is characterized by an ini-
tial state and state transitions. These are shown in
schematic form in Figure 8. The initial state con-
tains the output type?s literals and switch graph. A
lexical transition is from a state with an empty lexi-
cal stack to one containing the lexical literals of the
next token; the lexical entry?s switch graph merges
with the current one. A shift transition pops a literal
from the lexical stack and pushes it onto the global
stack. A reduce transition adds an axiom link be-
tween the top of the global stack and lexical stack
if they are complementary and are not connected in
the switch graph; the resulting switch graph results
from adding the axiom link and normalizing. The
stack discipline insures that all partial proof struc-
tures considered are planar.
Figure 10 displays as rows the shift-reduce search
24
Stack Lex Sw-Gr Op
A? gr(A?) start(A)
S G
S A? G ? gr(A?) lex(w, A)
Stack Lex Sw-Gr Op
AiS A jL G
S L (G ? i= j) ? {i, j} reduce(i, j)
AS BL G
BAS L G shift(B)
Figure 8: Shift-Reduce Parsing Schematic
N?1 N
?
2 N
?
4 N
?
5
N?0 N1/N?2
9999 
N?3 N4\N?5
9999 
N?1 N
?
2 N
?
4 N
?
5
N?0 N1/N?2
9999 
N?3 N4\N?5
9999 
Figure 9: Modifier Attachment Ambiguity Proof Nets
Stack Lex Tok Sw-Gr Ax Op
N?0 start
N?0 N
?
1 N
?
2 w1 1-2{} lex
? N?2 0=1 reduce
N?2 shift
N?2 N
?
3 w2 lex
N?3 N
?
2 shift
N?3 N
?
2 N
?
4 N
?
5 w3 4-5{} lex
N?2 N
?
5 3=4 reduce
2=5 reduce
Stack Lex Tok Sw-Gr Ax Op
N?0 start
N?0 N
?
1 N
?
2 w1 1-2{} lex
N?1 N
?
0 N
?
2 1-2{} shift
N?2 N
?
1 N
?
0 1-2{} shift
N?2 N
?
1 N
?
0 N
?
3 w2 1-2{} lex
N?1 N
?
0 2=3 reduce
N?1 N
?
0 N
?
4 N
?
5 w3 4-5{} lex
N?0 N
?
5 1=4 reduce
0=5 reduce
Figure 10: Modifier Attachment Ambiguity Shift-Reduce Search States
25
states corresponding to the two valid proof nets
shown in Figure 9. The subscripts on syntactic types
in the diagram is only so that they can be indexed
in the rows of the table describing the search states.
The initial state in both searches is created from the
output type?s literal. The third column of the dia-
grams indicate the token consumed at each lexical
entry. The switch graphs are shown for the rows
for which they?re active. Because there are no ?-
links, all sets of edges are empty. The fifth column
shows the axiom linking made at each reduce step.
The history of these decisions and lexical insertion
choices determines the final proof net. Finally, the
sixth column shows the operation used to derive the
result. Note that reduction is from the top of the lex-
ical stack to the top of the global stack and is only
allowed if the nodes to be linked are not connected in
the switch graph. This is why N?1 cannot reduce with
N?2 in the second diagram in Figure 10; the second
shift is mandatory at this point. Note that as active
nodes are removed, as in the first diagram reduction
step linking 0=2, the switch graph contracts to just
the unlinked nodes. After the reduction, only N?2 is
unlinked, so there can be no switch graph links. The
link between node 4 and 5 is similarly removed al-
most as soon as it?s introduced in the second reduc-
tion step. In the second diagram, the switch graph
links persist as lexical literals are pushed onto the
stack.
Shift-reduce parses stand in one-to-one corre-
spondence with proof nets. The shift and reduce op-
erations may be read directly from a proof net by
working left to right through the literals. Between
literals, the horizontal axiom links represent literals
on the stack. Literals in the current lexical syntac-
tic type represent the lexical stack. Literals that are
shifted to the global stack eventually reduce by ax-
iom linking with a literal to the right; literals that are
reduced from the lexical stack axiom link to their
left with a literal on the global stack.
3.3 Memoized Parsing
Using switch graphs, we reduce associative Lam-
bek calculus parsing to an infinite binary phrase-
structure grammar, where the non-terminals are
normalized switch graphs. The phrase structure
schemes are shown in Steedman notation in Fig-
ure 11. Lexical entries for syntactic type A are de-
rived from the input polar tree rooted at A?. This
polar tree yields a switch graph, which is always a
valid lexical entry in the phrase structure grammar.
Any result of axiom linking adjacent complemen-
tary pairs of literals in the polar tree that maintains
switch-graph acyclicity is also permitted. For in-
stance, allowing empty left-hand sides of sequents,
the input type A/(B/B)? would produce the literals
A?1B
?
2B
?
3 with links 1-2 : {L3}, 1-3 : {R3}. This could
be reduced by taking the axiom link 2=3, to pro-
duce the single node switch graph A?1. In contrast,
(B/B)/A? produces the switch graph B?1B?2A?3 with
links 1-2, 1-3, and 2-3. Thus the complementary B
literals may not be linked.
Given a pair of normal switch graphs, the binary
rule scheme provides a finite set of derived switch
graphs. One or more complementary literals may be
axiom linked in a nested fashion at the borders of
both switch graphs. These sequences are marked as
? and ? and their positions are given relative to the
other literals in the switch graph in Figure 11. Un-
linked combinations are not necessary because the
graph must eventually be connected. This scheme
is non-deterministic in choice of ?. For instance, an
adverb input (N1\S 2)/(N4\S 3)? produces the literals
N?1S
?
2S
?
3N
?
4 and connections 1-2, 1-3:{L4}, 1-4:{R4},
2-3:{L4}, and 2-4:{R4}. When it combines with a
verb phrase input N5\S ?6 with literals N
?
5S
?
6 and con-
nections 5-6, then either the nominals may be linked
(4=5), or the nominals and sentential literals may be
linked (4=5, 3=6). The result of the single linking is
N?1S
?
2S
?
3S
?
6 with connections 1-2, 1-3:{L4}, 1-6:{R4},
2-3:{L4}, and 2-6:{R4}. The result of the double link-
ing is simply N?1S
?
6 with connection 1-6, or in other
words, a verb phrase.
The dynamic programming equality condition is
that two analyses are considered equal if they lead
to the same normalized switch graphs. This equality
is only considered up to the renaming of nodes and
edges. Backpointers to derivations allow semantic
readings to be packed in the form of lexical choices
and axiom linkings. For instance, consider the two
parses in Figure 12.
With a finite set of lexical entries, bottom-up
memoized parsing schemes will terminate. We illus-
trate two derivations of a simple subject-verb-object
construction in Figure 13. This is a so-called spuri-
ous ambiguity because the two derivations produce
26
w
lex
?
G
?
?
?
?
?
?
?
?
?
?
?
?
Lex(w, A), and
A? has switch graph
w. literals ?, links G
?
?
?
?
?
?
?
?
?
?
?
?
?1 ?
G1
? ?2
G2 ? = ?
?1 ?2
(G1 ? G2) ? (? = ?)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? = Ai1 , . . . , Ain
? = A jn , . . . , A j1
(? = ?) = i1 = j1, . . . , in = jn
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 11: Phrase-Structure Schemes over Switch Graphs
a:N1/N2
a(x):N?1 x:N?2
1-2
b:N3
b:N?3
c:N4\N5
y:N?4 c(y):N?5
4-5 3=4
c(b):N?5
2=5
a(b(c)):N?1
a:N1/N2
a(x):N?1 x:N?2
1-2
b:N3
b:N?3
2=3
a(b):N?1
c:N4\N5
y:N?4 c(y):N?5
4-5 1=4
c(a(b)):N?5
Figure 12: Modifier Attachment Ambiguity Packing
the same semantic term. They are not spurious glob-
ally because the alternative linkings are required for
adverbial modification and object relativization re-
spectively. The ambiguity in the phrase structure
grammar results from the associativity of the combi-
nation of axiom linkings. The two derivations do not
propagate their ambiguity under the dynamic pror-
gramming scheme precisely because they produce
equivalent results. Nevertheless, a worthwhile opti-
mization is to restrict the structure of combinations
of linkings in the phrase-structure schemes to corre-
spond to an unambiguous left-most linking strategy;
this corresponds to the way in which other associa-
tive operators are parsed in programming language.
For instance, x+y+z will be assumed to be x+(y+z)
if + is defined to be right associative.
An unambiguous right-associative context-free
grammar for linkings M over literals A and their
complements A is:
M ? A A | A M A | A A M | A M A M
An example of packing for subject/object scope am-
biguities is shown in Figure 14. The derivations
in Figure 14 produce different semantic interpreta-
tions; one of these is subject-wide scope and the
other object-wide scope. Unsurprisingly, the mem-
oizing parser does not solve P = NP in the affirmi-
tive (Pentus 2003). The size of the switch graphs on
the intermediate structures is not bounded, nor is the
number of alternative switch-paths between literals.
It remains an open question as to whether the switch
graph parser could be bounded for a fixed lexicon
(Pentus 1997).
3.4 Empty Antecedents and Subtending
Lambek?s calculus required the antecedent ? in
a sequent ? ? ? : A to be non-empty.
Proof nets derive theorems ( ? CN/CN) and
((CN/CN)/(CN/CN) ? CN/CN), as shown in Fig-
ure 15. These derivations both allow the construc-
tion of an output, namely the identity term ?x.x and
modifier syntactic type CN/CN, out of no input.
A literal A is said to subtend a complementary
literal A if they are the leftmost and rightmost de-
scendants of a ?-link. In both of the examples in
Figure 15, the output adjective CN/CN? unfolds to
the sequence of literals CN?CN? in which the input
CN? subtends the output CN?. If literals that stand
in a subtending relation are never linked, the set of
theorems is restricted to those derivable in Lambek?s
original calculus.
Consider the proof net in Figure 16. An analysis
in which S ?8 linked to S
?
11 and N
?
9 linked to N
?
10 is
not ruled out by Danos-Regnier acyclicity. It is ruled
out by the subtending condition because S ?8 subtends
S ?11, being the leftmost and right most daughters of
the ?-node (N10\S 11)\(N9\S 8)?. Further note that
there are no cycles violating DR acyclicity; each of
the sixteen switchings is acyclic.
4 Conclusion
We have introduced switch graphs for shift-reduce
and CKY-style parsing of grammars in the asso-
ciative Lambek calculus. Switch graphs encode
27
NN?1
(N\S )/N
N?2 S
?
3 N
?
4
2-3, 2-4, 3-4
1=2
S ?3 N
?
4
3-4
N
N?5
4=5
S ?3
N
N?1
(N\S )/N
N?2 S
?
3 N
?
4
2-3, 2-4, 3-4
N
N?5
4=5
N?2 S
?
3
2-3
1=2
S ?3
N?2 S
?
3
?
N2\S ?3
?? 
N?4
?
N?1 (N2\S 3)/N?4
?? 
N?5
Figure 13: Left vs. Right Attachment: Packing Locally Spurious Attachment Ambiguity
S ?2 N?3 N?4 S ?5 N?7 S ?8
? ? ?
S ?1 N3\S ?2
/// 
N4\S ?5
/// 
N?6 S 8/N?7
/// 
S ?9
? ? ?
S 1/(N3\S 2)?
/// 
(N4\S 5)/N?6
/// 
(S 8/N7)\S ?9
/// 
S ?2 N?3 N?4 S ?5 N?7 S ?8
? ? ?
S ?1 N3\S ?2
/// 
N4\S ?5
/// 
N?6 S 8/N?7
/// 
S ?9
? ? ?
S 1/(N3\S 2)?
/// 
(N4\S 5)/N?6
/// 
(S 8/N7)\S ?9
/// 
S ?1S ?2N?3
1-2:{L3}, 1-3:{R3}
N?4 S ?5N?6
4-5, 4-6, 5-6
S ?1N?6
1-6
N?7 S ?8S ?9
7-9:{L7}, 8-9:{R7}
S ?9
S ?1S ?2N?3
1-2:{L3}, 1-3:{R3}
N?4 S ?5N?6
4-5, 4-6, 5-6
N?7 S ?8S ?9
7-9:{L7}, 8-9:{R7}
N?4 S ?9
4-9
S ?1
Figure 14: Scope Ambiguity: Partial Proof Structure Fragments with Phrase Structure
x : CN? CN?
?
?x.x : CN/CN?
????  y(?x.x)(w) : CN? CN? x : CN? CN?
? ?
w : CN? CN? y(?x.x) : CN/CN?
???? 
?x.x : CN/CN?
???? 
? ?
?w.y(?x.x)(w) : CN/CN?
???? 
y : (CN/CN)/(CN/CN)?
OOOOOO
oooooo
Figure 15: Subtending Examples
28
S ? N?5 N
?
6 S
?
7 S
?
8 N
?
9 N
?
10 S
?
11
? ? ? ?
N5\S ?4
////

N6\S ?7
////

N9\S ?8
////

N10\S ?11
////

S ?12 N
?
13 N
?
14 S
?
15
? ? ? ?
N?2 S
?
3 (N5\S 4)\(N6\S 7)?
?????

(N10\S 11)\(N9\S 8)?
?????

N13\S ?12
////

N14\S ?15
////

? ? ?
S ?0

















_____________________







N?1 N2\S
?
3
////

((N5\S 4)\(N6\S 7))/((N10\S 11)\(N9\S 8))?
OOOOOOO
ooooooo
(N13\S 12)\(N14\S 15)?
?????

Figure 16: Higher-Order Example: Adverbial Intensifier
the axiom-linking possibilities of a sequence of un-
linked literals deriving from underlying polar trees.
We introduced two parsers based on switch graphs.
The shift-reduce parsers are memory efficient and
parses correspond uniquely to (cut free) proof nets.
They can be made more efficient by bounding stack
size. The memoizing parsers are able to pack attach-
ment and scope distinctions that lead to different ?-
terms but have the same combinatory possibilities.
References
D. Bechet. 2003. Incremental parsing of lambek calculus
using proof-net interfaces. In Proc. of the 8th Interna-
tional Workshop on Parsing Technologies.
V. Danos and L. Regnier. 1989. The structure of multi-
plicatives. Arch. Math. Logic, 28:181?203.
P. de Groote and C. Retore?. 1996. On the semantic read-
ings of proof nets. In Proc. of Formal Grammar, pages
57?70.
M. Faddo and G. Morrill. 2005. The Lambek calculus
with brackets. In P. Scott, C. Casadio, and R. Seely,
editors, Language and Grammar: Studies in Math.
Ling. and Nat. Lang. CSLI Press, Stanford.
J.-Y. Girard. 1987. Linear logic. Theoret. Comput. Sci.,
50:1?102.
F. Lamarche. 1994. Proof nets for intuitionistic linear
logic I: Essential nets. Technical report, Imperial Col-
lege, London.
J. Lambek. 1958. The mathematics of sentence structure.
Amer. Math. Mon., 65:154?170.
R. Montague. 1970. Universal grammar. Theoria,
36:373?398.
G. Morrill. 1996. Memoisation of categorial proof nets:
parallelism in categorial processing. Technical Re-
port LSI-96-24-R, Dept. de Llenguatges i Sistemes In-
forma`tics, Universitat Polite`cnica de Catalunya.
G. Morrill. 2000. Incremental processing and accept-
ability. Comput. Ling., 26(3):319?338.
M. Pentus. 1995. Models for the Lambek calculus. An-
nals of Pure and Applied Logic, 75(1?2):179?213.
M. Pentus. 1997. Product-free Lambek calculus and
context-free grammars. Journal of Symbolic Logic,
62(2):648?660.
M. Pentus. 2003. Lambek calculus is NP-complete.
Technical Report TR-203005, CUNY Graduate Cen-
ter.
D. Roorda. 1991. Resource logics: Proof-theoretical
investigations. Ph.D. thesis, Universiteit van Amster-
dam.
29
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 169?172,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Character Language Models for Chinese Word Segmentation and Named
Entity Recognition
Bob Carpenter
Alias-i, Inc.
carp@alias-i.com
Abstract
We describe the application of the Ling-
Pipe toolkit to Chinese word segmentation
and named entity recognition for the 3rd
SIGHAN bakeoff.
1 Word Segmentation
Chinese is written without spaces between words.
For the word segmentation task, four training cor-
pora were provided with one sentence per line and
a single space character between words. Test data
consisted of Chinese text, one sentence per line,
without spaces between words. The task is to in-
sert single space characters between the words.
For this task and named entity recognition, we
used the UTF8-encoded Unicode versions of the
corpora converted from their native formats by the
bakeoff organizers.
2 Named Entity Recognition
Named entities consist of proper noun mentions
of persons (PER), locations (LOC), and organiza-
tions (ORG). Two training corpora were provided.
Each line consists of a single character, a single
space character, and then a tag. The tags were in
the standard BIO (begin/in/out) encoding. B-PER
tags the first character in a person entity, I-PER
tags subsequent characters in a person, and 0 char-
acters not part of entities. We segmented the
data into sentences by taking Unicode character
0x3002, which is rendered as a baseline-aligned
small circle, as marking end of sentence (EOS). As
judged by our own sentence numbers (see Figures
1 and 2), this missed around 20% of the sentence
boundaries in the City U NE corpus and 5% of
the boundaries in the Microsoft NE corpus. Test
data is in the same format as the word segmenta-
tion task.
3 LingPipe
LingPipe is a Java-based natural language process-
ing toolkit distributed with source code by Alias-i
(2006). For this bakeoff, we used two LingPipe
packages, com.aliasi.spell for Chinese
word segmentation and com.aliasi.chunk
for named-entity extraction. Both of these de-
pend on the character language modeling pack-
age com.aliasi.lm, and the chunker also
depends on the hidden Markov model package
com.alias.hmm. The experiments reported in
this paper were carried out in May 2006 using (a
prerelease version of) LingPipe 2.3.0.
3.1 LingPipe?s Character Language Models
LingPipe provides n-gram based character lan-
guage models with a generalized form of Witten-
Bell smoothing, which performed better than other
approaches to smoothing in extensive English tri-
als (Carpenter 2005). Language models provide
a probability distribution P (?) defined for strings
? ? ?? over a fixed alphabet of characters ?. We
begin with Markovian language models normal-
ized as random processes. This means the sum of
the probabilities for strings of a fixed length is 1.0.
The chain rule factors P (?c) = P (?) ? P (c|?)
for a character c and string ?. The n-gram Marko-
vian assumption restricts the context to the previ-
ous n?1 characters, taking P (cn|?c1 ? ? ? cn?1) =
P (cn|c1 ? ? ? cn?1).
The maximum likelihood estimator for n-grams
is P?ML(c|?) = count(?c)/extCount(?), where
count(?) is the number of times the sequence ?
was observed in the training data and extCount(?)
169
is the number of single-character extensions of ?
observed: extCount(?) =
?
c count(?c).
Witten-Bell smoothing uses linear interpolation
to form a mixture model of all orders of maximum
likelihood estimates down to the uniform estimate
PU (c) = 1/|?|. The interpolation ratio ?(d?)
ranges between 0 and 1 depending on the context:
P? (c|d?) = ?(d?)PML(c|d?)
+ (1 ? ?(d?))P? (c|?)
P? (c) = ?()PML(c)
+ (1 ? ?())(1/|?|)
Generalized Witten-Bell smoothing defines the
interpolation ratio with a hyperparameter ?:
?(?) =
extCount(?)
extCount(?) + ? ? numExts(?)
We take numExts(?) = |{c|count(?c) > 0}| to be
the number of different symbols observed follow-
ing ? in the training data. The original Witten-Bell
estimator set the hyperparameter ? = 1. Ling-
Pipe?s default sets ? equal to the n-gram order.
3.2 Noisy Channel Spelling Correction
LingPipe performs spelling correction with a
noisy-channel model. A noisy-channel model
consists of a source model Ps(?) defining the
probability of message ?, coupled with a chan-
nel model Pc(?|?) defining the likelihood of a sig-
nal ? given a message ?. In LingPipe, the source
model Ps is a character language model. The
channel model Pc is a (probabilistically normal-
ized) weighted edit distance (with transposition).
LingPipe?s decoder finds the most likely message
? to have produced a signal ?: argmax?P (?|?) =
argmax?P (?) ? P (?|?).
For spelling correction, the channel Pc(?|?) is
a model of what is likely to be typed given an in-
tended message. Uniform models work fairly well
and ones tuned to brainos and typos work even bet-
ter. The source model is typically estimated from
a corpus of ordinary text.
For Chinese word segmentation, the source
model is trained over the corpus with spaces in-
serted. The noisy channel deterministically elim-
inates spaces so that Pc(?|?) = 1.0 if ? is
identical to ? with all of the spaces removed,
and 0.0 otherwise. This channel is easily imple-
mented as a weighted edit distance where dele-
tion of a single space is 100% likely (log proba-
bility edit ?cost? is zero) and matching a charac-
ter is 100% likely, with any other operation be-
ing 0% likely (infinite cost). This makes any seg-
mentation equally likely according to the channel
model, reducing decoding to finding the highest
likelihood hypothesis consisting of the test string
with spaces inserted. This approach reduces to
the cross-entropy/compression-based approach of
(Teahan et al 2000). Experiments showed that
skewing these space-insertion/matching probabil-
ities reduces decoding accuracy.
3.3 LingPipe?s Named Entity Recognition
LingPipe 2.1 introduced a hidden Markov
model interface with several decoders: first-best
(Viterbi), n-best (Viterbi forward, A* backward
with exact Viterbi estimates), and confidence-
based (forward-backward).
LingPipe 2.2 introduced a chunking implemen-
tation that codes a chunking problem as an HMM
tagging problem using a refinement of the stan-
dard BIO coding. The refinement both introduces
context and greatly simplifies confidence estima-
tion over the approach using standard BIO cod-
ing in (Culotta and McCallum 2004). The tags
are B-T for the first character in a multi-character
entity of type T, M-T for a middle character in a
multi-character entity, E-T for the end character in
a multi-character entity, and W-T for a single char-
acter entity. The out tags are similarly contextual-
ized, with additional information on the start/end
tags to model their context. Specifically, the tags
used are B-O-T for a character not in an entity
following an entity of type T, I-O for any mid-
dle character not in an entity, and E-O-T for a
character not in an entity but preceding a charac-
ter in an entity of type T, and finally, W-O-T for
a character that is a single character between two
entities, the following entity being of type T. Fi-
nally, the first tag is conditioned on the begin-of-
sentence tag (BOS) and after the last tag, the end-
of-sentence tag (EOS) is generated. Thus the prob-
abilities normalize to model string/tag joint prob-
abilities.
In the HMM implementation considered here,
transitions between states (tags) in the HMM are
modeled by a maximum likelihood estimate over
the training data. Tag emissions are generated by
bounded character language models. Rather than
the process estimate P (X), we use P (X#|#),
where # is a distinguished boundary character
170
Corpus Encod Sents Chars Uniq Words Uniq Test S Test Ch Unseen
City U HK HKSCS (trad) 57K 4.3M 5113 1.6M 76K 7.5K 364K 0.046%
Microsoft gb18030 (simp) 46K 3.4M 4768 1.3M 63K 4.4K 173K 0.046%
Ac Sinica Big5 (trad) 709K 13.2M 6123 5.5M 146K 11.0K 146K 0.560%
Penn/Colo CP936 (simp) 19K 1.3M 4294 0.5M 37K 5.1K 256K 0.160%
Figure 1: Word Segmentation Corpora
Corpus Sents Chars Uniq LOC PER ORG Test S Test Ch Unseen
City U HK 48K 2.7M 5113 48.2K 36.4K 27.8K 7.5K 364K 0.046%
Microsoft 44K 2.2M 4791 36.9K 17.6K 20.6K 4.4K 173K 0.046%
Figure 2: Named Entity Recognition Corpora
not in the training or test character sets. We also
train with boundaries. For Chinese at the charac-
ter level, this bounding is irrelevant as all tokens
are length 1, so probabilities are already normal-
ized and there is no contextual position to take ac-
count of within a token. In the more usual word-
tokenized case, it normalizes probabilities over all
strings and accounts for the special status of pre-
fixes and suffixes (e.g. capitalization, inflection).
Consider the chunking consisting of the string
John J. Smith lives in Seattle. with John J. Smith a
person mention and Seattle a location mention. In
the coded HMM model, the joint estimate is:
P?ML(B-PER|BOS) ? P?B-PER(John#|#)
? P?ML(I-PER|B-PER) ? P?I-PER(J#|#)
? P?ML(I-PER|I-PER) ? P?I-PER(.#|#)
? P?ML(E-PER|I-PER) ? P?E-PER(Smith#|#)
? P?ML(B-O-PER|E-PER) ? P?B-O-PER(lives#|#)
? P?ML(E-O-LOC|B-O-PER) ? P?E-O-LOC(in#|#)
? P?ML(W-LOC|E-O-LOC) ? P?W-LOC(Seattle#|#)
? P?ML(W-O-EOS|W-LOC) ? P?W-O-EOS(.#|#)
? P?ML(EOS|W-O-EOS)
LingPipe 2.3 introduced an n-best chunking im-
plementation that adapts an underlying n-best
chunker via rescoring. In rescoring, each of these
outputs is scored on its own and the new best
output is returned. The rescoring model is a
longer-distance generative model that produces
alternating out/entity tags for all characters. The
joint probability of the specified chunking is:
P?OUT(cPER|cBOS)
? P?PER(John J. SmithcOUT|cOUT)
? P?OUT( lives in cLOC|cPER)
? P?LOC(SeattlecOUT|cOUT)
? P?OUT(.cEOS|cLOC)
where each estimator is a character language
model, and where the cT are distinct characters
not in the training/test sets that encode begin-of-
sentence (BOS), end-of-sentence (EOS), and type
(e.g. PER, LOC, ORG). In words, we generate an
alternating sequence of OUT and type estimates,
starting and ending with an OUT estimate. We
begin by conditioning on the begin-of-sentence
tag. Because the first character is in an entity, we
do not generate any text, but rather generate a
character indicating that we are done generating
the OUT characters and ready to switch to gen-
erating person characters. We then generate the
phrase John J. Smith in the person model; note
that type estimates always begin and end with the
cOUT character, essentially making them bounded
models. After generating the name and the
character to end the entity, we revert to generating
more out characters, starting from a person and
ending with a location. Note that we are generat-
ing the phrase lives in including the preceding and
following space. All such spaces are generated in
the OUT models for English; there are no spaces in
the Chinese input. Next, we generate the location
phrase the same way as the person phrase. Next,
we generate the final period in the OUT model
and then the end-of-sentence symbol. Note that
the OUT category?s language model shoulders
the brunt of the burden of estimating contextual
effects. It conditions on the preceding type, so
that the likelihood of lives in is conditioned on
following a person entity. Furthermore, the choice
to begin an entity of type location is based on
the fact that it follows lives in. This includes
begin-of-sentence and end-of-sentence effects,
so the model is sensitive to initial capitalization
in the out model as a distribution of character
sequences likely to follow BOS. Similarly, the
171
Corpus R P F1 Best F1 OOV ROOV
City Uni Hong Kong .966 .957 .961 .972 4.0% .555
Microsoft Research .959 .955 .957 .963 3.4% .494
Academia Sinica .951 .935 .943 .958 4.2% .389
U Penn and U Colorado .919 .895 .907 .933 8.8% .459
Figure 3: Word Segmentation Results (Closed Category)
Corpus R P F1 Best F1 PLOC RLOC PPER RPER PORG RORG
City Uni HK .8417 .8690 .8551 .8903 .8961 .8762 .8749 .8943 .6997 .8176
MS Research .8097 .8188 .8142 .8651 .8351 .8716 .7968 .8438 .7739 .6899
Figure 4: Named Entity Recognition Results (Closed Category)
end-of-sentence is conditioned on the preceding
text, in this case a single period. The resulting
model defines a (properly normalized) joint
probability distribution over chunkings.
4 Held-out Parameter Tuning
We ran preliminary tests on MUC 6 English and
City University of Hong Kong data for Chinese
and found baseline performance around 72% and
rescored performance around 82%. The underly-
ing model was designed to have good recall in gen-
erating hypotheses. Over 99% of the MUC test
sentences had their correct analysis in a 1024-best
list generated by the underlying model. Neverthe-
less, setting the number of hypotheses beyond 64
did not improve results in either English or Chi-
nese, so we reported runs with n-best set to 64.
We believe this is because the two language-model
based approaches make highly correlated ranking
decisions based on character n-grams.
Held-out scores peaked with 5-grams for Chi-
nese; 3-grams and 4-grams were not much worse
and longer n-grams performed nearly identically.
We used 7500 as the number of distinct charac-
ters, though this parameter is not at all sensitive
to within an order of magnitude. We used Ling-
Pipe?s default of setting the interpolation parame-
ter equal to the n-gram length; for the final eval-
uation ? = 5.0. Higher interpolation ratios favor
precision over recall, lower ratios favor recall. Val-
ues within an order of magnitude performed with
1% F-measure and 2% precision/recall.
5 Bakeoff Time and Effort
The total time spent on this SIGHAN bakeoff was
about 2 hours for the word segmentation task and
10 hours for the named-entity task (not including
writing this paper). We started from a working
word segmentation system for the last SIGHAN.
Most of the time was spent munging entity data,
with the rest devoted to held out analysis. The final
code was roughly one page per task, with only a
dozen or so LingPipe-specific lines. The final run,
including unpacking, training and testing, took 45
minutes on a 512MB home PC; most of the time
was named-entity decoding.
6 Results
Official bakeoff results for the four word segmen-
tation corpora are shown in Figure 3, and for the
two named entity corpora in Figure 4. Column
labels are R for recall, P for precision, F1 for
balanced F -measure, Best F1 for the best closed
system?s F1 score, OOV for the out-of-vocabulary
rate in the test corpus, and ROOV for recall on the
out-of-vocabulary items. For the named-entity re-
sults, precision and recall are also broken down by
category.
7 Distribution
LingPipe may be downloaded from its homepage,
http://www.alias-i.com/lingpipe. The code
for the bakeoff is available via anonymous CVS
from the sandbox. An Apache Ant makefile is pro-
vided to generate our bakeoff submission from the
official data distribution format.
References
Carpenter, B. 2005. Scaling high-order character language
models to gigabytes. ACL Software Workshop. Ann Arbor.
Culotta, A. and A. McCallum. 2004. Confidence estimation
for information extraction. HLT/NAACL 2004. Boston.
Teahan, W. J., Y. Wen, R. McNab, and I. H. Witten. 2000. A
compression-based algorithm for Chinese word segmenta-
tion. Computational Linguistics, 26(3):375?393.
172
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 187?195,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
The Benefits of a Model of Annotation
Rebecca J. Passonneau
Center for Computational Learning Systems
Columbia University
becky@ccls.columbia.edu
Bob Carpenter
Department of Statistics
Columbia University
carp@alias-i.com
Abstract
This paper presents a case study of a
difficult and important categorical anno-
tation task (word sense) to demonstrate
a probabilistic annotation model applied
to crowdsourced data. It is argued that
standard (chance-adjusted) agreement lev-
els are neither necessary nor sufficient
to ensure high quality gold standard la-
bels. Compared to conventional agree-
ment measures, application of an annota-
tion model to instances with crowdsourced
labels yields higher quality labels at lower
cost.
1 Introduction
The quality of annotated data for computational
linguistics is generally assumed to be good enough
if a few annotators can be shown to be consistent
with one another. Metrics such as pairwise agree-
ment and agreement coefficients measure consis-
tency among annotators. These descriptive statis-
tics do not support inferences about corpus quality
or annotator accuracy, and the absolute values one
should aim for are debatable, as in the review by
Artstein and Poesio (2008). We argue that high
chance-adjusted inter-annotator agreement is nei-
ther necessary nor sufficient to ensure high qual-
ity gold-standard labels. Agreement measures re-
veal little about differences among annotators, and
nothing about the certainty of the true label, given
the observed labels from annotators. In contrast, a
probabilistic model of annotation supports statis-
tical inferences about the quality of the observed
and inferred labels.
This paper presents a case study of a particu-
larly thorny annotation task that is of widespread
interest, namely word-sense annotation. The items
that were annotated are occurrences of selected
words in their sentence contexts, and the annota-
tion labels are WordNet senses (Fellbaum, 1998).
The annotations, collected through crowdsourc-
ing, consist of one WordNet sense for each item
from up to twenty-five different annotators, giv-
ing each word instance a large set of labels. Note
that application of an annotation model does not
require this many labels for each item, and crowd-
sourced annotation data does not require a prob-
abilistic model. This case study, however, does
demonstrate a mutual benefit.
A highly certain ground truth label for each an-
notated instance is the ultimate goal of data anno-
tation. Many issues, however, make this compli-
cated for word sense annotation. The number of
different senses defined for a word varies across
lexical resources, and pairs of senses within a sin-
gle sense inventory are not equally distinct (Ide
and Wilks, 2006; Erk and McCarthy, 2009). A
previous annotation effort using WordNet sense la-
bels demonstrates a great deal of variation across
words (Passonneau et al, 2012b). On over 116
words, chance-adjusted agreement ranged from
very high to chance levels. As a result, the ground
truth labels for many words are questionable. On a
random subset of 45 of the same words, the crowd-
sourced data presented here (available as noted be-
low) yields a certainty measure for each ground
truth label indicating high certainty for most in-
stances.
2 Chance-Adjusted Agreement
Current best practice for collecting and curating
annotated data involves iteration over four steps,
or variations of them: 1) design or redesign the
annotation task, 2) write or revise guidelines in-
187
structing annotators how to carry out the task, pos-
sibly with some training, 3) have two or more an-
notators work independently to annotate a sample
of data, and 4) measure the interannotator agree-
ment on the data sample. Once the desired agree-
ment has been obtained, a gold standard dataset
is created where each item is annotated by one
annotator. As noted in the introduction, how
much agreement is sufficient has been much dis-
cussed (Artstein and Poesio, 2008; di Eugenio and
Glass, 2004; di Eugenio, 2000; Bruce and Wiebe,
1998). The quality of the gold standard is not ex-
plicitly measured. Nor is the accuracy of the an-
notators. Since there are many ways to be inaccu-
rate, and only one way to be accurate, it is assumed
that if annotators agree, then the annotation must
be accurate. This is often but not always correct.
If two annotators do not agree well, this method
does not identify whether one annotator is more
accurate than the other. For the individual items
they disagree on, no information is gained about
the true label.
To get a high level sense of the limitations of
agreement metrics, we briefly discuss how they
are computed and what they tell us. For a com-
mon notation, let i ? 1:I represent the set of all
items, j ? 1:J all the annotators, k ? 1:K all the
label classes in a categorical labeling scheme (e.g.,
word senses), and yi,j ? 1:K the observed labels
from annotator j for item i (assuming every anno-
tator labels every item exactly once; we relax this
restriction later).
Agreement: Pairwise agreement Am,n between
two annotators m,n ? 1:J is defined as the pro-
portion of items 1:I for which the annotators sup-
plied the same label,
Am,n = 1I
?I
i=1 I(yi,m = yi,n),
where the indicator function I(s) = 1 if s is true
and 0 otherwise. Am,n is thus the maximum like-
lihood estimate that annotator m and n will agree.
Pairwise agreement can be extended to the en-
tire pool of annotators by averaging over all
(J
2
)
pairs,
A = 1
(J2)
?J
m=1
?J
n=m+1Am,n.
By construction, Am,n ? [0, 1] and A ? [0, 1].
Pairwise agreement does not take into account
the proportion of observed annotation values from
1:K. As a simple expected chance of agreement, it
provides little information about the resulting data
quality.
Chance-Adjusted Agreement: An agreement
coefficient, such as Cohen?s ? (Cohen, 1960) or
Krippendorff?s ? (Krippendorff, 1980), measures
the proportion of observed agreements that are
above the proportion expected by chance. Given
an estimate Am,n of the probability that two an-
notators m,n ? 1:J will agree on a label and
an estimate of the probability Cm,n that they
will agree by chance, the chance-adjusted inter-
annotator agreement coefficient IAm,n ? [?1, 1]
is defined by
IAm,n =
Am,n?Cm,n
1?Cm,n
.
For Cohen?s ? statistic, chance agreement is de-
fined to take into account the prevalence of the
individual labels in 1:K. Specifically, it is de-
fined to be the probability that a pair of labels
drawn at random for two annotators agrees. There
are two common ways to define this draw. The
first assumes each annotator draws uniformly at
random from her set of labels. Letting ?j,k =
1
I
?I
i=1 I(yi,j = k) be the proportion of the label k
in annotator j?s labels, this notion of chance agree-
ment for a pair of annotators m,n is estimated as
the sum over 1:K of the products of their propor-
tions ?:
Cm,n =
?K
k=1 ?m,k ? ?n,k.
Another computation of chance agreement in wide
use assumes each annotator draws uniformly at
random from the pooled set of labels from all an-
notators (Krippendorff, 1980). Letting ?k be the
proportion of label k in the entire set of labels, this
alternative estimate, C ?m,n =
?K
k=1 ?
2
k, does not
depend on the identity of the annotators m and n.
An inter-annotator agreement statistic like ?
suffers from multiple shortcomings. (1) Agree-
ment statistics are intrinsically pairwise, although
one can compare to a voted consensus or aver-
age over multiple pairwise agreements. (2) In
agreement-based analyses, two wrongs make a
right; if two annotators both make the same mis-
take, they agree. If annotators are 80% accurate
on a binary task, chance agreement on the wrong
category occurs at a 4% rate. (3) Chance-adjusted
agreement reduces to simple agreement as chance
agreement approaches zero. When chance agree-
ment is high, even high-accuracy annotators can
188
have low chance-adjusted agreement. For ex-
ample, in a binary task with 95% prevalence of
one category, two 90% accurate annotators have
a chance-adjusted agreement of 0.9?(.95
2+.052)
1?(.952+.052) =
?.053. Thus high chance-adjusted inter-annotator
agreement is not a necessary condition for a high-
quality corpus. (4) Inter-annotator agreement
statistics implicitly assume annotators are unbi-
ased; if they are biased in the same direction, as we
show they are for the sense data considered here,
then agreement is an overestimate of their accu-
racy. In the extreme case, in a binary labeling task,
two adversarial annotators who always provide the
wrong answer have a chance-adjusted agreement
of 100%. (5) Item-level effects such as difficulty
can inflate levels of agreement-in-error. For ex-
ample, hard-to-identify names in a named-entity
corpus have correlated false negatives among an-
notators, leading to higher agreement-in-error than
would otherwise be expected. (6) Inter-annotator
agreement statistics are rarely computed with con-
fidence intervals, which can be quite wide even
under optimistic assumptions of no annotator bias
or item-level effects. In a sample of MASC word
sense data, 100 annotations by 80% accurate an-
notators produce a 95% interval for accuracy of
+/- 6%. Agreement statistics have even wider er-
ror bounds. This introduces enough uncertainty to
span the rather arbitrary decision boundaries for
acceptable agreement.
Model-Based Inference: In contrast to agreement
metrics, application of a model of annotation can
provide information about the certainty of param-
eter estimates. The model of annotation presented
in the next section includes as parameters the true
categories of items in the corpus, and also the
prevalence of each label in the corpus and each
annotator?s accuracies and biases by category.
3 A Probabilistic Annotation Model
A probabilistic model provides a recipe to ran-
domly ?generate? a dataset from a set of model
parameters and constants.1 The utility of a math-
ematical model lies in its ability to support mean-
ingful inferences from data, such as the true preva-
lence of a category. Here we apply the probabilis-
tic model of annotation introduced in (Dawid and
Skene, 1979); space does not permit detailed dis-
1In a Bayesian setting, the model parameters are them-
selves modeled as randomly generated from a prior distribu-
tion.
n iin jjn yn
1 1 1 4
2 1 3 1
3 192 17 5
...
...
...
...
Table 1: Table of annotations y indexed by word
instance ii and annotator jj.
cussion here of the inference process (this will be
provided in a separate paper that is currently in
preparation). Dawid and Skene used their model
to determine a consensus among patient histories
taken by multiple doctors. We use it to estimate
the consensus judgement of category labels based
on word sense annotations provided by multiple
Mechanical Turkers. Inference is driven by accu-
racies and biases estimated for each annotator on
a per-category basis.
Let K be the number of possible labels or cate-
gories for an item, I the number of items to anno-
tate, J the number of annotators, and N the total
number of labels provided by annotators, where
each annotator may label each instance zero or
more times. Each annotation is a tuple consist-
ing of an item ii ? 1:I , an annotator jj ? 1:J ,
and a label y ? 1:K. As illustrated in Table 1, we
assemble the annotations in a database-like table
where each row is an annotation, and the values in
each column are indices over the item, annotator,
and label. For example, the first two rows show
that on item 1, annotators 1 and 3 assigned labels
4 and 1, respectively. The third row says that for
item 192 annotator 17 provided label 5.
Dawid and Skene?s model includes parameters
? zi ? 1:K for the true category of item i,
? pik ? [0, 1] for the probability that an item is
of category k, subject to
?K
k=1 pik = 1, and
? ?j,k,k? ? [0, 1] for the probabilty that annota-
tor j will assign the label k? to an item whose
true category is k, subject to
?K
k?=1 ?j,k,k? =
1.
The generative model first selects the true cate-
gory for item i according to the prevalence of cat-
egories, which is given by a Categorical distribu-
tion,2
zi ? Categorical(pi).
2The probability of n successes inm trials has a binomial
distribution, with each trial (m=1) having a Bernoulli dis-
tribution. Data with more than two values has a multinomial
189
Word Pos Senses ? Agreement
curious adj 3 0.94 0.97
late adj 7 0.84 0.89
high adj 7 0.77 0.91
different adj 4 0.13 0.60
severe adj 6 0.05 0.32
normal adj 4 0.02 0.38
strike noun 7 0.89 0.93
officer noun 4 0.85 0.91
player noun 5 0.83 0.93
date noun 8 0.48 0.58
island noun 2 0.10 0.78
success noun 4 0.09 0.39
combination noun 7 0.04 0.73
entitle verb 3 0.99 0.99
mature verb 6 0.86 0.96
rule verb 7 0.85 0.90
add verb 6 0.55 0.72
help verb 8 0.26 0.58
transfer verb 9 0.22 0.42
ask verb 7 0.10 0.37
justify verb 5 0.04 0.82
Table 2: Agreement results for MASC words with
the three highest and lowest ? scores, by part of
speech, along with additional words discussed in
the text (boldface).
The observed labels yn are generated based on
annotator jj[n]?s responses ?jj[n], z[ii[n]] to items
ii[n] whose true category is zz[ii[n]],
yn ? Categorical(?jj[n], z[ii[n]]).
We use additively smoothed maximum likelihood
estimation (MLE) to stabilize inference. This is
equivalent to maximum a posteriori (MAP) estima-
tion in a Bayesian model with Dirichlet priors,
?j,k ? Dirichlet(?k) pi ? Dirichlet(?).
The unsmoothed MLE is equivalent to the MAP es-
timate when ?k and ? are unit vectors. For our
experiments, we added a tiny fractional count to
unit vectors, corresponding to a very small degree
of additive smoothing applied to the MLE.
4 MASC Word Sense Sentence Corpus
MASC (Manually Annotated SubCorpus) is a very
heterogeneous 500,000 word subset of the Open
American National Corpus (OANC) with 16 types
of annotation.3 MASC contains a separate word
sense sentence corpus for 116 words nearly evenly
distribution (a generalization of the binomial). Each trial then
results in one of k outcomes with a categorical distribution.
3Both corpora are available from http://www.anc.
org. The crowdsourced MASC words and labels will also
be available for download.
balanced among nouns, adjectives and verbs (Pas-
sonneau et al, 2012a). Each sentence is drawn
from the MASC corpus, and exemplifies a partic-
ular word form annotated for a WordNet sense.
To motivate our aim, which is to compare MASC
word sense annotations with the annotations we
collected through crowdsourcing, we review the
MASC word sense corpus and some of its limita-
tions.
College students from Vassar, Barnard, and
Columbia were trained to carry out the MASC word
sense annotation (Passonneau et al, 2012a). Most
annotators stayed with the project for two to three
years. Along with general training in the anno-
tation process, annotators trained for each word
on a sample of fifty sentences to become famil-
iar with the sense inventory through discussion
with Christiane Fellbaum, one of the designers
of WordNet, and if needed, to revise the sense
inventory for inclusion in subsequent releases of
WordNet. After the pre-annotation sample, an-
notators worked independently to label 1,000 sen-
tences for each word using an annotation tool that
presented the WordNet senses and example us-
ages, plus four variants of none of the above. Pas-
sonneau et al describe the training and annotation
tools in (2012b; 2012a). For each word, 100 of the
total sentences were annotated by three or four an-
notators for assessment of inter-annotator reliabil-
ity using pairwise agreement and Krippendorff?s
?.
The MASC agreement measures varied widely
across words. Table 2 shows for each part of
speech the words with the three highest and three
lowest ? scores, along with additional words ex-
emplified below (boldface).4 The ? values in col-
umn 2 range from a high of 0.99 (for entitle, verb,
3 senses) to a low of 0.02 (normal, adjective, 3
senses). Pairwise agreement (column 3) has simi-
larly wide variation. Passonneau et al (2012b) ar-
gue that the differences were due in part to the dif-
ferent words: each word is a new annotation task.
The MASC project deviated from the best prac-
tices described in section 2 in that there was no
iteration to achieve some threshold of agreement.
All annotators, however, had at least two phases
of training. Table 2 illustrates that annotators can
agree on words with many senses, but at the same
time, there are many words with low agreement.
4This table differs from a similar one Passonneau et al
give in (2012b) due to completion of more words and other
updates.
190
Even with high agreement, the measures reported
in Table 2 provide no information about word in-
stance quality.
5 Crowdsourced Word Sense Annotation
Amazon Mechanical Turk is a venue for crowd-
sourcing tasks that is used extensively in the NLP
community (Callison-Burch and Dredze, 2010).
Human Intelligence Tasks (HITs) are presented to
turkers by requesters. For our task, we used 45
randomly selected MASC words, with the same
sentences and WordNet senses the trained MASC
annotators used. Given our 1,000 instances per
word, for a category whose prevalence is as low
as 0.10 (100 examples expected), the 95% interval
for observed examples, assuming examples are in-
dependent, will be 0.10 ? 0.06. One of our future
goals for this data is to build item difficulty into the
annotation model, so we collected 20 to 25 labels
per item to get reasonable confidence intervals for
the true label. This will also sharpen our estimates
of the true category significantly, as estimated er-
ror goes down as 1/
?
n with n independent anno-
tations; confidence intervals must be expanded as
correlation among annotator responses increases
due to annotator bias or item-level effects such as
difficulty or subject matter.
In each HIT, turkers were presented with ten
sentences for each word, with the word?s senses
listed below each sentence. Each HIT had a short
paragraph of instructions indicating that turkers
could expect their time per HIT to decrease as their
familiarity with a word?s senses increased (we
wanted multiple annotations per turker per word
for tighter estimates of annotator accuracies and
biases).
To insure a high proportion of instances with
high quality inferred labels, we piloted the HIT de-
sign and payment regimen with two trials of two
and three words each, and discussed both with
turkers on the Turker Nation message board. The
final procedure and payment were as follows. To
avoid spam workers, we required turkers to have
a 98% lifetime approval rating and to have suc-
cessfully completed 20,000 HITs. Our HITs were
automatically approved after fifteen minutes. We
considered manual approval and programming a
more sophisticated approval procedure, but both
were deemed too onerous given the scope of
our task. Instead, we monitored performance of
turkers across HITs by comparing each individ-
ual turker?s labels to the current majority labels.
Turkers with very poor performance were warned
to take more care, or be blocked from doing fur-
ther HITs. Of 228 turkers, five were blocked, with
one subsequently unblocked. The blocked turker
data is included in our analyses and in the full
dataset, which will be released in the near future;
the model-based approach to annotation is effec-
tive at adjusting for inaccurate annotators.
6 Annotator Accuracy and Bias
Through maximum likelihood estimation of the
parameters of the Dawid and Skene model, an-
notators? accuracies and error biases can be esti-
mated. Figure 1a) shows confusion matrices in the
form of heatmaps that plot annotator responses by
the estimated true labels for four of the 57 annota-
tors who contributed labels for add-v (the affixes
-v and -n represent part of speech). This word
had a reliability of ?=0.56 for four trained MASC
annotators on 100 sentences and pairwise agree-
ment=0.73. Figure 1b) shows heatmaps for four of
the 49 annotators on help-v, which had a reliability
of ?=0.26 for the MASC annotators, with pairwise
agreement=0.58. As indicated in the figure keys,
darker cells have higher probabilities. Perfect ac-
curacy of annotator responses (agreement with the
inferred reference label) would yield black squares
on the diagonal, with all the off-diagonal squares
in white.
The two figures show that the turkers were
generally more accurate on add-v than on help-
v, which is consistent with the differences in the
MASC agreement on these two words. In contrast
to the knowledge gained from agreement metrics,
inference based on the annotation model provides
estimates of bias towards specific category values.
Figure 1a shows the bias of these annotators to
overuse WordNet sense 1 for help-v; bias appears
in the plots as an uneven distribution of grey boxes
off the main diagonal. Further, there were no as-
signments of senses 6 or 8 for this word. The fig-
ures provide a succinct visual summary that there
were more differences across the four annotators
for help-v than for add-v, with more bias towards
overuse of not only sense 1, but also senses 2 (an-
notators 8 and 41) and 3 (annotator 9). When an-
notator 8 uses sense 1, the true label is often sense
6, thus illustrating how annotators provide infor-
mation about the true label even from inaccurate
responses.
191
(a) Four of 57 annotators for add-v
(b) Four of 49 annotators for help-v
Figure 1: Heatmaps of annotators? accuracies and biases
For the 45 words, average accuracies per word
ranged from 0.05 to 0.86, with most words show-
ing a large spread. Examination of accuracies by
sense shows that accuracy was often highest for
the more frequent senses. Accuracy for add-v
ranged from 0.25 to 0.73, but was 0.90 for sense
1, 0.79 for sense 2, and much lower for senses
6 (0.29) and 7 (0.19). For help-v, accuracy was
best on sense 1 (0.73), which was also the most
frequent, but it was also quite good on sense 4
(0.64), which was much less frequent. Accuracies
on senses of help-v ranged from 0.11 (senses 5, 7,
and other) to 0.73 (sense 1).
7 Estimates for Prevalence and Labels
That the Dawid and Skene model allows an-
notators to have distinct biases and accuracies
should match the intuitions of anyone who has
performed annotation or collected annotated data.
The power of their parameterization, however,
shows up in the estimates their model yields for
category prevalence (rate of each category) and for
the true labels on each instance. Figure 2 con-
trasts five ways to estimate the sense prevalence
of MASC words, two of which are based on models
estimated via MLE. The MLE estimates each have
an associated probability, thus a degree of cer-
tainty, with more certain estimates derived from
the larger sets of crowdsourced labels (AMT MLE).
MASC Freq is a simple ratio. Majority voted labels
tend to be superior to single labels, but do not take
annotators? biases into account.
The plots for the four words in Figure 2 are or-
dered by their ? scores from four trained MASC
annotators (see Table 2). There is a slight trend
for the various estimates to diverge less on words
where agreement is higher. The notable result,
however, is that for each word, the plot demon-
strates one or more senses where the AMT MLE es-
timate differs markedly from all other estimates.
For add-v, the AMT MLE estimate for sense 1 is
much lower (0.51) than any of the other measures
(0.61-0.64). For date-n, the AMT MLE estimate for
sense 4 is much closer to the other estimates than
AMT Maj, which sugggests that some AMT an-
notators are baised against sense 4. The AMT MLE
estimates for senses 6 and 7 are quite distinct. For
help-v, the AMT MLE estimates for senses 1 and 6
are also very distinct. For ask-v, there are more
differences across all estimates for senses 2 and 4,
with the AMT MLE estimate neither the highest nor
the lowest.
The estimates of label quality on each item are
perhaps the strongest reason for turning to model-
based approaches to assess annotated data. For the
same four words discussed above, Table 3 shows
the proportion of all instances that had an esti-
mated true label where the label probability was
greater than or equal to 0.99. For these words with
? scores ranging from 0.10 (ask-v) to 0.55 (add-v),
the proportion of very high quality inferred true
labels ranges from 81% to 94%. Even for help-
v, of the remaining 19% of instances, 13% have
probabilities greater than 0.75. Table 3 also shows
192
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6
add-v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(a) add-v (? = 0.55, agreement=0.72)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense
1
Sense
2
Sense
3
Sense Sense
5
Sense
6
Sense
7
Sense
8
date -n MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(b) date-n (? = 0.48, agreement=0.58)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense
1
Sense
2
Sense
3
Sense
4
Sense
5
Sense
6
Sense
7
Sense
8
hel p -v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(c) help-v (? = 0.26, agreement=0.58)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6 Sense 7
ask -v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(d) ask-v (? = 0.10, agreement=0.37)
Figure 2: Prevalence estimates for 4 MASC words; (MASC Freq) frequency of each sense in ? 1, 000
singly-annotated instances from the trained MASC annotators; (MASC Maj) frequency of majority vote
sense in ?100 instances annotated by four trained MASC annotators; (MASC MLE) estimated probability
of each sense in the same 100 instances annotated by four MASC annotators, using MLE; (AMT Maj)
frequency of each majority vote sense for ? 1000 instances annotated by ? 25 turkers; (AMT MLE)
estimated probability of each sense in the same ?1000 instances annotated by ?25 turkers, using MLE
Sense k ? 0.99 Prop.
0 9 0.01
1 461 0.48
2 135 0.14
3 107 0.11
4 50 0.05
5 50 0.05
6 93 0.10
SubTot 905 0.94
Rest 62 0.06
(a) add-v: 94%
Sense k ? 0.99 Prop.
0 19 0.02
1 68 0.07
2 19 0.02
3 83 0.09
4 173 0.18
5 190 0.20
6 133 0.14
7 236 0.25
8 5 0.01
SubTot 926 0.97
Rest 33 0.03
(b) date-n: 97%
Sense k ? 0.99 Prop.
0 0 0.00
1 279 0.30
2 82 0.09
3 201 0.21
4 24 0.03
5 0 0.00
6 169 0.18
7 0 0.00
8 5 0.01
SubTot 760 0.81
Rest 180 0.19
(c) help-v: 81%
Sense k ? 0.99 Prop.
0 6 0.01
1 348 0.36
2 177 0.18
3 9 0.01
4 251 0.26
5 0 0
6 0 0
7 6 0.01
8 6 0.01
SubTot 803 0.83
Rest 163 0.17
(d) ask-v: 83%
Table 3: Proportion of high quality labels per word
193
that the high quality labels for each word are dis-
tributed across many of the senses. Of the 45
words studied here, 22 had ? scores less than 0.50
from the trained annotators. For 42 of the same
45 words, 80% of the inferred true labels have a
probability higher than 0.99.
In contrast to current best practices, an annota-
tion model yields far more information about the
most essential aspect of annotation efforts, namely
how much uncertainty is associated with each gold
standard label, and how the uncertainty is dis-
tributed across other possible label categories for
each instance. An equally important benefit comes
from a comparison of the cost per gold standard
label. Over the course of a five-year period that
included development of the infrastructure, the
undergraduates who annotated MASC words were
paid an estimated total of $80,000 for 116 words
? 1000 sentences per word, which comes to a unit
cost of $0.70 per ground truth label. In a 12 month
period with 6 months devoted to infrastructure and
trial runs, we paid 224 turkers a total of $15,000
for 45 words? 1000 sentences per word, for a unit
cost of $0.33 per ground truth label. In short, the
AMT data cost less than half the trained annotator
data.
8 Related Work
The model proposed by Dawid and Skene (1979)
comes out of a long practice in epidemiology
to develop gold-standard estimation. Albert and
Dodd (2008) give a relevant discussion of dis-
ease prevalence estimation adjusted for accuracy
and bias of diagnostic tests. Like Dawid and
Skene (1979), Smyth (1995) used unsupervised
methods to model human annotation of craters on
images of Venus. In the NLP literature, Bruce
and Wiebe (1999) and Snow et al (2008) use
gold-standard data to estimate Dawid and Skene?s
model via maximum likelihood; Snow et al show
that combining noisy crowdsourced annotations
produced data of equal quality to five distinct pub-
lished gold standards. Rzhetsky et al (2009) and
Whitehill et al (2009) estimate annotation mod-
els without gold-standard supervision, but nei-
ther models annotator biases, which are criti-
cal for estimating true labels. Klebanov and
Beigman (2009) discuss censoring uncertain items
from gold-standard corpora. Sheng et al (2008)
apply similar models to actively select the next la-
bel to elicit from annotators. Smyth et al (1995),
Rogers et al (2010), and Raykar et al (2010)
all discuss the advantages of learning and evalu-
ation with probabilistically annotated corpora. By
now crowdsourcing is so widespread that NAACL
2010 sponsored a workshop on ?Creating Speech
and Language Data With Amazons Mechanical
Turk? and in 2011, TREC added a crowdsourcing
track.
9 Conclusion
The case study of word sense annotation presented
here demonstrates that in comparison to current
practice for assessment of annotated corpora, an
annotation model applied to crowdsourced labels
provides more knowledge and higher quality gold
standard labels at lower cost. Those who would
use the corpus for training benefit because they
can differentiate high from low confidence la-
bels. Cross-site evaluations of word sense dis-
ambiguation systems could benefit because there
are more evaluation options. Where the most
probable label is relatively uncertain, systems can
be penalized less for an incorrect but close re-
sponse (e.g., log loss). Systems that produce sense
rankings for each instance could be scored us-
ing metrics that compare probability distributions,
such as Kullbach-Leibler divergence (Resnik and
Yarowsky, 2000). Wider use of annotation mod-
els should lead to more confidence from users in
corpora for training or evaluation.
Acknowledgments
The first author was partially supported by from
NSF CRI 0708952 and CRI 1059312, and the
second by NSF CNS-1205516 and DOE DE-
SC0002099. We thank Shreya Prasad for data
collection, Mitzi Morris for feedback on the paper,
Marilyn Walker for advice on Mechanical Turk,
and Nancy Ide, Keith Suderman, Tim Brown and
Mitzi Morris for help with the sentence data.
References
Paul S. Albert and Lori E. Dodd. 2008. On esti-
mating diagnostic accuracy from studies with mul-
tiple raters and partial gold standard evaluation.
Journal of the American Statistical Association,
103(481):61?73.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
194
Rebecca F. Bruce and Janyce M. Wiebe. 1998. Word-
sense distinguishability and inter-coder agreement.
In Proceedings of Empirical Methods in Natural
Language Processing.
Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: a case study of manual tagging.
Natural Language Engineering, 1(1):1?16.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society.
Series C (Applied Statistics), 28(1):20?28.
Barbara di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational Lin-
guistics, 30(1):95?101.
Barbara di Eugenio. 2000. On the usage of kappa
to evaluate agreement on coding tasks. In Proceed-
ings of the Second International Conference on Lan-
guage Resources and Evaluation (LREC).
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Nancy Ide and Yorick Wilks. 2006. Making sense
about sense. In Word Sense Disambiguation: Al-
gorithms and Applications, pages 47?74. Springer
Verlag.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
putational Linguistics, 35(4):495?503.
Klaus Krippendorff. 1980. Content analysis: An in-
troduction to its methodology. Sage Publications,
Beverly Hills, CA.
Rebecca J. Passonneau, Collin F. Baker, Christiane
Fellbaum, and Nancy Ide. 2012a. The MASC
word sense corpus. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multi-
ply labeled word sense annotations. Language Re-
sources and Evaluation, 46(2):219?252.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
Journal of Machine Learning Research, 11:1297?
1322.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. Natu-
ral Language Engineering, 5(3):113?133.
Simon Rogers, Mark Girolami, and Tamara Polajnar.
2010. Semi-parametric analysis of multi-rater data.
Statistical Computing, 20:317?334.
Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur.
2009. How to get the most out of your curation ef-
fort. PLoS Computational Biology, 5(5):1?13.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of the Fourteenth ACM Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD).
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground
truth from subjectively-labeled images of Venus. In
Advances in Neural Information Processing Systems
7, pages 1085?1092. MIT Press.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but
is it good? evaluating non-expert annotations for
natural language tasks. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 254?263, Honolulu.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In Proceedings
of the 24th Annual Conference on Advances in Neu-
ral Information Processing Systems.
195
