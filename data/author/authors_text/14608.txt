Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689?692,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Generation of Personalized Annotation Tags for Twitter Users
Wei Wu, Bin Zhang, Mari Ostendorf
Electrical Engineering
University of Washington, Seattle, WA
{weiwu, binz, ostendor}@uw.edu
Abstract
This paper introduces a system designed for
automatically generating personalized annota-
tion tags to label Twitter user?s interests and
concerns. We applied TFIDF ranking and
TextRank to extract keywords from Twitter
messages to tag the user. The user tagging pre-
cision we obtained is comparable to the preci-
sion of keyword extraction fromweb pages for
content-targeted advertising.
1 Introduction
Twitter is a communication platform which com-
bines SMS, instant messages and social networks. It
enables users to share information with their friends
or the public by updating their Twitter messages.
A large majority of the Twitter users are individ-
ual subscribers, who use Twitter to share informa-
tion on ?what am I doing? or ?what?s happening
right now?. Most of them update their Twitter mes-
sages very frequently, in which case the Twitter mes-
sages compose a detailed log of the user?s everyday
life. These Twitter messages contain rich informa-
tion about an individual user, including what s/he is
interested in and concerned about. Identifying an
individual user?s interests and concerns can help po-
tential commercial applications. For instance, this
information can be employed to produce ?follow-
ing? suggestions, either a person who shares simi-
lar interests (for expanding their social network) or
a company providing products or services the user is
interested in (for personalized advertisement).
In this work, we focus on automatically gener-
ating personalized annotation tags to label Twitter
user?s interests and concerns. We formulate this
problem as a keyword extraction task, by selecting
words from each individual user?s Twitter messages
as his/her tags. Due to the lack of human generated
annotations, we employ an unsupervised strategy.
Specifically, we apply TFIDF ranking and TextRank
(Mihalcea and Tarau, 2004) keyword extraction on
Twitter messages after a series of text preprocess-
ing steps. Experiments on randomly selected users
showed good results with TextRank, but high vari-
ability among users.
2 Related Work
Research work related to Twitter message analysis
includes a user sentiment study (Jansen et al, 2009)
and information retrieval indexing. To our knowl-
edge, no previously published research has yet ad-
dressed problems on tagging user?s personal inter-
ests from Twitter messages via keyword extraction,
though several studies have looked at keyword ex-
traction using other genres.
For supervised keyword extraction, (Turney,
2000; Turney, 2003; Hulth, 2003; Yih et al, 2006;
Liu et al, 2008) employed TFIDF or its variants
with Part-of-Speech (POS), capitalization, phrase
and sentence length, etc., as features to train key-
word extraction models, and discriminative training
is usually adopted. Yih et al (2006) use logis-
tic regression to extract keywords from web pages
for content-targeted advertising, which has the most
similar application to our work. However, due to the
lack of human annotation on Twitter messages, we
have to adopt an unsupervised strategy.
For unsupervised keyword extraction, TFIDF
ranking is a popular method, and its effective-
ness has been shown in (Hulth, 2003; Yih et al,
2006). TextRank and its variants (Mihalcea and Ta-
rau, 2004; Wan et al, 2007; Liu et al, 2009) are
graph-based text ranking models, which are derived
from Google?s PageRank algorithm (Brin and Page,
1998). It outperforms TFIDF ranking on traditional
keyword extraction tasks. However, previous work
on both TFIDF ranking and TextRank has been done
mainly on academic papers, spoken documents or
689
web pages, whose language style is more formal (or,
less ?conversational?) than that of Twitter messages.
Twitter messages contain large amounts of ?noise?
like emoticons, internet slang words, abbreviations,
and misspelled words. In addition, Twitter messages
are a casual log of a user?s everyday life, which often
lacks of a coherent topic sequence compared to aca-
demic papers and most spoken documents. Hence,
it remains to see whether TFIDF ranking and Tex-
tRank are effective for identifying user?s interests
from Twitter messages.
3 System Architecture
Figure 1 shows the framework of our system for
tagging Twitter user?s interests. A preprocessing
pipeline is designed to deal with various types of
?noise? in Twitter messages and produce candidate
words for user tags. Then the TFIDF ranking or Tex-
tRank algorithm is applied to select user tags from
the candidate words.
Removing replying messages
Removing emoticons
Substituting/removing internet 
slang words and abbreviations
Part-of-Speech tagging and 
filtering
TFIDF ranking / TextRank
Personalized annotation tags for 
the Twitter user
Messages from 
one Twitter user
Preprocessing
Stemming and stopword removing
Figure 1: Framework of the personalized annotation tag
generation system for Twitter users
3.1 Preprocessing
In addition to messages describing ?What am I do-
ing? or ?what?s happening right now?, Twitter users
also write replying messages to comment on other
users? messages. This kind of message generally
contains more information about the users they re-
ply to than about themselves, and therefore they are
removed in the preprocessing pipeline.
Emoticons frequently appear in Twitter messages.
Although some of them help express user?s senti-
ment on certain topics, they are not directly helpful
for keyword analysis and may interfere with POS
tagging in the preprocessing pipeline. Therefore, we
designed a set of regular expressions to detect and
remove them.
Internet slang words and abbreviations are widely
used in Twitter messages. Most of them are out-of-
vocabulary words in the POS tagging model used
in the next step, and thus will deteriorate the POS
tagging accuracy. Hence, we build a lookup table
based on the list of abbreviations in the NoSlang on-
line dictionary,1 which we divide by hand into three
sets for different processing. The first set includes
422 content words and phrases, such as ?bff? (best
friend forever) and ?fone? (phone), with valid can-
didate words for user tags. The second set includes
67 abbreviations of function words that usually form
grammatical parts in a sentence, such as ?im? (i?m),
?abt? (about). Simply removing them will affect the
POS tagging. Thus, the abbreviations in both these
sets are replaced with the corresponding complete
words or phrases. The third set includes 4576 phrase
abbreviations that are usually separable parts of a
sentence that do not directly indicate discussion top-
ics, such as ?lol? (laugh out loud), ?clm? (cool like
me), which are removed in this step.
We apply the Stanford POS tagger (Toutanova
and Manning, 2000) on Twitter messages, and only
select nouns and adjectives as valid candidates for
user tags. At the end of the preprocessing pipeline,
the candidate words are processed with the rule-
based Porter stemmer2 and stopwords are filtered us-
ing a publicly available list.3
1www.noslang.com/dictionary
2tartarus.org/ martin/PorterStemmer/
3armandbrahaj.blog.al/2009/04/14/
list-of-english-stop-words/
690
3.2 User Tag Extraction
3.2.1 TFIDF ranking
In the TFIDF ranking algorithm, messages from
user u are put together as one document. The TFIDF
value of word i from this user?s messages is com-
puted as
tfidfi,u =
ni,u
?
j nj,u
log(
U
Ui
)
where ni,u is the count of word i in user u?s mes-
sages, Ui is the number of users whose messages
contain word i, and U is the total number of users in
the Twitter corpus. For each user, words with top N
TFIDF values are selected as his/her tags.
3.2.2 TextRank
According to the TextRank algorithm (Mihalcea
and Tarau, 2004), each candidate word is repre-
sented by a vertex in the graph; edges are added
between two candidate words according to their co-
occurrence. In the context of user tag extraction, we
build a TextRank graph with undirected edges for
each Twitter user. One edge is added between two
candidate words if they co-exist within at least one
message; the edge weight is set to be the total count
of within-message co-occurrence of the two words
throughout all messages of this user.
Starting with an arbitrarily assigned value (e.g.
1.0), the rank value R(Vi) of the candidate word at
vertex Vi is updated iteratively according to the fol-
lowing equation,
R(Vi) = (1? d)+ d
?
Vj?E(Vi)
wji
?
Vk?E(Vj) wjk
R(Vj)
where wji is the weight of the edge that links Vj
and Vi, E(Vi) is the set of vertices which Vi is con-
nected to, and d is a damping factor that is usually
set to 0.85 (Brin and Page, 1998). The rank update
iteration continues until convergence. The candidate
words are then sorted according to their rank values.
Words with top-N rank values are selected as tags
for this user.
4 Experiment
4.1 Experimental Setup
We employed the Twitter API to download Twitter
messages. A unigram English language model was
Precision (%) TFIDF TextRank
top-1 59.6 67.3
top-3 61.5 66.0
top-5 61.2 63.0
top-10 59.0 58.3
Table 1: Tagging precision on all users in the test set
used to filter out non-English users. We obtained
messages from 11,376 Twitter users, each of them
had 180 to 200 messages. The word IDF for TFIDF
ranking was computed over these users.
We adopted an evaluation measure similar to the
one proposed in (Yih et al, 2006) for identifying
advertising keywords on web pages, which empha-
sizes precision. We randomly selected 156 Twit-
ter users to evaluate the top-N precision of TFIDF
ranking and TextRank. After we obtained the top-
N outputs from the system, three human evaluators
were asked to judge whether the output tags from the
two systems (unidentified) reflected the correspond-
ing Twitter user?s interests or concerns according to
the full set of his/her messages.4 We adopted a con-
servative standard in the evaluation: when a person?s
name is extracted as a user tag, which is frequent
among Twitter users, we judge it as a correct tag
only when it is a name of a famous person (pop star,
football player, etc). The percentage of the correct
tags among the top-N selected tags corresponds to
the top-N precision of the system.
4.2 Experimental Results
Table 1 gives the top-N precision for TFIDF and
TextRank for different values of N, showing that
TextRank leads to higher precision for small N. Al-
though Twitter messages are much ?noisier? than
regular web pages, the top-N precision we obtained
for Twitter user tagging is comparable to the web
page advertising keyword extraction result reported
in (Yih et al, 2006).
Figure 2 shows an example of the candidate word
ranking result of a Twitter user by TextRank (the
font size is set to be proportional to each word?s
TextRank value). By examining the Twitter mes-
sages, we found that this user is an information tech-
4The pairwise Kappa value for inter-evaluator agreement
ranged from 0.77-0.83, showing good agreement.
691
Figure 2: Example of a Twitter user?s word ranks (the
font size is proportional to each word?s TextRank value)
Precision (%)
top-N ? >0.6 ? ?0.6 H>5.4 H?5.4
top-1 71.6 60.7 78.5 50.8
top-3 71.9 56.8 74.2 54.0
top-5 69.3 53.1 69.2 53.7
top-10 65.1 47.7 63.8 50.2
Table 2: TextRank tagging precision on users with dif-
ferent Top-10 TextRank value standard deviation (?) and
user message text entropy (H).
nology ?geek?, who is very interested in writing Ap-
ple?s iPhone applications, and also a user of Google
Wave. In this work, we use only isolated words as
user tags, however, ?google?, ?wave?, and ?palo?,
?alto? extracted in this example indicate that phrase
level tagging can bring us more information about
the user, which is typical of many users.
Although most Twitter users express their inter-
ests to some extent in their messages, there are some
users whose message content is not rich enough to
extract reliable information. We investigated two
measures for identifying such users: standard devi-
ation of the top-10 TextRank values and the user?s
message text entropy. Table 2 shows a compari-
son of tagging precision where the users are divided
into two groups with a threshold on each of the two
measures. It is shown that users with larger Tex-
tRank value standard deviation or message text en-
tropy tend to have higher tagging precision, and the
message text entropy has better correlation with the
top-10 tagging precision than TextRank value stan-
dard deviation (0.33 v.s. 0.20 absolute).
5 Summary
In this paper, we designed a system to automat-
ically extract keywords from Twitter messages to
tag user interests and concerns. We evaluated two
tagging algorithms, finding that TextRank outper-
formed TFIDF ranking, but both gave a tagging pre-
cision that was comparable to that reported for web
page advertizing keyword extraction. We noticed
substantial variation in performance across users,
with low entropy indicative of users with fewer key-
words, and a need for extracting key phrases (in ad-
dition to words). Other follow-on work might con-
sider temporal characteristics of messages in terms
of the amount of data needed for reliable tags vs.
their time-varying nature, as well as sentiment asso-
ciated with the identified tags.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 30(1-7):107?117.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proc. EMNLP,
pages 216?223.
B. J. Jansen, M. Zhang, K. Sobel, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. Journal of the American Society for Informa-
tion Science and Technology, 60(11):2169?2188.
F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword
extraction for the meeting corpus using supervised ap-
proach and bigram expansion. In Proc. IEEE SLT,
pages 181?184.
F. Liu, D. Pennell, F. Liu, and Y. Liu. 2009. Unsuper-
vised approaches for automatic keyword extraction us-
ing meeting transcripts. In Proc. HLT/NAACL, pages
620?628.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proc. EMNLP.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. EMNLP, pages 63?77.
P. D. Turney. 2000. Learning algorithms for keyphrase.
Information Retrieval, 2(4):303?336.
P. D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proc. IJCAI, pages 434?439.
X. Wan, J. Yang, and J. Xiao. 2007. Towards an iter-
ative reinforcement approach for simultaneous docu-
ment summarization and keyword extraction. In Proc.
ACL, pages 552?559.
W.-T. Yih, J. Goodman, and V. R. Carvalho. 2006. Find-
ing advertising keywords on web pages. In Proc. 15th
International Conference on World Wide Web, pages
213?222.
692
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 717?720,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Phrase Patterns with Minimum Redundancy
for Unsupervised Speaker Role Classification
Bin Zhang, Brian Hutchinson, Wei Wu and Mari Ostendorf?
University of Washington, Seattle, WA 98125
Abstract
This paper addresses the problem of learning
phrase patterns for unsupervised speaker role
classification. Phrase patterns are automati-
cally extracted from large corpora, and redun-
dant patterns are removed via a graph prun-
ing algorithm. In experiments on English and
Mandarin talk shows, the use of phrase pat-
terns results in an increase of role classifi-
cation accuracy over n-gram lexical features,
and more compact phrase pattern lists are ob-
tained due to the redundancy removal.
1 Introduction
The identification of speaker roles is fundamental to
the analysis of social content and information re-
liability in conversational speech. Previous work
has primarily used supervised learning in automatic
role classification. Barzilay et al (2000) described a
speaker role classification system for English broad-
cast news (BN), where the speakers were catego-
rized into three types: anchor, journalist, and guest.
The authors used supervised learning to discover n-
gram signature phrases for speaker introduction and
structural features such as duration, achieving an ac-
curacy of 80% on ASR derived transcripts. Liu et al
(2006) studied speaker role classification on TDT-4
Mandarin BN data. Hidden Markov and maximum
entropy models were used to label the sequence of
speaker turns with the roles anchor, reporter, and
other, based on n-gram features, which yielded 80%
classification accuracy on human transcripts.
Hutchinson et al (2010) extend previous work to
the case of unsupervised learning, with the goal of
portability across languages. That work explored
? This research was funded by the Office of the Director of
National Intelligence (ODNI), Intelligence Advanced Research
Projects Activity (IARPA). All statements of fact, opinion or
conclusions contained herein are those of the authors and should
not be construed as representing the official views or policies of
IARPA, the ODNI or the U.S. Government.
speaker role classification using structural and n-
gram features on talk show (or broadcast conversa-
tion (BC)) data. In this paper, we address a limita-
tion of n-grams as features by proposing a method
for learning phrases with gaps, which is particularly
important for conversational speech, since there are
more disfluencies that can cause failure of n-gram
matching. In addition, we want to avoid topic words
(e.g., proper nouns) in order to model speaker roles
rather than topics. For example, for identifying the
host, the phrase pattern ?We?ll be back with * in a
minute? is more general than the n-grams ?We?ll be
back with John Smith in a minute.? To prevent these
problems with n-grams, one must limit the length of
learned n-grams, making them less discriminative.
Phrase patterns have been used in other NLP ap-
plications such as (Sun et al, 2007). To remove the
redundancies in the automatically extracted phrase
patterns, we propose a redundancy removal algo-
rithm based on graph pruning that does not require
role-labeled data. The resulting set of patterns is
then used to extract lists of signature and conversa-
tional phrases, from which features are derived that
are used to distinguish between the different roles.
Using the phrase pattern-based lexical features in
clustering, we obtain 82-89% speaker role classifi-
cation accuracy on human transcripts of BC shows.
2 Method
Phrase patterns are generalizations of n-grams. A
phrase pattern p of length n is an ordered list of
words (w1, w2, . . . , wn). It is matched by a word se-
quence of lengthm ? n if the sequence contains the
words in the order defined by the pattern. Because
the words in a phrase pattern need not appear con-
tiguously in the sequence, phrase matching is less
sensitive to disfluencies and topic words.
2.1 Phrase Pattern Extraction
Phrase patterns can be extracted by the sequen-
tial pattern mining algorithm PrefixSpan (Pei et al,
717
2001). This algorithm efficiently extracts frequent
phrase patterns in a corpus (i.e., relative frequency
greater than a given threshold). Prior to the extrac-
tion, we perform text preprocessing including split-
ting the text into lines based on commas and peri-
ods to limit the pattern span, followed by case and
punctuation removal. The extracted phrase patterns
have variable length. As a result, longer patterns
may contain shorter patterns. Phrase patterns with
the same length may also be overlapped. These re-
dundancies should be removed; otherwise, the same
phrase may match several patterns, resulting in bi-
ased counts.
2.2 Phrase Pattern Redundancy Removal
Define a phrase pattern p as contained in another
phrase pattern q if q contains all the words in p in the
same order. p is called a parent pattern and q is the
corresponding child pattern. Instead of constructing
a tree as in (Siu et al, 2000) for variable length n-
grams, we create a graph of phrase patterns based
on containment, because a pattern can contain and
be contained by multiple patterns. Our redundancy
removal algorithm involves pruning this graph. With
the nodes being the phrase patterns, the edges of the
phrase pattern graph are constructed by connecting
length-n phrase pattern p to length-(n + 1) phrase
pattern q for all n, if p is contained in q. We con-
nect only phrase patterns that differ by one word in
length for computational efficiency, and this results
in a multi-layer structure: the phrase patterns in each
layer have the same length. For the convenience of
pruning, a virtual node T is created as the ?zeroth?-
layer, and it is directly connected to all the nodes in
the layer with the shortest pattern length.
Once a phrase pattern graph has been created, we
prune the graph in order to remove the redundant
nodes. First, we remove edges based on the ratio of
counts c(q)/c(p) between child node q and parent
node p. A large ratio implies that the child appears in
most of the cases where the parent appears. Hence,
we keep the edge to indicate that the child can be
used as a preferred substitute for the parent. On the
other hand, the edge is removed if the ratio is small
(less than a threshold t, see Fig. 1).
After this procedure is performed on all the edges
in the graph, we determine whether a node is pruned
based on its connectivity to parents and children. We
A B
A B C A B D A B E
A B C D F A B D A B E D
X
Figure 1: A fragment of an example phrase pattern graph.
The letters represent words. The edge between ?AB? and
?ABD? is removed because the ratio of counts is less than
the threshold.
define two levels of pruning, which differ in whether
a node can be preserved even if its connections to
parents are removed:
Conservative pruning A node is pruned if it has at
least one child.
Aggressive pruning A node is pruned if it has at
least one child or is not on a path connected to
T .
Both methods were investigated, in case some useful
phrase patterns ended up being pruned with the more
aggressive approach.
2.3 Features Based on Phase Patterns
Although (Hutchinson et al, 2010) uses both lex-
ical and structural features, here we use only lexi-
cal features to better assess impact. Once the graph
pruning has provided a list of phrase patterns (elimi-
nating phrases of length one because of low reliabil-
ity), two subsets are extracted to represent signature
phrases as might be used by a host and conversa-
tional phrases as might occur more frequently in live
interviews. The signature statistic
?1 =
DF
SF
+ ? log(fBC). (1)
is based on the speaker frequency (SF , # speak-
ers whose utterances match p), document frequency
(DF , # shows that match p), and genre-dependent
frequency fBC (# occurrences of p in BC), all com-
puted on the training data. The ratio DFSF favors
phrases that occur in many documents but few
speakers, e.g. one per show, as for a host. The log
BC frequency term is a penalty to eliminate infre-
quent patterns. The conversation statistic
?2 =
fBC
fBN + 1
1SF>? . (2)
718
uses frequency fBN (# occurrences of p in BN), to
look for phrases that are more frequent in BC data
than BN, ideally live discussion phenomena. The in-
dicator function 1SF>? eliminates phrases used by
a small number of speakers to avoid topic-related
phrases. Hyper-parameters ? and ? are tuned by in-
specting the top phrase patterns after ranking. We
use ? = 10?3, ? = 500 for English and ? =
10?4, ? = 1000 for Mandarin. Phrase patterns are
ranked by the two statistics to generate lists of sig-
nature and conversational patterns, respectively.
During speaker-level feature extraction in role de-
tection, each phrase pattern in the lists is matched
against a speaker?s utterances. The lexical features
have two dimensions: the count of matches using the
signature and conversational patterns, each normal-
ized by the total number of patterns matched in the
show to account for differences between shows.
3 Experiments
3.1 Task and Data
In the absence of speaker-role-labeled conversa-
tional speech training data, we perform unsuper-
vised speaker role classification with three classes:
host, expert guest, and soundbite. We evaluate
on two human-labeled evaluation sets (English and
Mandarin). The English eval set contains nine BC
shows (150 speakers), while the Mandarin eval set
contains 14 shows (140 speakers). There is an addi-
tional labeled Mandarin development set composed
of ten shows (71 speakers). There are on average
7.6k words and 7.5k characters per show for English
and Mandarin, respectively. The phrase patterns are
learned from much larger corpora with speaker la-
bels but without speaker role labels, including web
transcripts for 310 English shows and quick rich
transcripts for 4395 Mandarin shows. Because of
the larger amount of Mandarin data, we use a lower
threshold (5 ? 10?5) for phrase pattern extraction
than for English (10?4).
3.2 Classification
Spectral clustering (Shi and Malik, 2000) is used
in this work, since we found it to outperform other
clustering approaches such as k-means and Gaus-
sian mixture models. Given a two-dimensional fea-
ture vector for each speaker in a show, we con-
struct a speaker graph with edge weights defined
by Gaussian similarity exp
(
??xi?xj?
2
2?2
)
. The spec-
tral clustering is non-deterministic, because it uses
k-means as its final step (k = 3), which is ini-
tialized by randomly choosing k samples as ini-
tial centroids. We vary ? as an integer from 1 to
100 in combination with different random initial-
izations to generate multiple clustering alternatives,
and then use a partition selection algorithm to pick
the most common clustering among the candidates.
We use domain knowledge to map speaker clusters
into speaker roles: the cluster whose members have
the largest average number of speaker turns is the
host cluster, that with the smallest average number
of turns is the soundbite cluster, and the remaining
cluster contains the expert guests.
3.3 Results
The phase pattern pruning threshold t was tuned on
the Mandarin dev set. We varied t from 0.1 to 0.9,
and measured the classification accuracy. t = 0.8
was found to be optimal (Fig. 2).
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.7
0.75
0.8
0.85
0.9
 
 
Conservative
Aggressive
No pruning
Figure 2: Accuracy on Man dev vs. pruning threshold t
The list of classification results on all the data sets
is shown in Tab. 1. Aggressive pruning yields the
best classification performance on all the data sets. It
is also better than using n-gram matching for feature
extraction (the last row of the table).
Man dev Man eval Eng eval
No pruning 0.83 0.86 0.81
Cons. pruning 0.86 0.83 0.81
Aggr. pruning 0.89 0.89 0.82
N-gram 0.86 0.86 0.77
Table 1: Classification results
The size of phrase pattern lists is given in Tab.
2, and the number of redundant phrase patterns (the
patterns that are contained in other patterns) is in
Tab. 3 for different pruning levels. Using aggres-
sive pruning, the list size and number of redun-
719
dant phrase patterns are greatly reduced. However,
the classification accuracy does not decrease. This
demonstrates that the redundant phrase patterns are
not helpful and can be harmful for this task.
Signature ptn. Conv. ptn.
Pruning level Eng Man Eng Man
No pruning 2000 2000 1000 1000
Cons. pruning 1605 946 928 998
Aggr. pruning 244 370 465 835
Table 2: Phrase pattern list size
Signature ptn. Conv. ptn.
Pruning level Eng Man Eng Man
No pruning 396 1331 337 142
Cons. pruning 35 307 334 142
Aggr. pruning 6 59 8 0
Table 3: Number of redundant phrase patterns in the list
The unsupervised speaker role classification sys-
tem in (Hutchinson et al, 2010) uses both n-gram
and structural features, giving classification accu-
racy on English and Mandarin eval sets of 0.86
and 0.84, respectively. Adding structural features
to phrase-pattern-based lexical features improves
the performance on English but not Mandarin, per-
haps because soundbites in English tend to be much
shorter than those in Mandarin.
3.4 Discussion
The experiments reflect differences between the two
languages. We observe that the main gain in Man-
darin comes from improved classification of hosts,
due to the signature phrase patterns. In English, the
improvement is attributed to improved classification
of expert guests and soundbites, suggesting an im-
proved conversational dimension of the lexical fea-
tures. The performance difference of the two lan-
guages seems more related to the languages them-
selves, rather than the size of data sets on which
phrase patterns are learned, because we were able to
obtain similar performance on Mandarin even when
the training set size is reduced.
Anecdotal inspection of the phrase patterns
learned for the signature phrases suggests that the
combination of redundancy pruning and the heuris-
tic signature statistic is quite effective. For exam-
ple, we observed English signature patterns such
as ?back with after this? and ?let?s take a look
at.? The former pattern can be matched by phrases
with names or topics inserted, and the latter can be
matched by ?let?s just take a look at? or ?let?s take
a brief look at.? In the Mandarin signature patterns,
we also found patterns such as ????????
???? * ? * ??? (today the guest invited to
the studio is Professor from) and ????? *??
?? (thanks to the report from). These patterns can
be considered to be templates for hosts, where the
named-entities are skipped.
4 Conclusions
We have presented a method for extracting phrase
patterns with minimum redundancy for speaker role
classification. The proposed algorithm removes
most of the redundancies in the phrase patterns,
leading to more compact pattern lists and improved
classification accuracy over n-gram lexical features.
We can apply the algorithm to other applications
such as text classification, where phrase patterns
can be used in place of n-grams. One way to ex-
tend this work is to use the automatically extracted
phrase patterns as initial features, and then employ
supervised or semi-supervised learning techniques
to learn a more discriminative feature set.
References
R. Barzilay et al 2000. The Rules Behind Roles: Iden-
tifying Speaker Role in Radio Broadcasts Proc. AAAI,
pp. 679?684.
Y. Liu. 2006. Initial Study on Automatic Identification of
Speaker Role in Broadcast News Speech. Proc. HLT,
pp. 81?84.
B. Hutchinson et al 2010. Unsupervised Broadcast Con-
versation Speaker Role Labeling Proc. ICASSP, pp.
5322?5325.
G. Sun et al 2007. Detecting Erroneous Sentences Using
Automatically Mined Sequential Patterns. Proc. ACL,
pp. 81?88.
J. Pei et al 2001. PrefixSpan: Mining Sequential Pat-
terns Efficiently by Prefix-projected Pattern Growth.
Proc. ICDE, pp. 215?224.
M. Siu and M. Ostendorf. 2000. Variable N-grams and
Extensions for Conversational Speech Language Mod-
eling. IEEE Transactions on Speech and Audio Pro-
cessing, 8(1):63?75.
J. Shi and J. Malik. 2000. Normalized Cuts and Image
Segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 22(8):888?905.
720
