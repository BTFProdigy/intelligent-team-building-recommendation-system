Effects of Adjective Orientation and Gradability 
on Sentence Subjectivity 
Vas i le ios  Hatz ivass i log lou  
Depar tment  o1' Computer  Sc ience  
Co lumbia  Un ivers i l y  
New York,  NY  10027 
vh@cs ,  co lumbia ,  edu  
Janyce  M.  Wiebe  
Depar tment  o f  Computer  Sc ience  
New Mex ico  State Un ivers i ty  
Las  Cruces ,  NM 88003 
w iebe@cs ,  nmsu.  edu  
Abstract 
Subjectivity is a pragmatic, sentence-level feature that 
has important implications for texl processing applica- 
lions such as information exlractiou and information ic- 
lricwd. We study tile elfeels of dymunic adjectives, se- 
mantically oriented adjectives, and gradable ad.ieclivcs 
on a simple subjectivity classiiicr, and establish lhat 
lhcy arc strong predictors of subjectivity. A novel train- 
able mclhod thai statistically combines two indicators of 
gradability is presented and ewlhlalcd, complementing 
exisling automatic Icchniques for assigning orientation 
labels. 
1 I n t roduct ion  
In recent years, computalional tcchniqt,es for the deter- 
mination of &:deal semantic features have been proposed 
and ewdualed. Such features include sense, register, do- 
main spccilicity, pragmatic restrictions on usage, scnlan- 
lic markcdncss, and orientation, as well as automatically 
ictcnlifiecl links between words (e.g., semantic rclalcd- 
hess, syllollynly, antonylny, and tneronymy). Aulomal- 
ically learning features of this type from hugc corpora 
allows the construction or augmentation of lexicons, and 
the assignment of scmanlic htbcls lo words and phrases 
in running text. This information in turn can bc used to 
help dcterlninc addilional features at the It?teal, clause, 
sentence, or document level. 
Tiffs paper explores lira benelits that some lexical fea- 
tures of adjectives offer l'or the prediction of a contexlual 
sentence-level feature, suOjectivity. Subjectivity in nat- 
ural language re\['crs to aspects of language used to ex- 
press opinions and ewfluations. The computatiomtl task 
addressed here is to distinguish sentences used to present 
opinions and other tbrms of subjectivity (suOjective sen- 
tences, e.g., "At several different layers, it's a fascinating 
title") from sentences used to objectively present factual 
information (objective sentences, e.g., "Bell industries 
Inc. increased its quarterly to 10 cents from 7 cents a 
share"). 
Much research in discourse processing has focused 
on task-oriented and insmmtional dialogs. The task ad- 
dressed here comes to the fore in other genres, especially 
news reporting and lnternet lorums, in which opinions 
of various agents are expressed and where subjectivity 
judgements couht help in recognizing inllammatory rues- 
sages ("llanles') and mining online sources for product 
reviews. ()thor (asks for whicll subjectivity recognition 
is potentially very useful include infornmtion extraction 
and information retrieval. Assigning sub.icctivity labels 
to documents or portions of documents is an example of 
non-topical characteri?ation f information. Current in- 
formation extraction and rolricval lechnology focuses al- 
most exclusively on lhe subject matter of the documcnls. 
Yet, additiomtl components of a document inllucncc its 
relevance to imrlicuhu ? users or tasks, including, for ex- 
alnple, the evidential slatus el: lhc material presented, and 
attitudes adopted in fawn" or against a lmrticular person, 
event, or posilion (e.g., articles on a presidenlial cam- 
paign wrillen to promote a specific candidate). In sum- 
marization, subjectivity judgmcnls could be included in 
documcllt proiilcs to augment aulomatically produced 
docunacnt summaries, and to hel l) the user make rele- 
vance judgments when using a search engine. 
()thor work on sub.iectivity (Wicbc et al, 1999; Bruce 
and Wicbc, 2000) has established a positive and statisti- 
cally signilicant correlation with the presence of adiec- 
lives. ?incc the mere presence of one or iDoi'c adjectives 
is useful for prcdicling (hat a scntcrtce is subjective we 
investigate ill this paper (lie cfl'ccts of additional cxical 
scmanlic lcalurcs of adjectives that can be automatically 
learned from corpora. We consider two such l%atures: se- 
mantic orientation, which represents an ewdualivc har- 
acterization of a word's deviation from the norm for its 
semantic group (e.g., beauti/'ul is positively oriented, as 
opposed to ugly); and gradability, which characterizes a 
word's ability to express a property in wlrying degrees. 
In lira remainder of this paper, we \[irst address adjec- 
tive orientation in Section 2, summarizing a previously 
published method for automatically separating oriented 
adjectives into positive and negative classes. Then, Sec- 
tion 3 presents a novel method for learning gradablc ad- 
jectives using a largo corpus and a statistical feature com- 
bination naodel. In Section 4, we review earlier exper- 
iments on testing subjectivity using wu'ious fcatt, res as 
predictors, and then present comparative analyses of the 
effects that orientation and gradability have on our abil- 
ity to In'edict sentence subjectivity from adjectives. Wc 
show that both give us higher-quality features for recog- 
nizing st@icctive sentences, and conclude by discussing 
future extensions to Ibis work. 
299 
Ct' 
Number of Number of Average nnmber Ratio o1' average 
adjectives in links in of links for Accuracy 
test set (IAc~l) test set (IL~I) each adjective group frequencies 
730 2,568 7.04 78.08% 1.8699 
516 2,159 8.37 82.56% 1.9235 
369 1,742 9.44 87.26% 1.3486 
236 1,238 10.49 92.37% 1.4040 
Table 1: Evaluation o1' the adjective orientation classification and labeling methods (from (Hatzivassiloglou and McK- 
eown, 1997)). 
2 Semantic Orientation 
The semantic orientation or polarity of a word indicates 
the direction the word deviates fl'om the norm for its se- 
mantic group or lexicalfield (Lehrer, 1974). It is an eval- 
uative characteristic (Battistella, 1990) of the meaning of 
the word which restricts its usage to appropriate prag- 
matic contexts. Words that encode a desirable state (e.g., 
beautiful, unbiased) have a positive orientation, while 
words that represent undesirable states have a negative 
orientation. Within tile particular syntactic lass o1' ad- 
jectives, orientation can be expressed as the ability of an 
adjective to ascribe in general a positive or negative qual- 
ity to the modified item, making it better or worse than a 
similar unmodilied item. 
Most antonymous adjectives can be contrasted on 
the basis of orientation (e.g., beautil)d-ugly); similarly, 
nearly synonymous terms are often distinguished by dill 
fcrent orientations (e.g., simple-siml)listic). While ori- 
entation applies to many adjectives, there are also those 
that have no orientation, typically as members of groups 
of complementary, qualitative terms (Lyons, 1977) (e.g., 
domestic, medical, or red). Since orientation is inher- 
ently connected with cwduative judgements, it appears 
to be a promising feature for predicting subjectivity. 
Hatzivassiloglou and McKeown (1997) presented a 
method for autonmtically assigning a + or - orientation 
label to adjectives known to have some semantic orien- 
tation. Their method is based on information extracted 
fi'om conjunctions between adjectives in a large corpus I  
because orientation constrains the use of the words in 
specific contexts (e.g., compare corrupt and brutal with 
*corrupt but brutal), observed conjunctions of adjectives 
can be exploited to inl'er whether the conjoined words 
are of the same or different orientation. Using a shallow 
parser on a 21 million word corpus of Wall Street Jour- 
nal articles, Hatzivassiloglou and McKeown developed 
and trained a log-linear statistical model that predicts 
whether any two adjectives have the same orientation 
with 82% accuracy. The predicted links o1' same or dil L 
ferent orientation are automatically assigned a strength 
value (essentially, a confidence stimate) by tile model, 
and induce a graph that can be partitioned with a clus- 
tering algorithm into components so that all words in the 
same component belong to the same orientation class. 
Once the classes have been determined, fl'equency infor- 
mation is used to assign positive or negative labels to 
each class (there are slightly fewer positive terms, but 
with a significantly higher rate of occurrence than nega- 
tive terms). 
Hatzivassiloglou and McKeown applied their method 
to 1,336 (657 positive and 679 negative) adjectives which 
were all the oriented adjectives appearing in the corpus 
20 times or more. Orientation labels were assigned to 
these adjectives by hand. I Subsequent validation of the 
initial selection and label assignment steps with indepen- 
dent human judges showed an agreement of 89% t'or tile 
first step and 97% for the second step, establishing that 
orientation is a fairly objective semantic property. Be- 
cause the accuracy ol' the method depends on the den- 
sity of conjunctions per adjective, Hatzivassiloglou and 
MeKeown tested separately their algorithm for adjectives 
appearing in at least 2, 3, 4, or 5 conjunctions in the co l  
pus; their results are shown in Table I. 
In this paper, we use the model labels assigned by 
hand by Hatzivassiloglou and McKeown, and tile labels 
automatically obtained by their method and reported in 
(Hatzivassiloglou and McKeown, 1997) with the follow- 
ing extension: An adjective that appears in k conjunc- 
tions will receive (possibly different) labels when ana- 
lyzed together with all adjectives appearing in at least 2, 
3 . . . . .  k conjunctions; since performance generally in- 
creases with the number of conjunctions per adjective, 
we select as the orientation label the one assigned by 
the experi,nent t,sing the highest applicable conjunctions 
threshold. Overall, we have labels for 730 adjectives 2,
with a prediction accuracy of 81.51%. 
3 Gradability 
Gradability (or grading) (Sapir, 1944; Lyons, 1977, p. 
27 I) is the semantic property that enables a word to par- 
ticipate in comparative constructs and to accept mod- 
ifying expressions that act as intensitiers or diminish- 
ers. Gradable adjectives express properties in varying 
degrees ot' strength, relative to a norm either explicitly 
ISome adjectives with unclem; mnbiguous, or conlexl,-dependenl 
orientation were excluded. 
2Those appearing in the corpus in two conjunctions or inore, since 
some conjunction data nlust be left out to h'ain the link prediction algo- 
rithm. 
300 
cold 
Unmodilied by 
grading words 
Moditied by 
grading words 
civil 
Unmodilied by 
grading words 
Modified by 
grading words 
Uninllected 392 20 1,296 1 
Inllected for degree 18 0 0 0 
'litble 2: Extracted wdues of gradability indicators, i.e., frequencies of the word with or without he specitied intlection 
or moditication, for two adjectives, one gradable (cold) and one primarily non-gradable (civil). The frequencies were 
compt, ted li'om the 1987 Wall Street Journal corpus. 
mentioned or implicitly supplied by the modilied noun 
(for example, asmall planet is usually much larger thart a 
large house; cf. the distinction between absolute and tel- 
alive adjectives made by Katz (1972, p. 254)). This rel- 
ativism in the interpetation of gradable words indicates 
that gradability is likely to be a good predictor ?71' subjec- 
tivity. 
3.1 Indicators ofgradability 
Most gradable words appear at least several times in a 
large corpt, s either in forms inflected for degree (i.e., 
comparative and superlative), or in tile context of grading 
modilicrs such as veo,. However, non-gradable words 
may also occasionally appear in such contexts or forms 
under exceptional circumstances. For example, ve O, 
dead can be used tk)r emphasis, and re&let am~ re&let 
(as in "her lhce became redder and redder") can be used 
to indicate a progression of coloring, qb distinguish be- 
tween truly gradablc adjectives and non-gradable adjec- 
tives in these exceptional contexts, we have developed 
a trainable log-linear statistical model that lakes into ac- 
count tile number of times an ad.iective has been observed 
in a form or context indicating gradability relative to the 
number of limes it has been seen in non-gradable con- 
texts. 
We use a shallow parser to retrieve from a large corpus 
tagged for part-of-speech with Church's PARTS tagger 
(Church, 1988) all adjectives and their modifiers. Al- 
though the most common use of an adverb modifying 
an adjective is to function as an intensilier or diminisher 
(Quirk et al, 1985, p. 445), adverbs can also add to tile 
semantic ontent of the adjectival phrase instead of pro- 
viding a grading effect (e.g., immediately available, po- 
litically vuhmrable), or function as cmphasizers, adding 
to the force o1' tile base adjective and not lo its degree 
(e.g., virtually impossible; compare *re O, impossible). 
Therefore, we compiled by hand a list of 73 adverbs and 
noun phrases (such as a little, exceedingly, somewhat, 
and veo') that are fi'equently used as grading moditicrs. 
The number of times each adjective appears mod ilied by 
a term form this list becomes a first indicator of gradabil- 
ity. 
To detect inflected forms o1' adjectives (which, in 15> 
glish, always indicate gradability st, bject to the excel> 
tions discussed earlier), we have implemented an auto- 
matic lnorphology analysis component. This program 
recognizes several irregular forms (e.g., good-better- 
best) and strips tile grading suffixes -er and -est Dora 
regularly inllected adjectives, producing a list of candi- 
date base forms that if inflected would yield tilt origi- 
nal adjective (e.g., bigger produces three potential forms, 
big, bigg, and bigge). The frequency of these candi- 
date base words is checked against ile corpus, and tile 
form with signilicantly higher frequency is selected. To 
guard against cases of base adjective forms that end in -er 
or-est (e.g., sih,er), the original word is also included 
alllong tile candidates. The total number of times this 
procedure is successfully applied for each adjective be- 
comes a second indicator of gradability. 
3.2 l )etermlnlng radabil l ty 
The presence or absence of each of the above two indica- 
tors results in a 2 x 2 frequency table IBr each adjective; 
examples for one gradable and one non-gradable adjec- 
tive are given in "lhble 2. "lb convert lhese four numbers 
to a single decision on tile gradability of tile ad.iective, we 
use a log-linear model. Ix)g-linear models (Nantnef and 
l)ufly, 1989) construct a linear combination (weighted 
sum) of the predictor wlriables 1~, 
i=1 
and relate it to the actual response H. (in this case, 0 for 
non-gmdable and 1 for gradable) via the so-called logis- 
tic trcm,sJbrmation, 
1~- 
I -t- e'J 
Maximum likelihood estimates for the coefficients fli 
are obtained from training samples for which the correct 
response H, is known, using the iterative reweighted non- 
linear least squares algorithm (Bates and Watts, 1988). 
This statistical model is particularly suited for model- 
ing variables with a "yes"-"no" (binary) value, because, 
unlike linear models, it captures the dependency of IFs 
variance on its mean (Santner and Dully, 1989). 
We normalize the counts for the two indicators of 
g,'adability, and the cot, at ot'joint occurrences of both in- 
tleetion and modilication by grading moditiers, by divid- 
ing with the total frequency of the adjective in the corpus. 
In this manner, we obtain three real-valued predictors 
301 
Classitied as gradable: 
acceptable accurate afraid aware busy careful 
cautious el~eap creative critical dangerous 
different disappointing equal fair fanfiliar far  
favorable formal free frequent good grand 
inadequate intense interesting legitimate likely 
positive professional reasonable rich 
short-term significant slow solid sophisticated 
sound speculative thin tight tough uucertain 
widespread worth 
Classilied as non-gradable: 
additional alleged alternative annual antitrust 
automatic ertain criminal cumulative daily 
deputy domestic ldcrly false linaneial 
first-quarter full hefty illegal institutional 
internal egislative long-distance military 
min imum monthly moral national official 
one-time other outstanding present prior 
prospective punitive regional scientific 
secondary sexual subsidiary taxable 
three-nmnth three-year total tremendous 
two-year unfifir unsolicited upper vohmtary 
white wholesale world-wide wrong 
Figure 1: Automatically obtained classification of a 
sample of 100 adjectives as gradable or not. Correct 
decisions (according to the COBU1LD-based reference 
model) are indicated in bold. 
14", i = 1 , . . . ,  3 for the log-linear model. We also con- 
sider a modilied model, where any adjective for which 
any occurrence of simultaneous inflection and modilica- 
tion has been detected is automatically labeled gradable; 
the remaining two predictors are used to classify the ad- 
jectives that do not fullill this condition. This modilica- 
tion is motivated by the fact that observing an adjective 
in such a context offers a very high likelihood o1' grad- 
ability. 
3.3 Experimental results 
We extracted from the 1987 Wall Street Journal corpus 
(21 million words) all adjectives with a frequency o1' 300 
or more; this produced a collection of 496 words. Grad- 
ability labels specifying whether each word is gradable 
or not were manually assigned, using tim designations 
of the Collins COBUILD (Collins Birmingham Univer- 
sity International Language Database) dictionary (Sin- 
clair, 1987). COBUILD marks each sense of each adjec- 
tive with one of the labels QUALIT, CLASSIF, or COLOR, 
corresponding to gradable, non-gradable, and color ad- 
jectives. In cases where COBUILD supplies conflicting 
labels for different senses of a word, we either omitted 
that word or, if a sense were predominant, gave it the 
label of that sense. In some cases, the word did not 
appear in COBUILD; these typically were descriptive 
compounds peci\[ic to the domain (e.g., anti-takeover, 
over-the-coullter) and were in most cases marked as non- 
gradable adjectives. Overall, 453 of tile 496 adjeclives 
(91.33%) were assigned gradability labels by hand, while 
the remaining 53 words were discarded because they 
were misclassitied as adjectives by the part-ol:speech 
tagger (e.g., such) or because they coukt not be assigned 
a unique gradability label in accordance with COBUILD. 
Out of these words, 235 (51.88%) were manually classi- 
lied as gradable adjectives, and 218 (48.12%) were clas- 
silied as non-gradablc adjectives. 
Following the methodology of the preceding subsec- 
tion, we recovered the inflection and modilication indica- 
tors for these 453 adjectives, and trained both the unmod- 
ified and modilied log-linear models rcpcatedly, using a 
randomly selected subset ol' 300 adjectives for training 
and 100 adjectives for testing. The entire cycle of se- 
lecting random test and training sets, fitting the model's 
coefficients, making predictions, and evaluating the pre- 
dicted gradability labels is repeated 100 times, to ensure 
that the ewtluation is not affected by a lucky (or unlucky) 
partition of the data between training and test sets. This 
procedure yields over the 453 adjectives gradability clas- 
sifications with an average precision o1' 93.55% and av- 
erage recall o1' 82.24% (in terms of the gradable words 
reported or recovered, respectively). The overall accu- 
racy of the predicted gradability labels is 87.97%. These 
results were obtained with the modified log-linear model, 
which slightly ot, tperformed the model that uses all three 
predictors (in that case, we obtained an average precision 
of 93.86%, average recall ol' 81.70%, and average over- 
all accuracy o1' 87.70%). Figure I lists the gradability 
labels that were automatically assigned to one of the 100 
random test sets ttsing the moditied prediction algorithm. 
We also assigned automatically labels to the entire set of 
453 adjectives, using 4-fold cross-validation (repeatedly 
training on three-fourths of tim 453 adjectives and test- 
ing on the rest). This resulted in precision of 94.15%, 
recall of 82.13%, and accuracy of 88.08% for the entire 
adjective set. 
4 Subjectivity 
The main motivation for the present paper is to examine 
the effect that information about an adjective's semantic 
orientation and gradability has on its probability of oc- 
curring in a subjective sentence (and hence on its quality 
as a subjectivity predictor). We tirst review related work 
on subjectivity recognition and then present our results. 
4.1 Previous work on subjectivity recognition 
In work by Wiebc, Bruce, and O'Hara (Wiebe ct al., 
1999; Bruce and Wicbe, 2000), a corpus of 1,001 sen- 
tences 3 of the Wall Street Journal TreeBank Corpus 
3Conlpoutld sentences were manually segmented into their con- 
juncts, and each conjtmct treated as a scparale sentence. 
302 
(Marcus et al, 1993) was nlanually annotated with sub- 
jeciivity chlssifications. Specifically, each sentence was 
assigned a subjective or objective classitication, accord- 
ing to concensus lags derived by a slalistical analysis of 
lhe chisses assigned by three human judges (see (Wiebe 
et al, 1999) for further infornmtion). The total nulnber 
of subjective sentences in lhe data is 486, and the total 
number of objeclive sentences i 515. 
Bruce and Wiebe (2000) performed a statistical anal- 
ysis of the assigned classitications, linding lhat ac(iec- 
tivcs are statistically signilicantly and positively corre- 
lated with subjective sentences in the corpus on the basis 
(, . The proba- of the log-likelihood ratio test statistic -,2 
bility of a sentence being subjective, simply given din! 
there is at least one adjective in lhe sentellee, is 56%, 
even though there are more objective than subjective sen- 
lences in the corpus. In addition, Bruce and Wicbe iden- 
tiffed a type of adjective that is indicative of subjective 
sentences: those Quirk et al (1985) term dynamic, which 
"denote qualities that a,'e thoughl to be subjecl to con- 
trol by the possessor" (p. 434). IZxamples are "kind" and 
"careful". Bruce and Wiebe nianually applied synlactic 
tests to identify dynamic adjectives in hall' of the corpus 
nlentioned above. We inclutle such adjectives in the anal- 
ysis below, to assess whether additional lexical seinantic 
features associated with subjectivity hel I ) improve pro- 
dictability. 
Wiebe el al. (1999) developed an automatic system to 
perform st, bjectivily lagging. In 10-fold cross valida- 
lion experiments applied to the corpus described above, 
a probabilislic lassilier oblaincd an average accuracy on 
subjectivity lagging of 72.17%, nlorc Ihan 20 perccnlage 
poinls higher than the baseline accuracy obtained by al- 
ways choosing tile nlore frcquent class. A binary feature 
is included for each of lhe lbllowing: lhe presence in lhe 
sentence of a pl'ollotln, an adjective, a cardinal number, 
a modal other fllan will, and an adverb other than #lot. 
They also inchlded a binary feature representing whether 
or not the sentence begins a new lxu'agraph, l:inally, a 
feature was included representing co-occurrence of word 
tokens and punciuation marks with tile sul~jective and ob- 
jective classilicfition. An analysis of the system showed 
that the adjective \['cature was imporlant to realizing the 
inlprovolncnts over lllO baseline accuracy, in this \])apci', 
we use lhe performance of the simple adjcclive fealtu'e as 
a baseline, and identify higher quality adjeclive features 
based on gradability and orienlalion. 
4.2 Or ientat ion and gradabi l i ty  as subjectivity 
predictors: Results 
We measure the precision of a simple prediction method 
for subjectivity: a sonlence is classilicd as subjcclivc il' at 
least one nlonlbor of a set of adjectives N occurs in 1he 
sontonco, alld objeclive otherwise. By wirying 1tlo sot 
(e.g., all adjeclives, only gradable adjectives, only nega- 
tively orienied adjectives, etc.) we call assess the t, seful- 
heSS of ihe additional knowledge for predicting subjec- 
livity. 
For the present study, we use tile set of all adjectives 
automatically identified in tile corpt, s by Wiebc et al 
(1999) (Section 4.1 ); the set of dynamic adjectives Ill,{Inu- 
ally identified by Bruce and Wiebe (2000) (Section 4.1); 
tile set of scnmntic orientation labels assigned by Hatzi- 
vassiloglou and McKeown (1997), both manually and 
automatically with our extension described in Section 2; 
and the set of gradability labels, both manually and att- 
tomatically assigned according to the revised log-linear 
model of Section 3. We calculate restllts (shown in 'hi- 
ble 3) for each of lhese sets of all adjectives, dynamic, 
oriented and gradable adjectives, as well as for unions 
and intersections of lhose sets. Nole fliat these four sets 
have been extracted l'rom comparable but different cor- 
pora (different years of the Wall Street Journal), therefore 
sometimes adjectives in one corpus may not be present 
in the other corpus, reducing the size of intersection sets. 
Also, for gradability, we worked with a sample set of 100 
adjectives rather than all possible adjectives we could 
automatically calcuhtte gladabiliiy vahles for, since our 
goal in the present work is to measure correlations be- 
tween these sets and sul~jeciivity, rather than building a 
system for predicling subjectivity for as many ac\[iectives 
as possible. 
In Table 3, the second cohmm identifies 8, the set 
of ac\[iective types in question. The third cohimn gives 
the number of subjective sentences that contain one or 
more instances of members of S, and the fourth colunul 
gives lhe same ligure for ol~jective sentences. Therefore 
these two cohinuls together specify lhe coverage of tlm 
subjectivity indicator examined. The lifth cohimn gives 
111c onditional probability that a sentence is subjective. 
givell that (tile of iilorc illstatices of ti/enlbcl+S of +5; ap- 
pears. This is a precishm inetrie that assesses feature 
quality: if inslances of <"7 appear, how likely is the son- 
tence to be subjective? The last two colunuls contrast the 
observed conditional probability with the a priori prob- 
ability of subjective sentellees (i.e., chalice; sixth col- 
ulnn) and with the probability assigned by the baseline 
all-adjectives model (i.e., the lirst row in the table; sev- 
enth colunm). 
The nlost striking aspect of these results is lhat all sets 
involviug dynamic adiectives positive or negative po- 
larity, or gradability are better predictors of sul~jective 
sentenccs than the class of adjectives as a whole, lqve 
of the sets are at least 25 points better (LI4, LI6, L21, 
L23, and L24); four others are at least 20 points better 
(L2, L9, L13, and 1,15); and live others are at least 15 
points better (L4, LI I, 1,18, L20, and 1,22). In most of 
these cases, the difference between these predictors and 
all adjectives i  statistically signiticant 4 fit the 5% level or 
less; ahnost all of these predictors offer statistically sig- 
nificantly better than even odds in predicting subjectivity 
correctly. In nlany cases where statistical signilicance 
'Iwe applied achi-square l st Oll the 2 x 2 cross-classificalion able 
(Fleiss, 1981). 
303 
LI. 
L2. 
L3. 
L4. 
L5. 
L6. 
L7. 
L8. 
L9. 
L10. 
L l l .  
Ll2. 
LI3. 
LI4. 
LI5. 
L16. 
LI7. 
LI8. 
LI9. 
L20. 
L21. 
L22. 
L23. 
L24. 
Adjeclive Set S 
# Subj Sents 
with (s G ,5') + 
Dyn Adjs fq S of L5. 
# Obj Sents l'(Subj Sent I Significance 
with (s G ,5') + (~ e S) +) Against maiority Against all adjs 
All Adjectives 403 321 0.56 0.0041 N/A 
Dynamic Adjectives 92 32 0.74 1.1989 ? 1.0 - r  1..6369 - 10 -4 
Pol+, man 138 87 0.61 0.0007 0.1546 
Pol- ,  man 79 37 0.67 0.0001 0.0158 
Pol+ U Pol- ,  man 197 114 0.63 6.91.91 ? 10 -~ 0.0260 
Grad, man 193 115 0.63 1.9633 ? 10 -~ 0.0440 
Not Grad, man 172 147 0.54 0.1084 0.6496 
t'o1+, auto 121 79 0.60 0.0026 0.2537 
Pol- ,  auto 61 21 0.74 1.1635 ? 10 -~ 0.0017 
PoI+ U I'o1--, auto 170 95 0.64 8.5888 - 10 -~ 0.0202 
Grad, auto 30 14 0.68 0.0166 0.1418 
Not Grad, auto 63 51 0.55 0.2079 0.9363 
51 19 0.73 0.0001 0.0081 
8.0397.10 -~ Dyn Adjs 71 S of L6. 39 8 0.83 
l)yn Adjs 71 S of L I0. 50 19 0.72 0.0002 0.0103 
Dyn Adjs 71 S ofLl  I 7 2 0.78 0.1582 0.3220 
Grad 71 Pol+, man 90 58 0.61 0.0070 0.2891 
Grad 71 Pol-,  man 35 I6 0.69 0.0080 0.09711 
Grad 71 (Pol+ U Pol-), man 119 71 0.63 0.0005 0.1000 
Grad fl Pol+, auto 13 6 0.68 0.1376 0.3833 
Grad n Pol-,  auto 2 0 1.00 0.4556 0.5838 
Grad 71 (Pol+ U Pol-), auto 15 6 0.71 0.0636 0.2255 
l)yn Adjs N S o1' L22. 4 0 1.00 0.1203 0.2019 
I)yn Adjs ('1 ,_"; of L19. 24 5 0.83 0.0006 0.0070 
Key: (s G ,5')+: one or more instances of members ofS. Ib/+: positive polarity, l b l - :  negalive polarity. 
GtzM: gradable. Dyn: dynamic. Matt: manually identilied. Auto: automalically identified. 
Table 3: Subjectivity prediction results. 
4.3671.. 10 -4 
could not established this is due to small counts, caused 
by the small size of the set of adjectives automatically 
labeled for gradability. 
It is also important to note that, in most cases, 
tile automatically-classified adjectives are comparable 
or better predictors of subjective sentences than the 
manually-assigned ones. Comparing tile automatically 
generated classes with the manually identilied ones, the 
positive polarity set decreases by 1 percentage point (L3 
and L8), while the negative polarity set increases by 7 
points (L4 and L9), and the gradable sot increases by 5 
percentage points (L6 and LI 1). Among the intersection 
sets, in two cases the results are lower for tile computer- 
generated sets (Ll 3/LI 5 and L 14/L 16), but in tile other 4 
eases, the results are higher (LI 7/L20, L 18/L21, L19/L2, 
L24/L23). 
Finally, the table shows that, in most cases, pro- 
dictability improves or at worst remains essentially tile 
same as additional lexical features are considered. For 
tile set of dynamic adjectives, the predictability is 74% 
(L2), and improves in 4 of the 6 cases in which it is in- 
tersected with other sets (LI4, L l6, L23, and L24). For 
the other two (L 13 and LI 5), predictability is only 1 or 2 
points lower (not statistically significant). For the man- 
ually assigned polarity and gradability sets, in one case 
predictability is lower (L17 < L6), but in the other cases 
it remains the same or improves. The results are even 
better for the automatically assigned polarity and grad- 
ability sets: predictability improves when both features 
are considered in all but one case, when predictability 
remains the same (L20 > L8; L21 > L9; L22 > LI0; 
and LI 1 _< L20, L21, and L22). 
5 Conclusion and Future Work 
This paper presents an analysis of different adjective fea- 
tures for predicting subjectivity, showing that tlmy are 
more precise than those previously used for this task. Wc 
establish that lexical semantic features uch as seman- 
tic orientation and gradability determine in large part the 
subjectivity status of sentences in which they appear. We 
also present an automatic meflmd for extracting radabil- 
ity values reliably, complementing earlier work on se- 
mantic orientation and dynamic adjectives. 
In addition to finding more precise features for auto- 
marie subjectivity recognition, this kind of analysis could 
help efforts to encode subjective features in ontologies 
such as those described in (Knight and Luk, 1994; Ma- 
hesh and Nirenburg, 1995; Hovy, 1998). These on- 
tologies are useful for many NLP tasks, such as ma- 
chine translation, word-sense disambiguation, and gen- 
eration. Some subjective features are included in exist- 
ing ontologies (for example, Mikrokosmos (Mahesh and 
304 
Nirenburg, 1995) includes atlitude slots). Our corpus- 
based methods could help in idenlifying more or exlend- 
ing their coverage. 
To be able to use automatic subjectivily recognition 
in texl-processing applications, good ch,cs o1' sub.iccliv- 
ity mttst be found. The features developed in lhis paper 
are not only good clues of subjectivity, lhey can be Men- 
tilied automatically from corpora (see (Hatzivassiloglou 
and McKeown, 1997), and Section 3 in the present pa- 
per). In fact, the results in "Iable 3 show that the pre- 
dictability of the automatically determined gradability 
and polarity sets is better than or at least comparable to
the predictability of the manually determined sets. Thus, 
tile oriented and gradable adjectives in the particular ap- 
plication genre can be idenlified fo," use in subjectivity 
recognition. 
Ou, efforts in this paper are largely exploratory, aim- 
ing to establish correlations among tim wlrious features 
examined. In related work, we have begun to incorporale 
the features developed herc into systems for recognizing 
flames and mining reviews in lnternel forums, extend- 
ing subjectivity judgments froth the sentence to the doc- 
ument level. In addition, we are seeking ways lo extend 
the orientation and gradability methods o that individual 
word occurrences, rather than word lypes, are character- 
ized as oriented or gradable. We also pla n l{7 incorpo- 
rate the new features presented here in machine learning 
models for tile prediction of subjectivity (e.g., (Wiebe ct 
al., 1999)) and lest lheir interaclions wilh olhcr proposed 
features. 
Acknowledglnents 
This research was SUl~ported in part by the National Sci- 
ence Foundation under grant number IIS-9817434, and 
by |he Of lice of Nawtl Research under grant number 
N00014-95-1-0776. Any opinions, tindings, or recom- 
mendations a,e those of tile authors, and do not neces- 
sarily rellect the views of the above agencies. 
References 
I)ouglas M. Bates and 1)onald G. Watts. 1988. NoMi~> 
ear Regression Analysis and its Applicatiolls. Wiley, 
New York. 
Edwin L. Battistella. 1990. Markedness: 7he Evahiative 
Siq~etwtructure qf'Language. State University of New 
York Press, Albany, New York. 
Rebecca Bruce and ,lanyce Wiebe. 2000. Recognizing 
subjectivity: A case study of rllanual tagging. Natural 
Language E, gineering, 6(2). 
Kenneth W. Church. 1988. A stochastic paris p,'ogranl 
and noun phrase parser for unrestricted text. In Pro- 
ceedings of the Second Co,ference o, Applied Natu- 
ral Language Processing (ANLP-88), pages 136-143, 
Austin, Texas, February. Association for Computa- 
tional Linguistics. 
Joseph 1~. Fleiss. 1981. Statistical Methods for Rates 
and l'mportions. Wiley, New York, 2rid edition. 
Vasileios Hatzivassiloglou and Kathlcen R. McKeown. 
1997. Predicting tile semantic orientation of adjec- 
tives. In Pmeeedi,gs o\[' the 35th Annual Meeting 
q/' the ACL and the 8th Col!/'erence o/' rite Europeall 
Ch(q)ter of the ACL, pages 174-181, Madrid, Spain, 
July. Association re," Computational Linguistics. 
Eduard Hovy. 1998. Combining and slandardizing 
large-scale practical ontologies for machine lransla- 
lion and other uses. In Proceedings of the 1st Interna- 
tional Conference on Language Resources and Evaht- 
alien (LREC), Granada, Spain. 
Jerrold J. Kalz. 1972. Sema,tic Theory. Harper and 
Row, New York. 
Kevin Knight and Steve K. Luk. 1994. Building a large- 
scale knowledge base liw machine h'anslation. Ill Pro- 
ceedi,gs o\[" the 12th Natio,al Co,ference o, Artifi- 
cial l,telli,q,e, ce (AAAI-94), w)lume 1, pages 773-778, 
Sealtlc, Washinglon, July-Augt, st. American Associ- 
ation for Artificial Intelligence. 
Adrienne Lehrer. 1974. Sema, tic l,}'elds and Lexical 
Structztre. North Holland, Amster&tm and New York. 
John I,yons. 1977. Sema,tics, volume 1. Cambridge 
University Press, Cambridge, England. 
K. Mahesh and S. Nirenburg. 1995. A siluated ontol- 
ogy for practical NLP. In Pmceedi,gs of the Work- 
shol~ oil Basic Ontological Issues in Knowledge Shar- 
ing, 14th lntenmtio,al.loi, t Co,ference oil Artificial 
Intelligence (LICAI-95), MontrEal, Canada, Augusl. 
Milchell E /Vlarcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large aunotaled cor- 
pus of Fmglish: the Penn Treebank. Coml;tttatioltal 
Lin,~?uistics, 19(2):313-330, June. 
I~tandolph Quirk, Sidney Grecnbaum, Geoffrey l,eech, 
alld Jall Svartvik. 1985. A Complvhe,sive Grammar 
el'the English l.cmguage. Longman, London and New 
York. 
Thomas .l. Sanlner and Diane E. l)uffy. 1989. The Statis- 
tical Analysis of Discrete Data. Springer-Verlag, New 
York. 
tTAward Sapir. 1944. ()n grading: A study ill semantics. 
l~hilosol;hy qfScie,ce, 2:93-116. Reprinted in (Sapir, 
1949). 
Edwa,d Sapir. 1949. Selected Wiqtings i, Language, 
Culture and Personality. University of California 
Press, Be,'keley, California. Edited by David G. Mat> 
delbat, m. 
John M. Sinclair (editor in chiet). 1987. Collins 
COBU1LD English Language Dictionary. Collins, 
London. 
J. Wiebe, R. Bruce, and T. O'Hara. 1999. Develop- 
ment and use of a gold standard ata set for subjec- 
tivity classilieations. In Proceedings of tile 37th An- 
total Meeting of the Association for Computational 
Li,guistics (ACL-99), pages 246-253, Universily of 
Maryhmd, June. 
305 
A Formal Model for Information Selection in Multi-Sentence Text
Extraction
Elena Filatova
Department of Computer Science
Columbia University
New York, NY 10027, USA
filatova@cs.columbia.edu
Vasileios Hatzivassiloglou
Center for Computational Learning Systems
Columbia University
New York, NY 10027, USA
vh@cs.columbia.edu
Abstract
Selecting important information while account-
ing for repetitions is a hard task for both sum-
marization and question answering. We pro-
pose a formal model that represents a collec-
tion of documents in a two-dimensional space
of textual and conceptual units with an asso-
ciated mapping between these two dimensions.
This representation is then used to describe the
task of selecting textual units for a summary or
answer as a formal optimization task. We pro-
vide approximation algorithms and empirically
validate the performance of the proposed model
when used with two very different sets of fea-
tures, words and atomic events.
1 Introduction
Many natural language processing tasks involve the
collection and assembling of pieces of informa-
tion from multiple sources, such as different doc-
uments or different parts of a document. Text sum-
marization clearly entails selecting the most salient
information (whether generically or for a specific
task) and putting it together in a coherent sum-
mary. Question answering research has recently
started examining the production of multi-sentence
answers, where multiple pieces of information are
included in the final output.
When the answer or summary consists of mul-
tiple separately extracted (or constructed) phrases,
sentences, or paragraphs, additional factors influ-
ence the selection process. Obviously, each of the
selected text snippets should individually be impor-
tant. However, when many of the competing pas-
sages are included in the final output, the issue of
information overlap between the parts of the output
comes up, and a mechanism for addressing redun-
dancy is needed. Current approaches in both sum-
marization and long answer generation are primar-
ily oriented towards making good decisions for each
potential part of the output, rather than examining
whether these parts overlap. Most current methods
adopt a statistical framework, without full semantic
analysis of the selected content passages; this makes
the comparison of content across multiple selected
text passages hard, and necessarily approximated by
the textual similarity of those passages.
Thus, most current summarization or long-
answer question-answering systems employ two
levels of analysis: a content level, where every tex-
tual unit is scored according to the concepts or fea-
tures it covers, and a textual level, when, before
being added to the final output, the textual units
deemed to be important are compared to each other
and only those that are not too similar to other can-
didates are included in the final answer or summary.
This comparison can be performed purely on the ba-
sis of text similarity, or on the basis of shared fea-
tures that may be the same as the features used to
select the candidate text units in the first place.
In this paper, we propose a formal model for in-
tegrating these two tasks, simultaneously perform-
ing the selection of important text passages and the
minimization of information overlap between them.
We formalize the problem by positing a textual unit
space, from which all potential parts of the summary
or answer are drawn, a conceptual unit space, which
represents the distinct conceptual pieces of informa-
tion that should be maximally included in the final
output, and a mapping between conceptual and tex-
tual units. All three components of the model are
application- and task-dependent, allowing for dif-
ferent applications to operate on text pieces of dif-
ferent granularity and aim to cover different concep-
tual features, as appropriate for the task at hand. We
cast the problem of selecting the best textual units
as an optimization problem over a general scoring
function that measures the total coverage of concep-
tual units by any given set of textual units, and pro-
vide general algorithms for obtaining a solution.
By integrating redundancy checking into the se-
lection of the textual units we provide a unified
framework for addressing content overlap that does
not require external measures of similarity between
textual units. We also account for the partial overlap
of information between textual units (e.g., a single
shared clause), a situation which is common in nat-
ural language but not handled by current methods
for reducing redundancy.
2 Formal Model for Information Selection
and Packing
Our model for selecting and packing information
across multiple text units relies on three compo-
nents that are specified by each application. First,
we assume that there is a finite set T of textual units
t1, t2, . . . , tn, a subset of which will form the an-
swer or summary. For most approaches to sum-
marization and question answering, which follow
the extraction paradigm, the textual units ti will
be obtained by segmenting the input text(s) at an
application-specified granularity level, so each ti
would typically be a sentence or paragraph.
Second, we posit the existence of a finite set C
of conceptual units c1, c2, . . . , cm. The conceptual
units encode the information that should be present
in the output, and they can be defined in different
ways according to the task at hand and the prior-
ities of each system. Obviously, defining the ap-
propriate conceptual units is a core problem, akin
to feature selection in machine learning: There is
no exact definition of what an important concept is
that would apply to all tasks. Current summariza-
tion systems often represent concepts indirectly via
textual features that give high scores to the textual
units that contain important information and should
be used in the summary and low scores to those tex-
tual units which are not likely to contain informa-
tion worth to be included in the final output. Thus,
many summarization approaches use as conceptual
units lexical features like tf*idf weighing of words
in the input text(s), words used in the titles and sec-
tion headings of the source documents (Luhn, 1959;
H.P.Edmundson, 1968), or certain cue phrases like
significant, important and in conclusion (Kupiec et
al., 1995; Teufel and Moens, 1997). Conceptual
units can also be defined out of more basic concep-
tual units, based on the co-occurrence of important
concepts (Barzilay and Elhadad, 1997) or syntac-
tic constraints between representations of concepts
(Hatzivassiloglou et al, 2001). Conceptual units do
not have to be directly observable as text snippets;
they can represent abstract properties that particular
text units may or may not satisfy, for example, status
as a first sentence in a paragraph or generally posi-
tion in the source text (Lin and Hovy, 1997). Some
summarization systems assume that the importance
of a sentence is derivable from a rhetorical repre-
sentation of the source text (Marcu, 1997), while
others leverage information from multiple texts to
re-score the importance of conceptual units across
all the sources (Hatzivassiloglou et al, 2001).
No matter how these important concepts are de-
fined, different systems use text-observable features
that either correspond to the concepts of interest
(e.g., words and their frequencies) or point out those
text units that potentially contain important con-
cepts (e.g., position or discourse properties of the
text unit in the source document). The former class
of features can be directly converted to concep-
tual units in our representation, while the latter can
be accounted for by postulating abstract conceptual
units associated with a particular status (e.g., first
sentence) for a particular textual unit. We assume
that each conceptual unit has an associated impor-
tance weight wi that indicates how important unit ci
is to the overall summary or answer.
2.1 A first model: Full correspondence
Having formally defined the sets T and C of tex-
tual and conceptual units, the part that remains in
order to have the complete picture of the constraints
given by the data and summarization approach is the
mapping between textual units and conceptual units.
This mapping, a function f : T?C ? [0, 1], tells us
how well each conceptual unit is covered by a given
textual unit. Presumably, different approaches will
assign different coverage scores for even the same
sentences and conceptual units, and the consistency
and quality of these scores would be one way to de-
termine the success of each competing approach.
We first examine the case where the function f is
limited to zero or one values, i.e., each textual unit
either contains/matches a given conceptual feature
or not. This is the case with many simple features,
such as words and sentence position. Then, we de-
fine the total information covered by any given sub-
set S of T (a proposed summary or answer) as
I(S) =
?
i=1,...,m
wi ? ?i (1)
where wi is the weight of the concept ci and
?i =
{ 1, if ?j ? {1, . . . ,m} such that f(tj , ci) = 1
0, otherwise
In other words, the information contained in a
summary is the sum of the weights of the concep-
tual units covered by at least one of the textual units
included in the summary.
2.2 Partial correspondence between textual
and conceptual units
Depending on the nature of the conceptual units, the
assumption of a 0-1 mapping between textual and
conceptual units may or may not be practical or even
feasible. For many relatively simple representations
of concepts, this restriction poses no difficulties: the
concept is uniquely identified and can be recognized
as present or absent in a text passage. However, it is
possible that the concepts have some structure and
can be decomposed to more elementary conceptual
units, or that partial matches between concepts and
text are natural. For example, if the conceptual units
represent named entities (a common occurrence in
list-type long answers), a partial match between a
name found in a text and another name is possi-
ble; handling these two names as distinct concepts
would be inaccurate. Similarly, an event can be rep-
resented as a concept with components correspond-
ing to participants, time, location, and action, with
only some of these components found in a particular
piece of text.
Partial matches between textual and conceptual
units introduce a new problem, however: if two tex-
tual units partially cover the same concept, it is
not apparent to what extent the coverage overlaps.
Thus, there are multiple ways to revise equation (1)
in order to account for partial matches, depending
on how conservative we are on the expected over-
lap. One such way is to assume minimum overlap
(the most conservative assumption) and define the
total information in the summary as
I(S) =
?
i=1,...,m
wi ?maxj f(tj , ci) (2)
An alternative is to consider that f(tj , ci) repre-
sents the extent of the [0, 1] interval corresponding
to concept ci that tj covers, and assume that the
coverage is spread over that interval uniformly and
independently across textual units. Then the com-
bined coverage of two textual units tj and tk is
f(tj , ci) + f(tk, ci)? f(tj , ci) ? f(tk, ci)
This operator can be naturally extended to more
than two textual units and plugged into equation (2)
in the place of the max operator, resulting into an
equation we will refer to as equation (3). Note that
both of these equations reduce to our original for-
mula for information content (equation (1)) if the
mapping function f only produces 0 and 1 values.
2.3 Length and textual constraints
We have provided formulae that measure the infor-
mation covered by a collection of textual units un-
der different mapping constraints. Obviously, we
want to maximize this information content. How-
ever, this can only sensibly happen when additional
constraints on the number or length of the selected
textual units are introduced; otherwise, the full set
of available textual units would be a solution that
proffers a maximal value for equations (1)?(3), i.e.,
?S ? T, I(S) ? I(T ). We achieve this by assign-
ing a cost pi to each textual unit ti, i = 1, . . . , n,
and defining a function P over a set of textual units
that provides the total penalty associated with se-
lecting those textual units as the output. In our ab-
straction, replacing a textual unit with one or more
textual units that provide the same content should
only affect the penalty, and it makes sense to assign
the same cost to a long sentence as to two sentences
produced by splitting the original sentence. Also,
a shorter sentence should be preferable to a longer
sentence with the same information content. Hence,
our operational definitions for pi and P are
pi = length(ti), P (S) =
?
ti?S
pi
i.e., the total penalty is equal to the total length of
the answer in some basic unit (e.g., words).
Note however, than in the general case the pi?s
need not depend solely on the length, and the to-
tal penalty does not need to be a linear combina-
tion of them. The cost function can depend on
features other then length, for example, number of
pronouns?the more pronouns used in a textual unit,
the higher the risk of dangling references and the
higher the price should be. Finding the best cost
function is an interesting research problem by itself.
With the introduction of the cost function P (S)
our model has two generally competing compo-
nents. One approach is to set a limit on P (S) and
optimize I(S) while keeping P (S) under that limit.
This approach is similar to that taken in evaluations
that keep the length of the output summary within
certain bounds, such as the recent major summa-
rization evaluations in the Document Understand-
ing Conferences from 2001 to the present (Harman
and Voorhees, 2001). Another approach would be
to combine the two components and assign a com-
posite score to each summary, essentially mandat-
ing a specific tradeoff between recall and precision;
for example, the total score can be defined as a lin-
ear combination of I(S) and P (S), in which case
the weights specify the relative importance of cov-
erage and precision/brevity, as well as accounting
for scale differences between the two metrics. This
approach is similar to the calculation of recall, pre-
cision, and F-measure adopted in the recent NIST
evaluation of long answers for definitional questions
(Voorhees, 2003). In this paper, we will follow the
first tactic of maximizing I(S) with a limit on P (S)
rather than attempting to solve the thorny issues of
weighing the two components appropriately.
3 Handling Redundancy in
Summarization
Redundancy of information has been found useful
in determining what text pieces should be included
during summarization, on the basis that information
that is repeated is likely to be central to the topic or
event being discussed. Earlier work has also recog-
nized that, while it is a good idea to select among
the passages repeating information, it is also impor-
tant to avoid repetition of the same information in
the final output.
Two main approaches have been proposed for
avoiding redundancy in the output. One approach
relies on grouping together potential output text
units on the basis of their similarity, and outputting
only a representative from each group (Hatzivas-
siloglou et al, 2001). Sentences can be clustered
in this manner according to word overlap, or by us-
ing additional content similarity features. This ap-
proach has been recently applied to the construction
of paragraph-long answers (e.g., (Blair-Goldensohn
et al, 2003; Yu and Hatzivassiloglou, 2003)).
An alternative approach, proposed for the synthe-
sis of information during query-based passage re-
trieval is the maximum marginal relevance (MMR)
method (Goldstein et al, 2000). This approach as-
signs to each potential new sentence in the output a
similarity score with the sentences already included
in the summary. Only those sentences that contain a
substantial amount of new information can get into
the summary. MMR bases this similarity score on
word overlap and additional information about the
time when each document was released, and thus
can fail to identify repeated information when para-
phrasing is used to convey the same meaning.
In contrast to these approaches, our model han-
dles redundancy in the output at the same time it
selects the output sentences. It is clear from equa-
tions (1)?(3) that each conceptual unit is counted
only once whether it appears in one or multiple tex-
tual units. Thus, when we find the subset of textual
units that maximizes overall information coverage
with a constraint on the total number or length of
textual units, the model will prefer the collection of
textual units that have minimal overlap of covered
conceptual units. Our approach offers three advan-
tages versus both clustering and MMR: First, it in-
tegrates redundancy elimination into the selection
process, requiring no additional features for defin-
ing a text-level similarity between selected textual
units. Second, decisions are based on the same fea-
tures that drive the summarization itself, not on ad-
ditional surface properties of similarity. Finally, be-
cause all decisions are informed by the overlap of
conceptual units, our approach accounts for partial
overlap of information across textual units. To illus-
trate this last point, consider a case where three fea-
tures A, B, and C should be covered in the output,
and where three textual units are available, cover-
ing A and B, A and C, and B and C, respectively.
Then our model will determine that selecting any
two of the textual units is fully sufficient, while this
may not be apparent on the basis of text similarity
between the three text units; a clustering algorithm
may form three singleton clusters, and MMR may
determine that each textual unit is sufficiently dif-
ferent from each other, especially if A, B, and C
are realized with nearly the same number of words.
4 Applying the Model
Having presented a formal metric for the informa-
tion content (and optionally the cost) of any poten-
tial summary or answer, the task that remains is to
optimize this metric and select the corresponding
set of textual units for the final output. As stated
in Section 2.3, one possible way to do this is to fo-
cus on the information content metric and introduce
an additional constraint, limiting the total cost to a
constant. An alternative is to optimize directly the
composite function that combines cost and informa-
tion content into a single number.
We examine the case of zero-one mappings be-
tween textual and conceptual units, where the to-
tal information content is specified by equation (1).
The complexity of the problem depends on the
cost function, and whether we optimize I(S) while
keeping P (S) fixed or whether we optimize a com-
bined function of both of those quantities. We will
only consider the former case in the present paper.
We start by examining an artificially simple case,
where the cost assigned to each textual unit is 1, and
the function P for combining costs is their sum. In
this case, the total cost is equal to the number of
textual units used in a summary.
This problem, as we have formalized it above,
is identical to the Maximum Set Coverage problem
studied in theoretical computer science: given C, a
finite set of weighted elements, a collection T of
subsets of C, and an integer k, find those k sets that
maximize the total number of elements in the union
of T ?s members (Hochbaum, 1997). In our case,
the zero-one mapping allows us to view each textual
unit as a subset of the conceptual units space, con-
taining those conceptual units covered by the tex-
tual unit, and k is the total target cost. Unfortu-
nately, maximum set coverage is NP-hard, as it is
reducible to the classic set cover problem (given a
finite set and a collection of subsets of that set, find
the smallest subset of that collection whose mem-
bers? union is equal to the original set) (Hochbaum,
1997). It follows that more general formulations of
the cost function that actually are more realistic for
our problem (such as defining the total cost as the
sum of the lengths of the selected textual units and
allowing the textual units to have different lengths)
will also result in an NP-hard problem, as we can re-
duce these versions to the special case of maximum
set coverage.
Nevertheless, the correspondence with maximum
set coverage provides a silver lining. Since the
problem is known to be NP-hard, properties of
simple greedy algorithms have been explored, and
a straightforward local maximization method has
been proved to give solutions within a known bound
of the optimal solution. The greedy algorithm for
maximum set coverage has as follows: Start with an
empty solution S, and iteratively add to the S the
set Ti that maximizes I(S ? Ti). It is provable that
this algorithm is the best polynomial approximation
algorithm for the problem (Hochbaum, 1997), and
that it achieves a solution bounded as follows
I(OPT) ? I(GREEDY) ?
[
1?
(
1? 1k
)k]
I(OPT)
>
(
1? 1e
)
I(OPT) ? 0.6321? I(OPT)
where I(OPT) is the information content of the op-
timal summary and I(GREEDY) is the information
content of the summary produced by this greedy al-
gorithm.
For the more realistic case where cost is speci-
fied as the total length of the summary, and where
we try to optimize I(S) with a limit on P (S) (see
Section 2.3), we propose two greedy algorithms in-
spired by the algorithm above. Both our algorithms
operate by first calculating a ranking of the textual
units in decreasing order. This ranking is for the
first algorithm, which we call adaptive greedy algo-
rithm, identical to the ranking provided by the ba-
sic greedy algorithm, i.e., each textual unit receives
as score the increase in I(S) that it generates when
added to the output, in the order specified by the ba-
sic greedy algorithm. Our second greedy algorithm
(dubbed modified greedy algorithm below) modifies
this ranking by prioritizing the conceptual units with
highest individual weight wi; it ranks first the tex-
tual unit that has the highest contribution to I(S)
while covering this conceptual unit with the high-
est individual weight, and then iteratively proceeds
with the textual unit that has the highest contribu-
tion to I(S) while covering the next most important
unaccounted for conceptual unit.
Given the rankings of textual units, we can then
produce an output of a given length by adopting ap-
propriate stopping criteria for when to stop adding
textual units (in order according to their ranking)
to the output. There is no clear rule for conform-
ing to a specific length (for example, DUC 2001 al-
lowed submitted summaries to go over ?a reason-
able percentage? of the target length, while DUC
2004 cuts summaries mid-sentence at exactly the
target length). As the summary length in DUC is
measured in words, in our experiments we extracted
the specified number of words out of the top sen-
tences (truncating the last sentence if necessary).
5 Experiments
To empirically establish the effectiveness of the pre-
sented model we ran experiments comparing evalu-
ation scores on summaries obtained with a baseline
algorithm that does not account for redundancy of
information and with the two variants of greedy al-
gorithms described in Section 4. We chose summa-
rization as the evaluation task because ?ideal? out-
put (prepared by humans) and methods for scoring
arbitrary system output were available for this task,
but not for evaluating long answers to questions.
Data We chose as our input data the document
sets used in the evaluation of multidocument sum-
marization during the Document Understanding
Conference (DUC), organized by NIST in 2001
(Harman and Voorhees, 2001). This collection con-
tains 30 test document sets, each containing approx-
imately 10 news stories on different events; docu-
ment sets vary significantly in their internal cohere-
ness. For each document set 12 human-constructed
summaries are provided, 3 for each of the target
lengths of 50, 100, 200, and 400 words. We se-
lected DUC 2001 because unlike later DUCs, ideal
summaries are available for multiple lengths. We
consider sentences as our textual units.
Features In our experiments we used two sets of
features (i.e., conceptual units). First, we chose
a fairly basic and widely used set of lexical fea-
tures, namely the list of words present in each input
text. We set the weight of each feature to its tf*idf
value, taking idf values from http://elib.cs.
berkeley.edu/docfreq/.
Our alternative set of conceptual units was the list
of weighted atomic events extracted from the input
texts. An atomic event is a triplet consisting of two
named entities extracted from a sentence and a con-
nector expressed by a verb or an event-related noun
that appears in-between these two named entities.
The score of the atomic event depends on the fre-
quency of the named entities pair for the input text
and the frequency of the connector for that named
entities pair. Filatova and Hatzivassiloglou (2003)
define the procedure for extracting atomic events in
detail, and show that these triplets capture the most
important relations connecting the major constituent
parts of events, such as location, dates and partici-
pants. Our hypothesis is that using these events as
conceptual units would provide a reasonable basis
for summarizing texts that are supposed to describe
one or more events.
Evaluation Metric Given the difficulties in com-
ing up with a universally accepted evaluation mea-
sure for summarization, and the fact that judgments
by humans are time-consuming and labor-intensive,
we adopted an automated process for comparing
system-produced summaries to the ideal summaries
written by humans. The ROUGE method (Lin and
Hovy, 2003) is based on n-gram overlap between
the system-produced and ideal summaries. As such,
it is a recall-based measure, and it requires that
the length of the summaries be controlled in or-
der to allow for meaningful comparisons. Although
ROUGE is only a proxy measure of summary qual-
ity, it offers the advantage that it can be readily ap-
plied to compare the performance of different sys-
tems on the same set of documents, assuming that
ideal summaries are available for those documents.
Baseline Our baseline method does not consider
the overlap in information content between selected
textual units. Instead, we fix the score of each sen-
tence as the sum of tf*idf values or atomic event
scores. At every step we choose the remaining sen-
tence with the largest score, until the stopping crite-
rion for summary length is satisfied.
Results For every version of our baseline and
approximation algorithms, and separately for the
tf*idf -weighted words and event features, we get a
sorted list of sentences extracted according to a par-
ticular algorithm. Then, for each DUC document set
we create four summaries of each suggested length
(50, 100, 200, and 400 words) by extracting accord-
ingly the first 50, 100, 200, and 400 words from the
top sentences.
To evaluate the performance of our summarizers
we compare their outputs against the human models
of the corresponding length provided by DUC, us-
ing the ROUGE-created scores for unigrams. Since
scores are not comparable across different docu-
ment sets, instead of average scores we report the
number of document sets for which one algorithm
outperforms another. We compare each of our
Length Events tf*idf
50 +3 0
100 +4 ?4
200 +2 ?4
400 +5 0
Table 1: Adaptive greedy algorithm versus baseline.
Length Events tf*idf
50 0 + 7
100 +4 + 4
200 +8 + 6
400 +2 +14
Table 2: Modified greedy algorithm versus baseline.
approximation algorithms (adaptive and modified
greedy) to the baseline.
Table 1 shows the number of data sets for
which the adaptive greedy algorithm outperforms
our baseline. This implementation of our informa-
tion packing model improves the ROUGE scores in
most cases when events are used as features, while
the opposite is true when tf*idf provides the con-
ceptual units. This may be partly explained because
of the nature of the tf*idf -weighted word features:
it is possible that important words cannot be con-
sidered independently, and that the repetition of im-
portant words in later sentence does not necessarily
mean that the sentence offers no new information.
Thus words may not provide independent enough
features for our approach to work.
Table 2 compares our modified greedy algorithm
to the baseline. In that case, the model offers gains
in performance when both events and words are
used as features, and in fact the gains are most pro-
nounced with the word features. For both algo-
rithms, the gains are generally minimal for 50 word
summaries and most pronounced for the longest,
400 word summaries. This validates our approach,
as the information packing model has a limited op-
portunity to alter the set of selected sentences when
those sentences are very few (often one or two for
the shortest summaries).
It is worth noting that in direct comparisons be-
tween the adaptive and modified greedy algorithm
we found the latter to outperform the former. We
found also events to lead to better performance than
tf*idf -weighted words with statistically significant
differences. Events tend to be a particularly good
representation for document sets with well-defined
constituent parts (such as specific participants) that
cluster around a narrow event. Events not only give
us a higher absolute performance when compared
to just words but also lead to more pronounced im-
provement when our model is employed. A more
detailed analysis of the above experiments together
with the discussion of advantages and disadvantages
of our evaluation schema can be found in (Filatova
and Hatzivassiloglou, 2004).
6 Conclusion
In this paper we proposed a formal model for in-
formation selection and redundancy avoidance in
summarization and question-answering. Within
this two-dimensional model, summarization and
question-answering entail mapping textual units
onto conceptual units, and optimizing the selection
of a subset of textual units that maximizes the in-
formation content of the covered conceptual units.
The formalization of the process allows us to benefit
from theoretical results, including suitable approx-
imation algorithms. Experiments using DUC data
showed that this approach does indeed lead to im-
provements due to better information packing over
a straightforward content selection method.
7 Acknowledgements
We wish to thank Rocco Servedio and Mihalis
Yannakakis for valuable discussions of theoreti-
cal foundations of the set cover problem. This
work was supported by ARDA under Advanced
Question Answering for Intelligence (AQUAINT)
project MDA908-02-C-0008.
References
Regina Barzilay and Michael Elhadad. 1997. Us-
ing lexical chains for text summarization. In Pro-
ceedings of the ACL/EACL 1997 Workshop on In-
telligent Scalable Text Summarization, Spain.
Sasha Blair-Goldensohn, Kathleen R. McKeown,
and Andrew Hazen Schlaikjer. 2003. Defscriber:
A hybrid system for definitional qa. In Proceed-
ings of 26th Annual International ACM SIGIR
Conference, Toronoto, Canada, July.
Elena Filatova and Vasileios Hatzivassiloglou.
2003. Domain-independent detection, extraction,
and labeling of atomic events. In Proceedings of
Recent Advances in Natural Language Process-
ing Conference, RANLP, Bulgaria.
Elena Filatova and Vasileios Hatzivassiloglou.
2004. Event-based extractive summarization. In
Proceedings of ACL Workshop on Summariza-
tion, Barcelona, Spain, July.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell,
and Jamie Callan. 2000. Creating and evaluat-
ing multi-document sentence extract summaries.
In Proceedings of the ninth international con-
ference on Information and knowledge manage-
ment, pages 165?172.
Donna Harman and Ellen Voorhees, editors. 2001.
Proceedings of the Document Understanding
Conference (DUC). NIST, New Orleans, USA.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
Simfinder: A flexible clustering tool for summa-
rization. In Proceedings of workshop on Auto-
matic Summarization, NAACL, Pittsburg, USA.
Dorit S. Hochbaum. 1997. Approximating cov-
ering and packing problems: Set cover, vertex
cover, independent set, and related problems. In
Dorit S. Hochbaum, editor, Approximation Al-
gorithms for NP-hard Problems, pages 94?143.
PWS Publishing Company, Boston, MA.
H.P.Edmundson. 1968. New methods in automatic
extracting. Journal of the Association for Com-
puting Machinery, 23(1):264?285, April.
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Pro-
ceedings of 18th Annual International ACM SI-
GIR Conference, pages 68?73, Seattle, USA.
Chin-Yew Lin and Eduard Hovy. 1997. Identify-
ing topic by position. In Proceedings of the 5th
Conference on Applied Natural Language Pro-
cessing, ANLP, Washington, DC.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of 2003
Language Technology Conference (HLT-NAACL
2003), Edmonton, Canada, May.
H.P. Luhn. 1959. The automatic creation of litera-
ture abstracts. IBM Journal of Research and De-
velopment, 2(2):159?165, April.
Daniel Marcu. 1997. From discourse struc-
tures to text summaries. In Proceedings of the
ACL/EACL 1997 Workshop on Intelligent Scal-
able Text Summarization, pages 82?88, Spain.
Simone Teufel and Marc Moens. 1997. Sentence
extraction as a classification task. In Proceedings
of the ACL/EACL 1997 Workshop on Intelligent
Scalable Text Summarizaion, Spain.
Ellen M. Voorhees. 2003. Evaluating answers to
definition questions. In Proceedings of HLT-
NAACL, Edmonton, Canada, May.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), Sapporo, Japan, July.
Towards Answering Opinion Questions: Separating Facts from Opinions
and Identifying the Polarity of Opinion Sentences
Hong Yu
Department of Computer Science
Columbia University
New York, NY 10027, USA
hongyu@cs.columbia.edu
Vasileios Hatzivassiloglou
Department of Computer Science
Columbia University
New York, NY 10027, USA
vh@cs.columbia.edu
Abstract
Opinion question answering is a challenging task
for natural language processing. In this paper, we
discuss a necessary component for an opinion ques-
tion answering system: separating opinions from
fact, at both the document and sentence level. We
present a Bayesian classifier for discriminating be-
tween documents with a preponderance of opinions
such as editorials from regular news stories, and
describe three unsupervised, statistical techniques
for the significantly harder task of detecting opin-
ions at the sentence level. We also present a first
model for classifying opinion sentences as positive
or negative in terms of the main perspective be-
ing expressed in the opinion. Results from a large
collection of news stories and a human evaluation
of 400 sentences are reported, indicating that we
achieve very high performance in document classi-
fication (upwards of 97% precision and recall), and
respectable performance in detecting opinions and
classifying them at the sentence level as positive,
negative, or neutral (up to 91% accuracy).
1 Introduction
Newswire articles include those that mainly present
opinions or ideas, such as editorials and letters to
the editor, and those that mainly report facts such as
daily news articles. Text materials from many other
sources also contain mixed facts and opinions. For
many natural language processing applications, the
ability to detect and classify factual and opinion sen-
tences offers distinct advantages in deciding what in-
formation to extract and how to organize and present
this information. For example, information extrac-
tion applications may target factual statements rather
than subjective opinions, and summarization sys-
tems may list separately factual information and ag-
gregate opinions according to distinct perspectives.
At the document level, information retrieval systems
can target particular types of articles and even utilize
perspectives in focusing queries (e.g., filtering or re-
trieving only editorials in favor of a particular policy
decision).
Our motivation for building the opinion detec-
tion and classification system described in this pa-
per is the need for organizing information in the
context of question answering for complex ques-
tions. Unlike questions like ?Who was the first
man on the moon?? which can be answered with
a simple phrase, more intricate questions such as
?What are the reasons for the US-Iraq war?? require
long answers that must be constructed from multi-
ple sources. In such a context, it is imperative that
the question answering system can discriminate be-
tween opinions and facts, and either use the appro-
priate type depending on the question or combine
them in a meaningful presentation. Perspective in-
formation can also help highlight contrasts and con-
tradictions between different sources?there will be
significant disparity in the material collected for the
question mentioned above between Fox News and
the Independent, for example.
Fully analyzing and classifying opinions involves
tasks that relate to some fairly deep semantic and
syntactic analysis of the text. These include not only
recognizing that the text is subjective, but also de-
termining who the holder of the opinion is, what the
opinion is about, and which of many possible posi-
tions the holder of the opinion expresses regarding
that subject. In this paper, we are presenting three
of the components of our opinion detection and or-
ganization subsystem, which have already been in-
tegrated into our larger question-answering system.
These components deal with the initial tasks of clas-
sifying articles as mostly subjective or objective,
finding opinion sentences in both kinds of articles,
and determining, in general terms and without refer-
ence to a specific subject, if the opinions are positive
or negative. The three modules of the system dis-
cussed here provide the basis for ongoing work for
further classification of opinions according to sub-
ject and opinion holder and for refining the original
positive/negative attitude determination.
We review related work in Section 2, and then
present our document-level classifier for opinion or
factual articles (Section 3), three implemented tech-
niques for detecting opinions at the sentence level
(Section 4), and our approach for rating an opinion
as positive or negative (Section 5). We have evalu-
ated these methods using a large collection of news
articles without additional annotation (Section 6)
and an evaluation corpus of 400 sentences anno-
tated for opinion classifications (Section 7). The
results, presented in Section 8, indicate that we
achieve very high performance (more than 97%) at
document-level classification and respectable per-
formance (86?91%) at detecting opinion sentences
and classifying them according to orientation.
2 Related Work
Much of the earlier research in automated opinion
detection has been performed by Wiebe and col-
leagues (Bruce and Wiebe, 1999; Wiebe et al, 1999;
Hatzivassiloglou and Wiebe, 2000; Wiebe, 2000;
Wiebe et al, 2002), who proposed methods for dis-
criminating between subjective and objective text at
the document, sentence, and phrase levels. Bruce
and Wiebe (1999) annotated 1,001 sentences as sub-
jective or objective, and Wiebe et al (1999) de-
scribed a sentence-level Naive Bayes classifier using
as features the presence or absence of particular syn-
tactic classes (pronouns, adjectives, cardinal num-
bers, modal verbs, adverbs), punctuation, and sen-
tence position. Subsequently, Hatzivassiloglou and
Wiebe (2000) showed that automatically detected
gradable adjectives are a useful feature for subjec-
tivity classification, while Wiebe (2000) introduced
lexical features in addition to the presence/absence
of syntactic categories. More recently, Wiebe et al
(2002) report on document-level subjectivity classi-
fication, using a k-nearest neighbor algorithm based
on the total count of subjective words and phrases
within each document.
Psychological studies (Bradley and Lang, 1999)
found measurable associations between words and
human emotions. Hatzivassiloglou and McKeown
(1997) described an unsupervised learning method
for obtaining positively and negatively oriented ad-
jectives with accuracy over 90%, and demonstrated
that this semantic orientation, or polarity, is a con-
sistent lexical property with high inter-rater agree-
ment. Turney (2002) showed that it is possible
to use only a few of those semantically oriented
words (namely, ?excellent? and ?poor?) to label
other phrases co-occuring with them as positive or
negative. He then used these phrases to automati-
cally separate positive and negative movie and prod-
uct reviews, with accuracy of 66?84%. Pang et al
(2002) adopted a more direct approach, using super-
vised machine learning with words and n-grams as
features to predict orientation at the document level
with up to 83% precision.
Our approach to document and sentence classi-
fication of opinions builds upon the earlier work
by using extended lexical models with additional
features. Unlike the work cited above, we do not
rely on human annotations for training but only on
weak metadata provided at the document level. Our
sentence-level classifiers introduce additional crite-
ria for detecting subjective material (opinions), in-
cluding methods based on sentence similarity within
a topic and an approach that relies on multiple clas-
sifiers. At the document level, our classifier uses the
same document labels that the method of (Wiebe et
al., 2002) does, but automatically detects the words
and phrases of importance without further analy-
sis of the text. For determining whether an opin-
ion sentence is positive or negative, we have used
seed words similar to those produced by (Hatzivas-
siloglou and McKeown, 1997) and extended them to
construct a much larger set of semantically oriented
words with a method similar to that proposed by
(Turney, 2002). Our focus is on the sentence level,
unlike (Pang et al, 2002) and (Turney, 2002); we
employ a significantly larger set of seed words, and
we explore as indicators of orientation words from
syntactic classes other than adjectives (nouns, verbs,
and adverbs).
3 Document Classification
To separate documents that contain primarily opin-
ions from documents that report mainly facts, we ap-
plied Naive Bayes1, a commonly used supervised
machine-learning algorithm. This approach pre-
supposes the availability of at least a collection of ar-
ticles with pre-assigned opinion and fact labels at the
document level; fortunately, Wall Street Journal ar-
ticles contain such metadata by identifying the type
of each article as Editorial, Letter to editor, Business
and News. These labels are used only to provide the
correct classification labels during training and eval-
uation, and are not included in the feature space. We
used as features single words, without stemming or
stopword removal. Naive Bayes assigns a document
 
to the class  that maximizes 
 	
by applying
Bayes? rule 
 	
 

and assuming con-
ditional independence of the features.
Although Naive Bayes can be outperformed in
text classification tasks by more complex methods
such as SVMs, Pang et al (2002) report similar per-
formance for Naive Bayes and other machine learn-
ing techniques for a similar task, that of distinguish-
ing between positive and negative reviews at the
document level. Further, we achieved such high per-
formance with Naive Bayes (see Section 8) that ex-
ploring additional techniques for this task seemed
unnecessary.
4 Finding Opinion Sentences
We developed three different approaches to clas-
sify opinions from facts at the sentence level. To
avoid the need for obtaining individual sentence an-
notations for training and evaluation, we rely in-
stead on the expectation that documents classified
as opinion on the whole (e.g., editorials) will tend to
have mostly opinion sentences, and conversely doc-
uments placed in the factual category will tend to
have mostly factual sentences. Wiebe et al (2002)
report that this expectation is borne out 75% of the
time for opinion documents and 56% of the time for
factual documents.
4.1 Similarity Approach
Our first approach to classifying sentences as opin-
ions or facts explores the hypothesis that, within a
given topic, opinion sentences will be more simi-
lar to other opinion sentences than to factual sen-
1Using the Rainbow implementation, available from www.
cs.cmu.edu/?mccallum/bow/rainbow.
tences. We used SIMFINDER (Hatzivassiloglou et
al., 2001), a state-of-the-art system for measuring
sentence similarity based on shared words, phrases,
and WordNet synsets. To measure the overall simi-
larity of a sentence to the opinion or fact documents,
we first select the documents that are on the same
topic as the sentence in question. We obtain topics
as the results of IR queries (for example, by search-
ing our document collection for ?welfare reform?).
We then average its SIMFINDER-provided similari-
ties with each sentence in those documents. Then
we assign the sentence to the category for which the
average is higher (we call this approach the ?score?
variant). Alternatively, for the ?frequency? variant,
we do not use the similarity scores themselves but
instead we count how many of them, for each cate-
gory, exceed a predetermined threshold (empirically
set to 0.65).
4.2 Naive Bayes Classifier
Our second method trains a Naive Bayes classifier
(see Section 3), using the sentences in opinion and
fact documents as the examples of the two cate-
gories. The features include words, bigrams, and
trigrams, as well as the parts of speech in each sen-
tence. In addition, the presence of semantically ori-
ented (positive and negative) words in a sentence is
an indicator that the sentence is subjective (Hatzi-
vassiloglou and Wiebe, 2000). Therefore, we in-
clude in our features the counts of positive and neg-
ative words in the sentence (which are obtained with
the method of Section 5.1), as well as counts of
the polarities of sequences of semantically oriented
words (e.g., ?++? for two consecutive positively ori-
ented words). We also include the counts of parts
of speech combined with polarity information (e.g.,
?JJ+? for positive adjectives), as well as features en-
coding the polarity (if any) of the head verb, the
main subject, and their immediate modifiers. Syn-
tactic structure was obtained with Charniak?s statis-
tical parser (Charniak, 2000). Finally, we used as
one of the features the average semantic orientation
score of the words in the sentence.
4.3 Multiple Naive Bayes Classifiers
Our designation of all sentences in opinion or factual
articles as opinion or fact sentences is an approxima-
tion. To address this, we apply an algorithm using
multiple classifiers, each relying on a different sub-
set of our features. The goal is to reduce the training
set to the sentences that are most likely to be cor-
rectly labeled, thus boosting classification accuracy.
Given separate sets of features   			
  ,
we train separate Naive Bayes classifiers  ,
			 corresponding to each feature set. Assum-
ing as ground truth the information provided by the
document labels and that all sentences inherit the
status of their document as opinions or facts, we
first train   on the entire training set, then use the
resulting classifier to predict labels for the training
set. The sentences that receive a label different from
the assumed truth are then removed, and we train
  on the remaining sentences. This process is re-
peated iteratively until no more sentences can be re-
moved. We report results using five feature sets,
starting from words alone and adding in bigrams, tri-
grams, part-of-speech, and polarity.
5 Identifying the Polarity of Opinion
Sentences
Having distinguished whether a sentence is a fact or
opinion, we separate positive, negative, and neutral
opinions into three classes. We base this decision
on the number and strength of semantically oriented
words (either positive or negative) in the sentence.
We first discuss how such words are automatically
found by our system, and then describe the method
by which we aggregate this information across the
sentence.
5.1 Semantically Oriented Words
To determine which words are semantically ori-
ented, in what direction, and the strength of their
orientation, we measured their co-occurrence with
words from a known seed set of semantically ori-
ented words. The approach is based on the hypothe-
sis that positive words co-occur more than expected
by chance, and so do negative words; this hypothe-
sis was validated, at least for strong positive/negative
words, in (Turney, 2002). As seed words, we used
subsets of the 1,336 adjectives that were manually
classified as positive (657) or negative (679) by
Hatzivassiloglou and McKeown (1997). In earlier
work (Turney, 2002) only singletons were used as
seed words; varying their number allows us to test
whether multiple seed words have a positive effect
in detection performance. We experimented with
seed sets containing 1, 20, 100 and over 600 positive
and negative pairs of adjectives. For a given seed set
size, we denote the set of positive seeds as ADJ 
and the set of negative seeds as ADJ  . We then cal-
culate a modified log-likelihood ratio  Event-Based Extractive Summarization
Elena Filatova
Department of Computer Science
Columbia University
New York, NY 10027, USA
filatova@cs.columbia.edu
Vasileios Hatzivassiloglou
Center for Computational Learning Systems
Columbia University
New York, NY 10027, USA
vh@cs.columbia.edu
Abstract
Most approaches to extractive summarization define
a set of features upon which selection of sentences
is based, using algorithms independent of the fea-
tures themselves. We propose a new set of features
based on low-level, atomic events that describe rela-
tionships between important actors in a document or
set of documents. We investigate the effect this new
feature has on extractive summarization, compared
with a baseline feature set consisting of the words
in the input documents, and with state-of-the-art
summarization systems. Our experimental results
indicate that not only the event-based features of-
fer an improvement in summary quality over words
as features, but that this effect is more pronounced
for more sophisticated summarization methods that
avoid redundancy in the output.
1 Introduction
The main goal of extractive summarization can be
concisely formulated as extracting from the input
pieces of text which contain the information about
the most important concepts mentioned in the input
text or texts. This definition conceals a lot of impor-
tant issues that should be taken into consideration
in the process of summary construction. First, it is
necessary to identify the important concepts which
should be described in the summary. When those
important concepts are identified then the process
of summarization can be presented as:
1. Break the input text into textual units (sen-
tences, paragraphs, etc.).
2. See what concepts each textual unit covers.
3. Choose a particular textual unit for the output
according to the concepts present in all textual
units.
4. Continue choosing textual units until reaching
the desired length of the summary.
Some current summarization systems add a clus-
tering step, substituting the analysis of all the textual
units by the analysis of representative units from
each cluster. Clustering is helpful for avoiding rep-
etitions in the summary.
In this paper we propose a new representation
for concepts and correspondingly a new feature on
which summarization can be based. We adapt the
algorithm we proposed earlier (Filatova and Hatzi-
vassiloglou, 2003) for assigning to each sentence
a list of low-level, atomic events. These events
capture information about important named entities
for the input text or texts, and the relationships be-
tween these named entities. We also discuss a gen-
eral model which treats summarization as a three-
component problem, involving the identification of
the textual units into which the input text should
be broken and which are later used as the con-
stituent parts of the final summary, the textual fea-
tures which are associated with the important con-
cepts described in the input text, and the appropri-
ate algorithm for selecting the textual units to be in-
cluded into the summary.
We focus on the latter two of those steps and ex-
plore interdependencies between the choice of fea-
tures (step 2) and selection algorithm (step 3). We
experimentally test our hypothesis that event-based
features are helpful for summarization by compar-
ing the performance of three sentence selection al-
gorithms when we use such features versus the case
where we use another, widely used set of textual
features: the words in the input texts, weighted by
their tf*idf scores. The results establish that for
the majority of document sets in our test collection,
events outperform tf*idf for all algorithms consid-
ered. Furthermore, we show that this benefit is more
pronounced when the selection algorithm includes
steps to address potential repetition of information
in the output summary.
2 General Summarization Model
Many summarization systems (e.g., (Teufel and
Moens, 1997; McKeown et al, 1999; Lin and Hovy,
2000)) include two levels of analysis: the sentence
level, where every textual unit is scored according to
c1 c2 c3 c4 c5
t1 1 1 0 1 1
t2 1 0 0 1 0
t3 0 1 0 0 1
t4 1 0 1 1 1
Table 1: Matrix for Summarization Model
the concepts or features it covers, and the text level,
where, before being added to the final output, tex-
tual units are compared to each other on the basis of
those features.
In Section 1 we presented a four-step pipeline
for extractive summarization; existing summariza-
tion systems largely follow this pipeline, although
they introduce different approaches for every step
in it. We suggest a model that describes the extrac-
tive summarization task in general terms. Consider
the matrix in Table 1.
Rows of this matrix represent all textual units into
which the input text is divided. Columns represent
the concepts discovered for the input text. Every
concept is either absent or present in a given textual
unit. Each concept ci has also an associated weight
wi indicating the importance of this concept. These
weights can be used for scoring the textual units.
Thus, the input text and the important informa-
tion in it is mapped onto an m?n matrix. Using the
above matrix it is possible to formulate the extrac-
tive summarization problem as extracting the mini-
mal amount of textual units which cover all the con-
cepts that are interesting or important. To account
for the cost of long summaries, we can constrain the
total length of the summary, or balance it against the
total weight of covered concepts.
The presented model can be also used for com-
paring summaries consisting of different textual
units. For example, a summary consisting only of
textual unit t1 renders the same information as the
summary consisting of textual units t2 and t3. Both
these summaries cover the same set of concepts,
namely c1, c2 and c3. We explore properties of
this model in more detail in (Filatova and Hatzivas-
siloglou, 2004).
3 Associating Concepts with Features
Before extracting a summary, it is necessary to de-
fine what concepts in the input text are important
and should be covered by the output text. There is
no exact definition or even agreement between dif-
ferent approaches on what an important concept is.
In order to use the model of Section 2 one has to
approximate the notion of ?concept? with some tex-
tual features.
Current summarization approaches use text fea-
tures which give high scores to the textual units that
contain important information, and low scores to
those textual units which are not highly likely to
contain information worth to be included in the final
output.
There exist approaches that deal mainly with lex-
ical features, like tf*idf weighing of words in the
input text(s), words used in the titles and section
headings (Luhn, 1958; Edmundson, 1968), or the
presence or absence of certain cue phrases like sig-
nificant, important, and in conclusion (Kupiec et
al., 1995; Teufel and Moens, 1997). Other sys-
tems exploit the co-occurrence of particular con-
cepts (Barzilay and Elhadad, 1997; Lin and Hovy,
2000) or syntactic constraints between concepts
(McKeown et al, 1999). Concepts do not have to be
directly observable as text snippets?they can rep-
resent abstract properties that particular text units
may or may not satisfy, for example, status as a first
sentence in a paragraph or generally position in the
source text (Baxendale, 1958; Lin and Hovy, 1997).
Some summarization systems assume that the im-
portance of a sentence is derivable from a rhetorical
representation of the source text (Marcu, 1997).
The matrix representation of the previous section
offers a way to formalize the sharing of information
between textual units at the individual feature level.
Thus, this representation is most useful for content-
related concepts that should not be repeated in the
summary. The representation can however handle
independent features such as sentence position by
encoding them separately for each textual unit.
4 Atomic Events
Atomic events link major constituent parts of the
actions described in a text or collection of texts
through the verbs or action nouns labeling the event
itself. The idea behind this technique is that the
major constituent parts of events (participants, lo-
cations, times) are usually realized in text as named
entities. The more important the constituent part,
the more often the corresponding named entity is
mentioned.
Not all the constituent parts of events need to be
represented by named entities. For example, in an
airline crash it is important to report information
about the passengers and the crew. These are not
marked by named entities but are highly likely to be
among the most frequently used nouns. Thus, we
add the top ten most frequent nouns to the list of
named entities.
We use the algorithm for atomic event extraction
proposed in (Filatova and Hatzivassiloglou, 2003).
It involves the following steps:
1. Analyze each input sentence1 one at a time; ig-
nore sentences that do not contain at least two
named entities or frequent nouns.
2. Extract all the possible pairs of named enti-
ties/frequent nouns in the sentence, preserving
their order and all the words in between. We
call such pairs of named entities relations, and
the words in-between the named entities in a
relation connectors.
3. For each relation, count how many times this
relation is used in the input text(s).
4. Keep only connectors that are content verbs
or action nouns, according to WordNet?s (Fell-
baum, 1998) noun hierarchy. For each connec-
tor calculate how many times it is used for the
extracted relation.
After calculating the scores for all relations and
all connectors within each relation, we calculate
their normalized scores The normalized relation
score is the ratio of the count for the current rela-
tion (how many times we see the relation within a
sentence in the input) over the overall count of all
relations. The normalized connector score is the ra-
tio of the count for the current connector (how many
times we see this connector for the current relation)
over the overall count for all connectors for this re-
lation.
Thus, out of the above procedural definition, an
atomic event is a triplet of two named entities (or
frequent nouns) connected by a verb or an action-
denoting noun. To get a score for the atomic event
we multiply the normalized score for the relation by
the normalized score for the connector. The score
indicates how important the triplet is overall.
In the above approach to event detection we do
not address co-reference, neither we merge together
the triplets which describe the same event using
paraphrases, inflected forms and syntactic variants
(e.g., active/passive voice). Our method uses rel-
atively simple extraction techniques and shallow
statistics, but it is fully automatic and can serve as a
first approximation of the events in the input text(s).
Our approach to defining events is not the only
one proposed?this is a subject with substantial
work in linguistics, information retrieval, and infor-
mation extraction. In linguistics, events are often
defined at a fine-grained level as a matrix verb or a
single action noun like ?war? (Pustejovsky, 2000).
In contrast, recent work in information retrieval
1We earlier showed empirically (Filatova and Hatzivas-
siloglou, 2003) that a description of a single event is usually
bound within one sentence.
within the TDT framework has taken event to mean
essentially ?narrowly defined topic for search? (Al-
lan et al, 1998). Finally, for the information extrac-
tion community an event represents a template of re-
lationships between participants, times, and places
(Marsh and Perzanowski, 1997). It may be possible
to use these alternative models of events as a source
of content features.
We earlier established empirically (Filatova and
Hatzivassiloglou, 2003) that this technique for
atomic event extraction is useful for delineating the
major participants and their relationships from a set
of topically related input texts. For example, from a
collection of documents about an airplane crash the
algorithm assigns the highest score to atomic events
that link together the name of the airline, the source
and destination airports and the day when the crash
happened through the verb crashed or its synonyms.
It is thus plausible to explore the usefulness of these
event triplets as the concepts used in the model of
Section 2.
5 Textual Unit Selection
We have formulated the problem of extractive sum-
marization in terms of the matrix model, stating
that mapping concepts present in the input text onto
the textual units out of which the output is con-
structed can be accomplished by extracting the min-
imal amount of textual units which either cover
most of the important concepts. Every time we add
a new textual unit to the output it is possible to judge
what concepts in it are already covered in the final
summary. This observation can be used to avoid re-
dundancy: before adding a candidate textual unit to
the output summary, we check whether it contains
enough new important concepts.
We describe in this section several algorithms
for selecting appropriate textual units for the output
summary. These algorithms differ on whether they
take advantage of the redundancy reduction prop-
erty of our model, and on whether they prioritize im-
portant concepts individually or collectively. They
share, however, a common property: all of them op-
erate independently of the features chosen to repre-
sent important concepts, and thus can be used with
both our event-based features and other feature sets.
The comparison of the results allows us to empir-
ically determine whether event-based features can
help in summarization.
5.1 Static Greedy Algorithm
Our first text unit selection algorithm does not sup-
port any mechanism for avoiding redundant infor-
mation in the summary. Instead, it rates each textual
unit independently. Textual units are included in the
summary if and only if they cover lots of concepts.
More specifically,
1. For every textual unit, calculate the weight of
this textual unit as the sum of the weights of all
the concepts covered by this textual unit.
2. Choose the textual unit with the maximum
weight and add it to the final output.
3. Continue extracting other textual units in order
of total weight till we get the summary of the
desired length.
5.2 Avoiding Redundancy in the Summary
Two popular techniques for avoiding redundancy
in summarization are Maximal Marginal Relevance
(MMR) (Goldstein et al, 2000) and clustering
(McKeown et al, 1999). In MMR the determination
of redundancy is based mainly on the textual over-
lap between the sentence that is about to be added to
the output and the sentences that are already in the
output. Clustering offers an alternative: before start-
ing the selection process, the summarization system
clusters the input textual units. This step allows an-
alyzing one representative unit from each cluster in-
stead of all textual units.
We take advantage of the model matrix of Sec-
tion 2 to explore another way to avoid redundancy.
Rather than making decisions for each textual unit
independently, as in our Static Greedy Algorithm,
we globally select the subset of textual units that
cover the most concepts (i.e., information) present
in the input. Then our task becomes very similar to
a classic theory problem, Maximum Coverage.
Given C , a finite set of weighted elements, a col-
lection T of subsets of C , and a parameter k, the
maximum coverage problem is to find k members
of T such that the total weight of the elements cov-
ered (i.e., belonging to the k members of the solu-
tion) is maximized. This problem is NP-hard, as it
can be reduced to the well-known set cover problem
(Hochbaum, 1997). Thus, we know only approxi-
mation algorithms solving this problem in polyno-
mial time.
Hochbaum (1997) reports that a greedy algorithm
is the best possible polynomial approximation algo-
rithm for this problem. This algorithm iteratively
adds to the solution S the set ti ? T that locally
maximizes the increase in the total weight of ele-
ments covered by S ? ti. The algorithm gives a so-
lution with weight at least 1/(1 ? e) of the optimal
solution?s total weight.
5.3 Adaptive Greedy Algorithm
The greedy algorithm for the maximum coverage
problem is not directly applicable to summariza-
tion, because the formulation of maximum cover-
age assumes that any combination of k sets ti (i.e.,
k sentences) is equally good as long as they cover
the same total weight of concepts. A more realistic
limitation for the summarization task is to aim for a
fixed total length of the summary, rather than a fixed
total number of sentences; this approach has been
adopted in several evaluation efforts, including the
Document Understanding Conferences (DUC). We
consequently modify the greedy algorithm for the
maximum coverage problem to obtain the following
adaptive greedy algorithm for summarization:
1. For each textual unit calculate its weight as the
sum of weights of all concepts it covers.
2. Choose the textual unit with the maximum
weight and add it to the output. Add the con-
cepts covered by this textual unit to the list of
concepts covered in the final output.
3. Recalculate the weights of the textual units:
subtract from each unit?s weight the weight of
all concepts in it that are already covered in the
output.
4. Continue extracting text units in order of their
total weight (going back to step 2) until the
summary is of the desired length.
5.4 Modified Adaptive Greedy Algorithm
The adaptive greedy algorithm described above pri-
oritizes sentences according to the total weight of
concepts they cover. While this is a reasonable ap-
proach, an alternative is to give increased priority to
concepts that are individually important, so that sen-
tences mentioning them have a chance of being in-
cluded in the output even if they don?t contain other
important concepts. We have developed the fol-
lowing variation of our adaptive greedy algorithm,
termed the modified greedy algorithm:
1. For every textual unit calculate its weight as
the sum of weights of all concepts it covers.
2. Consider only those textual units that contain
the concept with the highest weight that has not
yet been covered. Out of these, choose the one
with highest total weight and add it to the final
output. Add the concepts which are covered by
this textual unit to the list of concepts covered
in the final output.
3. Recalculate the weights of the textual units:
subtract from each unit?s weight the weight of
all concepts in it that are already covered in the
output.
4. Continue extracting textual units, going back
to step 2 each time, until we get a summary of
the desired length.
The modified greedy algorithm has the same
mechanism for avoiding redundancy as the adaptive
greedy one, while according a somewhat different
priority to individual sentences (weight of most im-
portant concepts versus just total weight).
6 Experiments
We chose as our input data the document sets
used in the evaluation of multidocument summa-
rization during the first Document Understanding
Conference (DUC), organized by NIST (Harman
and Marcu, 2001). This collection contains 30 test
document sets, each with approximately 10 news
stories on different events; document sets vary sig-
nificantly in their internal coherence. For each doc-
ument set three human-constructed summaries are
provided for each of the target lengths of 50, 100,
200, and 400 words. We selected DUC 2001 be-
cause ideal summaries are available for multiple
lengths.
Concepts and Textual Units Our textual units
are sentences, while the features representing con-
cepts are either atomic events, as described in Sec-
tion 4, or a fairly basic and widely used set of
lexical features, namely the list of words present
in each input text. The algorithm for extracting
event triplets assigns a weight to each such triplet,
while for words we used as weights their tf*idf val-
ues, taking idf values from http://elib.cs.
berkeley.edu/docfreq/.
Evaluation Metric Given the difficulties in com-
ing up with a universally accepted evaluation mea-
sure for summarization, and the fact that obtain-
ing judgments by humans is time-consuming and
labor-intensive, we adopted an automated pro-
cess for comparing system-produced summaries to
?ideal? summaries written by humans. The method,
ROUGE (Lin and Hovy, 2003), is based on n-gram
overlap between the system-produced and ideal
summaries. As such, it is a recall-based measure,
and it requires that the length of the summaries be
controlled to allow meaningful comparisons.
ROUGE can be readily applied to compare the
performance of different systems on the same set
of documents, assuming that ideal summaries are
available for those documents. At the same time,
ROUGE evaluation has not yet been tested exten-
sively, and ROUGE scores are difficult to interpret
as they are not absolute and not comparable across
source document sets.
50 100 200 400
events better 53.3% 63.3% 80.0% 80.0%
tf*idf better 23.3% 26.7% 20.0% 20.0%
equal 23.3% 10.0% 0.0% 0.0%
Table 2: Static greedy algorithm, events versus
tf*idf
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
DUC document sets
R
O
U
GE
 
s
c
o
re
s
events
tf*idf
Figure 1: ROUGE scores for 400-word summaries
for static greedy algorithm, events versus tf*idf
50 100 200 400
events better 53.3% 66.7% 86.7% 80.0%
tf*idf better 23.3% 20.0% 13.3% 20.0%
equal 23.3% 13.3% 0.0% 0.0%
Table 3: Adaptive greedy algorithm, events versus
tf*idf
In our comparison, we used as reference sum-
maries those created by NIST assessors for the DUC
task of generic summarization. The human annota-
tors may not have created the same models if asked
for summaries describing the major events in the in-
put texts instead of generic summaries.
Summary Length For a given set of features and
selection algorithm we get a sorted list of sen-
tences extracted according to that particular algo-
rithm. Then, for each DUC document set we create
four summaries of length 50, 100, 200, and 400. In
all the suggested methods a whole sentence is added
at every step. We extracted exactly 50, 100, 200,
and 400 words out of the top sentences (truncating
the last sentence if necessary).
6.1 Results: Static Greedy Algorithm
In our first experiment we use the static greedy al-
gorithm to create summaries of various lengths. Ta-
ble 2 shows in how many cases out of the 30 docu-
ment sets the summary created according to atomic
events receives a higher or lower ROUGE score
than the summary created according to tf*idf fea-
tures (rows ?events better? and ?tf*idf better? re-
spectively). Row equal indicates how many of the
30 cases both systems produce results with the same
ROUGE score. We chose to report the number of
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
DUC document sets
R
O
U
GE
 
s
c
o
re
s
events
tf*idf 
Figure 2: ROUGE scores for 400-word summaries
for adaptive greedy algorithm, events versus tf*idf
times each system is better rather than the average
ROUGE score in each case because ROUGE scores
depend on each particular document set.
It is clear from Table 2 that the summaries cre-
ated using atomic events are better in the majority
of cases than the summaries created using tf*idf.
Figure 1 shows ROUGE scores for 400-word sum-
maries. Although in most cases the performance of
the event-based summarizer is higher than the per-
formance based on tf*idf scores, for some docu-
ment sets tf*idf gives the better scores. This phe-
nomenon can be explained through an additional
analysis of document sets according to their inter-
nal coherence. Atomic event extraction works best
for a collection of documents with well-defined con-
stituent parts of events and where documents are
clustered around one specific major event. For such
document sets atomic events are good features for
basing the summary on. In contrast, some DUC
2001 document sets describe a succession of mul-
tiple events linked in time or of different events of
the same type (e.g., Clarence Thomas? ascendancy
to the Supreme Court, document set 7 in Figure 1,
or the history of airplane crashes, document set 30
in Figure 1). In such cases, a lot of different par-
ticipants are mentioned with only few common ele-
ments (e.g., Clarence Thomas himself). Thus, most
of the atomic events have similar low weights and
it is difficult to identify those atomic events that can
point out the most important textual units.
6.2 Results: Adaptive Greedy Algorithm
For the second experiment we used the adaptive
greedy algorithm, which accounts for information
overlap across sentences in the summary. As in
the case of the simpler static greedy algorithm, we
observe that events lead to a better performance in
most document sets than tf*idf (Table 3). Table 3
is in fact similar to Table 2, with slightly increased
numbers of document sets for which events receive
higher ROUGE scores for the 100 and 200-word
50 100 200 400
static better 0.0% 3.3% 20.0% 23.3%
adaptive better 10.0% 16.7% 26.6% 40.0%
equal 90.0% 80.0% 53.3% 36.7%
Table 4: Adaptive greedy algorithm versus static
greedy algorithm, using events as features
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
DUC document sets
R
O
U
GE
 
s
c
o
re
 
ga
in
adaptive
static
Figure 3: Gain in ROUGE scores (400-word sum-
maries) when using events instead of tf*idf for the
static and adaptive greedy algorithms
50 100 200 400
static better 3.3% 26.7% 43.3% 50.0%
adaptive better 3.3% 13.3% 30.0% 50.0%
equal 93.3% 60.0% 26.7% 0.0%
Table 5: Adaptive greedy algorithm versus static
greedy algorithm, using tf*idf as features
summaries. It is interesting to see that the differ-
ence between the ROUGE scores for the summariz-
ers based on atomic events and tf*idf features be-
comes more distinct when the adaptive greedy al-
gorithm is used; Figure 2 shows this for 400-word
summaries.
As Table 4 shows, the usage of the adaptive
greedy algorithm improves the performance of a
summarizer based on atomic events in comparison
to the static greedy algorithm. In contrast, the re-
verse is true when tf*idf is used (Table 5). Figure 3
shows the change in ROUGE scores that the intro-
duction of the adaptive algorithm offers for 400-
word summaries. This indicates that tf*idf is not
compatible with our information redundancy com-
ponent; a likely explanation is that words are corre-
lated, and the presence of an important word makes
other words in the same sentence also potentially
important, a fact not captured by the tf*idf feature.
Events, on the other hand, exhibit less of a depen-
dence on each other, since each triplet captures a
specific interaction between two entities.
6.3 Results: Modified Greedy Algorithm
In the case of the modified adaptive greedy algo-
rithm we see improvement in performance in com-
50 100 200 400
static better 43.3% 43.3% 36.7% 43.3%
modified better 43.3% 56.7% 63.3% 56.7%
equal 13.3% 0.0% 0.0% 0.0%
Table 6: Modified adaptive greedy algorithm versus
static greedy algorithm, using events as features
50 100 200 400
static better 6.7% 26.7% 36.7% 26.7%
modified better 30.0% 40.0% 56.7% 73.3%
equal 63.3% 33.3% 6.7% 0.0%
Table 7: Modified adaptive greedy algorithm versus
static greedy algorithm, using tf*idf as features
50 100 200 400
events better 56.7% 70.0% 80.0% 66.6%
tf*idf better 33.3% 30.0% 20.0% 33.3%
equal 10.0% 0.0% 0.0% 0.0%
Table 8: Modified adaptive greedy algorithm, events
versus tf*idf
parison with the summarizers using the static greedy
algorithm for both events and tf*idf (Tables 6 and
7). In other words, the prioritization of individ-
ual important concepts addresses the correlation be-
tween words and allows the summarizer to benefit
from redundancy reduction even when using tf*idf
as the features. The modified adaptive algorithm of-
fers a slight improvement in ROUGE scores over
the unmodified adaptive algorithm. Also, as Table 8
makes clear, events remain the better feature choice
over tf*idf.
6.4 Results: Comparison with DUC systems
For our final experiment we used the 30 test doc-
ument sets provided for DUC 2003 competition,
for which the summaries produced by participat-
ing summarization systems were also released. In
DUC 2003 the task was to create summaries only of
length 100.
We calculated ROUGE scores for the released
summaries created by DUC participants and com-
pared them to the scores of our system with atomic
events as features and adaptive greedy algorithm as
the filtering method. In 14 out of 30 cases our sys-
tem outperforms the median of the scores of all the
15 participating systems over that specific document
set. We view this comparison as quite encourag-
ing, as our system does not employ any of the ad-
ditional features (such as sentence position or time
information) used by the best DUC summarization
systems, nor was it adapted to the DUC domain.
Again, the suitability (and relative performance) of
the event-based summarizer varies according to the
type of documents being summarized, indicating
that using our approach for a subset of document
sets is more appropriate. For example, our system
scored below all the other systems for the docu-
ment set about a meteor shower, which included a
lot of background information and no well-defined
constituents of events. On the contrary, our sys-
tem performed better than any DUC system for the
document set describing an abortion-related murder,
where it was clear who was killed and where and
when it happened.
7 Conclusion
We have introduced atomic events as a feature that
can be automatically extracted from text and used
for summarization, and described algorithms that
utilize this feature to select sentences for the sum-
mary while minimizing the overlap of information
in the output. Our experimental results indicate that
events are indeed an effective feature, at least in
comparison with words in the input texts that form
the basis of many of current summarizers? feature
sets. With all three of our summarization algo-
rithms, we achieved a gain in performance when
using events. This gain was actually more pro-
nounced with the more sophisticated sentence se-
lection methods, establishing that events also ex-
hibit less interdependence than features based di-
rectly on words. The advantage was also larger in
longer summaries.
Our approach to defining and extracting events
can be improved in many ways. We are currently
looking at ways of matching connectors that are
similar in meaning, representing paraphrases of the
same event, and methods for detecting and prioritiz-
ing special event components such as time and loca-
tion phrases. We are also considering merging infor-
mation across many related atomic events to a more
structured representation for each event, and allow-
ing for partial matches between such structures and
input sentences.
8 Acknowledgements
We wish to thank Rocco Servedio and Mihalis
Yannakakis for valuable discussions of theoreti-
cal foundations of the set cover problem. We
also thank Kathy McKeown and Noemie Elhadad
for comments on an earlier version. This work
was supported by ARDA under Advanced Question
Answering for Intelligence (AQUAINT) project
MDA908-02-C-0008. Any opinions, findings, or
recommendations are those of the authors.
References
James Allan, Jaime Carbonell, George Dodding-
ton, Jonathan Yamron, and Yiming Yang. 1998.
Topic detection and tracking plot study: Final re-
port. In Proceedings of the DARPA Broadcast
News Transscription Workshop, April.
Regina Barzilay and Michael Elhadad. 1997. Us-
ing lexical chains for text summarization. In Pro-
ceedings of the ACL/EACL 1997 Workshop on
Intelligent Scalable Text Summarizaion, Madrid,
Spain, July.
P. B. Baxendale. 1958. Machine-made index for
technical literature?An experiment. IBM Jour-
nal of Research and Development, 2:354?361.
H. P. Edmundson. 1968. New methods in automatic
extracting. Journal of the Association for Com-
puting Machinary, 23(1):264?285, April.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
Elena Filatova and Vasileios Hatzivassiloglou.
2003. Domain-independent detection, extraction,
and labeling of atomic events. In Proceedings
of RANLP, pages 145?152, Borovetz, Bulgaria,
September.
Elena Filatova and Vasileios Hatzivassiloglou.
2004. A formal model for information selection
in multi-sentence text extraction. In Proceedings
of COLING, Geneva, Switzerland, August.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Jamie Callan. 2000. Creating and evaluating
multi-document sentence extract summaries. In
Proceedings of the 9th CIKM Conference, pages
165?172.
Donna Harman and Daniel Marcu, editors. 2001.
Proceedings of the Document Understanding
Conference (DUC). NIST, New Orleans, USA,
September.
Dorit S. Hochbaum. 1997. Approximating cov-
ering and packing problems: Set cover, vertex
cover, independent set, and related problems. In
Dorit S. Hochbaum, editor, Approximation Al-
gorithms for NP-hard Problems, pages 94?143.
PWS Publishing Company, Boston, MA.
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Pro-
ceedings of the 18th ACM SIGIR Conference,
pages 68?73, Seattle, Washington, May.
Chin-Yew Lin and Eduard Hovy. 1997. Identify-
ing topic by position. In Proceedings of the 5th
ANLP Conference, Washington, DC.
Chin-Yew Lin and Eduard Hovy. 2000. The au-
tomated acquisition of topic signatures for text
summarization. In Proceedings of the COLING
Conference, Saarbru?cken, Germany, July.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of HLT-
NAACL, Edmonton, Canada, May.
H. P. Luhn. 1958. The automatic creation of lit-
erature abstracts. IBM Journal of Research and
Development, 2(2):159?165, April.
Daniel Marcu. 1997. From discourse struc-
tures to text summaries. In Proceedings of the
ACL/EACL 1997 Workshop on Intelligent Scal-
able Text Summarizaion, pages 82?88, Madrid,
Spain, July.
E. Marsh and D. Perzanowski. 1997. MUC-7 eval-
uation of IE technology: Overview of results. In
Proceedings of MUC-7.
Kathleen R. McKeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar
Eskin. 1999. Towards multidocument sum-
marization by reformulation: Progress and
prospects. In Proceedings of AAAI.
James Pustejovsky, 2000. Events and the Seman-
tics of Opposition, pages 445?482. CSLI Publi-
cations.
Simone Teufel and Marc Moens. 1997. Sentence
extraction as a classification task. In Proceed-
ings of the ACL/EACL 1997 Workshop on Intelli-
gent Scalable Text Summarization, pages 58?65,
Madrid, Spain, July.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207?214,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Creation of Domain Templates
Elena Filatova*, Vasileios Hatzivassiloglou? and Kathleen McKeown*
*Department of Computer Science
Columbia University
{filatova,kathy}@cs.columbia.edu
?Department of Computer Science
The University of Texas at Dallas
vh@hlt.utdallas.edu
Abstract
Recently, many Natural Language Processing
(NLP) applications have improved the quality of
their output by using various machine learning tech-
niques to mine Information Extraction (IE) patterns
for capturing information from the input text. Cur-
rently, to mine IE patterns one should know in ad-
vance the type of the information that should be
captured by these patterns. In this work we pro-
pose a novel methodology for corpus analysis based
on cross-examination of several document collec-
tions representing different instances of the same
domain. We show that this methodology can be
used for automatic domain template creation. As the
problem of automatic domain template creation is
rather new, there is no well-defined procedure for
the evaluation of the domain template quality. Thus,
we propose a methodology for identifying what in-
formation should be present in the template. Using
this information we evaluate the automatically cre-
ated domain templates through the text snippets re-
trieved according to the created templates.
1 Introduction
Open-ended question-answering (QA) systems
typically produce a response containing a vari-
ety of specific facts proscribed by the question
type. A biography, for example, might contain the
date of birth, occupation, or nationality of the per-
son in question (Duboue and McKeown, 2003;
Zhou et al, 2004; Weischedel et al, 2004; Fila-
tova and Prager, 2005). A definition may contain
the genus of the term and characteristic attributes
(Blair-Goldensohn et al, 2004). A response to a
question about a terrorist attack might include the
event, victims, perpetrator and date as the tem-
plates designed for the Message Understanding
Conferences (Radev and McKeown, 1998; White
et al, 2001) predicted. Furthermore, the type of in-
formation included varies depending on context. A
biography of an actor would include movie names,
while a biography of an inventor would include the
names of inventions. A description of a terrorist
event in Latin America in the eighties is different
from the description of today?s terrorist events.
How does one determine what facts are im-
portant for different kinds of responses? Often
the types of facts that are important are hand en-
coded ahead of time by a human expert (e.g., as
in the case of MUC templates). In this paper, we
present an approach that allows a system to learn
the types of facts that are appropriate for a par-
ticular response. We focus on acquiring fact-types
for events, automatically producing a template that
can guide the creation of responses to questions
requiring a description of an event. The template
can be tailored to a specific time period or coun-
try simply by changing the document collections
from which learning takes place.
In this work, a domain is a set of events of a par-
ticular type; earthquakes and presidential elections
are two such domains. Domains can be instanti-
ated by several instances of events of that type
(e.g., the earthquake in Japan in October 2004, the
earthquake in Afghanistan in March 2002, etc.).1
The granularity of domains and instances can be
altered by examining data at different levels of de-
tail, and domains can be hierarchically structured.
An ideal template is a set of attribute-value pairs,
with the attributes specifying particular functional
roles important for the domain events.
In this paper we present a method of domain-
independent on-the-fly template creation. Our
method is completely automatic. As input it re-
quires several document collections describing do-
main instances. We cross-examine the input in-
stances, we identify verbs important for the major-
ity of instances and relationships containing these
verbs. We generalize across multiple domain in-
stances to automatically determine which of these
relations should be used in the template. We re-
port on data collection efforts and results from four
domains. We assess how well the automatically
produced templates satisfy users? needs, as man-
ifested by questions collected for these domains.
1Unfortunately, NLP terminology is not standardized
across different tasks. Two NLP tasks most close to our
research are Topic Detection and Tracking (TDT) (Fiscus
et al, 1999) and Information Extraction (IE) (Marsh and
Perzanowski, 1997). In TDT terminology, our domains are
topics and our instances are events. In IE terminology, our
domains are scenarios and our domain templates are scenario
templates.
207
2 Related Work
Our system automatically generates a template
that captures the generally most important infor-
mation for a particular domain and is reusable
across multiple instances of that domain. Decid-
ing what slots to include in the template, and what
restrictions to place on their potential fillers, is
a knowledge representation problem (Hobbs and
Israel, 1994). Templates were used in the main
IE competitions, the Message Understanding Con-
ferences (Hobbs and Israel, 1994; Onyshkevych,
1994; Marsh and Perzanowski, 1997). One of the
recent evaluations, ACE,2 uses pre-defined frames
connecting event types (e.g., arrest, release) to a
set of attributes. The template construction task
was not addressed by the participating systems.
The domain templates were created manually by
experts to capture the structure of the facts sought.
Although templates have been extensively used
in information extraction, there has been little
work on their automatic design. In the Concep-
tual Case Frame Acquisition project (Riloff and
Schmelzenbach, 1998), extraction patterns, a do-
main semantic lexicon, and a list of conceptual
roles and associated semantic categories for the
domain are used to produce multiple-slot case
frames with selectional restrictions. The system
requires two sets of documents: those relevant to
the domain and those irrelevant. Our approach
does not require any domain-specific knowledge
and uses only corpus-based statistics.
The GISTexter summarization sys-
tem (Harabagiu and Maiorano, 2002) used
statistics over an arbitrary document collection
together with semantic relations from WordNet.
The created templates heavily depend on the top-
ical relations encoded in WordNet. The template
models an input collection of documents. If there
is only one domain instance described in the input
than the template is created for this particular
instance rather than for a domain. In our work,
we learn domain templates by cross-examining
several collections of documents on the same
topic, aiming for a general domain template. We
rely on relations cross-mentioned in different
instances of the domain to automatically prioritize
roles and relationships for selection.
Topic Themes (Harabagiu and La?ca?tus?u, 2005)
used for multi-document summarization merge
various arguments corresponding to the same se-
2http://www.nist.gov/speech/tests/ace/index.htm
mantic roles for the semantically identical verb
phrases (e.g., arrests and placed under arrest).
Atomic events also model an input document
collection (Filatova and Hatzivassiloglou, 2003)
and are created according to the statistics col-
lected for co-occurrences of named entity pairs
linked through actions. GISTexter, atomic events,
and Topic Themes were used for modeling a col-
lection of documents rather than a domain.
In other closely related work, Sudo et al (2003)
use frequent dependency subtrees as measured by
TF*IDF to identify named entities and IE patterns
important for a given domain. The goal of their
work is to show how the techniques improve IE
pattern acquisition. To do this, Sudo et al con-
strain the retrieval of relevant documents for a
MUC scenario and then use unsupervised learn-
ing over descriptions within these documents that
match specific types of named entities (e.g., Ar-
resting Agency, Charge), thus enabling learning
of patterns for specific templates (e.g., the Ar-
rest scenario). In contrast, the goal of our work
is to show how similar techniques can be used to
learn what information is important for a given
domain or event and thus, should be included
into the domain template. Our approach allows,
for example, learning that an arrest along with
other events (e.g., attack) is often part of a ter-
rorist event. We do not assume any prior knowl-
edge about domains. We demonstrate that frequent
subtrees can be used not only to extract specific
named entities for a given scenario but also to
learn domain-important relations. These relations
link domain actions and named entities as well as
general nouns and words belonging to other syn-
tactic categories.
Collier (1998) proposed a fully automatic
method for creating templates for information ex-
traction. The method relies on Luhn?s (1957) idea
of locating statistically significant words in a cor-
pus and uses those to locate the sentences in which
they occur. Then it extracts Subject-Verb-Object
patterns in those sentences to identify the most
important interactions in the input data. The sys-
tem was constructed to create MUC templates for
terrorist attacks. Our work also relies on corpus
statistics, but we utilize arbitrary syntactic pat-
terns and explicitly use multiple domain instances.
Keeping domain instances separated, we cross-
examine them and estimate the importance of a
particular information type in the domain.
208
3 Our Approach to Template Creation
After reading about presidential elections in dif-
ferent countries on different years, a reader has a
general picture of this process. Later, when read-
ing about a new presidential election, the reader al-
ready has in her mind a set of questions for which
she expects answers. This process can be called
domain modeling. The more instances of a partic-
ular domain a person has seen, the better under-
standing she has about what type of information
should be expected in an unseen collection of doc-
uments discussing a new instance of this domain.
Thus, we propose to use a set of document col-
lections describing different instances within one
domain to learn the general characteristics of this
domain. These characteristics can be then used to
create a domain template. We test our system on
four domains: airplane crashes, earthquakes, pres-
idential elections, terrorist attacks.
4 Data Description
4.1 Training Data
To create training document collections we used
BBC Advanced Search3 and submitted queries of
the type ?domain title + country?. For example,
??presidential election? USA?.
In addition, we used BBC?s Advanced Search
date filter to constrain the results to different date
periods of interest. For example, we used known
dates of elections and allowed a search for articles
published up to five days before or after each such
date. At the same time for the terrorist attacks or
earthquakes domain the time constraints we sub-
mitted were the day of the event plus ten days.
Thus, we identify several instances for each of
our four domains, obtaining a document collec-
tion for each instance. E.g., for the earthquake do-
main we collected documents on the earthquakes
in Afghanistan (March 25, 2002), India (January
26, 2001), Iran (December 26, 2003), Japan (Oc-
tober 26, 2004), and Peru (June 23, 2001). Using
this procedure we retrieve training document col-
lections for 9 instances of airplane crashes, 5 in-
stances of earthquakes, 13 instances of presiden-
tial elections, and 6 instances of terrorist attacks.
4.2 Test Data
To test our system, we used document clusters
from the Topic Detection and Tracking (TDT) cor-
3http://news.bbc.co.uk/shared/bsp/search2/
advanced/news_ifs.stm
pus (Fiscus et al, 1999). Each TDT topic has a
topic label, such as Accidents or Natural Disas-
ters.4 These categories are broader than our do-
mains. Thus, we manually filtered the TDT topics
relevant to our four training domains (e.g., Acci-
dents matching Airplane Crashes). In this way, we
obtained TDT document clusters for 2 instances
of airplane crashes, 3 instances of earthquakes, 6
instances of presidential elections and 3 instances
of terrorist attacks. The number of the documents
corresponding to the instances varies greatly (from
two documents for one of the earthquakes up to
156 documents for one of the terrorist attacks).
This variation in the number of documents per
topic is typical for the TDT corpus. Many of the
current approaches of domain modeling collapse
together different instances and make the decision
on what information is important for a domain
based on this generalized corpus (Collier, 1998;
Barzilay and Lee, 2003; Sudo et al, 2003). We,
on the other hand, propose to cross-examine these
instances keeping them separated. Our goal is to
eliminate dependence on how well the corpus is
balanced and to avoid the possibility of greater
impact on the domain template of those instances
which have more documents.
5 Creating Templates
In this work we build domain templates around
verbs which are estimated to be important for the
domains. Using verbs as the starting point we
identify semantic dependencies within sentences.
In contrast to deep semantic analysis (Fillmore
and Baker, 2001; Gildea and Jurafsky, 2002; Prad-
han et al, 2004; Harabagiu and La?ca?tus?u, 2005;
Palmer et al, 2005) we rely only on corpus statis-
tics. We extract the most frequent syntactic sub-
trees which connect verbs to the lexemes used in
the same subtrees. These subtrees are used to cre-
ate domain templates.
For each of the four domains described in Sec-
tion 4, we automatically create domain templates
using the following algorithm.
Step 1: Estimate what verbs are important for
the domain under investigation. We initiate our
algorithm by calculating the probabilities for all
the verbs in the document collection for one do-
main ? e.g., the collection containing all the in-
stances in the domain of airplane crashes. We
4In our experiments we analyze TDT topics used in
TDT-2 and TDT-4 evaluations.
209
discard those verbs that are stop words (Salton,
1971). To take into consideration the distribution
of a verb among different instances of the domain,
we normalize this probability by its VIF value
(verb instance frequency), specifying in how many
domain instances this verb appears.
Score(vbi) =
countvbi?
vbj?comb coll countvbj
? VIF(vbi) (1)
VIF(vbi) = # of domain instances containing vbi# of all domain instances (2)
These verbs are estimated to be the most impor-
tant for the combined document collection for all
the domain instances. Thus, we build the domain
template around these verbs. Here are the top ten
verbs for the terrorist attack domain:
killed, told, found, injured, reported,
happened, blamed, arrested, died, linked.
Step 2: Parse those sentences which contain the
top 50 verbs. After we identify the 50 most impor-
tant verbs for the domain under analysis, we parse
all the sentences in the domain document collec-
tion containing these verbs with the Stanford syn-
tactic parser (Klein and Manning, 2002).
Step 3: Identify most frequent subtrees containing
the top 50 verbs. A domain template should con-
tain not only the most important actions for the do-
main, but also the entities that are linked to these
actions or to each other through these actions. The
lexemes referring to such entities can potentially
be used within the domain template slots. Thus,
we analyze those portions of the syntactic trees
which contain the verbs themselves plus other lex-
emes used in the same subtrees as the verbs. To do
this we use FREQuent Tree miner.5 This software
is an implementation of the algorithm presented
by (Abe et al, 2002; Zaki, 2002), which extracts
frequent ordered subtrees from a set of ordered
trees. Following (Sudo et al, 2003) we are inter-
ested only in the lexemes which are near neighbors
of the most frequent verbs. Thus, we look only for
those subtrees which contain the verbs themselves
and from four to ten tree nodes, where a node is
either a syntactic tag or a lexeme with its tag. We
analyze not only NPs which correspond to the sub-
ject or object of the verb, but other syntactic con-
stituents as well. For example, PPs can potentially
link the verb to locations or dates, and we want to
include this information into the template. Table 1
contains a sample of subtrees for the terrorist at-
tack domain mined from the sentences containing
5http://chasen.org/?taku/software/freqt/
nodes subtree
8 (SBAR(S(VP(VBD killed)(NP(QP(IN at))(NNS people)))))
8 (SBAR(S(VP(VBD killed)(NP(QP(JJS least))(NNS people)))))
5 (VP(ADVP)(VBD killed)(NP(NNS people)))
6 (VP(VBD killed)(NP(ADJP(JJ many))(NNS people)))
5 (VP(VP(VBD killed)(NP(NNS people))))
7 (VP(ADVP(NP))(VBD killed)(NP(CD 34)(NNS people)))
6 (VP(ADVP)(VBD killed)(NP(CD 34)(NNS people)))
Table 1: Sample subtrees for the terrorist attack domain.
the verb killed. The first column of Table 1 shows
how many nodes are in the subtree.
Step 4: Substitute named entities with their re-
spective tags. We are interested in analyzing a
whole domain, not just an instance of this do-
main. Thus, we substitute all the named entities
with their respective tags, and all the exact num-
bers with the tag NUMBER. We speculate that sub-
trees similar to those presented in Table 1 can
be extracted from a document collection repre-
senting any instance of a terrorist attack, with the
only difference being the exact number of causal-
ities. Later, however, we analyze the domain in-
stances separately to identity information typi-
cal for the domain. The procedure of substitut-
ing named entities with their respective tags previ-
ously proved to be useful for various tasks (Barzi-
lay and Lee, 2003; Sudo et al, 2003; Filatova and
Prager, 2005). To get named entity tags we used
BBN?s IdentiFinder (Bikel et al, 1999).
Step 5: Merge together the frequent subtrees. Fi-
nally, we merge together those subtrees which
are identical according to the information encoded
within them. This is a key step in our algorithm
which allows us to bring together subtrees from
different instances of the same domain. For exam-
ple, the information rendered by all the subtrees
from the bottom part of Table 1 is identical. Thus,
these subtrees can be merged into one which con-
tains the longest common pattern:
(VBD killed)(NP(NUMBER)(NNS people))
After this merging procedure we keep only those
subtrees for which each of the domain instances
has at least one of the subtrees from the initial set
of subtrees. This subtree should be used in the in-
stance at least twice. At this step, we make sure
that we keep in the template only the information
which is generally important for the domain rather
than only for a fraction of instances in this domain.
We also remove all the syntactic tags as we want
to make this pattern as general for the domain as
possible. A pattern without syntactic dependencies
contains a verb together with a prospective tem-
210
plate slot corresponding to this verb:
killed: (NUMBER) (NNS people)
In the above example, the prospective template
slots appear after the verb killed. In other cases the
domain slots appear in front of the verb. Two ex-
amples of such slots, for the presidential election
and earthquake domains, are shown below:
(PERSON) won
(NN earthquake) struck
The above examples show that it is not enough to
analyze only named entities, general nouns con-
tain important information as well. We term the
structure consisting of a verb together with the as-
sociated slots a slot structure. Here is a part of the
slot structure we get for the verb killed after cross-
examination of the terrorist attack instances:
killed (NUMBER) (NNS people)
(PERSON) killed
(NN suicide) killed
Slot structures are similar to verb frames, which
are manually created for the PropBank annota-
tion (Palmer et al, 2005).6 An example of the
PropBank frame for the verb to kill is:
Roleset kill.01 ?cause to die?:
Arg0:killerArg1:corpseArg2:instrument
The difference between the slot structure extracted
by our algorithm and the PropBank frame slots is
that the frame slots assign a semantic role to each
slot, while our algorithm gives either the type of
the named entity that should fill in this slot or puts
a particular noun into the slot (e.g., ORGANIZA-
TION, earthquake, people). An ideal domain tem-
plate should include semantic information but this
problem is outside of the scope of this paper.
Step 6: Creating domain templates. After we get
all the frequent subtrees containing the top 50 do-
main verbs, we merge all the subtrees correspond-
ing to the same verb and create a slot structure for
every verb as described in Step 5. The union of
such slot structures created for all the important
verbs in the domain is called the domain template.
From the created templates we remove the slots
which are used in all the domains. For example,
(PERSON) told.2
The presented algorithm can be used to create a
template for any domain. It does not require pre-
defined domain or world knowledge. We learn do-
main templates from cross-examining document
collections describing different instances of the
domain of interest.
6http://www.cs.rochester.edu/?gildea/Verbs/
6 Evaluation
The task we deal with is new and there is no well-
defined and standardized evaluation procedure for
it. Sudo et al (2003) evaluated how well their
IE patterns captured named entities of three pre-
defined types. We are interested in evaluating how
well we capture the major actions as well as their
constituent parts.
There is no set of domain templates which are
built according to a unique set of principles against
which we could compare our automatically cre-
ated templates. Thus, we need to create a gold
standard. In Section 6.1, we describe how the gold
standard is created. Then, in Section 6.2, we eval-
uate the quality of the automatically created tem-
plates by extracting clauses corresponding to the
templates and verifying how many answers from
the questions in the gold standard are answered by
the extracted clauses.
6.1 Stage 1. Information Included into
Templates: Interannotator Agreement
To create a gold standard we asked people to create
a list of questions which indicate what is important
for the domain description. Our decision to aim
for the lists of questions and not for the templates
themselves is based on the following considera-
tions: first, not all of our subjects are familiar with
the field of IE and thus, do not necessarily know
what an IE template is; second, our goal for this
evaluation is to estimate interannotator agreement
for capturing the important aspects for the domain
and not how well the subjects agree on the tem-
plate structure.
We asked our subjects to think of their expe-
rience of reading newswire articles about various
domains.7 Based on what they remember from this
experience, we asked them to come up with a list
of questions about a particular domain. We asked
them to come up with at most 20 questions cover-
ing the information they will be looking for given
an unseen news article about a new event in the
domain. We did not give them any input informa-
tion about the domain but allowed them to use any
sources to learn more information about the do-
main.
We had ten subjects, each of which created one
list of questions for one of the four domains under
7We thank Rod Adams, Cosmin-Adrian Bejan, Sasha
Blair-Goldensohn, Cyril Cerovic, David Elson, David Evans,
Ovidiu Fortu, Agustin Gravano, Lokesh Shresta, John Yundt-
Pacheco and Kapil Thadani for the submitted questions.
211
Jaccard metric
Domain subj1 and subj1 and subj2 and
subj2 (and subj3) MUC MUC
Airplane crash 0.54 - -
Earthquake 0.68 - -
Presidential Election 0.32 - -
Terrorist Attack 0.50 0.63 0.59
Table 2: Creating gold standard. Jaccard metric values for in-
terannotator agreement.
analysis. Thus, for the earthquake and terrorist at-
tack domains we got two lists of questions; for the
airplane crash and presidential election domains
we got three lists of questions.
After the questions lists were created we studied
the agreement among annotators on what infor-
mation they consider is important for the domain
and thus, should be included in the template. We
matched the questions created by different anno-
tators for the same domain. For some of the ques-
tions we had to make a judgement call on whether
it is a match or not. For example, the following
question created by one of the annotators for the
earthquake domain was:
Did the earthquake occur in a well-known area
for earthquakes (e.g. along the San Andreas
fault), or in an unexpected location?
We matched this question to the following three
questions created by the other annotator:
What is the geological localization?
Is it near a fault line?
Is it near volcanoes?
Usually, the degree of interannotator agreement
is estimated using Kappa. For this task, though,
Kappa statistics cannot be used as they require
knowledge of the expected or chance agreement,
which is not applicable to this task (Fleiss et al,
1981). To measure interannotator agreement we
use the Jaccard metric, which does not require
knowledge of the expected or chance agreement.
Table 2 shows the values of Jaccard metric for in-
terannotator agreement calculated for all four do-
mains. Jaccard metric values are calculated as
Jaccard(domaind) = |QS
d
i ? QSdj |
|QSdi ? QSdj |
(3)
where QSdi and QSdj are the sets of questions cre-
ated by subjects i and j for domain d. For the air-
plane crash and presidential election domains we
averaged the three pairwise Jaccard metric values.
The scores in Table 2 show that for some do-
mains the agreement is quite high (e.g., earth-
quake), while for other domains (e.g., presiden-
tial election) it is twice as low. This difference
in scores can be explained by the complexity of
the domains and by the differences in understand-
ing of these domains by different subjects. The
scores for the presidential election domain are pre-
dictably low as in different countries the roles of
presidents are very different: in some countries the
president is the head of the government with a lot
of power, while in other countries the president is
merely a ceremonial figure. In some countries the
presidents are elected by general voting while in
other countries, the presidents are elected by par-
liaments. These variations in the domain cause the
subjects to be interested in different issues of the
domain. Another issue that might influence the in-
terannotator agreement is the distribution of the
presidential election process in time. For example,
one of our subjects was clearly interested in the
pre-voting situation, such as debates between the
candidates, while another subject was interested
only in the outcome of the presidential election.
For the terrorist attack domain we also com-
pared the lists of questions we got from our sub-
jects with the terrorist attack template created by
experts for the MUC competition. In this template
we treated every slot as a separate question, ex-
cluding the first two slots which captured informa-
tion about the text from which the template fillers
were extracted and not about the domain. The re-
sults for this comparison are included in Table 2.
Differences in domain complexity were stud-
ied by IE researchers. Bagga (1997) suggests a
classification methodology to predict the syntac-
tic complexity of the domain-related facts. Hut-
tunen et al (2002) analyze how component sub-
events of the domain are linked together and dis-
cuss the factors which contribute to the domain
complexity.
6.2 Stage 2. Quality of the Automatically
Created Templates
In section 6.1 we showed that not all the domains
are equal. For some of the domains it is much eas-
ier to come to a consensus about what slots should
be present in the domain template than for others.
In this section we describe the evaluation of the
four automatically created templates.
Automatically created templates consist of slot
structures and are not easily readable by human
annotators. Thus, instead of direct evaluation of
the template quality, we evaluate the clauses ex-
tracted according to the created templates and
212
check whether these clauses contain the answers
to the questions created by the subjects during the
first stage of the evaluation. We extract the clauses
corresponding to the test instances according to
the following procedure:
1. Identify all the simple clauses in the docu-
ments corresponding to a particular test in-
stance (respective TDT topic). For example,
for the sentence
Her husband, Robert, survived Thursday?s
explosion in a Yemeni harbor that killed at
least six crew members and injured 35.
only one part is output:
that killed at least six crew members and
injured 35
2. For every domain template slot check all the
simple clauses in the instance (TDT topic)
under analysis. Find the shortest clause (or
sequence of clauses) which includes both the
verb and other words extracted for this slot in
their respective order. Add this clause to the
list of extracted clauses unless this clause has
been already added to this list.
3. Keep adding clauses to the list of extracted
clauses till all the template slots are analyzed
or the size of the list exceeds 20 clauses.
The key step in the above algorithm is Step 2. By
choosing the shortest simple clause or sequence
of simple clauses corresponding to a particular
template slot, we reduce the possibility of adding
more information to the output than is necessary
to cover each particular slot.
In Step 3 we keep only the first twenty clauses
so that the length of the output which potentially
contains an answer to the question of interest is not
larger than the number of questions provided by
each subject. The templates are created from the
slot structures extracted for the top 50 verbs. The
higher the estimated score of the verb (Eq. 1) for
the domain the closer to the top of the template the
slot structure corresponding to this verb will be.
We assume that the important information is more
likely to be covered by the slot structures that are
placed near the top of the template.
The evaluation results for the automatically cre-
ated templates are presented in Figure 1. We cal-
culate what average percentage of the questions is
covered by the outputs created according to the
domain templates. For every domain, we present
the percentage of the covered questions separately
for each annotator and for the intersection of ques-
tions (Section 6.1).
0.00%10.00%
20.00%30.00%
40.00%50.00%
60.00%70.00%
80.00%
Attack Earthquake Presidentialelection Plane crash
IntersectSubj1Subj2Subj3
Figure 1: Evaluation results.
For the questions common for all the annota-
tors we capture about 70% of the answers for
three out of four domains. After studying the re-
sults we noticed that for the earthquake domain
some questions did not result in a template slot
and thus, could not be covered by the extracted
clauses. Here are two of such questions:
Is it near a fault line?
Is it near volcanoes?
According to the template creation procedure,
which is centered around verbs, the chances that
extracted clauses would contain answers to these
questions are low. Indeed, only one of the three
sentence sets extracted for the three TDT earth-
quake topics contain an answer to one of these
questions.
Poor results for the presidential election domain
could be predicted from the Jaccard metric value
for interannotator agreement (Table 2). There is
considerable discrepancy in the questions created
by human annotators which can be attributed to the
great variation in the presidential election domain
itself. It must be also noted that most of the ques-
tions created for the presidential election domain
were clearly referring to the democratic election
procedure, while some of the TDT topics catego-
rized as Elections were about either election fraud
or about opposition taking over power without the
formal resignation of the previous president.
Overall, this evaluation shows that using au-
tomatically created domain templates we extract
sentences which contain a substantial part of the
important information expressed in questions for
that domain. For those domains which have small
diversity our coverage can be significantly higher.
7 Conclusions
In this paper, we presented a robust method for
data-driven discovery of the important fact-types
213
for a given domain. In contrast to supervised meth-
ods, the fact-types are not pre-specified. The re-
sulting slot structures can subsequently be used
to guide the generation of responses to questions
about new instances of the same domain. Our ap-
proach features the use of corpus statistics derived
from both lexical and syntactic analysis across
documents. A comparison of our system output
for four domains of interest shows that our ap-
proach can reliably predict the majority of infor-
mation that humans have indicated are of interest.
Our method is flexible: analyzing document col-
lections from different time periods or locations,
we can learn domain descriptions that are tailored
to those time periods and locations.
Acknowledgements. We would like to thank Re-
becca Passonneau and Julia Hirschberg for the
fruitful discussions at the early stages of this work;
Vasilis Vassalos for his suggestions on the eval-
uation instructions; Michel Galley, Agustin Gra-
vano, Panagiotis Ipeirotis and Kapil Thadani for
their enormous help with evaluation.
This material is based upon work supported
in part by the Advanced Research Devel-
opment Agency (ARDA) under Contract No.
NBCHC040040 and in part by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023. Any opinions,
findings and conclusions expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of ARDA and DARPA.
References
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura,
and Setsuo Arikawa. 2002. Optimized substructure dis-
covery for semi-structured data. In Proc. of PKDD.
Amit Bagga. 1997. Analyzing the complexity of a domain
with respect to an Information Extraction task. In Proc.
7th MUC.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of HLT/NAACL.
Daniel Bikel, Richard Schwartz, and Ralph Weischedel.
1999. An algorithm that learns what?s in a name. Ma-
chine Learning Journal Special Issue on Natural Lan-
guage Learning, 34:211?231.
Sasha Blair-Goldensohn, Kathleen McKeown, and An-
drew Hazen Schlaikjer, 2004. Answering Definitional
Questions: A Hybrid Approach. AAAI Press.
Robin Collier. 1998. Automatic Template Creation for Infor-
mation Extraction. Ph.D. thesis, University of Sheffield.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural language
generation. In Proc. of EMNLP.
Elena Filatova and Vasileios Hatzivassiloglou. 2003.
Domain-independent detection, extraction, and labeling of
atomic events. In Proc. of RANLP.
Elena Filatova and John Prager. 2005. Tell me what you do
and I?ll tell you what you are: Learning occupation-related
activities for biographies. In Proc. of EMNLP/HLT.
Charles Fillmore and Collin Baker. 2001. Frame semantics
for text understanding. In Proc. of WordNet and Other
Lexical Resources Workshop, NAACL.
Jon Fiscus, George Doddington, John Garofolo, and Alvin
Martin. 1999. NIST?s 1998 topic detection and tracking
evaluation (TDT2). In Proc. of the 1999 DARPA Broad-
cast News Workshop, pages 19?24.
Joseph Fleiss, Bruce Levin, and Myunghee Cho Paik, 1981.
Statistical Methods for Rates and Proportions. J. Wiley.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sanda Harabagiu and Finley La?ca?tus?u. 2005. Topic themes
for multi-document summarization. In Proc. of SIGIR.
Sanda Harabagiu and Steven Maiorano. 2002. Multi-docu-
ment summarization with GISTexter. In Proc. of LREC.
Jerry Hobbs and David Israel. 1994. Principles of template
design. In Proc. of the HLT Workshop.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Complexity of event structure in IE scenarios. In
Proc. of COLING.
Dan Klein and Christopher Manning. 2002. Fast exact infer-
ence with a factored model for natural language parsing.
In Proc. of NIPS.
Hans Luhn. 1957. A statistical approach to mechanized en-
coding and searching of literary information. IBM Journal
of Research and Development, 1:309?317.
Elaine Marsh and Dennis Perzanowski. 1997. MUC-7 eval-
uation of IE technology: Overview of results. In Proc. of
the 7th MUC.
Boyan Onyshkevych. 1994. Issues and methodology for
template design for information extraction system. In
Proc. of the HLT Workshop.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In Proc. of HLT/NAACL.
Dragomir Radev and Kathleen McKeown. 1998. Gener-
ating natural language summaries from multiple on-line
sources. Computational Linguistics, 24(3):469?500.
Ellen Riloff and Mark Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In Proc. of
the 6th Workshop on Very Large Corpora.
Gerard Salton, 1971. The SMART retrieval system. Prentice-
Hall, NJ.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003.
An improved extraction pattern representation model for
automatic IE pattern acquisition. In Proc. of ACL.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan, 2004. Hy-
brid Approach to Answering Biographical Questions.
AAAI Press.
Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng,
David Pierce, and Kiri Wagstaff. 2001. Multi-document
summarization via information extraction. In Proc. of
HLT.
Mohammed Zaki. 2002. Efficiently mining frequent trees in
a forest. In Proc. of SIGKDD.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004. Multi-
document biography summarization. In Proc. of EMNLP.
214
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 21?22,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Detection of Tags for Political Blogs 
 
 
Khairun-nisa Hassanali Vasileios Hatzivassiloglou 
Human Language Technology Institute Human Language Technology Institute 
The University of Texas at Dallas The University of Texas at Dallas 
Richardson, TX 75080, USA Richardson, TX 75080, USA 
nisa@hlt.utdallas.edu vh@hlt.utdallas.edu 
 
  
 
Abstract 
This paper describes a technique for automati-
cally tagging political blog posts using SVM?s 
and named entity recognition. We compare 
the quality of the tags detected by this ap-
proach to earlier approaches in other domains, 
observing effects from the political domain 
and benefits from NLP techniques comple-
mentary to the core SVM method. 
1 Introduction 
Political blogs are a particular type of communica-
tion platform that combines analyses provided by 
the blog owner or a team of regular contributors 
with shorter, but far more numerous, entries by 
visitors. Given the enthusiasm that activities for or 
against a particular politician or party can generate, 
political blogs are a vibrant part of the blogos-
phere: more than 38,500 blogs specifically dedi-
cated to politics exist in the US alone according to 
Technorati, and some of the more active ones at-
tract more than 30 million unique visitors each 
month (double that number just before major elec-
tions). 
Political blogs provide a wealth of factual in-
formation about political events and activities, but 
also by their nature are colored by strong opinions. 
They are therefore a particularly attractive target 
for semantic analysis methods using natural lan-
guage processing technology. In fact, the past two 
years have brought an increased number of colla-
borations between NLP researchers and political 
scientists using data from political sources, includ-
ing two special issues of leading political science 
journals on such topics (see (Cardie and Wilker-
son, 2008) for an overview). Our motivation for 
working with this kind of data is the construction 
of a system that collates information across blog 
posts, combines evidence to numerically rate atti-
tudes of blogs on different topics, and traces the 
evolution of these attitudes across time and in re-
sponse to events. To enable these tasks, we first 
identify the major topics that each blog post cov-
ers. In the present paper, we describe our recogniz-
er of blog post topics. We show that, perhaps 
because of the richness of political blogs in named 
entities, an SVM-based keyword learning approach 
can be complemented with named entity recogni-
tion and co-reference detection to achieve preci-
sion and recall scores higher than those reported by 
earlier topic recognition systems in other domains. 
2 Related Work  
In our approach, as in earlier published work, we 
take tags assigned by many blogs to individual 
blog posts as a reference list of the topics covered 
by that post. Tags are single words or short phras-
es, most often noun phrases, and are usually cho-
sen by each post?s authors without a controlled 
vocabulary; examples include ?Michigan?, 
?George Bush?, ?democracy?, and ?health care?. 
Earlier work in predicting tags includes (Mishne, 
2006), who adopts a collaborative filtering ap-
proach; in contrast, we rely on training classifiers 
from earlier posts in each blog. Our approach is 
more similar to (Sood et al, 2007) and (Wang and 
Davison, 2008) who use different machine learning 
techniques applied to a training set. We differ from 
the last two approaches in our addition of proper 
noun and named entity recognition methods to our 
core SVM classifiers, in our exploration of specifi-
cally political data, and in our subsequent use of 
21
the predicted tags (for semantic analysis rather 
than tag set compression or query expansion). 
3 Data 
We collected data from two major political blogs, 
Daily Kos (www.dailykos.com) and Red State 
(www.redstate.com). Red State is a conserva-
tive political blog whereas Daily Kos is a liberal 
political blog. Both these blogs are widely read and 
tag each of their blog entries. We collected data 
from both these blogs over a period of two years 
(January 2008 ? February 2010). We collected a 
total of 100,000 blog posts from Daily Kos and 
70,000 blog posts from Red State and a total of 
787,780 tags across both blogs (an average of 4.63 
tags per post). 
4 Methods 
We used SVM Light (Joachims, 2002) to predict 
the tags for a given blog post. We constructed one 
classifier for each of the tags present in the training 
set. The features used were counts of each word 
encountered in the title or the body of a post (two 
counts per word), further subdivided by whether 
the word appears in any tags in the training data or 
not, and whether it is a synonym of known tag 
words. We extract the top five proposed tags for 
each post, corresponding to the five highest scoring 
SVM classifiers. 
We also attempt to detect the main entities being 
talked about. We perform shallow parsing and ex-
tract noun phrases and then proper nouns. The 
most frequent proper NPs are probable tags. We 
also added named entity recognition and co-
reference resolution using the OpenNLP toolkit 
(maxent.sourceforge.net). We found that 
named entity recognition proposes additional use-
ful tags while the effect of co-reference resolution 
is marginal, mostly because of limited success in 
actually matching co-referent entities. 
5 Results and Evaluation 
For evaluating our methods, we used 2,681 posts 
from Daily Kos and 571 posts from Red State. We 
compared the tags assigned by our tagger to the 
original tags of the blog post, using an automated 
method (Figure 1). A tag was considered a match if 
it exactly matched the original tag or was a word 
super set ? for example ?health care system? is 
considered a match to ?health care?. We also ma-
nually evaluated the relevance of the proposed tags 
on a small portion of our test set (100 posts). 
 
Method Precision Recall F-Score 
Single word SVM 27.3% 60.3% 37.6% 
+ Stemming 26.1% 59.5% 36.3% 
   + Proper Nouns 36.5% 56.8% 44.4% 
Named Entities 48.4% 49.1% 48.7% 
All Combined 21.1% 65.0% 31.9% 
Manual Scoring 67.0% 75.0% 70.8% 
    Single word SVM 19.0% 30.0% 23.3% 
+ Stemming 22.0% 30.2% 25.5% 
   + Proper Nouns 46.3% 54.0% 49.9% 
Named Entities 60.1% 41.5% 49.1% 
All Combined 20.3% 65.7% 31.0% 
Manual Scoring 47.0% 62.0% 53.5% 
Figure 1: Results on Daily Kos (top) and Red State 
(bottom) data. Best scores in bold. 
6 Conclusion  
We described and evaluated a tool for automatical-
ly tagging political blog posts. Political blogs differ 
from other blogs as they often involve named enti-
ties (politicians, organizations, and places). There-
fore, tagging of political blog posts benefits from 
using basic name entity recognition to improve the 
tagging. The recall in particular exceeds the score 
obtained by earlier techniques applied to other do-
mains (Sood et al (2007) report precision of 13% 
and recall of 23%; Wang and Davison (2008) re-
port precision of 45% and recall of 23%).  
 
References  
Claire Cardie and John Wilkerson (editors). ?Special 
Volume: Text Annotation for Political Science Re-
search?. Journal of Information Technology and 
Politics, 5(1):1-6, 2008. 
Thorsten Joachims. SVM-Light. 2002. http://www. 
svmlight.joachims.org. 
Gilad Mishne. ?AutoTag: A Collaborative Approach to 
Automated Tag Assignment for Weblog Posts?. In 
Proceedings of WWW, 2006. 
Sanjay C. Sood, Sara H. Owsley, Kristian J. Hammond, 
and Larry Birnbaum. ?TagAssist: Automatic Tag 
Suggestion for Blog Posts?. In Proceedings of 
ICWSM, 2007. 
Jian Wang and Brian D. Davison. ?Explorations in Tag 
Suggestion and Query Expansion?. In Proceedings of 
SSM ?08, 2008. 
22
