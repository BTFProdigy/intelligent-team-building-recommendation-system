Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 335?343,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Minimal-length linearizations for mildly context-sensitive dependency trees
Y. Albert Park
Department of Computer Science and Engineering
9500 Gilman Drive
La Jolla, CA 92037-404, USA
yapark@ucsd.edu
Roger Levy
Department of Linguistics
9500 Gilman Drive
La Jolla, CA 92037-108, USA
rlevy@ling.ucsd.edu
Abstract
The extent to which the organization of nat-
ural language grammars reflects a drive to
minimize dependency length remains little
explored. We present the first algorithm
polynomial-time in sentence length for obtain-
ing the minimal-length linearization of a de-
pendency tree subject to constraints of mild
context sensitivity. For the minimally context-
sensitive case of gap-degree 1 dependency
trees, we prove several properties of minimal-
length linearizations which allow us to im-
prove the efficiency of our algorithm to the
point that it can be used on most naturally-
occurring sentences. We use the algorithm
to compare optimal, observed, and random
sentence dependency length for both surface
and deep dependencies in English and Ger-
man. We find in both languages that anal-
yses of surface and deep dependencies yield
highly similar results, and that mild context-
sensitivity affords very little reduction in min-
imal dependency length over fully projective
linearizations; but that observed linearizations
in German are much closer to random and far-
ther from minimal-length linearizations than
in English.
1 Introduction
This paper takes up the relationship between two
hallmarks of natural language dependency structure.
First, there seem to be qualitative constraints on the
relationship between the dependency structure of the
words in a sentence and their linear ordering. In par-
ticular, this relationship seems to be such that any
natural language sentence, together with its depen-
dency structure, should be generable by a mildly
context-sensitivity formalism (Joshi, 1985), in par-
ticular a linear context-free rewrite system in which
the right-hand side of each rule has a distinguished
head (Pollard, 1984; Vijay-Shanker et al, 1987;
Kuhlmann, 2007). This condition places strong con-
straints on the linear contiguity of word-word de-
pendency relations, such that only limited classes of
crossing context-free dependency structures may be
admitted.
The second constraint is a softer preference for
words in a dependency relation to occur in close
proximity to one another. This constraint is perhaps
best documented in psycholinguistic work suggest-
ing that large distances between governors and de-
pendents induce processing difficulty in both com-
prehension and production (Hawkins, 1994, 2004;
Gibson, 1998; Jaeger, 2006). Intuitively there is
a relationship between these two constraints: con-
sistently large dependency distances in a sentence
would require many crossing dependencies. How-
ever, it is not the case that crossing dependencies
always mean longer dependency distances. For ex-
ample, (1) below has no crossing dependencies, but
the distance between arrived and its dependent Yes-
terday is large. The overall dependency length of the
sentence can be reduced by extraposing the relative
clause who was wearing a hat, resulting in (2), in
which the dependency Yesterday?arrived crosses
the dependency woman?who.
(1) Yesterday a woman who was wearing a hat arrived.
(2) Yesterday a woman arrived who was wearing a hat.
335
There has been some recent work on dependency
length minimization in natural language sentences
(Gildea and Temperley, 2007), but the relationship
between the precise constraints on available lin-
earizations and dependency length minimization re-
mains little explored. In this paper, we introduce
the first efficient algorithm for obtaining lineariza-
tions of dependency trees that minimize overall de-
pendency lengths subject to the constraint of mild
context-sensitivity, and use it to investigate the rela-
tionship between this constraint and the distribution
of dependency length actually observed in natural
languages.
2 Projective and mildly non-projective
dependency-tree linearizations
In the last few years there has been a resurgence
of interest in computation on dependency-tree struc-
tures for natural language sentences, spurred by
work such as McDonald et al (2005a,b) show-
ing that working with dependency-tree syntactic
representations in which each word in the sen-
tence corresponds to a node in the dependency tree
(and vice versa) can lead to algorithmic benefits
over constituency-structure representations. The lin-
earization of a dependency tree is simply the linear
order in which the nodes of the tree occur in a sur-
face string. There is a broad division between two
classes of linearizations: projective linearizations
that do not lead to any crossing dependencies in the
tree, and non-projective linearizations that involve
at least one crossing dependency pair. Example (1),
for example, is projective, whereas Example (2) is
non-projective due to the crossing between the Yes-
terday?arrived and woman?who dependencies.
Beyond this dichotomy, however, the homomor-
phism from headed tree structures to dependency
structures (Miller, 2000) can be used together with
work on the mildly context-sensitive formalism lin-
ear context-free rewrite systems (LCFRSs) (Vijay-
Shanker et al, 1987) to characterize various classes
of mildly non-projective dependency-tree lineariza-
tions (Kuhlmann and Nivre, 2006). The LCFRSs are
an infinite sequence of classes of formalism for gen-
erating surface strings through derivation trees in a
rule-based context-free rewriting system. The i-th
LCFRS class (for i = 0, 1, 2, . . . ) imposes the con-
Figure 1: Sample dependency subtree for Figure 2
straint that every node in the derivation tree maps to
to a collection of at most i+1 contiguous substrings.
The 0-th class of LCFRS, for example, corresponds
to the context-free grammars, since each node in the
derivation tree must map to a single contiguous sub-
string; the 1st class of LCFRS corresponds to Tree-
Adjoining Grammars (Joshi et al, 1975), in which
each node in the derivation tree must map to at most
a pair of contiguous substrings; and so forth. The
dependency trees induced when each rewrite rule in
an i-th order LCFRS distinguish a unique head can
similarly be characterized by being of gap-degree i,
so that i is the maximum number of gaps that may
appear between contiguous substrings of any subtree
in the dependency tree (Kuhlmann and Mo?hl, 2007).
The dependency tree for Example (2), for example,
is of gap-degree 1. Although there are numerous
documented cases in which projectivity is violated
in natural language, there are exceedingly few doc-
umented cases in which the documented gap degree
exceeds 1 (though see, for example, Kobele, 2006).
3 Finding minimal dependency-length
linearizations
Even under the strongest constraint of projectivity,
the number of possible linearizations of a depen-
dency tree is exponential in both sentence length
and arity (the maximum number of dependencies
for any word). As pointed out by Gildea and Tem-
perley (2007), however, finding the unconstrained
minimal-length linearization is a well-studied prob-
lem with an O(n1.6) solution (Chung, 1984). How-
ever, this approach does not take into account con-
straints of projectivity or mild context-sensitivity.
Gildea and Temperley themselves introduced a
novel efficient algorithm for finding the minimized
dependency length of a sentence subject to the con-
straint that the linearization is projective. Their al-
gorithm can perhaps be most simply understood by
making three observations. First, the total depen-
336
Figure 2: Dependency length factorization for efficient
projective linearization, using the dependency subtree of
Figure 1
dency length of a projective linearization can be
written as
?
wi
?
??D(wi, Ei) +
?
wj
dep?wi
D(wi, Ej)
?
?? (1)
where Ei is the boundary of the contiguous substring
corresponding to the dependency subtree rooted at
wi which stands between wi and its governor, and
D(wi, Ej) is the distance from wi to Ej , with the
special case of D(wroot, Eroot) = 0 (Figures 1
and 2). Writing the total dependency length this
way makes it clear that each term in the outer sum
can be optimized independently, and thus one can
use dynamic programming to recursively find op-
timal subtree orderings from the bottom up. Sec-
ond, for each subtree, the optimal ordering can be
obtained by placing dependent subtrees on alternat-
ing sides of w from inside out in order of increas-
ing length. Third, the total dependency lengths be-
tween any words withing an ordering stays the same
when the ordering is reversed, letting us assume that
D(wi, Ei) will be the length to the closest edge.
These three observations lead to an algorithm with
worst-case complexity of O(n log m) time, where
n is sentence length and m is sentence arity. (The
log m term arises from the need to sort the daugh-
ters of each node into descending order of length.)
When limited subclasses of nonprojectivity are
admitted, however, the problem becomes more diffi-
cult because total dependency length can no longer
be written in such a simple form as in Equation (1).
Intuitively, the size of the effect on dependency
length of a decision to order a given subtree discon-
tiguously, as in a woman. . . who was wearing a hat
in Example (2), cannot be calculated without con-
sulting the length of the string that the discontiguous
kh|c1| |c2|
hd12 d11 d21 d22d31d32
Figure 3: Factorizing dependency length at node w i of
a mildly context-sensitive dependency tree. This partial
linearization of head with dependent components makes
c1 the head component and leads to l = 2 links crossing
between c1 and c2.
subtree would be wrapped around. Nevertheless, for
any limited gap degree, it is possible to use a dif-
ferent factorization of dependency length that keeps
computation polynomial in sentence length. We in-
troduce this factorization in the next section.
4 Minimization with limited gap degree
We begin by defining some terms. We use the word
component to refer to a full linearization of a sub-
tree in the case where it is realized as a single con-
tiguous string, or to refer to any of of the contigu-
ous substrings produced when a subtree is realized
discontiguously. We illustrate the factorization for
gap-degree 1, so that any subtree has at most two
components. We refer to the component contain-
ing the head of the subtree as the head component,
the remaining component as the dependent compo-
nent, and for any given (head component, depen-
dent component) pair, we use pair component to re-
fer to the other component in the pair. We refer to
the two components of dependent dj as dj1 and dj2
respectively, and assume that dj1 is the head com-
ponent. When dependencies can cross, total depen-
dency length cannot be factorized as simply as in
Equation (1) for the projective case. However, we
can still make use of a more complex factorization
of the total dependency length as follows:
?
wi
?
??D(wi, Ei) +
?
wj
dep?wi
[
D(wi, Ej) + ljkj]
?
??
(2)
where lj is the number of links crossing between the
two components of dj , and kj is the distance added
between these two components by the partial lin-
earization at wi. Figure 3 illustrates an example of
337
such a partial linearization, where k2 is |d31|+ |d32|
due to the fact that the links between d21 and d22
have to cross both components of d3. The factor-
ization in Equation (2) allows us to use dynamic
programming to find minimal-length linearizations,
so that worst-case complexity is polynomial rather
than exponential in sentence length. However, the
additional term in the factorization means that we
need to track the number of links l crossing between
the two components of the subtree Si headed by wi
and the component lengths |c1| and |c2|. Addition-
ally, the presence of crossing dependencies means
that Gildea and Temperley?s proof that ordering de-
pendent components from the inside out in order
of increasing length no longer goes through. This
means that at each node wi we need to hold on to the
minimal-length partial linearization for each combi-
nation of the following quantities:
? |c2| (which also determines |c1|);
? the number of links l between c1 and c2;
? and the direction of the link between wi and its
governor.
We shall refer to a combination of these factors
as a status set. The remainder of this section de-
scribes a dynamic-programming algorithm for find-
ing optimal linearizations based on the factorization
in Equation (2), and continues with several further
findings leading to optimizations that make the al-
gorithm tractable for naturally occurring sentences.
4.1 Algorithm 1
Our first algorithm takes a tree and recursively finds
the optimal orderings for each possible status set of
each of its child subtrees, which it then uses to cal-
culate the optimal ordering of the tree. To calcu-
late the optimal orderings for each possible status
set of a subtree S, we use the brute-force method
of choosing all combinations of one status set from
each child subtree, and for each combination, we try
all possible orderings of the components of the child
subtrees, calculate all possible status sets for S, and
store the minimal dependency value for each appear-
ing status set of S. The number of possible length
pairings |c1|, |c2| and number of crossing links l
are each bounded above by the sentence length n,
so that the maximum number of status sets at each
node is bounded above by n2. Since the sum of the
status sets of all child subtrees is also bounded by
n2, the maximum number of status set combinations
is bounded by (n2m )m (obtainable from the inequal-
ity of arithmetic and geometric means). There are
(2m+1)!m possible arrangements of head word and
dependent components into two components. Since
there are n nodes in the tree and each possible com-
bination of status sets from each dependent sub tree
must be tried, this algorithm has worst-case com-
plexity of O((2m + 1)!mn(n2m )m). This algorithm
could be generalized for mildly context-sensitive
linearizations polynomial in sentence length for any
gap degree desired, by introducing additional l terms
denoting the number of links between pairs of com-
ponents. However, even for gap degree 1 this bound
is incredibly large, and as we show in Figure 7, al-
gorithm 1 is not computationally feasible for batch
processing sentences of arity greater than 5.
4.2 Algorithm 2
We now show how to speed up our algorithm by
proving by contradiction that for any optimal or-
dering which minimizes the total dependency length
with the two-cluster constraint, for any given sub-
tree S and its child subtree C , the pair components
c1 and c2 of a child subtree C must be placed on
opposite sides of the head h of subtree S.
Let us assume that for some dependency tree
structure, there exists an optimal ordering where c1
and c2 are on the same side of h. Let us refer to the
ordered set of words between c1 and c2 as v. None of
the words in v will have dependency links to any of
the words in c1 and c2, since the dependencies of the
words in c1 and c2 are either between themselves or
the one link to h, which is not between the two com-
ponents by our assumption. There will be j1 ? 0
links from v going over c1, j2 ? 0 dependency links
from v going over c2, and l ? 1 links between c1 and
c2. Without loss of generality, let us assume that h is
on the right side of c2. Let us consider the effect on
total dependency length of swapping c1 with v, so
that the linear ordering is v c1 c2 ? h. The total de-
pendency length of the new word ordering changes
by?j1|c1|?l|v|+j2|c1| if c2 is the head component,
and decreases by another |v| if c1 is the head com-
ponent. Thus the total change in dependency length
338
is less than or equal to
(j2 ? j1)|c1| ? l ? |v| < (j2 ? j1)|c1| (3)
If instead we swap places of v with c2 instead of c1
so that we have c1 c2 v ? h, we find that the total
change in dependency length is less than or equal to
(j1 ? j2)|c2| ? (l ? 1)|v| ? (j1 ? j2)|c2| (4)
It is impossible for the right-hand sides of (3) and (4)
to be positive at the same time, so swapping v with
either c1 or c2 must lead to a linearization with lower
overall dependency length. But this is a contradic-
tion to our original assumption, so we see that for
any optimal ordering, all split child subtree compo-
nents c1 and c2 of the child subtree of S must be
placed on opposite sides of the head h.
This constraint allows us to simplify our algo-
rithm for finding the minimal-length linearization.
Instead of going through all logically possible or-
derings of components of the child subtrees, we can
now decide on which side the head component will
be on, and go through all possible orderings for each
side. This changes the factorial part of our algorithm
run time from (2m + 1)!m to 2m(m!)2m, giving us
O(2m(m!)2mn(n2m )m), greatly reducing actual pro-
cessing time.
4.3 Algorithm 3
We now present two more findings for further in-
creasing the efficiency of the algorithm. First, we
look at the status sets which need to be stored for the
dynamic programming algorithm. In the straightfor-
ward approach we first presented, we stored the op-
timal dependency lengths for all cases of possible
status sets. We now know that we only need to con-
sider cases where the pair components are on op-
posite sides. This means the direction of the link
from the head to the parent will always be toward
the inside direction of the pair components, so we
can re-define the status set as (p, l) where p is again
the length of the dependent component, and l is the
number of links between the two pair components.
If the p values for sets s1 and s2 are equal, s1 has
a smaller number of links than s2 (ls1 ? ls2) and
s1 has a smaller or equal total dependency length
to s2, then replacing the components of s2 with s1
will always give us the same or more optimal total
Figure 4: Initial setup for latter part of optimization proof
in section 4.4. To the far left is the head h of subtree S.
The component pair C1 and C2 makes up S, and g is the
governor of h. The length of the substring v between C 1
and C2 is k. ci and ci+1 are child subtree components.
dependency length. Thus, we do not have to store
instances of these cases for our algorithm.
Next, we prove by contradiction that for any two
status sets s1 and s2, if ps1 > ps2 > 0, ls1 = ls2 , and
the TOTAL INTERNAL DEPENDENCY LENGTH t1 of
s1?defined as the sum in Equation (2) over only
those words inside the subtree headed by h?is less
than or equal to t2 of s2, then using s1 will be at least
as good as s2, so we can ignore s2. Let us suppose
that the optimal linearization can use s2 but not s1.
Then in the optimal linearization, the two pair com-
ponents cs2,1 and cs2,2 of s2 are on opposite sides
of the parent head h. WLOG, let us assume that
components cs1,1 and cs2,1 are the dependent com-
ponents. Let us denote the total number of links go-
ing over cs2,1 as j1 and the words between cs2,1 and
cs2,2 as v (note that v must contain h). If we swap
cs2,1 with v, so that cs2,1 lies adjacent to cs2,2, then
there would be j2+1 links going over cs2,1. By mov-
ing cs2,1 from opposite sides of the head to be right
next to cs2,2, the total dependency length of the sen-
tence changes by?j1|cs2,1|? ls2|v|+(j2+1)|cs2,1|.
Since the ordering was optimal, we know that
(j2 ? j1 + 1)|cs2,1| ? ls2 |v| ? 0
Since l > 0, we can see that j1 ? j2 ? 0. Now, in-
stead of swapping v with cs2,1, let us try substituting
the components from s1 instead of s2. The change
of the total dependency length of the sentence will
be:
j1 ? (|cs1,1| ? |cs2,1|) + j2 ? (|cs1,2|
?|cs2,2|) + t1 ? t2
= (j1 ? j2)? (ps1 ? ps2) + (t1 ? t2)
Since j1 ? j2 ? 0 and ps1 > ps2 , the first term
is less than or equal to 0 and since t1 ? t2 ? 0, the
total dependency length will have been be equal or
339
Figure 5: Moving ci+1 to C1
Figure 6: Moving ci to C2
have decreased. But this contradicts our assumption
that only s2 can be part of an optimal ordering.
This finding greatly reduces the number of sta-
tus sets we need to store and check higher up in
the algorithm. The worst-case complexity remains
O(2mm!2mn(n2m )m), but the actual runtime is re-
duced by several orders of magnitude.
4.4 Algorithm 4
Our last optimization is on the ordering among the
child subtree components on each side of the sub-
tree head h. The initially proposed algorithm went
through all combinations of possible orderings to
find the optimal dependency length for each status
set. By the first optimization in section 4.2 we have
shown that we only need to consider the orderings
in which the components are on opposite sides of
the head. We now look into the ordering of the com-
ponents on each side of the head. We first define the
rank value r for each component c as follows:
|c|
# links between c and its pair component+I(c)
where I(c) is the indicator function having value 1 if
c is a head component and 0 otherwise . Using this
definition, we prove by contradiction that the order-
ing of the components from the head outward must
be in order of increasing rank value.
Let us suppose that at some subtree S headed by
h and with head component C1 and dependent com-
ponent C2, there is an optimal linearization in which
there exist two components ci and ci+1 of immedi-
ate subtrees of S such that ci is closer to h, the com-
1 2 3 4 5 6 7
100
102
104
106
maximum number of dependencies per head
tim
e(m
s)
Execution times for algorithms 1 & 4
 
 
Algorithm 1
Algorithm 4
Figure 7: Timing comparison of first and fully optimized
algorithms
ponents have rank values ri and ri+1 respectively,
ri > ri+1, and no other component of the imme-
diate subtrees of S intervenes between ci and ci+1.
We shall denote the number of links between each
component and its pair component as li, li+1. Let
l?i = li + I(ci) and l?i+1 = li+1 + I(ci+1). There
are two cases to consider: either (1) ci and ci+1 are
within the same component of S, or (2) ci is at the
edge of C1 nearest C2 and ci+1 is at the edge of C2
neareast C1.
Consider case 1, and let us swap ci with ci+1; this
affects only the lengths of links involving connec-
tions to ci or ci+1. The total dependency length of
the new linearization will change by
?l?i+1|ci|+ l?i|ci+1| = ?l?il?i+1(ri ? ri+1) < 0
This is a contradiction to the assumption that we had
an optimal ordering.
Now consider case 2, which is illustrated in Fig-
ure 4. We denote the number of links going over
ci and ci+1, excluding links to ci, ci+1 as ?1 and
?2 respectively, and the length of words between
the edges of C1 and C2 as k. Let us move ci+1
to the outermost position of C1, as shown in Fig-
ure 5. Since the original linearization was optimal,
we have:
??2|ci+1|+ ?1|ci+1| ? l?i+1k ? 0
(?1 ? ?2)|ci+1| ? l?i+1k
(?1 ? ?2)ri+1 ? k
Let us also consider the opposite case of mov-
ing ci to the inner edge of C2, as shown in Fig-
ure 6. Once again due to optimality of the original
linearization, we have
340
DLA English GermanSurface Deep Surface Deep
Optimal with one crossing dependency 32.7 33.0 24.5 23.3
Optimal with projectivity constraint 34.1 34.4 25.5 24.2
Observed 46.6 48.0 43.6 43.1
Random with projectivity constraint 82.4 82.8 50.6 49.2
Random with two-cluster constraint 84.0 84.3 50.7 49.5
Random ordering with no constraint 183.2 184.2 106.9 101.1
Table 1: Average sentence dependency lengths(with max arity of 10)
??1|ci|+ ?2|ci|+ l?ik ? 0
(?2 ? ?1)|ci| ? ?l?ik
(?1 ? ?2)ri ? k
But this is a contradiction, since ri > ri+1. Com-
bining the two cases, we can see that regardless of
where the components may be split, in an optimal
ordering the components going outwards from the
head must have an increasing rank value.
This result allows us to simplify our algorithm
greatly, because we no longer need to go through
all combinations of orderings. Once it has been de-
cided which components will come on each side of
the head, we can sort the components by rank value
and place them from the head out. This reduces the
factorial component of the algorithm?s complexity
to m log m, and the overall worst-case complexity
to O(nm2 log m(2n2m )m). Although this is still ex-
ponential in the arity of the tree, nearly all sentences
encountered in treebanks have an arity low enough
to make the algorithm tractable and even very effi-
cient, as we show in the following section.
5 Empirical results
Using the above algorithm, we calculated minimal
dependency lengths for English sentences from the
WSJ portion of the Penn Treebank, and for German
sentences from the NEGRA corpus. The English-
German comparison is of interest because word or-
der is freer, and crossing dependencies more com-
mon, in German than in English (Kruijff and Va-
sishth, 2003). We extracted dependency trees from
these corpora using the head rules of Collins (1999)
for English, and the head rules of Levy and Man-
ning (2004) for German. Two dependency trees
were extracted from each sentence, the surface tree
extracted by using the head rules on the context-
free tree representation (i.e. no crossing dependen-
cies), and the deep tree extracted by first return-
ing discontinuous dependents (marked by *T* and
*ICH* in WSJ, and by *T* in the Penn-format ver-
sion of NEGRA) before applying head rules. Fig-
ure 7 shows the average time it takes to calculate
the minimal dependency length with crossing depen-
dencies for WSJ sentences using the unoptimized al-
gorithm of Section 4.1 and the fully optimized al-
gorithm of Section 4.4. Timing tests were imple-
mented and performed using Java 1.6.0 10 on a sys-
tem running Linux 2.6.18-6-amd64 with a 2.0 GHz
Intel Xeon processor and 16 gigs of memory, run on
a single core. We can see from Figure 7 that the
straight-forward dynamic programming algorithm
takes many more magnitudes of time than our op-
timized algorithm, making it infeasible to calculate
the minimal dependency length for larger sentences.
The results we present below were obtained with the
fully optimized algorithm from the sentences with
a maximum arity of 10, using 49,176 of the 49,208
WSJ sentences and 20,563 of the 20,602 NEGRA
sentences.
Summary results over all sentences from each cor-
pus are shown in Table 1. We can see that for both
corpora, the oberved dependency length is smaller
than the dependency length of random orderings,
even when the random ordering is subject to the
projectivity constraint. Relaxing the projectivity
constraint by allowing crossing dependencies intro-
duces a slightly lower optimal dependency length.
The average sentence dependency lengths for the
three random orderings are significantly higher than
the observed values. It is interesting to note that the
random orderings given the projectivity constraint
and the two-cluster constraint have very similar de-
pendency lengths, where as a total random ordering
341
0 10 20 30 40 50
0
10
0
20
0
30
0
40
0
English/Surface
Sentence length
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
0 10 20 30 40 50
0
10
0
20
0
30
0
40
0
English/Deep
Sentence length
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
0 10 20 30 40 50
0
10
0
20
0
30
0
40
0
German/Surface
Sentence length
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
0 10 20 30 40 50
0
10
0
20
0
30
0
40
0
German/Deep
Sentence length
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
Figure 8: Average sentence DL as a function of sentence length. Legend is ordered top curve to bottom curve.
1 2 3 4 5 6 7 8
0
10
0
20
0
30
0
40
0
English/Surface
Sentence Arity
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
1 2 3 4 5 6 7 8
0
10
0
20
0
30
0
40
0
English/Deep
Sentence Arity
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
1 2 3 4 5 6 7 8
0
10
0
20
0
30
0
40
0
German/Surface
Sentence Arity
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
1 2 3 4 5 6 7 8
0
10
0
20
0
30
0
40
0
German/Deep
Sentence Arity
Av
er
ag
e 
se
nt
en
ce
 D
L
Unconstrained Random
2?component Random
Projective Random
Observed
Projective Optimal
2?component Optimal
Figure 9: Average sentence DL as a function of sentence arity. Legend is ordered top curve to bottom curve.
increases the dependency length significantly.
NEGRA generally has shorter sentences than
WSJ, so we need a more detailed picture of depen-
dency length as a function of sentence length; this
is shown in Figure 8. As in Table 1, we see that
English, which has less crossing dependency struc-
tures than German, has observed DL closer to opti-
mal DL and farther from random DL. We also see
that the random and observed DLs behave very sim-
ilarly across different sentence lengths in English
and German, but observed DL grows faster in Ger-
man. Perhaps surprisingly, optimal projective DL
and gap-degree 1 DL tend to be very similar even
for longer sentences. The picture as a function of
sentence arity is largely the same (Figure 9).
6 Conclusion
In this paper, we have presented an efficient dynamic
programming algorithm which finds minimum-
length dependency-tree linearizations subject to
constraints of mild context-sensitivity. For the gap-
degree 1 case, we have proven several properties of
these linearizations, and have used these properties
to optimize our algorithm. This made it possible to
find minimal dependency lengths for sentences from
the English Penn Treebank WSJ and German NE-
GRA corpora. The results show that for both lan-
guages, using surface dependencies and deep de-
pendencies lead to generally similar conclusions,
but that minimal lengths for deep dependencies are
consistently slightly higher for English and slightly
lower for German. This may be because German
has many more crossing dependencies than English.
Another finding is that the difference between aver-
age sentence DL does not change much between op-
timizing for the projectivity constraint and the two-
cluster constraint: projectivity seems to give nat-
ural language almost all the flexibility it needs to
minimize DL. For both languages, the observed lin-
earization is much closer in DL to optimal lineariza-
tions than to random linearizations; but crucially, we
see that English is closer to the optimal linearization
and farther from random linearization than German.
This finding is resonant with the fact that German
has richer morphology and overall greater variability
in observed word order, and with psycholinguistic
results suggesting that dependencies of greater lin-
ear distance do not always pose the same increased
processing load in German sentence comprehension
as they do in English (Konieczny, 2000).
342
References
Chung, F. R. K. (1984). On optimal linear arrange-
ments of trees. Computers and Mathematics with
Applications, 10:43?60.
Collins, M. (1999). Head-Driven Statistical Models
for Natural Language Parsing. PhD thesis, Uni-
versity of Pennsylvania.
Gibson, E. (1998). Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68:1?76.
Gildea, D. and Temperley, D. (2007). Optimizing
grammars for minimum dependency length. In
Proceedings of ACL.
Hawkins, J. A. (1994). A Performance Theory of
Order and Constituency. Cambridge.
Hawkins, J. A. (2004). Efficiency and Complexity in
Grammars. Oxford University Press.
Jaeger, T. F. (2006). Redundancy and Syntactic Re-
duction in Spontaneous Speech. PhD thesis, Stan-
ford University, Stanford, CA.
Joshi, A. K. (1985). How much context-sensitivity
is necessary for characterizing structural descrip-
tions ? Tree Adjoining Grammars. In Dowty,
D., Karttunen, L., and Zwicky, A., editors, Nat-
ural Language Processing ? Theoretical, Com-
putational, and Psychological Perspectives. Cam-
bridge.
Joshi, A. K., Levy, L. S., and Takahashi, M. (1975).
Tree adjunct grammars. Journal of Computer and
System Sciences, 10(1).
Kobele, G. M. (2006). Generating Copies: An inves-
tigation into Structural Identity in Language and
Grammar. PhD thesis, UCLA.
Konieczny, L. (2000). Locality and parsing com-
plexity. Journal of Psycholinguistic Research,
29(6):627?645.
Kruijff, G.-J. M. and Vasishth, S. (2003). Quantify-
ing word order freedom in natural language: Im-
plications for sentence processing. Proceedings of
the Architectures and Mechanisms for Language
Processing conference.
Kuhlmann, M. (2007). Dependency Structures and
Lexicalized Grammars. PhD thesis, Saarland Uni-
versity.
Kuhlmann, M. and Mo?hl, M. (2007). Mildly
context-sensitive dependency languages. In Pro-
ceedings of ACL.
Kuhlmann, M. and Nivre, J. (2006). Mildly non-
projective dependency structures. In Proceedings
of COLING/ACL.
Levy, R. and Manning, C. (2004). Deep depen-
dencies from context-free statistical parsers: cor-
recting the surface dependency approximation. In
Proceedings of ACL.
McDonald, R., Crammer, K., and Pereira, F.
(2005a). Online large-margin training of depen-
dency parsers. In Proceedings of ACL.
McDonald, R., Pereira, F., Ribarov, K., and Hajic?,
J. (2005b). Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of
ACL.
Miller, P. (2000). Strong Generative Capacity: The
Semantics of Linguistic Formalism. Cambridge.
Pollard, C. (1984). Generalized Phrase Structure
Grammars, Head Grammars, and Natural Lan-
guages. PhD thesis, Stanford.
Vijay-Shanker, K., Weir, D. J., and Joshi, A. K.
(1987). Characterizing structural descriptions
produced by various grammatical formalisms. In
Proceedings of ACL.
343
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 934?944,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automated Whole Sentence Grammar Correction Using a Noisy Channel
Model
Y. Albert Park
Department of Computer Science and Engineering
9500 Gilman Drive
La Jolla, CA 92037-404, USA
yapark@ucsd.edu
Roger Levy
Department of Linguistics
9500 Gilman Drive
La Jolla, CA 92037-108, USA
rlevy@ucsd.edu
Abstract
Automated grammar correction techniques
have seen improvement over the years, but
there is still much room for increased perfor-
mance. Current correction techniques mainly
focus on identifying and correcting a specific
type of error, such as verb form misuse or
preposition misuse, which restricts the correc-
tions to a limited scope. We introduce a novel
technique, based on a noisy channel model,
which can utilize the whole sentence context
to determine proper corrections. We show
how to use the EM algorithm to learn the pa-
rameters of the noise model, using only a data
set of erroneous sentences, given the proper
language model. This frees us from the bur-
den of acquiring a large corpora of corrected
sentences. We also present a cheap and effi-
cient way to provide automated evaluation re-
sults for grammar corrections by using BLEU
and METEOR, in contrast to the commonly
used manual evaluations.
1 Introduction
The process of editing written text is performed by
humans on a daily basis. Humans work by first
identifying the writer?s intent, and then transform-
ing the text so that it is coherent and error free. They
can read text with several spelling errors and gram-
matical errors and still easily identify what the au-
thor originally meant to write. Unfortunately, cur-
rent computer systems are still far from such ca-
pabilities when it comes to the task of recogniz-
ing incorrect text input. Various approaches have
been taken, but to date it seems that even many
spell checkers such as Aspell do not take context
into consideration, which prevents them from find-
ing misspellings which have the same form as valid
words. Also, current grammar correction systems
are mostly rule-based, searching the text for de-
fined types of rule violations in the English gram-
mar. While this approach has had some success in
finding various grammatical errors, it is confined to
specifically defined errors.
In this paper, we approach this problem by mod-
eling various types of human errors using a noisy
channel model (Shannon, 1948). Correct sentences
are produced by a predefined generative proba-
bilistic model, and lesioned by the noise model.
We learn the noise model parameters using an
expectation-maximization (EM) approach (Demp-
ster et al, 1977; Wu, 1983). Our model allows us
to deduce the original intended sentence by looking
for the the highest probability parses over the entire
sentence, which leads to automated whole sentence
spelling and grammar correction based on contex-
tual information.
In Section 2, we discuss previous work, followed
by an explanation of our model and its implementa-
tion in Sections 3 and 4. In Section 5 we present
a novel technique for evaluating the task of auto-
mated grammar and spelling correction, along with
the data set we collected for our experiments. Our
experiment results and discussion are in Section 6.
Section 7 concludes this paper.
2 Background
Much of the previous work in the domain of auto-
mated grammar correction has focused on identi-
934
fying grammatical errors. Chodorow and Leacock
(2000) used an unsupervised approach to identify-
ing grammatical errors by looking for contextual
cues in a ?2 word window around a target word.
To identify errors, they searched for cues which did
not appear in the correct usage of words. Eeg-
olofsson and Knutsson (2003) used rule-based meth-
ods to approach the problem of discovering preposi-
tion and determiner errors of L2 writers, and var-
ious classifier-based methods using Maximum En-
tropy models have also been proposed (Izumi et al,
2003; Tetreault and Chodorow, 2008; De Felice and
Pulman, 2008). Some classifier-based methods can
be used not only to identify errors, but also to deter-
mine suggestions for corrections by using the scores
or probabilities from the classifiers for other possi-
ble words. While this is a plausible approach for
grammar correction, there is one fundamental dif-
ference between this approach and the way humans
edit. The output scores of classifiers do not take into
account the observed erroneous word, changing the
task of editing into a fill-in-the-blank selection task.
In contrast, editing makes use of the writer?s erro-
neous word which often encompasses information
neccessary to correctly deduce the writer?s intent.
Generation-based approaches to grammar correc-
tion have also been taken, such as Lee and Sen-
eff (2006), where sentences are paraphrased into an
over-generated word lattice, and then parsed to se-
lect the best rephrasing. As with the previously men-
tioned approaches, these approaches often have the
disadvantage of ignoring the writer?s selected word
when used for error correction instead of just error
detection.
Other work which relates to automated grammar
correction has been done in the field of machine
translation. Machine translation systems often gen-
erate output which is grammatically incorrect, and
automated post-editing systems have been created to
address this problem. For instance, when translat-
ing Japanese to English, the output sentence needs
to be edited to include the correct articles, since the
Japanese language does not contain articles. Knight
and Chander (1994) address the problem of select-
ing the correct article for MT systems. These types
of systems could also be used to facilitate grammar
correction.
While grammar correction can be used on the out-
put of MT systems, note that the task of grammar
correction itself can also be thought of as a machine
translation task, where we are trying to ?translate? a
sentence from an ?incorrect grammar? language to
a ?correct grammar? language. Under this idea, the
use of statistical machine translation techniques to
correct grammatical errors has also been explored.
Brockett et al (2006) uses phrasal SMT techniques
to identify and correct mass noun errors of ESL stu-
dents. De?silets and Hermet (2009) use a round-trip
translation from L2 to L1 and back to L2 to cor-
rect errors using an SMT system, focusing on errors
which link back to the writer?s native language.
Despite the underlying commonality between the
tasks of machine translation and grammar correc-
tion, there is a practical difference in that the field
of grammar correction suffers from a lack of good
quality parallel corpora. While machine translation
has taken advantage of the plethora of translated
documents and books, from which various corpora
have been built, the field of grammar correction does
not have this luxury. Annotated corpora of gram-
matical errors do exist, such as the NICT Japanese
Learner of English corpus and the Chinese Learner
English Corpus (Shichun and Huizhong, 2003), but
the lack of definitive corpora often makes obtaining
data for use in training models a task within itself,
and often limits the approaches which can be taken.
Using classification or rule-based systems for
grammatical error detection has proven to be suc-
cessful to some extent, but many approaches are not
sufficient for real-world automated grammar correc-
tion for various of reasons. First, as we have already
mentioned, classification systems and generation-
based systems do not make full use of the given
data when trying to make a selection. This limits the
system?s ability to make well-informed edits which
match the writer?s original intent. Second, many of
the systems start with the assumption that there is
only one type of error. However, ESL students often
make several combined mistakes in one sentence.
These combined mistakes can throw off error detec-
tion/correction schemes which assume that the rest
of the sentence is correct. For example, if a student
erroneously writes ?much poeple? instead of ?many
people?, a system trying to correct ?many/much? er-
rors may skip correction of much to many because it
does not have any reference to the misspelled word
935
?poeple?. Thus there are advantages in looking at the
sentence as a whole, and creating models which al-
low several types of errors to occur within the same
sentence. We now present our model, which sup-
ports the addition of various types of errors into one
combined model, and derives its response by using
the whole of the observed sentence.
3 Base Model
Our noisy channel model consists of two main com-
ponents, a base language model and a noise model.
The base language model is a probabilistic lan-
guage model which generates an ?error-free? sen-
tence1 with a given probability. The probabilistic
noise model then takes this sentence and decides
whether or not to make it erroneous by inserting
various types of errors, such as spelling mistakes,
article choice errors, wordform choice errors, etc.,
based on its parameters (see Figure 1 for example).
Using this model, we can find the posterior proba-
bility p(Sorig|Sobs) using Bayes rule where Sorig is
the original sentence created by our base language
model, and Sobs is the observed erroneous sentence.
p(Sorig|Sobs) =
p(Sobs|Sorig)p(Sorig)
p(Sobs)
For the language model, we can use various
known probabilistic models which already have de-
fined methods for learning the parameters, such as
n-gram models or PCFGs. For the noise model, we
need some way to learn the parameters for the mis-
takes that a group of specified writers (such as Ko-
rean ESL students) make. We address this issue in
Section 4.
Using this model, we can find the highest likeli-
hood error-free sentence for an observed output sen-
tence by tracing all possible paths from the language
model through the noise model and ending in the ob-
served sentence as output.
4 Implementation
To actually implement our model, we use a bigram
model for the base language model, and various
noise models which introduce spelling errors, ar-
ticle choice errors, preposition choice errors, etc.
1In reality, the language model will most likely produce sen-
tences with errors as seen by humans, but from the modeling
perspective, we assume that the language model is a perfect rep-
resentation of the language for our task.
Figure 1: Example of noisy channel model
All models are implemented using weighted finite-
state tranducers (wFST). For operations on the wF-
STs, we use OpenFST (Allauzen et al, 2007), along
with expectation semiring code supplied by Markus
Dryer for Dreyer et al (2008).
4.1 Base language model
The base language model is a bigram model imple-
mented by using a weighted finite-state transducer
(wFST). The model parameters are learned from
the British National Corpus modified to use Amer-
ican English spellings with Kneser-Ney smoothing.
To lower our memory usage, only bigrams whose
words are found in the observed sentences, or are
determined to be possible candidates for the correct
words of the original sentence (due to the noise mod-
els) are used. While we use a bigram model here for
simplicity, any probabilistic language model having
a tractable intersection with wFSTs could be used.
For the bigram model, each state in the wFST rep-
resents a bigram context, except the end state. The
arcs of the wFST are set so that the weight is the bi-
gram probability of the output word given the con-
text specified by the from state, and the output word
is a word of the vocabulary. Thus, given a set of n
words in the vocabulary, the language model wFST
had one start state, from which n arcs extended to
each of their own context states. From each of these
nodes, n + 1 arcs extend to each of the n context
states and the end state. Thus the number of states
in the language model is n + 2 and the number of
arcs is O(n2).
4.2 Noise models
For our noise model, we created a weighted finite-
state transducer (wFST) which accepts error-free in-
put, and outputs erroneous sentences with a spec-
ified probability. To model various types of human
errors, we created several different noise models and
936
Figure 2: Example of noise model
composed them together, creating a layered noise
model. The noise models we implement are spelling
errors, article choice errors, preposition choice er-
rors, and insertion errors, which we will explain in
more detail later in this section.
The basic design of each noise wFST starts with
an initial state, which is also the final state of the
wFST. For each word found in the language model,
an arc going from the initial state to itself is created,
with the input and output values set as the word.
These arcs model the case of no error being made.
In addition to these arcs, arcs representing prediction
errors are also inserted. For example, in the article
choice error model, an arc is added for each possible
(input, output) article pair, such as a:an for making
the mistake of writing an instead of a. The weights
of the arcs are the probabilities of introducing errors,
given the input word from the language model. For
example, the noise model shown in Figure 2 shows
a noise model in which a will be written correctly
with a probability of 0.9, and will be changed to an
or the with probabilities 0.03 and 0.07, respectively.
For this model to work correctly, the setting of the
probabilities for each error is required. How this is
done is explained in Section 4.3.
4.2.1 Spelling errors
The spelling error noise model accounts for
spelling errors made by writers. For spelling er-
rors, we allowed all spelling errors which were
a Damerau-Levenshtein distance of 1 (Damerau,
1964; Levenshtein, 1966). While allowing a DL dis-
tance of 2 or higher may likely have better perfor-
mance, the model was constrained to a distance of 1
due to memory constraints. We specified one param-
eter ?n for each possible word length n. This param-
eter is the total probability of making a spelling error
for a given word length. For each word length we
distributed the probability of each possible spelling
error equally. Thus for word length n, we have
n deletion errors, 25n substitution errors, n ? 1
transposition errors, and 26(n + 1) insertion er-
rors, and the probability for each possible error is
?n
n+25n+n?1+26(n+1) . We set the maximum word
length for spelling errors to 22, giving us 22 param-
eters.
4.2.2 Article choice errors
The article choice error noise model simulates in-
correct selection of articles. In this model we learn
n(n?1) parameters, one for each article pair. Since
there are only 3 articles (a, an, the), we only have 6
parameters for this model.
4.2.3 Preposition choice errors
The preposition choice error noise model simu-
lates incorrect selection of prepositions. We take
the 12 most commonly misused prepositions by ESL
writers (Gamon et al, 2009) and specify one param-
eter for each preposition pair, as we do in the article
choice error noise model, giving us a total of 132
parameters.
4.2.4 Wordform choice errors
The wordform choice error noise model simulates
choosing the incorrect wordform of a word. For ex-
ample, choosing the incorrect tense of a verb (e.g.
went?go), or the incorrect number marking on a
noun or verb (e.g. are?is) would be a part of this
model. This error model has one parameter for every
number of possible inflections, up to a maximum of
12 inflections, giving us 12 parameters. The param-
eter is the total probability of choosing the wrong
inflection of a word, and the probability is spread
evenly between each possible inflection. We used
CELEX (Baayen et al, 1995) to find all the possible
wordforms of each observed word.
4.2.5 Word insertion errors
The word insertion error model simulates the ad-
dition of extraneous words to the original sentence.
We create a list of words by combining the prepo-
sitions and articles found in the article choice and
preposition choice errors. We assume that the words
on the list have a probability of being inserted erro-
neously. There is a parameter for each word, which
937
is the probability of that word being inserted. Thus
we have 15 parameters for this noise model.
4.3 Learning noise model parameters
To achieve maximum performance, we wish to learn
the parameters of the noise models. If we had a
large set of erroneous sentences, along with a hand-
annotated list of the specific errors and their correc-
tions, it would be possible to do some form of super-
vised learning to find the parameters. We looked at
the NICT Japanese Learner of English (JLE) corpus,
which is a corpus of transcripts of 1,300 Japanese
learners? English oral proficiency interview. This
corpus has been annotated using an error tagset
(Izumi et al, 2004). However, because the JLE cor-
pus is a set of transcribed sentences, it is in a differ-
ent domain from our task. The Chinese Learner En-
glish Corpus (CLEC) contains erroneous sentences
which have been annotated, but the CLEC corpus
had too many manual errors, such as typos, as well
as many incorrect annotations, making it very diffi-
cult to automate the processing. Many of the correc-
tions themselves were also incorrect. We were not
able to find of a set of annotated errors which fit our
task, nor are we aware that such a set exists. Instead,
we collected a large data set of possibly erroneous
sentences from Korean ESL students (Section 5.1).
Since these sentences are not annotated, we need to
use an unsupervised learning method to learn our pa-
rameters.
To learn the parameters of the noise models, we
assume that the collected sentences are random out-
put of our model, and train our model using the
EM algorithm. This was done by making use of
the V -expectation semiring (Eisner, 2002). The
V -expectation semiring is a semiring in which the
weight is defined as R?0 ? V , where R can be used
to keep track of the probability, and V is a vector
which can be used to denote arc traversal counts or
feature counts. The weight for each of the arcs in the
noise models was set so that the real value was the
probability and the vector V denoted which choice
(having a specified error or not) was made by select-
ing the arc. We create a generative language-noise
model by composing the language model wFSTwith
the noise model wFSTs, as shown in Figure 3. By
using the expectation semiring, we can keep track of
the probability of each path going over an erroneous
0
1a:a/0.6
2an:an/0.4
3cat:cat/1
4ear:ear/1
5?:?/1?:?/1
0
a:a/0.95[0.95,0]a:an/0.05[0,0.05]
an:an/1cat:cat/1
ear:ear/1
0 1
a:a/0.57[0.57,0]
a:an/0.03[0,0.03]
2an:an/0.4
3cat:cat/1
4ear:ear/1
5?:?/1?:?/1
Figure 3: Example of language model (top) and noise
model (middle) wFST composition. The vector of the
V -expectation semiring weight is in brackets. The first
value of the vector denotes no error being made on writ-
ing ?a? and the second value denotes the error of writing
?an? instead of ?a?
arc or non-erroneous arc.
Once our model is set up for the E step using the
initial parameters, we must compute the expected
number of noise model arc traversals for use in cal-
culating our new parameters. To do this, we need to
find all possible paths resulting in the observed sen-
tence as output, for each observed sentence. Then,
for each possible path, we need to calculate the prob-
ability of the path given the output sentence, and get
the expected counts of going over each erroneous
and error-free arc to learn the parameters of the noise
model. To find a wFST with just the possible paths
for each observed sentence, we can compose the
language-noise wFST with the observed sentence
wFST. The observed sentence wFST is created in
the following manner. Given an observed sentence,
an initial state is created. For each word in the sen-
tence, in the order appearing in the sentence, a new
state is added, and an arc is created going from the
previously added state to the newly added state. The
new arc takes the observed word as input and also
uses it as output. The weight/probability for each
arc is set to 1. Composing the sentence wFST with
the language-noise wFST has the effect of restricting
the new wFST to only have sentences which out-
put the observed sentence from the language-noise
wFST. We now have a new wFST where all valid
paths are the paths which can produce the observed
938
sentence. To find the total weight of all paths, we
first change all input and output symbols into the
empty string. Since all arcs in this wFST are ep-
silon arcs, we can use the epsilon-removal operation
(Mohri, 2002), which will reduce the wFST to one
state with no arcs. This operation combines the to-
tal weight of all paths into the final weight of the
sole state, giving us the total expectation value for
that sentence. By doing this for each sentence, and
adding the expectation values for each sentence, we
can easily compute the expectation step, from which
we can find the maximizing parameters and update
our parameters accordingly.
4.4 Finding the maximum likelihood correction
Once the parameters are learned, we can use our
model to find the maximum likelihood error-free
sentence. This is done by again creating the lan-
guage model and noise model with the learned pa-
rameters, but this time we set the weights of the
noise model to just the probabilities, using the log
semiring, since we do not need to keep track of ex-
pected values. We also set the language model input
for each arc to be the same word as the output, in-
stead of using an empty string. Once again, we com-
pose the language model with the noise models. We
create a sentence wFST using the observed sentence
we wish to correct, the same way the observed sen-
tence wFST for training was created. This is now
composed with the language-noise wFST. Now all
we need to do is find the shortest path (when using
minus-log probabilities) of the new wFST, and the
input to that path will be our corrected sentence.
5 Experiment
We now present the data set and evaluation tech-
nique used for our experiments.
5.1 Data Set
To train our noise models, we collected around
25,000 essays comprised of 478,350 sentences writ-
ten by Korean ESL students preparing for the
TOEFL writing exam. These were collected from
open web postings by Korean ESL students ask-
ing for advice on their writing samples. In order
to automate the process, a program was written to
download the posts, and discard the posts that were
deemed too short to be TOEFL writing samples.
Also discarded were the posts that had a ?[re? or
?re..? in the title. Next, all sentences containing
Korean were removed, after which some characters
were changed so that they were in ASCII form. The
remaining text was separated into sentences solely
by punctuation marks ., !, and ?. This resulted in the
478,350 sentences stated above. Due to the process,
some of the sentences collected are actually sen-
tence fragments, where punctuation had been mis-
used. For training and evaluation purposes, the data
set was split into a test set with 504 randomly se-
lected sentences, an evaluation set of 1017 randomly
selected sentences, and a training set composed of
the remaining sentences.
5.2 Evaluation technique
In the current literature, grammar correction tasks
are often manually evaluated for each output cor-
rection, or evaluated by taking a set of proper sen-
tences, artificially introducing some error, and see-
ing how well the algorithm fixes the error. Man-
ual evaluation of automatic corrections may be the
best method for getting a more detailed evaluation,
but to do manual evaluation for every test output re-
quires a large amount of human resources, in terms
of both time and effort. In the case where artificial
lesioning is introduced, the lesions may not always
reflect the actual errors found in human data, and
it is difficult to replicate the actual tendency of hu-
mans to make a variety of different mistakes in a
single sentence. Thus, this method of evaluation,
which may be suitable for evaluating the correction
performance of specific grammatical errors, would
not be fit for evaluating our model?s overall perfor-
mance. For evaluation of the given task, we have
incorporated evaluation techniques based on current
evaluation techniques used in machine translation,
BLEU (Papineni et al, 2002) and METEOR (Lavie
and Agarwal, 2007).
Machine translation addresses the problem of
changing a sentence in one language to a sentence of
another. The task of correcting erroneous sentences
can also be thought of as translating a sentence from
a given language A, to another language B, where A
is a broken language, and B is the correct language.
Under this context, we can apply machine trans-
lation evaluation techniques to evaluate the perfor-
mance of our system. Our model?s sentence correc-
939
tions can be thought of as the output translation to be
evaluated. In order to use BLEU and METEOR, we
need to have reference translations on which to score
our output. As we have already explained in section
5.1, we have a collection of erroneous sentences, but
no corrections. To obtain manually corrected sen-
tences for evaluation, the test and evaluation set sen-
tences and were put on Amazon Mechanical Turk as
a correction task. Workers residing in the US were
asked to manually correct the sentences in the two
sets. Workers had a choice of selecting ?Impossi-
ble to understand?, ?Correct sentence?, or ?Incorrect
sentence?, and were asked to correct the sentences
so no spelling errors, grammatical errors, or punctu-
ation errors were present. Each sentence was given
to 8 workers, giving us a set of 8 or fewer corrected
sentences for each erroneous sentence. We asked
workers not to completely rewrite the sentences, but
to maintain the original structure as much as pos-
sible. Each hit was comprised of 6 sentences, and
the reward for each hit was 10 cents. To ensure the
quality of our manually corrected sentences, a native
English speaker research assistant went over each of
the ?corrected? sentences and marked them as cor-
rect or incorrect. We then removed all the incorrect
?corrections?.
Using our manually corrected reference sen-
tences, we evaluate our model?s correction perfor-
mance using METEOR and BLEU. Since METEOR
and BLEU are fully automated after we have our ref-
erence translations (manual corrections), we can run
evaluation on our tests without any need for further
manual input. While these two evaluation methods
were created for machine translation, they also have
the potential of being used in the field of grammar
correction evaluation. One difference between ma-
chine translation and our task is that finding the right
lemma is in itself something to be rewarded in MT,
but is not sufficient for our task. In this respect, eval-
uation of grammar correction should be more strict.
Thus, for METEOR, we used the ?exact? module for
evaluation.
To validate our evaluation method, we ran a sim-
ple test by calculating the METEOR and BLEU
scores for the observed sentences, and compared
them with the scores for the manually corrected sen-
tences, to test for an expected increase. The scores
for each correction were evaluated using the set of
METEOR BLEU
Original ESL sentences 0.8327 0.7540
Manual corrections 0.9179 0.8786
Table 1: BLEU and METEOR scores for ESL sentences
vs manual corrections on 100 randomly chosen sentences
METEOR BLEU
Aspell 0.824144 0.719713
Spelling noise model 0.825001 0.722383
Table 2: Aspell vs Spelling noise model
corrected sentences minus the correction sentence
being evaluated. For example, let us say we have the
observed sentence o, and correction sentences c1, c2,
c3 and c4 from Mechanical Turk. We run METEOR
and BLEU on both o and c1 using c2, c3 and c4 as
the reference set. We repeat the process for o and c2,
using c1, c3 and c4 as the reference, and so on, until
we have runMETEOR and BLEU on all 4 correction
sentences. With a set of 100 manually labeled sen-
tences, the average METEOR score for the ESL sen-
tences was 0.8327, whereas the corrected sentences
had an average score of 0.9179. For BLEU, the av-
erage scores were 0.7540 and 0.8786, respectively,
as shown in Table 1. Thus, we have confirmed that
the corrected sentences score higher than the ESL
sentence. It is also notable that finding corrections
for the sentences is a much easier task than finding
various correct translations, since the task of editing
is much easier and can be done by a much larger set
of qualified people.
6 Results
For our experiments, we used 2000 randomly se-
lected sentences for training, and a set of 1017 an-
notated sentences for evaluation. We also set aside
a set of 504 annotated sentences as a development
set. With the 2000 sentence training, the perfor-
mance generally converged after around 10 itera-
tions of EM.
6.1 Comparison with Aspell
To check how well our spelling error noise model is
doing, we compared the results of using the spelling
error noise model with the output results of using
the GNU Aspell 0.60.6 spelling checker. Since we
940
METEOR ? ? BLEU ? ?
ESL Baseline 0.821000 0.715634
Spelling only 0.825001 49 5 0.722383 53 8
Spelling, Article 0.825437 55 6 0.723022 59 9
Spelling, Preposition 0.824157 52 17 0.720702 55 19
Spelling, Wordform 0.825654 81 25 0.723599 85 27
Spelling, Insertion 0.825041 52 5 0.722564 56 8
Table 3: Average evaluation scores for various noise models run on 1017 sentences, along with counts of sentences
with increased (?) and decreased (?) scores. All improvements are significant by the binomial test at p < 0.001
are using METEOR and BLEU for our evaluation
metric, we needed to get a set of corrected sentences
for using Aspell. Aspell lists the suggested spelling
corrections of misspelled words in a ranked order, so
we replaced each misspelled word found by Aspell
with the word with the highest rank (lowest score)
for the Aspell corrections. One difference between
Aspell and our model is that Aspell only corrects
words which do not appear in the dictionary, while
our method looks at all words, even those found in
the dictionary. Thus our model can correct words
which look correct by themselves, but seem to be
incorrect due to the bigram context. Another differ-
ence is that Aspell has the capability to split words,
whereas our model does not allow the insertion of
spaces. A comparison of the scores is shown in Ta-
ble 2. We can see that our model has better per-
formance, due to better word selection, despite the
advantage that Aspell has by using phonological in-
formation to find the correct word, and the disadvan-
tage that our model is restricted to spellings which
are within a Damerau-Levenstein distance of 1. This
is due to the fact that our model is context-sensitive,
and can use other information in addition to the mis-
spelled word. For example, the sentence ?In contast,
high prices of products would be the main reason
for dislike.? was edited in Aspell by changing ?con-
tast? to ?contest?, while our model correctly selected
?contrast?. The sentence ?So i can reach the theater
in ten minuets by foot? was not edited by Aspell, but
our model changed ?minuets? to ?minutes?. Another
difference that can be seen by looking through the
results is that Aspell changes every word not found
in the dictionary, while our algorithm allows words
it has not seen by treating them as unknown tokens.
Since we are using smoothing, these tokens are left
in place if there is no other high probability bigram
to take its place. This helps leave intact the proper
nouns and words not in the vocabulary.
6.2 Noise model performance and output
Our next experiment was to test the performance of
our model on various types of errors. Table 3 shows
the BLEU and METEOR scores of our various error
models, along with the number of sentences achiev-
ing improved and reduced scores. As we have al-
ready seen in section 6.1, the spelling error model
increases the evaluation scores from the ESL base-
line. Adding in the article choice error model and
the word insertion error models in addition to the
spelling error noise model increases the BLEU score
performance of finding corrections. Upon observ-
ing the outputs of the corrections on the develop-
ment set, we found that the corrections changing
a to an were all correct. Changes between a and
the were sometimes correct, and sometimes incor-
rect. For example, ?which makes me know a exis-
tence about? was changed to ?which makes me know
the existence about?, ?when I am in a trouble.? was
changed to ?when I am in the trouble.?, and ?many
people could read a nonfiction books? was changed
to ?many people could read the nonfiction books?.
For the last correction, the manual corrections all
changed the sentence to contain ?many people could
read a nonfiction book?, bringing down the evalu-
ation score. Overall, the article corrections which
were being made seemed to change the sentence for
the better, or left it at the same quality.
The preposition choice error model decreased the
performance of the system overall. Looking through
the development set corrections, we found that many
correct prepositions were being changed to incorrect
prepositions. For example, in the sentence ?Distrust
about desire between two have been growing in their
941
relationship.?, about was changed to of, and in ?As
time goes by, ...?, by was changed to on. Since these
changes were not found in the manual corrections,
the scores were decreased.
For wordform errors, the BLEU and METEOR
scores both increased. While the wordform choice
noise model had the most sentences with increased
scores, it also had the most sentences with decreased
scores. Overall, it seems that to correct wordform
errors, more context than just the preceding and fol-
lowing word are needed. For example, in the sen-
tence ?There are a lot of a hundred dollar phones in
the market.?, phones was changed to phone. To infer
which is correct, you would have to have access to
the previous context ?a lot of?. Another example is
?..., I prefer being indoors to going outside ...?, where
going was changed to go. These types of cases illus-
trate the restrictions of using a bigram model as the
base language model.
The word insertion error model was restricted to
articles and 12 prepositions, and thus did not make
many changes, but was correct when it did. One
thing to note is that since we are using a bigram
model for the language model, the model itself is
biased towards shorter sentences. Since we only in-
cluded words which were needed when they were
used, we did not run into problems with this bias.
When we tried including a large set of commonly
used words, we found that many of the words were
being erased because of the bigrams models proba-
bilistic preference for shorter sentences.
6.3 Limitations of the bigram language model
Browsing through the development set data, we
found that many of our model?s incorrect ?correc-
tions? were the result of using a bigram model as our
language model. For example, ?.., I prefer being in-
doors to going outside in that...? was changed to ?..,
I prefer being indoors to go outside in that...?. From
the bigram model, the probabilities p(go to) and
p(outside go) are both higher than p(going to) and
p(outside going), respectively. To infer that going
is actually correct, we would need to know the previ-
ous context, that we are comparing ?being indoors?
to ?going outside?. Unfortunately, since we are using
a bigram model, this is not possible. These kind of
errors are found throughout the corrections. It seems
likely that making use of a language model which
can keep track of this kind of information would in-
crease the performance of the correction model by
preventing these kinds of errors.
7 Conclusion and future work
We have introduced a novel way of finding grammar
and spelling corrections, which uses the EM algo-
rithm to train the parameters of our noisy channel
approach. One of the benefits of this approach is that
it does not require a parallel set of erroneous sen-
tences and their corrections. Also, our model is not
confined to a specific error, and various error models
may be added on. For training our noise model, all
that is required is finding erroneous data sets. De-
pending on which domain you are training on, this
can also be quite feasible as we have shown by our
collection of Korean ESL students? erroneous writ-
ing samples. Our data set could have been for ESL
students of any native language, or could also be a
data set of other groups such as young native En-
glish speakers, or the whole set of English speakers
for grammar correction. Using only these data sets,
we can train our noisy channel model, as we have
shown using a bigram language model, and a wFST
for our noise model. We have also shown how to use
weighted finite-state transducers and the expectation
semiring, as well as wFST algorithms implemented
in OpenFST to train the model using EM. For evalu-
ation, we have introduced a novel way of evaluating
grammar corrections, using MT evaluation methods,
which we have not seen in other grammar correction
literature. The produced corrections show the re-
strictions of using a bigram language model. For fu-
ture work, we plan to use a more accurate language
model, and add more types of complex error models,
such as word deletion and word ordering error mod-
els to improve performance and address other types
of errors.
Acknowledgments
We are grateful to Randy West for his input and
assistance, and to Markus Dreyer who provided us
with his expectation semiring code. We would also
like to thank the San Diego Supercomputer Center
for use of their DASH high-performance computing
system.
942
References
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W.,
and Mohri, M. (2007). OpenFst: A general
and efficient weighted finite-state transducer li-
brary. In Proceedings of the Ninth International
Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lec-
ture Notes in Computer Science, pages 11?23.
Springer. http://www.openfst.org.
Baayen, H. R., Piepenbrock, R., and Gulikers, L.
(1995). The CELEX Lexical Database. Release 2
(CD-ROM). Linguistic Data Consortium, Univer-
sity of Pennsylvania, Philadelphia, Pennsylvania.
Brockett, C., Dolan, W. B., and Gamon, M. (2006).
Correcting ESL errors using phrasal SMT tech-
niques. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 249?256,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Chodorow, M. and Leacock, C. (2000). An unsu-
pervised method for detecting grammatical errors.
In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics
conference, pages 140?147, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Damerau, F. J. (1964). A technique for computer
detection and correction of spelling errors. Com-
mun. ACM, 7:171?176.
De Felice, R. and Pulman, S. G. (2008). A classifier-
based approach to preposition and determiner er-
ror correction in L2 English. In Proceedings of
the 22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08, pages
169?176, Morristown, NJ, USA. Association for
Computational Linguistics.
Dempster, A. P., Laird, N. M., and Rubin, D. B.
(1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the
Royal Statistical Society. Series B (Methodolog-
ical), 39(1):pp. 1?38.
De?silets, A. and Hermet, M. (2009). Using auto-
matic roundtrip translation to repair general errors
in second language writing. In Proceedings of the
twelfth Machine Translation Summit, MT Summit
XII, pages 198?206.
Dreyer, M., Smith, J., and Eisner, J. (2008). Latent-
variable modeling of string transductions with
finite-state methods. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 1080?1089, Hon-
olulu, Hawaii. Association for Computational
Linguistics.
Eeg-olofsson, J. and Knutsson, O. (2003). Au-
tomatic grammar checking for second language
learners - the use of prepositions. In In Nodalida.
Eisner, J. (2002). Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1?8,
Philadelphia.
Gamon, M., Leacock, C., Brockett, C., Dolan,
W. B., Gao, J., Belenko, D., and Klementiev,
A. (2009). Using statistical techniques and web
search to correct ESL errors. In Calico Journal,
Vol 26, No. 3, pages 491?511, Menlo Park, CA,
USA. CALICO Journal.
Izumi, E., Uchimoto, K., and Isahara, H. (2004).
The NICT JLE corpus exploiting the language
learnersspeech database for research and educa-
tion. In International Journal of the Computer, the
Internet and Management, volume 12(2), pages
119?125.
Izumi, E., Uchimoto, K., Saiga, T., Supnithi, T., and
Isahara, H. (2003). Automatic error detection in
the Japanese learners? English spoken data. In
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics - Volume 2,
ACL ?03, pages 145?148, Morristown, NJ, USA.
Association for Computational Linguistics.
Knight, K. and Chander, I. (1994). Automated
postediting of documents. In Proceedings of the
twelfth national conference on Artificial intelli-
gence (vol. 1), AAAI ?94, pages 779?784, Menlo
Park, CA, USA. American Association for Artifi-
cial Intelligence.
Lavie, A. and Agarwal, A. (2007). Meteor: an au-
tomatic metric for MT evaluation with high levels
of correlation with human judgments. In StatMT
?07: Proceedings of the Second Workshop on
943
Statistical Machine Translation, pages 228?231,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Lee, J. and Seneff, S. (2006). Automatic grammar
correction for second-language learners. In Pro-
ceedings of Interspeech.
Levenshtein, V. I. (1966). Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Mohri, M. (2002). Generic epsilon-removal
and input epsilon-normalization algorithms for
weighted transducers. In International Journal of
Foundations of Computer Science 13, pages 129?
143.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). Bleu: a method for automatic evaluation
of machine translation. In ACL, pages 311?318.
Shannon, C. (1948). A mathematical theory of
communications. Bell Systems Technical Journal,
27(4):623?656.
Shichun, G. and Huizhong, Y. (2003). Chinese
Learner English Corpus. Shanghai Foreign Lan-
guage Education Press.
Tetreault, J. R. and Chodorow, M. (2008). The
ups and downs of preposition error detection in
ESL writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics
- Volume 1, COLING ?08, pages 865?872, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Wu, C.-F. J. (1983). On the convergence properties
of the EM algorithm. Ann. Statist., 11(1):95?103.
944
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 170?179,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Bilingual Random Walk Models for Automated Grammar Correction of
ESL Author-Produced Text
Randy West and Y. Albert Park
Department of Computer Science & Engineering
University of California, San Diego
La Jolla, CA 92093-0533
{rdwest,yapark}@cs.ucsd.edu
Roger Levy
Department of Linguistics
University of California, San Diego
La Jolla, CA 92093-0533
rlevy@ucsd.edu
Abstract
We present a novel noisy channel model for
correcting text produced by English as a sec-
ond language (ESL) authors. We model the
English word choices made by ESL authors as
a random walk across an undirected bipartite
dictionary graph composed of edges between
English words and associated words in an au-
thor?s native language. We present two such
models, using cascades of weighted finite-
state transducers (wFSTs) to model language
model priors, random walk-induced noise, and
observed sentences, and expectation maxi-
mization (EM) to learn model parameters af-
ter Park and Levy (2011). We show that such
models can make intelligent word substitu-
tions to improve grammaticality in an unsu-
pervised setting.
1 Introduction
How do language learners make word choices as
they compose text in a language in which they are
not fluent? Anyone who has attempted to learn a for-
eign language can attest to spending a great deal of
time leafing through the pages of a bilingual dictio-
nary. However, dictionaries, especially those with-
out a wealth of example sentences or accompany-
ing word sense information, can often lead even the
most scrupulous of language learners in the wrong
direction. Consider an example: the English noun
?head? has several senses, e.g. the physical head and
the head of an organization. However, the Japanese
atama can only mean the physical head or mind, and
likewise shuchou, meaning ?chief,? can only map to
the second sense of head. A native English speaker
and Japanese learner faced with the choice of these
two words and no additional explanation of which
Japanese word corresponds to which sense is liable
to make a mistake on the flip of a coin.
One could of course conceive of more subtle ex-
amples where the semantics of a set of choices are
not so blatantly orthogonal. ?Complete? and ?en-
tire? are synonyms, but they are not necessarily in-
terchangeable. ?Complete stranger? is a common
two-word phrase, but ?entire stranger? sounds com-
pletely strange, if not entirely ungrammatical, to the
native English speaker, who will correct ?entire?
to ?complete? in a surprisingly automatic fashion.
Thus, correct word choice in non-native language
production is essential not only to the preservation
of intended meaning, but also to fluent expression of
the correct meaning.
The development of software to correct ESL text
is valuable for both learning and communication.
A language learner provided instant grammatical-
ity feedback during self-study is less likely to fall
into patterns of misuse, and the comprehension diffi-
culties one may encounter when corresponding with
non-native speakers would be ameliorated by an au-
tomated system to improve text fluency. Addition-
ally, since machine-translated text is often ungram-
matical, automated grammar correction algorithms
can be deployed as part of a machine translation sys-
tem to improve the quality of output.
We propose that word choice production errors
on the part of the language learner can be mod-
eled as follows. Given an observed word and an
undirected bipartite graph with nodes representing
170
words in one of two languages, i.e. English and the
sentence author?s native tongue, and edges between
words in each language and their dictionary trans-
lation in the other (see Figure 1 for an example),
there exists some function f 7? [0, 1] that defines
the parameters of a random walk along graph edges,
conditioned on the source word. By composing this
graph with a language model prior such as an n-
gram model or probabilistic context-free grammar,
we can ?correct? an observed sentence by inferring
the most likely unobserved sentence from which it
originated.
More concretely, given that we know f , we can
compute argmaxw? p(w?|w, f, ?), where w is the
observed sentence, ? is the language model, and w?
is the ?corrected,? unobserved sentence. Under this
view, some w? drawn from the distribution ? is sub-
jected to some noise process f , which perturbs the
sentence author?s intended meaning and outputs w.
We perform this computation in the standard way
from the statistical machine translation (SMT) liter-
ature (Brown et al, 1993), namely by using Bayes?
theorem to write
p(w?|w, f, ?) = p(w
?|?)p(w|w?, f, ?)
p(w|?)
Since the denominator of the RHS is independent of
w?, we can rewrite our argmax as
argmax
w?
p(w?|?)p(w|w?, f, ?)
We have now decomposed our original equation into
two manageable parts, a prior belief about the gram-
maticality of an unobserved sentence w?, which we
can compute using a language model ? learned sepa-
rately using standard supervised techniques (in par-
ticular, n-gram estimation), and the probability of
the observed sentence w given w?, f , and ?. To-
gether, these constitute a noisy channel model from
information theory (Shannon, 1948). All that re-
mains is to learn an appropriate f , for which we will
employ unsupervised methods, namely expectation
maximization.
The rest of this paper is organized as follows. In
Section 2, we will discuss related work. In Section
3, we will present the implementation, methodology
and results of two experiments with different f . In
Section 4, we will discuss our experimental results,
and we will conclude in Section 5.
2 Related Work
The literature on automated grammar correction
is mostly focused on rule-based methods and er-
ror identification rather than correction. However,
there has been a recent outgrowth in the applica-
tion of machine translation (MT) techniques to ad-
dress the problem of single-language grammar cor-
rection. Park and Levy (2011) propose a noisy chan-
nel model for learning to correct various types of er-
rors, including article and preposition errors, word-
form errors, and spelling mistakes, to which this pa-
per is an extension. As the present work builds on
Park and Levy?s basic model, we will reserve a more
detailed discussion of their work for Section 3.
Brockett et al (2006) use phrasal SMT techniques
to identify and correct mass noun errors of ESL stu-
dents with some success, but they correct no other
production error classes to our knowledge.
Lee and Seneff (2006) learn a method to aid ESL
students in language acquisition by reducing sen-
tences to their canonical form, i.e. a lemmatized
form devoid of articles, prepositions, and auxil-
iaries, and then building an over-specified lattice by
reinserting all word inflections and removed word
classes. They then score this lattice using a trigram
model and PCFG. While this method has many ad-
vantages, it does not take into account the full con-
text of the original sentence.
Kok and Brockett (2010) use random walks over
bi- and multilingual graphs generated by aligning
English sentences with translations in 10 other Eu-
ropean languages to learn paraphrases, which they
then evaluate in the context of the original sentence.
While their approach shares many high-level simi-
larities with ours, both their task, paraphrasing cor-
rect sentences, and the details of their methodology
are divergent from the present work.
De?silets and Hermet (2009) employ round-trip
machine translation from L1 to L2 and back again
to correct second language learner text by keep-
ing track of the word alignments between transla-
tions. They operate on a very similar hypothesis
to that of this work, namely that language learners
make overly-literal translations when the produce
text in their second language. However, they go
about correcting these errors in a very different way
than the present work, which is novel to the best of
171
Figure 1: Example English-Korean dictionary graph for a subset of the edges out of the English head, leader, and
chief.
head
??????? ?????? ??????
chiefleader
our knowledge, and their technique of using error-
annotated sentences for evaluation makes a compar-
ison difficult.
3 Model Implementation and Experiments
We present the results of two experiments with dif-
ferent random walk parametrizations. We begin by
describing our dataset, then proceed to an overview
of our model and experimental procedures, and fi-
nally detail the experiments themselves.
3.1 Dataset
We use the dataset of Park and Levy (2011), a col-
lection of approximately 25,000 essays comprised of
478,350 sentences scraped from web postings made
by Korean ESL students studying for the Test of En-
glish as a Foreign Language (TOEFL). Of these, we
randomly select 10,000 sentences for training, 504
as a development set, and 1017 held out for final
model evaluation.
Our English-Korean dictionary is scraped from
http://endic2009.naver.com, a widely-
used and trusted online dictionary source in South
Korea. We are unfortunately unaware of any freely
available, downloadable English-Korean dictionary
databases.
3.2 Model and Experimental Procedures
3.2.1 Overview
The bulk of our experimental methodology and
machinery is borrowed from Park and Levy (2011),
so we will summarize that portion of it only briefly
here. At a high level, there are three major compo-
nents to the model of a sentence: a language prior,
a noise model, and an observed sentence. Each
of these is implemented as a wFST and composed
together into a single transducer whose accepting
paths represent all possibilities of transducing from
an (unobserved) input sentence to the (observed)
output sentence, with the path weight being associ-
ated probability. See Figure 2 for an example.
3.2.2 Language Model
For our language model, we use a Kneser-Ney
smoothed trigram model learned from a version
of the British National Corpus modified to use
Americanized spellings (Chen and Goodman, 1996;
Burnard, 1995). The implementation of an n-gram
model as a wFST requires that each state represent a
context, and so one must necessarily instantiate arcs
for all words in the alphabet from each state. In order
to reduce model size and minimize memory usage, it
is standard practice to remove relatively uninforma-
tive higher-order n-grams from the model, but under
the wFST regime one cannot, for example, remove
some trigrams from a bigram context without re-
moving all of them. Instead, we retain only the 1,000
most informative bigram contexts, as measured by
the Kullback-Leibler divergence between each bi-
gram context and its unigram counterpart. This is
in contrast to standard cutoff models, which remove
n-grams occurring less than some cutoff number of
times in the corpus.
3.2.3 Noise Models
The structure of the noise wFST differs for each
noise model; for our model of word-choice error, we
can use a single initial/final state with arcs labeled
with unobserved words as input, observed words as
output, and a weight defined by the function f that
governs the parameters of a random walk across our
dictionary graph (again, see Figure 2 for an exam-
ple). We will reserve the definition of f , which is
172
Figure 2: Example wFSTs for the sentence ?head chief?.
From top to bottom, the pictured transducers are the ob-
served sentence s, a noise model n with parameter ?, a
unigram language model l representing the normalized
frequency of each word, and the fully composed model,
l ? n ? s.
0 1
head:head/1
2
chief:chief/1
Observed Sentence
0
leader:chief/(?/2)
leader:head/(?/2)
leader:leader/(1-?)
chief:leader/(?/2)
chief:head/(?/2)
chief:chief/?
head:chief/(?/2)
head:leader/(?/2)
head:head/(1-?)
Noise Model
0
chief:chief/(1/5)
leader:leader/(3/10)
head:head/(1/2)
Language Model
0 1
head:head/((1-?)/2)
leader:head/(3?/20)
chief:head/(?/10)
2
head:chief/(?/4)
leader:chief/(3?/20)
chief:chief/((1-?)/5)
Composed Model
different for each experiment, for Section 3.3.
We have thus far proceeded by describing the con-
struction of an ideal noise model that completely
implements the dictionary graph described previ-
ously. However, due to the size of the dictionary
graph, such a model would be computationally pro-
hibitive1. Moreover, we must handle the non-trivial
peculiars of arbitrary lookups in a roughly lem-
matized dictionary and preservation of word forms
through random walks, which we discuss now.
1The maximum degree of the dictionary graph is 515, mean-
ing that the upper bound on the number of paths in a random
walk of length 2 is 5152 = 265, 225!
Among its various capabilities, the CELEX database
(Baayen et al, 1995) provides interfaces for map-
ping arbitrary English words to their lemmas, query-
ing for lemma syntactic (sub)classes, and discover-
ing the morphological inflectional features of arbi-
trary words. We use these capabilities in conjunction
with unigram frequencies from our language model
and a standard stop word filter to build abridged sets
of random walk candidates as in Algorithm 1.
Algorithm 1 Build an abridged set of random walk
candidates C for an observed word w s.t. each
ci ? C has syntactic and morphological characteris-
tics similar to w and is in the top m such candidates
as sorted by word frequency.
Let G = (V,E) be the undirected dictionary
graph, m the max candidates per word, B the set
of stop words, I the set of inflectional features of
w, and C the set of random walk candidates for
w, initially {}
if w ? B then
return {}
end if
for lemmas l of w do
Let S be the set of syntactic classes of l
for l? generated from a random walk of length
2 in G from l do
if S ? {syntactic classes of l?} 6= {} then
for words w? related to l? do
if I ? {inflectional features of w?} 6=
{} ?w? 6? B then
C ? C ? {w?}
end if
end for
end if
end for
end for
if |C| > m then
C ? top m members of C by word frequency
end if
return C
3.2.4 Sentence Models
Sentences are simply identity transducers, i.e.
wFSTs with n + 1 states for a sentence of length
n and a single arc between each state 0 ? i < n and
state i+1 labeled with input and output token i from
173
the sentence and weight 1.
3.2.5 Training and Decoding
For training, we hold language model parameters
constant and use expectation maximization (Demp-
ster et al, 1977) to learn noise model parameters as
follows. We replace language model input symbols
and sentence model output symbols with the empty
symbol  and use the V-expectation semiring of Eis-
ner (2002) to annotate noise model arcs with ini-
tial parameter values. This is our M-step. Then,
we compose the language, noise, and sentence mod-
els, which produces a transducer with only -labeled
arcs, and use -removal to move expectation infor-
mation into a single state from which we can eas-
ily read off expected noise model parameter counts
thanks to the V-expectation semiring?s bookkeeping
(Eisner, 2002; Mohri, 2001). We repeat this pro-
cess over a batch of training sentences and add the
results together to yield a final vector of expected
counts. This is our E-step. Finally, we normalize the
expected parameter counts to recompute our param-
eters and rebuild the noise model in a repetition of
the M-step. This process goes back and forth from
E- to M-step until the parameters converge within
some threshold.
The decoding or inference process is performed
in a similar fashion, the main difference being that
we use the negative log Viterbi semiring for com-
puting shortest paths instead of the V-expectation
semiring. We first build a new noise model for each
sentence using the parameter values learned during
training. Then, the language, noise, and sentence
models (sans  substitutions) are composed together,
and the shortest path is computed.
3.2.6 wFST Implementation
All wFST manipulation is performed using Open-
FST (Allauzen et al, 2007), an open source
weighted finite-state transducer library written in
C++. Additionally, we use the V-expectation semir-
ing code of Dreyer et al (2008) for training.
3.2.7 Evaluation
The most probable unobserved sentence w? from
which the observed sentence w was generated under
our model, argmaxw? p(w?|?)p(w|w?, f, ?), can be
read off from the input of the transducer produced
during the decoding process. In order to evaluate
its quality versus the observed ESL sentence, we
use the METEOR2 and BLEU evaluation metrics for
machine translation (Lavie and Agarwal, 2007; Pap-
ineni et al, 2002). This evaluation is performed us-
ing a set of human-corrected sentences gathered via
Amazon Mechanical Turk, an online service where
workers are paid to perform a short task, and further
filtered for correctness by an undergraduate research
assistant. 8 workers were assigned to correct each
sentence from the development and evaluation sets
described in Section 3.1, and so after filtering we
had 8 or fewer unique corrected versions per sen-
tence available for evaluation. We note that the use
of METEOR and BLEU is justified inasmuch as the
process of grammar correction is translation from
an ungrammatical ?language? to a grammatical one
(Park and Levy, 2011). However, it is far from per-
fect, as we shall see shortly.
While human evaluation is far too costly to at-
tempt at every step during development, it is very
worthwhile to examine our corrections through a hu-
man eye for final evaluation, especially given the
somewhat tenuous suitability of METEOR and BLEU
for our evaluation task. In order to facilitate this, we
designed a simple task, again using Amazon Me-
chanical Turk, where native English speakers are
presented with side-by-side ESL and corrected sen-
tences and asked to choose which is more correct.
Workers are instructed to ?judge whether the cor-
rected sentence improves the grammaticality and/or
fluency of the ESL sentence without changing the
ESL sentence?s basic meaning.? They are then pre-
sented with two questions per sentence pair:
1. Question: ?Between the two sentences listed
above, which is more correct??
Answer choices: ?ESL sentence is more cor-
rect,? ?Corrected sentence is more correct,?
?Both are equally correct,? and, ?The sentences
are identical.?
2Although the METEOR ?synonymy? module may initially
seem appropriate to our evaluation task, we find that it does
little to improve or clarify evaluation results. For that reason,
and moreover since we do not wish for differing forms of the
same lemma to be given equal weight in a grammar correction
task, we instead use the ?exact? module for all evaluation in this
paper.
174
2. Question: ?Is the meaning of the corrected sen-
tence significantly different from that of the
ESL sentence??
Answer choices: ?Yes, the two sentences do not
mean the same thing,? and, ?No, the two sen-
tences have roughly the same meaning.?
Each task is 10 sentences long, 3 of which are iden-
tical filler sentences. When a worker mislabels more
than one sentence as identical in any single task, the
results for that task are thrown out and resubmitted
for another worker to complete. We additionally re-
quire that each sentence pair be judged by 5 unique,
U.S.-based workers.
3.3 Experiments
3.3.1 Experiment 1
Motivation and Noise Model For our first exper-
iment, we assume that the probability of arriving at
some word w? 6= w after a random walk of length
2 from an observed word w is uniform across all w.
This is perhaps not the most plausible model, but it
serves as a baseline by which we can evaluate more
complex models.
More concretely, we use a single parameter ?
modeling the probability of walking two steps along
the dictionary graph from an observed English word
w to its Korean definition(s), and then back to some
other English word w? 6= w. Since we treat un-
observed words as transducer input and observed
words as output, ? is normalized by |{w|w 6= w?}|,
i.e. the number of edges with different input and out-
put per input word, and p(w|w) = 1 ? ? such that
?
w p(w|w?) = 1.
Initialization and Other Settings We train two
variations on the same model, setting m from Al-
gorithm 1, i.e. the maximum number of allowed
random walk candidates per word, to 5 and 10. We
initialize ? to 0.01 for each.
Results We find that both variations converge af-
ter roughly 10 iterations3 . The parameters learned
are slightly lower than the initialization value (? =
3Running on a Linux server with two quad-core Intel Xeon
processors and 72GB of memory, training for all models in this
paper takes around 4 hours per model. Note that decoding is a
much quicker process, requiring less than one second per sen-
tence.
0.01), 0.007246 for the 5 candidate variation and
0.009528 for the 10 candidate variation. We inter-
pret the parameter value disparity between the two
model variations as follows. The larger the num-
ber of random walk candidates available for each
observed word, the more likely that at least one of
the candidates has a high probability in the sentence
context, so it makes sense that the 10 candidate vari-
ation would yield a higher value for ?. Moreover,
recalling that ? is normalized by the number of ob-
served words |{w|w 6= w?}| reachable from each un-
observed candidate word w?, it is reasonable that a
higher value of ? would need to be learned in order
to distribute enough probability mass to candidates
that are highly probable in the sentence context.
The METEOR and BLEU scores for this Experi-
ment are summarized in Table 1, and the final pa-
rameter values after 10 iterations are listed in Table
2. We discuss these in greater detail in Section 4.
Table 1: METEOR and BLEU scores for all experiments.
METEOR BLEU
ESL baseline 0.820802 0.715625
Exp. 1, 5 candidates 0.816055 0.708871
Exp. 1, 10 candidates 0.815703 0.708284
Exp. 2, 5 candidates 0.815162 0.707549
Exp. 2, 10 candidates 0.814533 0.706587
Table 2: Final parameter values after 10 iterations for Ex-
periment 1 with 5 and 10 word random walk candidate
limits.
Max 5 Candidates Max 10 Candidates
? 0.007246 0.009528
3.3.2 Experiment 2
Motivation and Noise Model For our second ex-
periment, we hypothesize that there is an inverse re-
lationship between unobserved word frequency and
random walk path probability. We motivate this by
observing that when a language learner produces a
common word, it is likely that she either meant to
use that word or used it in place of a rarer word that
she did not know. Likewise, when she uses a rare
word, it is likely that she chose it above any of the
175
common words that she knows. If the word that she
chose was erroneous, then, it is most likely that she
did not mean to use a common word but could have
meant to use a different rare word with a subtle se-
mantic difference. Hence, we should always prefer
to replace observed words, regardless of their fre-
quency, with rare words unless the language model
overwhelmingly prefers a common word.
In order to model this hypothesis, we introduce
a second parameter ? < 0 to which power the
unigram frequency of each unobserved word w?,
freq(w?), is raised. The resulting full model is
p(w|w?)w 6=w? = freq(w
?)??
|{w|w 6=w?}| and p(w|w) = 1 ?
freq(w)??. We approximate the full model to sim-
ple coin flips by bucketing the unique word frequen-
cies from the language model and initializing each
bucket using its average frequency and some appro-
priate initial values of ? and ?, leaving us with a
number of parameters equal to the number of fre-
quency buckets.
Initialization and Other Settings We train two
variations on the same model, setting m from Al-
gorithm 1 to 5 and 10. We initialize ? to 0.01 and ?
to ?0.1 for each and use 10 frequency buckets.
Results As in Experiment 1, we find that both
model variations converge after roughly 10 itera-
tions. The random walk parameters learned for
both variations in the highest frequency bucket,
freq(w?)?? ? 0.004803 and 0.004845 for 5 and
10 candidates, respectively, seem to validate our
hypothesis that we should prefer rare unobserved
words. However, the parameters learned for the pro-
ceeding buckets do not indicate the smooth positive
slope that we might have hoped for, which we dis-
cuss further in Section 4. The 10 candidate variation
learns consistently higher parameter values than the
5 candidate variation, and we interpret this disparity
in the same way as in Experiment 1.
The METEOR and BLEU scores for this Experi-
ment are summarized in Table 1, and the final pa-
rameter values after 10 iterations are listed in Table
3. We discuss these in greater detail in Section 4.
4 Discussion
At first glance, the experimental results are less than
satisfactory. However, METEOR and BLEU do not
Table 3: Final parameter values after 10 iterations for Ex-
periment 2 with 5 and 10 word random walk candidate
limits.
Word Frequency Max 5 Max 10
(high to low) Candidates Candidates
Bucket 1 0.004803 0.004845
Bucket 2 0.031505 0.052706
Bucket 3 0.019211 0.036479
Bucket 4 0.006871 0.013130
Bucket 5 0.002603 0.005024
Bucket 6 0.000032 0.000599
Bucket 7 0.001908 0.003336
Bucket 8 0.000609 0.002771
Bucket 9 0.001256 0.002014
Bucket 10 0.006085 0.006828
tell the whole story. At a high level, these metrics
work by computing the level of agreement, e.g. un-
igram and bigram precision, between the sentence
being evaluated and a pool of ?correct? sentences
(Lavie and Agarwal, 2007; Papineni et al, 2002).
When the correct sentences agree strongly with each
other, the evaluated sentence is heavily penalized
for any departures from the correct sentence pool.
This sort of penalization can occur even when the
model-corrected sentence is a perfectly valid correc-
tion that just had the misfortune of choosing a dif-
ferent replacement word than the majority of the hu-
man workers. For example, one ESL sentence in
our evaluation set reads, progress of medical science
helps human live longer. All four of our models cor-
rect this to progress of medical science helps peo-
ple live longer, but none of the workers correct to
?people,? instead opting for ?humans.? This issue is
exacerbated by the fact that Mechanical Turk work-
ers were instructed to change each ESL sentence as
little as possible, which helps their consistency but
hurts these particular models? evaluation scores.
With the exception of some mostly harmless but
ultimately useless exchanges, e.g. changing ?reduce
mistakes? to ?reduce errors,? the models actually do
fairly well when they correct ungrammatical words
and phrases. As we alluded to in Section 1, all four
model variations correct the sentence to begin with,
i?d rather not room with someone who is a entire
stranger to me from our development set to to be-
176
gin with, i?d rather not room with someone who is
a complete stranger to me. But only 2 out of 5 hu-
man workers make this correction, 2 retain ?entire,?
and 1 removes it altogether. As another example, all
model variations correct however, depending merely
on luck is very dangerous from our evaluation set to
however, depending solely on luck is very danger-
ous. However, only 1 worker corrects ?merely? to
?solely,? with the others either preferring to retain
?merely? or leaving it out entirely.
None of this is to say that the models suffer only
from an unfortunate difference in correction bias rel-
ative to the workers, or even that the models make
good corrections a majority of the time. In fact, they
make a range of false-positive corrections as well4.
These seem to fall into three major categories: slight
preferences for similar words that don?t fit in the
overall context of the sentence or change its mean-
ing in an undesired way, e.g. changing ?roommate?
to ?lodger? in you and your roommate must dev-
ide [sic] the housework, strong preferences for very
common words in the local context that render the
corrected sentence ungrammatical, e.g. changing
?compose? to ?take? in first, during childhood years,
we compose our personality, and misinterpretations
of ambiguous parts of speech that cause nouns to
be replaced with verbs, etc., e.g. changing ?circum-
stance? to ?go? in . . . that help you look abound your
circumstance and find out . . . .
Many of these issues can be blamed at least par-
tially on the myopia of the language model, which,
for example, vastly prefers ?go and find? to ?cir-
cumstance and find.? However, they can also be
attributed to the motivational intuition for Experi-
ment 2, which states that we should avoid replacing
observed words with common alternatives. While
Table 3 does demonstrate that the models in Ex-
periment 2 learn this preference to a degree for the
highest frequency bucket, the proceeding buckets do
not exhibit a smooth upwards slope analogous to the
function being approximated. Indeed, the words in
bucket 2 are preferred an order of magnitude more
4Although Type I errors are of course undesirable, Gamon
et al (2009) suggest that learners are able to effectively distin-
guish between good and bad corrections when presented with
possible error locations and scored alternatives. Such an inter-
active system is beyond the scope of this paper but nonetheless
feasible without significant model modification.
than those in bucket 1. This can be traced to the
truncation policy of Algorithm 1, which selects only
the highest frequency words from an over-sized set
of random walk candidates. While it is unclear how
to intelligently select a good candidate set of man-
ageable size, a policy that butts heads with our intu-
ition about which words we should be correcting is
clearly not the right one.
The differences between the models themselves
are somewhat more difficult to interpret. The 5 and
10 candidate variations of Experiment 1 and those
of Experiment 2 correct 103, 108, 115, and 130 sen-
tences out of 1017, respectively, and at least one
model differs from the others on 123 of those sen-
tences (they all agree on 42 sentences). These dis-
agreements are of all types: sometimes only a single
model corrects or vice versa, sometimes two models
are pitted against the other two, and occasionally all
four will choose a different word, but none of these
inconsistencies seem to follow any sort of pattern,
e.g. the two five candidate models agreeing more
often than the other two or the like.
Interestingly, however, the models tend to be in
agreement on the sentences that they correct the
most effectively. We explore this more concretely in
Table 4, in which we manually judge the quality of
sentence corrections versus the agreement between
models. Specifically, we judge a set of sentence
corrections as Good if all of the corrections made
between models improve sentence grammaticality,
Harmless if the corrections do not significantly im-
prove or reduce grammaticality, and Bad if at least
one of the corrections is either ungrammatical or
changes the sentence meaning. We note that Bad
corrections for the most part do not take grammatical
sentences and make them ungrammatical, only per-
turb them in some other erroneous fashion. Clearly,
there is a strong correlation between corrected sen-
tence quality and model agreement. We conclude
from this observation that the models are all learn-
ing to correct the most unambiguously incorrect sen-
tences in a consistent way, but where some deal of
ambiguity remains, they are subject to random dif-
ferences inherent in each?s construction.
To round out our evaluation of correction qual-
ity, we presented the corrected sentences from all
4 model variations to human workers for judgment
using the task detailed in Section 3.2.7. The results
177
Table 4: Manual judgments of model-corrected sentence
quality between experiments. If all models are in agree-
ment, a sentence is marked as Same, and Different oth-
erwise. We judge a set of sentence corrections as Good
if all of the corrections made between models improve
sentence grammaticality, Harmless if the corrections do
not significantly improve or reduce grammaticality, and
Bad if at least one of the corrections is either ungram-
matical or changes the sentence meaning. Only corrected
sentences are listed.
Model Judgment # of % ofAgreement Sentences Total
Same
Good 6 14.3%
Harmless 11 26.2%
Bad 25 59.5%
Total 42 ?
Different
Good 4 3.3%
Harmless 34 27.6%
Bad 85 69.1%
Total 123 ?
of this effort are detailed in Figure 3. The work-
ers are perhaps a bit more generous with their judg-
ments than we are, but overall, they tend towards the
same results that we do in our manual evaluation.
Aside from the conclusions already presented, the
worker judgments do expose one interesting finding:
When the corrected sentence is judged to be at least
as grammatical as the ESL sentence, it also tends
to preserve the ESL sentence?s meaning. However,
when the ESL sentence is judged more correct, the
meaning preservation trend is reversed. This obser-
vation leads us to believe that incorporating some
measure of semantic distance into our random walk
function f might prove effective.
5 Conclusion and Future Work
We have presented a novel noisy channel model for
correcting a broad class of language learner produc-
tion errors. Although our experimental results are
mixed, we believe that our model constitutes an in-
teresting and potentially very fruitful approach to
ESL grammar correction. There are a number of
opportunities for improvement available. Using a
richer language model, such as a PCFG, would un-
doubtedly improve our results. Noting that ESL er-
rors tend to occur in groups within sentences and
Figure 3: Human judgments of corrected sentences gath-
ered using Mechanical Turk. The items listed in the leg-
end are answers to the questions Between the [original
(ESL) and corrected] sentences, which is more correct? /
Is the meaning of the corrected sentence significantly dif-
ferent from that of the ESL sentence? See Section 3.2.7
for methodological details and Section 4 for results dis-
cussion.
12.1% 
14.4% 
16.2% 
14.8% 
20.0% 
18.8% 
19.4% 
21.3% 
29.3% 
28.5% 
31.4% 
30.1% 
18.2% 
16.1% 
15.4% 
14.5% 
7.7% 
7.3% 
6.2% 
6.1% 
12.7% 
14.9% 
11.4% 
13.2% 
0.0% 5.0% 10.0% 15.0% 20.0% 25.0% 30.0% 35.0%
Exp. 2
10 candidates
Exp. 2
5 candidates
Exp. 1
10 candidates
Exp. 1
5 candidates
Corrected better/Same meaning Corrected better/Different meaning
ESL better/Same meaning ESL better/Different meaning
Both equally good/Same meaning Both equally good/Different meaning
are often interdependent, the addition of other noise
models, such as those detailed in Park and Levy
(2011), would further improve things by allowing
the language model to consider a wider range of cor-
rected contexts around each word. Our random walk
model itself could also be improved by incorporat-
ing observed word frequency information or some
notion of semantic difference between observed and
unobserved words, or by learning separate parame-
ters for different word classes. Somewhat counter-
intuitively, a structured reduction of dictionary rich-
ness could also yield better results by limiting the
breadth of random walk candidates. Finally, a more
intelligent heuristic for truncating large sets of ran-
dom walk candidates would likely foster improve-
ment.
Acknowledgments
We would like to thank three anonymous reviewers
for their insightful comments and suggestions, and
Markus Dreyer for providing us with his expecta-
tion semiring code. Additionally, we are grateful to
the San Diego Supercomputer Center for allowing
us access to DASH.
178
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Open-
fst: a general and efficient weighted finite-state trans-
ducer library. In Proceedings of the 12th international
conference on Implementation and application of au-
tomata, CIAA?07, pages 11?23, Berlin, Heidelberg.
Springer-Verlag.
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX Lexical Database. Release
2 (CD-ROM). Linguistic Data Consortium, University
of Pennsylvania, Philadelphia, Pennsylvania.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting esl errors using phrasal smt tech-
niques. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 249?256, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263?311, June.
Lou Burnard. 1995. Users Reference Guide British Na-
tional Corpus Version 1.0. Oxford University Com-
puting Services, UK.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th annual meeting on
Association for Computational Linguistics, ACL ?96,
pages 310?318, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum likelihood from incomplete data via
the EM algorithm (with discussion). Journal of the
Royal Statistical Society B, 39:1?38.
Alain De?silets and Matthieu Hermet. 2009. Using au-
tomatic roundtrip translation to repair general errors in
second language writing. pages 198?206. MT Summit
XII.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 1080?1089, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 1?8, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Michael Gamon, Claudia Leacock, Chris Brockett,
William B Dolan, Jianfeng Gao, Dmitriy Belenko, and
Alexandre Klementiev. 2009. Using statistical tech-
niques and web search to correct esl errors. CALICO
Journal, 26:491?511.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 145?153, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings
of the Second Workshop on Statistical Machine Trans-
lation, StatMT ?07, pages 228?231, Stroudsburg, PA,
USA. Association for Computational Linguistics.
John Lee and Stephanie Seneff. 2006. Automatic gram-
mar correction for second-language learners. ICSLP.
Mehryar Mohri. 2001. Generic -removal algorithm
for weighted automata. In Shen Yu and Andrei Paun,
editors, Implementation and Application of Automata,
volume 2088 of Lecture Notes in Computer Science,
pages 230?242. Springer Berlin / Heidelberg.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Y. Albert Park and Roger Levy. 2011. Automated whole
sentence grammar correction using a noisy channel
model. ACL ?11. Association for Computational Lin-
guistics. In Press.
C. E. Shannon. 1948. A mathematical theory of commu-
nication. Bell Systems Technical Journal, 27:623?656.
179
