Proceedings of the ACL-HLT 2011 System Demonstrations, pages 26?31,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar
Chung-Chi Huang Mei-Hua Chen Shih-Ting Huang Jason S. Chang
Institute of Information Systems and Department of Computer Science,
Applications, National Tsing Hua University, National Tsing Hua University,
HsinChu, Taiwan, R.O.C. 300 HsinChu, Taiwan, R.O.C. 300
{u901571,chen.meihua,koromiko1104,Jason.jschang}@gmail.com
Abstract
We introduce a new method for learning to 
detect grammatical errors in learner?s writ-
ing and provide suggestions. The method 
involves parsing a reference corpus and 
inferring grammar patterns in the form of a 
sequence of content  words, function words, 
and parts-of-speech (e.g., ?play ~ role in 
Ving? and ?look forward to  Ving?). At run-
time, the given passage submitted by the 
learner is matched using an extended 
Levenshtein algorithm against  the set  of 
pattern rules in order to detect  errors and 
provide suggestions. We present a proto-
type implementation of the proposed 
method, EdIt, that  can handle a broad range 
of errors. Promising results are illustrated 
with three common types of errors in non-
native writing.
1 Introduction
Recently, an increasing number of research has 
targeted language learners? need in editorial assis-
tance including detecting and correcting grammar 
and usage errors in texts written in a second lan-
guage. For example, Microsoft  Research has de-
veloped the ESL Assistant, which provides such a 
service to ESL and EFL learners.
Much of the research in this area depends on 
hand-crafted rules and focuses on certain error 
types. Very little research provides a general 
framework for detecting and correcting all types of 
errors. However, in the sentences of ESL writing, 
there may be more than one errors and one error 
may affect the performance of handling other er-
rors. Erroneous sentences could be more efficiently 
identified and corrected if a grammar checker han-
dles all errors at  once, using a set of pattern rules 
that reflect the predominant usage of the English 
language.
Consider the sentences, ?He play an important 
roles to close this deals.? and ?He looks forward to 
hear you.? The first  sentence contains inaccurate 
word forms (i.e., play, roles, and deals), and rare 
usage (i.e., ?role to close?), while the second sen-
tence use the incorrect verb form of ?hear?. Good 
responses to these writing errors might  be (a) Use 
?played? instead of ?play.? (b) Use ?role? instead 
of ?roles?,  (c) Use ?in closing? instead of ?to 
close? (d) Use ?to hearing? instead of ?to hear?, 
and (e) insert  ?from? between ?hear? and ?you.? 
These suggestions can be offered by learning the 
patterns rules related to ?play ~ role? and ?look 
forward? based on analysis of ngrams and collo-
cations in a very large-scale reference corpus. With 
corpus statistics, we could learn the needed phra-
seological tendency in the form of pattern rules 
such as ?play ~ role in  V-ing) and ?look forward 
to V-ing.? The use of such pattern rules is in line 
with the recent  theory of Pattern Grammar put 
forward by Hunston and Francis (2000).
We present  a system, EdIt, that automatically 
learns to provide suggestions for rare/wrong usages 
in non-native writing. Example EdIt  responses to a 
26
text are shown in Figure 1. EdIt has retrieved the 
related pattern grammar of some ngram and collo-
cation sequences given the input  (e.g., ?play ~ role 
in V-ing
1
?, and ?look forward to V-ing?). EdIt 
learns these patterns during pattern extraction 
process by syntactically analyzing a collection of 
well-formed, published texts.
At run-time, EdIt first processes the input  pas-
sages in the article (e.g., ?He play an important 
roles to close ?) submitted by the L2 learner. And 
EdIt  tag the passage with part  of speech informa-
tion, and compares the tagged sentence against  the 
pattern rules anchored at  certain collocations (e.g., 
?play ~ role? and ?look forward?). Finally, EdIt 
finds the minimum-edit-cost  patterns matching the 
passages using an extended Levenshtein?s algo-
rithm (Levenshtein, 1966). The system then high-
lights the edits and displays the pattern rules as 
suggestions for correction. In our prototype, EdIt 
returns the preferred word form and preposition 
usages to the user directly (see Figure 1); alterna-
tively, the actual surface words (e.g., ?closing? and 
?deal?) could be provided.
Input:
Related pattern rules
play ~ role in Noun
play ~ role in V-ing
he plays DET
he played DET
look forward to V-ing
hear from PRON ...
Suggestion:
He played an important role in closing this deal. He looks 
forward to hearing from you.
He play an important roles to close this 
deals. He looks forward to hear you.
Figure 1. Example responses to the non-native writing.
2 Related Work
Grammar checking has been an area of active re-
search. Many methods, rule-oriented or data-
driven, have been proposed to tackle the problem 
of detecting and correcting incorrect grammatical 
and usage errors in learner texts. It is at  times no 
easy to distinguish these errors. But Fraser and 
Hodson (1978) shows the distinction between these 
two kinds of errors.
For some specific error types (e.g., article and 
preposition error), a number of interesting rule-
based systems have been proposed. For example, 
Uria et al (2009) and Lee et  al. (2009) leverage 
heuristic rules for detecting Basque determiner and 
Korean particle errors, respectively. Gamon et  al. 
(2009) bases some of the modules in ESL Assistant 
on rules derived from manually inspecting learner 
data. Our pattern rules, however, are automatically 
derived from readily available well-formed data, 
but nevertheless very helpful for correcting errors 
in non-native writing.
More recently, statistical approaches to develop-
ing grammar checkers have prevailed. Among un-
supervised checkers, Chodorow and Leacock 
(2000) exploits negative evidence from edited tex-
tual corpora achieving high precision but low re-
call, while Tsao and Wible (2009) uses general 
corpus only. Additionally, Hermet et al (2008) and 
Gamon and Leacock (2010) both use Web as a 
corpus to detect  errors in non-native writing. On 
the other hand, supervised models, typically treat-
ing error detection/correction as a classification 
problem, may train on well-formed texts as in the 
methods by De Felice and Pulman (2008) and Te-
treault et  al. (2010), or with additional learner texts 
as in the method proposed by Brockett et al 
(2006). Sun et  al. (2007) describes a method for 
constructing a supervised detection system trained 
on raw well-formed and learner texts without error 
annotation. 
Recent work has been done on incorporating 
word class information into grammar checkers. For 
example, Chodorow and Leacock (2000) exploit 
bigrams and trigrams of function words and part-
of-speech (PoS) tags, while Sun et al (2007) use 
labeled sequential patterns of function, time ex-
pression, and part-of-speech tags. In an approach 
similar to our work, Tsao and Wible (2009) use a 
combined ngrams of words forms, lemmas, and 
part-of-speech tags for research into constructional 
phenomena. The main differences are that  we an-
chored each pattern rule in lexical collocation so 
as to avoid deriving rules that  is may have two 
1
 In the pattern rules, we translate the part-of-speech tag to labels that are commonly used in learner dictionaries. For 
instance, we use V-ing for the tag VBG denoting the progressive verb form, and Pron and Pron$ denotes a pronoun 
and a possessive pronoun respectively.
27
consecutive part-of-speech tags (e.g, ?V Pron$ 
socks off?). The pattern rules we have derived are 
more specific and can be effectively used in detect-
ing and correcting errors.
In contrast  to the previous research, we intro-
duce a broad-coverage grammar checker that ac-
commodates edits such as substitution, insertion 
and deletion, as well as replacing word forms or 
prepositions using pattern rules automatically de-
rived from very large-scale corpora of well-formed 
texts.
3 The EdIt System
Using supervised training on a learner corpus is not 
very feasible due to the limited availability of 
large-scale annotated non-native writing. Existing 
systems trained on learner data tend to offer high 
precision but low recall. Broad coverage grammar 
checkers may be developed using readily available 
large-scale corpora. To detect  and correct errors in 
non-native writing, a promising approach is to 
automatically extract  lexico-syntactical pattern 
rules that  are expected to distinguish correct and in 
correct sentences.
3.1 Problem Statement
We focus on correcting grammatical and usage 
errors by exploiting pattern rules of specific collo-
cation (elastic or rigid such as ?play ~ rule? or 
?look forward?). For simplification, we assume 
that there is no spelling errors. EdIt provides sug-
gestions to common writing errors
2
 of the follow-
ing correlated with essay scores
3
.
(1)  wrong word form
(A) singular determiner preceding plural noun
(B) wrong verb form: concerning modal verbs (e.g., 
?would said?), subject-verb agreement, auxiliary 
(e.g., ?should have tell the truth?), gerund and in-
finitive usage (e.g., ?look forward to see you? and 
?in an attempt to helping you?)
(2) wrong preposition (or infinitive-to)
(A) wrong preposition (e.g., ?to depends of it?)
(B) wrong preposition and verb form (e.g., ?to play 
an important role to close this deal?)
(3) transitivity errors
(A) transitive verb (e.g., ?to discuss about the mat-
ter? and ?to affect to his decision?)
(B) intransitive verb (e.g., ?to listens the music?)
The system is designed to find pattern rules related 
to the errors and return suggestionst. We now for-
mally state the problem that we are addressing.
Problem  Statement: We are given a reference 
corpus C and a non-native passage T. Our goal is 
to detect  grammatical and usage errors in T and 
provide suggestions for correction. For this, we 
extract a set of pattern rules, u
1
,?, u
m
 from C 
such that the rules reflect the predominant usage 
and are likely to distinguish most errors in non-
native writing. 
In the rest  of this section, we describe our solu-
tion to this problem. First, we define a strategy for 
identifying predominant  phraseology of frequent 
ngrams and collocations in Section 3.2. Afer that, 
we show how EdIt proposes grammar correc-
tionsedits to non-native writing at  run-time in Sec-
tion 3.3.
3.2 Deriving Pattern Rules
We attempt  to derive patterns (e.g., ?play ~ role in 
V-ing?) from C expected to represent the immedi-
ate context  of collocations (e.g., ?play ~ role? or 
?look forward?). Our derivation process consists of 
the following four-stage:
Stage 1. Lemmatizing, POS Tagging and Phrase 
chunking. In the first  stage, we lemmatize and tag 
sentences in C. Lemmatization and POS tagging 
both help to produce more general pattern rules 
from ngrams or collocations. The based phrases are 
used to extract collocations.
Stage 2. Ngrams and Collocations. In the second 
stage of the training process, we calculate ngrams 
and collocations in C, and pass the frequent 
ngrams and collocations to Stage 4.
We employ a number of steps to acquire statisti-
cally significant collocations--determining the pair 
of head words in adjacent base phrases, calculating 
their pair-wise mutual information values, and fil-
tering out candidates with low MI values. 
Stage 3. onstructing Inverted Files. In the third 
stage in the training procedure, we build up in-
verted files for the lemmas in C for quick access in 
Stage 4. For each word lemma we store surface 
words, POS tags, pointers to sentences with base 
phrases marked.
2
 See (Nicholls, 1999) for common errors.
3
 See (Leacock and Chodorow, 2003) and (Burstein et al, 2004) for correlation.
28
procedure GrammarChecking(T,PatternGrammarBank)
(1) Suggestions=??//candidate suggestions
(2) sentences=sentenceSplitting(T)
for each sentence in sentences
(3)   userProposedUsages=extractUsage(sentence)
for each userUsage in userProposedUsages
(4)     patGram=findPatternGrammar(userUsage.lexemes, 
PatternGrammarBank)
(5)     minEditedCost=SystemMax; minEditedSug=??
for each pattern in patGram
(6)        cost=extendedLevenshtein(userUsage,pattern)
if cost<minEditedCost
(7)            minEditedCost=cost; minEditedSug=pattern
if minEditedCost>0
(8)       append (userUsage,minEditedSug) to Suggestions
(9) Return Suggestions
Figure 2. Grammar suggestion/correction at run-time
Stage 4. Deriving pattern rules. In the fourth and 
final stage, we use the method described in a pre-
vious work (Chen et al, 2011) and use the inverted 
files to find all sentences containing a give word 
and collocation. Words surrounding a collocation 
are identified and generalized based on their corre-
sponding POS tags. These sentences are then trans-
formed into a set  of n-gram of words and POS 
tags, which are subsequently counted and ranked to 
produce  pattern rules with high frequencies. 
3.3 Run-Time Error Correction
Once the patterns rules are derived from a corpus 
of well-formed texts, EdIt utilizes them to check 
grammaticality and provide suggestions for a given 
text via the procedure in Figure 2.
In Step (1) of the procedure, we initiate a set 
Suggestions to collect grammar suggestions to the 
user text T according to the bank of pattern gram-
mar PatternGrammarBank. Since EdIt  system fo-
cuses on grammar checking at  sentence level, T is 
heuristically split (Step (2)).
For each sentence, we extract ngram and POS 
tag sequences userUsage in T. For the example of 
?He play an important  roles. He looks forword to 
hear you?,  we extract ngram such as he V DET, 
play an JJ NNS, play ~ roles to V, this NNS, look 
forward to VB, and hear Pron. 
For each userUsage, we first access the pattern 
rules related to the word and collocation within 
(e.g., play-role  patterns for ?play ~ role to close?) 
Step (4). And then we compare userUsage against 
these rules (from Step (5) to (7)). We use the ex-
tended Levenshtein?s algorithm shown in Figure 3 
to compare userUsage and pattern rules.
Figure 3. Algorithm for identifying errors
If only partial matches are found for userUsage, 
that could mean we have found a potential errors. 
We use minEditedCost and minEditedSug to con-
train the patterns rules found for error suggestions 
(Step (5)). In the following, we describe how to 
find minimal-distance edits.
In Step (1) of the algorithm in Figure 3 we allo-
cate and initialize costArray to gather the dynamic 
programming based cost  to transform userUsage 
into a specific contextual rule pattern. Afterwards, 
the algorithm defines the cost of performing substi-
tution (Step (2)), deletion (Step (3)) and insertion 
(Step (4)) at  i-indexed userUsage and j-indexed 
pattern. If the entries userUsage[i] and pattern[j] 
are equal literally (e.g., ?VB? and ?VB?) or gram-
matically (e.g., ?DT? and ?Pron$?), no edit  is 
needed, hence, no cost  (Step (2a)). On the other 
hand, since learners tend to select wrong word 
form and preposition, we set a lower cost  for  sub-
stitution among different word forms of the same 
lemma or lemmas with the same POS tag (e.g., 
replacing V with V-ing or replacing to with in?. In 
addition to the conventional deletion and insertion 
(Step (3b) and (4b) respectively), we look ahead to 
the elements userUsage[i+1] and pattern[j+1] con-
sidering the fact that  ?with or without preposition? 
and ?transitive or intransitive verb? often puzzles 
EFL learners (Step (3a) and (4a)). Only a small 
edit cost is counted if the next  elements in use-
rUsage and Pattern are ?equal?. In Step (6) the 
extended Levenshtein?s algorithm returns the 
minimum edit  cost of revising userUsage using 
pattern.
Once we obtain the costs to transform the use-
rUsage into a similar, frequent pattern rules, we 
propose the minimum-cost rules as suggestions  for 
procedure extendedLevenshtein(userUsage,pattern)
(1) allocate and initialize costArray
for i in range(len(userUsage))
for j in range(len(pattern))
if equal(userUsage[i],pattern[j]) //substitution
(2a)       substiCost=costArray[i-1,j-1]+0
elseif sameWordGroup(userUsage[i],pattern[j])
(2b)       substiCost=costArray[i-1,j-1]+0.5
(2c)     else substiCost=costArray[i-1,j-1]+1
if equal(userUsage[i+1],pattern[j+1]) //deletion
(3a)       delCost=costArray[i-1,j]+smallCost
(3b)     else delCost=costArray[i-1,j]+1
if equal(userUsage[i+1],pattern[j+1]) //insertion
(4a)        insCost=costArray[i,j-1]+smallCost
(4b)      else insCost=costArray[i,j-1]+1
(5)        costArray[i,j]=min(substiCost,delCost,insCost)
(6) Return costArray[len(userUsage),len(pattern)]
29
correction (e.g., ?play ~ role in V-ing? for revising 
?play ~ role to V?) (Step (8) in Figure 2), if its 
minimum edit  cost  is greater than zero. Otherwise, 
the usage is considered valid. Finally, the Sugges-
tions accumulated for T are returned to users (Step 
(9)). Example input  and editorial suggestions re-
turned to the user are shown in Figure 1. Note that 
pattern rules involved flexible collocations are de-
signed to take care of long distance dependencies 
that might be always possible to cover with limited 
ngram (for n less than 6). In addition, the long pat-
ter rules can be useful even when it  is not  clear 
whether there is an error when looking at a very 
narrow context. For example, ?hear? can be either 
be transitive or intransitive depending on context. 
In the context of ?look forward to? and person 
noun object, it is should be intransitive and  require 
the preposition ?from? as suggested in the results 
provided by EdIt (see Figure 1).
In existing grammar checkers, there are typically 
many modules examining different types of errors 
and different module may have different  priority 
and conflict  with one another. Let  us note that this 
general framework for error detection and correc-
tion is an original contribution of our work. In ad-
dition, we incorporate probabilities conditioned on 
word positions in order to weigh edit  costs. For 
example, the conditional probability of V to imme-
diately follow ?look forward to? is virtually  0, 
while the probability of V-ing  to do so is approxi-
mates 0.3. Those probabilistic values are used to 
weigh different edits. 
4 Experimental Results
In this section, we first present the experimental 
setting in EdIt (Section 4.1). Since our goal is to 
provide to learners a means to efficient  broad-
coverage grammar checking, EdIt  is web-based 
and the acquisition of the pattern grammar in use is 
offline. Then, we illustrate three common types of 
errors, scores correlated, EdIt
4
 capable of handling.
4.1 Experimental Setting
We used British National Corpus (BNC) as our 
underlying general corpus C. It is a 100 million 
British English word collection from a wide range 
of sources. We exploited GENIA tagger to obtain 
the lemmas, PoS tags and shallow parsing results 
of C?s sentences, which were all used in construct-
ing inverted files and used as examples for GRASP 
to infer lexicalized pattern grammar.
Inspired by (Chen et al, 2011) indicating EFL 
learners tend to choose incorrect  prepositions and 
following word forms following a VN collocation, 
and (Gamon and Leacock, 2010) showing fixed-
length and fixed-window lexical items are the best 
evidence for correction, we equipped EdIt with 
pattern grammar rules consisting of fixed-length 
(from one- to five-gram) lexical sequences or VN 
collocations and their fixed-window usages (e.g., 
?IN(in) VBG? after ?play ~ role?, for window 2).
4.2 Results
We examined three types of errors and the mixture 
of them for our correction system (see Table 1). In 
this table, results of ESL Assistant  are shown for 
comparison, and grammatical suggestions are un-
derscored. As suggested, lexical and PoS informa-
tion in learner texts is useful for a grammar 
checker, pattern grammar EdIt  uses is easily acces-
sible and effective in both grammaticality and us-
age check, and a weighted extension to Leven-
shtein?s algorithm in EdIt  accommodates substitu-
tion, deletion and insertion edits to learners? fre-
quent mistakes in writing.
5 Future Work and Summary 
Many avenues exist  for future research and im-
provement. For example, we could augment  pat-
tern grammar with lexemes? PoS information in 
that the contexts of a word of different PoS tags 
vary. Take discuss for instance. The present tense 
verb discuss is often followed by determiners and 
nouns while the passive is by the preposition in  as 
in ?? is discussed in Chapter one.? Additionally, 
an interesting direction to explore is enriching pat-
tern grammar with semantic role labels (Chen et 
al., 2011) for simple semantic check.
In summary, we have introduced a method for 
correcting errors in learner text based on its lexical 
and PoS evidence. We have implemented the 
method and shown that the pattern grammar and 
extended Levenshtein algorithm in this method are 
promising in grammar checking. Concerning EdIt?s 
broad coverage over different  error types, simplic-
ity in design, and short  response time, we plan to 
evaluate it more fully: with or without  conditional 
probability using majority voting or not.
4
 At http://140.114.214.80/theSite/EdIt_demo2/
30
Erroneous sentence EdIt suggestion ESL Assistant suggestion
Incorrect word form
? a sunny days ? a sunny N a sunny day
every days, I ? every N every day
I would said to ? would V would say
he play a ? he V-ed none
? should have tell the truth should have V-en should have to tell
? look forward to see you look forward to V-ing none
? in an attempt to seeing you an attempt to V none
? be able to solved this problem able to V none
Incorrect preposition
he plays an important role to close ? play ~ role in none
he has a vital effect at her. have ~ effect on effect on her
it has an effect on reducing ? have ~ effect of V-ing none
? depend of the scholarship depend on depend on
Confusion between intransitive and transitive verb
he listens the music. missing ?to? after ?listens? missing ?to? after ?listens?
it affects to his decision. unnecessary ?to? unnecessary ?to?
I understand about the situation. unnecessary ?about? unnecessary ?about?
we would like to discuss about this matter. unnecessary ?about? unnecessary ?about?
Mixed
she play an important roles to close this deals. she V-ed; an Adj N;
play ~ role in V-ing; this N
play an important role;
close this deal
I look forward to hear you. look forward to V-ing;
missing ?from? after ?hear?
none
Table 1. Three common score-related error types and their examples with suggestions from EdIt and ESL Assistant.
References
C. Brockett, W. Dolan, and M. Gamon. 2006. Correcting ESL 
errors using phrasal SMT techniques. In Proceedings of the 
ACL.
J. Burstein, M. Chodorow, and C. Leacock. 2004. Automated 
essay evaluation: the criterion online writing  service. AI 
Magazine, 25(3):27-36.
M. H. Chen, C. C. Huang, S. T. Huang, H. C. Liou, and J. S. 
Chang. 2011. A cross-lingual pattern retrieval framework. 
In Proceedings of the CICLing.
M. Chodorow and C. Leacock. 2000. An unsupervised method 
for detecting grammatical  errors. In Proceedings of the 
NAACL, pages 140-147.
R. De Felice and S. Pulman. 2008. A classifer-based approach 
to  preposition and determiner error correction in  L2 Eng-
lish. In COLING.
I. S. Fraser and L. M. Hodson. 1978. Twenty-one kicks at the 
grammar horse. English Journal.
M. Gamon, C. Leacock, C. Brockett, W. B. Bolan, J. F. Gao, 
D. Belenko, and A. Klementiev.  Using statistical tech-
niques and web search to correct ESL errors. CALICO, 
26(3): 491-511.
M. Gamon and C. Leacock. 2010. Search right and thou shalt 
find ? using web queries for learner error detection. In 
Proceedings of the NAACL.
M. Hermet, A. Desilets, S. Szpakowicz. 2008. Using the web 
as a linguistic resource to automatically correct lexico-
syntatic errors. In LREC, pages 874-878.
S. Hunston and G. Francis. 2000. Pattern grammar: a corpus-
driven approach to the lexical grammar of English.
C. M. Lee, S. J. Eom, and M. Dickinson. 2009. Toward ana-
lyzing Korean learner particles. In CALICO.
V. I. Levenshtein. 1966. Binary codes capable of correcting 
deletions, insertions and reversals. Soviet Physics Doklady, 
10:707-710.
C. Leacock and M. Chodorow. 2003. Automated grammatical 
error detection.
D. Nicholls. 1999. The Cambridge Learner Corpus ? error 
coding and analysis for writing dictionaries and other 
books for English Learners.
G. H. Sun, X. H. Liu, G. Cong, M. Zhou, Z. Y. Xiong, J. Lee, 
and C. Y. Lin. 2007. Detecting erroneous sentences using 
automatically mined sequential patterns. In ACL.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse 
features for prepositions selection and error detection. In 
Proceedings of the ACL, pages 353-358.
N. L. Tsao and D. Wible. 2009. A method for unsupervised 
broad-coverage lexical error detection and correction. In 
NAACL Workshop, pages 51-54.
31
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 96?104,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
GRASP: Grammar- and Syntax-based Pattern-Finder in CALL 
 
 
Chung-Chi Huang*  Mei-Hua Chen* Shih-Ting Huang+  Hsien-Chin Liou**  Jason S. Chang+ 
  
* Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 300 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 300 
**  Department of Foreign Languages and Literature, NTHU, HsinChu, Taiwan, R.O.C. 300 
{u901571,chen.meihua,koromiko1104,hsienchin,jason.jschang}gmail.com 
 
 
 
 
 
 
Abstract 
We introduce a method for learning to 
describe the attendant contexts of a given 
query for language learning. In our 
approach, we display phraseological 
information in the form of a summary of 
general patterns as well as lexical bundles 
anchored at the query. The method 
involves syntactical analyses and inverted 
file construction. At run-time, grammatical 
constructions and their lexical 
instantiations characterizing the usage of 
the given query are generated and 
displayed, aimed at improving learners? 
deep vocabulary knowledge. We present a 
prototype system, GRASP, that applies the 
proposed method for enhanced collocation 
learning. Preliminary experiments show 
that language learners benefit more from 
GRASP than conventional dictionary 
lookup. In addition, the information 
produced by GRASP is potentially useful 
information for automatic or manual 
editing process. 
1 Introduction 
Many learners submit word or phrase queries (e.g., 
?role?) to language learning sites on the Web to 
get usage information every day, and an increasing 
number of services on the Web specifically target 
such queries. Language learning tools such as 
concordancers typically accept single-word queries 
and respond with example sentences containing the 
words. There are also collocation reference tools 
such as Sketch Engine and TANGO that provide 
co-occurring words for the query word. Another 
collocation tool, JustTheWord further organizes 
and displays collocation clusters. 
Learners may want to submit phrase queries 
(fixed or rigid collocaions) to learn further how to 
use the phrase in context, or in other words, to 
acquire the knowledge on the attendant 
phraseology of the query. These queries could be 
answered more appropriately if the tool accepted 
long queries and returned a concise summary of 
their surrounding contexts. 
Consider the query ?play role?. The best 
responses for this query are probably not just 
example sentences, but rather the phraseological 
tendencies described grammatically or lexically. A 
good response of such a summary might contain  
patterns such as ?play Det Adj role? (as in ?play an 
important role?) and ?play ~ role in V-ing? (as in 
?play ~ role in shaping ??). Intuitively, by 
exploiting simple part-of-speech analysis, we can 
derive such patterns, inspired by the grammatical 
theory of Pattern Grammar 1  in order to provide 
more information on demand beyond what is given 
in a grammar book. 
We present a system, GRASP, that provide a 
usage summary of the contexts of the query in the 
form of patterns and frequent lexical bundles. Such 
rich information is expected to help learners and 
lexicographers grasp the essence of word usages. 
An example GRASP response for the query ?play 
                                                           
1
 Please refer to (Hunston and Francis, 2000). 
96
role? is shown in Figure 1. GRASP has retrieved 
the sentences containing the query in a reference 
corpus. GRASP constructs these query-to-sentence 
index in the preparation stage (Section 3). 
 
Figure 1. An example GRASP search for ?play role?. 
 
At run-time, GRASP starts with a search query 
(e.g., ?play role?) submitted by the user. GRASP 
then retrieves example sentences and generates a 
summary of representative contexts, using patterns 
(e.g., ?play ~ role in V-ing?) and lexical bundles 
(e.g., ?play ~ role in shaping. In our 
implementation, GRASP also returns the 
translations and the example sentences of the 
lexical instances, so the learner can use their 
knowledge of native language to enhance the 
learning process. 
2 Related Work 
Computer-assisted language learning (CALL) has 
been an area of active research. Recently, more and 
more research based on natural language 
processing techniques has been done to help 
language learners. In our work, we introduce a 
language learning environment, where summarized 
usage information are provided, including how 
function words and verb forms are used in 
combination with the query. These usage notes 
often help contrast the common sources of error in 
learners? writing (Nicholls, 1999). In our pilot 
teaching experiment, we found learners have 
problems using articles and prepositions correctly 
in sentence composition (as high as 80% of the 
articles and 60% of the prepositions were used 
incorrectly), and GRASP is exactly aimed at 
helping ESL or EFL learners in that area. 
Until recently, collocations and usage 
information are compiled mostly manually 
(Benson et al, 1986). With the accessibility to 
large-scale corpora and powerful computers, it has 
become common place to compile a list of 
collocations automatically (Smadja, 1993). In 
addition, there are many collocation checkers 
developed to help non-native language learners 
(Chang et al, 2008), or learners of English for 
academic purposes (Durrant, 2009). 
Recently, automatic generation of collocations 
for computational lexicography and online 
language learning has drawn much attention. 
Sketch Engine (Kilgarriff et al, 2004) summarizes 
a word?s grammatical and collocation behavior, 
while JustTheWord clusters the co-occurring 
words of single-word queries and TANGO (Jian et 
al., 2004) accommodates cross-lingual collocation 
searches. Moreover, Cheng et al (2006) describe 
how to retrieve mutually expected words using 
concgrams. In contrast, GRASP, going one step 
further, automatically computes and displays the 
information that reveals the regularities of the 
contexts of user queries in terms of grammar 
patterns. 
Recent work has been done on incorporating 
word class information into the analyses of 
phraseological tendencies. Stubbs (2004) 
introduces phrase-frames, which are based on 
lexical ngrams with variable slots, while Wible et 
al. (2010) describe a database called StringNet, 
with lexico-syntactic patterns. Their methods of 
using word class information are similar in spirit to 
our work. The main differences are that our 
patterns is anchored with query words directly and 
generalizes query?s contexts via parts-of-speech, 
and that we present the query?s usage summary in 
Search query: 
Mapping query words to (position, sentence) pairs: 
?play? occurs in (10,77), (4,90), (6,102), ?, and so on. 
?role? occurs in (7,90), (12,122), (6,167), ?, and so on. 
A. In-between pattern grammar: 
   Distance 3 (1624): 
play DT JJ role (1364): 
e.g., ?play an important role? (259), ?play a major role? (168), ? 
play DT VBG role (123): 
e.g., ?play a leading role? (75), ?play a supporting role? (5), ? 
play DT JJR role (40): 
e.g., ?play a greater role? (17), ?play a larger role? (8), ? 
   Distance 2 (480): 
play DT role (63): 
e.g., ?play a role? (197), ?play the role? (123), ? 
play JJ role (63): 
e.g., ?play important role? (15), ?play different role? (6), ? 
   Distance 1 (6): 
play role (6) 
B. Subsequent pattern grammar: 
play ~ role IN(in) DT (707): 
e.g., ?play ~ role in the? (520), ?play ~ role in this? (24), ? 
play ~ role IN(in) VBG (407): 
e.g., ?play ~ role in shaping? (22), ? 
play ~ role IN(in) NN (166): 
e.g., ?play ~ role in society? (7), ?play ~ role in relation? (5), ? 
C. Precedent pattern grammar: 
NN MD play ~ role (83): 
e.g., ?communication will play ~ role ? (2), ? 
JJ NNS play ~ role (69): 
e.g., ?voluntary groups play ~ role? (2), ? 
Type your search query, and push GRASP! 
97
terms of function words as well as content word 
form (e.g., ?play ~ role in V-ing?), as well as 
elastic lexical bundles (e.g., ?play ~ role in 
shaping?). Additionally, we also use semantic 
codes (e.g., PERSON) to provide more information 
in a way similar what is provided in learner 
dictionaries. 
3 The GRASP System 
3.1 Problem Statement 
We focus on constructing a usage summary likely 
to explain the contexts of a given linguistic search. 
The usage summary, consisting of the query?s 
predominant attendant phraseology ranging from 
pattern grammar to lexical phrases, is then returned 
as the output of the system. The returned summary, 
or a set of patterns pivoted with both content and 
function words, can be used for learners? benefits 
directly, or passed on to an error detection and 
correction system (e.g., (Tsao and Wible, 2009) 
and some modules in (Gamon et al, 2009) as rules. 
Therefore, our goal is to return a reasonable-sized 
set of lexical and grammatical patterns 
characterizing the contexts of the query. We now 
formally state the problem that we are addressing. 
Problem Statement: We are given a reference 
corpus C from a wide range of sources and a 
learner search query Q. Our goal is to construct a 
summary of word usages based on C that is likely 
to represent the lexical or grammatical preferences 
on Q?s contexts. For this, we transform the words 
in Q into sets of (word position, sentence record) 
pairs such that the context information, whether 
lexically- or grammatical-oriented, of the querying 
words is likely to be acquired efficiently. 
In the rest of this section, we describe our 
solution to this problem. First, we define a strategy 
for preprocessing our reference corpus (Section 
3.2). Then, we show how GRASP generates 
contextual patterns, comprising the usage summary, 
at run-time (Section 3.3). 
3.2 Corpus Preprocessing 
We attempt to find the word-to-sentence mappings 
and the syntactic counterparts of the L1 sentences 
expected to speed up run-time pattern generation. 
Our preprocessing procedure has two stages. 
Lemmatizing and PoS Tagging. In the first stage, 
we lemmatize each sentence in the reference 
corpus C and generate its most probable POS tag 
sequence. The goal of lemmatization is to reduce 
the impact of morphology on statistical analyses 
while that of POS tagging is to provide a way to 
grammatically describe and generalize the 
contexts/usages of a linguistic query. Actually, 
using POS tags is quite natural: they are often used 
for general description in grammar books, such as 
one?s (i.e., possessive pronoun) in the phrase 
?make up one?s mind?, oneself (i.e., reflexive 
pronoun) in ?enjoy oneself very much?, 
superlative_adjective in ?the most 
superlative_adjective?, NN (i.e., noun) and VB (i.e., 
base form of a verb) in ?insist/suggest/demand that 
NN VB? and so on. 
Constructing Inverted Files. In the second stage, 
we build up inverted files of the lemmas in C for 
quick run-time search. For each lemma, we record 
the sentences and positions in which it occurs. 
Additionally, its corresponding surface word and 
POS tag are kept for run-time pattern grammar 
generation. 
 
Figure 2. Generating pattern grammar and usage 
summary at run-time. 
procedure GRASPusageSummaryBuilding(query,proximity,N,C) 
(1)  queries=queryReformulation(query) 
(2)  GRASPresponses= ?  
for each query in queries 
(3)    interInvList=findInvertedFile(w1 in query) 
for each lemma wi in query except for w1 
(4)      InvList=findInvertedFile(wi) 
//AND operation on interInvList and InvList 
(5a)    newInterInvList= ? ; i=1; j=1 
(5b)    while i<=length(interInvList) and j<=lengh(InvList) 
(5c)       if interInvList[i].SentNo==InvList[j].SentNo 
(5d)         if withinProximity(interInvList[i]. 
wordPosi,InvList[j].wordPosi,proximity) 
(5e)   Insert(newInterInvList, interInvList[i],InvList[j]) 
else if interInvList[i].wordPosi<InvList[j].wordPosi 
(5f)   i++ 
else //interInvList[i].wordPosi>InvList[j].wordPosi 
(5g)   j++ 
else if interInvList[i].SentNo<InvList[j].SentNo 
(5h)          i++ 
else //interInvList[i].SentNo>InvList[j].SentNo 
(5i)           j++ 
(5j)     interInvList=newInterInvList 
//construction of GRASP usage summary for this query 
(6)    Usage= ?  
for each element in interInvList 
(7)       Usage+={PatternGrammarGeneration(query,element,C)} 
(8a)  Sort patterns and their instances in Usage in descending order 
of frequency 
(8b)  GRASPresponse=the N patterns and instances in Usage with 
highest frequency 
(9)    append GRASPresponse to GRASPresponses 
(10) return GRASPresponses 
98
3.3 Run-Time Usage Summary Construction 
Once the word-to-sentence mappings and syntactic 
analyses are obtained, GRASP generates the usage 
summary of a query using the procedure in Figure 
2. 
In Step (1) we reformulate the user query into 
new ones, queries, if necessary. The first type of 
query reformulation concerns the language used in 
query. If it is not in the same language as C, we 
translate query and append the translations to 
queries as if they were submitted by the user. The 
second concerns the length of the query. Since 
single words may be ambiguous in senses and 
contexts or grammar patterns are closely associated 
with words? meanings (Hunston and Francis, 2000), 
we transform single-word queries into their 
collocations, particularly focusing on one word 
sense (Yarowsky, 1995), as stepping stones to 
GRASP patterns. Notice that, in implementation, 
users may be allowed to choose their own 
interested translation or collocation of the query 
for usage learning. The prototypes for first-
language (i.e., Chinese) queries and English 
queries of any length are at A2 and B3 respectively. 
The goal of cross-lingual GRASP is to assist EFL 
users even when they do not know the words of 
their searches and to avoid incorrect queries 
largely because of miscollocation, misapplication, 
and misgeneralization. 
Afterwards, we initialize GRASPresponses to 
collect usage summaries for queries (Step (2)) and 
leverage inverted files to extract and generate each 
query?s syntax-based contexts. In Step (3) we prep 
interInvList for the intersected inverted files of the 
lemmas in query. For each lemma wi within, we 
first obtain its inverted file, InvList (Step (4)) and 
perform an AND operation on interInvList 
(intersected results from previous iteration) and 
InvList (Step (5a) to (5j)4), defined as follows. 
First, we enumerate the inverted lists (Step (5b)) 
after the initialization of their indices i and j and 
temporary resulting intersection newInterInvList 
(Step (5a)). Second, we incorporate a new instance 
of (position, sentence), based on interInvList[i] and 
InvList[j], into newInterInvList (Step (5e)) if the 
sentence records of the indexed list elements are 
the same (Step (5c)) and the distance between their 
                                                           
2
 http://140.114.214.80/theSite/bGRASP_v552/ 
3
 http://140.114.214.80/theSite/GRASP_v552/ 
4
 These steps only hold for sorted inverted files. 
words are within proximity (Step (5d)). Otherwise, 
i and j are moved accordingly. To accommodate 
the contexts of queries? positional variants (e.g., 
?role to play? and ?role ~ play by? for the query 
?play role?), Step (5d) considers the absolute 
distance. Finally, interInvList is set for the next 
AND iteration (Step (5j)). 
Once we obtain the sentences containing query, 
we construct its context summary as below. For 
each element, taking the form ([wordPosi(w1), ?, 
wordPosi(wn)], sentence record) denoting the 
positions of query?s lemmas in the sentence, we 
generate pattern grammar involving replacing 
words in the sentence with POS tags and words in 
wordPosi(wi) with lemmas, and extracting fixed-
window 5  segments surrounding query from the 
transformed sentence. The result is a set of 
grammatical patterns with counts. Their lexical 
realizations also retrieved and displayed. 
The procedure finally generates top N 
predominant syntactic patterns and their N most 
frequent lexical phrases as output (Step (8)). The 
usage summaries GRASP returns are aimed to 
accelerate EFL learners? language understanding 
and learning and lexicographers? word usage 
navigation. To acquire more semantic-oriented 
patterns, we further exploit WordNet and majority 
voting to categorize words, deriving the patterns 
like ?provide PERSON with.? 
4 Experimental Results 
GRASP was designed to generate usage 
summarization of a query for language learning. 
As such, GRASP will be evaluated over CALL. In 
this section, we first present the setting of GRASP 
(Section 4.1) and report the results of different 
consulting systems on language learning in Section 
4.2. 
4.1 Experimental Setting 
We used British National Corpus (BNC) as our 
underlying reference corpus C. It is a British 
English text collection. We exploited GENIA 
tagger to obtain the lemmas and POS tags of C?s 
sentences. After lemmatizing and syntactic 
analyses, all sentences in BNC were used to build 
up inverted files and used as examples for 
grammar pattern extraction. 
                                                           
5
 Inspired by (Gamon and Leacock, 2010). 
99
English (E) sentence with corresponding Chinese (C) translation answer to 1st blank  answer to 2nd blank 
C: ????????????? 
E: Environmental protection has ___ impact ___. 
a profound on the Earth 
C: ????????????? 
E: The real estate agent ___ record profit ___. 
made a on house selling 
C: ?????????????? 
E: They plan to release their new album in ___ future 
the near none 
C: ???????????? 
E: He waited for her for a long time in ___ attempt ___ again. 
an to see her 
 
4.2 Results of Constrained Experiments 
In our experiments, we showed GRASP6  to two 
classes of Chinese EFL (first-year) college students. 
32 and 86 students participated, and were trained 
to use GRASP and instructed to perform a sentence 
translation/composition task, made up of pretest 
and posttest. In (30-minute) pretest, participants 
were to complete 15 English sentences with 
Chinese translations as hints, while, in (20-minute) 
posttest, after spending 20 minutes familiarizing 
word usages of the test candidates from us by 
consulting traditional tools or GRASP, participants 
were also asked to complete the same English 
sentences. We refer to the experiments as 
constrained ones since the test items in pre- and 
post-test are the same except for their order. A 
more sophisticated testing environment, however, 
are to be designed. 
Each test item contains one to two blanks as 
shown in the above table. In the table, the first item 
is supposed to test learners? knowledge on the 
adjective and prepositional collocate of ?have 
impact? while the second test the verb collocate 
make, subsequent preposition on, and preceding 
article a of ?record profit?. On the other hand, the 
third tests the ability to produce the adjective 
enrichment of ?in future?, and the fourth the in-
between article a or possessive his and the 
following infinitive of ?in attempt?. Note that as 
existing collocation reference tools retrieve and 
display collocates, they typically ignore function 
words like articles and determiners, which happen 
to be closely related to frequent errors made by the 
learners (Nicholls, 1999), and fail to provide an 
overall picture of word usages. In contrast, GRASP 
attempts to show the overall picture with 
appropriate function words and word forms. 
We selected 20 collocations and phrases 7 
manually from 100 most frequent collocations in 
                                                           
6
 http://koromiko.cs.nthu.edu.tw/grasp/ 
7
 Include the 15 test items. 
BNC whose MI values exceed 2.2 and used them 
as the target for learning between the pretest and 
posttest. To evaluate GRASP, half of the 
participants were instructed to use GRASP for 
learning and the other half used traditional tools 
such as online dictionaries or machine translation 
systems (i.e., Google Translate and Yahoo! Babel 
Fish). We summarize the performance of our 
participants on pre- and post-test in Table 1 where 
GRASP denotes the experimental group and TRAD 
the control group. 
 
 class 1 class 2 combined 
 pretest posttest  pretest  posttest  pretest posttest 
GRASP 26.4 41.9 43.6 58.4 38.9 53.9 
TRAD 27.1 32.7 43.8 53.4 39.9 48.6 
Table 1. The performance (%) on pre- and post-test. 
 
We observe in Table 1 that (1) the partition of 
the classes was quite random (the difference 
between GRASP and TRAD was insignificant 
under pretest); (2) GRASP summaries of words? 
contexts were more helpful in language learning 
(across class 1, class 2 and combined). Specifically, 
under the column of the 1st class, GRASP helped to 
boost students? achievements by 15.5%, almost 
tripled (15.5 vs. 5.6) compared to the gain using 
TRAD; (3) the effectiveness of GRASP in language 
learning do not confine to students at a certain 
level. Encouragingly, both high- and low-
achieving students benefited from GRASP if we 
think of students in class 2 and those in class 1 as 
the high and the low respectively (due to the 
performance difference on pretests). 
We have analyzed some participants? answers 
and found that GRASP helped to reduce learners? 
article and preposition errors by 28% and 8%, 
comparing to much smaller error reduction rate 7% 
and 2% observed in TRAD group. Additionally, an 
experiment where Chinese EFL students were 
asked to perform the same task but using GRASP 
as well as GRASP with translation information8 
                                                           
8
 http://koromiko.cs.nthu.edu.tw/grasp/ch 
100
was conducted. We observed that with Chinese 
translation there was an additional 5% increase in 
students? test performance. This suggests to some 
extent learners still depend on their first languages 
in learning and first-language information may 
serve as another quick navigation index even when 
English GRASP is presented. 
Overall, we are modest to say that (in the 
constrained experiments) GRASP summarized 
general-to-specific usages, contexts, or phrase-
ologies of words are quite effective in assisting 
learners in collocation and phrase learning. 
5 Applying GRASP to Error Correction 
To demonstrate the viability of GRASP-retrieved 
lexicalized grammar patterns (e.g., ?play ~ role In 
V-ING? and ?look forward to V-ING?) in error 
detection and correction, we incorporate them into 
an extended Levenshtein algorithm (1966) to 
provide broad-coverage sentence-level grammat-
ical edits (involving substitution, deletion, and 
insertion) to inappropriate word usages in learner 
text. 
Previously, a number of interesting rule-based 
error detection/correction systems have been 
proposed for some specific error types such as 
article and preposition error (e.g., (Uria et al, 
2009), (Lee et al, 2009), and some modules in 
(Gamon et al, 2009)). Statistical approaches, 
supervised or unsupervised, to grammar checking 
have become the recent trend. For example, 
unsupervised systems of (Chodorow and Leacock, 
2000) and (Tsao and Wible, 2009) leverage word 
distributions in general and/or word-specific 
corpus for detecting erroneous usages while 
(Hermet et al, 2008) and (Gamon and Leacock, 
2010) use Web as a corpus. On the other hand, 
supervised models, typically treating error 
detection/correction as a classification problem, 
utilize the training of well-formed texts ((De Felice 
and Pulman, 2008) and (Tetreault et al, 2010)), 
learner texts, or both pairwisely (Brockett et al, 
2006). Moreover, (Sun et al, 2007) describes a 
way to construct a supervised error detection 
system trained on well-formed and learner texts 
neither pairwise nor error tagged. 
In contrast to the previous work in grammar 
checking, our pattern grammar rules are 
automatically inferred from a general corpus (as 
described in Section 3) and helpful for correcting 
errors resulting from the others (e.g., ?to close? in 
?play ~ role to close?), our pattern grammar 
lexicalizes on both content and function words and 
lexical items within may be contiguous (e.g., ?look 
forward to V-ING PRP?) or non-contiguous (e.g., 
?play ~ role In V-ING?), and, with word class 
(POS) information, error correction or grammatical 
suggestion is provided at sentence level. 
5.1 Error Correcting Process 
Figure 3 shows how we check grammaticality and 
provide suggestions for a given text with accurate 
spelling. 
 
 
Figure 3. Procedure of grammar suggestion/correction. 
 
In Step (1), we initiate a set Suggestions to 
collect grammar suggestions to the user text T 
according to a bank of patterns 
PatternGrammarBank, i.e., a collection of 
summaries of grammatical usages (e.g., ?play ~ 
role In V-ING?) of queries (e.g., ?play role?) 
submitted to GRASP. Since we focus on grammar 
checking at sentence level, T is heuristically split 
(Step (2)). 
For each sentence, we extract user-proposed 
word usages (Step (3)), that is, the user 
grammatical contexts of ngram and collocation 
sequences. Take for example the (ungrammatical) 
sentences and their corresponding POS sequences 
?he/PRP play/VBP an/DT important/JJ roles/NNS 
to/TO close/VB this/DT deals/NNS? and ?he/PRP 
looks/VBZ forward/RB to/TO hear/VB you/PRP?. 
Ngram contexts include ?he VBP DT?, ?play an JJ 
NNS?, ?this NNS? for the first sentence and ?look 
forward to VB PRP? and ?look forward to hear 
PRP? for the second. And collocation contexts for 
procedure GrammarChecking(T,PatternGrammarBank) 
(1) Suggestions=??//candidate suggestions 
(2) sentences=sentenceSplitting(T) 
for each sentence in sentences 
(3)   userProposedUsages=extractUsage(sentence) 
for each userUsage in userProposedUsages 
(4)     patGram=findPatternGrammar(userUsage.lexemes, 
PatternGrammarBank) 
(5)     minEditedCost=SystemMax; minEditedSug=?? 
for each pattern in patGram 
(6)        cost=extendedLevenshtein(userUsage,pattern) 
if cost<minEditedCost 
(7)            minEditedCost=cost; minEditedSug=pattern 
if minEditedCost>0 
(8)       append (userUsage,minEditedSug) to Suggestions 
(9) Return Suggestions 
101
the first sentence are ?play ~ role to VERB? and 
?close ~ deal .? 
For each userUsage in the sentence (e.g., ?play 
~ role TO VB? and ?look forward to hear PRP?), 
we first acquire the pattern grammar of its lexemes 
(e.g., ?play role? and ?look forward to hear?) such 
as ?play ~ role in V-ing? and ?look forward to 
hear from? in Step (4), and we compare the user-
proposed usage against the corresponding 
predominant, most likely more proper, ones (from 
Step (5) to (7)). We leverage an extended 
Levenshtein?s algorithm in Figure 4 for usage 
comparison, i.e. error detection and correction, 
after setting up minEditedCost and minEditedSug 
for the minimum-cost edit from alleged error usage 
into appropriate one (Step (5)). 
 
 
Figure 4. Extended Levenshtein algorithm for correction. 
 
In Step (1) of the algorithm in Figure 4 we 
allocate and initialize costArray to gather the 
dynamic programming based cost to transform 
userUsage into a specific pattern. Afterwards, the 
algorithm defines the cost of performing 
substitution (Step (2)), deletion (Step (3)) and 
insertion (Step (4)) at i-indexed userUsage and j-
indexed pattern. If the entries userUsage[i] and 
pattern[j] are equal literally (e.g., ?VB? and ?VB?) 
or grammatically (e.g., ?DT? and ?PRP$?9), no edit 
                                                           
9
 ONE?S denotes possessives. 
is needed, hence, no cost (Step (2a)). On the other 
hand, since learners tend to select wrong word 
form and preposition, we make less the cost of the 
substitution of the same word group, say from 
?VERB? to ?V-ing?, ?TO? to ?In? and ?In? to 
?IN(on)? (Step (2b)) compared to a total edit (Step 
(2c)). In addition to the conventional deletion and 
insertion (Step (3b) and (4b) respectively), we look 
ahead to the elements userUsage[i+1] and 
pattern[j+1] considering the fact that ?with or 
without preposition? and ?transitive or intransitive 
verb? often puzzles EFL learners (Step (3a) and 
(4a)). Only a small edit cost is applied if the next 
elements in userUsage and Pattern are ?equal?. In 
Step (6) the extended Levenshtein?s algorithm 
returns the minimum cost to edit userUsage based 
on pattern. 
Once we obtain the costs to transform the 
userUsage into its related frequent patterns, we 
propose the minimum-cost one as its grammatical 
suggestion (Step (8) in Figure 3), if its minimum 
edit cost is greater than zero. Otherwise, the usage 
is considered valid. At last, the gathered 
suggestions Suggestions to T are returned to users 
(Step (9)). Example edits to the user text ?he play 
an important roles to close this deals. he looks 
forward to hear you.? from our working prototype, 
EdIt10, is shown in Figure 5. Note that we exploit 
context checking of collocations to cover longer 
span than ngrams?, and longer ngrams like 
fourgrams and fivegrams to (more or less) help 
semantic checking (or word sense disambiguation). 
For example, ?hear? may be transitive or 
intransitive, but, in the context of ?look forward 
to?, there is strong tendency it is used intransitively 
and follows by ?from?, as EdIt would suggest (see 
Figure 5). 
There are two issues worth mentioning on the 
development of EdIt. First, grammar checkers 
typically have different modules examining 
different types of errors with different priority. In 
our unified framework, we set the priority of 
checking collocations? usages higher than that of 
ngrams?, set the priority of checking longer 
ngrams? usages higher than that of shorter, and we 
do not double check. Alternatively, one may first 
check usages of all sorts and employ majority 
voting to determine the grammaticality of a 
sentence. Second, we further incorporate
                                                           
10
 http://140.114.214.80/theSite/EdIt_demo2/ 
procedure extendedLevenshtein(userUsage,pattern) 
(1) allocate and initialize costArray 
for i in range(len(userUsage)) 
for j in range(len(pattern)) 
//substitution 
if equal(userUsage[i],pattern[j]) 
(2a)       substiCost=costArray[i-1,j-1]+0 
elseif sameWordGroup(userUsage[i],pattern[j]) 
(2b)       substiCost=costArray[i-1,j-1]+0.5 
else 
(2c)       substiCost=costArray[i-1,j-1]+1 
//deletion 
if equal(userUsage[i+1],pattern[j+1]) 
(3a)       delCost=costArray[i-1,j]+smallCost 
else 
(3b)       delCost=costArray[i-1,j]+1 
//insertion 
if equal(userUsage[i+1],pattern[j+1])  
(4a)        insCost=costArray[i,j-1]+smallCost 
else 
(4b)       insCost=costArray[i,j-1]+1 
(5)       costArray[i,j]=min(substiCost,delCost,insCost) 
(6) Return costArray[len(userUsage),len(pattern)] 
102
Erroneous sentence EdIt suggestion ESL Assistant suggestion 
Wrong word form 
? a sunny days ? a sunny NN a sunny day 
every days, I ? every NN every day 
I would said to ? would VB would say 
he play a ? he VBD none 
? should have tell the truth should have VBN should have to tell 
? look forward to see you look forward to VBG none 
? in an attempt to seeing you an attempt to VB none 
? be able to solved this problem able to VB none 
Wrong preposition 
he plays an important role to close ? play ~ role IN(in) none 
he has a vital effect at her. have ~ effect IN(on) effect on her 
it has an effect on reducing ? have ~ effect IN(of) VBG none 
? depend of the scholarship depend IN(on) depend on 
Confusion between intransitive and transitive verb 
he listens the music. missing ?to? after ?listens? missing ?to? after ?listens? 
it affects to his decision. unnecessary ?to? unnecessary ?to? 
I understand about the situation. unnecessary ?about? unnecessary ?about? 
we would like to discuss about this matter. unnecessary ?about? unnecessary ?about? 
Mixture 
she play an important roles to close this deals. she VBD; an JJ NN; 
play ~ role IN(in) VBG; this NN 
play an important role; 
close this deal 
I look forward to hear you. look forward to VBG; 
missing ?from? after ?hear? 
none 
Table 2. Three common score-related error types and their examples with suggestions from EdIt and ESL Assistant. 
 
 
Figure 5. Example EdIt responses to the ungrammatical. 
 
probabilities conditioned on word positions to 
weigh edit costs. For example, the conditional 
probability of ?VERB? being the immediate 
follower of ?look forward to? is virtually zero, but 
the probability of ?V-ing? is around 0.3. 
5.2 Preliminary Results in Error Correction 
We examined three common error types in learner 
text that are highly correlated with essay scores 
(Leacock and Chodorow, 2003; Burstein et al, 
2004), to evaluate EdIt, (see Table 2). In Table 2, 
the results of a state-of-the-art checker, ESL 
Assistant (www.eslassistant.com/), are shown for 
comparison, and information produced by both 
systems are underscored. As indicated, GRASP 
retrieves patterns which are potential useful if 
incorporated into an extension of Levenshtein?s 
algorithm to correct substitution, deletion, and 
insertion errors in learner. 
6 Summary 
We have introduced a new method for producing a 
general-to-specific usage summary of the contexts 
of a linguistic search query aimed at accelerating 
learners? grasp on word usages. We have 
implemented and evaluated the method as applied 
to collocation and phrase learning and grammar 
checking. In the preliminary evaluations we show 
that GRASP is more helpful than traditional 
language learning tools, and that the patterns and 
lexical bundles provided are promising in detecting 
and correcting common types of errors in learner 
writing. 
References 
Morton Benson, Evellyn Benson, and Robert Ilson. 
1986. The BBI Combinatory Dictionary of English: A 
Article: 
Related pattern grammar 
(a) of collocation sequences includes ?play ~ role 
IN(in) NN?, ?play ~ role IN(in) DT?, ?play ~ role 
IN(in) VBG? and so on. 
(b) of ngram sequences includes ?he VBD DT?, ?play 
an JJ NN?, ?this NN?, ?look forward to VBG PRP? 
and ?look forward to hear IN(from) PRP? and so on. 
Grammatical/Usage suggestion: 
For sentence 1: 
(a) use the VBD of ?play?, (b) use the NN of ?roles?, 
(c) use the preposition ?in? and VBG of ?close?, 
instead of ?to close?. (d) use the NN of ?deals? 
For sentence 2: 
(a) insert the preposition ?from? after ?hear?, (b) use 
the ?VBG? of ?hear? 
he play an important roles to close this deals. 
he looks forward to hear you. 
Type your article and push the buttom ?EdIt? ! 
103
guide to word combinations. Philadelphia: John 
Benjamins. 
Chris Brockett, William B. Dolan, and Michael Gamon. 
2006. Correcting ESL errors using phrasal SMT 
techniques. In Proceedings of the ACL, pages 249-
256. 
Jill Burstein, Martin Chodorow, and Claudia Leacock. 
2004. Automated essay evaluation: the criterion 
online writing service. AI Magazine, 25(3): 27-36. 
Yu-Chia Chang, Jason S. Chang, Hao-Jan Chen, and 
Hsien-Chin Liou. 2008. An automatic collocation 
writing assistant for Taiwanese EFL learners: a case 
of corpus-based NLP technology. CALL, 21(3): 283-
299. 
Winnie Cheng, Chris Greaves, and Martin Warren. 2006. 
From n-gram to skipgram to concgram. Corpus 
Linguistics, 11(4): 411-433. 
Martin Chodorow and Claudia Leacock. 2000. An 
unsupervised method for detecting grammatical 
errors. In Proceedings of the NAACL, pages 140-147. 
Rachele De Felice and Stephen G. Pulman. 2008. A 
classifer-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of the COLING, pages 169-176. 
Philip Durrant. 2009. Investigating the viability of a 
collocation list for students of English for academic 
purposes. ESP, 28(3): 157-169. 
John R. Firth. 1957. Modes of meaning. In Papers in 
Linguistics. London: Oxford University Press, pages 
190-215. 
Michael Gamon, Claudia Leacock, Chris Brockett, 
William B. Dolan., Jianfeng Gao, Dmitriy Belenko, 
and Alexandre Klementiev. 2009. Using statistical 
techniques and web search to correct ESL errors. 
CALICO, 26(3): 491-511. 
Michael Gamon and Claudia Leacock. 2010. Search 
right and thou shalt find ? using web queries for 
learner error detection. In Proceedings of the NAACL. 
Matthieu Hermet, Alain Desilets, and Stan Szpakowicz. 
2008. Using the web as a linguistic resource to 
automatically correct lexico-syntatic errors. In 
Proceedings of the LREC, pages 874-878. 
Susan Hunston and Gill Francis. 2000. Pattern 
Grammar: A Corpus-Driven Approach to the Lexical 
Grammar of English. Amsterdam: John Benjamins. 
Jia-Yan Jian, Yu-Chia Chang, and Jason S. Chang. 2004. 
TANGO: Bilingual collocational concordancer. In 
ACL Poster. 
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David 
Tugwell. 2004. The sketch engine. In Proceedings of 
the EURALEX, pages 105-116. 
Chong Min Lee, Soojeong Eom, and Markus Dickinson. 
2009. Toward analyzing Korean learner particles. In 
CALICO Workshop. 
Claudia Leacock and Martin Chodorow. 2003. 
Automated grammatical error detection. In M.D. 
Shermis and J.C. Burstein, editors, Automated Essay 
Scoring: A Cross-Disciplinary Perspective, pages 
195-207. 
Vladimir I. Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. Soviet 
Physics Doklady, 10, page 707. 
Diane Nicholls. 1999. The Cambridge Learner Corpus ? 
error coding and analysis for writing dictionaries and 
other books for English Learners. 
John M. Sinclair. 1987. The nature of the evidence. In J. 
Sinclair (ed.) Looking Up. Collins: 150-159. 
Frank Smadja. 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-177. 
Michael Stubbs. 2004. At 
http://web.archive.org/web/20070828004603/http://www.u
ni-trier.de/uni/fb2/anglistik/Projekte/stubbs/icame-2004.htm. 
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, 
Zhongyang Xiong, John Lee, and Chin-Yew Lin. 
2007. Detecting erroneous sentences using 
automatically mined sequential patterns. In 
Proceedings of the ACL, pages 81-88. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for prepositions selection 
and error detection. In Proceedings of the ACL, pages 
353-358. 
Nai-Lung Tsao and David Wible. 2009. A method for 
unsupervised broad-coverage lexical error detection 
and correction. In NAACL Workshop, pages 51-54. 
Larraitz Uria, Bertol Arrieta, Arantza D. De Ilarraza, 
Montse Maritxalar, and Maite Oronoz. 2009. 
Determiner errors in Basque: analysis and automatic 
detection. Procesamiento del Lenguaje Natural, 
pages 41-48. 
David Wible and Nai-Lung Tsao. 2010. StringNet as a 
computational resource for discovering and 
investigating linguistic constructions. In NAACL 
Workshop, pages 25-31. 
David Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the ACL, pages 189-196. 
104
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 295?301,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
 
 
Helping Our Own: NTHU NLPLAB System Description 
  
Jian-Cheng Wu+, Joseph Z. Chang*, Yi-Chun Chen+, Shih-Ting Huang+, Mei-Hua Chen*, 
Jason S. Chang+ 
  * Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 30013 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 30013 
{wujc86, bizkit.tw, pieyaaa, koromiko1104, chen.meihua, 
jason.jschang}@gmail.com 
  
  
Abstract 
Grammatical error correction has been an active 
research area in the field of Natural Language 
Processing. In this paper, we integrated four 
distinct learning-based modules to correct 
determiner and preposition errors in leaners? 
writing. Each module focuses on a particular 
type of error. Our modules were tested in 
well-formed data and learners? writing. The 
results show that our system achieves high 
recall while preserves satisfactory precision. 
1. Introduction 
Researchers have demonstrated that prepositions 
and determiners are the two most frequent error 
types for language learners (Leacock et al 2010). 
According to Swan and Smith (2001), preposition 
errors might result from L1 interference. Chen and 
Lin (2011) also reveal that prepositions are the 
most perplexing problem for Chinese-speaking 
EFL learners mainly because there are no clear 
preposition counterparts in Chinese for learners to 
refer to. On the other hand, Swan and Smith (2001) 
predict that the possibility of determiner errors 
depends on learners? native language. The 
Cambridge Learners Corpus illustrates that 
learners of Chinese, Japanese, Korean, and Russian 
might have a poor command of determiners.  
In view of the fact that a large number of 
grammatical errors appear in non-native speakers? 
writing, more and more research has been directed 
towards the automated detection and correction of 
such errors to help improve the quality of that 
writing (Dale and Kilgarriff, 2010). In recent years, 
preposition error detection and correction has 
especially been an area of increasingly active 
research (Leacock et al 2010). The HOO 2012 
shared task also focuses on error detection and 
correction in the use of prepositions and 
determiners (Dale et al, 2012).  
Many studies have been done at correcting 
errors using hybrid modules: implementing distinct 
modules to correct errors of different types. In 
other word, instead of using a general module to 
correct any kind of errors, using different modules 
to deal with different error types seems to be more 
effective and promising. In this paper, we propose 
four distinct modules to deal with four kinds of 
determiner and preposition errors (inserting 
missing determiner, replacing erroneous 
determiner, inserting missing preposition, and 
replacing erroneous prepositions). Four 
learning-based approaches are used to detect and 
correct the errors of prepositions and determiners.   
In this paper, we describe our methods in the 
next section. Section 3 reports the evaluation 
results. Then we conclude this paper in Section 4.  
2. System Description 
2.1 Overview 
In this sub-section, we give a general view of our 
system. Figure 1 shows the architecture of the 
integrated error detection and error correction 
system. The input of the system is a sentence in a 
learner?s writing. First, the data is pre-processed 
using the GeniaTagger tool (Tsuruoka et al, 2005), 
which provides the base forms, part-of-speech tags, 
chunk tags and named entity tags. The tag result of 
295
  
the sample sentence ?This virus affects the defense 
system.? is shown in Table 1. The determiner error 
detection module then directly inserts the missing 
determiners and deletes the unnecessary 
determiners. Meanwhile, the error determiners are 
replaced with predicted answers by the determiner 
error correction module. After finishing the 
determiner error correction, the preposition error 
detection and correction module detects and 
corrects the preposition errors of the modified 
input sentence.  
In the following subsections, we first introduce 
the training and testing of the determiner error 
detection and correction modules (Section 3.2). 
Then in section 3.3 we focus on the training and 
testing of the preposition error detection and 
correction modules. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. System Architecture (Run-Time) 
 
 
Word Base form POS Chunk NE 
This This DT B-NP O 
virus virus NN I-NP O 
affects affect VBZ B-VP O 
the the DT B-NP O 
defence defence NN I-NP O 
system system NN I-NP O 
. . . O O 
Table 1. The tag result of sample sentence. 
2.2 Determiners 
In this section, we investigate the performance of 
two maximum entropy classifiers (Ratnaparkhi, 
1997), one for determining whether a noun phrase 
has a determiner or not and the other for selecting 
the appropriate determiner if one is needed.  
 From the British National Corpus (BNC), we 
extract 22,552,979 noun phrases (NPs). For 
determining which features are useful for this task, 
all NPs are divided into two sets, 20 million cases 
as a training set and the others as a validation set.  
For the classifier (named the DetClassifier 
hereafter) trained for predicting whether a NP has a 
determiner or not, the label set contains two labels: 
?Zero? and ?DET.? On the other hand, for the 
classifier (named the SelClassifier hereafter) which 
predicts appropriate determiners, the label set 
contains 9 labels: the, a, an, my, your, our, one, 
this, their. (In the training data, there are 7,249,218 
cases with those labels.) 
Both of the classifiers use contextual and 
syntactic information as features to predict the 
labels. The features include single features such as 
the headword of the NP, the part of speech (PoS) 
of the headword, the words and  PoSs in the 
chunks before or after the NP (pre-NP, post-NP), 
and all words and PoSs in the NP (excluding the 
determiner if there was one), etc. We also combine 
the single features to form more specific features 
for better performance. 
At run time, the given data are also tagged and 
all features for each NP in the data are extracted 
for classification. For testing, all determiners at the 
beginning of the NPs are ignored if they exist. At 
first, the DetClassifier is used to determine 
whether a NP needs a determiner or not. If the 
classifier predicts that the NP should not have a 
determiner but it does, there is an ?UD? 
(Unnecessary determiner) type mistake. In contrast, 
Preposition Error 
Choice 
Determiner Error 
Detection 
Determiner 
Choice 
Preposition Error 
Detection 
Input 
sentence 
Tagger & Parser 
Determiner 
Preposition 
Output 
296
  
if the classifier predicts that the NP should have a 
determiner but it does not, there is a ?MD? type 
mistake. For both ?MD? (Missing determiner) and 
?RD? (Replace determiner) mistake types, we 
would use the SelClassifier to predict which 
determiner is more appropriate for the given NP.  
2.3 Prepositions 
2.3.1 Preposition Error Detection 
In solving other problems in natural language 
processing, supervised training methods suffers 
from the difficulty of acquiring manually labeled 
data. This may not be the case with grammatical 
language error correction. Although high quality 
error learner?s corpora are not currently available 
to the public to provide negative cases, any 
ordinary corpus can used as positive cases at 
training time. 
In our method, we use an ordinary corpus to 
train a Conditional Random Field (CRF) tagger to 
identify the presence of a targeted lexical category. 
The input of the tagger is a sentence with all words 
in the targeting lexical category removed. The 
tagger will tag every word with a positive or 
negative tag, predicting the presence of a word in 
the targeted lexical category. In this paper, we 
choose the top 13 most frequent prepositions: of, to, 
in, for, on, with, as, at, by, from, about, like, since. 
Conditional Random Field 
The sequence labeling is the task of assigning 
labels from a finite set of categories sequentially to 
a set of observation sequences. This problem is 
encountered not only in the field of computational 
linguistics, but also many others, including 
bioinformatics, speech recognition, and pattern 
recognition. 
Traditionally sequence labeling problems are 
solved using the Hidden Markov Model (HMM). 
HMM is a directed graph model in which every 
outcome is conditioned on the corresponding 
observation node and only the previous outcomes. 
Conditional Random Field (CRF) is considered 
the state-of-the-art sequence labeling algorithm. 
One of the major differences of CRF is that it is 
modeled as a undirected graph. CRF also obeys the 
Markov property, with respect to the undirected 
graph, every outcome is conditioned on its 
neighboring outcomes and potentially the entire 
observation sequence. 
 
 
Figure 2. Simplified view of HMM and CRF 
 
Supervised Training 
Obtaining labeled training data is relatively easy 
for this task, that is, it requires no human labeler. 
For this task, we will use this method to target the 
lexical category preposition. To produce training 
data, we simply use an ordinary English corpus 
and use the presence of prepositions as the 
outcome, and remove all prepositions. For example, 
the sentence  
 
?Miss Hardbroom ?s eyes bored into Mildred 
like    a    laser-beam    the    moment    
they    came into view .? 
 
will produce  
 
?Miss _Hardbroom _?s _eyes _bored +Mildred 
_like _a _laser-beam _the _moment _they 
_came  +view .?  
 
where the underscores indicate no preposition 
presence and the plus signs indicate otherwise. 
Combined with additional features described in 
following sections, we use the CRF model to train 
a preposition presence detection tagger. Features 
additional to the words in the sentence are their 
corresponding lemmas, part-of-speech tags, upper 
or lower case, and word suffix. 
At runtime, we first remove all prepositional 
words in the user input sentence, generate 
additional features, and use the trained tagger to 
predict the presence of prepositions in the altered 
sentence. By comparing the tagged result with the 
original sentence, the system can output insertion 
and/or deletion of preposition suggestions. 
The process of generating features is identical to 
producing the training set. To generate 
297
  
part-of-speech tag features at runtime, one simple 
approach is to use an ordinary POS tagger to 
generate POS tags to the tokens in the altered 
sentences, i.e. English sentences without any 
prepositions. A more sophisticated approach is to 
train a specialized POS tagger to tag English 
sentences with their prepositions removed. A 
state-of-the-art part-of-speech tagger can achieve 
around 95% precision. In our implementation, we 
find that using an ordinary POS tagger to tag 
altered sentences yield near 94% precision, 
whereas a specialized POS tagger performed 
around 1% higher precision. 
We used a small portion of the British National 
Corpus (BNC) to train and evaluate our tagger (1M 
and 10M tokens, i.e. words and punctuation marks). 
The British National Corpus contains over 100 
million words of both written (90%) and spoken 
(10%) British English. The written part of the BNC 
is sampled from a wide variety of sources, 
including newspapers, journals, academic books, 
fictions, letter, school and university essays. A 
separate portion of the BNC is selected to evaluate 
the performance of the taggers. The test set 
contains 322,997 tokens (31,916 sentences). 
 
2.3.2 Preposition Error Correction 
Recently, the problem of preposition error 
correction has been viewed as a word sense 
disambiguation problem and all prepositions are 
considered as candidates of the intended senses. In 
previous studies, well-formed corpora and learner 
corpora are both used in training the classifiers. 
However, due to the limited size of learner corpora, 
it is difficult to use the learner corpora to train a 
classifier. A more feasible approach is to use a 
large well-formed corpus to train a model in 
choosing prepositions. Similar to the determiner 
error correction, we choose the maximum entropy 
model as our classifier to choose appropriate 
prepositions underlying certain contexts. In order 
to cover a large variety of genres in learners? 
writing, we use a balanced well-formed corpus, the 
BNC, to train a maximum entropy model.  
Our context features include four feature 
categories which are introduced as follows.  
? Word feature (f1): Word features include a 
window of five content words to the left and 
right with their positions. 
? Head feature (f2): We select two head words 
in the left and right of prepositions with their 
relative orders as head features. For example, 
in Table 2, we select the first head word, face, 
with its relative order, Rh1, as one of the 
head features of preposition, to. More 
specifically, ?Rh1=face? denotes first head 
word, face, right of the preposition, to. 
? Head combine feature (f3): Combine any 
two head features described above to get six 
features. For example, L1R2 denotes two 
head words surrounding the preposition. 
? Phrase combine feature (f4): Combine the 
head words of noun phrase and verb phrase 
where the preposition is between the phrases. 
For example, V_N feature denotes the head 
words of verb phrase and noun phrase where 
the preposition is followed by noun phrase 
and is preceded by verb phrase. 
   
 
Word Feature 
(f1) 
Lw1=leaving, Rw1=face,  
Rw2= chronic, Rw3= condition 
Head Feature 
(f2) 
Lh1=them, Lh2=leaving, 
Rh1=face, Rh2=condition 
Head Combine 
Feature (f3) 
L1L2= them_leaving,  
L1R1= them_face,  
L1R2= them_condition, ? 
Phrase Combine 
Feature (f4) 
N_N= them_condition,  
V_N= leaving_condition,  
N_V= them_face,  
V_V= leaving_face 
Table 2. Features example for leaving them to face this 
chronic condition 
At run time, we extract the features of each 
preposition in learners? writings and ask the model 
to predict the preposition. The preposition error 
detection model described in section 2.3.1 first 
removes all prepositions from test sentences and 
then marks the ?presence? and ?absence? labels in 
every blank of a sentence. For each blank labeled 
?presence?, the correction model predicts the 
preposition which best fits the blank underlying the 
contexts. The correction model does not predict 
when the blanks are labeled ?absence?. Although 
some blanks labeled ?absence? may still 
correspond to prepositions, we decide to reduce 
some recall score to ensure the accuracy of the 
results. 
298
  
3. Experimental Results 
In this section, we present the experimental results 
of the determiner and preposition modules 
respectively.   
3.1 Determiners 
Table 3 shows the performance of the 
DetClassifier of individual feature and Table 4 
shows the performance of the SelClassifier. We 
also wonder how the size of training data 
influences the performance of the models. Table 5 
and 6 show the precision of modes of different 
sizes of training data with the best feature ?whole 
words in NP and last word of pre-NP.? Because the 
performance converges while using more than 5 
million training cases, we use only 1 million 
training cases to investigate the performance of 
using multiple features.  When using all features, 
the precision increases from 84.8% to 85.8% for 
DetClassifier, and from 39.8% to 56.0% for 
SelClassifier. 
We also implement another data-driven model 
for determiner selection (including zero) by using 
the 5gram of Web 1T corpus. The basic concept of 
the model is to use the frequency of determiners 
which fit the context of the given test data to 
choose the determiner candidates. If the frequency 
of the determiner using in the given NP is lower 
than other candidate determiners, we would use the 
most frequent one as the suggestion. However, 
according to our observation during testing, we 
find that the model tends to cause false alarms. To 
reduce the probability of false alarm, we set a high 
threshold for the ratio f1/f2 where f1 is the frequency 
of the used determiner and f2 is the frequency of 
the most frequent determiner. The suggestion is 
accepted only when the ratio exceeds the threshold.  
The major limitation of the proposed method is 
that some errors are ignored due to parsing errors. 
For example, the given data ?the them? should be 
considered as one NP with the ?UD? type error. 
However, the parser would give the chunk result 
?the [B-NP] them [B-NP]? and the error would not 
be recognized. It might need some rules to handle 
these exceptions. Another weakness of the 
proposed methods is that the less frequently used 
determiners are usually considered as errors and 
suggested to be replaced with more frequently used 
ones. For example, possessives such as ?my? 
and ?your?, are usually replaced with ?the.? We 
need to integrate more informative features to 
improve performance. 
 
Features Precision 
head/PoS 79.1% 
word/PoS of pre-NP 70.0% 
word/PoS of all words in NP 85.9% 
PoS of all words in NP 77.8% 
word/PoS of post-NP 71.8% 
whole words in NP 87.2% 
last word/PoS of pre-NP and head/PoS 92.3% 
whole words in NP and last word of 
pre-NP 
96.8% 
Table 3. Precision of features used in the DetClassifier 
 
Features Precision 
head/PoS 55.2% 
word/PoS of pre-NP 49.5% 
word/PoS of all words in NP 53.9% 
PoS of all words in NP 45.3% 
word/PoS of post-NP 46.1% 
whole words in NP 60.4% 
last word/PoS of pre-NP and head/PoS 65.3% 
whole words in NP and last word of 
pre-NP 
70.8% 
Table 4. Precision of features used in the SelClassifier 
 
Size Precision 
1,000,000 84.8% 
5,000,000 96.8% 
10,000,000 96.8% 
15,000,000 96.8% 
20,000,000 96.8% 
Table 5. Precision of different training size for the 
DetClassifier 
 
Size Precision 
1,000,000 39.8% 
3,000,000 43.2% 
5,000,000 44.5% 
7,000,000 61.6% 
7,249,218 70.8% 
Table 6. Precision of different training size for the  
 SelClassifier 
 
 
 
299
  
3.2 Prepositions 
Two sets of evaluation were carried out for 
detection. First, we use a randomly-selected 
portion of the BNC containing 1 million tokens to 
train our tokenizer targeting the 34 highest 
frequency prepositions. Second, we use a larger 
training corpus containing 10 million tokens, also 
randomly selected from the BNC, and target a 
smaller set of the 13 highest frequency 
prepositions, due to the fact that these 13 
prepositions can cover over 90% of the preposition 
errors found in the development set. 
We evaluate the trained taggers using two 
different metrics. First we evaluate the overall 
tagging precision, which is defined as 
 
Poverall   =  # of correctly tagged words  / # of 
all words  
Ppresence =  # correctly tagged PRESENCE / #  
all words labeled with PRESENCE 
 
Since most answer tags are Non-presence, 
Poverall is not informative, we therefore focus on 
Ppresense, and further evaluate the recall of presence, 
defined as: 
 
Rpresence = # correctly tagged PRESENCE  / # 
word should be tagged with PRESENCE  
 
We then evaluate on Precision and Recall of the 
PRESENCE tag using different probabilities to 
threshold the CRF tagging results. Then we show 
the result of two evaluation sets. On the left is the 
tagger train with 1 million tokens, targeting 34 
prepositions. On the right is the tagger trained with 
10 million tokens, targeting 13 prepositions. Only 
the latter tagger is used for producing the 
submitted runs. 
We used the development data released as part 
of HOO 2012 Shared Task as the gold standard for 
the evaluation of our preposition correction module. 
In order to observe the effect of different feature 
sets in training, we first extracted the MT and RT 
instances marked by the gold standard and then ask 
the correction module to correct these prepositions 
directly. Table 7 shows the precision of the models 
trained on different feature sets. The definition of 
precision is the same as the definition in the HOO 
2012 Shared Task. The results shows that the 
model trained using four feature sets achieved 
higher precision.   
Features Precision 
MT RT MT+RT 
f1 43.62% 39.15% 40.48% 
f1+f2 52.58% 43.47% 46.18% 
f1+f2+f3 55.20% 46.77% 49.27% 
f1+f2+f3+f4 55.11% 47% 49.41% 
Table 7. The feature selection and accuracy of the 
preposition correction module. 
 
In addition to the evaluation on the effect of 
different feature sets, we also conducted an 
evaluation done on the development data of HOO 
2012 Shared Task to observe the performance of 
the correction model when combined with the 
detection model. The correction model corrected 
three different types of preposition errors, MT, RT 
and MT+RT simultaneously (Table 8). 
 
 
  MT RT MT+RT 
Precision 1.16% 3.80% 4.96% 
Recall 29.86% 41.14% 37.79% 
  
Table 8. Precision and recall scores of the correction 
modules when combined with the detection module.  
 
Note that when we only corrected the 
preposition errors marked MT by preposition error 
detection module, the precision and recall are both 
lower than that of RT. The amount of false alarm 
instances of detection module in MT seems to be 
too high, thus in this paper, we won?t correct the 
instance marked MT to insure the higher precision 
of overall preposition correction. 
 
4. Conclusion 
In this paper, we integrate four learning-based 
methods in determiner and preposition error 
detection and correction. The integrated system 
simply parses and tags the test sentences and then 
corrects determiners and prepositions step by step. 
The training of our system relies on well-formed 
corpora and thus seems to be easier to 
re-implement it. The large well-formed corpus 
might also insure higher recall.  
In the future, we plan to integrate the system in 
a more flexible way. The detection modules could 
300
  
pass probabilities to the correction modules. The 
correction modules thus could decide whether to 
correct the instances or not. In addition, we plan to 
reduce the false alarm rate of the detection module. 
Besides, a more considerable evaluation would be 
conducted in the near future. 
Acknowledgements 
We would acknowledge the funding support 
from the Project (NSC 100-2627-E-007-001) 
and the help of the participants. Thanks also go to 
the comments of anonymous reviewers on this 
paper. 
References  
Mei-Hua Chen and Maosung Lin, 2011. Factors and 
Analyses of Common Miscollocations of College 
Students in Taiwan. Studies in English Language and 
Literature, 28, pp. 57-72. 
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving 
prepositions. In Proceedings of the Fourth 
ACL-SIGSEM Workshop on Prepositions, pp.  
25-30. 
Robert Dale and Adam Kilgarriff. 2010. Helping Our 
Own: Text massaging for computational linguistics 
as a new shared task. In Proceedings of the 6th 
International Natural Language Generation 
Conference, pp. 261?266. 
Robert Dale, Ilya Anisimoff and George Narroway 
(2012) HOO 2012: A Report on the Preposition and 
Determiner Error Correction Shared Task. In 
Proceedings of the Seventh Workshop on Innovative 
Use of NLP for Building Educational Applications. 
Rachele De Felice and Stephen G. Pulman. 2007. 
Automatically acquiring models of preposition use. 
In Proceedings of the Fourth ACL-SIGSEM 
Workshop on Prepositions, pp. 45-50. 
Claudia Leacock, Martin Chodorow, Michael Gamon, 
and Joel Tetreault. 2010. Automated Grammatical 
Error Detection for Language Learners. Synthesis 
Lectures on Human Language Technologies. Morgan 
and Claypool. 
Adwait Ratnaparkhi. 1997. A linear observed time 
statistical parser based on maximum entropy models. 
In Proceedings of the Second Conference on 
Empirical Methods in Natural Language Processing, 
Brown University, Providence, Rhode Island. 
Michael Swan and Bernard Smith, editors. Learner 
English: A teacher?s guide to interference and other 
problems. Cambridge University Press, 2 edition, 
2001. DOI: 10.1017/CBO9780511667121 19, 23, 91 
Tsuruoka Y, Tateishi Y, Kim JD, Ohta T, McNaught J, 
Ananiadou S, Tsujii J. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In 
Advances in Informatics, 10th Panhellenic 
Conference on Informatics; 11-13 November 2005 
Volos, Greece. Springer; pp. 382-392. 
 
301
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 34?37,
Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational Linguistics
GLANCE Visualizes Lexical Phenomena for Language Learning  
 
Mei-Hua Chen*, Shih-Ting Huang+, Ting-Hui Kao+, Sun-Wen Chiu+, Tzu-His Yen+ 
* Department of Foreign Languages and Literature, Hua Fan University, Taipei, Taiwan, 
R.O.C. 22301 
+ Department of Computer Science, National Tsing Hua University, HsinChu, Taiwan, 
R.O.C. 30013 
{chen.meihua,koromiko1104,maxis1718,chiuhsunwen, joseph.yen}@gmail.com 
 
 
Abstract 
Facilitating vocabulary knowledge is a 
challenging aspect for language learners. 
Although current corpus-based reference 
tools provide authentic contextual clues, the 
plain text format is not conducive to fully 
illustrating some lexical phenomena. Thus, 
this paper proposes GLANCE 1 , a text 
visualization tool, to present a large amount 
of lexical phenomena using charts and graphs, 
aimed at helping language learners 
understand a word quickly and intuitively. To 
evaluate the effectiveness of the system, we 
designed interfaces to allow comparison 
between text and graphics presentation, and 
conducted a preliminary user study with ESL 
students. The results show that the visualized 
display is of greater benefit to the 
understanding of word characteristics than 
textual display. 
1 Introduction 
Vocabulary is a challenging aspect for language 
learners to master. Extended word knowledge, 
such as word polarity and position, is not widely 
available in traditional dictionaries. Thus, for 
most language learners, it is very difficult to 
have a good command of such lexical phenome-
na.  
Current linguistics software programs use 
large corpus data to advance language learning. 
The use of corpora exposes learners to authentic 
contextual clues and lets them discover patterns 
or collocations of words from contextual clues 
(Partington, 1998). However, a huge amount of 
data can be overwhelming and time-consuming 
(Yeh et al., 2007) for language learners to induce 
rules or patterns. On the other hand, some lexical 
phenomena seem unable to be comprehended 
                                                 
1 http://glance-it.herokuapp.com/ 
fast and directly in plain text format (Koo, 2006). 
For example, in the British National Corpus 
(2007), ?however? seems more negative than 
?but?. Also, compared with ?but?, ?however? 
appears more frequently at the beginning of a 
sentence. 
With this in mind, we proposed GLANCE1, a 
text visualization tool, which presents corpus 
data using charts and graphs to help language 
learners understand the lexical phenomena of a 
word quickly and intuitively. In this paper, we 
focused on five types of lexical phenomena: po-
larity, position, POS, form and discipline, which 
will be detailed in the Section 3. Given a single 
query word, the GLANCE system shows graph-
ical representations of its lexical phenomena se-
quentially within a single web page.  
Additionally we believe that the use of 
graphics also facilitates the understanding of the 
differences between two words. Taking this into 
consideration, we introduce a comparison mode 
to help learners differentiate two words at a 
glance. Allowing two word input, GLANCE 
draws the individual representative graphs for 
both words and presents these graphs in a two-
column view. The display of parallel graphs de-
picts the distinctions between the two words 
clearly. 
 
2 Related Work 
Corpus-based language learning has widened the 
perspectives in second and foreign language edu-
cation, such as vocabulary learning (Wood, 
2001). In past decades, various corpus-based ref-
erence tools have been developed. For example, 
WordSmith (Scott, 2000), Compleat Lexical Tu-
tor (Cobb, 2007), GRASP (Huang et al., 2011), 
PREFER (Chen et al, 2012). 
Recently, some interactive visualization tools 
have been developed for the purpose of illustrat-
ing various linguistic phenomena. Three exam-
34
ples are Word Tree, a visual concordance (Wat-
tenberg and  i gas, 2008), WORDGRAPH, a 
visual tool for context-sensitive word choice 
(Riehmann et al., 2012) and Visual Thesaurus, a 
3D interactive reference tool (ThinkMap Inc., 
2005). 
 
3 Design of the GLANCE System 
The GLANCE system consists of several com-
ponents of corpus data visualization. We design 
and implement these visualization modules sepa-
rately to ensure all graphs are simple and clear 
enough for users to capture and understand the 
lexical phenomena quickly. 
In this paper, we use the d3.js (Data-Driven 
Documents) (Bostock et al., 2011) to visualize 
the data. The d3.js enables direct inspection and 
manipulation of a standard document object 
model (DOM) so that we are able to transform 
numeric data into various types of graphs when 
fitting these data to other visualization tools. In 
this section, we describe the ways we extract the 
data from the corpus and how we translate these 
data into informative graphs. 
 
3.1 Data Preprocessing 
We use the well-formed corpus, the BNC, to ex-
tract the data. In order to obtain the Part-of-
speech tags for each text, we use the GENIA 
tagger (Tsuruoka et al., 2005) to analyze the sen-
tences of the BNC and build a list of <POS-tag, 
frequency> pairs for each word in the BNC. Also 
the BNC contains the classification code as-
signed to the text in a genre-based analysis car-
ried out at Lancaster University by Lee (2001). 
For each word, the classification codes are ag-
gregated to a list of <code, frequency> pairs.  
  
3.2 Visualization of Lexical Phenomena  
Polarity 
A word may carry different sentiment polarities 
(i.e., positive, negative and objective). To help 
users quickly determine the proper sentiment 
polarity of a word, we introduce the sentiment 
polarity information of SentiWordNet 
(Baccianella et al., 2010) into our system. For 
each synset of a word, GLANCE displays the 
polarity in a bar with three different colors. The 
individual length of the three parts in the bar cor-
responds to the polarity scores of a synset (Fig-
ure 1). 
 
 
Figure 1. Representation of sentiment polarity  
 
Position 
The word position in a sentence is also an im-
portant lexical phenomenon. By calculating the 
word position in each sentence, we then obtain 
the location distribution. GLANCE visualizes the 
distribution information of a word using a bar 
chart. Figure 2 shows a plot of distribution of 
word position on the x-axis against the word fre-
quency on the y-axis. 
 
 
Figure 2. Distribution of word position 
 
Part Of Speech (POS) 
A lexical item may have more than one part of 
speech. Knowing the distribution of POS helps 
users quickly understand the general usage of a 
word.  
GLANCE displays a pie chart for each word 
to differentiate between its parts of speech. We 
use the maximum likelihood probability of a 
POS tag for a word as the arc length of the pie 
chart (Figure 3). 
 
 
35
Figure 3. POS representation 
 
Form 
The levels of formality of written and spoken 
language are different, which also confuse lan-
guage learners. Pie charts are used to illustrate 
the proportion of written and spoken English of 
individual words as shown in Figure 4. 
We derive the frequencies of both forms from 
the BNC classification code for each word. The 
arc length of each sector is proportional to the 
maximum likelihood probability of forms. 
 
 
Figure 4. Form representation 
 
Discipline 
Similar to language form, the discipline infor-
mation (e.g., newspaper or fiction) was gathered 
from the BNC classification code. The relations 
of the disciplines of a word are presented using a 
sunburst graph, a radial space-filling tree layout 
implemented with prefuse (Heer et al., 2005). In 
the sunburst graph (Figure 5.), each level corre-
sponds to the relation of the disciplines of a cer-
tain word. The farther the level is away from the 
center, the more specific the discipline is. Each 
level is given equal width, but the circular angle 
swept out by a discipline corresponds to the fre-
quency of the disciplines. 
 
  
Figure 5. Discipline relations 
 
4 Results 
4.1 Experimental Setting 
We performed a preliminary user study to assess 
the efficiency of our system in assisting language 
learners in grasping lexical phenomena. To ex-
amine the effectiveness of visualization, we built 
a textual interface for comparison with the 
graphical interface. 
Ten pre-intermediate ESL college students 
participated in the study. A total of six pairs of 
similar words were listed on the worksheet. After 
being introduced to GLANCE, all students were 
randomly divided into two groups. One group 
was required to consult the first three pairs using 
the graphical interface and the second three pairs 
the textual interface, and vice versa. The partici-
pants were allowed a maximum of one minute 
per pair, which meets the goal of this study of 
quickly glancing at the graphics and grasping the 
concepts of words. Then a test sheet containing 
the same six similar word pairs was used to ex-
amine the extent of students? word understanding. 
Note that during the test, no tool supports were 
provided. The student scored one point if he gave 
the correct answers to each question. In other 
words he would be awarded 6 points (the highest 
number of points) if he provided all the correct 
answers. They also completed a questionnaire, 
described below, evaluating the system. 
 
4.2 Experimental Results 
To determine the effectiveness of visualization of 
lexical phenomena, the students? average scores 
were used as performance indicators. Students 
achieved the average score 61.9 and 45.0 out of 
100.00 after consulting the graphic interface and 
textual interface respectively. Overall, the visual-
ized display of word characteristics outper-
formed the textual version. 
The questionnaire revealed that all the partici-
pants showed a positive attitude to visualized 
word information. Further analyses showed that 
all ten participants appreciated the position dis-
play and nine of them the polarity and form dis-
plays. In short, the graphical display of lexical 
phenomena in GLANCE results in faster assimi-
lation and understanding of word information. 
Moreover, the participants suggested several in-
teresting aspects for improving the GLANCE 
system. For example, they preferred bilingual 
environment, further information concerning an-
tonyms, more example sentences, and increased 
36
detail in the sunburst representation of disci-
plines. 
 
5 Conclusion and Future Work 
In this paper, we proposed GLANCE, a text vis-
ualization tool, which provides graphical display 
of corpus data. Our goal is to assist language 
learners in glancing at the graphics and grasping 
the lexical knowledge quickly and intuitively. To 
evaluate the efficiency and effectiveness of 
GLANCE, we conducted a preliminary user 
study with ten non-native ESL learners. The re-
sults revealed that visualization format outper-
formed plain text format. 
Many avenues exist for future research and 
improvement. We attempt to expand the single 
word to phrase level. For example, the colloca-
tion behaviors are expected to be deduced and 
displayed. Moreover, we are interested in sup-
porting more lexical phenomena, such as hypo-
nyms, to provide learners with more lexical rela-
tions of the word with other words. 
 
Reference 
Baccianella, S., Esuli, A., & Sebastiani, F. (2010, 
May). SentiWordNet 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion Min-
ing. In LREC (Vol. 10, pp. 2200-2204). 
Bostock, M., Ogievetsky, V., & Heer, J. (2011). D? 
data-driven documents. Visualization and Comput-
er Graphics, IEEE Transactions on, 17(12), 2301-
2309. 
Chen, M. H., Huang, S. T., Huang, C. C., Liou, H. C., 
& Chang, J. S. (2012, June). PREFER: using a 
graph-based approach to generate paraphrases for 
language learning. In Proceedings of the Seventh 
Workshop on Building Educational Applications 
Using NLP (pp. 80-85). Association for Computa-
tional Linguistics. 
Cobb, T. (2007). The compleat lexical tutor. Retrieved 
September, 22, 2009. 
Heer, J., Card, S. K., & Landay, J. A. (2005, April). 
Prefuse: a toolkit for interactive information visual-
ization. In Proceedings of the SIGCHI conference 
on Human factors in computing systems (pp. 421-
430). ACM. 
Huang, C. C., Chen, M. H., Huang, S. T., Liou, H. C., 
& Chang, J. S. (2011, June). GRASP: grammar-
and syntax-based pattern-finder in CALL. 
In Proceedings of the 6th Workshop on Innovative 
Use of NLP for Building Educational Applications 
(pp. 96-104). Association for Computational Lin-
guistics. 
Kyosung Koo (2006). Effects of using corpora and 
online reference tools on foreign language writing: 
a study of Korean learners of English as a second 
language. PhD. dissertation, University of Iowa. 
Lee, D. Y. (2001). Genres, registers, text types, do-
mains and styles: Clarifying the concepts and 
nevigating a path through the BNC jungle. 
Partington, A. (1998). Patterns and meanings: Using 
corpora for English language research and teach-
ing (Vol. 2). John Benjamins Publishing. 
Riehmann, P., Gruendl, H., Froehlich, B., Potthast, M., 
Trenkmann, M., & Stein, B. (2011, March). The 
NETSPEAK WORDGRAPH: Visualizing key-
words in context. In Pacific Visualization Sympo-
sium (PacificVis), 2011 IEEE (pp. 123-130). IEEE. 
Scott, M. (2004). WordSmith tools version 4. 
The British National Corpus, version 3 (BNC XML 
Edition). 2007. Distributed by Oxford University 
Computing Services on behalf of the BNC Consor-
tium. URL: http://www.natcorp.ox.ac.uk/ 
ThinkMap Inc. (2005). Thinkmap Visual Thesaurus. 
Available from http:// www.visualthesaurus.com 
Tsuruoka, Y., Tateishi, Y., Kim, J. D., Ohta, T., 
McNaught, J., Ananiadou, S., & Tsujii, J. I. (2005). 
Developing a robust part-of-speech tagger for bio-
medical text. In Advances in informatics (pp. 382-
392). Springer Berlin Heidelberg. 
Wattenberg, M., & Vi?gas, F. B. (2008). The word 
tree, an interactive visual concord-
ance. Visualization and Computer Graphics, IEEE 
Transactions on, 14(6), 1221-1228. 
Wood, J. (2001). Can software support children?s 
vocabulary development.Language Learning & 
Technology, 5(1), 166-201. 
Yeh, Y., Liou, H. C., & Li, Y. H. (2007). Online syn-
onym materials and concordancing for EFL college 
writing. Computer Assisted Language Learning, 
20(2), 131-152. 
 
37
