Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 89?98, Dublin, Ireland, August 23-29 2014.
Group Non-negative Matrix Factorization with Natural Categories for
Question Retrieval in Community Question Answer Archives
Guangyou Zhou, Yubo Chen, Daojian Zeng, and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,yubo.chen,djzeng,jzhao}@nlpr.ia.ac.cn
Abstract
Community question answering (CQA) has become an important service due to the popularity of
CQA archives on the web. A distinctive feature is that CQA services usually organize questions
into a hierarchy of natural categories. In this paper, we focus on the problem of question re-
trieval and propose a novel approach, called group non-negative matrix factorization with natural
categories (GNMFNC). This is achieved by learning the category-specific topics for each cate-
gory as well as shared topics across all categories via a group non-negative matrix factorization
framework. We derive an efficient algorithm for learning the factorization, analyze its complex-
ity, and provide proof of convergence. Experiments are carried out on a real world CQA data set
from Yahoo! Answers. The results show that our proposed approach significantly outperforms
various baseline methods and achieves the state-of-the-art performance for question retrieval.
1 Introduction
Community question answering (CQA) such as Yahoo! Answers
1
and Quora
2
, has become an important
service due to the popularity of CQA archives on the web. To make use of the large-scale questions and
their answers, it is critical to have functionality of helping users to retrieve previous answers (Duan et
al., 2008). Typically, such functionality is achieved by first retrieving the historical questions that best
match a user?s queried question, and then using answers of these returned questions to answer the queried
question. This is what we called question retrieval in this paper.
The major challenge for question retrieval, as for most information retrieval tasks, is the lexical gap
between the queried questions and the historical questions in the archives. For example, if a queried ques-
tion contains the word ?company? but a relevant historical question instead contains the word ?firm?, then
there is a mismatch and the historical question may not be easily distinguished from an irrelevant one.
To solve the lexical gap problem, most researchers focused on translation-based approaches since the
relationships between words (or phrases) can be explicitly modeled through word-to-word (or phrases)
translation probabilities (Jeon et al., 2005; Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008; Bern-
hard and Gurevych, 2009; Zhou et al., 2011; Singh, 2012). However, these existing methods model the
relevance ranking without considering the category-specific and shared topics with natural categories, it
is not clear whether this information is useful for question retrieval.
A distinctive feature of question-answer pairs in CQA is that CQA services usually organize questions
into a hierarchy of natural categories. For example, Yahoo! Answers contains a hierarchy of 26 categories
at the first level and more than 1262 subcategories at the leaf level. When a user asks a question, the user
is typically required to choose a category label for the question from a predefined hierarchy. Questions in
the predefined hierarchy usually share certain generic topics while questions in different categories have
their specific topics. For example, questions in categories ?Arts & Humanities? and ?Beauty & Style?
may share the generic topic of ?dance? but they also have the category-specific topics of ?poem? and
?wearing?, respectively.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
1
http://answers.yahoo.com/
2
http://www.quora.com/
89
Inspired by the above observation, we propose a novel approach, called group non-negative matrix
factorization with natural categories (GNMFNC). GNMFNC assumes that there exists a set of category-
specific topics for each of the category, and there also exists a set of shared topics for all of the categories.
Each question in CQA is specified by its category label, category-specific topics, as well as shared topics.
In this way, the large-scale question retrieval problem can be decomposed into small-scale subproblems.
In GNMFNC, questions in each category are represented as a term-question matrix. The term-question
matrix is then approximated as the product of two matrices: one matrix represents the category-specific
topics as well as the shared topics, and the other matrix denotes the question representation based on
topics. An objective function is defined to measure the goodness of prediction of the data with the
model. Optimization of the objective function leads to the automatic discovery of topics as well as
the topic representation of questions. Finally, we calculate the relevance ranking between the queried
questions and the historical questions in the latent topic space.
Past studies by (Cao et al., 2009; Cao et al., 2010; Ming et al., 2010; Cai et al., 2011; Ji et al., 2012;
Zhou et al., 2013) confirmed a significant retrieval improvement by adding the natural categories into
various existing retrieval models. However, all these previous work regarded natural categories indi-
vidually without considering the relationships among them. On the contrary, this paper can effectively
capture the relationships between the shared aspects and the category-specific individual aspects with
natural categories via a group non-negative matrix factorization framework. Also, our work models the
relevance ranking in the latent topic space rather than using the existing retrieval models. To date, no at-
tempts have been made regarding group non-negative matrix factorization in studies of question retrieval,
which remains an under-explored area.
The remainder of this paper is organized as follows. Section 2 describes our proposed group non-
negative matrix factorization with natural categories for question retrieval. Section 3 presents the exper-
imental results. In Section 4, we conclude with ideas for future research.
2 Group Non-negative Matrix Factorization with Natural Categories
2.1 Problem Formulation
In CQA, all questions are usually organized into a hierarchy of categories. When a user asks a question,
the user is typically required to choose a category label for the question from a predefined hierarchy of
categories. Hence, each question in CQA has a category label. Suppose that we are given a question col-
lection D in CQA archive with size N , containing terms from a vocabulary V with size M . A question
d is represented as a vector d ? R
M
where each entry denotes the weight of the corresponding term,
for example tf-idf is used in this paper. Let C = {c
1
, c
2
, ? ? ? , c
P
} denote the set of categories (subcat-
egories) of question collection D, where P is the number of categories (subcategories). The question
collection D is organized into P groups according to their category labels and can be represented as
D = {D
1
,D
2
, ? ? ? ,D
P
}. D
p
= {d
(p)
1
, ? ? ? ,d
(p)
N
p
} ? R
M?N
p
is the term-question matrix corresponding
to category c
p
, in which each row stands for a term and each column stands for a question. N
p
is the
number of questions in category c
p
such that
?
P
p=1
N
p
= N .
LetU
?
p
= [U
s
,U
p
] ? R
M?(K
s
+K
p
)
be the term-topic matrix corresponding to category c
p
, where K
s
is the number of shared topics, K
p
is the number of category-specific topics corresponding to category
c
p
, and p ? [1, P ]. Term-topic matrix U
s
can be represented as U
s
= [u
(s)
1
, ? ? ? ,u
(s)
K
s
] ? R
M?K
s
, in
which each column corresponds to a shared topic. While the term-topic matrix U
p
can be represented
as U
p
= [u
(p)
1
, ? ? ? ,u
(p)
K
p
] ? R
M?K
p
. The total number of topics in the question collection D is K =
K
s
+ PK
p
. Let V
p
= [v
(p)
1
, ? ? ? ,v
(p)
N
p
] ? R
(K
s
+K
p
)?N
p
be the topic-question matrix corresponding to
category c
p
, in which each column denotes the question representation in the topic space. We also denote
V
T
p
= [H
T
p
,W
T
p
], where H
p
? R
K
s
?N
p
and W
p
? R
K
p
?N
p
correspond to the coefficients of shared
topicsU
s
and category-specific topicsU
p
, respectively.
Thus, given a question collection D = {D
1
,D
2
, ? ? ? ,D
P
} together with the category labels C =
{c
1
, c
2
, ? ? ? , c
P
}, our proposed GNMFNC amounts to modeling the question collection D with P group
90
simultaneously, arriving at the following objective function:
O =
P
?
p=1
{
?
p
?
?
D
p
? [U
s
,U
p
]V
p
?
?
2
F
+ R(U
s
,U
p
)
}
(1)
where ?
p
, ?D
p
?
?2
F
. R(U
s
,U
p
) is a regularization term used to penalize the ?similarity? between the
shared topics and category-specific topics throughU
s
andU
p
.
In this paper, we aim to ensure that matrix U
s
captures only shared topics and matrix U
p
captures
only the category-specific topics. For example, if matricesU
s
andU
p
are mutually orthogonal, we have
U
T
s
U
p
= 0. To impose this constraint, we attempt to minimize the sum-of-squares of entries of the
matrix U
T
s
U
p
(e.g., ?U
T
s
U
p
?
2
F
which uniformly optimizes each entry of U
T
s
U
p
). With this choice, the
regularization term of R(U
s
,U
p
) is given by
R(U
s
,U
p
) =
P
?
p=1
?
p
?
?
U
T
s
U
p
?
?
2
F
+
P
?
l=1,l?=p
?
l
?
?
U
T
p
U
l
?
?
2
F
(2)
where ?
p
and ?
l
are the regularization parameters, ?p ? [1, P ], ?l ? [1, P ].
Learning the objective function in equation (1) involves the following optimization problem:
min
U
s
,U
p
,V
p
?0
L = O + ?
1
?
?
U
T
s
1
M
? 1
K
s
?
?
2
F
+ ?
2
?
?
U
T
p
1
M
? 1
K
p
?
?
2
F
+ ?
3
?
?
V
p
1
N
p
? 1
K
s
+K
p
?
?
2
F
(3)
where ?
1
, ?
2
and ?
3
are the shrinkage regularization parameters. Based on the shrinkage methodology,
we can approximately satisfy the normalization constraints for each column of [U
s
,U
p
] and V
T
p
by
guaranteeing the optimization converges to a stationary point.
2.2 Learning Algorithm
We present the solution to the GNMFNC optimization problem in equation (3) as the following theorem.
The theoretical aspects of the optimization are presented in the next subsection.
Theorem 2.1. UpdatingU
s
,U
p
andV
p
using equations (4)?(6) corresponds to category c
p
will mono-
tonically decrease the objective function in equation (3) until convergence.
U
s
? U
s
?
[
?
P
p=1
?
p
D
p
H
T
p
]
[
?
P
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+ ?
p
U
p
U
T
p
U
s
]
(4)
U
p
? U
p
?
[
?
p
D
p
W
T
p
]
[
?
p
[U
s
,U
p
]V
p
W
T
p
+ ?
p
U
s
U
T
s
U
p
+
?
P
l=1,l?=p
?
l
U
l
U
T
l
U
p
]
(5)
V
p
? V
p
?
[
?
p
D
T
p
[U
s
,U
p
]
]
[
?
p
V
T
p
[U
s
,U
p
]
T
[U
s
,U
p
]
]
(6)
where operator ? is element-wise product and
[?]
[?]
is element-wise division.
Based on Theorem 2.1, we note that multiplicative update rules given by equations (4)?(6) are ob-
tained by extending the updates of standard NMF (Lee and Seung, 2001). A number of techniques can
be used here to optimize the objective function in equation (3), such as alternating least squares (Kim
and Park, 2008), the active set method (Kim and Park, 2008), and the projected gradients approach (Lin,
2007). Nonetheless, the multiplicative updates derived in this paper have reasonably fast convergence
behavior as shown empirically in the experiments.
2.3 Theoretical Analysis
In this subsection, we give the theoretical analysis of the optimization, convergence and computational
complexity.
91
Without loss of generality, we only show the optimization ofU
s
and formulate the Lagrange function
with constraints as follows:
L(U
s
) = O + ?
1
?
?
U
T
s
1
M
? 1
K
s
?
?
2
F
+ Tr(?
s
U
T
s
)
(7)
where Tr(?) denotes the trace of a matrix, ?
s
? R
K
s
?K
s
is the Lagrange multiplier for the nonnegative
constraintU
s
? 0.
The partial derivative of L(U
s
) w.r.t. U
s
is
?
U
s
L(U
s
) = ?2
P
?
p=1
?
p
D
p
H
T
p
+ 2
P
?
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+ 2
P
?
p=1
?
p
U
p
U
T
p
U
s
+ 2?
1
U
s
? 2?
1
+ ?
s
(8)
Using the Karush-Kuhn-Tucker (KKT) (Boyd and Vandenberghe, 2004) condition ?
s
?U
s
= 0, we
obtain
?
U
s
L(U
s
) ?U
s
=
{
?
?
P
p=1
?
p
D
p
H
T
p
+
?
P
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+
?
P
p=1
?
p
U
p
U
T
p
U
s
+ ?
1
U
s
? ?
1
}
?U
s
= 0 (9)
After normalization ofU
s
, the terms ?
1
U
s
and ?
1
are in fact equal. They can be safely ignored from
the above formula without influencing convergence. This leads to the updating rule for U
s
in equation
(4). Following the similar derivations as shown above, we can obtain the updating rules for the rest
variablesU
p
andV
p
in GNMFNC optimization, as shown in equations (5) and (6).
2.3.1 Convergence Analysis
In this subsection, we prove the convergence of multiplicative updates given by equations (4)?(6). We
first introduce the definition of auxiliary function as follows.
Definition 2.1. F(X,X
?
) is an auxiliary function for L(X) if L(X) ? F(X,X
?
) and equality holds if
and only if L(X) = F(X,X).
Lemma 2.1. (Lee and Seung, 2001) If F is an auxiliary function for L, L is non-increasing under the
update
X
(t+1)
= argmin
X
F(X,X
(t)
)
Proof. By Definition 2.1, L(X
(t+1)
) ? F(X
(t+1)
,X
(t)
) ? F(X
(t)
,X
(t)
) = L(X
(t)
)
Theorem 2.2. Let L(U
(t+1)
s
) denote the sum of all terms in L that containU
(t+1)
s
, the following function
is an auxiliary function for L(U
(t+1)
s
)
F(U
(t+1)
s
,U
(t)
s
) = L(U
(t)
s
) + (U
(t+1)
s
?U
(t)
s
)?
U
(t)
s
L(U
(t)
s
) +
1
2
(U
(t+1)
s
?U
(t)
s
)
2
P(U
(t)
s
)
(10)
P(U
(t)
s
) =
?
ij
[
?
P
p=1
?
p
[U
(t)
s
,U
p
]V
p
W
T
p
+ ?
p
U
p
U
T
p
U
(t)
s
+ ?
1
U
(t)
s
]
ij
?
ij
[U
(t)
s
]
ij
where ?
U
(t)
s
L(U
(t)
s
) is the first-order derivative of L(U
(t)
s
) with respect toU
(t)
s
. Theorem 2.2 can be
proved similarly to (Lee and Seung, 2001) by validating L(U
(t+1)
s
) ? F(U
(t+1)
s
,U
(t)
s
), L(U
(t+1)
s
) =
F(U
(t+1)
s
,U
(t+1)
s
), and the Hessian matrix ??
U
(t+1)
s
F(U
(t+1)
s
,U
(t)
s
) ? 0. Due to limited space, we
omit the details of the validation.
92
addition multiplication division overall
GNMFNC:U
s
P (3MN
p
K
s
+MN
p
K
p
+MK
2
s
) P (3MN
p
K
s
+MN
p
K
p
+MK
2
s
) MK
s
O(PMN
p
K
max
)
GNMFNC:U
p
3MN
p
K
p
+MN
p
K
s
+ PM
2
K
?
3MN
p
K
p
+MN
p
K
s
+ PM
2
K
?
MK
p
O(PMRK
?
)
GNMFNC:V
p
3MN
p
K
?
3MN
p
K
?
N
p
K
?
O(MN
p
K
?
)
Table 1: Computational operation counts for each iteration in GNMFNC.
Based on Theorem 2.2, we can fixU
(t)
s
and minimize F(U
(t+1)
s
,U
(t)
s
) with respect toU
(t+1)
s
. When
setting ?
U
(t+1)
s
F(U
(t+1)
s
,U
(t)
s
) = 0, we get the following updating rule
U
(t+1)
s
? U
(t)
s
?
[
?
P
p=1
?
p
D
p
H
T
p
+ ?
1
]
[
?
P
p=1
?
p
[U
(t)
s
,U
p
]V
p
W
T
p
+ ?
p
U
p
U
T
p
U
(t)
s
+ ?
1
U
(t)
s
]
(11)
which is consistent with the updating rule derived from the KKT conditions aforementioned.
By Lemma 2.1 and Theorem 2.2, we have L(U
(0)
s
) = F(U
(0)
s
,U
(0)
s
) ? F(U
(1)
s
,U
(0)
s
) ?
F(U
(1)
s
,U
(1)
s
) = L(U
(1)
s
) ? ? ? ? ? L(U
(Iter)
s
), where Iter is the number of iterations. Therefore,
U
s
is monotonically decreasing. Since the objective function L is lower bounded by 0, the correctness
and convergence of Theorem 2.1 is validated.
2.3.2 Computational Complexity
In this subsection, we discuss the time computational complexity of the proposed algorithm GNMFNC.
Besides expressing the complexity of the algorithm using big O notation, we also count the number of
arithmetic operations to provide more details about running time. We show the results in Table 1, where
K
max
= max{K
s
,K
p
}, K
?
= K
s
+ K
p
and R = max{M,N
p
}.
Suppose the multiplicative updates stop after Iter iterations, the time cost of multiplicative updates
then becomes O(Iter ? PMRK
?
). We set Iter = 100 empirically in rest of the paper. Therefore, the
overall running time of GNMFNC is linear with respect to the size of word vocabulary, the number of
questions and categories.
2.4 Relevance Ranking
The motivation of incorporating matrix factorization into relevance ranking is to learn the word rela-
tionships and reduce the ?lexical gap? (Zhou et al., 2013a). To do so, given a queried question q with
category label c
p
from Yahoo! Answers, we first represent it in the latent topic space as v
q
,
v
q
= argmin
v?0
?q? [U
s
,U
p
]v?
2
2
(12)
where vector q is the tf-idf representation of queried question q in the term space.
For each historical question d (indexed by r) in question collection D, with representation v
d
= r-th
column ofV, we compute its similarity with queried question v
q
as following
s
topic
(q, d) =
< v
q
,v
d
>
?v
q
?
2
? ?v
d
?
2
(13)
The latent topic space score s
topic
(q, d) is combined with the conventional term matching score
s
term
(q, d) for final relevance ranking. There are several ways to conduct the combination. Linear
combination is a simple and effective way. The final relevance ranking score s(q, d) is:
s(q, d) = ?s
topic
(q, d) + (1? ?)s
term
(q, d) (14)
where ? ? [0, 1] is the parameter which controls the relative importance of the latent topic space score
and term matching score. s
term
(q, d) can be calculated with any of the conventional relevance models
such as BM25 (Robertson et al., 1994) and LM (Zhai and Lafferty, 2001).
93
3 Experiments
3.1 Data Set and Evaluation Metrics
We collect the data set from Yahoo! Answers and use the getByCategory function provided in Yahoo!
Answers API
3
to obtain CQA threads from the Yahoo! site. More specifically, we utilize the resolved
questions and the resulting question repository that we use for question retrieval contains 2,288,607 ques-
tions. Each resolved question consists of four parts: ?question title?, ?question description?, ?question
answers? and ?question category?. We only use the ?question title? and ?question category? parts, which
have been widely used in the literature for question retrieval (Cao et al., 2009; Cao et al., 2010). There
are 26 first-level categories in the predefined natural hierarchy, i.e., each historical question is categorized
into one of the 26 categories. The categories include ?Arts & Humanities?, ?Beauty & Style?, ?Business
& Finance?, etc.
In order to evaluate our approach, we randomly select 2,000 questions as queried questions from the
above data collection to construct the validation/test sets, and the remaining data collection as training
set. Note that we select the queried questions in proportion to the number of questions and categories
against the whole distribution to have a better control over a possible imbalance. To obtain the ground-
truth, we employ the Vector Space Model (VSM) (Salton et al., 1975) to retrieve the top 10 results and
obtain manual judgements. The top 10 results don?t include the queried question itself. Given a returned
result by VSM, an annotator is asked to label it with ?relevant? or ?irrelevant?. If a returned result
is considered semantically equivalent to the queried question, the annotator will label it as ?relevant?;
otherwise, the annotator will label it as ?irrelevant?. Two annotators are involved in the annotation
process. If a conflict happens, a third person will make judgement for the final result. In the process
of manually judging questions, the annotators are presented only the questions. As a result, there are in
total 20,000 judged question pairs. We randomly split the 2,000 queried questions into validation/test
sets, each has 1,000/1,000 queried questions. We use the validation set for parameter tuning and the test
set for evaluation.
Evaluation Metrics: We evaluate the performance of question retrieval using the following metrics:
Mean Average Precision (MAP) and Precision@N (P@N). MAP rewards methods that return relevant
questions early and also rewards correct ranking of the results. P@N reports the fraction of the top-N
questions retrieved that are relevant. We perform a significant test, i.e., a t-test with a default significant
level of 0.05.
There are several parameters used in the paper, we tune these parameters on the validation set.
Specifically, we set the number of category-specific topics per category and the number of shared
topics in GNMFNC as (K
s
,K
p
) = {(5, 2), (10, 4), (20, 8), (40, 16), (80, 32)}, resulting in K =
{57, 114, 228, 456, 912} total number of topics. (Note that the total number of topics in GNMFNC
is K
s
+ 26 ?K
p
, where 26 is the number of categories in the first-level predefined natural hierarchy
4
).
Finally, we set (K
s
,K
p
) = (20, 8) and K = 228 empirically as this setting yields the best performance.
For regularization parameters ?
p
and ?
l
, it is difficult to directly tune on the validation set, we present
an alternative way by adding a common factor a to look at the objective function of optimization problem
in equation (3) on the training data. In other words, we set ?
p
=
a
K
s
?K
p
and ?
l
=
a
K
p
?K
l
. Therefore, we
tune the parameters ?
p
and ?
l
by alternatively adjusting the common factor a via grid search. As a result,
we set a = 100, resulting in ?
p
= ?
l
= 0.625 in the following experiments. The trade-off parameter ?
in the linear combination is set from 0 to 1 in steps of 0.1 for all methods. We set ? = 0.6 empirically.
For shrinkage regularization parameters, we empirically set ?
1
= ?
2
= ?
3
= 1.
3.2 Question Retrieval Results
In this experiment, we present the experimental results for question retrieval on the test data set. Specif-
ically, for our proposed GNMFNC, we combine the latent topic matching scores with the term matching
scores given by BM25 and LM, denoted as ?BM25+GNMFNC? and ?LM+GNMFNC?. Table 2 shows
3
http://developer.yahoo.com/answers
4
Here we do not use the leaf categories because we find that it is not possible to run GNMFNC with such large number of
topics on the current machines, and we will leave it for future work.
94
Table 2: Comparison with different methods
for question retrieval.
# Methods MAP P@10
1 BM25 0.243 0.225
2 LM 0.286 0.232
3 (Jeon et al., 2005) 0.327 0.235
4 (Xue et al., 2008) 0.341 0.238
5 (Zhou et al., 2011) 0.365 0.243
6 (Singh, 2012) 0.354 0.240
7 (Cao et al., 2010) 0.358 0.242
8 (Cai et al., 2011) 0.331 0.236
9 BM25+GNMFNC 0.369 0.248
10 LM+GNMFNC 0.374 0.251
Table 3: Comparison of matrix factoriza-
tions for question retrieval.
# Methods MAP P@10
1 BM25 0.243 0.225
2 BM25+NMF 0.325 0.235
3 BM25+CNMF 0.344 0.239
4 BM25+GNMF 0.361 0.242
5 BM25+GNMFNC 0.369 0.248
6 LM 0.286 0.232
7 LM+NMF 0.337 0.237
8 LM+CNMF 0.352 0.240
9 LM+GNMF 0.365 0.243
10 LM+GNMFNC 0.374 0.251
the main retrieval performances under the evaluation metrics MAP, P@1 and P@10. Row 1 and row
2 are the baseline systems, which model the relevance ranking using BM25 (Robertson et al., 1994)
and language model (LM) (Zhai and Lafferty, 2001) in the term space. Row 3 is word-based transla-
tion model (Jeon et al., 2005), and row 4 is word-based translation language model (TRLM) (Xue et
al., 2008). Row 5 is phrase-based translation model (Zhou et al., 2011), and row 6 is the entity-based
translation model (Singh, 2012). Row 7 to row 11 explore the natural categories for question retrieval.
In row 7, Cao et al. (2010) employed the natural categories to compute the local and global relevance
with different model combination, here we use the combination VSM + TRLM for comparison because
this combination obtains the superior performance than others. In row 8, Cai et al. (2011) proposed a
category-enhanced TRLM for question retrieval. There are some clear trends in the results of Table 2:
(1) BM25+GNMFNC and LM+GNMFNC perform significantly better than BM25 and LM respec-
tively (t-test, p-value < 0.05, row 1 vs. row 9; row 2 vs. row 10), indicating the effective of GNMFNC.
(2) BM25+GNMFNC and LM+GNMFNC perform better than translation methods, some improve-
ments are statistical significant (t-test, p-value < 0.05, row 3 and row 4 vs. row 9 and row 10). The
reason may be that GNMFNC models the relevance ranking in the latent topic space, which can also
effectively solve the the lexical gap problem.
(3) Capturing the shared aspects and the category-specific individual aspects with natural categories
in the group modeling framework can significantly improve the performance of question retrieval (t-test,
p-value < 0.05, row 7 and row 8 vs. row 9 and row 10).
(4) Natural categories are useful and effectiveness for question retrieval, no matter in the group mod-
eling framework or existing retrieval models (row 3? row 6 vs. row 7?row 10).
3.3 Comparison of Matrix Factorizations
We note that our proposed GNMFNC is related to non-negative matrix factorization (NMF) (Lee and
Seung, 2001) and its variants, we introduce three baselines. The first baseline is NMF, which is trained
on the whole training data. The second baseline is CNMF, which is trained on each category without
considering the shared topics. The third baseline is GNMF (Lee and Choi, 2009; Wang et al., 2012),
which is similar to our GNMFNC but there are no constraints on the category-specific topics to prevent
them from capturing the information from the shared topics.
NMF and GNMF are trained on the training data with the same parameter settings in section 4.1 for
fair comparison. For CNMF, we also train the model on the training data with the same parameter settings
in section 4.1, except parameter K
s
, as there exists no shared topics in CNMF.
Table 3 shows the question retrieval performance of NMF families on the test set, obtained with the
best parameter settings determined by the validation set. From the results, we draw the following obser-
vations:
(1) All of these methods can significantly improve the performance in comparison to the baseline
BM25 and LM (t-test, p-value < 0.05).
(2) GNMF and GNMFNC perform significantly better than NMF and CNMF respectively (t-test, p-
value < 0.05), indicating the effectiveness of group matrix factorization framework, especially the use
of shared topics.
95
0 20 40 60 80 1000.41
0.42
0.43
0.44
0.45
0.46
0.47
0.48
0.49
0.5
Iteration number
Obje
ctive 
funct
ion v
alue
Figure 1: Convergence curve of GNMFNC.
-4 -3 -2 -1 0 1 2 3 40.414
0.416
0.418
0.42
0.422
0.424
0.426
0.428
0.43
Log10a
Conv
erged
 obje
ctive 
funct
ion v
alue
Figure 2: Objective function value vs. factor a.
(3) GNMFNC performs significantly better than GNMF (t-test, p-value < 0.05, row 4 vs. row 5; row
9 vs. row 10), indicating the effectiveness of the regularization term on the category-specific topics to
prevent them from capturing the information from the shared topics.
From the experimental results reported above, we can conclude that our proposed GNMFNC is useful
for question retrieval with high accuracies. To the best of our knowledge, it is the first time to investigate
the group matrix factorization for question retrieval.
3.4 Convergence Behavior
In subsection 2.3.1, we have shown that the multiplicative updates given by equations (4)?(6) are con-
vergent. Here, we empirically show the convergence behavior of GNMFNC.
Figure 1 shows the convergence curve of GNMFNC on the training data set. From the figure, y-axis is
the value of objective function and x-axis denotes the iteration number. We can see that the multiplicative
updates for GNMFNC converge very fast, usually within 80 iterations.
3.5 Regularization Parameters Selection
One success of this paper is to use regularized constrains on the category-specific topics to prevent them
from capturing the information from the shared topics. It is necessary to give an in-depth analysis of
the regularization parameters used in the paper. Consider the regularization term used in equation (2),
each element in U
T
s
U
p
and U
T
p
U
l
has a value between 0 and 1 as each column of U
s
, U
p
and U
l
is
normalized. Therefore, it is appropriate to normalize the term having ?U
T
s
U
p
?
2
F
by K
s
K
p
since there
are K
s
?K
p
elements inU
T
s
U
p
. Similarly, ?U
T
p
U
l
?
2
F
is normalized by K
l
K
p
. Note that K
l
= K
p
and
l ?= p. As discussed in subsection 4.1, we present an alternative way by adding a common factor a and
set ?
p
=
a
K
s
?K
p
and ?
l
=
a
K
p
?K
l
. The common factor a is used to adjust a trade-off between the matrix
factorization errors and the mutual orthogonality, which cannot directly tune on the validation set. Thus,
we look at the objective function of optimization problem in equation (3) on the training data and find
the optimum value for a.
Figure 2 shows the objective function value vs. common factor a, where y-axis denotes the converged
objective function value, and x-axis denotes Log
10
a . We can see that the optimum value of a is 100.
Therefore, the common factor a can be fixed at 100 for our data set used in the paper, resulting in
?
p
= ?
l
= 0.625. Note that the optimum value of (K
s
,K
p
) are set as (20, 8) in subsection 4.1. Due to
limited space, we do not give an in-depth analysis for other parameters.
4 Conclusion and Future Work
In this paper, we propose a novel approach, called group non-negative matrix factorization with natural
categories (GNMFNC). The proposed method is achieved by learning the category-specific topics for
each category as well as shared topics across all categories via a group non-negative matrix factorization
framework. We derive an efficient algorithm for learning the factorization, analyze its complexity, and
96
provide proof of convergence. Experiments show that our proposed approach significantly outperforms
various baseline methods and achieves state-of-the-art performance for question retrieval.
There are some ways in which this research could be continued. First, the optimization of GNMFNC
can be decomposed into many sub-optimization problems, a natural avenue for future research is to
reduce the running time by executing the optimization in a distributed computing environment (e.g.,
MapReduce (Dean et al., 2004)). Second, another combination approach will be used to incorporate the
latent topic match score as a feature in a learning to rank model, e.g., LambdaRank (Burges et al., 2007).
Third, we will try to investigate the use of the proposed approach for other kinds of data sets with larger
categories, such as categorized documents from ODP project.
5
Acknowledgments
This work was supported by the National Natural Science Foundation of China (No. 61333018 and
No. 61303180), the Beijing Natural Science Foundation (No. 4144087), CCF Opening Project of Chi-
nese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund. We thank the
anonymous reviewers for their insightful comments.
References
D. Bernhard and I. Gurevych. 2009. Combining lexical semantic resources with question & answer archives for
translation-based answer finding. In Proceedings of ACL, pages 728-736.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization. Cambridge university press.
C. Boutsidis and E. Gallopoulos. 2008. SVD based initialization: a head start for nonnegative matrix factorization.
Pattern Recognition, 41(4):1350-1362.
C. Burges, R. Ragno, and Q. Le. 2007. Learning to rank with nonsmooth cost function. In Proceedings of NIPS.
L. Cai, G. Zhou, K. Liu, and J. Zhao. 2011. Learning the latent topics for question retrieval in community QA. In
Proceedings of IJCNLP.
X. Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang. 2009. The use of categorization information in language
models for question retrieval. In Proceedings of CIKM, pages 265-274.
X. Cao, G. Cong, B. Cui, and C. Jensen. 2010. A generalized framework of exploring category information for
question retrieval in community question answer archives. In Proceedings of WWW.
J. Dean, S. Ghemanwat, and G. Inc. 2004. Mapreduce: simplified data processing on large clusters. In Proceed-
ings of OSDI.
H. Duan, Y. Cao, C. Lin, and Y. Yu. 2008. Searching questions by identifying questions topics and question focus.
In Proceedings of ACL, pages 156-164.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar questions in large question and answer archives. In Proceed-
ings of CIKM, pages 84-90.
Z. Ji, F. Xu, and B. Wang. 2012. A category-integrated language model for question retrieval in community
question answering. In Proceedings of AIRS, pages 14-25.
H. Kim and H. Park. 2008. Non-negative matrix factorization based on alternating non-negativity constrained
least squares and active set method. SIAM J Matrix Anal Appl, 30(2):713-730.
A. Langville, C. Meyer, R. Albright, J. Cox, and D. Duling. 2006. Initializations for the nonnegative matrix
factorization. In Proceedings of KDD.
J. Lee, S. Kim, Y. Song, and H. Rim. 2008. Bridging lexical gaps between queries and questions on large online
Q&A collections with compact translation models. In Proceedings of EMNLP, pages 410-418.
D. Lee and H. Seung. 2001. Algorithms for non-negative matrix factorization. In Proceedings of NIPS.
5
http://www.dmoz.org/
97
H. Lee and S. Choi. 2009. Group nonnegative matrix factorization for eeg classification. In Proceedings of
AISTATS, pages 320-327.
C. Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural Comput, 19(10):2756-
2779.
Z. Ming, T. Chua, and G. Cong. 2010. Exploring domain-specific term weight in archived question search. In
Proceedings of CIKM, pages 1605-1608.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proceedings of ACL, pages 464-471.
S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. 1994. Okapi at trec-3. In Proceedings
of TREC, pages 109-126.
G. Salton, A. Wong, and C. Yang. 1975. A vector space model for automatic indexing. Communications of the
ACM, 18(11):613-620.
A. Singh. 2012. Entity based q&a retrieval. In Proceedings of EMNLP-CoNLL, pages 1266-1277.
Q. Wang, Z. Cao, J. Xun, and H. Li. 2012. Group matrix factorizaiton for scalable topic modeling. In Proceedings
of SIGIR.
X. Xue, J. Jeon, and W. Croft. 2008. Retrieval models for question and answer archives. In Proceedings of SIGIR,
pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth methods for language models applied to ad hoc information
retrieval. In Proceedings of SIGIR, pages 334-342.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-based translation model for question retrieval in community
question answer archives. In Proceedings of ACL, pages 653-662.
G. Zhou, F. Liu, Y. Liu, S. He, and J. Zhao. 2013. Statistical machine translation improves question retrieval in
community question answering via matrix factorization. In Proceedings of ACL, pages 852-861.
G. Zhou, Y. Chen, D. Zeng, and J. Zhao. 2013. Toward faster and better retrieval models for question search. In
Proceedings of CIKM, pages 2139-2148.
98
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1764?1773,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Opinion Words and Opinion Targets in a Two-Stage Framework
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, ybchen, jzhao}@nlpr.ia.ac.cn
Abstract
This paper proposes a novel two-stage
method for mining opinion words and
opinion targets. In the first stage, we
propose a Sentiment Graph Walking algo-
rithm, which naturally incorporates syn-
tactic patterns in a Sentiment Graph to ex-
tract opinion word/target candidates. Then
random walking is employed to estimate
confidence of candidates, which improves
extraction accuracy by considering confi-
dence of patterns. In the second stage, we
adopt a self-learning strategy to refine the
results from the first stage, especially for
filtering out high-frequency noise terms
and capturing the long-tail terms, which
are not investigated by previous meth-
ods. The experimental results on three real
world datasets demonstrate the effective-
ness of our approach compared with state-
of-the-art unsupervised methods.
1 Introduction
Opinion mining not only assists users to make in-
formed purchase decisions, but also helps busi-
ness organizations understand and act upon cus-
tomer feedbacks on their products or services in
real-time. Extracting opinion words and opinion
targets are two key tasks in opinion mining. Opin-
ion words refer to those terms indicating positive
or negative sentiment. Opinion targets represent
aspects or attributes of objects toward which opin-
ions are expressed. Mining these terms from re-
views of a specific domain allows a more thorough
understanding of customers? opinions.
Opinion words and opinion targets often co-
occur in reviews and there exist modified relations
(called opinion relation in this paper) between
them. For example, in the sentence ?It has a clear
screen?, ?clear? is an opinion word and ?screen? is
an opinion target, and there is an opinion relation
between the two words. It is natural to identify
such opinion relations through common syntactic
patterns (also called opinion patterns in this pa-
per) between opinion words and targets. For ex-
ample, we can extract ?clear? and ?screen? by us-
ing a syntactic pattern ?Adj-{mod}-Noun?, which
captures the opinion relation between them. Al-
though previous works have shown the effective-
ness of syntactic patterns for this task (Qiu et al,
2009; Zhang et al, 2010), they still have some lim-
itations as follows.
False Opinion Relations: As an example, the
phrase ?everyday at school? can be matched by
a pattern ?Adj-{mod}-(Prep)-{pcomp-n}-Noun?,
but it doesn?t bear any sentiment orientation. We
call such relations that match opinion patterns but
express no opinion false opinion relations. Pre-
vious pattern learning algorithms (Zhuang et al,
2006; Kessler and Nicolov, 2009; Jijkoun et al,
2010) often extract opinion patterns by frequency.
However, some high-frequency syntactic patterns
can have very poor precision (Kessler and Nicolov,
2009).
False Opinion Targets: In another case, the
phrase ?wonderful time? can be matched by
an opinion pattern ?Adj-{mod}-Noun?, which is
widely used in previous works (Popescu and Et-
zioni, 2005; Qiu et al, 2009). As can be seen, this
phrase does express a positive opinion but unfortu-
nately ?time? is not a valid opinion target for most
domains such as MP3. Thus, false opinion targets
are extracted. Due to the lack of ground-truth
knowledge for opinion targets, non-target terms
introduced in this way can be hardly filtered out.
Long-tail Opinion Targets: We further no-
tice that previous works prone to extract opinion
targets with high frequency (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Qiu et al, 2009; Zhu
et al, 2009), and they often have difficulty in iden-
tifying the infrequent or long-tail opinion targets.
1764
To address the problems stated above, this pa-
per proposes a two-stage framework for mining
opinion words and opinion targets. The under-
lying motivation is analogous to the novel idea
?Mine the Easy, Classify the Hard? (Dasgupta and
Ng, 2009). In our first stage, we propose a Senti-
ment Graph Walking algorithm to cope with the
false opinion relation problem, which mines easy
cases of opinion words/targets. We speculate that
it may be helpful to introduce a confidence score
for each pattern. Concretely, we create a Sen-
timent Graph to model opinion relations among
opinion word/target/pattern candidates and apply
random walking to estimate confidence of them.
Thus, confidence of pattern is considered in a uni-
fied process. Patterns that often extract false opin-
ion relations will have low confidence, and terms
introduced by low-confidence patterns will also
have low confidence accordingly. This could po-
tentially improve the extraction accuracy.
In the second stage, we identify the hard cases,
which aims to filter out false opinion targets and
extract long-tail opinion targets. Previous super-
vised methods have been shown to achieve state-
of-the-art results for this task (Wu et al, 2009; Jin
and Ho, 2009; Li et al, 2010). However, the big
challenge for fully supervised method is the lack
of annotated training data. Therefore, we adopt a
self-learning strategy. Specifically, we employ a
semi-supervised classifier to refine the target re-
sults from the first stage, which uses some highly
confident target candidates as the initial labeled
examples. Then opinion words are also refined.
Our main contributions are as follows:
? We propose a Sentiment Graph Walking al-
gorithm to mine opinion words and opinion
targets from reviews, which naturally incor-
porates confidence of syntactic pattern in a
graph to improve extraction performance. To
our best knowledge, the incorporation of pat-
tern confidence in such a Sentiment Graph
has never been studied before for opinion
words/targets mining task (Section 3).
? We adopt a self-learning method for refining
opinion words/targets generated by Sentiment
Graph Walking. Specifically, it can remove
high-frequency noise terms and capture long-
tail opinion targets in corpora (Section 4).
? We perform experiments on three real world
datasets, which demonstrate the effectiveness
of our method compared with state-of-the-art
unsupervised methods (Section 5).
2 Related Work
In opinion words/targets mining task, most unsu-
pervised methods rely on identifying opinion rela-
tions between opinion words and opinion targets.
Hu and Liu (2004) proposed an association mining
technique to extract opinion words/targets. The
simple heuristic rules they used may potentially
introduce many false opinion words/targets. To
identify opinion relations more precisely, subse-
quent research work exploited syntax information.
Popescu and Etzioni (2005) used manually com-
plied syntactic patterns and Pointwise Mutual In-
formation (PMI) to extract opinion words/targets.
Qiu et al (2009) proposed a bootstrapping frame-
work called Double Propagation which intro-
duced eight heuristic syntactic rules. While man-
ually defining syntactic patterns could be time-
consuming and error-prone, we learn syntactic
patterns automatically from data.
There have been extensive works on mining
opinion words and opinion targets by syntac-
tic pattern learning. Riloff and Wiebe (2003)
performed pattern learning through bootstrapping
while extracting subjective expressions. Zhuang
et al (2006) obtained various dependency re-
lationship templates from an annotated movie
corpus and applied them to supervised opinion
words/targets extraction. Kobayashi et al (2007)
adopted a supervised learning technique to search
for useful syntactic patterns as contextual clues.
Our approach is similar to (Wiebe and Riloff,
2005) and (Xu et al, 2013), all of which apply
syntactic pattern learning and adopt self-learning
strategy. However, the task of (Wiebe and Riloff,
2005) was to classify sentiment orientations in
sentence level, while ours needs to extract more
detailed information in term level. In addition,
our method extends (Xu et al, 2013), and we
give a more complete and in-depth analysis on
the aforementioned problems in the first section.
There were also many works employed graph-
based method (Li et al, 2012; Zhang et al, 2010;
Hassan and Radev, 2010; Liu et al, 2012), but
none of previous works considered confidence of
patterns in the graph.
In supervised approaches, various kinds of
models were applied, such as HMM (Jin and Ho,
2009), SVM (Wu et al, 2009) and CRFs (Li et al,
2010). The downside of supervised methods was
the difficulty of obtaining annotated training data
in practical applications. Also, classifiers trained
1765
on one domain often fail to give satisfactory re-
sults when shifted to another domain. Our method
does not rely on annotated training data.
3 The First Stage: Sentiment Graph
Walking Algorithm
In the first stage, we propose a graph-based al-
gorithm called Sentiment Graph Walking to mine
opinion words and opinion targets from reviews.
3.1 Opinion Pattern Learning for Candidates
Generation
For a given sentence, we first obtain its depen-
dency tree. Following (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Qiu et al, 2009), we regard all
adjectives as opinion word candidates (OC) and
all nouns or noun phrases as opinion target can-
didates (TC). A statistic-based method in (Zhu et
al., 2009) is used to detect noun phrases. Then
candidates are replaced by wildcards ?<OC>? or
?<TC>?. Figure 1 gives a dependency tree exam-
ple generated by Minipar (Lin, 1998).
p red s det
m od
gor geous<OC>
is(VBE)
style<TC>
the(Det)
of(P r ep) scr een<TC>pcom p-n
the(Det)
det
Figure 1: The dependency tree of the sentence
?The style of the screen is gorgeous?.
We extract two kinds of opinion patterns: ?OC-
TC? pattern and ?TC-TC? pattern. The ?OC-
TC? pattern is the shortest path between an OC
wildcard and a TC wildcard in dependency tree,
which captures opinion relation between an opin-
ion word candidate and an opinion target can-
didate. Similarly, the ?TC-TC? pattern cap-
tures opinion relation between two opinion tar-
get candidates.1 Words in opinion patterns are
replaced by their POS tags, and we constrain
that there are at most two words other than
wildcards in each pattern. In Figure 1, there
are two opinion patterns marked out by dash
lines: ?<OC>{pred}(VBE){s}<TC>? for the
?OC-TC? type and ?<TC>{mod}(Prep){pcomp-
n}<TC>? for the ?TC-TC? type. After all pat-
1We do not identify the opinion relation ?OC-OC? be-
cause this relation is often unreliable.
terns are generated, we drop those patterns with
frequency lower than a threshold F .
3.2 Sentiment Graph Construction
To model the opinion relations among opinion
words/targets and opinion patterns, a graph named
as Sentiment Graph is constructed, which is a
weighted, directed graph G = (V,E,W ), where
? V = {Voc ? Vtc ? Vp} is the set of vertices in
G, where Voc, Vtc and Vp represent the set of
opinion word candidates, opinion target can-
didates and opinion patterns, respectively.
? E = {Epo?Ept} ? {Vp?Voc}?{Vp?Vtc}
is the weighted, bi-directional edge set in G,
where Epo and Ept are mutually exclusive
sets of edges connecting opinion word/target
vertices to opinion pattern vertices. Note that
there are no edges between Voc and Vtc.
? W : E ? R+ is the weight function which
assigns non-negative weight to each edge.
For each (e : va ? vb) ? E, where
va, vb ? V , the weight function w(va, vb) =
freq(va, vb)/freq(va), where freq(?) is the
frequency of a candidate extracted by opinion
patterns or co-occurrence frequency between
two candidates.
Figure 2 shows an example of Sentiment Graph.
n icelarge
screen display
<OC>{mod}<TC> <OC>{mod}<TC>{con j}<TC>
1
0.8
0.7
0.2
0.3
0.4
0.2
0.33
0.33
0.33
0.6
0.4
0.2 0.2
Figure 2: An example of Sentiment Graph.
3.3 Confidence Estimation by Random
Walking with Restart
We believe that considering confidence of patterns
can potentially improve the extraction accuracy.
Our intuitive idea is: (i) If an opinion word/target
is with higher confidence, the syntactic patterns
containing this term are more likely to be used to
express customers? opinion. (ii) If an opinion pat-
tern has higher confidence, terms extracted by this
pattern are more likely to be correct. It?s a rein-
forcement process.
1766
We use Random Walking with Restart (RWR)
algorithm to implement our idea described above.
Let Moc p denotes the transition matrix from Voc
to Vp, for vo ? Voc, vp ? Vp, Moc p(vo, vp) =
w(vo, vp). Similarly, we have Mtc p, Mp oc,
Mp tc. Let c denotes confidence vector of candi-
dates so ctoc, cttc and ctp are confidence vectors for
opinion word/target/pattern candidates after walk-
ing t steps. Initially c0oc is uniformly distributed
on a few domain-independent opinion word seeds,
then the following formula are updated iteratively
until cttc and ctoc converge:
ct+1p = MToc p ? ctoc +MTtc p ? cttc (1)
ct+1oc = (1? ?)MTp oc ? ctp + ?c0oc (2)
ct+1tc = MTp tc ? ctp (3)
where MT is the transpose of matrix M and ? is
a small probability of teleporting back to the seed
vertices which prevents us from walking too far
away from the seeds. In the experiments below, ?
is set 0.1 empirically.
4 The Second Stage: Refining Extracted
Results Using Self-Learning
At the end of the first stage, we obtain a ranked
list of opinion words and opinion targets, in which
higher ranked terms are more likely to be correct.
Nevertheless, there are still some issues needed to
be addressed:
1) In the target candidate list, some high-
frequency frivolous general nouns such as
?thing? and ?people? are also highly ranked.
This is because there exist many opinion ex-
pressions containing non-target terms such as
?good thing?, ?nice people?, etc. in reviews.
Due to the lack of ground-truth knowledge
for opinion targets, the false opinion target
problem still remains unsolved.
2) In another aspect, long-tail opinion targets
may have low degree in Sentiment Graph.
Hence their confidence will be low although
they may be extracted by some high qual-
ity patterns. Therefore, the first stage is in-
capable of dealing with the long-tail opinion
target problem.
3) Furthermore, the first stage also extracts
some high-frequency false opinion words
such as ?every?, ?many?, etc. Many terms
of this kind are introduced by high-frequency
false opinion targets, for there are large
amounts of phrases like ?every time? and
?many people?. So this issue is a side effect
of the false opinion target problem.
To address these issues, we exploit a self-
learning strategy. For opinion targets, we use a
semi-supervised binary classifier called target re-
fining classifier to refine target candidates. For
opinion words, we use the classified list of opin-
ion targets to further refine the extracted opinion
word candidates.
4.1 Opinion Targets Refinement
There are two keys for opinion target refinement:
(i) How to generate the initial labeled data for tar-
get refining classifier. (ii) How to properly repre-
sent a long-tail opinion target candidate other than
comparing frequency between different targets.
For the first key, it is clearly improper to select
high-confidence targets as positive examples and
choose low-confidence targets as negative exam-
ples2, for there are noise with high confidence and
long-tail targets with low confidence. Fortunately,
a large proportion of general noun noises are the
most frequent words in common texts. Therefore,
we can generate a small domain-independent gen-
eral noun (GN) corpus from large web corpora to
cover some most frequently used general noun ex-
amples. Then labeled examples can be drawn from
the target candidate list and the GN corpus.
For the second key, we utilize opinion words
and opinion patterns with their confidence scores
to represent an opinion target. By this means, a
long-tail opinion target can be determined by its
own contexts, whose weights are learnt from con-
texts of frequent opinion targets. Thus, if a long-
tail opinion target candidate has high contextual
support, it will have higher probability to be found
out in despite of its low frequency.
Creation of General Noun Corpora. 1000
most frequent nouns in Google-1-gram3 were se-
lected as general noun candidates. On the other
hand, we added all nouns in the top three levels of
hyponyms in four WordNet (Miller, 1995) synsets
?object?, ?person?, ?group? and ?measure? into
the GN corpus. Our idea was based on the fact that
a term is more general when it sits in higher level
in the WordNet hierarchy. Then inapplicable can-
didates were discarded and a 3071-word English
2Note that the ?positive? and ?negative? here denote opin-
ion targets and non-target terms respectively and they do not
indicate sentiment polarities.
3http://books.google.com/ngrams.
1767
GN corpus was created. Another Chinese GN cor-
pus with 3493 words was generated in the similar
way from HowNet (Gan and Wong, 2000).
Generation of Labeled Examples. Let T =
{Y+1,Y?1} denotes the initial labeled set, where
N most highly confident target candidates but not
in our GN corpora are regarded as the positive ex-
ample set Y+1, other N terms from GN corpora
which are also top ranked in the target list are se-
lected as the negative example set Y?1. The re-
minder unlabeled candidates are denoted by T ?.
Feature Representation for Classifier. Given
T and T ? in the form of {(xi, yi)}. For a target
candidate ti, xi = (o1, . . . , on, p1, . . . , pm)T rep-
resents its feature vector, where oj is the opinion
word feature and pk is the opinion pattern feature.
The value of feature is defined as follows,
x(oj) = conf(oj)?
?
pk freq(ti, oj , pk)
freq(oj)
(4)
x(pk) = conf(pk)?
?
oj freq(ti, oj , pk)
freq(pk)
(5)
where conf(?) denotes confidence score estimated
by RWR, freq(?) has the same meaning as in Sec-
tion 3.2. Particularly, freq(ti, oj , pk) represents
the frequency of pattern pk extracting opinion tar-
get ti and opinion word oj .
Target Refinement Classifier: We use support
vector machine as the binary classifier. Hence, the
classification problem can be formulated as to find
a hyperplane < w, b > that separates both labeled
set T and unlabeled set T ? with maximum mar-
gin. The optimization goal is to minimize over
(T ,T ?,w, b, ?1, ..., ?n, ??1 , ..., ??k):
1
2 ||w||
2 + C
n?
i=0
?i + C?
k?
j=0
??j
subject to : ?ni=1 : yi[w ? xi + b] ? 1? ?i
?kj=1 : y?j [w ? x?j + b] ? 1? ??j
?ni=1 : ?i > 0
?kj=1 : ??j > 0
where yi, y?j ? {+1,?1}, xi and x?j represent
feature vectors, C and C? are parameters set by
user. This optimization problem can be imple-
mented by a typical Transductive Support Vector
Machine (TSVM) (Joachims, 1999).
4.2 Opinion Words Refinement
We use the classified opinion target results to re-
fine opinion words by the following equation,
s(oj) =
?
ti?T
?
pk
s(ti)conf(pk)freq(ti, oj , pk)
freq(ti)
where T is the opinion target set in which each el-
ement is classified as positive during opinion tar-
get refinement, s(ti) denotes confidence score ex-
ported by the target refining classifier. Particularly,
freq(ti) =
?
oj
?
pk freq(ti, oj , pk). A higher
score of s(oj) means that candidate oj is more
likely to be an opinion word.
5 Experiments
5.1 Datasets and Evaluation Metrics
Datasets: We select three real world datasets to
evaluate our approach. The first one is called
Customer Review Dataset (CRD) (Hu and Liu,
2004) which contains reviews on five different
products (represented by D1 to D5) in English.
The second dataset is pre-annotated and published
in COAE084, where two domains of Chinese re-
views are selected. At last, we employ a bench-
mark dataset in (Wang et al, 2011) and named it
as Large. We manually annotated opinion words
and opinion targets as the gold standard. Three
annotators were involved. Firstly, two annotators
were required to annotate out opinion words and
opinion targets in sentences. When conflicts hap-
pened, the third annotator would make the final
judgment. The average Kappa-values of the two
domains were 0.71 for opinion words and 0.66
for opinion targets. Detailed information of our
datasets is shown in Table 1.
Dataset Domain #Sentences #OW #OT
Large
(English)
Hotel 10,000 434 1,015
MP3 10,000 559 1,158
COAE08(Chinese)
Camera 2,075 351 892
Car 4,783 622 1,179
Table 1: The detailed information of datasets. OW
stands for opinion words and OT stands for targets.
Pre-processing: Firstly, HTML tags are re-
moved from texts. Then Minipar (Lin, 1998)
is used to parse English corpora, and Standford
Parser (Chang et al, 2009) is used for Chinese
4http://ir-china.org.cn/coae2008.html
1768
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
Ours-Stage1 0.79 0.85 0.82 0.82 0.87 0.84 0.83 0.87 0.85 0.78 0.88 0.83 0.82 0.88 0.85 0.84
Ours-Full 0.86 0.82 0.84 0.88 0.83 0.85 0.89 0.86 0.87 0.83 0.86 0.84 0.89 0.85 0.87 0.86
Table 2: Results of opinion target extraction on the Customer Review Dataset.
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
Ours-Stage1 0.61 0.75 0.67 0.55 0.80 0.65 0.63 0.75 0.68 0.60 0.69 0.64 0.68 0.70 0.69 0.67
Ours-Full 0.64 0.74 0.69 0.59 0.79 0.68 0.66 0.71 0.68 0.65 0.67 0.66 0.72 0.67 0.69 0.68
Table 3: Results of opinion word extraction on the Customer Review Dataset.
corpora. Stemming and fuzzy matching are also
performed following previous work (Hu and Liu,
2004).
Evaluation Metrics: We evaluate our method
by precision(P), recall(R) and F-measure(F).
5.2 Our Method vs. the State-of-the-art
Three state-of-the-art unsupervised methods are
used as competitors to compare with our method.
Hu extracts opinion words/targets by using ad-
jacency rules (Hu and Liu, 2004).
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009).
Zhang is an enhanced version of DP and em-
ploys HITS algorithm (Kleinberg, 1999) to rank
opinion targets (Zhang et al, 2010).
Ours-Full is the full implementation of our
method. We employ SVMlight (Joachims, 1999)
as the target refining classifier. Default parameters
are used except the bias item is set 0.
Ours-Stage1 only uses Sentiment Graph Walk-
ing algorithm which does?t have opinion word and
opinion target refinement.
All of the above approaches use same five
common opinion word seeds. The choice of opin-
ion seeds seems reasonable, as most people can
easily come up with 5 opinion words such as
?good?, ?bad?, etc. The performance on five prod-
ucts of CRD dataset is shown in Table 2 and Ta-
ble 3. Zhang does not extract opinion words so
their results for opinion words are not taken into
account. We can see that Ours-Stage1 achieves
superior recall but has some loss in precision com-
pared with DP and Zhang. This may be because
the CRD dataset is too small and our statistic-
based method may suffer from data sparseness.
In spite of this, Ours-Full achieves comparable F-
measure with DP, which is a well-designed rule-
based method.
The results on two larger datasets are shown
in Table 4 and Table 5, from which we can have
the following observation: (i) All syntax-based-
methods outperform Hu, showing the importance
of syntactic information in opinion relation identi-
fication. (ii) Ours-Full outperforms the three com-
petitors on all domains provided. (iii) Ours-Stage1
outperforms Zhang, especially in terms of recall.
We believe it benefits from our automatical pattern
learning algorithm. Moreover, Ours-Stage1 do
not loss much in precision compared with Zhang,
which indicates the applicability to estimate pat-
tern confidence in Sentiment Graph. (iv) Ours-
Full achieves 4-9% improvement in precision over
the most accurate method, which shows the effec-
tiveness of our second stage.
5.3 Detailed Discussions
This section gives several variants of our method
to have a more detailed analysis.
Ours-Bigraph constructs a bi-graph between
opinion words and targets, so opinion patterns
are not included in the graph. Then RWR algo-
rithm is used to only assign confidence to opinion
word/target candidates.
Ours-Stage2 only contains the second stage,
which doesn?t apply Sentiment Graph Walking al-
gorithm. Hence the confidence score conf(?) in
Equations (4) and (5) have no values and they are
set to 1. The initial labeled examples are exactly
the same as Ours-Full. Due to the limitation of
space, we only give analysis on opinion target ex-
traction results in Figure 3.
1769
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.53 0.55 0.54 0.55 0.57 0.56 0.63 0.65 0.64 0.62 0.58 0.60 0.58
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
Zhang 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
Ours-Stage1 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
Ours-Full 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
Table 4: Results of opinion targets extraction on Large and COAE08.
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.48 0.65 0.55 0.51 0.68 0.58 0.72 0.74 0.73 0.70 0.71 0.70 0.64
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.80 0.73 0.76 0.79 0.71 0.75 0.68
Ours-Stage1 0.59 0.69 0.64 0.61 0.71 0.66 0.79 0.78 0.78 0.77 0.77 0.77 0.71
Ours-Full 0.64 0.67 0.65 0.67 0.69 0.68 0.82 0.78 0.80 0.80 0.76 0.78 0.73
Table 5: Results of opinion words extraction on Large and COAE08.
Figure 3: Opinion target extraction results.
5.3.1 The Effect of Sentiment Graph Walking
We can see that our graph-based methods (Ours-
Bigraph and Ours-Stage1) achieve higher recall
than Zhang. By learning patterns automatically,
our method captures opinion relations more ef-
ficiently. Also, Ours-Stage1 outperforms Ours-
Bigraph, especially in precision. We believe it is
because Ours-Stage1 estimated confidence of pat-
terns so false opinion relations are reduced. There-
fore, the consideration of pattern confidence is
beneficial as expected, which alleviates the false
opinion relation problem. On another hand, we
find that Ours-Stage2 has much worse perfor-
mance than Ours-Full. This shows the effective-
ness of Sentiment Graph Walking algorithm since
the confidence scores estimated in the first stage
are indispensable and indeed key to the learning
of the second stage.
5.3.2 The Effect of Self-Learning
Figure 4 shows the average Precision@N curve of
four domains on opinion target extraction. Ours-
GN-Only is implemented by only removing 50
initial negative examples found by our GN cor-
pora. We can see that the GN corpora work quite
well, which find out most top-ranked false opin-
ion targets. At the same time, Ours-Full has much
better performance than Ours-GN-Only which in-
dicates that Ours-Full can filter out more noises
other than the initial negative examples. There-
fore, our self-learning strategy alleviates the short-
coming of false opinion target problem. More-
over, Table 5 shows that the performance of opin-
ion word extraction is also improved based on the
classified results of opinion targets.
Figure 4: The average precision@N curve of the
four domains on opinion target extraction.
1770
ID Pattern Example #Ext. Conf. PrO PrT
#1 <OC>{mod}<TC> it has a clear screen 7344 0.3938 0.59 0.66
#2 <TC>{subj}<OC> the sound quality is excellent 2791 0.0689 0.62 0.70
#3 <TC>{conj}<TC> the size and weight make it convenient 3620 0.0208 N/A 0.67
#4 <TC>{subj}<TC> the button layout is a simplistic plus 1615 0.0096 N/A 0.67
#5 <OC>{pnmod}<TC> the buttons easier to use 128 0.0014 0.61 0.34
#6 <TC>{subj}(V){s}(VBE){subj}<OC> software provided is simple 189 0.0015 0.54 0.33
#7 <OC>{mod}(Prep){pcomp-c}(V){obj}<TC> great for playing audible books 211 0.0013 0.43 0.48
Table 6: Examples of English patterns. #Ext. represent number of terms extracted, Conf. denotes confi-
dence score estimated by RWR and PrO/PrT stand for precisions of extraction on opinion words/targets
of a pattern respectively. Opinion words in examples are in bold and opinion targets are in italic.
Figure 5 gives the recall of long-tail opinion
targets5 extracted, where Ours-Full is shown to
have much better performance than Ours-Stage1
and the three competitors. This observation proves
that our method can improve the limitation of
long-tail opinion target problem.
Figure 5: The recall of long-tail opinion targets.
5.3.3 Analysis on Opinion Patterns
Table 6 shows some examples of opinion pattern
and their extraction accuracy on MP3 reviews in
the first stage. Pattern #1 and #2 are the two
most high-confidence opinion patterns of ?OC-
TC? type, and Pattern #3 and #4 demonstrate two
typical ?TC-TC? patterns. As these patterns ex-
tract too many terms, the overall precision is very
low. We give Precision@400 of them, which is
more meaningful because only top listed terms
in the extracted results are regarded as opinion
targets. Pattern #5 and #6 have high precision
on opinion words but low precision on opinion
targets. This observation demonstrates the false
opinion target problem. Pattern #7 is a pattern ex-
ample that extracts many false opinion relations
and it has low precision for both opinion words
and opinion targets. We can see that Pattern #7 has
5Since there is no explicit definition for the notion ?long-
tail?, we conservatively regard 60% opinion targets with the
lowest frequency as the ?long-tail? terms.
a lower confidence compared with Pattern #5 and
#6 although it extracts more words. It?s because
it has a low probability of walking from opinion
seeds to this pattern. This further proves that our
method can reduce the confidence of low-quality
patterns.
5.3.4 Sensitivity of Parameters
Finally, we study the sensitivity of parameters
when recall is fixed at 0.70. Figure 6 shows the
precision curves at different N initial training ex-
amples and F filtering frequency. We can see that
the performance saturates when N is set to 50 and
it does not vary much under different F , showing
the robustness of our method. We thus set N to
50, and F to 3 for CRD, 5 for COAE08 and 10 for
Large accordingly.
Figure 6: Influence of parameters.
1771
6 Conclusion and Future Work
This paper proposes a novel two-stage framework
for mining opinion words and opinion targets. In
the first stage, we propose a Sentiment Graph
Walking algorithm, which incorporates syntactic
patterns in a Sentiment Graph to improve the ex-
traction performance. In the second stage, we pro-
pose a self-learning method to refine the result of
first stage. The experimental results show that our
method achieves superior performance over state-
of-the-art unsupervised methods.
We further notice that opinion words are not
limited to adjectives but can also be other type of
word such as verbs or nouns. Identifying all kinds
of opinion words is a more challenging task. We
plan to study this problem in our future work.
Acknowledgement
Thanks to Prof. Yulan He for her insightful
advices. This work was supported by the Na-
tional Natural Science Foundation of China (No.
61070106, No. 61272332 and No. 61202329),
the National High Technology Development 863
Program of China (No. 2012AA011102), the
National Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, SSST
?09, pages 51?59.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: a semi-supervised approach to au-
tomatic sentiment classification. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 701?709.
Kok Wee Gan and Ping Wai Wong. 2000. Anno-
tating information structures in chinese texts using
hownet. In Proceedings of the second workshop on
Chinese language processing: held in conjunction
with the 38th Annual Meeting of the Association for
Computational Linguistics - Volume 12, CLPW ?00,
pages 85?92, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 395?
403, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 585?594,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning, ICML
?09, pages 465?472.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pages 200?209.
Jason Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking
of linguistic configurations. In Proceedings of the
Third International AAAI Conference on Weblogs
and Social Media.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-
of relations in opinion mining. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1065?1074, June.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
653?661, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
410?419, July.
1772
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on Evaluation of Parsing Sys-
tems at ICLRE.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL ?12, pages 1346?1356,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, EMNLP ?03,
pages 105?112, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th international
conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing?05, pages 486?497.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Walk and learn: A two-stage approach
for opinion words and opinion targets co-extraction.
In Proceedings of the 22nd International World Wide
Web Conference, WWW ?13.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM conference on Information and knowledge
management, CIKM ?09, pages 1799?1802.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management, CIKM
?06, pages 43?50.
1773
