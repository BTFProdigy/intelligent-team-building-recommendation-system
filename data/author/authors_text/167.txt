Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 103?108,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generating XTAG Parsers from Algebraic Specifications?
Carlos Go?mez-Rodr??guez and Miguel A. Alonso
Departamento de Computacio?n
Universidade da Corun?a
Campus de Elvin?a, s/n
15071 A Corun?a, Spain
{cgomezr, alonso}@udc.es
Manuel Vilares
E. S. de Ingenier??a Informa?tica
Universidad de Vigo
Campus As Lagoas, s/n
32004 Ourense, Spain
vilares@uvigo.es
Abstract
In this paper, a generic system that gener-
ates parsers from parsing schemata is ap-
plied to the particular case of the XTAG
English grammar. In order to be able to
generate XTAG parsers, some transforma-
tions are made to the grammar, and TAG
parsing schemata are extended with fea-
ture structure unification support and a
simple tree filtering mechanism. The gen-
erated implementations allow us to study
the performance of different TAG parsers
when working with a large-scale, wide-
coverage grammar.
1 Introduction
Since Tree Adjoining Grammars (TAG) were in-
troduced, several different parsing algorithms for
these grammars have been developed, each with
its peculiar characteristics. Identifying the advan-
tages and disadvantages of each of them is not
trivial, and there are no comparative studies be-
tween them in the literature that work with real-
life, wide coverage grammars. In this paper, we
use a generic tool based on parsing schemata to
generate implementations of several TAG parsers
and compare them by parsing with the XTAG En-
glish Grammar (XTAG, 2001).
The parsing schemata formalism (Sikkel, 1997)
is a framework that allows us to describe parsers in
a simple and declarative way. A parsing schema
? Partially supported by Ministerio de Educacio?n y Cien-
cia and FEDER (TIN2004-07246-C03-01, TIN2004-07246-
C03-02), Xunta de Galicia (PGIDIT05PXIC30501PN,
PGIDIT05PXIC10501PN, PGIDIT05SIN044E and
PGIDIT05SIN059E), and Programa de becas FPU (Mi-
nisterio de Educacio?n y Ciencia). We are grateful to Eric
Villemonte de la Clergerie and Franc?ois Barthelemy for their
help in converting the XTAG grammar to XML.
is a representation of a parsing algorithm as a
set of inference rules which are used to perform
deductions on intermediate results called items.
These items represent sets of incomplete parse
trees which the algorithm can generate. An input
sentence to be analyzed produces an initial set of
items. Additionally, a parsing schema must de-
fine a criterion to determine which items are final,
i.e. which items correspond to complete parses of
the input sentence. If it is possible to obtain a fi-
nal item from the set of initial items by using the
schema?s inference rules (called deductive steps),
then the input sentence belongs to the language de-
fined by the grammar. The parse forest can then be
retrieved from the intermediate items used to infer
the final items, as in (Billot and Lang, 1989).
As an example, we introduce a CYK-based
algorithm (Vijay-Shanker and Joshi, 1985) for
TAG. Given a tree adjoining grammar G =
(VT , VN , S, I, A)1 and a sentence of length n
which we denote by a1 a2 . . . an2, we de-
note by P (G) the set of productions {N? ?
N?1 N
?
2 . . . N?r } such that N? is an inner node of
a tree ? ? (I ? A), and N?1 N
?
2 . . . N?r is the or-
dered sequence of direct children of N? .
The parsing schema for the TAG CYK-based
algorithm (Alonso et al, 1999) is a function that
maps such a grammar G to a deduction system
whose domain is the set of items
{[N? , i, j, p, q, adj]}
verifying that N? is a tree node in an elementary
1Where VT denotes the set of terminal symbols, VN the
set of nonterminal symbols, S the axiom, I the set of initial
trees and A the set of auxiliary trees.
2From now on, we will follow the usual conventions by
which nonterminal symbols are represented by uppercase let-
ters (A, B . . .), and terminals by lowercase letters (a, b . . .).
Greek letters (?, ?...) will be used to represent trees, N? a
node in the tree ?, and R? the root node of the tree ?.
103
tree ? ? (I ? A), i and j (0 ? i ? j) are string
positions, p and q may be undefined or instanti-
ated to positions i ? p ? q ? j (the latter only
when ? ? A), and adj ? {true, false} indi-
cates whether an adjunction has been performed
on node N? .
The positions i and j indicate that a substring
ai+1 . . . aj of the string is being recognized, and
positions p and q denote the substring dominated
by ??s foot node. The final item set would be
{[R?, 0, n,?,?, adj] | ? ? I}
for the presence of such an item would indicate
that there exists a valid parse tree with yield a1 a2
. . . an and rooted at R?, the root of an initial tree;
and therefore there exists a complete parse tree for
the sentence.
A deductive step ?1...?m? ? allows us to infer
the item specified by its consequent ? from those
in its antecedents ?1 . . . ?m. Side conditions (?)
specify the valid values for the variables appearing
in the antecedents and consequent, and may refer
to grammar rules or specify other constraints that
must be verified in order to infer the consequent.
The deductive steps for our CYK-based parser are
shown in figure 1. The steps DScanCYK and D?CYK are
used to start the bottom-up parsing process by rec-
ognizing a terminal symbol for the input string, or
none if we are using a tree with an epsilon node.
The DBinaryCYK step (where the operation p ? p? re-
turns p if p is defined, and p? otherwise) represents
the bottom-up parsing operation which joins two
subtrees into one, and is analogous to one of the
deductive steps of the CYK parser for CFG. The
DUnaryCYK step is used to handle unary branching pro-
ductions. DFootCYK and D
Adj
CYK implement the adjunc-
tion operation, where a tree ? is adjoined into a
node N? ; their side condition ? ? adj(N?) means
that ? must be adjoinable into the node N? (which
involves checking that N? is an adjunction node,
comparing its label to R??s and verifying that no
adjunction constraint disallows the operation). Fi-
nally, the DSubsCYK step implements the substitution
operation in grammars supporting it.
As can be seen from the example, parsing
schemata are simple, high-level descriptions that
convey the fundamental semantics of parsing algo-
rithms while abstracting implementation details:
they define a set of possible intermediate results
and allowed operations on them, but they don?t
specify data structures for storing the results or an
order for the operations to be executed. This high
abstraction level makes schemata useful for defin-
ing, comparing and analyzing parsers in pencil and
paper without worrying about implementation de-
tails. However, if we want to actually execute
the parsers and analyze their results and perfor-
mance in a computer, they must be implemented
in a programming language, making it necessary
to lose the high level of abstraction in order to ob-
tain functional and efficient implementations.
In order to bridge this gap between theory and
practice, we have designed and implemented a
system able to automatically transform parsing
schemata into efficient Java implementations of
their corresponding algorithms. The input to this
system is a simple and declarative representation
of a parsing schema, which is practically equal to
the formal notation that we used previously. For
example, this is the DBinaryCYK deductive step shown
in figure 1 in a format readable by our compiler:
@step CYKBinary
[ Node1 , i , k , p , q , adj1 ]
[ Node2 , k , j , p? , q? , adj2 ]
-------------------------------- Node3 -> Node1 Node2
[ Node3 , i , j , Union(p;p?) , Union(q;q?) , false ]
The parsing schemata compilation technique
used by our system is based on the following fun-
damental ideas (Go?mez-Rodr??guez et al, 2006a):
? Each deductive step is compiled to a Java class
containing code to match and search for an-
tecedent items and generate the corresponding
conclusions from the consequent.
? The step classes are coordinated by a deduc-
tive parsing engine, as the one described in
(Shieber et al, 1995). This algorithm ensures
a sound and complete deduction process, guar-
anteeing that all items that can be generated
from the initial items will be obtained.
? To attain efficiency, an automatic analysis of
the schema is performed in order to create in-
dexes allowing fast access to items. As each
different parsing schema needs to perform dif-
ferent searches for antecedent items, the index
structures we generate are schema-specific. In
this way, we guarantee constant-time access to
items so that the computational complexity of
our generated implementations is never above
the theoretical complexity of the parsers.
? Since parsing schemata have an open notation,
for any mathematical object can potentially
appear inside items, the system includes an ex-
tensibility mechanism which can be used to
define new kinds of objects to use in schemata.
104
DScanCYK =
[a, i, i + 1]
[N? , i, i + 1 | ?,? | false] a = label(N
?) D?CYK = [N? , i, i | ?,? | false] ? = label(N
?)
DUnaryCYK =
[M? , i, j | p, q | adj]
[N? , i, j | p, q] | false] N
? ? M? ? P(?) DBinaryCYK =
[M? , i, k | p, q | adj1],
[P ? , k, j | p?, q? | adj2]
[N? , i, j | p ? p?, q ? q? | false] N
? ? M?P ? ? P(?)
DFootCYK =
[N? , i, j | p, q | false]
[F? , i, j | i, j | false] ? ? adj(N
?) DAdjCYK =
[R? , i?, j? | i, j | adj],
[N? , i, j | p, q | false]
[N? , i?, j? | p, q | true] ? ? adj(N
?)
DSubsCYK =
[R?, i, j | ?,? | adj]
[N? , i, j | ?,? | false] ? ? subs(N
?)
Figure 1: A CYK-based parser for TAG.
2 Generating parsers for the XTAG
grammar
By using parsing schemata as the ones in (Alonso
et al, 1999; Nederhof, 1999) as input to our sys-
tem, we can easily obtain efficient implementa-
tions of several TAG parsing algorithms. In this
section, we describe how we have dealt with the
particular characteristics of the XTAG grammar
in order to make it compatible with our generic
compilation technique; and we also provide em-
pirical results which allow us to compare the per-
formance of several different TAG parsing algo-
rithms in the practical case of the XTAG gram-
mar. It shall be noted that similar comparisons
have been made with smaller grammars, such as
simplified subsets of the XTAG grammar, but not
with the whole XTAG grammar with all its trees
and feature structures. Therefore, our compari-
son provides valuable information about the be-
havior of various parsers on a complete, large-
scale natural language grammar. This behavior
is very different from the one that can be ob-
served on small grammars, since grammar size be-
comes a dominant factor in computational com-
plexity when large grammars like the XTAG are
used to parse relatively small natural language sen-
tences (Go?mez-Rodr??guez et al, 2006b).
2.1 Grammar conversion
The first step we undertook in order to generate
parsers for the XTAG grammar was a full conver-
sion of the grammar to an XML-based format, a
variant of the TAG markup language (TAGML).
In this way we had the grammar in a well-defined
format, easy to parse and modify. During this con-
version, the trees? anchor nodes were duplicated in
order to make our generic TAG parsers allow ad-
junctions on anchor nodes, which is allowed in the
XTAG grammar.
2.2 Feature structure unification
Two strategies may be used in order to take uni-
fication into account in parsing: feature structures
can be unified after parsing or during parsing. We
have compared the two approaches for the XTAG
grammar (see table 1), and the general conclusion
is that unification during parsing performs better
for most of the sentences, although its runtimes
have a larger variance and it performs much worse
for some particular cases.
In order to implement unification during parsing
in our parsing schemata based system, we must ex-
tend our schemata in order to perform unification.
This can be done in the following way:
? Items are extended so that they will hold a fea-
ture structure in addition to the rest of the infor-
mation they include.
? We need to define two operations on feature
structures: the unification operation and the
?keep variables? operation. The ?keep vari-
ables? operation is a transformation on feature
structures that takes a feature structure as an
argument, which may contain features, values,
symbolic variables and associations between
them, and returns a feature structure contain-
ing only the variable-value associations related
to a given elementary tree, ignoring the vari-
ables and values not associated through these
relations, and completely ignoring features.
? During the process of parsing, feature structures
that refer to the same node, or to nodes that are
taking part in a substitution or adjunction and
105
Strategy Mean T. Mean 10% T. Mean 20% 1st Quart. Median 3rd Quart. Std. Dev. Wilcoxon
During 108,270 12,164 7,812 1,585 4,424 9,671 388,010 0.4545After 412,793 10,710 10,019 2,123 9,043 19,073 14,235
Table 1: Runtimes in ms of an Earley-based parser using two different unification strategies: unification
during and after parsing. The following data are shown: mean, trimmed means (10 and 20%), quartiles,
standard deviation, and p-value for the Wilcoxon paired signed rank test (the p-value of 0.4545 indicates
that no statistically significant difference was found between the medians).
are going to collapse to a single node in the final
parse tree, must be unified. For this to be done,
the test that these nodes must unify is added as
a side condition to the steps that must handle
them, and the unification results are included
in the item generated by the consequent. Of
course, considerations about the different role
of the top and bottom feature structures in ad-
junction and substitution must be taken into ac-
count when determining which feature struc-
tures must be unified.
? Feature structures in items must only hold
variable-value associations for the symbolic
variables appearing in the tree to which the
structures refer, for these relationships hold the
information that we need in order to propa-
gate values according to the rules specified in
the unification equations. Variable-value asso-
ciations referring to different elementary trees
are irrelevant when parsing a given tree, and
feature-value and feature-variable associations
are local to a node and can?t be extrapolated to
other nodes, so we won?t propagate any of this
information in items. However, it must be used
locally for unification. Therefore, steps perform
unification by using the information in their an-
tecedent items and recovering complete feature
structures associated to nodes directly from the
grammar, and then use the ?keep-variables? op-
eration to remove the information that we don?t
need in the consequent item.
? In some algorithms, such as CYK, a single de-
ductive step deals with several different elemen-
tary tree nodes that don?t collapse into one in the
final parse tree. In this case, several ?keep vari-
ables? operations must be performed on each
step execution, one for each of these nodes. If
we just unified the information on all the nodes
and called ?keep variables? at the end, we could
propagate information incorrectly.
? In Earley-type algorithms, we must take a de-
cision about how predictor steps handle fea-
ture structures. Two options are possible: one
is propagating the feature structure in the an-
tecedent item to the consequent, and the other is
discarding the feature structure and generating
a consequent whose associated feature structure
is empty. The first option has the advantage that
violations of unification constraints are detected
earlier, thus avoiding the generation of some
items. However, in scenarios where a predic-
tor is applied to several items differing only in
their associated feature structures, this approach
generates several different items while the dis-
carding approach collapses them into a single
consequent item. Moreover, the propagating
approach favors the appearance of items with
more complex feature structures, thus making
unification operations slower. In practice, for
XTAG we have found that these drawbacks of
propagating the structures overcome the advan-
tages, especially in complex sentences, where
the discarding approach performs much better.
2.3 Tree filtering
The full XTAG English grammar contains thou-
sands of elementary trees, so performance is not
good if we use the whole grammar to parse each
sentence. Tree selection filters (Schabes and Joshi,
1991) are used to select a subset of the grammar,
discarding the trees which are known not to be
useful given the words in the input sentence.
To emulate this functionality in our parsing
schema-based system, we have used its exten-
sibility mechanism to define a function Selects-
tree(a,T) that returns true if the terminal symbol a
selects the tree T. The implementation of this func-
tion is a Java method that looks for this informa-
tion in XTAG?s syntactic database. Then the func-
tion is inserted in a filtering step on our schemata:
106
[a, i, j]
[Selected, ?] alpha ? Trees/SELECTS-TREE(A;?)
The presence of an item of the form
[Selected, ?] indicates that the tree ? has
been selected by the filter and can be used for
parsing. In order for the filter to take effect, we
add [Selected, ?] as an antecedent to every step
in our schemata introducing a new tree ? into the
parse (such as initters, substitution and adjoining
steps). In this way we guarantee that no trees that
don?t pass the filter will be used for parsing.
3 Comparing several parsers for the
XTAG grammar
In this section, we make a comparison of several
different TAG parsing algorithms ? the CYK-
based algorithm described at (Vijay-Shanker
and Joshi, 1985), Earley-based algorithms with
(Alonso et al, 1999) and without (Schabes, 1994)
the valid prefix property (VPP), and Nederhof?s
algorithm (Nederhof, 1999) ? on the XTAG En-
glish grammar (release 2.24.2001), by using our
system and the ideas we have explained. The
schemata for these algorithms without unification
support can be found at (Alonso et al, 1999).
These schemata were extended as described in the
previous sections, and used as input to our sys-
tem which generated their corresponding parsers.
These parsers were then run on the test sentences
shown in table 2, obtaining the performance mea-
sures (in terms of runtime and amount of items
generated) that can be seen in table 3. Note that
the sentences are ordered by minimal runtime.
As we can see, the execution times are not as
good as the ones we would obtain if we used
Sarkar?s XTAG distribution parser written in C
(Sarkar, 2000). This is not surprising, since our
parsers have been generated by a generic tool
without knowledge of the grammar, while the
XTAG parser has been designed specifically for
optimal performance in this grammar and uses ad-
ditional information (such as tree usage frequency
data from several corpora, see (XTAG, 2001)).
However, our comparison allows us to draw
conclusions about which parsing algorithms are
better suited for the XTAG grammar. In terms
of memory usage, CYK is the clear winner, since
it clearly generates less items than the other al-
gorithms, and a CYK item doesn?t take up more
memory than an Earley item.
On the other hand, if we compare execution
times, there is not a single best algorithm, since the
performance results depend on the size and com-
plexity of the sentences. The Earley-based algo-
rithm with the VPP is the fastest for the first, ?eas-
ier? sentences, but CYK gives the best results for
the more complex sentences. In the middle of the
two, there are some sentences where the best per-
formance is achieved by the variant of Earley that
doesn?t verify the valid prefix property. Therefore,
in practical cases, we should take into account the
most likely kind of sentences that will be passed
to the parser in order to select the best algorithm.
Nederhof?s algorithm is always the one with the
slowest execution time, in spite of being an im-
provement of the VPP Earley parser that reduces
worst-case time complexity. This is probably be-
cause, when extending the Nederhof schema in
order to support feature structure unification, we
get a schema that needs more unification opera-
tions than Earley?s and has to use items that store
several feature structures. Nederhof?s algorithm
would probably perform better in relation to the
others if we had used the strategy of parsing with-
out feature structures and then performing unifica-
tion on the output parse forest.
4 Conclusions
A generic system that generates parsers from al-
gebraic specifications (parsing schemata) has been
applied to the particular case of the XTAG gram-
mar. In order to be able to generate XTAG parsers,
some transformations were made to the grammar,
and TAG parsing schemata were extended with
feature structure unification support and a simple
tree filtering mechanism.
The generated implementations allow us to
compare the performance of different TAG parsers
when working with a large-scale grammar, the
XTAG English grammar. In this paper, we have
shown the results for four algorithms: a CYK-
based algorithm, Earley-based algorithms with
and without the VPP, and Nederhof?s algorithm.
The result shows that the CYK-based parser is the
least memory-consuming algorithm. By measur-
ing execution time, we find that CYK is the fastest
algorithm for the most complex sentences, but the
Earley-based algorithm with the VPP is the fastest
for simpler cases. Therefore, when choosing a
parser for a practical application, we should take
107
1. He was a cow 9. He wanted to go to the city
2. He loved himself 10. That woman in the city contributed to this article
3. Go to your room 11. That people are not really amateurs at intelectual duelling
4. He is a real man 12. The index is intended to measure future economic performance
5. He was a real man 13. They expect him to cut costs throughout the organization
6. Who was at the door 14. He will continue to place a huge burden on the city workers
7. He loved all cows 15. He could have been simply being a jerk
8. He called up her 16. A few fast food outlets are giving it a try
Table 2: Test sentences.
Sentence Runtimes in milliseconds Items generatedParser Parser
CYK Ear. no VPP Ear. VPP Neder. CYK Ear. no VPP Ear. VPP Neder.
1 2985 750 750 2719 1341 1463 1162 1249
2 3109 1562 1219 6421 1834 2917 2183 2183
3 4078 1547 1406 6828 2149 2893 2298 2304
4 4266 1563 1407 4703 1864 1979 1534 2085
5 4234 1921 1421 4766 1855 1979 1534 2085
6 4485 1813 1562 7782 2581 3587 2734 2742
7 5469 2359 2344 11469 2658 3937 3311 3409
8 7828 4906 3563 15532 4128 8058 4711 4716
9 10047 4422 4016 18969 4931 6968 5259 5279
10 13641 6515 7172 31828 6087 8828 7734 8344
11 16500 7781 15235 56265 7246 12068 13221 13376
12 16875 17109 9985 39132 7123 10428 9810 10019
13 25859 12000 20828 63641 10408 12852 15417 15094
14 54578 35829 57422 178875 20760 31278 40248 47570
15 62157 113532 109062 133515 22115 37377 38824 59603
16 269187 3122860 3315359 68778 152430 173128
Table 3: Runtimes and amount of items generated by different XTAG parsers on several sentences. The
machine used for all the tests was an Intel Pentium 4 / 3.40 GHz, with 1 GB RAM and Sun Java Hotspot
virtual machine (version 1.4.2 01-b06) running on Windows XP. Best results for each sentence are shown
in boldface.
into account the kinds of sentences most likely to
be used as input in order to select the most suitable
algorithm.
References
M. A. Alonso, D. Cabrero, E. de la Clergerie, and M.
Vilares. 1999. Tabular algorithms for TAG parsing.
Proc. of EACL?99, pp. 150?157, Bergen, Norway.
S. Billot and B. Lang. 1989. The structure of shared
forest in ambiguous parsing. Proc. of ACL?89, pp.
143?151, Vancouver, Canada.
C. Go?mez-Rodr??guez, J. Vilares and M. A.
Alonso. 2006. Automatic Generation of
Natural Language Parsers from Declarative
Specifications. Proc. of STAIRS 2006, Riva
del Garda, Italy. Long version available at
http://www.grupocole.org/GomVilAlo2006a long.pdf
C. Go?mez-Rodr??guez, M. A. Alonso and M. Vilares.
2006. On Theoretical and Practical Complexity of
TAG Parsers. Proc. of Formal Grammars 2006,
Malaga, Spain.
M.-J. Nederhof. 1999. The computational complexity
of the correct-prefix property for TAGs. Computa-
tional Linguistics, 25(3):345?360.
A. Sarkar. 2000. Practical experiments in parsing us-
ing tree adjoining grammars. Proc. of TAG+5, Paris.
Y. Schabes and A. K. Joshi. 1991. Parsing with lexi-
calized tree adjoining grammar. In Masaru Tomita,
editor, Current Issues in Parsing Technologies, pp.
25?47. Kluwer Academic Publishers, Norwell.
Y. Schabes. 1994. Left to right parsing of lexical-
ized tree-adjoining grammars. Computational Intel-
ligence, 10(4):506?515.
S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24(1?2):3?36.
K. Sikkel. 1997. Parsing Schemata ? A Frame-
work for Specification and Analysis of Parsing Al-
gorithms. Springer-Verlag, Berlin.
K. Vijay-Shanker and A. K. Joshi. 1985. Some com-
putational properties of tree adjoining grammars.
Proc. of ACL?85, pp. 82?93, Chicago, USA.
XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for english. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
108
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 411?415,
Dublin, Ireland, August 23-24, 2014.
LyS: Porting a Twitter Sentiment Analysis Approach
from Spanish to English
David Vilares, Miguel Hermo, Miguel A. Alonso, Carlos Go?mez-Rodr??guez, Yerai Doval
Grupo LyS, Departamento de Computacio?n, Facultade de Informa?tica
Universidade da Corun?a, Campus de A Corun?a
15071 A Corun?a, Spain
{david.vilares, miguel.hermo, miguel.alonso, carlos.gomez, yerai.doval}@udc.es
Abstract
This paper proposes an approach to solve
message- and phrase-level polarity classi-
fication in Twitter, derived from an exist-
ing system designed for Spanish. As a
first step, an ad-hoc preprocessing is per-
formed. We then identify lexical, psycho-
logical and semantic features in order to
capture different dimensions of the human
language which are helpful to detect sen-
timent. These features are used to feed a
supervised classifier after applying an in-
formation gain filter, to discriminate irrel-
evant features. The system is evaluated on
the SemEval 2014 task 9: Sentiment Anal-
ysis in Twitter. Our approach worked com-
petitively both in message- and phrase-
level tasks. The results confirm the robust-
ness of the approach, which performed
well on different domains involving short
informal texts.
1 Introduction
Millions of opinions, conversations or just trivia
are published each day in Twitter by users of dif-
ferent cultures, countries and ages. This provides
an effective way to poll how people praise, com-
plain or discuss about virtually any topic. Compre-
hending and analysing all this information has be-
come a new challenge for organisations and com-
panies, which aim to find out a way to make quick
and more effective decisions for their business. In
particular, identifying the perception of the public
with respect to an event, a service or an entity are
some of their main goals in a short term. In this
respect, sentiment analysis, and more specifically
polarity classification, is playing an important role
This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
in order to automatically analyse subjective infor-
mation in texts.
This paper describes our participation at Sem-
Eval 2014 task 9: Sentiment Analysis in Twit-
ter. Specifically, two subtasks were presented:
(A) contextual polarity disambiguation and (B)
message polarity classification. The first sub-
task consists on determining the polarity of words
or phrases extracted from short informal texts,
the scope of extracts being provided by the Se-
mEval organisation. Subtask B focusses on clas-
sifying the content of the whole message. In
both cases, three possible sentiments are consid-
ered: positive, negative and neutral (which in-
volves mixed and non-opinionated instances). Al-
though the training set only contains tweets, the
test set also includes short informal texts from
other domains, in order to measure cross-domain
portability. You can test the model for subtask B
at miopia.grupolys.org.
2 SemEval 2014-Task 9: Sentiment
Analysis in Twitter
Our contribution is a reduced version of a Span-
ish sentiment classification system (Vilares et al.,
2013a; Vilares et al., 2013b) that participated in
TASS 2013 (Villena-Roma?n et al., 2014), achiev-
ing the 5th place on the global sentiment classifi-
cation task and the 1st place on topic classification
on tweets. In this section we describe how we have
ported to English this system originally designed
for Spanish. Tasks A and B are addressed from
the same perspective, which is described below.
2.1 Preprocessing
We implement a naive preprocessing algorithm
which seeks to normalise some of the most com-
mon ungrammatical elements. It is intended for
Twitter, but many of the issues addressed would
also be valid in other domains:
411
? Replacement of frequent abbreviations The
list of the most frequent ones was extracted
from the training set, taking the Penn Tree-
bank (Marcus et al., 1993) as our dictionary.
A term is considered ungrammatical if it does
not appear in our dictionary. We then carry
out a manual review to distinguish between
unknown words and abbreviations, providing
a correction in the latter case. For example,
?c?mon? becomes ?come on? and ?Sat? is re-
placed by ?Saturday?.
? Emoticon normalisation: We employ the
emoticon collection published in (Agarwal et
al., 2011). Each emoticon is replaced with
one of these five labels: strong positive (ESP),
positive (EP), neutral (ENEU), negative (EN)
or strong negative (ESN).
? Laughs : Multiple forms used in social media
to reflect laughs (e.g. ?hhahahha?, ?HHEHE-
HEH?) are preprocessed in a homogeneous
way to obtain a pattern of the form ?hxhx?
where x ? {a, e, i, o, u}.
? URL normalisation: External links are re-
placed by the string ?url?.
? Hashtags (?#?) and usernames (?@?): If the
hashtag appears at the end or beginning of
the tweet, we remove the hashtag. Based
on other participant approaches at SemEval
2013 (Nakov et al., 2013), we realized maybe
this is not the best option, although we be-
lieve hashtags will not be useful in most of
cases, since they refer to very specific events.
Otherwise, only the ?#? is removed, hypothe-
sising the hashtag is used to emphasise a term
(e.g. ?Matthew #Mcconaughey has won the
Oscar?).
2.2 Feature Extraction
Our approach only takes into account information
extracted from the text, without considering any
kind of meta-data. Extracted features combine
lexical, psychological and semantic knowledge in
order to build a linguistic model able to analyse
tweets, but also other kinds of messages. These
features can be divided into two types: corpus-
extracted features and lexicon-extracted features.
All of them take the total number of occurrences
of the respective feature as the weighting factor to
then feed the supervised classifier.
2.2.1 Corpus-extracted features
Given a corpus, we use it to extract the following
set of features:
? Word forms: A model based on this type of
features is our baseline. Each single word is
considered as a feature in order to feed the
supervised classifier. This often becomes a
simple and acceptable start point which ob-
tains a decent performance.
? Part-of-speech (PoS) information: some
coarse-grained PoS-tags such as adjective or
adverb are usually good indicators of subjec-
tive texts while some fine-grained PoS tags
such as third person personal pronoun pro-
vide evidence of non-opinionated messages
(Pak and Paroubek, 2010).
2.2.2 Lexicon-extracted features
We also consider information obtained from exter-
nal lexicons in order to capture linguistic informa-
tion that can not be extracted from a training cor-
pus by means of bag-of-words and PoS-tag mod-
els. We rely on two manually-build lexicons:
? Pennebaker et al. (2001) psychometric dictio-
naries. Linguistic Inquiry and Word Count1
(LIWC) is a software which includes a seman-
tic dictionary to measure how people use dif-
ferent kinds of words over a wide number of
texts. It categorises terms into psychometric
properties, which correspond to different di-
mensions of the human language. The dictio-
nary relates terms with psychological prop-
erties (e.g. anger or anxiety), but also with
topics (e.g. family, friends, religion) or even
morphological features (e.g. future time, past
time or exclamations).
? Hu and Liu (2004) opinion lexicon. It is a col-
lection of positive and negative words. Many
of the occurrences are misspelled, since they
often come from web environments.
2.2.3 Syntactic features
We also parsed the tweets using MaltParser (Nivre
et al., 2007) in order to obtain dependency triplets
of the form (w
i
, arc
ij
, w
j
), where w
i
is the head
word w
j
, the dependent one and arc
ij
the exist-
ing syntactic relation between them. We tried to
incorporate generalised dependency triplets (Joshi
1http://www.liwc.net/
412
and Penstein-Rose?, 2009), following an enriched
perspective presented in Vilares et al. (2014). A
generalisation consists on backing off the words
to more abstracted terms. For example, a valid de-
pendency triplet for the phrase ?awesome villain?
is (villain, modifier, awesome), which could be
generalised into (anger, modifier, assent) by means
of psychometric properties. However, experimen-
tal results over the development corpus using these
features decreased performance with respect to
our best model, probably due to the small size of
the training corpus, since dependency triplets tend
to suffer from sparsity, so a larger training corpus
is needed to exploit them in a proper way (Vilares
et al., 2014).
2.3 Feature Selection
For a machine learning approach, sparsity could
be an issue. In particular, due to the size of the cor-
pus, many of the terms extracted from the training
set only appear a few times in it. This makes it
impossible to properly learn the polarity of many
tokens. Thus, we carry out a filtering step before
feeding our classifier. In particular, we rely on
the information gain (IG) method to then rank the
most relevant features. Information gain measures
the relevance of an attribute with respect to a class.
It takes values between 0 and 1, where a higher
value implies a higher relevance. Table 1 shows
the top five relevant features based on their infor-
mation gain for our best model. The top features
for task A were very similar. Our official runs only
consider features with an IG greater than zero.
IG Feature Category
0.140 positive emotion Pennebaker et al. (2001)
0.137 #positive-words Hu and Liu (2004)
0.126 affect Pennebaker et al. (2001)
0.089 #negative-words Hu and Liu (2004)
0.083 negative emotion Pennebaker et al. (2001)
Table 1: Most relevant features for task B. ?#? must
be read this table as ?the number of?and not as a
hashtag.
2.4 Classifier
We have trained our runs with a SVMLibLINEAR
classifier (Fan et al., 2008) taking the implementa-
tion provided in WEKA (Hall et al., 2009). The
selection was motivated by the acceptable results
that some of the participants in SemEval 2013, e.g.
Becker et al. (2013), obtained using this imple-
mentation. We configured the multi-class support
vector machine by Crammer and Singer (2002) as
the SVMtype. Since the corpus was unbalanced,
we tuned the weights for the classes using the de-
velopment corpus: 1 for the positive class, 2 for
negative and 0.5 for neutral. The rest of parame-
ters were set to default values.
3 Experimental Results
The SemEval 2014 organisation provides a stan-
dard training corpus for both tasks A an B. For task
A, each tweet is marked with a list of the words
and phrases to analyse, and for each one its senti-
ment label is provided. In addition, a development
corpus was released for tuning the system parame-
ters. The training and the development corpus can
be used jointly (constrained runs) to train mod-
els that are then evaluated over the test corpus.2
Some participants used external annotated corpora
(unconstrained runs) to build their models. With
respect to the test corpus, it contains texts from
tweets but also from LiveJournal texts, which we
are abbreviating as LJ, and SMS messages.
Table 2 contains the statistics of the corpora we
used. Sharing data is a violation of Twitter?s terms
of service, so we had to download them. Unfortu-
nately, some of the tweets were no longer available
for several reasons, e.g., user or a tweet does not
exist anymore or the privacy settings of a user have
changed. As a result, the size of our training and
development corpora may be different from those
of other participant?s corpora.
Task Set Positive Negative Neutral
Train 4,917 2,591 385
A Dev 555 365 45
Test 6,354 3,771 556
Train 3,063 1,202 3,935
B Dev 493 290 633
Test 3,506 1,541 3,940
Table 2: SemEval 2014 corpus statistics.
3.1 Evaluation Metrics
F-measure is the official score to measure how sys-
tems behave on each class. In order to rank partic-
ipants, the SemEval 2014 organisation proposed
the averaged F-measure of positive and negative
tweets.
2We followed this angle.
413
3.2 Performance on Sets
Tables 3 and 4 show performance on the test set
of different combinations of the proposed features.
Table 5 shows the performance of our run on task
A. The results over the corresponding sets for task
B are illustrated in Table 6. They are significant
lower than in task A. This suggests that when a
message involves more than one of two tokens, a
lexical approach is not enough. Improving perfor-
mance should involve taking into account context
and linguistic phenomena that appear in sentences
to build a model based on the composition of lin-
guistic information.
Model LJ SMS Twitter Twitter Twitter2013 2014 Sarcasm
WPLT 82.21 82.32 84.82 81.69 71.19(no IG)
WPL 83.55 81.04 84.85 80.64 68.79
WPLT* 83.96 81.46 85.63 79.93 71.98
WP 78.53 80.97 80.34 73.35 74.18
P 75.70 78.74 73.58 65.75 71.82
W 61.58 65.45 64.56 59.16 62.93
L 66.04 64.11 62.96 53.81 61.26
T 47.07 51.37 71.82 43.64 49.37
Table 3: Performance on the test set for task A.
The model marked with a * was our official run. W
stands for features obtained from a bag-of-words
approach, L from Hu and Liu (2004), P from Pen-
nebaker et al. (2001) and T for fine-grained PoS-
tags. They can be combined, e.g., a model named
WP use both words and psychometric properties.
Model LJ SMS Twitter Twitter Twitter2013 2014 Sarcasm
WPLT* 69.79 60.45 66.92 64.92 42.40
WPL 70.19 61.41 66.71 64.51 45.72
WP 66.84 60.22 65.29 63.90 45.90
WPLT 66.38 57.01 61.96 62.84 43.71(no IG)
W 65.12 56.00 62.87 62.64 48.75
P 63.42 54.80 60.05 57.66 54.20
T 45.99 35.85 46.53 45.99 48.58
L 57.53 45.14 48.80 44.48 49.14
Table 4: Performance on the test set for task B.
4 Conclusions
This papers describes the participation of the LyS
Research Group (http://www.grupolys.
org) at the SemEval 2014 task 9: Sentiment Anal-
ysis in Twitter, with a system that attained com-
petitive performance both in message and phrase-
Test set Positive Negative Neutral
DEV 86.30 81.60 4.30
TWITTER 2013 88.70 81.90 17.60(full)
TWITTER 2013 88.81 82.57 20.75(progress subset)
LJ 84.34 83.56 13.84
SMS 80.31 82.56 7.10
TWITTER 2014 89.02 70.82 4.44
TWITTER SARCASM 85.71 57.63 28.57
Table 5: Performance on different sets for our
model on task A. The model evaluated on the de-
velopment set was only built using the training set.
Test set Positive Negative Neutral
DEV 69.80 60.40 66.70
TWITTER 2013 72.50 64.30 72.30(full)
TWITTER 2013 71.92 61.92 71.22(progress subset)
LJ 71.94 67.65 66.23
SMS 63.83 57.06 73.76
TWITTER 2014 74.26 55.58 66.76
TWITTER SARCASM 55.17 29.63 51.61
Table 6: Performance on different sets for our
model on task B.
Test set Task A Task B
LiveJournal 2014 4 / 27 13 / 50
SMS 2013 12 / 27 19 / 50
Twitter 2013 9 / 27 10 / 50
Twitter 2014 11 / 27 18 / 50
Twitter 2014 Sarcasm 10 / 27 33 / 50
Table 7: Position of our submission on each cor-
pus and task, according to results provided by the
organization on April 22, 2014.
level tasks, as can be observed in Table 7. This
system is a reduced version of a sentiment classifi-
cation model for Spanish texts that performed well
in the TASS 2013 (Villena et al., 2013). The offi-
cial results show how our approach works com-
petitively both on tasks A and B without needing
large and automatically-built resources. The ap-
proach is based on a bag-of-words that includes
word-forms and PoS-tags. We also extract psy-
chometric and sentiment information from exter-
nal lexicons. In order to reduce sparsity problems,
we firstly apply an information gain filter to select
only the most relevant features. Experiments on
the development set showed a significant improve-
ment on the same model with respect to skipping
it on subtask B.
414
Acknowledgements
Research reported in this paper has been partially
funded by Ministerio de Econom??a y Competitivi-
dad and FEDER (Grant TIN2010-18552-C03-02)
and by Xunta de Galicia (Grant CN2012/008).
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of Twitter data. In Proceedings of the Work-
shop on Languages in SocialMedia, LSM ?11, pages
30?38, Stroudsburg, PA, USA. ACL.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. AVAYA: Sentiment Analysis on
Twitter with Self-Training and Polarity Lexicon Ex-
pansion. Atlanta, Georgia, USA, page 333.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265?292.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. The Journal
of Machine Learning Research, 9:1871?1874.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18, November.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Manesh Joshi and Carolyn Penstein-Rose?. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, ACLShort ?09, pages 313?316,
Suntec, Singapore.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. pages 312?320, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
A. Pak and P. Paroubek. 2010. Twitter as a Corpus for
Sentiment Analysis and Opinion Mining. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10),
pages 1320?1326, Valletta, Malta, May. European
Language Resources Association (ELRA).
J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001.
Linguistic inquiry and word count: LIWC 2001.
Mahway: Lawrence Erlbaum Associates, 71.
David Vilares, Miguel A. Alonso, and Carlos Go?mez-
Rodr??guez. 2013a. LyS at TASS 2013: Analysing
Spanish tweets by means of dependency pars-
ing, semantic-oriented lexicons and psychometric
word-properties. In Alberto D??az Esteban, In?aki
Alegr??a Loinaz, and Julio Villena Roma?n, editors,
XXIX Congreso de la Sociedad Espan?ola de Proce-
samiento de Lenguaje Natural (SEPLN 2013). TASS
2013 - Workshop on Sentiment Analysis at SEPLN
2013, pages 179?186, Madrid, Spain, September.
David Vilares, Miguel A. Alonso, and Carlos Go?mez-
Rodr??guez. 2013b. Supervised polarity classifica-
tion of Spanish tweets based on linguistic knowl-
edge. In DocEng?13. Proceedings of the 13th ACM
Symposium on Document Engineering, pages 169?
172, Florence, Italy, September. ACM.
David Vilares, Miguel A. Alonso, and Carlos Go?mez-
Rodr??guez. 2014. On the usefulness of lexical
and syntactic processing in polarity classification of
Twitter messages. Journal of the Association for In-
formation Science Science and Technology, to ap-
pear.
Julio Villena-Roma?n, Janine Garc??a-Morera, Cristina
Moreno-Garc??a, Sara Lana-Serrano, and Jose? Carlos
Gonza?lez-Cristo?bal. 2014. TASS 2013 ? a sec-
ond step in reputation analysis in Spanish. Proce-
samiento del Lenguaje Natural, 52:37?44, March.
415
