Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 1?7,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Multimodal Human-Machine Interaction
for Service Robots in Home-Care Environments
Stefan Goetze1, Sven Fischer1, Niko Moritz1, Jens-E. Appell1, Frank Wallhoff 1,2
1Fraunhofer Institute for Digital Media Technology (IDMT), Project group
Hearing, Speech and Audio Technology (HSA), 26129 Oldenburg, Germany
2Jade University of Applied Sciences, 26129 Oldenburg, Germany
{s.goetze,sven.fischer,niko.moritz,jens.appell,frank.wallhoff}@idmt.fraunhofer.de
Abstract
This contribution focuses on multimodal in-
teraction techniques for a mobile communi-
cation and assistance system on a robot plat-
form. The system comprises of acoustic, vi-
sual and haptic input modalities. Feedback is
given to the user by a graphical user interface
and a speech synthesis system. By this, mul-
timodal and natural communication with the
robot system is possible.
1 Introduction
The amount of older people in modern societies
constantly grows due to demographic changes (Eu-
ropean Commision Staff, 2007; Statistical Federal
Office of Germany, 2008). These people desire
to stay in their own homes as long as possible,
however suffer from first health problems, such as
decreased physical strength, cognitive decline (Pe-
tersen, 2004), visual and hearing impairments (Rud-
berg et al, 1993; Uimonen et al, 1999; Goetze et
al., 2010b). This poses great challenges to the care
systems since care services require a high amount of
temporal and personnel efforts. Furthermore, older
people living alone may suffer from social isolation
since family members, friends and acquaintances
may live at distant places and frequent face-to-face
communication may be hard to realize.
It is nowadays commonly accepted that support
by means of technical systems in the care sector
will be inevitable in the future to cope with these
challenges (Alliance, 2009). Examples for such as-
sistive devices are reminder systems (Boll et al,
2010), medical assistance and tele-healthcare sys-
tems (Lisetti et al, 2003), personal emergency re-
sponse systems, accessible human-machine interac-
tion (Rennies et al, 2011) or social robotics (Chew
et al, 2010).
This contribution describes the human-machine
interaction modalities for a social robot called
ALIAS (adaptable ambient living assistant) that is
depicted in Figure 1. ALIAS is a mobile robot plat-
form to foster communication and social interaction
between the user and his/her social network as well
as between the user and the robot platform. The aim
of ALIAS is to ensure the maintenance of existing
contacts to prevent social isolation instead of making
human-to-human communication obsolete. ALIAS
is supposed to act as a companion that encourages
its owner to cultivate relationships and contacts to
the real world.
Figure 1: ALIAS robot platform.
Instead of classical interaction techniques solely
1
by using mouse and keybord, multi-modal human-
machine interaction techniques allow for more natu-
ral and convenient human-machine interaction (Ovi-
att, 1999; Jaimes and Sebe, 2007; Goetze et al,
2010a). Especially for technology in the domain of
ambient assisted living (AAL) which is mostly in-
tended to be used by older users - these users often
are less technophile than younger users (Meis et al,
2007) - multi-modal interaction strategies including
modalities like speech and touch pads show high ac-
ceptance (Boll et al, 2010).
A touch display and a robust speech recognition
and synthesis system enable the ALIAS robot plat-
form to interact with the user via speech or using
the mounted touch display (cf. Figure 1). Besides
communication with the robot by speech input and
output, communication with relatives and acquain-
tances via telephone channels, mobile phone chan-
nels and the internet is a central goal. An automatic
reminder system motivating the user to participate
actively in social interaction is developed. In ad-
dition, the user is encouraged to perform cognitive
activities in order to preserve quality of life.
The following Section 2 briefly describes the sys-
tem components of the ALIAS robot platform be-
fore Section 3 focuses on the multi-modal user-
interaction strategies.
2 System Components and Applied
Technologies
The ALIAS robot system has a variety of human-
machine communication features and sensors. Fig-
ure 2 shows the general overview of the robots soft-
ware modules which will be briefly introduced in the
following.
The dialogue manager (DM) is the robot?s most
central component since it is the software module
which is responsible for all decisions the robot has to
take. Therefore, it is connected to almost every other
module. The DM collects inputs and events from all
these modules, interprets them, and decides which
actions to perform, i.e. commands to send to which
modules. It may move the robot to check on its user,
initiate a video telephone call, or ask for a game of
chess. The dialogue manager runs on the Windows
computer, which is one of the two computer systems
in the ALIAS system.
Figure 2: Overview of the ALIAS robot?s software mod-
ules, distributed on two computers.
The graphical user interface (GUI) has a close
link to the dialogue manager since it integrates sev-
eral applications and receives user inputs of the Win-
dows computer?s operating system. Thus, it reacts
to touch input and displays menus and all software
modules with graphical output. (Section 3.1 pro-
vides more detailed information on the GUI.)
The automatic speech recognition (ASR) mod-
ule enables the robot to understand and react on spo-
ken commands (Moritz et al, 2011). It receives
recorded audio signals from the Jack audio-server
and converts it to a textual representation of spoken
words. This list of recognized words will be sent
to the DM for interpretation (cf. Section 3.2 for de-
tails).
The speech synthesis module enables the robot to
communicate with its owner verbally (together with
the ASR sytem). Speech synthesis (Taylor, 2009)
is the artificial production of human voice. Text-
to-speech (TTS) systems are used to convert writ-
ten text into speech. An advanced system should be
able to take any arbitrary text input and convert it
into speech, whereby the language of the text must
be known to be able to create the correct pronunci-
ation. Several systems for speech synthesis are al-
ready commercially available to realize such a sys-
tem. Speech output was found to be a desired user
interaction strategy for assistive systems (Goetze et
al., 2010a) if output phrases are properly designed
2
since there?s no need to reach out for the robot?s dis-
play unit in order to interact with it.
A link to the world-wide web is established by
integration of an easy-to-use web browser which
is seamlessly integrated into the GUI. To counter-
act isolation an event search web service was real-
ized (Khrouf and Troncy., 2011) that visualizes var-
ious events and corresponding pictures to the user
that have taken place or will take place close to the
user?s location. To achieve this the robot connects
to an online event search service. The service will
provide him/her with a personalized selection of so-
cial event near his/her current location and personal
preferences.
An input modality suitable for users that are
unable to touch the robot?s screen or to verbal-
ize a speech command (e.g. after a stroke) is the
brain computer interface (BCI) of the robot (Hin-
termu?ller et al, 2011). It uses a set of electrodes
placed on the user?s skull to measure electrical re-
sponses of the brain. These electrical potentials are
evoked by means of visual stimuli, e.g. flashing im-
ages on a control display. By focusing on certain
items on the BCI control display the user?s brain ac-
tivity can control the GUI of the robot. The BCI may
also be used for writing text messages which can be
sent using the integrated Skype? chat functionality
of the robot.
To distinguish between its owner and other per-
sons, the robot uses an acoustic speaker recogni-
tion module. This provides ALIAS with additional
information which can be used to differentiate be-
tween persons and interprete multiple speech inputs
according to their individual context.
In order to achieve more human-like character-
istics, the robot uses a face identification module.
So it is able to adjust its eyes to face the person it?s
talking to. The face detection algorithm utilizes the
robot?s 360? panorama camera located on top of the
robot?s head, and thus covers the robots surround-
ings, completely.
The navigation module handles the actual move-
ment, collision prevention, and odometry of the
robot. It drives ALIAS by plotting waypoints on
a pre-recorded map. Obstacles are detected using
ultra-sonic sensors, the laser scanner, and the front
camera. In case the robot?s path is blocked, the nav-
igation module will plot an alternative route in order
to reach the designated target location, evading the
obstacle (Kessler et al, 2011). The navigation mod-
ule may also be remotely controlled by another per-
son in order to check on the robot?s owner in case
an accident has been detected or the owner has re-
quested for help.
3 Multimodal Interaction Strategies
The robot?s user interface features different input
modalities; speech commands, the BCI, or the touch
screen (GUI). For speech input, the ASR mod-
ule processes the recorded speech commands and
translates them into multiple textual representations,
which are then sent to the DM for interpretation.
BCI and GUI include a display unit to provide
feedback to the user. Thus they require an addi-
tional pathway for receiving commands from the
DM. In case of the BCI, available items on its con-
trol screen may be switched by the DM to reflect the
current dialogue state, i.e. a selection of audio books
if the audio book module has been accessed. For the
GUI, which integrates several software applications
into one single module, there is also the possibility
of non-user related events, such as incoming phone
calls from the integrated Skype module. The GUI
has to relay these events to the DM for decision.
All user inputs and relevant system events are
gathered by the DM. As the ALIAS system?s cen-
tral control unit, the DM keeps track of all active
robot modules and relevant sensor data. It merges
all provided inputs, puts them into context, inter-
prets them, and decides which actions to perform.
Whereas some inputs may be redundant, others may
be invalid or highly dependent on the context.
For example, pushing a button on the touch screen
is most likely related to the application that is run-
ning on the screen. Whereas the spoken phrase ?on
the right? could mean that the user wants ALIAS to
push a button that is located on the right hand side of
its screen. Another interpretation would be that the
user wants the robot to turn to the right and move
aside. Or the user was talking to another person in
the room, possibly even on ALIAS? video telephone,
and the spoken statement is not to concern the robot
at all.
This section provides a closer look on the ALIAS
robot?s most frequently used user interfaces and
3
their design.
3.1 Graphical User Interface
The GUI consists of a series of menus containing a
few large buttons, each of them leading to another
menu or starting an application, i.e. an integrated
software module. The GUI?s main menu is shown
in Figure 3.
The GUI uses a minimalistic design, including
some light gradients and blur for non-essential back-
ground components. Whereas the actual buttons fea-
ture comprehensive icons and text labels with large
fonts, enclosed by high-contrast black frames. This
eases distinction between buttons and background.
Taking visual impairments into account the GUI
remains usable, while still being visually pleasing
for people with unimpaired vision. Due to each
users individual color perception, colors are used
sparsely and mustn?t be the sole cue to carry es-
sential information. Instead combinations of colors,
shapes, and labels are preferred.
Figure 3: ALIAS robot?s main menu.
The GUI depends on animations; buttons flash in
a different (dark) colors when pressed and menus
sliding on and off the screen when switched. Such
animations provide visual feedback to user inputs
and are unlikely to be missed by the user, since they
involve the whole screen, usually.
The GUI makes a clear distinction between menus
and application modules, though both are supposed
to look quite similar on the screen. Menus provide
access to sub-menus and integrated software mod-
ules i.e. applications, using a tree-like menu struc-
ture which is defined by a configuration file.
Application modules implement their very own
individual layouts, buttons, features, and remote-
control capabilities for the DM. By this, some fea-
tures are available after the related application has
been started, only. The GUI features a selection
of integrated application modules, like a Skype?-
based video telephone, a web-browser, a television
module, an audio book player, a selection of serious
games, and access to the robot?s Wii gaming con-
sole.
The GUI processes two kinds of user inputs; di-
rect inputs and indirect inputs. Both input types will
be further outlined below.
3.1.1 Direct Inputs
The GUI accepts normal user inputs, as they are
provided by the host computer?s operating system.
In case of the ALIAS robot the main source of such
inputs will be the touch screen. These inputs are
considered as direct inputs, since they are provided
by the computer?s operating system and are handled
by the GUI directly.
More generally every input falls into the group
of direct inputs if the GUI is directly receiving it.
Accordingly even an incoming phone call is a di-
rect input, because it is triggered by an integrated
GUI module. So, unless properly handled and prop-
agated, no other module would ever know about it.
Thus, most direct inputs also need to be relayed to
the dialogue manager that takes over the role of a
state machine to keep all modules on the robot syn-
chronized. If, for example, any input in the current
situation is not allowed or even undesirable the DM
can intervene and reject those inputs.
3.1.2 Indirect Inputs
A second kind of user inputs is represented by the
group of indirect inputs. Indirect inputs are system
messages, received by the GUI. Basically indirect
inputs are inputs that are handled by another mod-
ule, but require a reaction by the GUI. Typically such
indirect inputs are generated by the Dialogue Man-
ager, as response to a speech input for example.
The user may issue a verbal command to the
robot: ?Call Britta, please!? The sound wave is
picked up by the robot?s microphones, converted
4
into a sampled audio signal that is redirected by the
Jack Audio Server to the speech recognition mod-
ule. The speech recognition module converts the
audio signal to a textual representation that will be
interpreted and processed by the dialogue manager.
In case the dialogue manager finds a contact named
?Britta? in its data base, it sends a series of network
messages to the GUI, containing the required com-
mands to bring up the telephone application and ini-
tiate the phone call.
3.1.3 Multi-modal Input
Most parts of the GUI can be controlled by touch
display as well as by spoken commands. Further-
more, a control by the BCI is possible for parts of the
GUI (currently Skype chat and entertainment such
as audio books).
Figure 4: Multi-modal input dialog for appointments.
An example for a multi-modal interaction is the
appointment input window depicted in Figure 4. It
contains information about the category, the title, the
start and end time of the appointment and a possibil-
ity to set a reminder. The interface can be controlled
by mouse and keyboard as well as via speech com-
mands following a structured dialogue. By this, the
used is free to chose if he/she wants to use mouse
and keyboard as a fast way to enter an appointment
or speech if he/she is not close enough to the robot?s
touch display and is either not willing or not capable
to reach it.
3.2 Speech Recognition
Creating an automatic speech recognition (ASR) de-
vice requires different processing steps. Figure 5 il-
lustrates exemplary the structure of such a system.
Figure 5: Schematic technical design of the ASR system.
A very important step is to collect a sufficiently
large amount of speech data from many different
speakers. This speech data is used to train the acous-
tic models, which in this case are hidden Markov
models (HMM), and described in terms of well-
known Mel frequency cepstral coefficients (MFCCs)
(Benesty et al, 2008). Besides the HMM models
of known words also so-called garbage models are
trained, since the ASR device needs to be capable to
distinguish not only between words that were trained
from the training utterances but also between known
and out-of-vocabulary (OOV) words.
In addition to the acoustic models a proper speech
recognition system also needs a language model.
The language model provides grammatical infor-
mation about the utterances that are presented by
the subjects to the ASR system. Language models
can be separated into groups of statistical and non-
statistical models. The ALIAS ASR system com-
prises of two recognition systems that are running at
5
different grammatical rules (cf. Figure 6). The first
ASR system uses a non-statistical language model
that is typically used for ASR systems with small
vocabulary size and very strict input expectations.
This ASR system can be considered as a keyword
spotter. In contrast, N-gram models can also be used
for continuous speech recognition systems, where
the grammatical information can get a lot more com-
plex. Thus, the second recognizer uses statistical
grammar rules (N-gram) which consists of a 2-gram
forward and 3-gram backward model and enables
the system to make a more soft decision on the rec-
ognized sentence.
By this two-way approach, the keyword spotting
system can do a reliable search for important catch-
words, whereby the second recognizer tries to under-
stand more context from the spoken sentence. This
ensures an even broader heuristic processing for the
DM.
LVASR
keyword
spotter
system
dialog
manager
action
GUI
Figure 6: Two-way ASR system.
With the acoustic models and a valid language
model the speech recognition device is now able to
operate. The user utters any command, which is
picked up by a microphone. Since in real-world sce-
narios the microphones do not only pick up the de-
sired speech content but also disturbances like ambi-
ent noise or sounds produced by the (moving) robot
system itself, the microphone signal has to be en-
hanced by appropriate signal processing schemes
(Ha?nsler and Schmidt, 2004; Goetze et al, 2010a;
Cauchi et al, 2012) before ASR features (MFCCs)
are extracted from the speech input.A The extracted
features are then transferred to the decoding system
where the content of speech is analyzed.
ASR processing deals in terms of probabilities.
Although speech recognition has been identified as
a highly desired input modality for assistive systems
(Goetze et al, 2010a) the acceptance drastically de-
creases if the recognition rate is not sufficiently high.
For every acoustic input there are multiple recog-
nition alternatives, with varying probabilities. In-
stead of using only the most probable recognition
for output, the ASR module provides the DM with
a few additional alternatives. This allows the DM a
more thorough analysis and thus a more precise in-
terpretation of the provided speech input to decide
for an output on the GUI or an action (e.g. moving
the roboter).
4 Conclusion
This paper presented multimodal interaction strate-
gies for an robot assistant which has its main fo-
cus on support of communication. This includes
both, fostering of human-to-human communication
by providing communication capabilities over dif-
ferent channels and reminding on neglected relation-
ships as well as communication between the techni-
cal system and its user by means of speech recogni-
tion and speech output.
Acknowledgments
This work was partially supported by the project
AAL-2009-2-049 ?Adaptable Ambient Living As-
sistant? (ALIAS) co-funded by the European Com-
mission and the Federal Ministry of Education and
Research (BMBF) in the Ambient Assisted Living
(AAL) program and the by the project Design of En-
vironments for Ageing (GAL) funded by the Lower
Saxony Ministry of Science and Culture through the
Niederschsisches Vorab grant programme (grant ZN
2701).
References
The European Ambient Assisted Living Innovation Al-
liance. 2009. Ambient Assisted Living Roadmap.
VDI/VDE-IT AALIANCE Office.
J. Benesty, M.M. Sondhi, and Y. Huang. 2008. Springer
handbook of speech recognition. Springer, New York.
S. Boll, W. Heuten, E.M. Meyer, and M. , Meis. 2010.
Development of a Multimodal Reminder System for
Older Persons in their Residential Home. Informatics
for Health and Social Care, SI Ageing & Technology,
35(4), December.
6
B. Cauchi, S. Goetze, and S. Doclo. 2012. Reduction of
Non-stationary Noise for a Robotic Living Assistant
using Sparse Non-negative Matrix Factorization. In
Proc. Speech and Multimodal Interaction in Assistive
Environments (SMIAE 2012), Jeju Island, Republic of
Korea, Jul.
Selene Chew, Willie Tay, Danielle Smit, and Christoph
Bartneck. 2010. Do social robots walk or roll? In
Shuzhi Ge, Haizhou Li, John-John Cabibihan, and
Yeow Tan, editors, Social Robotics, volume 6414 of
Lecture Notes in Computer Science, pages 355?361.
Springer Berlin / Heidelberg.
European Commision Staff. 2007. Working Document.
Europes Demografic Future: Facts and Figures. Tech-
nical report, Commission of the European Communi-
ties, May.
S. Goetze, N. Moritz, J.-E. Appell, M. Meis, C. Bartsch,
and J. Bitzer. 2010a. Acoustic User Interfaces for
Ambient Assisted Living Technologies. Informatics
for Health and Social Care, SI Ageing & Technology,
35(4):161?179, December.
S. Goetze, F. Xiong, J. Rennies, T. Rohdenburg, and J.-
E. Appell. 2010b. Hands-Free Telecommunication
for Elderly Persons Suffering from Hearing Deficien-
cies. In 12th IEEE International Conference on E-
Health Networking, Application and Services (Health-
com?10), Lyon, France, July.
S. Goetze, J. Schro?der, S. Gerlach, D. Hollosi, J.-E. Ap-
pell, and F. Wallhoff. 2012. Acoustic Monitoring and
Localization for Social Care. Journal of Computing
Science and Engineering (JCSE), SI on uHealthcare,
6(1):40?50, March.
E. Ha?nsler and G. Schmidt. 2004. Acoustic Echo and
Noise Control: a Practical Approach. Wiley, Hobo-
ken.
C. Hintermu?ller, C. Guger, and G. Edlinger. 2011. Brain-
computer interface: Generic control interface for so-
cial interaction applications.
A. Jaimes and N. Sebe. 2007. Multimodal human-
computer interaction: A survey. Comput. Vis. Image
Underst., 108(1-2):116?134, October.
J. Kessler, A. Scheidig, and H.-M. Gross. 2011. Ap-
proaching a person in a socially acceptable manner us-
ing expanding random trees. In Proceedings of the 5th
European Conference on Mobile Robots, pages 95?
100, Orebro, Sweden.
H. Khrouf and R. Troncy. 2011. Eventmedia: Visual-
izing events and associated media. In Demo Session
at the 10th International Semantic Web Conference
(ISWC?2011), Bonn, Germany, Oct.
C. Lisetti, F. Nasoz, C. LeRouge, O. Ozyer, and K. Al-
varez. 2003. Developing multimodal intelligent affec-
tive interfaces for tele-home health care. International
Journal of Human-Computer Studies, 59(1-2):245 ?
255. Applications of Affective Computing in Human-
Computer Interaction.
M. Meis, J.-E. Appell, V. Hohmann, N v. Son,
H. Frowein, A.M. ?Oster, and A. Hein. 2007. Tele-
monitoring and Assistant System for People with
Hearing Deficiencies: First Results from a User Re-
quirement Study. In Proceedings of European Confer-
ence on eHealth (ECEH), pages 163?175.
N. Moritz, S. Goetze, and J.-E. Appell. 2011. Ambi-
ent Voice Control for a Personal Activity and House-
hold Assistant. In R. Wichert and B. Eberhardt, ed-
itors, Ambient Assisted Living - Advanced Technolo-
gies and Societal Change, Springer Lecture Notes in
Computer Science (LNCS), number 978-3-642-18166-
5, pages 63?74. Springer Science, January.
S.T. Oviatt. 1999. Ten myths of multimodal interaction.
Communications of the ACM. ACM New York, USA,
42(11):74?81, Nov.
R.C. Petersen. 2004. Mild Cognitive Impairment as
a Diagnostic Entity. Journal of Internal Medicine,
256:183?194.
J. Rennies, S. Goetze, and J.-E. Appell. 2011. Consid-
ering Hearing Deficiencies in Human-Computer Inter-
action. In M. Ziefle and C.Ro?cker, editors, Human-
Centered Design of E-Health Technologies: Concepts,
Methods and Applications, chapter 8, pages 180?207.
IGI Global. In press.
M.A. Rudberg, S.E. Furner, J.E. Dunn, and C.K. Cassel.
1993. The Relationship of Visual and Hearing Im-
pairments to Disability: An Analysis Using the Lon-
gitudinal Study of Aging. Journal of Gerontology,
48(6):M261?M265.
Statistical Federal Office of Germany. 2008. Demo-
graphic Changes in Germany: Impacts on Hospital
Treatments and People in Need of Care (In German
language: Demografischer Wandel in Deutschland -
Heft 2 - Auswirkungen auf Krankenhausbehandlungen
und Pflegebedu?rftige im Bund und in den La?ndern).
Technical report.
P. Taylor. 2009. Text-to-Speech Synthesis. Cambridge
University Press.
S. Uimonen, Huttunen K., K. Jounio-Ervasti, and
M. Sorri. 1999. Do We Know the Real Need for Hear-
ing Rehabilitation at the Population Level? Hearing
Impairments in the 5- to 75-Year Old Cross-Sectional
Finnish Population. British J. Audiology, 33:53?59.
7
Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 28?33,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Reduction of Non-stationary Noise for a Robotic Living Assistant using
Sparse Non-negative Matrix Factorization
Benjamin Cauchi 1, Stefan Goetze 1, Simon Doclo 1,2
1Fraunhofer Institute for Digital Media Technology (IDMT), Project group
Hearing, Speech and Audio Technology (HSA), 26129 Oldenburg, Germany
2University of Oldenburg, Signal Processing group, 26129 Oldenburg, Germany
{benjamin.cauchi,s.goetze,simon.doclo}@idmt.fraunhofer.de
Abstract
Due to the demographic changes, support by
means of assistive systems will become in-
evitable for home care and in nursing homes.
Robot systems are promising solutions but
their value has to be acknowledged by the pa-
tients and the care personnel. Natural and in-
tuitive human-machine interfaces are an es-
sential feature to achieve acceptance of the
users. Therefore, automatic speech recogni-
tion (ASR) is a promising modality for such
assistive devices. However, noises produced
during movement of robots can degrade the
ASR performances. This work focuses on
noise reduction by a non-negative matrix fac-
torization (NMF) approach to efficiently sup-
press non stationary noise produced by the
sensors of an assisting robot system.
1 Introduction
The amount of older people in today?s societies con-
stantly grows due to demographic changes (Euro-
pean Commision Staff, 2007). Technical systems
become more and more common to support for rou-
tine tasks of care givers or to assist older persons
living alone in their home environments (Alliance,
2009). Various technical assistive systems have been
developed recently (Lisetti et al, 2003), ranging
from reminder systems (Boll et al, 2010; Goetze
et al, 2010) to assisting robots (Chew et al, 2010;
Goetze et al, 2012). If robot systems are supposed
to navigate autonomously they usually rely on vision
sensors (Aragon-Camarasa et al, 2010) or acous-
tic sensors (Youssef et al, ). Acoustic signals are
usually picked up by microphones mounted on the
robot. In real-world scenarios not only the desired
signal part is picked up by these microphones as pre-
sented in Figure 1.
spectrum speech
NRFilter
y[k]=s[k]+n[k]
h[k]
hats[k]
0.5 1 1.5 2
0
1
2
3
4
-25
-20
-15
-10
-5
0
0.5 1 1.5 2
0
1
2
3
4
-25
-20
-15
-10
-5
0
0.5 1 1.5 2
0
1
2
3
4
-25
-20
-15
-10
-5
0
n[k]
0.5 1 1.5 2
0
1
2
3
4
-25
-20
-15
-10
-5
0
fre
que
ncy
s[k]
time
fre
que
ncy
spectrum noise
time time
time
fre
que
ncy
fre
que
ncy
spectrum mic spectrum outputspectrum outputspectrum output
spectrum output
spectrum output
time
time
time
time
y(k)=s(k)+n(k)
h(k)
( )
^
n(k)
s(k)
Figure 1: General denoising scheme
The desired signal part is usually superposed with
disturbing noise originating from the environment or
the robot system itself. This disturbance has to be
removed from the microphone signal before it can
be further processed, e.g. for navigation, position
estimation, acoustic event detection, speaker detec-
tion or automatic speech recognition. This contri-
bution focuses on acoustic input for a robot system
and more specifically on the noise reduction pre-
processing which is needed to clean up noisy sound
signals.
Automatic speech recognition (Huang et al,
2001; W?lfel et al, 2009) is a convenient way to in-
teract with robot assistants since speech is the most
natural form of communication. However, to en-
sure acceptance of speech recognition systems a suf-
28
ficiently high recognition rate has to be achieved
(Pfister and T., 2008). Today?s speech recognition
systems succeed in achieving this recognition rate
for environments with low amount of noise and re-
verberation. Unfortunately, while moving, robots
can produce noise degrading the reliability of the
ASR.
This work focuses on a specific application, sup-
pressing the non stationary noise produced by the
ultra-sonic sensors of a robotic assistant while mov-
ing. Please note that although in theory ultrasonic
sensors do not produce sound disturbances in the au-
dible range, artefacts due to the fast activation and
deactivation of the sensors are present in the audible
range and are clearly perceivable as a disturbance in
the picked up microphone signal as shown later in
Figure 6.
Ultrasonic
Sensors
Figure 2: Lower part of the robot with ultrasonic sen-
sors (Metralabs, 2010).
Non-negative Matrix Factorization (NMF) is an
approach introduced by Lee & Seung (Lee and Se-
ung, 2001) in which the data is described as the
product of a set of basis and of a set of activation co-
efficients both being non-negative. We will apply the
NMF approach to remove the disturbances caused
by the ultrasonic sensors from the microphone input
signal in the following. NMF and its various exten-
sions have been proven efficient in sources separa-
tion (Cichocki et al, ; Virtanen, 2007), supervised
detection of acoustic events (Cotton and Ellis, 2011)
or to wind noise reduction (Schmidt et al, ). As
the NMF algorithm can be fed with prior informa-
tion about the content to identify, it is a handy way
to suppress the non stationary noise produced by the
sensors of the considered robotic assistant.
The remainder of this paper is organized as fol-
lows: The general NMF algorithm is presented in
Section 2 and the proposed denoising method is
described in Section 3. An experiment using the
TIMIT (Zue et al, 1990) speech corpus is presented
in Section 4 and finally the performances are eval-
uated in terms of achieved signal enhancement in
Section 5 before Section 6 concludes the paper.
2 Sparse Non-negative Matrix
Factorization
2.1 NMF algorithm
NMF is a low-rank approximation technique for
multivariate data decomposition. Given a real val-
ued non-negative matrix V of size n?m and a pos-
itive integer r < min(n,m), it aims to find a factor-
ization of V into a n? r real matrix W and a r ?m
real matrix H such that:
V ?W ?H (1)
The multivariate data to decompose is stacked into
V, whose columns represent the different observa-
tions, and whose rows represent the different vari-
ables. In the case of information extraction from au-
dio files, V could be the amplitude of the spectro-
gram and therefore, W would be a basis of spectral
features when H would represent the levels of acti-
vation of each of those features along time. The rank
r of the factorization corresponds to the number of
elements present in the dictionary W, and thereof, to
the number of rows within H.
NMF is an iterative process that can be fed with
information about the contents to extract. As an il-
lustration of this ability, an artificial spectrogram of
a mixture of two chords, C and D, has been created.
Figure 3 shows the initialization of the NMF algo-
rithm. V is the spectrogram of the mixture in which
the two chords contain only notes?fundamentals and
overlap each other. The Algorithm is fed with the
spectral content of the C chord.
Figure 4 shows that during the iterative process,
the elements of W corresponding to the C chord
remain unchanged while the other elements of W
have been updated to fit the spectral content of the
D chord. The output time activations within H cor-
29
respond to the presence of both chords within the
matrix V.
Figure 3: Illustration of the initialization of the NMF al-
gorithm. The spectral content of the C chord is input into
W while the other element of dictionary and activation
coefficients in H are randomly initialized.
Figure 4: Illustration of the output of the NMF algorithm.
The spectral content of the D chord has been learned
while the updated H corresponds to the activations of the
chords C and D along time.
2.2 Sparseness Constraint
The very definition of sparseness (or sparsity) is that
a vector is sparse when most of its elements are zero.
In its application to NMF, the addition of a sparse-
ness constraint ? permits to trade off between the
fitness of the factorization and the sparseness of H.
At each iteration, the process aims at reducing a
cost function C. In this paper, a generalized version
of the Kullback Leibler divergence is used as cost
function:
D(V,WH) =
?
?
?
?V? log
V
W ?H
? V + W ?H
?
?
?
? (2)
In 2 the multiplication ? and the division are
element-wise. The sparseness constraint results in
the new cost function:
C(V,WH) = D(V,WH) + ?
?
ij
Hij (3)
The norm of each of the objects within W is fixed to
unity.
3 Supervised NMF denoising
3.1 Method overview
The method is supervised in the sense that it uses
a noise dictionary Wn built from a recording of the
known noise to be reduced. The noise spectrogram
?n, i.e. the short-term fourier transform (STFT), is
computed using a hamming window of 32ms and a
50% overlap. The magnitude Vn of ?n is input to
the NMF algorithm with a sparseness constraint ?
and an order rn, providing the noise dictionary of rn
spectral vectors. The spectrogram Vs of the noisy
speech is then input to the NMF algorithm along
with Wn in order to obtain the denoised speech spec-
trogram.
3.2 Separation of the speech signal
H
s
H
n
Spectrogram
W
n
W
s
Noisy Denoised
Synthesis
NMF
Figure 5: Overview of the NMF based denoising.
The denoising is summarized in Figure 5. The
spectrum ?s of the noisy speech and its amplitude
Vs are computed as in Section 3.1. Vs is input to
the NMF algorithm along with Wn. The order of
factorization r is equal to rn + rs, rs being the num-
ber of spectral vector used in the speech dictionary
Ws. Different sparseness constraint ?n and ?s can
30
be applied to the activation matrices Hn and Hs.
Given V ? Rn?m+ , r ? N
? s.t. r < min(n,m)
minimize C(V,WH) w.r.t.W,H (4)
subject to W ? Rn?r+ , H ? R
r?m
+
The update rules on W and H can be expressed as
multiplicative updates:
Ws ?Ws ?
V
WsHs
?HTs
1?HTs
H? H?
WT ? VWH
WT ?1
(5)
The NMF algorithm provides thereof Ws and Hs to
be used to approximate the spectrogram of the de-
noised speech. Therefore, ? being the matrix prod-
uct:
V?s = Ws ?Hs V?n = Wn ?Hn
??s = ?s ? V?sV?s+V?n
(6)
The denoised speech signal is finally obtained by
applying ISTFT on the spectrogram S?s. The inter-
ested reader is referred to (O?Grady and Pearlmutter,
2006) for a more detailed discussion of the needed
derivations for Eqs. (5)-(6).
4 Experiment
4.1 Context
The robot platform Scitos A5 (Metralabs, 2012) can
be used as a robotic assistant for elderly care. Its
built-in microphones allow to interact with the robot
using if their signal is analysed by an ASR sys-
tem. However, while in motion, the robot uses ul-
trasonic sensors (c.f. Figure 2) to detect potential
obstacles. Their constant activation and deactivation
produces artifacts that can sever the ASR reliability.
The following experiment aims to evaluate the effi-
ciency of the denoising method proposed in Section
3 on speech signals corrupted by this specific sen-
sors noise. The Figure 6 examplarily presents the
spectrogram of a corrupted speech signal.
4.2 Protocol
The noise produced by the sensors and the room
impulse response (RIR) have been recorded in a
quiet office room using the robot?s microphone. The
test data has been built from the test portion of the
 
Freq
uenc
y in 
Hz
 
 8000
7000
6000
5000
4000
3000
2000
1000
?30
?25
?20
?15
?10
?5
0
 Time in s
 
Freq
uenc
y in 
Hz
 
 
0 1 2 3 4 5
8000
7000
6000
5000
4000
3000
2000
1000
?30
?25
?20
?15
?10
?5
0
Figure 6: Spectrogram of a speech sentence from the
TIMIT corpora: ?She had your dark suit in greasy wash
water all year.?, clean (top) and with added sensors noise
at SNR=10dB.
TIMIT corpus (Zue et al, 1990). The clean speech
files have been built concatenating a silent period of
0.5 seconds in their beginning, to allow for compari-
son with methods relying on a voice activity detector
(VAD), and convolving it with the measured RIR.
From those prepared clean files, noisy corpora have
been built by adding the recorded sensors noise with
a SNR set to 10, 5, 0 and -5 dB. In real scenarios, the
SNR of the speech corrupted by the sensors noise
vary between 5 and 10 dB depending on the loud-
ness of the speaker and the distance between him
and the robot.
When applying the NMF algorithm the cost func-
tion (3) has been used but no stop criterion has been
set and a fixed number of 25 iterations has been
run. Wn has been built by applying the NMF al-
gorithm with rn = 64 and ? = 0 to a 10 seconds
noise recording. When applying the algorithm to the
speech samples denoising, r has been set to 128. A
different sparseness constraint has been applied to
Hn and Hs with ?n = 0 and ?s = 0.2.
As a reference, the noisy sound samples have as
31
well been processed using a state-of-the-art single-
channel noise reduction scheme, i.e. the decision-
directed approach according to (Ephraim and Malah,
1985) based on two different noise estimation
schemes, i.e. the minimum statistics approach (MS)
as described in (Martin, 2001) and the minimum
mean square error (MMSE) approach according
to (Gerkmann and Hendriks, 2011).
5 Results
The achieved denoising is evaluated with the SNR
of the denoised samples and with the noise reduc-
tion (NR) as described in (Loizou, 2007). For both
scores, the presented values are the mean of the
achieved scores on all tested speech samples and
the standard deviation along the corpus. The results
are presented in Figure 7 for varying input SNR and
spectrograms of a denoised speech sample using the
three methods is shown in Figure 8. It appears that
the NMF based method provides better results, both
in term of signal enhancement and of reliability.
10 5 0 ?5?8
?4
0
4
8
SNR of noisy signal in dB
NR 
of d
enoi
sed 
sign
al in
 dB
12
16
20
24
28
SNR
 of d
enoi
sed 
sign
al in
 dB
 
 NMFMMSEMS
10 5 0 ?5?8
?4
0
4
8
SNR of noisy signal in dB
NR 
of d
enoi
sed 
sign
al in
 dB
12
16
20
24
28
SNR
 of d
enoi
sed 
sign
al in
 dB
 
 NMFMMSEMS
Figure 7: Mean and standard deviation of the achieved
SNR and NR for the three tested methods and for differ-
ent noise levels (SNR).
 
Freq
uenc
y in 
Hz
 
 8000
7000
6000
5000
4000
3000
2000
1000
?30
?25
?20
?15
?10
?5
0
 
Freq
uenc
y in 
Hz
 
 8000
7000
6000
5000
4000
3000
2000
1000
?30
?25
?20
?15
?10
?5
0
 Time in s
 
Freq
uenc
y in 
Hz
 
 
0 1 2 3 4 5
8000
7000
6000
5000
4000
3000
2000
1000
?30
?25
?20
?15
?10
?5
0
Figure 8: Spectrogram of a denoised signal using the
three different methods, MS (top), MMSE (middle) and
NMF.
6 Conclusion
A NMF based method to enhance speech signal
when provided with spectral knowledge of the noise
has been presented. This method has been applied to
the reduction of the non stationary noise produced
by the sensors of a robotic assistant. When tested
on a corpus of speech signals, the proposed method
achieved better performances than well known VAD
based denoising.
Further works would include fine tuning of the
method, such as determining the optimal number of
iterations to obtain the best trade off between en-
hancement and computing cost, as well as the use of
spectro temporal patches as elements of dictionary.
32
7 Acknowledgement
This work was partially supported by the "Adapt-
able Ambient Living Assistant" (ALIAS) project co-
funded by the European Commission and the Fed-
eral Ministry of Education and Research (BMBF).
References
The European Ambient Assisted Living Innovation Al-
liance. 2009. Ambient Assisted Living Roadmap.
VDI/VDE-IT AALIANCE Office.
G. Aragon-Camarasa, H. Fattah, and J. Paul Siebert.
2010. Towards a unified visual framework in a binoc-
ular active robot vision system. Robotics and Au-
tonomous Systems, 58(3):276?286.
S. Boll, W. Heuten, E.M. Meyer, and M. , Meis. 2010.
Development of a Multimodal Reminder System for
Older Persons in their Residential Home. Informatics
for Health and Social Care, SI Ageing & Technology,
35(4).
Selene Chew, Willie Tay, Danielle Smit, and Christoph
Bartneck. 2010. Do social robots walk or roll? In
Shuzhi Ge, Haizhou Li, John-John Cabibihan, and
Yeow Tan, editors, Social Robotics, volume 6414 of
Lecture Notes in Computer Science, pages 355?361.
Springer Berlin / Heidelberg.
A. Cichocki, R. Zdunek, and S. Amari. New algorithms
for non-negative matrix factorization in applications
to blind source separation. In Proc. of Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006.,
volume 5, pages V?V, Toulouse, France.
C.V. Cotton and D.P.W. Ellis. 2011. Spectral vs. spectro-
temporal features for acoustic event detection. In
Proc. of 2011 IEEE Workshop on Applications of Sig-
nal Processing to Audio and Acoustics (WASPAA),
pages 69 ?72, New Paltz, NY, USA, oct.
Y. Ephraim and D. Malah. 1985. Speech enhancement
using a minimum mean-square error log-spectral am-
plitude estimator. IEEE Transactions on Acoustics,
Speech and Signal Processing, 33(2):443?445.
European Commision Staff. 2007. Working Document.
Europes Demografic Future: Facts and Figures. Tech-
nical report, Commission of the European Communi-
ties.
T. Gerkmann and R.C. Hendriks. 2011. Noise power es-
timation based on the probability of speech presence.
In Proc. of 2011 IEEE Workshop on Applications of
Signal Processing to Audio and Acoustics (WASPAA),
pages 145?148, New Paltz, NY, USA.
S. Goetze, N. Moritz, J.E. Appell, M. Meis, C. Bartsch,
and J. Bitzer. 2010. Acoustic user interfaces for
ambient-assisted living technologies. Informatics for
Health and Social Care.
S. Goetze, S. Fischer, N. Moritz, J.E. Appell, and F. Wall-
hoff. 2012. Multimodal human-machine interaction
for service robots in home-care environments. Jeju,
Republic of Korea.
X. Huang, A. Acero, H.W. Hon, et al 2001. Spoken
language processing, volume 15. Prentice Hall PTR
New Jersey.
D.D. Lee and H.S. Seung. 2001. Algorithms for non-
negative matrix factorization. Advances in neural in-
formation processing systems, 13.
C. Lisetti, F. Nasoz, C. LeRouge, O. Ozyer, and K. Al-
varez. 2003. Developing multimodal intelligent affec-
tive interfaces for tele-home health care. International
Journal of Human-Computer Studies, 59(1-2):245 ?
255. Applications of Affective Computing in Human-
Computer Interaction.
P.C. Loizou. 2007. Speech Enhancement: Theory and
Practice. CRC Press Inc., Boca Raton, USA.
R. Martin. 2001. Noise power spectral density estima-
tion based on optimal smoothing and minimum statis-
tics. IEEE Transactions on Speech and Audio Process-
ing, 9(5):504?512.
Metralabs. 2010. Technical manual.
Metralabs. 2012. http://www.metralabs.com.
P.D. O?Grady and B.A. Pearlmutter. 2006. Convolu-
tive non-negative matrix factorisation with a sparse-
ness constraint. In Proc. of the 2006 16th IEEE Sig-
nal Processing Society Workshop on Machine Learn-
ing for Signal Processing, Maynooth, Ireland.
B. Pfister and Kaufmann T. 2008. Speech processing
Fundamentals and mthods for speech synthesis and
speech recognition (German original title: Sprachver-
arbeitun Grundlagen und Methoden der Sprachsyn-
these und Spracherkennung). Springer, Berlin Heidel-
berg.
M.N. Schmidt, J. Larsen, and F.T. Hsiao. Wind noise re-
duction using non-negative sparse coding. In Proc. of
the 2007 17th IEEE Signal Processing Society Work-
shop on Machine Learning for Signal Processing,
Thessaloniki, Greece.
T. Virtanen. 2007. Monaural sound source separa-
tion by nonnegative matrix factorization with tempo-
ral continuity and sparseness criteria. IEEE Trans-
actions on Audio, Speech, and Language Processing,
15(3):1066?1074.
M. W?lfel, J.W. McDonough, and Inc Ebrary. 2009. Dis-
tant speech recognition. Wiley Online Library.
K. Youssef, S. Argentieri, and J.L. Zarader. Binaural
speaker recognition for humanoid robots. In Proc. of
2010 11th International Conference on Control Au-
tomation Robotics & Vision (ICARCV), Singapore, Re-
public of Singapore.
V. Zue, S. Seneff, and J. Glass. 1990. Speech database
development at MIT: TIMIT and beyond. Speech
Communication, 9(4):351?356.
33
