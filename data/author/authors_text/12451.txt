Proceedings of the Workshop on BioNLP, pages 117?124,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Transforming Controlled Natural Language Biomedical Queries
into Answer Set Programs
Esra Erdem and Reyyan Yeniterzi
Faculty of Engineering and Natural Sciences
Sabanc? University
Orhanl?, Tuzla 34956 Istanbul, Turkey
Abstract
We introduce a controlled natural language for
biomedical queries, called BIOQUERYCNL,
and present an algorithm to convert a biomed-
ical query in this language into a program
in answer set programming (ASP)?a for-
mal framework to automate reasoning about
knowledge. BIOQUERYCNL allows users to
express complex queries (possibly containing
nested relative clauses and cardinality con-
straints) over biomedical ontologies; and such
a transformation of BIOQUERYCNL queries
into ASP programs is useful for automat-
ing reasoning about biomedical ontologies by
means of ASP solvers. We precisely describe
the grammar of BIOQUERYCNL, implement
our transformation algorithm, and illustrate its
applicability to biomedical queries by some
examples.
1 Introduction
The rapid increase in the popularity and usage of
Web leads researchers to store data and make it pub-
licly available in many ways. In particular, to facili-
tate access to its desired parts, it is stored in a struc-
tured form, like ontologies. These ontologies can
be queried with an SQL-like formal query language.
However, since these ontologies have been devel-
oped for and widely used by people that lacks the
necessary knowledge in a formal query language,
a simpler and more commonly known language is
needed to represent queries. A natural language is
the perfect answer, but ambiguities in its grammar
and vocabulary make it difficult to automate reason-
ing about queries in natural language. Therefore, to
represent queries, we consider a middle ground be-
tween these two options: a Controlled Natural Lan-
guage (CNL).
A CNL is a subset of a natural language, with a re-
stricted grammar and vocabulary, that overcomes the
ambiguity of natural languages. Since we consider
queries in a specific domain, namely biomedicine,
and over specific sources of information, namely
biomedical ontologies, a CNL designed and devel-
oped for reasoning about biomedical ontologies is
sufficient to represent biomedical queries. Essen-
tially, a CNL is a formal language but with a look
of a natural language. Therefore, compared to a
natural language, a CNL can be easily converted to
some other formalisms. This allows us to use au-
tomated reasoners, specifically developed for such
formalisms, to find answers to queries expressed in
a CNL.
One such formalism is Answer Set Programming
(ASP) (Baral, 2003). ASP is a new knowledge rep-
resentation and reasoning paradigm which supports
representation of defaults, constraints, preferences,
aggregates, etc., and provides technologies that al-
low us to automate reasoning with incomplete in-
formation, and to integrate other technologies, like
description logics reasoners and Semantic Web tech-
nologies. For instance, in (Bodenreider et al, 2008),
the authors illustrate the applicability and effective-
ness of using ASP to represent a rule layer that inte-
grates relevant parts of some biomedical ontologies
in RDF(S)/OWL, and to compute answers to some
complex biomedical queries over these ontologies.
Although CNLs are appropriate for expressing
biomedical queries, and methods and technologies
117
Figure 1: Architecture of the Overall System
of ASP are appropriate for automated reasoning
about biomedical ontologies, there is no algorithm
to convert a CNL biomedical query into a program.
In (Bodenreider et al, 2008), biomedical queries are
represented as programs in ASP; however, these pro-
grams are constructed manually. However, manually
constructing ASP programs to represent biomedi-
cal queries is not only time consuming but also
requires expertise in ASP. This prevents automat-
ing the whole process of computing an answer to a
query, once it is given in a CNL.
In this paper, we design and develop a CNL
(called BIOQUERYCNL) for expressing biomedical
queries over some ontologies, and introduce an al-
gorithm to convert a biomedical query expressed in
this CNL into a program in ASP. The idea is to au-
tomatically compute an answer to the query using
methods of (Bodenreider et al, 2008), once the user
types the query. This idea is illustrated in Figure 1.
Similar approaches of using a CNL for querying
ontologies have been investigated in various stud-
ies. For instance, (Bernstein et al, 2005) consid-
ers queries in the controlled natural language, At-
tempto Controlled English (ACE) (Attempto, 2008),
and transforms them into queries in PQL (Klein and
Bernstein, 2004) to be evaluated by a query engine.
(Bernstein et al, 2006) presents a system that guides
the user to write a query in ACE, and translates the
query into SPARQL to be evaluated by the reasoner
of JENA (Jena, 2008). On the other hand, (Kauf-
mann et al, 2006) transforms a given natural lan-
guage query to a SPARQL query (using the Stan-
ford Parser and WORDNET) to be evaluated by a
reasoner like that of JENA. Our work is different
from these studies in two ways: we consider queries
over biomedical ontologies (thus different forms of
queries, and vocabulary), and we transform a query
into an ASP program to automate reasoning over a
rule layer presented in ASP.
Transformations of natural language sentences
into ASP has been studied in (Baral et al, 2008) and
(Baral et al, 2007). In (Baral et al, 2008), the au-
thors introduce methods to transform some simple
forms of sentences into ASP using Lambda Calcu-
lus. In (Baral et al, 2007), the authors use C&C
tools (CC, 2009) to parse the some forms of natu-
ral language input, and perform a semantic analysis
over the parser output using BOXER (Boxer, 2009),
to do reasoning in ASP. Our work is different in that
we consider a CNL to express queries, and intro-
duce a different method for converting CNL to a pro-
gram in ASP, via Discourse Representation Struc-
tures (DRS) (Kamp, 1981).
In the rest of the paper, first we briefly discuss
ASP with some examples (Section 2). Then we de-
fine the grammatical structure of BIOQUERYCNL
and give some examples (Section 3). Next, we
introduce our algorithm for transforming a BIO-
QUERYCNL query into an ASP program and ex-
plain it by an example (Section 4). We conclude
with a discussion of challenges related to the im-
plementation of our algorithm (Section 5) and other
related problems that we are working on (Section 6).
2 Answer Set Programming
Answer Set Programming (ASP) (Lifschitz, 1999;
Marek and Truszczyn?ski, 1999; Niemela?, 1999;
Baral, 2003) is a new knowledge representation
and reasoning paradigm which supports representa-
tion of defaults, constraints, preferences, aggregates,
etc., and provides technologies that allow us to auto-
mate reasoning with incomplete information, and to
integrate other technologies, like description logics
reasoners and Semantic Web technologies.
In ASP, knowledge is represented as a ?program?
(a finite set of ?rules?) whose meaning is captured
by its models (called ?answer sets? (Gelfond and
Lifschitz, 1988)). Answer sets for a program can
be computed by ?answer set solvers? such as DLV
118
(DLV, 2009). Consider for instance the program:
gene_gene(??ADRB1??,??CHRM5??).
gene_gene(??CHRM1??,??CHRM5??).
chain(X,Y) :- gene_gene(X,Y).
chain(X,Y) :- gene_gene(Y,X).
chain(X,Y) :- gene_gene(X,Z), chain(Z,Y).
The first rule expresses that the gene ADRB1 inter-
acts with the gene CHRM5. The second rule ex-
presses that the gene CHRM1 interacts with the gene
CHRM5. The third, the fourth, and the fifth rules
express a chain of such interactions. In a rule con-
taining :-, the left-hand-side of :- is called the head
of the rule, the right-hand-side is called the body of
the rule. Such a rule p :- q, r. is read as ?p if q
and r?. Here the head atom is p, and the body atoms
are q and r. The answer set for this program de-
scribes that there is a chain of interactions between
CHRM1 and CHRM5, ADRB1 and CHRM5, and
ADRB1 and CHRM1.
As mentioned above, the language of ASP is ex-
pressive enough to represent defaults, constraints,
preferences, aggregates, etc.. For instance, the rule
treats_2diseases(R) :-
#count{D:treats(R,D)}>=2, drug(R).
describes drugs R that treat at least 2 diseases.
3 A Controlled Natural Language for
Biomedical Queries
We introduce a controlled natural language, called
BIOQUERYCNL, to express biomedical queries,
whose grammar is shown in Table 1. This gram-
mar should be considered in connection with the
given biomedical ontologies. The italic words in the
grammar, for instance, represent the information ex-
tracted from the related ontologies. We call these
italic words ontology functions; the detailed descrip-
tion of these functions are given in Table 2.
With BIOQUERYCNL, the users can ask simple
queries, queries with nested relative clauses (with
any number of conjunctions and disjunctions), and
queries with cardinalities. Some sample queries are
given below.
(Q1) Which symptoms are alleviated by the drug
Epinephrine?
(Q2) What are the side-effects of the drugs that treat
the disease Asthma?
(Q3) What are the genes that are related to the
disease Asthma and are targeted by the drug
Epinephrine?
(Q4) What are the symptoms of the diseases that are
related to the gene ADRB1 or that are treated
by the drug Epinephrine?
(Q5) Which genes are targeted by at least 2 drugs
and are related to at most 3 diseases?
BIOQUERYCNL is a subset of Attempto Con-
trolled English (ACE) (Attempto, 2008), which can
represent a wide range of queries (Fuchs et al,
2008), specialized for biomedical ontologies.
4 Converting Controlled Natural
Language Queries to Programs
We have implemented an algorithm, QUERY, pre-
sented in Algorithm 1, that obtains an ASP rule
Head ? Body from a query Q expressed in BIO-
QUERYCNL, via transforming Q into a DRS. We
will explain the main steps of the QUERY algorithm
by an example, considering query (Q4).
Algorithm 1 QUERY(Q)
Input: A query Q
Output: An ASP rule Head ? Body
1: D := Find the DRS of Q
2: Head := HEAD(D)
3: Body? := BODY(D)
4: Body := POSTPROCESSING(Body?)
5: return Head ? Body
4.1 Transforming a CNL Query into DRS
Attempto Controlled English (ACE) text can be
converted into Discourse Representation Structures
(DRS) (Kamp, 1981) ? a variant of the first-order
logic that is used for the dynamic interpretation of
natural language and systematic translation of natu-
ral language into logical form ? without any am-
biguity, using tools like Attempto Parsing Engine
(APE). APE converts ACE text to DRS by an ap-
proach similar to (Blackburn and Bos, 2005), as ex-
plained in (Fuchs et al, 2008). For instance, APE
transforms query (Q4) into the following DRS:
119
Table 1: The Grammar of BIOQUERYCNL
QUERY? YESNOQUERY |WHQUERY QUESTIONMARK
YESNOQUERY? DODOESQUERY | ISAREQUERY
WHQUERY? WHATQUERY |WHICHQUERY
DODOESQUERY? [ Do | Does ] Type() Instance(T ) PREDICATERELATION
ISAREQUERY? [ Is | Are ] Type() Instance(T ) V erb(T )
WHATQUERY? What BE Type() that PREDICATERELATION
WHATQUERY? What BE OFRELATION that PREDICATERELATION
WHATQUERY? What BE OFRELATIONINSTANCE that PREDICATERELATION
WHICHQUERY? Which Type() PREDICATERELATION
OFRELATION? Noun(T ) of Type()
OFRELATIONINSTANCE? Noun(T ) of Type() Instance(T )
PREDICATERELATION? ACTIVERELATION (CONNECTOR (that)? PREDICATERELATION)*
PREDICATERELATION? PASSIVERELATION (CONNECTOR (that)? PREDICATERELATION)*
ACTIVERELATION? V erb(T, T ?) Type() Instance(T ?)
ACTIVERELATION? V erb(T, T ?) GENERALISEDQUANTOR PositiveNumber Type()
PASSIVERELATION? BE V erb(T ?, T ) by Type() Instance(T ?)
PASSIVERELATION? BE V erb(T ?, T ) by GENERALISEDQUANTOR PositiveNumber Type()
BE? is | are
CONNECTOR? and | or
GENERALISEDQUANTOR? at least | at most | more than | less than | exactly
QUESTIONMARK? ?
Table 2: The Ontology Functions
Type() returns the type information the ontologies keep, ex. gene, disease, drug
Instance(T ) returns instances of the type T , ex. Asthma for type disease
V erb(T ) returns the verbs related to the type T , ex. approve for type drug
V erb(T, T ?) returns the verbs where type T is the subject and type T ? is the object, ex. drug treat disease
Noun(T ) returns the nouns that are related to the type T , ex. symptom for type disease
[A,B,C,D]
query(A,what)-1
predicate(B,be,A,C)-1
relation(C,of,D)-1
object(C,symptoms,countable,na,eq,1)-1
[E,F,G]
modifier_pp(F,to,E)-1
property(G,related,pos)-1
predicate(F,be,D,G)-1
object(E,gene_ADRB1,countable,na,eq,1)-1
v
[H,I]
predicate(I,treated,H,D)-1
object(H,drug_Epinephrine,
countable,na,eq,1)-1
object(D,diseases,countable,na,geq,2)-1
Note that the DRS consists of two kinds of expres-
sions. The lines with a list of uppercase letters, like
[E,F,G], describe the domain of the DRS; each up-
percase letter is a referent. The rest of the DRS de-
scribe the conditions about the domain.
The DRS above contains some predefined pred-
icates, such as object, property, predicate,
query, etc.. All the nouns, adjectives, verbs, mod-
ifiers, etc. are represented with one of them. For
instance,
? object describes objects and the relevant
forms of nouns denoting them (like ?diseases?)
? predicate describes relations that are pro-
120
duced by different forms of verbs (like
?treated?),
? relation describes relations that are produced
by of-constructions (like ?symptoms of dis-
ease?),
? query describes the form of the query and the
objects that the query is referring to.
Ontologies represent relations between concepts.
A rule layer over ontologies introduce further con-
cepts integrating them. ASP takes into account
relevant concepts and relations to answer a given
query about these ontologies. In the biomedical
queries we consider, the concepts and instances are
represented with object and the relations between
these concepts are represented with predicate and
relation. The query is also important in terms of
the type of information the user asks for.
4.2 Constructing the Head and the Body Atoms
Once the corresponding DRS is obtained from a
given BIOQUERYCNL query, the head and the body
atoms are constructed by analyzing the conditions in
the DRS, as described in Algorithms 2 and 3.
The HEAD algorithm is about the query pred-
icate, which refers to objects or relations that
are asked for in the given query. By following
the referents, starting from the one mentioned
in query, the algorithm finds out the type of
the information that is asked for in the given
query. Consider, for instance, query (Q4). The
referent mentioned in query(A,what) is A.
It is mentioned in predicate(B,be,A,C)-1,
and here it denotes an object with referent
C. Now let?s find where C is mentioned: in
object(C,symptoms,countable,na,eq,1)-1 to
denote symptoms. Therefore, the query asks for
symptoms. Based on this information, Algorithm 2
returns the head of the ASP rules as follows:
what_be_symptoms(SYM1)
The BODY algorithm analyzes the predicate and
the relation predicates. These two predicates de-
scribe relations between objects described by the
object predicates. The algorithm starts from the
predicate and the relation predicates, and then,
by following the referents, it returns the body atoms
of the ASP rule. For instance, Algorithm 3 returns
the following body atoms for query (Q4):
symptoms_of_diseases(symptom_SYM1,
disease_DIS1)
diseases_be_related_to_gene(disease_DIS1,
gene_??ADRB1??)
drug_treated_diseases(drug_??Epinephrine??,
disease_DIS1)
These body atoms are given to POSTPROCESSING
step, to produce bodies of the ASP rules.
4.3 Constructing the ASP Rules
POSTPROCESSING is the last step of the QUERY
algorithm. At this step, first the number of rules
is determined, and then the body atoms are placed
in the bodies of these rules. In ASP, a conjunc-
tive query can be represented by a rule. However,
disjunctive queries are represented by several rules
with same head but different bodies. For instance,
query (Q4) is a disjunctive query (a disjunction of
two queries), so there will be two rules representing
this query:
what_be_symptoms(SYM1) :-
symptoms_of_diseases(symptom_SYM1,
disease_DIS1),
diseases_be_related_to_gene(disease_DIS1,
gene_??ADRB1??).
what_be_symptoms(SYM1) :-
drug_treated_diseases(drug_??Epinephrine??,
disease_DIS1),
symptoms_of_diseases(symptom_SYM1,
disease_DIS1).
Next, the predicate names in the bodies of these
rules are matched with the names of the already de-
fined predicates in ontologies or in the rule layer
over these ontologies. After matching the predicate
names, the parameters of the predicates may have to
be reordered.
The matching of the predicates very much de-
pends on the user interface (UI). If UI enforces users
to use a specific grammar and lexicon while form-
ing the query, then the matching can be done with
an easy table look-up method. If the UI allows more
flexibility of constructing a query, then the match-
ing algorithm should use some basic Natural Lan-
guage Processing methods and similarity metrics to
find the most probable matching.
After matching the predicates, the ordering of the
parameters can be done easily. The BODY algorithm
121
Algorithm 2 HEAD(D)
Input: A DRS
Output: Head of an ASP rule
1: query(Ref ,QuestionWord) // e.g., query(A,which) for ?Which drug ...?
2: if Ref is an object then
3: Object := REFERSTO(Ref ) // e.g., A refers to a ?drug? DRG1
4: Head := CONCAT(QuestionWord,Object,Ref ) // e.g., which drug(DRG1)
5: else if Ref is a subject of a predicate then // query(A,what) for ?What are the genes ...?
6: Object := REFERSTO(Ref ) // e.g., A refers to ?genes? GENE1
7: Head := CONCAT(QuestionWord,Predicate,Object,Ref ) // e.g., what be genes(GENE1)
8: end if
9: return Head
returns the body predicates with the parameters. In
these parameters, the type and the instance names
are kept together. Thus, ordering of those parame-
ters are done just by using the type information. Af-
ter the ordering is done, the type information part is
removed from the parameters.
For instance, after matching the predicates, we get
the following ASP rule for query (Q4).
what_be_symptoms(SYM1) :-
disease_symptom(DIS1,SYM1),
disease_gene(DIS1,??ADRB1??).
what_be_symptoms(SYM1) :-
treats_disease(??Epinephrine??,DIS1),
disease_symptom(DIS1,SYM1).
With an ASP rule layer over ontologies, and
this ASP program, an ASP solver, like DLVHEX
(DLVHEX, 2009), returns an answer to query (Q4).
For instance, consider the ASP rule layer, and
the gene, disease, drug ontologies of (Bodenreider
et al, 2008). The ontologies of (Bodenreider et
al., 2008) are obtained from the ontologies PHAR-
MGKB (PharmGKB, 2008), UNIPROT (UniProt,
2008), GENE ONTOLOGY (GO) (GeneOntology,
2008), GENENETWORK database (GeneNetwork,
2008), DRUGBANK (DrugBank, 2008), and the
Medical Symptoms and Signs of Disease web page
(MedicalSymptomsSignsDisease, 2008). With this
rule layer and the ontologies, and the ASP program
above, the following is a part of the answer DLVHEX
finds to the query above:
noisy breathing faster breathing
shortness of breath coughing
chest tightness wheezing
4.4 Another Example
The algorithm discussed above returns the following
ASP program for query (Q5):
which_genes(GN1) :-
2<=#count{DRG1:drug_gene(DRG1,GN1)},
#count{DIS1:disease_gene(DIS1,GN1)}<=3.
Since query (Q5) contains cardinality constraints,
the ASP program uses the aggregate #count.
More examples of biomedical queries, and
the ASP programs generated by our program
can be seen at http://people.sabanciuniv.edu/
esraerdem/bioquery-asp/bionlp09/ .
5 Implementational Issues
We have implemented the algorithms explained
above in PERL. We have used Attempto Parsing
Engine APE to convert a given BIOQUERYCNL
query into a DRS. Since BIOQUERYCNL is about
biomedical ontologies, we provided APE some in-
formation about biomedical concepts, such as gene,
drug, and words that represent relations between
these concepts such as treat, target etc..
However, providing such information is not suf-
ficient to convert all BIOQUERYCNL biomedical
queries into programs, mainly due to specific in-
stances of these concepts (consider, for instance,
various drug names that appear in ontologies). One
way to deal with this problem is to extract from the
ontologies all instances of each concept and provide
them to APE as an additional lexicon. This may not
be the perfect solution since this process has to be
repeated when an instance is added to the ontology.
An alternative way can be enforcing the user to enter
122
Algorithm 3 BODY(D)
Input: A DRS
Output: Body of an ASP rule
1: Body := empty string
2: for each predicate P do
3: // P can be of the form predicate(Ref ,Verb, SubRef ), like predicate(H, targeted, A)
4: Subject := REFERSTO(SubRef ) // e.g., A refers to ?genes? GENE1
5: if P has a verb phrase modifier then
6: ?Modifier,Object? := REFERSTO(Ref ) // e.g., H refers to ? ?by?, ?drug? DRG1 ?
7: end if
8: if P has an object then // P can be of the form predicate(Ref ,Verb, SubRef ,ObjRef )
9: Object := REFERSTO(ObjRef )
10: end if
11: Body := CONCAT(Body, Subject,Verb,Modifier,Object)
12: // e.g., genes targeted by drugs(GENE1, DRG1)
13: end for
14: for each relation R do
15: // R can be of the form relation(Ref1, of ,Ref2), like relation(C, of , D)
16: Object1 := REFERSTO(Ref1) // e.g., C refers to ?symptoms? SYM1
17: Object2 := REFERSTO(Ref2) // e.g., D refers to ?diseases? DIS1
18: Body := CONCAT(Body,Object1,??of??,Object2)
19: // e.g., symptoms of diseases(SYM1, DIS1)
20: end for
21: return Body
the concept name just before the instance (like ?the
drug Epinephrine?) in the query. This is how we deal
with instance names, in the current version of our
implementations. However, such BIOQUERYCNL
queries are not in the language of APE; so, with
some preprocessing, we rewrite these queries in the
correct syntax for APE.
6 Conclusion
We have designed and developed a Controlled Nat-
ural Language (CNL), called BIOQUERYCNL, to
represent biomedical queries over some ontologies,
and provided a precise description of its grammati-
cal structure.
We have introduced an algorithm to convert
queries in BIOQUERYCNL to a program in Answer
Set Programming (ASP). The idea is to compute an-
swers to these queries automatically, by means of
automated reasoners in ASP, over biomedical on-
tologies in RDF(S)/OWL and a rule layer in ASP
integrating these ontologies. Our algorithm can
handle various forms of simple/complex disjunc-
tive/conjunctive queries that may contain (nested)
relative clauses and cardinality constraints.
We have implemented this algorithm in PERL,
and tried it with the ASP rule layer, and the ontolo-
gies of (Bodenreider et al, 2008).
One essential part of the overall system is an in-
telligent user interface that allows a user to enter
biomedical queries in BIOQUERYCNL. Design and
implementation of such a user-interface is a part of
our ongoing work.
Acknowledgments
Thanks to Tobias Kuhn for his help with ACE. This
work is supported by the Scientific and Technolog-
ical Research Council of Turkey (TUBITAK) grant
108E229.
References
Attempto. 2008. http://attempto.ifi.uzh.
ch/site/.
123
Chitta Baral, Juraj Dzifcak, and Luis Tari. 2007. To-
wards overcoming the knowledge acquisition bottle-
neck in answer set prolog applications: Embracing
natural language inputs. In Proc. of ICLP, pages 1?
21.
Chitta Baral, Juraj Dzifcak, and Tran Cao Son. 2008.
Using answer set programming and lambda calculus
to characterize natural language sentences with nor-
matives and exceptions. In Proc. of AAAI, pages 818?
823.
Chitta Baral. 2003. Knowledge Representation, Rea-
soning and Declarative Problem Solving. Cambridge
University Press.
Abraham Bernstein, Esther Kaufmann, Anne Go?hring,
and Christoph Kiefer. 2005. Querying ontologies: A
controlled english interface for end-users. In Proc. of
ISWC, pages 112?126.
Abraham Bernstein, Esther Kaufmann, Christian Kaiser,
and Christoph Kiefer. 2006. Ginseng: A guided in-
put natural language search engine for querying on-
tologies. In Jena User Conference.
Patrick Blackburn and Johan Bos. 2005. Representation
and Inference for Natural Language. A First Course in
Computational Semantics. CSLI.
Olivier Bodenreider, Zeynep Hande C?oban, Mahir Can
Dog?anay, Esra Erdem, and Hilal Kos?ucu. 2008. A
preliminary report on answering complex queries re-
lated to drug discovery using answer set programming.
In Proc. of ALPSWS.
Boxer. 2009. http://svn.ask.it.usyd.edu.
au/trac/candc/wiki/boxer.
CC. 2009. http://svn.ask.it.usyd.edu.au/
trac/candc/wiki.
DLV. 2009. http://www.dbai.tuwien.ac.at/
proj/dlv.
DLVHEX. 2009. http://con.fusion.at/
dlvhex/.
DrugBank. 2008. http://redpoll.pharmacy.
ualberta.ca/drugbank/.
Norbert E. Fuchs, Kaarel Kaljurand, and Tobias Kuhn.
2008. Discourse representation structures for ace 6.0.
Technical Report IFI-2008.02, Department of Infor-
matics, University of Zurich.
Michael Gelfond and Vladimir Lifschitz. 1988. The
stable model semantics for logic programming. In
Robert Kowalski and Kenneth Bowen, editors, Logic
Programming: Proceedings of the Fifth International
Conference and Symposium.
GeneNetwork. 2008. http://humgen.med.uu.
nl/?lude/genenetwork.
GeneOntology. 2008. http://www.
geneontology.org.
Jena. 2008. http://jena.sourceforge.net/.
Hans Kamp. 1981. A theory of truth and semantic rep-
resentation. In J. A. G. Groenendijk, T. M. V. Janssen,
and M. B. J. Stokhof, editors, Formal Methods in the
Study of Language, volume 1, pages 277?322.
Esther Kaufmann, Abraham Bernstein, and Renato Zum-
stein. 2006. Querix: A natural language interface
to query ontologies based on clarification dialogs. In
Proc. of ISWC.
Mark Klein and Abraham Bernstein. 2004. Toward high-
precision service retrieval. IEEE Internet Computing,
8(1):30?36.
Vladimir Lifschitz. 1999. Action languages, answer sets
and planning. In The Logic Programming Paradigm:
a 25-Year Perspective. Springer.
Victor Marek and Miros?aw Truszczyn?ski. 1999. Sta-
ble models and an alternative logic programming
paradigm. In The Logic Programming Paradigm: a
25-Year Perspective. Springer.
MedicalSymptomsSignsDisease. 2008. http:
//www.medicinenet.com/symptoms_and_
signs/article.htm.
Ilkka Niemela?. 1999. Logic programs with stable model
semantics as a constraint programming paradigm. An-
nals of Mathematics and Artificial Intelligence, 25.
PharmGKB. 2008. http://www.pharmgkb.org.
UniProt. 2008. http://www.ebi.uniprot.org/
index.shtml.
124
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 454?464,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical
Machine Translation from English to Turkish
Reyyan Yeniterzi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
reyyan@cs.cmu.edu
Kemal Oflazer
Computer Science
Carnegie Mellon University-Qatar
PO Box 24866, Doha, Qatar
ko@cs.cmu.edu
Abstract
We present a novel scheme to apply fac-
tored phrase-based SMT to a language pair
with very disparate morphological struc-
tures. Our approach relies on syntac-
tic analysis on the source side (English)
and then encodes a wide variety of local
and non-local syntactic structures as com-
plex structural tags which appear as ad-
ditional factors in the training data. On
the target side (Turkish), we only per-
form morphological analysis and disam-
biguation but treat the complete complex
morphological tag as a factor, instead of
separating morphemes. We incrementally
explore capturing various syntactic sub-
structures as complex tags on the En-
glish side, and evaluate how our transla-
tions improve in BLEU scores. Our max-
imal set of source and target side trans-
formations, coupled with some additional
techniques, provide an 39% relative im-
provement from a baseline 17.08 to 23.78
BLEU, all averaged over 10 training and
test sets. Now that the syntactic analy-
sis on the English side is available, we
also experiment with more long distance
constituent reordering to bring the English
constituent order close to Turkish, but find
that these transformations do not provide
any additional consistent tangible gains
when averaged over the 10 sets.
1 Introduction
Statistical machine translation into a morphologi-
cally complex language such as Turkish, Finnish
or Arabic, involves the generation of target words
with the proper morphology, in addition to prop-
erly ordering the target words. Earlier work on
translation from English to Turkish (Oflazer and
Durgar-El-Kahlout, 2007; Oflazer, 2008; Durgar-
El-Kahlout and Oflazer, 2010) has used an ap-
proach which relied on identifying the contextu-
ally correct parts-of-speech, roots and any mor-
phemes on the English side, and the complete se-
quence of roots and overt derivational and inflec-
tional morphemes for each word on the Turkish
side. Once these were identified as separate to-
kens, they were then used as ?words? in a stan-
dard phrase-based framework (Koehn et al, 2003).
They have reported that, given the typical com-
plexity of Turkish words, there was a substantial
percentage of words whose morphological struc-
ture was incorrect: either the morphemes were
not applicable for the part-of-speech category of
the root word selected, or the morphemes were
in the wrong order. The main reason given for
these problems was that the same statistical trans-
lation, reordering and language modeling mecha-
nisms were being employed to both determine the
morphological structure of the words and, at the
same time, get the global order of the words cor-
rect. Even though a significant improvement of a
standard word-based baseline was achieved, fur-
ther analysis hinted at a direction where morphol-
ogy and syntax on the Turkish side had to be dealt
with using separate mechanisms.
Motivated by the observation that many lo-
cal and some nonlocal syntactic structures in En-
glish essentially map to morphologically complex
words in Turkish, we present a radically different
approach which does not segment Turkish words
into morphemes, but uses a representation equiv-
alent to the full word form. On the English side,
we rely on a full syntactic analysis using a depen-
dency parser. This analysis then lets us abstract
and encode many local and some nonlocal syn-
tactic structures as complex tags (dynamically, as
opposed to the static complex tags as proposed by
Birch et al (2007) and Hassan et al (2007)). Thus
454
we can bring the representation of English syntax
closer to the Turkish morphosyntax.
Such an approach enables the following: (i)
Driven by the pattern of morphological structures
of full word forms on the Turkish side represented
as root words and complex tags, we can iden-
tify and reorganize phrases on the English side,
to ?align? English syntax to Turkish morphology
wherever possible. (ii) Continuous and discontin-
uous variants of certain (syntactic) phrases can be
conflated during the SMT phrase extraction pro-
cess. (iii) The length of the English sentences can
be dramatically reduced, as most function words
encoding syntax are now abstracted into complex
tags. (iv) The representation of both the source and
the target sides of the parallel corpus can now be
mostly normalized. This facilitates the use of fac-
tored phrase-based translation that was not pre-
viously applicable due to the morphological com-
plexity on the target side and mismatch between
source and target morphologies.
We find that with the full set of syntax-to-
morphology transformations and some additional
techniques we can get about 39% relative im-
provement in BLEU scores over a word-based
baseline and about 28% improvement of a factored
baseline, all experiments being done over 10 train-
ing and test sets. We also find that further con-
stituent reordering taking advantage of the syntac-
tic analysis of the source side, does not provide
tangible improvements when averaged over the 10
data sets.
This paper is organized as follows: Sec-
tion 2 presents the basic idea behind syntax-
to-morphology alignment. Section 3 describes
our experimental set-up and presents results from
a sequence of incremental syntax-to-morphology
transformations, and additional techniques. Sec-
tion 4 summarizes our constituent reordering ex-
periments and their results. Section 5 presents a
review of related work and situates our approach.
We assume that the reader is familiar with the
basics of phrase-based statistical machine transla-
tion (Koehn et al, 2003) and factored statistical
machine translation (Koehn and Hoang, 2007).
2 Syntax-to-Morphology Mapping
In this section, we describe how we map between
certain source language syntactic structures and
target words with complex morphological struc-
tures. At the top of Figure 1, we see a pair of
(syntactic) phrases, where we have (positionally)
aligned the words that should be translated to each
other. We can note that the function words on and
Figure 1: Transformation of an English preposi-
tional phrase
their are not really aligned to any of the Turkish
words as they really correspond to two of the mor-
phemes of the last Turkish word.
When we tag and syntactically analyze the En-
glish side into dependency relations, and morpho-
logically analyze and disambiguate the Turkish
phrase, we get the representation in the middle of
Figure 1, where we have co-indexed components
that should map to each other, and some of the
syntactic relations that the function words are in-
volved in are marked with dependency links.1
The basic idea in our approach is to take various
function words on the English side, whose syntac-
tic relationships are identified by the parser, and
then package them as complex tags on the related
content words. So, in this example, if we move
the first two function words from the English side
and attach as syntactic tags to the word they are in
dependency relation with, we get the aligned rep-
resentation at the bottom of Figure 1.2,3 Here we
can note that all root words and tags that corre-
spond to each other are nicely structured and are
in the same relative order. In fact, we can treat
each token as being composed of two factors: the
roots and the accompanying tags. The tags on the
Turkish side encode morphosyntactic information
encoded in the morphology of the words, while the
1The meanings of various tags are as follows: Depen-
dency Labels: PMOD - Preposition Modifier; POS - Pos-
sessive. Part-of-Speech Tags for the English words: +IN -
Preposition; +PRP$ - Possessive Pronoun; +JJ - Adjective;
+NN - Noun; +NNS - Plural Noun. Morphological Feature
Tags in the Turkish Sentence: +A3pl - 3rd person plural;
+P3sg - 3rd person singular possessive; +Loc - Locative case.
Note that we mark an English plural noun as +NN NNS to in-
dicate that the root is a noun and there is a plural morpheme
on it. Note also that economic is also related to relations but
we are not interested in such content words and their rela-
tions.
2We use to prefix such syntactic tags on the English side.
3The order is important in that we would like to attach the
same sequence of function words in the same order so that
the resulting tags on the English side are the same.
455
(complex) tags on the English side encode local
(and sometimes, non-local) syntactic information.
Furthermore, we can see that before the transfor-
mations, the English side has 4 words, while af-
terwards it has only 2 words. We find (and elab-
orate later) that this reduction in the English side
of the training corpus, in general, is about 30%,
and is correlated with improved BLEU scores. We
believe the removal of many function words and
their folding into complex tags (which do not get
involved in GIZA++ alignment ? we only align the
root words) seems to improve alignment as there
are less number of ?words? to worry about during
that process.4
Another interesting side effect of this represen-
tation is the following. As the complex syntac-
tic tags on the English side are based on syntactic
relations and not necessarily positional proximity,
the tag for relations in a phrase like in their cul-
tural, historical and economic relations would be
exactly the same as above. Thus phrase extrac-
tion algorithms can conflate all constructs like in
their . . . economic relations as one phrase, regard-
less of the intervening modifiers, assuming that
parser does its job properly.
Not all cases can be captured as cleanly as the
example above, but most transformations capture
local and nonlocal syntax involving many function
words and then encode syntax with complex tags
resembling full morphological tags on the Turk-
ish side. These transformations, however, are not
meant to perform sentence level constituent re-
ordering on the English side. We explore these
later.
We developed set of about 20 linguistically-
motivated syntax-to-morphology transformations
which had variants parameterized depending on
what, for instance, the preposition or the adverbial
was, and how they map to morphological struc-
ture on the Turkish side. For instance, one general
rule handles cases like while . . . verb and if . . . verb
etc., mapping these to appropriate complex tags.
It is also possible that multiple transformations
can apply to generate a single English complex
tag: a portion of the tag can come from a verb
complex transformation, and another from an ad-
verbial phrase transformation involving a marked
such as while. Our transformations handle the fol-
lowing cases:
? Prepositions attach to the head-word of their
4Fraser (2009) uses the first four letters of German words
after morphological stripping and compound decomposition
to help with alignment in German to English and reverse
translation.
complement noun phrase as a component in
its complex tag.
? Possessive pronouns attach to the head-word
they specify.
? The possessive markers following a noun
(separated by the tokenizer) attached to the
noun.
? Auxiliary verbs and negation markers attach
to the lexical verb that they form a verb com-
plex with.
? Modals attach to the lexical verb they modify.
? Forms of be used as predicates with adjecti-
val or nominal dependents attach to the de-
pendent.
? Forms of be or have used to form passive
voice with past participle verbs, and forms of
be used with -ing verbs to form present con-
tinuous verbs, attach to the verb.
? Various adverbial clauses formed with if,
while, when, etc., are reorganized so that
these markers attach to the head verb of the
clause.
As stated earlier, these rules are linguistically mo-
tivated and are based on the morphological struc-
ture of the target language words. Hence for dif-
ferent target languages these rules will be differ-
ent. The rules recognize various local and nonlo-
cal syntactic structures in the source side parse tree
that correspond to complex morphological of tar-
get words and then remove source function words
folding them into complex tags. For instance, the
transformations in Figure 1 are handled by scripts
that process Malt Parser?s dependency structure
output and that essentially implement the follow-
ing sequence of rules expressed as pseudo code:
1) if (<Y>+PRP$ POS <Z>+NN<TAG>)
then {
APPEND <Y>+PRP$ TO <Z>+NN<TAG>
REMOVE <Y>+PRP$
}
2) if (<X>+IN PMOD <Z>+NN<TAG>)
then {
APPEND <X>+IN TO <Z>+NN<TAG>
REMOVE <X>+IN
}
Here <X>, <Y> and <Z> can be considered as Pro-
log like-variables that bind to patterns (mostly root
words), and the conditions check for specified de-
pendency relations (e.g., PMOD) between the left
and the right sides. When the condition is satis-
fied, then the part matching the function word is
removed and its syntactic information is appended
to form the complex tag on the noun (<TAG> would
either match null string or any previously ap-
pended function word markers.)5
5We outline two additional rules later when we see a more
complex example in Figure 2.
456
There are several other rules that handle more
mundane cases of date and time constructions (for
which, the part of the date construct which the
parser attaches a preposition, is usually different
than the part on the Turkish side that gets inflected
with case markers, and these have to be reconciled
by overriding the parser output.)
The next section presents an example of a sen-
tence with multiple transformations applied, after
discussing the preprocessing steps.
3 Experimental Setup and Results
3.1 Data Preparation
We worked on an English-Turkish parallel corpus
which consists of approximately 50K sentences
with an average of 23 words in English sentences
and 18 words in Turkish sentences. This is the
same parallel data that has been used in earlier
SMT work on Turkish (Durgar-El-Kahlout and
Oflazer, 2010). Let?s assume we have the follow-
ing pair of parallel sentences:
E: if a request is made orally the authority must
make a record of it
T: istek so?zlu? olarak yap?lm?s?sa yetkili makam bunu
kaydetmelidir
On the English side of the data, we use the Stan-
ford Log-Linear Tagger (Toutanova et al, 2003),
to tag the text with Penn Treebank Tagset. On
the Turkish side, we perform a full morphological
analysis, (Oflazer, 1994), and morphological dis-
ambiguation (Yuret and Tu?re, 2006) to select the
contextually salient interpretation of words. We
then remove any morphological features that are
not explicitly marked by an overt morpheme.6 So
for both sides we get,
E: if+IN a+DT request+NN is+VBZ made+VBN orally+RB
the+DT authority+NN must+MD make+VB a+DT record+NN
of+IN it+PRP
T: istek+Noun so?zlu?+Adj olarak+Verb+ByDoingSo
yap+Verb+Pass+Narr+Cond yetkili+Adj makam+Noun
bu+Pron+Acc kaydet+Verb+Neces+Cop
Finally we parse the English sentences using
MaltParser (Nivre et al, 2007), which gives us
labeled dependency parses. On the output of the
parser, we make one more transformation. We re-
place each word with its root, and possibly add an
additional tag for any inflectional information con-
veyed by overt morphemes or exceptional forms.
This is done by running the TreeTagger (Schmid,
1994) on the English side which provides the roots
in addition to the tags, and then carrying over this
information to the parser output. For example,
is is tagged as be+VB VBZ, made is tagged as
make+VB VBN, and a word like books is tagged
6For example, the morphological analyzer outputs +A3sg
to mark a singular noun, if there is no explicit plural mor-
pheme. Such markers are removed.
as book+NN NNS (and not as books+NNS). On
the Turkish side, each marker with a preceding
+ is a morphological feature. The first marker
is the part-of-speech tag of the root and the re-
mainder are the overt inflectional and derivational
markers of the word. For example, the analy-
sis kitap+Noun+P2pl+A3pl+Gen for a word
like kitap-lar-?n?z-?n7 (of your books)
represents the root kitap (book), a Noun, with
third person plural agreement A3pl, second per-
son plural possessive agreement, P2pl and geni-
tive case Gen.
The sentence representations in the middle part
of Figure 2 show these sentences with some of the
dependency relations (relevant to our transforma-
tions) extracted by the parser, explicitly marked as
labeled links. The representation at the bottom of
this figure (except for the co-indexation markers)
corresponds to the final transformed form of the
parallel training and test data. The co-indexation
is meant to show which root words on one side
map to which on the other side. Ultimately we
would want the alignment process to uncover the
root word alignments indicated here. We can also
note that the initial form of the English sentence
has 14 words and the final form after transforma-
tions, has 7 words (with complex tags).8
3.2 Experiments
We evaluated the impact of the transformations
in factored phrase-based SMT with an English-
Turkish data set which consists of 52712 parallel
sentences. In order to have more confidence in the
impact of our transformations, we randomly gen-
erated 10 training, test and tune set combinations.
For each combination, the latter two were 1000
sentences each and the remaining 50712 sentences
were used as training sets.9,10
We performed our experiments with the Moses
toolkit (Koehn et al, 2007). In order to encourage
long distance reordering in the decoder, we used
a distortion limit of -1 and a distortion weight of
7- shows surface morpheme boundaries.
8We could give two more examples of rules to process
the if-clause in the example in Figure 2. These rules would
be applied sequentially: The first rule recognizes the pas-
sive construction mediated by be+VB<AGR> forming a verb
complex (VC) with <Y>+VB_VBN and appends the former
to the complex tag on the latter and then deletes the former
token. The second rule then recognizes <X>+IN relating to
<Y>+VB<TAGS>with VMOD and appends the former to the
complex tag on the latter and then deletes the former token.
9The tune set was not used in this work but reserved for
future work so that meaningful comparisons could be made.
10It is possible that the 10 test sets are not mutually exclu-
sive.
457
Figure 2: An English-Turkish sentence pair with multiple transformations applied
0.1.11 We did not use MERT to further optimize
our model.12
For evaluation, we used the BLEU metric (Pap-
ineni et al, 2001). Each experiment was repeated
over the 10 data sets. Wherever meaningful, we
report the average BLEU scores over 10 data sets
along with the maximum and minimum values and
the standard deviation.
11These allow and do not penalize unlimited distortions.
12The experience with MERT for this language pair has
not been very positive. Earlier work on Turkish indicates that
starting with default Moses parameters and applying MERT
to the resulting model does not even come close to the per-
formance of the model with those two specific parameters set
as such (distortion limit -1 and distortion weight 0.1), most
likely because the default parameters do not encourage the
range of distortions that are needed to deal with the con-
stituent order differences. Earlier work on Turkish also shows
that even when the weight-d parameter is initialized with this
specific value, the space explored for distortion weight and
other parameters do not produce any improvements on the
test set, even though MERT claims there are improvements
on the tune set.
The other practical reasons for not using MERT were
the following: at the time we performed this work, the
discussion thread at http://www.mail-archive.
com/moses-support@mit.edu/msg01012.html
indicated that MERT was not tested on multiple factors.
The discussion thread at http://www.mail-archive.
com/moses-support@mit.edu/msg00262.html
claimed that MERT does not help very much with factored
models. With these observations, we opted not to experiment
with MERT with the multiple factor approach we employed,
given that it would be risky and time consuming to run
MERT needed for 10 different models and then not neces-
sarily see any (consistent) improvements. MERT however
is orthogonal to the improvements we achieve here and can
always be applied on top of the best model we get.
3.2.1 The Baseline Systems
As a baseline system, we built a standard phrase-
based system, using the surface forms of the words
without any transformations, and with a 3-gram
LM in the decoder. We also built a second baseline
system with a factored model. Instead of using just
the surface form of the word, we included the root,
part-of-speech and morphological tag information
into the corpus as additional factors alongside the
surface form.13 Thus, a token is represented with
three factors as Surface|Root|Tags where
Tags are complex tags on the English side, and
morphological tags on the Turkish side.14
Moses lets word alignment to align over any of
the factors. We aligned our training sets using only
the root factor to conflate statistics from different
forms of the same root. The rest of the factors are
then automatically assumed to be aligned, based
on the root alignment. Furthermore, in factored
models, we can employ different language models
for different factors. For the initial set of experi-
ments we used 3-gram LMs for all the factors.
For factored decoding, we employed a model
whereby we let the decoder translate a surface
form directly, but if/when that fails, the decoder
can back-off with a generation model that builds
a target word from independent translations of the
root and tags.
13In Moses, factors are separated by a ?|? symbol.
14Concatenating Root and Tags gives the Surface
form, in that the surface is unique given this concatenation.
458
The results of our baseline models are given in
top two rows of Table 1. As expected, the word-
based baseline performs worse than the factored
baseline. We believe that the use of multiple lan-
guage models (some much less sparse than the sur-
face LM) in the factored baseline is the main rea-
son for the improvement.
3.2.2 Applying Syntax-to-Morphology
Mapping Transformations
To gauge the effects of transformations separately,
we first performed them in batches on the En-
glish side. These batches were (i) transforma-
tions involving nouns and adjectives (Noun+Adj),
(ii) transformations involving verbs (Verb), (iii)
transformations involving adverbs (Adv), and
(iv) transformations involving verbs and adverbs
(Verb+Adv).
We also performed one set of transformations
on the Turkish side. In general, English preposi-
tions translate as case markers on Turkish nouns.
However, there are quite a number of lexical post-
positions in Turkish which also correspond to En-
glish prepositions. To normalize these with the
handling of case-markers, we treated these postpo-
sitions as if they were case-markers and attached
them to the immediately preceding noun, and then
aligned the resulting training data (PostP).15
The results of these experiments are presented
in Table 1. We can observe that the com-
bined syntax-to-morphology transformations on
the source side provide a substantial improvement
by themselves and a simple target side transfor-
mation on top of those provides a further boost
to 21.96 BLEU which represents a 28.57% rel-
ative improvement over the word-based baseline
and a 18.00% relative improvement over the fac-
tored baseline.
Experiment Ave. STD Max. Min.
Baseline 17.08 0.60 17.99 15.97
Factored Baseline 18.61 0.76 19.41 16.80
Noun+Adj 21.33 0.62 22.27 20.05
Verb 19.41 0.62 20.19 17.99
Adv 18.62 0.58 19.24 17.30
Verb+Adv 19.42 0.59 20.17 18.13
Noun+Adj 21.67 0.72 22.66 20.38
+Verb+Adv
Noun+Adj+Verb 21.96 0.72 22.91 20.67
+Adv+PostP
Table 1: BLEU scores for a variety of transforma-
tion combinations
We can see that every transformation improves
15Note than in this case, the translations would be gener-
ated in the same format, but we then split such postpositions
from the words they are attached to, during decoding, and
then evaluate the BLEU score.
the baseline system and the highest performance is
attained when all transformations are performed.
However when we take a closer look at the indi-
vidual transformations performed on English side,
we observe that not all of them have the same ef-
fect. While Noun+Adj transformations give us an
increase of 2.73 BLEU points, Verbs improve the
result by only 0.8 points and improvement with
Adverbs is even lower. To understand why we
get such a difference, we investigated the corre-
lation of the decrease in the number of tokens on
both sides of the parallel data, with the change in
BLEU scores. The graph in Figure 3 plots the
BLEU scores and the number of tokens in the two
sides of the training data as the data is modified
with transformations. We can see that as the num-
ber of tokens in English decrease, the BLEU score
increases. In order to measure the relationship
between these two variables statistically, we per-
formed a correlation analysis and found that there
is a strong negative correlation of -0.99 between
the BLEU score and the number of English tokens.
We can also note that the largest reduction in the
number of tokens comes with the application of
the Noun+Adj transformations, which correlates
with the largest increase in BLEU score.
It is also interesting to look at the n-gram pre-
cision components of the BLEU scores (again av-
eraged). In Table 2, we list these for words (ac-
tual BLEU), roots (BLEU-R) to see how effective
we are in getting the root words right, and mor-
phological tags, (BLEU-M), to see how effective
we are in getting just the morphosyntax right. It
1-gr. 2-gr. 3-gr. 4-gr.
BLEU 21.96 55.73 27.86 16.61 10.68
BLEU-R 27.63 68.60 35.49 21.08 13.47
BLEU-M 27.93 67.41 37.27 21.40 13.41
Table 2: Details of Word, Root and Morphology
BLEU Scores
seems we are getting almost 69% of the root words
and 68% of the morphological tags correct, but
not necessarily getting the combination equally as
good, since only about 56% of the full word forms
are correct. One possible way to address is to use
longer distance constraints on the morphological
tag factors, to see if we can select them better.
3.2.3 Experiments with higher-order
language models
Factored phrase-based SMT allows the use of mul-
tiple language models for the target side, for dif-
ferent factors during decoding. Since the number
of possible distinct morphological tags (the mor-
phological tag vocabulary size) in our training data
459
Figure 3: BLEU scores vs number of tokens in the training sets
(about 3700) is small compared to distinct num-
ber of surface forms (about 52K) and distinct roots
(about 15K including numbers), it makes sense to
investigate the contribution of higher order n-gram
language models for the morphological tag factor
on the target side, to see if we can address the ob-
servation in the previous section.
Using the data transformed with Noun+Adj-
+Verb+Adv+PostP transformations which previ-
ously gave us the best results overall, we experi-
mented with using higher order models (4-grams
to 9-grams) during decoding, for the morphologi-
cal tag factor models, keeping the surface and root
models at 3-gram. We observed that for all the 10
data sets, the improvements were consistent for up
to 8-gram. The BLEU with the 8-gram for only
the morphological tag factor averaged over the 10
data sets was 22.61 (max: 23.66, min: 21.37, std:
0.72) compared to the 21.96 in Table 1. Using a 4-
gram root LM, considerably less sparse than word
forms but more sparse that tags, we get a BLEU
score of 22.80 (max: 24.07, min: 21.57, std: 0.85).
The details of the various BLEU scores are shown
in the two halves of Table 3. It seems that larger
n-gram LMs contribute to the larger n-gram preci-
sions contributing to the BLEU but not to the uni-
gram precision.
3-gram root LM 1-gr. 2-gr. 3-gr. 4-gr.
BLEU 22.61 55.85 28.21 17.16 11.36
BLEU-R 28.21 68.67 35.80 21.55 14.07
BLEU-M 28.68 67.50 37.59 22.02 14.22
4-gram root LM 1-gr. 2-gr. 3-gr. 4-gr.
BLEU 22.80 55.85 28.39 17.34 11.54
BLEU-R 28.48 68.68 35.97 21.79 14.35
BLEU-M 28.82 67.49 37.63 22.17 14.40
Table 3: Details of Word, Root and Morphology
BLEU Scores, with 8-gram tag LM and 3/4-gram
root LMs
3.2.4 Augmenting the Training Data
In order to alleviate the lack of large scale parallel
corpora for the English?Turkish language pair, we
experimented with augmenting the training data
with reliable phrase pairs obtained from a previous
alignment. Phrase table entries for the surface fac-
tors produced by Moses after it does an alignment
on the roots, contain the English (e) and Turkish (t)
parts of a pair of aligned phrases, and the proba-
bilities, p(e|t), the conditional probability that the
English phrase is e given that the Turkish phrase
is t, and p(t|e), the conditional probability that
the Turkish phrase is t given the English phrase is
e. Among these phrase table entries, those with
p(e|t) ? p(t|e) and p(t|e) + p(e|t) larger than
some threshold, can be considered as reliable mu-
tual translations, in that they mostly translate to
each other and not much to others. We extracted
460
from the phrase table those phrases with 0.9 ?
p(e|t)/p(t|e) ? 1.1 and p(t|e) + p(e|t) ? 1.5
and added them to the training data to further bias
the alignment process. The resulting BLEU score
was 23.78 averaged over 10 data sets (max: 24.52,
min: 22.25, std: 0.71).16
4 Experiments with Constituent
Reordering
The transformations in the previous section do
not perform any constituent level reordering, but
rather eliminate certain English function words as
tokens in the text and fold them into complex syn-
tactic tags. That is, no transformations reorder
the English SVO order to Turkish SOV,17 for in-
stance, or move postnominal prepositional phrase
modifiers in English, to prenominal phrasal mod-
ifiers in Turkish. Now that we have the parses
of the English side, we have also investigated a
more comprehensive set of reordering transforma-
tions which perform the following constituent re-
orderings to bring English constituent order more
in line with the Turkish constitent order at the top
and embedded phrase levels:
? Object reordering (ObjR), in which the ob-
jects and their dependents are moved in front
of the verb.
? Adverbial phrase reordering (AdvR), which
involve moving post-verbal adverbial phrases
in front of the verb.
? Passive sentence agent reordering (PassAgR),
in which any post-verbal agents marked by
by, are moved in front of the verb.
? Subordinate clause reordering (SubCR)
which involve moving postnominal relative
clauses or prepositional phrase modifers in
front of any modifiers of the head noun.
Similarly any prepositional phrases attached
to verbs are moved to in front of the verb.
We performed these reorderings
on top of the data obtained with the
Noun+Adj+Verb+Adv+PostP transformations
earlier in Section 3.2.2 and used the same decoder
parameters. Table 4 shows the performance
obtained after various combination of reordering
operations over the 10 data sets. Although there
were some improvements for certain cases, none
16These experiments were done on top of the model in
3.2.3 with a 3-gram word and root LMs and 8-gram tag LM.
17Although Turkish is a free-constituent order language,
SOV is the dominant order in text.
of reordering gave consistent improvements for
all the data sets. A cursory examinations of
the alignments produced after these reordering
transformations indicated that the resulting root
alignments were not necessarily that close to
being monotonic as we would have expected.
Experiment Ave. STD Max. Min.
Baseline 21.96 0.72 22.91 20.67
ObjR 21.94 0.71 23.12 20.56
ObjR+AdvR 21.73 0.50 22.44 20.69
ObjR+PassAgR 21.88 0.73 23.03 20.51
ObjR+SubCR 21.88 0.61 22.77 20.92
Table 4: BLEU scores of after reordering transfor-
mations
5 Related Work
Statistical Machine Translation into a morpholog-
ically rich language is a challenging problem in
that, on the target side, the decoder needs to gen-
erate both the right sequence of constituents and
the right sequence of morphemes for each word.
Furthermore, since for such languages one can
generate tens of hundreds of inflected variants,
standard word-based alignment approaches suf-
fer from sparseness issues. Koehn (2005) applied
standard phrase-based SMT to Finnish using the
Europarl corpus and reported that translation to
Finnish had the worst BLEU scores.
Using morphology in statistical machine trans-
lation has been addressed by many researchers for
translation from or into morphologically rich(er)
languages. Niessen and Ney (2004) used mor-
phological decomposition to get better alignments.
Yang and Kirchhoff (2006) have used phrase-
based backoff models to translate unknown words
by morphologically decomposing the unknown
source words. Lee (2004) and Zolmann et al
(2006) have exploited morphology in Arabic-
English SMT. Popovic and Ney (2004) investi-
gated improving translation quality from inflected
languages by using stems, suffixes and part-of-
speech tags. Goldwater and McClosky (2005)
use morphological analysis on the Czech side to
get improvements in Czech-to-English statistical
machine translation. Minkov et al (2007) have
used morphological postprocessing on the target
side, to improve translation quality. Avramidis and
Koehn (2008) have annotated English with addi-
tional morphological information extracted from a
syntactic tree, and have used this in translation to
Greek and Czech. Recently, Bisazza and Federico
(2009) have applied morphological segmentation
in Turkish-to-English statistical machine transla-
tion and found that it provides nontrivial BLEU
461
score improvements.
In the context of translation from English to
Turkish, Durgar-El Kahlout and Oflazer (2010)
have explored different representational units of
the lexical morphemes and found that selectively
splitting morphemes on the target side provided
nontrivial improvement in the BLEU score. Their
approach was based on splitting the target Turk-
ish side, into constituent morphemes while our ap-
proach in this paper is the polar opposite: we do
not segment morphemes on the Turkish side but
rather join function words on the English side to
the related content words. Our approach is some-
what similar to recent approaches that use com-
plex syntactically-motivated complex tags. Birch
et al (2007) have integrated more syntax in a
factored translation approach by using CCG su-
pertags as a separate factor and have reported
a 0.46 BLEU point improvement in Dutch-to-
English translations. Although they used su-
pertags, these were obtained not via syntactic anal-
ysis but by supertagging, while we determine, on
the fly, the appropriate syntactic tags based on syn-
tactic structure. A similar approach based on su-
pertagging was proposed by Hassan et al (2007).
They used both CCG supertags and LTAG su-
pertags in Arabic-to-English phrase-based transla-
tion and have reported about 6% relative improve-
ment in BLEU scores. In the context of reorder-
ing, one recent work (Xu et al, 2009), was able
to get an improvement of 0.6 BLEU points by us-
ing source syntactic analysis and a constituent re-
ordering scheme like ours for English-to-Turkish
translation, but without using any morphology.
6 Conclusions
We have presented a novel way to incorporate
source syntactic structure in English-to-Turkish
phrase-based machine translation by parsing the
source sentences and then encoding many local
and nonlocal source syntactic structures as addi-
tional complex tag factors. Our goal was to ob-
tain representations of source syntactic structures
that parallel target morphological structures, and
enable us to extend factored translation, in appli-
cability, to languages with very disparate morpho-
logical structures.
In our experiments over a limited amount train-
ing data, but repeated with 10 different training
and test sets, we found that syntax-to-morphology
mapping transformations on the source side sen-
tences, along with a very small set of transforma-
tions on the target side, coupled with some ad-
ditional techniques provided about 39% relative
improvement in BLEU scores over a word-based
baseline and about 28% improvement of a factored
baseline. We also experimented with numerous
additional syntactic reordering transformation on
the source to further bring the constituent order in
line with the target order but found that these did
not provide any tangible improvements when av-
eraged over the 10 different data sets.
It is possible that the techniques presented in
this paper may be less effective if the available
data is much larger, but we have reasons to be-
lieve that they will still be effective then also. The
reduction in size of the source language side of
the training corpus seems to be definitely effective
and there no reason why such a reduction (if not
more) will not be observed in larger data. Also,
the preprocessing of English prepositional phrases
and many adverbial phrases usually involve rather
long distance relations in the source side syntactic
structure18 and when such structures are coded as
complex tags on the nominal or verbal heads, such
long distance syntax is effectively ?localized? and
thus can be better captured with the limited win-
dow size used for phrase extraction.
One limitation of the approach presented here
is that it is not directly applicable in the reverse
direction. The data encoding and set-up can di-
rectly be employed to generate English ?transla-
tion? expressed as a sequence of root and complex
tag combinations, but then some of the complex
tags could encode various syntactic constructs. To
finalize the translation after the decoding step, the
function words/tags in the complex tag would then
have to be unattached and their proper positions
in the sentence would have to be located. The
problem is essentially one of generating multiple
candidate sentences with the unattached function
words ambiguously positioned (say in a lattice)
and then use a second language model to rerank
these sentences to select the target sentence. This
is an avenue of research that we intend to look at
in the very near future.
Acknowledgements
We thank Joakim Nivre for providing us with the
parser. This publication was made possible by the
generous support of the Qatar Foundation through
Carnegie Mellon University?s Seed Research pro-
gram. The statements made herein are solely the
responsibility of the authors.
18For instance, consider the example in Figure 2 involving
if with some additional modifiers added to the intervening
noun phrase.
462
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statis-
tical machine translation. In Proceedings of ACL-
08/HLT, pages 763?770, Columbus, Ohio, June.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored translation models.
In Proceedings of SMT Workshop at the 45th ACL.
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological pre-processing for Turkish to English sta-
tistical machine translation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation, Tokyo, Japan, December.
I?lknur Durgar-El-Kahlout and Kemal Oflazer. 2010.
Exploiting morphology and local word reordering in
English to Turkish phrase-based statistical machine
translation. IEEE Transactions on Audio, Speech,
and Language Processing. To Appear.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 115?119, Athens,
Greece, March. Association for Computational Lin-
guistics.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In Proceedings of HLT/EMNLP-2005, pages
676?683, Vancouver, British Columbia, Canada,
October.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of the 45th ACL, pages 288?
295, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL-2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th ACL?demonstration
session, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit X.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In Proceedings of
HLT/NAACL-2004 ? Companion Volume, pages 57?
60.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th ACL, pages
128?135, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic information. Computational Lin-
guistics, 30(2):181?204.
Joakim Nivre, Hall Johan, Nilsson Jens, Chanev
Atanas, Gu?ls?en Eryig?it, Sandra Ku?bler, Marinov
Stetoslav, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering
Journal, 13(2):99?135.
Kemal Oflazer and I?lknur Durgar-El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of Statistical Machine Translation
Workshop at the 45th Annual Meeting of the
Association for Computational Linguistics, pages
25?32.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2):137?148.
Kemal Oflazer. 2008. Statistical machine translation
into a morphologically complex language. In Pro-
ceedings of the Conference on Intelligent Text Pro-
cessing and Computational Linguistics (CICLing),
pages 376?387.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th ACL, pages 311?318.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical ma-
chine translation. In Proceedings of the 4th LREC,
pages 1585?1588, May.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT/NAACL-2003, pages 252?
259.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings HLT/NAACL-2009, pages 245?253, June.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of EACL-2006,
pages 41?48.
463
Deniz Yuret and Ferhan Tu?re. 2006. Learning mor-
phological disambiguation rules for Turkish. In
Proceedings of HLT/NAACL-2006, pages 328?334,
New York City, USA, June.
Andreas Zollmann, Ashish Venugopal, and Stephan
Vogel. 2006. Bridging the inflection morphol-
ogy gap for Arabic statistical machine translation.
In Proceedings of HLT/NAACL-2006 ? Companion
Volume, pages 201?204, New York City, USA, June.
464
Proceedings of the ACL-HLT 2011 Student Session, pages 105?110,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Exploiting Morphology in Turkish Named Entity Recognition System
Reyyan Yeniterzi ?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
reyyan@cs.cmu.edu
Abstract
Turkish is an agglutinative language with
complex morphological structures, therefore
using only word forms is not enough for many
computational tasks. In this paper we an-
alyze the effect of morphology in a Named
Entity Recognition system for Turkish. We
start with the standard word-level representa-
tion and incrementally explore the effect of
capturing syntactic and contextual properties
of tokens. Furthermore, we also explore a new
representation in which roots and morphologi-
cal features are represented as separate tokens
instead of representing only words as tokens.
Using syntactic and contextual properties with
the new representation provide an 7.6% rela-
tive improvement over the baseline.
1 Introduction
One of the main tasks of information extraction is
the Named Entity Recognition (NER) which aims to
locate and classify the named entities of an unstruc-
tured text. State-of-the-art NER systems have been
produced for several languages, but despite all these
recent improvements, developing a NER system for
Turkish is still a challenging task due to the structure
of the language.
Turkish is a morphologically complex language
with very productive inflectional and derivational
processes. Many local and non-local syntactic struc-
tures are represented as morphemes which at the
?The author is also affiliated with iLab and the Center for the
Future of Work of Heinz College, Carnegie Mellon University
end produces Turkish words with complex morpho-
logical structures. For instance, the following En-
glish phrase ?if we are going to be able to make
[something] acquire flavor? which contains the nec-
essary function words to represent the meaning can
be translated into Turkish with only one token ?tat-
land?rabileceksek? which is produced from the root
?tat? (flavor) with additional morphemes +lan (ac-
quire), +d?r (to make), +abil (to be able), +ecek (are
going), +se (if) and +k (we).
This productive nature of the Turkish results in
production of thousands of words from a given root,
which cause data sparseness problems in model
training. In order to prevent this behavior in our
NER system, we propose several features which
capture the meaning and syntactic properties of the
token in addition to the contextual properties. We
also propose using a sequence of morphemes repre-
sentation which uses roots and morphological fea-
tures as tokens instead of words.
The rest of this paper is organized as follows:
Section 2 summarizes some previous related works,
Section 3 describes our approach, Section 4 details
the data sets used in the paper, Section 5 reports
the experiments and results and Section 6 concludes
with possible future work.
2 Related Work
The first paper (Cucerzan and Yarowski, 1999)
on Turkish NER describes a language independent
bootstrapping algorithm that learns from word inter-
nal and contextual information of entities. Turkish
was one of the five languages the authors experi-
mented with. In another work (Tur et al, 2003),
105
the authors followed a statistical approach (HMMs)
for NER task together with some other Information
Extraction related tasks. In order to deal with the
agglutinative structure of the Turkish, the authors
worked with the root-morpheme level of the word
instead of the surface form. A recent work (Ku?cu?k
and Yazici, 2009) presents the first rule-based NER
system for Turkish. The authors used several in-
formation sources such as dictionaries, list of well
known entities and context patterns.
Our work is different from these previous works
in terms of the approach. In this paper, we present
the first CRF-based NER system for Turkish. Fur-
thermore, all these systems used word-level tok-
enization but in this paper we present a new to-
kenization method which represents each root and
morphological feature as separate tokens.
3 Approach
In this work, we used two tokenization methods. Ini-
tially we started with the sequence of words rep-
resentation which will be referred as word-level
model. We also introduced morpheme-level model
in which morphological features are represented as
states. We used several features which were cre-
ated from deep and shallow analysis of the words.
During our experiments we used Conditional Ran-
dom Fields (CRF) which provides advantages over
HMMs and enables the use of any number of fea-
tures.
3.1 Word-Level Model
Word-level tokenization is very commonly used in
NER systems. In this model, each word is repre-
sented with one state. Since CRF can use any num-
ber of features to infer the hidden state, we develop
several feature sets which allow us to represent more
about the word.
3.1.1 Lexical Model
In this model, only the word tokens are used in
their surface form. This model is effective for many
languages which do not have complex morpholog-
ical structures. However for morphologically rich
languages, further analysis of words is required in
order to prevent data sparseness problems and pro-
duce more accurate NER systems.
3.1.2 Root Feature
An analysis (Hakkani-Tu?r, 2000) on English and
Turkish news articles with around 10 million words
showed that on the average 5 different Turkish word
forms are produced from the same root. In order to
decrease this high variation of words we use the root
forms of the words as an additional feature.
3.1.3 Part-of-Speech and Proper-Noun
Features
Named entities are mostly noun phrases, such as
first name and last name or organization name and
the type of organization. This property has been
used widely in NER systems as a hint to determine
the possible named entities.
Part-of-Speech tags of the words depend highly
on the language and the available Part-of-Speech
tagger. Taggers may distinguish the proper nouns
with or without their types. We used a Turkish mor-
phological analyzer (Of lazer, 1994) which analyzes
words into roots and morphological features. An ex-
ample to the output of the analyzer is given in Ta-
ble 1. The part-of-speech tag of each word is also
reported by the tool 1. We use these tags as addi-
tional features and call them part-of-speech (POS)
features.
The morphological analyzer has a proper name
database, which is used to tag Turkish person, lo-
cation and organization names as proper nouns. An
example name entity with this +Prop tag is given
in Table 1. Although, the use of this tag is limited
to the given database and not all named entities are
tagged with it, we use it as a feature to distinguish
named entities. This feature is referred as proper-
noun (Prop) feature.
3.1.4 Case Feature
As the last feature, we use the orthographic case
information of the words. The initial letter of most
named entities is in upper case, which makes case
feature a very common feature in NER tasks. We
also use this feature and mark each token as UC or
LC depending on the initial letter of it. We don?t do
1The meanings of various Part-of-Speech tags are as fol-
lows: +A3pl - 3rd person plural; +P3sg - 3rd person singular
possessive; +Gen - Genitive case; +Prop - Proper Noun; +A3sg
- 3rd person singular; +Pnon - No possesive agreement; +Nom
- Nominative case.
106
Table 1: Examples to the output of the Turkish morphological analyzer
WORD + ROOT + POS + MORPHEMES
beyinlerinin (of their brains) + beyin + Noun + A3pl+P3sg+Gen
Amerika (America) + Amerika + Noun + Prop+A3sg+Pnon+Nom
anything special for the first words in sentences.
An example phase in word-level model is given in
Table 2 2. In the figure each row represents a state.
The first column is the lexical form of the word and
the rest of the columns are the features and the tag is
in the last column.
3.2 Morpheme-Level Model
Using Part-of-Speech tags as features introduces
some syntactic properties of the word to the model,
but still there is missing information of other mor-
phological tags such as number/person agreements,
possessive agreements or cases. In order to see the
effect of these morphological tags in NER, we pro-
pose a morpheme-level tokenization method which
represents a word in several states; one state for a
root and one state for each morphological feature.
In a setting like this, the model has to be restricted
from assigning different labels to different parts of
the word. In order to do this, we use an additional
feature called root-morph feature. The root-morph
is a feature which is assigned the value ?root? for
states containing a root and the value ?morph? for
states containing a morpheme. Since there are no
prefixes in Turkish, a model trained with this feature
will give zero probability (or close to zero probabil-
ity if there is any smoothing) for assigning any B-*
(Begin any NE) tag to a morph state. Similarly, tran-
sition from a state with B-* or I-* (Inside any NE)
tag to a morph state with O (Other) tag will get zero
probability from the model.
In morpheme-level model, we use the following
features:
? the actual root of the word for root and mor-
phemes of the token
? the Part-of-speech tag of the word for the root
part and the morphological tag for the mor-
phemes
2One can see that Ilias which is Person NE is not tagged as
Prop (Proper Noun) in the example, mainly because it is missing
in the proper noun database of the morphological analyzer.
? the root-morph feature which assigns ?root? to
the roots and ?morph? to the morphemes
? the proper-noun feature
? the case feature
An example phrase in root-morpheme-based
chunking is given in Table 3. In the figure each row
represents a state and each word is represented with
several states. The first row of each word contains
the root, POS tag and Root value for the root-morph
feature. The rest of the rows of the same word con-
tains the morphemes and Morph value for the root-
morph feature.
4 Data Set
We used training set of the newspaper articles data
set that has been used in (Tur et al, 2003). Since we
do not have the test set they have used in their paper,
we had to come up with our own test set. We used
only 90% of the train data for training and left the
remaining for testing.
Three types of named entities; person, organiza-
tion and location, were tagged in this dataset. If the
word is not a proper name, then it is tagged with
other. The number of words and named entities for
each NE type from train and tests sets are given in
Table 4.
Table 4: The number of words and named entities in train
and test set
#WORDS #PER. #ORG. #LOC.
TRAIN 445,498 21,701 14,510 12,138
TEST 47,344 2,400 1,595 1,402
5 Experiments and Results
Before using our data in the experiments we applied
the Turkish morphological analyzer tool (Of lazer,
1994) and then used Morphological disambiguator
(Sak et al, 2008) in order to choose the correct mor-
phological analysis of the word depending on the
107
Table 2: An example phrase in word-level model with all features
LEXICAL ROOT POS PROP CASE TAG
Ayval?k Ayval?k Noun Prop UC B-LOCATION
dog?umlu dog?um (birth) Noun NotProp LC O
yazar yazar (author) Noun NotProp LC O
Ilias ilias Noun NotProp UC B-PERSON
Table 3: An example phrase in morpheme-level model with all features
ROOT POS ROOT-MORPH PROP CASE TAG
Ayval?k Noun Root Prop UC B-LOCATION
Ayval?k Prop Morph Prop UC I-LOCATION
Ayval?k A3sg Morph Prop UC I-LOCATION
Ayval?k Pnon Morph Prop UC I-LOCATION
Ayval?k Nom Morph Prop UC I-LOCATION
dog?um Noun Root NotProp LC O
dog?um Adj Morph NotProp LC O
dog?um With Morph NotProp LC O
yazar Noun Root NotProp LC O
yazar A3sg Morph NotProp LC O
yazar Pnon Morph NotProp LC O
yazar Nom Morph NotProp LC O
Ilias Noun Root NotProp UC B-PERSON
Ilias A3sg Morph NotProp UC I-PERSON
Ilias Pnon Morph NotProp UC I-PERSON
Ilias Nom Morph NotProp UC I-PERSON
context. In experiments, we used CRF++ 3, which
is an open source CRF sequence labeling toolkit and
we used the conlleval 4 evaluation script to report
F-measure, precision and recall values.
5.1 Word-level Model
In order to see the effects of the features individu-
ally, we inserted them to the model one by one it-
eratively and applied the model to the test set. The
F-measures of these models are given in Table 5. We
can observe that each feature is improving the per-
formance of the system. Overall the F-measure was
increased by 6 points when all the features are used.
5.2 Morpheme-level Model
In order to make a fair comparison between the
word-level and morpheme-level models, we used all
the features in both models. The results of these
experiments are given in Table 6. According to
the table, morpheme-level model achieved better re-
sults than word-level model in person and location
3CRF++: Yet Another CRF toolkit
4www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
entities. Even though word-level model got better
F-Measure score in organization entity, morpheme-
level is much better than word-level model in terms
of recall.
Using morpheme-level tokenization to introduce
morphological information to the model did not hurt
the system, but it also did not produce a signifi-
cant improvement. There may be several reasons for
this. One can be that morphological information is
not helpful in NER tasks. Morphemes in Turkish
words are giving the necessary syntactic meaning to
the word which may not be useful in named entity
finding. Another reason for not seeing a significant
change with morpheme usage can be our represen-
tation. Dividing the word into root and morphemes
and using them as separate tokens may not be the
best way of using morphemes in the model. Other
ways of representing morphemes in the model may
produce more effective results.
As mentioned in Section 4, we do not have the
same test set that has been used in Tur et al (Tur
et al, 2003). Even though it is impossible to make a
fair comparison between these two systems, it would
108
Table 5: F-measure Results of Word-level Model
PERSON ORGANIZATION LOCATION OVERALL
LEXICAL MODEL (LM) 80.88 77.05 88.40 82.60
LM + ROOT 83.32 80.00 90.30 84.96
LM + ROOT + POS 84.91 81.63 90.18 85.98
LM + ROOT + POS + PROP 86.82 82.66 90.52 87.18
LM + ROOT + POS + PROP + CASE 88.58 84.71 91.47 88.71
Table 6: Results of Morpheme-Level (Morp) and Word-Level Models (Word)
PRECISION RECALL F-MEASURE
MORP WORD MORP WORD MORP WORD
PERSON 91.87% 91.41% 86.92% 85.92% 89.32 88.58
ORGANIZATION 85.23% 91.00% 81.84% 79.23% 83.50 84.71
LOCATION 94.15% 92.83% 90.23% 90.14% 92.15 91.47
OVERALL 91.12% 91.81% 86.87% 85.81% 88.94 88.71
Table 7: F-measure Comparison of two systems
OURS (TUR ET AL., 2003)
BASELINE MODEL 82.60 86.01
BEST MODEL 88.94 91.56
IMPROVEMENT 7.6% 6.4%
be good to note how these systems performed with
respect to their baselines which is lexical model in
both. As it can be seen from Table 7, both models
improved upon their baselines significantly.
6 Conclusion and Future Work
In this paper, we explored the effects of using fea-
tures like root, POS tag, proper noun and case to the
performance of NER task. All these features seem to
improve the system significantly. We also explored
a new way of including morphological information
of words to the system by using several tokens for a
word. This method produced compatible results to
the regular word-level tokenization but did not pro-
duce a significant improvement.
As future work we are going to explore other ways
of representing morphemes in the model. Here we
represented morphemes as separate states, but in-
cluding them as features together with the root state
may produce better models. Another approach we
will also focus is dividing words into characters and
applying character-level models (Klein et al, 2003).
Acknowledgments
The author would like to thank William W. Cohen,
Kemal Of lazer, Go?khan Tur and Behrang Mohit for
their valuable feedback and helpful discussions. The
author also thank Kemal Of lazer for providing the
data set and the morphological analyzer. This publi-
cation was made possible by the generous support of
the iLab and the Center for the Future of Work. The
statements made herein are solely the responsibility
of the author.
References
Silviu Cucerzan and David Yarowski. 1999. Language
independent named entity recognition combining mor-
phological and contextual evidence. In Proceedings of
the Joint SIGDAT Conference on EMNLP and VLC,
pages 90?99.
Dilek Z. Hakkani-Tu?r. 2000. Statistical Language Mod-
elling for Turkish. Ph.D. thesis, Department of Com-
puter Engineering, Bilkent University.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003 - Volume 4, pages 180?183.
Dilek Ku?cu?k and Adnan Yazici. 2009. Named entity
recognition experiments on Turkish texts. In Proceed-
ings of the 8th International Conference on Flexible
Query Answering Systems, FQAS ?09, pages 524?535,
Berlin, Heidelberg. Springer-Verlag.
Kemal Of lazer. 1994. Two-level description of Turk-
109
ish morphology. Literary and Linguistic Computing,
9(2):137?148.
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2008.
Turkish language resources: Morphological parser,
morphological disambiguator and web corpus. In Ad-
vances in Natural Language Processing, volume 5221
of Lecture Notes in Computer Science, pages 417?427.
Go?khan Tur, Dilek Z. Hakkani-Tu?r, and Kemal Of lazer.
2003. A statistical information extraction system for
Turkish. In Natural Language Engineering, pages
181?210.
110
