Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 328?334,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning Morphological Disambiguation Rules for Turkish
Deniz Yuret
Dept. of Computer Engineering
Koc? University
Istanbul, Turkey
dyuret@ku.edu.tr
Ferhan Tu?re
Dept. of Computer Engineering
Koc? University
Istanbul, Turkey
fture@ku.edu.tr
Abstract
In this paper, we present a rule based
model for morphological disambiguation
of Turkish. The rules are generated by a
novel decision list learning algorithm us-
ing supervised training. Morphological
ambiguity (e.g. lives = live+s or life+s)
is a challenging problem for agglutinative
languages like Turkish where close to half
of the words in running text are morpho-
logically ambiguous. Furthermore, it is
possible for a word to take an unlimited
number of suffixes, therefore the number
of possible morphological tags is unlim-
ited. We attempted to cope with these
problems by training a separate model for
each of the 126 morphological features
recognized by the morphological analyzer.
The resulting decision lists independently
vote on each of the potential parses of a
word and the final parse is selected based
on our confidence on these votes. The
accuracy of our model (96%) is slightly
above the best previously reported results
which use statistical models. For compari-
son, when we train a single decision list on
full tags instead of using separate models
on each feature we get 91% accuracy.
1 Introduction
Morphological disambiguation is the task of select-
ing the correct morphological parse for a given word
in a given context. The possible parses of a word
are generated by a morphological analyzer. In Turk-
ish, close to half the words in running text are mor-
phologically ambiguous. Below is a typical word
?masal?? with three possible parses.
masal+Noun+A3sg+Pnon+Acc (= the story)
masal+Noun+A3sg+P3sg+Nom (= his story)
masa+Noun+A3sg+Pnon+Nom?DB+Adj+With
(= with tables)
Table 1: Three parses of the word ?masal??
The first two parses start with the same root,
masal (= story, fable), but the interpretation of the
following +? suffix is the Accusative marker in one
case, and third person possessive agreement in the
other. The third parse starts with a different root,
masa (= table) followed by a derivational suffix +l?
(= with) which turns the noun into an adjective. The
symbol ?DB represents a derivational boundary and
splits the parse into chunks called inflectional groups
(IGs).1
We will use the term feature to refer to individual
morphological features like +Acc and +With; the
term IG to refer to groups of features split by deriva-
tional boundaries (?DB), and the term tag to refer to
the sequence of IGs following the root.
Morphological disambiguation is a useful first
step for higher level analysis of any language but it
is especially critical for agglutinative languages like
Turkish, Czech, Hungarian, and Finnish. These lan-
guages have a relatively free constituent order, and
1See (Oflazer et al, 1999) for a detailed description of the
morphological features used in this paper.
328
syntactic relations are partly determined by morpho-
logical features. Many applications including syn-
tactic parsing, word sense disambiguation, text to
speech synthesis and spelling correction depend on
accurate analyses of words.
An important qualitative difference between part
of speech tagging in English and morphological dis-
ambiguation in an agglutinative language like Turk-
ish is the number of possible tags that can be as-
signed to a word. Typical English tag sets include
less than a hundred tag types representing syntac-
tic and morphological information. The number of
potential morphological tags in Turkish is theoret-
ically unlimited. We have observed more than ten
thousand tag types in our training corpus of a mil-
lion words. The high number of possible tags poses
a data sparseness challenge for the typical machine
learning approach, somewhat akin to what we ob-
serve in word sense disambiguation.
One way out of this dilemma could be to ignore
the detailed morphological structure of the word and
focus on determining only the major and minor parts
of speech. However (Oflazer et al, 1999) observes
that the modifier words in Turkish can have depen-
dencies to any one of the inflectional groups of a
derived word. For example, in ?mavi masal? oda? (=
the room with a blue table) the adjective ?mavi? (=
blue) modifies the noun root ?masa? (= table) even
though the final part of speech of ?masal?? is an ad-
jective. Therefore, the final part of speech and in-
flection of a word do not carry sufficient information
for the identification of the syntactic dependencies
it is involved in. One needs the full morphological
analysis.
Our approach to the data sparseness problem is
to consider each morphological feature separately.
Even though the number of potential tags is un-
limited, the number of morphological features is
small: The Turkish morphological analyzer we use
(Oflazer, 1994) produces tags that consist of 126
unique features. For each unique feature f , we take
the subset of the training data in which one of the
parses for each instance contain f . We then split this
subset into positive and negative examples depend-
ing on whether the correct parse contains the feature
f . These examples are used to learn rules using the
Greedy Prepend Algorithm (GPA), a novel decision
list learner.
To predict the tag of an unknown word, first the
morphological analyzer is used to generate all its
possible parses. The decision lists are then used to
predict the presence or absence of each of the fea-
tures contained in the candidate parses. The results
are probabilistically combined taking into account
the accuracy of each decision list to select the best
parse. The resulting tagging accuracy is 96% on a
hand tagged test set.
A more direct approach would be to train a single
decision list using the full tags as the target classifi-
cation. Given a word in context, such a decision list
assigns a complete morphological tag instead of pre-
dicting individual morphological features. As such,
it does not need the output of a morphological ana-
lyzer and should be considered a tagger rather than
a disambiguator. For comparison, such a decision
list was built, and its accuracy was determined to be
91% on the same test set.
The main reason we chose to work with decision
lists and the GPA algorithm is their robustness to ir-
relevant or redundant features. The input to the deci-
sion lists include the suffixes of all possible lengths
and character type information within a five word
window. Each instance ends up with 40 attributes on
average which are highly redundant and mostly irrel-
evant. GPA is able to sort out the relevant features
automatically and build a fairly accurate model. Our
experiments with Naive Bayes resulted in a signif-
icantly worse performance. Typical statistical ap-
proaches include the tags of the previous words as
inputs in the model. GPA was able to deliver good
performance without using the previous tags as in-
puts, because it was able to extract equivalent infor-
mation implicit in the surface attributes. Finally, un-
like most statistical approaches, the resulting models
of GPA are human readable and open to interpreta-
tion as Section 3.1 illustrates.
The next section will review related work. Sec-
tion 3 introduces decision lists and the GPA training
algorithm. Section 4 presents the experiments and
the results.
2 Related Work
There is a large body of work on morphological dis-
ambiguation and part of speech tagging using a va-
riety of rule-based and statistical approaches. In the
329
rule-based approach a large number of hand crafted
rules are used to select the correct morphological
parse or POS tag of a given word in a given context
(Karlsson et al, 1995; Oflazer and Tu?r, 1997). In
the statistical approach a hand tagged corpus is used
to train a probabilistic model which is then used to
select the best tags in unseen text (Church, 1988;
Hakkani-Tu?r et al, 2002). Examples of statisti-
cal and machine learning approaches that have been
used for tagging include transformation based learn-
ing (Brill, 1995), memory based learning (Daele-
mans et al, 1996), and maximum entropy models
(Ratnaparkhi, 1996). It is also possible to train sta-
tistical models using unlabeled data with the ex-
pectation maximization algorithm (Cutting et al,
1992). Van Halteren (1999) gives a comprehensive
overview of syntactic word-class tagging.
Previous work on morphological disambiguation
of inflectional or agglutinative languages include
unsupervised learning for of Hebrew (Levinger
et al, 1995), maximum entropy modeling for Czech
(Hajic? and Hladka?, 1998), combination of statistical
and rule-based disambiguation methods for Basque
(Ezeiza et al, 1998), transformation based tagging
for Hungarian (Megyesi, 1999).
Early work on Turkish used a constraint-based ap-
proach with hand crafted rules (Oflazer and Kuruo?z,
1994). A purely statistical morphological disam-
biguation model was recently introduced (Hakkani-
Tu?r et al, 2002). To counter the data sparseness
problem the morphological parses are split across
their derivational boundaries and certain indepen-
dence assumptions are made in the prediction of
each inflectional group.
A combination of three ideas makes our approach
unique in the field: (1) the use of decision lists and
a novel learning algorithm that combine the statis-
tical and rule based techniques, (2) the treatment of
each individual feature separately to address the data
sparseness problem, and (3) the lack of dependence
on previous tags and relying on surface attributes
alone.
3 Decision Lists
We introduce a new method for morphological dis-
ambiguation based on decision lists. A decision list
is an ordered list of rules where each rule consists
of a pattern and a classification (Rivest, 1987). In
our application the pattern specifies the surface at-
tributes of the words surrounding the target such as
suffixes and character types (e.g. upper vs. lower
case, use of punctuation, digits). The classification
indicates the presence or absence of a morphological
feature for the center word.
3.1 A Sample Decision List
We will explain the rules and their patterns using the
sample decision list in Table 2 trained to identify the
feature +Det (determiner).
Rule Class Pattern
1 1 W=?c?ok R1=+DA
2 1 L1=?pek
3 0 W=+AzI
4 0 W=?c?ok
5 1 ?
Table 2: A five rule decision list for +Det
The value in the class column is 1 if word W
should have a +Det feature and 0 otherwise. The
pattern column describes the required attributes of
the words surrounding the target word for the rule
to match. The last (default) rule has no pattern,
matches every instance, and assigns them +Det.
This default rule captures the behavior of the ma-
jority of the training instances which had +Det in
their correct parse. Rule 4 indicates a common
exception: the frequently used word ?c?ok? (mean-
ing very) should not be assigned +Det by default:
?c?ok? can be also used as an adjective, an adverb,
or a postposition. Rule 1 introduces an exception to
rule 4: if the right neighbor R1 ends with the suffix
+DA (the locative suffix) then ?c?ok? should receive
+Det. The meanings of various symbols in the pat-
terns are described below.
When the decision list is applied to a window of
words, the rules are tried in the order from the most
specific (rule 1) to the most general (rule 5). The first
rule that matches is used to predict the classification
of the center word. The last rule acts as a catch-all;
if none of the other rules have matched, this rule as-
signs the instance a default classification. For exam-
ple, the five rule decision list given above classifies
the middle word in ?pek c?ok alanda? (matches rule
330
W target word A [ae]
L1, L2 left neighbors I [?iuu?]
R1, R2 right neighbors D [dt]
== exact match B [bp]
=? case insensitive match C [cc?]
=+ is a suffix of K [kgg?]
Table 3: Symbols used in the rule patterns. Capital
letters on the right represent character groups useful
in identifying phonetic variations of certain suffixes,
e.g. the locative suffix +DA can surface as +de, +da,
+te, or +ta depending on the root word ending.
1) and ?pek c?ok insan? (matches rule 2) as +Det,
but ?insan c?ok daha? (matches rule 4) as not +Det.
One way to interpret a decision list is as a se-
quence of if-then-else constructs familiar from pro-
gramming languages. Another way is to see the last
rule as the default classification, the previous rule as
specifying a set of exceptions to the default, the rule
before that as specifying exceptions to these excep-
tions and so on.
3.2 The Greedy Prepend Algorithm (GPA)
To learn a decision list from a given set of training
examples the general approach is to start with a de-
fault rule or an empty decision list and keep adding
the best rule to cover the unclassified or misclassi-
fied examples. The new rules can be added to the
end of the list (Clark and Niblett, 1989), the front of
the list (Webb and Brkic, 1993), or other positions
(Newlands and Webb, 2004). Other design decisions
include the criteria used to select the ?best rule? and
how to search for it.
The Greedy Prepend Algorithm (GPA) is a variant
of the PREPEND algorithm (Webb and Brkic, 1993).
It starts with a default rule that matches all instances
and classifies them using the most common class in
the training data. Then it keeps prepending the rule
with the maximum gain to the front of the grow-
ing decision list until no further improvement can be
made. The algorithm can be described as follows:
GPA(data)
1 dlist ? NIL
2 default -class ? MOST-COMMON-CLASS(data)
3 rule ? [if TRUE then default -class ]
4 while GAIN(rule , dlist , data) > 0
5 do dlist ? prepend(rule , dlist)
6 rule ? MAX-GAIN-RULE(dlist , data)
7 return dlist
The gain of a candidate rule in GPA is defined
as the increase in the number of correctly classified
instances in the training set as a result of prepend-
ing the rule to the existing decision list. This is
in contrast with the original PREPEND algorithm
which uses the less direct Laplace preference func-
tion (Webb and Brkic, 1993; Clark and Boswell,
1991).
To find the next rule with the maximum gain, GPA
uses a heuristic search algorithm. Candidate rules
are generated by adding a single new attribute to the
pattern of each rule already in the decision list. The
candidate with the maximum gain is prepended to
the decision list and the process is repeated until no
more positive gain rules can be found. Note that if
the best possible rule has more than one extra at-
tribute compared to the existing rules in the decision
list, a suboptimal rule will be selected. The origi-
nal PREPEND uses an admissible search algorithm,
OPUS, which is guaranteed to find the best possible
candidate (Webb, 1995), but we found OPUS to be
too slow to be practical for a problem of this scale.
We picked GPA for the morphological disam-
biguation problem because we find it to be fast and
fairly robust to the existence of irrelevant or redun-
dant attributes. The average training instance has
40 attributes describing the suffixes of all possible
lengths and character type information in a five word
window. Most of this information is redundant or
irrelevant to the problem at hand. The number of
distinct attributes is on the order of the number of
distinct word-forms in the training set. Nevertheless
GPA is able to process a million training instances
for each of the 126 unique morphological features
and produce a model with state of the art accuracy
in about two hours on a regular desktop PC.2
2Pentium 4 CPU 2.40GHz
331
4 Experiments and Results
In this section we present the details of the data,
the training and testing procedures, the surface at-
tributes used, and the accuracy results.
4.1 Training Data
documents 2383
sentences 50673
tokens 948404
parses 1.76 per token
IGs 1.33 per parse
features 3.29 per IG
unique tokens 111467
unique tags 11084
unique IGs 2440
unique features 126
ambiguous tokens 399223 (42.1%)
Table 4: Statistics for the training data
Our training data consists of about 1 million
words of semi-automatically disambiguated Turkish
news text. For each one of the 126 unique morpho-
logical features, we used the subset of the training
data in which instances have the given feature in at
least one of their generated parses. We then split this
subset into positive and negative examples depend-
ing on whether the correct parse contains the given
feature. A decision list specific to that feature is cre-
ated using GPA based on these examples.
Some relevant statistics for the training data are
given in Table 4.
4.2 Input Attributes
Once the training data is selected for a particular
morphological feature, each instance is represented
by surface attributes of five words centered around
the target word. We have tried larger window sizes
but no significant improvement was observed. The
attributes computed for each word in the window
consist of the following:
1. The exact word string (e.g. W==Ali?nin)
2. The lowercase version (e.g. W=?ali?nin) Note:
all digits are replaced by 0?s at this stage.
3. All suffixes of the lowercase version (e.g.
W=+n, W=+In, W=+nIn, W=+?nIn, etc.) Note:
certain characters are replaced with capital let-
ters representing character groups mentioned in
Table 3. These groups help the algorithm rec-
ognize different forms of a suffix created by the
phonetic rules of Turkish: for example the loca-
tive suffix +DA can surface as +de, +da, +te, or
+ta depending on the ending of the root word.
4. Attributes indicating the types of characters at
various positions of the word (e.g. Ali?nin
would be described with W=UPPER-FIRST,
W=LOWER-MID, W=APOS-MID, W=LOWER-
LAST)
Each training instance is represented by 40 at-
tributes on average. The GPA procedure is responsi-
ble for picking the attributes that are relevant to the
decision. No dictionary information is required or
used, therefore the models are fairly robust to un-
known words. One potentially useful source of at-
tributes is the tags assigned to previous words which
we plan to experiment with in future work.
4.3 The Decision Lists
At the conclusion of the training, 126 decision lists
are produced of the form given in Table 2. The num-
ber of rules in each decision list range from 1 to
6145. The longer decision lists are typically for part
of speech features, e.g. distinguishing nouns from
adjectives, and contain rules specific to lexical items.
The average number of rules is 266. To get an esti-
mate on the accuracy of each decision list, we split
the one million word data into training, validation,
and test portions using the ratio 4:1:1. The train-
ing set accuracy of the decision lists is consistently
above 98%. The test set accuracies of the 126 deci-
sion lists range from 80% to 100% with the average
at 95%. Table 5 gives the six worst features with test
set accuracy below 89%; these are the most difficult
to disambiguate.
4.4 Correct Tag Selection
To evaluate the candidate tags, we need to combine
the results of the decision lists. We assume that the
presence or absence of each feature is an indepen-
dent event with a probability determined by the test
set accuracy of the corresponding decision list. For
example, if the +P3pl decision list predicts YES,
we assume that the +P3pl feature is present with
332
87.89% +Acquire To acquire (noun)
86.18% +PCIns Postposition subcat.
85.11% +Fut Future tense
84.08% +P3pl 3. plural possessive
80.79% +Neces Must
79.81% +Become To become (noun)
Table 5: The six features with the worst test set ac-
curacy.
probability 0.8408 (See Table 5). If the +Fut deci-
sion list predicts NO, we assume the +Fut feature is
present with probability 1 ? 0.8511 = 0.1489. To
avoid zero probabilities we cap the test set accura-
cies at 99%.
Each candidate tag indicates the presence of cer-
tain features and the absence of others. The prob-
ability of the tag being correct under our indepen-
dence assumption is the product of the probabilities
for the presence and absence of each of the 126 fea-
tures as determined by our decision lists. For effi-
ciency, one can neglect the features that are absent
from all the candidate tags because their contribu-
tion will not effect the comparison.
4.5 Results
The final evaluation of the model was performed on
a test data set of 958 instances. The possible parses
for each instance were generated by the morpholog-
ical analyzer and the correct one was picked manu-
ally. 40% of the instances were ambiguous, which
on the average had 3.9 parses. The disambiguation
accuracy of our model was 95.82%. The 95% confi-
dence interval for the accuracy is [0.9457, 0.9708].
An analysis of the mistakes in the test data show
that at least some of them are due to incorrect tags
in our training data. The training data was semi-
automatically generated and thus contained some er-
rors. Based on hand evaluation of the differences be-
tween the training data tags and the GPA generated
tags, we estimate the accuracy of the training data to
be below 95%. We ran two further experiments to
see if we could improve on the initial results.
In our first experiment we used our original model
to re-tag the training data. The re-tagged training
data was used to construct a new model. The result-
ing accuracy on the test set increased to 96.03%, not
a statistically significant improvement.
In our second experiment we used only unam-
biguous instances for training. Decision list training
requires negative examples, so we selected random
unambiguous instances for positive and negative ex-
amples for each feature. The accuracy of the result-
ing model on the test set was 82.57%. The problem
with selecting unambiguous instances is that certain
common disambiguation decisions are never repre-
sented during training. More careful selection of
negative examples and a sophisticated bootstrapping
mechanism may still make this approach workable.
Finally, we decided to see if our decision lists
could be used for tagging rather than disambigua-
tion, i.e. given a word in a context decide on the full
tag without the help of a morphological analyzer.
Even though the number of possible tags is unlim-
ited, the most frequent 1000 tags cover about 99%
of the instances. A single decision list trained with
the full tags was able to achieve 91.23% accuracy
using 10000 rules. This is a promising result and
will be explored further in future work.
5 Contributions
We have presented an automated approach to learn
morphological disambiguation rules for Turkish us-
ing a novel decision list induction algorithm, GPA.
The only input to the rules are the surface attributes
of a five word window. The approach can be gener-
alized to other agglutinative languages which share
the common challenge of a large number of poten-
tial tags. Our approach for resolving the data sparse-
ness problem caused by the large number of tags is
to generate a separate model for each morphologi-
cal feature. The predictions for individual features
are probabilistically combined based on the accu-
racy of each model to select the best tag. We were
able to achieve an accuracy around 96% using this
approach.
Acknowledgments
We would like to thank Kemal Oflazer of Sabanc?
University for providing us with the Turkish mor-
phological analyzer, training and testing data for dis-
ambiguation, and valuable feedback.
333
References
Brill, E. (1995). Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Church, K. W. (1988). A stochastic parts program and
noun phrase parser for unrestricted text. In Proceed-
ings of the Second Conference on Applied Natural
Language Processing, pages 136?143.
Clark, P. and Boswell, R. (1991). Rule induction with
CN2: Some recent improvements. In Kodratoff,
Y., editor, Machine Learning ? Proceedings of the
Fifth European Conference (EWSL-91), pages 151?
163, Berlin. Springer-Verlag.
Clark, P. and Niblett, T. (1989). The CN2 induction al-
gorithm. Machine Learning, 3:261?283.
Cutting, D., Kupiec, J., Pedersen, J., and Sibun, P. (1992).
A practical part-of-speech tagger. In Proceedings of
the 3rd Conference on Applied Language Processing,
pages 133?140.
Daelemans, W. et al (1996). MBT: A memory-based
part of speech tagger-generator. In Ejerhead, E. and
Dagan, I., editors, Proceedings of the Fourth Workshop
on Very Large Corpora, pages 14?27.
Ezeiza, N. et al (1998). Combining stochastic and rule-
based methods for disambiguation in agglutinative lan-
guages. In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics (COL-
ING/ACL98), pages 379?384.
Hajic?, J. and Hladka?, B. (1998). Tagging inflective lan-
guages: Prediction of morphological categories for a
rich, structured tagset. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL98), pages 483?490, Mon-
treal, Canada.
Hakkani-Tu?r, D. Z., Oflazer, K., and Tu?r, G. (2002).
Statistical morphological disambiguation for aggluti-
native languages. Computers and the Humanities,
36:381?410.
Karlsson, F., Voutialinen, A., Heikkila?, J., and Anttila, A.
(1995). Constraint Grammar - A Language Indepen-
dent System for Parsing Unrestricted Text. Mouton de
Gruyter.
Levinger, M., Ornan, U., and Itai, A. (1995). Learning
morpho-lexical probabilities from an untagged corpus
with an application to hebrew. Computational Lin-
guistics, 21(3):383?404.
Megyesi, B. (1999). Improving brill?s pos tagger for an
agglutinative language. In Pascale, F. and Joe, Z., ed-
itors, Proceedings of the Joing SIGDAT Conference
on Empirical Methods in Natural Language and Very
Large Corpora, pages 275?284, College Park, Mary-
land, USA.
Newlands, D. and Webb, G. I. (2004). Alternative strate-
gies for decision list construction. In Proceedings of
the Fourth Data Mining Conference (DM IV 03), pages
265?273.
Oflazer, K. (1994). Two-level description of turkish
morphology. Literary and Linguistic Computing,
9(2):137?148.
Oflazer, K., Hakkani-Tu?r, D. Z., and Tu?r, G. (1999).
Design for a turkish treebank. In Proceedings of
the Workshop on Linguistically Interpreted Corpora,
EACL 99, Bergen, Norway.
Oflazer, K. and Kuruo?z, I?. (1994). Tagging and morpho-
logical disambiguation of turkish text. In Proceedings
of the 4th Applied Natural Language Processing Con-
ference, pages 144?149. ACL.
Oflazer, K. and Tu?r, G. (1997). Morphological disam-
biguation by voting constraints. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL97, EACL97), Madrid, Spain.
Ratnaparkhi, A. (1996). A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Rivest, R. L. (1987). Learning decision lists. Machine
Learning, 2:229?246.
van Halteren, H., editor (1999). Syntactic Wordclass Tag-
ging. Text, Speech and Language Technology. Kluwer
Academic Publishers.
Webb, G. I. (1995). Opus: An efficient admissible algo-
rithm for unordered search. JAIR, 3:431?465.
Webb, G. I. and Brkic, N. (1993). Learning decision lists
by prepending inferred rules. In Proceedings of the AI
93 Workshop on Machine Learning and Hybrid Sys-
tems, pages 6?10, Melbourne.
334
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589?599,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning to Translate: A Query-Specific Combination Approach for
Cross-Lingual Information Retrieval
Ferhan Ture
Raytheon BBN Technologies
10 Moulton St
Cambridge, MA, 02138 USA
fture@bbn.com
Elizabeth Boschee
Raytheon BBN Technologies
10 Moulton St
Cambridge, MA, 02138 USA
eboschee@bbn.com
Abstract
When documents and queries are pre-
sented in different languages, the com-
mon approach is to translate the query into
the document language. While there are
a variety of query translation approaches,
recent research suggests that combining
multiple methods into a single ?structured
query? is the most effective. In this pa-
per, we introduce a novel approach for
producing a unique combination recipe for
each query, as it has also been shown
that the optimal combination weights dif-
fer substantially across queries and other
task specifics. Our query-specific combi-
nation method generates statistically sig-
nificant improvements over other combi-
nation strategies presented in the litera-
ture, such as uniform and task-specific
weighting. An in-depth empirical anal-
ysis presents insights about the effect of
data size, domain differences, labeling and
tuning on the end performance of our ap-
proach.
1 Introduction
Cross-lingual information retrieval (CLIR) is a
special case of information retrieval (IR) in which
documents and queries are presented in different
languages. In order to overcome the language
barrier, the most commonly adopted method is
to translate queries into the document language.
Many methods have been introduced for translat-
ing queries for CLIR, ranging from word-by-word
dictionary lookups (Xu and Weischedel, 2005;
Darwish and Oard, 2003) to sophisticated use of
machine translation (MT) systems (Magdy and
Jones, 2011; Ma et al., 2012). Previous research
has shown that combining evidence from differ-
ent translation approaches is superior to any sin-
gle query translation method (Braschler, 2004;
Herbert et al., 2011). While there are numer-
ous combination-of-evidence techniques for both
mono-lingual and cross-lingual IR, recent work
suggests that there is no one-size-fits-all solution.
In fact, the optimal combination weights (i.e.,
weights assigned to each piece of evidence in a
linear combination) differ greatly across queries,
tasks, languages, and other variants (Ture et al.,
2012; Berger and Savoy, 2007).
In this paper, we introduce a novel method for
learning optimal combination weights when build-
ing a linear combination of existing query transla-
tion approaches. From standard query-document
relevance judgments we train a set of classifiers,
which produce a unique combination recipe for
each query, based on a large set of features ex-
tracted from the query and collection. Experi-
mental results show that the effectiveness of our
method is significantly higher than state-of-the-art
query translation methods and other combination
strategies.
2 Related Work
The earliest approaches to query translation for
CLIR used machine-readable bilingual dictio-
naries (Hull and Grefenstette, 1996; Balles-
teros and Croft, 1996), achieving around up to
60% of monolingual IR effectiveness. Xu and
Weischedel (2005) showed that effectiveness can
be increased to around 80% by weighting each
translation proportional to its rank in the dictio-
nary. The practice of weighting translation candi-
dates was later formulated as a ?structured query?,
in which each query term is represented by a prob-
ability distribution over its translations in the doc-
ument language (Pirkola, 1998; Kwok, 1999; Dar-
wish and Oard, 2003). Our approach is based on
the structured query formulation.
Some of the earliest studies in IR discovered
that with different underlying models, the re-
trieved document set would vary substantially, al-
589
though the effectiveness was similar (McGill et
al., 1979). Later studies showed that combining
different representations of the query and/or doc-
ument often produced superior output (Rajashekar
and Croft, 1995; Turtle and Croft, 1990; Fox,
1983). This intuitive idea was supported theoret-
ically by Pearl (1988), concluding that multiple
pieces of evidence estimates relevance more accu-
rately, but that the benefit strongly depends on the
quality and independence of each piece. Experi-
ments by Belkin et al. (1995) indicated the need to
properly weight each representation with respect
to its effectiveness. These so-called ?combination-
of-evidence? techniques became more powerful
with the introduction of Indri, a probabilistic re-
trieval framework specifically designed for com-
bining multiple query and document representa-
tions (Metzler and Croft, 2005). Croft (2000) pro-
vides a detailed summary of earlier query combi-
nation approaches in IR, while Peters et al. (2012)
cites more recent related work.
The benefits of combination-of-evidence trans-
fer to the cross-lingual case especially well, since
the inherent ambiguity of translation readily pro-
vides a diverse set of representations. Most CLIR
approaches implement a post-retrieval merging
of ranked lists, each generated from different
query (Hiemstra et al., 2001; Savoy, 2001; Gey
et al., 2001; Chen and Gey, 2004) or docu-
ment (Lopez and Romary, 2009) representations,
also called ?data fusion?. In contrast, we focus on
a pre-retrieval combination at the modeling stage,
so that a single complex query is used in retrieval,
instead of multiple simpler ones. Two advantages
of the former are easier implementation (since
the approach requires no changes to the modeling
side) and the possibly greater diversity that can be
achieved by having separate retrieval runs. How-
ever, each ranked list needs to be limited in size,
which might cause some potentially useful docu-
ments not to be considered in the combination at
all. Since the focus of this paper is on the model-
ing end of retrieval, pre-retrieval combination was
a more suitable choice, though we think that the
two approaches have complementary benefits.
The idea of combining query translations
before retrieval has been explored previously.
Braschler (2004) combines three translation ap-
proaches: output of an MT system, a novel trans-
lation approach based on a similarity thesaurus
built automatically from a comparable corpus,
and a dictionary-based translation. The main
reason that this combination does not provide
much benefit is due to the lower coverage of
the thesaurus-based and dictionary-based trans-
lation methods. A similar approach by Herbert
et al. (2011) uses Wikipedia to provide transla-
tions of certain phrases and entities, and combin-
ing that with the Google Translate MT sys-
tem yields statistically significant improvements
in English-to-German retrieval. More recently,
Ture et al. (2012) presented a more sophisti-
cated translation approach using the internal rep-
resentation of an MT system, and reported sta-
tistically significant improvements when a pre-
retrieval combination was performed.
All of the previously cited approaches either
use uniform weights for combination, or select
weights based on collection-level information.
However, as stated previously, numerous stud-
ies suggest that certain methods work better on
certain queries, collections, languages. In fact,
when weights are optimized separately on each
collection, they differ substantially across differ-
ent collections (Ture et al., 2012). For monolin-
gual retrieval, there has been a series of learning-
to-rank (LTR) papers that determine weights for
query concepts (Bendersky et al., 2011), such
that retrieval effectiveness is maximized. A re-
cent study extends this idea to the cross-lingual
case, by learning how to weight each translated
word for English-Persian CLIR (Azarbonyad et
al., 2013). In contrast, we extract translated word
weights from diverse and sophisticated translation
methods, then learn how to weight each trans-
lated structured query, We call this ?learning-to-
translate? (LTT), which can be formulated as a
simpler learning problem. In CLIR, both LTR and
LTT are under-explored problems, with a common
goal of applying machine learning techniques to
improve query translation, yet with complemen-
tary benefits.
To our knowledge, there has been one prior LTT
approach: a classifier was trained to predict ef-
fectiveness of each query translation, using fea-
tures based on statistics of the query terms (Berger
and Savoy, 2007). Instead of weighting, the
translations with highest classifier scores were
concatenated, yielding statistically significant im-
provements over using the single-best translation
method. However, the translation methods ex-
plored in this paper are all based on one-best MT
590
systems, making it difficult to draw strong conclu-
sions.
3 Query Translation
The primary contribution of this paper is to show
how a diverse set of query translation (QT) meth-
ods can be combined effectively into a single
weighted structured query, with improved retrieval
effectiveness. While our approach can applied to
any set of translation methods, we focus on three
methods that have complementary strengths and
that have shown promise in CLIR: word-based
probabilistic translation, one-best MT, and n-best
probabilistic MT. We briefly present our imple-
mentation of each method; more details can be
found in earlier work (Darwish and Oard, 2003;
Ture et al., 2012).
Each QT method generates a representation of
the query in the document language. In the case of
word-based and n-best MT approaches, the repre-
sentation is a structured query itself, where each
query word is represented by a probability distri-
bution over translation alternatives. For one-best
MT, the query is represented by a bag of translated
words.
3.1 One-Best MT
A query translation approach that has become
more popular recently is to simply run the query
through an MT system, and use the best output as
the query:
t
1
t
2
. . . t
l
= MT(s
1
s
2
. . . s
k
) (1)
where s = s
1
s
2
. . . s
k
is the query and t =
t
1
t
2
. . . t
l
is the translated query.
Since modern statistical MT systems generate
high-quality translations for many language pairs,
this one-best strategy works reasonably well for
retrieval and provides a competitive baseline. A
practical advantage of this approach is the ease of
implementation ? one can simply use any MT in-
terface (e.g., Google Translate) as a black
box in their CLIR system.
3.2 Probabilistic n-best MT
The top translation might sometimes be incorrect,
or might lack some of the alternative representa-
tions that are very useful in retrieval. Therefore,
considering the n highest scored translations (also
referred to as the n-best list in MT literature) has
become increasingly popular in CLIR approaches.
In order to benefit from the diversity amongst
the n-best translations, one can simply concate-
nate them together, forming a large list of query
terms. However, statistical MT systems also
assign probabilities to each translation, which
can be incorporated into the query representation
for better effectiveness, as suggested by Ture et
al. (2012).
In this approach, each of the top n transla-
tion candidates from the MT system are processed
one by one. For each translation candidate, the
MT system provides a translation probability, and
alignments between words in the query and its
translation. As we process each of the n transla-
tions, for each query word s
i
, we accumulate prob-
abilities on each translated word t
ij
aligned to s
i
.
Finally, we normalize the translation probabilities
to get Pr
nbest
(t
ij
|s
i
).
3.3 Word-based
One of the most widely used approaches in CLIR
is based on translating each query word s
i
in-
dependently, with probabilities assigned to each
translation candidate t
ij
. Translations are de-
rived automatically from a bilingual corpus using
statistical word alignment techniques, which are
used as part of the training of statistical MT sys-
tems (Brown et al., 1993). These probabilities can
be exploited for retrieval based on the technique
of Darwish and Oard (2003) for ?projecting? text
into the document language. After cleaning up the
automatically learned translation probabilities (de-
tails omitted for space considerations), we end up
with the translation probabilities Pr
word
(t
ij
|s
i
).
4 Combination of Evidence
Once we have multiple ways to represent the query
q in the document language (QT
i
(q), i = 1 . . .m),
it is possible to combine these ?pieces of evi-
dence? into a single representation as follows:
QT(q) =
m
?
i=1
w
i
(q)QT
i
(q)
and each combination-of-evidence approach dif-
fers by how the combination weights w
i
are com-
puted:
Uniform In this baseline method, we ignore any
information we have about the collection or query
and assign equal weights to each method (i.e.,
w
i
(q) = 1/m). In our case, this means a weight
591
of 33.3% to each of the one-best, probabilistic n-
best, and word-based QT methods.
Task-specific We can optimize the combination
weights by overall effectiveness on a specific re-
trieval task. Given a query set and collection,
we perform a grid search on combination weights
(with a step interval of 0.1) and select the weights
that maximize retrieval effectiveness. The training
is performed in a leave-one-out manner: weights
for test query q are optimized on all queries except
for q.
Query-specific We propose a novel method to
compute combination weights specifically for
each query, resulting in a more customized op-
timization that can take into account how effec-
tiveness of each translation method varies across
queries.
In the remainder of this section, we describe
the details of our novel query-specific combina-
tion method.
4.1 Overview of Query-Specific Combination
We present a novel approach for determining
query-specific combination weights by training a
classifier for each QT method. Prior to train-
ing the classifier, we first run retrieval using each
QT method, and evaluate the effectiveness of the
retrieved documents. The effectiveness of the
i
th
method on query q (i.e., f
i
(q)) is then con-
verted into a binary label (further described in
Section 4.2). Treating each query as a separate
instance, a classifier is trained for each method,
generating classifiers C
1
, . . . , C
m
. During re-
trieval (i.e., at test time), for each query q, each
trained classifier C
i
is applied to the query, re-
sulting in a predicted label l
i
(q) and the classi-
fier?s confidence in a positive label, C
i
(q).
1
These
values are then used to determine combination
weights w
1
(q), . . . , w
m
(q) that are custom-fit for
the query.
4.2 Labeling
First of all, we discard queries in which the dif-
ference between the best and worst performing
methods is small (specifically, the worst perform-
ing method scores at least k
1
% of the best per-
forming one). For such queries, generating fair
training labels is more difficult and therefore more
1
The confidence in a negative label is 1? C
i
(q).
likely to introduce noise into the process.
2
More-
over, these are exactly the queries where choos-
ing optimal combination weights is less important
(since all methods perform relatively similarly), so
it is reasonable to exclude them from training. In
fact, a high number of such queries would indi-
cate lower potential for combination-of-evidence
approaches.
For each QT method i, we create training in-
stances per query, per retrieval task. Since our
goal is to select the best among existing methods,
the training label should reflect the effectiveness
of method i relative to other methods. A strategy
that we call best-by-measure assigns a label of 1
if the effectiveness of the i
th
method (i.e., f
i
(q)) is
at least k
2
% of the maximum effectiveness for that
query, and 0 otherwise. While this directly corre-
lates with retrieval effectiveness, labels might be
distributed in an unbalanced manner, which might
affect the training process negatively. A balanced
labeling requires sorting all training instances by
how much better the i
th
method is than other meth-
ods (max
i
?
6=i
(f
i
?
(q)/f
i
(q))), and then assigning a
label of 1 to the lower half and 0 to the higher half.
This strategy is called best-by-rank.
4.3 Features
We introduce a diverse set of features, in order to
train a robust classifier for predicting when each
QT method performs better and worse than others.
We split the feature set into four meaningful cate-
gories, so that we can measure the impact of each
subset separately:
Surface features These features do not require
a deep analysis of the query: (a) Number of words
in query and the translated query, (b) Type of
query that we automatically classify based on pre-
defined templates (e.g., fact question, cause-effect,
etc.), and (c) Number of stop words in the query
and the translated query.
Parse-based features These features are ex-
tracted from a deeper syntactic analysis of the
query text: (a) Number of related names found in
a named entity database, and (b) Existence of syn-
tactic constituents in query and its translation (e.g.,
?is there a VVB in the query parse tree?).
2
We also experimented with including these queries with
a third label (e.g., ?same?) and train a ternary classifier. Hav-
ing more labels requires more training data, which is not easy
to obtain for this task. Also, obtaining a balanced label dis-
tribution becomes even more difficult with three labels.
592
Translation-based features These features
consist of statistics computed from the query and
its translation: (a) Number of query words that
were unaligned in at least half of the n-best query
translations, (b) Number of query words that were
aligned to multiple target words in at least half
of the n-best query translations, (c) Number of
query words that were self-aligned (i.e., target
word is exactly same string) in at least half of the
n-best query translations, (d) Average / Standard
deviation / Maximum / Minimum of entropy of
Pr
nbest
of each query word, and (e) Average /
Standard deviation / Maximum / Minimum of
entropy of Pr
word
of each query word.
Index-based features These features are based
on frequency statistics from a representative col-
lection:
3
(a) Average / Standard deviation / Max-
imum / Minimum of document frequency (df) of
query words and their translations, (b) Average /
Standard deviation / Maximum / Minimum of term
frequency averaged across query words and their
translations, and (c) Sum / Maximum / Minimum
of total probability assigned to words that do not
appear in the collection (df = 0).
Additionally, the target language is a default
feature in all of our experiments. For each clas-
sification task, we train a separate classifier on
each subset of these four feature categories, so that
there are 16 different sets (including the empty
set). After we select which categories to pull fea-
tures from, we optionally perform feature selec-
tion to reduce the number of features by a pre-
defined percentage.
In our experimentation, we observed that
collection-based features were most useful for
classifying the one-best method, whereas parse-
based features were most discriminative for prob-
abilistic 10-best. For the word-based QT method,
the translation-based features were most effective
in our experiments. We further analyze the effect
of various features in Section 5.
4.4 Training and Tuning Classifiers
The scikit-learn package was used for the
training pipeline (Pedregosa et al., 2012). Using
an established toolkit allowed us to experiment
with many options for classification, such as the
learner type (support vector machine, maximum
entropy, decision tree), feature set (16 subsets of
3
We used the BOLT collection in our experiments.
the four categories described earlier) and two fea-
ture selection methods (recursive elimination or
selection based on univariate statistical tests). In
the end, we get 96 different parameter combina-
tions while training a classifier for a particular QT
method, resulting in the need for tuning ? picking
the parameters that produce highest accuracy on a
representative tuning set.
Given that we have a set of queries for testing
purposes, there are few strategies for selecting a
training and tuning set. One approach is to apply a
leave-one-out strategy, so that a classifier is trained
and tuned on all but one of the test queries, and
then applied on the remaining query to predict its
label. We call this the fully-open setting.
In a more realistic scenario, there will not be
relevance judgments for the test queries, yet there
might be a small amount of labeled data similar to
the test task (e.g., different queries on same col-
lection) that can be utilized for tuning purposes,
and a larger set of training queries from different
collections. We call this the half-blind setting.
If testing in a new domain, queries of similar
type are not available for training and tuning pur-
poses. This is a more challenging scenario than
the previous two, yet it is important for real-world
applications. In order to demonstrate the effec-
tiveness of the training pipeline in this case, we
hold out test queries entirely, then train and tune
on queries from a completely different task (i.e.,
different queries and collection). We call this the
fully-blind setting.
4.5 Retrieval
Once we have classifiers trained for all QT meth-
ods, we can apply them to a given query on-the-fly,
and compute query-specific combination weights.
One approach is hard weighting, putting all weight
onto a single method ? when there are more than
one methods classified with label 1, we can ei-
ther pick one randomly or use the classifier con-
fidence value as a tie-breaker. An alternative is
soft weighting, where the weight of the i
th
method
can be computed either using classifier confidence
C
i
(i.e., how confident the model is that the i
th
method will perform well), precision on tuning set
precision
i
(i.e., how precise the model is at its pre-
593
dictions for the i
th
method), or both:
w
s1
i
(q) =C
i
(q)
w
s2
i
(q) =precision
i
(1)? l
i
(q)
+(1?precision
i
(0))? (1? l
i
(q))
w
s3
i
(q) =precision
i
(1)? C
i
(q)
+(1?precision
i
(0))? (1? C
i
(q))
The intuition behind all of these weighting
schemes is to produce a weight for each QT
method, by taking into account the confidence of
the classifier, and/or the precision of the classifier
on tuning instances.
The computed weights are normalized before
constructing the final query for retrieval:
w
final
i
(q) = w
i
(q)/
?
m
j=1
w
j
(q)
When compared empirically, we noticed that
soft weighting is more effective than hard weight-
ing, as the latter is more sensitive to classifier er-
rors. Among the three soft weighting functions,
differences were mostly negligible in our exper-
iments. Hence, we decided to use the simplest
weighting function w
s1
.
4.6 Analytical Model
It is time-consuming to implement various
combination-of-evidence approaches and run re-
trieval experiments. Therefore, it is useful to have
an analytical model of the process that can pro-
vide a rough estimate of how fruitful it would be
to spend this effort, given certain details about the
task. The model we present in this section esti-
mates the effectiveness of combining QT methods
1 . . .m on a query set Q, given (1) the effective-
ness of each method on Q and (2) error rate of
binary classifiers C
1
. . . C
m
on Q. Using this for-
mulation, one can assess the benefit of combina-
tion without running retrieval, based only on er-
ror rates ? this saves precious time during de-
velopment. Moreover, even without trained clas-
sifiers, this model can be used to estimate poten-
tial benefits by plugging in hypothetical error val-
ues. In other words, one can ask the question ?If
I had classifiers with x% error on this query set,
what would be the benefit of using these classi-
fiers to combine QT methods?? before developing
any combination approach at all.
The analytical model considers a special case of
weighted combination: for each query q, we pick a
single QT method i = 1 . . .m, for which the clas-
sifier predicts a label of 1. If there are more than
one such method, one of them is picked randomly.
This simplified version allows us to compute ex-
pected effectiveness for q as follows:
E[f(q)] =
?
method i
Pr(pick i|q)f
i
(q)
While f
i
(q) is an observed value (the effective-
ness of the i
th
method on query q), Pr(pick i|q)
needs to be estimated (the probability of selecting
the i
th
method). Since this depends on the pre-
dicted labels, we consider all possible scenarios
l = l
1
l
2
. . . l
m
, where each value is the prediction
of a classifier. For instance, ?l=010? means that
classifiers C
1
and C
3
predicted a label of 0, while
C
2
predicted a positive label. Marginalizing over
the 2
m
possible scenarios gives us the following
estimate:
Pr(pick i|q)
=
?
?
1
?
l
1
=0
. . .
1
?
l
m
=0
Pr(l|q)
?
?
? Pr(pick i|l, q)
=
?
?
1
?
l
1
=0
. . .
1
?
l
m
=0
m
?
i=1
Pr(l
i
|q)
?
?
? Pr(i|l, q)
In the final step, we assumed that classifiers make
predictions independent of each other, which is
a desired property for successful combination.
Pr(l
i
|q) can be estimated using classifier error
statistics:
Pr(l
i
|q) ?
count(predicted = l
i
, true = l
q
)
count(true = l
q
)
where l
q
is the true label of q. If l
i
= l
q
, this ex-
pression becomes the true positive or true negative
rate, depending on the value. Similarly, if l
i
6= l
q
,
it is either the false positive or false negative rate.
Finally, the probability that the i
th
method is se-
lected in a particular scenario depends solely on
the predicted labels, since it is a random selection:
Pr(pick i|l) = l
i
/
?
m
j=1
l
j
This concludes the derivation of the analytical
model of query evidence combination, which we
use in Section 5.1 to evaluate the effectiveness of
labeling approaches.
5 Evaluation
We evaluated our approach on four different CLIR
tasks: TREC 2002 English-Arabic CLIR, NTCIR-
8 English-Chinese Advanced Cross-Lingual Infor-
594
mation Access (ACLIA), and two forum post re-
trieval tasks as part of the DARPA Broad Oper-
ational Language Technologies (BOLT) program:
English-Arabic (BOLT
ar
) and English-Chinese
(BOLT
ch
). The query language is English in all
cases, and we preprocess the queries using BBN?s
information extraction toolkit SERIF (Ramshaw et
al., 2011). State-of-the-art English-Arabic (En-
Ar) and English-Chinese (En-Ch) MT systems
were trained on parallel corpora released in NIST
OpenMT 2012, in addition to parallel forum data
collected as part of the BOLT program (10m En-
Ar words; 30m En-Ch words). From these data,
word alignments were learned with GIZA++ (Och
and Ney, 2003), using five iterations of each of
IBM Models 1?4 and HMM.
3-gram Chinese and 5-gram Arabic Kneser-Ney
language models were trained from the Gigaword
corpus (1b words each) and non-English side of
the training corpus. Chinese and English parallel
text were preprocessed through the Treebank Tok-
enizer,
4
while no special treatment was performed
on Arabic.
For retrieval, we used Indri, a state-of-the-
art probabilistic relevance model that supports
weighted query representations through operators
#combine and #weight (Metzler and Croft,
2005). A character-based index was built for
Chinese collections, whereas Arabic text was
stemmed using Lucene before indexing.
5
En-
glish text was preprocessed by Indri?s imple-
mentation of the Porter stemmer (Porter, 1997).
Statistics for each collection and query set are
summarized in Table 1.
Before performing any combination, we first
ran the three baseline QT methods individually
and evaluated the retrieved documents. Mean
average precision (MAP) was used to measure
retrieval effectiveness, which is a widely used
and stable metric, estimating the area under the
precision-recall curve. We set n = 10 for the
n-best probabilistic translation method. Baseline
scores are reported in Table 2. The average preci-
sion (AP) of each query in these tasks was used to
label the query and construct training data accord-
ingly.
In subsequent sections, we evaluate the effect of
several variants in the training pipeline.
4
http://www.cis.upenn.edu/?treebank
5
http://lucene.apache.org
5.1 Effect of Labeling
In Section 4.2, we introduced two ways to label
instances. In our evaluation, we set the free pa-
rameters k
1
= k
2
= 90, which filters out 33% of
queries from the training set of the BOLT
ar
task;
this percentage is 29% in BOLT
ch
, 44% in TREC,
and 27% in NTCIR.
Labeling determines which query translation
method is considered effective or not, which con-
sequently determines what the ?learning problem?
is (since the objective of the classifier is to sep-
arate differently labeled instances). As a result,
there are two dimensions to consider when com-
paring labeling strategies. One is the accuracy of
the classifiers on held-out data, and the other is
how well the trained classifier reflects this accu-
racy when used in retrieval. To clarify the dis-
tinction, consider a case where every instance is
labeled 1. This generates a trivial learning prob-
lem with no test errors, yet this does not entail that
using these classifiers in retrieval will be more ef-
fective than other labeling strategies. If, even with
high classifier accuracy, the retrieval effectiveness
is low, that indicates a bad choice for labeling.
We can theoretically analyze how suitable each
labeling method is by applying the analytical
model to each CLIR task, setting parameters based
on a perfect classifier: true positive/negative rate
of 1 and false positive/negative rate of 0 (see Sec-
tion 4.6). Table 2 shows these results in the ?Per-
fect? column, since these scores represent what
could be achieved if classifiers were trained to pre-
dict labels perfectly (no training or retrieval is ac-
tually performed). There are two values in each
row of the ?Perfect? column, one for each labeling
strategy. In each row, we found these two values to
be statistically significantly higher than any of the
baseline scores. This shows that both labeling ap-
proaches have the potential to improve effective-
ness significantly.
We also made an empirical comparison of the
two labeling approaches by actually training clas-
sifiers with each labeling, and then using the clas-
sifiers to combine query translations in retrieval.
The ?Trained? column in Table 2 shows the MAP
we get on each CLIR task (and average classifier
accuracies), using either labeling.
6
Based on these results, we conclude that best-
6
For a fair comparison, we fixed the train-tune setting to
fully-open, trained classifiers on the test collection and re-
ported leave-one-out accuracies.
595
Lang
Collection
Topics
MT Training data
Source Size (docs) Source (domain) Size (words)
Arabic TREC-02 383,872 50 OpenMT-12 (news/web)
10m
Arabic BOLT 12,258,904 45 BOLT (forum)
Chinese NTCIR-8 388,589 100 OpenMT-12 (news/web)
30m
Chinese BOLT 6,693,951 45 BOLT (forum)
Table 1: Summary of the CLIR tasks in our evaluations.
Task
Baseline Perfect Trained
one-best ten-best word measure rank measure rank
BOLT
ar
0.296 0.311 0.318 0.341 0.341 0.342 (74) 0.330 (72)
BOLT
ch
0.370 0.406 0.407 0.458 0.462 0.438 (68) 0.426 (60)
TREC 0.292 0.298 0.301 0.327 0.330 0.305 (59) 0.316 (59)
NTCIR 0.146 0.152 0.141 0.180 0.177 0.163 (56) 0.162 (61)
Table 2: Retrieval effectiveness of baseline QT methods is presented on the left side, and a comparison of
labeling strategies is provided on the right side. All numbers represent MAP values, except for classifier
accuracy shown in percentage values (in parantheses). Analytically computed values are shown in italics.
by-measure labeling is more useful in practice,
supported by typically higher accuracy and effec-
tiveness. Best-by-rank yields better results only
on TREC, but a closer look reveals that the in-
crease in MAP is due to only two outlier queries.
For BOLT
ar
, on the other hand, retrieval with best-
by-measure labeling is more effective (statistically
significant) than best-by-rank; hence, the former is
used in remaining parts of our evaluation.
5.2 Effect of Train-Tune Setting
In Section 4.4, we introduced three major train-
tune settings: fully-open, half-blind, and fully-
blind. In order to implement these settings, we
treat each of the three query sets (BOLT, TREC,
NTCIR) as a separate training dataset and experi-
ment with a variety of combinations.
For simplicity, let us demonstrate the variety of
experiments assuming the test collection is BOLT.
For the fully-open case, the default training data is
all of the BOLT queries (this training set is referred
to as b). Additionally, one can include queries
from TREC (referred to as t) and NTCIR (referred
to as n) into the training data. This gives us four
different training datasets for the fully-open case:
b, b + n, b + t, b + t + n. Similarly, each of the
half-blind and fully-blind settings can be applied
to three different training sets: For BOLT, these
are t, n, t + n.
7
This results in ten different ex-
periments run for each task ? in each experiment,
7
In the case of half-blind, b is split into two: 20% is used
for tuning and the remainder is used for testing.
we train a classifier for each QT method, select the
best meta-parameters on the tuning set, and then
compute combination weights for retrieval using
the classifiers.
Each cell on the left side of Table 3 (under col-
umn ?Query-specific Combination?) shows the re-
sults of the most effective experiment for a partic-
ular task and train-tune setting. Accuracy values
for classifiers varied widely across these experi-
ments. Still, even when accuracies dropped close
to or below 50% (i.e. random baseline), combined
retrieval was always more effective than any single
QT approach, which emphasizes the robustness of
our approach. For instance, in the fully-blind set-
ting for the NTCIR task, the individual classifiers
had accuracies of only 56%, 49%, and 44% but
MAP was 0.163, which is higher than the MAP of
any individual method for that collection (0.146,
0.152, or 0.141).
Another key observation in Table 3 is that
the domain effect (i.e., training and/or tuning on
queries similar to test queries) is only noticeable
on the two BOLT tasks. For NTCIR and TREC,
we do not observe a boost in MAP when queries
from the same task are included in training (i.e.,
fully-open setting). This can be explained by the
BOLT-centric nature of our system components:
the text analysis tool and MT systems are tuned
mainly for forum data, and the collection-based
features are extracted from BOLT. Due to this bias,
BOLT queries were most useful in our experi-
ments, supported by the fact that BOLT is always
596
Task
Query-specific Combination
Uniform
Task-
Max
fully-open half-blind fully-blind specific
BOLT
ar
0.342
??
b 0.330 t+n 0.329 t+n 0.324
12
0.329
1
0.346
BOLT
ch
0.438
??
b 0.428 n 0.426 t+n 0.422
1
0.431
1
0.466
TREC 0.321 b+t 0.324
??
b+n 0.321 b+n 0.314
1
0.318
1
0.332
NTCIR 0.164
?
b+n 0.163 b+t 0.163 b 0.162
13
0.162
13
0.182
Table 3: A comparison of query combination approaches. For query-specific combination, MAP and
training data are shown for the most effective experiment of each train-tune setting. For each task, the
highest MAP achieved with our approach is shown in bold. Superscripts 1, 2, and 3 indicate statisti-
cally significant improvements over baseline methods one-best, probabilistic 10-best, and word-based,
whereas * indicates improvements over all three. Superscript ? indicates results significantly better than
uniform and task-specific combination methods.
included in the train set when testing on TREC or
NTCIR (see lowest two rows in Table 3). Also,
when there is no domain effect (i.e., half-blind and
fully-blind ), more data yields higher effectiveness
in 6 out of 8 cases (see two right columns on the
left side of Table 3).
5.3 Retrieval Effectiveness
In this section, we compare our novel query-
specific combination-of-evidence approach to the
baseline CLIR approaches, as well as comparable
combination methods (uniform and task-specific
combination) in terms of retrieval effectiveness.
Based on a randomized significance test (Smucker
et al., 2007), the best query-specific combina-
tion method (shown in boldface in Table 3) out-
performs all baseline QT methods in all tasks
with 95% confidence (indicated by superscript *
in Table 3). This is not the case for uniform or
task-specific query combination, which are statis-
tically indistinguishable from at least one of the
QT methods, depending on the task (indicated by
superscripts 1, 2, and 3 for one-best, probabilis-
tic 10-best, and word-based QT methods, respec-
tively). When we directly compare our query-
specific combination approach to other combina-
tion methods, the differences are statistically sig-
nificant for all tasks but NTCIR (indicated by su-
perscript ?).
For reference, we also computed effectiveness
for a hypothetical system (denoted by ?Max? in
Table 3) that could select the best QT method for
each query and use only that for retrieval. This is
not a strict upper bound, since correctly weight-
ing each method can produce better results, but
it is still a reasonable target for effectiveness. In
our experiments, Arabic retrieval runs were very
close to this target with our combination approach,
while the gap for Chinese is still substantial, which
is worth further exploration.
6 Conclusions and Future Work
In this paper, we introduced a novel combination-
of-evidence approach for CLIR, which learns a
custom combination recipe for each query. We for-
mulate this as a set of binary classification prob-
lems, and show that trained classifiers can be used
to produce query-specific combination weights ef-
fectively. Our deep exploration of many variants
(e.g., labeling, training-tuning, weight computa-
tion, analytical formulation) and extensive empiri-
cal analysis on four different tasks provide insights
for future research on the under-studied problem
of combining translations for CLIR.
Our approach advances the state of the art of
CLIR, yielding higher effectiveness than three ad-
vanced query translation approaches, all based
on state-of-the-art MT systems. Furthermore, on
three of the four tasks, our combination strategy is
statistically significantly better than two compara-
ble combination techniques. Experimental results
also suggest that even a uniform combination of
query translations is consistently better than any
individual method. While it is known that com-
bining translations helps CLIR, we confirm this on
a set of modern CLIR tasks, including two target
languages and a variety of text domains.
Having a simple linear learning problem allows
us to train robust models with relatively simpler
features. Nevertheless, we are interested in ex-
perimenting with more sophisticated learning ap-
proaches. In terms of non-linear classifiers, our
experience with decision trees in this paper indi-
cated a higher tendency to overfit. In terms of
597
combining queries in a non-linear fashion, our fu-
ture plans include integrating our approach into a
LTR framework, and directly optimize MAP. This
will also allow us to explore more complex fea-
tures extracted from query and document text, as
well as external sources.
Another possible future endeavor is to extend
these ideas to (i) other query translation ap-
proaches and (ii) document translation. While the
exact same problem can be formulated for learning
to translate documents effectively, a more compli-
cated infrastructure and longer running times are
two challenges that need to be considered.
Finally, we hope this to be a significant step to-
wards more context-dependent and robust CLIR
models, by taking advantage of modern translation
technologies, as well as machine learning tech-
niques.
Acknowledgments
This work was supported by DARPA/I2O Con-
tract No. HR0011-12-C-0014 under the BOLT
program (Approved for Public Release, Distribu-
tion Unlimited). The views, opinions, and/or find-
ings contained in this article are those of the author
and should not be interpreted as representing the
official views or policies, either expressed or im-
plied, of the Defense Advanced Research Projects
Agency or the Department of Defense.
References
Hosein Azarbonyad, Azadeh Shakery, and Heshaam
Faili. 2013. Exploiting multiple translation re-
sources for english-persian cross language informa-
tion retrieval. In Proceedings of the Cross-Language
Evaluation Forum on Cross-Language Information
Retrieval and Evaluation, CLEF ?13, pages 93?99.
Lisa Ballesteros and W. Bruce Croft. 1996. Dictionary
methods for cross-lingual information retrieval. In
Proceedings of the 7th International DEXA Confer-
ence on Database and Expert Systems Applications,
pages 791?801.
Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information
retrieval. Information Processing & Management,
31(3):431?448, May.
Michael Bendersky, Donald Metzler, and W. Bruce
Croft. 2011. Parameterized concept weighting in
verbose queries. In Proceedings of the 34th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?11,
pages 605?614, New York, NY, USA. ACM.
Pierre-Yves Berger and Jacques Savoy. 2007. Se-
lecting automatically the best query translations. In
Large Scale Semantic Access to Content (Text, Im-
age, Video, and Sound), RIAO ?07, pages 287?300,
Paris, France, France. Le Centre de Hautes Etudes
Internationales D?Informatique Documentaire.
Martin Braschler. 2004. Combination approaches for
multilingual text retrieval. Information Retrieval,
7(1-2):183?204, January.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311.
Aitao Chen and Fredric C. Gey. 2004. Multilingual in-
formation retrieval using machine translation, rele-
vance feedback and decompounding. Inf. Retr., 7(1-
2):149?182, January.
W. Bruce Croft. 2000. Combining approaches to in-
formation retrieval. In W. Bruce Croft, editor, Ad-
vances in Information Retrieval, volume 7 of The
Information Retrieval Series, pages 1?36. Springer.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings of
the 26th Annual International ACM SIGIR Confer-
ence on Research and Development in Informaion
Retrieval, SIGIR ?03, pages 338?344.
Edward A. Fox. 1983. Extending the Boolean
and Vector Space Models of Information Retrieval
with P-norm Queries and Multiple Concept Types.
Ph.D. thesis, Cornell University, Ithaca, NY, USA.
AAI8328584.
Fredric C. Gey, Hailing Jiang, Vivien Petras, and
Aitao Chen. 2001. Cross-language retrieval for
the clef collections - comparing multiple meth-
ods of retrieval. In Revised Papers from the
Workshop of Cross-Language Evaluation Forum on
Cross-Language Information Retrieval and Evalua-
tion, CLEF ?00, pages 116?128, London, UK, UK.
Springer-Verlag.
Benjamin Herbert, Gy?orgy Szarvas, and Iryna
Gurevych. 2011. Combining query transla-
tion techniques to improve cross-language informa-
tion retrieval. In Proceedings of the 33rd Euro-
pean Conference on Advances in Information Re-
trieval, ECIR?11, pages 712?715, Berlin, Heidel-
berg. Springer-Verlag.
Djoerd Hiemstra, Wessel Kraaij, Ren?ee Pohlmann,
and Thijs Westerveld. 2001. Translation re-
sources, merging strategies, and relevance feedback
for cross-language information retrieval. In Revised
Papers from the Workshop of Cross-Language Eval-
uation Forum on Cross-Language Information Re-
trieval and Evaluation, CLEF ?00, pages 102?115,
London, UK, UK. Springer-Verlag.
598
David A. Hull and Gregory Grefenstette. 1996. Query-
ing across languages: a dictionary-based approach
to multilingual information retrieval. In Proceed-
ings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ?96, pages 49?57.
Kui-Lam Kwok. 1999. English-Chinese cross-
language retrieval based on a translation package.
In Workshop on Machine Translation for Cross Lan-
guage Information Retrieval, Machine Translation
Summit VII, pages 8?13.
Patrice Lopez and Laurent Romary. 2009. Patatras:
Retrieval model combination and regression mod-
els for prior art search. In Proceedings of the 10th
Cross-language Evaluation Forum Conference on
Multilingual Information Access Evaluation: Text
Retrieval Experiments, CLEF?09, pages 430?437,
Berlin, Heidelberg. Springer-Verlag.
Yanjun Ma, Jian-Yun Nie, Hua Wu, and Haifeng Wang.
2012. Opening machine translation black box for
cross-language information retrieval. In Information
Retrieval Technology, pages 467?476. Springer.
Walid Magdy and Gareth J. F. Jones. 2011. Should
MT systems be used as black boxes in CLIR? In
Proceedings of the 33rd European Conference on In-
formation Retrieval, ECIR ?11, pages 683?686.
Michael McGill, Matthew Koll, and Terry Noreault.
1979. An Evaluation of Factors Affecting Document
Ranking by Information Retrieval Systems. ERIC
reports. School of Information Studies, Syracuse
University.
Donald Metzler and W. Bruce Croft. 2005. A Markov
random field model for term dependencies. In Pro-
ceedings of the 28th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, SIGIR ?05, pages 472?479.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Judea Pearl. 1988. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2012.
Scikit-learn: Machine learning in python. CoRR,
abs/1201.0490.
Carol Peters, Martin Braschler, and Paul Clough. 2012.
Multilingual Information Retrieval - From Research
To Practice. Springer.
Ari Pirkola. 1998. The effects of query struc-
ture and dictionary-setups in dictionary-based cross-
language information retrieval. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?98, pages 55?63.
M. F. Porter. 1997. Readings in information retrieval.
chapter An Algorithm for Suffix Stripping, pages
313?316. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
T. B. Rajashekar and W. Bruce Croft. 1995. Com-
bining automatic and manual index representations
in probabilistic retrieval. J. Am. Soc. Inf. Sci.,
46(4):272?283, May.
Lance Ramshaw, Elizabeth Boschee, Marjorie Freed-
man, Jessica MacBride, Ralph Weischedel, and Alex
Zamanian. 2011. Serif language processing ? ef-
fective trainable language understanding. In J. Olive
et al., editor, Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global
Autonomous Language Exploitation, pages 626?
631. Springer.
Jacques Savoy. 2001. Report on CLEF-2001 exper-
iments: Effective combined query-translation ap-
proach. In CLEF, pages 27?43.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of the 16th ACM conference on Conference on In-
formation and Knowledge Management, CIKM ?07,
pages 623?632.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics, COLING ?12, pages
2685?2702.
Howard Turtle and W. Bruce Croft. 1990. Inference
networks for document retrieval. In Proceedings of
the 13th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?90, pages 1?24, New York, NY,
USA. ACM.
Jinxi Xu and Ralph Weischedel. 2005. Empirical stud-
ies on the impact of lexical resources on CLIR per-
formance. Information Processing & Management,
41(3):475?487, May.
599
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417?426,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Encouraging Consistent Translation Choices
Ferhan Ture,
1
Douglas W. Oard,
2,4
Philip Resnik
3,4
1
Department of Computer Science
2
College of Information Studies
3
Department of Linguistics
4
Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20740 USA
fture@cs.umd.edu, oard@umd.edu, resnik@umd.edu
Abstract
It has long been observed that monolingual text
exhibits a tendency toward ?one sense per dis-
course,? and it has been argued that a related
?one translation per discourse? constraint is op-
erative in bilingual contexts as well. In this pa-
per, we introduce a novel method using forced
decoding to confirm the validity of this con-
straint, and we demonstrate that it can be ex-
ploited in order to improve machine translation
quality. Three ways of incorporating such a
preference into a hierarchical phrase-based MT
model are proposed, and the approach where all
three are combined yields the greatest improve-
ments  for  both  Arabic-English  and  Chinese-
English translation experiments.
1 Introduction
In statistical Machine Translation (MT), the state-of-
the-art approach is to translate phrases in the context
of a sentence and to re-order those phrases appro-
priately. Intuitively, it seems as if it should also be
possible to draw on information outside of a single
sentence to further improve translation quality. In
this paper, we challenge the conventional approach
of translating each sentence independently, and ar-
gue that it can indeed also be beneficial to consider
document-scale context when translating text. Mo-
tivated by the success of a ?one sense per discourse?
heuristic in Word Sense Disambiguation (WSD), we
explore the potential  benefit of leveraging a ?one
translation per discourse? heuristic in MT.
The paper is organized as follows. We begin with
related work in Section 2. Next, we provide new
confirmation that the hypothesized one-translation-
per-discourse  condition  does  indeed  often  hold,
based  on  a  novel  analysis  using  forced  decoding
(Section 3). We incorporate this idea into a hierarchi-
cal MT framework by adding three new document-
scale features to the translation model (Section 4).
We then present  experimental  results  demonstrat-
ing  solid  improvements  in  translation  quality  ob-
tained by leveraging these features, both for Arabic-
English (Ar-En) and Chinese-English (Zh-En) trans-
lation (Section 5). Conclusions and future work are
presented in Section 6.
2 Related work
Exploiting  discourse-level  context  has  to  date
received  only  limited  attention  in  MT re-
search (e.g., (Gime?nez  and  Ma`rquez, 2007; Liu
et al, 2010; Carpuat, 2009; Brown, 2008; Xiao et
al., 2011)). Exploratory analysis of reference trans-
lations by Carpuat  (2009)  motivates  a  hypothesis
that MT systems might benefit from the ?one sense
per discourse? heuristic, first introduced by Gale et
al. (1992), which has proven to be effective in the
context of WSD (Yarowsky, 1995). Carpuat?s ap-
proach was to do post-processing on the translation
output to impose a ?one translation per discourse?
constraint where the system would otherwise have
made a different choice. A manual evaluation on
a sample of sentences suggested promise from the
technique, which  Carpuat  suggested  in  favor  of
exploring more integrated approaches.
Xiao et al (2011) took this one step further and
implement an approach where they identified am-
biguous translations within each document, and at-
417
tempt to fix them by replacing each ambiguity with
the most frequent translation choice. Based on their
error analysis, the authors indicate two shortcomings
when trying to find the correct translation of a given
phrase. First, frequency may not provide sufficient
information to distinguish between translation can-
didates, which is why we take rareness into account
when scoring translation candidates. Another prob-
lem is, like any other heuristic, that there may be
cases where the heuristic fails and there are multi-
ple senses per discourse. Guaranteeing consistency
hurts performance in such situations, which is why
we implement the heuristic as a model feature, and
let the model score decide for each case.
We are aware of a few other analyses that have
shown promising results based on a similar motiva-
tion. For instance, Wasser and Dorr (2008)?s ap-
proach biases the MT system based on term statistics
from relevant documents in comparable corpora. Ma
et al (2011) show that a translation memory can be
used to find similar source sentences, and consecu-
tively adapt translation choices towards consistency.
Domain adaptation for MT has has also been shown
to be useful in some cases (Bertoldi and Federico,
2009; Hildebrand et al, 2005; Sanchis-Trilles and
Casacuberta, 2010; Tiedemann, 2010; Zhao et al,
2004), so to the extent we consider documents to be
micro-domains we might expect similar approaches
to be useful at document scale. Indeed, hints that
such ideas may work have been available for some
time. For example, there is clear evidence that the
behavior of human translators can provide evidence
that is often useful for automating WSD (Diab and
Resnik, 2002; Ng et al, 2003). When coupled with
the one-sense-per-discourse heuristic, this suggests
that the reverse may also be true.
3 Exploratory analysis
It is well known that writing styles vary by genre,
and in particular that the amount of vocabulary vari-
ation within a document depends to some extent on
the genre (e.g., higher in poetry than in engineering
writing). The degree to which authors tend to make
consistent word choices in any particular genre is,
therefore, an empirical question. In order to gain in-
sight into the extent to which human translators make
consistent vocabulary choices in the types of materi-
als that we wish to translate (in this work, news sto-
ries), we first explore the degree of support for our
one-translation-per-discourse hypothesis in the ref-
erence translations of a standard MT test collection.
We used the Ar-En MT08 data set, which con-
tains 74 newswire documents with a total  of 813
sentences, each of which has four reference trans-
lations. Throughout this paper we consistently use
the  document  (i.e., one  news  story)  as  a  conve-
nient discourse unit, although of course finer-scale or
broader-scale discourse units might also be explored
in future work. Moreover, throughout this paper we
use the hierarchical phrase-based translation system
(Hiero), which is based on a synchronous context-
free grammar (SCFG) model (Chiang, 2005). In a
SCFG, the rule [X] ||| ? ||| ? indicates that con-
text free expansion X ? ? in the source language
can occur synchronously with X ? ? in the target
language. In this case, we call ? the left hand side
(LHS) of the rule, and ? the right hand side (RHS)
of the rule.
To determine the extent and nature of translation
consistency choices made by human translators, we
randomly selected one of the four sets of reference
translations (first set, with id 0) and we used forced
decoding to find all possible sequences of rules that
could transform the source sentence into the target
sentence. In forced decoding, given a pair of source
and target sentences, and a grammar consisting of
learned translation rules with associated probabili-
ties, the decoder searches all possible derivations for
the one sequence of rules that is most likely (under
the learned translation model) to synchronously pro-
duce the source sentence on the LHS and the target
sentence on the RHS. For instance, consider the fol-
lowing Arabic sentence as input:
???? ??? ?????????? ??????? .
and its uncased reference translation:
there is a link between the three attacks .
The following four rules, which are part of the SCFG
learned from the the same translation pairs, allows
the decoder to find a sequence of derivations that
?translates? the source-side Arabic sentence into the
418
X16 
X7  ??? X12 
X3  ??????????  ????  ??????? .
R1 
R2 R3 
R4 
X16 
X7 between X12 
X3  the  there  attacks .  three  is  a  link 
R1 
R2 R3 
R4 
Figure 1: Illustration of forced decoding.
target-side reference translation.
1
R1. [X12] ||| ???? ||| there is a link
R2. [X16] ||| [2] ??? [1] ||| [X12, 1] between [X7, 2]
R3. [X7] ||| [1] ?????????? . ||| [X3, 1] attacks .
R4. [X3] ||| ??????? ||| the three
Figure 1 illustrates how the decoder uses these
rules  to  produce the source and target  sides  syn-
chronously.
As we repeated this  procedure  for  all  sentence
pairs, we kept track of all rules that were actually
used by the decoder to generate a reference English
translation from the corresponding Arabic sentences.
Our next step was to identify cases in which the
SCFG could reasonably have produced a substan-
tially  different  translation. Whenever  an  Arabic
phrase f occurs multiple times in a document, and f
appears on the LHS of two or more different gram-
mar rules in the SCFG, we count this as a single
?case?.
2
These cases correspond to unique (source
phrase f , document d) pairs in which a translation
process using that SCFG could have chosen to pro-
duce two or more different translations of f in d.
Since the multiple appearances of f are distributed
among sentences of d, each counted case may cor-
respond to a number of sentences ranging from 1 to
the number of sentences in that document.
Table 1 shows a small sample of the cases (i.e.,
(source phrase f , document d) pairs) identified as a
result of forced decoding. There were 321 such cases
in our dataset and there were 672 sentences in which
at least one case occurred. This is not an uncommon
phenomenon; these 672 sentences comprise 83% of
1
Since our goal was an exploratory analysis, the MT08 test
set was combined with the training set in order to ensure reach-
ability of the reference translations using the learned grammar.
Proper train/dev/test splits were, of course, used for the evalua-
tion results reported in Section 5.
2
We define a phrase as any text that constitutes the entire
LHS of a grammar rule.
the test set. However, many of these cases repre-
sent either unlikely choices or inconsequential dif-
ferences, so some post-processing is called for.
Since  grammar  rules  are  typically  more  fine-
grained than is necessary for our purposes (e.g., to
capture various punctuation and determiner differ-
ences that do not affect the ?sense? of the transla-
tion), we applied a few simple heuristics to edit the
source and target  sides and group all  such minor
variations into a single ?mega-rule? (e.g., ?how???,
how?, ?third???a third?, ?want???we want?). For
this, we removed nonterminal symbols and punc-
tuation, and  considered  two target  phrases e and
e? to  be different only  if edit distance(e, e?) >
max(length(e), length(e?))/2, where the edit dis-
tance is based on character removal and insertion.
For instance, the third example in Table 1 would
have been considered to be translated consistently
as a result of this heuristic, as opposed to the first
example. We also eliminated cases in which no rea-
sonable alternatives were available in the translation
grammar (i.e., cases where the second most probable
rule with the same LHS was assigned a probability
below 0.1 in the grammar). Cases 4 and 5 would
have been removed by this heuristic.
After this filtering and aggregation we were left
with 176 (f , d) pairs in which the translation model
could reasonably have selected between rules that
would have produced substantially different English
translations of f in d (such as cases 1?3 and 6?9).
It was these 176 cases, affecting a total of 512 sen-
tences (63% of test set) for which we then examined
what forced decoding could tell us about translation
consistency.
So now that we know what the human who pro-
duced the reference translations actually did (accord-
ing to forced decoding), and in which cases they
might reasonably have chosen to do something sub-
stantially different (according to the SCFG), we can
ask in which cases the human (effectively) made a
consistent choice of translation rules when encoun-
tering the same Arabic phrase in the same document.
In 128 of the 176 cases, that is what they did (i.e.,
when the same phrase occurred multiple times in a
single document and more than one translation was
reasonably possible, forced decoding indicated that
the human translator translated that phrase in essen-
tially the same way). These cases affected the trans-
419
Case
Translation counts
Source phrase Doc #
???? 566 that killed = 1
killing of = 1
??????? 782 hostages = 2
??????? 138 hostage = 1
hostages = 2
????? 466 korea = 2
????? 763 korea = 2
?? 30 from = 2
?? 7 of = 1
from = 1
?? ?????? 717 of the current = 2
???? 30 the = 1
which =1
Table 1: A sample of cases (i.e., (source phrase f , docu-
ment d) pairs) identified as a result of forced decoding.
lation of 455 sentences (56% of the test set), suggest-
ing that if we can replicate this human behavior in a
system, it might affect a nontrivial number of trans-
lation choices.
These statistics also suggest, however, that there
may be some risk incurred in such a process, since
in 48 of the 176 cases, the human translator opted
for a substantially different translation. When we
closely examined these 48 instances, we found that
19 (40%) involved changing a content-bearing word
(sometimes to a word with similar meaning). The re-
maining 29 (60%) involved function words or simi-
lar constructions. See Figures 2 and 3 for examples.
1a. [X] ||| ???? ||| had allowed
1b. [X] ||| ???? ||| has permitted
2a. [X] ||| [X,1] ???? ||| examining [X,1]
2b. [X] ||| [X,1] ???? ||| is considering [X,1]
3a. [X] ||| [X,1] ?????? ||| neighbors
3b. [X] ||| [X,1] ?????? ||| neighboring countries
Figure 2: Examples of differences in lexical choice for
content-bearing words within the same document.
We can make several observations based on this
analysis. First, there does indeed seem to be ev-
idence to support the one-translation-per-discourse
heuristic, and to suggest that respecting that heuris-
4a. [X] ||| ?? ||| on
4b. [X] ||| ?? ||| in
4c. [X] ||| ?? ||| ?s
5a. [X] ||| ?? ||| had
5b. [X] ||| ?? ||| was
Figure 3: Examples of differences in lexical choice for
other types of lexical units within the same document.
tic could improve translation outcomes for a substan-
tial number of sentences. Second, even when a ref-
erence translation contains different translations of
the same phrase, this may sometimes be the result of
stylistic choices rather than an intent by the transla-
tor to affect the expressed meaning. If a system were
try to ?fix? such cases by enforcing consistent trans-
lation, the resulting translation might be somewhat
more stilted, but perhaps not less accurate or less in-
telligible. Finally, sentence structure conventions or
other language-specific phenomena may sometimes
require the same phrase to be translated differently,
so some way of encouraging consistency while still
allowing the model to consider other contextual fac-
tors might be better than always imposing a hard con-
sistency constraint.
4 Approach
To incorporate document-level features into an MT
system  that  would  otherwise  operate  with  only
sentence-level  evidence, we  added  three  super-
sentential ?consistency features? to the translation
model. The decoder computes scores for these fea-
tures in two passes over each document; in each pass,
each sentence in the document is decoded. In the
first pass, the decoder keeps track of the number of
occurrences of some aspects of each grammar rule
and stores that information. The consistency fea-
tures are disabled during this pass, and do not affect
decoder scoring. In the second pass, each grammar
rule is assigned as many as three consistency feature
scores, each of which is based on some frozen counts
from the first pass. These features are designed to
introduce a bias towards translation consistency, but
to leave the final decision to the decoder, which of
course also has access to other  features from the
translation and language model. At this point we are
more interested in effectiveness than efficiency, so
420
we simply note that this approach doubles the run-
ning time of the decoder and that future work on a
more elegant implementation might be productive.
We explore three ways to compute features in this
section. The essential idea behind all of them is to
define some feature function that increases monoton-
ically with an increase in some count that we believe
to be informative, and in which the rate of increase is
damped more strongly as that count increases. Sev-
eral feature functions could satisfy those broad re-
quirements; in this section, we describe three vari-
ants, C1, C2 and C3, and discuss the potential bene-
fits and drawbacks of each.
C1: Counting rules In this variant, we count in-
stances of the same entire grammar rule, where a rule
r contains both the source phrase f and the target
phrase e. During the first pass, whenever a grammar
rule is chosen by the decoder for the one-best output,
the count for that rule is incremented. Given a gram-
mar rule r and the number of times r was counted in
the first pass (given by N{r}), the consistency fea-
ture score is computed as follows:
C1(r) =
2.2N{r}
1.2 + N{r}
(1)
Equation 1 is the term frequency component of the
well known Okapi BM25 term weighting function,
when parameters are set to the conventional values
k = 1.2, b = 0. This is an increasing and con-
cave function in which the count has a diminishing
marginal effect on the feature score. It has proven
to be useful in information retrieval applications, in
which the goal is to model ?aboutness? based on term
counts (Robertson et al, 1994). Because our goal is
to demonstrate the potential of consistency features,
it seemed reasonable to work with some simple func-
tion that has a shape like the one we desired. We
leave exploration of optimal damping functions for
future work.
A drawback of this C1 approach is that as we saw
in Section 3, grammar rules in phrase-based MT sys-
tems tend to be somewhat more fine-grained than
seems optimal for constructing a consistency fea-
ture. For instance, consider the following rules that
all translate the same Arabic term:
R1. [X] ||| [X,1] ????? ||| [X,1] the bodies
R2. [X] ||| [X,1] ????? ||| [X,1] the organs
R3. [X] ||| [X,1] ????? ||| [X,1] organs
R4. [X] ||| ????? [X,1] ||| the organs of [X,1]
R5. [X] ||| ????? [X,1] ||| [X,1] bodies
Based on these grammar rules, we as human read-
ers infer that this Arabic phrase can be translated in
two different ways: as organs or as bodies. An opti-
mal application of the one-translation-per-discourse
heuristic would thus group the rules based on the
presence of one of those words. However, in the C1
variant, each of these rules would be counted sepa-
rately because of differences that in some cases do
not directly affect the choice of content words. For
instance, on the source side, the Arabic token ap-
pears to the right of the nonterminal symbol in R1,
R2 andR3, while it is to the left of the nonterminal in
R4 andR5. On the target side, differences are due to
both nonterminal symbol position and the existence
of determiners. Motivated by many examples like
this, we came up with an alternative way of count-
ing rules.
C2: Counting target tokens To partially address
this sparseness issue, variant C2 focuses only on the
target side. We extract all target tokens whenever a
grammar rule is used by the decoder in a one-best
derivation and increment a counter for each. Since
we are mainly interested in content words (e.g. bod-
ies, organs), we use simple pattern matching to dis-
card nonterminal symbols and punctuation, and we
ignore terms that appear in more than 50% of all doc-
uments (a convenient way of discarding common to-
kens such as the, or, and). This approach separates
the rules in the example above into two groups: rules
with bodies on the target side and rules with organs
on the target side. Upon completion of the first pass,
the consistency feature score for rule r is then de-
termined by first computing a score for each unique
target-side token w using:
bm25(w) = 2.2N{w}
1.2 + N{w}
log
D + 1
DF (w) + 0.5
(2)
where in this caseN{w}maps tokens to their respec-
tive counts in the document, D is the total number
of documents in the collection, and DF (document
frequency) is the number of documents in which the
token occurs. This is a fuller version of the BM25
function in which (in the information retrieval ap-
plication) both high term frequencies and rare terms
421
are rewarded. We then set the feature score for each
rule r to the maximum score of any of its target-side
terminal tokens:
C2(r) = max
e?RHS(r)
bm25(e) (3)
Our motivation for choosing the maximum is that
when there is more than one content word that sur-
vives the pruning of common terms, we want the
score to be influenced most strongly by the most im-
portant of those terms. Since BM25 term weights
can be thought of as a measure of term importance,
taking the maximum is a simple expedient.
Although counting only target-side tokens yields
coarser granularity than counting rules, ignoring the
source side of the rule risks combining target side
statistics from translations of unrelated source lan-
guage terms. Consider the following grammar rule:
R6. [X] ||| <s> [X,1] ????? ||| <s> [X,1] life support
Since the counter for life and support will both be
incremented whenever rule R6 fires in the one-best
decoding during the first pass, problems could arise
if a rule with a different LHS that also contains sup-
port on the RHS were to fire in the same document,
for example:
R7. [X] ||| ?????? ||| support
If we don?t take the source side into account, both oc-
currences of support will be grouped together when
counting and R7 will receive extra score from the
consistency feature whenever R6 is used by the de-
coder. Of course, this problem will only arise when
the LHS of R6 and R7 are present in the same doc-
ument, and how often that happens (and thus how
large the risk from this factor is) is an empirical ques-
tion. We therefore developed a third alternative as a
middle ground between the fine-grained C1 and the
coarse-grained C2.
C3: Counting  token  translation  pairs In  this
variant, we count each terminal (source token, tar-
get token) pair that survives pruning. Specifically,
if grammar rule [X]|||f1f2...fm|||e1e2...en fires, we
increment the count of every pair ?fi, ej?, where fi
is aligned to ej . After the first pass, we compute the
feature value of each observed pair, based on this
count and the DF of the target-side of the pair. We
chose to use only the target token in the DF com-
putation (i.e., aggregating over all source tokens) to
reduce sparsity effects. Similar to C2, the feature of
a rule r is defined by the maximum of scores of all
pairs extracted from r.
C3(r) = max
f?LHS(r)
e?RHS(r)
?f,e? aligned
bm25(?f, e?) (4)
Since each variant has its benefits and drawbacks, we
can include all three in the system and let the tuning
process decide on how each should be weighted.
5 Evaluation and Discussion
We have evaluated the one-translation-per-discourse
feature using the cdecMT system (Dyer et al, 2010).
We started by building a baseline system using stan-
dard features in cdec: lexical and phrase transla-
tion probabilities in both directions, word and arity
penalty features, and a 5-gram language model. We
then added each of the three consistency feature vari-
ants, along with all two-way and the one three-way
combinations of them, thus yielding a total of eight
systems for comparison, including the baseline.
For training the Ar-En system, we used the dataset
from the DARPA GALE evaluation (Olive et  al.,
2011), which consists of NIST and LDC releases.
The corpus was filtered to remove sentence pairs
with  anomalous  length  ratios  and  subsampled  to
yield a training set containing 3.4 million parallel
sentence pairs. The Arabic text was preprocessed to
produce two different segmentations (simple punctu-
ation tokenization with orthographic normalization,
and LDC?s ATBv3 representation (Maamouri et al,
2008)), represented together using cdec?s lattice in-
put format (Dyer et al, 2008).
The Zh-En system was trained on parallel train-
ing text consisting of the non-UN portions and non-
HK Hansards portions of the NIST training corpora.
Chinese was automatically segmented by the Stan-
ford segmenter (Tseng et al, 2005), and traditional
characters were simplified. After subsampling and
filtering, we obtain a training corpus of 1.6 million
parallel sentences.
Both  training  sets  were  word-aligned  with
GIZA++ (Och and Ney, 2003), using 5 Model  1
and  5  HMM iterations. A SCFG was  then  ex-
tracted from these alignments using a suffix array
extractor (Chiang, 2007). Evaluation was done with
multi-reference BLEU (Papineni et al, 2002) on test
422
sets with four references for each language pair, and
MIRA was used for tuning (Crammer et al, 2006).
In our experiments, we run the first decoding phase
using feature weights that are guessed heuristically
based on weights from previously tuned systems.
All feature weights, including the discourse feature,
were then tuned together, based on the output  of
the  second  decoding  phase. For  Ar-En  parame-
ter  tuning, we  used  the  MT06 newswire  dataset,
which contains 104 documents and a total of 1,797
sentences. For testing, we used the MT08 dataset
described  above  (74  documents, 813  sentences).
For Zh-En experiments, the MT02 newswire dataset
(100 documents, 878 sentences) was used for tuning,
and evaluation was done on the MT06 test set (79
documents, 1,664 sentences). For  both language
pairs, DF values were computed from the tuning
set for both tuning and evaluation experiments.
When we used NIST?s official metric (BLEU-4)
to compare our results to the official NIST evalu-
ation (NIST, 2006; NIST, 2008), our baseline sys-
tem achieved 54.70 for  Ar-En and 31.69 for  Zh-
En. Based on reported NIST results, our baseline
would have ranked 4
th
in the Zh-En MT06 evalua-
tion, and would have outperformed all Ar-En MT08
systems. We used a slightly different IBM-BLEU
metric for the rest of our evaluation. In this case,
the baseline system achieved 53.07 BLEU points
for  Ar-En  and  30.43  points  for  Zh-En. Among
more recent papers, the best reported results were
56.87  for  Ar-En  MT08 (Zhao  et  al., 2011a)  and
35.87 for Zh-En MT06 (Zhao et al, 2011b), although
many papers report BLEU scores below 53 points
for Arabic (Carpuat et al, 2011) and 32 points for
Chinese (Monz, 2011). The systems that outper-
formed our baseline applied novel techniques, and
used larger language models, as well as many non-
standard features. We argue that these novelties are
complementary to our approach, and therefore do not
damage the credibility of our baseline.
Among the single-feature runs, C3 had the best
performance  in  Ar-En  experiments, with  53.84
BLEU points, whereas C2 yielded the best results
for Zh-En with a BLEU score of 30.96. In any case,
all three variants outperformed the baseline (see Ta-
ble 2). When multiple features were combined, we
generally observed an increase in BLEU, suggesting
that our features have usefully different error char-
Method BLEU
Ar-En Zh-En
Baseline 53.07 30.43
C1 53.82 30.59
C2 53.70 30.96
C3 53.84 30.54
C12 53.82 30.79
C13 53.82 30.76
C23 53.88 30.63
C123 53.98 31.42
Table 2: Evaluation results: BLEU scores with four ref-
erences for Ar-En and Zh-En experiments.
Method # documents
Ar-En Zh-En
Docs 74 79
C1 37 30
C2 37 35
C3 42 36
C123 43 41
Table 3: Doc-level analysis: Number of documents where
each variant outperforms baseline.
acteristics. The combination of all three variants,
C123, yielded the best results, nearly 1.0 BLEU point
higher  than  the  baseline  for  both  language  pairs.
Evaluation results are summarized in Table 2.
Given our focus on documents, it  is  natural  to
ask  what  fraction  of  the  documents  were  helped
or  harmed  by  consistency  features. Document-
level  BLEU scores  for  Arabic-to-English  transla-
tions show that C3 outperformed the baseline on a
larger number of documents than any other single
feature (42/74=57%), compared with 37/74 (50%)
for both C1 and C2. C123 did better by this measure
as well, with BLEU increasing for 43 of the docu-
ments. There were no documents where the BLEU
score  was  exactly  the  same, therefore  the  BLEU
score declined for the remaining documents. As Ta-
ble 3 indicates, document-level BLEU for the Zh-En
experiments shows similar results.
We can also look at our results in a more fine-
grained way, focusing on differences in how each
system translated the same source-language phrase.
For  this  analysis, we  defined  English  phrases e
and e? to  be different if edit distance(e, e?) >
423
Method Ar-En Zh-En
Cases Test set Cases Test set
C1 77 24% 401 48%
C2 127 35% 686 60%
C3 101 33% 491 53%
Any 197 68% 968 94%
C123 141 41% 651 59%
Table 4: Effect of applying variants of the consistency
feature (Any=C1 or C2 or C3).
max(length(e), length(e?))/2. By  this  way  of
counting, there are 197 unique (Arabic phrase, docu-
ment) pairs for which at least one single-feature sys-
tem produced translations differently from the base-
line system. Together, these cases affect 553 sen-
tences (68%) in 67 of the 74 documents, with as
many as 12 differences observed in a single doc-
ument. The  number  of  such  differences  is  even
higher for Chinese-to-English translation, probably
due to lower confidence from the translation model
and longer documents. Table 4 shows the number of
changes by each system, and the percentage of the
test set affected by these changes.
In order to gain greater insight into the effect of
the consistency features, we randomly sampled 60
of the 197 cases and analyzed the influence of the
change to the document BLEU score. In 25 of the
sampled cases, at least one of the three systems made
a change that improved the BLEU score, whereas the
score was adversely affected for at least one system
in 13 cases. BLEU remained unchanged in the re-
maining 22 cases, mostly due to the use of multi-
ple reference translations. When we analyze the ef-
fect of each system separately, we see that C2 was
the most aggressive, making 25 changes that influ-
enced BLEU (16 positive, 9 negative). C1 was the
most conservative, with only 13 such changes (8 pos-
itive, 5 negative). Consistent with the overall BLEU
scores, C3 evidenced the best ratio between benefit
and harm, making 20 changes that affected the score
(16 positive, 4 negative).
Looking at specific cases can yield some insight
into how the consistency features achieve improve-
ments. For example, results improved when trans-
lating the phrase ???????, (Eng. organizational,
regulatory), which appears in the context of organi-
zational groups that support terrorist ideology. The
baseline system translated this as organizational in
one case, and regulatory in another. Variants C1
and C2 changed this behavior, so that the translation
was organizational in both cases. One of the refer-
ence translations used organizational in one case and
dropped the phrase in the other, and the other three
translators  provided  consistent  translations  (using
organized and organizational). As a result, applying
the one-translation-per-discourse heuristic improved
the multi-reference BLEU score.
On the other hand, here is one of the cases where
our  feature  hurt  performance. The  phrase ??
?? (Eng. border/frontier troops/guards) appears
in two sentences of a Chinese news story about vio-
lence along the India - Nepal border. All reference
translations consistently used the word border in the
translation, as it is a better choice in this context.
The baseline system translated the phrase as fron-
tier guards and border troops in the two sentences.
All system variants replaced border with frontier to
maintain consistency, and therefore produced worse
translations, causing a decrease in BLEU score.
Examples can, however, also point up limitations
in our ability to measure improvements. In one of
the test documents, the Arabic phrase ?????? ???
(Eng. sneak, infiltrate, enter without approval) ap-
pears in the context of Turkey trying to enter the Eu-
ropean Union. This was translated by the baseline
system as sneak into in one occurrence and infiltrate
into in another. C1 didn?t change the output, but
C2 and C3 translated the phrase as infiltrate into in
both cases. Although all of the four reference trans-
lators were consistent within their choices, each of
them chose different translations, namely worm its
way, enter, sneak and sneak into. This resulted in
a decrease in BLEU score for the two systems that
chose infiltrate into. This case illustrates a limita-
tion to fine-grained use of BLEU alone as a basis
for analysis, since we might argue that infiltrate into
is no less appropriate than sneak into in this con-
text. In other words, some of the reductions we see
in BLEU may not be actual errors but rather sim-
ply changes that take us outside of the coverage of
the test set. We did not find any cases in our sample
in which improvements in BLEU seemed to reward
changes that adversely affected meaning. From this,
424
we conclude that BLEU is a somewhat conservative
measure when used in this way, and that the actual
overall improvement in translation quality over our
baseline may be somewhat more than our roughly
1.0 measured BLEU improvement would suggest.
6 Conclusions and Future Work
In this paper, we started with a new way of look-
ing at, and largely supporting, the ?one translation
per discourse? hypothesis using forced decoding of
human reference translations. We then leveraged
insights  from that  analysis  to  design  the  transla-
tion model consistency features, obtaining solid im-
provements for both Ar-En and Zh-En translation.
In future work, we plan to explore additional vari-
ants. For example, we can further address sparsity by
incorporating monolingual paraphrase detection on
the source side, the target side or both. We can and
should explore other monotonically increasing con-
cave feature functions in addition to the Okapi BM25
function that we have found to be useful in this work,
we should explore alternatives to our use of the max-
imum function in C2 and C3, and we should con-
sider optimizing to measures other than BLEU (e.g.,
METEOR) that extend the range of rewarded lexical
choices by leveraging monolingual paraphrase evi-
dence.
In designing our features we were guided by our
intuition about which kinds of consistency should be
rewarded. Data can be superior to intuition, how-
ever, and our forced decoding technique might also
be helpful in generating new insights that could help
to guide the design of even more useful features. For
example, our forced decoding clearly points to cases
in which translators have chosen different structural
variants when translating the same phrase, and closer
examination of these cases might help us to automat-
ically detect which kinds of structural variation can
most profitably be moderated using a consistency
feature. We should also note that we have only done
forced decoding to date in one language pair (Ar-
En), and there might be more to be learned about
language-specific issues from doing the same anal-
ysis for additional language pairs.
Finally, the time seems propitious to reconsider
our choice of document-scale as our discourse con-
text. Documents have much to recommend them, but
much of the content that we might wish to translate
(conversational speech, text chat, email threads, . . . )
doesn?t present the kinds of obvious and unambigu-
ous document boundaries that we find in MT test
collections that are built from news stories. More-
over, some documents (e.g., textbooks) may be too
diverse for an entire document to be the right scale
for consistency. We might also be able to produc-
tively group similar documents into clusters in which
the vocabulary choices are (or should be) mutually
reinforcing.
We therefore  end where  we began, with  many
questions to be answered. Now, however, we have
somewhat different questions ? not whether to en-
courage consistency at a super-sentential scale, but
rather when and how best to do that.
Acknowledgements
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA.
References
Nicola  Bertoldi  and  Marcello  Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation (StatMT
?09), pages 182?189.
Ralf D. Brown. 2008. Exploiting document-level context
for data-driven machine translation. In Proceedings of
the the Eighth Conference of the Association for Ma-
chine Translation in the Americas (AMTA ?08).
Marine Carpuat, Yuval Marton, and Nizar Habash. 2011.
Improved Arabic-to-English statistical machine trans-
lation  by  reordering  post-verbal  subjects  for  word
alignment. Machine Translation, pages 1?16.
Marine Carpuat. 2009. One translation per discourse. In
Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ?09,
pages 19?27.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL ?05.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
425
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL ?02.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. In Pro-
ceedings of ACL-HLT?08, pages 1012?1020, June.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: a
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACLDe-
mos ?10, pages 7?12.
William A.  Gale, Kenneth W.  Church, and  David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, HLT ?91, pages 233?237.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proceedings  of  StatMT ?07, pages
159?166.
AS Hildebrand, M Eck, S Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of The European Association for Machine
Translation (EAMT ?05).
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2010.
Improving statistical machine translation with mono-
lingual collocation. In ACL ?10.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrimina-
tive learning: a translation memory-inspired approach.
In Proceedings of ACL-HLT?11, pages 1239?1248.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhancing the Arabic Treebank: A Collaborative Ef-
fort toward New Annotation Guidelines. In LREC ?08.
Christof Monz. 2011. Statistical Machine Translation
with Local Language Models. In EMNLP ?11.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In ACL ?03.
NIST. 2006. http://www.itl.nist.gov/iad/mig/tests/mt/2006/.
NIST. 2008. http://www.itl.nist.gov/iad/mig/tests/mt/2008/.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Joseph Olive, Caitlin  Christianson, and John McCary.
2011. Handbook  of  Natural  Language  Processing
and Machine Translation: DARPAGlobal Autonomous
Language  Exploitation. Springer  Publishing  Com-
pany, Inc., 1st edition.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02.
Stephen E. Robertson, Steve Walker, Susan Jones, Miche-
line  Hancock-Beaulieu, and  Mike  Gatford. 1994.
Okapi at TREC-3. In TREC.
Germa?n  Sanchis-Trilles  and  Francisco  Casacuberta.
2010. Bayesian  adaptation  for  statistical  machine
translation. In Proceedings of the workshop on Struc-
tural and Syntactic Pattern Recognition (SSPR ?10),
pages 620?629.
Jo?rg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the workshop on
Domain Adaptation for Natural Language Processing
(DANLP ?10), pages 8?15.
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional  random  field  word  segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Michael M.  Wasser  and  Bonnie  Dorr. 2008. Ma-
chine  translation  with  cross-lingual  information  re-
trieval based document relevance scores. Unpublished.
Tong Xiao, Jingbo Zhu, Shujie  Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. InMachine Translation Summit XIII
(MTS?11).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL ?95.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Lan-
guage model adaptation for statistical machine transla-
tion with structured query models. In COLING ?04.
Bing Zhao, Young-Suk Lee, Xiaoqiang Luo, and Liu Li.
2011a. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In ACL-HLT ?11, pages 846?855.
Yinggong Zhao, Yangsheng Ji, Ning Xi, Shujian Huang,
and Jiajun Chen. 2011b. Language model weight
adaptation based on cross-entropy for statistical ma-
chine translation. In Pacific Asia Conference on Lan-
guage, Information and Computation (PACLIC ?11).
426
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626?630,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Why Not Grab a Free Lunch? Mining Large Corpora for
Parallel Sentences to Improve Translation Modeling
Ferhan Ture
Dept. of Computer Science,
University of Maryland
fture@cs.umd.edu
Jimmy Lin
The iSchool
University of Maryland
jimmylin@umd.edu
Abstract
It is well known that the output quality of
statistical machine translation (SMT) systems
increases with more training data. To ob-
tain more parallel text for translation mod-
eling, researchers have turned to the web to
mine parallel sentences, but most previous ap-
proaches have avoided the difficult problem
of pairwise similarity on cross-lingual docu-
ments and instead rely on heuristics. In con-
trast, we confront this challenge head on us-
ing the MapReduce framework. On a mod-
est cluster, our scalable end-to-end processing
pipeline was able to automatically gather 5.8m
parallel sentence pairs from English and Ger-
man Wikipedia. Augmenting existing bitext
with these data yielded significant improve-
ments over a state-of-the-art baseline (2.39
BLEU points in the best case).
1 Introduction
It has been repeatedly shown that ?throwing more
data at the problem? is effective in increasing SMT
output quality, both for translation modeling (Dyer
et al, 2008) and for language modeling (Brants et
al., 2007). In this paper, we bring together two re-
lated research threads to gather parallel sentences for
improved translation modeling: cross-lingual pair-
wise similarity to mine comparable documents and
classification to identify sentence pairs that are mu-
tual translations.
Unlike most previous work, which sidesteps the
computationally-intensive task of pairwise compar-
isons to mine comparable documents and instead re-
lies on heuristics, we tackle the challenge head on.
This paper describes a fully open-source, scalable
MapReduce-based processing pipeline that is able to
automatically extract large quantities of parallel sen-
tences. Experiments examine the impact data size
has on a state-of-the-art SMT system.
We acknowledge that different components of this
work are not novel and the general principles behind
?big data? MT are well known. However, when con-
sidered together with our previous work (Ture et al,
2011), to our knowledge this is the first exposition
in which all the pieces have been ?put together? in
an end-to-end pipeline that is accessible to academic
research groups. The framework described in this
paper is entirely open source, and the computational
resources necessary to replicate our results are rela-
tively modest.
Starting from nothing more than two corpora in
different languages (in German and English, in our
case), we are able to extract bitext and improve
translation quality by a significant margin (2.39
BLEU points), essentially ?for free?. By varying
both the quantity and quality of the bitext, we char-
acterize the tradeoffs between the amount of data,
computational costs, and translation quality.
2 Related Work
The idea of mining parallel sentences, particularly
from the web, is of course not new. Most adopt a
two step process: 1. identify comparable documents
and generate candidate sentence pairs, and 2. filter
candidate pairs to retain parallel sentences.
The general solution to the first step involves com-
puting pairwise similarities across multi-lingual cor-
pora. As this is computationally intensive, most
626
studies fall back to heuristics, e.g., comparing news
articles close in time (Munteanu and Marcu, 2005),
exploiting ?inter-wiki? links in Wikipedia (Smith et
al., 2010), or bootstrapping off an existing search
engine (Resnik and Smith, 2003). In contrast, we
adopt a more exhaustive approach by directly tack-
ling the cross-lingual pairwise similarity problem,
using MapReduce on a modest cluster. We perform
experiments on German and English Wikipedia (two
largest available), but our technique is general and
does not depend on sparse, manually-created inter-
wiki links. Thus, compared to those approaches, we
achieve much higher recall.
The second step (filtering candidate sentence
pairs) is relatively straightforward, and we adopt
the classification approach of Munteanu and
Marcu (2005). However, unlike in previous work,
we need to classify large volumes of data (due to
higher recall in the first step). Therefore, we care
about the relationship between classification accu-
racy and the speed of the classifier. Our two-stage
approach gives us both high effectiveness (accuracy)
and efficiency (speed).
A recent study from Google describes a general
solution to our problem that scales to web collec-
tions (Uszkoreit et al, 2010). The authors translate
all documents from one language into another, thus
transforming the problem into identifying similar
mono-lingual document pairs. Nevertheless, our ap-
proach makes several additional contributions. First,
we explore the effect of dataset size on results. Our
conclusions are more nuanced than simply ?more
data is better?, since there is a tradeoff between qual-
ity and quantity. Our experiments involve orders
of magnitude less data, but we nevertheless observe
significant gains over a strong baseline. Overall, our
approach requires far less computational resources
and thus is within the reach of academic research
groups: we do not require running an MT system
on one side of the entire collection, and we care-
fully evaluate and control the speed of sentence-
classification. Finally, in support of open science,
our code1 and data2 are available as part of Ivory, an
open-source Hadoop toolkit for web-scale informa-
tion retrieval (Lin et al, 2009).
1ivory.cc
2github.com/ferhanture/WikiBitext
3 Generating Candidate Sentences
We applied our approach on English Wikipedia
(10.9m documents, 30.6GB) and German Wikipedia
(2.4m articles, 8.5GB), using XML dumps from Jan-
uary 2011. English and German Wikipedia were se-
lected because they are the largest Wikipedia collec-
tions available, and we want to measure effects in a
language for which we already have lots of bitext.
In both collections, redirect pages and stub articles
were discarded.
To mine comparable documents, we used our
previously described algorithm (Ture et al, 2011),
based on local-sensitive hashing, also implemented
in Hadoop MapReduce. The reader is referred to
the paper for details. On a 16 node (96 core) cluster,
we were able to extract 64m (de, df ) document pairs
(with cosine similarity ? 0.3) in 8.8 hours.
For each of the (de, df ) pairs, the next process-
ing step involves generating the Cartesian product of
sentences in both documents as candidate sentence
pairs: this itself is a non-trivial problem. Although
in this particular case it may be possible to load both
document collections in memory, we envision scal-
ing up to collections in the future for which this is
not possible. Therefore, we devised a scalable, dis-
tributed, out-of-memory solution using Hadoop.
The algorithm works as follows: We map over
(docid n, document d) pairs from both the German
and English collections. In each mapper all (de, df )
similarity pairs are loaded in memory. If the input
document is not found in any of these pairs, no work
is performed. Otherwise, we extract all sentences
and retain only those that have at least 5 terms and
at least 3 unique terms. Sentences are converted into
BM25-weighted vectors in the English term space;
for German sentences, translation into English is ac-
complished using the technique proposed by Dar-
wish and Oard (2003). For every (de, df ) pair that
the input document is found in, the mapper emits the
list of weighted sentence vectors, with the (de, df )
pair as the key. As all intermediate key-value pairs
in MapReduce are grouped by their keys for reduce-
side processing, the reducer receives the key (de, df )
and weighted sentence vectors for both the German
and English articles. From there, we generate the
Cartesian product of sentences in both languages.
As an initial filtering step, we discard all pairs where
627
the ratio of sentence lengths is more than two, a
heuristic proposed in (Munteanu and Marcu, 2005).
Each of the remaining candidate sentences are then
processed by two separate classifiers: a less accurate,
fast classifier and a more accurate, slow classifier.
This is described in the next section.
This algorithm is a variant of what is commonly
known as a reduce-side join in MapReduce (Lin
and Dyer, 2010), where (de, df ) serves as the
join key. Note that in this algorithm, sentence
vectors are emitted multiple times, one for each
(de, df ) pair that they participate in: this results
in increased network traffic during the sort/shuffle
phase. We experimented with an alternative algo-
rithm that processes all foreign documents similar
to the same English document together, e.g., pro-
cessing (de, [df1, df2, . . .]) together. This approach,
counter-intuitively, was slower despite reduced net-
work traffic, due to skew in the distribution of sim-
ilar document pairs. In our experiments, half of the
source collection was not linked to any target docu-
ment, whereas 4% had more than 100 links. This re-
sults in reduce-side load imbalance, and while most
of the reducers finish quickly, a few reducers end
up performing substantially more computation, and
these ?stragglers? increase end-to-end running time.
4 Parallel Sentence Classification
We built two MaxEnt parallel sentence classifiers us-
ing the OpenNLP package, with data from a sam-
ple of the Europarl corpus of European parliament
speeches. For training, we sampled 1000 parallel
sentences from the German-English subset of the
corpus as positive instances, and 5000 non-parallel
sentence pairs as negative instances. For testing, we
sampled another 1000 parallel pairs and generated
all possible non-parallel pairs by the Cartesian prod-
uct of these samples. This provides a better approx-
imation of the task we?re interested in, since most of
the candidate sentence pairs will be non-parallel in a
comparable corpus. We report precision, recall, and
F-score, using different classifier confidence scores
as the decision threshold (see Table 1).
Our first, simple classifier, which uses cosine sim-
ilarity between the sentences as the only feature,
achieved a maximum F-score of 74%, with 80%
precision and 69% recall. Following previous work
Classifier Measure Value
Simple
Recall @ P90 0.59
Recall @ P80 0.69
Best F-score 0.74
Complex
Recall @ P90 0.69
Recall @ P80 0.79
Best F-score 0.80
Table 1: Accuracy of the simple and complex sentence
classifiers on Europarl data.
(Smith et al, 2010), we also report recall with pre-
cision at 80% and 90% in Table 1; the classifier ef-
fectiveness is comparable to the previous work. The
second, complex classifier uses the following addi-
tional features: ratio of sentence lengths, ratio of
source-side tokens that have translations on the tar-
get side, ratio of target-side tokens that have trans-
lations on the source side. We also experimented
with features using the word alignment output, but
there was no improvement in accuracy. The com-
plex classifier showed better performance: recall of
79% at 80% precision and 69% at precision of 90%,
with a maximum F-score of 80%.
Due to the large amounts of data involved in our
experiments, we were interested in speed/accuracy
tradeoffs between the two classifiers. Microbench-
marks were performed on a commodity laptop run-
ning Mac OS X on a 2.26GHz Intel Core Duo CPU,
measuring per-instance classification speed (includ-
ing feature computation time). The complex classi-
fier took 100 ?s per instance, about 4 times slower
than the simple one, which took 27 ?s.
The initial input of 64m similar document pairs
yielded 400b raw candidate sentence pairs, which
were first reduced to 214b by the per-sentence length
filter, and then to 132b by enforcing a maximum sen-
tence length ratio of 2. The simple classifier was
applied to the remaining pairs, with different confi-
dence thresholds. We adjusted the threshold to ob-
tain different amounts of bitext, to see the effect on
translation quality (this condition is called S1 here-
after). The positive results of the first classifier was
then processed by the second classifier (this two-
level approach is called S2 hereafter).
Candidate generation was completed in 2.4 hours
on our cluster with 96 cores. These candidates went
through the MapReduce shuffle-and-sort process in
0.75 hours, which were then classified in 4 hours.
628
Processing by the more complex classifier in S2 took
an additional 0.52 hours.
5 End-to-End MT Experiments
In all experiments, our MT system learned a syn-
chronous context-free grammar (Chiang, 2007), us-
ing GIZA++ for word alignments, MIRA for pa-
rameter tuning (Crammer et al, 2006), cdec for de-
coding (Dyer et al, 2010), a 5-gram SRILM for
language modeling, and single-reference BLEU for
evaluation. The baseline system was trained on the
German-English WMT10 training data, consisting
of 3.1m sentence pairs. For development and test-
ing, we used the newswire datasets provided for
WMT10, including 2525 sentences for tuning and
2489 sentences for testing.
Our baseline system includes all standard fea-
tures, including phrase translation probabilities in
both directions, word and arity penalties, and lan-
guage model scores. It achieves a BLEU score
of 21.37 on the test set, which would place it 5th
out of 9 systems that reported comparable results
in WMT10 (only three systems achieved a BLEU
score over 22). Many of these systems used tech-
niques that exploited the specific aspects of the task,
e.g., German-specific morphological analysis. In
contrast, we present a knowledge-impoverished, en-
tirely data-driven approach, by simply looking for
more data in large collections.
For both experimental conditions (one-step classi-
fication, S1, and two-step classification, S2) we var-
ied the decision threshold to generate new bitext col-
lections of different sizes. Each of these collections
was added to the baseline training data to induce
an entirely new translation model (note that GIZA
additionally filtered out some of the pairs based on
length). The final dataset sizes, along with BLEU
scores on the test data, are shown in Fig. 1. In S1, we
observe that increasing the amount of data (by low-
ering the decision threshold) initially leads to lower
BLEU scores (due to increased noise), but there is a
threshold after which the improvement coming from
the added data supersedes the noise. The S2 condi-
tion increases the quality of bitext by reducing this
noise: the best run, with 5.8m pairs added to the
baseline (final dataset has 8.1m pairs), yields 23.76
BLEU (labeled P on figure), 2.39 points above the
 23
 23.5
 3  3.5  4  4.5  5  5.5  6  6.5  7  7.5  8  8.5
BLE
U sc
ore
Training data size (millions)
Baseline BLEU = 21.37
P
S1 (1-step classification)S2 (2-step classification)Sampling data from training set P
Figure 1: Evaluation results on the WMT10 test set.
baseline (and higher than the best WMT10 result).
These results show that the two-step classification
process, while slower, is worth the additional pro-
cessing time.
Our approach yields solid improvements even
with less data added: with only 382k pairs added
to the baseline, the BLEU score increases by 1.84
points. In order to better examine the effect of
data size alone, we created partial datasets from P
by randomly sampling sentence pairs, and then re-
peated experiments, also shown in Fig. 1. We see
an increasing trend of BLEU scores with respect to
data size. By comparing the three plots, we see that
S2 and random sampling from P work better than
S1. Also, random sampling is not always worse than
S2, since some pairs that receive low classifier con-
fidence turn out to be helpful.
6 Conclusions
In this paper, we describe a scalable MapReduce im-
plementation for automatically mining parallel sen-
tences from arbitrary comparable corpora. We show,
at least for German-English MT, that an impover-
ished, data-driven approach is more effective than
task-specific engineering. With the distributed bi-
text mining machinery described in this paper, im-
provements come basically ?for free? (the only cost
is a modest amount of cluster resources). Given the
availability of data and computing power, there is
simply no reason why MT researchers should not
ride the large-data ?tide? that lifts all boats. For the
benefit of the community, all code necessary to repli-
cate these results have been open sourced, as well as
the bitext we?ve gathered.
629
Acknowledgments
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015; NSF un-
der awards IIS-0916043 and CCF-1018625. Any
opinions, findings, conclusions, or recommenda-
tions expressed in this paper are those of the authors
and do not necessarily reflect the view of the spon-
sors. The second author is grateful to Esther and
Kiri for their loving support and dedicates this work
to Joshua and Jacob.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867, Prague, Czech Re-
public.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Kareem Darwish and Douglas W. Oard. 2003. Analysis
of anchor text for web search. Proceedings of the 26th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR 2003), pages 261?268, Toronto, Canada.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin.
2008. Fast, easy, and cheap: Construction of statisti-
cal machine translation models with MapReduce. Pro-
ceedings of the Third Workshop on Statistical Machine
Translation at ACL 2008, pages 199?207, Columbus,
Ohio.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. Proceedings
of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, July.
Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text
Processing with MapReduce. Morgan & Claypool
Publishers.
Jimmy Lin, Donald Metzler, Tamer Elsayed, and Lidan
Wang. 2009. Of Ivory and Smurfs: Loxodontan
MapReduce experiments for web search. Proceedings
of the Eighteenth Text REtrieval Conference (TREC
2009), Gaithersburg, Maryland.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. Proceedings
of Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2010), pages 403?411, Los Angeles, California.
Ferhan Ture, Tamer Elsayed, and Jimmy Lin. 2011.
No free lunch: Brute force vs. locality-sensitive hash-
ing for cross-lingual pairwise similarity. Proceedings
of the 34th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval (SIGIR 2011), pages 943?952, Beijing, China.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING 2010), pages 1101?1109, Beijing,
China.
630
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199?204,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mr. MIRA: Open-Source Large-Margin Structured Learning on
MapReduce
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{eidelman,wuke,fture,resnik,jimmylin}@umd.edu
Abstract
We present an open-source framework
for large-scale online structured learning.
Developed with the flexibility to handle
cost-augmented inference problems such
as statistical machine translation (SMT),
our large-margin learner can be used with
any decoder. Integration with MapReduce
using Hadoop streaming allows efficient
scaling with increasing size of training
data. Although designed with a focus
on SMT, the decoder-agnostic design of
our learner allows easy future extension to
other structured learning problems such as
sequence labeling and parsing.
1 Introduction
Structured learning problems such as sequence la-
beling or parsing, where the output has a rich in-
ternal structure, commonly arise in NLP. While
batch learning algorithms adapted for structured
learning such as CRFs (Lafferty et al, 2001)
and structural SVMs (Joachims, 1998) have re-
ceived much attention, online methods such as
the structured perceptron (Collins, 2002) and a
family of Passive-Aggressive algorithms (Cram-
mer et al, 2006) have recently gained promi-
nence across many tasks, including part-of-speech
tagging (Shen, 2007), parsing (McDonald et
al., 2005) and statistical machine translation
(SMT) (Chiang, 2012), due to their ability to deal
with large training sets and high-dimensional in-
put representations.
Unlike batch learners, which must consider all
examples when optimizing the objective, online
learners operate in rounds, optimizing using one
example or a handful of examples at a time. This
online nature offers several attractive properties,
facilitating scaling to large training sets while re-
maining simple and offering fast convergence.
Mr. MIRA, the open source system1 de-
scribed in this paper, implements an online large-
margin structured learning algorithm based on
MIRA (?2.1), for cost-augmented online large-
scale training in high-dimensional feature spaces.
Our contribution lies in providing the first pub-
lished decoder-agnostic parallelization of MIRA
with Hadoop for structured learning.
While the current demonstrated application fo-
cuses on large-scale discriminative training for
machine translation, the learning algorithm is gen-
eral with respect to the inference algorithm em-
ployed. We are able to decouple our learner en-
tirely from the MT decoder, allowing users to
specify their own inference procedure through a
simple text communication protocol (?2.2). The
learner only requires k-best output with feature
vectors, as well as the specification of a cost func-
tion. Standard automatic evaluation metrics for
MT, such as BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006), have already been imple-
mented. Furthermore, our system can be extended
to other structured learning problems with a min-
imal amount of effort, simply by implementing a
task-specific cost function and specifying an ap-
propriate decoder.
Through Hadoop streaming, our system can
take advantage of commodity clusters to handle
large-scale training (?3), while also being capable
of running in environments ranging from a single
machine to a PBS-managed batch cluster. Experi-
mental results (?4) show that it scales linearly and
makes fast parameter tuning on large tuning sets
for SMT practical.
2 Learning and Inference
2.1 Online Large-Margin Learning
MIRA is a popular online large-margin structured
learning method for NLP tasks (McDonald et al,
2005; Chiang et al, 2009; Chiang, 2012). The
1https://github.com/kho/mr-mira
199
main intuition is that we want our model to enforce
a margin between the correct and incorrect out-
puts of a sentence that agrees with our cost func-
tion. This is done by making the smallest update
we can to our parameters, w, on every sentence,
that will ensure that the difference in model scores
?fi(y?) = w>(f(xi, y+) ? f(xi, y?)) between the
correct output y+ and incorrect output y? is at least
as large as the cost, ?i(y?), incurred by predicting
the incorrect output:2
wt+1 = arg minw
1
2 ||w ?wt||
2 + C?i
s.t. ?y? ? Y(xi), ?fi(y?) ? ?i(y?)? ?i
where Y(xi) is the space of possible structured
outputs we are able to produce from xi, and
C is a regularization parameter that controls the
size of the update. In practice, we can de-
fine Y(xi) to be the k-best output. With a
passive-aggressive (PA) update, the ?y? constraint
above can be approximated by selecting the sin-
gle most violated constraint, which maximizes
y? ? arg maxy?Y(xi) w>f(xi, y) + ?i(y). This
optimization problem is attractive because it re-
duces to a simple analytical solution, essentially
performing a subgradient descent step with the
step size adjusted based on each example:
?? min
(
C, ?i(y
?)? ?fi(y?)
?f(xi, y+)? f(xi, y?)?2
)
w? w + ??
(
f(xi, y+)? f(xi, y?)
)
The user-defined cost function is a task-specific
external measure of quality that relays how bad se-
lecting y? truly is on the task we care about. The
cost can take any form as long as it decomposes
across the local parts of the structure, just as the
feature functions. For instance, it could be the
Hamming loss for sequence labeling, F-score for
parsing, or an approximate BLEU score for SMT.
Cost-augmented Inference For most struc-
tured prediction problems in machine learning,
yi ? Y(xi), that is, the model is able to produce,
and thus score, the correct output structure, mean-
ing y+ = yi. However, for certain NLP prob-
lems this may not be the case. For instance in
SMT, our model may not be able to produce or
reach the correct reference translation, which pro-
hibits our model from scoring it. This problem
2For a more formal description we refer the reader
to (Crammer et al, 2006; Chiang, 2012).
necessitates cost-augmented inference, where we
select y+ ? arg maxy?Y(xi) w>f(xi, y)??i(y)
from the space of structures our model can pro-
duce, to stand in for the correct output in optimiza-
tion. Our system was developed to handle both
cases, with the decoder providing the k-best list
to the learner, specifying whether to perform cost-
augmented selection.
Sparse Features While utilizing sparse features
is a primary motivation for performing large-scale
discriminative training, which features to use and
how to learn their weights can have a large im-
pact on the potential benefit. To this end, we in-
corporate `1/`2 regularization for joint feature se-
lection in order to improve efficiency and counter
overfitting effects (Simianer et al, 2012). Further-
more, the PA update has a single learning rate ?
for all features, which specifies how much the fea-
ture weights can change at each update. How-
ever, since dense features (e.g., language model)
are observed far more frequently than sparse fea-
tures (e.g., rule id), we may instead want to use
a per-feature learning rate that allows larger steps
for features that do not have much support. Thus,
we allow setting an adaptive per-feature learning
rate (Green et al, 2013; Crammer et al, 2009;
Duchi et al, 2011).
2.2 Learner/Decoder Communication
Training requires communication between the de-
coder and the learner. The decoder needs to re-
ceive weight updates and the input sentence from
the learner; and the learner needs to receive k-best
output with feature vectors from the decoder. This
is essentially all the required communication be-
tween the learner and the decoder. Below, we de-
scribe a simple line-based text protocol.
Input sentence and weight updates Follow-
ing common practice in machine translation, the
learner encodes each input sentence as a single-
line SGML entry named seg and sends it to the
decoder. The first line of Figure 1 is an exam-
ple sentence in this format. In addition to the
required sentence ID (useful in parallel process-
ing), an optional delta field is used to encode
the weight updates, as a sparse vector indexed
by feature names. First, for each name and up-
date pair, a binary record consisting of a null-
terminated string (name) and a double-precision
floating point number in native byte order (up-
date) is created. Then, all binary records are con-
200
<seg id="123" delta="TE0AexSuR+F6hD8="> das ist ein kleine haus </seg>
<seg id="124"> ein kleine haus </seg>\tein kleine ||| a small\thaus ||| house
Figure 1: Example decoder input in SGML
5
123 ||| 5 ||| this is a small house ||| TE0AAAAA... <base64> ||| 120.3
123 ||| 5 ||| this is the small house ||| <base64> ||| 118.4
123 ||| 5 ||| this was small house ||| <base64> ||| 110.5
<empty>
<empty>
Figure 2: Example k-best output
catenated and encoded in base64. In the example
above, the value of delta is the base64 encod-
ing of 0x4c 0x4d 0x00 0x7b 0x14 0xae 0x47
0xe1 0x7a 0x84 0x3f. The first 3 bytes store the
feature name (LM) and the next 8 bytes is its update
(0.01), to be added to the decoder?s current value
of the corresponding feature weight.
The learner also allows the user to pass any ad-
ditional information to the decoder, as long as it
can be encoded as a single-line text string. Such
information, if given, is appended after the seg en-
try, with a leading tab character as the delimiter.
For example, the second line of Figure 1 passes
two phrase translation rules to the decoder.
k-best output The decoder reads from standard
input and outputs the k-best output for one input
sentence before consuming the next line. For the
k-best output, the decoder first outputs to standard
output a line consisting of a single integerN . Next
the decoder outputs N lines where each line can
be either empty or an actual hypothesis. When the
line is an actual hypothesis, it consists of the fol-
lowing parts:
SID ||| LEN ||| TOK ||| FEAT [ REST ]
SID is the sentence ID of the corresponding input;
LEN is the length of source sentence;3 TOK contains
the tokens of the hypothesis sentence separated by
spaces; FEAT is the feature vector, encoded in the
same way as the weight updates, delimited by a
whitespace. Everything after FEAT until the end of
the line is discarded. See Figure 2 for an example
of k-best output. Note the scores after the last |||
are discarded by the learner.
Overall workflow The learner reads lines from
standard input in the following tab-delimited for-
mat:
3This is used in computing the smoothed cost. Usually
this is identical for all hypotheses if the input is a plain sen-
tence. But in applications such as lattice-based translation,
each hypothesis can be produced from different source sen-
tences, resulting in different lengths.
SRC<tab>REF<tab>REST
SRC is the actual input sentence as a seg entry; REF
is the gold output for the input sentence, for ex-
ample, reference translations in MT;4 REST is the
additional information that will be appended after
the seg entry and passed to the decoder.
The learner creates a sub-process for the de-
coder and connects to the sub-process? standard
input and output with pipes. Then it processes the
input lines one by one. For each line, it first sends
a composed input message to the decoder, combin-
ing the input sentence, weight updates, and user-
supplied information. Next it collects the k-best
output from the decoder, solves the QP problem to
obtain weight updates and repeats.
The learner produces two types of output. First,
the 1-best hypothesis for each input sentence, in
the following format:
SID<tab>TOK
Second, when there are no more input lines, the
learner outputs final weights and the number of
lines processed, in the following format:
-1<tab>NUM ||| WEIGHTS
The 1-best hypotheses can be scored against ref-
erences to obtain an estimate of cost. The final
weights are stored in a way convenient for averag-
ing in a parallel setting, as we shall discuss next.
3 Large-Scale Discriminative Training
3.1 MapReduce
With large amounts of data available today,
distributed computations have become essen-
tial. MapReduce (Dean and Ghemawat, 2004)
has emerged as a popular distributed process-
ing framework for commodity clusters that has
gained widespread adoption in both industry and
academia, thanks to its simplicity and the avail-
ability of the Hadoop open-source implementa-
tion. MapReduce provides a higher level of
4There can be multiple references, separated by |||.
201
abstraction for designing distributed algorithms
compared to, say, MPI or pthreads, by hiding
system-level details (e.g., deadlock, race condi-
tions, machine failures) from the developer.
A single MapReduce program begins with a
map phase, where mapper processes input key-
value pairs to produce an arbitrary number of in-
termediate key-value pairs. The mappers execute
in parallel, consuming data splits independently.
Following the map phase, all key-value pairs emit-
ted by the mappers are sorted by key and dis-
tributed to the reducers, such that all pairs shar-
ing the same key are guaranteed to arrive at the
same reducer. Finally, in the reduce phase, each
reducer processes the intermediate key-value pairs
it receives and emits final output key-value pairs.
3.2 System Architecture
Algorithm design We use Hadoop streaming to
parallelize the training process. Hadoop stream-
ing allows any arbitrary executable to serve as the
mapper or reducer, as long as it handles key-value
pairs properly.5 One iteration of training is im-
plemented as a single Hadoop streaming job. In
the map step, our learner can be directly used as
the mapper. Each mapper loads the same initial
weights, processes a single split of data and pro-
duces key-value pairs: the one-best hypothesis of
each sentence is output with the sentence ID as
the key (non-negative); the final weights with re-
spect to the split are output with a special negative
key. In the reduce step, a single reducer collects all
key-value pairs, grouped and sorted by keys. The
one-best hypotheses are output to disk in the or-
der they are received, so that the order matches the
reference translation set. The reducer also com-
putes the feature selection and weighted average
of final weights received from all of the mappers.
Assuming mapper i produces the final weights wi
after processing ni sentences, the weighted aver-
aged is defined as w? =
?
iwi?ni?
i ni
. Although aver-
aging yields different result from running a single
learner over the entire data, we have found the dif-
ference to be quite small in terms of convergence
and quality of tuned weights in practice.
After the reducer finishes, the averaged weights
are extracted and used as the initial weights for the
next iteration; the emitted hypotheses are scored
5By default, each line is treated as a key-value pair en-
coded in text, where the key and the value are separated by a
<tab>.
against the references, which allows us to track the
learning curve and the progress of convergence.
Scalability In an application such as SMT, the
decoder requires access to the translation gram-
mar and language model to produce translation hy-
potheses. For small tuning sets, which have been
typical in MT research, having these files trans-
ferred across the network to individual servers
(which then load the data into memory) is not
a problem. However, for even modest input on
the order of tens of thousands of sentences, this
creates a challenge. For example, distributing
thousands of per-sentence grammar files to all the
workers in a Hadoop cluster is time-consuming,
especially when this needs to be performed prior
to every iteration.
To benefit from MapReduce, it is essential to
avoid dependencies on ?side data? as much as
possible, due to the challenges explained above
with data transfer. To address this issue, we ap-
pend the per-sentence translation grammar as user-
supplied additional information to each input sen-
tence. This results in a large input file (e.g., 75 gi-
gabytes for 50,000 sentences), but this is not an is-
sue since the data reside on the Hadoop distributed
file system and MapReduce optimizes for data lo-
cality when scheduling mappers.
Unfortunately, it is much more difficult to ob-
tain per-sentence language models that are small
enough to handle in this same manner. Currently,
the best solution we have found is to use Hadoop?s
distributed cache to ship the single large language
model to each worker.
4 Evaluation
We evaluated online learning in Hadoop Map-
Reduce by applying it to German-English ma-
chine translation, using our hierarchical phrase-
based translation system with cdec as the de-
coder (Dyer et al, 2010). The parallel training
data consist of the Europarl and News Commen-
tary corpora from the WMT12 translation task,6
containing 2.08M sentences. A 5-gram language
model was trained on the English side of the bi-
text along with 750M words of news using SRILM
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996).
We experimented with two feature sets: (1) a
small set with standard MT features, including
6http://www.statmt.org/wmt12/translation-task.html
202
Tuning set size Time/iteration # splits # features Tuning BLEU Test
(corpus) (on disk, GB) (in seconds) BLEU TER
dev 3.3 119 120 16 22.38 22.69 60.61
5k 7.8 289 120 16 32.60 22.14 59.60
10k 15.2 432 120 16 33.16 22.06 59.43
25k 37.2 942 300 16 32.48 22.21 59.54
50k 74.5 1802 600 16 32.21 22.21 59.39
dev 3.3 232 120 85k 23.08 23.00 60.19
5k 7.8 610 120 159k 33.70 22.26 59.26
10k 15.2 1136 120 200k 34.00 22.12 59.24
25k 37.2 2395 300 200k 32.96 22.35 59.29
50k 74.5 4465 600 200k 32.86 22.40 59.15
Table 1: Evaluation of our Hadoop implementation of MIRA, showing running time as well as BLEU
and TER values for tuning and testing data.
dev test 5k 10k 25k 50k
Sentences 3003 3003 5000 10000 25000 50000
Tokens en 75k 74k 132k 255k 634k 1258k
Tokens de 74k 73k 133k 256k 639k 1272k
Table 2: Corpus statistics
phrase and lexical translation probabilities in both
directions, word and arity penalties, and language
model scores; and (2) a large set containing the top
200k sparse features that might be useful to train
on large numbers of instances: rule id and shape,
target bigrams, insertions and deletions, and struc-
tural distortion features.
All experiments were conducted on a Hadoop
cluster (running Cloudera?s distribution, CDH
4.2.1) with 16 nodes, each with two quad-core 2.2
GHz Intel Nehalem Processors, 24 GB RAM, and
three 2 TB drives. In total, the cluster is configured
for a capacity of 128 parallel workers, although
we do not have direct control over the number
of simultaneous mappers, which depends on the
number of input splits. If the number of splits is
smaller than 128, then the cluster is under-utilized.
To note this, we report the number of splits for
each setting in our experimental results (Table 1).
We ran MIRA on a number of tuning sets, de-
scribed in Table 2, in order to test the effective-
ness and scalability of our system. First, we used
the standard development set from WMT12, con-
sisting of 3,003 sentences from news domain. In
order to show the scaling characteristics of our ap-
proach, we then used larger portions of the train-
ing bitext directly to tune parameters. In order to
avoid overfitting, we used a jackknifing method
to split the training data into n = 10 folds, and
built a translation system on n ? 1 folds, while
adjusting the sampling rate to sample sentences
from the other fold to obtain tuning sets ranging
from 5,000 sentences to 50,000 sentences. Table 1
shows details of experimental results for each set-
ting. The second column shows the space each
tuning set takes up on disk when we include refer-
ence translations and grammar files along with the
sentences. The reported tuning BLEU is from the
iteration with best performance, and running times
are reported from the top-scoring iteration as well.
Even though our focus in this evaluation is to
show the scalability of our implementation to large
input and feature sets, it is also worthwhile to men-
tion the effectiveness aspect. As we increase the
tuning set size by sampling sentences from the
training data, we see very little improvement in
BLEU and TER with the smaller feature set. This
is not surprising, since sparse features are more
likely to gain from additional tuning instances. In-
deed, tuning scores for all sets improve substan-
tially with sparse features, accompanied by small
increases on test.
While tuning on dev data results in better BLEU
on test data than when tuning on the larger sets, it
is important to note that although we are able to
tune more features on the larger bitext tuning sets,
they are not composed of the same genre as the
dev and test sets, resulting in a domain mismatch.
203
Therefore, we are actually comparing a smaller in-
domain tuning set with a larger out-of-domain set.
While this domain adaptation is problematic (Had-
dow and Koehn, 2012), the ability to discrimina-
tively tune on larger sets remains highly desirable.
In terms of running time, we observe that the al-
gorithm scales linearly with respect to input size,
regardless of the feature set. With more features,
running time increases due to a more complex
translation model, as well as larger intermediate
output (i.e., amount of information passed from
mappers to reducers). The scaling characteristics
point out the strength of our system: our scalable
MIRA implementation allows one to tackle learn-
ing problems where there are many parameters,
but also many training instances.
Comparing the wall clock time of paralleliza-
tion with Hadoop to the standard mode of 10?20
learner parallelization (Haddow et al, 2011; Chi-
ang et al, 2009), for the small 25k feature set-
ting, after one iteration, which takes 4625 sec-
onds using 15 learners on our PBS cluster, the tun-
ing score is 19.5 BLEU, while in approximately
the same time, we can perform five iterations
with Hadoop and obtain 30.98 BLEU. While this
is not a completely fair comparison, as the two
clusters utilize different resources and the num-
ber of learners, it suggests the practical benefits
that Hadoop can provide. Although increasing the
number of learners on our PBS cluster to the num-
ber of mappers used in Hadoop would result in
roughly equivalent performance, arbitrarily scal-
ing out learners on the PBS cluster to handle larger
training sets can be challenging since we?d have to
manually coordinate the parallel processes in an
ad-hoc manner. In contrast, Hadoop provides scal-
able parallelization in a manageable framework,
providing data distribution, synchronization, fault
tolerance, as well as other features, ?for free?.
5 Conclusion
In this paper, we presented an open-source
framework that allows seamlessly scaling struc-
tured learning to large feature-rich problems with
Hadoop, which lets us take advantage of large
amounts of data as well as sparse features. The
development of Mr. MIRA has been motivated pri-
marily by application to SMT, but we are planning
to extend our system to other structured prediction
tasks in NLP such as parsing, as well as to facili-
tate its use in other domains.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship. Any opinions, findings, con-
clusions, or recommendations expressed are those
of the authors and do not necessarily reflect views
of the sponsors.
References
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In ACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new fea-
tures for statistical machine translation. In NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative training
of statistical translation models. JMLR, 13:1159?1187.
M. Collins. 2002. Ranking algorithms for named-entity ex-
traction: boosting and the voted perceptron. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive
regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blun-
som, H. Setiawan, V. Eidelman, and P. Resnik. 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL
System Demonstrations.
S. Green, S. Wang, D. Cer, and C. Manning. 2013. Fast and
adaptive online training of feature-rich translation models.
In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect of out-
of-domain data on smt systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. In WMT.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features. In
ECML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL.
L. Shen. 2007. Guided learning for bidirectional sequence
classification. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-scale
discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
204
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 128?133,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Towards Efficient Large-Scale Feature-Rich Statistical Machine
Translation
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{vlad,wuke,fture,resnik,jimmylin}@umiacs.umd.edu
Abstract
We present the system we developed to
provide efficient large-scale feature-rich
discriminative training for machine trans-
lation. We describe how we integrate with
MapReduce using Hadoop streaming to
allow arbitrarily scaling the tuning set and
utilizing a sparse feature set. We report our
findings on German-English and Russian-
English translation, and discuss benefits,
as well as obstacles, to tuning on larger
development sets drawn from the parallel
training data.
1 Introduction
The adoption of discriminative learning methods
for SMT that scale easily to handle sparse and lex-
icalized features has been increasing in the last
several years (Chiang, 2012; Hopkins and May,
2011). However, relatively few systems take full
advantage of the opportunity. With some excep-
tions (Simianer et al, 2012), most still rely on
tuning a handful of common dense features, along
with at most a few thousand others, on a relatively
small development set (Cherry and Foster, 2012;
Chiang et al, 2009). While more features tuned
on more data usually results in better performance
for other NLP tasks, this has not necessarily been
the case for SMT.
Thus, our main focus in this paper is to improve
understanding into the effective use of sparse fea-
tures, and understand the benefits and shortcom-
ings of large-scale discriminative training. To
this end, we conducted experiments for the shared
translation task of the 2013 Workshop on Statis-
tical Machine Translation for the German-English
and Russian-English language pairs.
2 Baseline system
We use a hierarchical phrase-based decoder im-
plemented in the open source translation system
cdec1 (Dyer et al, 2010). For tuning, we use
Mr. MIRA2 (Eidelman et al, 2013), an open
source decoder agnostic implementation of online
large-margin learning in Hadoop MapReduce. Mr.
MIRA separates learning from the decoder, allow-
ing the flexibility to specify the desired inference
procedure through a simple text communication
protocol. The decoder receives input sentences
and weight updates from the learner, while the
learner receives k-best output with feature vectors
from the decoder.
Hadoop MapReduce (Dean and Ghemawat,
2004) is a popular distributed processing frame-
work that has gained widespread adoption, with
the advantage of providing scalable parallelization
in a manageable framework, taking care of data
distribution, synchronization, fault tolerance, as
well as other features. Thus, while we could oth-
erwise achieve the same level of parallelization, it
would be in a more ad-hoc manner.
The advantage of online methods lies in their
ability to deal with large training sets and high-
dimensional input representations while remain-
ing simple and offering fast convergence. With
Hadoop streaming, our system can take advantage
of commodity clusters to handle parallel large-
scale training while also being capable of running
on a single machine or PBS-managed batch clus-
ter.
System design To efficiently encode the infor-
mation that the learner and decoder require (source
sentence, reference translation, grammar rules) in
a manner amenable to MapReduce, i.e. avoiding
dependencies on ?side data? and large transfers
across the network, we append the reference and
1http://cdec-decoder.org
2https://github.com/kho/mr-mira
128
per-sentence grammar to each input source sen-
tence. Although this file?s size is substantial, it is
not a problem since after the initial transfer, it re-
sides on Hadoop distributed file system, and Map-
Reduce optimizes for data locality when schedul-
ing mappers.
A single iteration of training is performed as
a Hadoop streaming job. Each begins with a
map phase, with every parallel mapper loading the
same initial weights and decoding and updating
parameters on a shard of the data. This is followed
by a reduce phase, with a single reducer collect-
ing final weights from all mappers and computing
a weighted average to distribute as initial weights
for the next iteration.
Parameter Settings We tune our system toward
approximate sentence-level BLEU (Papineni et al,
2002),3 and the decoder is configured to use cube
pruning (Huang and Chiang, 2007) with a limit
of 200 candidates at each node. For optimiza-
tion, we use a learning rate of ?=1, regularization
strength of C=0.01, and a 500-best list for hope
and fear selection (Chiang, 2012) with a single
passive-aggressive update for each sentence (Ei-
delman, 2012).
Baseline Features We used a set of 16 stan-
dard baseline features: rule translation relative
frequency P (e|f), lexical translation probabilities
Plex(e|f) and Plex(f |e), target n-gram language
model P (e), penalties for source and target words,
passing an untranslated source word to the tar-
get side, singleton rule and source side, as well
as counts for arity-0,1, or 2 SCFG rules, the total
number of rules used, and the number of times the
glue rule is used.
2.1 Data preparation
For both languages, we used the provided Eu-
roparl and News Commentary parallel training
data to create the translation grammar neces-
sary for our model. For Russian, we addi-
tionally used the Common Crawl and Yandex
data. The data were lowercased and tokenized,
then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many align-
ments in both directions and symmetrized sing the
grow-diag-final-and method (Koehn et al, 2003).
3We approximate corpus BLEU by scoring sentences us-
ing a pseudo-document of previous 1-best translations (Chi-
ang et al, 2009).
We constructed a 5-gram language model us-
ing SRILM (Stolcke, 2002) from the provided
English monolingual training data and parallel
data with modified Kneser-Ney smoothing (Chen
and Goodman, 1996), which was binarized using
KenLM (Heafield, 2011). The sentence-specific
translation grammars were extracted using a suffix
array rule extractor (Lopez, 2007).
For German, we used the 3,003 sentences in
newstest2011 as our Dev set, and report results
on the 3,003 sentences of the newstest2012 Test
set using BLEU and TER (Snover et al, 2006).
For Russian, we took the first 2,000 sentences of
newstest2012 for Dev, and report results on the re-
maining 1,003. For both languages, we selected
1,000 sentences from the bitext to be used as an
additional testing set (Test2).
Compound segmentation lattices As German
is a morphologically rich language with produc-
tive compounding, we use word segmentation lat-
tices as input for the German translation task.
These lattices encode alternative segmentations of
compound words, allowing the decoder to auto-
matically choose which segmentation is best. We
use a maximum entropy model with recommended
settings to create lattices for the dev and test sets,
as well as for obtaining the 1-best segmentation of
the training data (Dyer, 2009).
3 Evaluation
This section describes the experiments we con-
ducted in moving towards a better understanding
of the benefits and challenges posed by large-scale
high-dimensional discriminative tuning.
3.1 Sparse Features
The ability to incorporate sparse features is the pri-
mary reason for the recent move away from Min-
imum Error Rate Training (Och, 2003), as well as
for performing large-scale discriminative training.
We include the following sparse Boolean feature
templates in our system in addition to the afore-
mentioned baseline features: rule identity (for ev-
ery unique rule in the grammar), rule shape (map-
ping rules to sequences of terminals and nontermi-
nals), target bigrams, lexical insertions and dele-
tions (for the top 150 unaligned words from the
training data), context-dependent word pairs (for
the top 300 word pairs in the training data), and
structural distortion (Chiang et al, 2008).
129
Dev Test Test2 5k 10k 25k 50k
en 75k 74k 27k 132k 255k 634k 1258k
de 74k 73k 26k 133k 256k 639k 1272k
Table 1: Corpus statistics in tokens for German.
Dev Test Test2 15k
ru 46k 24k 24k 350k
en 50k 27k 25k 371k
Table 2: Corpus statistics in tokens for
Russian.
Set # features Tune Test
?BLEU ?BLEU ?TER
de-en 16 22.38 22.69 60.61
+sparse 108k 23.86 23.01 59.89
ru-en 16 30.18 29.89 49.05
+sparse 77k 32.40 30.81 48.40
Table 3: Results with the addition of sparse fea-
tures for German and Russian.
All of these features are generated from the
translation rules on the fly, and thus do not have
to be stored as part of the grammar. To allow for
memory efficiency while scaling the training data,
we hash all the lexical features from their string
representation into a 64-bit integer.
Altogether, these templates result in millions of
potential features, thus how to select appropriate
features, and how to properly learn their weights
can have a large impact on the potential benefit.
3.2 Adaptive Learning Rate
The passive-aggressive update used in MIRA has a
single learning rate ? for all features, which along
with ? limits the amount each feature weight can
change at each update. However, since the typical
dense features (e.g., language model) are observed
far more frequently than sparse features (e.g., rule
identity), it has been shown to be advantageous
to use an adaptive per-feature learning rate that
allows larger steps for features that do not have
much support (Green et al, 2013; Duchi et al,
2011). Essentially, instead of having a single pa-
rameter ?,
?? min
(
C, cost(y
?)?w>(f(y+)? f(y?))
?f(y+)? f(y?)?2
)
w? w + ??
(
f(y+)? f(y?)
)
we instead have a vector ? with one entry for each
feature weight:
??1 ? ??1 + ?diag
(
ww>
)
w? w + ??1/2
(
f(y+)? f(y?)
)
?=1 
?=0.01 
?=0.1 
22.2 
22.4 
22.6 
22.8 
23 
23.2 
23.4 
23.6 
23.8 
24 
BLE
U 
Iteration 
Figure 1: Learning curves for tuning when using
a single step size (?) versus different per-feature
learning rates.
In practice, this update is very similar to that of
AROW (Crammer et al, 2009; Chiang, 2012).
Figure 1 shows learning curves for sparse mod-
els with a single learning rate, and adaptive learn-
ing with ?=0.01 and ?=0.1, with associated re-
sults on Test in Table 4.4 As can be seen, using
a single ? produces almost no gain on Dev. How-
ever, while both settings using an adaptive rate fare
better, the proper setting of ? is important. With
?=0.01 we observe 0.5 BLEU gain over ?=0.1 in
tuning, which translates to a small gain on Test.
Henceforth, we use an adaptive learning rate with
?=0.01 for all experiments.
Table 3 presents baseline results for both lan-
guages. With the addition of sparse features, tun-
ing scores increase by 1.5 BLEU for German, lead-
ing to a 0.3 BLEU increase on Test, and 2.2 BLEU
for Russian, with 1 BLEU increase on Test. The
majority of active features for both languages are
rule id (74%), followed by target bigrams (14%)
and context-dependent word pairs (11%).
3.3 Feature Selection
As the tuning set size increases, so do the num-
ber of active features. This may cause practi-
cal problems, such as reduced speed of computa-
tion and memory issues. Furthermore, while some
4All sparse models are initialized with the same tuned
baseline weights. Learning rates are local to each mapper.
130
Adaptive # feat. Tune Test
?BLEU ?BLEU ?TER
none 74k 22.75 22.87 60.19
?=0.01 108k 23.86 23.01 59.89
?=0.1 62k 23.32 22.92 60.09
Table 4: Results with different ? settings for using a per-feature learning rate with sparse features.
Set # feat. Tune Test
?BLEU ?BLEU ?TER
all 510k 32.99 22.36 59.26
top 200k 200k 32.96 22.35 59.29
all 373k 34.26 28.84 49.29
top 200k 200k 34.45 28.98 49.30
Table 5: Comparison of using all features versus
top k selection.
sparse features will generalize well, others may
not, thereby incurring practical costs with no per-
formance benefit. Simianer et al (2012) recently
explored `1/`2 regularization for joint feature se-
lection for SMT in order to improve efficiency and
counter overfitting effects. When performing par-
allel learning, this allows for selecting a reduced
set of the top k features at each iteration that are
effective across all learners.
Table 5 compares selecting the top 200k fea-
tures versus no selection for a larger German and
Russian tuning set (?3.4). As can be seen, we
achieve the same performance with the top 200k
features as we do when using double that amount,
while the latter becomes increasing cumbersome
to manage. Therefore, we use a top 200k selection
for the remainder of this work.
3.4 Large-Scale Training
In the previous section, we saw that learning
sparse features on the small development set leads
to substantial gains in performance. Next, we
wanted to evaluate if we can obtain further gains
by scaling the tuning data to learn parameters di-
rectly on a portion of the training bitext. Since the
bitext is used to learn rules for translation, using
the same parallel sentences for grammar extrac-
tion as well as for tuning feature weights can lead
to severe overfitting (Flanigan et al, 2013). To
avoid this issue, we used a jackknifing method to
split the training data into n = 10 folds, and built
a translation system on n?1 folds, while sampling
sentences from the News Commentary portion of
the held-out fold to obtain tuning sets from 5,000
to 50,000 sentences for German, and 15,000 sen-
tences for Russian.
Results for large-scale training for German are
presented in Table 6. Although we cannot com-
pare the tuning scores across different size sets,
we can see that tuning scores for all sets improve
substantially with sparse features. Unfortunately,
with increasing tuning set size, we see very little
improvement in Test BLEU and TER with either
feature set. Similar findings for Russian are pre-
sented in Table 7. Introducing sparse features im-
proves performance on each set, respectively, but
Dev always performs better on Test.
While tuning on Dev data results in better BLEU
on Test than when tuning on the larger sets, it is
important to note that although we are able to tune
more features on the larger bitext tuning sets, they
are not composed of the same genre as the Tune
and Test sets, resulting in a domain mismatch.
This phenomenon is further evident in German
when testing each model on Test2, which is se-
lected from the bitext, and is thus closer matched
to the larger tuning sets, but is separate from both
the parallel data used to build the translation model
and the tuning sets. Results on Test2 clearly show
significant improvement using any of the larger
tuning sets versus Dev for both the baseline and
sparse features. The 50k sparse setting achieves
almost 1 BLEU and 2 TER improvement, showing
that there are significant differences between the
Dev/Test sets and sets drawn from the bitext.
For Russian, we amplified the effects by select-
ing Test2 from the portion of the bitext that is sepa-
rate from the tuning set, but is among the sentences
used to create the translation model. The effects of
overfitting are markedly more visible here, as there
is almost a 7 BLEU difference between tuning on
Dev and the 15k set with sparse features. Further-
more, it is interesting to note when looking at Dev
that using sparse features has a significant nega-
tive impact, as the baseline tuned Dev performs
131
Tuning Test
?BLEU ?TER
5k 22.81 59.90
10k 22.77 59.78
25k 22.88 59.77
50k 22.86 59.76
Table 8: Results for German with 2 iterations of
tuning on Dev after tuning on larger set.
reasonably well, while the introduction of sparse
features leads to overfitting the specificities of the
Dev/Test genre, which are not present in the bitext.
We attempted two strategies to mitigate this
problem: combining the Dev set with the larger
bitext tuning set from the beginning, and tuning
on a larger set to completion, and then running 2
additional iterations of tuning on the Dev set using
the learned model. Results for tuning on Dev and a
larger set together are presented in Table 7 for Rus-
sian and Table 6 for German. As can be seen, the
resulting model improves somewhat on the other
genre and strikes a middle ground, although it is
worse on Test than Dev.
Table 8 presents results for tuning several ad-
ditional iterations after learning a model on the
larger sets. Although this leads to gains of around
0.5 BLEU on Test, none of the models outperform
simply tuning on Dev. Thus, neither of these two
strategies seem to help. In future work, we plan
to forgo randomly sampling the tuning set from
the bitext, and instead actively select the tuning
set based on similarity to the test set.
4 Conclusion
We explored strategies for scaling learning for
SMT to large tuning sets with sparse features.
While incorporating an adaptive per-feature learn-
ing rate and feature selection, we were able to
use Hadoop to efficiently take advantage of large
amounts of data. Although discriminative training
on larger sets still remains problematic, having the
capability to do so remains highly desirable, and
we plan to continue exploring methods by which
to leverage the power of the bitext effectively.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship.
References
S. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
ACL.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A decoder, alignment, and
learning framework for finite-state and context-free
translation models. In ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA: Open-
source large-margin structured learning on map-
reduce. In ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In WMT.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In NAACL.
S. Green, S. Wang, D. Cer, and C. Manning. 2013.
Fast and adaptive online training of feature-rich
translation models. In ACL.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
132
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 22.38 22.69 60.61 29.31 54.26
5k 120 16 32.60 22.14 59.60 29.69 52.96
10k 120 16 33.16 22.06 59.43 29.93 52.37
Dev+10k 120 16 19.40 22.32 59.37 30.17 52.45
25k 300 16 32.48 22.21 59.54 30.03 51.71
50k 600 16 32.21 22.21 59.39 29.94 52.55
Dev 120 108k 23.86 23.01 59.89 29.65 53.86
5k 120 159k 33.70 22.26 59.26 30.53 51.84
10k 120 200k 34.00 22.12 59.24 30.51 51.71
Dev+10k 120 200k 19.62 22.42 59.17 30.26 52.21
25k 300 200k 32.96 22.35 59.29 30.39 52.14
50k 600 200k 32.86 22.40 59.15 30.54 51.88
Table 6: German evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 30.18 29.89 49.05 57.14 32.56
15k 200 16 34.65 28.60 49.63 59.64 30.65
Dev+15k 200 16 33.97 28.88 49.37 58.24 31.81
Dev 120 77k 32.40 30.81 48.40 52.90 36.85
15k 200 200k 35.05 28.34 49.69 59.81 30.59
Dev+15k 200 200k 34.45 28.98 49.30 57.61 32.71
Table 7: Russian evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint fea-
ture selection in distributed stochastic learning for
large-scale discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP.
133
