Rapid Prototyping of Robust Language Understanding Modules
for Spoken Dialogue Systems
?Yuichiro Fukubayashi, ?Kazunori Komatani, ?Mikio Nakano,
?Kotaro Funakoshi, ?Hiroshi Tsujino, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{fukubaya,komatani}@kuis.kyoto-u.ac.jp
{ogata,okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
nakano@jp.honda-ri.com
{funakoshi,tsujino}@jp.honda-ri.com
Abstract
Language understanding (LU) modules for
spoken dialogue systems in the early phases
of their development need to be (i) easy
to construct and (ii) robust against vari-
ous expressions. Conventional methods of
LU are not suitable for new domains, be-
cause they take a great deal of effort to
make rules or transcribe and annotate a suf-
ficient corpus for training. In our method,
the weightings of the Weighted Finite State
Transducer (WFST) are designed on two
levels and simpler than those for conven-
tional WFST-based methods. Therefore,
our method needs much fewer training data,
which enables rapid prototyping of LU mod-
ules. We evaluated our method in two dif-
ferent domains. The results revealed that our
method outperformed baseline methods with
less than one hundred utterances as training
data, which can be reasonably prepared for
new domains. This shows that our method
is appropriate for rapid prototyping of LU
modules.
1 Introduction
The language understanding (LU) of spoken dia-
logue systems in the early phases of their devel-
opment should be trained with a small amount of
data in their construction. This is because large
amounts of annotated data are not available in the
early phases. It takes a great deal of effort and time
to transcribe and provide correct LU results to a
Figure 1: Relationship between our method and con-
ventional methods
large amount of data. The LU should also be robust,
i.e., it should be accurate even if some automatic
speech recognition (ASR) errors are contained in its
input. A robust LU module is also helpful when col-
lecting dialogue data for the system because it sup-
presses incorrect LU and unwanted behaviors. We
developed a method of rapidly prototyping LU mod-
ules that is easy to construct and robust against var-
ious expressions. It makes LU modules in the early
phases easier to develop.
Several methods of implementing an LU mod-
ule in spoken dialogue systems have been proposed.
Using grammar-based ASR is one of the simplest.
Although its ASR output can easily be transformed
into concepts based on grammar rules, complicated
grammars are required to understand the user?s ut-
terances in various expressions. It takes a great deal
of effort to the system developer. Extracting con-
210
Figure 2: Example of WFST for LU
cepts from user utterances by keyword spotting or
heuristic rules has also been proposed (Seneff, 1992)
where utterances can be transformed into concepts
without major modifications to the rules. However,
numerous complicated rules similarly need to be
manually prepared. Unfortunately, neither method
is robust against ASR errors.
To cope with these problems, corpus-based (Su-
doh and Tsukada, 2005; He and Young, 2005) and
Weighted Finite State Transducer (WFST)-based
methods (Potamianos and Kuo, 2000; Wutiwi-
watchai and Furui, 2004) have been proposed as LU
modules for spoken dialogue systems. Since these
methods extract concepts using stochastic analy-
sis, they do not need numerous complicated rules.
These, however, require a great deal of training data
to implement the module and are not suitable for
constructing new domains.
Here, we present a new WFST-based LU module
that has two main features.
1. A statistical language model (SLM) for ASR
and a WFST for parsing that are automatically
generated from the domain grammar descrip-
tion.
2. Since the weighting for the WFST is simpler
than that in conventional methods, it requires
fewer training data than conventional weight-
ing schemes.
Our method accomplishes robust LU with less ef-
fort using SLM-based ASR and WFST parsing. Fig-
ure 1 outlines the relationships between our method
and conventional schemes. Since rule- or grammar-
based approaches do not require a large amount of
data, they take less effort than stochastic techniques.
However, they are not robust against ASR errors.
Stochastic approaches, on the contrary, take a great
deal of effort to collect data but are robust against
ASR errors. Our method is an intermediate approach
that lies between these. That is, it is more robust than
rule- or grammar-based approaches and takes less
effort than stochastic techniques. This characteristic
makes it easier to rapidly prototype LU modules for
a new domain and helps development in the early
phases.
2 Related Work and WFST-based
Approach
A Finite State Transducer (FST)-based LU is ex-
plained here, which accepts ASR output as its in-
put. Figure 2 shows an example of the FST for a
video recording reservation domain. The input, ?,
means that a transition with no input is permitted at
the state transition. In this example, the LU mod-
ule returns the concept [month=2, day=22] for the
utterance ?It is February twenty second please?.
Here, a FILLER transition in which any word is ac-
cepted is appropriately allowed between phrases. In
Figure 2, ?F? represents 0 or more FILLER tran-
sitions. A FILLER transition from the start to the
end is inserted to reject unreliable utterances. This
FILLER transition enables us to ignore unnecessary
words listed in the example utterances in Table 1.
The FILLER transition helps to suppress the inser-
tion of incorrect concepts into LU results.
However, many output sequences are obtained for
one utterance due to the FILLER transitions, be-
cause the utterance can be parsed with several paths.
We used a WFST to select the most appropriate
path from several output sequences. The path with
the highest cumulative weight, w, is selected in a
211
Table 2: Many LU results for input ?It is February twenty second please?
LU output LU result w
It is February twenty second please month=2, day=22 2.0
It is FILLER twenty second please day=22 1.0
It is FILLER twenty second FILLER day=22 1.0
FILLER FILLER FILLER FILLER FILLER FILLER n/a 0
Table 1: Examples of utterances with FILLERs
ASR output
Well, it is February twenty second please
It is uhm, February twenty second please
It is February, twe-, twenty second please
It is February twenty second please, OK?
(LU result = [month=2, day=22])
WFST-based LU. In the example in Table 2, the
concept [month=2, day=22] has been selected, be-
cause its cumulative weight, w, is 2.0, which is the
highest.
The weightings of conventional WFST-based ap-
proaches used an n-gram of concepts (Potamianos
and Kuo, 2000) and that of word-concept pairs (Wu-
tiwiwatchai and Furui, 2004). They obtained the
n-grams from several thousands of annotated ut-
terances. However, it takes a great deal of ef-
fort to transcribe and annotate a large corpus. Our
method enables prototype LU modules to be rapidly
constructed that are robust against various expres-
sions with SLM-based ASR and WFST-based pars-
ing. The SLM and WFST are generated automat-
ically from a domain grammar description in our
toolkit. We need fewer data to train WFST, because
its weightings are simpler than those in conventional
methods. Therefore, it is easy to develop an LU
module for a new domain with our method.
3 Domain Grammar Description
A developer defines grammars, slots, and concepts
in a domain in an XML file. This description en-
ables an SLM for ASR and parsing WFST to be au-
tomatically generated. Therefore, a developer can
construct an LU module rapidly with our method.
Figure 3 shows an example of a descrip-
tion. A definition of a slot is described in
keyphrase-class tags and its keyphrases and
...
<keyphrase-class name="month">
...
<keyphrase>
<orth>February</orth>
<sem>2</sem>
</keyphrase>
...
</keyphrase-class>
...
<action type="specify-attribute">
<sentence> {It is} [*month] *day [please]
</sentence>
</action>
Figure 3: Example of a grammar description
the values are in keyphrase tags. The month is
defined as a slot in this figure. February and 2 are
defined as one of the phrases and values for the slot
month. A grammar is described in a sequence of
terminal and non-terminal symbols. A non-terminal
symbol represents a class of keyphrases, which is
defined in keyphrase-class. It begins with an
asterisk ?*? in a grammar description in sentence
tags. Symbols that can be skipped are enclosed
by brackets []. The FILLER transition described
in Section 2 is inserted between the symbols un-
less they are enclosed in brackets [] or braces {}.
Braces are used to avoid FILLER transitions from
being inserted. For example, the grammar in Figure
3 accepts ?It is February twenty second please.? and
?It is twenty second, OK??, but rejects ?It is Febru-
ary.? and ?It, uhm, is February twenty second.?.
A WFST for parsing can be automatically gener-
ated from this XML file. The WFST in Figure 2 is
generated from the definition in Figure 3. Moreover,
we can generate example sentences from the gram-
mar description. The SLM for the speech recognizer
is generated with our method by using many exam-
ple sentences generated from the defined grammar.
212
4 Weighting for ASR Outputs on Two
Levels
We define weights on two levels for a WFST. The
first is a weighting for ASR outputs, which is set to
select paths that are reliable at a surface word level.
The second is a weighting for concepts, which is
used to select paths that are reliable on a concept
level. The weighting for concepts reflects correct-
ness at a more abstract level than the surface word
level. The weighting for ASR outputs consists of
two categories: a weighting for ASR N-best outputs
and one for accepted words. We will describe the
definitions of these weightings in the following sub-
sections.
4.1 Weighting for ASR N-Best Outputs
The N-best outputs of ASR are used for an input of
a WFST. Weights are assigned to each sentence in
ASR N-best outputs. Larger weights are given to
more reliable sentences, whose ranks in ASR N-best
are higher. We define this preference as
wis =
e??scorei
?N
j e??scorej
,
where wis is a weight for the i-th sentence in ASRN-best outputs, ? is a coefficient for smoothing, and
scorei is the log-scaled score of the i-th ASR out-put. This weighting reflects the reliability of the
ASR output. We set ? to 0.025 in this study after
a preliminary experiment.
4.2 Weighting for Accepted Words
Weights are assigned to word sequences that have
been accepted by the WFST. Larger weights are
given to more reliable sequences of ASR outputs at
the surface word level. Generally, longer sequences
having more words that are not fillers and more re-
liable ASR outputs are preferred. We define these
preferences as the weights:
1. word(const.): ww = 1.0,
2. word(#phone): ww = l(W ), and
3. word(CM): ww = CM(W ) ? ?w.
The word(const.) gives a constant weight to
all accepted words. This means that sequences
with more words are simply preferred. The
word(#phone) takes the length of each accepted
word into consideration. This length is measured by
its number of phonemes, which are normalized by
that of the longest word in the vocabulary. The nor-
malized values are denoted as l(W ) (0 < l(W ) ?
1). By adopting word(#phone), the length of se-
quences is represented more accurately. We also
take the reliability of the accepted words into ac-
count as word(CM). This uses confidence measures
(Lee et al, 2004) for a word, W , in ASR outputs,
which are denoted as CM(W ). The ?w is the thresh-old for determining whether word W is accepted or
not. The ww obtains a negative value for an unreli-able word W when CM(W ) is lower than ?w. Thisrepresents a preference for longer and more reliable
sequences.
4.3 Weighting for Concepts
In addition to the ASR level, weights on a concept
level are also assigned. The concepts are obtained
from the parsing results by the WFST, and contain
several words. Weights for concepts are defined by
using the measures of all words contained in a con-
cept.
We prepared three kinds of weights for the con-
cepts:
1. cpt(const.): wc = 1.0,
2. cpt(avg):
wc =
?
W (CM(W ) ? ?c)
#W , and
3. cpt(#pCM(avg)):
wc =
?
W (CM(W ) ? l(W ) ? ?c)
#W ,
where W is a set of accepted words, W , in the corre-
sponding concept, and #W is the number of words
in W .
The cpt(const.) represents a preference for
sequences with more concepts. The cpt(avg)
is defined as the weight by using the CM(W )
of each word contained in the concept. The
cpt(#pCM(avg)) represents a preference for longer
and reliable sequences with more concepts. The ?cis the threshold for the acceptance of a concept.
213
Table 3: Examples of weightings when parameter set is: word(CM) and cpt(#pCM(avg))
ASR onput No, it is February twenty second
LU output FILLER it is February twenty second
CM(W ) 0.3 0.7 0.6 0.9 1.0 0.9
l(W ) 0.3 0.2 0.2 0.9 0.6 0.5
Concept - - - month=2 day=22
word - 0.7 ? ?w 0.6 ? ?w 0.9 ? ?w 1.0 ? ?w 0.9 ? ?wcpt - - - (0.9 ? 0.9 ? ?c)/1 (1.0 ? 0.6 ? ?c + 0.9 ? 0.5 ? ?c)/2
'
&
$
%
Reference From June third please
ASR output From June third uhm FIT please LU result
CM(W ) 0.771 0.978 0.757 0.152 0.525 0.741
LU reference From June third FILLER FILLER FILLER month:6, day:3
Our method From June third FILLER FILLER FILLER month:6, day:3
Keyword spotting From June third FILLER FIT please month:6, day:3, car:FIT
(?FIT? is the name of a car.)
Figure 4: Example of LU with WFST
4.4 Calculating Cumulative Weight and
Training
The LU results are selected based on the weighted
sum of the three weights in Subsection 4.3 as
wi = wis + ?w
?
ww + ?c
?
wc
The LU module selects an output sequence with
the highest cumulative weight, wi, for 1 ? i ? N .
Let us explain how to calculate cumulative weight
wi by using the example specified in Table 3. Here,
word(CM) and cpt(#pCM(avg)) are selected as pa-
rameters. The sum of weights in this table for ac-
cepted words is ?w(4.1 ? 5?w), when the input se-quence is ?No, it is February twenty second.?.
The sum of weights for concepts is ?c(1.335 ? 2?c)because the weight for ?month=2? is ?c(0.81 ? ?c)and the weight for ?day=22? is ?c(0.525 ? ?c).Therefore, cumulative weight wi for this input se-
quence is wis + ?w(4.1 ? 5?w) + ?c(1.335 ? 2?c).In the training phase, various combinations of pa-
rameters are tested, i.e., which weightings are used
for each of ASR output and concept level, such as
N = 1 or 10, coefficient ?w,c = 1.0 or 0, and thresh-old ?w,c = 0 to 0.9 at intervals of 0.1, on the train-ing data. The coefficient ?w,c = 0 means that acorresponding weight is not added. The optimal pa-
rameter settings are obtained after testing the various
combinations of parameters. They make the concept
error rate (CER) minimum for a training data set.
We calculated the CER in the following equation:
CER = (S +D + I)/N , where N is the number of
concepts in a reference, and S, D, and I correspond
to the number of substitution, deletion, and insertion
errors.
Figure 4 shows an example of LU with our
method, where it rejects misrecognized concept
[car:FIT], which cannot be rejected by keyword
spotting.
5 Experiments and Evaluation
5.1 Experimental Conditions
We discussed our experimental investigation into the
effects of weightings in Section 4. The user utter-
ance in our experiment was first recognized by ASR.
Then, the i-th sentence of ASR output was input to
WFST for 1 ? i ? N , and the LU result for the
highest cumulative weight, wi, was obtained.
We used 4186 utterances in the video recording
reservation domain (video domain), which consisted
of eight different dialogues with a total of 25 differ-
ent speakers. We also used 3364 utterances in the
rent-a-car reservation domain (rent-a-car domain) of
214
eight different dialogues with 23 different speakers.
We used Julius 1 as a speech recognizer with an
SLM. The language model was prepared by using
example sentences generated from the grammars of
both domains. We used 10000 example sentences in
the video and 40000 in the rent-a-car domain. The
number of the generated sentences was determined
empirically. The vocabulary size was 209 in the
video and 891 in the rent-a-car domain. The average
ASR accuracy was 83.9% in the video and 65.7%
in the rent-a-car domain. The grammar in the video
domain included phrases for dates, times, channels,
commands. That of the rent-a-car domain included
phrases for dates, times, locations, car classes, op-
tions, and commands. The WFST parsing mod-
ule was implemented by using the MIT FST toolkit
(Hetherington, 2004).
5.2 Performance of WFST-based LU
We evaluated our method in the two domains: video
and rent-a-car. We compared the CER on test data,
which was calculated by using the optimal settings
for both domains. We evaluated the results with 4-
fold cross validation. The number of utterances for
training was 3139 (=4186*(3/4)) in the video and
2523 (=3364*(3/4)) in the rent-a-car domain.
The baseline method was simple keyword spot-
ting because we assumed a condition where a large
amount of training data was not available. This
method extracts as many keyphrases as possible
from ASR output without taking speech recogni-
tion errors and grammatical rules into consideration.
Both grammar-based and SLM-based ASR outputs
are used for input in keyword spotting (denoted as
?Grammar & spotting? and ?SLM & spotting? in
Table 4). The grammar for grammar-based ASR
was automatically generated by the domain descrip-
tion file. The accuracy of grammar-based ASR was
66.3% in the video and 43.2% in the rent-a-car do-
main.
Table 4 lists the CERs for both methods. In key-
word spotting with SLM-based ASR, the CERs were
improved by 5.2 points in the video and by 22.2
points in the rent-a-car domain compared with those
with grammar-based ASR. This is because SLM-
based ASR is more robust against fillers and un-
1http://julius.sourceforge.jp/
Table 4: Concept error rates (CERs) in each domain
Domain Grammar &spotting SLM &spotting Ourmethod
Video 22.1 16.9 13.5
Rent-a-car 51.1 28.9 22.0
known words than grammar-based ASR. The CER
was improved by 3.4 and 6.9 points by optimal
weightings for WFST. Table 5 lists the optimal pa-
rameters in both domains. The ?c = 0 in the videodomain means that weights for concepts were not
used. This result shows that optimal parameters de-
pend on the domain for the system, and these need
to be adapted for each domain.
5.3 Performance According to Training Data
We also investigated the relationship between the
size of the training data for our method and the CER.
In this experiment, we calculated the CER in the
test data by increasing the number of utterances for
training. We also evaluated the results by 4-fold
cross validation.
Figures 5 and 6 show that our method outper-
formed the baseline methods by about 80 utterances
in the video domain and about 30 utterances in
the rent-a-car domain. These results mean that our
method can effectively be used to rapidly prototype
LU modules. This is because it can achieve robust
LU with fewer training data compared with conven-
tional WFST-based methods, which need over sev-
eral thousand sentences for training.
6 Conclusion
We developed a method of rapidly prototyping ro-
bust LU modules for spoken language understand-
ing. An SLM for a speech recognizer and a WFST
for parsing were automatically generated from a do-
main grammar description. We defined two kinds
of weightings for the WFST at the word and con-
cept levels. These two kinds of weightings were
calculated by ASR outputs. This made it possi-
ble to create an LU module for a new domain with
less effort because the weighting scheme was rel-
atively simpler than those of conventional methods.
The optimal parameters could be selected with fewer
training data in both domains. Our experiment re-
215
Table 5: Optimal parameters in each domain
Domain N ?w ww ?c wc
Video 1 1.0 word(const.) 0 -
Rent-a-car 10 1.0 word(CM)-0.0 1.0 cpt(#pCM(avg))-0.8
 0
 5
 10
 15
 20
 25
 30
 35
 40
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 5: CER in video domain
 0
 5
 10
 15
 20
 25
 30
 50
 55
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 6: CER in rent-a-car domain
vealed that the CER could be improved compared to
the baseline by training optimal parameters with a
small amount of training data, which could be rea-
sonably prepared for new domains. This means that
our method is appropriate for rapidly prototyping
LU modules. Our method should help developers
of spoken dialogue systems in the early phases of
development. We intend to evaluate our method on
other domains, such as database searches and ques-
tion answering in future work.
Acknowledgments
We are grateful to Dr. Toshihiko Ito and Ms. Yuka
Nagano of Hokkaido University for constructing the
rent-a-car domain system.
References
Yulan He and Steve Young. 2005. Spoken Language
Understanding using the Hidden Vector State Model.Speech Communication, 48(3-4):262?275.
Lee Hetherington. 2004. The MIT finite-state trans-ducer toolkit for speech and language processing. InProc. 6th International Conference on Spoken Lan-guage Processing (INTERSPEECH-2004 ICSLP).
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawahara.
2004. Real-time word confidence scoring using lo-cal posterior probabilities on tree trellis search. InProc. 2004 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2004),volume 1, pages 793?796.
Alexandors Potamianos and Hong-Kwang J. Kuo. 2000.
Statistical recursive finite state machine parsingfor speech understanding. In Proc. 6th Interna-tional Conference on Spoken Language Processing(INTERSPEECH-2000 ICSLP), pages 510?513.
Stephanie Seneff. 1992. TINA: A natural language sys-tem for spoken language applications. ComputationalLinguistics, 18(1):61?86.
Katsuhito Sudoh and Hajime Tsukada. 2005. Tightly in-
tegrated spoken language understanding using word-to-concept translation. In Proc. 9th European Con-ference on Speech Communication and Technology(INTERSPEECH-2005 Eurospeech), pages 429?432.
Chai Wutiwiwatchai and Sadaoki Furui. 2004. Hybridstatistical and structural semantic modeling for Thaimulti-stage spoken language understanding. In Proc.HLT-NAACL Workshop on Spoken Language Under-standing for Conversational Systems and Higher LevelLinguistic Information for Speech Processing, pages
2?9.
216
Proceedings of NAACL HLT 2009: Short Papers, pages 133?136,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Speech Understanding Framework
that Uses Multiple Language Models and Multiple Understanding Models
?Masaki Katsumaru, ?Mikio Nakano, ?Kazunori Komatani,
?Kotaro Funakoshi, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{katumaru, komatani}@kuis.kyoto-u.ac.jp
{ogata, okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
{nakano, funakoshi}@jp.honda-ri.com
Abstract
The optimal combination of language model
(LM) and language understanding model
(LUM) varies depending on available training
data and utterances to be handled. Usually, a
lot of effort and time are needed to find the op-
timal combination. Instead, we have designed
and developed a new framework that uses
multiple LMs and LUMs to improve speech
understanding accuracy under various situa-
tions. As one implementation of the frame-
work, we have developed a method for select-
ing the most appropriate speech understand-
ing result from several candidates. We use
two LMs and three LUMs, and thus obtain six
combinations of them. We empirically show
that our method improves speech understand-
ing accuracy. The performance of the oracle
selection suggests further potential improve-
ments in our system.
1 Introduction
The speech understanding component in a spoken
dialogue system consists of an automatic speech
recognition (ASR) component and a language un-
derstanding (LU) component. To develop a speech
understanding component, we need to prepare an
ASR language model (LM) and a language under-
standing model (LUM) for the dialogue domain
of the system. There are many types of LMs
such as finite-state grammars and N-grams, and
many types of LUMs such as finite-state transduc-
ers (FST), weighted finite-state transducers (WFST),
and keyphrase-extractors (extractor). Selecting a
suitable combination of LM and LUM is necessary
for robust speech understanding against various user
utterances.
Conventional studies of speech understanding
have investigated which LM and LUM give the best
performance by using fixed training and test data
such as the Air Travel Information System (ATIS)
corpus. However, in real system development, re-
sources such as training data for statistical models
and efforts to write finite-state grammars vary ac-
cording to the available human resources or budgets.
Domain-dependent training data are particularly dif-
ficult to obtain. Therefore, in conventional system
development, system developers determine the types
of LM and LUM by trial and error. Every LM and
LUM has some advantages and disadvantages, so it
is difficult for a single combination of LM and LUM
to gain high accuracy except in a situation involv-
ing a lot of training data and effort. Therefore, using
multiple speech understanding methods is a more ef-
fective approach.
In this paper, we propose a speech understand-
ing framework called ?Multiple Language models
and Multiple Understanding models (MLMU)?, in
which multiple LMs and LUMs are used, to achieve
better performance under the various development
situations. It selects the best speech understanding
result from the multiple results generated by arbi-
trary combinations of LMs and LUMs.
So far there have been several attempts to im-
prove ASR and speech understanding using mul-
tiple speech recognizers and speech understanding
modules. ROVER (Fiscus, 1997) tried to improve
ASR accuracy by integrating the outputs of multi-
ple ASRs with different acoustic and language mod-
133
LM 1utterance LM 2
LM n 
resultconfidenceintegrationcomponent
LUcomponent speechunderstandingresultsASRcomponent
LUM n LUM 2
LUM 1
LM: Language Model
LUM: Language Understanding Model
Figure 1: Flow of speech understanding in MLMU
els. The work is different from our study in the fol-
lowing two points: it does not deal with speech un-
derstanding, and it assumes that each ASR is well-
developed and achieves high accuracy for a variety
of speech inputs. Eckert et al (1996) used multiple
LMs to deal with both in-grammar utterances and
out-of-grammar utterances, but did not mention lan-
guage understanding. Hahn et al (2008) used mul-
tiple LUMs, but just a single language model.
2 Speech Understanding Framework
MLMU
MLMU is a framework by which system developers
can use multiple speech understanding methods by
preparing multiple LMs and multiple LUMs. Fig-
ure 1 illustrates the flow of speech understanding in
MLMU. System developers list available LMs and
LUMs for each system?s domain, and the system
understands utterances by using these models. The
framework selects one understanding result from
multiple results or calculates a confidence score of
the result by using the generated multiple under-
standing results.
MLMU can improve speech understanding for the
following reason. The performance of each speech
understanding (a combination of LM and LUM)
might not be very high when either training data for
the statistical model or available expertise and ef-
fort for writing grammar are insufficient. In such
cases, some utterances might not be covered by the
system?s finite-state grammar LM, and probability
estimation in the statistical models may not be very
good. Using multiple speech understanding mod-
els is expected to solve this problem because each
model has different specialities. For example, finite-
state grammar LMs and FST-based LUMs achieve
high accuracy in recognizing and understanding in-
grammar utterances, whereas out-of-grammar utter-
ances are covered by N-gram models and LUMs
based on WFST and keyphrase-extractors. There-
fore it is more possible that the understanding results
of MLMU will include the correct result than a case
when a single understanding model is used.
The understanding results of MLMU will be help-
ful in many ways. We used them to achieve better
understanding accuracy by selecting the most reli-
able one. This selection is based on features con-
cerning ASR results and language understanding re-
sults. It is also possible to delay the selection, hold-
ing multiple understanding result candidates that
will be disambiguated as the dialogue proceeds (Bo-
hus, 2004). Furthermore, confidence scores, which
enable an efficient dialogue management (Komatani
and Kawahara, 2000), can be calculated by ranking
these results or by voting on them, by using multi-
ple speech understanding results. The understanding
results can be used in the discourse understanding
module and the dialogue management module. They
can choose one of the understanding results depend-
ing on the dialogue situation.
3 Implementation
3.1 Available Language Models and Language
Understanding Models
We implemented MLMU as a library of RIME-
TK, which is a toolkit for building multi-domain
spoken dialogue systems (Nakano et al, 2008).
With the current implementation, developers can use
the following LMs:
1. A LM based on finite-state grammar (FSG)
2. A domain-dependent statistical N-gram model
(N-gram)
and the following LUMs:
1. Finite-state transducer (FST)
2. Weighted FST (WFST)
3. Keyphrase-extractor (extractor).
System developers can use multiple finite-state-
grammar-based LMs or N-gram-based LMs, and
134
also multiple FSTs and WFSTs. They can specify
the combination for each domain by preparing LMs
and LUMs. They can specify grammar models when
sufficient human labor is available for writing gram-
mar, and specify statistical models when a corpus for
training models is available.
3.2 Selecting Understanding Result based on
ASR and LU Features
We also implemented a mechanism for selecting one
of the understanding results as the best hypothesis.
The mechanism chooses the result with the highest
estimated probability of correctness. Probabilities
are estimated for each understanding result by using
logistic regression, which uses several ASR and LU
features.
We define Pi as the probability that speech under-
standing result i is correct, and we select one result
based on argmax
i
Pi. We denote each speech un-
derstanding result as i (i = 1,. . . ,6). We constructed
a logistic regression model for Pi. The regression
function can be written as:
Pi = 11 + exp(?(ai1Fi1 + . . . + aimFim + bi)) .(1)
The coefficients ai1, . . . , aim, bi were fitted us-
ing training data. The independent variables
Fi1, Fi2, ..., Fim are listed in Table 1. In the table,
n indicates the number of understanding results, that
is, n = 6 in this paper?s experiment. Here, we denote
the features as Fi1, Fi2, ..., Fim.
Features from Fi1 to Fi3 represent characteristics
of ASR results. The acoustic scores were normal-
ized by utterance durations in seconds. These fea-
tures are used for verifying its ASR result. Features
from Fi4 to Fi9 represent characteristics of LU re-
sults. Features from Fi4 to Fi6 are defined on the
basis of the concept-based confidence scores (Ko-
matani and Kawahara, 2000).
4 Preliminary Experiment
We conducted a preliminary experiment to show the
potential of the framework by using the two LMs
and three LUMs noted in Section 3.1.
Table 1: Features from speech understanding result i
Fi1: acoustic score of ASR
Fi2: difference between Fi1 and acoustic score
of ASR for utterance verification
Fi3: utterance duration [sec.]
Fi4: average confidence scores for concepts in i
Fi5: average of Fi4 ( 1n
?n
i Fi4)
Fi6: proportion of Fi4 (Fi4 /?ni Fi5)
Fi7: average # concepts ( 1n
?n
i #concepti)
Fi8: max. # concepts (max (#concepti) )
Fi9: min. # concepts (min (#concepti) )
4.1 Preparing LMs and LUMs
The finite-state grammar rules were written in sen-
tence units manually. A domain-dependent statisti-
cal N-gram model was trained on 10,000 sentences
randomly generated from the grammar. The vocab-
ulary sizes of the grammar LM and the domain-
dependent statistical LM were both 278. We
also used a domain-independent statistical N-gram
model for obtaining acoustic scores for utterance
verification, which was trained onWeb texts (Kawa-
hara et al, 2004). Its vocabulary size was 60,250.
The grammar used in the FST was the same as the
FSG used as one of the LMs, which was manually
written by a system developer. The WFST-based LU
was based on a method to estimate WFST parame-
ters with a small amount of data (Fukubayashi et al,
2008). Its parameters were estimated by using 105
utterances of just one user. The keyphrase extrac-
tor extracts as many concepts as possible from an
ASR result on the basis of a grammar while ignor-
ing words that do not match the grammar.
4.2 Target Data for Evaluation
We used 3,055 utterances in the rent-a-car reserva-
tion domain (Nakano et al, 2007). We used Julius
(ver. 4.0.2) as the speech recognizer and a 3000-
state phonetic tied-mixture (PTM) triphone model
as the acoustic model1. ASR accuracy in mora ac-
curacy when using the FSG and the N-gram model
were 71.9% and 75.5% respectively. We used con-
cept error rates (CERs) to represent the speech un-
derstanding accuracy, which is calculated as fol-
1http://julius.sourceforge.jp/
135
Table 2: CERs [%] for each speech understanding
method
speech understanding method
(LM + LUM) CER
(1) FSG + FST 26.9
(2) FSG + WFST 29.9
(3) FSG + extractor 27.1
(4) N-gram + FST 35.2
(5) N-gram + WFST 25.3
(6) N-gram + extractor 26.0
selection from (1) through (6) (our method) 22.7
oracle selection 13.5
lows:
CER = # error concepts#concepts in utterances . (2)
We manually annotated whether an understanding
result of each utterance was correct or not, and
used them as training data to fit the coefficients
ai1, . . . , aim, bi.
4.3 Evaluation in Concept Error Rates
We fitted the coefficients of regression functions and
selected understanding results with a 10-fold cross
validation. Table 2 lists the CERs based on combi-
nations of single LM and LUM and by our method.
Of all combinations of single LM and LUM, the best
accuracy was obtained with (5) (N-gram + WFST).
Our method improved by 2.6 points over (5). Al-
though we achieved a lower CER, we used a lot
of data to estimate logistic regression coefficients.
Such a large amount of data may not be available in a
real situation. We will conduct more experiments by
changing the amount of training data. Table 2 also
shows the accuracy of the oracle selection, which
selected the best speech understanding result man-
ually. The CER of the oracle selection was 13.5%,
a significant improvement compared to all combina-
tions of a LM and LUM. There is no combination of
a LM and LUM whose understanding results were
not selected at all in the oracle selection and our
method?s selection. These results show that using
multiple LMs and multiple LUMs can potentially
improve speech understanding accuracy.
5 Ongoing work
We will conduct more experiments in other domains
or with other resources to evaluate the effectiveness
of our framework. We plan to investigate the case
in which a smaller amount of the training data is
used to estimate the coefficients of the logistic re-
gressions. Furthermore, finding a way to calculate
confidence scores of speech understanding results is
on our agenda.
References
Dan Bohus. 2004. Error awareness and recovery in
task-oriented spoken dialogue systems. Ph.D. thesis,
Carnegie Mellon University.
Wieland Eckert, Florian Gallwitz, and Heinrich Nie-
mann. 1996. Combining stochastic and linguistic lan-
guage models for recognition of spontaneous speech.
In Proc. ICASSP, pages 423?426.
Jonathan G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer Out-
put Voting Error Reduction (ROVER). In Proc. ASRU,
pages 347?354.
Yuichiro Fukubayashi, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tetsuya
Ogata, and Hiroshi G. Okuno. 2008. Rapid prototyp-
ing of robust language understanding modules for spo-
ken dialogue systems. In Proc. IJCNLP, pages 210?
216.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008.
System combination for spoken language understand-
ing. In Proc. Interspeech, pages 236?239.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR Engine Julius and
Japanese model repository. In Proc. ICSLP, pages
3069?3072.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. COLING, volume 1, pages 467?
473.
Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshi-
hiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsu-
jino. 2007. Analysis of user reactions to turn-taking
failures in spoken dialogue systems. In Proc. SIGdial,
pages 120?123.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. SIGdial, pages 88?91.
136
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 88?91,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Framework for Building Conversational Agents Based on a Multi-Expert
Model
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, Hiroshi Tsujino
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama 351-0188, Japan
{nakano, funakoshi, yuji.hasegawa, tsujino}@jp.honda-ri.com
Abstract
This paper presents a novel framework for
building symbol-level control modules of an-
imated agents and robots having a spoken di-
alogue interface. It features distributed mod-
ules called experts each of which is special-
ized to perform certain kinds of tasks. A com-
mon interface that all experts must support is
specified, and any kind of expert can be incor-
porated if it has the interface. Several modules
running in parallel coordinate the experts by
accessing them through the interface, so that
the whole system can achieve flexible control,
such as interruption handling and parallel task
execution.
1 Introduction
As much attention is recently paid to autonomous
agents such as robots and animated agents, spoken
dialogue is expected to be a natural interface be-
tween users and such agents. Our objective is to es-
tablish a framework for developing the intelligence
module of such agents.
In establishing such a framework, we focus on
achieving the following features. (1) Multi-domain
dialogue: Since agents are usually expected to per-
form multiple kinds of tasks, they need to work in
multiple domains and switch domains according to
user utterances. (2) Interruption handling: It is cru-
cial for human-agent interaction to be able to handle
users? interrupting utterances while speaking or per-
forming tasks. (3) Parallel task execution: Agents,
especially robots that perform physical actions, are
expected to be able to execute multiple tasks in par-
allel when possible. For example, robots should be
able to engage in a dialogue while moving. (4) Ex-
tensibility: Since the agents can be used for a vari-
ety of tasks, various strategies for dialogue and task
planning should be able to be incorporated.
Although a number of models for conversational
agents have been proposed, no model has all of the
above properties. Several multi-domain dialogue
system models have been proposed and they are ex-
tensible, but it is not clear how they handle interrup-
tions to system utterances and actions (e.g., O?Neill
et al (2004), Lin et al (1999), and Hartikainen et al
(2004)). There are several spoken dialogue agents
and robots that can handle interruptions thanks to
their asynchronous control (Asoh et al, 1999; Boye
et al, 2000; Blaylock et al, 2002; Lemon et al,
2002), they do not focus on making it easy to add
new dialogue domains with a variety of dialogue
strategies.
This paper presents a framework called RIME
(Robot Intelligence based on Multiple Experts),
which employs modules called experts.1 Each ex-
pert is specialized for achieving certain kinds of
tasks by performing physical actions and engaging
in dialogues. It corresponds to the symbol-level con-
trol module of a system that can engage in tasks in
a single small domain, and it employs fixed con-
trol strategies. Only some of the experts take charge
in understanding user utterances and decide actions.
The basic idea behind RIME is to specify a com-
mon interface of experts for coordinating them and
to achieve flexible control. In RIME, several mod-
1RIME is an improved version of our previous model
(Nakano et al, 2005), whose interruption handling was too sim-
ple and which could not achieve parallel task execution.
88
ules run in parallel for coordinating experts. They
are understander, which is responsible for speech
understanding, action selector, which is responsible
for selecting actions, and task planner, which is re-
sponsible for deciding which expert should work to
achieve tasks.
RIME achieves the above mentioned features.
Multi-domain dialogues are possible by selecting an
appropriate expert which is specialized to dialogues
in a certain domain. Interruption handling is possi-
ble because each expert must have methods to de-
tect interruptions and decide actions to handle in-
terruptions, and coordinating modules can use these
methods. Parallel task execution is possible because
experts have methods for providing information to
decide which experts can take charge at the same
time, and the task planner utilizes that information.
Extensibility is achieved because any kind of expert
can be incorporated if it supports the common inter-
face. This makes it possible for agent developers to
build a variety of conversational agents.
2 Multi-Expert Model
This section explains RIME in detail. Fig. 1 depicts
its module architecture.
2.1 Experts
Each expert is a kind of object in the object-oriented
programming framework. In this paper, we call
tasks performed by one expert primitive tasks. Ex-
perts should be prepared for each primitive task type.
For example, if there is an expert for a primitive task
type ?telling someone?s extension number?, ?telling
person A?s extension number? is a primitive task.
By performing a series of primitive tasks, a com-
plicated task can be performed. For example, a mu-
seum guide robot can perform ?explaining object B?
by executing ?moving to B? and ?giving an explana-
tion on B?. Among the experts, a small number of
experts can perform tasks at one time. Such experts
are called being in charge.
Each expert holds information on the progress of
the primitive task. It includes task-type-independent
information, such as which action in this primitive
task is being performed and whether the previous
robot action finished, and task-type-dependent in-
formation such as the user intention understanding
understander
expert 1
expert 2
expert 3
expert n
action 
selector
task 
planner
global 
context
input 
processor
action 
executor
speech 
recognition
result
score
expert 
selection 
information
speech 
recognition
result
action
(from experts 
in charge)
action
execution
report
exec. report (to the expert 
that selected the action)
charge 
/discharge new task information
across tasks
.
.
.
.
.
microphone etc. agent & speech synthesizer
Figure 1: Architecture for RIME-Based Systems
results and dialogue history. The contents and the
data structure for the task-type-dependent informa-
tion for each expert can be designed by the system
developer.
Experts are classified into system-initiative task
experts and user-initiative task experts. In this pa-
per, the initiative of a task means who can initiate
the task. For example, the task ?understanding a
request for weather information? is a user-initiative
task, and the task ?providing weather information?
is a system-initiative task.
In RIME, executing multiple tasks in parallel be-
comes possible by making multiple experts take
charge. To check whether two experts can take
charge simultaneously, we currently use two fea-
tures verbal and physical. Two experts having the
same feature cannot take charge simultaneously.
The interface of experts consists of methods for
accessing its internal state. Below are some of the
task-type-dependent methods, which need to be im-
plemented by system developers.
The understand method updates the internal state
based on the user speech recognition results, us-
ing domain-dependent sentence patterns for utter-
ance understanding. This method returns a score
which indicates the plausibility the user utterance
should be dealt with by the expert. Domain selection
techniques in multi-domain spoken dialogue sys-
tems (Komatani et al, 2006) can be applied to obtain
the score. The select-action method outputs one ac-
tion based on the content of the internal state. Here,
an action is a multimodal command which includes
a text to speak and/or a physical action command.
89
The action can be an empty action, which means do-
ing nothing. The detect-interruption method returns
a Boolean value that indicates whether the previous
user utterance is an interruption to the action being
performed when this expert is being in charge. The
handle-interruption method returns the action to be
performed after an interruption is detected. For ex-
ample, an instruction to stop the utterance can be
returned.
In the definition of these methods, experts can
access a common database called global context to
store and utilize information across domains, such
as information on humans, information on the envi-
ronment, and past dialogue topics.
2.2 Modules Coordinating Experts
To exploit experts, three processes, namely the un-
derstander, the action selector, and the task planner,
work in parallel.
The understander receives output of an input pro-
cessor, which typically performs speech recogni-
tion. Each time the understander receives a user
speech recognition result from the input processor,
it performs the following process. First it dispatches
the speech recognition result to the experts in charge
and the user-initiative experts with their understand
methods, which then returns the scores mentioned
above. The expert that returns the highest score is
selected as the expert to take charge. If the selected
expert is not in charge, it tells the task planner that
the expert is selected as the user-initiative expert to
take charge. If the selected expert is in charge, it
calls the detect-interruption method of the expert. If
true is returned, it tells the action selector that an
interruption utterance is detected.
The action selector repeats the following process
for each expert being in charge in a short cycle.
When an interruption for the expert is detected, it
calls the expert?s handle-interruption method, and
it then sends the returned action to the action ex-
ecutor, which is assumed to execute multimodal ac-
tions by controlling agents, speech synthesizers, and
other modules. Otherwise, unless it is not waiting
for a user utterance, it calls the expert?s select-action
methods, and then sends the returned action to the
action executor. The returned action can be an empty
action. Note that it is assumed that the action execu-
tor can perform two or more actions in parallel when
verbalagentexplaining placesG
physicalagentmoving to show the way F
verbaluserunderstanding requests for guiding to placesE
verbalagentproviding extension numbersD
verbaluserunderstanding extension number requestsC
verbalagentproviding weather informationB
verbaluserunderstanding weather information requestsA 
featureinitiativetask typeID 
Table 1: Experts in the Example Robotic System
Human: "Where is the meeting 
room?"
Robot: "Would you like to know 
where the meeting room is?"
Human: "yes."
Human: "Tell me A's extension 
number."
Robot: "Please come this way."
(start moving)
Robot: "A's extension number is 
1234."
Robot: (stop moving)
Expert E
Expert G
Expert C
Expert D
understand request
to show the way
show the way
tell A's ext. 
number
understand 
request for A's 
ext. number
Robot: "The meeting room is over
there."
Utterances and physical actions Experts in charge and tasks
move to 
show the 
way
Expert F
Figure 2: Expert Selection in a Parallel Task Execution
Example
possible.
The task planner is responsible for deciding which
experts take charge and which experts do not. It
sometimes makes an expert take charge by setting
a primitive task, and sometimes it discharges an ex-
pert to cancel the execution of its primitive task. To
make such decisions, it receives several pieces of in-
formation from other modules. First it receives from
the understander information on which expert is se-
lected to understand a new utterance. It also receives
information on the finish of the primitive task from
an expert being in charge. In addition, it receives
new tasks from the experts that understand human
requests. The task planner also consults the global
context to access the information shared by the ex-
perts and the task planner. In this paper we do not
discuss the details of task planning algorithms, but
we have implemented a task planner with a simple
hierarchical planning mechanism.
There can be other processes whose output is
written in the global context. For example, a robot
and human localization process using image pro-
cessing and other sensor information processing can
be used.
90
3 Implementation as a Toolkit
The flexibility of designing experts increases the
amount of effort for programming in building ex-
perts. We therefore developed RIME-TK (RIME-
ToolKit), which provides libraries that facilitate
building systems based on RIME. It is implemented
in Java, and contains an abstract expert class hier-
archy. The system developers can create new ex-
perts by extending those abstract classes. Those ab-
stract classes have frequently used functions such
as WFST-based language understanding, template-
based language generation, and frame-based dia-
logue management. RIME-TK also contains the im-
plementations of the understander and the action se-
lector. In addition, it specifies the interfaces for the
input processor, the action executor, and the task
planner. Example implementations of these mod-
ules are also included in RIME-TK. Using RIME-
TK, conversational agents can be built by creating
experts, an input processor, an action executor, and
a task planner.
As an example, we have built a robotic system,
which is supposed to work at a reception, and can
perform several small tasks such as providing ex-
tension numbers of office members and guiding to
several places near the reception such as a meeting
room and a restroom. Some experts in the system
are listed in Table 1. Fig. 2 shows an example inter-
action between a human and the robotic system that
includes parallel task execution and how experts are
charged. The detailed explanation is omitted for the
lack of the space.
By developing several other robotic systems and
spoken dialogue systems (e.g., Komatani et al
(2006), Nakano et al (2006), and Nishimura et al
(2007)), we have confirmed that RIME and RIME-
TK are viable.
4 Concluding Remarks
This paper presented RIME, a framework for build-
ing conversational agents. It is different from pre-
vious frameworks in that it makes it possible to
build agents that can handle interruptions and exe-
cute multiple tasks in parallel by employing experts
which have a common interface. Although the cur-
rent implementation is useful for building various
kinds of systems, we believe that preparing more
kinds of expert templates and improving expert se-
lection for understanding utterances facilitate build-
ing a wider variety of systems.
Acknowledgments We would like to thank all
people who helped us to build RIME-TK and its ap-
plications.
References
H. Asoh, T. Matsui, J. Fry, F. Asano, and S. Hayamizu.
1999. A spoken dialog system for a mobile office
robot. In Proc. Eurospeech-99, pages 1139?1142.
N. Blaylock, J. Allen, and G. Ferguson. 2002. Synchro-
nization in an asynchronous agent-based architecture
for dialogue systems. In Proc. Third SIGdial Work-
shop, pages 1?10.
J. Boye, B. A. Hockey, and M. Rayner. 2000. Asyn-
chronous dialogue management: Two case-studies. In
Proc. Go?talog-2000.
M. Hartikainen, M. Turunen, J. Hakulinen, E.-P. Salo-
nen, and J. A. Funk. 2004. Flexible dialogue manage-
ment using distributed and dynamic dialogue control.
In Proc. Interspeech-2004, pages 197?200.
K. Komatani, N. Kanda, M. Nakano, K. Nakadai, H. Tsu-
jino, T. Ogata, and H. G. Okuno. 2006. Multi-domain
spoken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proc. 7th
SIGdial Workshop, pages 9?17.
O. Lemon, A. Gruenstein, A. Battle, and S. Peters. 2002.
Multi-tasking and collaborative activities in dialogue
systems. In Proc. Third SIGdial Workshop, pages
113?124.
B. Lin, H. Wang, and L. Lee. 1999. Consistent dialogue
across concurrent topics based on an expert system
model. In Proc. Eurospeech-99, pages 1427?1430.
M. Nakano, Y. Hasegawa, K. Nakadai, T. Nakamura,
J. Takeuchi, T. Torii, H. Tsujino, N. Kanda, and H. G.
Okuno. 2005. A two-layer model for behavior and
dialogue planning in conversational service robots. In
Proc. 2005 IEEE/RSJ IROS, pages 1542?1547.
M. Nakano, A. Hoshino, J. Takeuchi, Y. Hasegawa,
T. Torii, K. Nakadai, K. Kato, and H. Tsujino. 2006.
A robot that can engage in both task-oriented and non-
task-oriented dialogues. In Proc. 2006 IEEE/RAS Hu-
manoids, pages 404?411.
Y. Nishimura, S. Minotsu, H. Dohi, M. Ishizuka,
M. Nakano, K. Funakoshi, J. Takeuchi, Y. Hasegawa,
and H. Tsujino. 2007. A markup language for describ-
ing interactive humanoid robot presentations. In Proc.
IUI-07.
I. O?Neill, P. Hanna, X. Liu, and M. McTear. 2004.
Cross domain dialogue modelling: an object-based ap-
proach. In Proc. Interspeech-2004, pages 205?208.
91
Proceedings of the 12th European Workshop on Natural Language Generation, pages 191?194,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Probabilistic Model of Referring Expressions for Complex Objects
Kotaro Funakoshi? Philipp Spanger?
?Honda Research Institute Japan Co., Ltd.
Saitama, Japan
funakoshi@jp.honda-ri.com
nakano@jp.honda-ri.com
Mikio Nakano? Takenobu Tokunaga?
?Tokyo Institute of Technology
Tokyo, Japan
philipp@cl.cs.titech.ac.jp
take@cl.cs.titech.ac.jp
Abstract
This paper presents a probabilistic model
both for generation and understanding of
referring expressions. This model intro-
duces the concept of parts of objects, mod-
elling the necessity to deal with the char-
acteristics of separate parts of an object in
the referring process. This was ignored or
implicit in previous literature. Integrating
this concept into a probabilistic formula-
tion, the model captures human character-
istics of visual perception and some type
of pragmatic implicature in referring ex-
pressions. Developing this kind of model
is critical to deal with more complex do-
mains in the future. As a first step in our
research, we validate the model with the
TUNA corpus to show that it includes con-
ventional domain modeling as a subset.
1 Introduction
Generation of referring expressions has been stud-
ied for the last two decades. The basic orientation
of this research was pursuing an algorithm that
generates a minimal description which uniquely
identifies a target object from distractors. Thus
the research was oriented and limited by two con-
straints: minimality and uniqueness.
The constraint on minimality has, however,
been relaxed due to the computational complexity
of generation, the perceived naturalness of redun-
dant expressions, and the easiness of understand-
ing them (e.g., (Dale and Reiter, 1995; Spanger et
al., 2008)). On the other hand, the other constraint
of uniqueness has not been paid much attention
to. One major aim of our research is to relax this
constraint on uniqueness because of the reason ex-
plained below.
The fundamental goal of our research is to deal
with multipartite objects, which have constituents
with different attribute values. Typical domain set-
tings in previous literature use uniform objects like
the table A shown in Figure 1. However, real life
is not so simple. Multipartite objects such as ta-
bles B and C can be found easily. Therefore this
paper introduces the concept of parts of objects to
deal with more complex domains containing such
objects. Hereby the constraint on uniqueness be-
comes problematic because people easily generate
and understand logically ambiguous expressions
in such domains.
For example, people often use an expression
such as ?the table with red corners? to identify
table B. Logically speaking, this expression is
equally applicable both to A and to B, that is, vio-
lating the constraint on uniqueness. And yet peo-
ple seem to have no problem identifying the in-
tended target correctly and have little reluctance to
use such an expression (Evidence is presented in
Section 3). We think that this reflects some type of
pragmatic implicature arising from human charac-
teristics of visual perception and that is important
both for understanding human-produced expres-
sions and for generating human-friendly expres-
sions in a real environment. This paper proposes a
model of referring expressions both for generation
and understanding. Our model uses probabilities
to solve ambiguity under the relaxed constraint on
uniqueness while considering human perception.
No adequate data is currently available in or-
der to provide a comprehensive evaluation of our
model. As a first step in our research, we validate
the model with the TUNA corpus to show that it
includes conventional domain modeling.
Figure 1: An example scene
191
2 Related work
Horacek (2005) proposes to introduce probabili-
ties to overcome uncertainties due to discrepan-
cies in knowledge and cognition between subjects.
While our model shares the same awareness of is-
sues with Horacek?s work, our focus is on rather
different issues (i.e., handling multipartite objects
and relaxing the constraint on uniqueness). In
addition, Horacek?s work is concerned only with
generation while our model is available both for
generation and understanding. Roy (2002) also
proposes a probabilistic model for generation but
presupposes uniform objects.
Horacek (2006) deals with references for struc-
tured objects such as documents. Although it con-
siders parts of objects, the motivation and focus of
the work are on quite different aspects from ours.
3 Evidence against logical uniqueness
We conducted two psycholinguistic experiments
using the visual stimulus shown in Figure 1.
In the first experiment, thirteen Japanese sub-
jects were presented with an expression ?kado no
akai tukue (the table with red corners)? and asked
to choose a table from the three in the figure.
Twelve out of the thirteen chose table B. Seven
out of the twelve subjects answered that the given
expression was not ambiguous.
In the second experiment, thirteen different
Japanese subjects were asked to make a descrip-
tion for table B without using positional relations.
Ten out of the thirteen made expressions seman-
tically equivalent to the expression used in the
first experiment. Only three subjects made log-
ically discriminative expressions such as ?asi to
yotu kado dake akai tukue (the table whose four
corners and leg only are red).?
These results show that people easily gener-
ate/understand logically ambiguous expressions.
4 Proposed model
We define pi = {p1, p2, . . . , pk} as the set of k
parts of objects (classes of sub-parts) that appears
in a domain. Here p1 is special and always means
the whole of an object. In a furniture domain, p1
means a piece of furniture regardless of the kind
of the object (chair, table, whatever). pi(i 6= 1)
means a sub-part class such as leg. Note that pi is
defined not for each object but for a domain. Thus,
objects may have no part corresponding to pi (e.g.,
some chairs have no leg.).
A referring expression e is represented as a set
of n pairs of an attribute value expression eaj and a
part expression epj modified by eaj as
e = {(ep1, e
a
1), (e
p
2, e
a
2), . . . , (epn, ean)}. (1)
For example, an expression ?the white table with
a red leg? is represented as
{(?table?, ?white?), (?leg?, ?red?)}.
Given a set of objects ? and a referring ex-
pression e, the probability with which the expres-
sion e refers to an object o ? ? is denoted as
Pr(O = o|E = e,? = ?). If we seek to provide
a more realistic model, we can model a probabilis-
tic distribution even for ?. In this paper, however,
we assume that ? is fixed to ? and it is shared by
interlocutors exactly. Thus, hereafter, Pr(o|e) is
equal to Pr(o|e, ?).
Following the definition (1), we estimate
Pr(o|e) as follows:
Pr(o|e) ? N
?
i
Pr(o|epi , e
a
i ). (2)
Here, N is a normalization coefficient. According
to Bayes? rule,
Pr(o|epi , e
a
i ) =
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (3)
Therefore,
Pr(o|e) ? N
?
i
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (4)
We decompose Pr(epi , eai |o) as
?
u
?
v
Pr(epi |pu, o)Pr(e
a
i |av, o)Pr(pu, av|o)
(5)
where pu is one of parts of objects that could be
expressed with epi , and av is one of attribute val-
ues1 that could be expressed with eai . Under the
simplifying assumption that epi and eai are not am-
biguous and are single possible expressions for
a part of objects and an attribute value indepen-
dently of objects 2,
Pr(o|e) ? N
?
i
Pr(o)Pr(pi, ai|o)
Pr(pi, ai)
(6)
? N
?
i
Pr(o|pi, ai) (7)
1Each attribute value belongs to an attribute ?, a set of
attribute values. E.g., ?color = {red, white, . . .}.
2That is, we ignore lexical selection matters in this paper,
although our model is potentially able to handle those matters
including training from corpora.
192
Pr(o|p, a) concerns attribute selection in gen-
eration of referring expressions. Most attribute
selection algorithms presented in past work are
based on set operations over multiple attributes
with discrete (i.e., symbolized) values such as col-
ors (red, brown, white, etc) to find a uniquely dis-
tinguishing description. The simplest estimation
of Pr(o|p, a) following this conventional Boolean
domain modeling is
Pr(o|p, a) ?
{
|??|?1 (p in o has a)
0 (p in o does not have a) (8)
where ?? is the subset of ?, each member of which
has attribute value a in its part of p.
As Horacek (2005) pointed out, however, this
standard approach is problematic in a real envi-
ronment because many physical attributes are non-
discrete and the symbolization of these continuous
attributes have uncertainties. For example, even
if two objects are blue, one can be more blueish
than the other. Some subjects may say it?s blue
but others may say it?s purple. Moreover, there
is the problem of logical ambiguity pointed out
in Section 1. That is, even if an attribute itself
is equally applicable to several objects in a logi-
cal sense, other available information (such as vi-
sual context) might influence the interpretation of
a given referring expression.
Such phenomena could be captured by estimat-
ing Pr(o|p, a) as
Pr(o|p, a) ? Pr(a|p, o)Pr(p|o)Pr(o)
Pr(p, a)
. (9)
Pr(a|p, o) represents the relevance of attribute
value a to part p in object o. Pr(p|o) represents
the salience of part p in object o. The underlying
idea to deal with the problem of logical ambiguity
is ?If some part of an object is mentioned, it should
be more salient than other parts.? This is related
to Grice?s maxims in a different way from mat-
ters discussed in (Dale and Reiter, 1995). Pr(p|o)
could be computed in some manner by using the
saliency map (Itti et al, 1998). Pr(o) is the prior
probability that object o is chosen. If potential
functions (such as used in (Tokunaga et al, 2005))
are used for computing Pr(o), we can naturally
rank objects, which are equally relevant to a given
referring expression, according to distances from
interlocutors.
5 Algorithms
5.1 Understanding
Understanding a referring expression e is identify-
ing the target object o? from a set of objects ?. This
is formulated in a straightforward way as
o? = argmax
o??
Pr(o|e). (10)
5.2 Generation
Generation of a referring expression is choosing
the best appropriate expression e? to discriminate a
given object o? from a set of distractors. A simple
formulation is
e? = argmax
e??
Pr(e)Pr(o?|e). (11)
? is a pre-generated set of candidate expressions
for o?. This paper does not explain how to generate
a set of candidates.
Pr(e) is the generation probability of an ex-
pression e independent of objects. This probabil-
ity can be learned from a corpus. In the evaluation
described in Section 6, we estimate Pr(e) as
Pr(e) ? Pr(|e|)
?
i
Pr(?i). (12)
Here, Pr(|e|) is the distribution of expression
length in terms of numbers of attributes used.
Pr(?) is the selection probability of a specific at-
tribute ? (SP (a) in (Spanger et al, 2008)).
6 Preliminary evaluation
As mentioned above, no adequate corpus is cur-
rently available in order to provide an initial vali-
dation of our model which we present in this pa-
per. In this section, we validate our model us-
ing the TUNA corpus (the ?Participant?s Pack?
available for download as part of the Generation
Challenge 2009) to show that it includes tradi-
tional domain modeling. We use the training-
part of the corpus for training our model and the
development-part for evaluation.
We note that we here assume a homogeneous
distribution of the probability Pr(o|p, a), i.e., we
are applying formula (8) here in order to calculate
this probability. We first implemented our proba-
bilistic model for the area of understanding. This
means our algorithm took as input the user?s selec-
tion of attribute?value pairs in the description and
calculated the most likely target object. This was
193
Table 1: Initial evaluation of proposed model for
generation in TUNA-domain
Furniture People
Total cases 80 68
Mean Dice-score 0.78 0.66
carried out for both the furniture and people do-
mains. Overall, outside of exceptional cases (e.g.,
human error), our algorithm was able to distin-
guish the target object for all human descriptions
(precision of 100%). This means it covers all the
cases the original approach dealt with.
We then implemented our model for the case of
generation. We measured the similarity of the out-
put of our algorithm with the human-produced sets
by using the Dice-coefficient (see (Belz and Gatt,
2007)). We evaluated this both for the Furniture
and People domain. The results are summarized
in Table 1.
Our focus was here to fundamentally show how
our model includes traditional modelling as a sub-
set, without much focus or effort on tuning in order
to achieve a maximum Dice-score. However, we
note that the Dice-score of our algorithm was com-
parable to the top 5-7 systems in the 2007 GRE-
Challenge (see (Belz and Gatt, 2007)) and thus
produced a relatively good result. This shows how
our algorithm ? providing a model of the referring
process in a more complex domain ? is applica-
ble as well to the very simple TUNA-domain as a
special case.
7 Discussion
In past work, parts of objects were ignored or im-
plicit. In case of the TUNA corpus, while the Fur-
niture domain ignores parts of objects, the People
domain contained parts of objects such as hair,
glasses, beard, etc. However, they were implic-
itly modeled by combining a pair of a part and its
attribute as an attribute such as hairColor. One
major advantage of our model is that, by explicitly
modelling parts of objects, it can handle the prob-
lem of logical ambiguity that is newly reported in
this paper. Although it might be possible to han-
dle the problem by extending previously proposed
algorithms in some ways, our formulation would
be clearer. Moreover, our model is directly avail-
able both for generation and understanding. Re-
ferring expressions using attributes (such as dis-
cussed in this paper) and those using discourse
contexts (such as ?it?) are separately approached
in past work. Our model possibly handles both of
them in a unified manner with a small extension.
This paper ignored relations between objects.
We, however, think that it is not difficult to prepare
algorithms handling relations using our model.
Generation using our model is performed in a
generate-and-test manner. Therefore computa-
tional complexity is a matter of concern. However,
that could be controlled by limiting the numbers
of attributes and parts under consideration accord-
ing to relevance and salience, because our model is
under the relaxed constraint of uniqueness unlike
previous work.
As future work, we have to gather data to eval-
uate our model and to statistically train lexical se-
lection in a new domain containing multipartite
objects.
References
Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proc. the MT Summit XI Workshop Using
Corpora for Natural Language Generation: Lan-
guage Generation and Machine Translation (UC-
NLG+MT), pages 75?83.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In Proc.
ENLG 05.
Helmut Horacek. 2006. Generating references to parts
of recursively structured objects. In Proc. ACL 06.
L Itti, C. Koch, and E. Niebur. 1998. A model of
saliency-based visual attention for rapid scene anal-
ysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(11):1254?1259.
Deb Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3).
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting
attributes for generating referring expressions. In
Proc. COLING 08.
Takenobu Tokunaga, Tomonori Koyama, and Suguru
Saito. 2005. Meaning of Japanese spatial nouns.
In Proc. the Second ACL-SIGSEM Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistics Formalisms and Appli-
cations, pages 93 ? 100.
194
	 
 
  	 
 	
 	 	   	
	 
 
	 


  
 



  Generation of Relative Referring Expressions based on Perceptual Grouping
Kotaro FUNAKOSHI
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
koh@cl.cs.titech.ac.jp
Satoru WATANABE
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
satoru w@cl.cs.titech.ac.jp
Naoko KURIYAMA
Department of Human System Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
kuriyama@hum.titech.ac.jp
Takenobu TOKUNAGA
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
take@cl.cs.titech.ac.jp
Abstract
Past work of generating referring expressions
mainly utilized attributes of objects and bi-
nary relations between objects. However, such
an approach does not work well when there
is no distinctive attribute among objects. To
overcome this limitation, this paper proposes a
method utilizing the perceptual groups of ob-
jects and n-ary relations among them. The key
is to identify groups of objects that are natu-
rally recognized by humans. We conducted psy-
chological experiments with 42 subjects to col-
lect referring expressions in such situations, and
built a generation algorithm based on the re-
sults. The evaluation using another 23 subjects
showed that the proposed method could effec-
tively generate proper referring expressions.
1 Introduction
In the last two decades, many researchers have stud-
ied the generation of referring expressions to enable
computers to communicate with humans about con-
crete objects in the world.
For that purpose, most past work (Appelt, 1985;
Dale and Haddock, 1991; Dale, 1992; Dale and
Reiter, 1995; Heeman and Hirst, 1995; Horacek,
1997; Krahmer and Theune, 2002; van Deemter,
2002; Krahmer et al, 2003) makes use of attributes
of an intended object (the target) and binary rela-
tions between the target and others (distractors) to
distinguish the target from distractors. Therefore,
these methods cannot generate proper referring ex-
pressions in situations where no significant surface
difference exists between the target and distractors,
and no binary relation is useful to distinguish the
target. Here, a proper referring expression means
a concise and natural linguistic expression enabling
hearers to distinguish the target from distractors.
For example, consider indicating object b to per-
son P in the situation shown in Figure 1. Note that
person P does not share the label information such
as a and b with the speaker. Because object b is
not distinguishable from objects a or c by means of
their appearance, one would try to use a binary re-
lation between object b and the table, i.e., ?A ball
to the right of the table?. 1 However, ?to the right
of ? is not a discriminatory relation, for objects a
and c are also located to the right of the table. Us-
ing a and c as a reference object instead of the ta-
ble does not make sense, since a and c cannot be
uniquely identified because of the same reason that
b cannot be identified. Such situations have never
drawn much attention, but can occur easily and fre-
quently in some domains such as object arrange-
ment (Tanaka et al, 2004).
van der Sluis and Krahmer (2000) proposed us-
ing gestures such as pointing in situations like those
shown in Figure 1. However, pointing and gazing
are not always available depending on the positional
relation between the speaker and the hearer.
In the situation shown in Figure 1, a speaker can
indicate object b to person P with a simple expres-
sion ?the front ball? without using any gesture. In
order to generate such an expression, one must be
able to recognize the salient perceptual group of the
objects and use the n-ary relative relations in the
group. 2
In this paper, we propose a method of generat-
1In this paper, we simply assume that all participants share
the appropriate reference frame (Levinson, 2003). We mention
this issue in the last section.
2Although Krahmer et al claim that their method can han-
dle n-ary relations (Krahmer et al, 2003), they provide no de-
tails. We think their method cannot directly handle situations
we discuss here.
ing referring expressions that utilizes n-ary relations
among members of a group. Our method recognizes
groups by using Tho?risson?s algorithm (Tho?risson,
1994). As the first step of our research project, we
deal with the limited situations where only homoge-
neous objects are randomly arranged (see Figure 2).
Therefore, we handle positional n-ary relation only,
and other types of n-ary relation such as size, e.g.,
?the biggest one?, are not mentioned.
Speakers often refer to multiple groups in the
course of referring to the target. In these cases, we
can observe two types of relations: the intra-group
relation such as ?the front two among the five near
the desk?, and the inter-group relation such as ?the
two to the right of the five?. We define that a sub-
sumption relation between two groups is an intra-
group relation.
In what follows, Section 2 explains the exper-
iments conducted to collect expressions in which
perceptual groups are used. The proposed method is
described and evaluated in Section 3. In Section 4,
we examine a possibility to predict the adequacy of
an expression in terms of perceptual grouping. Fi-
nally, we conclude the paper in Section 5.
P
a
b
c
Table
Figure 1: An example of problematic situations
2 Data Collection
We conducted a psychological experiment with 42
Japanese undergraduate students to collect referring
expressions in which perceptual groups are used. In
order to evaluate the collected expressions, we con-
ducted another experiment with a different group of
44 Japanese undergraduate students. There is no
overlap between the subjects of those two experi-
ments. Details of this experiment are described in
the following subsections.
2.1 Collecting Referring Expressions
Method Subjects were presented 2-dimensional
bird?s-eye images in which several objects of the
same color and the same size were arranged and the
subjects were requested to convey a target object to
the third person drawn in the same image. We used
12 images of arrangements. In each image, three
to nine objects were arranged manually so that the
objects distributes non-uniformly. An example of
images presented to subjects is shown in Figure 2.
Labels a, . . . , f, x in the image are assigned for pur-
poses of illustration and are not assigned in the ac-
tual images presented to the subjects. Each subject
was asked to describe a command so that the person
in the image picks a target object that is enclosed
with dotted lines. When a subject could not think
of a proper expression, she/he was allowed to aban-
don that arrangement and proceed to the next one.
Referring expressions designating the target object
were collected from these subjects? commands.
P
a
b
e
f
c d
x
Figure 2: A visual stimulus of the experiment
Analysis We presented 12 arrangements to 42
subjects and obtained 476 referring expressions.
Twenty eight judgments were abandoned in the ex-
periment. Observing the collected expressions, we
found that starting from a group with all of the ob-
jects, subjects generally narrow down the group to
a singleton group that has the target object. There-
fore, a referring expression can be formalized as a
sequence of groups (SOG) reflecting the subject?s
narrowing down process.
The following example shows an observed ex-
pression describing the target x in Figure 2 with the
corresponding SOG representation below it.
?hidari oku ni aru mittu no tama no uti no
itiban migi no tama.?
(the rightmost ball among the three balls
at the back left)
SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}] 3
where
{a, b, c, d, e, f, x} denotes all objects in
the image (total set),
{a, b, x} denotes the three objects at the
back left, and
{x} denotes the target.
3We denote an SOG representation by enclosing groups
with square brackets.
Since narrowing down starts from the total set,
the SOG representation starts with a set of all ob-
jects and ends with a singleton group with the tar-
get. Translating the collected referring expressions
into the SOG representation enables us to abstract
and classify the expressions. On average, we ob-
tained about 40 expressions for each arrangement,
and classified them into 8.4 different SOG represen-
tations.
Although there are two types of relations be-
tween groups as we mentioned in Section 1, the ex-
pressions using only intra-group relations made up
about 70% of the total.
2.2 Evaluating the Collected Expressions
Method Subjects were presented expressions col-
lected in the experiment described in Section 2.1 to-
gether with the corresponding images, and were re-
quested to indicate objects referred to by the expres-
sions. The presented images are the same as those
used in the previous experiment except that there are
no marks on the targets. At the same time, subjects
were requested to express their confidence in select-
ing the target, and evaluate the conciseness, and the
naturalness of the given expressions on a scale of 1
to 8.
Because the number of expressions that we could
evaluate with subjects was limited, we chose a max-
imum of 10 frequent expressions for each arrange-
ment. The expressions were chosen so that as many
different SOG representations were included as pos-
sible. If an arrangement had SOGs less than 10,
several expressions that had the same SOG but dif-
ferent surface realizations were chosen. The resul-
tant 117 expressions were evaluated by 49 subjects.
Each subject evaluated about 29.5 expressions.
Analysis Discarding incomplete answers, we ob-
tained 1,429 evaluations in total. 12.2 evaluations
were obtained for each expression on average.
We measured the quality of each expression in
terms of an evaluation value that is defined in (1).
This measure is used to analyze what kind of ex-
pressions are preferred and to set up a scoring func-
tion (6) for machine-generated expressions as de-
scribed in Section 3.1.
(evaluation value)
= (accuracy)? (confidence)
?
(naturalness) + (conciseness)
2
(1)
According to our analysis, the expressions with
only intra-group relations (84 samples) obtained
high accuracies (Ave. 79.3%) and high evaluation
values (Ave. 33.1), while the expressions with inter-
group relations (33 samples) obtained lower accura-
cies (Ave. 69.1%) and lower evaluation values (Ave.
19.7).
The expressions with only intra-group relations
are observed more than double as many as the ex-
pressions with inter-group relations. We provide a
couple of example expressions indicating object x
in Figure 2 to contrast those two types of expres-
sions below.
? without inter-group relations
? ?the rightmost ball among the three balls
at the back left?
? with inter-group relations
? ?the ball behind the two front balls?
In addition, expressions explicitly mentioning all
the objects obtained lower evaluation values. Con-
sidering these observations, we built a generation
algorithm using only intra-group relations and did
not mention all the objects explicitly.
Among these expressions, we selected those with
which the subjects successfully identified the target
with more than 90% accuracy. These expressions
are used to extract parameters of our generation al-
gorithm in the following sections.
3 Generating Referring Expressions
3.1 Generation Algorithm
Given an arrangement of objects and a target, our al-
gorithm generates referring expressions by the fol-
lowing three steps:
Step 1: enumerate perceptual groups based on the
proximity between objects
Step 2: generate the SOG representations by com-
bining the groups
Step 3: translate the SOG representations into lin-
guistic expressions
In the rest of this section, we illustrate how these
three steps generate referring expressions in the sit-
uation shown in Figure 2.
Step 1: Enumerating Perceptual Groups.
To generate perceptual groups from an arrangement,
Tho?risson?s algorithm (Tho?risson, 1994) is adopted.
Given a list of objects in an arrangement, the al-
gorithm generates groups based on the proximity of
the objects and returns a list of groups. Only groups
containing the target, that is x, are chosen because
SOG: [{a, b, c, d, e, f, x}, {a, b, x}, {x}]
? E(R({a, b, c, d, e, f, x}, {a, b, x})) + E({a, b, x}) + E(R({a, b, x}, {x})) + E({x})
? ?hidari oku no?+?mittu no tama?+?no uti no migihasi no?+?tama?
(at the back left) (three balls) (rightmost . . . among) (ball)
Figure 3: An example of surface realization
we handle intra-group relations only as mentioned
before, and that implies that all groups mentioned
in an expression must include the target. Then, the
groups are sorted in descending order of the group
size. Finally a singleton group consisting of the tar-
get is added to the end of the list if such a group is
missing in the list. The resultant group list, GL, is
the output of Step 1.
For example, the algorithm recognizes the fol-
lowing groups given the arrangement shown in Fig-
ure 2:
{{a, b, c, d, e, f, x}, {a, b, c, d, x},
{a, b, x}, {c, d}, {e, f}}.
After filtering out the groups without the target and
adding a singleton group with the target, we obtain
the following list:
{{a, b, c, d, e, f, x}, {a, b, c, d, x}, {a, b, x}, {x}}.
(2)
Step 2: Generating the SOG Representations.
In this step, the SOG representations introduced in
Section 2 are generated from the GL of Step 1,
which generally has a form like (3), where G
i
de-
notes a group, and G
0
is a group of all the objects.
Here, we narrow down the objects starting from the
total set (G
0
) to the target ({x}).
[G
0
, G
1
, . . . , G
m?2
, {x}] (3)
Given a group list GL, all possible SOGs are gen-
erated. From a group list of size m, 2m?2 SOG
representations can be generated since G
0
and {x}
should be included in the SOG representation. For
example, from a group list of {G
0
, G
1
, G
2
, {x}},
we obtain four SOGs: [G
0
, {x}], [G
0
, G
1
, {x}],
[G
0
, G
2
, {x}], and [G
0
, G
1
, G
2
, {x}].
For example, one of the SOG representations
generated from list (2) is
[{a, b, c, d, e, f, x}, {a, b, x}, {x}]. (4)
Note that any two groups G
i
and G
j
in a list of
groups generated by Tho?risson?s algorithm with re-
gard to one feature, e.g., proximity in this paper, are
mutually disjoint (G
i
?G
j
= ?), otherwise one sub-
sumes the other (G
i
? G
j
or G
j
? G
i
). No inter-
secting groups without a subsumption relation are
generated.
Step 3: Generating Linguistic Expressions.
In the last step, the SOG representations are trans-
lated into linguistic expressions. Since Japanese is
a head-final language, the order of linguistic ex-
pressions for groups are retained in the final lin-
guistic expression for the SOG representation. That
is, an SOG representation [G
0
, G
1
, . . . , G
n?2
, {x}]
can be realized as shown in (5), where E(X) de-
notes a linguistic expression for X, R(X,Y ) de-
notes a relation between X and Y , and ?+? is a
string concatenation operator.
E(G
0
) + E(R(G
0
, G
1
)) + E(G
1
) + . . .
+E(R(G
n?2
, {x})) + E({x}) (5)
As described in Section 2.2, expressions that ex-
plicitly mention all the objects obtain lower evalu-
ation values, and expressions using intra-group re-
lations obtain high evaluation values. Considering
these observations, our algorithm does not use the
linguistic expression corresponding to all the ob-
jects, that is E(G
0
), and only uses intra-group re-
lations for R(X,Y ).
Possible expressions of X are collected from the
experimental data in Section 2.1, and the first ap-
plicable expression is selected when realizing a lin-
guistic expression for X, i.e., E(X). Therefore,
this algorithm produces one linguistic expression
for each SOG even though there are some other pos-
sible expressions.
For example, the SOG representation (4) is real-
ized as shown in Figure 3.
Note that there is no mention of all the objects,
{a, b, c, d, e, f, x}, in the linguistic expression.
3.2 Evaluation of Generated Expressions
We implemented the algorithm described in Sec-
tion 3.1, and evaluated the output with 23 under-
graduate students. The subjects were different from
those of the previous experiments but were of the
same age group, and the experimental environment
Accuracy (%) Naturalness Conciseness Confidence Eval. val.
Human-12-all 87.3 4.82 5.27 6.14 29.3
Human-12-90 97.9 5.20 5.62 6.50 35.0
Human-12-100 100 5.36 5.73 6.65 37.2
System-12 91.0 5.60 6.25 6.32 40.1
System-20 88.4 5.09 5.65 6.25 35.2
System-Average 89.2 5.24 5.82 6.27 36.6
Table 1: Summary of evaluation
was the same. The evaluation of the output was per-
formed in the same manner as that of Section 2.2.
The results are shown in Table 1. ?Human-
12-all? shows the average values of all expres-
sions collected from humans with 12 arrangements
as described in Section 2.2. ?Human-12-90? and
?Human-12-100? show the average values of ex-
pressions by humans that gained more than 90%
and 100% in accuracy in the same evaluation ex-
periment respectively.
?System-12? shows the average values of expres-
sions generated by the algorithm for the 12 arrange-
ments used in the data collection experiment de-
scribed in Section 2.1. The algorithm generated 18
expressions for the 12 arrangements, which were
presented to each subject in random order for eval-
uation.
?System-20? shows the average values of expres-
sions generated by the algorithm for 20 randomly
generated arrangements that generate at least two
linguistic expressions each. The algorithm gen-
erated 48 expressions for these 20 arrangements,
which were evaluated in the same manner as that
of ?System-12?.
?System-Average? shows the micro average of
expressions of both ?System-12? and ?System-20?.
?Accuracy? shows the rates at which the sub-
jects could identify the correct target objects from
the given expressions. Comparing the accuracies of
?Human-12-*? and ?System-12?, we find that the
algorithm generates good expressions. Moreover,
the algorithm is superior to human in terms of ?Nat-
uralness? and ?Conciseness?. However, this result
should be interpreted carefully. Further investiga-
tion of the expressions revealed that humans often
sacrificed naturalness and conciseness in order to
describe the target as precisely as possible for com-
plex arrangements.
4 Scoring SOG Representations
The algorithm presented in the previous section out-
puts several possible expressions. Therefore, we
have to choose one of the expressions by calculat-
ing their scores.
The scores can be computed using various mea-
sures, such as complexity of expressions, and
salience of referent objects. In this section, we in-
vestigate whether the adequacies of the courses of
narrowing down can be predicted: that is, whether
meaningful scores of SOG representations can be
calculated.
4.1 Method for SOG Scoring
An SOG representation has a form as stated in (3).
We presumed that, when a speaker tries to narrow
down an object group from G
i
to G
i+1
, there is
an optimal ratio between the dimensions of G
i
and
G
i+1
. In other words, narrowing down a group from
a very big one to a very small one might cause hear-
ers to become confused.
For example, consider the following two expres-
sions that both indicate object x in Figure 2. Hearers
would prefer (i) to (ii) though (ii) is simpler than (i).
(i) ?the rightmost ball among the three balls at the
back left?
(ii) ?the fourth ball from the right?
In fact, we found (i) among the expressions col-
lected in Section 2.1, but did not find (ii) among
them. Our algorithm generated both (i) and (ii)
in Section 3.2, and the two expressions gained the
evaluation values of 44.4 and 32.1 respectively.
If our presumption is correct, we can expect
to choose better expressions by choosing expres-
sions that have adequate dimension ratios between
groups.
Calculation Formula
The total score of an SOG representation is calcu-
lated by averaging the scores given by functions f
1
and f
2
whose parameters are dimension ratios be-
tween two consecutive groups as given in (6), where
n is the number of groups in the SOG.
score(SOG) = 1
n ? 1
{
n?3
?
i=0
f
1
(
dim(G
i+1
)
dim(G
i
)
)
+ f
2
(
dim({x})
dim(G
n?2
)
)
} (6)
The dimension of a group dim is defined as the
average distance between the centroid of the group
and that of each object. The dimension of the sin-
gleton group {x} is defined as a constant value. Be-
cause of this idiosyncrasy of the singleton group
{x} compared to other groups, f
2
was introduced
separately from f
1
even though both functions rep-
resent the same concept, as described below.
The optimal ratio between two groups, and that
from a group to the target were found through the
quadratic regression analysis of data collected in the
experiment described in Section 2.2. f
1
and f
2
are
the two regression curves found through analysis
representing correlations between dimension ratios
and values calculated based on human evaluation as
in formula (1).We could not find direct correlations
between dimension ratios and accuracies.
4.2 Results
We checked to what extent the scores of generated
expressions given by formula (6) conformed with
the human evaluation given by formula (1) as agree-
ment. Agreement was calculated as follows using
20 randomly generated arrangements described in
Section 3.2.
First, the generated expressions were ordered ac-
cording to the score given by formula (6) and the
human evaluation given by formula (1). All binary
order relations between two expressions were ex-
tracted from these two ordered lists of expressions.
The agreement was defined as the ratio of the same
binary order relations among all binary order rela-
tions.
The agreement between scores and the human
evaluation was 45.8%. The score did not predict
SOG representations that would generate better ex-
pressions very well. Further research is required to
conclusively rule out the use of dimension ratios for
prediction or whether other factors are involved.
5 Concluding Remarks and Future Work
This paper proposed an algorithm that generates re-
ferring expressions using perceptual groups and n-
ary relations among them. The algorithm was built
on the basis of the analysis of expressions that were
collected through psychological experiments. The
performance of the algorithm was evaluated by 23
subjects and it generated promising results.
In the following, we look at future work to be
done.
Recognizing salient geometric formations:
Tho?risson?s algorithm (Tho?risson, 1994) cannot
recognize a linear arrangement of objects as a
group, although such arrangements are quite salient
for humans. This is one of the reasons for the
disconformity between the evaluations given by the
algorithm and those of the humans subjects.
We can enumerate most of such geometric ar-
rangements salient for human subject by referring to
geometric terms found in lexicons and thesauri such
as ?line?, ?circle?, ?square? and so on. Tho?risson?s
algorithm should be extended to recognize these ar-
rangements.
Using relations other than positional relations:
In this paper, we focused on positional relations of
perceptual groups. Other relations such as degree of
color and size should be treated in the same manner.
Tho?risson?s original algorithm (Tho?risson, 1994)
takes into account these relations as well as posi-
tional relations of objects when calculating similar-
ity between objects to generate groups. However, if
we generate groups using multiple relations simul-
taneously, the assumption used in Step 1 of our al-
gorithm that any pair of groups in an output list do
not intersect without a subsumption relation cannot
be held. Therefore, the mechanism generating SOG
representations (Step 2 in Section 3.1) must be re-
considered.
Resolving reference frames and differences of
perspective: We assumed that all participants in
a conversation shared the same reference frame.
However, when we apply our method to conversa-
tional agent systems, e.g., (Cavazza et al, 2002;
Tanaka et al, 2004), reference frames must be prop-
erly determined each time to generate referring ex-
pressions. Although there are many studies con-
cerning reference frames, e.g., (Clark, 1973; Her-
skovits, 1986; Levinson, 2003), little attention has
been paid to how reference frames are determined in
terms of the perceptual groups and their elements.
In addition to reference frames, differences of
perspective also have to be taken into account to
produce proper referring expressions since humans
often view spatial relations between objects in a
3-dimensional space by projecting them on a 2-
dimensional plane. In the experiments, we pre-
sented the subjects with 2-dimensional bird?s-eye
images. The result might have been different if we
had used 3-dimensional images instead, because the
projection changes the sizes of objects and spatial
relations among them.
Integration with conventional methods: In this
paper, we focused on a limited situation where in-
herent attributes of objects do not serve any identi-
fying function, but this is not the case in general. An
algorithm integrating conventional attribute-based
methods and the proposed method should be formu-
lated to achieve the end goal.
A possible direction would be to enhance the al-
gorithm proposed by Krahmer et al (Krahmer et
al., 2003). They formalize an object arrangement
(scene) as a labeled directed graph in which ver-
tices model objects and edges model attributes and
binary relations, and regard content selection as a
subgraph construction problem. Their algorithm
performs searches directed by a cost function on a
graph to find a unique subgraph.
If we consider a perceptual group as an ordinary
object as shown in Figure 4, their algorithm is appli-
cable. It will be able to handle not only intra-group
relations (e.g., the edges with labels ?front?, ?mid-
dle?, and ?back? in Figure 4) but also inter-group re-
lations (e.g., the edge from ?Group 1? to ?Table? in
Figure 4). However, introducing perceptual groups
as vertices makes it difficult to design the cost func-
tion. A well-designed cost function is indispensable
for generating concise and comprehensible expres-
sions. Otherwise, an expression like ?a ball in front
of a ball in front of a ball? for the situation shown in
Figure 1 would be generated.
Group 1
b c a
Table
front_of front_of right_of
back_of back_of left_of
front
middle back
right_of
right_of
right_of
Figure 4: A simplified graph with a group vertex for
the situation shown in Figure 1
References
Douglas E. Appelt. 1985. Planning English refer-
ring expressions. Artificial Intelligence, 26:1?33.
Mark Cavazza, Fred Charles, and Steven J. Mead.
2002. Character-based interactive stroytelling.
IEEE Intelligent Systems, 17(4):17?24.
Herbert H. Clark. 1973. Space, time, semantics,
and the child. In T. E. Moore, editor, Cogni-
tive development and the acquisition of language,
pages 65?110. Academic Press.
Robert Dale and Nicholas Haddock. 1991. Gener-
ating referring expressions involving relations. In
Proceedings of the Fifth Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics(EACL?91), pages 161?166.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gen-
eration of referring expressions. Cognitive Sci-
ence, 19(2):233?263.
Robert Dale. 1992. Generating referring expres-
sions: Constructing descriptions in a domain of
objects and processes. MIT Press, Cambridge.
Peter Heeman and Graem Hirst. 1995. Collabo-
rating referring expressions. Computational Lin-
guistics, 21(3):351?382.
Annette Herskovits. 1986. Language and Spa-
tial cognition: an interdisciplinary study of the
prepositions in English. Cambridge University
Press.
Helmut Horacek. 1997. An algorithm for gener-
ating referential descriptions with flexible inter-
faces. In Proceedings of the 35th Annual Meeting
of the Association for Computational Linguistics,
pages 206?213.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of descriptions. In
Kees van Deemter and Rodger Kibble, editors,
Information Sharing: Givenness and Newness in
Language Processing. CSLI Publications, Stan-
ford, California.
Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg. 2003. Graph-based generation of re-
ferring expressions. Computational Linguistics,
29(1):53?72.
Stephen C. Levinson, editor. 2003. Space in Lan-
guage and Cognition. Cambridge University
Press.
Hozumi Tanaka, Takenobu Tokunaga, and Yusuke
Shinyama. 2004. Animated agents capable of
understanding natural language and perform-
ing actions. In Helmut Prendinger and Mituru
Ishizuka, editors, Life-Like Characters, pages
429?444. Springer.
Kristinn R. Tho?risson. 1994. Simulated perceptual
grouping: An application to human-computer in-
teraction. In Proceedings of the Sixteenth An-
nual Conference of the Cognitive Science Society,
pages 876?881.
Kees van Deemter. 2002. Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28(1):37?
52.
Ielka van der Sluis and Emiel Krahmer. 2000.
Generating referring expressions in a multimodal
context: An empirically oriented approach. Pre-
sented at the CLIN meeting 2000, Tilburg.
Identifying Repair Targets in Action Control Dialogue
Kotaro Funakoshi and Takenobu Tokunaga
Department of Computer Science,
Tokyo Institute of Technology
2-12-1 Oookayama Meguro, Tokyo, JAPAN
{koh,take}@cl.cs.titech.ac.jp
Abstract
This paper proposes a method for deal-
ing with repairs in action control dialogue
to resolve participants? misunderstanding.
The proposed method identifies the re-
pair target based on common grounding
rather than surface expressions. We extend
Traum?s grounding act model by introduc-
ing degree of groundedness, and partial
and mid-discourse unit grounding. This
paper contributes to achieving more natu-
ral human-machine dialogue and instanta-
neous and flexible control of agents.
1 Introduction
In natural language dialogue, misunderstanding
and its resolution is inevitable for the natural
course of dialogue. The past research dealing
with misunderstanding has been focused on the di-
alogue involving only utterances. In this paper,
we discuss misunderstanding problem in the di-
alogue involving participant?s actions as well as
utterances. In particular, we focus on misunder-
standing in action control dialogue.
Action control dialogue is a kind of task-
oriented dialogue in which a commander con-
trols the actions1 of other agents called followers
through verbal interaction.
This paper deals with disagreement repair ini-
tiation utterances2 (DRIUs) which are used by
commanders to resolve followers? misunderstand-
ings3, or to correct commanders? previous erro-
neous utterances. These are so called third-turn
1We use the term ?action? for the physical behavior of
agents except for speaking.
2This denomination is lengthy and may be still controver-
sial. However we think this is most descriptively adequate for
the moment.
3Misunderstanding is a state where miscommunication
has occurred but participants are not aware of this, at least
initially (Hirst et al, 1994).
repair (Schegloff, 1992). Unlike in ordinary dia-
logue consisting of only utterances, in action con-
trol dialogue, followers? misunderstanding could
be manifested as their inappropriate actions in re-
sponse to a given command.
Let us look at a sample dialogue (1.1 ? 1.3). Ut-
terance (1.3) is a DRIU for repairing V?s mis-
understanding of command (1.1) which is mani-
fested by his action performed after saying ?OK?
in (1.2).
(1.1) U: Put the red book on the shelf to the right.
(1.2) V: OK. <V performs the action>
(1.3) U: Not that.
It is not easy for machine agents to under-
stand DRIUs because they can sometimes be so
elliptical and context-dependent that it is difficult
to apply traditional interpretation methodology to
DRIUs.
In the rest of this paper, we describe the dif-
ficulty of understanding DRIUs and propose a
method to identify repair targets. The identifica-
tion of repair targets plays a key role in under-
standing DRIUs and this paper is intensively fo-
cused on this issue.
2 Difficulty of Understanding DRIUs
Understanding a DRIU consists of repair tar-
get identification and repair content interpretation.
Repair target identification identifies a target to be
repaired by the speaker?s utterance. Repair con-
tent interpretation recovers the speaker?s intention
by replacing the identified repair target with the
correct one.
One of the major source of difficulties in un-
derstanding DRIUs is that they are often elliptical.
Repair content interpretation depends heavily on
repair targets but the information to identify re-
pair targets is not always mentioned explicitly in
DRIUs.
Let us look at dialogue (1.1 ? 1.3) again. The
DRIU (1.3) indicates that V failed to identify U?s
intended object in utterance (1.1). However, (1.3)
does not explicitly mention the repair target, i.e.,
either book or shelf in this case.
The interpretation of (1.3) changes depending
on when it is uttered. More specifically, the inter-
pretation depends on the local context and the sit-
uation when the DRIU is uttered. If (1.3) is uttered
when V is reaching for a book, it would be natu-
ral to consider that (1.3) is aimed at repairing V?s
interpretation of ?the book?. On the other hand,
if (1.3) is uttered when V is putting the book on a
shelf, it would be natural to consider that (1.3) is
aimed at repairing V?s interpretation of ?the shelf
to the right?.
Assume that U uttered (1.3) when V was putting
a book in his hand on a shelf, how can V identify
the repair target as shelf instead of book? This pa-
per explains this problem on the basis of common
grounding (Traum, 1994; Clark, 1996). Common
grounding or shortly grounding is the process of
building mutual belief among a speaker and hear-
ers through dialogue. Note that in action control
dialogue, we need to take into account not only
utterances but also followers? actions. To identify
repair targets, we keep track of states of grounding
by treating followers? actions as grounding acts
(see Section 3). Suppose V is placing a book in
his hand on a shelf. At this moment, V?s inter-
pretation of ?the book? in (1.1) has been already
grounded, since U did not utter any DRIU when
V was taking the book. This leads to the interpre-
tation that the repair target of (1.1) is shelf rather
than already grounded book.
3 Grounding
This section briefly reviews the grounding acts
model (Traum, 1994) which we adopted in our
framework. We will extend the grounding act
model by introducing degree of groundedness that
have a quaternary distinction instead of the orig-
inal binary distinction. The notions of partial
grounding and mid-discourse unit grounding are
also introduced for dealing with action control di-
alogue.
3.1 Grounding Acts Model
The grounding acts model is a finite state transi-
tion model to dynamically compute the state of
grounding in a dialogue from the viewpoint of
each participant.
This theory models the process of grounding
with a theoretical construct, namely the discourse
unit (DU). A DU is a sequence of utterance units
(UUs) assigned grounding acts (GAs). Each UU
in a dialogue has at least one GA, except fillers or
several cue phrases, which are considered useful
for turn taking but not for grounding. Each DU
has an initiator (I) who opened it, and other par-
ticipants of that DU are called responders (R).
Each DU is in one of seven states listed in Ta-
ble 1 at a time. Given one of GAs shown in Table 2
as an input, the state of DU changes according to
the current state and the input. A DU starts with
a transition from initial state S to state 1, and fin-
ishes at state F or D. DUs in state F are regarded
as grounded.
Analysis of the grounding process for a sam-
ple dialogue is illustrated in Figure 1. Speaker B
can not understand the first utterance by speaker
A and requests a repair (ReqRep-R) with his ut-
terance. Responding to this request, A makes a
repair (Repair-I). Finally, B acknowledges to
show he has understood the first utterance and the
discourse unit reaches the final state, i.e., state F.
State Description
S Initial state
1 Ongoing
2 Requested a repair by a responder
3 Repaired by a responder
4 Requested a repair by the initiator
F Finished
D Canceled
Table 1: DU states
Grounding act Description
Initiate Begin a new DU
Continue Add related content
Ack Present evidences of understanding
Repair Correct misunderstanding
ReqRepair Request a repair act
ReqAck Request an acknowledge act
Cancel Abandon the DU
Table 2: Grounding acts
UU DU1
A : Can I speak to Jim Johnstone
please?
Init-I 1
B : Senior? ReqRep-R 2
A : Yes Repair-I 1
B : Yes Ack-R F
Figure 1: An example of grounding (Ishizaki and
Den, 2001)
178
3.2 Degree of Groundedness and Evidence
Intensity
As Traum admitted, the binary distinction between
grounded and ungrounded in the grounding acts
model is an oversimplification (Traum, 1999). Re-
pair target identification requires more finely de-
fined degree of groundedness. The reason for this
will be elucidated in Section 5.
Here, we will define the four levels of evidence
intensity and equate these with degrees of ground-
edness, i.e., if an utterance is grounded with evi-
dence of level N intensity, the degree of ground-
edness of the utterance is regarded as level N .
(2) Levels of evidence intensity
Level 0: No evidence (i.e., not grounded).
Level 1: The evidence shows that the re-
sponder thinks he understood the utter-
ance. However, it does not necessar-
ily mean that the responder understood
it correctly. E.g., the acknowledgment
?OK? in response to the request ?turn to
the right.?
Level 2: The evidence shows that the re-
sponder (partially) succeeded in trans-
ferring surface level information. It does
not yet ensure that the interpretation of
the surface information is correct. E.g.,
the repetition ?to the right? in response
to the request ?turn to the right.?
Level 3: The evidence shows that the re-
sponder succeeded in interpretation.
E.g., turning to the right as the speaker
intended in response to the request ?turn
to the right.?
3.3 Partial and mid-DU Grounding
In Traum?s grounding model, the content of a DU
is uniformly grounded. However, things in the
same DU should be more finely grounded at var-
ious levels individually. For example, if one ac-
knowledged by saying ?to the right? in response
to the command ?put the red chair to the right of
the table?, to the right of should be regarded as
grounded at Level 2 even though other parts of the
request are grounded at Level 1.
In addition, in Traum?s model, the content of a
DU is grounded all at once when the DU reaches
the final state, F. However, some elements in a DU
can be grounded even though the DU has not yet
reached state F. For example, if one requested a
repair as ?to the right of what?? in response to
the command ?put the red chair to the right of
the table?, to the right of should be regarded as
grounded at level 2 even though table has not yet
been grounded.
Although Traum admitted these problems ex-
isted in his model, he retained it for the sake of
simplicity. However, such partial and mid-DU
grounding is necessary to identify repair targets.
We will describe the usage of these devices to
identify repair targets in Section 5. In brief, when
a level 3 evidence is presented by the follower and
negative feedback (i.e., DRIUs) is not provided by
the commander, only propositions supported by
the evidence are considered to be grounded even
though the DU has not yet reached state F.
4 Treatment of Actions in Dialogue
In general, past work on discourse has targeted di-
alogue consisting of only utterances, or has con-
sidered actions as subsidiary elements. In contrast,
this paper targets action control dialogue, where
actions are considered to be primary elements of
dialogue as well as utterances.
Two issues have to be mentioned for handling
action control dialogue in the conventional se-
quential representation as in Figure 1. We will in-
troduce assumptions (3) and (4) as shown below.
Overlap between utterances and actions
Actions in dialogue do not generally obey turn
allocation rules as Clark pointed out (Clark, 1996).
In human-human action control dialogue, follow-
ers often start actions in the middle of a comman-
der?s utterance. This makes it difficult to analyze
discourse in sequential representation. Given this
fact, we impose the three assumptions on follow-
ers as shown in (3) so that followers? actions will
not overlap the utterances of commanders. These
requirements are not unreasonable as long as fol-
lowers are machine agents.
(3) Assumptions on follower?s actions
(a) The follower will not commence action
until turn taking is allowed.
(b) The follower immediately stops the ac-
tion when the commander interrupts
him.
(c) The follower will not make action as pri-
mary elements while speaking. 4
4We regard gestures such as pointing as secondary ele-
179
Hierarchy of actions
An action can be composed of several sub-
actions, thus has a hierarchical structure. For ex-
ample, making tea is composed of boiling the wa-
ter, preparing the tea pot, putting tea leaves in the
pot, and pouring the boiled water into it, and so
on. To analyze actions in dialogue as well as ut-
terances in the traditional way, a unit of analysis
should be determined. We assume that there is a
certain granularity of action that human can recog-
nize as primitive. These actions would correspond
to basic verbs common to humans such as ?walk?,
?grasp?, ?look?, etc.We call these actions funda-
mental actions and consider them as UUs in action
control dialogue.
(4) Assumptions on fundamental actions
In the hierarchy of actions, there is a cer-
tain level consisting of fundamental actions
that human can commonly recognize as prim-
itives. Fundamental actions can be treated as
units of primary presentations in an analogy
with utterance units .
5 Repair Target Identification
In this section, we will discuss how to identify the
repair target of a DRIU based on the notion of
grounding. The following discussion is from the
viewpoint of the follower.
Let us look at a sample dialogue (5.1 ? 5.5),
where U is the commander and V is the fol-
lower. The annotation Ack1-R:F in (5.2) means
that (5.2) has grounding act Ack by the respon-
der (R) for DU1 and the grounding act made DU1
enter state F. The angle bracketed descriptions in
(5.3) and (5.4) indicate the fundamental actions by
V.
Note that thanks to assumption (4) in Section 4,
a fundamental action itself can be considered as a
UU even though the action is performed without
any utterances.
(5.1) U: Put the red ball on the left box. (Init1-I:1)
(5.2) V: Sure. (Ack1-R:F)
(5.3) V: <V grasps the ball> (Init2-I:1)
(5.4) V: <V moves the ball> (Cont2-I:1)
(5.5) U: Not that. (Repair1-R:3)
The semantic content of (5.1) can be repre-
sented as a set of propositions as shown in (6).
ments when they are presented in parallel with speech. There-
fore, this constraint does not apply to them.
(6) ? = Request(U, V, Put(#Agt1, #Obj1, #Dst1))
(a) speechActType(?)=Request
(b) presenter(?)=U
(c) addressee(?)=V
(d) actionType(content(?))=Put
(e) agent(content(?))=#Agt1,
referent(#Agt1)=V
(f) object(content(?))=#Obj1,
referent(#Obj1)=Ball1
(g) destination(content(?))=#Dst1,
referent(#Dst1)=Box1
? represents the entire content of (5.1). Sym-
bols beginning with a lower case letter are func-
tion symbols. For example, (6a) means the speech
act type for ? is ?Request?. Symbols beginning
with an upper case letter are constants. ?Request?
is the name of a speech act type and ?Move? is
that of fundamental action respectively. U and V
represents dialogue participants and ?Ball1? rep-
resents an entity in the world. Symbols beginning
with # are notional entities introduced in the dis-
course and are called discourse referents. A dis-
course referent represents something referred to
linguistically. During a dialogue, we need to con-
nect discourse referents to entities in the world, but
in the middle of the dialogue, some discourse ref-
erents might be left unconnected. As a result we
can talk about entities that we do not know. How-
ever, when one takes some actions on a discourse
referent, he must identify the entity in the world
(e.g., an object or a location) corresponding to the
discourse referent. Many problems in action con-
trol dialogue are caused by misidentifying entities
in the world.
Follower V interprets (5.1) to obtain (6), and
prepares an action plan (7) to achieve ?Put(#Agt1,
#Obj1, #Dst1)?. Plan (7) is executed downward
from the top.
(7) Plan for Put(#Agt1, #Obj1, #Dst1)
Grasp(#Agt1, #Obj1),
Move(#Agt1, #Obj1, #Dst1),
Release(#Agt1, #Obj1)
Here, (5.1 ? 5.5) are reformulated as in (8.1 ?
8.5). ?Perform? represents performing the action.
(8.1) U: Request(U, V, Put(#Agt1, #Obj1, #Dst1))
(8.2) V: Accept(V, U, ?)
(8.3) V: Perform(V, U, Grasp(#Agt1, #Obj1))
180
(8.4) V: Perform(V, U,Move(#Agt1, #Obj1, #Dst1))
(8.5) U: Inform(U, V, incorrect(X))
To understand DRIU (5.5), i.e., (8.5), follower
V has to identify repair target X in (8.5) referred
to as ?that? in (5.5). In this case, the repair target
of (5.5) X is ?the left box?, i.e., #Dst1.5 However,
the pronoun ?that? cannot be resolved by anaphora
resolution only using textual information.
We treat propositions, or bindings of variables
and values, such as (6a ? 6g), as the minimum
granularity of grounding because the identification
of repair targets requires that granularity. We then
make the following assumptions concerning repair
target identification.
(9) Assumptions on repair target identification
(a) Locality of elliptical DRIUs: The target
of an elliptical DRIU that interrupted the
follower?s action is a proposition that is
given an evidence of understanding by
the interrupted action.
(b) Instancy of error detection: A dialogue
participant observes his dialogue con-
stantly and actions presenting strong ev-
idence (Level 3). Thus, when there is an
error, the commander detects it immedi-
ately once an action related to that error
occurs.
(c) Instancy of repairs: If an error is
found, the commander immediately in-
terrupts the dialogue and initiates a re-
pair against it.
(d) Lack of negative evidence as positive
evidence: The follower can determine
that his interpretation is correct if the
commander does not initiates a repair
against the follower?s action related to
the interpretation.
(e) Priority of repair targets: If there are
several possible repair targets, the least
grounded one is chosen.
(9a) assumes that a DRIU can only be ellipti-
cal when it presupposes the use of local context to
identify its target. It also predicts that if the target
of a repair is neither local nor accessible within
local information, the DRIU will not be elliptical
depending on local context but contain explicit and
5We assume that there is a sufficiently long interval be-
tween the initiations of (5.4) and (5.5).
sufficient information to identify the target. (9b)
and (9c) enable (9a).
Nakano et al (2003) experimentally confirmed
that we observe negative responses as well as pos-
itive responses in the process of grounding. Ac-
cording to their observations, speakers continue
dialogues if negative responses are not found even
when positive responses are not found. This evi-
dence supports (9d).
An intuitive rationale for (9e) is that an issue
with less proof would more probably be wrong
than one with more proof.
Now let us go through (8.2) to (8.5) again ac-
cording to the assumptions in (9). First, ? is
grounded at intensity level 1 by (8.2). Second, V
executes Grasp(#Agt1, #Obj1) at (8.3). Because
V does not observe any negative response from U
even after this action is completed, V considers
that the interpretations of #Agt1 and #Obj1 have
been confirmed and grounded at intensity level 3
according to (9d) (this is the partial and mid-DU
grounding mentioned in Section 3.3). After initiat-
ing Move(#Agt1, #Obj1, #Dst1), V is interrupted
by commander U with (8.5) in the middle of the
action.
V interprets elliptical DRIU (5.5) as ?Inform(S,
T, incorrect(X))?, but he cannot identify repair tar-
get X. He tries to identify this from the discourse
state or context. According to (9a), V assumes that
the repair target is a proposition that its interpre-
tation is demonstrated by interrupted action (8.4).
Due to the nature of the word ?that?, V knows that
possible candidates are not types of action or the
speech act but discourse referents #Agt1, #Obj1
and #Dst16. Here, #Agt1 and #Obj1 have been
grounded at intensity level 3 by the completion of
(8.3). Now, (9e) tells V that the repair target is
#Dst1, which has only been grounded at intensity
level 1 7.
(10) below summarizes the method of repair tar-
get identification based on the assumptions in (9).
(10) Repair target identification
6We have consistently assumed Japanese dialogues in this
paper although examples have been translated into English.
?That? is originally the pronoun ?sotti? in Japanese, which
can only refer to objects, locations, or directions, but cannot
refer to actions.
7There are two propositions concerned with #Dst1:
destination(content(?)) = #Dst1 and referent(#Dst1) = Box1.
However if dest(content(?)) = #Dst1 is not correct, this
means that V grammatically misinterpreted (8.1). It seems
hard to imagine for participants speaking in their mother
tongue and thus one can exclude dest(content(?)) = #Dst1
from the candidates of the repair target.
181
(a) Specify the possible types of the repair
target from the linguistic expression.
(b) List the candidates matching the types
determined in (10a) from the latest pre-
sented content.
(c) Rank candidates based on groundedness
according to (9e) and choose the top
ranking one.
Dependencies between Parameters
The follower prepares an action plan to achieve
the commander?s command as in plan (7). Here,
the planned actions can contain parameters not di-
rectly corresponding to the propositions given by
the commander. Sometimes a selected parameter
by using (10) is not the true target but the depen-
dent of the target. Agents must retrieve the true
target by recognizing dependencies of parameters.
For example, assume a situation where objects
are not within the follower?s reach as shown in
Figure 2. Then, the commander issues command
(6) to the follower (Agent1 in Figure 2) and he
prepares an action plan (11).
(11) Agent1?s plan (partial) for (6) in Figure 2.
Walk(#Agt1, #Dst1),
Grasp(#Agt1, #Obj1),
. . .
The first Walk is a prerequisite action for Grasp
and #Dst1 depends on #Obj1. In this case, if refer-
ent(#Obj1) is Object1 then referent(#Dst1) is Po-
sition1, or if referent(#Obj1) is Object2 then ref-
erent(#Dst1) is Position2. Now, assume that the
commander intends referent(#Obj1) to be Object2
with (6), but the follower interprets this as refer-
ent(#Obj1) = Object1 (i.e., referent(#Dst1) = Po-
sition1) and performs Walk(#Agt1, #Dst1). The
commander then observes the follower moving to-
ward a direction different from his expectation and
infers the follower has misunderstood the target
object. He, then, interrupts the follower with the
utterance ?not that? at the timing illustrated in Fig-
ure 3. Because (10c) chooses #Dst2 as the repair
target, the follower must be aware of the depen-
dencies between parameters #Dst1 and #Obj1 to
notice his misidentification of #Obj1.
6 Implementation and Some Problems
We implemented the repair target identification
method described in Section 5 into our prototype
Position1
?Agent1 Object1 (wrong)
Object2 (correct)
?Position2
Figure 2: Situation with dependent parameters
Time
Walk(#Agt1, #Dst1) Grasp(#Agt1, #Obj1)
  " Not that "
Figure 3: Dependency between parameters
dialogue system (Figure 4). The dialogue system
has animated humanoid agents in its visualized 3D
virtual world. Users can command the agent by
speech to move around and relocate objects.
Figure 4: Snapshot of the dialogue system
Because our domain is rather small, current pos-
sible repair targets are agents, objects and goals
of actions. According to the qualitative evalua-
tion of the system through interaction with sev-
eral subjects, most of the repair targets were cor-
rectly identified by the proposed method described
in Section 5. However, through the evaluation, we
found several important problems to be solved as
below.
6.1 Feedback Delay
In a dialogue where participants are paying atten-
tion to each other, the lack of negative feedback
can be considered as positive evidence (see (9d)).
However, it is not clear how long the system needs
to wait to consider the lack of negative feedback as
positive evidence. In some cases, it will be not ap-
propriate to consider the lack of negative feedback
182
as positive evidence immediately after an action
has been completed. Non-linguistic information
such as nodding and gazing should be taken into
consideration to resolve this problem as (Nakano
et al, 2003) proposed.
Positive feedback is also affected by delay.
When one receives feedback shortly after an action
is completed and begins the next action, it may be
difficult to determine whether the feedback is di-
rected to the completed action or to the just started
action.
6.2 Visibility of Actions
The visibility of followers? actions must be con-
sidered. If the commander cannot observe the fol-
lower?s action due to environmental conditions,
the lack of negative feedback cannot be positive
evidence for grounding.
For example, assume the command ?bring me
a big red cup from the next room? is given and
assume that the commander cannot see the inside
of the next room. Because the follower?s funda-
mental action of taking a cup in the next room is
invisible to the commander, it cannot be grounded
at that time. They have to wait for the return of the
follower with a cup.
6.3 Time-dependency of Grounding
Utterances are generally regarded as points on the
time-line in dialogue processing. However, this
approximation cannot be applied to actions. One
action can present evidences for multiple propo-
sitions but it will present these evidences at con-
siderably different time. This affects repair target
identification.
Let us look at an action Walk(#Agt, #Dst),
where agent #Agt walks to destination #Dst. This
action will present evidence for ?who is the in-
tended agent (#Agt)? at the beginning. However,
the evidence for ?where is the intended position
(#Dst)? will require the action to be completed.
However, if the position intended by the follower
is in a completely different direction from the one
intended by the commander, his misunderstanding
will be evident at a fairly early stage of the action.
6.4 Differences in Evidence Intensities
between Actions
Evidence intensities vary depending on the char-
acteristics of actions. Although the symbolic de-
scription of actions such as (12) and (13) does not
explicitly represent differences in intensity, there
is a significant difference between (12) where
#Agent looks at #Object at a distance, and (13)
where #Agent directly contacts #Object. Agents
must recognize these differences to conform with
human recognition and share the same state of
grounding with participants.
(12) LookAt(#Agent, #Object)
(13) Grasp(#Agent, #Object)
6.5 Other Factors of Confidence in
Understanding
Performing action can provide strong evidence of
understanding and such evidence enables partic-
ipants to have strong confidence in understand-
ing. However, other factors such as linguistic con-
straints (not limited to surface information) and
plan/goal inference can provide confidence in un-
derstanding without grounding. Such factors of
confidence also must be incorporated to explain
some repairs.
Let us see a sample dialogue below, and assume
that follower V missed the word red in (14.3).
(14.1) U: Get the white ball in front of the table.
(14.2) V: OK. <V takes a white ball>
(14.3) U: Put it on the (red) table.
(14.4) V: Sure. <V puts the white ball holding in
his hand on a non-red table>
(14.5) U: I said red.
When commander U repairs V?s misunder-
standing by (14.5), V cannot correctly decide that
the repair target is not ?it? but ?the (red) table? in
(14.3) by using the proposed method, because the
referent of ?it? had already been in V?s hand and
no explicit action choosing a ball was performed
after (14.3). However, in such a situation we seem
to readily doubt misunderstanding of ?the table?
because of strong confidence in understanding of
?it? that comes from outside of grounding process.
Hence, we need a unified model of confidence in
understanding that can map different sources of
confidence into one dimension. Such a model is
also useful for clarification management of dia-
logue systems.
7 Discussion
7.1 Advantage of Proposed Method
The method of repair target identification pro-
posed in this paper less relies on surface infor-
mation to identify targets. This is advantageous
183
against some sort of misrecognitions by automatic
speech recognizers and contributes to the robust-
ness of spoken dialogue systems.
Only surface information is generally insuffi-
cient to identify repair targets. For example, as-
sume that there is an agent acting in response to
(15) and his commander interrupts him with (16).
(15) Put the red ball on the table
(16) Sorry, I meant blue
If one tries to identify the repair target with sur-
face information, the most likely candidate will
be ?the red ball? because of the lexical similar-
ity. Such methods easily break down. They can-
not deal with (16) after (17). If, however, one pays
attention to the state of grounding as our proposed
method, he can decide which one is likely to be re-
paired ?the red ball? or ?the green table? depend-
ing on the timing of the DRIU.
(17) Put the red ball on the green table
7.2 Related Work
McRoy and Hirst (1995) addressed the detection
and resolution of misunderstandings on speech
acts using abduction. Their model only dealt with
speech acts and did not achieve our goals.
Ardissono et al (1998) also addressed the same
problem but with a different approach. Their
model could also handle misunderstanding regard-
ing domain level actions. However, we think that
their model using coherence to detect and resolve
misunderstandings cannot handle DRIUs such as
(8.5), since both possible repairs for #Obj1 and
#Dst1 have the same degree of coherence in their
model.
Although we did not adopt this, the notion of
QUD (questions under discussion) proposed by
Ginzburg (Ginzburg, 1996) would be another pos-
sible approach to explaining the problems ad-
dressed in this paper. It is not yet clear whether
QUD would be better or not.
8 Conclusion
Identifying repair targets is a prerequisite to un-
derstand disagreement repair initiation utterances
(DRIUs). This paper proposed a method to iden-
tify the target of a DRIU for conversational agents
in action control dialogue. We explained how a re-
pair target is identified by using the notion of com-
mon grounding. The proposed method has been
implemented in our prototype system and eval-
uated qualitatively. We described the problems
found in the evaluation and looked at the future
directions to solve these problems.
Acknowledgment
This work was supported in part by the Ministry of
Education, Science, Sports and Culture of Japan as
the Grant-in-Aid for Creative Basic Research No.
13NP0301.
References
L. Ardissono, G. Boella, and R. Damiano. 1998. A
plan based model of misunderstandings in cooper-
ative dialogue. International Journal of Human-
Computer Studies, 48:649?679.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Jonathan Ginzburg. 1996. Interrogatives: ques-
tions, facts and dialogue. In Shalom Lappin, editor,
The Handbook of Contemporary Semantic Theory.
Blackwell, Oxford.
G. Hirst, S. McRoy, P. Heeman, P. Edmonds, and
D. Horton. 1994. Repairing conversational misun-
derstandings and non-understandings. Speech Com-
munication, 15:213?230.
Masato Ishizaki and Yasuharu Den. 2001. Danwa
to taiwa (Discourse and Dialogue). University of
Tokyo Press. (In Japanese).
Susan Weber McRoy and Graeme Hirst. 1995. The re-
pair of speech act misunderstandings by abductive
inference. Computational Linguistics, 21(4):435?
478.
Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-
tine Cassell. 2003. Towards a model of face-to-face
grounding. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
553?561.
E. A Schegloff. 1992. Repair after next turn: The
last structurally provided defense of intersubjectiv-
ity in conversation. American Journal of Sociology,
97(5):1295?1345.
David R. Traum. 1994. Toward a Computational
Theory of Grounding. Ph.D. thesis, University of
Rochester.
David R. Traum. 1999. Computational models of
grounding in collaborative systems. In Working
Papers of AAAI Fall Symbosium on Psychological
Models of Communication in Collaborative Systems,
pages 137?140.
184
Controlling Animated Agents in Natural Language
Kotaro Funakoshi
Department of Computer Science,
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8552, Japan
koh@cl.cs.titech.ac.jp
Takenobu Tokugana
Department of Computer Science,
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8552, Japan
take@cl.cs.titech.ac.jp
Abstract
This paper presents a prototype dia-
logue system, K3 , in which a user can
instruct agents through speech input to
manipulate various objects in a 3-D vir-
tual world. In this paper, we focus
on two distinctive features of the K3
system: plan-based anaphora resolution
and handling vagueness in spatial ex-
pressions. After an overview of the sys-
tem architecture, each of these features
is described. We also look at the future
research agenda of this system.
1 Introduction
SHRDLU (?) can be considered as the most im-
portant natural language understanding system.
Although SHRDLU was not ?embodied?, hav-
ing had only a small stick to manipulate objects,
it certainly had several features that a conversa-
tional agent is supposed to have. It had a great
potential, and it was very promising for future re-
search on natural language understanding.
Recently better technologies have become
available in speech recognition and natural lan-
guage processing. Major breakthroughs in the
area of computer graphics have enabled us to gen-
erate complex, yet realistic 3-D animated agents
or embodied life-like agents in a virtual environ-
ment. Researchers are now in a good position to
go beyond SHRDLU by combining these tech-
nologies (?). This paper presents a conversational
animated agent system, K3 .
Since all the actions carried out by an agent of
the K3 system are visible, we can evaluate the
performance of the system by observing its an-
imation. Visualizing the agents? actions yields
many interesting issues from a cognitive science
point of view; more complex processes are in-
volved than those found in most conventional nat-
ural language understanding systems.
After sketching out the overview of the K3 sys-
tem in section 2, Two distinctive features of K3
are discussed in section 3, and 4. Finally, sec-
tion 5 concludes the paper and looks at future re-
search agenda.
2 System Overview
A screen shot of K3 is shown in Fig. 1. There are
two agents and several objects in a virtual world.
The current system accepts simple Japanese utter-
ances with anaphoric and elliptical expressions,
such as ?Walk to the desk.? and ?Further?. The
size of the lexicon is about 100 words.
Figure 1: A screenshot of K3
91
The architecture of the K3 is illustrated in
Fig. 2. system. The speech recognition module
receives the user?s speech input and generates a
sequence of words. The syntactic/semantic anal-
ysis module analyzes the word sequence to ex-
tract a case frame. This module accepts ill-formed
speech input including postposition omission, in-
version, and self-correction. At this stage, not
all case slots are necessarily filled, because of el-
lipses in the utterance. Even in cases where there
is no ellipsis, instances of objects are not identi-
fied at this stage.
Resolving ellipses and anaphora, and identify-
ing instances in the world are performed by the
discourse analysis module. Anaphora resolution
and instance identification are achieved by using
plan-knowledge, which will be described in sec-
tion 3.
The discourse analysis module extracts the
user?s goal as well and hands it over to the plan-
ning modules, which build a plan to generate the
appropriate animation. In other words, the plan-
ning modules translate the user?s goal into anima-
tion data. However, the properties of these two
ends are very different and straightforward trans-
lation is rather difficult. The user?s goal is repre-
sented in terms of symbols, while the animation
data is a sequence of numeric values. To bridge
this gap, we take a two-stage approach ? macro-
and micro-planning.
During the macro-planning, the planner needs
to know the physical properties of objects, such
as their size, location and so on. For example, to
pick up a ball, the agent first needs to move to the
location at which he can reach the ball. In this
planning process, the distance between the ball
and the agent needs to be calculated. This sort
of information is represented in terms of coordi-
nate values of the virtual space and handled by the
micro-planner.
To interface the macro- and micro-planning,
we introduced the SPACE object to represent a lo-
cation in the virtual space by its symbolic and nu-
meric character. The SPACE object is described in
section 4.
3 Plan-based Anaphora Resolution
3.1 Surface-clue-based Resolution vs.
Plan-based Resolution
Consider the following two dialogue examples.
(1-1) ?Agent X, push the red ball.?
(1-2) ?Move to the front of the blue ball.?
(1-3) ?Push it.?
(2-1) ?Agent X, pick up the red ball.?
(2-2) ?Move to the front of the blue ball.?
(2-3) ?Put it down.?
The second dialogue is different from the first
one only in terms of the verbs in the first and third
utterances. The syntactic structure of each sen-
tence in the second dialogue (2-1)?(2-3) is the
same as the corresponding sentence in the first
dialogue (1-1)?(1-3). However, pronoun ?it? in
(1-3) refers to ?the blue ball? in (1-2), and pro-
noun ?it? in (2-3) refers to ?the red ball? in (2-1).
The difference between these two examples is not
explained by the theories based on surface clues
such as the centering theory (?; ?).
In the setting of SHRDLU-like systems, the
user has a certain goal of arranging objects in the
world, and constructs a plan to achieve it through
interaction with the system. As Cohen pointed
out, users tend to break up the referring and pred-
icating functions in speech dialogue (?). Thus,
each user?s utterance suggests a part of plan rather
than a whole plan that the user tries to perform.
To avoid redundancy, users need to use anaphora.
From these observations, we found that consid-
ering a user?s plan is indispensable in resolving
anaphora in this type of dialogue system and de-
veloped an anaphora resolution algorithm using
the relation between utterances in terms of partial
plans (plan operators) corresponding to them.
The basic idea is to identify a chain of plan op-
erators based on their effects and preconditions.
Our method explained in the rest of this section
finds preceding utterances sharing the same goal
as the current utterance with respect to their cor-
responding plan operators as well as surface lin-
guistic clues.
92
!"#$%&?()**"*+
,-.)*/"#
0"#/"%*)$1
2-*0-$"*+
,1*/)#/"#3
,-.)*/"#&)*)(14"4
5()*("6$)$1
7"$/8)(
9%$(0
://-$)*#-
;"4/%$1
<*/%(%+1=%$00"#/"%*)$1>)*+8)+-.%0-(
=%$0&4-?8-*#- @)4-&A$).- B%)(
C)4"#
.%D-.-*/
!"#$%
&?()*+
@%%$0"*)/-&D)(8-
,?--#;&"*?8/
E*".)/"%*
F"4#%8$4-)*)(14"4
,?)#-$-#%+*"/"%*
!)#$%
?()**"*+
!%D-.-*/
+-*-$)/"%*
,?--#;$-#%+*"/"%*
Figure 2: The system architecture of K3
3.2 Resolution Algorithm
Recognized speech input is transformed into a
case frame. At this stage, anaphora is not re-
solved. Based on this case frame, a plan opera-
tor is retrieved in the plan library. This process
is generally called ?plan recognition.? A plan
operator used in our system is similar to that of
STRIPS (?), which consists of precondition, ef-
fect and action description.
Variables in the retrieved plan operator are
filled with case fillers in the utterance. There
might be missing case fillers when anaphora (zero
pronoun) is used in the utterance. The system
tries to resolve these missing elements in the plan
operator. To resolve the missing elements, the
system again uses clue words and the plan library.
An overview of the anaphora resolution algorithm
is shown in Figure 3.
When the utterance includes clue words, the
system uses them to search the history database
for the preceding utterance that shares the same
goal as the current utterance. Then, it identifies
the referent on the basis of case matching.
There are cases in which the proper preceding
utterance cannot be identified even with the clue
words. These cases are sent to the left branch in
Fig. 3 where the plan library is used to resolve
anaphora.
When there is no clue word or the clue word
does not help to resolve the anaphora, the process
goes through the left branch in Fig. 3. First, the
system enumerates the candidates of referents us-
ing the surface information, then filters them out
with linguistic clues and the plan library. For ex-
ample, demonstratives such as ?this?, ?that? are
usually used for objects that are in the user?s view.
Therefore, the referent of anaphora with demon-
stratives is restricted to the objects in the current
user?s view.
If the effect of a plan operator satisfies the pre-
condition of another plan operator, and the utter-
ances corresponding to these plan operators are
uttered in discourse, they can be considered to
intend the same goal. Thus, identifying a chain
of effect-precondition relations gives important
information for grouping utterances sharing the
same goal. We can assume an anaphor and its
referent appear within the same utterance group.
Once the utterance group is identified, the sys-
tem finds the referent based on matching variables
between plan operators.
After filtering out the candidates, there still
might be more than one candidate left. In such a
case, each candidate is assigned a score that is cal-
culated based on the following factors: saliency,
agent?s view, and user?s view.
4 Handling Spatial Vagueness
To interface the macro- and micro-planning, we
introduced the SPACE object which represents a
location in the virtual world. Because of space
limitations, we briefly explain the SPACE object.
The macro planner uses plan operators de-
scribed in terms of the logical forms. Thus, the
93
Utterance 
includes clue 
word?
Enumerate 
candidates by 
surface information
Identify utterance 
including referent
by clue word
Anaphora
resolved?
Unique
candidate?
Filtering
candidates
Unique
candidate?
Scoring
Referent 
identified
no
yes
no
no
yes
Resolve anaphora
by case matching
no
yes
yes
Figure 3: Anaphora resolution algorithm
SPACE object is designed to behave as a sym-
bolic object in the macro-planning by referring to
its unique identifier. On the other hand, a loca-
tion could be vague and the most plausible place
changes depending on the situation. Therefore, it
should be treated as a certain region rather than a
single point. To fulfill this requirement, we adopt
the idea of the potential model proposed by Ya-
mada et al (?). Vagueness of a location is nat-
urally realized as a potential function embedded
in the SPACE object. The most plausible point is
calculated by using the potential function with the
Steepest Descent Method on request.
Consider the following short conversation be-
tween a human (H) and a virtual agent (A).
H: Do you see a ball in front of the desk?
A: Yes.
H: Put it on the desk.
When the first utterance is given in the situation
shown in Fig. 1, the discourse analysis module
identifies an instance of ?a ball? in the following
steps.
back
left
right
Desk
front
Viewpoint
Ball(1)
(2)
(3)
(4)
Figure 4: Adjustment of axis
(A) space#1 := new inFrontOf(desk#1, viewpoint#1,
MIRROR)
(B) list#1 := space#1.findObjects()
(C) ball#1 := list#1.getFirstMatch(kindOf(BALL))
In step (A), an instance of SPACE is created as
an instance of the class inFrontOf. The construc-
tor of inFrontOf takes three arguments: the ref-
erence object, the viewpoint, and the axis order.
Although it is necessary to identify the reference
frame, we focus on the calculation of potential
functions given a reference frame.
Suppose the parameters of inFrontOf have been
resolved in the preceding steps, and the discourse
analysis module chooses the axis mirror order and
the orientation of the axis based on the viewpoint
of the light-colored arrows in Fig. 4. The closest
arrow to the viewpoint-based ?front? axis ((1) in
Fig. 4) is chosen as the ?front? of the desk. Then,
the parameters of potential function correspond-
ing to ?front? are set.
In step (B), the method matchObjects() returns
a list of objects located in the potential field of
space#1 shown in Fig. 5. The objects in the list are
sorted in descending order of the potential value
of their location.
In step (C), the most plausible object satisfy-
ing the type constraint (BALL) is selected by the
method getFirstMatch().
When receiving the next utterance, ?Put it on
the desk.?, the discourse analysis module resolves
the referent of the pronoun ?it? and extracts the
user?s goal.
walk(inFrontOf(ball#1, viewpoint#1, MIRROR)
AND reachableByHand(ball#1)
AND NOT(occupied(ball#1)))
The movement walk takes a SPACE object rep-
resenting its destination as an argument. In this
example, the conjunction of three SPACE objects
94
Viewpoint
Figure 5: Potential field of space#1
is given as the argument. The potential function
of the resultant SPACE is calculated by multiply-
ing the values of the corresponding three potential
functions at each point.
As this example illustrates, the SPACE object
effectively plays a role as a mediator between the
macro and micro planning.
5 Conclusions and Future Work
We have introduced our prototype systemK3 , two
distinctive features of which are described in this
paper. Plan-based anaphora resolution enables
K3 to interpret the user?s intention more pre-
cisely than the previous, surface-cue-based reso-
lution algorithms. The SPACE object is designed
to bridge the gap between the symbolic system
(language processing) and the continuous system
(animation generation) . In what follows, we de-
scribe the research agenda of our project.
One-to-many Conversation. Conversational
agent systems should deal with one-to-many con-
versations as well as one-to-one conversations.
In a one-to-many conversation, it is not easy to
decide who is the intended listener. The situation
gets worse when a speaker is concerned with
only performing an action without caring who
does it. In such cases, agents have to request
clarifications or negotiate among themselves.
Agent Coordination. In one-to-many conver-
sations, agents must coordinate each other. Some
sorts of coordination are explicitly requested by
user, e.g., ?Agent A and B tidy up the table,
please.? But other kinds of coordination are im-
plicitly requested, e.g., ?Agent A hands agent B
the box, please.? In this case, the speaker asks
agent B nothing explicitly. However, agent B
must react to the request for agent A and coor-
dinate with agent A to receive a box.
Parallel Actions. Most intelligent agent sys-
tems perform only one action at a time. Yet, if we
want to make systems become more flexible, we
must enable them to handle more than one action
at a time. Hence, they must speak while walking,
wave while nodding, and so on.
Memory System. A history database is not
enough to serve realistic dialogue in the domain
of K3 . In such a domain, people often mention a
previous state, e.g., ?Put the ball back to the place
where it was.? To comply such a request, agents
must have a human-like memory system.
Interruption Handling. Agents sometimes
misunderstand requests and perform not intended
actions. In case of human conversations, a
speaker usually interrupts hearer and try to repair
misunderstanding. Conversational agents also
should be able to accept such interruptions.
Interruption handling is also essential to request
a next action before agents finish actions.
Acknowledgment
This work is partially supported by a Grant-in-
Aid for Creative Scientific Research 13NP0301,
the Ministry of Education, Culture, Sports, Sci-
ence and Technology of Japan. The URL of
the project is http://www.cl.cs.titech.ac.
jp/sinpro/en/index.html.
References
P. R. Cohen. 1984. The pragmatics of referring and
the modality of communication. Computational
Linguistics, 10(2):97?146.
R. E. Fikes. 1971. STRIPS: A new approach to the
application of theorem problem solving. Artificial
Intelligence, 2:189?208.
B. J. Grosz, A. K. Joshi, and P. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
H. Tanaka, T. Tokunaga, and Y. Shinyama. 2004. An-
imated agents capable of understanding natural lan-
guage and performing actions. In Life-Like Char-
acters, pages 429?444. Springer.
95
M. A. Walker, A. K. Joshi, and E. F. Prince, editors.
1998. Centering Theory in Discourse. Clarendon
Press Oxford.
T. Winograd. 1972. Understanding Natural Lan-
guage. Academic Press.
A. Yamada, T. Nishida, and S. Doshita. 1988. Fig-
uring out most plausible interpretation from spa-
tial description. In the 12th International Con-
ference on Computational Linguistics (COLING),
pages 764?769.
96
Proceedings of the Fourth International Natural Language Generation Conference, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Group-based Generation of Referring Expressions
Funakoshi Kotaro ? Watanabe Satoru ?
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro O?okayama 2-12-1, 152-8552, Japan
take@cl.cs.titech.ac.jp
Tokunaga Takenobu
Abstract
Past work of generating referring expres-
sions mainly utilized attributes of objects
and binary relations between objects in or-
der to distinguish the target object from
others. However, such an approach does
not work well when there is no distinc-
tive attribute among objects. To over-
come this limitation, this paper proposes
a novel generation method utilizing per-
ceptual groups of objects and n-ary re-
lations among them. The evaluation us-
ing 18 subjects showed that the proposed
method could effectively generate proper
referring expressions.
1 Introduction
In the last two decades, many researchers have
studied the generation of referring expressions to
enable computers to communicate with humans
about objects in the world.
In order to refer to an intended object (the tar-
get) among others (distractors), most past work
(Appelt, 1985; Dale and Haddock, 1991; Dale,
1992; Dale and Reiter, 1995; Heeman and Hirst,
1995; Horacek, 1997; Krahmer and Theune, 2002;
van Deemter, 2002; Krahmer et al, 2003) utilized
attributes of the target and binary relations be-
tween the target and distractors. Therefore, these
methods cannot generate proper referring expres-
sions in situations where there is no significant
surface difference between the target and distrac-
tors, and no binary relation is useful to distinguish
the target. Here, a proper referring expression
?Currently at Honda Research Institute Japan Co., Ltd.
?Currently at Hitachi, Ltd.
means a concise and natural linguistic expression
enabling hearers to identify the target.
For example, consider indicating object b to per-
son P in the situation of Figure 1. Note that la-
bels a, b and c are assigned for explanation to the
readers, and person P does not share these labels
with the speaker. Because object b is not distin-
guishable from objects a or c by means of their
appearance, one would try to use a binary relation
between object b and the table, i.e., ?a ball to the
right of the table?. However, ?to the right of? is
not a discriminatory relation, for objects a and c
are also located to the right of the table. Using a
and c as a reference object instead of the table does
not make sense, since a and c cannot be uniquely
identified because of the same reason that b cannot
be identified. Such situations have drawn less at-
tention (Stone, 2000), but can frequently occur in
some domains such as object arrangement (Tanaka
et al, 2004).
P
a
b
c
Table
Figure 1: An example of problematic situations
In the situation of Figure 1, the speaker can indi-
cate object b to person P with a simple expression
?the front ball?. In order to generate such an ex-
pression, one must be able to recognize the salient
perceptual group of the objects and use the n-ary
relative relations in the group.
73
To overcome the problem described above, Fu-
nakoshi et al (2004) proposed a method of gen-
erating Japanese referring expressions that utilizes
n-ary relations among members of a group. They,
however, dealt with the limited situations where
only homogeneous objects are randomly arranged
(see Figure 2). Thus, their method could han-
dle only spatial n-ary relation, and could not han-
dle attributes and binary relations between objects
which have been the main concern of the past re-
search.
In this paper, we extend the generation method
proposed by (Funakoshi et al, 2004) so as to han-
dle object attributes and binary relations between
objects as well. In what follows, Section 2 shows
an extension of the SOG representation that was
proposed in (Funakoshi et al, 2004). Our new
method will be described in Section 3 and eval-
uated in Section 4. Finally we conclude the paper
in Section 5.
2 SOG representation
Funakoshi et al (2004) proposed an intermedi-
ate representation between a referring expression
and the situation that is referred to by the expres-
sion. The intermediate representation represents
a course of narrowing down to the target as a se-
quence of groups from the group of all objects to
the singleton group of the target object. Thus it is
called SOG (Sequence Of Groups).
The following example shows an expression de-
scribing the target x in Figure 2 with the cor-
responding SOG representation below it. Since
Japanese is a head-final language, the order of
groups in the SOG representation can be retained
in the linguistic expression.
hidari oku ni aru
(1)
mittu no tama no uti no
(2)
itiban migi no tama
(3)
(the rightmost ball
(3)
among the three balls
(2)
at the back left
(1)
)
SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}],
where {a, b, c, d, e, f, x} denotes all objects in
the situation, {a, b, x} denotes the three objects
at the back left, and {x} denotes the target.
2.1 Extended SOG
As mentioned above, (Funakoshi et al, 2004) sup-
posed the limited situations where only homoge-
neous objects are randomly arranged, and consid-
ered only spatial subsumption relations between
consecutive groups. Therefore, relations between
P
a
b
e
f
c d
x
Figure 2: An example from (Funakoshi et al,
2004)
groups are not explicitly denoted in the original
SOGs as shown below.
SOG: [G
0
, G
1
, . . . , G
n
]
G
i
: a group
In this paper, however, other types of relations
between groups are also considered. We propose
an extended SOG representation where types of
relations are explicitly denoted as shown below. In
the rest of this paper, we will refer to this extended
SOG representation by simply saying ?SOG?.
SOG: [G
0
R
0
G
1
R
1
. . . G
i
R
i
. . . G
n
]
G
i
: a group
R
i
: a relation between G
i
and G
i+1
2.2 Relations between groups
R
i
, a relation between groups G
i
and G
i+1
, de-
notes a shift of attention from G
i
to G
i+1
with
a certain focused feature. The feature can be an
attribute of objects or a relation between objects.
There are two types of relations between groups:
intra-group relation and inter-group relation.
Intra-group relation When R
i
is an intra-group
relation, G
i
subsumes G
i+1
, that is, G
i
? G
i+1
.
Intra-group relations are further classified into the
following subcategories according to the feature
used to narrow down G
i
to G
i+1
. We denote these
subcategories with the following symbols.
space
?? : spatial subsumption
type
?? : the object type
shape
?? : the shape of objects
color
?? : the color of objects
size
?? : the size of objects
With respect to this classification, (Funakoshi et
al., 2004) dealt with only the
space
?? relation.
74
Inter-group relation When R
i
is an inter-group
relation, G
i
and G
i+1
are mutually exclusive, that
is, G
i
? G
i+1
= ?. An inter-group relation is a
spatial relation and denoted by symbol ?.
Example R
i
can be one of
space
?? ,
type
??,
shape
?? ,
color
??,
size
?? and ?. We show a referring expres-
sion indicating object b1 and the corresponding
SOG in the situation of Figure 3. In the SOG,
{all} denotes the total set of objects in the situ-
ation. The indexed underlines denote correspon-
dence between SOG and linguistic expressions.
As shown in the figure, we allow objects being on
the other objects.
marui
(1)
futatu no tukue no uti no
(2)
hidari no
(3)
tukue no
(4)
ue no
(5)
tama
(6)
(the ball
(6)
on
(5)
the left
(3)
table
(4)
among the two
(2)
round
(1)
tables
(2)
)
SOG: [{all}
type
?? {t1, t2, t3}
shape
??
(1)
{t1, t2}
(2)
space
??
(3)
{t1}
(4)
?
(5)
{b1}
(6)
]
b2
b1
b5
t3
t2
p1
t1
b3
b4
blue
black
red
Figure 3: An example situation
3 Generation
Our generation algorithm proposed in this section
consists of four steps: perceptual grouping, SOG
generation, surface realization and scoring. In the
rest of this section, we describe these four steps by
using Figure 3 as an example.
3.1 Step 1: Perceptual grouping
Our algorithm starts with identifying groups of
objects that are naturally recognized by humans.
We adopt Tho?risson?s perceptual grouping algo-
rithm (Tho?risson, 1994) for this purpose. Per-
ceptual grouping is performed with objects in the
situation with respect to each of the following
features: type, shape, color, size, and proxim-
ity. Three special features, total, singleton, and
closure are respectively used to recognize the to-
tal set of objects, groups containing each single
object, and objects bounded in perceptually sig-
nificant regions (table tops in the domain of this
paper). These three features are handled not by
Tho`risson?s algorithm but by individual proce-
dures.
Type is the most dominant feature because hu-
mans rarely recognize objects of different types as
a group. Thus, first we group objects with respect
to types, and then group objects of the same type
with respect to other features (except for total).
Although we adopt Tho?risson?s grouping algo-
rithm, we use different grouping strategies from
the original. Tho?risson (1994) lists the following
three combinations of features as possible strate-
gies of perceptual grouping.
? shape and proximity
? color and proximity
? size and proximity
However, these strategies are inappropriate to gen-
erate referring expressions. For example, because
two blue balls b1 and b2 in Figure 3 are too
much distant from each other, Tho?risson?s algo-
rithm cannot recognize the group consisting of b1
and b2 with the original strategies. However, the
expression like ?the left blue ball? can naturally
refer to b1. When using such an expression, we
assume an implicit group consisting of b1 and b2.
Hence, we do not combine features but use them
separately.
The results of perceptual grouping of the situa-
tion in Figure 3 are shown below. Relation labels
are assigned to recognized groups with respect to
features used in perceptual grouping. We define
six labels: all, type, shape, color, size, and
space. Features singleton, proximity and closure
share the same label space. A group may have
several labels.
feature label recognized groups
total all {t1, t2, t3, p1, b1, b2, b3, b4, b5}
singleton space {t1}, {t2}, {t3}, {p1}, {b1}, {b2},
{b3}, {b4}, {b5}
type type {t1, t2, t3}, {p1}, {b1, b2, b3, b4, b5}
shape shape {t1, t2}, {t3}
color color {b1, b2}, {b3}, {b4, b5}
size size {b1, b3, b4}, {b2, b5}
proximity space {t2, t3}, {b1, b3, b4, b5}, {b3, b4, b5}
closure space {b1}, {b3, b4}
75
Target # target object
AllGroups # all generated groups
SOGList # list of generated SOGs
01:makeSOG()
02: SOG = []; # list of groups and symbols
03: All = getAll(); # total set
04: add(All, SOG); # add All to SOG
05: TypeList = getAllTypes(All);
# list of all object types
06: TargetType = getType(Target);
# type of the target
07: TargetSailency = saliency(TargetType);
# saliency of the target type
08: for each Type in TypeList do
# {Table, Plant, Ball}
09: if saliency(Type) ?
TargetSaliency then
# saliency: Table > Plant > Ball
10: Group = getTypeGroup(Type);
# get the type group of Type
11: extend(SOG, Group);
12: end if
13: end for
14:return
Figure 4: Function makeSOG
3.2 Step 2: SOG generation
The next step is generating SOGs. This is so-
called content planning in natural language gen-
eration. Figure 4, Figure 5 and Figure 6 show the
algorithm of making SOGs.
Three variables Target, AllGroups, and
SOGList defined in Figure 4 are global variables.
Target holds the target object which the refer-
ring expression refers to. AllGroups holds the
set of all groups recognized in Step 1. Given
Target and AllGroups, function makeSOG
enumerates possible SOGs in the depth-first man-
ner, and stores them in SOGList.
makeSOG (Figure 4) makeSOG starts with a list
(SOG) that contains the total set of objects in the
domain. It chooses groups of objects that are more
salient than or equal to the target object and calls
function extend for each of the groups.
extend (Figure 5) Given an SOG and a group
to be added to the SOG, function extend extends
the SOG with the group for each label attached to
the group. This extension is done by creating a
copy of the given SOG and adding to its end an
intra-group relation symbol defined in Section 2.2
corresponding to the given label and group. Fi-
nally it calls search with the copy.
search (Figure 6) This function takes an SOG
as its argument. According to the last group in
01:extend(SOG, Group)
02: Labels = getLabels(Group);
03: for each Label in Labels do
04: SOGcopy = copy(SOG);
05: add(
Label
??, SOGcopy);
06: add(Group, SOGcopy);
07: search(SOGcopy);
08: end for
09:return
Figure 5: Function extend
the SOG (LastGroup), it extends the SOG as
described below.
1. If LastGroup is a singleton of the target
object, append SOG to SOGList and return.
2. If LastGroup is a singleton of a non-target
object, find the groups that contain the target
object and satisfy the following three condi-
tions: (a), (b) and (c).
(a) All objects in the group locate in
the same direction from the object of
LastGroup (the reference). Possi-
ble directions are one of ?back?, ?back
right?, ?right?, ?front right?, ?front?,
?front left?, ?left?, ?left back? and ?on?.
The direction is determined on the basis
of coordinate values of the objects, and
is assigned to the group for the use of
surface realization.
(b) There is no same type object located be-
tween the group and the reference.
(c) The group is not a total set of a certain
type of object.
Then, for each of the groups, make a copy
of the SOG, and concatenate ??? and the
group to the copy, and call search recur-
sively with the new SOG.
3. If LastGroup contains the target object
together with other objects, let the inter-
section of LastGroup and each group in
AllGroups be NewG, and copy the label
from each group to NewG. If NewG contains
the target object, call function extend un-
less Checked contains NewG.
4. If LastGroup contains only non-target ob-
jects, call function extend for each group
(Group) in AllGroupswhich is subsumed
by LastGroup.
Figure 7 shows the SOGs generated to refer to
object b1 in Figure 3.
76
1. [{all}
type
?? {t1, t2, t3}
space
?? {t1} ?{b1}]
2. [{all}
type
?? {t1, t2, t3}
shape
?? {t1, t2}
space
?? {t1} ?{b1}]
3. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1}]
4. [{all}
type
?? {b1, b2, b3, b4, b5}
color
?? {b1, b2}
space
?? {b1}]
5. [{all}
type
?? {b1, b2, b3, b4, b5}
color
?? {b1, b2}
size
?? {b1}]
6. [{all}
type
?? {b1, b2, b3, b4, b5}
size
?? {b1, b4, b3}
space
?? {b1}]
7. [{all}
type
?? {b1, b2, b3, b4, b5}
size
?? {b1, b4, b3}
color
?? {b1}]
8. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
space
?? {b1}]
9. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
color
?? {b1}]
10. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
size
?? {b1, b3, b4}
space
?? {b1}]
11. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
size
?? {b1, b3, b4}
color
?? {b1}]
Figure 7: Generated SOGs from the situation in Figure 3
01:search(SOG)
02: LastGroup = getLastElement(SOG);
# get the rightmost group in SOG
03: Card = getCardinality(LastGroup);
04: if Card == 1 then
05: if containsTarget(LastGroup) then
# check if LastGroup contains
# the target
06: add(SOG, SOGList);
07: else
08: GroupList =
searchTargetGroups(LastGroup);
# find groups containing the target
09: for each Group in GroupList do
10: SOGcopy = copy(SOG);
11: add(?, SOGcopy);
12: add(Group, SOGcopy);
13: search(SOGcopy);
14: end for
15: end if
16: elsif containsTarget(LastGroup) then
17: Checked = [ ];
18: for each Group in AllGroups do
19: NewG = Intersect(Group, LastGroup);
# make intersection
20: Labels = getLabels(Group);
21: setLabels(Labels, NewG);
# copy labels from Group to NewG
22: if containsTarget(NewG) &
!contains(Checked, NewG) then
23: add(NewG, Checked);
24: extend(SOG, Group);
25: end if
26: end for
27: else
28: for each Group of AllGroups do
29: if contains(LastGroup, Group) then
30: extend(SOG, Group);
31: end if
32: end for
33: end if
34:return
Figure 6: Function search
3.3 Step 3: Surface realization
A referring expression is generated by determin-
istically assigning a linguistic expression to each
element in an SOG according to Rule 1 and 2.
As Japanese is a head-final language, simple con-
catenation of element expressions makes a well-
formed noun phrase1. Rule 1 generates expres-
sions for groups and Rule 2 does for relations.
Each rule consists of several subrules which are
applied in this order.
[Rule 1]: Realization of groups
Rule 1.1 The total set ({all}) is not realized.
(Funakoshi et al, 2004) collected referring
expressions from human subjects through ex-
periments and found that humans rarely men-
tioned the total set. According to their obser-
vation, we do not realize the total set.
Rule 1.2 Realize the type name for a singleton.
Type is realized as a noun and only for a sin-
gleton because the type feature is used first to
narrow down the group, and the succeeding
groups consist of the same type objects until
reaching the singleton. When the singleton is
not the last element of SOG, particle ?no? is
added.
Rule 1.3 The total set of the same type objects is
not realized.
This is because the same reason as Rule 1.1.
Rule 1.4 The group followed by the relation
space
??
is realized as ?[cardinality] [type] no-uti
(among)?, e.g., ?futatu-no (two) tukue (desk)
no-uti (among)?. The group followed by
1Although different languages require different surface
realization rules, we presume perceptual grouping and SOG
generation (Step 1 and 2) are applicable to other languages as
well.
77
the relation ? is realized as ?[cardinality]
[type] no?.
When consecutive groups are connected by
other than spatial relations (
space
?? and ?),
they can be realized as a sequence of relations
ahead of the noun (type name). For example,
expression ?the red ball among big balls? can
be simplified to ?the big red ball?.
Rule 1.5 Other groups are not realized.
[Rule 2]: Realization of relations
Rule 2.1 Relation
type
?? is not realized.
See Rule 1.2.
Rule 2.2 Relations
shape
?? ,
color
?? and
size
?? are real-
ized as the expressions corresponding to their
attribute values. Spatial relations (
space
?? and
?) are realized as follows, where |G
i
| de-
notes the cardinality of G
i
.
Intra-group relation (G
i
space
?? G
i+1
)
If |G
i
| = 2 (i.e., |G
i+1
| = 1), based on the
geometric relations among objects, generate
one of four directional expressions ?{migi,
hidari, temae, oku} no ({right, left, front,
back})?.
If |G
i
| ? 3 and |G
i+1
| = 1, based on the
geometric relations among objects, generate
one of eight directional expressions ?itiban
{migi, hidari, temae, oku, migi temae, hi-
dari temae, migi oku, hidari oku} no ({right,
left, front, back, front right, front left, back
right, back left}-most)? if applicable. If none
of these expressions is applicable, generate
expression ?mannaka no (middle)? if appli-
cable. Otherwise, generate one of four ex-
pressions ?{hidari, migi, temae, oku} kara
j-banme no (j-th from {left, right, front,
back})?.
If |G
i+1
| ? 2, based on the geometric rela-
tions among objects, generate one of eight di-
rectional expressions ?{migi, hidari, temae,
oku, migi temae, hidari temae, migi oku, hi-
dari oku} no ({right, left, front, back, front
right, front left, back right, back left})?.
Inter-group relation (G
i
?G
i+1
)
|G
i
| = 1 should hold because of search
in Step 2. According to the direction as-
signed by search, generate one of nine ex-
pressions : ?{migi, hidari, temae, oku, migi
temae, hidari temaen, migi oku, hidari oku,
ue} no ({right, left, front, back, front right,
front left, back right, back left, on})?.
Figure 8 shows the expressions generated from
the first three SOGs shown in Figure 7. The num-
bers in the parentheses denote coindexes of frag-
ments between the SOGs and the realized expres-
sions.
3.4 Step 4: Scoring
We assign a score to each expression by taking into
account the relations used in the expression, and
the length of the expression.
First we assign a cost ranging over [0, 1] to each
relation in the given SOG. Costs of relations are
decided as below. These costs conform to the pri-
orities of features described in (Dale and Reiter,
1995).
type
?? : No cost (to be neglected)
shape
?? : 0.2
color
?? : 0.4
size
?? : big(est): 0.6, small(est): 0.8, middle: 1.0
space
?? ,? : Cost functions are defined according to the
potential functions proposed in (Tokunaga
et al, 2005). The cost for relation ?on? is
fixed to 0.
Then, the average cost of the relations is calcu-
lated to obtain the relation cost, C
rel
. The cost of
surface length (C
len
) is calculated by
C
len
=
length(expression)
max
i
length(expression
i
)
,
where the length of an expression is measured by
the number of characters.
Using these costs, the score of an expression is
calculated by
score =
1
?? C
rel
+ (1? ?)? C
len
.
? was set to 0.5 in the following experiments.
4 Evaluation
4.1 Experiments
We conducted two experiments to evaluate expres-
sions generated by the proposed method.
Both experiments used the same 18 subjects and
the same 20 object arrangements which were gen-
erated automatically. For each arrangement, all
factors (number of objects, positions of objects, at-
tributes of objects, and the target object) were ran-
domly decided in advance to conform to the fol-
lowing conditions: (1) the proposed method can
generate more than five expressions for the given
target and (2) more than two other objects exist
which are the same type as the target.
78
1. SOG: [{all}
type
?? {t1, t2, t3}
space
??
(1)
{t1}
(2)
?
(3)
{b1}
(4)
]
itiban hidari no
(1)
tukue no
(2)
ue no
(3)
tama
(4)
(the ball
(4)
on
(3)
the leftmost
(1)
table
(2)
)
2. SOG: [{all}
type
?? {t1, t2, t3}
shape
??
(1)
{t1, t2}
(2)
space
??
(3)
{t1}
(4)
?
(5)
{b1}
(6)
]
marui
(1)
futatu no tukue no uti
(2)
hidari no
(3)
tukue no
(4)
ue no
(5)
tama
(6)
(the ball
(6)
on
(5)
the left
(3)
table
(4)
among
(2)
the round
(1)
two tables
(2)
)
3. SOG: [{all}
type
?? {b1, b2, b3, b4, b5}
space
??
(1)
{b1}
(2)
]
itiban hidari no
(1)
tama
(2)
(the leftmost
(1)
ball
(2)
)
Figure 8: Realized expressions
?20?
20/20
????????
t1
b3
b1
b4
t2
p1b2
Figure 9: An example stimulus of Experiment 1
Experiment 1 Experiment 1 was designed to
evaluate the ability of expressions to identify the
targets. The subjects were presented an arrange-
ment with a generated referring expression which
gained the highest score at a time, and were in-
structed to choose the object referred to by the ex-
pression. Figure 9 is an example of visual stimuli
used in Experiment 1. Each subject responded to
all 20 arrangements.
Experiment 2 Experiment 2 was designed to
evaluate validity of the scoring function described
in Section 3.4. The subjects were presented an
arrangement with a marked target together with
the best five generated expressions referring to the
target at a time. Then the subjects were asked
to choose the best one from the five expressions.
Figure 10 is an example of visual stimuli used in
Experiment 2. Each subject responded to the all
20 arrangements. The expressions used in Experi-
ment 2 include those used in Experiment 1.
4.2 Results
Table 1 shows the results of Experiment 1. The
average accuracy of target identification is 95%.
Figure 10: An example stimulus of Experiment 2
This shows a good performance of the generation
algorithm proposed in this paper.
The expression generated for arrangement
No. 20 (shown in Figure 9) resulted in the excep-
tionally poor accuracy. To refer to object b1, our
algorithm generated expression ?itiban temae no
tama (the most front ball)? because b1 is the most
close object to person P in terms of the vertical
axis. Humans, however, chose the object that is the
closest to P in terms of Euclidean distance. Some
psychological investigation is necessary to build
a more precise geometrical calculation model to
solve this problem (Landragin et al, 2001).
Table 2 shows the results of Experiment 2. The
first row shows the rank of expressions based on
their score. The second row shows the count of hu-
man votes for the expression. The third row shows
the ratio of the votes. The top two expressions oc-
cupy 72% of the total. This concludes that our
scoring function works well.
5 Conclusion
This paper extended the SOG representation pro-
posed in (Funakoshi et al, 2004) to generate refer-
79
Table 1: Result of Experiment 1
Arrangement No. 1 2 3 4 5 6 7 8 9 10
Accuracy 0.89 1.0 1.0 1.0 1.0 1.0 1.0 0.94 1.0 1.0
11 12 13 14 15 16 17 18 19 20 Ave.
1.0 0.94 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.17 0.95
Table 2: Result of Experiment 2
Rank 1 2 3 4 5 Total
Vote 134 125 59 22 20 360
Share 0.37 0.35 0.16 0.06 0.06 1
ring expressions in more general situations.
The proposed method was implemented and
evaluated through two psychological experiments
using 18 subjects. The experiments showed that
generated expressions had enough discrimination
ability and that the scoring function conforms to
human preference well.
The proposed method would be able to handle
other attributes and relations as far as they can be
represented in terms of features as described in
section 3. Corresponding surface realization rules
might be added in that case.
In the implementation, we introduced rather ad
hoc parameters, particularly in the scoring func-
tion. Although this worked well in our experi-
ments, further psychological validation is indis-
pensable.
This paper assumed a fixed reference frame is
shared by all participants in a situation. How-
ever, when we apply our method to conversational
agent systems, e.g., (Tanaka et al, 2004), refer-
ence frames change dynamically and they must
be properly determined each time when generat-
ing referring expressions.
In this paper, we focused on two dimensional
situations. To apply our method to three dimen-
sional worlds, more investigation on human per-
ception of spatial relations are required. We ac-
knowledge that a simple application of the current
method does not work well enough in three dimen-
sional worlds.
References
Douglas E. Appelt. 1985. Planning English referring expres-
sions. Artificial Intelligence, 26:1?33.
Robert Dale and Nicholas Haddock. 1991. Generating re-
ferring expressions involving relations. In Proceedings of
the Fifth Conference of the European Chapter of the As-
sociation for Computational Linguistics(EACL?91), pages
161?166.
Robert Dale and Ehud Reiter. 1995. Computational interpre-
tations of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233?263.
Robert Dale. 1992. Generating referring expressions: Con-
structing descriptions in a domain of objects and pro-
cesses. MIT Press, Cambridge.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama, and
Takenobu Tokunaga. 2004. Generating referring expres-
sions using perceptual groups. In Proceedings of the 3rd
International Conference on Natural Language Genera-
tion: INLG04, pages 51?60.
Peter Heeman and Graeme Hirst. 1995. Collaborating refer-
ring expressions. Computational Linguistics, 21(3):351?
382.
Helmut Horacek. 1997. An algorithm for generating refer-
ential descriptions with flexible interfaces. In Proceedings
of the 35th Annual Meeting of the Association for Compu-
tational Linguistics, pages 206?213.
Emiel Krahmer and Marie?t Theune. 2002. Efficient context-
sensitive generation of descriptions. In Kees van Deemter
and Rodger Kibble, editors, Information Sharing: Given-
ness and Newness in Language Processing. CSLI Publica-
tions, Stanford, California.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg. 2003.
Graph-based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
Fre?de?ric Landragin, Nadia Bellalem, and Laurent Romary.
2001. Visual salience and perceptual grouping in mul-
timodal interactivity. In Proceedings of International
Workshop on Information Presentation and Natural Mul-
timodal Dialogue (IPNMD), pages 151?155.
Matthew Stone. 2000. On identifying sets. In Proceedings
of the 1st International Conference on Natural Language
Generation: INLG00, pages 116?123.
Hozumi Tanaka, Takenobu Tokunaga, and Yusuke Shinyama.
2004. Animated agents capable of understanding natural
language and performing actions. In Helmut Prendinger
and Mituru Ishizuka, editors, Life-Like Characters, pages
429?444. Springer.
Kristinn R. Tho?risson. 1994. Simulated perceptual grouping:
An application to human-computer interaction. In Pro-
ceedings of the Sixteenth Annual Conference of the Cog-
nitive Science Society, pages 876?881.
Takenobu Tokunaga, Tomofumi Koyama, and Suguru Saito.
2005. Meaning of Japanese spatial nouns. In Proceedings
of the Second ACL-SIGSEM Workshop on the Linguistic
Dimentions of Prepositions and their Use in Computa-
tional Linguistics: Formalisms and Applications, pages
93?100.
Kees van Deemter. 2002. Generating referring expressions:
Boolean extensions of the incremental algorithm. Compu-
tational Linguistics, 28(1):37?52.
80
Coling 2010: Poster Volume, pages 579?587,
Beijing, August 2010
Automatic Allocation of Training Data for Rapid Prototyping
of Speech Understanding based on Multiple Model Combination
Kazunori Komatani? Masaki Katsumaru? Mikio Nakano?
Kotaro Funakoshi? Tetsuya Ogata? Hiroshi G. Okuno?
? Graduate School of Informatics, Kyoto University
{komatani,katumaru,ogata,okuno}@kuis.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd.
{nakano,funakoshi}@jp.honda-ri.com
Abstract
The optimal choice of speech understand-
ing method depends on the amount of
training data available in rapid prototyp-
ing. A statistical method is ultimately
chosen, but it is not clear at which point
in the increase in training data a statisti-
cal method become effective. Our frame-
work combines multiple automatic speech
recognition (ASR) and language under-
standing (LU) modules to provide a set
of speech understanding results and se-
lects the best result among them. The
issue is how to allocate training data to
statistical modules and the selection mod-
ule in order to avoid overfitting in training
and obtain better performance. This paper
presents an automatic training data alloca-
tion method that is based on the change
in the coefficients of the logistic regres-
sion functions used in the selection mod-
ule. Experimental evaluation showed that
our allocation method outperformed base-
line methods that use a single ASR mod-
ule and a single LU module at every point
while training data increase.
1 Introduction
Speech understanding in spoken dialogue systems
is the process of extracting a semantic represen-
tation from a user?s speech. That is, it consists
of automatic speech recognition (ASR) and lan-
guage understanding (LU). Because vocabularies
and language expressions depend on individual
systems, it needs to be constructed for each sys-
tem, and accordingly, training data are required
for each. To collect more real training data, which
will lead to higher performance, it is more desir-
able to use a prototype system than that based on
the Wizard-of-Oz (WoZ) method where real ASR
errors cannot be observed, and to use a more ac-
curate speech understanding module. That is, in
the bootstrapping phase, spoken dialogue systems
need to operate before sufficient real data have
been collected.
We have been addressing the issue of rapid pro-
totyping on the basis of the ?Multiple Language
model for ASR and Multiple language Under-
standing (MLMU)? framework (Katsumaru et al,
2009). In MLMU, the most reliable speech un-
derstanding result is selected from candidates pro-
duced by various combinations of multiple ASR
and LU modules using hand-crafted grammar and
statistical models. A grammar-based method is
still effective at an early stage of system devel-
opment because it does not require training data;
Schapire et al (2005) also incorporated human-
crafted prior knowledge into their boosting al-
gorithm. By combining multiple understanding
modules, complementary results can be obtained
by different kinds of ASR and LU modules.
We propose a novel method to allocate avail-
able training data to statistical modules when the
amount of training data increases. The training
data need to be allocated adaptively because there
are several modules to be trained, and they would
cause overfitting without data allocation. There
are speech understanding modules that have lan-
guage models (LMs) for ASR and LU models
579
(LUMs), and a selection module that selects the
most reliable speech understanding result from
multiple candidates in the MLMU framework.
When the amount of available training data is
small, and an LUM and the selection module are
trained on the same data set, they are trained un-
der a closed-set condition, and thus the training
data for the selection module include too many
correct understanding results. In such cases, the
data need to be divided into subdata sets to avoid
overfitting. On the other hand, when the amount
of available training data is large, so that overfit-
ting does not occur, all available data should be
used to train each statistical module to prepare as
much training data as possible.
We therefore develop a method for switching
data allocation policies. More specifically, two
points are automatically determined at which sta-
tistical modules with more parameters start to be
trained. As a result, better overall performance
is achieved at every point while the amount of
training data increases, compared with all combi-
nations of a single ASR module and a single LU
module.
2 Related Work
It is important to consider the amount of available
training data when designing a speech understand-
ing module. Many statistical LU methods have
been studied, e.g., (Wang and Acero, 2006; Jeong
and Lee, 2006; Raymond and Riccardi, 2007;
Hahn et al, 2008; Dinarelli et al, 2009). They
generally outperform grammar-based LU meth-
ods when a sufficient amount of training data is
available; but sufficient training data are not nec-
essarily available during rapid prototyping. Sev-
eral LU methods were constructed using a small
amount of training data (Fukubayashi et al, 2008;
Dinarelli et al, 2009). Fukubayashi et al (2008)
constructed an LU method based on the weighted
finite state transducer (WFST), in which filler
transitions accepting arbitrary inputs and transi-
tion weights were added to a hand-crafted FST.
This method is placed between a grammar-based
method and a statistical method because a sta-
tistically selected weighting scheme is applied
to a hand-crafted grammar model. Therefore,
the amount of training data can be smaller com-
pared with general statistical LU methods, but this
method does not outperform them when plenty of
training data are available. Dinarelli et al (2009)
used a generative model for which overfitting is
less prone to occur than discriminative models
when the amount of training data is small, but
they did not use a grammar-based model, which is
expected to achieve reasonable performance even
when the amount of training data is very small.
Raymond et al (2007) compared the perfor-
mances of statistical LU methods for various
amounts of training data. They used a statis-
tical finite-state transducer (SFST) as a genera-
tive model and a support vector machine (SVM)
and conditional random fields (CRF) as discrim-
inative models. The generative model was more
effective when the amount of data was small,
and the discriminative models were more effec-
tive when it was large. This shows that the perfor-
mance of an LUmethod depends on the amount of
training data available, and therefore, LU meth-
ods need to be switched automatically. Wang et
al. (2002) developed a two-stage speech under-
standing method by applying statistical methods
first and then grammatical rules. They also ex-
amined the performance of the statistical methods
at their first stage for various amounts of train-
ing data and confirmed that the performance is not
very high when a small amount of data is used.
Schapire et al (2005) showed that accuracy
of call classification in spoken dialogue systems
improved by incorporating hand-crafted prior
knowledge into their boosting algorithm. Their
idea is the same as ours in that they improve the
system?s performance by using hand-crafted hu-
man knowledge while only a small amount of
training data is available. We furthermore solve
the data allocation problem because there are mul-
tiple statistical models to be trained in speech
understanding, while their call classification has
only one statistical model.
3 MLMU Framework
MLMU is the framework for selecting the most
reliable speech understanding result from multi-
ple speech understanding modules (Katsumaru et
al., 2009). In this paper, we furthermore adapt the
selection module to the amount of available train-
580
LU model
#1
Language
model #1
LU
modules
ASR
modules
Result:
1
CM
N
CM
MN
CM
?
i
i
CMmaxarg
N
1
Selection module
LU
results
ASR
results
Utterance
ASR: automatic speech recognition
LU: language understanding
CM: confidence measure
M ?
M ?
Speech understanding
Language
model #2
Language
model #N
LU model
#2
LU model
#M
Logistic
regression #1
Logistic
regression #N
Logistic
regression #
Figure 1: Overview of speech understanding framework MLMU
ing data. More specifically, the allocation policy
of training data is changed and thus appropriate
LMs and LUMs are selected as its result.
An overview of MLMU is shown in Figure 1.
MLMU uses multiple LMs for ASR and multi-
ple LUMs and selects the most reliable speech un-
derstanding result from all combinations of them.
We denote a speech understanding module as SUi
(i = 1, . . . , n). Its result is a semantic representa-
tion consisting of a set of concepts. The concept is
either a semantic slot and its value or an utterance
type. Note that n = N ? M , when N LMs and
M LUMs are used. The confidence measure per
utterance for a result of i-th speech understanding
module SUi is denoted as CMi. The speech un-
derstanding result having the highest confidence
measure is selected as the final result for the ut-
terance. That is, the result is the output of SUm
where m = argmaxi CMi.
The confidence measure is calculated by logis-
tic regression based on the features of each speech
understanding result. A logistic regression func-
tion is constructed for each speech understanding
module SUi:
CMi =
1
1 + e?(ai1Fi1+...+ai7Fi7+bi) . (1)
Parameters ai1, . . . , ai7 and bi are determined by
using training data. In the training phase, teacher
signal 1 is given when a speech understanding re-
sult is completely correct; that is, when no error is
contained in the result. Otherwise, 0 is given. We
use seven features, Fi1, Fi2, . . . , Fi7, as indepen-
dent variables. Each feature value is normalized
Table 1: Features of speech understanding result
obtained from SUi
Fi1: Acoustic score normalized by utterance length
Fi2: Difference between Fi1 and normalized acoustic
scores of verification ASR
Fi3: Average concept CM in understanding result
Fi4: Minimum concept CM in understanding result
Fi5: Number of concepts in understanding result
Fi6: Whether any understanding result is obtained
Fi7: Whether understanding result is yes/no
CM: confidence measure
so as to make its mean zero and its variance one.
The features used are listed in Table 1. Com-
pared with those used in our previous paper (Kat-
sumaru et al, 2009), we deleted ones that were
highly correlated with other features and added
ones regarding content of the speech understand-
ing results. Features Fi1 and Fi2 are obtained
from an ASR result. Another ASR with a gen-
eral large vocabulary LM is executed for verifying
the i-th ASR result. Fi2 is the difference between
its score and Fi1 (Komatani et al, 2007). These
two features represent the reliability of the ASR
result. Fi3 and Fi4 are calculated for each concept
in the LU result on the basis of the posterior prob-
ability of the 10-best ASR candidates (Komatani
and Kawahara, 2000). Fi5 is the number of con-
cepts in the LU result. This feature is effective be-
cause the LU results of lengthy utterances tend to
be erroneous in a grammar-based LU. Fi6 repre-
sents the case when an ASR result is not accepted
by the subsequent LU module. In such cases, no
speech understanding result is obtained, which is
581
U1: It is June ninth.
ASR result:
- grammar ?It is June ninth.?
- N-gram ?It is June noon and?
LU result:
- grammar + FST ?month:6 day:9 type:refer-time?
- N-gram + WFST ?month:6 type:refer-time?
U2: I will borrow it on twentieth.
(Underlined part is out-of-grammar.)
ASR result:
- grammar ?Around two pm on twentieth.?
- N-gram ?Around two at ten on twentieth.?
LU result:
- grammar + FST ?day:20 hour:14 type:refer-time?
- N-gram + WFST ?day:20 type:refer-time?
Combination of LM and LUM is denoted as ?LM+LUM?.
Figure 2: Example of speech understanding re-
sults in MLMU framework
regarded as an error. Fi7 is added because affirma-
tive and negative responses, typically ?Yes? and
?No?, tend to be correctly recognized and under-
stood.
Figure 2 depicts an example when multiple
ASRs based on LMs and multiple LUs are used.
In short, the correct speech understanding result is
obtained from a different combination of LMs and
LUMs.
4 Automatic Allocation of Training Data
Using Change in Coefficients
The training data need to be allocated to the
speech understanding modules (i.e., statistical LM
and statistical LUM) and the selection module. If
more data are allocated to the ASR and LU mod-
ules, the performances of these modules are im-
proved, but the overall performance is degraded
because of the low performance of the selection
module. On the other hand, even if more training
data are allocated to the selection module, the per-
formance of each ASR and LU module remains
low.
4.1 Allocation Policy
We focus on the convergence of the logistic re-
gression functions when the amount of training
data increases. The convergence is defined as
the change in their coefficients, which will appear
later as Equation 2, and determines two points
1. All data are used to
train selection modules
2. Data are allocated to SU
and selection modules
3. Data are
not divided
No No
Yes
Yes
Selection module 
first converges?
No over-fitting
occurs?
Amount of training data increases
SU: speech understanding
Figure 3: Flowchart of data allocation
during the increase in training data, and thus three
phases are defined. The flowchart of data alloca-
tion is depicted in Figure 3. The three phases are
explained below.
In the first phase, the first priority is given to
the selection module. This is because the lo-
gistic regression functions used in the selection
module converge with relatively less training data
than those in the statistical ASR and LU mod-
ules for speech understanding; there are eight pa-
rameters for each logistic regression function as
shown in Equation 1, far fewer than for other sta-
tistical models such as N-gram and CRF. The out-
put from a speech understanding module that em-
ploys grammar-based LM and LUM would be the
most reliable in many cases because its perfor-
mance is better than that of other statistical mod-
ules when a very small amount of training data is
available. As a result, equivalent or better perfor-
mance would be achieved than methods using a
single ASR module and a single LU module.
In the second phase, the training data are also
allocated to the speech understanding modules af-
ter the selection module converges. This aims
to improve the performance of the speech under-
standing modules by allocating as much training
data to them as possible. The amount of train-
ing data is fixed in this phase to the amount al-
located to the selection module determined in the
first phase. The remaining data are used to train
the speech understanding modules.
When the performances of all the speech under-
standing modules stabilize, the allocation phase
proceeds to the third one. After this point, we
hypothesize that overfitting does not occur in this
phase because plenty of training data are avail-
able. All available data are used to train all mod-
582
ules without dividing the data in this phase.
4.2 Determining When to Switch Allocation
Policies
Automatic switching from one phase to the next
requires the determination of two points in the
number of training utterances: when the selec-
tion module first converges (konlysel) and when
the speech understanding modules all become sta-
ble (knodiv). These points are determined by fo-
cusing on the changes in the coefficients of the
logistic regression functions when the number of
utterances used as training data increases. We ob-
serve the sum of the changes in the coefficients of
the functions and then identify the points at which
the changes converge. The points are determined
individually by the following algorithm.
Step 1 Construct two logistic regression func-
tions for speech understanding module SUi
by using k and (k + ?k) utterances out of
kmax utterances, where kmax is the amount
of training data available.
Step 2 Calculate the change in coefficients from
the two logistic regression functions by
?i(k) =
?
j
|aij(k + ?k) ? aij(k)|
+|bi(k + ?k) ? bi(k)|, (2)
where aij(k) and bi(k) denote the param-
eters of the logistic regression functions,
shown in Equation 1, for speech understand-
ing module SUi, when k utterances are used
to train the functions.
Step 3 If ?i(k) becomes smaller than threshold
?, consider that the training of the functions
has converged, and record this k as the point
of convergence. If not, return to Step 1 after
k ? k + ?k.
The ?k is the minimum unit of training data con-
taining various utterances. We set it as the number
of utterances in one dialogue session, whose aver-
age was 17. Threshold ? was set to 8, which corre-
sponds to the number of parameters in the logistic
regression functions. No experiments were con-
ducted to determine if better performance could
be achieved with other choices of ?1.
The first point, konlysel, is determined using the
speech understanding module that uses no training
data. Specifically, we used ?grammar+FST? as
method SUi. Here, ?LM+LUM? denotes a com-
bination of LM for ASR and LUM. If the func-
tion converges at k utterances, we set konlysel to
k and fix the k utterances as training data used by
the selection module. The remaining (kmax ? k)
utterances are allocated to the speech understand-
ing modules, that is, the LMs and LUMs. Note
that if k becomes equal to kmax before ?i con-
verges, all training data are allocated to the selec-
tion module; that is, no data are allocated to the
LMs and LUMs. In this case, no output is ob-
tained from statistical speech understanding mod-
ules, and only outputs from the grammar-based
modules are used.
The second point, knodiv , is determined on the
basis of the speech understanding module that
needs the largest amount of data for training. The
amount of data needed depends on the number of
parameters. Specifically, we used ?N-gram+CRF?
as SUi in Equation 2. If the function converges,
we hypothesize that the performance of all the
speech understanding modules stabilize and thus
overfitting does not occur. We then stop the divi-
sion of training data, and use all available data to
train the statistical modules.
5 Experimental Evaluation
5.1 Target Data and Implementation
We used a data set previously collected through
actual dialogues with a rent-a-car reservation sys-
tem (Nakano et al, 2007) with 39 participants.
Each participant performed 8 dialogue sessions,
and 5900 utterances were collected in total. Out
of these utterances, we used 5240 for which the
automatic voice activity detection (VAD) results
agreed with manual annotation. We divided the
utterances into two sets: 2121 with 16 participants
as training data and 3119 with 23 participants as
the test data.
1We do not think the value is very critical after seeing the
results shown in Figure 4.
583
We constructed another rent-a-car reservation
system to evaluate our allocation method. The
system included two language models (LMs)
and four language understanding models (LUMs).
That is, eight speech understanding results in total
were obtained. The two LMs were a grammar-
based LM (?grammar?, hereafter) and a domain-
specific statistical LM (?N-gram?). The grammar
model was described by hand to be equivalent to
the FST model used in LU. The N-gram model
was a class 3-gram and was trained on a tran-
scription of the available training data. The vo-
cabulary size was 281 for the grammar model and
420 for the N-gram model when all the training
data were used. The ASR accuracies of the gram-
mar and N-gram models were 67.8% and 90.5%
for the training data and 66.3% and 85.0% for the
test data when all the training data were used. We
used Julius (ver. 4.1.2) as the speech recognizer
and a gender-independent phonetic-tied mixture
model as the acoustic model (Kawahara et al,
2004). We also used a domain-independent statis-
tical LM with a vocabulary size of 60250, which
was trained on Web documents (Kawahara et al,
2004), as the verification model.
The four LUMs were a finite-state transducer
(FST) model, a weighted FST (WFST) model,
a keyphrase-extractor (Extractor) model, and a
conditional random fields (CRF) model. In the
FST-based LUM, the FST was constructed by
hand. The WFST-based LUM is based on the
method developed by Fukubayashi et al (2008).
The WFSTs were constructed by using the MIT
FST Toolkit (Hetherington, 2004). The weight-
ing scheme used for the test data was selected by
using training data (Fukubayashi et al, 2008). In
the extractor-based LUM, as many parts as pos-
sible in the ASR result were simply transformed
into concepts. As the CRF-based LUM, we used
open-source software, CRF++2, to construct the
LUM. As its features, we use a word in the ASR
result, its first character, its last character, and the
ASR confidence of the word. Its parameters were
estimated by using training data.
The metric used for speech understanding per-
formance was concept understanding accuracy,
2http://crfpp.sourceforge.net/
Table 2: Absolute degradation in oracle accuracy
when each module was removed
Case (A) (B)
With all modules (%) 86.6 90.1
w/o grammar ASR -12.0 -1.1
w/o N-gram ASR -6.1 -7.7
w/o FST LUM -0.4 0.0
w/o WFST LUM -1.2 -0.5
w/o Extractor LUM -0.1 0.0
w/o CRF LUM -0.6 -3.7
(w/o FST & Extractor LUMs) -1.0 -0.1
(A): 141 utterances with 1 participant
(B): 2121 utterances with 16 participants
defined as
1 ? SUB + INS + DEL
no. of concepts in correct results,
where SUB, INS, and DEL denote the numbers of
substitution, insertion, and deletion errors.
5.2 Effectiveness of Using Multiple LMs and
LUMs
We investigated how much the performance of our
framework degraded when one ASR or LU mod-
ule was removed. We used the oracle accuracies,
i.e., when the most appropriate result was selected
by hand. The result reveals the contribution of
each ASR and LU module to the performance of
the framework. A module is regarded as more im-
portant when the accuracy is degraded more when
it is removed than when another one is removed.
Two cases (A) and (B) were defined: when the
amount of available training data was (A) small
and (B) large. We used 141 utterances with 1 par-
ticipant for case (A) and 2121 utterances with 16
participants for case (B). The results are shown in
Table 2.
When a small amount of training data was
available (case (A)), the accuracy was degraded by
12.0 points when the grammar-based ASRmodule
was removed and 6.1 points when the N-gram-
based ASR module was removed. The accuracy
was thus degraded substantially when either ASR
module was removed. This indicates that the two
ASR modules work complementarily.
584
020
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(a) grammar+FST
0
20
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(b) N-gram+CRF
Figure 4: Change in the sum of coefficients ?i when amount of training data increases (?LM+LUM?
denotes combination of LM and LUM)
On the other hand, when a large amount of
training data was available (case (B)), the ac-
curacy was degraded by 1.1 points when the
grammar-based ASR was removed. This means
that it became less important when there are
plenty of training data because the coverage of the
N-gram-based ASR became wider. In short, espe-
cially when the amount of training data is smaller,
speech understanding modules based on a hand-
crafted grammar are more important because of
the low performance of statistical modules.
Concerning the LUMs, the accuracy was de-
graded when any of the LUM modules was re-
moved when a small amount of training data was
available. When a large amount of training data
was available, the module based on CRF in par-
ticular became more important.
5.3 Results and Evaluation of Automatic
Allocation
Figure 4 shows the change in the sum of the co-
efficients, ?i, with the increase in the amount of
training data. In Figure 4(a), the change was very
large while the amount of training data was small,
and decreased dramatically and converged around
one hundred utterances. By applying ? (=8) to ?i,
we set 111 utterances as the first point, konlysel,
up to which all the training data are allocated to
the selection module, as described in Section 4.1.
Similarly, from the results shown in Figure 4(b),
we set 207 utterances as the second point, knodiv,
from which the training data are not divided.
To evaluate our method for allocating training
55
60
65
70
75
80
85
90
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
Our method
Na?ve allocation
No division
Figure 5: Results of allocation methods
data, we compared it with two baseline methods:
? No-division method: All data available at
each point were used to train both the speech
understanding modules and the selection
module. That is, the same data set was used
to train them.
? Naive-allocation method: Training data
available at each point were allocated equally
to the speech understanding modules and the
selection module.
As shown in Figure 5, our method had the best
concept understanding accuracy when the amount
of training data was small, that is, up to about
278 utterances. This indicates that our method for
allocating the available training data is effective
when the amount of training data is small.
This result is explained more specifically by us-
585
Table 3: Concept understanding accuracy for 141
utterances
Accuracy (%)
Our method 77.9
Naive allocation 73.5
No division 74.1
ing the case in which 141 utterances were used as
the training data. 111 (= konlysel) were secured to
train the selection module and 30 utterances were
allocated to train the speech understanding mod-
ules. As shown in Table 3, the accuracy with our
method was 3.8 points higher than that with the
no-division baseline method. This was achieved
by avoiding the overfitting of the logistic regres-
sion functions; i.e., the data input to the functions
became similar to the test data due to allocation,
so the concept understanding accuracy for the test
set was improved. The accuracy with our method
was 4.4 points higher than that with the naive al-
location baseline method. This was because the
amount of training data allocated to the selection
module was less than our method, and accordingly
the selection module was not trained sufficiently.
5.4 Comparison with methods using a single
ASR and a single LU
Figure 6 plots concept understanding accuracy
with our method against baseline methods using
a single ASR module and a single LU module for
various amounts of training data. Each module for
comparison was constructed by using all available
training data at each point while training data in-
creased; i.e., the same condition as our method.
The accuracies of only three speech understand-
ing modules are shown in the figure, out of the
eight obtained by combining two LMs for ASR
and four LUMs. These three are the ones with the
highest accuracies while the amount of training
data increased. Our method switched the alloca-
tion phase at 111 and 207 utterances, as described
in Section 5.3.
Our method performed equivalently or better
than all baseline methods even when only a small
amount of training data was available. As a result,
our method outperformed all the baseline methods
55
60
65
70
75
80
85
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
our method
grammar+FST
N-gram+WFST
N-gram+CRF
Figure 6: Comparison with baseline methods us-
ing single speech understanding
at every point while training data increase.
6 Conclusion
We developed a method to automatically allo-
cate training data to statistical modules so as to
avoid performance degradation caused by overfit-
ting. Experimental evaluation showed that speech
understanding accuracies achieved by our method
were equivalent or better than the baseline meth-
ods based on all combinations of a single ASR
module and a single LU module at every point
while training data increase. This includes a case
when a very small amount of training data is avail-
able. We also showed empirically that the training
data should be allocated while an amount of train-
ing data is not sufficient. Our method allocated
available training data on the basis of our alloca-
tion policy described in Section 4.1, and outper-
formed the two baselines where the training data
were equivalently allocated and not allocated.
When plenty of training data were available,
there was no difference between our method and
the speech understanding method that requires the
most training data, i.e., N-gram+CRF, as shown in
Figure 6. It is possible that our method combin-
ing multiple speech understanding modules would
outperform it as Schapire et al (2005) reported.
In their data, there were some examples that only
a hand-crafted rules can parse. Including such a
task as more complicated language understanding
grammar is required, verification of our method in
other tasks is one of the future works.
586
References
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-Ranking Models for Spoken
Language Understanding. In Proc. European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 202?210.
Fukubayashi, Yuichiro, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tet-
suya Ogata, and Hiroshi G. Okuno. 2008. Rapid
prototyping of robust language understanding mod-
ules for spoken dialogue systems. In Proc. Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP), pages 210?216.
Hahn, Stefan, Patrick Lehnen, and Hermann Ney.
2008. System Combination for Spoken Language
Understanding. In Proc. Annual Conference of the
International Speech Communication Association
(INTERSPEECH), pages 236?239.
Hetherington, Lee. 2004. The MIT Finite-State Trans-
ducer Toolkit for Speech and Language Processing.
In Proc. Int?l Conf. Spoken Language Processing
(ICSLP), pages 2609?2612.
Jeong, Minwoo and Gary Geunbae Lee. 2006. Ex-
ploiting non-local features for spoken language un-
derstanding. In Proc. COLING/ACL 2006 Main
Conference Poster Sessions, pages 412?419.
Katsumaru, Masaki, Mikio Nakano, Kazunori Ko-
matani, Kotaro Funakoshi, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2009. Improving speech un-
derstanding accuracy with limited training data us-
ing multiple language models and multiple under-
standing models. In Proc. Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 2735?2738.
Kawahara, Tatsuya, Akinobu Lee, Kazuya Takeda,
Katsunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and Japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Komatani, Kazunori and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proc. Int?l Conf. Computa-
tional Linguistics (COLING), pages 467?473.
Komatani, Kazunori, Yuichiro Fukubayashi, Tetsuya
Ogata, and Hiroshi G. Okuno. 2007. Introducing
utterance verification in spoken dialogue system to
improve dynamic help generation for novice users.
In Proc. 8th SIGdial Workshop on Discourse and
Dialogue, pages 202?205.
Nakano, Mikio, Yuka Nagano, Kotaro Funakoshi,
Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hi-
roshi Tsujino. 2007. Analysis of user reactions to
turn-taking failures in spoken dialogue systems. In
Proc. 8th SIGdial Workshop on Discourse and Dia-
logue, pages 120?123.
Raymond, Christian and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proc. Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH), pages 1605?
1608.
Shapire, Robert E., Marie Rochery, Mazin Rahim, and
Narendra Gupta. 2005. Boosting with prior knowl-
edge for call classification. IEEE Trans. on Speech
and Audio Processing, 13(2):174?181.
Wang, Ye-Yi and Alex Acero. 2006. Discrimina-
tive models for spoken language understanding. In
Proc. Int?l Conf. Spoken Language Processing (IN-
TERSPEECH), pages 2426?2429.
Wang, Ye-Yi, Alex Acero, Ciprian Chelba, Brendan
Frey, and Leon Wong. 2002. Combination of Sta-
tistical and Rule-based Approaches for Spoken Lan-
guage Understanding. In Proc. Int?l Conf. Spoken
Language Processing (ICSLP), pages 609?612.
587
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 176?184,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Non-humanlike Spoken Dialogue: A Design Perspective
Kotaro Funakoshi
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako
Saitama, Japan
funakoshi@jp.honda-ri.com
Mikio Nakano
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako
Saitama, Japan
nakano@jp.honda-ri.com
Kazuki Kobayashi
Shinshu University
4-17-1 Wakasato, Nagano
Nagano, Japan
kby@shinshu-u.ac.jp
Takanori Komatsu
Shinshu University
3-15-1 Tokida, Ueda
Nagano, Japan
tkomat@shinshu-u.ac.jp
Seiji Yamada
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda
Tokyo, Japan
seiji@nii.ac.jp
Abstract
We propose a non-humanlike spoken di-
alogue design, which consists of two el-
ements: non-humanlike turn-taking and
non-humanlike acknowledgment. Two ex-
perimental studies are reported in this pa-
per. The first study shows that the pro-
posed non-humanlike spoken dialogue de-
sign is effective for reducing speech colli-
sions. It also presents pieces of evidence
that show quick humanlike turn-taking is
less important in spoken dialogue system
design. The second study supports a hy-
pothesis found in the first study that user
preference on response timing varies de-
pending on interaction patterns. Upon re-
ceiving these results, this paper suggests a
practical design guideline for spoken dia-
logue systems.
1 Introduction
Speech and language are owned by humans.
Therefore, spoken dialogue researchers tend to
pursue a humanlike spoken dialogue. Only a few
researchers positively investigate restricted (i.e.,
non-humanlike) spoken dialogue design such as
(Ferna?ndez et al, 2007).
Humanlikeness is a very important concept and
sometimes it is really useful to design machines /
interactions. Machines are, however, not humans.
We believe humanlikenss cannot be the dominant
factor, or gold-standard, for designing spoken dia-
logues.
Pursuing humanlikeness has at least five criti-
cal problems. (1) Cost: in general, humanlikeness
demands powerful and highly functional hardware
and software, and highly integrated systems re-
quiring top-grade experts both for development
and maintenance. All of them lead to cost over-
run. (2) Performance: sometimes, humanlikeness
forces performance to be compromised. For ex-
ample, achieving quick turn-taking which humans
do in daily conversations forces automatic speech
recognizers, reasoners, etc. to be compromised to
enable severe real-time processing. (3) Applicabil-
ity: differences in cultures, genders, generations,
situations limit the applicability of a humanlike
design because it often accompanies a rigid char-
acter. For example, Shiwa et al (2008) succeeded
in improving users? impression for slow responses
from a robot by using a filler but obviously use
of such a filler is limited by social appropriate-
ness. (4) Expectancy: humanlike systems induce
too much expectancy of users that they are as in-
telligent as humans. It will result in disappoint-
ments (Komatsu and Yamada, 2010) and may re-
duce users? willingness to use systems. Keeping
high willingness is quite important from the view-
point of both research (for collecting data from
users to improve systems) and business (for con-
tinuously selling systems with limited functional-
ity). (5) Risk: Although it is not verified, what is
called the uncanny valley (Bartneck et al, 2007)
probably exists. It is commonly observed that peo-
ple hate imperfect humanlike systems.
We try to avoid these problems rather than over-
come them. Our position is positively exploring
non-humanlike spoken dialogue design. This pa-
176
per focuses on its two elements, i.e., decelerated
dialogues as non-humanlike turn-taking and an ar-
tificial subtle expression (ASE) as non-humanlike
acknowledgment1, and presents two experimental
studies regarding these two elements. ASEs, de-
fined by the authors in (Komatsu et al, 2010), are
simple expressions suitable for artifacts, which in-
tuitively notify users about artifacts? internal states
while avoiding the above five problems.
In Section 2, the first study, which was pre-
viously reported in (Funakoshi et al, 2010), is
summarized and shows that the proposed non-
humanlike spoken dialogue design is effective for
reducing speech collisions. It also presents pieces
of evidence that shows quick humanlike turn-
taking is less important in designing spoken dia-
logue systems (SDSs). In Section 3, the second
study, which is newly reported in this paper, shows
a tendency supporting a hypothesis found in the
first study that user preference on response timing
varies depending on interaction patterns. Upon re-
ceiving the results of the two experiments, a design
guideline for SDSs is suggested in Section 4.
2 Study 1: Reducing Speech Collisions
with an Artificial Subtle Expression
in a Decelerated Dialogue
An important issue in SDSs is the management of
turn-taking. Failures of turn-taking due to sys-
tems? end-of-turn misdetection cause undesired
speech collisions, which harm smooth communi-
cation and degrade system usability.
There are two approaches to reducing speech
collisions due to end-of-turn misdetection. The
first approach is using machine learning tech-
niques to integrate information from multiple
sources for accurate end-of-turn detection in early
timing. The second approach is to make a long in-
terval after the user?s speech signal ends and be-
fore the system replies simply because a longer
interval means no continued speech comes. As
far as the authors know, all the past work takes
the first approach (e.g., (Kitaoka et al, 2005;
Raux and Eskenazi, 2009)) because the second ap-
proach deteriorates responsiveness of SDSs. This
choice is based on the presumption that users pre-
fer a responsive system to less responsive systems.
The presumption is true in most cases if the sys-
1In this paper, acknowledgment denotes that at the level 1
of the joint action ladder (Clark, 1996), which communicates
the listener?s identifying the signal presented by the speaker.
B l i n k i n g L E D
Figure 1: Interface robot with an embedded LED
tem?s performance is at human level. However, if
the system?s performance is below human level,
high responsiveness might not be vital or even be
harmful. For instance, Hirasawa et al (1999) re-
ported that immediate overlapping backchannels
can cause users to have negative impressions. Ki-
taoka et al (2005) also reported that the familiarity
of an SDS with backchannels was inferior to that
without backchannels due to a small portion of er-
rors even though the overall timing and frequency
of backchannels was fairly good (but did not come
up to human operators). Technologies are advanc-
ing but they are still below human level. We chal-
lenge the past work that took the first approach.
The second approach is simple and sta-
ble against user differences and environmental
changes. Moreover, it can afford to employ more
powerful but computationally expensive speech
processing or to build systems on small devices
with limited resources. A concern with this ap-
proach is debasement of user experience due to
poor responsiveness as stated above. Another is-
sue is speech collisions due to users? following-
up utterances such as repetitions. Slow responses
tend to induce such collision-eliciting speech.
This section shows the results of the experiment
in which participants engaged in hotel reservation
tasks with an SDS equipped with an ASE-based
acknowledging method, which intuitively notified
a user about the system?s internal state (process-
ing). The results suggest that the method can re-
duce speech collisions and provide users with pos-
itive impressions. The comparisons of evaluations
between systems with a slow reply speed and a
moderate reply speed suggest that users of SDSs
do not care about slow replies. These results in-
dicate that decelerating spoken dialogues is not a
bad idea.
2.1 Experiment
System An SDS that can handle a hotel reserva-
tion domain was built. The system was equipped
177
USER
SYSTEM
VAD tail margin wait interval
processing delay
blinking LED (artificial subtle expression)
short pauses
detected speech onset detected end-of-turn
X Y
system speech
user speech
time
Figure 2: Behavior of the dialogue system along a timeline
with an interface robot with an LED attached to
its chest (see Figure 1). Participants? utterances
were recognized by an automatic speech recog-
nizer Julius2, and interpreted by an in-house lan-
guage understander. The robot?s utterances were
voiced by a commercial speech synthesizer. The
LCD monitor in Figure 1 was used only to show
reservation details at last.
Julius output a recognition result to the system
at 400 msec after an input speech signal ended, but
the system awaited the next input for a fixed inter-
val (wait interval, whose length is given as an ex-
perimental factor). If the system received an addi-
tional input, it awaited the next input for the same
interval again. Otherwise, the system replied.
The LED started blinking at 1/30 sec even-
intervals when a speech signal was detected and
stopped when the system started replying. The
basic function of the blinking light expression is
similar to hourglass icons used in GUIs. A big
difference is that basically GUIs can ignore any in-
put while they are showing those icons, but SDSs
must accept successive speech while it is blink-
ing an LED. What we intend to do is to suppress
only collision-eliciting speech such as repetitions
(we call them follow-ups) which are negligible
but difficult to be automatically distinguished from
barge-ins. Barge-ins are not negligible.
Conditions and participants Two experimen-
tal factors were set-up, that is, the reply speed
factor (moderate or slow reply speed) and the
blinking light factor (with or without a blinking
light), resulting in four conditions:
A: slow reply speed, with a blinking light,
B: slow reply speed, without a blinking light,
C: moderate reply speed, with a blinking light,
D: moderate reply speed, without a blinking light.
We randomly assigned 48 Japanese participants
2http://julius.sourceforge.jp/
(mean age 30.9) to one of the four conditions.
A reply speed depends on a wait interval for
which the dialogue system awaits the next input.
Shiwa et al (2008) showed that the best reply
speed for a conversational robot was one second.
Thus we chose 800 msec as the wait interval for
the moderate reply speed because an actual reply
speed was the accumulation of the wait interval
and a delay for processing a user request, and 800
msec is simply twice the default length (the VAD
tail margin) by which the Julius speech recognizer
recognizes the end of a speech. For the slow reply
speed, we chose 4 sec as the wait interval. Wait
intervals include the VAD tail margin.
Figure 2 shows how the system and the LED
work along with user speech. In this figure, a user
utters a continuous speech with a rather long pause
that is longer than the VAD tail margin but shorter
than the wait interval. If the system detects the
end of the user?s turn and starts speaking within
the interval marked with an ?X?, a speech collision
would occur. If the user utters a follow-up within
the interval marked with a ?Y?, a speech collision
would occur, too. We try to suppress the former
speech collision by decelerating dialogues and the
latter by using a blinking light as an ASE.
Method The experiment was conducted in a
room for one participant at one time. Participants
entered the room and sat on a chair in front of a
desk as shown in Figure 1.
The experimenter gave the participants instruc-
tions so as to reserve hotel rooms five times by
talking with the robot in front of them. All of them
were given the same five tasks which require them
to reserve several rooms (one to three) at the same
time. The meaning of the blinking light expres-
sion was not explained to them. After giving the
instructions, the experimenter left the participants,
and they began tasks when the robot started to talk
to them. Each task was limited to up to three min-
utes. After finishing the tasks, the participants an-
178
swered a questionnaire. Figure 5 and Figure 6 in
the appendix show one of the five task instructions,
and a dialogue on that task, respectively.
2.2 Results
Reply speeds Averages of observed reply
speeds were calculated from the timestamps in
transcripts. They were 4.53 sec for the slow con-
ditions and 1.42 sec for the moderate conditions.
Task completion The average number of com-
pleted tasks in the four conditions A, B, C, and D
were 4.00, 3.83, 3.83, and 4.33, respectively. An
ANOVA did not find any significant difference.
Speech collisions We counted speech collisions
for which the SDS was responsible, that is, the
cases where the robot spoke while participants
were talking (i.e., end-of-turn misdetections). Of
course, there were speech collisions for which par-
ticipants were responsible, that is, the cases where
participants intentionally spoke while the robot
was talking (i.e., barge-ins). These speech colli-
sions were not the targets, hence they were not in-
cluded in the counts.
Speech collisions due to participants? back-
channel feedbacks were not included, either. We
think that it is possible to filter out such feedback
because feedback utterances are usually very short
and variations are small. On the other hand, as
we mentioned above, it is not easy to automat-
ically distinguish negligible speech such as rep-
etitions from barge-ins. We want to suppress
only such speech negligible but hard to distinguish
from other not negligible speech.
The number of observed speech collisions in
the four conditions A, B, C, and D were 5, 11,
45, and 30, respectively. First we performed an
ANOVA on the number of collisions. The interac-
tion effect was not significant (p = 0.24). A sig-
nificant difference on the reply speed factor was
found (p < 0.005). This result confirms that de-
celerating dialogues reduces collisions. The ef-
fect of the blinking light factor was not significant
(p = 0.60).
Next we performed a Fisher?s exact test (one-
side) on the number of participants who had
speech collisions between the two conditions of
the slow reply speed (3 out of 12 for A and 8 out
of 12 for B). The test found a significant difference
(p < 0.05). This result indicates that the blinking
light can reduce speech collisions by suppressing
users? follow-ups in decelerated dialogues.
Impression on the dialogue and robot The par-
ticipants rated 38 positive-negative adjective pairs
(such as smooth vs. rough) for evaluating both the
dialogue and the robot. The ratings are based on a
seven-point Likert scale.
An ANOVA found a positive marginal signifi-
cance (p = 0.07) for the blinking light in the com-
fortableness factor extracted by a factor analysis
for the impression on the dialogue. In addition,
an ANOVA found a positive marginal significance
(p = 0.07) for the slow reply speed in the mod-
esty factor extracted by a factor analysis for the
impression on the robot. Surprisingly, no signifi-
cant negative effect for the slow reply speed was
found.
System evaluations The participants evaluated
the SDS in two measures on a scale from 1 to 7,
that is, the convenience of the system and their
willingness to use the system. The greater the
evaluation value is, the higher the degree of con-
venience or willingness.
The average scores of convenience in the four
conditions A, B, C, and D were 3.50, 3.17, 3.17,
and 3.92, respectively. Those of willingness were
3.58, 2.58, 2.83, and 3.42, respectively. ANOVAs
did not find any significant difference among the
four conditions both for the two measures.
Discussion on user preference The analysis of
the questionnaire suggests that the blinking light
expression gives users a comfortable impression
on the dialogue. The analysis also suggests that
the slow reply speed gives users a modest impres-
sion on the interface robot. Meanwhile, no neg-
ative impression with a statistical significance is
found on the slow reply speed.
Although no statistically significant difference
is found between the four conditions, numbers
of completed tasks and convenience are strongly
correlated. However, users? willingness to use
the systems, which is the most important mea-
sure for systems, is inverted between condition
A and D. Convenience will be primarily domi-
nated by what degree a user?s purpose (reserving
rooms) is achieved, thus, it is reasonable that con-
venience scores correlate with the number of com-
pleted tasks. On the other hand, willingness will
be dominated by not only practical usefulness but
also overall usability or experience. Therefore,
we can interpret that the improvements in impres-
sions and reduction in aversive speech collisions
179
let condition A have the highest score for willing-
ness. These results indicate that decelerating spo-
ken dialogues is not a bad idea in contradiction
to the common design policy in human-computer
interfaces (HCIs), and they suggest to exploit mer-
its provided by decelerating dialogues rather than
pursuing quickly responding humanlike systems.
Our finding contradicts not only the com-
mon design policy in HCIs but also the de-
sign policy in human-robot interaction found by
Shiwa et al (2008), that is, the best response tim-
ing of a communication robot is at one second. We
think this contradiction is superficial and is ascrib-
able to the following four major differences be-
tween their study and our study.
? They adopted a within-subjects experimental
design while we adopted a between-subjects
design. A within-subjects design makes sub-
jects do relative evaluations and tends to em-
phasis differences.
? Their question was specific in terms of re-
sponse timing. Our questions were overall
ratings of the system such as convenience.
? They assumed a perfect machine (Wizard-of-
Oz experiment). Our system was elaborately
crafted but still far from perfect.
? Our system quickly returns non-verbal re-
sponses even if verbal responses are delayed.
From these differences, we hypothesize that re-
sponse timing has no significant impact on the us-
ability of SDSs in an absolute and holistic context
at least in the current state of the art spoken dia-
logue technology, even though users prefer a sys-
tem which responds quickly to a system which re-
sponds slowly when they compare them with each
other directly, given an explicit comparison metric
on response timing with perfect machines.
3 Study 2: Uncovering Comfortableness
of Response Timing under Different
Interaction Patterns
Our conclusion in Section 2 is that SDSs do not
need to quickly respond verbally as long as they
quickly respond non-verbally by showing their in-
ternal states with an ASE, while many researchers
try to make them verbally respond as fast as pos-
sible. Decelerating a dialogue has many practical
advantages as stated above.
However, through the experiment, we have also
suspected that this conclusion is not valid in some
specific cases. That is, we think in some situa-
tions users feel uncomfortable with slow verbal re-
sponses primordially, and those situations are such
as when users simply reply to systems? yes-no-
questions or greetings. Our hypothesis is that users
expect quick verbal responses (and hate slow ver-
bal responses) only when users expect that it is not
difficult for systems to understand their responses
or to decide next actions. This section reports the
experiment validating this hypothesis.
3.1 Experiment
To validate the hypothesis described above, we
conducted a Wizard-of-Oz experiment using fixed
scenarios. Participants engaged in short interac-
tions with an interface robot and evaluated re-
sponse timing of the robot. Three experimental
factors were interaction patterns, response timing
(wait interval), and existence of a blinking light.
Interaction patterns Five interaction patterns
were setup to see the differences between situa-
tions. Each pattern consisted of three utterances.
The first utterance was from the system. Upon re-
ceiving the utterance, a participant as a user of the
system replied with the second utterance. Then
the system responded after the given wait interval
(1 sec or 4 sec) with the third utterance. Partic-
ipants evaluated this interval between the second
utterance and the third utterance in a measure of
comfortableness.
The patterns with scenarios are shown in Fig-
ure 3. They will be referred to by abbreviations
(PGG, QYQ, QNQ, PSQ, PLQ) in what follows.
Note that the scenarios are originally in Japanese.
Here, RequestS and RequestL mean a short re-
quest and a long request, respectively. YNQues-
tion and WhQuestion mean a yes-no-question and
a wh-question, respectively. According to the hy-
pothesis, we can predict that the reported com-
fortableness for the longer wait interval (4 sec)
are worse for short and formulaic cases such as
PGG and QYQ than for the long request case (i.e.,
PLQ). In addition, we can predict that the reported
comfortableness for longer intervals improves for
PLQ if the robot?s light blinks, while that does not
improve for PGG and QYQ.
System We used the same interface robot and
the LCD monitor as study 1. The experiment in
this study, however, was conducted using a WOZ
system.
180
Prompt-Greeting-Greeting (PGG)
S: Welcome to our Hotel. May I help you?
U: Hello.
S: Hello.
YNQuestion-Yes-WhQuestion (QYQ)
S: Welcome to our Hotel. Will you stay tonight?
U: Yes.
S: Can I ask your name?
YNQuestion-No-WhQuestion (QNQ)
S: Welcome to our Hotel. Will you stay tonight?
U: No.
S: How may I help you?
Prompt-RequestS-WhQuestion (PSQ)
S: Welcome to our Hotel. May I help you?
U: I would like to reserve a room from tomorrow.
S: How long will you stay?
Prompt-RequestL-WhQuestion (PLQ)
S: Welcome to our Hotel. May I help you?
U: I would like to reserve rooms with breakfast from to-
morrow, one single room and one double room, non-
smoking and smoking, respectively.
S: How long will you stay?
Figure 3: Interaction patterns and scenarios
First the WOZ system presents an instruction to
the participant on the LCD monitor, which reveals
the robot?s first utterance of the given scenario
(e.g., ?Welcome to our Hotel. May I help you??)
and indicates the participant?s second utterance
(e.g., ?Hello.?). Two seconds after the participant
clicks the OK button on the monitor with a com-
puter mouse, the system makes the robot utter the
first utterance. Then, the participant replies, and
the operator of the system end-points the end of
participant?s speech by clicking a button shown in
another monitor for the operator in the room next
to the participant?s room. After the end-pointing,
the system waits for the wait interval (one second
or four seconds) and makes the robot utter the third
utterance of the scenario. One second after, the
system asks the participant to evaluate the com-
fortableness of the response timing of the robot?s
third utterance on a scale from 1 to 7 (1:very un-
comfortable, 4:neutral, 7:very comfortable) on the
LCD monitor.
Conditions and participants Forty participants
(mean age 28.8, 20 males and 20 females) engaged
in the experiment. No participant had engaged in
study 1. They were randomly assigned to one of
two groups (gender was balanced). The groups
correspond to one of two levels of the experi-
mental factor of the existence of a blinking light.
For one group, the robot blinked its LED when it
was waiting. For the other group, the robot did
not blink the LED. We refer to the former group
(condition) as BL (Blinking Light, n=20) and the
later as NL (No Light, n=20). In summary, this
experiment is within-subjects design with regard
to interaction patterns and response timing and is
between-subjects design with regard to the blink-
ing light.
Method The experiment was conducted in a
room for one participant at one time. Participants
entered the room and sat on a chair in front of a
desk as shown in Figure 1, but they did not wear
headphones this time.
The experimenter gave the participants instruc-
tions so as to engage in short dialogues with the
robot in front of them. They engaged in each of
five scenarios shown in Figure 3 six times (three
times with a 1 sec wait interval and three with
4 sec), resulting in 30 dialogues (5? 3? 2 = 30).
The order of scenarios and intervals was random-
ized. The existence and meaning of the blinking
light expression was not explained to them. They
were not told that the systemwas operated by a hu-
man operator, either. After giving the instructions,
the experimenter left the participants, and they
practiced one time. This practice used a Prompt-
RequestM-WhQuestion3 type scenario with a wait
interval of two seconds. Then, thirty dialogues
were performed. Short breaks were inserted af-
ter ten dialogues. Each dialogue proceeded as ex-
plained above.
3.2 Results
End-pointing errors End-pointing was done by
a fixed operator. We obtained 1,184 dialogues out
of 1,200 (= 30 ? 40) after removing dialogues
in which end-pointing failed (failures were self-
reported by the operator). We sampled 30 dia-
logues from the 1,184 dialogues and analyzed end-
pointing errors in the recorded speech data. The
average error was 84.6 msec (SD=89.6).
Comfortableness This experiment was de-
signed to grasp a preliminary sense on our
hypothesis as much as possible with a limited
number of participants in exchange for aban-
donment of use of statistical tests, because this
study involved multiple factors and the interaction
pattern factor was complex by itself. Therefore,
in the following discussion on comfortableness,
we do not refer to statistical significances.
3The request utterance is longer than that of RequestS and
shorter than that of RequestL.
181
!"#$
"%&'(
)*+,,
,+! ,+!
!"#$
"%&'(
)*+,,
,+! ,+!
Figure 4: Comfortableness (Left: without a blinking light (NL), right: with a blinking light (BL))
Figure 4 shows regression lines obtained from
the 1,184 dialogues in the two graphs for NL and
BL (Detailed values are shown in Table 1). The
X axes in the graphs correspond to response tim-
ing, that is, the two wait intervals of 1 sec and
4 sec. The Y axes correspond to comfortableness
reported in a scale from 1 to 7. Obviously, with or
without a blinking light effected comfortableness.
The results shown in the graphs support the pre-
dictions made in Section 3.1. The scores of PGG
and QYQ are worse than that of PLQ at 4 sec.
PGG and QYQ show no difference between NL
and BL. QNQ and PSQ show differences. PLQ
shows the biggest difference. In case of PLQ, the
reported comfortableness at 4 sec shifted to al-
most the neutral position (score 4) by presenting a
blinking light. This indicates that a blinking light
ASE can allay the debasement of impression due
to slow responses only in non-formulaic cases.
Interestingly, the blinking light expression at-
tracted comfortableness scores to neutral both at
1 sec and at 4 sec. We can make two hypotheses
on this result. One is that the blinking light expres-
sion has a negative effect which degrades comfort-
ableness at 1 sec. The other is that the blinking
light expression makes participants difficult to see
differences between 1 sec and 4 sec, therefore, re-
ported scores converge to neutral. At this stage we
think that the later is more probable than the for-
mer because the scores of PGG and QYQ should
be degraded at 1 sec if the former is true.
4 A Practical Design Guideline for SDSs
Summarizing the results of the experiments pre-
sented in Section 2 and Section 3, we suggest a
twofold design guideline for SDSs, especially for
task-oriented systems. Some interaction-oriented
systems such as chatting systems are out of scope
of this guideline. In what follows, first the guide-
line is presented and then a commentary on the
guideline is described.
The guideline
(1) Never be obsessed with quick turn-taking
but acknowledge users immediately
Quick turn-taking will not recompense your ef-
forts, resources inputted, etc. Pursue it only af-
ter accomplishing all you can do without compro-
mising performance in other elements of dialogue
systems and only if it does not make system devel-
opment and maintenance harder. However, quick
(possibly non-verbal) acknowledgment is a requi-
site. You can compensate for the debasement of
user experience due to slow verbal responses just
by using an ASE such as a tiny blinking LED to
acknowledge user speech. No instruction about
the ASE is needed for users.
(2) Think of users? expectations
Users expect rather quick verbal responses to their
greetings and yes-answers. ASEs will be ineffec-
tive for them. Thus it is recommended to enable
your systems to quickly respond verbally to such
utterances. Fortunately it is easy to anticipate such
utterances. Greetings usually occur only at the be-
ginning of dialogues or after tasks were accom-
plished. Yes-answers will come only after yes-no-
questions. Therefore it will be able to implement
an SDS that quickly responds verbally to greeting
and yes-answers both without increasing develop-
ment / maintenance costs and without decreasing
182
recognition performance, etc.
However, you should keep in mind that too
quick verbal responses (0 sec interval or overlap-
ping) may not be welcomed (Hirasawa et al, 1999;
Shiwa et al, 2008). They may also induce too
much expectancy in users and result in disappoint-
ments to your systems after some interactions.
Commentary on the guideline
The guideline was constructed so as to avoid the
five problems pointed out in Section 1. The first
point of the guideline is induced mainly from the
results of study 1, and the second point is induced
mainly from the results of study 2.
Although the results of study 2 indicate users
prefer quick responses to slow ones as presup-
posed in past literature, note that the experiment
in study 2 is within-subjects design with regard to
the response timing factor and that within-subjects
design tends to emphasis differences as discussed
at the end of Section 2. The results of study 1
suggested that such an emphasized difference (i.e.,
preference for quick responses) has no significant
impact on the usability of SDSs on the whole.
5 Conclusion
This paper proposed a non-humanlike spoken di-
alogue design, which consists of two elements:
non-humanlike turn-taking and acknowledgment.
Two experimental studies were reported regarding
these two elements. The first study showed that the
proposed non-humanlike spoken dialogue design
is effective for reducing speech collisions. This
study also presented pieces of evidence that show
quick humanlike turn-taking is less important in
spoken dialogue system (SDS) design. The second
study showed a tendency supporting a hypothesis
found in the first study that user preference on re-
sponse timing varies depending on interaction pat-
terns in terms of comfortableness. Upon receiving
these results, a practical design guideline for SDSs
was suggested, that is, (1) never be obsessed with
quick turn-taking but acknowledge users immedi-
ately and (2) think of users? expectations.
Our non-humanlike acknowledging method us-
ing an LED-based artificial subtle expression
(ASE) can apply to any interfaces on wearable /
handheld devices, vehicles, whatever. It is, how-
ever, difficult to directly apply it to call-centers
(i.e., telephone interfaces), which occupy a big
portion of the deployed SDSs pie. Yet, the un-
derlying concept: decelerated dialogues accom-
panied by an ASE will be applicable even to tele-
phone interfaces by using an auditory ASE, which
is to be explored in future work.
The guideline is supported by findings in a
rather hypothetical stage. More experiments are
necessary to confirm these findings. In addition,
the guideline is for the current transitory period
in which intelligence technologies such as auto-
matic recognition, language processing, reasoning
etc. are below human level. In that sense, the con-
tribution of this paper might be limited. However,
this period will last until a decisive paradigm shift
occurs in intelligence technologies. It may come
after a year, a decade, or a century.
References
C. Bartneck, T. Kanda, H. Ishiguro, and N. Hagita.
2007. Is the uncanny valley an uncanny cliff? In
Proc. RO-MAN 2007.
H. Clark. 1996. Using Language. Cambridge U. P.
R. Ferna?ndez, D. Schlangen, and T. Lucht. 2007.
Push-to-talk ain?t always bad! comparing different
interactivity settings in task-oriented dialogue. In
Proc. DECALOG 2007.
K. Funakoshi, K. Kobayashi, M. Nakano, T. Komatsu,
and S. Yamada. 2010. Reducing speech collisions
by using an artificial subtle expression in a deceler-
ated spoken dialogue. In Proc. 2nd Intl. Symp. New
Frontiers in Human-Robot Interaction.
J. Hirasawa, M. Nakano, T. Kawabata, and K. Aikawa.
1999. Effects of system barge-in responses on user
impressions. In Proc. EUROSPEECH?99.
N. Kitaoka, M. Takeuchi, R. Nishimura, and S. Nak-
agawa. 2005. Response timing detection us-
ing prosodic and linguistic information for human-
friendly spoken dialog systems. Journal of The
Japanese Society for AI, 20(3).
T. Komatsu and S. Yamada. 2010. Effects of adapta-
tion gap on user?s variation of impressions of artifi-
cial agents. In Proc. WMSCI 2010.
T. Komatsu, S. Yamada, K. Kobayashi, K. Funakoshi,
and M. Nakano. 2010. Artificial subtle expressions:
Intuitive notification methodology of artifacts. In
Proc. CHI 2010.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Proc.
NAACL-HLT 2009.
T. Shiwa, T. Kanda, M. Imai, H. Ishiguro, and
N. Hagita. 2008. How quickly should communi-
cation robots respond? In Proc. HRI 2008.
183
Hotel Reservation Task 3
Reserve rooms as below
Stay
Room
Twin, 1 room, non-smoking
Double, 1 room, non-smoking
As specified with the orange-colored frameon the calendar 
Figure 5: One of the five task instructions used in study 1
S: Welcome to Hotel Wakamatsu-Kawada. May I help you?
U: I want to stay from March 10th to 11th.
S: What kind of room would you like?
U: One non-smoking twin room and one non-smoking double room.
S: Are your reservation details correctly shown on the screen?
U: Yes. No problem.
S: Your reservation has been accepted. Thank you for using us.
Figure 6: A successful dialogue observed with the task shown in Figure 5 (translated into English)
Table 1: Detailed comfortableness scores in study 2
Interaction pattern PGG QYQ QNQ PSQ PLQ
Condition NL BL NL BL NL BL NL BL NL BL
1 sec
mean 5.34 5.36 5.55 5.56 5.48 5.25 5.09 4.73 5.13 4.41
s.d. 1.00 1.17 1.10 1.00 1.02 1.04 1.12 1.09 1.14 1.20
p-value 0.93 0.96 0.23 0.09 0.001
4 sec
mean 3.12 3.16 3.37 3.36 3.28 3.52 3.43 3.52 3.54 3.83
s.d. 0.94 1.04 0.78 0.93 0.76 0.93 0.81 0.87 0.95 0.87
p-value 0.83 0.98 0.14 0.59 0.08
p-values were obtained by two-sided t-tests between NL and BL. Those are shown just for reference.
184
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18?29,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Two-Stage Domain Selection Framework for Extensible Multi-Domain
Spoken Dialogue Systems
Mikio Nakano
Honda Research Institute Japan
Wako, Saitama, Japan
nakano@jp.honda-ri.com
Shun Sato
Tokyo Denki University
Hatoyama, Saitama, Japan
rela.relakuma@gmail.com
Kazunori Komatani
Nagoya University
Nagoya, Aichi, Japan
komatani@nuee.nagoya-u.ac.jp
Kyoko Matsuyama?
Kyoto University
Kyoto, Kyoto, Japan
matuyama@kuis.kyoto-u.ac.jp
Kotaro Funakoshi
Honda Research Institute Japan
Wako, Saitama, Japan
funakoshi@jp.honda-ri.com
Hiroshi G. Okuno
Kyoto University
Kyoto, Kyoto, Japan
okuno@i.kyoto-u.ac.jp
Abstract
This paper describes a general and effective
domain selection framework for multi-domain
spoken dialogue systems that employ dis-
tributed domain experts. The framework con-
sists of two processes: deciding if the current
domain continues and estimating the probabil-
ities for selecting other domains. If the current
domain does not continue, the domain with
the highest activation probability is selected.
Since those processes for each domain expert
can be designed independently from other ex-
perts and can use a large variety of informa-
tion, the framework achieves both extensibil-
ity and robustness against speech recognition
errors. The results of an experiment using
a corpus of dialogues between humans and
a multi-domain dialogue system demonstrate
the viability of the proposed framework.
1 Introduction
As spoken dialogue interfaces are becoming more
widely utilized, they will be expected to be able to
engage in dialogues in a wide variety of topics. Par-
ticularly, spoken dialogue interfaces for office robots
(Asoh et al, 1999) and multimodal kiosk systems
(Gustafson and Bell, 2000) are expected to deal with
people?s various requests, unlike automated call cen-
ter systems that are dedicated to specific tasks.
One effective methodology to build such a sys-
tem is to integrate systems in small domains by
employing distributed multi-domain system archi-
tecture. This architecture has distributed modules
?Currently with Panasonic Corporation.
that independently manage their own dialogue state
and knowledge for speech understanding and ut-
terance generation (e.g., Lin et al (1999)). From
an engineering viewpoint, such architecture has an
advantage in that each domain expert can be de-
signed independently and that it is easy to add new
domains. It enables each domain expert to em-
ploy a dialogue strategy very different from those
for other domains. For example, the strategy may
be frame-based mixed-initiative, finite-state-based
system-initiative, or plan-based dialogue manage-
ment (McTear, 2004).
One of the crucial issues with distributed multi-
domain spoken dialogue systems is how to select an
appropriate domain for each user utterance so that
the system can appropriately understand it and an-
swer it. So far several methods have been proposed
but none of them satisfy two basic requirements at
the same time: the ability to be used with a variety
of domain experts (extensibility) and being robust
against ASR (Automatic Speech Recognition) errors
(robustness). We suspect that this is one of the
main reasons why not many multi-domain spoken
dialogue systems have been developed even though
their utility is widely recognized.
This paper presents a new general framework for
domain selection that satisfies the above two require-
ments. In our framework, each expert needs to have
two additional submodules: one for estimating the
probability that it is newly activated, and one for de-
ciding domain continuation when it is already acti-
vated. Since these submodules can be designed in-
dependently from those of other experts, there is no
restriction on designing experts in our framework,
18
and thus extensibility is achieved. Robustness is also
achieved because those submodules can be designed
so that they can utilize domain-dependent informa-
tion, including information on speech understanding
and dialogue history, without detracting from ex-
tensibility. Especially the submodule for deciding
domain continuation has the ability to utilize dia-
logue history to avoid erroneous domain shifts that
often occur in previous approaches. Note that we do
not focus on classifying each utterance without con-
textual information (e.g., Chu-Carroll and Carpenter
(1999)). Rather, we try to estimate the user inten-
tion with regard to continuing and shifting domains
in the course of dialogues.
In what follows, Section 2 explains the distributed
multi-domain spoken dialogue system architecture
and requirements for domain selection. Section 3
discusses previous work, and Section 4 presents our
proposed framework. Section 5 describes an exam-
ple implementation and its evaluation results, and
Section 6 concludes the paper.
2 Domain Selection in Multi-Domain
Spoken Dialogue Systems
2.1 Distributed Architecture
In distributed multi-domain spoken dialogue archi-
tecture (Figure 1), distributed modules indepen-
dently manage their own dialogue state and knowl-
edge for speech understanding and utterance gener-
ation (Lin et al, 1999; Salonen et al, 2004; Pakucs,
2003; Nakano et al, 2008). Although those modules
are referred to with various names in that literature,
we call them domain experts in this paper. In this
architecture, when an input utterance is received, its
ASR results are sent to domain experts. They try to
understand the ASR results using their own knowl-
edge for understanding. The domain selector gathers
information from those experts and decides which
expert should deal with the utterance and then de-
cide on the system utterances. In this paper, the do-
main expert engaging in understanding user utter-
ances and deciding system utterances is called acti-
vated.
2.2 Example Systems
So far many multi-domain spoken dialogue sys-
tems based on distributed architecture have been
user utterance
information for domain selection
domain selector
activate/deactivate
system utterance(from the activatedexpert)
speech understanding utterance generation
domain expert 1
domain expert 2
domain expert 3
dialoguehistory
Figure 1: Distributed multi-domain spoken dialogue sys-
tem architecture.
built and have demonstrated their ability to engage
in dialogues in a variety of domains. For exam-
ple, several systems integrated information provid-
ing and database searches in multiple domains (Lin
et al, 1999; Komatani et al, 2006; O?Neill et al,
2004; Gustafson and Bell, 2000). Some other sys-
tems integrated domain experts that employ very
different dialogue strategies. Lee et al (2009) and
Nakano et al (2006) integrated task-oriented and
non-task-oriented dialogue managements. Nakano
et al (2008) integrated domain experts for not only
dialogues but also tasks requiring physical actions.
Below we explain an example system that we
used to collect dialogue data for the domain se-
lection experiment described in Section 5. It is a
Japanese system that can provide information on
UNESCOWorld Heritage Sites using speech, slides,
and Microsoft Agent1 gestures. It employs the fol-
lowing ten domain experts:
A question answering (QA) expert: It has a
database consisting of question-answer pairs on
World Heritage Sites (Narimatsu et al, 2010). Each
question-answer pair consists of a set of example
question sentences and answers to them. The an-
swers consist of one or more utterances, plus slides.
Keyphrase-based matching is used to select an ex-
ample question that matches the input utterance.
Eight interactive presentation (IP) experts: Each
of them explains in detail a famous World Her-
itage Site and adds a detailed explanation or skips
some explanation according to the user?s interrup-
tions (Nishimura et al, 2007). Patterns of interrup-
tion utterances are described in the expert?s knowl-
edge base.
1http://www.microsoft.com/MSAgent/
19
utterance expert
S1: Hello. I can answer questions about fa-
mous World Heritage Sites in Italy, Ger-
many, Greece, and other areas. I can also
explain some World Heritage Sites in de-
tail. Please ask me anything.
QA
U1: Show me the list of the World Heritage
Sites in Italy.
QA
S2: (show a slide of the list) I can tell you about
the World Heritage Sites in these areas in
Italy.
QA
U2: Can you show me the list of World Her-
itage Sites you can explain in detail?
QA
S3: (show a slide of the list) I can explain these
in detail.
QA
U3: Can you explain Maritime Greenwich? RU
S4: Maritime Greenwich, right? RU
U4: Yes. RU
S5: (show a slide on Greenwich) Okay. I?ll ex-
plain Maritime Greenwich, a historic port
town known for Greenwich Mean Time.
IP1
U5: What is Greenwich Mean Time? IP1
S6: Greenwich Mean Time is the mean solar
time at the Royal Greenwich Observatory.
IP1
S means system utterance and U means user utterance.
IP1 is one of the IP experts.
Figure 2: An example dialogue with the system for data
collection (translation from Japanese).
A request understanding (RU) expert: It under-
stands the user?s request to start one of the interac-
tive presentations explained above, and engages in
a dialogue to confirm the request. When the under-
standing finishes, the understood request is sent to
a module called task planner (Nakano et al, 2008;
Nakano et al, 2011). The task planner then activates
another expert to perform the requested presentation
(S5 in Figure 2).
Figure 2 shows an example dialogue between a
human and this system. Note that user utterances
are relatively short and include words related to spe-
cific World Heritage Sites or area names. If those
words are misrecognized, domain selection is diffi-
cult unless dialogue context information is used.
This figure also indicates the domain experts that
understood each user utterance and selected each
system utterance. The domain expert that should
deal with a user utterance is decided based on the set
of user utterances that the expert is designed to deal
with. The domains of utterances U1 and U3 are dif-
ferent because the QA expert has knowledge for un-
derstanding U1 and the RU expert has knowledge for
understanding U3. Thus, in this study, the domain of
each utterance is determined based on the design of
the experts employed in the system. If none of the
experts can deal with an utterance, it is considered
as an out-of-domain utterance. Sometimes the cor-
rect domain needs to be determined using contextual
information. For example, utterance U4 ?Yes? can
appear in all domains, but, since this is a reply to S4,
its domain is RU.
This definition of domain is different from that of
domain (or topic) recognition and adaptation stud-
ies in text, monologue, and human-human conver-
sation processing, in which reference domains are
annotated based on human perspectives rather than
system perspectives. From a human perspective, all
user utterances in Figure 2 may be in ?World Her-
itage Site? domain. However, it is not always easy
to build domain experts according to such domain
definitions, because different dialogue tasks in one
such domain may require different dialogue strate-
gies (such as question answering and request under-
standing).
2.3 Requirements for Domain Selection
We pursue a method for domain selection that can
be used in distributed architecture. Such a method
must satisfy the following two requirements.
Extensibility It must not detract from the extensi-
bility of distributed architecture, that is, any kind of
expert must be able to be incorporated, and each ex-
pert must be able to be designed independently from
other experts. This requires the interface between
each domain expert and the domain selector to be as
simple as possible.
Robustness It needs to be robust against ASR er-
rors; that is, the system needs to be able to avoid
erroneous domain transition caused by ASR errors.
3 Previous Work
So far various methods for domain selection have
been proposed, but, as far as we know, no method
satisfies both extensibility and robustness. Isobe et
al. (2003) estimate a score for each domain from the
20
ASR result and select the domain with the highest
score (hereafter referred to as RECSCORE). Since
each domain expert has only to output a numeric
score, it satisfies extensibility. However, because
this method does not take into account dialogue con-
text, it tends to erroneously shift domains when the
score of some experts becomes high by chance. For
example, if U4:?Yes? in Figure 2 is recognized as
?Italy? with a high recognition score in the QA ex-
pert, the domain erroneously shifts to QA and the
system explains about World Heritage Sites in Italy.
Thus this method is not robust.
To avoid erroneous domain shifts, Lin et al
(1999) give preference to the preceding domain
(the domain in which the previous system utterance
was made) by adding a certain value to the score
of the preceding domain (hereafter called REC-
SCORE+BIAS ). However, to what extent the do-
main tends to continue varies depending on the dia-
logue context. For example, if a dialogue task in one
domain finishes (e.g., when an IP expert finishes its
presentation and says ?This is the end of the presen-
tation. Do you have any questions??), the domain is
likely to shift. So, adding a fixed score does not al-
ways work. O?Neill et al?s (2004) system does not
change the dialogue domain until it finishes a task
in the domain, but it cannot recover from erroneous
domain shifts.
To achieve robustness against ASR errors, several
domain selection methods based on a classifier that
uses features concerning dialogue history as well as
ones concerning speech understanding results have
been developed (Komatani et al, 2006; Ikeda et al,
2008; Lee et al, 2009). These studies, however, use
some features available only in some specific type
of domain experts, such as features concerning slot-
filling, so they cannot be used with other kinds of
domain experts. That is, these methods do not sat-
isfy extensibility.
Methods that use classifiers based on word (and
n-gram) frequencies have been developed for utter-
ance classification (e.g., Chu-Carroll and Carpenter
(1999)), topic estimation for ASR of speech cor-
pora (e.g., Hsu and Glass (2006) and Heidel and
Lee (2007)) and human-human dialogues (Lane and
Kawahara, 2005). These methods can be applied to
domain selection in multi-domain spoken dialogue
systems. However, since they require training data
in the same set of domains as the target system, it
detracts from extensibility. In addition, they are not
robust because they cannot utilize a variety of di-
alogue and understanding related features. Word
frequencies are not always effective when two do-
mains share words as in our system described in Sec-
tion 2.2.
4 Proposed Framework
4.1 Basic Idea
To achieve extensibility, we need to restrict the infor-
mation that each expert sends to the domain selector
to a simple one such as numeric scores. Although
RECSCORE and RECSCORE+BIAS satisfy this, they
would not achieve high accuracy as explained above.
One possible extension to those methods to im-
prove accuracy is to use not only recognition scores
but also various expert-dependent features such as
ones concerning dialogue history and speech under-
standing. Each expert first estimates the probability
that the input utterance is in its domain using such
features, and then the expert with the highest proba-
bility is selected (hereafter called MAXPROB). This
method retains extensibility because the domain se-
lector does not directly use those expert-dependent
features. However, it suffers from the same prob-
lem as RECSCORE and RECSCORE+BIAS; if one of
the experts other than the preceding domain?s expert
outputs a high probability by mistake, the domain
shifts regardless of the dialogue state in the preced-
ing domain?s expert.
We focus attention on the fact that the domain
does not often shift. Our idea is to decide if the do-
main continues or not by using information available
in the preceding domain?s expert. This prevents er-
roneous domain shifts when the utterance is consid-
ered not to change the domain. When it is decided
that the currently active domain does not continue,
each remaining expert estimates the probability of
being newly activated using information available in
the expert, and the expert whose probability is the
highest is selected as the new domain expert.
We further refine this idea in two ways. One is by
taking into account how likely the input utterance is
to activate one of the other domain experts. We pro-
pose to use the maximum value of probabilities for
other experts? activation (maximum activation prob-
21
user utterance
expert for the preceding domain
experts for other domains
maximumprobability
select thepreceding domain
select the domain with maximumactivation probability
features
Stage 1
Stage 2
...
domain continuationdecision maker
select expert with maximumactivationprobability
speech understanding
features activation probability estimator
speech understanding
features activation probability estimator
speech understanding
decision is tocontinue? yes
no
out-of-domain
handle as an out-of-domain utterance
dialoguehistory
dialoguehistory
dialoguehistory
Figure 3: Two-stage domain selection framework.
ability) in the decision regarding domain continua-
tion. Since the maximum activation probability is
just a numeric score, this does not spoil extensibil-
ity. Unlike RECSCORE and RECSCORE+BIAS, in
our method, even if the maximum activation prob-
ability is very high, the preceding domain?s expert
can decide to continue or not to continue based on
its internal state. This makes it possible to retain ro-
bustness.
The other refinement is to explicitly deal with ut-
terances that are not in any domains (out-of-domain
(OOD) utterances). They include fillers and mur-
murs. They should be treated separately, because
they appear context-independently. So we make the
expert detect OOD utterances when deciding do-
main continuation. That is, it performs three-fold
classification, continue, not-continue, and OOD.
4.2 Two-Stage Domain Selection Framework
This idea can be summarized as a domain selection
framework which consists of two stages (Figure 3).
It assumes that each domain expert has two submod-
ules: activation probability estimator and a domain
continuation decision maker, which use information
available in the expert itself.
When a new input utterance is received, at Stage
1, the activation probability estimators of all non-
activated experts estimate probabilities and send
them to the domain selector. Then at Stage 2, the
domain selector sends their maximum value to the
expert of the preceding domain and asks it to decide
whether it continues to deal with the new input utter-
ances or does not continue, or it deals with the utter-
ance as out-of-domain. If it decides not to continue,
the domain selector selects the expert that outputs
the highest probability at Stage 1.
The reason we use the term ?framework? is that
it does not specify the details of the algorithm and
features used in each domain expert?s submodules
for domain selection. It rather specifies the inter-
faces of those submodules. Note that RECSCORE,
RECSCORE+BIAS, and MAXPROB can be consid-
ered as one of the implementations of this frame-
work. This framework, however, allows developers
to use a wider variety of features and gives flexibility
in designing those submodules.
5 Example Implementation and
Evaluation
Since the proposed framework is an extension of
the previous methods, if the activation probability
estimator and domain continuation decision maker
for each expert are designed well and trained using
enough data, it should outperform previous methods
that satisfy extensibility. We believe that this theo-
retical consideration and an experimental result us-
ing a human-system dialogue corpus show the via-
bility of the framework. Below we explain our im-
plementation and an experiment.
5.1 Data
For the implementation and evaluation, we used a
corpus of dialogues between human users and the
World Heritage Site information system described
in Section 2.2. Domain selection of this system was
performed using hand-crafted rules.
35 participants (17 males and 18 females) whose
ages range from 19 to 57 were asked to engage in
22
domain preceding training training test
domain data A data B data
RU RU 134 169 145
QA 51 102 59
IP 21 16 23
subtotal 206 287 227
QA RU 46 55 51
QA 783 870 888
IP 59 87 66
subtotal 888 1,012 1,005
IP RU 2 1 3
QA 7 11 18
IP 311 305 277
subtotal 320 317 298
OOD RU 24 19 39
QA 168 155 183
IP 66 68 113
subtotal 258 242 335
total 1,672 1,858 1,865
Table 1: Number of utterances in each domain in the
training and test data.
conversation with the system four times. Each ses-
sion lasted eight minutes. For each utterance, the
correct domain or an OOD label was manually an-
notated. We also annotated its preceding domain,
i.e., the domain in which the previous system utter-
ance was made. It can be different from the previous
user utterance?s domain because of the system?s er-
roneous domain selection. Utterances including re-
quests in two domains at the same time should be
given an OOD label but there are no such utterances.
We used data from 23 participants (3,530 utterances)
for training and those from the remaining 12 par-
ticipants (1,865 utterances) for testing. We further
split the training data into training data A (1,672
utterances) and B (1,858 utterances) to train each
of the two submodules. Each training data set in-
cludes data from two sessions for each participant.
Table 1 shows detailed numbers of utterances in the
data sets.
5.2 Implementation
5.2.1 Expert Classes
Among the ten experts, eight IP (Interactive Pre-
sentation) experts have the same dialogue strategy
and most of the predicted user utterance patterns. In
addition, the number of training utterances for each
expert class QA IP RU
LM for ASR trigram trigram finite-state
grammar
language keyphrase keyphrase finite-state
understanding -based -based transducer
vocabulary 1,140 407 79
size (word)
phone error 10.95 19.47 23.60
rate (%)
Table 2: Speech understanding in each expert.
IP expert?s domain is small. We therefore used all
training utterances in the IP domains to build a com-
mon ASR language model (LM), a common acti-
vation probability estimator, and a common domain
continuation decision maker for all IP experts. Here-
after we call the set of IP experts the IP expert class.
The RU (Request Understanding) expert and the QA
(Question Answer) expert are themselves also expert
classes.
5.2.2 Speech Understanding
For all experts, we used the Julius speech recog-
nizer and the acoustic model in the Japanese model
repository (Kawahara et al, 2004).2 Features of
speech understanding in each expert class are shown
in Table 2. Compared to the system used for data
collection, LMs are enhanced based on the training
data. We obtained the ASR performance on the ut-
terances in each domain in the test data in terms of
phone error rates. This is because Japanese has no
standard word boundaries so it is not easy to cor-
rectly compute word error rates. The poor perfor-
mance of ASR for IP is mainly due to the small
amount of training utterances for LM and that for
RU is mainly due to out-of-grammar utterances.
5.2.3 Stage 1
For Stage 1, we used logistic regression to es-
timate the probability that a non-activated expert
would be activated by a user utterance. Features for
logistic regression include those concerning speech
recognition and understanding results as well as dia-
logue history (see Table 5 for the full list of features).
These features are expert-dependent. This makes it
possible to estimate how the input utterance is suit-
2Multiple LMs can be used at the same time with Julius.
23
able to the dialogue context more precisely than us-
ing just features available in any kind of expert.
To train the activation probability estimators, we
fitted logistic regression coefficients using Weka
data mining toolkit ver.3.6.2 (Witten and Frank,
2005)3 and training data A. In the training for each
expert class, we used utterances whose preceding
domain was not that of the class because activation
probabilities are estimated only for such utterances
during domain selection. If the utterance is in a do-
main of the expert class, it is assigned an activate la-
bel and otherwise not-activate. Next, we performed
feature selection to avoid overfitting. We used back-
ward stepwise selection so that the weighted (by the
sizes of activate and not-activate labels) average of
the F1 scores for training set B could be maximized.
Table 6 lists the remaining features and their sig-
nificances in terms of the F1 score obtained when
each feature is removed. Then, we duplicated the
activate-labeled utterances in the training data A so
that the ratio of activate-labeled utterances to not-
activate-labeled utterances became 1 to 3. This is
because the training data include a larger number of
not-activate-labeled utterances and thus the results
would be biased. The ratio was decided by trial and
error so that the weighted average of the F1 scores
for training data B becomes high.
5.2.4 Stage 2
For Stage 2, we used multi-class support vector
machines (SVMs)4 to decide if the activated expert
should continue to be activated, should not continue,
or should regard the input utterance as OOD. We
used the same set of features as Stage 1 as well
as the maximum activation probability obtained at
Stage 1. The training data for the SVM of each ex-
pert class is the set of utterances in training data B
whose preceding domain is in that expert class, be-
cause domain continuation is decided only for such
utterances during domain selection. They are la-
beled continue, not-continue, or OOD. Next, we
performed backward stepwise feature selection so
that the weighted average of F1 scores for continue,
not-continue, and OOD utterance detection on train-
ing data A could be maximized. Remaining fea-
3Multinominal logistic regression model with a ridge esti-
mator with Weka?s default values.
4Weka?s SMO with the linear kernel and its default values.
tures are listed in Table 7. The maximum activa-
tion probability was found to be significant in all ex-
pert classes. This suggests our two-stage framework
that uses maximum activation probability is viable.
Then, we duplicated utterances with not-continue la-
bel and OOD label in the training data so that the
ratio of continue, not-continue, and OOD utterances
became 3:1:1. This is because the number of utter-
ances with the continue label is far greater than oth-
ers. The ratio was experimentally decided by trial
and error so that the weighted average of F1 scores
on training data A becomes high.
5.3 Evaluation
5.3.1 Compared Methods
We compared the full implementation described in
Section 5.2 (FULLIMPL hereafter) with the follow-
ing four methods which satisfy extensibility. Note
that the first three methods were mentioned in Sec-
tion 4.
RECSCORE: This chooses the expert class whose
recognition score is the maximum (Isobe et al,
2003). We used the ASR acoustic score normalized
by the duration of the utterance. If the IP expert class
was chosen, the IP expert that had been most re-
cently activated was chosen, because, in this system,
domain shifts to other IP experts never occur due to
the system constraints and the user did not try to do
it. If none of the experts had a higher score than a
fixed threshold, it recognized the utterance as OOD.
The threshold was experimentally determined using
the training data so that the weighted (by the sizes
of OOD and non-OOD utterances) average of the
F1 scores of OOD/non-OOD classification is max-
imized.
RECSCORE+BIAS: This is the same as REC-
SCORE except that a fixed value (bias) is added to
the score used in RECSCORE for the expert of the
preceding domain. This is basically the same as Lin
et al?s (1999) method but we use a different recog-
nition score since the recognition score they used
cannot be used in our system due to the difference
of speech understanding methods. The most appro-
priate bias for each expert class was decided using
the training data so that the weighted average of the
F1 scores could be maximized. OOD detection was
done in the same way as RECSCORE.
24
method class recall prec- F1 weighted
ision ave. F1
RECSCORE cont. 0.763 0.867 0.812
shift 0.559 0.239 0.335
OOD 0.501 0.848 0.630 0.789
RECSCORE cont. 0.917 0.824 0.868
+BIAS shift 0.400 0.421 0.410
OOD 0.501 0.848 0.630 0.838
MAXPROB cont. 0.925 0.843 0.882
shift 0.282 0.264 0.273
OOD 0.275 0.477 0.348 0.832
NOACTIV cont. 0.875 0.890 0.882
PROB shift 0.464 0.385 0.421
OOD 0.785 0.843 0.813 0.849
FULLIMPL cont. 0.902 0.907 0.904
shift 0.591 0.565 0.578
OOD 0.824 0.829 0.826 0.883
CLASSIFIER cont. 0.956 0.881 0.917
(reference) shift 0.545 0.759 0.635
OOD 0.755 0.885 0.815 0.899
Table 3: Evaluation results (?cont.? means ?continue.?).
MAXPROB: The activation probabilities for all ex-
perts were obtained using logistic regression and the
expert whose probability was the maximum was se-
lected. IP experts that had never been activated were
excluded because they cannot be activated due to
system constraint. For logistic regression, in addi-
tion to the features used in FULLIMPL, the previous
domain was used as a feature so that domain conti-
nuity was taken into account. Feature selection was
also performed. The probability that the utterance is
OOD was estimated in the same way using the fea-
tures concerning speech understanding. If the maxi-
mum probability of OOD detection was greater than
the maximum activation probability, then the utter-
ance was considered to be OOD.
NOACTIVPROB: This is the same as FULLIMPL
except that Stage 2 does not use the result of Stage
1, i.e., maximum activation probability.
5.3.2 Evaluation Results
To evaluate the domain selection, we focused on
domain shifts rather than the selected domain. We
classified the domain selection results into domain
continuations, domain shifts, and OOD utterance
detection. As the evaluation metric, we used the
weighted average of F1 scores for those classes.
Here the weight is the ratio of those classes of cor-
rect labels. Note that shifting to an incorrect do-
main is counted as a false positive when calculat-
ing precision for domain shifts. Table 3 shows the
results. In addition, the confusion matrices for the
three best methods are shown in Table 4. We found
FULLIMPL outperforms the other four methods. We
also found that the differences between the results of
the compared methods are all statistically significant
(p < .01) by two-tailed binomial tests.
For reference, we also evaluated a classifier-based
method that uses features from all the experts. Note
that this method does not satisfy extensibility be-
cause it requires training data in the same set of do-
mains as the target system. We evaluated this just
for estimating how well our proposed method works
while satisfying extensibility. It classifies each ut-
terance into one of four categories: the QA expert?s
domain, the RU expert?s domain, the most recently
activated IP expert?s domain, and OOD. If no IP ex-
pert has been activated before the utterance, three-
fold classification was performed. The training and
test data were split depending on whether one of the
IP experts has been activated before, and training
and testing were separately conducted. The training
data A was used for training SVM classifiers. Then
feature selection was performed using the training
data B. The performance of this method is shown
as CLASSIFIER in Tables 3 and 4. Although this
method outperforms FULLIMPL, FULLIMPL?s per-
formance is close to this method. This shows that
our method does not degrade its performance very
much even though it satisfies extensibility.
5.3.3 Discussion
One of the reasons why FULLIMPL outperforms
other methods is that its precision for domain shifts
is relatively higher than the other methods. This
suggests it can avoid erroneous domain shifts, thus
the proposed two-stage framework is more robust.
RECSCORE+BIAS performed relatively well despite
it used only limited features. We guess this is be-
cause adding preferences to the preceding domain
was effective since domain shifts are rare in these
data. Its low F1 score for OOD utterances suggests
using just recognition scores is insufficient to detect
them. The comparison of FULLIMPL with NOAC-
TIVPROB shows the effectiveness of using maxi-
mum activation probability in the second stage.
The F1 score for domain shifts is low even with
25
RECSCORE+BIAS:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,201 - 82 27 1,310
shift 115 88 14 3 220
OOD 142 - 25 168 335
total 1,458 88 121 198 1,865
NOACTIVPROB:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,146 - 123 41 1,310
shift 92 102 18 8 220
OOD 50 - 22 263 335
total 1,288 102 163 312 1,865
FULLIMPL:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,181 - 77 52 1,310
shift 70 130 15 5 220
OOD 51 - 8 276 335
total 1,302 130 100 333 1,865
CLASSIFIER (reference):
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,252 - 30 28 1310
shift 92 120 3 5 220
OOD 77 - 5 253 335
total 1,421 120 38 286 1,865
Table 4: Confusion matrices for the domain shifts.
FULLIMPL, although it is higher than those with
other methods. One typical reason for this is that
when one keyword in the ASR result of an utter-
ance to shift the domain is also in the vocabulary of
the preceding domain?s expert, the selection tends to
continue the previous domain by mistake. For ex-
ample, an utterance ?tell me about other World Her-
itage Sites? to shift from an IP domain to the QA
domain is sometimes misclassified as an IP domain
utterance, because ?World Heritage Sites? is also in
IP domains? vocabulary. We think this is because
the training data do not include a sufficient amount
of utterances that shift domains, and that a larger
amount of training data would solve this problem.
6 Concluding Remarks
This paper presented a novel general framework for
domain selection in extensible multi-domain spoken
dialogue systems. This framework makes it possi-
ble to build a robust domain selector because of its
flexibility in exploiting features and taking into ac-
count domain continuity. An experiment with data
collected with an example multi-domain system sup-
ported the viability of the proposed framework. We
believe that this framework will promote the devel-
opment of multi-domain spoken dialogue systems
and conversational robots/agents.
Among future work is to investigate how accurate
the activation probability estimator and the domain
continuation decision maker in each domain expert
should be for achieving a reasonable accuracy in do-
main selection. We also plan to conduct experiments
with systems that have a larger number of domain
experts to verify the scalability of this framework.
In addition, we will explore a way to estimate the
confidence of the domain selection to reduce erro-
neous domain selections.
Acknowledgments
The authors would like to thank Hiroshi Tsujino,
Yuji Hasegawa, and Hiromi Narimatsu for their sup-
port for this research.
References
Hideki Asoh, Toshihiro Matsui, John Fry, Futoshi Asano,
and Satoru Hayamizu. 1999. A spoken dialog system
for a mobile office robot. In Proc. 6th Eurospeech,
pages 1139?1142.
Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational
Linguistics, 25(3):361?388.
Joakim Gustafson and Linda Bell. 2000. Speech tech-
nology on trial: Experiences from the August system.
Natural Language Engineering, 6(3&4):273?286.
Aaron Heidel and Lin-shan Lee. 2007. Robust topic in-
ference for latent semantic language model adaptation.
In Proc. ASRU-07, pages 177?182.
Bo-June (Paul) Hsu and James Glass. 2006. Style and
topic language model adaptation using HMM-LDA.
In Proc. EMNLP ?06, pages 373?381,.
Satoshi Ikeda, Kazunori Komatani, Tetsuya Ogata, and
Hiroshi G. Okuno. 2008. Extensibility verification
26
of robust domain selection against out-of-grammar ut-
terances in multi-domain spoken dialogue system. In
Proc. Interspeech-2008 (ICSLP), pages 487?490.
T. Isobe, S. Hayakawa, H. Murao, T. Mizutani,
K. Takeda, and F. Itakura. 2003. A study on do-
main recognition of spoken dialogue systems. In Proc.
Eurospeech-2003, pages 1889?1892.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR engine Julius and
Japanese model repository. In Proc. Interspeech-2004
(ICSLP), pages 3069?3072.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robustness
against speech recognition errors. In Proc. 7th SIGdial
Workshop, pages 9?17.
Ian R. Lane and Tatsuya Kawahara. 2005. Incorporating
dialogue context and topic clustering in out-of-domain
detection. In Proc. ICASSP-2005, pages 1045?1048.
Cheongjae Lee, Sangkeun Jung, Seokhwan Kim, and
Gary Geunbae Lee. 2009. Example-based dialog
modeling for practical multi-domain dialog system.
Speech Communication, 51(5):466?484.
Bor-shen Lin, Hsin-ming Wang, and Lin-shan Lee. 1999.
A distributed architecture for cooperative spoken dia-
logue agents with coherent dialogue state and history.
In Proc. ASRU-99.
Michael F. McTear. 2004. Spoken Dialogue Technology.
Springer.
Mikio Nakano, Atsushi Hoshino, Johane Takeuchi,
Yuji Hasegawa, Toyotaka Torii, Kazuhiro Nakadai,
Kazuhiko Kato, and Hiroshi Tsujino. 2006. A robot
that can engage in both task-oriented and non-task-
oriented dialogues. In Proc. Humanoids-2006, pages
404?411.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. 9th SIGdial Workshop, pages 88?91.
Mikio Nakano, Yuji Hasegawa, Kotaro Funakoshi, Jo-
hane Takeuchi, Toyotaka Torii, Kazuhiro Nakadai,
Naoyuki Kanda, Kazunori Komatani, Hiroshi G.
Okuno, and Hiroshi Tsujino. 2011. A multi-expert
model for dialogue and behavior control of conversa-
tional robots and agents. Knowledge-Based Systems,
24(2):248?256.
Hiromi Narimatsu, Mikio Nakano, and Kotaro Fu-
nakoshi. 2010. A classifier-based approach to
supporting the augmentation of the question-answer
database for spoken dialogue systems. In Proc. 2nd
IWSDS, pages 182?187.
Yoshitaka Nishimura, Shinichiro Minotsu, Hiroshi Dohi,
Mitsuru Ishizuka, Mikio Nakano, Kotaro Funakoshi,
Johane Takeuchi, Yuji Hasegawa, and Hiroshi Tsujino.
2007. A markup language for describing interactive
humanoid robot presentations. In Proc. IUI?07, pages
333?336.
Ian O?Neill, Philip Hanna, Xingkun Liu, and Michael
McTear. 2004. Cross domain dialogue modelling:
an object-based approach. In Proc. Interspeech-2004
(ICSLP), pages 205?208.
Botond Pakucs. 2003. Towards dynamic multi-domain
dialogue processing. In Proc. Eurospeech-2003, pages
741?744.
Esa-Pekka Salonen, Mikko Hartikainen, Markku Tu-
runen, Jaakko Hakulinen, and J. Adam Funk. 2004.
Flexible dialogue management using distributed and
dynamic dialogue control. In Proc. Interspeech-2004
(ICSLP), pages 197?200.
Ian H.Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques, 2nd Edi-
tion. Morgan Kaufmann, San Francisco.
27
expert Features
class
all Fi,r1 If SRRi,1 is obtained or not
classes Fi,r2 If SRRi,1 contains a filler or not
i = ru, Fi,r3 min (CMs of words in SRRi,1)
ip, qa Fi,r4 avg (CMs of words in SRRi,1)
Fi,r5 (acoustic score of SRRi,1) / duration
Fi,r6 LM score of SRRi,1
Fi,r7 # of words in SRRi,1
Fi,r8 # of words in SRRi,all
Fi,r9 (Fi,r5 - (acoustic score of SRRlv,1))
/ duration
RU Fru,r10 If SRRru,1 is an affirmative response
Fru,r11 If SRRru,1 is a denial response
Fru,r12 # of ASR results with LMru
Fru,r13 If SRRru,1 contains the name of a
World Heritage Site
Fru,r14 max (CMs of words comprising the
name of a World Heritage Site)
Fru,r15 ave (CMs of words comprising the
name of a World Heritage Site)
Fru,h1 If SRRru,1 is an affirmative response
(Stage 2 only)
Fru,h2 # of turns since this expert is acti-
vated
Fru,h3 # of denial responses recognized
since this expert is activated
Fru,h4 Fru,h4/Fru,h3
Fru,h5 If the previous system utterance is a
confirmation request to a user request
for starting a presentation
Fru,h6 If the previous system utterance is
an utterance to react to a non-
understandable user utterance
Fru,h7 If the system has made a confirma-
tion request to a user request for start-
ing a presentation since this expert
was activated
Fru,h8 If the system has made an utterance
to react to a non-understandable user
utterance since this expert was acti-
vated
Fru,h9 If the system has made a confirma-
tion request to a user request for start-
ing a presentation before
Fru,h10 If the system has made an utterance
to react to a non-understandable user
utterance before
expert Features
class
IP Fip,r10 If the SRRip,1 is out of database
Fip,r11
P
j((# of keyphrases in SRRip,j) / (# of words in
SRRip,j) ) / (# of ASR results)
Fip,r12 mini( # of keyphrasei in SRRip,all / (# of ASR re-
sults))
Fip,r13 maxi( # of keyphrasei in SRRip,all / (# of ASR re-
sults))
Fip,r14 avg( CM of keyphrasei in SRRip,1)
Fip,r15 mini ( CM of keyphrasei in SRRip,1)
Fip,r16 maxi ( CM of keyphrasei in SRRip,1)
Fip,h1 If this expert has been activated before
Fip,h2 Same as Fru,h2
Fip,h3 If the previous system utterance is the final utter-
ance of the presentation
Fip,h4 If the previous system utterance is an utterance to
react to a user interruption
Fip,h5 Same as Fru,h6
Fip,h6 If the system has made the final utterance of the pre-
sentation since this expert was activated
Fip,h7 If the system has made an utterance to react to a user
interruption since this expert was activated
Fip,h8 Same as Fru,h8
Fip,h9 If the system has made the final utterance of the pre-
sentation before
Fip,h10 If the system has made an utterance to react to a user
interruption before
Fip,h11 Same as Fru,h10
QA Fqa,r10 Same as Fip,r12
Fqa,r11 Same as Fip,r13
Fqa,r12 Same as Fip,r14
Fqa,r13 Same as Fip,r15
Fqa,r14 Same as Fip,r16
Fqa,r15 Same as Fip,r17
Fqa,r16 If SRRqa,1 is an acknowledgment
Fqa,h1 Same as Fru,h1
Fqa,h2 Same as Fru,h2
Fqa,h3 Same as Fru,h3
Fqa,h4 Fqa,h4/Fqa,h3
Fqa,h5 If the previous system utterance is the final utter-
ance of an answer
Fqa,h6 Same as Fru,h6
Fqa,h7 If the system has made the final utterance of an an-
swer since this expert was activated
Fqa,h8 Same as Fru,h8
Fqa,h9 If the system has made the final utterance of an an-
swer before
Fqa,h10 Same as Fru,h10
SRRi,j means j-th speech recognition result with the language model (LM) for expert class i. SRRi,all means all the recognition
results in the n-best list. Fi,rx are speech understanding related features and Fi,hx are dialogue history related features. SRRlv,j
is an ASR result with a large-vocabulary (60,250 words) statistical model (Kawahara et al, 2004), which we used for utterance
verification. CM means confidence measure.
Table 5: Features used in the experiment.
28
expert class
(F1 score
obtained
after feature
selection)
remaining features (F1 score obtained
when each feature is removed)
RU(0.948) Fru,r9 (0.922), Fru,h8 (0.939), Fru,r5
(0.940), Fru,r14 (0.941), Fru,r2
(0.944), Fru,h9 (0.944), Fru,h5
(0.944), Fru,r13 (0.945), Fru,h10
(0.945), Fru,r10 (0.946), Fru,r8
(0.946), Fru,r7 (0.946)
IP(0.837) Fip,r7 (0.771), Fip,r6 (0.772), Fip,h9
(0.781), Fip,h7 (0.781), Fip,h11
(0.786), Fip,r4 (0.79), Fip,r2 (0.799),
Fip,r16 (0.809), Fip,r5 (0.809), Fip,r3
(0.809), Fip,h4 (0.809), Fip,r9 (0.814),
Fip,r15 (0.833), Fip,r12 (0.834), Fip,r13
(0.835), Fip,h10 (0.836)
QA(0.836) Fqa,r14 (0.813), Fqa,r7 (0.817),
Fqa,r16 (0.817), Fqa,r10 (0.818),
Fqa,h6 (0.820), Fqa,r6 (0.822), Fqa,r3
(0.831), Fqa,r5 (0.832)
Table 6: Features that remained after feature selection at
Stage 1 and their significances in terms of the F1 score
obtained when each feature is removed.
expert class
(F1 score
obtained
after feature
selection)
remaining features (F1 score obtained
when each feature is removed)
RU(0.773) Fru,r3 (0.728), Fru,a (0.737), Fru,h5
(0.743), Fru,h1 (0.751), Fru,r9 (0.754),
Fru,h10 (0.757), Fru,h8 (0.757),
Fru,r5 (0.758), Fru,r2 (0.759), Fru,r13
(0.762), Fru,r14 (0.763), Fru,h9
(0.767), Fru,r15 (0.768), Fru,r10
(0.768), Fru,h3 (0.772)
IP(0.827) Fip,h5 (0.808), Fip,r5 (0.809), Fip,r4
(0.810), Fip,r6 (0.811), Fip,a (0.812),
Fip,h4 (0.812), Fip,r13 (0.813), Fip,h3
(0.817), Fip,r15 (0.818), Fip,r3 (0.818),
Fip,h10 (0.819), Fip,r12 (0.820),
Fip,h7 (0.821), Fip,r11 (0.822), Fip,r10
(0.822), Fip,h8 (0.822), Fip,h6 (0.822),
Fip,r2 (0.824), Fip,r8 (0.824), Fip,h9
(0.824), Fip,h2 (0.825)
QA(0.873) Fqa,a (0.838), Fqa,r5 (0.857), Fqa,h1
(0.859), Fqa,r3 (0.862), Fqa,r6 (0.865),
Fqa,h8 (0.867), Fqa,r7 (0.868), Fqa,r15
(0.870), Fqa,r8 (0.870), Fqa,h7 (0.870),
Fqa,r12 (0.871), Fqa,r2 (0.871), Fqa,r16
(0.871), Fqa,h4 (0.871), Fqa,h3 (0.871),
Fqa,r11 (0.872), Fqa,h6 (0.872), Fqa,h5
(0.872)
Table 7: Features that remained after feature selection at
Stage 2 and their significances in terms of the F1 score
obtained when each feature is removed. Fru,a, Fip,a, and
Fqa,a are the maximum activation probabilities obtained
at Stage 1.
29
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 237?246,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Unified Probabilistic Approach to Referring Expressions
Kotaro Funakoshi Mikio Nakano
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako,
Saitama 351-0188, Japan
{funakoshi,nakano}@jp.honda-ri.com
Takenobu Tokunaga Ryu Iida
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8550, Japan
{take,ryu-i}@cl.cs.titech.ac.jp
Abstract
This paper proposes a probabilistic approach
to the resolution of referring expressions for
task-oriented dialogue systems. The approach
resolves descriptions, anaphora, and deixis in
a unified manner. In this approach, the notion
of reference domains serves an important role
to handle context-dependent attributes of enti-
ties and references to sets. The evaluation with
the REX-J corpus shows promising results.
1 Introduction
Referring expressions (REs) are expressions in-
tended by speakers to identify entities to hearers.
REs can be classified into three categories: descrip-
tions, anaphora, and deixis; and, in most cases,
have been studied within each category and with a
narrowly focused interest. Descriptive expressions
(such as ?the blue glass on the table?) exploit at-
tributes of entities and relations between them to
distinguish an entity from the rest. They are well
studied in natural language generation, e.g., (Dale
and Reiter, 1995; Krahmer et al, 2003; Dale and Vi-
ethen, 2009). Anaphoric expressions (such as ?it?)
refer to entities or concepts introduced in the pre-
ceding discourse and are studied mostly on textual
monologues, e.g., (Kamp and Reyle, 1993; Mitkov,
2002; Ng, 2010). Deictic (exophoric) expressions
(such as ?this one?) refer to entities outside the pre-
ceding discourse. They are often studied focusing
on pronouns accompanied with pointing gestures in
physical spaces, e.g., (Gieselmann, 2004).
Dialogue systems (DSs) as natural human-
machine (HM) interfaces are expected to han-
dle all the three categories of referring expres-
sions (Salmon-Alt and Romary, 2001). In fact, the
three categories are not mutually exclusive. To be
concrete, a descriptive expression in conversation is
either deictic or anaphoric. It is, however, not easy to
tell whether a RE is deictic or anaphoric in advance
of a resolution (regardless of whether the RE is de-
scriptive or not). Therefore, we propose a general
unified approach to the above three kinds of REs.
We employ a Bayesian network (BN) to model a
RE. Dealing with continuous information and vague
situations is critical to handle real world problems.
Probabilistic approaches enable this for reference re-
solvers. Each BN is dynamically constructed based
on the structural analysis result of a RE and contex-
tual information available at that moment. The BN
is used to estimate the probability with which the
corresponding RE refers to an entity.
One of the two major contributions of this paper is
our probabilistic formulation that handles the above
three kinds of REs in a unified manner. Previously
Iida et al (2010) proposed a quantitative approach
that handles anaphoric and deictic expressions in a
unified manner. However it lacks handling of de-
scriptive expressions. Our formulation subsumes
and extends it to handle descriptive REs. So far, no
previously proposed method for reference resolution
handles all three types of REs.
The other contribution is bringing reference
domains into that formulation. Reference do-
mains (Salmon-Alt and Romary, 2000) are sets of
referents implicitly presupposed at each use of REs.
By considering them, our approach can appropri-
ately interpret context-dependent attributes. In ad-
dition, by treating a reference domain as a referent,
REs referring to sets of entities are handled, too. As
far as the authors know, this work is the first that
takes a probabilistic approach to reference domains.
237
1.1 Reference domains
First, we explain reference domains concretely. Ref-
erence domains (RDs) (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
are theoretical constructs, which are basically sets
of entities presupposed at each use of REs. RDs in
the original literature are not mere sets of entities
but mental objects equipped with properties such
as type, focus, or saliency and internally structured
with partitions. In this paper, while we do not ex-
plicitly handle partitions, reference domains can be
nested as an approximation of partitioning, that is,
an entity included in a RD is either an individual en-
tity or another RD. Each RD d has its focus and de-
gree of saliency (a non-negative real number). Here-
after, two of them are denoted as foc(d) and sal(d)
respectively. RDs are sorted in descending order ac-
cording to saliency.
We illustrate reference domains with figure 1. It
shows a snapshot of solving a Tangram puzzle (the
puzzle and corpus are explained in section 3.1). RDs
are introduced into our mental spaces either linguis-
tically (by hearing a RE) or visually (by observing
a physical situation). If one says ?the two big tri-
angles? in the situation shown in figure 1, we will
recognize a RD consisting of pieces 1 and 2. If we
observe one moves piece 1 and attaches it to piece
2, we will perceptually recognize a RD consisting
of pieces 1, 2, and 6 due to proximity (Tho?risson,
1994). In a similar way, a RD consisting of pieces 5
and 7 also can be recognized. Hereafter, we indicate
a RD with the mark @ with an index, and denote
its elements by enclosing them with [ ]. E.g., @1 =
[1, 2], @2 = [1, 2, 6], @3 = [5, 7]. The focused en-
tity is marked by ?*?. Thus, foc([1?, 2]) = 1.
The referent of a RE depends on which RD is pre-
supposed. That is, if one presupposes @1 or @2, the
referent of ?the right piece? should be piece 1. If
one presupposes @3, the referent of the same RE
should be piece 5. This is the context-dependency
mentioned above.
Previous work on RDs (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
employ not probabilistic but formal approaches.
1.2 Probabilistic approaches to REs
Here, previous probabilistic approaches to REs are
explained and differences between ours and theirs
Figure 1: Tangram puzzle. (The labels 1 to 7 are for il-
lustration purposes and not visible to participants.)
are highlighted. Bayesian networks (Pearl, 1988;
Jensen and Nielsen, 2007) have been not often but
occasionally applied to problems in natural language
processing/computational linguistics since (Char-
niak and Goldman, 1989). With regard to REs,
Burger and Connolly (1992) proposed a BN special-
ized for anaphora resolution. Weissenbacher (2005;
2007) proposed a BN for the resolution of non-
anaphoric ?it? and also a BN for the resolution of
pronominal anaphora. They used pre-defined fixed
BNs for their tasks while our approach dynamically
tailors a BN for each RE.
Cho and Maida (1992) and Roy (2002) adopted
not exactly BNs but similar probabilistic approaches
for reference resolution and generation respectively.
However, their foci are only on descriptions.
Lison et al (2010) proposed an approach using
Markov logic networks (MLNs) (Richardson and
Domingos, 2006) to reference resolution. They
dealt with only deictic and descriptive REs. Even
though MLNs are also a probabilistic framework, it
is difficult for DS developers to provide quantitative
domain knowledge needed to resolve REs because
MLNs accept domain knowledge in the form of for-
mal logic rules with weights, which must be deter-
mined globally. In contrast, BNs are more flexible
and easy in providing quantitative knowledge to DSs
in the form of conditional probability tables, which
can be determined locally.
As just described, there are several probabilis-
tic approaches to REs but none of them incorpo-
rates reference domains. In the next section, we in-
troduce our REBNs (Referring Expression Bayesian
Networks), a novel Bayesian network-based model-
ing approach to REs that incorporates reference do-
mains.
238
W C X D
Figure 2: WCXD fundamental structure.
2 Bayesian Network-based Modeling of
Referring Expressions
Each REBN is dedicated for a RE in the context at
the moment. Its structure is determined by the syn-
tactic and semantic information in the RE and prob-
ability tables are determined by the context.
2.1 Structures
Figure 2 shows the fundamental network structure
of REBNs. We call this structure WCXD. The four
nodes (random variables)W ,C,X , andD represent
an observed word, the concept denoted by the word,
the referent of the RE, and the presupposed RD, re-
spectively. Here, a word means a lexical entry in
the system dictionary defined by the DS developer
(concept dictionary; section 3.2.1).
Each REBN is constructed by modifying or mul-
tiply connecting the WCXD structure as shown in
figures 3 and 4. Figure 3 shows the network for REs
indicating one referent such as ?that table.? EachWi
node has a corresponding word wi. Figure 4 shows
the network for REs indicating two referents such as
?his table.? We call the class of the former REs s-
REX (simple Referring EXpression) and the class of
the latter REs c-REX (compound Referring EXpres-
sion). Although REBNs have the potential to deal
with c-REX, hereafter we concentrate on s-REX be-
cause the page space is limited and the corpus used
for evaluation contains very few c-REX instances.
Although, in section 1, we explained that (Iida et
al., 2010) handles anaphoric and deictic expressions
in a unified manner, it handles anaphora to instances
only and does not handle that to concepts. There-
fore, it cannot satisfactorily resolve such an expres-
sion ?Bring me the red box, and the blue one, too.?
Here, ?one? does not refer to the physical referent
of ?the red box? but refers to the concept of ?box?.
TheC nodes will enable handling of such references
to concepts. This is one of the important features of
REBNs but will be investigated in future work.
W
1
C
1
X D
W
2
C
2
Figure 3: BN for two-word REs indicating one referent.
W
1
C
1
X
1
D
1
W
2
C
2
X
2
D
2
Figure 4: BN for two-word REs indicating two referents.
2.2 Domains of random variables
A REBN for an s-REX instance of N words
has 2N + 2 discrete random variables:
W1, . . . ,WN , C1, . . . , CN , X , and D. The do-
main of each variable depends on the corresponding
RE and the context at the moment. Here, D(V )
denotes the domain of a random variable V .
D(Wi) contains the corresponding observed word
wi and a special symbol ? that represents other pos-
sibilities, i.e., D(Wi) = {wi,?}. Each Wi has a
corresponding node Ci.
D(Ci) containsM concepts that can be expressed
by wi and a special concept ? that represents other
possibilities, i.e., D(Ci) = {c1i , . . . , cMi ,?}. cji
(j = 1 . . .M ) are looked up from the concept dic-
tionary (see section 3.2.1, table 2).
D(D) contains L + 1 RDs recognized up to that
point in time, i.e., D(D) = {@0,@1, . . . ,@L}. @0
is the ground domain that contains all the individ-
ual entities to be referred to in a dialogue. At the
beginning of the dialogue, D(D) = {@0}. Other
L RDs are incrementally added in the course of the
dialogue.
D(X) contains all the possible referents, i.e., K
individual entities and L + 1 RDs. Thus, D(X) =
{x1, . . . , xK ,@0, . . . ,@L}. Including RDs enables
handling of references to sets.
Then reference resolution is formalized as below:
x? = argmax
x?D(X)
P (X = x|W1 = w1, . . . ,WN = wN ). (1)
P (X|W1, . . . ,WN ) is obtained by marginalizing
the joint probabilities that are computed with the
probability tables described in the next subsection.
239
2.3 Probability tables
Probability distributions are given as (conditional)
probability tables since all the random variables
used in a REBN are discrete. Here, four types of
probability tables used by REBNs are described.
2.3.1 P (Wi|Ci, X)
P (Wi = w|Ci = c,X = x) is the probability that
a hearer observes w from c and x which the speaker
intends to indicate.
In most cases, Wi does not depend on X , i.e.,
P (Wi|Ci, X) ? P (Wi|Ci). X is, however, nec-
essary to handle individualized terms (names).
There are several conceivable ways of probabil-
ity assignment. One simple way is: for each cji ,
P (W = wi|C = cji ) = 1/T, P (W = ?|C =
cji ) = (T ? 1)/T , and for ?, P (W = wi|C =
?) = ", P (W = ?|C = ?) = 1 ? ". Here T is the
number of possible words for cji . " is a predefined
small number such as 10?8. We use this assignment
in the evaluation.
2.3.2 P (Ci|X,D)
P (Ci = c|X = x,D = d) is the probability that
concept c is chosen from D(Ci) to indicate x in d.
The developers of DSs cannot provide
P (Ci|X,D) in advance because D(Ci) is context-
dependent. Therefore, we take an approach of
composing P (Ci|X = x,D = d) from R(cji , x, d)
(cji ? D(Ci)\{?}). Here R(cji , x, d) is the rele-
vancy of concept cji to referent x with regard to d,
and 0 ? R(cji , x, d) ? 1. 1 means full relevancy
and 0 means no relevancy. 0.5 means neutral. For
example, a concept BOX will have a high relevancy
to a suitcase such as 0.8 but a concept BALL will
have a low relevancy to the suitcase such as 0.1.
If x is not in d, R(cji , x, d) is 0. Algorithm 1
in appendix A shows an algorithm to compose
P (Ci|X = x,D = d) from R(cji , x, d). Concept
? will be assigned a high probability if none of
cji ? D(Ci)\{?} has a high relevancy to x.
If cji is static,1 R(cji , x, d) is numerically given in
advance in the form of a table. If not static, it is im-
plemented as a function by the DS developer, that is,
R(cji , x, d) = fcji (x, d, I). Here I is all the informa-tion available from the DS.
1Whether a concept is static or not depends on each DS.
For example, given a situation such as shown in
figure 1, the relevancy function of a positional con-
cept LEFT (suppose a RE such as ?the left piece?)
can be implemented as below:
fLEFT(x, d, I) = (ux ? ur)/(ul ? ur). (2)
Here, ux, ul and ur are respectively the horizontal
coordinates of x, the leftmost piece in d, and the
rightmost piece in d, which are obtained from I . If
x is a RD, the relevancy is given as the average of
entities included in the RD.
2.3.3 P (X|D)
P (X = x|D = d) is the probability that entity x
in RD d is referred to, which is estimated according
to the contextual information at the time the corre-
sponding RE is uttered but irrespective of attributive
information in the RE. The contextual information
includes the history of referring so far (discourse)
and physical statuses such as the gaze of the referrer
(situation). We call P (X = x|D = d) the predic-
tion model.
The prediction model can be constructed by us-
ing a machine learning-based method. We use a
ranking-based method (Iida et al, 2010). The score
output by the method is input into the standard sig-
moid function and normalized to be a probability. If
x is not in d, P (X = x|D = d) is 0.
2.3.4 P (D)
P (D = d) is the probability that RD d is presup-
posed at the time the RE is uttered. We cannot col-
lect data to estimate this probabilistic model because
RDs are implicit. Therefore, we examine three a pri-
ori approximation functions based on the saliency of
d. Saliency is proportional to recency.2
Uniformmodel This model ignores saliency. This
is introduced to see the importance of saliency.
P (D = d) = 1/|D(D)| (3)
Linear model This model distributes probabilities
in proportion to saliency. This is an analogy of the
method used in (Denis, 2010).
P (D = d) = sal(d)?
d??D(D) sal(d?)
(4)
2Assignment of saliency is described in section 3.2.3.
240
Exponential model This model puts emphasis on
recent RDs. This function is so called soft-max.
P (D = d) = exp(sal(d))?
d??D(D) exp(sal(d?))
(5)
3 Experimental Evaluation
We evaluated the potential of the proposed frame-
work by using a situated human-human (HH) dia-
logue corpus.
3.1 Corpus
We used the REX-J Japanese referring expression
corpus (Spanger et al, 2010). The REX-J corpus
consists of 24 HH dialogues in each of which two
participants solve a Tangram puzzle of seven pieces
(see figure 1). The goal of the puzzle is combining
seven pieces to form a designated shape (such as a
swan). One of two subjects takes the role of opera-
tor (OP) and the other takes the role of solver (SV).
The OP can manipulate the virtual puzzle pieces dis-
played on a PC monitor by using a computer mouse
but does not know the goal shape. The SV knows
the goal shape but cannot manipulate the pieces. The
states of the pieces and the mouse cursor operated by
the OP are shared by the two subjects in real time.
Thus, the two participants weave a collaborative dia-
logue including many REs to the pieces. In addition
to REs, the positions and directions of the pieces, the
position of the mouse cursor, and the manipulation
by the OP were recorded with timestamps and the
IDs of relevant pieces.
3.1.1 Annotation
Each RE is annotated with its referent(s) as shown
in table 1. The 1st RE okkiisankaku3 big triangle ?a
big triangle? in the table is ambiguous and refers to
either piece 1 or 2. The 7th and 8th REs refer to
the set of pieces 1 and 2. The other REs refer to an
individual piece.
To skip the structural analysis of REs to avoid
problems due to errors in such analysis, we have
additionally annotated the corpus with intermediate
structures, from which REBNs are constructed. Be-
cause we focus on s-REX only in this paper, the
3Words are not separated by white spaces in Japanese.
intermediate structures are straightforward:4 paren-
thesized lists of separated words as shown in ta-
ble 1. The procedure to generate a REBN of s-REX
from such an intermediate structure is also straight-
forward and thus it is not explained due to the page
limitation.
3.2 Implementations
We use BNJ5 for probabilistic computation. Here
we describe the implementations of resources and
procedures that are more or less specific to the task
domain of REX-J.
3.2.1 Concept dictionary
Table 2 shows an excerpt of the concept dictio-
nary defined for REX-J. We manually defined 40
concepts by observing the dialogues.
3.2.2 Static relevancy table and relevancy
functions
For 13 concepts out of 40, their relevancy values
were manually determined by the authors. Table 3
shows an excerpt of the static relevancy table defined
for the seven pieces shown in figure 1. TRI is rele-
vant only to pieces 1 to 5, and SQR is relevant only
to pieces 6 and 7 but is not totally relevant to piece 7
because it is not a square in a precise sense. FIG is
equally but not very relevant to all the pieces,6
For the remaining 27 concepts, we implemented
relevancy functions (see appendix B).
3.2.3 Updating the list of RDs
In our experiment, REs are sequentially resolved
from the beginning of each dialogue in the corpus.
In the course of resolution, RDs are added into a list
and updated by the following procedure. RDs are
sorted in descending order according to saliency.
At each time of resolution, we assume that all the
previous REs are correctly resolved. Therefore, af-
ter each time of resolution, if the correct referent of
the last RE is a set, we add a new RD equivalent
to the set into the list of RDs, unless the list con-
tains another equivalent RD already. In either case,
the saliency of the RD equivalent to the set is set to
?+1 unless the RD is at the head of the list already.
4In the case of c-REX, graph-like structures are required.
5http://bnj.sourceforge.net/
6This is because concept FIG in REX-J is usually used to
refer to not a single piece but a shaped form (combined pieces).
241
D-ID Role Start End Referring expression Referents Intermediate structure
0801 SV 17.345 18.390 okkiisankaku big triangle 1 or 2 (okkii sankaku)
0801 SV 20.758 21.368 sore it 1 (sore)
0801 SV 23.394 24.720 migigawanookkiisankaku right big triangle 1 (migigawano okkii sankaku)
0801 SV 25.084 25.277 kore this 1 (kore)
0801 SV 26.512 26.671 sono that 1 (sono)
0801 SV 28.871 29.747 konookkiisankaku this big triangle 2 (kono okkii sankaku)
0801 OP 46.497 48.204 okkinasankakkei big triangle 1, 2 (okkina sankakkei)
0801 OP 51.958 52.228 ryo?ho? both 1, 2 (ryo?ho?)
?D-ID? means dialogue ID. ?Start? and ?End? mean the end points of a RE.
Table 1: Excerpt of the corpus annotation (w/ English literal translations).
Concept Words
TRI triangle, right triangle
SQR quadrate, square, regular tetragon
FIG figure, shape
Table 2: Dictionary (excerpted and translated in English).
Concept Relevancy values by piece(1) (2) (3) (4) (5) (6) (7)
TRI 1 1 1 1 1 0 0
SQR 0 0 0 0 0 1 0.8
FIG 0.3 0.3 0.3 0.3 0.3 0.3 0.3
Table 3: Static relevancy table.
Here, ? is the largest saliency value in the list at the
moment (the saliency value of the head RD).
Before each time of resolution, we check whether
the piece that is most recently manipulated after the
previous RE constitutes a perceptual group by using
the method explained in section 3.2.4 at the onset
time of the target RE. If such a group is recognized,
we add a new RD equivalent to the recognized group
unless the list contains another equivalent RD. In ei-
ther case, the saliency of the RD equivalent is set to
?+1 unless the RD is at the head of the list already,
and the focus of the equivalent RD is set to the most
recently manipulated piece.
When a new RD@m is added to the list, a comple-
mentary RD @n and a subsuming RD @l are also in-
serted just after @m in the list. Here, @n = @0\@m
and @l = [@m?,@n]. This operation is required to
handle a concept REST, e.g., ?the remaining pieces.?
3.2.4 Perceptual grouping
There is a generally available method of simulated
perceptual grouping (Tho?risson, 1994). It works
well in a spread situation such as shown in figure 1
but tends to produce results that do not match our
intuition when pieces are tightly packed at the end
of a dialogue. Therefore, we adopt a simple method
that recognizes a group when a piece is attached to
another. This method is less general but works sat-
isfactorily in the REX-J domain due to the nature of
the Tangram puzzle.
3.2.5 Ranking-based prediction model
As mentioned in section 2.3.3, a ranking-based
method (Iida et al, 2010) using SVMrank (Joachims,
2006) was adopted for constructing the prediction
model P (X|D). This model ranks entities accord-
ing to 16 binary features such as whether the tar-
get entity is previously referred to (a discourse fea-
ture), whether the target is under the mouse cursor
(a mouse cursor feature), etc.7
When a target is a set (i.e., a RD), discourse fea-
tures for it are computed as in the case of a piece;
meanwhile, mouse cursor features are handled in a
different manner. That is, if one of the group mem-
bers meets the criterion of a mouse cursor feature,
the group is judged as meeting the criterion.
In (Iida et al, 2010), preparing different models
for pronouns and non-pronouns achieved better per-
formance. Therefore we trained two linear kernel
SVM models for pronouns and non-pronouns with
the 24 dialogues.
3.3 Experiment
We used the 24 dialogues for evaluation.8 As men-
tioned in section 2.1, we focused on s-REX. These
24 dialogues contain 1,474 s-REX instances and 28
c-REX instances. In addition to c-REX, we ex-
cluded REs mentioning complicated concepts, for
which it is difficult to implement relevancy func-
tions in a short time.9 After excluding those REs,
7Following the results shown in (Iida et al, 2010), we did
not use the 6 manipulation-related features (CO1 . . . CO6).
8We used the same data to train the SVM-rank models. This
is equivalent to assuming that we have data large enough to sat-
urate the performance of the prediction model.
9Mostly, those are metaphors such as ?neck? and concepts
related to operations such as ?put.? For example, although
242
P (D) model Most-recent Mono-domain Uniform Linear Exponential
Category Single Plural Total Single Plural Total Single Plural Total Single Plural Total Single Plural Total
w/o S/P info. 42.4 28.8 40.0 77.5 47.3 73.3 77.1 40.6 72.0 78.3 45.1 73.7 76.2 48.4 72.3
w/ S/P info. 44.3 35.4 42.7 84.8 58.8 81.2 84.4 55.0 80.3 85.6 61.0 82.1 83.4 68.1 81.3
Table 4: Results of reference resolution (Accuracy in %).
1,310 REs were available. Out of the 1,310 REs, 182
REs (13.9%) refers to sets, and 612 REs (46.7%) are
demonstrative pronouns such as sore ?it.?
3.3.1 Settings
We presupposed the following conditions.
Speaker role independence: We assumed REs
are independent of speaker roles, i.e., SV and OP.
All REs were mixed and processed serially.
Perfect preprocessing and past information:
As mentioned in sections 3.1.1 and 3.2.3, we as-
sumed that no error comes from preprocessing in-
cluding speech recognition, morphological analysis,
and syntactic analysis;10 and all the correct referents
of past REs are known.11
No future information: In HH dialogue, some-
times information helpful for resolving a RE is pro-
vided after the RE is uttered. We, however, do not
consider such future information.
Numeral information: Many languages includ-
ing English grammatically require indication of nu-
meral distinctions by using such as articles, singu-
lar/plural forms of nouns and copulas, etc. Although
Japanese does not have such grammatical devices,12
it would be possible to predict such distinctions by
using a machine learning technique with linguistic
?putting a piece? and ?getting a piece out? are distinguished
due to speakers? intentions, they are (at least superficially) ho-
mogeneous in the physical data available from the corpus and
difficult for machines to distinguish each other.
10In general, the speech and expressions in human-machine
(HM) dialogue are less complex and less difficult to process
than those in HH dialogue data. This is typcially observed as
fewer disfluencies (Shriberg, 2001) and simpler sentences with
fewer omissions (Itoh et al, 2002). Therefore, when we apply
our framework to real DSs, we can expect clearer and simpler
input and thus better performance. We supposed that the condi-
tion of perfect preprocessing in HH dialogue approximates the
results to those obtained when HM dialogue data is used.
11If a reference is misinterpreted (i.e., wrongly resolved) in a
dialogue, usually that misinterpretation will be repaired by the
interlocutors in the succeeding interaction once the misinterpre-
tation becomes apparent. Therefore, accumulating all past er-
rors in resolution is rather irrational as an experimental setting.
12Japanese has a plurality marker -ra (e.g., sore-ra), but use
of it is not mandatory (except for personal pronouns).
and gestural information. Therefore we observed the
effect of providing such information. In the follow-
ing experiment we provide the singular/plural dis-
tinction information to REBNs by looking at the an-
notations of the correct referents in advance. This
is achieved by adding a special evidence node C0,
where D(C0) = {S,P}. P (C0 = S|X = x) = 1
and P (P|x) = 0 if x is a piece. On the contrary,
P (S|x) = 0 and P (P|x) = 1 if x is a set.
3.3.2 Baselines
To our best knowledge, there is no directly com-
parable method. We set up two baselines. The first
baseline uses the most recent as the resolved refer-
ent for each RE (Initial resolution of each dialogue
always fails). This baseline is called Most-recent.
As the second baseline, we prepared another
P (D) model in addition to those explained in sec-
tion 2.3.4, which is called Mono-domain. In Mono-
domain, D(D) consists of only a single RD @?0,
which contains individual pieces and the RDs recog-
nized up to that point in time. That is, @?0 = D(X).
Resolution using this model can be considered as
a straightforward extension of (Iida et al, 2010),
which enables handling of richer concepts in REs13
and handling of REs to sets14.
3.3.3 Results
The performance of reference resolution is pre-
sented by category and by condition in terms of ac-
curacy (# of correctly resolved REs/# of REs).
We set up the three categories in evaluating res-
olution, that is, Single, Plural, and Total. Category
Single is the collection of REs referring to a single
piece. Plural is the collection of REs referring to a
set of pieces. Total is the sum of them. Ambigu-
ous REs such as the first one in table 1 are counted
as ?Single? and the resolution of such a RE is con-
sidered correct if the resolved result is one of the
possible referents.
13(Iida et al, 2010) used only object types and sizes. Other
concepts such as LEFT were simply ignored.
14(Iida et al, 2010) did not deal with REs to sets.
243
?w/o S/P info.? indicates experimental results
without singular/plural distinction information. ?w/
S/P info.? indicates experimental results with it.
Table 4 shows the results of reference resolution
per P (D) modeling method.15 Obviously S/P infor-
mation has a significant impact.
While the best performance for category Single
was achieved with the Linear model, the best perfor-
mance for Plural was achieved with the Exponen-
tial model. If it is possible to know whether a RE
is of Single or Plural, that is, if S/P information is
available, we can choose a suitable P (D) model.
Therefore, by switching models, the best perfor-
mance of Total with S/P information reached 83.4%,
and a gain of 2.0 points against Mono-domain was
achieved (sign test, p < 0.0001).
Because the corpus did not include many in-
stances to which the notion of reference domains is
effective, the impact of RDs may appear small on the
whole. In fact, the impact was not small. By intro-
ducing RDs, resolution in category Plural achieved
a significant advancement. The highest gain from
Mono-domain was 9.3 points (sign test, p < 0.005).
Moreover, more REs containing positional concepts
such as LEFT and RIGHT were correctly resolved
in the cases of Uniform, Linear, and Exponential.
Table 5 summarizes the resolution results of four
positional concepts (with S/P information). While
Mono-domain resolved 65% of them, Linear cor-
rectly resolved 75% (sign test, p < 0.05).
As shown in table 4, the performance of the Uni-
form model was worse than that of Mono-domain.
This indicates that RDs introduced without an ap-
propriate management of them would be harmful
noise. Conversely, it also suggests that there might
be a room for improvement by looking deeply into
the management of RDs (e.g., forgetting old RDs).
4 Conclusion
This paper proposed a probabilistic approach to ref-
erence resolution, REBNs, which stands for Refer-
ring Expression Bayesian Networks. At each time
of resolution, a dedicated BN is constructed for the
15According to the results of preliminary experiments, even
in the case of the Uniform/Linear/Exponential models, we re-
solved the REs having demonstratives with the Mono-domain
model. This is in line with the finding of separating models
between pronouns and non-pronouns in (Iida et al, 2010).
Concept Count Mono Uni. Lin. Exp.
LEFT 21 11 12 16 13
RIGHT 33 23 23 25 27
UPPER 9 6 6 6 4
LOWER 6 5 4 5 4
Total 69 45 45 52 48
(Count means the numbers of occurrence of each concept. Mono, Uni.,
Lin., and Exp. correspond to Mono-domain, Uniform, Linear and Ex-
ponential.)
Table 5: Numbers of correctly resolved REs containing
positional concepts.
RE in question. The constructed BN deals with ei-
ther descriptive, deictic or anaphoric REs in a uni-
fied manner. REBNs incorporate the notion of ref-
erence domains (RDs), which enables the resolution
of REs with context-dependent attributes and han-
dling of REs to sets. REBNs are for task-oriented
dialogue systems and presuppose a certain amount
of domain-dependent manual implementation by de-
velopers. Therefore, REBNs would not be suited
to general text processing or non-task-oriented sys-
tems. However, REBNs have the potential to be a
standard approach that can be used for any and all
task-oriented applications such as personal agents in
smart phones, in-car systems, service robots, etc. ?
The proposed approach was evaluated with the
REX-J human-human dialogue corpus and promis-
ing results were obtained. The impact of incorpo-
rating RDs in the domain of the REX-J corpus was
recognizable but not so large on the whole. How-
ever, in other types of task domains where grouping
and comparisons of objects occur frequently, the im-
pact would be larger. Note that REBNs are not lim-
ited to Japanese, even though the evaluation used a
Japanese corpus. Evaluations with human-machine
dialogue are important future work.
Although this paper focused on the simple type of
REs without relations, REBNs are potentially able
to deal with complex REs with relations. The eval-
uation for complex REs is necessary to validate this
potential of REBN. Currently REBN assumes REs
whose referents are concrete entities. An extension
for handling abstract entities (Byron, 2002; Mu?ller,
2007) is important future work. Another direction
would be generating REs with REBNs. A generate-
and-test approach is a naive application of REBN
for generation. More efficient method is, however,
necessary.
244
References
John D. Burger and Dennis Connoly. 1992. Probabilistic
resolution of anaphoric reference. In Proceedings of
the AAAI Fall Symposium on Intelligent Probabilistic
Approaches to Natural Language, pages 17?24.
Donna Byron. 2002. Resolving pronominal reference
to abstract entities. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 80?87.
Eugene Charniak and Robert Goldman. 1989. A se-
mantics for probabilistic quantifier-free first-order lan-
guages with particular application to story understand-
ing. In Proceedings of the Eleventh International Joint
Conference on Artificial Intelligence (IJCAI), pages
1074?1079, Menlo Park, CA, USA.
Sehyeong Cho and Anthony Maida. 1992. Using a
Bayesian framework to identify the referent of definite
descriptions. In Proceedings of the AAAI Fall Sympo-
sium on Intelligent Probabilistic Approaches to Natu-
ral Language, pages 39?46.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the Gricean maxims in the generation
of referring expressions. Cognitive Science, 18:233?
263.
Robert Dale and Jette Viethen. 2009. Referring expres-
sion generation through attribute-based heuristics. In
Proceedings of the the 12th European Workshop on
Natural Language Generation (ENLG), pages 59?65,
Athens, Greece, March.
Alexandre Denis. 2010. Generating referring expres-
sions with reference domain theory. In Proceedings
of the 6th International Natural Language Generation
Conference (INLG), pages 27?35.
Petra Gieselmann. 2004. Reference resolution mech-
anisms in dialogue management. In Proceedings of
the 8th workshop on the semantics and pragmatics of
dialogue (CATALOG), pages 28?34, Barcelona, Italy,
July.
Ryu Iida, Shumpei Kobayashi, and Takenobu Tokunaga.
2010. Incorporating extra-linguistic information into
reference resolution in collaborative task dialogue. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1259?
1267, Uppsala, Sweden, July.
Toshihiko Itoh, Atsuhiko Kai, Tatsuhiro Konishi, and
Yukihiro Itoh. 2002. Linguistic and acoustic changes
of user?s utterances caused by different dialogue situa-
tions. In Proceedings of the 7th International Confer-
ence on Spoken Language Processing (ICSLP), pages
545?548.
Finn V. Jensen and Thomas D. Nielsen. 2007. Bayesian
Networks and Decision Graphs. Springer, second edi-
tion.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), pages
217?226, Philadelphia, PA, USA, August.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic Publishers.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29:53?72.
Pierre Lison, Carsten Ehrler, and Geert-Jan M. Kruijff.
2010. Belief modelling for situation awareness in
human-robot interaction. In Proceedings of the 19th
International Symposium on Robot and Human In-
teractive Communication (RO-MAN), pages 138?143,
Viareggio, Italy, September.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in
Language and Linguistics. Pearson Education.
Christoph Mu?ller. 2007. Resolving it, this, and that in
unrestricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 816?823.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo, CA, USA.
Matthew Richardson and Pedor Domingos. 2006.
Markov logic networks. Machine Learning, 62(1?
2):107?136.
Deb K. Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3):353?385.
Susanne Salmon-Alt and Laurent Romary. 2000. Gen-
erating referring expressions in multimodal contexts.
In Proceedings of the INLG 2000 workshop on Coher-
ence in Generated Multimedia, Mitzpe Ramon, Israel,
June.
Susanne Salmon-Alt and Laurent Romary. 2001. Ref-
erence resolution within the framework of cognitive
grammar. In Proceedings of the International Col-
loqium on Cognitive Science, San Sebastian, Spain,
May.
Elizabeth Shriberg. 2001. To ?errrr? is human: ecology
and acoustics of speech disfluencies. Journal of the
International Phonetic Association, 31(1):153?169.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, Takenobu
Tokunaga, Asuka Terai, and Naoko Kuriyama. 2010.
REX-J: Japanese referring expression corpus of sit-
uated dialogs. Language Resources and Evaluation.
Online First, DOI: 10.1007/s10579-010-9134-8.
245
Kristinn R. Tho?risson. 1994. Simulated perceptual
grouping: An application to human-computer interac-
tion. In Proceedings of the 16th Annual Conference
of the Cognitive Science Society, pages 876?881, At-
lanta, GA, USA.
Davy Weissenbacher and Adeline Nazarenko. 2007. A
Bayesian approach combining surface clues and lin-
guistic knowledge: Application to the anaphora reso-
lution problem. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing (RANLP), Borovets, Bulgaria.
Davy Weissenbacher. 2005. A Bayesian network for the
resolution of non-anaphoric pronoun it. In Proceed-
ings of the NIPS 2005 Workshop on Bayesian Meth-
ods for Natural Language Processing, Whistler, BC,
Canada.
A Algorithm to compose P (C|X,D)
Algorithm 1 Composing P (C|X = x,D = d).
Input: D(C); R(c, x, d) for all c ? D(C)\{?}
Output: P (C|X = x,D = d)
1: n ? 0, s ? 0, S = D(C)\{?}
2: for all c ? S do
3: r[c] ? R(c, x, d) #{Relevancy of concept c}
4: s ? s+ r[c] #{Sum of relevancy r[c]}
5: n ? n+ (1? r[c]) #{Sum of residual (1? r[c])}
6: end for
7: r[?] ? n/|S|
8: s ? s+ r[?]
9: for all c ? D(C) do
10: P (C = c|X = x,D = d) ? r[c]/s
11: end for
(#{. . . } is a comment.)
B Relevancy functions
As explained in section 2.3.2, the relevancy func-
tions for positional concepts such as LEFT and
RIGHT were implemented as geometric calcula-
tions. Here several other relevancy functions are
shown with corresponding example REs.
?this figure?:
R(FIG, x, d)
=
?
?
?
0.3 : if single(x)
1 : if not single(x) and shape(x)
0 : otherwise
(single(x) means x is a single piece. shape(x)
means x is a set of pieces that are concatenated and
form a shape. 0.3 comes from the static relevancy
table.)
?both the triangles?:
R(BOTH, x, d) =
{
1 : if |x| = 2
0 : otherwise
?another one?:
R(ANOTHER, x, d) =
{
1 : if foc(d) 6= x
0 : otherwise
?the remaining ones?:
R(REST, x, d) =
{
1 : if d = [x, y?]
0 : otherwise
(REST requires |d| = 2, and both x and y are sets.
ANOTHER does not.)
?all?:
R(ALL, x, d) =
{
1 : if x = d
0 : otherwise
(ALL does not always refer to @0.)
246
Proceedings of the SIGDIAL 2013 Conference, pages 369?371,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A Robotic Agent in a Virtual Environment that Performs Situated
Incremental Understanding of Navigational Utterances
Takashi Yamauchi
Seikei University
3-3-1 Kichijoji-Kitamachi
Musashino, Tokyo, Japan
dm126222@cc.seikei.ac.jp
Mikio Nakano
Honda Research Institute
Japan Co., Ltd.
8-1 Honcho, Wako
Wako, Saitama, Japan
nakano@jp.honda-ri.com
Kotaro Funakoshi
Honda Research Institute
Japan Co., Ltd.
8-1 Honcho, Wako
Wako, Saitama, Japan
funakoshi@jp.honda-ri.com
Abstract
We demonstrate a robotic agent in a 3D
virtual environment that understands hu-
man navigational instructions. Such an
agent needs to select actions based on not
only instructions but also situations. It
is also expected to immediately react to
the instructions. Our agent incrementally
understands spoken instructions and im-
mediately controls a mobile robot based
on the incremental understanding results
and situation information such as the lo-
cations of obstacles and moving history. It
can be used as an experimental system for
collecting human-robot interactions in dy-
namically changing situations.
1 Introduction
Movable robots are ones that can execute tasks
by moving around. If such robots can understand
spoken language navigational instructions, they
will become more useful and will be widely used.
However, spoken language instructions are some-
times ambiguous in that their meanings differ de-
pending on the situations such as robot and obsta-
cle locations, so it is not always easy to make them
understand spoken language instructions. More-
over, when they receive instructions while they are
moving and they understand instructions only af-
ter they finish, accurate understanding is not easy
since the situation may change during the instruc-
tion utterances.
Although there have been several pieces of
work on robots that receive linguistic navigational
instructions (Marge and Rudnicky, 2010; Tellex et
al., 2011), they try to understand instructions be-
fore moving and they do not deal with instructions
when situations dynamically change.
We will demonstrate a 3D virtual robotic system
that understands spoken language navigational in-
structions in a situation-dependent way. It incre-
mentally understands instructions so that it can un-
derstand them based on the situation at that point
in time when the instructions are made.
2 A Mobile Robot in a 3D Virtual
Environment
We use a robotic system that works in a virtual
environment built on top of SIROS (Raux, 2010),
which was originally developed for collecting di-
alogues between two participants who are engag-
ing in an online video game. As an example, a
convenience store environment was developed and
a corpus of interaction was collected (Raux and
Nakano, 2010). One of the participants, the oper-
ator, controls a (simulated) humanoid robot whose
role is to answer all customer requests. The other
participant plays the role of a remote manager who
sees the whole store but can only interact with
the operator through speech. The operator has the
robot view (whose field of view and depth are lim-
ited to simulate a robot?s vision) and the manager
has a birds-eye view of the store (Figure 1). Cus-
tomers randomly visit the store and make requests
at various locations. The manager guides the op-
erator towards customers needing attention. The
operator then answers the customer?s requests and
gets points for each satisfied request.
Using the virtual environment described above,
we have developed a system that operates the robot
according to the human manager?s instructions.
Currently we deal with only navigational instruc-
tions for moving the robot to a customer.
Figure 2 depicts the architecture for our system.
We use Sphinx-4 (Lamere et al, 2003) for speech
recognition. Its acoustic model is trained on the
Wall Street Journal Corpus and its trigram lan-
guage model was trained on 1,616 sentences in the
human-human dialogue corpus described above.
Its vocabulary size is 275 words. We use Festival
(Black et al, 2001) for speech synthesis.
369
Robot Clerk
Customers
Figure 1: The manager?s view of the convenience
store.
Speech Recognition
global context
Navigation Expertrobot moving history,robot & obstacle locations
Task Execution Expert
robot location,task info. task execution command
robot & obstacle locations,task info.
Dialogue and Behavior Controller (HRIME)speech recognition result
Manager ViewDisplay
SIROS Server
Speech Synthesisutterance text
utterance text
Figure 2: System architecture.
We use HRIME (HRI Intelligence Platform
based on Multiple Experts) (Nakano et al, 2008)
for dialogue and behavior control. In an HRIME
application, experts, which are modules dedicated
to specific tasks, are activated at appropriate times
and perform tasks. The navigation expert is ac-
tivated when the system receives a navigational
instruction. There are seven semantic categories
of instructions; they are turn-right, turn-left, go-
forward, go-back, repeat-the-previous-action, do-
the-opposite-action-of-the-previous-one, and stop.
Utterances that do not fall into any of these are ig-
nored. We assume that there are rules that match
linguistic patterns and those semantic categories.
For example, ?right? corresponds to turn-right,
and ?more? corresponds to repeat-the-previous-
action. The navigation expert sends the SIROS
server navigation commands based on the rec-
ognized semantic categories. Those commands
move the robot in the same way as a human op-
erator operates the robot using the keyboard, and
the results are shown on the display the manager
is watching. When the robot starts moving and it
cannot move because of an obstacle, it reports it to
the manager by sending its utterance to the speech
synthesizer.
When the robot has approached a customer who
is requesting help, the task is automatically per-
formed by the task execution expert.
The global context in the dialogue and behavior
controller stores information on the environment
which is obtained from the SIROS server, and it
can be used by the experts. As in the same way in
the human-human interaction, it holds information
only on customers and obstacles close to the robot
so that restricted robot vision can be simulated.
3 Situated Incremental Understanding
Sometimes manager utterances last without pauses
like ?right, right, more right, stop?, and the sit-
uation changes during the utterances because the
robot and the customers can move. So our sys-
tem employs incremental speech recognition and
moves the robot if a navigational instruction pat-
tern is found in the incremental output. To obtain
incremental speech recognition outputs, we em-
ployed InproTK (Baumann et al, 2010), which is
an extension to Sphinx-4. It enables the system
to receive tentative results every 10ms, which is a
hypothesis for the interval from the beginning of
speech to the point in time.
However, since incremental outputs are some-
times unstable and the instructions are ambiguous
in that the amount of movement is not specified,
not only incremental speech recognition outputs
but also obstacle locations and moving history is
used to determine the navigation commands.
In our system, the robot navigation expert re-
ceives incremental recognition results and if it
finds a navigational instruction pattern, it consults
the situation information in the global context,
and issues a navigation command based on sev-
eral situation-dependent understanding rules that
are manually written. Below are examples.
? If there is an obstacle in the direction that the
recognized instruction indicates, ignore the
recognized instruction. For example, when
?go forward? is recognized but there is an ob-
stacle ahead, it is guessed that the recognition
result was an error.
370
Turn left Turn to the left Go straight
I?m turning left I?m turning left I?m going forward9.95 11.135.873.41 4.39 4.77
9.80 12.139.132.43 3.12 4.32
3.45 7.54 16.159.81
Initial position Turn to the left Make the orientation parallel to an obstacle Go forward
Left turn Going forward
Manager?sutteranceRobot?sutteranceRobot?saction
Go straight3.46 12.13 14.0912.93I?m going forward
Robotorientation
Figure 3: Interaction example.
? When rotating, adjust the degree of rotation
so that the resulting orientation becomes par-
allel to obstacles such as a display shelf. This
enables the robot to smoothly go down the
aisles.
Figure 3 shows an example interaction. In the
demonstration, we will show how the robot moves
according to the spoken instructions by a human
looking at the manager display. We will compare
our system with its non-incremental version and a
version that does not use situation-dependent un-
derstanding rules to show how incremental situ-
ated understanding is effective.
4 Future Work
We are using this system for collecting a corpus
of human-robot interaction in dynamically chang-
ing situations so that we can analyze how hu-
mans make utterances in such situations. Fu-
ture work includes to make the system understand
more complicated utterances such as ?turn a lit-
tle bit to the left?. We are also planning to work
on automatically learning the situation-dependent
action selection rules from such a corpus (Vogel
and Jurafsky, 2010) to navigate the robot more
smoothly.
Acknowledgments
We thank Antoine Raux and Shun Sato for their
contribution to building the previous versions of
this system. Thanks also go to Timo Baumann
Okko Bu?, and David Schlangen for making their
InproTK available.
References
Timo Baumann, Okko Bu?, and David Schlangen.
2010. InproTK in Action: Open-Source Software
for Building German-Speaking Incremental Spoken
Dialogue Systems. In Proc. of ESSV.
Alan Black, Paul Taylor, Richard Caley, Rob Clark,
Korin Richmond, Simon King, Volker Strom,
and Heiga Zen. 2001. The Festival Speech
Synthesis System, Version 1.4.2. Unpublished
document available via http://www. cstr. ed. ac.
uk/projects/festival. html.
Paul Lamere, Philip Kwok, William Walker, Evandro
Gouvea, Rita Singh, Bhiksha Raj, and Peter Wolf.
2003. Design of the CMU Sphinx-4 decoder. In
Proc. of Eurospeech-2003.
Matthew Marge and Alexander I. Rudnicky. 2010.
Comparing spoken language route instructions for
robots across environment representations. In Proc.
of SIGDIAL-10.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for build-
ing conversational agents based on a multi-expert
model. In Proc. of SIGDIAL-08, pages 88?91.
Antoine Raux and Mikio Nakano. 2010. The dynam-
ics of action corrections in situated interaction. In
Proc. of SIGDIAL-10, pages 165?174.
Antoine Raux. 2010. SIROS: A framework for human-
robot interaction research in virtual worlds. In Proc.
of the AAAI 2010 Fall Symposium on Dialog with
Robots.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. of AAAI-2011.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proc. of ACL-2010,
pages 806?814.
371
